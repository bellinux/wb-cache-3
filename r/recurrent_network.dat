835|5520|Public
25|$|A {{precursor}} to this approach, {{and one of}} the first model types to account for the dimension of time in linguistic comprehension and production was Elman's simple <b>recurrent</b> <b>network</b> (SRN). By making use of a feedback network to represent the system's past states, SRNs were able in a word-prediction task to cluster input into self-organized grammatical categories based solely on statistical co-occurrence patterns.|$|E
50|$|Moreover, Schaffer {{collateral}} axons develop excitatory synapses {{that are}} {{scattered over the}} dendritic arborization of hippocampal CA1 pyramidal neurons. In the early stage of long-term potentiation, Schaffer collaterals release glutamate that binds to AMPA receptors of CA1-dendrites. The process of developing a network of CA3-to-CA1 recurrent excitatory glutamatergic synapses alters the frequency of spontaneous action potentials in Schaffer collaterals. By adulthood, CA3 <b>recurrent</b> <b>network</b> activity is reduced, the frequency of spontaneous action potentials is decreased in Schaffer collaterals, and a single release locus synapse with one dendritic spine on a given CA1 pyramidal neuron can be developed by Schaffer collateral axons.|$|E
50|$|The nodes {{are also}} {{connected}} to each other, thus they can send activation {{to one another}} like neurons. These connections can be unidirectional, creating a feedforward network, or they can be bidirectional, creating a <b>recurrent</b> <b>network.</b> Each of the connections between the nodes has a '`weight'`, or strength, {{and it is in}} these weights are where the knowledge is 'stored'. The weights act to multiply the output of a node. They can be excitatory (a positive value) or inhibitory (a negative value). For example, if a node has an output of 1.0 and it is connected to another node with a weight of -0.5 then the second node will receive an input signal of 1.0 &times; (-0.5) = -0.5. Since any one node can receive multiple inputs, the sum of all of these inputs must be taken to calculate the net input.|$|E
40|$|Holographic <b>Recurrent</b> <b>Networks</b> (HRNs) are <b>recurrent</b> <b>networks</b> which {{incorporate}} {{associative memory}} techniques for storing sequential structure. HRNs {{can be easily}} and quickly trained using gradient descent techniques to generate sequences of discrete outputs and trajectories through continuous space. The performance of HRNs {{is found to be}} superior to that of ordinary <b>recurrent</b> <b>networks</b> on these sequence generation tasks...|$|R
50|$|The on-line {{algorithm}} called causal recursive backpropagation (CRBP), implements and combines BPTT and RTRL paradigms for locally <b>recurrent</b> <b>networks.</b> It {{works with}} the most general locally <b>recurrent</b> <b>networks.</b> The CRBP algorithm can minimize the global error term. This fact improves stability of the algorithm, providing a unifying view on gradient calculation techniques for <b>recurrent</b> <b>networks</b> with local feedback.|$|R
40|$|M. Ing. This {{dissertation}} {{discusses the}} results of a literature survey into the theoretical aspects and development of <b>recurrent</b> neural <b>networks.</b> In particular, the various architectures and training algorithms developed for <b>recurrent</b> <b>networks</b> are discussed. The various characteristics of importance for the efficient implementation of <b>recurrent</b> neural <b>networks</b> to model dynamical nonlinear processes have also been investigated and are discussed. Process control has been identified as a field of application where <b>recurrent</b> <b>networks</b> may play an important role. The model based adaptive control strategy is briefly introduced and the application of <b>recurrent</b> <b>networks</b> to both the direct- and the indirect adaptive control strategy highlighted. In conclusion, the important areas of future research for the successful implementation of <b>recurrent</b> <b>networks</b> in adaptive nonlinear control are identifie...|$|R
50|$|Not {{only did}} the pseudo-recurrent model show reduced {{interference}} but also it models list-length and list-strength effects seen in humans. The list-length effect means that adding new items to a list harms the memory of earlier items. Like humans, the pseudo <b>recurrent</b> <b>network</b> showed a more gradual forgetting when to be trained list is lengthened. The list-strength effect means that when the strength of recognition for one item is increased, there is {{no effect on the}} recognition of the other list items. This is an important finding as other models often exhibit a decrease in the recognition of other list items when one list item is strengthened. Since the direct copying of weights from the early processing area to the final storage area does not seem highly biologically plausible, the transfer of information to the final storage area can be done through training the final storage area with pseudopatterns created by the early processing area. However, a disadvantage of the pseudo-recurrent model is that the number of hidden units in the early processing and final storage sub-networks must be identical.|$|E
5000|$|French (1997) {{proposed}} {{the idea of}} a pseudo-recurrent backpropagation network in order to help reduce catastrophic interference (see Figure 2). In this model the network is separated into two functionally distinct but interacting sub-networks. This model is biologically inspired and is based on research from McClelland, McNaughton, and O'Reilly (1995). In this research McClelland et al. (1995), suggested that the hippocampus and neocortex act as separable but complementary memory systems. Specifically, the hippocampus short term memory storage and acts gradually over time to transfer memories into the neocortex for long term memory storage. They suggest that the information that is stored can be [...] "brought back" [...] to the hippocampus during active rehearsal, reminiscence, and sleep and renewed activation is what acts to transfer the information to the neocortex over time. In the pseudo-recurrent network, one of the sub-networks acts as an early processing area, akin to the hippocampus, and functions to learn new input patters. The other sub-network acts as a final-storage area, akin to the neocortex. However, unlike in McClelland et al. (1995) model, the final-storage area sends internally generated representation back to the early processing area. This creates a <b>recurrent</b> <b>network.</b> French proposed that this interleaving of old representations with new representations {{is the only way to}} reduce radical forgetting. Since the brain would most likely not have access to the original input patterns, the patterns that would be fed back to the neocortex would be internally generated representations called pseudopatterns. These pseudopatterns are approximations of previous inputs and they can be interleaved with the learning of new inputs. The use of these pseudopatterns could be biologically plausible as parallels between the consolidation of learning that occurs during sleep and the use of interleaved pseudopatterns. Specifically, they both serve to integrate new information with old information without disruption of the old information. When given an input (and a teacher value) is fed into the pseudo-recurrent network would act as follows: ...|$|E
40|$|Differential {{geometry}} is used {{to investigate}} the structure of neural-network-based control systems. The key aspect is relative order—an invariant property of dynamic systems. Finite relative order allows the specification of a minimal architecture for a <b>recurrent</b> <b>network.</b> Any system with finite relative order has a left inverse. It is shown that a <b>recurrent</b> <b>network</b> with finite relative order has a local inverse that is also a <b>recurrent</b> <b>network</b> with the same weights. The results have implications {{for the use of}} recurrent networks in the inverse-model-based control of nonlinear systems...|$|E
40|$|This paper {{provides}} guidance {{to some of the}} concepts surrounding <b>recurrent</b> neural <b>networks.</b> Contrary to feedforward <b>networks,</b> <b>recurrent</b> <b>networks</b> can be sensitive, and be adapted to past inputs. Backpropagation learning is described for feedforward networks, adapted to suit our (probabilistic) modeling needs, and extended to cover <b>recurrent</b> <b>networks.</b> The aim of this brief paper is to set the scene for applying and understanding <b>recurrent</b> neural <b>networks...</b>|$|R
40|$|<b>Recurrent</b> neural <b>networks</b> are {{important}} models of neural computation. This work relates {{the power of}} them {{to those of other}} conventional models of computation like Turing machines and nite automata and proves results about their learning capabilities. Speci cally, it shows, (a) Probabilistic <b>recurrent</b> <b>networks</b> and Probabilistic turing machine models are equivalent. (b) Probabilistic <b>recurrent</b> <b>networks</b> with bounded error probabilities are no more powerful than determistic nite automata. (c) Deterministic <b>recurrent</b> <b>networks</b> have the capability of learning P-complete language problems (which are the hardest problems in P to parallelize). (d) Restricting the weight-threshold relationship in deterministic <b>recurrent</b> <b>networks</b> may allow the network to learn only weaker classes of languages (the NC class) ...|$|R
40|$|This thesis explores how <b>recurrent</b> neural <b>networks</b> can be {{exploited}} for learning certain high-dimensional mappings. <b>Recurrent</b> <b>networks</b> are {{shown to be}} as powerful as Turing machines {{in terms of the}} class of functions they can compute. Given this computational power, a natural question to ask is how <b>recurrent</b> <b>networks</b> can be used to simplify the problem of learning from examples. Some researchers have proposed using <b>recurrent</b> <b>networks</b> for learning fixed point mappings that can also be learned on a feedforward network even though learning algorithms for <b>recurrent</b> <b>networks</b> are more complex. An important question is whether <b>recurrent</b> <b>networks</b> provide an advantage over feedforward networks for such learning tasks. The main problem with learning high-dimensional functions is the curse of dimensionality which roughly states that the number of examples needed to learn a function increases exponentially with input dimension. Reducing the dimensionality of the function being learned is therefore [...] ...|$|R
40|$|In {{this paper}} {{differential}} geometry {{is used to}} investigate the structure of neural network based control systems. The key aspect is relative order, an invariant property, of dynamic systems. Finite relative order allows the specification of a minimal architecture for a <b>recurrent</b> <b>network.</b> Any system with finite relative order has a leftinverse. It is shown that a <b>recurrent</b> <b>network</b> with finite relative order has a local inverse that is also a <b>recurrent</b> <b>network</b> with the same weights. The results have implications {{for the use of}} recurrent networks in the inverse model based control of nonlinear systems...|$|E
40|$|This paper uses {{techniques}} from {{control theory}} {{in the analysis of}} trained recurrent neural networks. Differential geometry is used as a framework, which allows the concept of relative order to be applied to neural networks. Any system possessing finite relative order has a left-inverse. Any <b>recurrent</b> <b>network</b> with finite relative order also has an inverse, which is shown to be a <b>recurrent</b> <b>network...</b>|$|E
40|$|Abstract—this paper {{presents}} a multi-context <b>recurrent</b> <b>network</b> for time series analysis. While simple <b>recurrent</b> <b>network</b> (SRN) are very popular among recurrent neural networks, {{they still have}} some shortcomings in terms of learning speed and accuracy {{that need to be}} addressed. To solve these problems, we proposed a multi-context <b>recurrent</b> <b>network</b> (MCRN) with three different learning algorithms. The performance of this network is evaluated on some real-world application such as handwriting recognition and energy load forecasting. We study the performance of this network and we compared it to a very well established SRN. The experimental results showed that MCRN is very efficient and very well suited to time series analysis and its applications...|$|E
40|$|There {{are many}} {{variations}} on the topology of <b>recurrent</b> <b>networks.</b> Models with fullyconnected recurrent weights may not be superior to those models with sparsely connected recurrent weights in terms of capacity, time for training and generalization ability. In this paper, we show that the fully-connected <b>recurrent</b> <b>networks</b> {{can be reduced to}} functionally equivalent partially-connected <b>recurrent</b> <b>networks</b> with the <b>recurrent</b> weight matrices in the form of asymmetric connections, self-feedback connections, sub-groups connections, companion connections and band diagonal connections under certain conditions...|$|R
40|$|This paper {{gives an}} {{overview}} on learning and representation of discrete-time, discrete-space dynamical systems in discretetime, continuous-space <b>recurrent</b> neural <b>networks.</b> We limit our discussion to dynamical systems (<b>recurrent</b> neural <b>networks)</b> {{which can be}} represented as finite-state machines (e. g. discrete event systems [53]). In particular, we discuss how a symbolic representation of the learned states and dynamics can be extracted from trained neural networks, and how (partially) known deterministic finitestate automata (DFAs) can be encoded in <b>recurrent</b> <b>networks.</b> While the DFAs that can be learned exactly with <b>recurrent</b> neural <b>networks</b> are generally small ({{on the order of}} 20 states), there exist subclasses of DFAs with on the order of 1000 states that can be learned by small <b>recurrent</b> <b>networks.</b> However, recent work in natural language processing implies that <b>recurrent</b> <b>networks</b> can possibly learn larger state systems [35]. I. Introduction Answering the questions "What are the rul [...] ...|$|R
40|$|Holographic <b>Recurrent</b> <b>Networks</b> (HRNs) are <b>recurrent</b> <b>networks</b> which {{incorporate}} {{associative memory}} techniques for storing sequential structure. HRNs {{can be easily}} and quickly trained using gradient descent techniques to generate sequences of discrete outputs and trajectories through continuous space. The performance of HRNs {{is found to be}} superior to that of ordinary <b>recurrent</b> <b>networks</b> on these sequence generation tasks. 1 INTRODUCTION The representation and processing of data with complex structure in neural networks remains a challenge. In a previous paper [Plate, 1991 b] I described Holographic Reduced Representations (HRRs) which use circular-convolution associative-memory to embody sequential and recursive structure in fixed-width distributed representations. This paper introduces Holographic <b>Recurrent</b> <b>Networks</b> (HRNs), which are recurrent nets that incorporate these techniques for generating sequences of symbols or trajectories through continuous space. The recurrent componen [...] ...|$|R
3000|$|We now {{specifically}} {{consider a}} <b>recurrent</b> <b>network</b> of excitatory and inhibitory neurons, as discussed above. It {{is assumed that}} N [...]...|$|E
40|$|A <b>recurrent</b> <b>network</b> {{was trained}} from {{sentence}} examples to construct symbolic parses of sentence forms. Hundreds of sentences, representing significant syntactic complexity, were formulated and then divided into training and testing sets {{to evaluate the}} ability of a <b>recurrent</b> <b>network</b> to learn their structure. The network is shown to generalize well over test sentences and the errors that do remain are found to be of a single type and related to human limitations of sentence processing...|$|E
40|$|This paper {{presents}} a nonlinear model for computing the time-dependent {{evolution of the}} variance in time series of returns on assets. First, we design a <b>recurrent</b> <b>network</b> representation of the variance, which extends the typically linear models. Second, we derive temporal training equations with which the network weights are inferred so as to maximize the likelihood of the data. Experimental results show that this dynamic <b>recurrent</b> <b>network</b> model yields results with improved statistical characteristics and economic performance...|$|E
40|$|<b>Recurrent</b> neural <b>networks</b> {{are complex}} {{parametric}} dynamic systems that can exhibit {{a wide range}} of different behavior. We consider the task of grammatical inference with <b>recurrent</b> neural <b>networks.</b> Specifically, we consider the task of classifying natural language sentences as grammatical or ungrammatical - can a <b>recurrent</b> neural <b>network</b> be made to exhibit the same kind of discriminatory power which is provided by the Principles and Parameters linguistic framework, or Government and Binding theory? We attempt to train a network, without the bifurcation into learned vs. innate components assumed by Chomsky, to produce the same judgments as native speakers on sharply grammatical/ungrammatical data. We consider how a <b>recurrent</b> neural <b>network</b> could possess linguistic capability, and investigate the properties of Elman, Narendra & Parthasarathy (N&P) and Williams & Zipser (W&Z) <b>recurrent</b> <b>networks,</b> and Frasconi-Gori-Soda (FGS) locally <b>recurrent</b> <b>networks</b> in this setting. We show that both Elman [...] ...|$|R
40|$|In {{this paper}} we explore {{continuous}} time <b>recurrent</b> <b>networks</b> for gram-matical induction. A higher-level generating/processing scheme {{can be used to}} tackle the grammar induction problem. Experiments are per-formed on several types of grammars, including a family of languages known as Tomita languages and a context-free language. The system and the experiments demonstrate that continuous time <b>recurrent</b> <b>networks</b> can learn certain grammatical induction tasks. ...|$|R
40|$|We {{introduce}} a recurrent architecture having a modular structure and we formulate a training procedure {{based on the}} EM algorithm. The resulting model has similarities to hidden Markov models, but supports <b>recurrent</b> <b>networks</b> processing style and allows to exploit the supervised learning paradigm while using maximum likelihood estimation. 1 INTRODUCTION Learning problems involving sequentially structured data cannot be effectively dealt with static models such as feedforward <b>networks.</b> <b>Recurrent</b> <b>networks</b> allow to model complex dynamical systems and can store and retrieve contextual information in a flexible way. Up until the present time, research efforts of supervised learning for <b>recurrent</b> <b>networks</b> have almost exclusively focused on error minimization by gradient descent methods. Although effective for learning short term memories, practical difficulties {{have been reported in}} training <b>recurrent</b> neural <b>networks</b> to perform tasks in which the temporal contingencies present in the input/ou [...] ...|$|R
3000|$|... {{is a pure}} {{modulation}} vector, {{with zero}} baseline. If the input is processed by a <b>recurrent</b> <b>network</b> that operates linearly on the input according to [...]...|$|E
40|$|This paper {{proposes a}} new Eye-based <b>Recurrent</b> <b>Network</b> Architecture (ERNA) for image classification. The new {{architecture}} is trained {{by a combination}} of Qlearning and RPROP. The classification performance is compared with other network architectures on the task of determining connectedness between pixels in small binary images. The experiments show that ERNA outperforms both the standard multi-layer perceptron network and the fully-connected <b>recurrent</b> <b>network</b> on the task mentioned above. This performance leads us {{to the conclusion that the}} eye facilitates learning in the topologically-structured domain of image classification...|$|E
40|$|Despite {{a variety}} of Artificial Neural Network (ANN) categories, Backpropagation Network (BP) and Elman <b>Recurrent</b> <b>Network</b> (ERN) are the {{widespread}} modus operandi in real applications. However, there are many drawbacks in BP network, for instance, confinement in finding local minimum and may get stuck at regions of a search space or trap in local minima. To solve these problems, various optimization techniques such as Particle Swarm Optimization (PSO) and Genetic Algorithm (GA) have been executed to improve ANN performance. In this study, we exploit errors optimization of Elman <b>Recurrent</b> <b>Network</b> with Backpropagation (ERNBP) and Elman <b>Recurrent</b> <b>Network</b> with Particle Swarm Optimization (ERNPSO) to probe the performance of both networks. The comparisons are done with PSO that is integrated with Neural Network (PSONN) and GA with Neural Network (GANN). The results show that ERNPSO furnishes promising outcomes in terms of classification accuracy and convergence rate compared to ERNBP, PSONN and GAN...|$|E
30|$|These {{problems}} can be overcome {{through the use of}} <b>recurrent</b> neural <b>networks</b> that do not transform temporal patterns into spatial ones. Rather, <b>recurrent</b> <b>networks</b> process a time-dependent signal one frame at a time, using feedback connections that create a memory of previous inputs.|$|R
40|$|In our {{research}} we evolve robot controllers for executing elementary behaviours. This paper {{focuses on the}} behaviour of pushing a box between two walls. Successful execution of this behaviour is critically dependent on the robot's ability {{to distinguish between the}} walls and the box. Using evolutionary algorithms, we optimised the box-pushing performances of two types of networks: feedforward <b>networks</b> and <b>recurrent</b> <b>networks.</b> In addition, we varied the nature of the input (with or without edge-detecting inputs), the number of hidden nodes and the transfer functions (linear vs. non-linear) in the <b>recurrent</b> <b>networks.</b> The best results were obtained with a <b>recurrent</b> neural <b>network</b> with three linear hidden nodes and edge-detecting inputs. We discuss the practical and biological implications of these results. We conclude by stating that (1) linear <b>recurrent</b> <b>networks</b> yield better box-pushing performance than feedforward <b>networks</b> or non-linear <b>recurrent</b> networks; (2) biological plausi [...] ...|$|R
40|$|This paper {{describes}} and {{evaluates the}} behavior of preference-based <b>recurrent</b> <b>networks</b> which process text sequences. First, we train a <b>recurrent</b> plausibility <b>network</b> to learn a semantic classication of the Reuters news title corpus. Then we analyze the robustness and incremental learning behavior of these networks in more detail. We demonstrate that these <b>recurrent</b> <b>networks</b> use their <b>recurrent</b> connections to support incremental processing. In particular, we compare {{the performance of the}} real title models with reversed title models and even random title models. We nd that the <b>recurrent</b> <b>networks</b> can, even under these severe conditions, provide good classication results. We claim that previous context in recurrent connections and a meaning spotting strategy are pursued by the network which supports this robust processing. 1 Introduction In the last decade, {{there has been a lot}} of work demonstrating that neural networks can be used for a wide variety of problem domains and ta [...] ...|$|R
40|$|Synaptic {{changes at}} sensory inputs to the dorsal {{nucleus of the}} lateral {{amygdala}} (LAd) {{play a key role}} in the acquisition and storage of associative fear memory. However, neither the temporal nor spatial architecture of the LAd network response to sensory signals is understood. We developed a method for the elucidation of network behavior. Using this approach, temporally patterned polysynaptic <b>recurrent</b> <b>network</b> responses were found in LAd (intra-LA), both in vitro and in vivo, in response to activation of thalamic sensory afferents. Potentiation of thalamic afferents resulted in a depression of intra-LA synaptic activity, indicating a homeostatic response to changes in synaptic strength within the LAd network. Additionally, the latencies of thalamic afferent triggered <b>recurrent</b> <b>network</b> activity within the LAd overlap with known later occurring cortical afferent latencies. Thus, this <b>recurrent</b> <b>network</b> may facilitate temporal coincidence of sensory afferents within LAd during associative learning...|$|E
40|$|Support Vector Machines {{are gaining}} {{more and more}} {{acceptance}} thanks to their success in many real-world problems. We propose in this work a solution for implementing SVM in hardware. The main idea {{is to use a}} <b>recurrent</b> <b>network</b> for SVM learning that guarantees the globally convergence to the optimal solution without the use of penalty terms. This network improves our and other authors' previous solutions. The <b>recurrent</b> <b>network</b> is suitable for a straightforward analog VLSI realization; the digital solution can be derived through a discretization (in time) of the circuit...|$|E
40|$|A {{method for}} storing {{sequences}} of varying lengths in a fixed-width vector is described. The method is implemented, in an adaptive form, in a <b>recurrent</b> <b>network</b> which learns to generate sequences. The performance of this network is {{compared with that}} of a more conventional <b>recurrent</b> <b>network</b> on the same task. 1 Introduction Sequence processing in recurrent networks often requires the storage of information about variable length sequences in the activations of a fixed-size group of units. However, there have been few studies of how this can be accomplished. In one paper on the topic, Simard and Le Cun [1992 a] reported difficulties with teaching a <b>recurrent</b> <b>network</b> to generate a variety of trajectories through a continuous space. As well as being interesting for its own sake, the problem of storing sequences in a fixed-size vector of activations is interesting for several other reasons: ffl the study of it might give some insight into learning difficulties in recurrent networks for se [...] ...|$|E
40|$|In the past, <b>recurrent</b> <b>networks</b> {{have been}} used mainly in neurocognitive or psycholinguistically {{oriented}} approaches of language processing. Here we examine <b>recurrent</b> neural <b>networks</b> for their potential in a dicult spoken language classification task. This paper describes an approach to learning classification of recorded operator assistance telephone utterances. We explore simple <b>recurrent</b> <b>networks</b> using a large, unique telecommunication corpus of spontaneous spoken language. Performance of the network indicates that a semantic SRN network is quite useful for learning classification of spontaneous spoken language in a robust manner, which may lead to their use in helpdesk call routing...|$|R
40|$|A set of {{simulations}} {{demonstrate that}} <b>recurrent</b> <b>networks</b> can exhibit generalization by abstraction from extremely sparse but structurally homogenous symbolic data. By cascading two <b>recurrent</b> <b>networks</b> [...] feeding the second network with discretized hidden {{states of the}} first [...] {{it is also possible}} to generalize according to complex structure. By automatic discretization the cascaded architecture assists in scaling up sequential learning tasks and offers explanations to the apparent systematicity and generativity of language use. ...|$|R
40|$|Abstract — <b>Recurrent</b> <b>networks</b> can {{generate}} spatiotemporal neural patterns that may look quite chaotic. Nonetheless a proximity measure between these patterns {{may be defined}} through comparison of the synaptic weight matrices that generate them. Following the Dynamic Neural Filter (DNF) formalism we demonstrate this concept by comparing teacher and student <b>recurrent</b> <b>networks.</b> We define an SVM variant of DNF in order to specify the optimal weight matrix {{and to deal with}} noise. I...|$|R
