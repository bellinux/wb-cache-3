17|120|Public
25|$|To {{ensure that}} it {{automatically}} runs every time Windows starts, it drops a copy of itself or its EXE component using a <b>random</b> <b>file</b> name into the %APPDATA% folder.|$|E
5000|$|Lisa Kudrow as Sheila. She {{appeared}} in the episode [...] "Mom Sizemore", in which Allen thought she was his biological mother. Allen brought her {{to live with him}} and Richard, which forced Jeremy to move out. When she started to get strict with him, Allen realized {{there was nothing wrong with}} Jeremy, so he got him back and she left. Later, Allen finds out that Carl just pulled a <b>random</b> <b>file</b> and that he still does not know who his real mother is.|$|E
5000|$|The DLL file {{contains}} {{the bulk of}} the virus code. The file with the extension [...] ".dl_" [...] is the compressed copy. Recent variants of Sality, such as Virus:Win32-Sality.AM, do not drop the DLL, but instead load it entirely in memory without writing it to disk. This variant, along with others, also drops a driver with a <b>random</b> <b>file</b> name in the folder %SYSTEM%\drivers. Other malware may also drop Sality in the computer. For example, a Sality variant detected as Virus:Win32-Sality.AU is dropped by Worm:Win32-Sality.AU. Some variants of Sality, may also include a rootkit by creating a device with the name Device\amsint32 or \DosDevices\amsint32.|$|E
40|$|A {{complete}} catalog {{is presented}} for the <b>random</b> access <b>files</b> {{used by the}} ATLAS integrated structural analysis and design system. ATLAS consists of several technical computation modules which output data matrices to corresponding <b>random</b> access <b>file.</b> A description of the matrices written on these files is contained herein...|$|R
5000|$|... {{collection}} uses downloadable {{software to}} scan users' hard drives, glean <b>random</b> <b>files,</b> and store the collected {{information on a}} shared server. The combined data is then displayed, creating what {{has been described as}} a virtual networked collective unconscious. It has been featured in Sydney, Barcelona, and in the 2002 Whitney Biennial.|$|R
40|$|A {{system of}} {{independent}} computer {{programs for the}} processing of digitized pulse code modulated (PCM) and frequency modulated (FM) data is described. Information is stored {{in a set of}} <b>random</b> <b>files</b> and accessed to produce both statistical and graphical output. The software system is designed primarily to present these reports within a twenty-four hour period for quick analysis of the helicopter's performance...|$|R
40|$|Abstract. A {{new data}} {{structure}} called interpolation search tree (1 ST) is presented which supports interpolation search and insertions and deletions. Amortized insertion and deletion cost is O(log n). The expected search {{time in a}} <b>random</b> <b>file</b> is O(log log n). This is not only true for the uniform distribution but for a wide class of probability distributions. Categories and Subject Descriptors: E. 1 [Data Structures]: trees; F. 2 [Analysis of Algorithms an...|$|E
40|$|Micro PL/CS is {{a version}} of PL/CS {{developed}} for a single-user, interactive environment. A file system extension makes PL/CS self-sufficient for standalone file processing and secondary storage management. The basis of the file system extension is the Unrestricted File Organization which provides a free mixture of sequential, indexed and <b>random</b> <b>file</b> operations. The structure, operation, and system-interfaced procedures of the UFO are presented and explained. The Micro-PL/CS file extension implementation is then sketched {{in terms of the}} UFO primitives...|$|E
40|$|AbstractA {{search tree}} grown from an n-long <b>random</b> <b>file</b> of {{numerical}} records is studied. Each node {{of the tree}} accommodates an ordered subfile consisting of at most (m− 1) records; no particular assumptions are made about how the local search within a node is executed. The depth and {{the total number of}} comparisons of the search are shown to be asymptotically Gaussian with means a 1 lnn, a 2 lnn, and covariance matrix ∥aijlnn∥. The a's depend on m and the first and second order moments of the local search time. The locally binary and sequential cases serve as an illustration...|$|E
3000|$|Source model B: Consider {{a spatial}} Gaussian random {{field in which}} the {{correlation}} function decays with distance d according to the squared exponential model [19]. We define the random vectors X 1 and X 2 to be observations picked-up {{by a pair of}} sensor arrays placed in this <b>random</b> <b>filed.</b> In this case, the auto-covariance matrix of X 1 is given by [...]...|$|R
40|$|Users rarely {{consider}} their media player {{as a security}} critical application. However, with an increasing amount of media content available on the web, users are exposing themselves to attack by downloading possibly malicious content. We focus on identifying vulnerabilities in three media formats (AVI, MPEG and Ogg) and two media players (MPlayer and VLC). We use a modification of traditional format-free fuzzing techniques to identify vulnerabilities in the format-strict environment of media players. We build upon typical fuzzing techniques by (1) adding structure to <b>random</b> <b>files</b> and (2) randomizing real files. We find that with these added techniques, fuzzing {{can be used to}} find bugs in applications with strict format requirements. Randomizing real files can, with no knowledge of file structure, identify a wide variety of bugs. While strategically adding structure to <b>random</b> <b>files</b> can produce a greater number of crashes, this was not correlated with finding a greater number of unique bugs. ...|$|R
40|$|Abstract — MPI-I/O {{defines a}} {{high-level}} file interface that enables multiple ranks {{to share a}} <b>random</b> access <b>file.</b> It would be highly attractive to grid-computing users that files are automatically partitioned and transfered to remote sites where their jobs can access the files through MPI-I/O. We are currently implementing in the AgentTeamwork grid-computing middleware system a series of these file-handling features including: file partitioning into strides, stride distribution to multiple processes, stride access through our MPI-I/O-oriented <b>random</b> access <b>file</b> class, stride exchange among processes, and barrier synchronization support. Particularly focusing on a design of our <b>random</b> access <b>file</b> class, this paper presents an implementation and performance results of AgentTeamwork’s file-partitioning and stride-maintenance schemes. I...|$|R
3000|$|For the privacy-preserving public {{auditing}} scheme, author feels soundness, completeness {{and data}} privacy are three security requirements. Completeness {{means that when}} interacting with the cloud server who keeps the data unchanged, the interactive protocol proof will always result in [...] P [...] V = 1 [...] when the cloud server and the TPA follow the protocol honestly. Thus, the concept of zero-knowledge data privacy helps to capture that the TPA learns no knowledge about the processed content except publically available information-based <b>random</b> <b>file</b> name. This is further strengthen {{on the basis of}} evaluation properties and proved the security including soundness is efficient and can be used in practice.|$|E
40|$|In {{this work}} we study the {{uncertainty}} of nuclear model parameters for neutron induced Fe- 56 reactions in the fast neutron region by using the Total Monte Carlo method. We perform {{a large number of}} TALYS runs and compare the calculated results with the experimental data of the cross sections to obtain the uncertainties of the model parameters. Based on the derived uncertainties another 1000 TALYS runs have been performed to create random cross section files. For comparison with the experimental data we calculate a weighted chi(2) value for each <b>random</b> <b>file</b> as well as the ENDF/B-VII. 1, JEFF- 3. 1, JENDL- 4. 0 and CENDL- 3. 1 data libraries. Furthermore, we investigate the optical model parameters correlation obtained by way of this procedure...|$|E
40|$|In this work, {{we study}} the {{uncertainty}} of nuclear model parameters for neutron induced ^ 56 Fe reactions in fast neutron region by using the Total Monte Carlo method. We perform {{a large number of}} TALYS runs and compare the calculated results with the experimental data of the cross sections to obtain the uncertainties of the model parameters. Based on the derived uncertainties another 1000 TALYS runs have been performed to create random cross section files. For comparison with the experimental data we calculate a weighted χ^ 2 value for each <b>random</b> <b>file</b> as well as the ENDF/B-VII. 1, JEFF 3. 1, JENDL 4. 0 and CENDL 3. 1 data libraries. Furthermore, we investigate the optical model parameters correlation obtained by way of this procedure. Comment: 7 pages, 3 figure...|$|E
50|$|Files in an OS4000 {{filesystem}} are typed, {{which means}} that the filesystem can hold several different types of file, and understands how the contents are structured. Most common are logical files which contain a record structure. These are split into sequential and <b>random</b> <b>files,</b> with <b>random</b> <b>files</b> having all records the same length to enable seeking to record numbers. Finally, text and binary files are distinguished, mainly to prevent applications which expect textual data from accidentally using a binary file. This results in a set of logical file types identified by three letters, e.g. Logical Sequential Text is LST. The logical file types are LST, LSB, LRT, LRB. The converse to logical files are physical files, which are accessed block at a time, and these are known as Physical <b>Random</b> Binary (PRB) <b>files.</b> File types PST, PSB, PRT also exist in theory, but have the same capabilities as PRB and are not generally used. Additionally, there is a Logical Indexed Sequential (LIS) filetype, which is an ISAM file and always appears to be sorted on its key field, and a Byte stream (BYT) filetype, which was added in Rel 6.5 to better support the OS4000 NFS server. A filetype CAT is used to hold catalogues—it is actually the same as an LSB file, but can only be modified by the filesystem itself.|$|R
40|$|In {{the present}} paper, a {{three-dimensional}} problem of bearing capacity of square footing on random soil medium is analyzed. The random fields of strength parameters c and φ are generated using LAS procedure (Local Average Subdivision, Fenton and Vanmarcke 1990). The procedure used is re-implemented {{by the authors}} in Mathematica environment in order to combine it with commercial program. Since the procedure is still tested the <b>random</b> <b>filed</b> has been assumed as one-dimensional: the strength properties of soil are random in vertical direction only...|$|R
40|$|We {{study the}} one {{dimensional}} Ising model with ferromagnetic, {{long range interaction}} which decays as |i-j|^{- 2 +a}, 1 / 2 < a< 1, {{in the presence of}} an external <b>random</b> <b>filed.</b> we assume that the random field is given by a collection of independent identically distributed random variables, subgaussian with mean zero. We show that for temperature and strength of the randomness (variance) small enough with P= 1 with respect to the distribution of the random fields {{there are at least two}} distinct extremal Gibbs measures...|$|R
40|$|Following the smashing {{success of}} XRootd-based USCMS data-federation, AAA project {{investigated}} {{extensions of the}} federation architecture by developing two sample implementations of an XRootd, disk-based, caching-proxy. The first one simply starts fetching a whole file {{as soon as a}} file-open request is received and is suitable when completely <b>random</b> <b>file</b> access is expected or it is already known that a whole file be read. The second implementation supports on-demand downloading of partial files. Extensions to the Hadoop file-system have been developed to allow foran immediate fallback to network access when local HDFS storage fails to provide the requested block. Tools needed to analyze and to tweak block replication factors and to inject downloaded blocks into a running HDFS installation have also been developed. Both cache implementations are in operation at UCSD and several tests were also performed at UNL and UW-M. Operational experience and applications to automatic storage healing and opportunistic computing, especially on T 3 sites and campus resources, will be discussed...|$|E
40|$|Aims: The {{comorbidity}} {{of substance}} use and mental health problems poses a significant challenge for alcohol and other drug (AOD) treatment services. In many cases, AOD practitioners do not have experience or training in identifying or managing mental health conditions. Methods: This project examined the implementation of screening and intervention practices for mental health disorders among AOD clients. Training and supervision was provided to 20 AOD practitioners across five sites in four agencies {{with a focus on}} enhancing skills in detection of, and intervention for, mental health conditions among their clients. A package developed for this purpose, known as PsyCheck, was used. A <b>random</b> <b>file</b> audit was undertaken to examine changes in detection of mental health conditions. Findings: There were significant improvements in detection after training and supervision, with detection rates almost doubling in this time. Conclusions: Training and supervision using the PsyCheck package appears to have the potential to improve mental health detection and intervention in AOD services. This study shows promise for the implementation of mental health intervention in AOD services...|$|E
40|$|Grid {{technology}} is developed to share data across many organizations in different geographical locations. The idea of replication is to store data into different locations to improve data access performance. When different sites hold replicas, {{there are significant}} benefits realized when selecting the best replica. Current research shows that both network bandwidth and disk I/O plays major role in file transfer. In this paper, we describe a new optimization technique that considers both disk throughput and network latencies when selecting the best replica. Previous history of data transfer can help in predicting the best site that can hold replica. The k-nearest neighbor rule is one such predictive technique. In this technique, when a new request arrives for the best replica, it looks at all previous data to find a subset of previous file requests {{that are similar to}} it and uses them to predict the best site that can hold the replica. In this work, we implement and test k-nearest algorithm for various file access patterns and compare results with the traditional replica catalog based model. The results demonstrate that our model outperforms the traditional model for sequential and unitary <b>random</b> <b>file</b> access requests...|$|E
40|$|A Method to {{determine}} sample size to estimate reliable characteristic value of soil parameters are proposed. It {{is based on}} the <b>random</b> <b>filed</b> theory and aim is to estimate a local average (LA) of the soil parameters. Concept of condition adn non-conditional estimatoin is introduced where the former is to estimate LA at the location where the observations are made, and the latter is a obtain LA at arbitrary locations within the same layer. It is emphasized that the required number of samples are quite different in these two situations...|$|R
40|$|Some {{criticism}} {{has been directed}} towards the Total Monte Carlo method because experimental information has not been {{taken into account in}} a statistically well-founded manner. In this work, a Bayesian calibration method is implemented by assigning weights to the <b>random</b> nuclear data <b>files</b> and the method is illustratively applied to a few applications. In some considered cases, the estimated nuclear data uncertainties are significantly reduced and the central values are significantly shifted. The study suggests that the method can be applied both to estimate uncertainties in a more justified way and in the search for better central values. Some improvements are however necessary; for example, the treatment of outliers and cross-experimental correlations should be more rigorous and <b>random</b> <b>files</b> that are intended to be prior files should be generated...|$|R
40|$|In this paper, {{we propose}} Markov random field models for pattern recognition, which provide a {{flexible}} and natural framework for modelling {{the interactions between}} spatially related random variables in their neighbourhood systems. The proposed approach is superior to conventional approaches in many aspects. This paper introduces the concept of states into Markov <b>random</b> <b>filed</b> models, presents a theoretic analysis of the approach, discusses issues of designing neighbourhood system and cliques, and analyses properties of the models. We have applied our method to the recognition of unconstrained handwritten numerals. The experimental {{results show that the}} proposed approach can achieve high performance...|$|R
40|$|In {{this paper}} we present the {{analysis}} of two large-scale network file system workloads. We measured CIFS traffic for two enterprise-class file servers deployed in the NetApp data center for a three month period. One file server was used by marketing, sales, and finance departments and the other by the engineering department. Together these systems represent over 22 TB of storage used by over 1500 employees, making this the first ever large-scale study of the CIFS protocol. We analyzed how our network file system workloads compared to those of previous file system trace studies and took an in-depth look at access, usage, and sharing patterns. We found that our workloads were quite different from those previously studied; for example, our analysis found increased read-write file access patterns, decreased read-write ratios, more <b>random</b> <b>file</b> access, and longer file lifetimes. In addition, we found a number of interesting properties regarding file sharing, file re-use, and the access patterns of file types and users, showing that modern file system workload {{has changed in the}} past 5 – 10 years. This change in workload characteristics has implications on the future design of network file systems, which we describe in the paper. ...|$|E
40|$|With {{the rapid}} {{increases}} in processor speed, disk I/Os will eventually become a system bottleneck. We have recently proposed a new disk I/O architecture called DCD (Disk Caching Disk) that can drastically improve disk I/O write performance {{as shown by}} simulation experiments [2, 4]. To validate whether DCD can {{live up to its}} promise in the real world, this paper presents a design and implementation of one DCD configuration as a filter driver under Windows NT, {{one of the most popular}} operating systems in today’s computing world. Performance measurements have been carried out using synthetic benchmarks consisting of <b>random</b> <b>file</b> writes. Experimental results show that DCD can improve the synchronous write performance by a factor of up to 13. 8 for intensive small write requests. In addition, because DCD uses part of hard disk as disk cache, it has excellent reliability both for meta-data and user data. Furthermore, the DCD driver is completely transparent to the upper file system (NTFS) and the lower level physical device. It does not require any modifications to the original OS nor the existing on-disk layout. As a result, it can be inserted into the existing NT driver hierarchy to obtain immediate performance and reliability improvements. ...|$|E
40|$|The way of data {{handling}} in personal computer has been treated. BASIC language {{is suitable for}} using in the field, because {{it is easy to}} use and modify its contents. However the time of executing some jobs in BASIC is longer than that of the compiler language, for example FORTRAN or C. In this study several examples are shown {{in order to reduce the}} time of {{data handling}}. It takes two minutes in BASIC to transfer 204 S data from AID converter (DM- 7100) to personal computer (PS-SO). The time can be reduced to three seconds if the part of data transfer routine is replaced to machine language routine. The BASIC commands, BSAVE and BLOAD, are convenient for treating array data, and need only few seconds to save or load 64 K byte data. The size of two dimensional integer array (ISO x ISO) in the floppy disk which BSAVE stores is 64 S 01 bytes. This size is one byte greater than that which is stored as random data file for the same array. The sequential data file is about three times greater in size than the <b>random</b> <b>file</b> and BSAVE file. Co-processor (SOS 7 or S 02 S 7) can improve efficiency in frequency analysis (FFT andInverse FFT) and terrain correction calculation. Machine language routines are examined in detail as to how replacing BASIC routine...|$|E
40|$|The JEFF library {{does not}} contain fission yield covariances, but simply best {{estimates}} and uncertainties. This situation is not unique as all libraries are facing this deficiency, firstly {{due to the lack}} of a defined format. An alternative approach is to provide a set of random fission yields, themselves reflecting covariance information. In this work, these <b>random</b> <b>files</b> are obtained combining the information from the JEFF library (fission yields and uncertainties) and the theoretical knowledge from the GEF code. Examples of this method are presented for the main actinides together with their impacts on simple burn-up and decay heat calculations...|$|R
50|$|The actual outputs {{are done}} by Appenders. There are {{numerous}} Appenders available, with descriptive names, such as FileAppender, RollingFileAppender, ConsoleAppender, SocketAppender, SyslogAppender, and SMTPAppender. Log4j 2 added Appenders that write to Apache Flume, the Java Persistence API, Apache Kafka, NoSQL databases, Memory-mapped <b>files,</b> <b>Random</b> Access <b>files</b> and ZeroMQ endpoints. Multiple Appenders can {{be attached to}} any Logger, so it's possible to log the same information to multiple outputs; for example to a file locally and to a socket listener on another computer.|$|R
40|$|The energy-energy and reaction-reaction {{covariance}} matrices {{were calculated}} for the n + 56 Fe damage cross-sections by Total Monte Carlo method using the TENDL- 2013 <b>random</b> <b>files.</b> They were represented in the ENDF- 6 format and added to the unperturbed evaluation file. The uncertainties for the spectrum averaged radiation quantities in the representative fission, fusion and spallation facilities were first time assessed as 5 – 25 %. Additional 5 to 20 % have {{to be added to}} the atom displacement rate uncertainties to account for accuracy of the primary defects simulation in materials. The reaction-reaction correlation were shown to be 1 % or less...|$|R
40|$|Thesis (M. Ing. (Electrical and Electronic Engineering)) [...] North-West University, Potchefstroom Campus, 2005. Process {{industries}} today enjoy significant {{benefits from}} advances {{made in the}} field of simulation as well as technologies that model normal plant operation. Plants however continue to suffer during abnormal operation such as start-up, shutdown and equipment failures leading to production losses or personal injury. One of the key elements that determines the operational safety of any plant is the training and technical knowledge of its personnel regarding plant behaviour. Simulators {{play a vital role in}} training personnel, preparing them for normal plant operation, abnormal plant operation as well as emergency and accident situations. This would not only enhance plant safety, but also decrease total downtime. In order to create such a simulator, mathematical models are required for each of the numerous components within the plant. This dissertation focuses on the development of a mathematical model for a control valve; a component commonly found in various industries to control and manipulate processes. Different modelling methods are compared, taking into account applicable modelling criteria such as training data, algorithm complexity, oscillations near endpoints, degree of system integration and model limitations. Based on these criteria, fuzzy logic with the nearest neighbourhood clustering algorithm is chosen as an appropriate modelling technique especially due to its ability to deal with large quantities of data. In order to meaningfully train the fuzzy logic system (FLS), a comprehensive set of physical operational data is required, covering all the different operational characteristics. To capture physical data, the development of a data acquisition (DAQ) system is introduced using two common DAQ systems to create a hybrid solution. Transducer signals are converted tom mA to V, using custom developed signal conversion hardware. This will allow data to be sampled by a standard DAQ card and processed by accompanying software. Two post- processing software applications are created. The first application solves the governing equations (mass rate of flow, Reynolds number, expansibility factor and choke status) and the second application is used to graphically display the acquired and calculated data. A set of experiments are conducted, covering all relevant working areas, to capture the behaviour of the control valve. This is achieved using five initial pressures ranging from 200 kPa to 400 kPa in increments of 50 kPa. At each initial pressure a set of unit step responses with valve command signals ranging from 0 % to 100 % in increments of 5 % is acquired. 24 data files at each initial pressure set (200, 250, 300, 350 and 400 kPa) are acquired. Before training the FLS, the optimal fuzzy logic parameters need to be determined e. g. radius (r), sigma (σ) the number of time delays, the time delay increments and the impact of the input signals. Determining these parameters is an iterative process. Only a single data set, with initial pressure of 300 kPa, is used to derive the optimal fuzzy logic parameters. Four performance criteria namely maximum error average (MEA), mean square error (MSE), root mean square error (RMSE) and coefficient of variation of the error residuals (CVRE) are used as benchmarks to obtain the optimal fuzzy parameters. During both the search for the optimal fuzzy parameters and the training of the fuzzy models using these optimal fuzzy parameters, 70 % of the data are used for training and verification while the remaining 30 % of the data are used for validation. Once the optimal fuzzy parameters are obtained using only the single data set, it is used to derive a number of fuzzy control valve models based on all the available data sets. All derived fuzzy models use the same parameters, except for a unique <b>random</b> <b>file</b> sequence associated with each of the models. The only prerequisite for the fuzzy models is that the generated file sequence be truly random. Irrespective of the <b>random</b> <b>file</b> sequence, fuzzy models with the same parameters, produce models with more or less the same performance. Therefore the performance criteria (MEA, MSE, RMSE and CVRE), for each data file, in the respective initial pressure sets, remains more or less the same. This method is found to be very useful in deriving a dynamic fuzzy logic control valve model. Averaging the performance criteria of these five models, an overall modelling accuracy of 90 % is achieved. It is recommended that a flow meter be installed to measure the mass rate of flow through the pipe network. This eliminates the need for an orifice, differential pressure transducer and the use of the first principle governing equation for the mass rate of flow. If a flow meter cannot be installed, a differential pressure transmitter with large range should be considered accompanied by a single orifice. Master...|$|E
50|$|Gaobot.ee is {{a variant}} of Agobot. It {{is also known as}} the W32.HLLW.Gaobot.EE. It is a {{malicious}} computer worm that tends to come from the P2P network Ares, installing from its virus form, Ares.exe. It has rather odd characteristics for a virus, with the unique ability to download and install <b>random</b> <b>files</b> (perhaps to create more sharers) from its members, such as music, pornography, and even full games. Gaobot.ee is a worm that sends large numbers of unsolicited e-mails using its own SMTP engine. This worm also opens a backdoor on a random TCP port, notifies attackers through a predetermined IRC channel, and attempts to terminate various security products and system monitoring tools.|$|R
5000|$|The term [...] "fuzzing" [...] {{originates}} from a 1988 class project, {{taught by}} Barton Miller at the University of Wisconsin. To fuzz test a Unix utility meant to automatically generate <b>random</b> <b>files</b> and command-line parameters for the utility. The project {{was designed to}} test the reliability of Unix programs by executing {{a large number of}} random inputs in quick succession until they crashed. It also provided early debugging tools to determine the cause and category of each detected failure. To allow other researchers to conduct similar experiments with other software, the source code of the tools, the test procedures, and the raw result data were made publicly available. Later, the term fuzzing was not limited only to command-line utilities.|$|R
5000|$|ISAM (an {{acronym for}} indexed {{sequential}} access method) is a method for creating, maintaining, and manipulating indexes of key-fields extracted from <b>random</b> data <b>file</b> records to achieve fast retrieval of required file records. IBM developed ISAM for mainframe computers. Today the term is used for several related concepts: ...|$|R
40|$|Magnetic Resonance Imaging {{is one of}} {{the most}} {{important}} medical imaging techniques for the investigating diseases of the human brain. A novel method for automatic segmentation Magnetic resonance brain image framework is proposed in this paper. This method consists of three-step segmentation procedures step. The method first uses level set method for the non-brain structures removal. Second, the bias correction method is based on computing estimates or tissue intensity distributions variation. Finally, we consider a statistical model method based on bayesian estimation, with prior Markov <b>random</b> <b>filed</b> models, for Magnetic resonance brain image classification. The algorithm consists of an energy function, based on the Potts model, which models the segmentation of an image. The algonthm was evaluated using simulated Magnetic resonance images and real Magnetic resonance brain images. Facultad de Informátic...|$|R
40|$|Abstract. A {{communication}} theory for a transmitter broadcasting to many receivers presented.   In this case, energetic considerations cannot neglected as in Shannon theory.   It is shown that, when energy {{is assigned to}} the information bit, information theory complies with classical thermodynamic and is part of it. To provide a thermodynamic theory of communication {{it is necessary to}} define equilibrium for informatics systems that are not in thermal equilibrium and to calculate temperature, heat, and entropy with accordance to Clausius inequality.   It shown that for a binary file, the temperature is proportional to the bit energy and that information is thermodynamic entropy.   Equilibrium exists in <b>random</b> <b>files</b> that cannot compressed. Thermodynamic bounds on the computing power of a physical device, and the maximum information that an antenna can broadcast are calculated. Keywords. Information theory, Thermodynamics, Entropy. JEL. C 62...|$|R
