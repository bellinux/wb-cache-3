115|4582|Public
5000|$|<b>Run</b> <b>Length</b> <b>Encoding</b> (RLE): This is a {{basic form}} of {{compression}} that is comparable to that used by standard Targa files.|$|E
5000|$|Scalable TrueType and OpenType fonts, device fonts (including double-byte), {{grayscale}} printing, font substitution, <b>run</b> <b>length</b> <b>encoding</b> (RLE), Tag Image File Format (TIFF) version 4.0, and Delta Row Compression (DRC).|$|E
50|$|The {{second stage}} {{performs}} <b>run</b> <b>length</b> <b>encoding</b> on the 32-bit pixel values. This has a limited {{impact on the}} size of most rendered images, but it is fast and simple.|$|E
50|$|Several {{compression}} schemes {{have been}} introduced in the ANIM format. Most of these are strictly of historical interest as the only one currently used is the vertical <b>run</b> <b>length</b> <b>encoded</b> byte encoding developed by Atari software programmer Jim Kent.|$|R
5000|$|ST412, an ST-506 variant was {{available}} in either MFM or RLL (<b>Run</b> <b>Length</b> Limited) <b>encoding</b> variants.|$|R
50|$|RLE {{may also}} be used to refer to an early {{graphics}} file format supported by CompuServe for compressing black and white images, but was widely supplanted by their later Graphics Interchange Format. RLE also refers to a little-used image format in Windows 3.x, with the extension rle, which is a <b>Run</b> <b>Length</b> <b>Encoded</b> Bitmap, used to compress the Windows 3.x startup screen.|$|R
50|$|The total bitmap uses between 32 and 272 bits {{of storage}} (4-34 bytes). For contrast, the DEFLATE {{algorithm}} would show {{the absence of}} symbols by encoding the symbols as having a (zero) bit-length with <b>Run</b> <b>Length</b> <b>Encoding</b> and additional Huffman coding.|$|E
50|$|Typically {{this process}} {{will result in}} {{matrices}} with values primarily in the upper left (low frequency) corner. By using a zig-zag ordering to group the non-zero entries and <b>run</b> <b>length</b> <b>encoding,</b> the quantized matrix can be much more efficiently stored than the non-quantized version.|$|E
50|$|This {{technique}} is relatively fast in software {{at the cost}} of less accurate sampling and potentially worse image quality compared to ray casting. There is memory overhead for storing multiple copies of the volume, for the ability to have near axis aligned volumes. This overhead can be mitigated using <b>run</b> <b>length</b> <b>encoding.</b>|$|E
50|$|First a wavelet {{transform}} is applied. This produces as many coefficients {{as there are}} pixels in the image (i.e., there is no compression yet since {{it is only a}} transform). These coefficients can then be compressed more easily because the information is statistically concentrated in just a few coefficients. This principle is called transform coding. After that, the coefficients are quantized and the quantized values are entropy <b>encoded</b> and/or <b>run</b> <b>length</b> <b>encoded.</b>|$|R
50|$|Ones-density {{is often}} {{controlled}} using precoding {{techniques such as}} <b>Run</b> <b>Length</b> Limited <b>encoding,</b> where the PCM code is expanded into a slightly longer code with a guaranteed bound on ones-density before modulation into the channel. In other cases, extra framing bits are added into the stream which guarantee at least occasional symbol transitions.|$|R
40|$|Given two strings, {{the longest}} common {{subsequence}} (LCS) problem computes a common subsequence {{that has the}} maximum length. In this paper, we present new and efficient algorithms for solving the LCS problem for two strings {{one of which is}} <b>run</b> <b>length</b> <b>encoded</b> (RLE). We first present an algorithm that runs in O(gN) time, where g is the length of the RLE string and N is the length of uncompressed string. Then based on the ideas of the above algorithm we present another algorithm that runs in O(Rlog(log g) +N) time, where R is the total number of ordered pairs of positions at which the two strings match. Our first algorithm matches the best algorithm in the literature for the same problem. On the other hand, for R < gN/ log(log) g, our second algorithm outperforms the best algorithms in the literature...|$|R
50|$|It is {{possible}} to create data structures and functions that manipulate them {{that do not have}} the problems associated with character termination and can in principle overcome length code bounds. It is also possible to optimize the string represented using techniques from <b>run</b> <b>length</b> <b>encoding</b> (replacing repeated characters by the character value and a length) and Hamming encoding.|$|E
50|$|The header {{consists}} of many parts and may include quantization tables and 2048 bits of user data. Each frame also has two GUIDs and timestamp. The frame header is packed into big-endian dwords. Actual frame data {{consists of}} packed macroblocks using a technique {{almost identical to}} JPEG: DC prediction and variable-length codes with <b>run</b> <b>length</b> <b>encoding</b> for other 63 coefficients.DC coefficient is not quantized.|$|E
5000|$|Sixel also {{included}} a rudimentary form of compression, using <b>run</b> <b>length</b> <b>encoding</b> (RLE). This was accomplished with the [...] character followed by a decimal number of the times to repeat, and then a single sixel character to be repeated. Since the [...] and decimal digits could not be valid sixel data, lying outside the encoded range, the encoding was easy to identify and expand back out in software.|$|E
40|$|Image {{compression}} is a {{key technology}} {{in the development of}} various multimedia and communication applications. In this paper, we have proposed a new algorithm of the image compression using byte compression technique. The encoding processes starts by implementing a modified decimal <b>Run</b> <b>Length</b> <b>Encode</b> (RLE). The RLE counts the occurrences of the pixel values of the original image and stores the occurrences in one byte with the pixels value to merge the two bytes in one byte (value, occurrences). The output from the previous step entered into another compression stage by applying Huffman's code that boosts the compression ratio of the image. To improve quality and compression ratio of the reconstructed image, an adaptive filter is implemented. The results show that the proposed algorithm provides superior performance in terms of compression ratio and exhibiting highest (PSNR) is retained for the image in additional to low Mean Square Error (MSE) ...|$|R
40|$|This paper {{introduces}} {{three techniques}} for autotuning a <b>run</b> <b>length</b> <b>encoded</b> (RLE) robotic vision system. These are a noise limiting technique and two iterative enlargement techniques, an incremental {{one and a}} global one. The incremental iterative enlargement update process is applied after each pixel is processed while the global iterative enlargement update process is applied after processing the whole image. A calibration technique for auto-positioning of the vision system’s camera is also presented {{as well as an}} automatic tracking algorithm for camera pan, tilt and zoom control. The image processing system discussed operates in realtime (16. 67 ms), within the 60 fields per second field rate of an NTSC video signal. The auto-tuning can take place online but concurrently in an asynchronous manner satisfying soft real-time constraints (66 ms). The colour signatures of the objects to be identified are YUV range values since they are more robust to light intensity variation than the RGB values given by the image capture card. Experiments show that the calibration and autotuning is reliable and robust, performing well in variations of light intensity of 800 - 2000 lux. Keywords: Real-time vision systems, calibration and auto-tuning 1...|$|R
40|$|Recent {{advances}} in {{software and hardware}} technology have made direct ray-traced volume rendering of 3 -d scalar data a feasible and effective method for imaging of the data’s con-tents. The time costs of these rendering techniques still do not permit full interaction with the data, {{and all of the}} pa-rameters effecting the resulting images. This paper presents a set of real-time interaction techniques which have been de-veloped to permit exploration of a volume data set. Within the limitation of a static viewpoint, the user is able to inter-actively alter the position and shape of an area of interest, and modify local viewing parameters. A <b>run</b> <b>length</b> <b>encoded</b> cache of volume rendering samples provides the means to rerender the volume at interactive rates. The user locates and plants “seeds ” in areas of interest through the use of data slicing and isosurface techniques. Image processing techniques applied to volumes (i. e. volume processing), can then automatically form regions of interest which in turn modify the rendering parameters. This “region growing” of “seedlings ” incrementally alters the image in real-time providing further visual cues concerning the contents of the data. These tools allow interactive exploration of internal structures in the data which may be obscured by other imag-ing algorithms. Magnetic Resonance Angiography (MRA) provides a driving application for this technology. Results from preliminary studies of MRA data are included. ...|$|R
50|$|Almost {{all data}} {{compression}} methods involve {{the use of}} a model, a prediction of the composition of the data. When the data matches the prediction made by the model, the encoder can usually transmit the content of the data at a lower information cost, by making reference to the model.This general statement is a bit misleading as general data compression algorithms would include the popular LZW and LZ77 algorithms,which are hardly comparable to compression techniques typically called adaptive.Run-length encoding and the typical JPEG compression with <b>run</b> <b>length</b> <b>encoding</b> and predefined Huffman codes do not transmit a model.A lot of other methods adapt their model to the current file and need to transmit it in addition to the encoded data, because both the encoder and the decoder need to use the model.|$|E
40|$|In this paper, we are {{evaluating}} {{the performance of}} Huffman and <b>Run</b> <b>Length</b> <b>Encoding</b> compression algorithms with multimedia data. We have used different types of multimedia formats such as images and text. Extensive experimentation with different file sizes {{was used to compare}} both algorithms {{evaluating the}} compression ratio and compression time. Huffman algorithm showed consistent performance compared to <b>Run</b> <b>Length</b> <b>encoding...</b>|$|E
40|$|While <b>run</b> <b>length</b> <b>encoding</b> is {{a popular}} {{technique}} for binary image compression, a raster (line by line) scanning technique is almost always assumed and scant {{attention has been given}} to the possibilities of using other techniques to scan an image as it is encoded. This thesis looks at five different image scanning techniques and how their relation ship to image features and scanning density (resolution) affects the overall compression that can be achieved with <b>run</b> <b>length</b> <b>encoding.</b> This thesis also compares the performance of <b>run</b> <b>length</b> <b>encoding</b> with an application of Huffman coding for binary image compression. To realize these goals a complete system of computer routines, the Image, Scanning and Compression (ISC) System has been developed and is now avail able for continued {{research in the area of}} binary image compression...|$|E
40|$|<b>Run</b> <b>length</b> {{codes are}} useful for the {{compression}} of binary data. Suppose you have a binary datavector. If the rst sample in the datavector is &quot;, {{then there will be}} positive integers r 1;r 2;:::;r k (the <b>run</b> <b>lengths)</b> such that the datavector will consist of r 1 zeroes, followed by r 2 ones, followed by r 3 zeroes, etc. If the rst sample is &quot;, then the only di erence will be that you have r 1 ones, followed by r 2 zeroes, etc. A <b>run</b> <b>length</b> code <b>encodes</b> the binary datavector via encoding of the sequence of <b>run</b> <b>lengths</b> r 1;r 2;:::;r k. (In addition, one would pre x the <b>run</b> <b>length</b> encoder output with a &quot; or &quot; to denote how the datavector begins.) To <b>encode</b> the <b>run</b> <b>lengths,</b> one could use a Hu man code, arithmetic code, or a prefabricated code which works well for a certain type of data. For a little bit better compression, one could separately encode the odd indexed <b>run</b> <b>lengths</b> (r 1;r 3;r 5;:::) and the even indexed runlengths (r 2;r 4;r 6;:::). In Section 7. 1, we apply <b>run</b> <b>length</b> codes to the compression of binary images. In Section 7. 2, we consider prefabricated <b>run</b> <b>length</b> codes. In 7. 3 - 7. 4, we cover techniques via which <b>run</b> <b>length</b> coding can be applied to gray level images...|$|R
40|$|This paper {{presents}} a new lossless binary image compression method. The method {{consists of four}} stages: preprocessing, quadtree compressing, <b>run</b> <b>length</b> compressing and statistical model-based compressing. The preprocessing stage {{is to reduce the}} entropy of a binary image. In the quadtree compressing stage, a breadth first traversal (BFT) linear quadtree is used to compress the image. The <b>run</b> <b>length</b> compressing stage uses the <b>run</b> <b>length</b> method to <b>encode</b> the tree list and colour list of the BFT linear quadtree. The statistical model-based compressing stage applies the Huffman coding algorithm to compress the remaining data of the BFT linear quadtree further. Experimental results show that this method can provide an impressive compression result...|$|R
40|$|Abstract. Data Compression {{is one of}} {{the most}} {{challenging}} arenas both for algorithm design and engineering. This is particularly true for Burrows and Wheeler Compression a technique that is important in itself and for the design of compressed indexes. There has been considerable debate on how to design and engineer compression algorithms based on the BWT paradigm. In particular, Move-to-Front Encoding is generally believed to be an “inefficient ” part of the Burrows-Wheeler compression process. However, only recently two theoretically superior alternatives to Move-to-Front have been proposed, namely Compression Boosting and Wavelet Trees. The main contribution of this paper is to provide the first experimental comparison of these three techniques, giving a much needed methodological contribution to the current debate. We do so by providing a carefully engineered compression boosting library that can be used, on the one hand, to investigate the myriad new compression algorithms that can be based on boosting, and on the other hand, to make the first experimental assessment of how Move-to-Front behaves with respect to its recently proposed competitors. The main conclusion is that Boosting, Wavelet Trees and Move-to-Front yield quite close compression performance. Finally, our extensive experimental study of boosting technique brings to light a new fact overlooked in 10 years of experiments in the area: a fast adapting order-zero compressor is enough to provide state of the art BWT compression by simply compressing the <b>run</b> <b>length</b> <b>encoded</b> transform. In other words, Move-to-Front, Wavelet Trees, and Boosters can all be by-passed by a fast learner...|$|R
40|$|This {{bachelor}} thesis {{deals with}} the compression of ECG signals using wavelet transform and <b>run</b> <b>length</b> <b>encoding.</b> The principles of electrocardiography, compression methods, wavelet transform and <b>run</b> <b>length</b> <b>encoding</b> are described in this thesis. There was created a program to compress and decompress ECG signals in MATLAB interface. The wavelet transform settings and their influence on compress ratio and percentage root mean square difference were tested. Subsequently, the appropriate adjustment of algoritm was found and it was used on CSE database compression...|$|E
40|$|Image {{compression}} is {{reducing the}} size in bytes of a graphics file without degrading {{the quality of}} the image to an unacceptable level. The reduction in file size allows more images to be stored in a given amount of disk drives or memory space. It also consumes the time required for images to be sent over the Internet or downloaded from Web pages. Data compression schemes give the optimized solution to transfer the data and store the data in secondary storage. In this paper we are going to propose new idea about enhanced version of <b>run</b> <b>length</b> <b>encoding</b> algorithm. We enhanced the draw backs of <b>run</b> <b>length</b> <b>encoding</b> scheme to provide an optimum compressio...|$|E
40|$|ABSTRACT. In {{lossless}} compression techniques, perfectly identical {{copy of the}} original image can be reconstructed from the compressed image. The paper implements three {{lossless compression}} techniques namely Huffman Encoding, <b>Run</b> <b>Length</b> <b>Encoding</b> and DPCM techniques using MPI. The experimental results show considerable reduction in execution time and better compression ratio for certain types of images...|$|E
5000|$|In {{computer}} science, group coded recording {{or group}} code recording (GCR) refers to several distinct but related encoding methods for magnetic media. The first, used in [...] bpi magnetic tape since 1973, is an error-correcting code combined with a <b>run</b> <b>length</b> limited (RLL) <b>encoding</b> scheme, belonging into the group of modulation codes. The others are different mainframe hard disk as well as floppy disk encoding methods used in some microcomputers until the late 1980s. GCR is a modified form of a NRZI code, but necessarily with a higher transition density.|$|R
30|$|In the characteristic-based {{compression}} (CBC) algorithm [10, 11], where different compression {{techniques used}} before {{the transfer of}} VM memory pages. Based {{on the type of}} pages, different techniques such as Wkdm for high similarity ratio pages, LZO for pages with low similarity were used to reduce their size. An improved algorithm was presented in [12] where memory-to-disk mappings were sent to the target instead of memory pages. From NAS the target host fetches the memory pages after deduplication with the help of NFS fetch queue. MDD (Migration with Data Deduplication) [6] was introduced in live migration for data deduplication of run-time memory image. Zero pages, similar pages were identified using hash-based fingerprints and were eliminated using RLE (<b>Run</b> <b>Length</b> <b>Encode).</b> To cut down the deduplication time multithreading was used by MDD. Extreme binning [13] in which Rabin fingerprints were used to identify the duplicated blocks. MD 5 and SHA collision resistant algorithms were used to find the fingerprints. VM scheduling strategies [14] combined with VM replication strategies were introduced to reduce migration latencies associated with live migration of VMs across WAN. Scalability and storage issues of VM images and were resolved by using a liquid distributed file system [15]. Gang Migration using global deduplication (GMGD) [16] was used to detect duplicates in VM memory pages and avoids their retransmission to target host in a cluster. Thus, reducing network overhead while running on several hosts. 42 % reduction [17] in migration time was achieved. An optimized Incremental Modulo-K(INC-K) algorithm [18], modulo arithmetic in nature was used for deduplication and achieved 66 % better deduplication ratio. In [19], Inner VM and cross-VM duplicates were removed by using a multi-level selective deduplication method. Parallelism was also increased by using local and global deduplication and a high deduplication ratio achieved. Different chunking strategies [20] were applied on various VM disk image dataset and proved that compression rate varied for different operating system versions, software configuration and achieved more savings in storage by identifying zero-filled blocks. In [21] heuristic prediction was used to determine non-duplicates by using adaptive block skipping, implemented on disk images of size 1 [*]TB which leads to maximum 40 % of space savings. Rapid Asymmetric Maximum (RAM) [22], a modified content defined chunking was used, which increases more throughput with less hash-based computations.|$|R
50|$|Since Golomb-Rice {{codes are}} quite inefficient for {{encoding}} low entropy distributions because the coding rate {{is at least}} one bit per symbol, significant redundancy may be produced because the smooth regions in an image can be encoded at less than 1 bit per symbol. To avoid having excess code length over the entropy, one can use alphabet extension which codes blocks of symbols instead of coding individual symbols. This spreads out the excess coding length over many symbols. This is the “run” mode of JPEG-LS and it is executed once a flat or smooth context region characterized by zero gradients is detected. A run of west symbol “a” is expected and the end of run occurs when a new symbol occurs or the end of line is reached. The total <b>run</b> of <b>length</b> is <b>encoded</b> and the encoder would return to the “regular” mode.|$|R
40|$|We {{analyze the}} {{connection}} between the autocorrelation of a binary sequence and its run structure given by the <b>run</b> <b>length</b> <b>encoding.</b> We show that both the periodic and the aperiodic autocorrelation of a binary sequence can be formulated in terms of the run structure. The run structure is given by the consecutive runs of the sequence. Let C=(C(0), C(1), [...] .,C(n)) denote the autocorrelation vector of a binary sequence. We prove that the kth component of the second order difference operator of C can be directly calculated by using the consecutive runs of total length k. In particular this shows that the kth autocorrelation is already determined by all consecutive runs of total length L<k. In the aperiodic case we show how the run vector R can be efficiently calculated and give a characterization of skew-symmetric sequences in terms of their <b>run</b> <b>length</b> <b>encoding.</b> Comment: [v 3]: minor revisions, accepted for publication in IEEE Trans. Inf. Theory, 17 page...|$|E
30|$|A set of pixels {{with the}} same gray level in each row is called a stroke and each stroke only records the gray level and the pixel length. Therefore, <b>run</b> <b>length</b> <b>encoding</b> {{can be used to}} compile the key {{information}} region of image. By using the range coding to compress the gray level region effectively, the intersection and union of the key information areas will be simplified.|$|E
40|$|This article {{describes}} improved algorithms for the <b>run</b> <b>length</b> <b>encoding,</b> inversion frequencies and weighted frequency count stages that follow the Burrows-Wheeler Transform. Results for compression rates are presented for different {{variations of the}} algorithm together with compression and decompression times. Finally, an implementation with a compression rate of 2. 238 bps on the Calgary Corpus is introduced, {{which is the best}} result published in this field to date...|$|E
5000|$|Additionally, the {{shortest}} possible <b>length</b> <b>encoding</b> {{must be used}} ...|$|R
40|$|Since the <b>run</b> <b>length</b> {{distribution}} is generally highly skewed, a significant concern about focusing {{too much on}} the average <b>run</b> <b>length</b> (ARL) criterion is that we may miss some crucial information about a control chart’s performance. Thus it is important to investigate the entire <b>run</b> <b>length</b> distribution of a control chart for an in-depth understanding before implementing the chart in process monitoring. In this paper, the percentiles of the <b>run</b> <b>length</b> distribution for the double sampling (DS) X chart with estimated process parameters are computed. Knowledge of the percentiles of the <b>run</b> <b>length</b> distribution provides a more comprehensive understanding of the expected behaviour of the <b>run</b> <b>length.</b> This additional information includes the early false alarm, the skewness of the <b>run</b> <b>length</b> distribution, and the median <b>run</b> <b>length</b> (MRL). A comparison of the <b>run</b> <b>length</b> distribution between the optimal ARL-based and MRL-based DS X chart with estimated process parameters is presented in this paper. Examples of applications are given to aid practitioners to select the best design scheme of the DS X chart with estimated process parameters, based on their specific purpose...|$|R
40|$|<b>Run</b> <b>length</b> coding using {{standard}} <b>run</b> <b>lengths</b> {{has been}} proposed by Cherry et al [7]. Their analysis has been mostly experimental for specific types of data. In this thesis the globally optimum single standard <b>run</b> <b>length</b> has been derived for the binary independent source and globally optimum single standard <b>run</b> <b>lengths</b> of zeros and ones have been derived for the binary first order Markov source. It is assumed that the output symbols are subsequently block coded in each case. A recursion relationship between standard <b>run</b> <b>lengths</b> is derived for two specific coding algorithms. A simple single standard <b>run</b> <b>length</b> scheme using a non-block code on the output symbols has also been derived for the binary independent source. </p...|$|R
