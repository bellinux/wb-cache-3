0|19|Public
50|$|Amiga can use various filesystems. The {{historical}} {{standards are}} the original Amiga filesystem, called the Old File System. This {{was good for}} floppy <b>disks</b> but <b>wasted</b> space on hard disks and is considered obsolete.|$|R
50|$|A {{cluster is}} the {{smallest}} logical amount of disk space that can be allocated to hold a file. Storing small files on a filesystem with large clusters will therefore <b>waste</b> <b>disk</b> space; such <b>wasted</b> <b>disk</b> space is called slack space. For cluster sizes which are small versus the average file size, the wasted space per file will be statistically {{about half of the}} cluster size; for large cluster sizes, the wasted space will become greater. However, a larger cluster size reduces bookkeeping overhead and fragmentation, which may improve reading and writing speed overall. Typical cluster sizes range from 1 sector (512 B) to 128 sectors (64 KiB).|$|R
40|$|ABSTRACT: Mobile {{computers}} typically {{spin down}} their hard disk after a fixed period of inactivity. If this threshold is too long, the <b>disk</b> <b>wastes</b> energy; {{if it is}} too short, the delay due to spinning the disk up again frushates the user. Usage patterns change over time, so a single fixed threshold may not be appropriate at all times. Also, different users may have varying priorities with respect to trading off energy conservation against performance. We describe a method for varying the spin-down threshold dynamically by adapting to the user's access patterns and priorities. Adaptive spin-down can in some circumstances reduce by up to 507 o the number of disk spin-ups that are deemed by the user to be inconvenient, while only moderately increasing energy consumption...|$|R
25|$|With {{the huge}} cluster sizes (16 KB, 32 KB, 64 KB) forced by larger FAT partitions, {{internal}} fragmentation {{in form of}} <b>disk</b> space <b>waste</b> by file slack due to cluster overhang (as files are rarely exact multiples of cluster size) starts {{to be a problem}} as well, especially when there are a great many small files.|$|R
5000|$|The {{encryption}} method {{should not}} <b>waste</b> <b>disk</b> space (i.e., {{the amount of}} storage used for encrypted data should not be significantly larger than the size of plaintext).|$|R
40|$|Abstract An {{experimental}} {{assessment of}} the defence hypothesis of nickel (Ni) hyperaccumulation in Alyssum was lacking. Also, to date no study had investigated the effects of hyperaccumulator litter on a detritivore species. We performed several experiments with model arthropods representatives of two trophic levels: Tribolium castaneum (herbivore) and Porcellio dilatatus (detritivore). In no-choice trials using artificial food disks with different Ni concentrations, T. castaneum fed significantly less as Ni concentration increased and totally <b>rejected</b> <b>disks</b> with the highest Ni concentration. In choice tests, insects preferred disks without Ni. In the no-choice experiment, mortality was low and {{did not differ significantly}} among treatments. Hence, this suggested a deterrent effect of high Ni diet. Experiments with P. dilatatus showed that isopods fed A. pintodasilvae litter showed significantly greater mortality (83 %) than isopods fed litter from the non-hyperaccumulator species Iberis procumbens (8 %), Micromeria juliana (no mortality) or Alnus glutinosa (no mortality). Also, isopods consumed significantly greater amounts of litter from the non-hyperaccumulator plant species. The behaviour of isopods fed A. pintodasilvae litter suggested an antifeedant effect of Ni, possibly due to post-ingestive toxic effects. Our results support the view that Ni defends the Portuguese serpentine hyperaccumulator A. pintodasilvae against herbivores, indicating that Ni can account both for feeding deterrence and toxic effects. The effects of hyperaccumulator litter on the detritivore P. dilatatus suggest that the activity of these important organisms may be significantly impaired with potential consequences on the decomposition processes. [URL]...|$|R
50|$|By convention, all {{the control}} {{structures}} were organized to fit inside the first track, thus avoiding head movement during {{read and write}} operations, although this varied depending on the manufacturer and physical format of the disk. A limitation which was not addressed until much later (with FAT32) was that any bad sector in the control structures area, track 0, could prevent the disk from being usable. The DOS formatting tool <b>rejected</b> such <b>disks</b> completely. Bad sectors were allowed only in the file data area and (since DOS 2.0) were marked with the reserved value 0xFF7 in the FAT. They made the entire containing cluster unusable.|$|R
500|$|GE {{records of}} the second disk having the serial number of the crash disk {{indicate}} that it was made with an RMI titanium billet supplied by Alcoa. [...] Research of GE records showed no other titanium parts were manufactured at GE from this RMI titanium billet {{during the period of}} 1969 to 1990. [...] GE records indicate that final finishing and inspection of the crash disk were completed on December 11, 1971. [...] Alcoa records indicate that this RMI titanium billet was first cut in 1972 and that all forgings made from this material were for airframe parts. [...] If the Alcoa records were accurate, the RMI titanium could not have been used to manufacture the crash disk, indicating that the initially <b>rejected</b> TIMET <b>disk</b> with “an unsatisfactory ultrasonic indication” was the crash disk.|$|R
40|$|The {{relative}} {{motion of the}} friction and separator plates in wet clutches during the disengaged mode causes viscous shear stresses in the fluid passing through the 100 microns gap. This results in a drag torque on both the <b>disks</b> that <b>wastes</b> energy and decreases fuel economy. The objective {{of the study is}} to develop an accurate mathematical model for the above problem with verification using FLUENT and experiments. Initially we two consider flat disks. The mathematical model calculates the drag torque on the disks and the 2 D axisymmetric solver verifies the solution. The surface pressure distribution on the plates is also verified. Then, 3 D models of one grooved and one flat disk are tested using CFD, experiments and an approximate 3 D mathematical model. The number of grooves, depth of groove and clearance between the disks are studied to understand their effect on the torque. The study determines the pressure field that eventually affects aeration incipience (not studied here). The results of the model, computations and experiments corroborate well in the single-phase regime. �DOI: 10. 1115 / 1. 2162553...|$|R
40|$|Abstract—XML is an {{important}} standard of data exchange and representation. As a mature database system, using relational database to support XML data may bring some advantages. But storing XML in relational database has obvious redundancy that <b>wastes</b> <b>disk</b> space, bandwidth and disk I/O when querying XML data. For the efficiency of storage and query XML, {{it is necessary to}} use compressed XML data in relational database. In this paper, a compressed relational database technology supporting XML data is presented. Original relational storage structure is adaptive to XPath query process. The compression method keeps this feature. Besides traditional relational database techniques, additional query process technologies on compressed relations and for special structure for XML are presented. In this paper, technologies for XQuery process in compressed relational database are presented [...] Keywords—XML, compression, query processing I...|$|R
40|$|Information Access, a {{subdivision}} {{of the research}} cluster Information Systems. This thesis {{is ready to be}} marked. Date: Author’s signature: This thesis is ready to be verified by the second reader. Date: Supervisor’s signature: Data-intensive query processing tasks like data mining, scientific data analysis, and decision support can leave a database system severely I/O-bound, even when common RAID configurations are used. Traditionally, this problem has been tackled by adding more and more disks, connected through expensive interconnect networks. This brute-force approach results in systems of which the price is dominated by the cost of their disk subsystems and a lot of <b>disk</b> space is <b>wasted</b> as <b>disks</b> are only added to gain bandwidth. A more subtle and cost-effective solution can be found in data compression, which has the potential to alleviate the I/O bottleneck. However, traditional algorithms like Huffman coding, Arithmetic coding and Lempel-Ziv style dictionary methods are not suited for this goal due to high processing overheads. I...|$|R
40|$|A {{number of}} recent {{technological}} trends have made data intensive applications such as continuous media (audio and video) servers a reality. These servers are expected {{to play an important}} role in applications such as video-on-demand, digital library, news-on-demand, distance learning, etc. Con-tinuous media applications are data intensive and might re-quire storage subsystems that consist of hundreds of (multi-zone) disk drives. With the current technological trends, a homogeneous disk subsystem might evolve to consist of a heterogeneous collection of disk drives. Given such a storage subsystem, the system must continue to support a hiccup-free display of audio and video clips. This study de-scribes extensions of four continuous display techniques for multi-zone disk drives to a heterogeneous platform. These techniques include IBM’s Logical Track [21], HP’s Track Pairing [4], and USC’s FIXB [9] and deadline driven tech-niques [la]. We quantify the performance tradeoff associated with these techniques using analytical models and simula-tion studies. The obtained results demonstrate tradeoffs between the cost per simultaneous stream supported by a technique, the <b>wasted</b> <b>disk</b> space, and the incurred startup latency. ...|$|R
40|$|The Alex {{filesystem}} {{provides users}} and applications transparent read access to files in Internet anonymous FTP sites. Today {{there are thousands}} of anonymous FTP sites with a total of a few million files and roughly a terabyte of data. The standard approach to accessing these files involves logging in to the remote machine. This means that an application can not access remote files and that users do not have any of their aliases or local tools available when connected to a remote site. Users who want to use an application on a remote file must first manually make a local copy of the file. Not only is this inconvenient, it creates two more problems. First, there is no mechanism for automatically updating this local copy when the remote file changes. The users must keep track of where they get their files from and check {{to see if there are}} updates, and then fetch these. Second, many different users at the same site may have made copies of the same remote file, thus <b>wasting</b> <b>disk</b> space. Ale [...] ...|$|R
40|$|AbstractThe {{blast furnace}} slag is rich in {{high-temperature}} waste heat, however, most of the slag sensible heat is wasted in the current “wet granulation” method. Dry granulation method based on rotary disk atomizer has received growing attention. However, the waste heat recovery efficiency of this method is mainly determined by the slag particle size. In this paper, a simple model is proposed to characterize the ligament mode disintegration of liquid slag film at the rotary disk rim. The {{results indicated that the}} ligament number increases with increasing rotational speed, slag density and slag tapping rate but decreases with increasing viscosity and surface tension. The Sauter mean diameter of the droplet increases with increasing viscosity and surface tension but decreases with increasing rotational speed and slag density. Compared with density and rotational speed, increasing of viscosity, surface tension and slag tapping rate results in slight increases of the Sauter mean diameters. The results in this paper can be applicable for characterizing the dry granulation of slag by rotary <b>disk</b> in the <b>waste</b> heat recovery process of molten slag {{in a wide range of}} operation condition...|$|R
40|$|Normalization is {{the process}} of {{organizing}} data in a database. This includes creating tables and establishing relationships between those tables according to rules designed both to protect the data and to make the database more flexible by eliminating two factors: redundancy and inconsistent dependency. Redundant data <b>wastes</b> <b>disk</b> space and creates maintenance problems. If data that exists in more than one place must be changed, the data must be changed {{in exactly the same way}} in all locations. A customer address change is much easier to implement if that data is stored only in the Customers table and nowhere else in the database. What is an "inconsistent dependency"? While it is intuitive for a user to look in the Customers table for the address of a particular customer, it may not make sense to look there for the salary of the employee who calls on that customer. The employee's salary is related to, or dependent on, the employee and thus should be moved to the Employees table. Inconsistent dependencies can make data difficult to access; the path to find the data may be missing or broken. There are a few rules for database normalization. Each rule is called a "normal form. " I...|$|R
40|$|The Heliospheric Imager (HI) {{is part of}} the SECCHI {{suite of}} {{instruments}} on-board the two STEREO spacecrafts to be launched in 2005. The two HI instruments will provide stereographic image pairs of solar coronal plasma and address the observational problem of very faint coronal mass ejections (CME) over a wide field of view (~ 90 degree(s)) ranging from 13 to 330 R[SUB] 0 [/SUB]. The key element of the instrument design is to <b>reject</b> the solar <b>disk</b> light, with straylight attenuation of the order of 10 [SUP]- 13 [/SUP] to 10 [SUP]- 15 [/SUP] in the camera systems. This attenuation is accomplished by a specific design of straylight baffling system, and two separate observing cameras with complimentary FOV's cover the wide FOV. A multi-vane diffractive system has been theoretically optimized to achieve the lower requirement (10 [SUP]- 13 [/SUP] for HI- 1) and is combined with a secondary baffling system to reach the 10 [SUP]- 15 [/SUP] rejection performance in the second camera system (HI- 2). This paper presents the design concept of the HI optics and baffles, and the preparation of verification tests that will demonstrate the instrument straylight performances. The baffle design has been optimized according to accommodation constrains on the spacecraft, and the optics were studied to provide adequate light gathering power and image quality. Straylight has been studied in the complete configuration, including the lens barrels and the focal plane assemblies. A specific testing facility is currently being studied to characterize the effective straylight rejection of the HI baffling. An overview of the developments for those tests is presented...|$|R
5000|$|The {{origins of}} the crash disk are {{uncertain}} because of significant irregularities and gaps, noted in the NTSB report, in the manufacturing records of GE Aircraft Engines (GEAE) and its suppliers. [...] Records found after the accident indicated that two rough-machined forgings having the serial number of the crash disk had been routed through GEAE manufacturing. Records indicated that Alcoa supplied GE with TIMET titanium forgings for one disk with the serial number of the crash disk. Some records show that this <b>disk</b> “was <b>rejected</b> for an unsatisfactory ultrasonic indication”, that an outside lab performed an ultrasound inspection of this disk, that this disk was subsequently returned to GE, and that this disk should have been scrapped. The FAA report stated “There is no record of warranty claim by GEAE for defective material and no record of any credit for GEAE processed by Alcoa or TIMET”. [...] GE records of the second disk having the serial number of the crash disk indicate that it was made with an RMI titanium billet supplied by Alcoa. Research of GE records showed no other titanium parts were manufactured at GE from this RMI titanium billet {{during the period of}} 1969 to 1990. GE records indicate that final finishing and inspection of the crash disk were completed on December 11, 1971. Alcoa records indicate that this RMI titanium billet was first cut in 1972 and that all forgings made from this material were for airframe parts. [...] If the Alcoa records were accurate, the RMI titanium could not have been used to manufacture the crash disk, indicating that the initially <b>rejected</b> TIMET <b>disk</b> with “an unsatisfactory ultrasonic indication” was the crash disk. [...] CF6 engines like that containing the crash disk were used to power many civilian and military aircraft {{at the time of the}} crash. Due to concerns that the accident could recur, a large number of disks that were in service were examined by ultrasound for indications of defects. At least two “sister disks” were found to have defects like that of the crash disk. Prioritization and efficiency of inspections of the many engines under suspicion would have been aided by determination of the titanium source of the crash disk. Chemical analyses of the crash disk intended to determine its source were inconclusive. The NTSB report stated that if examined disks were not from the same source, “the records on a large number of GEAE disks are suspect. It also means that any AD (Airworthiness Directive) action that is based on the serial number of a disk could fail to have its intended effect because suspect disks could remain in service.” [...] The FAA report did not explicitly address the impact of these uncertainties on operations of military aircraft that might have contained a suspect disk.|$|R
40|$|Human space {{missions}} generate trash with {{a substantial}} amount of plastic (20 % or greater by mass). The trash also contains water trapped in food residue and paper products and other trash items. The Heat Melt Compactor (HMC) under development by NASA Ames Research Center (ARC) compresses the waste, dries it to recover water and melts the plastic to encapsulate the compressed trash. The resulting <b>waste</b> <b>disk</b> or puck represents an approximately ten-fold reduction in the volume of the initial trash loaded into the HMC. In the current design concept being pursued, the trash is compressed by a piston after it is loaded into the trash chamber. The piston face, the side walls of the waste processing chamber and the end surface in contact with the waste can be heated to evaporate the water and to melt the plastic. Water is recovered by the HMC in two phases. The first is a pre-process compaction without heat or with the heaters initially turned on but before the waste heats up. Tests have shown that during this step some liquid water may be expelled from the chamber. This water is believed to be free water (i. e., not bound with or absorbed in other waste constituents) that is present in the trash. This phase is herein termed Phase A of the water recovery process. During HMC operations, it is desired that liquid water recovery in Phase A be eliminated or minimized so that water-vapor processing equipment (e. g., condensers) downstream of the HMC are not fouled by liquid water and its constituents (i. e., suspended or dissolved matter) exiting the HMC. The primary water recovery process takes place next where the trash is further compacted while the heated surfaces reach their set temperatures for this step. This step will be referred to herein as Phase B of the water recovery process. During this step the waste chamber may be exposed to different selected pressures such as ambient, low pressure (e. g., 0. 2 atm), or vacuum. The objective for this step is to remove both bound and any remaining free water in the trash by evaporation. The temperature settings of the heated surfaces are usually kept above the saturation temperature of water but below the melting temperature of the plastic in the waste during this step to avoid any encapsulation of wet trash which would reduce the amount of recovered water by blocking the vapor escape. In this paper, we analyze the water recovery rate during Phase B where the trash is heated and water leaves the waste chamber as vapor, for operation of the HMC in reduced gravity. We pursue a quasi-one-dimensional model with and without sidewall heating to determine the water recovery rate and the trash drying time. The influences of the trash thermal properties, the amount of water loading, and the distribution of the water in the trash on the water recovery rates are determined...|$|R

