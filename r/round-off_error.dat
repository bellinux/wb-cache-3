311|409|Public
25|$|Increasing the {{precision}} of the floating point representation generally reduces the amount of accumulated <b>round-off</b> <b>error</b> caused by intermediate calculations.|$|E
25|$|The {{alternative}} rounding modes {{are also}} useful in diagnosing numerical instability: {{if the results}} of a subroutine vary substantially between rounding to + and − infinity then it is likely numerically unstable and affected by <b>round-off</b> <b>error.</b>|$|E
25|$|<b>Round-off</b> <b>error</b> {{can affect}} the {{convergence}} and accuracy of iterative numerical procedures. As an example, Archimedes approximated π by calculating the perimeters of polygons inscribing and circumscribing a circle, starting with hexagons, and successively doubling the number of sides. As noted above, computations may be rearranged {{in a way that}} is mathematically equivalent but less prone to error (numerical analysis).|$|E
40|$|When solving {{problems}} of mathematical physics using numerical methods we always encounter three basic types of errors: modeling error, discretization <b>error,</b> and <b>round-off</b> <b>errors.</b> In this survey, we present several pathological examples which may appear during numerical calculations. We will mostly {{concentrate on the}} influence of <b>round-off</b> <b>errors...</b>|$|R
30|$|Solution {{from normal}} {{equations}} can have <b>round-off</b> <b>errors</b> so QR decomposition of matrix X is done.|$|R
3000|$|..., a {{generalized}} reflexive solution group of Problem I {{can be obtained}} with finite iteration steps {{in the absence of}} <b>round-off</b> <b>errors.</b>|$|R
25|$|The {{numerical}} {{methods for}} linear least squares {{are important because}} linear regression models {{are among the most}} important types of model, both as formal statistical models and for exploration of data-sets. The majority of statistical computer packages contain facilities for regression analysis that make use of linear least squares computations. Hence it is appropriate that considerable effort has been devoted to the task of ensuring that these computations are undertaken efficiently and with due regard to <b>round-off</b> <b>error.</b>|$|E
500|$|Given r tied {{observations}} from x'i to x'i+r−1, let δ represent the <b>round-off</b> <b>error.</b> All {{of the true}} values should then fall in the range [...] The corresponding points on the distribution should now fall between [...] and [...] Cheng and Stephens suggest assuming that the rounded values are uniformly spaced in this interval, by defining ...|$|E
500|$|The maximum {{operation}} [...] "max (a, b)" [...] is {{a binary}} operation similar to addition. In fact, if two nonnegative numbers a and b are of different orders of magnitude, then their sum is approximately equal to their maximum. This approximation is extremely {{useful in the}} applications of mathematics, for example in truncating Taylor series. However, it presents a perpetual difficulty in numerical analysis, essentially since [...] "max" [...] is not invertible. If b {{is much greater than}} a, then a straightforward calculation of [...] can accumulate an unacceptable <b>round-off</b> <b>error,</b> perhaps even returning zero. See also Loss of significance.|$|E
50|$|A high-precision {{arithmetic}} {{version of}} BFGS (pBFGS), implemented in C++ and {{integrated with the}} high-precision arithmetic package ARPREC is robust against numerical instability (e.g. <b>round-off</b> <b>errors).</b>|$|R
40|$|It is {{well known}} that the {{computation}} of accurate trajectories of the Lorenz system is a difficult problem. Computed solutions are very sensitive to the discretization error determined by the time step size and polynomial order of the method, as well as <b>round-off</b> <b>errors.</b> In this work, we show how <b>round-off</b> <b>errors</b> limit the computability of the Lorenz system and quantify exactly the length of intervals over which solutions can be computed, expressed in terms of the floating point precision. Using adjoint-based a posteriori error analysis techniques, we estimate the stability of computations with respect to initial data, discretization, and <b>round-off</b> <b>errors,</b> respectively. The analysis is verified by computing an accurate solution on the time interval [0, 1000] using a very high order (order 200) finite element method and very high floating point precision 400 digits). Comment: Proceedings of "International conference on Adaptive Modeling and Simulation (ADMOS) 2013...|$|R
40|$|We {{consider}} some unusual phenomena {{that appear in}} long consecutive calculations due to a new sort of <b>round-off</b> <b>errors</b> of Pentium floating point division bug type. Mathematical model of these <b>round-off</b> <b>errors</b> is proposed and new phenomena in numerical simulations of dynamical systems (differential equations) and other numerical procedures are discussed. It is shown also that these new phenomena could appear not only with Pentium floating point division bugs, but in a more general situation. 1 Introduction When solving differential equations or other dynamical systems on a computer, we inevitably run into perturbations caused by various types of discretizations. The first of them is time discretization, that is, the transition from differential equations or flows to difference equations or maps. The second type is space discretization, that is, the transition from continuous phase space to a discrete lattice. In numerical simulations these discretizations correspond to <b>round-off</b> <b>errors</b> [...] . ...|$|R
2500|$|Note {{that the}} lowest three digits {{of the second}} operand (654) are {{essentially}} lost. This is <b>round-off</b> <b>error.</b> In extreme cases, the sum of two non-zero numbers may be equal to one of them: ...|$|E
2500|$|An FFT computes the DFT and {{produces}} {{exactly the same}} result as evaluating the DFT definition directly; the most important difference is that an FFT is much faster. (In the presence of <b>round-off</b> <b>error,</b> many FFT algorithms are also much more accurate than evaluating the DFT definition directly, as discussed below.) ...|$|E
2500|$|One {{problem with}} the Kalman filter is its {{numerical}} stability. [...] If the process noise covariance Qk is small, <b>round-off</b> <b>error</b> often causes a small positive eigenvalue to be computed as a negative number. [...] This renders the numerical representation of the state covariance matrix P indefinite, while its true form is positive-definite.|$|E
25|$|<b>Round-off</b> <b>errors</b> arise {{because it}} is {{impossible}} to represent all real numbers exactly on a machine with finite memory (which is what all practical digital computers are).|$|R
2500|$|... "This {{algorithm}} ... requires {{expected time}} proportional to λ as λ→∞. For large λ, <b>round-off</b> <b>errors</b> proliferate, which {{provides us with}} another reason for avoiding large values of λ." ...|$|R
5000|$|Irreversible Color Transform (ICT) {{uses the}} well known YCBCR color space. It is called [...] "irreversible" [...] {{because it has}} to be {{implemented}} in floating or fix-point and causes <b>round-off</b> <b>errors.</b>|$|R
2500|$|On {{the other}} hand, {{rounding}} of exact numbers will introduce some <b>round-off</b> <b>error</b> in the reported result. [...] Rounding is almost unavoidable when reporting many computations — especially when dividing two numbers in integer or fixed-point arithmetic; when computing mathematical {{functions such as}} square roots, logarithms, and sines; or when using a floating point representation with a fixed number of significant digits. [...] In a sequence of calculations, these rounding errors generally accumulate, and in certain ill-conditioned cases they may make the result meaningless.|$|E
2500|$|In a {{computer}} implementation, {{as the three}} s'j sums become large, {{we need to consider}} <b>round-off</b> <b>error,</b> arithmetic overflow, and arithmetic underflow. The method below calculates the running sums method with reduced rounding errors. This is a [...] "one pass" [...] algorithm for calculating variance of n samples without the need to store prior data during the calculation. Applying this method to a time series will result in successive values of standard deviation corresponding to n data points as n grows larger with each new sample, rather than a constant-width sliding window calculation.|$|E
2500|$|These {{formulas}} {{are much}} easier to evaluate than the quadratic formula under the condition of one large and one small root, because the quadratic formula evaluates the small root as the difference of two very nearly equal numbers (the case of large [...] ), which causes <b>round-off</b> <b>error</b> in a numerical evaluation. Figure5 shows the difference between (i)a direct evaluation using the quadratic formula (accurate when the roots are near each other in value) and (ii)an evaluation based upon the above approximation of Vieta's formulas (accurate when the roots are widely spaced). As the linear coefficient [...] increases, initially the quadratic formula is accurate, and the approximate formula improves in accuracy, leading to a smaller difference between the methods as [...] increases. However, {{at some point the}} quadratic formula begins to lose accuracy because of round off error, while the approximate method continues to improve. Consequently, the difference between the methods begins to increase as the quadratic formula becomes worse and worse.|$|E
5000|$|... where [...] is the {{equivalence}} relational operator.The previous {{statements are}} also valid MATLAB expressions if {{the third one}} is executed before the others (numerical comparisons may be false because of <b>round-off</b> <b>errors).</b>|$|R
50|$|Other {{methods to}} process data include Schur {{decomposition}} and Cholesky decomposition. In comparison to these, Levinson recursion (particularly split Levinson recursion) tends to be faster computationally, but more sensitive to computational inaccuracies like <b>round-off</b> <b>errors.</b>|$|R
30|$|The ant colony {{algorithm}} {{parameters are}} assigned as follows: The number of ants is 50 and {{the maximum number}} of iterations is 100. Considering the slight computational and <b>round-off</b> <b>errors,</b> the maximum error is set at 5.|$|R
50|$|<b>Round-off</b> <b>error</b> can be {{introduced}} by division operations due to limited precision.|$|E
5000|$|Limitations of finite {{representations}} (e.g., integer bounds, imprecision of floating-point representations, and <b>round-off</b> <b>error)</b> ...|$|E
5000|$|It is {{possible}} to determine the <b>round-off</b> <b>error</b> by using the Taylor series formula for the square root: ...|$|E
30|$|There is no {{need for}} {{linearization}} or perturbations, large computational work and <b>round-off</b> <b>errors</b> are avoided. It has been used to solve effectively, easily and accurately a large class of linear and nonlinear problems with approximations [16 – 25].|$|R
3000|$|The series (19) {{certainly}} converges significantly {{faster than}} (5) and (17). In fact, taking six {{terms of the}} first series and two terms of the second and {{paying attention to the}} remainders and <b>round-off</b> <b>errors,</b> we get the inequalities [...]...|$|R
30|$|The {{performance}} of the proposed engine is measured against floating point Matlab model to assess its accuracy {{and the effect of}} <b>round-off</b> <b>errors.</b> The implementation efficiency is projected for the resulting core area and power consumption to implement the supported operations.|$|R
50|$|A <b>round-off</b> <b>error,</b> {{also called}} {{rounding}} error, {{is the difference}} between the calculated approximation of a number and its exact mathematical value due to rounding. This is a form of quantization error. One of the goals of numerical analysis is to estimate errors in calculations, including <b>round-off</b> <b>error,</b> when using approximation equations or algorithms, especially when using finitely many digits to represent real numbers (which in theory have infinitely many digits).|$|E
5000|$|Increasing the {{precision}} of the floating point representation generally reduces the amount of accumulated <b>round-off</b> <b>error</b> caused by intermediate calculations.Less common IEEE formats include: ...|$|E
5000|$|Occasionally, <b>round-off</b> <b>error</b> (the {{consequence}} of using finite precision [...] {{floating point numbers}} on computers) is also called truncation error, especially if the number is rounded by truncation.|$|E
40|$|Modified {{program has}} been {{developed}} using improved variation of Encke method which avoids accumulation of <b>round-off</b> <b>errors</b> and avoids numerical ambiguities arising from near-circular orbits of low inclination. Variety of interplanetary trajectory problems can be computed with maximum accuracy and efficiency...|$|R
40|$|International audienceOrdinary {{differential}} equations are ubiquitous in scientific computing. Solving exactly these equations {{is usually not}} possible, except for special cases, hence the use of numerical schemes to get a discretized solution. We are interested in such numerical integration methods, for instance Euler's method or the Runge-Kutta methods. As they are implemented using floating-point arithmetic, <b>round-off</b> <b>errors</b> occur. In order to guarantee their accuracy, we aim at providing bounds on the <b>round-off</b> <b>errors</b> of explicit one-step numerical integration methods. Our methodology is to apply a fine-grained analysis to these numerical algorithms. Our originality is that our floating-point analysis {{takes advantage of the}} linear stability of the scheme, a mathematical property that vouches the scheme is well-behaved...|$|R
30|$|The {{iteration}} obtained after {{applying a}} nonlinear equation solver on the mapping (8) and its reciprocal, {{could be used}} for polar decomposition. But here, the experimental results show that the reciprocal form (10) is more stable in the presence of <b>round-off</b> <b>errors.</b>|$|R
