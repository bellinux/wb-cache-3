351|14|Public
25|$|At SIGGRAPH 2009, Nvidia {{announced}} OptiX, a free API for real-time {{ray tracing}} on Nvidia GPUs. The API exposes seven programmable entry points within the ray tracing pipeline, allowing for custom cameras, ray-primitive intersections, shaders, shadowing, etc. This flexibility enables bidirectional path tracing, Metropolis light transport, {{and many other}} rendering algorithms that cannot be implemented with tail recursion. Nvidia has shipped over 350,000,000 OptiX capable GPUs as of April 2013. OptiX-based <b>renderers</b> are used in Adobe AfterEffects, Bunkspeed Shot, Autodesk Maya, 3ds max, and many other <b>renderers.</b>|$|E
25|$|ANGLE (Almost Native Graphics Layer Engine) {{is an open}} source, BSD-licensed {{graphics}} engine abstraction layer developed by Google. The API is mainly designed to bring high-performance OpenGL compatibility to Windows computers and to web browsers such as Chromium by translating OpenGL calls to Direct3D, which has much better driver support. There are two backend <b>renderers</b> for ANGLE: the oldest one uses Direct3D 9.0c, while the newer one uses Direct3D 11.|$|E
25|$|There {{still remains}} {{problems}} with fonts that have minimum coverage in their mapping, because text <b>renderers</b> still not correctly reorder the isolated Buginese vowel e when it follows something else than NBSP or a Buginese consonant (for example when it follows the standard U+0020 SPACE, or the U+25CC DOTTED CIRCLE symbolic placeholder, as recommended in OpenType designs), or because fonts {{do not have}} correct kerning rules for additional pairs using {{any one of the}} 5 Buginese vowel signs.|$|E
50|$|The {{governing}} board {{is made up}} of 15 directors elected by voting members. Seven of the directors are elected by weighted votes. The remaining eight are elected by straight votes - two are feedstock producer organization voting delegates who are actively engaged in farming; two are voting delegates of producer or marketer members who are members of either the National <b>Renderer’s</b> Association or the Fats and Proteins Research Foundation (FPRF); and four are voting delegates of producer or marketer members and/or feedstock producer members who are not members of the National <b>Renderer’s</b> Association or FPRF.|$|R
5000|$|A Beit (also spelled bait, بيت , {{literally}} [...] "a house") is a metrical unit of Arabic, Iranian, Urdu and Sindhi poetry. It {{corresponds to}} a line, though sometimes improperly <b>renderered</b> as [...] "couplet" [...] since each beit {{is divided into two}} hemistichs of equal length, each containing two, three or four feet, or from 16 to 32 syllables.|$|R
40|$|Development of {{multiplatform}} applications {{requires a}} lot of effort to implement very different user interfaces. Automatic user interface generation can reduce time-to-prototype and time-to market for general purpose applications. This paper describes the Multichannel Object <b>REnderer</b> (MORE), which provides a model-based framework for platform independent and automatic runtime generation of user interfaces, able to support a large set of devices, including iTV MHP-enabled set-top boxes...|$|R
25|$|In 2001, the Center for Science in the Public Interest {{petitioned the}} United States Department of Agriculture to require meat packers to remove spinal cords before {{processing}} cattle carcasses for human consumption, a measure designed {{to lessen the}} risk of infection by variant Creutzfeldt–Jakob disease. The petition {{was supported by the}} American Public Health Association, the Consumer Federation of America, the Government Accountability Project, the National Consumers League, and Safe Tables Our Priority. This was opposed by the National Cattlemen's Beef Association, the National <b>Renderers</b> Association, the National Meat Association, the Pork Producers Council, sheep raisers, milk producers, the Turkey Federation, and eight other organizations from the animal-derived food industry.|$|E
500|$|Shortly after {{changing}} {{the game from}} I5 to Ratchet & Clank, Naughty Dog asked Insomniac {{if they would be}} interested in sharing the game technology used in Naughty Dog's , asking that Insomniac in turn share with them any improvements that were made. Insomniac agreed, resulting in most of the Ratchet & Clank engine technology being developed in-house by Insomniac, but some very important <b>renderers</b> were those developed by Naughty Dog. Looking back on the agreement, Ted Price said that [...] "Naughty Dog's generosity gave us a huge leg up and allowed us to draw the enormous vistas in the game." [...] Some years later, Ted Price clarified Insomniac's stance on engine technology while obliquely mentioning the shared renderers: ...|$|E
2500|$|Additionally, {{the third}} vowel [...] must appear before (to the left) the {{consonant}} that it modifies, but must remain logically encoded after that consonant, in conforming Unicode implementations of fonts and text <b>renderers</b> (this case of prepended vowels which occurs in many Indic scripts, {{does not follow}} {{the exception to the}} Unicode logical encoding order, admitted only for the prepended vowels in the Thai, Lao and Tai Viet scripts). Currently, many fonts or text <b>renderers</b> do not implement this single reordering rule for the Buginese script, and may still incorrectly display that vowel at the wrong position.|$|E
50|$|Currently in {{development}} is gpu940, a soft 3D renderer {{that can do}} many rendering types, including true perspective texture mapping/lighting. It utilizes the ARM940T CPU of the GP2X, and allows for the GP2X to run basic OpenGL functions. In January 2007, the <b>renderer's</b> OpenGL functions allowed for the 3D roleplaying game Egoboo to be ported to the GP2X at a playable speed, and a month later updated with increased speed and added lighting effects.|$|R
40|$|I 3 DL 2 3 D audio {{rendering}} guidelines {{gives the}} minimum rendering {{requirements for the}} 3 D audio developers, <b>renderer’s,</b> and vendors. I 3 DL 2 defines how the 3 D technology {{is applied to the}} current PC systems. Creative EAX reverb API is based on these guidelines and hardware acceleration is supported under OpenAL and Microsoft DirectSound 3 D TM. EAX Advanced HD is the next generation of 3 D audio rendering API and today only a few hardware and software applications support it. ...|$|R
40|$|Force {{feedback}} {{coupled with}} visual display {{allows people to}} interact intuitively with complex virtual environments. For this synergy of haptics and graphics to flourish, however, haptic systems must be capable of modeling environments with the same richness, complexity and interactivity {{that can be found}} in existing graphic systems. To help meet this challenge, we have developed a haptic rendering system that allows for the efficient tactile display of graphical information. The system uses a common high-level framework to model contact constraints, surface shading, friction and texture. The multilevel control system also helps ensure that the haptic device will remain stable even as the limits of the <b>renderer's</b> capabilities are reached...|$|R
2500|$|In Windows Vista, Microsoft has {{attempted}} to contain the risks of bugs such as buffer overflows by running the Internet Explorer process with limited privileges. Google Chrome similarly confines its page <b>renderers</b> to their own [...] "sandbox".|$|E
2500|$|Blade Runner was {{advertised as}} a [...] "real-time 3D {{adventure}} game," [...] {{since it was}} one of the first adventure games to use both 3D character rendering and a game world which progressed in real-time (as opposed to waiting for the player's actions). Unlike many games of its time, which used polygon-based <b>renderers</b> exploiting 3D accelerators, Westwood opted for their own software-based renderer using voxel technology.|$|E
2500|$|The {{internal}} {{rendering of}} all Bézier curves in font or vector graphics <b>renderers</b> will split them recursively {{up to the}} point where the curve is flat enough to be drawn as a series of linear or circular segments. The exact splitting algorithm is implementation dependent, only the flatness criteria must be respected to reach the necessary precision and to avoid non-monotonic local changes of curvature. The [...] "smooth curve" [...] feature of charts in Microsoft Excel also uses this algorithm.|$|E
40|$|The task {{has been}} to develop a network media {{renderer}} on an embedded linux system running on a Spartan 6 FPGA. One of the challenges have been {{to make the best}} use of the limited FPGA area. MP 3 have been the prioritised format. To achieve fast MP 3 decoding a MicroBlaze soft processor have been configured for speed with concern to the small area availabe. Also the software MP 3 decoding process have been accelerated with hardware. MP 3 files with full quality (320 kbit/s) can be decoded with real time requirements. A sound interface hardware have been designed to handle the decoded sound samples and convert them to the S/PDIF standard interface. Also UPnP commands have been implemented with the MP 3 player software to complete the <b>renderer’s</b> network functionality...|$|R
40|$|As {{part of the}} Living Textbook Project, a {{three-dimensional}} terrain rendering program has been developed. Unfortunately, three-dimensional rendering requires intense computation and is thus rather slow. As a partial solution to this problem, a two-dimensional terrain renderer has been produced. This new renderer uses C and a high-level package called Tcl/Tk, and betters the 3 D <b>renderer's</b> speed. 1 Background and Motivation 1. 1 The Living Textbook Project The Living Textbook Project, a joint undertaking of the Northeast Parallel Architectures Center (NPAC), the School of Education at Syracuse University, NYNEX, and Columbia University, aims to utilize high-performance computing {{in the development of}} interactive educational tools [4]. The project includes a "flight simulator" program, which converts satellite photographs and elevation data of New York State into {{a three-dimensional}} geographical rendering. The user can traverse the rendering and thereby take a virtual tour of the state. [...] ...|$|R
40|$|We {{present a}} {{visualization}} system to assist designers of schedulingbased multi-threaded out-of-core algorithms. Our system facilitates the understanding and improving of the algorithm through {{a stack of}} visual widgets that effectively correlate the out-of-core system state with scheduling decisions. The stack presents an increasing refinement in the scope of both time and abstraction level; {{at the top of}} the stack, the evolution of a derived efficiency measure is shown for the scope of the entire out-of-core system execution and at the bottom the details of a single scheduling decision are displayed. The stack provides much more than a temporal zoom-effect as each widget presents a different view of the scheduling decision data, presenting distinct aspects of the out-of-core system state as well as correlating them with the neighboring widgets in the stack. This approach allows designers to hone in on problems in scheduling or algorithm design. As a case study we consider a global illumination renderer and show how visualization of the scheduling behavior has led to key improvements of the <b>renderer’s</b> performance...|$|R
2500|$|... {{encoding}} Buginese {{texts in}} a way not conforming to the Unicode standard, for example encoding texts with the vowel [...] before the consonant (also without warranty of stability for the future, when conforming fonts and text <b>renderers</b> will be available, because they will then reorder the vowel [...] with any consonant encoded before that vowel; this solution also does not work as it already creates the incorrect grapheme cluster boundaries, the vowel being already grouped with the previous character instead of the following, notably in text editors); ...|$|E
2500|$|The {{greatest}} {{complaints to}} the St. Louis Board of Health, however, {{were due to}} the presence of industries engaged in rendering, a process in which decaying animal carcasses were converted into useful products. Generally, after an animal was slaughtered for meat consumption, hides were sent to be cured and tanned, while the remaining fat and bones were sent to <b>renderers.</b> Most rendering factories produced particularly noxious fumes, often regarded as health hazards. Smells from the factories and offal sent to them were reportedly [...] "so putrid that a wagon loaded with [...] can be smelled for miles ... and while rendering the fetid smell sickens inhabitants for miles around." [...] The stench of bone rendering factories was said to have been so thick and bad that it [...] "slowed the incoming trains." ...|$|E
2500|$|As {{a result}} of the {{technological}} improvements made possible by successful fund raisers, in 1996 KFJC went international, with live broadcasts from Brixton, England of live sets from Ascension, the Bevis Frond, Ramleh and the Shadow Ring over two weekends. In that year, KFJC also began streaming over the Internet. The 'lower' production studio was remodeled over the summer of 1996, with a custom-made desk replacing the original card table. KFJC's next international broadcasts occurred in 2000, when the staff went to Dunedin, New Zealand for 6 nightly broadcasts of the [...] "Dunedin Sound" [...] showcase at the Otago Festival Of The Arts. These broadcasts featured performances by such legendary underground groups as the Clean, the Chills, the Dead C., Alastair Galbraith, the <b>Renderers,</b> Snapper, and the Verlaines. The following year, a double CD documenting these broadcasts was produced for the Station's annual fund raiser.|$|E
40|$|In {{this paper}} {{we present a}} multi-GPU {{parallel}} volume rendering implemention built using the MapReduce programming model. We give implementation details of the library, including specific optimizations made for our rendering and compositing design. We analyze the theoretical peak performance and bottlenecks for all tasks required and show that our system significantly reduces computation as a bottleneck in the ray-casting phase. We demonstrate that our rendering speeds are adequate for interactive visualization (our system is capable of rendering a 1024 3 floating-point sampled volume in under one second using 8 GPUs), and that our system is capable of delivering both in-core and out-of-core visualizations. We argue that a multi-GPU MapReduce library is {{a good fit for}} parallel volume <b>renderering</b> because it is easy to program for, scales well, and eliminates the need to focus on I/O algorithms thus allowing the focus to be on visualization algorithms instead. We show that our system scales with respect {{to the size of the}} volume, and (given enough work) the number of GPUs...|$|R
40|$|Omeka 2. 4. 1 was {{released}} on May 25, 2016. It is the first maintenance release in the 2. 4 series. Bugs Fixed Drag and drop sorting failed for longer lists The "linkText" option was ignored for the file markup <b>renderer's</b> default link-only display. (# 690) Labels weren't properly associated with sitewide search record type checkboxes when unchecked (# 698) The show page for Collections had the title "Edit Collection" A Javascript error could sometimes occur when refreshing an item edit page Improvements SSL/HTTPS detection is now more reliable (# 685) The default extension and mimetype whitelists are expanded to cover the newly-supported filetypes from the 2. 4 release (# 683) Fewer characters will be entity-encoded in HTML values when HTMLPurifier is disabled Localization New translation for Belarusian (be_BY) Updates to many existing translations External Libraries Omeka 2. 4. 1 reverts to the following versions of its external dependencies: jQuery UI 1. 11. 2 Bundled Add-ons Plugins Exhibit Builder 3. 3. 1 Acknowledgements The following members of the Omeka community contributed code, fixes, and improvements to Omeka 2. 4. 1 : Pasi Kallinen (# 698) Sarah Weissman (# 690...|$|R
40|$|The idea of {{computer}} vision as the Bayesian inverse problem to computer graphics {{has a long}} history and an appealing elegance, but it has proved difficult to directly implement. Instead, most vision tasks are approached via complex bottom-up processing pipelines. Here we show {{that it is possible to}} write short, simple prob-abilistic graphics programs that define flexible generative models and to automati-cally invert them to interpret real-world images. Generative probabilistic graphics programs (GPGP) consist of a stochastic scene generator, a renderer based on graphics software, a stochastic likelihood model linking the <b>renderer’s</b> output and the data, and latent variables that adjust the fidelity of the renderer and the toler-ance of the likelihood. Representations and algorithms from computer graphics are used as the deterministic backbone for highly approximate and stochastic gen-erative models. This formulation combines probabilistic programming, computer graphics, and approximate Bayesian computation, and depends only on general-purpose, automatic inference techniques. We describe two applications: read-ing sequences of degraded and adversarially obscured characters, and inferring 3 D road models from vehicle-mounted camera images. Each of the probabilistic graphics programs we present relies on under 20 lines of probabilistic code, and yields accurate, approximately Bayesian inferences about real-world images. ...|$|R
50|$|Autodesk {{develops}} {{and purchased}} many specific-purpose <b>renderers</b> but many Autodesk products had been bundled with third-party <b>renderers</b> such as NVIDIA MentalRay or Iray.|$|E
50|$|This {{difference}} between displacement mapping in micropolygon <b>renderers</b> vs. displacement mapping in a non-tessellating (macro)polygon <b>renderers</b> can {{often lead to}} confusion in conversations between people whose exposure to each technology or implementation is limited. Even more so, as in recent years, many non-micropolygon <b>renderers</b> have added {{the ability to do}} displacement mapping of a quality similar to that which a micropolygon renderer is able to deliver naturally. To distinguish between the crude pre-tessellation-based displacement these <b>renderers</b> did before, the term sub-pixel displacement was introduced to describe this feature.|$|E
5000|$|Includes {{hardware-accelerated}} JOGL, OpenGL and Direct3D <b>renderers</b> (depending on platform) ...|$|E
40|$|We present several {{technical}} advancements {{developed at}} Rhythm & Hues for the efficient rendering of photorealistic fur in Ang Lee’s Oscar-winning feature film Life of Pi. We drew heavily on this work to stereoscopically render the tiger Richard Parker {{and several other}} animals, all with sufficient realism and aesthetic control to capture the director’s ambitious vision. We summarize existing work on which our implementation builds, and describe in detail some recent improvements {{in the areas of}} hair shading, performance optimizations, and post-rendering tools for motion blur and stereo image synthesis. A large proportion of render time was spent testing occlusion along hair-based gather rays, with layers of semitransparent fur being the most common and expensive occluder. We employed two techniques to accelerate this: The first was a modified form of the volumetric occlusion approximation described in [Neulander 2010], which reliably identified rays that were likely to be blocked by nearby skin, alowing the renderer to avoid tracing them. The remaining non-skin-bound rays were attenuated using an accurate raytraced estimation of their occlusion. The second optimization leveraged our <b>renderer’s</b> dual representation of fur as both scanline triangles and raytraced hair primitives. This allowed using screen door transparency to accelerate the ray tracing: For the ray-occluding fur, semitransparent tube primitives were made opaque and their thicknesses were correspondingly reduced so as to preserve coverage. These operations were applied at the control vertices of each strand, allowing for precise lengthwise variation in opacity and thickness. By making these strands thin and opaque, we eliminated the need to trace multiple levels of transparency rays through them, and we also reduced the number of ray intersections by shrinking the ray targets. This produced a dramatic, artifact-free speed increase for secondary gather rays, while preserving the desired look of true transparency for the primary rays...|$|R
40|$|Completed in 1966, St Peter's Seminary at Cardross, Dunbartonshire, {{has been}} {{described}} as the ‘greatest modernist building in Scotland’. Designed as a space for the collective training of Roman Catholic priests, the building was effectively <b>renderered</b> obsolete before it was even completed when Vatican II (1962) championed community-based training programmes for priests. Eventually abandoned in 1980, the building has fallen into dilapidation and ruin, although the performance company NVA are planning an experimental restoration of at least certain bare elements of the building complex. This article explores {{the extent to which the}} initial structure and the ruin of Cardross can be viewed through the lens of Negri's definition of the transition to postmodernity as a 1960 s crisis of modernist regimes of measure and the functional logic of the factory system. That system provided the organisational basis of post-war society in imposing a series of spatial and temporal divisions in everyday social life in order to organise production. The organising principle of a Seminary like St Peter's Cardross can be conceived of as imposing a similar, indeed an even more comprehensive control over the entire spatio-temporal existence of the young priests within its walls. To a certain extent this rigid division can be seen in the concrete form of the building. The Seminary was designed such that the modulor dimension of one student priest's dorm is presented as an arch on the façade, and the series of these arches making up the length of the building is thus an expression of the individual's fully incorporated existence within the institution. To what extent, then, can the abandonment and gradual ruination of the strict spatially segregated and segmented complex of St Peter's Cardross be read as an extreme paradigm for the dismantling of the regime based on measures that came along with the western crisis in capital and collapse of industrial production from the 1960 s on? This question is examined through an analysis of the cinematic representation of St Peter's in two experimental films, Space and Light (1972, Murray Grigor), produced when it was a working seminary, and Space and Light Revisited (2009, Murray Grigor), an attempted shot-for-shot remake of the original filmed when it was in a state of decay. Both films have been screened simultaneously and side-by-side (at the film's 2009 Glasgow premiere and at the 2010 Berlinale) and the article explores the specific experiences of this viewing experience. Cinema regularly deploys ruins as the locus for melodramatic action, operating as a visual parallel of the onscreen action; however, in these films St Peter's is the central object of attention, and a celebration of, and lament for, the original is offered up. While early experimental filmmakers experimented with temporality by rewinding film to reconstruct demolished walls or buildings, in this setting, the indexical qualities of the cinematic apparatus are deployed to juxtapose past and present. In doing so, they highlight the referential nature of both the cinema and the ruin as this high point of Scottish architectural modernism, commissioned by the church, lies in a state of ruination...|$|R
50|$|Recent {{developments}} seem {{to indicate}} that some of the <b>renderers</b> that use sub-pixel displacement move towards supporting higher level geometry too. As the vendors of these <b>renderers</b> are likely to keep using the term sub-pixel displacement, this will probably lead to more obfuscation of what displacement mapping really stands for, in 3D computer graphics.|$|E
5000|$|The {{widespread}} use of [...] "boxed beef", where the beef was cut into consumer portions at packing plants rather than local butcher shops and markets, meant that fat and meat scraps for <b>renderers</b> stayed at the packing plants and were rendered there by packer <b>renderers,</b> {{rather than by the}} [...] "independent" [...] rendering companies.|$|E
5000|$|As of Chrome version 23, seccomp-bpf {{is used to}} sandbox the <b>renderers.</b>|$|E
5000|$|QuickDraw GX [...] "font scalers" [...] were <b>renderers</b> for the {{different}} font formats.|$|E
50|$|For example, Arion,FluidRay Indigo Renderer,LuxRender,mental ray,Mitsuba,Octane Render, Spectral Studio,Thea RenderandOceandescribe {{themselves}} as spectral <b>renderers.</b>|$|E
50|$|AV {{receivers}} {{may also}} be known as digital audio-video receivers or digital media <b>renderers.</b>|$|E
5000|$|It {{is one of}} the <b>renderers</b> {{available}} for use in GNU's Gnash Flash player.|$|E
