7|51|Public
40|$|Abstract—We {{present a}} lesion {{detection}} and characterization method for-fluorodeoxyglucose positron emission tomog-raphy—computed tomography (FDG PET-CT) {{images of the}} thorax {{in the evaluation of}} patients with primary nonsmall cell lung cancer (NSCLC) with regional nodal disease. Lesion detection can be difficult due to low contrast between lesions and normal anatomical structures. Lesion characterization is also challenging due to similar spatial characteristics between the lung tumors and abnormal lymph nodes. To tackle these problems, we propose a context driven approximation (CDA) method. There are two main components of our method. First, a sparse representation tech-nique with region-level contexts was designed for lesion detection. To discriminate low-contrast data with sparse representation, we propose a <b>reference</b> <b>consistency</b> constraint and a spatial consis-tent constraint. Second, a multi-atlas technique with image-leve...|$|E
40|$|Efficient {{techniques}} are introduced {{in this paper}} for {{the identification of the}} occlusion and visible background and foreground areas in a noisy stereoscopic image pair. Three different Bayes decision methods are tested for this purpose. The first, and simplest, uses three hypotheses for the formulation of the Bayes decision rules, adopting the right image as a reference. After performing a dual-Bayes decision test having each time as a different image of the stereo pair as <b>reference,</b> <b>consistency</b> checking is added to these tests to form the second method. Finally, four compound hypotheses are used in the third method, which is the most accurate but also the more detailed and computationally involved of the three. Experimental results illustrating the performance of the proposed {{techniques are}} presented and evaluated...|$|E
40|$|We {{give details}} of models for {{rational}} torus equivariant homotopy theory based on (a) all subgroups, connected subgroups or dimensions of subgroups and (b) on pairs or general flags. We provide comparison functors {{and show the}} models are equivalent. This is used in (A) An algebraic model for rational torus equivariant spectra (with B. Shipley; the new version of 1101. 2511), (B) An algebraic model for the toral part of rational equivariant G-spectra for an arbitrary compact Lie group (1501. 03425) and (C) construction of torus equivariant spectra from algebraic geometric data. It also highlights {{the role of the}} localization theorem in these models. Comment: v 2 is a minor polishing. Improved wording, cross <b>reference,</b> <b>consistency</b> of notation. Added exampl...|$|E
40|$|In writing <b>reference</b> material, <b>consistency</b> of {{organization}} and presentation is key. If {{the same information}} is presented in a consistent order and style throughout the publication or information set, it enhances the readability and usability of the material for the consumer. Reference materials may include such documents as encyclopedias, dictionaries, parts supply lists, maintenanc...|$|R
40|$|The paper {{describes}} the basic {{ideas and the}} main features of {{a new class of}} constitutive laws, in the framework of incrementally non-linear constitutive equations. CLoE is a generic name for that new class of laws, with <b>reference</b> to <b>consistency</b> at the limit surface, and explicit localization analysis. A top-down analysis of the model is presented, and illustrated by examples. Peer reviewe...|$|R
40|$|Health Officer, and the Office of Risk Management {{to provide}} minimum {{standards}} for guidance and consistency in {{management of the}} Automated External Defibrillator (AED) device programs for the University of Michigan Ann Arbor campus. The regional campuses are encouraged to use this document as a <b>reference</b> and for <b>consistency...</b>|$|R
40|$|We {{participated in}} NTCIR- 5 WEB Navigational Retrieval Subtask(Navi- 2) {{in order to}} verify the most {{effective}} retrieval method for the index of anchor texts by using a retrieval system that indexed only anchor texts instead of full texts of Web pages. We introduced retrieval methods that combine {{one or more of}} six retrieval measures: (a) anchor frequency (af), (b) <b>reference</b> <b>consistency</b> (rc), (c) query weight (qw), (d) page representativeness (rep), (e) site relevancy (sr), and (f) inverse anchor document frequency (iadf). The experimental results revealed that: (1) it could be implied that the retrieval method that used only anchor frequency for the index of anchor texts was more effective than the retrieval method for the index of only full texts of Web pages, and that (2) the retrieval method that contained sr or iadf was effective for the index of anchor texts, and that sr was more effective than iadf...|$|E
40|$|The Transactional memory Coherence and Consistency (TCC) {{provides}} a shared memory {{model in which}} atomic transactions are always the basic unit of parallel work, communication, memory coherence, and memory <b>reference</b> <b>consistency.</b> TCC greatly simplifies parallel software by {{eliminating the need for}} synchronization using conventional locks and semaphores, along with their complexities. TCC hardware must combine all writes from each transaction region in a program into a single packet and broadcast this packet to the permanent shared memory state atomically as a large block. This simplifies the coherence hardware because it reduces the need for small, low-latency messages and completely eliminates the need for conventional snoopy cache coherence protocols, as multiple speculatively written versions of a cache line may safely coexist within the system. Meanwhile, automatic, hardwarecontrolled rollback of speculative transactions resolves any correctness violations that may occur when several processors attempt to read and write the same data simultaneously. The cost of this simplified scheme is the higher interprocessor bandwidth...|$|E
40|$|This {{article was}} {{published}} in the journal GPS Solutions [© Springer-Verlag]. The final publication is available at link. springer. com and the definitive version is available at: [URL] the traditional Maximum Likelihood-based range domain multiple <b>reference</b> <b>consistency</b> check (MRCC) has limitations in satisfying the integrity requirement of CAT II/III for civil aviation, a Kalman filter-based position domain method has been developed for fault detection and exclusion in the Local Area Augmentation System MRCC process. The position domain method developed in this paper seeks to address the limitations of range domain-based MRCC by focusing not only on improving the performance of the fault detection but also on the integrity risk requirement for MRCC. In addition, the issue of the stability of the Kalman filter in relation to the position domain approach is considered. GPS range corrections from multiple reference receivers are fused by the adaptive Kalman filter at the master station for detecting and excluding the single reference receiver’ failure. The performance of the developed Kalman filter-based MRCC has been compared with the traditional method using experimental data. The results reveal that the vertical protection level is slightly better in the traditional method compared with the developed Kalman filter-based approach under the fault-free case. However, the availability can be improved to over 97...|$|E
5000|$|Jordan {{also used}} the column to {{highlight}} the inconsistencies in the F.A. disciplinary panel. The same panel decided no disciplinary action would be taken against Paul Jewell, manager of Wigan, who had been charged {{on the basis of}} making comments similar to Jordan's. Jordan described the compliance process as being [...] "based on mood", noting: [...] "There's no frame of <b>reference,</b> no <b>consistency</b> - and, yes, it's personal. It amounts to me not being able to say a referee is incompetent while others can, using the same language." ...|$|R
40|$|To make a {{prediction}} {{of a response}} variable from an explanatory one which takes into account features such as multimodality, a nonparametric approach based on {{an estimate of the}} conditional density is advocated and considered. In particular, we build point and interval predictors based on the quantile-copula estimator of the conditional density by Faugeras (20098. Faugeras, O. P. (2009). A quantile-copula approach to conditional density estimation. J. Multivariate Anal. 100 (9) : 2083 – 2099. View all <b>references).</b> The <b>consistency</b> of these predictors is proved through a uniform consistency result of the conditional density estimator. Eventually, the practical implementation of these predictors is discussed. A simulation on a real data set illustrates the proposed methods...|$|R
40|$|An {{integrated}} approach using advanced bioinformatics tools and targeted {{gene expression analysis}} was carried out to evaluate the potential use of seven “housekeeping” genes, commonly employed as internal controls in real-time PCR analysis. Adequate testing of <b>reference</b> gene <b>consistency,</b> is always necessary to validate data from any new experimental conditions, since a unique “housekeeping” suitable for every species, organ, developmental stage and treatment does not exist. Thus, the expression stability of glyceraldehyde- 3 -phosphate dehydrogenase, elongation factor 1 _, actin 11, beta- 6 tubulin, polyubiquitin 10, 18 S rRNA and 5 S rRNA genes were evaluated after exposure to low temperatures and in different organs of Beta vulgaris ssp. vulgaris plantlets. The cDNA sequences were derived from GenBank (NCBI) and TIGR-BvGI (Beta Vulgaris Gene Index), a platform providing high-fidelity tentative consensus (TC), obtained by a reliable and stringent ESTs analysis...|$|R
40|$|Rationale and ObjectivesDevelopment of imaging {{biomarkers}} often {{relies on}} their correlation with histopathology. Our {{aim was to}} compare two approaches for correlating pathology to multiparametric magnetic resonance (MR) imaging (mpMRI) for localization and quantitative assessment of prostate cancer (PCa) index tumor using whole mount (WM) pathology (WMP) as the reference. Materials and MethodsPatients (N =  30) underwent mpMRI that included diffusion-weighted imaging and dynamic contrast-enhanced (DCE) MRI at 3 T before radical prostatectomy (RP). RP specimens were processed using WM technique (WMP) and findings summarized in a standard surgical pathology report (SPR). Histology index tumor volumes (HTVs) were compared to MR tumor volumes (MRTVs) using two approaches for index lesion identification on mpMRI using annotated WMP slides as the reference (WMP) and using routine SPR as the <b>reference.</b> <b>Consistency</b> of index tumor localization, tumor volume, and mean values of the derived quantitative parameters (mean apparent diffusion coefficient [ADC], Ktrans, and ve) were compared. ResultsIndex lesions from 16 of 30 patients met the selection criteria. There was WMP/SRP agreement in index tumor in 13 of 16 patients. ADC-based MRTVs were larger (P    0. 8; P < . 05). No significant differences were observed in the mean values of Ktrans and ADC between the WMP and SPR. ConclusionsWMP correlation is superior to SPR for accurate localization of all index lesions. The use of WMP is however not required to distinguish significant differences of mean values of quantitative MRI parameters within tumor volume...|$|E
40|$|The method {{introduced}} here {{allows us}} to use a data set with a non-restricted number of outcomes, here 21. Hence, our method complements the other ones developed {{in the domain of}} the probability triangle. Individual parameters are estimated for expected utility and various non-expected utility theories. We use CRRA and CARA utility functions, both without and with the assumption of weakly concavity. Rank-dependent utility, prospective <b>reference</b> and cognitive <b>consistency</b> theories emerge from the others. Copyright Kluwer Academic Publishers 2002 experiments, individual functionals, non-expected utility,...|$|R
40|$|Single ion {{activity}} coeffients of NaCl, KCl, HCl and CaCl 2 {{in aqueous solution}} have been estimated by means of ion selective electrode (ISE) potentiometric measurements. Two methods are described for the calibration of the electrodes within the extended Debye-Hückel concentration range, using the Henderson-Bates approximation for the diffusion potential arising at the <b>reference</b> electrode. The <b>consistency</b> of the results indicates that the junction potentials in the examined systems calculated by the Henderson-Bates approximation are of reasonable precision. The published methods and data might be useful to develop single ion parameters for individual {{ion activity}} coefficient models...|$|R
40|$|The American Society of Clinical Oncology/College of American Pathologists {{guidelines}} {{highlighted the}} critical importance of quality assurance in diagnostic testing for HER 2. Unstained formalin-fixed, paraffin-embedded human breast carcinoma cell line sections were circulated to scheme participants on 9 occasions. “Reference laboratories” reported {{results for the}} HER 2 /chromosome 17 ratio and HER 2 copy number for 3 years for each cell line, including 418 sets of results (1, 671 results total). The number of participants was 62 laboratories in the final analysis. The mean and SD of results from <b>reference</b> laboratories demonstrated <b>consistency</b> during the 3 -year period. The percentage of laboratories achieving “appropriate” results ranged from 45...|$|R
40|$|We {{discuss how}} {{integrity}} consistency constraints between different UML models can be precisely defined at a language level. In doing so, we introduce a formal object-oriented metamodeling approach. In the approach, integrity consistency constraints between UML models are {{defined in terms}} of invariants of the UML model elements used to define the models at the language-level. Adopting a formal approach, constraints are formally defined using Object-Z. We demonstrate how integrity consistency constraints for UML models can be precisely defined at the language-level and once completed, the formal description of the consistency constraints will be a precise <b>reference</b> of checking <b>consistency</b> of UML models as well as for tool development...|$|R
40|$|This {{dissertation}} {{defines a}} Type Theory based semantics for Java-like reference type constructors. The primary focus is made on finding an adequate axiomatization of reference types in Type Theory. An extension of Type Theory, called Reference Type Theory, is introduced. It {{adds to the}} Type Theory language a reference type constructor and operations on reference type elements as primitive notions. The dissertation provides informal graph-based semantics for the Reference Type Theory, describes inference rules for this theory, and proves their <b>consistency.</b> <b>Reference</b> Type Theory is formalized in the Nuprl Proof Development System. This formalization is used to define a formal semantics for a fragment of the Java programming language and to verify several simple Java programs...|$|R
3000|$|... [7, 8], the {{literature}} was analyzed and recommendations were formulated. A level of proof was defined for each literature reference {{according to the}} type of study, and could be reevaluated taking into account the methodological quality of the study. The literature references common to each assessment criterion were then grouped. An overall level of proof was determined for each assessment criterion taking into account the levels of proof of each literature <b>reference,</b> the <b>consistency</b> of the results between the different studies, the cost-effectiveness analysis, the directness of the evidence, etc. A “strong” level of proof enabled a “strong” recommendation (“must be done,” “must not be done…”). A “moderate,” “weak,” or “very weak” level of proof resulted in a “conditional” recommendation (“should probably be done”, “should probably not be done…”). The proposed recommendations were presented and discussed one by one. The aim was not necessarily to reach a single and convergent opinion for all proposals, but rather to highlight points of agreement, and points of disagreement or indecision. Each recommendation was then scored by each of the experts on a scale of 1 (complete disagreement) to 9 (complete agreement). The collective scoring was established using the RAND/UCLA appropriateness method [...]...|$|R
40|$|Animals {{can learn}} that {{initially}} neutral stimuli (conditioned stimuli, CSs) may predict biologically significant events (unconditioned stimuli, USs). They {{respond to the}} CS with behaviour anticipating the US, irrespective of whether the US {{is a consequence of}} their own behaviour (operant or instrumental conditioning; Skinner, 1938) or appears independently of it (classical or Pavlovian conditioning; Pavlov, 1927). During the investigation of associative learning, a number of phenomena have been found that are consistently observed across various experimental designs as well as across species (see e. g. Lattal and Nakajima, 1998; Mackintosh, 1990; Pearce, 1997; Williams, 1994, and <b>references</b> therein). This <b>consistency</b> has led to the conclusion that some ‘learning rules’ might be common to all animal species, at least among vertebrates (e. g. Mackintosh, 1975; McHose and Moore, 1976...|$|R
40|$|The recent BICEP 2 {{measurement}} of B-modes in the polarization {{of the cosmic}} microwave background suggests that inflation was driven by a field at an energy scale of 2 × 10 ^ 16 GeV. I explore the potential of upcoming CMB polarization experiments to further constrain the physics underlying inflation. If the signal is confirmed, then two sets of experiments covering larger area will shed light on inflation. Low resolution measurements can pin down the tensor to scalar ratio at the percent level, thereby distinguishing models from one another. A high angular resolution experiment will be necessary to measure the tilt of the tensor spectrum, testing the consistency relation that relates the tilt to the amplitude. Comment: 5 pages, 7 figures; <b>references</b> updated and <b>consistency</b> relation fixed leading to some plot change...|$|R
50|$|Inter-consistency is {{achieved}} when references across language boundaries can be resolved. This kind of consistency {{can be further}} subdivided into (1) model-to-model consistency and (2) model-to-code consistency. Model-to-model consistency concerns the referential integrity as well as high-level constraints of the system. In the create survey example, the default-entity-name attribute from the Service listing refers to the name attribute from Entity listing. If we change one of these values without updating the other, we break the <b>reference.</b> More high-level <b>consistency</b> constraints across different models also exist as discussed later. A project can have certain patterns or conventions for naming and relating model elements. Current development environments must be tailored to specific languages with handwritten plugins or similar mechanisms in order to enforce consistency between languages such as those from the previous example.|$|R
40|$|The aims of {{the draft}} Financial Services Reform Bill 2000 (CLERP 6) are hard to fault — being to {{increase}} harmonisation, efficiency and consistency {{in the conduct of}} financial markets and regulation of financial products. These aims were expressed in the original position paper, CLERP 6 (December 1997), but any assessment of whether the legislation, released as an exposure draft on 11 February this year, achieves the aims is problematic since, despite repeated <b>references</b> to <b>consistency,</b> harmonisation of rules, single market licences and single regulators, the original proposals themselves contained many exclusions and variations. Even at the high policy level of CLERP 6, the claims that the many current differences and inconsistencies could be resolved with uniform and consistent treatment of the market and products on a functional basis seemed contradicted by the proposed level of exemption, variation, flexibility and interpretation, either stated explicitly or proposed to be exercisable by the three regulators (ASIC, APRA and RBA) and industry bodies. The generally favourable response by industry participants (see eg Hutley, (2000) BCLB [69]) can be attributed in part to support of the worthy aims and in part to the lack of detail available for assessment. Before another version of the draft legislation arrives, it is worth re-examining the underlying principles stated earlier and questioning whether the aims are achievable, appropriate or desirable...|$|R
40|$|Knowledge Management (KM) {{has become}} the focus of a lot of {{scientific}} research {{during the second half of}} the twentieth century as researchers discovered the importance of the knowledge resource to business organizations. Recent research developed ontology-based knowledge management systems (KMS) to provide a standardized <b>reference</b> for knowledge <b>consistency.</b> However, use of ontologies has been impeded by the difficulties encountered in building ontologies, especially difficulties in the knowledge acquisition stage. It is hypothesized that NLP tools can be usefully implemented to assist in the knowledge acquisition stage for ontology building in specific, and to develop effective KMS’s in general. The proposed system, CRISP, utilizes a shallow parser for extracting concept relations from construction contract documents to assist in the development of an ontology-based KMS. When compared with human evaluators, CRISP achieved almost 80 % of the average kappa score attained by the evaluators, and approximately 90 % of their F-measure score...|$|R
40|$|ABSTRACT. The space {{astrometry}} mission GAIA will construct a dense optical QSO-based celestial <b>reference</b> frame. For <b>consistency</b> between the optical and radio positions, {{it will be}} important to align the GAIA frame and the International Celestial Reference Frame (ICRF) with the highest accuracy. Currently, it is found that only 10 % of the ICRF sources are suitable to establish this link, either because they are not bright enough at optical wavelengths or because they have significant extended radio emission which precludes reaching the highest astrometric accuracy. In order to improve the situation, we have initiated a VLBI survey dedicated to finding additional high-quality radio sources for aligning the two frames. The sample consists of about 450 sources, typically 20 times weaker than the current ICRF sources, which have been selected by cross-correlating optical and radio catalogues. The paper presents the observing strategy and includes preliminary results of observation of 224 of these sources with th...|$|R
40|$|In {{considering}} {{the task of}} a teachers college, it is natural {{that those who are}} in charge of similar subjects at the college and attached schools should work up problems common among them. Collaborative Research Group of Social Studies (organized last spring; abreviated "CRS") consists of social study-educators and classroom-teachers of schools (from kindergarten to senior high school) attached to our college. CRS is attempting to analyze and review continuously theoretical and practical problems which are urged to be resolved in schools today. This is the first report that we have met together and discussed lessons of social studies since last spring. In this report are contained discussions on the progress, the research theme, and the perspective of CRS. Our present sub-themes are as follows: (1) Organization of teaching contents, with particulary <b>reference</b> to the <b>consistency</b> and principle of selection. (2) Development of children concerning knowledge, concept, position and behavior. (3) Analysis of the teaching and learning process, with materials, methods, and structures in the process...|$|R
40|$|In Distributed Shared Memory {{design the}} memory {{consistency}} model used {{can have a}} major impact on the efficiency of the system. In this paper we describe previous work on consistency models with particular <b>reference</b> to weak <b>consistency</b> models. Weak consistency models use synchronization points as checkpoints at which the memory is made consistent. When designing a consistency model for RHODOS, we propose to go one step further and use the synchronization operations to demarcate the sections of code in which shared data is accessed. Only this shared data will be kept consistent. 1. Introduction Uniprocessors offer programmers a simple and intuitive memory model since any read operation returns the most recently written value of the given memory location. Similarly, a write operation will alter the value of the memory location which will be maintained/stored until the next write operation. Multiprocessors, however, have a far more complex memory implementation and model. The terms most [...] ...|$|R
40|$|AbstractBackgroundIrritable Bowel Syndrome (IBS) is a {{disorder}} characterized by abdominal pain or discomfort associated {{with changes in}} bowel habit. Currently there are no objective outcome measures for evaluating the effectiveness of treatments for this disorder. AimsTo determine the usefulness of a method of analysis that employs polar vectors {{to evaluate the effectiveness}} of IBS treatments. MethodsData from a Phase IV clinical study with 1677 active IBS-Rome III patients who received 100 mg of pinaverium bromide+ 300 mg of simethicone (PB+S) po bid for a period of four weeks were used for the analysis. Using the Bristol Stool Scale as a <b>reference,</b> the <b>consistency</b> and frequency of each type of bowel movement were recorded weekly in a Bristol Matrix (BM) and the data were expressed as polar vectors. ResultsThe analysis showed a differential response to the PB+S treatment among the IBS subtypes: in reference to the IBS with constipation subtype, the magnitude of the vector increased from 10. 2 to 12. 5, reaching maximum improvement at two weeks of treatment (p< 0. 05, Scheffé). In the IBS with diarrhea and mixed IBS subtypes, the magnitude of the vector decreased from 19 to 14 (p< 0. 05) and from 16. 5 to 13 (p< 0. 05), respectively, with continuous improvement for a period of four weeks. There was no definable vectorial pattern in the unsubtyped IBS group. ConclusionsAnalysis with polar vectors enables treatment response to be measured in different IBS subtypes. All the groups showed improvement with PB+S, but each one had its own characteristic response in relation to vector magnitude and direction. The proposed method can be implemented in clinical studies to evaluate the efficacy of IBS treatments...|$|R
40|$|International audienceThe European space {{astrometry}} mission Gaia will construct a dense optical QSO-based celestial <b>reference</b> frame. For <b>consistency</b> between optical and radio positions, {{it will be}} fundamental to align the Gaia and VLBI frames with the highest accuracy. A proper alignment is also important {{in the framework of}} astrophysics, for example to probe properly the AGN jets properties and the physics of these objects. The VLBI-Gaia frame alignment requires quasars that are bright at optical wavelength, that have a compact radio core, and that do not exhibit complex structures. In this paper, we draw prospects for this alignment, based on the ICRF 2 catalogue and an ongoing dedicated VLBI project designed to observe additional weaker extragalactic radio sources for this purpose. The list of suitable sources will have to be monitored to check the relevance of the sources for the alignment, especially in terms of position stability and structures. Accordingly, we present the observations we envision in the framework of the IVS and other VLBI networks, before and during the Gaia mission...|$|R
40|$|Group work {{is one of}} the {{teaching}} strategies used by teachers who attempt to make their teaching more effective. Collaborative work is an important aspect of group work. Even though group work is used by some teachers in their classrooms, there is a considerable variation in the extent and nature of such work. Therefore, it is necessary to examine group and collaborative work, to indicate their importance and the variable use of these techniques. In this study, firstly, collaborative work has been defined and then its importance, purposes, benefits for both students and teachers, and some problems that teachers confront in their classrooms have been discussed. Secondly, the variation between the teachers who are using group work, collaborative work, individual or whole class work have been explored and different kinds of group work have been described. Then, some research evidences on group work have been <b>referenced</b> and the <b>consistency</b> of results has been discussed. Finally, some recommendations for teachers in using group work and collaborative work have been made...|$|R
40|$|Sustainable {{development}} places {{emphasis on}} long-term benefits to society. This paper explores the evolution since 1992 of criteria {{for sustainable development}} at the operational level, comparing the contrasting policy contexts of Hong Kong and Guangzhou, both major cities in the Pearl River Delta of southern China. It contrasts the translation of Agenda 21, a global action guide, into local land use policy in both cities. The paper examines the organizational structure for land use administration and the response to Agenda 21 of both cities. It compares the characteristics of local policies for sustainable development, namely SUSDEV 21 (for Hong Kong) and Guangzhou Agenda 21 by <b>reference</b> to their <b>consistency</b> with Agenda 21 and contrasts apparent differences in their respective response to Agenda 21. More to the point, both cities illustrate the shortcomings of local policy in providing effective support to sustainable development at the operational level. It suggests the establishment of clear criteria in local land use policy for implementing sustainable development. Department of Building and Real Estat...|$|R
40|$|Background: Irritable Bowel Syndrome (IBS) is a {{disorder}} characterized by abdominal pain or discomfort associated {{with changes in}} bowel habit. Currently there are no objective outcome measures for evaluating the effectiveness of treatments for this disorder. Aims: To determine the usefulness of a method of analysis that employs polar vectors {{to evaluate the effectiveness}} of IBS treatments. Methods: Data from a Phase IV clinical study with 1677 active IBS-Rome III patients who received 100  mg of pinaverium bromide +  300  mg of simethicone (PB + S) po bid for a period of four weeks were used for the analysis. Using the Bristol Stool Scale as a <b>reference,</b> the <b>consistency</b> and frequency of each type of bowel movement were recorded weekly in a Bristol Matrix (BM) and the data were expressed as polar vectors. Results: The analysis showed a differential response to the PB + S treatment among the IBS subtypes: in reference to the IBS with constipation subtype, the magnitude of the vector increased from 10. 2 to 12. 5, reaching maximum improvement at two weeks of treatment (p <  0. 05, Scheffé). In the IBS with diarrhea and mixed IBS subtypes, the magnitude of the vector decreased from 19 to 14 (p< 0. 05) and from 16. 5 to 13 (p <  0. 05), respectively, with continuous improvement for a period of four weeks. There was no definable vectorial pattern in the unsubtyped IBS group. Conclusions: Analysis with polar vectors enables treatment response to be measured in different IBS subtypes. All the groups showed improvement with PB + S, but each one had its own characteristic response in relation to vector magnitude and direction. The proposed method can be implemented in clinical studies to evaluate the efficacy of IBS treatments...|$|R
40|$|A {{recurrent}} {{theme in}} economic analysis is {{the criticism of}} the traditional method based on the investigation of ‘normal’ or long-period positions (LPP) considered as centres of gravitation for the actual positions of the economy. The paper discusses this criticism with particular <b>reference</b> to the <b>consistency</b> of the long-period method (LPM) with the analysis of technological change and competition. It is argued {{that the role of}} the tendency towards the LPP was seen by traditional economists as {{part of the process of}} selection from among competing technologies. This process was regarded as leading to the emergence of dominant techniques determining normal prices and profitability. The concept of competition covered by the notion of gravitation towards the LPP is not, therefore, restricted to the diffusion of given technologies, but includes the introduction of new techniques. In addition, traditional economists did not look at the adjustment towards the LPP as a fully accomplished process, but only as tendency obstructed by proprietary knowledge and by the emergence of further innovations. The paper underlines the limits of the Schumpeterian criticism to marginalism and shows the advantages of the Classical approch vis-à-visthe traditional and neo-Walrasian neoclassical approaches in dealing with technical change...|$|R
40|$|Java {{virtual machine}} (JVM) is a widely-used code {{execution}} environment {{which does not}} depend on any architecture, and it is recently used not only with Java language but also with other languages such as Scheme and ML. On JVM, however, all values are statically-typed as either immediate or <b>reference,</b> and its <b>consistency</b> is veried before execution to prove that invalid memory access will never happen. This property sometimes makes implementation of other languages on JVM inecient. In particular, implementation of dynamically-typed language is very inecient because all possible values including frequently-used ones such as integers must be represented by instances of a class. In this thesis, I extended the JVM by conversions between references and integers and by runtime legality check of the references, without modifying the instruction set. This makes implementation of dynamically-typed language on JVM more ecient, without breaking both binary-compatibility of existing bytecode and safety from in-valid memory accesses. I also modied an existing Scheme implementation to use this extension and got about 93 % reduction in calculation time of integer recursive func-tions. Performance penalty for existing code is currently from 0 % to about 20 % and can be removed by static type information...|$|R
40|$|National audienceThe space {{astrometry}} mission Gaia will construct a dense optical QSO-based celestial <b>reference</b> frame. For <b>consistency</b> between optical and radio positions, {{it will be}} important to align the Gaia frame and the International Celestial Reference Frame (ICRF) with the highest accuracy. Currently, it is found that only 10 % of the ICRF sources (70 sources) are suitable to establish this link, either because they are not bright enough at optical wavelengths or because they have significant extended radio emission which precludes reaching the highest astrometric accuracy. In order to improve the situation, we have initiated a VLBI survey dedicated to finding additional suitable radio sources for aligning the two frames. The sample consists of about 450 sources, typically 20 times weaker than the current ICRF sources, which have been selected by cross-correlating optical and radio catalogues. This paper presents the observing strategy to detect, image, and measure accurate positions for these sources. It also provides results about the VLBI detectability of the sources, as derived from initial observations with the European VLBI Network in June and October 2007. Based on these observations, an excellent detection rate of 89 % is found, which is very promising for the continuation of this project...|$|R
40|$|The space {{astrometry}} mission Gaia will construct a dense optical QSO-based celestial <b>reference</b> frame. For <b>consistency</b> between optical and radio positions, {{it will be}} important to align the Gaia and VLBI frames (International Celestial Reference Frame) with the highest accuracy. In this respect, it is found that only 10 % of the ICRF sources are suitable to establish this link (70 sources), either because most of the ICRF sources are not bright enough at optical wavelengths or because they show extended radio emission which precludes reaching the highest astrometric accuracy. In order to improve the situation, we initiated a multi-step VLBI observational project, dedicated to finding additional suitable radio sources for aligning the two frames. The sample consists of about 450 optically-bright radio sources, typically 20 times weaker than the ICRF sources, which have been selected by cross-correlating optical and radio catalogs. The initial observations, aimed at checking whether these sources are detectable with VLBI, and conducted with the European VLBI Network (EVN) in 2007, showed an excellent 90 % detection rate. This paper reports on global VLBI observations carried out in March 2008 to image 105 from the 398 previously detected sources. All sources were successfully imaged, revealing compact VLBI structure for about half of them, which is very promising for the future...|$|R
