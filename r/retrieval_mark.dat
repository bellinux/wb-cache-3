0|17|Public
40|$|The Tibetan trisyllabic light verb {{construction}} {{is a type}} of widely used verb phrase that is composed of a disyllabic noun or adjective and a light verb. A large number of Tibetan trisyllabic light verb constructions are widely found in Tibetan. Successfully recognizing this type of phrase greatly contributes to Tibetan information processing, however, thorough and systematic academic research in this field has not yet been launched. Therefore, we propose a model for the recognition of Tibetan trisyllabic light verb constructions based on an integrated strategy in this paper. Firstly, we extract all trisyllabic light verb construction candidates from a Tibetan corpus. In this step, light verbs are used as <b>retrieval</b> <b>marks.</b> Secondly, we filter candidates using a statistics-based model, rule-based model, and integrated model separately. Experimental results show that the integrated model performs much better than the other strategies, which proves that linguistic features contribute a lot to the automatic recognition of Tibetan trisyllabic light verb constructions by computers...|$|R
30|$|This method {{draws on}} the {{accuracy}} and recall rate in text <b>retrieval,</b> first <b>marking</b> the segmented image, and mapping {{the relevance of the}} tag to the text, so that the performance evaluation in the text retrieval can be introduced into the image segmentation performance evaluation. The accuracy rate P refers to the ratio of all pixel markers whose correct pixel markings match the artificial results. The recall rate R refers to the proportion of pixels marked correctly in the mark of the result of artificial marking. Gupta S [21] and others used AP as the evaluation standard parameter for image segmentation in research results.|$|R
40|$|International audienceNowadays, content <b>retrieval</b> is <b>marking</b> the Internet usage. User {{communications}} are {{no longer}} tied up to host interconnection. Information Centric Networking (ICN) models are proposed to cope with these changes. The new paradigm redesigns the Internet architecture to bring out content to the first level. Over the last decade, many key projects have proposed a large solution spectrum to rebuilt networking primitives focused on the content. One important and direct challenge of this shift is {{the large amount of}} routing states due to identifying contents rather than hosts. In this paper, we focus especially on DONA, one of the first ICN architecture, and analyse the required memory space to store routing states. Our study shows that today’s technologiesare not able to satisfy the content routing needs. Thus, we propose an enhancement of DONA called BADONA to deal with this problem. It uses a Bloom filter to drastically reduce theusage of the memory space...|$|R
40|$|In {{the frame}} of the ESA-funded project “GDP 5. 0 -Upgrade of the GOME Data Processor for Improved Total Ozone Columns”, total ozone {{estimates}} from different GODFIT/GDP 5 configurations were compared with the current operational GOME GDP 4. x data products (including the latest algorithm improvements) and with ground based total ozone data from quality controlled Brewer, Dobson and SAOZ measurements available at WOUDC and NDACC. The different configurations of the total ozone retrieval algorithms include the use of different input temperatures (ECMWF analyses or TOMS v 8 climatological database with a retrieved temperature shift), different a priori ozone profile databases (TOMS v 8, and various modes of NNORSY) and different schemes for the cloud treatment (OCRA/ROCINN, FRESCO+). GOME retrievals using all possible combinations were also compared with GDP 4. x data products. The use of different combinations of the above a priori choices in the <b>retrievals</b> introduce <b>marked</b> deviations in the amplitude of the seasonal dependence of the differences between satellite and ground-based data, in the solar zenith angle dependence of the satellite retrievals, and in their offset and latitudinal dependence. Irrespective of said differences in retrieval configuration, all GDP 5 products are in better agreement with the ground-based measurements for extreme cases such as high solar zenith angles, low stratospheric temperatures, low total ozone conditions, etc. than the respective GDP 4. x total ozone data products...|$|R
40|$|Our {{group has}} built an {{information}} retrieval {{system based on}} a complex semantic markup of medical textbooks. We describe {{the construction of a}} set of webbased knowledge-acquisition tools that expedites the collection and maintenance of the concepts required for text markup and the search interface required for information <b>retrieval</b> from the <b>marked</b> text. In the text markup system, domain experts (DEs) identify sections of text that contain one or more elements from a finite set of concepts. End users can then query the text using a predefined set of questions, each of which identifies a subset of complementary concepts. The search process matches that subset of concepts to relevant points in the text. The current process requires that the DE invest significant time to generate the required concepts and questions. We propose a new system [...] - called ACQUIRE (Acquisition of Concepts and Queries in an Integrated Retrieval Environment) [...] - that assists a DE in two essential tasks in the te [...] ...|$|R
40|$|Enterprise intranets {{are often}} sparse in nature, with limited use of {{alternative}} lexical representations between authors, making query expansion (QE) ineffective. Hence, for some enterprise search queries, {{it can be}} advantageous to instead use the well-known collection enrichment (CE) method to gather higher quality pseudo-feedback documents from a more diverse external resource. However, {{it is not always}} clear for which queries the collection enrichment technique should be applied. In this paper, we study two different approaches, namely a predictor-based approach and a divergence-based approach, to decide on when to apply CE. We thoroughly evaluate both approaches on the TREC Enterprise track CERC test collection and its corresponding topic sets, in combination with three different external resources and nine different query performance predictors. Our results show that both approaches are effective to selectively apply CE for enterprise search. In particular, the divergence-based approach leads to consistent and <b>marked</b> <b>retrieval</b> improvements over the systematic application of QE or CE on all external resources...|$|R
40|$|While a {{dominant}} (additive, stationary) Gaussian noise component in image data {{will ensure that}} wavelet coefficients are of Gaussian distribution, long tailed distributions (symptomatic, for example, of extreme values) may well hold in practice for wavelet coefficients. Energy (2 nd order moment) has often been used for image characterization for image content-based retrieval, and higher order moments may be important also, not least for capturing long tailed distributional behavior. In this work, we assess 2 nd, 3 rd and 4 th order moments of multiresolution transform – wavelet and curvelet transform – coefficients as features. As analysis methodology, taking account of image types, multiresolution transforms, and moments of coefficients in the scales or bands, we use correspondence analysis as well as k-nearest neighbors supervised classification. Key words: image grading, content-based image retrieval, wavelet and curvelet transforms, moments, variance, skewness, kurtosis. 1 Image Grading as a Content-Based Image Retrieval Problem The success of content-based image finding and <b>retrieval</b> is most <b>marked</b> whe...|$|R
40|$|Abstract—Soft {{biometric}} traits {{embedded in}} a face (e. g., gender and facial marks) are ancillary information and are not fully distinctive by themselves in face-recognition tasks. However, this information can be explicitly combined with face matching score to improve the overall face-recognition accuracy. Moreover, in certain application domains, e. g., visual surveillance, where a face image is occluded or is captured in off-frontal pose, soft biometric traits can provide even more valuable information for face matching or <b>retrieval.</b> Facial <b>marks</b> can also be useful to differentiate identical twins whose global facial appearances are very similar. The similarities found from soft biometrics can also be useful {{as a source of}} evidence in courts of law because they are more descriptive than the numerical matching scores generated by a traditional face matcher. We propose to utilize demographic information (e. g., gender and ethnicity) and facial marks (e. g., scars, moles, and freckles) for improving face image matching and retrieval performance. An automatic facial mark detection method has been developed that uses 1) the active appearance model for locating primary facial features (e. g., eyes, nose, and mouth), 2) the Laplacian-of-Gaussian blob detection, and 3) morphological operators. Experimental results based on the FERET database (426 images of 213 subjects) and two mugshot databases from the forensic domain (1225 images of 671 subjects and 10 000 images of 10 000 subjects, respectively) show that the use of soft biometric traits is able to improve the face-recognition performance of a state-of-the-art commercial matcher. Index Terms—Demographic information, face marks, face recognition, face retrieval, soft biometrics...|$|R
40|$|Abstract. [...] Lake trout Salvelinus {{namaycush}} fry with thermal {{marks in}} their otoliths were stocked annually at Sve’s Reef in Lake Superior from 1994 to 1996 {{in an attempt}} to supple-ment lake trout populations that include both naturally reproduced fish and those stocked as fin-clipped yearlings. Lake trout from these year classes were sampled during annual gill net assessments from 2000 to 2002, and otoliths from unclipped lake trout captured near the stocking reef were processed and examined for the presence of a thermal mark. Thermal marks were identified in otoliths from 21 fish out of 836 examined. The accuracy of mark identification was confirmed by examining and correctly identifying otoliths from a random mixture of hatchery-reared marked fish and recaptured clipped lake trout (without thermal marks). While thermal marks are relatively simple and inexpensive to apply to large numbers of fry, <b>mark</b> <b>retrieval</b> is more labor-intensive and time-consuming. Many otoliths were ex-amined from fish in non-target year-classes due to the wide overlap in size-at-age of this species. Most lake trout in the targeted year-classes were immature...|$|R
40|$|We {{propose to}} utilize micro features, namely facial marks (e. g., freckles, moles, and scars) to improve face {{recognition}} and <b>retrieval</b> performance. Facial <b>marks</b> {{can be used}} in three ways: i) to supplement the features in an existing face matcher, ii) to enable fast retrieval from a large database using facial mark based queries, and iii) to enable matching or retrieval from a partial or profile face image with marks. We use Active Appearance Model (AAM) to locate and segment primary facial features (e. g., eyes, nose, and mouth). Then, Laplacian-of-Gaussian (LoG) and morphological operators are used to detect facial marks. Experimental results based on FERET (426 images, 213 subjects) and Mugshot (1, 225 images, 671 subjects) databases show that the use of facial marks improves the rank- 1 identification accuracy of a stateof-the-art face recognition system from 92. 96 % to 93. 90 % and from 91. 88 % to 93. 14 %, respectively. Index Terms — face recognition, facial marks, soft biometrics, local features, Active Appearance Model 1...|$|R
40|$|This paper {{describes}} the operational {{implementation of the}} processor phi-IASI over the Mediterranean sea. The phi-IASI model implements two physically based inversion algorithms for the sequential retrieval of (a) the thermodynamic state of the atmosphere and (b) the tropospheric content of CO, CO 2, CH 4, and N 2 O from hyperspectral radiance observations of the Infrared Atmospheric Sounding Interferometer (IASI). The retrieval algorithm for trace gases exploits the concept of partially scanned interferogram technique, which is a tool mostly suited for Fourier transform spectrometers in the infrared. Minor and trace gases retrievals for July 2010 are presented and compared to in situ observations from five Mediterranean, permanent, stations of the Global Atmospheric Watch (GAW) network. The comparison evidences a good general consistency between satellite and in situ observations. IASI <b>retrievals</b> show a <b>marked</b> southeastern gradient, which is shown {{to be consistent with}} the general tropospheric circulation over the Mediterranean basin. These patterns are barely seen from in situ observations, a fact which stresses the importance of satellite (trace gases) data assimilation to improve the performance and quality of trace gases transport models...|$|R
40|$|This study evaluates NO 2 {{vertical}} tropospheric column densities (VTCs) {{retrieved from}} {{measurements of the}} Scanning Imaging Absorption Spectrometer for Atmospheric Chartography (SCIAMACHY) above Switzerland and the Alpine region. A clear relationship between a spatially and temporally highly resolved Swiss NO x emission inventory and SCIAMACHY NO 2 columns under anticyclonic meteorological conditions supports the general ability of SCIAMACHY to detect sources of NO x pollution in Switzerland. Summertime NO x lifetime estimates derived from this relation agree reasonably with values from literature. A further evaluation of the SCIAMACHY data {{is based on the}} comparison with NO 2 VTCs retrieved from the Global Ozone Monitoring Experiment (GOME). The annual mean NO 2 VTCs calculated from both data sets clearly show the advantage of the improved SCIAMACHY pixel resolution for qualitatively estimating the NO x pollution distribution in a small country such as Switzerland. However, a more quantitative comparison of seasonally averaged NO 2 VTCs gives evidence for SCIAMACHY NO 2 VTCs being systematically underestimated over the Swiss Plateau during winter. A possible explanation for this problem (not reported in earlier literature) is the use of inaccurate satellite pixel surface pressures derived from coarsely resolved global models in the <b>retrieval.</b> The <b>marked</b> topography in the Alpine region can lead to deviations of several hundred meters between the assumed and the real mean surface height over a pixel. A sensitivity study based on selected clear sky SCIAMACHY NO 2 VTCs over the Swiss Plateau and two fixed a priori NO 2 profile shapes indicates that inaccurate pixel surface pressures have a considerable effect of up to 40 % on the retrieved NO 2 columns. For retrievals in the UV-visible spectral range with a decreasing sensitivity towards the earth's surface, this effect is of major importance when the NO 2 resides close to the ground, which occurs most pronounced during the winter season...|$|R
40|$|We are {{focusing}} on information access tasks characterized by large volume of hypermedia connected technical documents, a need for rapid and effective access to familiar information, and long-term interaction with evolving information. The problem for technical users is to build and maintain a personalized task-oriented model of the information to quickly access relevant information. We propose a solution which provides user-centered adaptive information retrieval and navigation. This solution supports users in customizing information access over time. It is complementary to information discovery methods which provide access to new information, since it lets users customize future access to previously found information. It relies on a technique, called Adaptive Relevance Network, which creates and maintains a complex indexing structure to represent personal user's information access maps organized by concepts. This technique is integrated within the Adaptive HyperMan system, which helps NASA Space Shuttle flight controllers organize and access large amount of information. It allows users to select and mark any part of a document as interesting, and to index that part with user-defined concepts. Users can then do subsequent <b>retrieval</b> of <b>marked</b> portions of documents. This functionality allows users to define and access personal collections of information, which are dynamically computed. The system also supports collaborative review by letting users share group access maps. The adaptive relevance network provides long-term adaptation based both on usage and on explicit user input. The indexing structure is dynamic and evolves over time. Leading and generalization support flexible retrieval of information under similar concepts. The network is geared towards more recent information access, and automatically manages its size {{in order to maintain}} rapid access when scaling up to large hypermedia space. We present results of simulated learning experiments...|$|R
40|$|This study evaluates NO 2 {{vertical}} tropospheric column densities (VTCs) {{retrieved from}} {{measurements of the}} Scanning Imaging Absorption Spectrometer for Atmospheric Chartography (SCIAMACHY) above Switzerland and the Alpine region. The close correlation between pixel averaged NO x emission rates from a spatially and temporally highly resolved inventory and the NO 2 VTCs under anticyclonic meteorological conditions demonstrates the general ability of SCIAMACHY to detect sources of NO x pollution in Switzerland. This correlation is further used to infer seasonal mean NO x lifetimes carefully {{taking into account the}} influence of the strong diurnal cycle in NO x emissions on these estimates. Lifetimes are estimated to 3. 6 (± 0. 8) hours in summer and 13. 1 (± 3. 8) hours in winter, the winter value being somewhat lower than previous estimates. A comparison between the 2003 - 2005 mean NO 2 VTC distribution over Switzerland and the corresponding 1996 – 2003 mean from the Global Ozone Monitoring Experiment (GOME) illustrates the much better capability of SCIAMACHY to resolve regional scale pollution features. However, the comparison of seasonal averages over the Swiss Plateau with GOME and ground based in situ observations indicates that SCIAMACHY exhibits a too weak seasonal cycle with comparatively high values in summer and low values in winter. A problem likely contributing to the reduced values in winter (not reported in earlier literature) is the use of inaccurate satellite pixel surface pressures derived from a coarse resolution global model in the <b>retrieval.</b> The <b>marked</b> topography in the Alpine region can lead to deviations of several hundred meters between the model assumed and the real pixel-averaged surface height. A sensitivity study based on selected clear sky SCIAMACHY NO 2 VTCs over the Swiss Plateau and two fixed a priori NO 2 profile shapes indicates that inaccurate pixel surface pressures affect retrieved NO 2 columns over complex terrain by up to 40 %. For retrievals in the UV-visible spectral range with a decreasing sensitivity towards the earth's surface, this effect is of major importance when the NO 2 resides close to the ground, a situation most frequently observed during winter...|$|R
40|$|In {{the present}} paper {{we set out to}} discuss the role of mark-up in a large XML-annotated, TEI-conformant corpus. The corpus in {{question}} – called CorDis – is a large multimodal, multigenre collection of texts representing {{a significant portion of the}} political and media discourse on 2003 Iraq conflict. Our main concern here is to deal with some key methodological issues from the point of view of those who got their hands dirty tagging the texts for assembly in a homogeneously encoded corpus. CorDis was generated from various subcorpora assembled by various research groups for various discourse analytical purposes. At the outset, each subcorpus was “mildly” annotated on the basis of specific research objectives and hypotheses. This heterogeneity of data corresponded to a wide range of methods employed to mark up the texts, annotation being added on each occasion according to the specific research interests of each group. Clearly, once the CorDis corpus was set up, a considerable amount of work on standardization had to be done, especially to make all documents XML-valid and therefore ready to be indexed and interrogated with Xaira (XML-Aware Indexing and <b>Retrieval</b> Application). The <b>marking</b> up of the whole corpus – and of the corpus as a whole – entailed various levels of interpretation accounting for a series of choices: from the selection of relevant information, through the choice of appropriate tag sets, to the harmonization of mark-up. The TEI (Text Encoding Initiative) guidelines proved a valid instrument to achieve standardization of mark-up, providing for a hierarchical organization of metadata and giving the corpus a sound structure. The main purpose of this paper is precisely to show the process of harmonization whereby a loose collection of texts has become a stable architecture. By means of examples we discuss issues like consistency and re-usability. In particular, we argue that the crucial role of annotation leads to a reconsideration of the definition of corpus itself, in which special emphasis is placed on mark-up being part and parcel of the corpus, rather than a superimposed accessory...|$|R

