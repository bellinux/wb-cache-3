10000|213|Public
25|$|Tasks {{that fall}} within the {{paradigm}} of <b>reinforcement</b> <b>learning</b> are control problems, games and other sequential decision making tasks.|$|E
25|$|The {{three major}} {{learning}} paradigms each correspond {{to a particular}} learning task. These are supervised learning, unsupervised learning and <b>reinforcement</b> <b>learning.</b>|$|E
25|$|Going {{beyond the}} {{specific}} problem of learning states and transformations, {{the task of}} clustering also admits a fully quantum version, wherein both the oracle which returns the distance between data-points and the information processing device which runs the algorithm are quantum. Finally, a general framework spanning supervised, unsupervised and <b>reinforcement</b> <b>learning</b> in the fully quantum setting was introduced in, where it was also shown {{that the possibility of}} probing the environment in superpositions permits a quantum speedup in <b>reinforcement</b> <b>learning.</b>|$|E
40|$|Abstract — A new <b>reinforcement</b> <b>learned</b> neural network, {{that follows}} {{the ideas of the}} minibrain network but {{includes}} exploration and learns through both positive and negative feedback, is proposed. The proposed ReL network is evaluated against the minibrain network in the n × n grid world domain and the taxi domain and is shown to perform significantly better than the minibrain network. I...|$|R
30|$|Finally, the {{proposed}} framework is {{entirely in the}} realm of supervised learning. Additional research can explore extending the framework to problems of interest in the realms of unsupervised or <b>reinforcement</b> machine <b>learning.</b>|$|R
5000|$|... 3. It is {{sustained}} {{according to}} the <b>reinforcement</b> principles of <b>learning</b> theory ...|$|R
25|$|<b>Reinforcement</b> <b>learning</b> {{has been}} used to acquire {{behavior}} in a hierarchical control system in which each node can learn to improve its behavior with experience.|$|E
25|$|Deep {{feedforward}} {{neural networks}} {{were used in}} conjunction with <b>reinforcement</b> <b>learning</b> by AlphaGo, Google Deepmind's program that was the first to beat a professional human Go player.|$|E
25|$|<b>Reinforcement</b> <b>learning</b> {{is a third}} {{branch of}} machine learning, {{distinct}} from supervised and unsupervised learning, which also admits quantum enhancements. In quantum-enhanced <b>reinforcement</b> <b>learning,</b> a quantum agent interacts with a classical environment and occasionally receives rewards for its actions, which allows the agent to adapt its behaviour—in other words, to learn {{what to do in}} order to gain more rewards. In some situations, either because of the quantum processing capability of the agent, or due to the possibility to probe the environment in superpositions, a quantum speedup may be achieved. Implementations of these kinds of protocols in superconducting circuits and in systems of trapped ions have been proposed.|$|E
40|$|Copyright © 2015, International Foundation for Autonomous Agents and Multiagent Systems (www. ifaamas. org). All rights reserved. We {{propose a}} new {{conceptual}} multi-agent framework which, given {{a game with}} an undesirable Nash equilibrium, will almost surely generate a new Nash equilibrium at some predetennined, more desirable pure action profile. The agent(s) targeted for <b>reinforcement</b> <b>learn</b> independently according to a standard model-free algorithm, using internally-generated states corresponding to high-level preference rankings over outcomes. We focus in particular on the {{case in which the}} additional reward can be considered as resulting from an internal (re-) appraisal, such that the new equilibrium is stable independent of the continued application of the procedure...|$|R
50|$|With {{the aim of}} {{adapting}} the curriculum to individual needs the school runs enrichment classes. These are held after school hours and consist of small groups where discussion, problem-solving, <b>reinforcement</b> of <b>learning</b> and close teacher student interaction takes place.|$|R
50|$|Burrhus F. Skinner (1904-1990) was {{the founder}} of operant {{conditioning}} which uses punishment and <b>reinforcement</b> as <b>learning</b> tools. This learning method is not as limited as the previous learning form. Operant conditioning is only limited by what {{can be used as}} reinforcement or punishment.|$|R
25|$|Amplitude {{amplification}} {{is often}} combined with quantum walks {{to achieve the}} same quadratic speedup. Quantum walks have been proposed to enhance Google's PageRank algorithm as well as the performance of <b>reinforcement</b> <b>learning</b> agents in the projective simulation framework.|$|E
25|$|In August 2014 at Usenix WoOT conference, Bursztein et al. {{presented}} the first generic CAPTCHA-solving algorithm based on <b>reinforcement</b> <b>learning</b> and demonstrated its efficiency against many popular CAPTCHA schemas. They concluded that text distortion based CAPTCHAs schemes {{should be considered}} insecure moving forward.|$|E
25|$|ANNs are {{frequently}} used in <b>reinforcement</b> <b>learning</b> {{as part of}} the overall algorithm. Dynamic programming was coupled with ANNs (giving neurodynamic programming) by Bertsekas and Tsitsiklis and applied to multi-dimensional nonlinear problems such as those involved in vehicle routing, natural resources management or medicine, because of the ability of ANNs to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of the original control problems.|$|E
40|$|We {{address the}} problem of how to <b>reinforcement</b> <b>learn</b> in ultra-complex environments, with huge state spaces, where one must learn to exploit compact {{structure}} of the problem domain. The approach we propose is to simulate the evolution of an artificial economy of computer programs. The economy is constructed based on two simple principles so as to assign credit to the individual programs for collaborating on problem solutions. We find empirically that, starting from programs that are random computer code, we are able to evolve systems that solve hard problems. In particular our economy as learned to solve almost all random Blocks World problems with goal stacks 200 blocks high. Competing methods solve such problems only up to goal stacks of at most 8 blocks. Our economy has also learned to unscramble about half a randomly scrambled Rubik's cube, and to solve several among a collection of commercially sold puzzles...|$|R
40|$|Exploration is a {{key part}} of <b>reinforcement</b> <b>learnning.</b> In the classic setting, {{autonomous}} agents are supposed to learn a model of their environment to succesfuly complete a task. Recent works in the field and in related fields have suggested the use of quantities based on Shannon’s information theory to enable agents to do so. The underlying concepts of exploration vary between those works. In this thesis, these different notions of exploration will be introduced and compared. Further, two algorithms based on established dynamic programming methods are introduced to maximize two information theoretic quantities, the entropy of the state distribution and predictive information, a quantity relating {{the past and the future}} of the agent. These algorithms are evaluated in two settings: planning with the true world model and in interaction with the environment without prior knowledge. Entropy maximation proved to be possible in both settings while predictive information maximization was only succesful i...|$|R
40|$|This work covers with {{artificial}} intelligence of strategy computer games, however {{many of these}} methods are usable in other areas. These are different methods used in deciding (finite state machina, fuzzy logic, Markov Process), planning (Goal-oriented action planning, Montecarlo planning, Case-based planning) and machine <b>learning</b> (<b>Reinforcement</b> leasing, Decision <b>Learning</b> and Neural Networks). Objective of this thesis is to study this methods from different sources and explain their base principle. Then few of this methods resolve in more details and implement them (goal oriented planning and state machine). This thesis focuses on game engine ORTS, which is used in implementing and testing methods...|$|R
25|$|Markov {{chains are}} used {{throughout}} information processing. Claude Shannon's famous 1948 paper A Mathematical Theory of Communication, {{which in a}} single step created the field of information theory, opens by introducing the concept of entropy through Markov modeling of the English language. Such idealized models can capture many of the statistical regularities of systems. Even without describing the full structure of the system perfectly, such signal models can make possible very effective data compression through entropy encoding techniques such as arithmetic coding. They also allow effective state estimation and pattern recognition. Markov chains also {{play an important role}} in <b>reinforcement</b> <b>learning.</b>|$|E
25|$|Unsupervised {{learning}} {{is the ability}} to find patterns in a stream of input. Supervised learning includes both classification and numerical regression. Classification is used to determine what category something belongs in, after seeing a number of examples of things from several categories. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. In <b>reinforcement</b> <b>learning</b> the agent is rewarded for good responses and punished for bad ones. The agent uses this sequence of rewards and punishments to form a strategy for operating in its problem space. These three types of learning can be analyzed in terms of decision theory, using concepts like utility. The mathematical analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory.|$|E
500|$|Kenji Doya {{has argued}} that the cerebellum's {{function}} is best understood not in terms of the behaviors it affects, but the neural computations it performs; the cerebellum consists {{of a large number of}} more or less independent modules, all with the same geometrically regular internal structure, and therefore all, it is presumed, performing the same computation. If the input and output connections of a module are with motor areas (as many are), then the module will be involved in motor behavior; but, if the connections are with areas involved in non-motor cognition, the module will show other types of behavioral correlates. Thus the cerebellum has been implicated in the regulation of many differing functional traits such as affection. emotion and behavior. The cerebellum, Doya proposes, is best understood as predictive action selection based on [...] "internal models" [...] of the environment or a device for supervised learning, in contrast to the basal ganglia, which perform <b>reinforcement</b> <b>learning,</b> and the cerebral cortex, which performs unsupervised learning.|$|E
40|$|We {{introduce}} relational temporal difference {{learning as}} an effective approach to solving multi-agent Markov decision problems with large state spaces. Our algorithm uses temporal difference <b>reinforcement</b> to <b>learn</b> a distributed value function represented over a conceptual hierarchy of relational predicates. We present experiments using two domains from the General Game Playing repository, in which we observe that our system achieves higher learning rates than nonrelational methods. We also discuss related work and directions for future research. 1...|$|R
40|$|Abstract: Computer {{technology}} {{has made it}} very easy to access great resources for language education, but students still need guidance from instructors. Most often at the intermediate level, students experience learning plateaus where they cannot see clear progress despite continued effort, and the internet, as a vast resource of authentic material, often daunts them. How can we, as language instructors, help them? In this paper, I experiment with one approach employing dictation, or audial input with visual <b>reinforcement</b> for <b>learning</b> retentio...|$|R
40|$|Techniques for peer {{reinforcement}} {{of social work}} training programs {{appear to be a}} promising complement to educational supervision. The authors conducted a quasi-experimental evaluation of its effectiveness for <b>reinforcement</b> of <b>learning,</b> of interviewing skills. Subjects were income maintenance workers in a large state public welfare agency. Findings indicated that peer reinforcement may have resulted in knowledge retention and use of skills which were superior overall to those demonstrated among trainees denied access to peer reinforcement techniques. Interpretation of findings and productive areas for future research are suggested...|$|R
500|$|As a god, {{the player}} can teach their {{creature}} to perform {{tasks such as}} stocking the village store or performing miracles. The creature is taught what and when to eat, and how to attack or impress enemy villages. Fighting skills may be taught in one-on-one battles with other creatures; attack and defence abilities can be improved. Teaching is performed using a <b>reinforcement</b> <b>learning</b> system: if the creature does something the player does not want, it can be discourage with a slap. If the creature does something the player approves of, it can be stroked. The creature remembers the response to various actions and gradually changes its behaviour accordingly. With time and repetition, it can perform complex functions that allow it {{to serve as the}} player's avatar. Three types of leashes are used to command the creature to go to a specific place, and can be tied to a building to restrict movement. One leash encourages the creature to pay attention when actions are demonstrated; the others encourage either benevolent or malevolent behaviour. [...] The game reinforces the creature's choices and learning by providing visual feedback, and the creature has an alignment separate from the player's. Evil wolves sport glowing eyes and large fangs and claws; good ones turn a shade of purple and glow gently.|$|E
2500|$|<b>Reinforcement</b> <b>{{learning}}</b> [...] an area {{of machine}} learning in computer science, concerned with how an agent ought to take actions in an environment so as to maximize some notion of cumulative reward.|$|E
2500|$|In <b>reinforcement</b> <b>learning,</b> data [...] {{are usually}} not given, but {{generated}} by an agent's interactions with the environment. At each point in time , the agent performs an action [...] and the environment generates an observation [...] and an instantaneous cost , according to some (usually unknown) dynamics. The aim is to discover a policy for selecting actions that minimizes some measure of a long-term cost, e.g., the expected cumulative cost. The environment's dynamics and the long-term cost for each policy are usually unknown, but can be estimated.|$|E
40|$|I am {{indebted to}} Alexander M. Buchwald for his {{painstaking}} criticisms Although {{the interpretation of}} <b>reinforcement</b> in animal <b>learning</b> has been a focus of theoretical controversy for many decades, the corresponding issue as it arises in experimentation on human learning has been strangely quiescent {{for a very long}} period...|$|R
40|$|Learning needs {{analysis}} {{is important for}} designing continuing medical education (CME) to optimise clinical relevance, personal relevance and <b>reinforcement</b> of <b>learning.</b> 1, 2 A systematic review has shown that change in physician behaviour is greatest where educa-tion addresses deficiencies or barriers to change. 3 A learning needs measurement is not necessary. 2 The nature of the education dictates the form of assessment. Thus, CME providers undertake different needs assess-ments to registrars within The Royal Australian College of General Practitioners (RACGP) training program. 2 There is marked diversity in sources o...|$|R
40|$|Right {{brain damaged}} {{patients}} show impairments in sequential decision making tasks for which healthy {{people do not}} show any difficulty. We hypothesized that this difficulty {{could be due to}} the failure of right brain damage patients to develop well-matched models of the world. Our motivation is the idea that to navigate uncertainty, humans use models of the world to direct the decisions they make when interacting with their environment. The better the model is, the better their decisions are. To explore the model building and updating process in humans and the basis for impairment after brain injury, we used a computational model of non-stationary sequence <b>learning.</b> RELPH (<b>Reinforcement</b> and Entropy <b>Learned</b> Pruned Hypothesis space) was able to qualitatively and quantitatively reproduce the results of left and right brain damaged patient groups and healthy controls playing a sequential version of Rock, Paper, Scissors. Our results suggests that, in general, humans employ a sub-optimal <b>reinforcement</b> based <b>learning</b> method rather than an objectively better statistical learning approach, and that differences between right brain damaged and healthy control groups can be explained by different exploration policies, rather than qualitatively different learning mechanisms...|$|R
2500|$|Greater {{investment}} in brain emulation and associated cognitive science might enhance {{the ability of}} artificial intelligence (AI) researchers to create [...] "neuromorphic" [...] (brain-inspired) algorithms, such as neural networks, <b>reinforcement</b> <b>learning,</b> and hierarchical perception. This could accelerate risks from uncontrolled AI. Participants at a 2011 AI workshop estimated an 85% probability that neuromorphic AI would arrive before brain emulation. This {{was based on the}} idea that brain emulation would require understanding some brain components, and it would be easier to tinker with these than to reconstruct the entire brain in its original form. By a very narrow margin, the participants on balance leaned toward the view that accelerating brain emulation would increase expected AI risk.|$|E
2500|$|... {{and then}} culled {{according}} to the fitness function. The candidate controllers used in ER applications may be drawn from some subset of the set of artificial neural networks, although some applications (including SAMUEL, developed at the Naval Center for Applied Research in Artificial Intelligence) use collections of [...] "IF THEN ELSE" [...] rules as the constituent parts of an individual controller. [...] It is theoretically possible to use any set of symbolic formulations of a control law (sometimes called a policy in the machine learning community) as the space of possible candidate controllers. Artificial neural networks {{can also be used}} for robot learning outside the context of evolutionary robotics. [...] In particular, other forms of <b>reinforcement</b> <b>learning</b> can be used for learning robot controllers.|$|E
2500|$|Many of the {{commonly}} used {{machine learning algorithms}} require a set of training examples consisting of both a hypothetical input and a desired answer. [...] In many robot learning applications the desired answer is an action for the robot to take. These actions are usually not known explicitly a priori, instead the robot can, at best, receive a value indicating {{the success or failure}} of a given action taken. [...] Evolutionary algorithms are natural solutions to this sort of problem framework, as the fitness function need only encode {{the success or failure of}} a given controller, rather than the precise actions the controller should have taken. [...] An alternative to the use of evolutionary computation in robot learning is the use of other forms of <b>reinforcement</b> <b>learning,</b> such as q-learning, to learn the fitness of any particular action, and then use predicted fitness values indirectly to create a controller.|$|E
40|$|I study {{learning}} {{problems that have}} features of both <b>reinforcement</b> and supervised <b>learning.</b> Specifically, I have developed new algorithms for apprenticeship learning, imitation learning, and contextual bandit problems. I have also worked on game theory and boosting. Thesis research with Prof. Robert E. Schapire. ⋄ Adaptive search results for temporal queries June 2008 - September 200...|$|R
40|$|Using verbal material, {{this study}} {{explored}} {{the effect of the}} temporal interval of feedback as it interacted with two other variables: (1) method of presenting learning matena! (inductive or deductive), and (2) activity of the learner during the delay interval (activity relevant or irrelevant to the materia. The major objective was to compare the effect of immediate versus delayed feedback of knowledge upon the learning of principles, using principles typical of the content of lower division college instruction. The subjects were 277 lower division college students selected from a large introductoryipsychology class. The universal superiority of immediate reinforcement did not appear. The educational implications are: (1) the usual assumption of the efficiency of immediate reinforcement is not valid with respect to academic instruction; and (2) the apparent superiority of delayed knowledge <b>reinforcement</b> with deductive <b>learning</b> and immediate knowledge <b>reinforcement</b> with inductive <b>learning</b> may be utilized across...|$|R
40|$|Abstract. The {{influence}} of different surface forms on GFRP reinforcement mainly manifest in bonding between reinforcement materials and concrete, {{especially when the}} GFRP bars are used in slope, {{the form of the}} surface of GFRP bar will affect reinforced material’s torsion and shear properties directly. This article made some tensile tests on several different surface forms of GFRP <b>reinforcement,</b> and <b>learned</b> that the form of the surface wound of GFRP bar make the influence on tensile strength, ultimate elongation and modulus of elasticity. Test results show that the surface of the winding way will affect on the tensile strength, ultimate elongation and modulus of elasticity of GFRP bar...|$|R
