275|6283|Public
25|$|Below are the {{previous}} 5 matches {{of the national}} team. For all past match results, see the team's <b>results</b> <b>page.</b>|$|E
25|$|Google {{implemented}} {{an easter}} egg of Conway's Game of Life in 2012. Users who {{search for the}} term are shown an implementation {{of the game in}} the search <b>results</b> <b>page.</b>|$|E
25|$|Filth was the 1998 {{novel by}} Scottish writer Irvine Welsh. It was adapted into a 2013 {{film of the}} same name, {{directed}} by Jon S. Baird with James McAvoy in the lead role. In the film the Hearts supporting Officer goes to view his team's results in a shop window, {{at the top of}} the <b>results</b> <b>page</b> Dunfermline were said to have beaten Celtic at home by four goals to nil.|$|E
40|$|Most modern web {{search engines}} yield {{a list of}} {{documents}} of a fixed length (usually 10) {{in response to a}} user query. The next ten search results are usually available in one click. These documents either replace the current <b>result</b> <b>page</b> or are appended to the end. Hence, in order to examine more documents than the first 10 the user needs to explicitly express her intention. Although clickthrough numbers are lower for documents on the second and later <b>result</b> <b>pages,</b> they still represent a noticeable amount of trac. We propose a modification of the Dynamic Bayesian Net-work (DBN) click model by explicitly including into the model the probability of transition between <b>result</b> <b>pages.</b> We show that our new click model can significantly better capture user behavior on the second and later <b>result</b> <b>pages</b> while giving the same performance on the first <b>result</b> <b>page...</b>|$|R
40|$|When a query is {{submitted}} to a search engine, the search engine returns a dynamically generated <b>result</b> <b>page</b> containing the <b>result</b> records, each of which usually consists of a link to and/or snippet of a retrieved Web page. In addition, such a <b>result</b> <b>page</b> often also contains information irrelevant to the query, such as information related to the hosting site of the search engine and advertisements. In this paper, we present a technique for automatically producing wrappers {{that can be used}} to extract search result records from dynamically generated <b>result</b> <b>pages</b> returned by search engines. Automatic search result record extraction is very important for many applications that need to interact with search engines such as automatic construction and maintenance of metasearch engines and deep Web crawling. The novel aspect of the proposed technique is that it utilizes both the visual content features on the <b>result</b> <b>page</b> as displayed on a browser and the HTML tag structures of the HTML source file of the <b>result</b> <b>page.</b> Experimental <b>results</b> indicate that this technique can achieve very high extraction accuracy...|$|R
5000|$|The search engine's <b>result</b> <b>pages</b> {{provided}} {{access to}} three major components: ...|$|R
25|$|In October 2010, Soghoian filed a {{complaint}} with the FTC, in which he claimed that Google was intentionally leaking search queries to the sites that users visited after they clicked on a link from the search <b>results</b> <b>page.</b> Two weeks later, a law firm filed a class action lawsuit against Google for this practice. The lawsuit extensively quoted from Soghoian's FTC complaint. In October 2011, Google stopped leaking search queries to the sites that users visited, and then in 2015, the company settled the search query leakage class action lawsuit for 8.5 million dollars.|$|E
25|$|The {{search engine}} <b>results</b> <b>page</b> (SERP) {{is the actual}} result {{returned}} by a search engine {{in response to a}} keyword query. The SERP consists of a list of links to web pages with associated text snippets. The SERP rank of a web page refers to the placement of the corresponding link on the SERP, where higher placement means higher SERP rank. The SERP rank of a web page is a function not only of its PageRank, but of a relatively large and continuously adjusted set of factors (over 200). Search engine optimization (SEO) is aimed at influencing the SERP rank for a website or a set of web pages.|$|E
25|$|The popular idea {{seems to}} be that the Title is among the ranking factors and the {{description}} is more of an advertising for the searcher looking at the search results. In other words, while the Title tag is one of the factors determining if a page will rank for a certain keyword, the meta description will only help a search result stand out (or not, depending upon how well-written it is). If more people click on a result with a good description, it may reasonably be expected that Google will reward that same result with a higher ranking. This, of course, if visitors to the link do not immediately leave the page after visiting - that would indicate misleading or poor content. This, in fact, would be likely to demote the result on SERP (Search Engine <b>Results</b> <b>Page).</b>|$|E
5000|$|... #Caption: PROSESS {{overall and}} {{category}} quality indices {{on the main}} <b>result</b> <b>page</b> ...|$|R
5000|$|Search engine <b>result</b> <b>pages</b> are {{protected}} from automated access by {{a range of}} defensive mechanisms and the terms of service. These <b>result</b> <b>pages</b> are the primary data source for SEO companies, the website placement for competitive keywords became an important field of business and interest. Google has even used twitter to warn users against this practice ...|$|R
40|$|Abstract: Web {{databases}} generate query <b>result</b> <b>pages</b> {{based on}} a userâ€™s query. Automatically extracting the data from these query <b>result</b> <b>pages</b> {{is very important for}} many applications, such as data integration, which need to cooperate with multiple web databases. We present a novel data extraction and alignment method called CTVS that combines both tag and value similarity. CTVS automatically extracts data from query <b>result</b> <b>pages</b> by first identifying and segmenting the query result records (QRRs) in the query <b>result</b> <b>pages</b> and then aligning the segmented QRRs into a table, in which the data values from the same attribute are put into the same column. We also design a new record alignment algorithm that aligns the attributes in a record, first pair wise and then holistically, by combining the tag and data value similarity information. Experimental results show that CTVS achieves high precision and outperforms existing state-of-the-art data extraction methods...|$|R
2500|$|For all past match {{results of}} the {{national}} team, see [...] and the team's <b>results</b> <b>page.</b>|$|E
2500|$|Results: [...] PSD is {{produced}} by the program. [...] The <b>results</b> <b>page</b> is typically customizable for reporting units and graph style.|$|E
2500|$|On January 29, 2007, Local Universal {{results were}} {{upgraded}} and more data {{included in the}} main Google <b>results</b> <b>page.</b> On February 28, Google Traffic info was officially launched to automatically include real-time traffic flow conditions to the maps of 30 major cities in the United States. [...] On March 8, the Local Business Center was upgraded. On May 16, Google rolled out Universal search results, including more Map information on the main Google <b>results</b> <b>page.</b> On May 18, Google added neighborhood search capabilities. On May 29, Google driving directions support {{was added to the}} Google Maps API. The same day saw the launch of Street View, which gave a ground-level 360-degree view of streets in the major cities of the United States.|$|E
50|$|The <b>result</b> <b>page</b> {{for real}} estate listings {{integrates}} Google Maps to geolocate properties.|$|R
40|$|A <b>result</b> <b>page</b> of {{a modern}} {{commercial}} search engine often contains documents of different types targeted to satisfy different user intents (news, blogs, multimedia). When evaluating system performance and making design decisions we need to better understand user behavior on such <b>result</b> <b>pages.</b> To address this problem various click models have previously been proposed. In this paper we focus on <b>result</b> <b>pages</b> containing fresh <b>results</b> and propose a way to model user intent distribution and bias due to different document presentation types. To {{the best of our}} knowledge this is the first work that successfully uses intent and layout information to improve existing click models...|$|R
50|$|Results Pageviews / Search = Pageviews {{of search}} <b>result</b> <b>pages</b> / Total Unique Searches.|$|R
2500|$|The Entrez {{front page}} provides, by default, {{access to the}} global query. [...] All {{databases}} indexed by Entrez can be searched via a single query string, supporting boolean operators and search term tags to limit parts of the search statement to particular fields. [...] This returns a unified <b>results</b> <b>page,</b> that shows the number of hits for the search {{in each of the}} databases, which are also links to actual search results for that particular database.|$|E
2500|$|Websites {{consisting}} {{mostly of}} affiliate links have previously held a negative reputation for underdelivering quality content. In 2005 there were active changes made by Google, where certain websites were labeled as [...] "thin affiliates". Such websites were either removed from Google's index or were relocated within the <b>results</b> <b>page</b> (i.e., {{moved from the}} top-most results to a lower position). To avoid this categorization, affiliate marketer webmasters must create quality content on their websites that distinguishes their work {{from the work of}} spammers or banner farms, which only contain links leading to merchant sites.|$|E
50|$|For all past match {{results of}} the {{national}} team, see the team's 1922-69 <b>results</b> <b>page,</b> 1970-99 <b>results</b> <b>page</b> and 2000-present <b>results</b> <b>page.</b>|$|E
50|$|Search engine {{marketing}} (SEM) {{is a form}} of Internet marketing {{that involves}} the promotion of websites by increasing their visibility in search engine <b>results</b> <b>pages</b> (SERPs) primarily through paid advertising. SEM may incorporate search engine optimization (SEO), which adjusts or rewrites website content and site architecture to achieve a higher ranking in search engine <b>results</b> <b>pages</b> to enhance pay per click (PPC) listings.|$|R
40|$|A {{search engine}} {{returned}} <b>result</b> <b>page</b> may contain search {{results that are}} organized into multiple dynamically generated sections {{in response to a}} user query. Furthermore, such a <b>result</b> <b>page</b> often also contains information irrelevant to the query, such as information related to the hosting site of the search engine. In this paper, we present a method to automatically generate wrappers for extracting search result records from all dynamic sections on <b>result</b> <b>pages</b> returned by search engines. This method has the following novel features: (1) it aims to explicitly identify all dynamic sections, including those that are not seen on sample <b>result</b> <b>pages</b> used to generate the wrapper, and (2) it addresses the issue of correctly differentiating sections and records. Experimental results indicate that this method is very promising. Automatic search result record extraction is critical for applications that need to interact with search engines such as automatic construction and maintenance of metasearch engines and deep Web crawling. 1...|$|R
40|$|AbstractWe {{study the}} problem of caching query <b>result</b> <b>pages</b> in Web search engines. Popular search engines receive {{millions}} of queries per day, and for each query, return a <b>result</b> <b>page</b> to the user who submitted the query. The user may request additional <b>result</b> <b>pages</b> for the same query, submit a new query, or quit searching altogether. An efficient scheme for caching query <b>result</b> <b>pages</b> may enable search engines to lower their response time and reduce their hardware requirements. This work studies query result caching {{within the framework of}} the competitive analysis of algorithms. We define a discrete time stochastic model for the manner in which queries are submitted to search engines by multiple user sessions. We then present an adaptation of a known online paging scheme to this model. The expected number of cache misses of the resulting algorithm is no greater than 4 times the expected number of misses that any online caching algorithm will experience under our specific model of query generation...|$|R
5000|$|Large {{portions}} {{of a web}} site may be hidden in the Deep Web. For example, the <b>results</b> <b>page</b> behind a web form can lie in the Deep Web if crawlers cannot follow {{a link to the}} <b>results</b> <b>page.</b>|$|E
50|$|For all past match results, see the team's <b>results</b> <b>page.</b>|$|E
5000|$|... (See also Spokane County Auditor's {{candidate}} filing {{page and}} primary <b>results</b> <b>page)</b> ...|$|E
40|$|Web {{sites that}} rely on {{databases}} for their content are now ubiquitous. Query <b>result</b> <b>pages</b> are dynamically generated from these databases in response to user-submitted queries. Automatically extracting structured data from query <b>result</b> <b>pages</b> is a challenging problem, as {{the structure of the}} data is not explicitly represented. While humans have shown good intuition in visually understanding data records on a query <b>result</b> <b>page</b> as displayed by a web browser, no existing approach to data record extraction has made full use of this intuition. We propose a novel approach, in which we make use of the common sources of evidence that humans use to understand data records on a displayed query <b>result</b> <b>page.</b> These include structural regularity, and visual and content similarity between data records displayed on a query <b>result</b> <b>page.</b> Based on these observations we propose new techniques that can identify each data record individually, while ignoring noise items, such as navigation bars and adverts. We have implemented these techniques in a software prototype, rExtractor, and tested it using two datasets. Our experimental results show that our approach achieves significantly higher accuracy than previous approaches. Furthermore, it establishes the case for use of vision-based algorithms in the context of data extraction from web sites...|$|R
40|$|This paper {{investigates the}} {{composition}} of search engine <b>results</b> <b>pages.</b> We define what elements the most popular web search engines use on their <b>results</b> <b>pages</b> (e. g., organic results, advertisements, shortcuts) and to which degree they are used for popular vs. rare queries. Therefore, we send 500 queries of both types to the major search engines Google, Yahoo, Live. com and Ask. We count how often the different elements are used by the individual engines. In total, our study is based on 42, 758 elements. Findings include that search engines use quite different approaches to <b>results</b> <b>pages</b> composition and therefore, the user gets to see quite different results sets depending on the search engine and search query used. Organic results still play the {{major role in the}} <b>results</b> <b>pages,</b> but different shortcuts are of some importance, too. Regarding the frequency of certain host within the results sets, we find that all search engines show Wikipedia results quite often, while other hosts shown depend on the search engine used. Both Google and Yahoo prefer results from their own offerings (such as YouTube or Yahoo Answers). Since we used the. com interfaces of the search engines, results may not be valid for other country-specific interfaces. Comment: Search engines, evaluation, search engine <b>results</b> <b>pages,</b> search shortcut...|$|R
5000|$|... "Find in page" [...] {{search that}} carries Web search {{keywords}} into search <b>result</b> <b>pages</b> (2007) ...|$|R
5000|$|On certain sites, search {{from within}} the website on the <b>results</b> <b>page</b> ...|$|E
50|$|For all past match {{results of}} the {{national}} team, see the team's <b>results</b> <b>page.</b>|$|E
5000|$|For all past match {{results of}} the {{national}} team, see the teams <b>results</b> <b>page</b> ...|$|E
5000|$|Left side {{navigation}} pane. Includes navigation and, on <b>results</b> <b>pages,</b> related {{searches and}} prior searches ...|$|R
5000|$|Replace the set R of {{regions in}} <b>page</b> with the <b>resulting</b> <b>pages</b> from {{splitting}} S.|$|R
40|$|Abstract. Large {{portions}} of the Web are buried behind user-oriented interfaces, {{which can only be}} accessed by filling out forms. To make the therein contained information accessible to automatic processing, one of the major hurdles is to navigate to the actual <b>result</b> <b>page.</b> In this paper we present a framework for navigating these so-called Deep Web sites based on the page-keyword-action paradigm: the system fills out forms with provided input parameters and then submits the form. Afterwards it checks if it has already found a <b>result</b> <b>page</b> by looking for pre-specified keyword patterns in the current page. Based on the outcome either further actions to reach a <b>result</b> <b>page</b> are executed or the resulting URL is returned...|$|R
