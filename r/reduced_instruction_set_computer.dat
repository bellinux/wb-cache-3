143|7284|Public
25|$|MIPS is a <b>reduced</b> <b>{{instruction}}</b> <b>set</b> <b>computer</b> (RISC) {{instruction set}} architecture (ISA) developed by MIPS Technologies (formerly MIPS Computer Systems). The early MIPS architectures were 32-bit, with 64-bit versions added later. There are multiple versions of MIPS: including MIPS I, II, III, IV, and V; {{as well as}} five releases of MIPS32/64 (for 32- and 64-bit implementations, respectively). As of April 2017, the current version is MIPS32/64 Release 6. MIPS32/64 primarily differs from MIPS IV by defining the privileged kernel mode System Control Coprocessor {{in addition to the}} user mode architecture.|$|E
2500|$|A <b>reduced</b> <b>{{instruction}}</b> <b>set</b> <b>computer,</b> or RISC (pronounced 'risk', [...] ), is {{one whose}} instruction set architecture (ISA) has {{a set of}} attributes that allows it to have a lower cycles per instruction (CPI) than a {{complex instruction set computer}} (CISC). Various suggestions have been made regarding a precise definition of RISC, but the general concept is that of a computer that has a small set of simple and general instructions, rather than a large set of complex and specialized instructions. Another common RISC trait is their load/store architecture, where memory is only accessed through specific instructions, rather than as a part of most instructions.|$|E
2500|$|Intel {{had tried}} unsuccessfully to push Apple to migrate the Macintosh {{platform}} to Intel chips. Apple concluded that Intel's CISC (Complex Instruction Set Computer) architecture ultimately {{would not be able}} to compete against RISC (<b>Reduced</b> <b>Instruction</b> <b>Set</b> <b>Computer)</b> processors. While the Motorola 68040 [...] offered the same features as the Intel 80486 and could on a clock-for-clock basis significantly outperform the Intel chip, the 486 had the ability to be clocked significantly faster without suffering from overheating problems, especially the clock-doubled i486DX2 which ran the CPU logic at twice the external bus speed, giving such equipped IBM compatible systems a significant performance lead over their Macintosh equivalents. Apple's product design and engineering did not help matters as they restricted the use of the '040 to their expensive Quadras for a time while the 486 was readily available to OEMs as well as enthusiasts who put together their own machines. In late 1991, as the higher-end Macintosh desktop lineup transitioned to the '040, Apple was unable to offer the '040 in their top-of-the-line PowerBooks until early 1994 with the PowerBook 500 series, several years after the first 486-powered IBM compatible laptops hit the market which cost Apple considerable sales. In 1993 Intel rolled out the Pentium processors as the successor to the 486, while the Motorola 68050 was never released, leaving the Macintosh platform a generation behind IBM compatibles in the latest CPU technology. In 1994, Apple abandoned Motorola CPUs for the RISC PowerPC architecture developed by the AIM alliance of Apple Computer, IBM, and Motorola. The Power Macintosh line, the first to use the new chips, proved to be highly successful, with over a million PowerPC units sold in nine months. However, in the long run, spurning Intel for the PowerPC was a mistake as the commoditization of Intel-architecture chips meant Apple couldn't compete on price against [...] "the Dells of the world".|$|E
50|$|Their design uses a fixed {{architecture}} - {{it requires}} {{changes in the}} wiring if the <b>instruction</b> <b>set</b> is modified or changed.This architecture is preferred in <b>reduced</b> <b>instruction</b> <b>set</b> <b>computers</b> (RISC) as they use a simpler <b>instruction</b> <b>set.</b>|$|R
50|$|Chris Rowen {{from the}} Cadence Design Systems, San Jose, CA was named Fellow of the Institute of Electrical and Electronics Engineers (IEEE) in 2016 for {{leadership}} {{in the development of}} microprocessors and <b>reduced</b> <b>instruction</b> <b>set</b> <b>computers.</b>|$|R
40|$|<b>Reduced</b> <b>instruction</b> <b>set</b> <b>computers</b> aim {{for both}} {{simplicity}} in hardware and synergy between architectures and compilers. Optimizing compilers {{are used to}} compile programming languages down to instructions that are as unencumbered as microinstructions in a large virtual address space, {{and to make the}} instruction cycle time as fast as possible. As circuit technologies reduce the relative cost of proc-essing and memory, <b>instruction</b> <b>sets</b> that are too com-plex become a distinct liability to performance. The designers of <b>reduced</b> <b>instruction</b> <b>set</b> <b>computers</b> (RISCs) strive for both simplicity in hardware and synergy be-tween architecture and compilers, in order to stream-line processing as much as possible. Early experience indicates that RISCs can in fact run much faster than more conventionally designed machines. BACKGROUN...|$|R
50|$|The acronym ZISC {{alludes to}} <b>reduced</b> <b>instruction</b> <b>set</b> <b>computer</b> (RISC).|$|E
50|$|POWER is a <b>reduced</b> <b>{{instruction}}</b> <b>set</b> <b>computer</b> (RISC) {{instruction set}} architecture (ISA) developed by IBM. The name is {{an acronym for}} Performance Optimization With Enhanced RISC.|$|E
5000|$|In 2002, he {{was made}} a Fellow of the Computer History Museum [...] "for his {{development}} and implementation of <b>reduced</b> <b>instruction</b> <b>set</b> <b>computer</b> architecture and program optimization technology." ...|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimitedA definition of <b>Reduced</b> <b>Instruction</b> <b>Set</b> <b>Computers</b> is developed. A computer performance model {{which allows the}} evaluation of architectural alternatives is presented. An example {{on the use of}} the model to compute the performance alternatives for a given application is presented to study the effect of the addition of an instruction to. a processor <b>instruction</b> <b>set.</b> [URL] Portuguese Nav...|$|R
40|$|Retargetable Instruction Scheduling for Pipelined Processors by David Gordon Bradlee Chairperson of the Supervisory Committee: Professor Susan J. Eggers Department of Computer Science and Engineering Retargetable code {{generators}} {{for complex}} <b>instruction</b> <b>set</b> <b>computers</b> (CISCs) {{have focused on}} sophisticated pattern matching code selection, because CISCs provide many machine instruction sequence choices. Recent pipelined processors, known as <b>reduced</b> <b>instruction</b> <b>set</b> <b>computers</b> (RISCs), provide fewer instruction sequence choices, but expose pipeline and functional unit costs to the compiler. For RISCs the compiler's emphasis must be shifted from code selection to instruction scheduling, resulting in code generation issues that are different than those for CISCs. In particular, the machine description language for a retargetable RISC compiler must contain scheduling requirements. Also, the interaction between register allocation and instruction scheduling is significant. This dissertation com [...] ...|$|R
40|$|Patterson and Ditzel [12] did not invent <b>reduced</b> <b>instruction</b> <b>set</b> <b>{{computer}}s</b> (RISC) in 1980. Earlier <b>computers</b> all had <b>reduced</b> <b>instruction</b> <b>sets.</b> Instead, {{they argued}} that trends in computer architecture had gotten off the sweet spot, and that by dropping back {{a few years and}} forking a new version of architectures, leveraging what had been learned, they could get better computers by employing simpler <b>instruction</b> <b>sets.</b> It is again time for a change in direction in computer architecture. Architectures currently strive for superior average-case performance that regrettably ignores predictability and repeatability of timing properties. “Correct ” execution of the SPECint benchmark suite {{has nothing to do with}} how long it takes to perform any particular action. C says nothing about timing, so timing is not considered part of correctness. Architectures have developed deep pipelines with speculative execution and dynamic dispatch. Memor...|$|R
50|$|The Parallax P8X32A Propeller is a {{multi-core}} processor {{parallel computer}} architecture microcontroller chip with eight 32-bit <b>reduced</b> <b>instruction</b> <b>set</b> <b>computer</b> (RISC) {{central processing unit}} (CPU) cores. Introduced in 2006, it is designed and sold by Parallax, Inc.|$|E
50|$|In {{addition}} to providing underclocking features, manufacturers can choose to limit the capability of a machine {{in order to make}} it more efficient. <b>Reduced</b> <b>instruction</b> <b>set</b> <b>computer</b> (RISC) models can help makers build devices that work on less power.|$|E
50|$|PA-RISC is an {{instruction}} set architecture (ISA) developed by Hewlett-Packard. As the name implies, it is a <b>reduced</b> <b>instruction</b> <b>set</b> <b>computer</b> (RISC) architecture, where the PA stands for Precision Architecture. The design is {{also referred to}} as HP/PA for Hewlett Packard Precision Architecture.|$|E
40|$|Although {{previous}} {{studies have shown that}} a large file of overlapping register windows can greatly reduce procedure call/return overhead, the effects of register windows in a multiprogramming environment are poorly understood. This paper investigates the performance of multiprogrammed, <b>reduced</b> <b>instruction</b> <b>set</b> <b>computers</b> (RISCs) as a function of window management strategy. Using an analytic model that reflects context switch and procedure call overheads, we analyze the performance of simple, linearly self-recursive programs. For more complex programs, we present the results of a simulation study. These studies show that a simple strategy that saves all windows prior to a context switch, but restores only a single window following a context switch, performs near optimally. Champaign. tSupported by a Senior Fulbright Scholarship while on leave at the University of Illinoisat Urbane...|$|R
50|$|The size or {{length of}} an {{instruction}} varies widely, from {{as little as}} four bits in some microcontrollers to many hundreds of bits in some VLIW systems. Processors used in personal computers, mainframes, and supercomputers have instruction sizes between 8 and 64 bits. The longest possible instruction on x86 is 15 bytes (120 bits). Within an <b>instruction</b> <b>set,</b> different <b>instructions</b> may have different lengths. In some architectures, notably most <b>reduced</b> <b>instruction</b> <b>set</b> <b>computers</b> (RISC), , typically corresponding with that architecture's word size. In other architectures, instructions have variable length, typically integral multiples of a byte or a halfword. Some, such as the ARM with Thumb-extension have mixed variable encoding, that is two fixed, usually 32-bit and 16-bit encodings, where instructions can not be mixed freely but must be switched between on a branch (or exception boundary in ARMv8).|$|R
40|$|A {{brief summary}} of the {{computer}} environment used for calculating three dimensional unsteady Computational Fluid Dynamic (CFD) results is presented. This environment requires a super computer as well as massively parallel processors (MPP's) and clusters of workstations acting as a single MPP (by concurrently working on the same task) provide the required computational bandwidth for CFD calculations of transient problems. The cluster of <b>reduced</b> <b>instruction</b> <b>set</b> <b>computers</b> (RISC) is a recent advent based on the low cost and high performance that workstation vendors provide. The cluster, with the proper software {{can act as a}} multiple instruction/multiple data (MIMD) machine. A new set of software tools is being designed specifically to address visualizing 3 D unsteady CFD results in these environments. Three user's manuals for the parallel version of Visual 3, pV 3, revision 1. 00 make up the bulk of this report...|$|R
50|$|In {{the history}} of {{computer}} hardware, some early <b>reduced</b> <b>instruction</b> <b>set</b> <b>computer</b> central processing units (RISC CPUs) used a very similar architectural solution, now called a classic RISC pipeline. Those CPUs were: MIPS, SPARC, Motorola 88000, and later the notional CPU DLX invented for education.|$|E
5000|$|The {{overall design}} {{also called for}} a [...] "universal controller" [...] to handle {{primarily}} input-output operations outside of the main processor. That universal controller would have a very limited instruction set, restricted to those operations required for I/O, pioneering {{the concept of a}} <b>reduced</b> <b>instruction</b> <b>set</b> <b>computer</b> (RISC).|$|E
50|$|In {{the mid-1980s}} to early 1990s, {{a crop of}} new {{high-performance}} <b>reduced</b> <b>instruction</b> <b>set</b> <b>computer</b> (RISC) microprocessors appeared, influenced by discrete RISC-like CPU designs such as the IBM 801 and others. RISC microprocessors were initially used in special-purpose machines and Unix workstations, but then gained wide acceptance in other roles.|$|E
40|$|International Telemetering Conference Proceedings / October 28 - 31, 1996 / Town and Country Hotel and Convention Center, San Diego, CaliforniaDeutsche Telekom {{has been}} {{operating}} different communication satellites for several years. The Satellite Control Center (SCC) of Deutsche Telekom is located near Usingen, about 50 km northwest of Frankfurt/Main. The system has been under operation since {{the launch of the}} first flight model DFS in June 1989. The entire computer system was based on Digital Equipment Corporation (DEC) VAX type computers. The maintenance costs of these old Complex <b>Instruction</b> <b>Sets</b> <b>Computers</b> (CISC) were increased significantly during the last years. Due to the high operational costs Deutsche Telekom decided to exchange the operational computer system. Present-day information technology world uses more and more powerful <b>Reduced</b> <b>Instruction</b> <b>Set</b> <b>Computers</b> (RISC). These new designs allow operational costs to be reduced appreciably. The VAX type computers will be replaced by DEC Alpha AXP Computers. This paper describes the transition process from CISC to RISC computers in an operational realtime environment...|$|R
40|$|Original article can {{be found}} at : [URL] Copyright Elsevier Ltd. DOI: 10. 1016 / 0026 - 2692 (92) 90043 -Z [Full text of this article is not {{available}} in the UHRA]RISC (<b>Reduced</b> <b>Instruction</b> <b>Set</b> <b>Computers)</b> processors have established an impressive performance standard by executing one instruction in each processor cycle. More recently, VLIW (Very Long Instruction Word) and superscalar architectures have attempted to improve processor performance by fetching and dispatching multiple instructions in each cycle. This paper presents the Hatfield Advanced RISC Processor (iHARP). iHARP is a parallel pipelined <b>reduced</b> <b>instructions</b> <b>set</b> processor that is currently under development at Hatfield Polytechnic. Earlier work at Hatfield centred around the design of an abstract HARP architectural model [1]. iHARP is a physical realisation of the HARP architectural model within the constraints of a single VLSI chip. The major aim of the HARP project is to develop a VLIW RISC processor capable of executing more than one instruction per clock cycle...|$|R
40|$|The use of <b>Reduced</b> <b>Instruction</b> <b>Set</b> <b>Computers</b> for {{scientific}} applications is common today. Advances in processor design and compiler technology {{make it possible}} to perform large-scale computations on RISC workstations. The RISC design provides simple instructions that operate at high speed and compiler optimisations employ the machine's capabilities to perform naively programmed operations with reasonable efficiency. However, there remain opportunities for improvement in execution performance by optimising the design of the highlevel language program, and, conversely, there are also traps for the unwary programmer. Partial unrolling of loops and employing scalar variables to assist the compiler in the use of registers are two simple techniques that can yield performance improvements of up to 50 % depending upon the compiler and the architecture. Unexpected performance penalties as high as 500 % may be paid for cache conflicts, where two large data arrays are contending for the same cache [...] ...|$|R
50|$|Eventually, most machine-language {{programming}} came to {{be generated}} by compilers and report generators. The <b>reduced</b> <b>instruction</b> <b>set</b> <b>computer</b> returned full-circle to the PDP-8's emphasis on a simple instruction set and achieving multiple actions in a single instruction cycle, {{in order to maximize}} execution speed, although the newer computers have much longer instruction words.|$|E
50|$|By 1989, {{researchers}} at HP recognized that <b>reduced</b> <b>instruction</b> <b>set</b> <b>computer</b> (RISC) architectures were reaching a limit at one instruction per cycle. They began {{an investigation into}} a new architecture, later named EPIC. The basis for the research was VLIW, in which multiple operations are encoded in every instruction, and then processed by multiple execution units.|$|E
50|$|Meanwhile, John Cocke, one of {{the chief}} {{designers}} of early IBM computers, began a research project to design the first <b>reduced</b> <b>instruction</b> <b>set</b> <b>computer</b> (RISC). In the long run, the RISC architecture, which eventually evolved into IBM's Power and PowerPC architecture, proved to be vastly cheaper to implement and capable of achieving much higher clock rate.|$|E
40|$|High-speed {{parallel}} multipliers {{are one of}} {{the keys}} in RISCs (<b>Reduced</b> <b>Instruction</b> <b>Set</b> <b>Computers),</b> DSPs (Digital Signal Processors), and graphics accelerators and so on. Array multiplier, Booth Multiplier and Wallace Tree multipliers are some of the standard approaches used in implementation of binary multiplier which are suitable for VLSI implementation. A simple digital multiplier (henceforth referred to as Vedic Multiplier in short VM) architecture based on the Urdhva Tiryakbhyam (Vertically and Cross wise) Sutra of Vedic Mathematics is presented. An improved technique for low power and high speed multiplier of two binary numbers (16 bit each) is developed. An algorithm is proposed and implemented on 16 nm CMOS technology. The designed 16 x 16 bit multiplier dissipates a power of 0. 17 mW. The propagation delay time of the proposed architecture is 27. 15 ns. These results are many improvements over power dissipations and delays reported in literature for Vedic and Booth Multiplier...|$|R
40|$|Candidates for quantum {{computing}} which offer only restricted control, e. g., {{due to lack}} of access to individual qubits, are not useful for general purpose {{quantum computing}}. We present concrete proposals for the use of systems with such limitations as RISQ - <b>reduced</b> <b>instruction</b> <b>set</b> quantum <b>computers</b> and devices - for simulation of quantum dynamics, for multi-particle entanglement and squeezing of collective spin variables. These tasks are useful in their own right, and they also provide experimental probes for the functioning of quantum gates in pre-mature proto-types of quantum computers. Comment: 11 pages, talk given at Fundamentals of Modern Optics V, to appear in Journal of Modern Optic...|$|R
40|$|Cyber-Physical Systems (CPS) are {{integrations}} of computation {{with physical}} processes. Embedded computers and networks monitor {{and control the}} physical processes, usually with feedback loops where physical processes affect computations and vice versa. In the physical world, {{the passage of time}} is inexorable and concurrency is intrinsic. Neither of these properties is present in today’s computing and networking abstractions. As a consequence, these abstractions require some fundamental rethinking. It is tempting to believe that the CPS problems can be solved by overlaying higher-level abstractions on top of existing computing technology. Indeed, it would be a scary prospect to suggest that much of the foundation of existing technology is flawed and must be rebuilt. How could this possibly result in a practical research program that will see results in our lifetime? In this position paper, we make a case that core computing abstractions must be and can be effectively and practically rebuilt. The objective is to enable a new generation of cyberphysical systems where computation and physical processes are tightly intertwined. This requires re-introducing properties that were deliberately and systematically abstracted away in the 20 -th century view of computation. We approach the problem bottom-up. We must first rebuild the computational engines, and then build revised higher level abstractions on top of these. 1 The Problem In 1980, Patterson and Ditzel [12] did not invent <b>reduced</b> <b>instruction</b> <b>set</b> <b>computers</b> (RISC). Earlier <b>computers</b> all had <b>reduced</b> <b>instruction</b> <b>sets.</b> Instead, they argued that trends in computer architecture had gotten off the sweet spot, and that by dropping back a few years and forking a new version of architectures, leveraging what had been learned, they could get better computers by employing simpler <b>instruction</b> <b>sets...</b>|$|R
50|$|The 7600 was an {{architectural}} landmark, {{and most of}} its features are still standard parts of computer design. It is a <b>reduced</b> <b>instruction</b> <b>set</b> <b>computer</b> with a 15-bit instruction word containing a six-bit operation code. There are only 64 machine codes, including a no operation code, with no fixed-point multiply or divide operations in the central processor.|$|E
50|$|PRISM (Parallel Reduced Instruction Set Multiprocessor) was Apollo Computer's {{high-performance}} CPU used {{in their}} DN10000 series workstations. It was for some time the fastest microprocessor available, a high fraction of a Cray-1 in a workstation. Hewlett Packard purchased Apollo in 1989, ending development of PRISM, although some of PRISM's ideas were later used in HP's own HP-PA <b>Reduced</b> <b>instruction</b> <b>set</b> <b>computer</b> (RISC) and Itanium processors.|$|E
50|$|In the past, {{microprocessor}} {{design technology}} evolved from {{complex instruction set computer}} (CISC) to <b>reduced</b> <b>instruction</b> <b>set</b> <b>computer</b> (RISC). In {{the early days}} of the computer industry, compiler technology did not exist and programming was done in assembly language. To make programming easier, computer architects created complex instructions which were direct representations of high level functions of high level programming languages. Another force that encouraged instruction complexity was the lack of large memory blocks.|$|E
40|$|Network {{management}} {{relies on}} an up-to-date and accurate {{view of many}} traffic metrics for tasks such as traffic engi-neering (e. g., heavy hitters), anomaly detection (e. g., entropy of source addresses), and security (e. g., DDoS detection). Obtaining an accurate estimate of these metrics while using little router CPU and memory is challenging. This in turn has inspired {{a large body of}} work in data streaming devoted to developing optimized algorithms for individual monitor-ing tasks, as well as recent approaches to make it simpler to implement these algorithms (e. g., OpenSketch). While this body of work has been seminal, we argue that this tra-jectory of crafting special purpose algorithms is untenable in the long term. We make a case for a "RISC " approach for flow monitoring analogous to a <b>reduced</b> <b>instruction</b> <b>set</b> in <b>computer</b> architecture—a simple and generic monitoring primitive from which a range of metrics can be computed with high accuracy. Building on recent theoretical advances in universal streaming, we show that this “holy grail ” for flow monitoring might be well within our reach...|$|R
40|$|Today's {{submicron}} silicon technology enables integration of much more functionalities onto a single chip than ever before. At the same time, {{the gap between}} chip performance and capacity becomes larger and larger, and power as well as energy consumption issues become extremely critical. The Multi-Processor System-on-Chip (MPSoC) technology offers a promising solution to these problems from the system architecture perspective. The key is its task-level parallelism. By running tasks on multiple processors in parallel, MPSoCs are able to achieve high performance at a low clock frequency, consequently at a low supply voltage, which largely reduces power and energy consumption. This implies that efficient task management is highly important in MPSoCs. Furthermore, in today's embedded systems, the system behavior is very dynamic, which naturally calls for dynamic task management to enable the system to adapt to different situations and scenarios. Dynamic task management support in MPSoCs is a challenging task. It should consider both efficiency and flexibility, which are typically contradictory to each other. In literature, most implementations of dynamic task management systems are based on either <b>Reduced</b> <b>Instruction</b> <b>Set</b> <b>Computers</b> (RISCs) or Application-Specific Integrated Circuits (ASICs). However, they can only partly {{meet the requirements of}} good dynamic task management. While the former are not efficient enough for large systems, the latter lack in flexibility. This thesis proposes an implementation based on the concept of Application-Specific Instruction-set Processors (ASIPs) in order to combine the flexibility of RISCs and the efficiency of ASICs. As a result, an ASIP called OSIP (Operating System application-specific Instruction-set Processor) is developed. It employs special architectural features and customized instructions to speed up typical operations in dynamic task management, such as task-level comparisons and operations related to list-based data structures. The efficiency of OSIP is compared with a RISC-based and a hypothetical extremely fast ASIC task manager in a system context, considering different system sizes and OSIP workloads. Especially, the impact of communication architectures on the OSIP efficiency is investigated. The evaluation results confirm the high efficiency of OSIP in task management, and show that a well-designed communication architecture is important for full exploitation of the OSIP efficiency. The advantages of the OSIP flexibility are highlighted by extending the functionality of OSIP in software to support an advanced spinlock control mechanism based on a-priori application knowledge. With this extension, significant performance improvement is achieved, and the OSIP-based systems have even better performance than the systems using the hypothetical ASIC task manager in many cases. Based on the analysis of the OSIP efficiency and flexibility, it is clearly shown that an ASIP task manager can meet the challenges of dynamic task management. The task management in OSIP-based systems is organized in a centralized way, which is well suited for MPSoCs using traditional bus-based communication architectures. However, Networks-on-Chip (NoCs) are nowadays more and more widely used in modern large-scale systems due to their advantages in scalability. This communication architecture paradigm presents distributive architectural characteristics, which somehow do not fit into the concept of using a central task manager. In this thesis, a proxy-based approach is proposed for the integration of OSIP into NoC-based systems, which can effectively convert remote communications with OSIP over a NoC into local communications through a simple bus, thereby reducing the communication overhead caused by the task management of the OSIP...|$|R
25|$|Alpha, {{originally}} {{known as}} Alpha AXP, is a 64-bit <b>reduced</b> <b>instruction</b> <b>set</b> computing (RISC) <b>instruction</b> <b>set</b> architecture (ISA) developed by Digital Equipment Corporation (DEC), designed to replace their 32-bit VAX complex <b>instruction</b> <b>set</b> <b>computer</b> (CISC) ISA. Alpha was implemented in microprocessors originally developed and fabricated by DEC. These microprocessors were most prominently {{used in a}} variety of DEC workstations and servers, which eventually formed the basis for almost all of their mid-to-upper-scale lineup. Several third-party vendors also produced Alpha systems, including PC form factor motherboards.|$|R
