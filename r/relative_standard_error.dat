126|10000|Public
2500|$|A {{good reason}} why the number of bins should be {{proportional}} to [...] is the following: suppose that the data are obtained as [...] independent realizations of a bounded probability distribution with smooth density. Then the histogram remains equally [...] "rugged" [...] as [...] tends to infinity. If [...] is the [...] "width" [...] of the distribution (e. g., the standard deviation or the inter-quartile range), then the number of units in a bin (the frequency) is of order [...] and the <b>relative</b> <b>standard</b> <b>error</b> is of order [...] Comparing to the next bin, the relative change of the frequency is of order [...] provided that the derivative of the density is non-zero. These two are of the same order if [...] is of order , so that [...] is of order [...] This simple cubic root choice can also be applied to bins with non-constant width.|$|E
5000|$|As {{an example}} of the use of the <b>relative</b> <b>standard</b> <b>error,</b> {{consider}} two surveys of household income that both result in a sample mean of $50,000. If one survey has a standard error of $10,000 and the other has a standard error of $5,000, then the relative standard errors are 20% and 10% respectively. The survey with the lower <b>relative</b> <b>standard</b> <b>error</b> can be said to have a more precise measurement, since it has proportionately less sampling variation around the mean. In fact, data organizations often set reliability standards that their data must reach before publication. For example, the U.S. National Center for Health Statistics typically does not report an estimated mean if its <b>relative</b> <b>standard</b> <b>error</b> exceeds 30%. (NCHS also typically requires at least 30 observations - if not more - for an estimate to be reported.) ...|$|E
50|$|The <b>relative</b> <b>{{standard}}</b> <b>error</b> of {{a sample}} mean is the standard error divided by the mean and expressed as a percentage. It can only be calculated if the mean is a non-zero value.|$|E
40|$|This paper studies {{performance}} of synthetic ratio estimator and composite estimator, {{which is a}} weighted sum of direct and synthetic ratio estimators, under Lahiri – Midzuno (L-M) sampling scheme. The synthetic estimator under L-M scheme is unbiased and consistent if the assumption of synthetic estimator is satisfied. Further, this paper compares {{performance of}} the synthetic and composite estimators empirically under L-M and SRSWOR schemes for estimating crop acreage for small domains. The study shows that both the estimators perform better under L-M scheme as having comparatively smaller absolute relative biases and <b>relative</b> <b>standard</b> <b>errors.</b> ...|$|R
40|$|It is {{extremely}} dangerous to extrapolate a polynomial trend. Although {{there are other}} sources of danger, this study concentrates on the statistical error, which is very large for end values and increases alarmingly with extrapolations. <b>Relative</b> <b>standard</b> <b>errors</b> of polynomial trend values are summarized, and generalizations are made concerning the relative magnitude of the <b>standard</b> <b>errors</b> with changes in: (1) the degree of equation; (2) the number of observations; (3) {{the position of the}} trend value in time. The most interesting generalization is that the relative variance of a polynomial of degree m, as the position of the trend value departs farther and farther from the central position in time, may be described by a polynomial equation of degree 2 m. ...|$|R
40|$|International audienceOBJECTIVES: To {{design a}} {{pharmacokinetic}} (PK) study using adult prior information and evaluate robustness of the recommended design, through a case-study on mefloquine. METHODS: PK data {{for adults and}} children were available from two different randomised studies for treatment of malaria with the same artesunate-mefloquine combination regimen. A recommended design for paediatric study on mefloquine was optimised based on an extrapolated model built from adult data through the following approach: (i) a PK model was built in adults, and parameters were estimated using the SAEM algorithm; (ii) paediatric PK parameters were then obtained by adding allometry and maturation to the adult model; (iii) a D-optimal design in children was obtained with PFIM assuming the extrapolated design. Finally, the robustness of the recommended design was evaluated {{in terms of the}} relative bias and <b>relative</b> <b>standard</b> <b>errors</b> (RSE) of the parameters in a simulation study with four different models, and was compared to the empirical design actually performed in the paediatric study. RESULTS: Combining pharmacokinetic modelling, extrapolation and design optimisation led to a design for children with 5 sampling times. Pharmacokinetic parameters were well estimated with this design with low <b>relative</b> <b>standard</b> <b>errors.</b> Although the extrapolated model did not predict the observed mefloquine concentrations in children very accurately, it allowed precise and unbiased estimates across various model assumptions, contrary to the empirical design. CONCLUSION: Using prior adult information combined with allometry and maturation can help provide robust designs for paediatrics studie...|$|R
5000|$|A {{good reason}} why the number of bins should be {{proportional}} to [...] is the following: suppose that the data are obtained as [...] independent realizations of a bounded probability distribution with smooth density. Then the histogram remains equally [...] "rugged" [...] as [...] tends to infinity. If [...] is the [...] "width" [...] of the distribution (e. g., the standard deviation or the inter-quartile range), then the number of units in a bin (the frequency) is of order [...] and the <b>relative</b> <b>standard</b> <b>error</b> is of order [...] Comparing to the next bin, the relative change of the frequency is of order [...] provided that the derivative of the density is non-zero. These two are of the same order if [...] is of order , so that [...] is of order [...] This simple cubic root choice can also be applied to bins with non-constant width.|$|E
40|$|Introduction The National Hospital Discharge Survey is {{a primary}} data source for {{epidemiology}} research in the United States. To ensure that estimates are reliable, confidence intervals need to be calculated. The original survey data source is not available to the public, and the usual statistical methods are unsuitable for calculating confidence intervals. Instead, calculating confidence intervals requires using the statistical methods and relative standard errors that the U. S. National Center for Health Statistics has provided. However, the <b>relative</b> <b>standard</b> <b>error</b> parameters differ by hospital, patient category, and group. They also change yearly with sampling and are expressed differently before and during or after 1988. Consequently, manual computations of confidence intervals with multiple groups, diseases, and years are inefficient and prone to error. We developed a SAS program to compute confidence intervals for National Hospital Discharge Survey data from 1979 through 2000, newborns excluded. Methods We transposed 22 tables of <b>relative</b> <b>standard</b> <b>error</b> parameters (one for each year) into two new parameter tables that maintain the sampling designs before 1988 and during and after 1988 but are similar in overall structure. We unified all values to make each set of <b>relative</b> <b>standard</b> <b>error</b> parameters unique. We developed a program, COMPURSE, to search for <b>relative</b> <b>standard</b> <b>error</b> parameters for inputted estimates and to calculate confidence intervals. We set up an interface program for users to enter data, time period, confidence interval level, and output location; to read the <b>relative</b> <b>standard</b> <b>error</b> parameter tables; and to run the COMPURSE program. Results For different sets of National Hospital Discharge Survey data, COMPURSE efficiently and correctly retrieved relevant <b>relative</b> <b>standard</b> <b>error</b> parameters for estimates and accurately calculated relative standard errors, standard errors, and confidence intervals for annual estimates, multiple-year summaries, and average annual estimates. Conclusion The program COMPURSE helps users analyze National Hospital Discharge Survey data efficiently...|$|E
30|$|The {{analysis}} of variance (ANOVA) of the quadratic model was utilized to evaluate {{the adequacy of the}} proposed model and to identify the significance of significance of the independent variables and their interactions. In addition, fit quality of the developed quadratic model was evaluated by the coefficient of determination (R 2), the root-mean-square error of prediction (RMSEP), and the <b>relative</b> <b>standard</b> <b>error</b> of prediction (RSEP) (Singh et al. 2010, 2012 b).|$|E
3000|$|Owing to the {{simplicity}} of the sampling schemes adopted, {{there was no need for}} simulation to determine the performance of the four sampling strategies. Each strategy gave rise to design-unbiased estimators, so that their accuracy could be determined exactly from their variance expressions, rather than approximated by Monte Carlo distributions, as is customary in more complex cases. More precisely, the variances of HT and D estimators obtained with SRSWOR were determined using equations (14) and (17), respectively, while those from OPSS were determined by equations (21) and (23). From these quantities, the values of <b>relative</b> <b>standard</b> <b>errors</b> (RSE) were determined as the ratio SE/Y of the square root of the variance (SE) to the value under estimation, i.e., Y.|$|R
40|$|The {{inversion}} {{of a large}} data set, with it errors, is demonstrated and the tradeoff curve, or resolving power calculations, are discussed. Consideration is given to inverse problems of the earth, represented in the idealized form of a single scaler function of a single coordinate. The difference between the functional for the real earth and the functional for some model is formulated. A procedure is given for using the linear relationships between {{the differences in the}} data and the differences in the model as side conditions. Through a process of diagonalization a model is created that fits the data. The data are culled by accepting only those <b>relative</b> <b>standard</b> <b>errors</b> that are less than 100 percent...|$|R
40|$|A sensitive, {{simple and}} {{accurate}} reverse phase HPLC method {{was developed to}} determine plasma levels of gliclazide (GL) in rat. The method uses liquid-liquid extraction that is not expensive and ibuprofen as internal standard, that is easily available. Mean recovery for GL and internal standard was 80 % and 82 % respectively. <b>Relative</b> <b>standard</b> <b>errors</b> and CV% ranged from 0. 47 to 8. 99 and from 0. 72 to 12. 54 % for intraday and interday HPLC injections, respectively. The limit of quantitation (LOQ) and the limit of detection (LOD) for GL in serum were 0. 12 μg/mL and 0. 06 μg/mL respectively. The developed HPLC method is a suitable and rapid method to be adopted for pharmacokinetic studies in rat and human...|$|R
40|$|Unit {{fuel costs}} for {{electric}} power plants are discussed, and the accuracy {{with which they}} might be measured after a conversion of units. Consideration {{is given to the}} variability of the fuel heat content, for which there may be inadequate data. This has application to unit cost data in other fields. Key words: <b>relative</b> <b>standard</b> <b>error,</b> bias, variance, covariance, ratio of variables, receipts, total costs, heat content, unit conversions, electric power plants, measuremen...|$|E
40|$|Nonlinear mixed-effects {{modeling}} {{was applied}} to explore the relationship between lopinavir and ritonavir concentrations over 72 h following drug cessation and also to assess other lopinavir and ritonavir dosing strategies compared to the standard 400 -mg– 100 -mg twice-daily dose. Data from 16 healthy volunteers were included. Possible covariates influencing lopinavir and ritonavir pharmacokinetics were also assessed. Data were modeled first separately and then together by using individually predicted ritonavir pharmacokinetic parameters in the final lopinavir model. The model was evaluated {{by means of a}} visual predictive check and external validation. A maximum-effect model in which ritonavir inhibited the elimination of lopinavir best described the relationship between ritonavir concentrations and lopinavir clearance (CL/F). A ritonavir concentration of 0. 06 mg/liter was associated with a 50 % maximum inhibition of the lopinavir CL/F. The population prediction of the lopinavir CL/F in the absence of ritonavir was 21. 6 liters/h (<b>relative</b> <b>standard</b> <b>error,</b> 14. 0 %), and the apparent volume of distribution and absorption rate constant were 55. 3 liters (<b>relative</b> <b>standard</b> <b>error,</b> 10. 2 %) and 0. 57 h− 1 (<b>relative</b> <b>standard</b> <b>error,</b> 0. 39 %), respectively. Overall, 92 % and 94 % of the observed concentrations were encompassed by the 95 % prediction intervals for lopinavir and ritonavir, respectively, which is indicative of an adequate model. Predictions of concentrations from an external data set (HIV infected) (n = 12) satisfied predictive performance criteria. Simulated lopinavir exposures at lopinavir-ritonavir doses of 200 mg- 150 mg and 200 mg- 50 mg twice daily were 38 % and 65 % lower, respectively, than that of the standard dose. The model allows {{a better understanding of the}} interaction between lopinavir and ritonavir and may allow a better prediction of lopinavir concentrations and assessments of different dosing strategies...|$|E
40|$|Data sets in {{the social}} and {{behavioral}} sciences are often heavy-tailed. Previous studies have demonstrated that small samples or leptokurtic distributions adversely affect the performance of coefficient alpha. To address these concerns, we propose an alternative estimator of reliability based on L-comoments. The empirical results of this study demonstrate that when sample sizes are small and distributions are heavy-tailed that the proposed coefficient L-alpha has substantial advantages over Cronbach’s estimator of reliability in terms of relative bias and <b>relative</b> <b>standard</b> <b>error...</b>|$|E
40|$|Simultaneous {{spectrophotometric}} determination of Mn, Zn and Co was studied by two methods, classical partial least-squares (PLS) and kernel partial least-squares (KPLS), with 2 -(5 -bromo- 2 - pyridylazo) - 5 -diethylaminephenol (5 -Br-PADAP) and cetyl pyridinium bromide (CPB). Two programs, SPGRPLS and SPGRKPLS, {{were designed to}} perform the calculations. Eight error functions were calculated for deducing the number of factors. Data reductions were performed using principle component analysis. The KPLS method was applied for the rapid determination from a data matrix with many wavelengths and fewer numbers of samples. The <b>relative</b> <b>standard</b> <b>errors</b> of prediction (RSEP) for all components with KPLS and PLS methods were the same (0. 0247). Experimental results showed both methods to be successful even where there was severe overlap of spectra...|$|R
40|$|This paper {{presented}} a novel method named wavelet packet transform-based partial 		least squares method (WPTPLS) for simultaneous spectrophotometric determination of 		α-naphthylamine, p-nitroaniline, and benzidine. Wavelet packet representations of signals provided a local time-frequency description and separation ability between information and noise. The {{quality of the}} noise removal can be improved by using best-basis algorithm and thresholding operation. Partial least squares (PLS) method uses both the response and concentration information to enhance its ability of prediction. In this case, by optimization, wavelet function and decomposition level for WPTPLS method were selected as Db 16 and 3, respectively. The <b>relative</b> <b>standard</b> <b>errors</b> of prediction (RSEP) for all components with WPTPLS and PLS were 2. 23 % and 2. 71 %, respectively. Experimental results showed WPTPLS method to be successful and better than PLS...|$|R
40|$|With {{the concern}} that {{imperviousness}} can be quantified differently depending on data sources and methods, regression equations to translate between imperviousness estimates using land use and land cover were developed. In addition, this study examined how quantitatively different imperviousness estimates affect the prediction of hydrological response. The regressions between indicators of hydrological response and imperviousness-descriptors were evaluated by examining goodness-of-fit measures such as explained variance or <b>relative</b> <b>standard</b> <b>errors.</b> The results show that imperviousness estimates using land use are better predictors of hydrological response than imperviousness estimates using land cover. Also, this study reveals that flow variability is more sensitive to spatially distributed models than lumped models, while thermal variability is equally responsive to both models. The {{findings from this study}} can be further examined from a policy perspective with regard to policies that are based on a threshold concept for imperviousness impacts on the ecological and hydrological system...|$|R
40|$|This study {{aimed to}} {{evaluate}} traditional and generic models {{to estimate the}} height-diameter relationship of Pinus caribaea var. hondurensis. The data used were from trees of different ages, sites and planting densities. Total height of the trees were estimate using ten models. The models were evaluate according to the adjusted coefficient of determination, standard error of estimate, <b>relative</b> <b>standard</b> <b>error,</b> bias, mean differences and standard deviation of differences. Beyond these statistics, the models were evaluated by plotting the observed height versus estimated height. There was a greater accuracy of the generic models. </p...|$|E
40|$|A vortex decay {{model for}} {{predicting}} temporal evolution of peak vorticity in a wake behind a cylinder is presented. Where the wake vortices are in stable region, results {{have shown that}} the correlation has a good capability of predicting temporal evolu-tion of peak vorticity within an advecting vortex across a wide range of flow parameters. The correlation is also generalized to predict the dacay behaviour of wake vortices in a class of mag-netohydrodynamic duct flows. Comparison with published data demonstrates the capability of this model in predicting vortex strength with a <b>relative</b> <b>standard</b> <b>error</b> of less than 2 %...|$|E
40|$|This is the publisher’s final pdf. The {{published}} {{article is}} copyrighted by the Canadian Aeronautics and Space Institute {{and can be}} found at: [URL] emergence {{of a new generation of}} remote sensing and geopositioning technologies, as well as increased capabilities in image processing, computing, and inferential techniques, have enabled the development and implementation of increasingly efficient and cost-effective multilevel sampling designs for forest inventory. In this paper, we (i) describe the conceptual basis of multilevel sampling, (ii) provide a detailed review of several previously implemented multilevel inventory designs, (iii) describe several important technical considerations that can influence the efficiency of a multilevel sampling design, and (iv) demonstrate the application of a modern multilevel sampling approach for estimating the forest biomass resources in a remote area of interior Alaska. This approach utilized a combination of ground plots, lidar strip sampling, satellite imagery (multispectral and radar), and classified land cover information. The variability in the total biomass estimate was assessed using a bootstrapping approach. The results indicated only marginal improvement in the precision of the total biomass estimate when the lidar sample was post-stratified using the classified land cover layer (reduction in <b>relative</b> <b>standard</b> <b>error</b> from 7. 3 % to 7. 0 %), whereas there was a substantial improvement in the precision when the estimate was based on the biomass map derived via nearest-neighbor imputation (reduction in <b>relative</b> <b>standard</b> <b>error</b> from 7. 3 % to 5. 1 %) ...|$|E
40|$|Objectives. To examine {{mortality}} rates {{and quality of}} race reporting for multiple-race individuals in California using the new multiple-race data avail-able on the death certificate. Methods. Death data were drawn from California vital statistics for 2000 and 2001. Denominator data were drawn from the 2000 census Modified Race Data Summary File. The authors calculated {{mortality rates}} and <b>relative</b> <b>standard</b> <b>errors</b> for multiple-race individuals {{as a whole and}} by county, and for the three largest reported multiple-race groups (African American and white, American Indian/Alaska Native and white, and Asian and white). Results. Decedents reported to be of more than one race were disproportion-ately young, Hispanic, male, and never-married. Age-adjusted mortality rates for multiple-race groups were approximately one-sixth as high as rates for single-race individuals. There was substantial variability in rates for multiple-race decedents according to county of residence. Conclusions. Mortality rates for multiple-race people were implausibly low, an...|$|R
40|$|Chylomicron {{remnants}} can penetrate {{into the}} artery wall, {{where they can}} initiate atherogenesis. Since {{it is difficult to}} isolate these particles from human blood because of contamination with other lipoproteins, the use of lipid emulsions as chylomicron remnant-like particles (CRLPs) has been proposed to study their metabolism. This study was aimed to evaluate the methodology for the preparation of CRLP. Artificial chylomicrons were prepared by sonication of a lipid mixture and separated by density gradient centrifugation. Lipid classes were analyzed by HPLC and fatty acids by GC. Particle size was measured by dynamic light scattering and the presence of apolipoprotein E by immunoblotting. The highest lipid content was found in the 60 [*][*] 400. This latter fraction presented the highest triacylglycerol (TAG) concentration, which was dramatically reduced in the 20 [*]<[*]Sf[*]<[*] 60 fraction. Fatty acid composition in TAG and phospholipids resembled that of the standards used with little modifications. The repeatability of the method was excellent, showing <b>relative</b> <b>standard</b> <b>errors</b> below 10...|$|R
40|$|Despite {{its huge}} {{ecological}} importance, microbial oxygen respiration in pelagic waters is little studied, {{primarily due to}} methodological difficulties. Respiration measurements are challenging because of the required high resolution of oxygen concentration measurements. Recent improvements in oxygen sensing techniques bear great potential to overcome these limitations. Here we compare 3 different methods to measure oxygen consumption rates at low oxygen concentrations, utilizing amperometric Clark type sensors (STOX), optical sensors (optodes), and mass spectrometry in combination with (18 - 18) O 2 labeling. Oxygen concentrations and consumption rates agreed well between the different methods when applied in the same experimental setting. Oxygen consumption rates between 30 and 400 nmol L(- 1) h(- 1) were measured with high precision and <b>relative</b> <b>standard</b> <b>errors</b> of less than 3 %. Rate detection limits {{in the range of}} 1 nmol L(- 1) h(- 1) were suitable for rate determinations in open ocean water and were lowest at the lowest applied O 2 concentration...|$|R
40|$|This paper {{presents}} a novel auto-calibration technique for eliminating sensor mismatch in CMOS-based chemical imagers. Designed using an 8 × 8 array comprising of pH-sensitive ion-sensitive field-effect transistors (ISFETs), the chemical imager {{is capable of}} implementing a gradient-based calibration algorithm by biasing programmable-gate (PG) ISFETs at a common operating point when exposed to a solution of homogenous pH. The system was fabricated in a typical 0. 35 -?m CMOS technology and demonstrated a fast rate of convergence (500 ms per iteration) while a convergence accuracy of 45 mV on a gain of 10 (0. 5 % <b>relative</b> <b>standard</b> <b>error</b> and 2 % pixel-to-pixel variation) was achieved. A maximum pH sensitivity of 57 mV/pH is also reported...|$|E
40|$|The aim of {{this study}} was to {{determine}} the measurement reproducibility for a procedure evaluating the mastication process and to estimate the smallest detectable differences of 3 D kinematic and surface electromyography (sEMG) variables. Kinematics of mandible movements and sEMG activity of the masticatory muscles were obtained over two sessions with four conditions: two food textures (biscuit and bread) of two sizes (small and large). Twelve healthy adults (mean age 29. 1 years) completed the study. The second to the fifth chewing cycle of 5 bites were used for analyses. The reproducibility per outcome variable was calculated with an intraclass correlation coefficient (ICC) and a Bland-Altman analysis was applied to determine the standard error of measurement relative error of measurement and smallest detectable differences of all variables. ICCs ranged from 0. 71 to 0. 98 for all outcome variables. The outcome variables consisted of four bite and fourteen chewing cycle variables. The <b>relative</b> <b>standard</b> <b>error</b> of measurement of the bite variables was up to 17. 3 % for 'time-to-swallow', 'time-to-transport' and 'number of chewing cycles', but ranged from 31. 5 % to 57. 0 % for 'change of chewing side'. The <b>relative</b> <b>standard</b> <b>error</b> of measurement ranged from 4. 1 % to 24. 7 % for chewing cycle variables and was smaller for kinematic variables than sEMG variables. In general, measurements obtained with 3 D kinematics and sEMG are reproducible techniques to assess the mastication process. The duration of the chewing cycle and frequency of chewing were the best reproducible measurements. Change of chewing side could not be reproduced. The published measurement error and smallest detectable differences will aid the interpretation of the results of future clinical studies using the same study variables...|$|E
40|$|Abstract—This paper {{presents}} a novel auto-calibration tech-nique for eliminating sensor mismatch in CMOS-based chemical imagers. Designed using an 8 × 8 array comprising of pH sensitive Ion Sensitive Field Effect Transistors (ISFETs), the chemical imager {{is capable of}} implementing a gradient based calibration algorithm by biasing programmable-gate (PG) ISFETs at a com-mon operating point when exposed to a solution of homogenous pH. The system was fabricated in a typical 0. 35 µm CMOS technology and demonstrated a fast rate of convergence (500 ms per iteration) while a convergence accuracy of 45 mV on a gain of 10 (0. 5 % <b>relative</b> <b>standard</b> <b>error</b> and 2 % pixel-to-pixel variation) was achieved. A maximum pH sensitivity of 57 mV/pH is also reported. Index Terms—lab-on-chip, ISFET, chemical imager, calibra-tion, array I...|$|E
40|$|Differences and {{uncertainties}} of alternative methods applicable to estimation of biomass in national greenhouse gas inventories are evaluated. The alternative methods employed to obtain biomass estimates of trees are (1) aggregated stand-level volume estimates multiplied by biomass expansion factors (BEF), and (2) biomass equations applied to tree-wise data {{of a national}} forest inventory. In comparison to the reference value obtained using tree-wise biomass equations, the age-dependent BEFs {{for the whole of}} Sweden resulted in a 6. 7 % lower aboveground biomass estimate. The estimates were the closest for conifer-dominated forests in central Sweden, and the largest discrepancies were for spruce in southern Sweden. This result indicates that these age-dependent BEFs cannot be applied to conditions where stand development deviates from the conditions under which the BEFs were developed. The degree of uncertainty in both methods was highest in the young age-classes. At the regional level, the <b>relative</b> <b>standard</b> <b>errors</b> of the BEF-based biomass estimates were in the range of 4 – 13 %...|$|R
30|$|Estimation {{of arsenic}} [As(III) and As(V)] {{was carried out}} spectrophotometrically by silver diethyl {{dithiocarbamate}} (SDDC) method (De 2008; Gupta and Sankararamakrishnan 2010) with precautions to prevent formation of sulfide {{at the time of}} arsine generation. The lower detection limit was found to be 2  μg. Each sample was analyzed three times and the results were found reproducible within ± 3  % (<b>relative</b> <b>standard</b> deviation) <b>error</b> limit. Calibration was also carried out daily with a freshly prepared arsenic standard, before analysis.|$|R
40|$|Background: Psychological {{stress is}} {{suggested}} {{to accelerate the}} rate of biological aging. We investigated whether work-related exhaustion, an indicator of prolonged work stress, is associated with accelerated biological aging, as indicated by shorter leukocyte telomeres, that is, the DNA-protein complexes that cap chromosomal ends in cells. Methods: We used data from {{a representative sample of}} the Finnish working-age population, the Health 2000 Study. Our sample consisted of 2911 men and women aged 30 – 64. Work-related exhaustion was assessed using the Maslach Burnout Inventory- General Survey. We determined relative leukocyte telomere length using a quantitative real-time polymerase chain reaction (PCR) -based method. Results: After adjustment for age and sex, individuals with severe exhaustion had leukocyte telomeres on average 0. 043 <b>relative</b> units shorter (<b>standard</b> <b>error</b> of the mean 0. 016) than those with no exhaustion (p = 0. 009). The association between exhaustion and relative telomere length remained significant after additional adjustment for marital and socioeconomic status, smoking, body mass index, and morbidities (adjusted difference 0. 044 <b>relative</b> units, <b>standard</b> <b>error</b> of the mean 0. 017, p = 0. 008) ...|$|R
30|$|The {{estimation}} start-time (t*) was varied systematically from 1.5 to 20  min for {{a subset}} of five [11 C]HED studies to determine the optimal value {{to be used for}} the main analysis. Goodness-of-fit was evaluated on the Logan plot as the Pearson correlation (r 2) of the points from t* to 40  min, indicating the subset of points best described by a line. Since the r 2 is not effective to assess goodness-of-fit of the near-horizontal fitted plane on the MA 1 plots, an alternative metric was computed using the <b>relative</b> <b>standard</b> <b>error</b> of the estimate (rSEE) as 1 [*]−[*]SEE/mean. The optimal t* was determined by comparing VT values from the graphical methods to the 1 TC model standard. Then, all subsequent analysis was performed using the same start-time for both Logan and MA 1 models.|$|E
40|$|Purpose The {{research}} {{presented here}} {{was motivated by}} an interest in understanding the magnitude of sampling error in crop production unit process data developed for life cycle assessments (LCAs) of food, biofuel, and bioproduct production. More broadly, uncertainty data are placed {{within the context of}} conclusive interpretations of comparative bioproduct LCA results. Methods Data from the United States Department of Agriculture 2 ̆ 7 s Agricultural Resource Management Survey were parameterized for 466 crop–state–year combinations, using 146 variables representing the previous crop, tillage and seed operations, irrigation, and applications of synthetic fertilizer, lime, nitrogen inhibitor, organic fertilizer, and pesticides. Data are described by Student 2 ̆ 7 s t distributions representing sampling error through the <b>relative</b> <b>standard</b> <b>error</b> (RSE) and are organized by the magnitude of the RSE by data point. Also, instances in which the bounds of the 95...|$|E
40|$|Background: The {{influence}} of water immersion on neuromuscular function is {{of importance to}} a number of disciplines; however, the reliability of surface electromyography (SEMG) following water immersion is not known. This study examined the reliability of SEMG amplitude during maximal voluntary isometric contractions (MVICs) of the vastus lateralis following water immersion. Methods: Using a Biodex isokinetic dynamometer and in a randomized order, 12 healthy male subjects performed four MVICs at 60 ° knee flexion on both the dominant and nondominant kicking legs, and the SEMG was recorded. Each subject 2 ̆ 7 s dominant and nondominant kicking leg was then randomly assigned to have SEMG electrodes removed or covered during 15 min of water immersion (20 °C- 25 °C). Following water immersion, subjects performed a further four MVICs. Results: Intraclass correlation coefficient (ICC) and the <b>relative</b> <b>standard</b> <b>error</b> of measurement (...|$|E
40|$|OBJECTIVES: To examine {{mortality}} rates {{and quality of}} race reporting for multiple-race individuals in California using the new multiple-race data available on the death certificate. METHODS: Death date were drawn from California vital statistics for 2000 and 2001. Denominator data were drawn from the 2000 census Modified Race Data Summary File. The authors calculated {{mortality rates}} and <b>relative</b> <b>standard</b> <b>errors</b> for multiple-race individuals {{as a whole and}} by county, and for the three largest reported multiple-race groups (African American and white, American Indian/Alaska Native and white, and Asian and white). RESULTS: Decedents reported to be of more than one race were disproportionately young, Hispanic, male, and never-married. Age-adjusted mortality rates for multiple-race groups were approximately one-sixth as high as rates for single-race individuals. There was substantial variability in rates for multiple-race decedents according to county of residence. CONCLUSIONS: Mortality rates for multiple-race people were implausibly low, and death certificates for multiple-race individuals were geographically clustered. Race reporting on death certificates will need to be improved before accurate death rates can be calculated for those of multiple races...|$|R
40|$|AbstractThere {{is a need}} internationally for an {{agreement}} on reference values for the calibration of mercury vapour concentration measurements. These reference values are obtained from the relationship between Hg vapour mass concentrations at saturation in air (ɣHg, in ng cm− 3) and temperature (T, in K). This relationship was re-evaluated by fitting replicated (n[*]=[*] 3, 4) SI traceable measurement results obtained for mL size mercury-saturated air samples at temperatures of around 288. 4, 293. 3, 298. 2 and 303. 1 [*]K (2. 4 %, 0. 20 %, 0. 51 % and 1. 6 % <b>relative</b> <b>standard</b> <b>errors</b> of the means, respectively). It gave ɣHg[*]=[*]C/T* 10 (−B/T) with B[*]=[*] 3282. 92 [*]K and C[*]=[*] 6. 73257 [*]×[*] 1014 [*]K ng cm− 3, at an estimated relative expanded (combined) uncertainty of 5. 9 % (k[*]=[*] 2). The measurement procedure was based on temperature controlled automated sampling of the gaseous mercury, isotope dilution of the mercury following trapping into the liquid phase under closed circuit conditions, and inductively coupled plasma mass spectrometry for the signal acquisition...|$|R
40|$|Fishing {{and natural}} {{mortality}} rates and tag reporting rate for rock lobsters (Jasus edwardsii) in northwest Tasmania, Australia, were estimated using multiyear tagging models. These estimates {{are necessary for}} assessment of the resource. Several models were examined that had either two or three tagging events each year, and either combined sexes or kept sexes separate. The model that best described {{the dynamics of the}} fishery utilized three tagging events within a year. The year was divided into discrete periods and, within each year, fishing effort and duration of period were used to apportion fishing and natural mortalities, respectively, to the periods. The separation of fishing mortalities by sex was not found to improve the models. Although high (1. 0 - 1. 2. year-), the instantaneous fishing mortality estimates were comparable to estimates obtained from other methods and the <b>relative</b> <b>standard</b> <b>errors</b> were low. Reporting rate estimates were also precise and indicated a lack of participation by the fishing industry. Estimates of natural mortality were low (0. 00 - 0. 02. year-) but imprecise...|$|R
