9|375|Public
40|$|A light curve {{data file}} is presented. Each {{citation}} includes the year, all authors, journal or book name, volume, and {{first and last}} page numbers. The asteroid list is ordered by asteroid number, followed by asteroid name, the period in hours, amplitude of variation or range of amplitude observed, and a <b>reliability</b> <b>code...</b>|$|E
40|$|Radio {{frequency}} (RF) {{windows are}} the most likely place for catastrophic failure to occur in input power couplers for particle accelerators. Reliable RF windows are essential for the success of the Accelerator Production of Tritium (APT) program because there are over 1000 windows on the accelerator, and it takes more than one day to recover from a window failure. The goals of this research are to analytically predict the lifetime of the windows, to develop a conditioning procedure, and to evaluate the performance of the RF windows. The analytical goal is to predict the lifetime of the windows. The probability of failure is predicted by the combination of a finite element model of the window, Weibull probabilistic analysis, and fracture mechanics. The window assembly is modeled in a finite element electromagnetic code in order to calculate the electric fields in the window. The geometry (i. e. mesh) and electric fields are input into a translator program to generate the mesh and boundary conditions for a finite element thermal structural code. The temperatures and stresses are determined in the thermal/structural code. The geometry and thermal structural results are input into another translator program to generate an input file for the <b>reliability</b> <b>code.</b> Material, geometry and service data are also input into the <b>reliability</b> <b>code.</b> To obtain accurate Weibull and fatigue data for the analytical model, four point bend tests were done. The analytical model is validated by comparing the measurements to the calculations. The lifetime of the windows is then determined using the <b>reliability</b> <b>code.</b> The analytical model shows the window has a good thermal mechanical design and that fast fracture is unlikely to occur below a power level of 9 Mw. The experimental goal is to develop a conditioning procedure and evaluate the performance of RF windows. During the experimental evaluation, much was learned about processing of the windows to improve the RF performance. Methods of processing included grit blasting and using various coatings...|$|E
40|$|This {{document}} is a reference guide for GAETR, Graphical Analysis of Event Trees, a software package developed at Sandia National Laboratories. GAETR {{may be used}} as a stand-alone code or as a module in the ARRAMIS{trademark} risk and <b>reliability</b> <b>code</b> suite. GAETR is designed to graphically create event trees and plot SETAC (Sandia Event Tree Analysis Code) output on IBM-compatible personal computers using the Microsoft{reg_sign} Windows{trademark} 95 /NT operating environment. This manual explains the fundamentals of creating an event tree, including formatting, saving sequence information, printing, editing, and importing graphics to other software packages...|$|E
50|$|He {{was known}} for his {{contributions}} in design of experiments as well as in multivariate analysis, survey sampling, <b>reliability,</b> <b>coding</b> theory, combinatorial theory, and other areas of statistics and mathematics. Srivastava code was invented by him.|$|R
5000|$|... #Subtitle level 2: Coefficients {{incompatible}} with alpha and the <b>reliability</b> of <b>coding</b> ...|$|R
50|$|The Motor Industry Software <b>Reliability</b> Association C++ <b>coding</b> {{standard}} references High Integrity C++.|$|R
40|$|International audienceThis paper {{deals with}} a {{multilingual}} relational lexical database of proper name, Prolexbase, a free resource available on the CNRTL website. The Prolex model is based on two main concepts: firstly, a language independent pivot and, secondly, the prolexeme (the projection of the pivot onto particular language), that {{is a set of}} lemmas (names and derivatives). These two concepts model the variations of proper name: firstly, independent of language and, secondly, language dependent by morphology or knowledge. Variation processing is very important for NLP: the same proper name can be written in different instances, maybe in different parts of speech, and it can also be replaced by another one, a lexical anaphora (that reveals semantic link). The pivot represents different referent's points of view, i. e. language independent variations of name. Pivots are linked by three semantic relations (quasi-synonymy, partitive relation and associative relation). The prolexeme is a set of variants (aliases), quasi-synonyms and morphosemantic derivatives. Prolexemes are linked to classifying contexts and <b>reliability</b> <b>code...</b>|$|E
40|$|The {{spectral}} stochastic {{finite element}} method (SSFEM) aims at constructing a probabilistic representation of the response of a mechanical system, whose material properties are random fields. The response quantities, e. g. the nodal displacements, are represented by a polynomial series expansion in terms of standard normal random variables. This expansion is usually post-processed to obtain the second-order statistical moments of the response quantities. However, in the literature, the SSFEM has also been suggested as a method for reliability analysis. No careful examination of this potential has been made yet. In this paper, the SSFEM is considered {{in conjunction with the}} first-order reliability method (FORM) and with importance sampling for finite element reliability analysis. This approach is compared with the direct coupling of a FORM <b>reliability</b> <b>code</b> and a finite element code. The two procedures are applied to the reliability analysis of the settlement of a foundation lying on a randomly heterogeneous soil layer. The results are used to make a comprehensive comparison of the two methods in terms of thei...|$|E
40|$|The coal-bed methane (CBM) {{recoverability}} is {{the basic}} premise of CBM development practice; in order to effectively evaluate the CBM recoverability, the attribute synthetic evaluation model is established based on the theory and method of attribute mathematics. Firstly, five indexes are chosen to evaluate the recoverability through analyzing the influence factors of CBM, including seam thickness, gas saturation, permeability, reservoir pressure gradient, and hydrogeological conditions. Secondly, the attribute measurement functions of each index are constructed based on the attribute mathematics theory, and the calculation methods of the single index attribute measurement and the synthetic attribute measurement also are provided. Meanwhile, the weight of each index is given with the method of similar number and similar weight; the evaluation results also {{are determined by the}} confidence criterion <b>reliability</b> <b>code.</b> At last, according to the application results of the model in some coal target area of Fuxin and Hancheng mine, the evaluation results are basically consistent with the actual situation, which proves that the evaluation model can be used in the CBM recoverability prediction, and an effective method of the CBM recoverability evaluation is also provided...|$|E
30|$|Despite such limitations, {{the benefit}} {{of being able to}} handle large volumes of email more than makes up for the {{possible}} slight loss in <b>reliability</b> of <b>coding,</b> especially considering that human coders are also not perfectly reliable.|$|R
40|$|This manual {{describes}} the model [...] specifically the observation procedures and coding systems [...] {{used in a}} longitudinal study of how children learn to comprehend what they read, with particular emphasis on science texts. Included are procedures for the following: identifying students; observing [...] recording observations and diagraming the room; writing transcripts [...] noting instructional and noninstructional utterances, and reporting other than whole-class instruction; updating, coding, and storing rosters; preparing coding sheets; and assuring <b>reliability.</b> <b>Codes</b> and categories describing the following are defined: group and individual student-teacher interactions [...] frequency of group meetings and combined groups; noninstructional time [...] teacher directed instruction, including all other subject areas, adult reading, teacher assigned centers, and testing; teacher-initiated instructional interactions (questions an...|$|R
50|$|Certain networks, such as ones {{used for}} {{cellular}} wireless broadcasting, {{do not have}} a feedback channel. Applications on these networks still require <b>reliability.</b> Fountain <b>codes</b> in general, and LT codes in particular, get around this problem by adopting an essentially one-way communication protocol.|$|R
40|$|Promotor: Jacek Kitowski, Włodzimierz Funika. Recenzent: Marian Wysocki, Stanisław Kozielski. Niepublikowana praca doktorska. Tyt. z ekranu tyt. Praca doktorska. AGH University of Science and Technology. Faculty of Computer Science, Electronics and Telecommunications. Department of Computer Science, 2014. Zawiera bibliogr. Dostępna również w wersji drukowanej. Tryb dostępu: Internet. High availability, self-healing, agent based systems, reliability, {{technology}} background, {{discovery and}} communication, Service Location Protocol SLP, Simple Service Discovery protocol SSDP, JINI, JXTA, JGroups, election algorithms, Bully Algorithm, Ring Algorithm, Santoro Rotem Algorithm, other algorithms, overview of existing monitoring systems, Ganglia, AutoPilot, Gemini, Aksum, JavaPSL, OCM-G / G-PM, J-OCM, JXM, SemMon, Dynamic Monitoring Framework, self-healing and self-adaptive Software, PANACEA, MUSIC, GRAVA, use cases and requirements, introduction, use cases, user use cases, advanced user administrator use cases, application programmer use cases, system requirements definition, functional requirements, non-functional requirements, functional requirements specification, non-functional requirements specification, architecture and design, introduction, distributed system, multi-agent based system MAS, self-healing, distributed monitoring system architecture, AgeMon architecture, agent, agent name and collision detection, monitoring result, monitoring source and measurable capability, messaging, agent communication Layer, discovery finding agents, discovery {{based on the}} multicast, discovery based on the Gossip Server, tunneling, discovery, mixed solution, roles, GUI Role, monitoring manager, visualization and visualization manager, rules manager component, monitoring role, handling feedbacks enabling the healing of the monitored system, persistence role, rule role, command line interface role, system implementation, introduction, agent, agent group, role implementation, monitoring role, GUI role, AgentGraph, monitoring component, rule component, persistence component, visualisation component, rule role, persistence role, CLI role, system-wide architectural concepts, service interface and start up/shutting down procedure, foundation services, dependency injection, communication in the AgeMon System, transport layer, abstract agent layer, abstract monitoring layer, message types, high availability and self-healing, high availability and self-healing in monitoring system, automatic discovery, reliable transport protocols, network failures tolerance, absence of single point of failure, roles redundancy, substitute agents – failover, cooperative mode – rules, advanced rules, self-healing, high availability/self-healing strategies, healing of the monitored system, transparent healing, application aware healing, testing, system deployment, performance tests, scenario 1, scenario 2, scenario 3, performance of the low-level communication layer, latency in the system, decision time, persistence time, network latencies, latency summary, self-healing tests / high availability tests, substitute persistence agent, restart monitoring agent, Healing SUM, soak test, <b>reliability,</b> <b>code</b> quality, code statistics, violations, duplications, cyclomatic complexity, LCOM 4, code quality, novel concepts introduced, future development, enlarge agent autonomy, reasoning from the historical results, advanced data analysis, learning, advanced metrics transformations, rules and action enhacements, complex event processing, advanced fault detection in monitoring system, GUI Enhancements – support for different types of charts, non-relational database...|$|E
40|$|Aquifer {{properties}} {{are subject to}} uncertainty due to randomness nature of geologic and hydraulic environmental systems. Therefore, parameter uncertainty in groundwater models casts big doubts in {{the accuracy of the}} model output. The failure in determination and taken into consideration the affect of uncertainty in model parameters could considerably reduce the possibility of success of any management or remediation scheme. Stochastic approaches in groundwater modelling are usually used to quantify the uncertainty in model parameters. The first order reliability method (FORM) has been recently used in probabilistic modelling of structural reliability applications to estimate the occurrence of low probability events. Recently, this approach was extended and used in risk and uncertainty analysis in groundwater and contaminant transport modelling. The advantage of this approach that it does not require a large number of computations in compare with other methods (e. g. Monte Carlo simulation) when applied to simple problems and it produces reasonable accurate results. However, it has been found that the computations of (FORM) can equal or exceed that of Monte Carlo in case of large number of variables. The primary difficulty in (FORM) that it requires solving of optimisation problem to locate the failure point. This optimisation procedure requires solving for the first order derivative of the limit state function at each iteration in the problem of concern. The second difficulty is the large number of variables when solving contaminant transport problems. To eliminate the limitations of (FORM), a new approach was proposed with less computation effort. The problem of optimisation approach was solved by using of automatic differentiation to obtain the Jacobian matrix of the limit state function. Therefore, the derivative code of the limit state function was coupled with <b>reliability</b> <b>code</b> and the first order derivative was obtained with a very good accuracy. The problem of large number of variables was solved by introducing a zonation approach. In this approach, the spatial variables of aquifer parameters were zoned into sub-areas based on hydrogeological properties of the aquifer and thus, the number of variables was reduced. Since the probabilistic modelling requires the best estimate of parameters and their statistical descriptors, the first challenge in implementation of probabilistic model is the determination of input data estimates and their uncertainty. The input parameters and their statistical descriptors were estimated based on the measured data in the field (i. e. pumping test results) and the historical data. However, the most difficult parameter to estimate was the groundwater recharge. This parameter was estimated using two different models for groundwater recharge estimation and the results were compared with results in literature. Statistical analysis were done finally and the mean and standard deviation of each variable were obtained besides the probability distribution. In this research, the developed probabilistic model was applied on two case studies. In the first case, (FORM) model was coupled with a three dimensional finite difference groundwater flow model. The derivative code was obtained using automatic differentiation of Fortran (ADIFOR). The results of probabilistic groundwater flow model were compared with Monte Carlo. Besides the probability of failure, sensitivity results were obtained for the given limit state function. In the second case study, FORM was coupled with a two-dimensional finite element groundwater flow and contaminant transport model. FORM-Contaminant transport model was coupled with the derivative code {{as in the case of}} groundwater flow model. In both cases, hydraulic conductivity and groundwater recharge were treated as random variables. The results of the proposed probabilistic method were compared with Monte Carlo simulation and other methods. Based on the obtained results, it is found that the use of FORM is a very good tool for probabilistic risk assessment in groundwater and contaminant transport modelling. The developed FORM approach is shown to produce results that are comparable with those obtained by other methods but with less computational effort and more accurate results. Moreover, reliability approach produces sensitivity results without any further computations...|$|E
50|$|Krippendorff’s alpha generalizes several known statistics, {{often called}} {{measures}} of inter-coder agreement, inter-rater <b>reliability,</b> <b>reliability</b> of <b>coding</b> given sets of units (as distinct from unitizing) {{but it also}} distinguishes itself from statistics that are called reliability coefficients but are unsuitable to the particulars of coding data generated for subsequent analysis.|$|R
40|$|Abstract—The {{widely used}} TCP has many {{limitations}} {{in meeting the}} throughput and latency requirements of applications in wireless networks, high-speed data center networks and heterogeneous multi-path networks. Instead of relying purely on retransmissions upon packet loss, coding has potential to improve the performance of TCP by ensuring better transmission <b>reliability.</b> <b>Coding</b> has been verified to work well at the link layer but has not been fully studied at the transport layer. There are many advantages but also challenges in exploiting coding at the transport layer. In this paper, we focus on how to leverage end-to-end coding in TCP. We reveal the problems TCP meets and the opportunities coding can bring to improve the TCP performance. We further analyze the challenges faced when applying the coding techniques to TCP and present the current applications of coding in TCP. Index Terms—End-to-end, Coding, TCP, Multi-Path TCP...|$|R
40|$|The {{researcher}} {{presents the}} details, findings, and critique of a pre-pilot study conducted on a codebook created for a textbook comparison. She used Cohen’s alpha and percent agreement to determine inter-rater <b>reliabilities</b> for <b>coding</b> categories. These values revealed changes {{needed in the}} coding scheme and in the coder training process for the future comparison study...|$|R
40|$|Abstract — This paper {{studies the}} random-coding {{exponent}} of joint source-channel coding for a scheme where source messages {{are assigned to}} disjoint subsets (referred to as classes), and codewords are independently generated according to a distri-bution {{that depends on the}} class index of the source message. For discrete memoryless systems, two optimally chosen classes and product distributions are found to be sufficient to attain the sphere-packing exponent in those cases where it is tight. Index Terms — Joint source-channel <b>coding,</b> <b>reliability</b> function, random <b>coding,</b> product distributions, sphere-packing bound. I...|$|R
40|$|To find {{factors which}} {{influence}} the <b>reliability</b> of <b>coded</b> answers to open questions. Number of categories {{allowed to use}} / information about research from which question to be coded was obtained / information about coding by other experts / approach and criteria used for coding / revising categories and restart of coding / measures of agreement. Background variables: basic characteristics/ educatio...|$|R
50|$|The California Building Standards Code, is the {{foundation}} for the design and implementation of building codes within California. The building codes include the implementation of improved safety methods, sustainability measures, consistency, new technology and construction methods, and <b>reliability.</b> These <b>codes</b> are revamped every 18 months through the Triennial and Intervening Code Adoption Cycle. These implementations are paramount to the development of building codes.|$|R
40|$|In CSCL research, {{collaboration}} through chat has primarily {{been studied}} in dyadic settings. In VMT’s larger groups it becomes harder to specify procedures for coding postings because the interactions are more complicated and ambiguous. This chapter discusses four issues that emerged during {{the development of a}} multidimensional coding procedure for smallgroup chat communication: (a) the unit of analysis and unit fragmentation, (b) the reconstruction of the response structure, (c) determining reliability without overestimation, and (d) the validity of constructs inspired by diverse theoretical-methodological stances. Threading, i. e., connections between analysis units, proved essential to handle unit fragmentation, to reconstruct the response structure and for <b>reliability</b> of <b>coding.</b> In addition, a risk for reliability overestimation is illustrated. Implications for reliability, validity and analysis methodology in CSCL are discussed. Keywords: Unit of analysis, response structure, <b>reliability,</b> validity, <b>coding</b> scheme, methodology Coding of communication processes (content analysis) to determine effects of computer-supported collaborative learning (CSCL) has become a common researc...|$|R
40|$|The Reliability and Validity of the Thin Slice Technique: Observational Research on Video Recorded Medical Interactions Introduction: Observational {{research}} using the thin slice technique has been routinely incorporated in observational research methods, however {{there is limited}} evidence supporting use of this technique compared to full interaction coding. The {{purpose of this study}} was to determine if this technique could be <b>reliability</b> <b>coded,</b> if ratings are consistent between the first, second and third slice, and if they are indeed representative of full interactions. Methods: Three 30 -second thin slices were sampled from the beginning, middle and end of a full-length video-recorded patient/physician interaction collected a part of a larger research study in a low income urban primary care clinic. Thin slice excerpts and full interactions were rated on five dimensions (liking, attention, coordination, trust and rapport) using a nine point Likert scale ranging from `none 2 ̆ 7 to `high 2 ̆ 7 by eight independent coders. Reliability was assessed using the intraclass correlation measure, validity of thin slices was assessed using the Friedman test (non-parametric equivalent of the Repeated measures ANOVA), and the comparison of thin slice coding to full interaction coding was assessed using the Wilcoxon Sign Ranks test (nonparametric version of the Paired t-test). Results: Thin slice reliability on Likert scale items ranged from. 762 -. 910 with an average IRR of. 850. Friedman tests conducted on all five variables (liking, attention, coordination, trust and rapport) comparing the rating of the three slices of the interaction were non-significant. Results of the Wilcoxon Signed Ranks test indicated there was a significant difference between the composite thin slice rating (average across three slices) and the full interaction ratings with full interaction variables rated consistently higher than their respective thin slice composite. Conclusion: Results indicate that thin slices can be <b>reliability</b> <b>coded</b> by independent coders with a high degree of agreement across coders. Observational ratings across thin slices sampled at the beginning middle and end of an interaction were not significantly different demonstrating convergent validity. However, there was a significant difference between ratings obtained from thin slices and ratings obtained from the interaction as a whole, indicating care should be taken when thin slices are used to represent the interaction as a whole...|$|R
40|$|Legacy {{software}} systems often {{suffer from}} code quality problems. Maintenance of legacy systems {{can therefore be}} costly, {{and the value of}} legacy systems may diminish {{due to a lack of}} adaptability and <b>reliability.</b> Many <b>code</b> quality problems are caused by idiomatic implementation of crosscutting concerns. This work studies the idiomatic implementation of crosscutting concerns in legacy systems, and examines whether modern language technology like aspect-oriented programming can improve the situation. 1...|$|R
40|$|Objective: Interventions are {{increasingly}} described as theory-based; however, {{the basis for}} this is often not clear. Advancing behavioral science requires {{a good understanding of}} how interventions are informed by, and test, theory. This study aims to develop a reliable method for assessing the extent to which behavioral interventions are theory-based. Design: The reliability, usability, and comprehensiveness of an initial coding scheme were improved in 13 iterative stages {{on the basis of its}} application to 29 papers, from a systematic review of interventions to promote physical activity and healthy eating. Results: The final Theory Coding Scheme contained 19 items, each with satisfactory interrater <b>reliabilities,</b> <b>coding</b> whether a theory or model was mentioned, how theories were used in intervention design, how intervention evaluations tested theory, and the implications of the results for future theory development. Conclusion: The Theory Coding Scheme is an important methodological innovation, providing a research tool to reliably describe the theoretical base of interventions, inform evidence syntheses within literature reviews and meta-analyses, and stimulate the use of empirical data for theory development...|$|R
40|$|Agreement and Information in the <b>Reliability</b> of <b>Coding</b> Coefficients that {{assess the}} {{reliability}} of data making processes – coding text, transcribing interviews, or categorizing observations into analyzable terms – are mostly conceptualized {{in terms of the}} agreement a set of coders, observers, judges, or measuring instruments exhibit. When variation is low, reliability coefficients reveal their dependency on an often neglected phenomenon, the amount of information that reliability data provide about the <b>reliability</b> of the <b>coding</b> process or the data it generates. This paper explores the concept of reliability, simple agreement, four conceptions of chance to correct that agreement, sources of information deficiency, and develops two measures of information about reliability, akin to the power of a statistical test, intended as a companion to traditional reliability coefficients, especially Krippendorff‟s (2004, pp. 221 - 250; Hayes & Krippendorff, 2007) alpha...|$|R
30|$|Since {{the project}} was {{undertaken}} (during 2013 – 2014), Lyles and Stevens (2014) have produced a list of seven procedures that plan quality researchers should follow. These relate to the replication of existing protocols, describing the scoring system, clarifying who coded the plans and the training received, double coding, pretesting, accessibility of plans, and <b>reliability</b> of <b>coding.</b> The methods used for the plan analysis addressed each of these procedures (Saunders and Ruske 2014).|$|R
40|$|A {{numerical}} {{method for}} solving the radiative transfer equation in three spatial dimensions is briefly discussed focusing on an efficient acceleration algorithm. The <b>reliability</b> of <b>coding</b> {{and accuracy of}} the algorithm are evaluated by benchmarking. Parameterization of the method and results of a simulation are presented to document {{the utility of the}} method for remote sensing applications. Attention is also given to a simple model of the hot spot effect and sample calculations...|$|R
40|$|In this paper, we {{introduce}} a novel Java implementation of multiple inter-rater agreement mea-sures, which we make available as open-source software. Besides assessing the <b>reliability</b> of <b>coding</b> tasks using S, pi, κ, α, etc., we particularly support unitizing tasks by measuring αU as {{the agreement of}} {{the boundaries of the}} identified annotation units. We provide a unified interface and data model for both tasks as well as multiple diagnostic devices for analyzing the results. ...|$|R
40|$|A {{structural}} ceramic {{analysis and}} <b>reliability</b> evaluation <b>code</b> {{has recently been}} developed encompassing volume and surface flaw induced fracture, modeled by the two-parameter Weibull probability density function. A segment of the software involves computing the Weibull polydimensional stress state crack density coefficient from uniaxial stress experimental fracture data. The relationship of the polydimensional stress coefficient to the uniaxial stress coefficient is derived for a shear-insensitive material with a random surface flaw population...|$|R
40|$|This paper {{considers}} {{an empirical}} Bayes approach to estimating {{the number of}} defectives in a hypergeometric distribution. It involves application of an empirical Bayes identity which has proved useful in providing simple Bayes estimates for various other models. Data on the <b>reliability</b> of <b>coding</b> the answers to questions from a household survey illustrate how an empirical Bayes estimate can be computed without knowledge of the prior distribution {{of the number of}} defectives. 1985 Biometrika Trust...|$|R
40|$|A <b>reliability</b> based <b>code</b> {{calibration}} procedure is a two step procedure. In {{the first step}} target reliabilities are set {{on the basis of}} experience or optimisation and in the second step corresponding partial factors and other safety elements (e. g. PSI-values as in Eurocode 1) are derived. This paper illustrates the use of the JCSS Probabilistic Model Code in both steps. Some illustrative examples, based on the method of the standardised FORM coefficients, according to ISO 2394, are presente...|$|R
40|$|Abstract—In this paper, it {{is shown}} that each Slepian–Wolf coding problem {{is related to}} a dual channel coding problem {{in the sense that}} the sphere packing exponents, random coding exponents, and correct {{decoding}} exponents in these two problems are mirror-symmetrical to each other. This mirror symmetry is interpreted as a manifestation of the linear codebook-level duality between Slepian–Wolf coding and channel coding. Furthermore, this duality, in conjunction with a systematic analysis of the expurgated exponents, reveals that nonlinear Slepian–Wolf codes can strictly outperform linear Slepian–Wolf codes in terms of rate-error tradeoff at high rates. The linear codebook-level duality is also established for general sources and channels. Index Terms—Channel coding, duality, error exponent, linear <b>code,</b> <b>reliability</b> function, Slepian–Wolf <b>coding.</b> I...|$|R
3000|$|... times {{until the}} message is {{successfully}} received by all nodes within a communication range or until the message timer expires. In safety broadcast applications where the feedback channel or acknowledgement (ACK) packets are not normally used to ensure fast message exchanges, this coding technique can congest the channel and worsen the already present problems in the random access MAC. Due to these limitations, we propose the use of high <b>reliability</b> rateless <b>codes</b> with a low encoding and decoding complexity, known as raptor codes.|$|R
40|$|Abstract. Text {{categorization}} {{technology can}} be used to streamline the process of content analysis of corpus data. However, while recent results for automatic corpus analysis show great promise, tools that are currently being used for HCI research and practice do not make use of it. Here, we empirically evaluate trade-offs between semi automatic and hand labeling of data in terms of speed, validity, and <b>reliability</b> of <b>coding</b> in order to assess the usefulness of incorporating this technology into HCI tools...|$|R
40|$|Objective: To {{examine the}} sources of coding {{discrepancy}} for injury morbidity data and explore {{the implications of these}} sources for injury surveillance. [...] Method: An on-site medical record review and recoding study was conducted for 4373 injury-related hospital admissions across Australia. Codes from the original dataset were compared to the recoded data to explore the <b>reliability</b> of <b>coded</b> data aand sources of discrepancy. [...] Results: The most common reason for differences in coding overall was assigning the case to a different external cause category with 8. 5...|$|R
50|$|Coefficients {{measuring}} {{the degree to}} which coders are statistically dependent on each other. When the <b>reliability</b> of <b>coded</b> data is at issue, the individuality of coders can have no place in it. Coders need to be treated as interchangeable. Alpha, Scott’s pi, and Pearson’s original intraclass correlation accomplish this by being definable as a function of coincidences, not only of contingencies. Unlike the more familiar contingency matrices, which tabulate N pairs of values and maintain reference to the two coders, coincidence matrices tabulate the n pairable values used in coding, regardless of who contributed them, in effect treating coders as interchangeable. Cohen’s kappa, by contrast, defines expected agreement in terms of contingencies, as the agreement that would be expected if coders were statistically independent of each other. Cohens conception of chance fails to include disagreements between coders’ individual predilections for particular categories, punishes coders who agree on their use of categories, and rewards those who do not agree with higher kappa-values. This is the cause of other noted oddities of kappa. The statistical independence of coders is only marginally related to the statistical independence of the units coded and the values assigned to them. Cohen’s kappa, by ignoring crucial disagreements, can become deceptively large when the <b>reliability</b> of <b>coding</b> data is to be assessed.|$|R
