12|0|Public
40|$|Many dynamic {{planning}} and management problems are typically characterised by a level of uncertainty regarding the value of data input such as supply and demand patterns. Assigning inaccurate values to them could invalidate {{the results of the}} study. Consequently, deterministic models are inadequate for the representation of these problems where the most crucial parameters are either unknown or are based on an uncertain future. In these cases, the scenario analysis technique could be an alternative approach. Scenario analysis can model many real problems in which decisions are based on an uncertain future, whose uncertainty is described by means of a set of possible future outcomes, called "scenarios". In this paper we present a scenario analysis approach to dynamic multi-period systems by integrating scenario optimisation and subsequent deterministic <b>reoptimisation.</b> In the scenario optimisation phase we represent data uncertainty by a robust chance optimisation model obtaining a so-called barycentric value with respect to selected decision variables. The successive <b>reoptimisation</b> model based on this barycentric solution allows planning a part of the risk of a wrong decision, reducing the negative consequences deriving from it...|$|E
40|$|This paper {{develops}} {{and estimates}} an open economy dynamic stochastic general equilibrium (DSGE) {{model of the}} Hong Kong economy. The model features short-run price rigidities generated by monopolistic competition and staggered <b>reoptimisation.</b> The model is enhanced with wealth effects due to stock price dynamics, which {{we believe to be}} important. For this reason we adopt a perpetual youth approach. Model parameters and unobserved components are estimated with a Bayesian maximum likelihood procedure, conditional on prior information concerning the values of parameters. DSGE models; wealth effects; open economy; Hong Kong...|$|E
40|$|A Neutrino Factory {{producing}} an intense beam composed of nu_e(nubar_e) and nubar_mu(nu_mu) from muon decays {{has been shown}} to have the greatest sensitivity to the two currently unmeasured neutrino mixing parameters, theta_ 13 and delta_CP. Using the `wrong-sign muon' signal to measure nu_e to nu_mu(nubar_e to nubar_mu) oscillations in a 50 ktonne Magnetised Iron Neutrino Detector (MIND) sensitivity to delta_CP could be maintained down to small values of theta_ 13. However, the detector efficiencies used in previous studies were calculated assuming perfect pattern recognition. In this paper, MIND is re-assessed taking into account, for the first time, a realistic pattern recognition for the muon candidate. <b>Reoptimisation</b> of the analysis utilises a combination of methods, including a multivariate analysis similar to the one used in MINOS, to maintain high efficiency while suppressing backgrounds, ensuring that the signal selection efficiency and the background levels are comparable or better than the ones in previous analyses...|$|E
40|$|In {{this paper}} {{we deal with}} the {{optimisation}} problem involved in determining the maximal margin separation hyperplane in support vector machines. We consider three dierent formulations, based on L 2 norm distance (the standard case), L 1 norm, and L 1 norm. We consider separation in the original space of the data (i. e., there are no kernel transformations). For any of these cases, we focus on the following problem: having the optimal solution for a given training data set, one is given a new training example. The purpose is to use the information about the solution of the problem without the additional example in order to speed up the new optimisation problem. We also consider the case of <b>reoptimisation</b> after removing an example from the data set. We report results obtained for some standard benchmark problems. 1 Introduction The support vector machine standard formulation of the problem of optimal separation of two classes of points can be found, for example, in [9]. It consist [...] ...|$|E
40|$|Sets out {{the reasons}} for these changes {{and their impact on}} the levels of detail ONS will publishIn January 2008, as part of a wider reprioritisation of the Office for National Statistics? (ONS) business, the sample size for the Monthly Production Inquiry (MPI) will be reduced by 17 per cent. Of itself, the reduced sample size would lead to lower-quality {{estimates}} of the change in production output as published in the Index of Production (IoP) First Release. However, ONS will introduce a number of methodological changes in March 2008, including a <b>reoptimisation</b> of the MPI sample. These changes will maintain, and in some cases improve, the quality of the aggregate IoP indices. At the same time, the level of detail published for the IoP will be reduced, providing greater focus on the aggregate series. This article sets out {{the reasons for}} the methodological changes and their impact on the levels of detail which ONS will publish. Economic & Labour Market Review (2008) 2, 30 – 37; doi: 10. 1057 /elmr. 2008. 7...|$|E
40|$|ABSTRACT: The {{conversion}} between multiple- and single-quantum coherences {{is integral}} to many nuclear magnetic resonance (NMR) experiments of quadrupolar nuclei. This conversion is relatively inefficient when effected by a single pulse, and many composite pulse schemes {{have been developed to}} improve this efficiency. To provide the maximum improvement, such schemes typically require time-consuming experimental optimization. Here, we demonstrate an approach for generating amplitude-modulated pulses to enhance the efficiency of the triple- to single-quantum conversion. The optimization is performed using the SIMPSON and MATLAB packages and results in efficient pulses that can be used without experimental <b>reoptimisation.</b> Most significant signal enhancements are obtained when good estimates of the inherent radio-frequency nutation rate and the magnitude of the quadrupolar coupling are used as input to the optimization, but the pulses appear robust to reasonable variations in either parameter, producing significant enhancements compared to a single-pulse conversion, and also comparable or improved efficiency over other commonly used approaches. In all cases, the ease of implementation of our method is advantageous, particularly for cases with low sensitivity, where the improvement is most needed (e. g., low gyromagnetic ratio or high quadrupolar coupling). Our approac...|$|E
40|$|A Neutrino Factory {{producing}} an intense beam composed of v(e) ((v) over bar (e)) and (v) over bar (mu) (v(mu)) from muon decays {{has been shown}} to have the greatest sensitivity to the two currently unmeasured neutrino mixing parameters theta(13) and delta(CP) Using the wrong-sign muon signal to measure v(e) -> v(mu) ((v) over bar (e) ->(v) over bar (mu)) oscillations in a 50 kt Magnetised Iron Neutrino Detector (MIND) sensitivity to delta(CP) could be maintained down to small values of theta(13) However the detector efficiencies used in these previous studies were calculated assuming perfect pattern recognition In this paper MIND is reassessed taking into account {{for the first time a}} realistic pattern recognition for the muon candidate <b>Reoptimisation</b> of the analysis utilises a combination of methods including a multivariate analysis similar to the one used in MINOS to maintain high efficiency while suppressing backgrounds ensuring that the signal selection efficiency and the background levels are comparable or better than the ones in previous analyses As a result MIND remains the most sensitive future facility for the discovery of CP violation from neutrino oscillations. Thanks to the STFC (UK) and Ministerio de Clencia e Innovacion (Spain) for support and to all collegues in both Glasgow and Valencia for all their help and advice. Peer reviewe...|$|E
40|$|We thank EPSRC (EP/E 041825 / 1 and EP/J 501542 / 1) for support, for {{the award}} of a studentship to H. C. We also thank the ERC (EU FP 7 Consolidator Grant 614290 “EXONMR”). The {{conversion}} between multiple- and single-quantum coherences {{is integral to}} many nuclear magnetic resonance (NMR) experiments of quadrupolar nuclei. This conversion is relatively inefficient when effected by a single pulse, and many composite pulse schemes {{have been developed to}} improve this efficiency. To provide the maximum improvement, such schemes typically require time-consuming experimental optimization. Here, we demonstrate an approach for generating amplitude-modulated pulses to enhance the efficiency of the triple- to single-quantum conversion. The optimization is performed using the SIMPSON and MATLAB packages and results in efficient pulses that can be used without experimental <b>reoptimisation.</b> Most significant signal enhancements are obtained when good estimates of the inherent radio-frequency nutation rate and the magnitude of the quadrupolar coupling are used as input to the optimization, but the pulses appear robust to reasonable variations in either parameter, producing significant enhancements compared to a single-pulse conversion, and also comparable or improved efficiency over other commonly used approaches. In all cases, the ease of implementation of our method is advantageous, particularly for cases with low sensitivity, where the improvement is most needed (e. g., low gyromagnetic ratio or high quadrupolar coupling). Our approach offers the potential to routinely improve the sensitivity of high-resolution NMR spectra of nuclei and systems that would, perhaps, otherwise be deemed "too challenging". Publisher PDFPeer reviewe...|$|E
40|$|Although {{most current}} cloud providers, such as Amazon Web Services (AWS) and Microsoft Azure offer {{different}} types of computing instances with different capacities, cloud users tend to hire a cluster of instances of particular type to ensure performance predictability for their applications. Nowadays, many large-scale applications including big data analytics applications feature workload patterns that have heterogeneous resource demands, for which, accounting for heterogeneity of virtual cloud instances to allocate would be highly advantageous to the application performance. However, performance predictability has been always an issue in such clusters of heterogeneous resources. In particular, to precisely decide on what instances from which types to enclose in a cluster, such that the desired performance is attained, remains an open question. To this end, we devise a resource allocation mechanism by formulating it as a Mixed-Integer programming model representing an optimization problem. Our resource allocation mechanism incorporates predictable average performance as a unified performance metric, which concerns two key performance-related issues: (a) Performance variation within same-type instances, and (b) Correlations of performance variabilities across different types. Our experimental results demonstrate that target performance is predictable and attainable for clusters of heterogeneous resources. Our mechanism constructs clusters whose performance is within 95 percent {{of the performance of}} optimal ones, hence deadlines are always met. By <b>reoptimisation,</b> our mechanism can react to performance mispredictions and support autoscaling for varying workloads. We experimentally verify our findings using clusters on Amazon EC 2 with MapReduce workloads, and on a private cloud as well. We conduct comparison experiments with an existing recent resource allocation approach in literature. 16 page(s...|$|E
40|$|Reliable and {{accurate}} quantification of nutrient species in natural waters {{is essential to}} biogeochemical studies, environmental management and compliance with legislation regarding water quality. Chapter one describes phosphorus, nitrogen and silicon species in the aquatic environment and the two flow techniques routinely used to quantify them. Air segmented flow analysis is a well documented technique used for the determination of nutrients in environmental samples and methods suitable for the determination of nitrate/nitrite (0. 5 - 100 µg L- 1 N), molybdate reactive phosphorus (0. 4 - 500 µg L- 1 P) and silicate (0. 02 - 93 mg L- 1 Si) in fresh and marine waters are discussed in chapter 2. The quality assurance procedures undertaken to ensure the methods in chapter 2 achieve good data quality, including the development of standardised procedures, assessment against international recognised standards (ISO 9000) and participation in an international collaborative exercise for development of reference materials for nutrients in seawater, are discussed in chapter 3. Satisfactory results (z scores ≤ 2) were obtained for the determination of nitrate/nitrite, molybdate reactive phosphate and silicate using the segmented flow analyser. The <b>reoptimisation</b> of a previously developed flow injection system for the high sample throughput (45 samples h- 1) determination of molybdate reactive phosphorus (2 - 100 µg L- 1 P) in freshwaters and seawater is detailed in chapter 4, The performance of the flow injection system is compared with an accredited segmented flow analysis technique and when used to quantify molybdate reactive phosphate in freshwaters, the results were in good agreement (paired t-test; p = 0. 05) with the segmented flow reference method...|$|E
40|$|The {{authors would}} like to thank EPSRC (EP/K 503162 / 1) for the award of a studentship to HFC and the ERC (EU FP 7 Consolidator Grant 614290 “EXONMR”) for support. SEA would also like to thank the Royal Society and Wolfson Foundation for a merit award. The UK 850 MHz {{solid-state}} NMR Facility used in this research was funded by EPSRC and BBSRC (contract reference PR 140003), as well as the University of Warwick including via part funding through Birmingham Science City Advanced Materials Projects 1 and 2 supported by Advantage West Midlands (AWM) and the European Regional Development Fund (ERDF). Financial support from the TGIR-RMN-THC Fr 3050 CNRS to access the 800 MHz spectrometer (Lille) is gratefully acknowledged. Although a popular choice for obtaining high-resolution solid-state NMR spectra of quadrupolar nuclei, the inherently low sensitivity of the multiple-quantum magic-angle spinning (MQMAS) experiment has limited its application for nuclei with low receptivity or when the available sample volume is limited. A number of methods have been introduced in the literature to attempt to address this problem. Recently, we have introduced an alternative, automated approach, based on numerical simulations, for generating amplitude-modulated pulses (termed FAM-N pulses) to enhance the efficiency of the triple- to single-quantum conversion step within MQMAS. This results in efficient pulses that can be used without experimental <b>reoptimisation,</b> ensuring that this method is particularly suitable for challenging nuclei and systems. In this work, we investigate the applicability of FAM-N pulses to a wider variety of systems, and their robustness under more challenging experimental conditions. These include experiments performed under fast MAS, nuclei with higher spin quantum numbers, samples with multiple distinct sites, low-γ nuclei and nuclei subject to large quadrupolar interactions. Publisher PDFPeer reviewe...|$|E
40|$|Vehicle Routing and Scheduling (VRS) {{constitute}} {{an important part}} of logistics management. Given the fact that the worldwide cost on physical distribution is evermore increasing, the global competition and the complex nature of logistics problems, one area, which determines the efficiency of all others, is the VRS activities. The application of Decision Support Systems (DSS) to assist logistics management with an efficient VRS could be of great benefit. Although the benefits of DSS in VRS are well documented, however in practice many organisations perform these activities manually using combination of skills, intuition and expertise. A comprehensive review of literature revealed several drawbacks in the existing methods for addressing VRS. The traditional optimisation approaches have very limited applications and these require high computation time. Also, heuristic approaches are capable only to specific variation, a slight difference {{in the structure of the}} problem make the algorithm inefficient. Furthermore, metaheuristics methods require higher computation time and they are context dependent. Also, further investigations on the VRS problem formulations suggest that heuristic approaches usually address a single objective of distance minimisation. However in the real world there may be a number of conflicting objectives. In general, there is a lack of considerations for route selections, resource utilisation, unhlfilled demands, underused capacities, reliability of deliveries, fleet size, human fitness and operational cost. Also, these approaches fail to realise non-linearity within objectives and constraints defined for VRS problems. Furthermore, there are no clear distinctions between hard and soft constraints considered in these methods. Finally, the existing approaches fail to capture stochastic and dynamic nature of the logistics processes. In order to overcome the above-mentioned drawbacks, this study designed and developed a hybrid DSS to assist logistics managers with VRS tasks. The capabilities of the developed DSS have then been applied to a Liquefied Petroleum Gas (LPG) distribution company. The architecture of this DSS is composed of Genetic Algorithm (GA) optimisation tool and a simulation model. The GA module aims to provide a pool of near optimum transportation schedules. The simulation module is used to further evaluate the generated schedules. The feed back from the simulation module is used to update the GA for <b>reoptimisation.</b> Some unique features of this DSS are such as: development of a multi modal genetic algorithm to address VRS problems; considering supply chain performance measures as part of VRS problem formulation; allowing consideration of different objectives, soft or hard constraints concerning the supply chain, considering linearlnonlinear relationships within objectives and constraints defined and finally, considering stochastic and dynamic behaviours of the supply chain system. The GA and simulation tool integration provides unique benefits that have not been in the literature such as consideration of practical requirements, uncertainties, dynamic and stochastic behaviours, considering several criteria and producing different alternative solutions. Also, this integration allows the GA model to filter out solutions that are less competitive and therefore reducing the simulation time evaluation, which is computationally expensive. Furthermore, the human interaction with the system assists in generating higher quality of solutions. Finally, the clear benefit of this DSS is the fact that it greatly influences the applicability of the GA generated schedules and provides better confidence in implementation of these solution...|$|E

