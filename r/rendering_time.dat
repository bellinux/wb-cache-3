329|973|Public
5|$|WALL-E was {{the most}} complex Pixar {{production}} since Monsters, Inc. because {{of the world and}} the history that had to be conveyed. Whereas most Pixar films have up to 75,000 storyboards, WALL-E required 125,000. Production designer Ralph Eggleston wanted the lighting of the first act on Earth to be romantic, and that of the second act on the Axiom to be cold and sterile. During the third act, the romantic lighting is slowly introduced into the Axiom environment. Pixar studied Chernobyl and the city of Sofia to create the ruined world; art director Anthony Christov was from Bulgaria and recalled Sofia used to have problems storing its garbage. Eggleston bleached out the whites on Earth to make WALL-E feel vulnerable. The overexposed light makes the location look more vast. Because of the haziness, the cubes making up the towers of garbage had to be large, otherwise they would have lost shape (in turn, this helped save <b>rendering</b> <b>time).</b> The dull tans of Earth subtly become soft pinks and blues when EVE arrives. When WALL-E shows EVE all his collected items, all the lights he has collected light up to give an inviting atmosphere, like a Christmas tree. Eggleston tried to avoid the colors yellow and green so WALL-E—who was made yellow to emulate a tractor—would not blend into the deserted Earth, and to make the plant more prominent.|$|E
500|$|ILM's {{visual effects}} {{supervisor}} Scott Farrar said that [...] "not {{only were the}} film's effects ambitious, they also had to be designed for 3-D", and explained the company's solutions for the new perspective: [...] "We did make sure things are as bright as possible; Michael called up theatre owners {{to make sure they}} keep the lamps bright in the theatres... make everything a little sharper, because we know that through the steps, no matter what, {{when you get to the}} final screening things tend to go less sharp." [...] On the last weekend of ILM's work on Dark of the Moon, the company's entire render farm was being used for the film, giving ILM more than 200,000 hours of rendering power a day—or equivalent to 22.8 years of <b>rendering</b> <b>time</b> in a 24-hour period. Farrar embraced the detail in creating giant robots for 3-D, making sure that in close-ups of the Transformers' faces [...] "you see all the details in the nooks and crannies of these pieces. It's totally unlike a plain surface subject like a human head or an animated head." [...] The supervisor said that Bay's style of cinematography helped integrate the robots into the scenes, as [...] "Michael is keen on having foreground/midground/background depth in his shots, even in normal live-action shots. He'll say, ’Put some stuff hanging here!' It could be women's stockings or forks and knives dangling from a string out of focus – it doesn't matter, but it gives you depth, and focus depth, and makes it more interesting." ...|$|E
500|$|According to Miyamoto, Pilotwings 64 was {{designed}} to allow gamers to experience free flight in realistic 3D environments on the Nintendo 64. Prior to the game's conception, Paradigm had worked on military vehicle and flight simulators, but not video games. Dave Gatchel of Paradigm disclosed that with regard to creating the game, they began with a [...] "physics-based approach", but deviated from this {{in order to gain}} a balance between accuracy and fun for players. He indicated that there was never an issue as to whether Pilotwings 64 should be more of an arcade game or a simulation, as their goal was to [...] "always have a more arcade feel". The technical team studied the original Pilotwings extensively during development. Pilotwings on the SNES makes use {{of the power of the}} 16-bit console, principally its Mode 7 capability. Similarly, Pilotwings 64 prominently demonstrates the graphical features of its own console. Gatchel suggested that just as design elements present in the game generated its production requirements, these same elements were influenced by the Nintendo 64's technology during development. The large islands within the game were created using Paradigm's own 3D development tool Vega UltraVision. Navigation of these environments is relatively smooth thanks to Pilotwings 64 taking advantage of several key Nintendo 64 hardware features. Conventional level of detail and mipmapping were used to reduce the computational load of distant landscape objects and terrains when they were rendered. The processes respectively substitute simpler geometrical shapes for more complex ones and less detailed textures for more detailed ones, lowering the polygon count and 3D <b>rendering</b> <b>time</b> for a given frame and thus putting less demand on the geometric engine. Pilotwings 64 also applies z-buffering, which keeps track of an object's depth and tells the graphics processor which portions of the object to render and which to hide. This, along with texture filtering and anti-aliasing, makes the object appear solid and smooth along its edges rather than pixelated.|$|E
40|$|Current {{methods to}} display {{a new home in}} the {{architectural}} visualization industry involve long <b>render</b> <b>times</b> and hundreds of frames that require <b>rendering.</b> Many <b>time,</b> these virtual tours that are produced are slow, methodical, and limit the viewer 2 ̆ 7 s perspective of the home. This research looks into using computer game engines to display the virtual tour in real time, thus removing the long <b>render</b> <b>time</b> requirements and limited viewer perspective. ...|$|R
5000|$|... System of authorship {{where both}} author's {{make changes in}} non-real <b>time</b> (<b>render</b> <b>time</b> or not at the time).|$|R
50|$|Performance {{goals are}} chiefly {{concerned}} with <b>render</b> <b>time,</b> manipulating the HTML, CSS, and JavaScript {{to ensure that}} the site opens up quickly.|$|R
2500|$|... "However, {{there is}} also a balance between look and technology," [...] adds Tippett. [...] "The body count of the wolves escalates and because we're adding a great deal more hair to get the right texture, that fur really ups the <b>rendering</b> <b>time.</b> We've gone from four wolves to eight to twelve, to sixteen in Part 2. So {{we have to be very}} careful about that balance, because it takes {{hundreds}} of hours to render each wolf." ...|$|E
5000|$|Network and {{co-operative}} rendering: <b>Rendering</b> <b>time</b> can {{be reduced}} by combining the processing power of multiple computers. IPv6 is also supported.|$|E
50|$|Speed: Rendering {{a two-hour}} movie at 24 {{frames per second}} in one year allows 3 minutes <b>rendering</b> <b>time</b> per frame, on average.|$|E
50|$|The frametimes {{benchmark}} feature (logging {{of individual}} frame <b>render</b> <b>times)</b> gained attention in 2013 on computer review sites in debate about micro stuttering in games.|$|R
5000|$|Speeding up <b>rendering</b> <b>times,</b> {{either by}} {{reducing}} the number of texels sampled to render each pixel, or increasing the memory locality of the samples taken; ...|$|R
5000|$|The eighth version updates to Camtasia have {{eliminated}} {{some of the}} prior release issues including excessive <b>rendering</b> <b>times</b> and excessive consumption of system resources during production: ...|$|R
50|$|CGI {{animated}} films can {{be rendered}} as stereoscopic 3D version by using two virtual cameras. Because the entire movie {{is basically a}} 3D model, it only takes twice the <b>rendering</b> <b>time</b> and a little effort to properly set up stereoscopic views.|$|E
50|$|Slicing {{is mainly}} used for 2D {{computer}} graphics with single-layered interfaces. Multi-layered interfaces may use slices, {{but may also}} use vector graphics (including 3D models) with the drawback of added (most often unnoticeable) <b>rendering</b> <b>time</b> and with the advantage of more options and flexibility in altering {{the appearance of the}} individual image. These alternate individual images are commonly referred to as sprites.|$|E
50|$|Performance Testing: Test Studio offers web {{performance}} testing to analyze metrics on the server {{and on the}} client, explore the HTTP traffic requests and performance counter data. Benchmarks can be created against which later results are compared for regression detection or goal setting. Performance metrics supplement the existing functional UI tests to include such data as server processing time, network latency, and client <b>rendering</b> <b>time.</b>|$|E
40|$|Abstract. In {{computer}} graphics, global illumination algorithms such as photon mapping require {{to gather}} {{large volumes of}} data which can be heavily redundant. We propose both a new characterization of useful data and a new optimization method for the photon mapping algorithm using structures borrowed from Artificial Intelligence such as autonomous agents. Our autonomous lighting agents efficiently gather large amounts of useful data and are used to make decisions during rendering. It induces less photons being cast and shorter <b>rendering</b> <b>times</b> in both photon casting and rendering phase of the photon mapping algorithm which leads to an important decrease of memory occupation and slightly shorter <b>rendering</b> <b>times</b> for equal image quality. ...|$|R
40|$|To improve {{situational}} awareness in power systems, one useful tool used in control centers is bus (or substation) data contouring. Traditionally, the methods developed have used CPU processing, leading to long contour <b>rendering</b> <b>times</b> that reduce interactivity with the visualization. To improve interactivity {{and increase the}} data rate which can be handled, contouring methods utilizing graphical processing units (GPU’s) show much promise. This paper proposes a GPU-based contouring algorithm which can easily outperform state-of-the-art CPU-based contouring algorithms. In addition, sample <b>rendering</b> <b>times</b> for a typical power system display, along with comments on the relative {{advantages and disadvantages of}} using the CPU and GPU to perform contouring, are provided...|$|R
40|$|Texture atlases are {{commonly}} used to reduce draw calls, and therefore decrease <b>render</b> <b>time</b> in real-time games. While this is common practice, little empirical {{research has been done}} on the effects of texture atlases on image quality and <b>render</b> <b>time</b> as compared un-atlased textures. In two studies, this research examined multiple tools (Ninja UV, Easy Atlas, and Texture Atlas Creator) that work in conjunction with Autodesk 2 ̆ 7 s Maya and compared their performance in <b>render</b> <b>time</b> and image quality. Results indicated that texture atlases predictably reduce <b>render</b> <b>time,</b> which is influenced by the number of original textures and sub-objects in atlased models, and primarily, by draw calls. Additionally, tools that reconfigured UVs in Maya and sampled from original textures produced superior image quality in comparison to other tools. Study 2 examined unique methods of allocating space within an atlas (world space, object space, and tiled) using Texture Atlas Creator, as well as the impact of various model properties on image quality. This study indicated that image quality in atlases increases as more sub-objects and textures are included, but decreases as a factor of triangle count and number of pixels in the original textures. Analysis of space allocation found that texture atlases created with World Space space allocation a power of 2 lower than a tiled texture atlas (i. e. World Space 210 and Tiled 2 11) were not significantly different from each other in terms of color difference. The results of this research indicate that game developers may be able to save file size by creating smaller more efficient texture atlases using World Space allocation. ...|$|R
50|$|The {{adaptive}} {{sampling strategy}} dramatically reduces the <b>rendering</b> <b>time</b> for high-quality rendering - the higher quality and/or size of data-set, the more significant {{advantage over the}} regular/even sampling strategy. However, adaptive ray casting upon a projection plane and adaptive sampling along each individual ray do not map well to the SIMD architecture of modern GPU. Multi-core CPUs, however, are a perfect fit for this technique, making them suitable for interactive ultra-high quality volumetric rendering.|$|E
5000|$|... "However, {{there is}} also a balance between look and technology," [...] adds Tippett. [...] "The body count of the wolves escalates and because we're adding a great deal more hair to get the right texture, that fur really ups the <b>rendering</b> <b>time.</b> We've gone from four wolves to eight to twelve, to sixteen in Part 2. So {{we have to be very}} careful about that balance, because it takes {{hundreds}} of hours to render each wolf." ...|$|E
50|$|Applications of the {{watercolor}} illusion {{capitalize on the}} limitation of the human visual system. The watercolor effect {{can be used by}} artists or illustrators who want to create the effect that the illusion gives off. If they want to create a light hue of a color they can take advantage of this effect and do not have to use any color {{to make it look like}} the object is filled in. Another possible application is in computer graphics rendering. If a certain tint or light color wants to fill a small space, {{the watercolor}} illusion can be applied to reduce <b>rendering</b> <b>time</b> as well amount of color used.|$|E
50|$|The RealFlow Renderkit (RFRK) {{is a set}} {{of tools}} {{designed}} to facilitate the rendering of fluids. The RFRK enables the generation of procedural geometry at <b>render</b> <b>time</b> and the <b>rendering</b> of individual fluid particles. With this interface, fluids can also be rendered as foam and spray.|$|R
50|$|The {{origin of}} the term, mipmap, is an initialism of Latin Multum In Parvo (much in a small space), and map, {{modelled}} on bitmap.The term 'pyramids' is still commonly used in a GIS context. In GIS software, pyramids are primarily used for speeding up <b>rendering</b> <b>times.</b>|$|R
5000|$|The program uses a shader script model {{inspired}} by the RenderMan Shading Language, allowing different shading styles to be written as a script that's interpreted at the <b>render</b> <b>time.</b> The different <b>rendering</b> styles are based on [...] "style modules" [...] that are written in Python programming language.|$|R
5000|$|Flash Player 10 (initially called Astro): Added basic 3D manipulation, such as {{rotating}} on the X, Y, and Z axis, a 3D drawing API, {{and texture}} mapping. Ability to create custom filters using Adobe Pixel Bender. Several visual processing tasks are now offloaded to the GPU which gives a noticeable decrease to <b>rendering</b> <b>time</b> for each frame, resulting in higher frame rates, especially with H.264 video. There {{is a new}} sound API which allows for custom creation of audio in flash, something {{that has never been}} possible before. [...] Furthermore, Flash Player 10 supports Peer to Peer (P2P) communication with Real Time Media Flow Protocol (RTMFP).|$|E
50|$|During {{the period}} of nascent {{commercial}} enthusiasm for radiosity-enhanced imagery, but prior to the democratization of powerful computational hardware, architects and graphic artists experimented with time-saving 3D rendering techniques. By darkening areas of texture maps corresponding to corners, joints and recesses, and applying maps via self-illumination or diffuse mapping in a 3D program, a radiosity-like effect of patch interaction could be created with a standard scan-line renderer. Successful emulation of radiosity required a theoretical understanding and graphic application of patch view factors, path tracing and global illumination algorithms. Texture maps were usually produced with image editing software, such as Adobe Photoshop. The advantage of this method is decreased <b>rendering</b> <b>time</b> and easily modifiable overall lighting strategies.|$|E
50|$|Each GPU renders entire frames in sequence. For example, in a Two-Way setup, one GPU {{renders the}} odd frames, {{the other the}} even frames, one after the other. Finished outputs are sent to the master for display. Ideally, this {{would result in the}} <b>rendering</b> <b>time</b> being cut by the number of GPUs available. In their advertising, Nvidia claims up to 1.9x the {{performance}} of one card with the Two-Way setup. While AFR may produce higher overall framerates than SFR, it also exhibits the temporal artifact known as Micro stuttering, which may affect frame rate perception. It is noteworthy that while the frequency at which frames arrive may be doubled, the time to produce the frame is not reduced - which means that AFR is not a viable method of reducing input lag.|$|E
40|$|A major {{challenge}} in Virtual Reality {{is to achieve}} realism at interactive rates. However, the computational time required for realistic image synthesis is significant, precluding such realism in real-time. This paper demonstrates a concept that may be exploited to reduce <b>rendering</b> <b>times</b> substantially without compromising perceived visual quality in interactive tasks. We demonstrat...|$|R
40|$|Ray tracing is an {{inherently}} parallel visualization algorithm. However to achieve good performance, at interactive frame rates, an acceleration structure {{to decrease the}} number of per ray primitive intersections is required. Grid acceleration structures {{have some of the}} fastest build times, with O(N) complexity, but traditionally achieved this at a high memory cost. Recent research has reduced the memory footprint by employing compression for one-level grids. <b>Render</b> <b>time</b> performance can be improved using multi-level grids. We describe two methods for building such multi-level grids. In the first method we employ a recursive compressed grid in which grid cells are adaptively subdivided in a variable fashion. The second method uses a finely divided compressed grid, with a lower resolution macrocell overlay to speed up traversal. We analyze the performance of these new algorithms, which enable improved <b>render</b> <b>times,</b> versus existing solutions...|$|R
40|$|I hereby {{declare that}} the {{following}} work is my own {{and has not been}} submitted, in full or in part, for any other academic assessment, coursework or presentation. All secondary data has been referenced throughout Signed: Date: II In today’s society, computer generated imagery can be found everywhere. In video games, television commercials and special effects for cinema and it is becoming harder for most viewers to be able to distinguish between computer generated imagery and reality. However, this realism comes at a cost and rendering must be efficiently carried out if <b>render</b> <b>times</b> are to be kept to a minimum. One of the key aspects when rendering is the lighting method that is being used. Certain lighting calculations can escalate <b>render</b> <b>times</b> beyond acceptable levels. This thesis aimed to determine more efficient rendering techniques that simulat...|$|R
50|$|Adobe Bridge {{is often}} used to {{organize}} files by renaming a group of them at once, assigning colored labels or star ratings assigned to files from the respective Adobe software suite, edit embedded or associated XMP and IPTC Information Interchange Model metadata, or sort or categorize them based on their metadata. It can utilize these options through different versions of a file that is part of an Adobe Version Cue project. However, it lacks the photo editing functions of Adobe Photoshop Lightroom. Image files can be shown in different sized thumbnails, slide shows or lists. Each folder, which can be bookmarked, has a cache file for speeding up <b>rendering</b> <b>time</b> of images when viewing a thumbnail. The cache can be in one central location or in individual folders.|$|E
50|$|In 2012, the School of Architecture, Art and Historic Preservation {{initiated}} a technology pilot program {{which led to}} a partnership with Samsung in the fall of 2013 to install 27-inch touchscreen monitors in classrooms across the university linked to a cloud computing network. The monitors replace traditional projectors and allow students and instructors to directly manipulate images on the screens and has dramatically reduced the need for large-format printing of drawings and posters. The rCloud system hosts CPU-intensive software applications that can be accessed via browser-based client software on- and off-campus. Because the processing for these applications happens in the cloud, students no longer need high-end computers for resource-intensive applications. In addition, the rCloud system has reduced 3-D <b>rendering</b> <b>time</b> for some architectural applications from days to hours.|$|E
50|$|There are {{two main}} {{disadvantages}} of the tiled approach. One is that some triangles may be drawn several times if they overlap several tiles. This means the total <b>rendering</b> <b>time</b> would be higher than an immediate-mode rendering system. There are also possible issues when the tiles have to be stitched {{together to make a}} complete image, but this problem was solved long ago. More difficult to solve is that some image techniques are applied to the frame as a whole, and these are difficult to implement in a tiled render where the idea is to not have to work with the entire frame. These tradeoffs are well known, and of minor consequence for systems where the advantages are useful; tiled rendering systems are widely found in handheld computing devices.|$|E
50|$|The {{process of}} {{rasterising}} 3D models onto a 2D plane for display {{on a computer}} screen ("screen space") is often carried out by fixed function hardware within the graphics pipeline. This is because there is no motivation for modifying the techniques for rasterisation used at <b>render</b> <b>time</b> and a special-purpose system allows for high efficiency.|$|R
40|$|We {{describe}} an approach for determining potentially visible sets in dynamic architectural models. Our scheme divides the models into cells and portals, computing {{a conservative estimate}} of which cells are visible at <b>render</b> <b>time.</b> The technique is simple to implement and can be easily integrated into existing systems, providing increased interactive performance on large architectural models...|$|R
40|$|Almost all {{scientific}} visualization involving surfaces {{is currently}} done via triangles. The {{speed at which}} such triangulated surfaces can be displayed is crucial to interactive visualization and is bounded by {{the rate at which}} triangulated data can be sent to the graphics subsystem for rendering. Partitioning polygonal models into triangle strips can significantly reduce <b>rendering</b> <b>times</b> over transmitting each triangle individually...|$|R
