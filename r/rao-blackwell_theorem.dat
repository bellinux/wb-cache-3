43|5|Public
25|$|This {{result is}} known as the <b>Rao–Blackwell</b> <b>theorem.</b>|$|E
25|$|The <b>Rao–Blackwell</b> <b>theorem,</b> {{a result}} which yields {{a process for}} finding the best {{possible}} unbiased estimator (in the sense of having minimal mean squared error). The MLE is often a good starting place for the process.|$|E
25|$|In statistics, the <b>Rao–Blackwell</b> <b>theorem,</b> {{sometimes}} referred to as the Rao–Blackwell–Kolmogorov theorem, is a result which characterizes the transformation of an arbitrarily crude estimator into an estimator that is optimal by the mean-squared-error criterion or any of a variety of similar criteria.|$|E
40|$|AbstractMultivariate kernel density estimators {{are known}} to {{systematically}} deviate from the true value near critical points of the density surface. To overcome this difficulty a method based on <b>Rao–Blackwell's</b> <b>theorem</b> is proposed. Local corrections of kernel density estimators are achieved by conditioning these estimators with respect to locally sufficient statistics. The asymptotic {{as well as the}} small sample size behavior of the improved estimators are studied. Asymptotic bias and variance are investigated and weak and complete consistency are derived under mild hypothesis...|$|R
40|$|Multivariate kernel density estimators {{are known}} to {{systematically}} deviate from the true value near critical points of the density surface. To overcome this difficulty a method based on <b>Rao-Blackwell's</b> <b>theorem</b> is proposed. Local corrections of kernel density estimators are achieved by conditioning these estimators with respect to locally sufficient statistics. The asymptotic {{as well as the}} small sample size behavior of the improved estimators are studied. Asymptotic bias and variance are investigated and weak and complete consistency are derived under mild hypothesis. Multivariate kernel density estimator, Rao-Blackwellization, locally sufficient statistics...|$|R
40|$|We {{establish}} the conjecture of Moore [1973. A note on Srinivasan's goodness-of-fit test. Biometrika 60, 209 - 211] that the usual plug-in estimate of a distribution function and the Rao-Blackwell {{estimate of the}} distribution function are asymptotically equivalent for a wide class of exponential family distributions. Empirical distribution function Goodness-of-fit Local central limit <b>theorem</b> <b>Rao-Blackwell...</b>|$|R
25|$|The <b>Rao–Blackwell</b> <b>theorem</b> {{states that}} if g(X) is {{any kind of}} {{estimator}} of a parameter θ, then the conditional expectation of g(X) given T(X), where T is a sufficient statistic, is typically a better estimator of θ, and is never worse. Sometimes one can very easily construct a very crude estimator g(X), and then evaluate that conditional expected value to get an estimator that is in various senses optimal.|$|E
2500|$|The {{more general}} {{version of the}} <b>Rao–Blackwell</b> <b>theorem</b> speaks of the [...] "expected loss" [...] or risk function: ...|$|E
2500|$|The theorem {{is named}} after Calyampudi Radhakrishna Rao and David Blackwell. [...] The process of {{transforming}} an estimator using the <b>Rao–Blackwell</b> <b>theorem</b> is sometimes called Rao–Blackwellization. The transformed estimator is called the Rao–Blackwell estimator.|$|E
40|$|Bayesian {{filtering}} is {{an important}} issue in Hidden Markov Chains (HMC) models. In many problems it is of interest to compute both the a posteriori filtering pdf at each time instant n and a moment Θn thereof. Sequential Monte Carlo (SMC) techniques, which include Particle filtering (PF) and Auxiliary PF (APF) algorithms, propagate a set of weighted particles which approximate that filtering pdf at time n, and then compute a Monte Carlo (MC) estimate of Θn. In this paper we show that in models where the so-called Fully Adapted APF (FA-APF) algorithm can be used such as semi-linear Gaussian state-space models, one can compute an estimate of the moment of interest at time n based only on the new observation yn and on the set of particles at time n − 1. This estimate does not suffer from the extra MC variation due to the sampling of new particles at time n, and is thus preferable to that based on that new set of particles, due to the <b>Rao-Blackwell</b> (RB) <b>theorem.</b> We finally extend our solution to models where the FA-APF cannot be used any longer. 1...|$|R
2500|$|Sufficiency finds {{a useful}} {{application}} in the <b>Rao–Blackwell</b> <b>theorem,</b> which [...] states that if g(X) is {{any kind of}} estimator of θ, then typically the conditional expectation of g(X) given sufficient statistic T(X) is a better estimator of θ, and is never worse. [...] Sometimes one can very easily construct a very crude estimator g(X), and then evaluate that conditional expected value to get an estimator that is in various senses optimal.|$|E
2500|$|An {{example of}} an improvable Rao–Blackwell improvement, when using a minimal {{sufficient}} statistic that is not complete, was provided by Galili and Meilijson in 2016. Let [...] be a random sample from a scale-uniform distribution [...] with unknown mean [...] and known design parameter [...] In the search for [...] "best" [...] possible unbiased estimators for [...] it is natural to consider [...] as an initial (crude) unbiased estimator for [...] {{and then try to}} improve it. Since [...] is not a function of , the minimal sufficient statistic for [...] (where [...] and [...] ), it may be improved using the <b>Rao–Blackwell</b> <b>theorem</b> as follows: ...|$|E
50|$|This {{result is}} known as the <b>Rao-Blackwell</b> <b>theorem.</b>|$|E
5000|$|Concerning such [...] "best {{unbiased}} estimators", {{see also}} Cramér-Rao bound, Gauss-Markov theorem, Lehmann-Scheffé theorem, <b>Rao-Blackwell</b> <b>theorem.</b>|$|E
5000|$|The {{more general}} {{version of the}} <b>Rao-Blackwell</b> <b>theorem</b> speaks of the [...] "expected loss" [...] or risk function: ...|$|E
50|$|Baddeley is {{a leading}} {{advocate}} of statistical ideas in stereology.With Cruz-Orive he demonstrated {{the role of the}} Horvitz-Thompson weighting principle and the <b>Rao-Blackwell</b> <b>theorem</b> in stereological sampling.|$|E
50|$|The theorem {{is named}} after Calyampudi Radhakrishna Rao and David Blackwell. The process of {{transforming}} an estimator using the <b>Rao-Blackwell</b> <b>theorem</b> is sometimes called Rao-Blackwellization. The transformed estimator is called the Rao-Blackwell estimator.|$|E
5000|$|By the <b>Rao-Blackwell</b> <b>theorem,</b> if [...] is an {{unbiased}} estimator of θ then [...] defines an {{unbiased estimator}} of θ with the property that its variance is not {{greater than that}} of [...]|$|E
5000|$|The <b>Rao-Blackwell</b> <b>theorem,</b> {{a result}} which yields {{a process for}} finding the best {{possible}} unbiased estimator (in the sense of having minimal mean squared error). The MLE is often a good starting place for the process.|$|E
50|$|In statistics, the <b>Rao-Blackwell</b> <b>theorem,</b> {{sometimes}} referred to as the Rao-Blackwell-Kolmogorov theorem, is a result which characterizes the transformation of an arbitrarily crude estimator into an estimator that is optimal by the mean-squared-error criterion or any of a variety of similar criteria.|$|E
5000|$|If an {{unbiased}} estimator of [...] exists, then one can prove {{there is an}} essentially unique MVUE. Using the <b>Rao-Blackwell</b> <b>theorem</b> one can also prove that determining the MVUE is {{simply a matter of}} finding a complete sufficient statistic for the family [...] and conditioning any {{unbiased estimator}} on it.|$|E
50|$|David Harold Blackwell (April 24, 1919 - July 8, 2010) was Professor Emeritus of Statistics at the University of California, Berkeley, {{and is one}} of the eponyms of the <b>Rao-Blackwell</b> <b>theorem.</b> Born in Centralia, Illinois, he was {{the first}} African American {{inducted}} into the National Academy of Sciences, and the first black tenured faculty member at UC Berkeley.|$|E
50|$|Among his best-known discoveries are the Cramér-Rao {{bound and}} the <b>Rao-Blackwell</b> <b>theorem</b> both {{related to the}} quality of estimators. Other areas he worked in include multivariate analysis, {{estimation}} theory, and differential geometry. His other contributions include the Fisher-Rao Theorem, Rao distance, and orthogonal arrays. He is the author of 14 books and has published over 400 journal publications.|$|E
50|$|The <b>Rao-Blackwell</b> <b>theorem</b> {{states that}} if g(X) is {{any kind of}} {{estimator}} of a parameter θ, then the conditional expectation of g(X) given T(X), where T is a sufficient statistic, is typically a better estimator of θ, and is never worse. Sometimes one can very easily construct a very crude estimator g(X), and then evaluate that conditional expected value to get an estimator that is in various senses optimal.|$|E
50|$|In {{cases where}} {{statistically}} independent data are modelled by a parametric family of distributions {{other than the}} normal distribution, the population standard deviation will, if it exists, {{be a function of}} the parameters of the model. One general approach to estimation would be maximum likelihood. Alternatively, {{it may be possible to}} use the <b>Rao-Blackwell</b> <b>theorem</b> as a route to finding a good estimate of the standard deviation. In neither case would the estimates obtained usually be unbiased. Notionally, theoretical adjustments might be obtainable to lead to unbiased estimates but, unlike those for the normal distribution, these would typically depend on the estimated parameters.|$|E
5000|$|An {{example of}} an improvable Rao-Blackwell improvement, when using a minimal {{sufficient}} statistic that is not complete, was provided by Galili and Meilijson in 2016. Let [...] be a random sample from a scale-uniform distribution [...] with unknown mean [...] and known design parameter [...] In the search for [...] "best" [...] possible unbiased estimators for , it is natural to consider [...] as an initial (crude) unbiased estimator for [...] {{and then try to}} improve it. Since [...] is not a function of , the minimal sufficient statistic for [...] (where [...] and [...] ), it may be improved using the <b>Rao-Blackwell</b> <b>theorem</b> as follows: ...|$|E
50|$|Over the years, {{researchers}} of ISI made fundamental {{contributions in}} various fields of Statistics such as Design of Experiments, Sample Survey, Multivariate statistics and Computer Science. Mahalanobis introduced the measure Mahalanobis distance {{which is used}} in multivariate statistics and other related fields. Raj Chandra Bose, {{who is known for}} his contributions in coding theory, worked on Design of Experiments during his tenure at ISI, {{and was one of the}} three mathematicians, who disproved Euler's conjecture on orthogonal Latin squares. Anil Kumar Bhattacharya is credited with introduction of the measures Bhattacharyya distance and Bhattacharya coefficient. Samarendra Nath Roy is known for his pioneering contributions in multivariate statistics. Among colleagues of Mahalanobis, other notable contributors were K. R. Nair in Design of experiments, Jitendra Mohan Sengupta in Sample Survey, Ajit Dasgupta in Demography and Ramkrishna Mukherjea in Quantitative Sociology. C. R. Rao's contributions during his association with ISI include two theorems of Statistical Inference known as Cramér-Rao inequality and <b>Rao-Blackwell</b> <b>Theorem,</b> and introduction of orthogonal arrays in Design of Experiments. Anil Kumar Gain is known for his contributions to the Pearson product-moment correlation coefficient with his colleague Sir Ronald Fisher at the University of Cambridge.|$|E
40|$|Preliminary test estimators {{are defined}} for {{estimating}} vector parameters. This note illustrates {{the use of}} the <b>Rao-Blackwell</b> <b>theorem</b> for uniformly reducing the risk of these estimators when one is based upon the sufficient statistic. The results given are valid for a class of risk functions, including the generalized mean squared error, the trace, and the largest characteristic root. <b>Rao-Blackwell</b> <b>theorem</b> preliminary test estimators generalized mean squared error trace largest characteristic root...|$|E
40|$|AbstractPreliminary test estimators {{are defined}} for {{estimating}} vector parameters. This note illustrates {{the use of}} the <b>Rao-Blackwell</b> <b>theorem</b> for uniformly reducing the risk of these estimators when one is based upon the sufficient statistic. The results given are valid for a class of risk functions, including the generalized mean squared error, the trace, and the largest characteristic root...|$|E
40|$|In {{this paper}} we review {{different}} {{meanings of the}} word intrinsic in statistical estimation, focusing our attention {{on the use of}} this word in the analysis of the properties of an estimator. We review the intrinsic versions of the bias and the mean square error and results analogous to the Cram´er-Rao inequality and <b>Rao-Blackwell</b> <b>theorem.</b> Different results related to the Bernoulli and normal distributions are also considered. Peer Reviewe...|$|E
40|$|The <b>Rao-Blackwell</b> <b>theorem</b> is {{utilized}} {{to analyze and}} improve the scalability of inference in large probabilistic models that exhibit symmetries. A novel marginal density estimator is introduced and shown both analytically and empirically to outperform standard estimators by several orders of magnitude. The developed theory and algorithms apply to a broad class of probabilistic models including statistical relational models considered not susceptible to lifted probabilistic inference. Comment: To appear in proceedings of AAAI 201...|$|E
40|$|We {{present a}} new design and {{inference}} method for estimating population {{size of a}} hidden population best reached through a link-tracing design. The strategy involves the <b>Rao-Blackwell</b> <b>Theorem</b> applied to a sufficient statistic markedly different from the usual one that arises in sampling from a finite population. An empirical application is described. The result demonstrates that the strategy can efficiently incorporate adaptively selected members of the sample into the inference procedure. Comment: 23 pages, 2 tables, 4 figure...|$|E
40|$|The {{method of}} Pao-Zhuan Yin-Yu {{introduced}} by Fu and Li (1992) is a data-based computer intensive resampling technique {{which provides a}} numerical solution for the minimum variance unbiased estimator of one-dimensional parameter. In this paper, the method is generalized to cover multi-dimensional parameters and functions of the parameters. This method also provides a solution to cases with nuisance parameters. Numerical examples are given to illustrate the method. Parametric resampling Method of Pao-Zhuan Yin-Yu <b>Rao-Blackwell</b> <b>theorem</b> Minimum variance unbiased estimate...|$|E
30|$|Back in the 1940 s Rao [20] and Blackwell [21] {{showed that}} an {{estimator}} {{can be improved}} by using information about conditional probabilities. Furthermore, they showed how the estimator based on this knowledge should be constructed as a conditioned expected value of an estimator not taking the extrainformation into consideration. The <b>Rao-Blackwell</b> <b>theorem</b> [22, Theorem 6.4] specifies that any convex loss function improves if a conditional probability is utilized. An important special case of the theorem is that {{it shows that the}} variance of the estimate will not increase.|$|E
40|$|The paper {{extends the}} {{principle}} of cutset sampling over Bayesian networks, presented previously for Gibbs sampling, to likelihood weighting (LW). Cutset sampling is motivated by the <b>Rao-Blackwell</b> <b>theorem</b> which implies that sampling over a subset of variables requires fewer samples for convergence due to the reduction in sampling variance. The scheme exploits the network structure in selecting cutsets that allow efficient computation of the sampling distributions. In particular, as we show empirically, likelihood weighting over a loop-cutset (abbreviated LWLC), is time-wise cost-effective. We also provide an effective way for caching the probabilities of the generated samples which improves {{the performance of the}} overall scheme. We compare LWLC against regular liklihood-weighting and against Gibbsbased cutset sampling. ...|$|E
30|$|It is {{possible}} to utilize the <b>Rao-Blackwell</b> <b>theorem</b> in recursive filtering given some properties of the involved distributions. There are mainly two reasons to use an RBPF instead of a regular particle filter. One reason is the performance gain obtained from the Rao-Blackwellization itself; however, often more important is that, by reducing the dimension of the state space where particles are used, it {{is possible}} to use less particles while maintaining the same performance. In [23] the authors compare the number of particles needed to obtain equivalent performance using different partitions of the state space in particle filter states and Kalman filter states. The RBPF method has also enabled efficient implementation of recursive Bayesian estimation in many applications, ranging between automotive, aircraft, UAV and naval applications [11, 24 – 30].|$|E
