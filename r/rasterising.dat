11|32|Public
25|$|Sun {{released}} the source {{code of the}} Class library under GPL on May 8, 2007, except some limited parts that were licensed by Sun from third parties {{who did not want}} their code to be released under a free software and open-source license. Some of the encumbered parts turned out to be fairly key parts of the platform such as font rendering and 2D <b>rasterising,</b> but these were released as open-source later by Sun (see OpenJDK Class library).|$|E
50|$|The {{process of}} <b>rasterising</b> 3D models onto a 2D plane for display on a {{computer}} screen ("screen space") is often carried out by fixed function hardware within the graphics pipeline. This is because there is no motivation for modifying the techniques for rasterisation used at render time and a special-purpose system allows for high efficiency.|$|E
50|$|Sun {{released}} the source {{code of the}} Class library under GPL on May 8, 2007, except some limited parts that were licensed by Sun from 3rd parties {{who did not want}} their code to be released under a free software and open-source license. Some of the encumbered parts turned out to be fairly key parts of the platform such as font rendering and 2D <b>rasterising,</b> but these were released as open-source later by Sun (see OpenJDK Class library).|$|E
50|$|Beyond {{projection}} of vertices & 2D clipping, near clipping {{is required to}} correctly <b>rasterise</b> 3D primitives; this is because vertices may have been projected behind the eye. Near clipping ensures that all the vertices used have valid 2D coordinates. Together with far-clipping it also helps prevent overflow of depth-buffer values. Some early texture mapping hardware (using forward texture mapping) in video games suffered from complications associated with near clipping and UV coordinates.|$|R
40|$|As {{more and}} more {{scientific}} documents become available in PDF format, their automatic analysis becomes increasingly important. We present a procedure that extracts mathematical symbols from PDF documents by examining both the original PDF file and a <b>rasterised</b> version. This provides more precise information than is available either directly from the PDF file or by traditional character recognition techniques. The data can then {{be used to improve}} mathematical parsing methods that transform the mathematics into richer formats such as MathML. 1...|$|R
40|$|When I {{worked with}} the {{research}} and development of computer-aided design for landscape architecture with Edinburgh University in the 1980 s, the race was on to find meaningful software and affordable hardware (outside of Caltech) which could cope with the curved line. How we envied the architect’s rectilinear form as we struggled with our <b>rasterised</b> grids to prepare 3 D models which, through narrowed eyes, could approximate to a ground surface - albeit one de-nuded of any meaningful life form. As we laboured with our vectors, we did not realise that the shadow obscuring the sun was that of decaying modernism...|$|R
40|$|Current {{research}} in 3 D systems and environments has produced high-quality results for traditional graphical representations. However, non-decorative text, {{serving as a}} substantive information-transfer medium, remains unintegrated. Through the combination of traditional 2 D <b>rasterising</b> technologies and the innovation of novel 3 D rendering algorithms, this project aims to provide high-quality text representations in 3 D elds of application such as user interface design, VR, and scientic visualisation...|$|E
40|$|Expected {{backgrounds}} in {{the final}} accelerator-to-target region of the European Spallation Source, {{to be built in}} Lund, Sweden, have been calculated using the MCNPX program. We consider the effects of losses from the beam, both along the full length and localised at the bending magnets, and also backsplash from the target. The prompt background is calculated, and also the residual dose, as a function of time, arising from activation of the beam components. Activation of the air is also determined. The model includes the focussing and <b>rasterising</b> magnets, and shows the effects of the concrete walls of the tunnel. We give the implications for the design and operation of the accelerator...|$|E
40|$|An {{algorithm}} {{that automatically}} detects {{the boundary between}} heartwood and sapwood within fresh Norway spruce (Picea abies (L.) Karst.) logs was developed. Stacks of digital images of the internal structure of the logs were obtained through computed tomography. Boards, oriented in the longitudinal direction of the logs and passing about through the pith line, were reconstructed from stacks of images. The reconstructed boards were inspected by three operators who independently mapped the heartwood/sapwood boundaries using an image manipulation program. Their digitised mappings were collated and then sorted into two sets. One set signified close agreement between readings of the three operators, the other signified greater variation. The data sets were used {{as a basis for}} validation of the algorithm. The algorithm initially detected the pith location. Then, for the heartwood containing no knots, the boundary was detected by drawing (<b>rasterising)</b> 360 radii (with an angle of 1 ° between adjacent radii) from the pith to the first pixel whose intensity exceeded a given threshold. A post-processing was developed to separate knots in the heartwood from sapwood by longitudinal interpolations of the boundary. The automatically detected boundary was found to be in good agreement with the mappings of the operators (mean and median differences of 1. 8 and 0. 9 mm, respectively, where mappings of operators were concordant), especially {{at the bottom of the}} stems and for the oldest trees...|$|E
40|$|This paper {{describes}} {{a system for}} cross synthesis of <b>rasterised</b> time-domain audio. Rasterisation of the audio allows alignment of the macroscopic features of audio samples of instrument tones prior to principal component analysis (PCA). Specifically a novel algorithm for straightening and aligning rastogram features has been developed {{which is based on}} an interactive process incorpo-rating the Canny detection algorithm and variable resampling. Timbral cross-synthesis is achieved by projecting a given instru-ment tone onto the principal components derived from a training set of sounds for a different tone. The alignment algorithm im-proves the efficiency of PCA for resynthesizing tones. 1...|$|R
50|$|Three-dimensional NPR is {{the style}} {{that is most}} {{commonly}} seen in video games and movies. The output from this technique is almost always a 3D model that has been modified from the original input model to portray a new artistic style. In many cases, the geometry of the model {{is identical to the}} original geometry, and only the material applied to the surface is modified. With increased availability of programmable GPU's, shaders have allowed NPR effects to be applied to the <b>rasterised</b> image that is to be displayed to the screen. The majority of NPR techniques applied to 3D geometry are intended to make the scene appear two-dimensional.|$|R
50|$|Another {{technique}} for voxels involves raster graphics where you simply raytrace every pixel {{of the display}} into the scene, tracking an error term to determine when to step. A typical implementation will raytrace each pixel of the display starting {{at the bottom of}} the screen using what is known as a y-buffer. When a voxel is reached that has a higher y value on the display it is added to the y-buffer overriding the previous value and connected with the previous y-value on the screen interpolating the color values. There is a major downside to voxel rasterization when transformation is applied which causes severe aliasing. The advantage was the ability to <b>rasterise</b> using cheap integer calculations on a CPU without hardware acceleration.|$|R
40|$|This thesis was {{submitted}} for {{the degree of}} Doctor of Philosophy and awarded by Brunel University. The commercial need to capture, process and represent the shape and form of an outline has {{lead to the development}} of a number of spline routines. These use a mathematical curve format that approximates the contours of a given shape. The modelled outline lends itself to be used on, and for, a variety of purposes. These include graphic screens, laser printers and numerically controlled machines. The latter can be employed for cutting foil, metal. plastic and stone. One of the most widely used software design packages has been the lKARUS system. This, developed by URW of Hamburg (Gennany), employs a number of mathematical descriptions that facilitate the process of both modelling and representation of font characters. It uses a variety of curve formats, including Bezier cubics, general conics and parabolics. The work reported in this dissertation focuses on developing improved techniques, primarily. for the lKARUS system. This includes two algorithms which allow a Bezier cubic description, two for a general conic representation and, yet another, two for the parabolic case. In addition, a number of algorithms are presented which promote conversions between these mathematical forms; for example, Bezier cubics to a general conic form. Furthennore, algorithms are developed to assist the process of <b>rasterising</b> both cubic and quadratic arcs. This study was partly funded by the Science and Education Research Council (SERC) ...|$|E
40|$|The {{commercial}} need to capture, {{process and}} represent the shape and fonn of an outline has {{lead to the development}} of a number of spline routines. These use a mathematical curve fonnat that approximates the contours of a given shape. The modelled outline lends itself to be used on, and for, a variety of purposes. These include graphic screens, laser printers and numerically controlled machines. The latter can be employed for cutting foil, metal. plastic and stone. One of the most widely used software design packages has been the lKARUS system. This, developed by URW of Hamburg (Gennany), employs a number of mathematical descriptions that facilitate the process of both modelling and representation of font characters. It uses a variety of curve fonnats, including Bezier cubics, general conics and parabolics. The work reported in this dissertation focuses on developing improved techniques, primarily. for the lKARUS system. This includes two algorithms which allow a Bezier cubic description. two for a general conic representation and, yet another, two for the parabolic case. In addition, a number of algorithms are presented which promote conversions between these mathematical fonns; for example, Bezier cubics to a general conic fonn. Furthennore, algorithms are developed to assist the process of <b>rasterising</b> both cubic and quadratic arcs. page iAcknowledgements I would like to thank my supervisor, Professor Michael L V Pitteway, for providing an exciting research environment throughout my time at BruneI. Through him, opportunities have been presented to me that might otherwise have been missed. My thanks goes to both Professor Pitteway and Professor Wright for providin...|$|E
40|$|Daily global solar {{radiation}} is fundamental to most ecological and biophysical processes because it {{plays a key role}} in the local and global energy budget. However, gridded information about the spatial distribution of {{solar radiation}} is limited. This study aims to parameterise the Bristow-Campbell model for the daily {{global solar radiation}} estimation in the Tibetan Plateau and propose a method to rasterise the daily global solar radiation. Observed daily solar radiation and diurnal temperature data from eleven stations over the Tibetan Plateau during 1971 - 2010 were used to calibrate and validate the Bristow-Campbell radiation model. The extra-terrestrial radiation and clear sky atmospheric transmittance were calculated on a Geographic Information System (GIS) platform. Results show that the Bristow-Campbell model performs well after adjusting the parameters, the average Pearson's correlation coefficients (r), Nash-Sutcliffe equation (NSE), ratio of the root mean square error to the standard deviation of measured data (RSR), and root mean-square error (RMSE) of 11 stations are 0. 85, 2. 81 MJ m(- 2) day(- 1), 0. 3 and 0. 77 respectively. Gridded maximum and minimum average temperature data were obtained using Parameter-elevation Regressions on Independent Slopes Model (PRISM) and validated by the Chinese Ecosystem Research Network (CERN) stations' data. The spatial daily global solar radiation distribution pattern was estimated and analysed by combining the solar radiation model (Bristow-Campbell model) and meteorological interpolation model (PRISM). Based on the overall results, it can be concluded that a calibrated Bristow-Campbell performs well for the Tibetan Plateau and can provide reasonably accurate global solar radiation estimates. The Bristow-Campbell radiation model coupled with the PRISM is effective in <b>rasterising</b> global solar radiation. (C) 2013 Elsevier Ltd. All rights reserved...|$|E
40|$|This paper {{presents}} a methodology for planimetric adjustment of elevation {{data to the}} SRTM Digital Elevation Model (SRTM-DEM), by correlation. It {{can be applied to}} geo-reference any geographic dataset with an elevation component, such as <b>rasterised</b> maps with contours or DEMs. The accuracy around 5 meters was achieved by this geo-referencing method assessed using elevation data from Portugal derived by photogrammetric techniques. The method was then used to geo-reference old topographic maps of former Portuguese colonies, which were in local geodetic datums not well related to WGS- 84. The consistency of planimetric datum shifts determined was consistent with the scales of the maps analysed. The method allowed for the use of these old maps without any extra field work. 1...|$|R
5000|$|The {{process of}} {{transforming}} representations of objects, {{such as the}} middle point coordinate of a sphere and a point on its circumference into a polygon representation of a sphere, is called tessellation. This step is used in polygon-based rendering, where objects are broken down from abstract representations ("primitives") such as spheres, cones etc., to so-called meshes, which are nets of interconnected triangles. Meshes of triangles (instead of e.g. squares) are popular as they {{have proven to be}} easy to <b>rasterise</b> (the surface described by each triangle is planar, so the projection is always convex); [...] Polygon representations are not used in all rendering techniques, and in these cases the tessellation step is not included in the transition from abstract representation to rendered scene.|$|R
40|$|It {{always has}} been a need for the abiltiy to create color proofs. When an error occurs late in the {{production}} process, itis allways complicated and difficult to correct the error. In this project, digital proofs been made and discussions havebeen held with several people in the printing industry, {{in order to examine}} how well excisting digital proofs, meet thedemand of the market. And how close the digital proofs can come to the actual printsheat from the press. The study hasbeen shown that the one thing that has had the most influence on the outcome for the quality of a digital proof, is theprintshop operator’s knowledge about color management and proofing systems. Many advertising agencies in the graphicindustry think <b>rasterised</b> proofs are not necessesary and expensive. Therefor they prefer a cheaper alternative, whichdoesn’t show colors as well as the <b>rasterised</b> proof, but well enough to be content with it. There are a good awarenessconcerning lack of communication between printshop, reproduction and advertising agency. Advertising agencies thinkthat printshop rarely listen to what they have to say, while the printshop think that the advertising agency doesn’t understandwhat they are trying to tell them. The outcome of the printed proofs in this study can’t be representive for howgood digital proofs are conducted in regular basis in the industry. The divergence between the print press sheat and thedigital proof that was made was bigger than expected. This shows that implementation of ICC profiles in a color managementflow, not alone is the answer to making perfect digital proofs. There are so many other issues that has to be examined,like color management software, measure tools and correct color management module. In order to make a perfectproof, {{you have to look at}} the whole picture. In the end, the human eye finally has the last word on wheather theproof is good or not...|$|R
40|$|This paper {{presents}} {{the findings of}} a year's study of the document analysis and document classification of PDF material. The first of these terms covers the decomposition of CCC 0894 [...] 3982 / 95 / 020207 [...] 14 Received 3 April 1996 1995 by John Wiley & Sons, Ltd. Revised 4 July 1996 208 WILLIAM S. LOVEGROVE AND DAVID F. BRAILSFORD a page into geometric elements (e. g a group of text lines, or a photograph, that form a `block' of some sort) followed by the demarcation of these blocks using paragraph breaks, inter-column gutters and so on. After a preliminary tagging of these blocks according to their apparent type (e. g. photograph, heading, paragraph etc.) {{it may be possible to}} combine this type classification with the known geometric block layout to discern which headings (for example) govern which particular text and graphic blocks. In this way one can begin document classification by inferring whether this document is a journal paper, newspaper, brochure, business letter or some other document type [4]. Previous research in this field has taken scanned bitmap images as the input to the document analysis system and classification of the document components is often guided by a priori knowledge of the document's class [5 [...] 9]. It is noteworthy that there has been hardly any research in using PostScript as a starting point for document analysis. Certainly, if a PostScript file has been designed for maximum <b>rasterising</b> efficiency it can be a daunting task even to reconstruct the `reading order' of the document. It may also be the case that previous workers have presumed that a well-structured source text will always be available to match PostScript output and, therefore, that working `bottom up' from PostScript would seldom be necessary. However, we shall find that documents can [...] ...|$|E
40|$|National Science and {{technology}} support program 2012 BAC 19 B 10 2013 BAC 03 B 04 ；Chinese Academy of Sciences XDA 05090307 Daily {{global solar radiation}} is fundamental to most ecological and biophysical processes because it {{plays a key role}} in the local and global energy budget. However, gridded information about the spatial distribution of solar radiation is limited. This study aims to parameterise the Bristow-Campbell model for the daily global solar radiation estimation in the Tibetan Plateau and propose a method to rasterise the daily global solar radiation. Observed daily solar radiation and diurnal temperature data from eleven stations over the Tibetan Plateau during 1971 - 2010 were used to calibrate and validate the Bristow-Campbell radiation model. The extra-terrestrial radiation and clear sky atmospheric transmittance were calculated on a Geographic Information System (GIS) platform. Results show that the Bristow-Campbell model performs well after adjusting the parameters, the average Pearson's correlation coefficients (r), Nash-Sutcliffe equation (NSE), ratio of the root mean square error to the standard deviation of measured data (RSR), and root mean-square error (RMSE) of 11 stations are 0. 85, 2. 81 MJ m(- 2) day(- 1), 0. 3 and 0. 77 respectively. Gridded maximum and minimum average temperature data were obtained using Parameter-elevation Regressions on Independent Slopes Model (PRISM) and validated by the Chinese Ecosystem Research Network (CERN) stations' data. The spatial daily global solar radiation distribution pattern was estimated and analysed by combining the solar radiation model (Bristow-Campbell model) and meteorological interpolation model (PRISM). Based on the overall results, it can be concluded that a calibrated Bristow-Campbell performs well for the Tibetan Plateau and can provide reasonably accurate global solar radiation estimates. The Bristow-Campbell radiation model coupled with the PRISM is effective in <b>rasterising</b> global solar radiation...|$|E
30|$|In the area-based approach, {{statistical}} metrics {{and other}} nonphysical distribution-related features of LiDAR height measurements can be extracted either directly from normalised LiDAR point clouds {{or from a}} normalised <b>rasterised</b> representation of laser hits (e.g. the normalised Digital Surface Model or Canopy Height Model (CHM) (Hyyppä et al. 2008; White et al., 2013). By using the CHM to derive metrics, the authors acknowledge the under-utilisation of the information content of the LiDAR point cloud. However this approach {{has proven to be}} operationally robust and requires less computational input (Hyyppä et al. 2008). Both area-based distributional metrics and individual tree level metrics have been extracted from CHMs in an operational workflow designed for automatic inventory estimates of Pinus radiata D. Don from LiDAR data, and this operationally orientated approach (Chen and Zhu 2013) was followed here.|$|R
40|$|Facsimile copying {{of images}} on {{polished}} mineral surfaces {{can be performed}} by rastration (dithering). when the original halftone image is replaced by the microstroke image. Different halftones are reproduced by pulse modulation of 2 -dimensional signal without loss in the visual perception due to the Nyquist theorem. Technological process of <b>rasterised</b> facsimile engraving provides the integral optical density {{on the length of}} raster element step. such that it is approximately equivalent to the optical density of the original on the same length. In the facsimile engraving, the destruction of mineral is considered in "small deviations". Theoretical analysis of the process results in the transfer function of required energy of chisel from the depth of penetration into mineral. Analysis of work in "small deviations" results in the formulation of three technological stages of transfer function, which allow to separate the deformation and destruction of compression kernel into the primary and the secondary with a minimal effect of additional chip...|$|R
40|$|The media {{industry}} is demanding increasing fidelity for their rendered images. Despite {{the advent of}} modern GPUs, the computational requirements of physically based global illumination algorithms are such {{that it is still}} not possible to render high-fidelity images in real time. The time constraints of commercial rendering are such that the user would like to have an idea as to just {{how long it will take}} to render an animated sequence, prior the actual rendering. This information is necessary to determine whether the desired quality is achievable in the time available or indeed if it is possible to a#ord to carry out the work on a render farm for example. This paper presents a comparison of different pixel profiling strategies which may be used to predict the overall rendering cost of a high fidelity global illumination solution. A fast <b>rasterised</b> scene preview is proposed which provides a more accurate positioning and weighting of samples, to achieve accurate cost prediction...|$|R
40|$|Water run-off {{modelling}} applied within {{urban areas}} requires an appropriate detailed surface model {{represented by a}} raster height grid. Accurate simulations at this scale level {{have to take into}} account small but important water barriers and flow channels given by the large-scale map definitions of buildings, street infrastructure, and other terrain objects. Thus, these 3 D features have to be <b>rasterised</b> such that each cell represents the height of the object class as good as possible given the cell size limitations. Small grid cells will result in realistic run-off modelling but with unacceptable computation times; larger grid cells with averaged height values will result in less realistic run-off modelling but fast computation times. This paper introduces a height grid generalisation approach in which the surface characteristics that most influence the water run-off flow are preserved. The first step is to create a detailed surface model (1 : 1. 000), combining high-density laser data with a detailed topographic base map. The topographic map objects are triangulated to a set of TIN-objects by taking into account the semantics of the different map object classes. These TIN objects are then <b>rasterised</b> to two grids with a 0. 5 m cell-spacing: one grid for the object class labels and the other for the TIN interpolated height values. The next step is to generalise both raster grids to a lower resolution using a procedure that considers the class label of each cell and that of its neighbours. The results of this approach are tested and validated by water run-off model runs for different cellspaced height grids at a pilot area in Amersfoort (the Netherlands). Two national datasets were used in this study: the large scale Topographic Base map (BGT, map scale 1 : 1. 000), and the National height model of the Netherlands AHN 2 (10 points per square meter on average). Comparison between the original AHN 2 height grid and the semantically enriched and then generalised height grids shows that water barriers are better preserved with the new method. This research confirms the idea that topographical information, mainly the boundary locations and object classes, can enrich the height grid for this hydrological application. OTB ResearchOTB Research Institute for the Built Environmen...|$|R
40|$|ABSTRACT: This {{paper is}} about an {{initiative}} to produce an INSPIRE compliant pan-European image data set of known spatial accuracy. Two major motivations are behind developing this product. One {{is to have a}} trusted reference for any other <b>rasterised</b> data which is already orthorectified or requires orthorectification. The other is to have a gap-free and seamless image data repository which allows a highly automated land cover analysis or change detection. To achieve these goals a cartographic reference system has to be defined and a suitable set of images must be identified. This paper describes the cartographic frame and how {{it is based on the}} proposals of the INSPIRE initiative. Furthermore the repositories for Landsat ETMþimagery mainly acquired in or around the year 2000 which were considered for this work are introduced. Both data sets had to undergo a number of preparatory steps in order to be compatible and consistent. Once properly prepared the data sets can now easily be composited or compared. Some examples are given for ongoing and future applications. ...|$|R
40|$|This {{paper is}} about an {{initiative}} to produce an INSPIRE comliant pan-European image data set of known spatial accuracy. Two major motivations are behind developing this product. One {{is to have a}} trusted reference for any other <b>rasterised</b> data which is already orthorectified or requires orthorectification. The other is to have a gap-free and seamless image data repository which allows a highly automated land cover analysis or change detection. To achieve these goals a cartographic reference system has to be defined and a suitable set of images must be identified. This paper describes the cartographic frame and how {{it is based on the}} proposals of the INSPIRE initiative. Furthermore the repositories for Landsat ETM+imagery mainly acquired in or around the year 2000 which were considered for this work are introduced. Both data sets had to undergo a number of preparatory steps in order to be compatible and consistent. Once properly prepared the data sets can now easily be composited or compared. Some examples are given for ongoing and future applications. JRC. H. 7 -Land management and natural hazard...|$|R
40|$|Since {{the days}} the {{investigating}} officers used ”pin maps” {{to locate and}} to think about crime events, crime mapping has become widespread thanks to spatial analysis mainly supplied by GIS-like software. In particular these methods suit well to geographic profiling devoted to crime series characterised by a single offender and hence limited space and time variability. Although spatial techniques are now regularly performed to delineate an offender’s area of residence, the temporal dimension is underemployed due to the wider uncertainty of time records. This paper proposes a methodology based on a least-squares adjustment in order {{to cope with this}} temporal issue for determining the most probable offender’s residence. Moreover, a chi-square test is described to check the significance of the solutions suggested by the method. The process is carried out on the real road network which has been discretised (<b>rasterised)</b> for computing convenience. Three simulations show the validity of the reasoning. Finally the main time and speed assumptions introduced in the model are discussed paving the way for further research. Peer reviewe...|$|R
40|$|A {{heuristic}} solution {{method to}} identify a predefined number of cells in raster maps optimizing both multiple intrinsic cell characteristics and cell patch compactness (Vanegas et al. in Lect. Notes Comput. Sci. 5072 : 389 – 404, 2008), is extended so that also spatial interaction is taken into account. Spatial interaction is e. g., present {{in the flow of}} sediment between cells in a <b>rasterised</b> watershed as modified by {{the presence or absence of}} tree cover in a given cell. The resulting HLC-method is elaborated and tested in the context of a site location problem for reforestation in which the following criteria are considered: (1) the sediment flow reaching the outlet of the watershed is minimized, (2) two intrinsic environmental performances of the cells are maximized/minimized, and (3) the selected cells form a compact patch. To evaluate the performance of the HLC, its results are compared to the ones obtained with an Integer Programming (IP) formulation. This comparison suggests that the HLC identifies high quality, near-to-optimal patches of cells and that its computational efficiency makes it applicable to regional sized cases. vol. 219; no. ...|$|R
40|$|Abstract—Since {{the days}} the {{investigating}} officers used ”pin maps ” {{to locate and}} to think about crime events, crime mapping has become widespread thanks to spatial analysis mainly supplied by GIS-like software. In particular these methods suit well to geographic profiling devoted to crime series characterised by a single offender and hence limited space and time variability. Although spatial techniques are now regularly performed to delineate an offender’s area of residence, the temporal dimension is underemployed due to the wider uncertainty of time records. This paper proposes a methodology based on a least-squares adjustment in order {{to cope with this}} temporal issue for determining the most probable offender’s residence. Moreover, a chi-square test is described to check the significance of the solutions suggested by the method. The process is carried out on the real road network which has been discretised (<b>rasterised)</b> for computing convenience. Three simulations show the validity of the reasoning. Finally the main time and speed assumptions introduced in the model are discussed paving the way for further research. Keywords-crime mapping; geographic profiling; temporal data quality; least-squares adjustment; raster diffusion pro-cess; error propagation I...|$|R
40|$|Abstract. This paper {{describes}} a new {{sheet metal forming}} process {{for the production of}} sheet metal components for limited-lot productions and prototypes. The kinematic based generation of the shape is implemented by means of a new forming machine comprising of two industrial robots. Compared to conventional sheet metal forming machines this newly developed sheet metal forming process offers a high geometrical form flexibility and also shows comparatively small deformation forces for high deformation degrees. The principle of the procedure is based on flexible shaping by means of a freely programmable path-synchronous movement of the two robots. The sheet metal components manufactured in first attempts are simple geometries like truncated pyramids and cones as well as spherical cups. Among other things the forming results could be improved by an adjustment of the movement strategy, a variation of individual process parameters and geometric modifications of the tools. Apart from a measurement of the form deviations of the sheet metal with a Coordinate Measurement Machine <b>rasterised</b> and deformed sheet metals were used for deformation analyses. In {{order to be able to}} use the potential of this process, a goal-oriented process design is as necessary as specific process knowledge. In order to achieve process stability and safety the essential process parameters and the process boundaries have to be determined...|$|R
40|$|The {{quantification}} {{of forest}} ecosystems {{is important for}} a variety of purposes, including the assessment of wildlife habitat, nutrient cycles, timber yield and fire propagation. This research assesses the estimation of forest structure, composition and deadwood variables from small-footprint airborne lidar data, both discrete return (DR) and full waveform (FW), acquired under leaf-on and leaf-off conditions. The field site, in the New Forest, UK, includes managed plantation and ancient, semi-natural, coniferous and deciduous woodland. Point clouds were rendered from the FW data through Gaussian decomposition. An area-based regression approach (using Akaike Information Criterion analysis) was employed, separately for the DR and FW data, to model 23 field-measured forest variables. A combination of plot-level height, intensity/amplitude and echo-width variables (the latter for FW lidar only) generated from both leaf-on and leaf-off point cloud data were utilised, together with individual tree crown (ITC) metrics from <b>rasterised</b> leaf-on height data. Statistically significant predictive models (p b 0. 05) were generated for all 23 forest metrics using both the DR and FW lidar datasets, with R 2 values for the best fit models in the range R 2 = 0. 43 – 0. 94 for the DR data and R 2 = 0. 28 – 0. 97 for the FW data (with normalised RMSE values being 18...|$|R
40|$|Urban {{areas are}} rapidly {{changing}} {{over time in}} most of the countries. Buildings are the main entities of urban areas, and building boundaries are one of the key factors for urban mapping and city modelling. Hence, building-boundary extraction from various data sources has been widely studied. Approaches to the building-boundary extraction can be categorised as either raster-based methods or point-based methods. Many research efforts have been made to raster-based methods, while few point-based methods exist. The main aim {{of this study is to}} detect and extract building boundaries from airborne lidar data using a vector-based method. Three steps are applied directly to the raw lidar points rather than the <b>rasterised</b> lidar data. Firstly, an adaptive morphological filter is developed and applied to separate ground and non-ground points. Secondly, a fusion of methods including Normalized Difference Vegetation Index (NDVI), hierarchical clustering and thresholding is developed in order to remove unwanted points such as trees and cars. Finally, building-boundary polygons are extracted and delineated, based on alpha-shape and Douglas-Peucker algorithms. The test results show that the adaptive morphological filter can accurately classify ground and non-ground points. The extracted building-boundary polygons are proven to be reliable statistically and visually. It is concluded that the proposed point-based method is able to achieve accurate results, even though the low-accuracy horizontal coordinates of the lidar data have limited the algorithm design. The proposed algorithm can also be used for more accurate building-boundary extraction if supplemental data resources such as aerial images are provided...|$|R
40|$|Information on {{existing}} landuse pattern and its spatial distribution is a pre-requisite for any watershed management programme. With {{the advent of}} remote sensing tools, with their inherent characteristics, it has been possible to prepare this dynamic resource map at various levels of confidence. But the application of multi-spectral classification techniques in vegetation discrimination {{has been the subject}} of discussion for many years. The paper describes a methodology developed to compile agricultural landuse map of a hilly watershed (where the rainfall erosivity is an ever-present threat) using geographical information systems (GIS). Multi-seasonal (monsoon-Kharif and post-monsoon-Rabi) and multi-sensor remotely-sensed data were used for mapping a landuse pattern of the watershed. The <b>rasterised</b> classified images, and the relevant watershed resources, were input and stored as separate layers in the GIS and then geometrically co-registered to a regular 30 m grid. Knowledge-based rules were developed from the informed opinion of multi-disciplinary experts and field checkings, in addition to the knowledge of local landuse patterns. These expert rules were used to manipulate the information databases to discriminate various spectrally inseparable information classes, finding out the landuse/cover categories of the Kharif season under the cloud and its shadow areas. Finally, the improved and the agricultural landuse pattern map was classified into thick forest, sparse forest, degraded pastures, open (with/without scrub) areas, cropped (Kharif + Rabi) and fallow (Kharif + Rabi) lands. The areal extent of improved/final landuse map classes compared favourably with the natural conditions of the agro-climatic region of the watershed. The paper also envisages some future studies for watershed management policies...|$|R
40|$|Degradation of land due to {{waterlogging}} in sugarcane {{track of}} western Maharashtra {{is a serious}} problem. Here researchers had understood {{the problem of the}} waterlogged areas by preparing an inventory using GIS and GPS techniques. Krishna Canal Command area is lies in Satara and Sangli districts. Total length of canal is 86 km, out of which 21 km length is comes under study area. The command area is divided in 4 segments, out of which, the present study deals with Segment IV. Water table map was prepared using water depth data at 231 well locations. The entire analysis was carried out in ArcGIS software. GIS – Spatial Analysis tool, point data was <b>rasterised,</b> and surfaces for water table were generated. Risk zonation maps were prepared using the critical limits given by IRD, Pune. Water table surface was intersected with village boundary layers. Risk zonation map of water table suggests that the two villages namely Nagral and Burli situated close to canal are most vulnerable due to high water table. Almost 36 % of the total study area of land covering partially the villages Nagral, Burli, Shirasgaon, Amnapur, Palus, and Yelavi were found under critical water table condition. In segment IV total area of 792 ha in villages Nagral, Burli, Amnapur, Yelavi, and Bhilawadi fall under the P 1 category affected most badly by the waterlogging condition. Prioritization of villages based on AWI suggests that out of the total eight villages, five villages namely Amnapur, Burli, Bhilawadi, Nagral and Yelavi are falling under P 1 priority category, need immediate attention for remedial measure...|$|R
40|$|This paper {{presents}} {{some results}} of the SPIDER project currently achieved {{in and around the}} city of Liège located in a hilly landscape. A Quick Bird image bundle product acquired on the 26 th September 2003 is used. Digital spatial data like Very High Resolution (VHR) satellite images could be provided to the public authorities of the Walloon region thanks to the Walloon cartographic gateway. In urban context and especially in hilly region, such images must be orthorectified using a Digital Surface Model (DSM). A 1 m resolution DSM was elaborated by exploiting all the 3 D data available in the study area. Firstly, a TIN was generated from vector data (points, lines and polygons) selected, on the one hand, from the 1 : 50 000 (DTED-WGS) and the 1 : 10 000 (DTM- 10000, Top 10 v-GIS) cartographic database provided by the Belgian NGI and, on the other hand, from the 1 : 1 000 3 D topographic database of the Walloon region (PICC). Then the TIN was <b>rasterised</b> and the DSM airborne LIDAR acquired along the main river-valleys is superimposed on the result. The ortho-image was then produced. The public authorities consider it as an end-product by itself which could be used in the place of the obsolete aerial orthophotos in the fast changing urban areas. Normally, the ortho-image of Liège should be available on the map server of the Walloon region in 2005, in order to check its usefulness for local and regional authorities. Moreover the local planning agency wish to take the opportunity of this study to update the spatial databases covering industrial and business development areas delineated on the urban plan. Peer reviewe...|$|R
40|$|In {{order to}} model the nitrate {{concentration}} of the recharge water in a spatially distributed way for the agricultural areas of the Walloon Region of Belgium, the EPIC model was first adapted to the specific soil description by modifying the reservoir sizes. It was also adapted to the regional crop production by modifying classcrop files in relation with observed data (both aerial and underground crop growth, yield) in wheat, sugar beet, and potato fields. As the vadose zone presents a depth between 1. 5 and 104 m in this region, new reservoirs were added according to the geological descriptions available. Deep nitrate transfer was validated in a specific site where cropping history was known. Nitrate nitrogen after harvest in the root zone was validated for wheat within different crop rotations using the first results of a nitrate-monitoring program planned by the authorities to test {{the effectiveness of the}} mitigation measures in agriculture. This extended model was also linked to a GIS (geographical information system) using 1 km 2 -cells. All the required data were <b>rasterised</b> to allow HRU (hydrological response unit) identification within the cells. The cell’s daily water flows are weighted flows of each HRU depending on their relative area within the cell. Water balances at catchment scale allow us to validate the calculation. Taking into account the evolution of distributed land use and observed climatic data, we have built maps of fast indicators and long-term indicators. The first map represents nitrate concentration in the water leaving the root zone and the second one represents the time transfer for nitrate from 1. 5 m depth to the groundwater table and nitrate concentration in recharge water. These maps constitute major tools for nitrogen management at a regional level. Peer reviewe...|$|R
