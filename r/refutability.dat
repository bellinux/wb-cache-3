59|0|Public
25|$|During the mid-20th century, Karl Popper {{emphasized}} {{the criterion of}} falsifiability to distinguish science from nonscience. Statements, hypotheses, or theories have falsifiability or <b>refutability</b> if there is the inherent possibility {{that they can be}} proven false. That is, if it is possible to conceive of an observation or an argument which negates them. Popper used astrology and psychoanalysis as examples of pseudoscience and Einstein's theory of relativity as an example of science. He subdivided nonscience into philosophical, mathematical, mythological, religious and metaphysical formulations on one hand, and pseudoscientific formulations on the other, though he did not provide clear criteria for the differences.|$|E
2500|$|Herbert A. Simon called [...] "the {{sciences}} of the artificial" [...] {{these new}} sciences (cybernetics, cognitive sciences, decision and organisation sciences) that, {{because of the}} abstraction of their object (information, communication, decision), cannot match with the classical epistemology and its experimental method and <b>refutability.</b>|$|E
2500|$|Approaches {{exist that}} allow for {{resolution}} of inconsistent beliefs without violating any of the intuitive logical principles. Most such systems use multi-valued logic with Bayesian inference and the Dempster-Shafer theory, allowing that no non-tautological belief is completely (100%) irrefutable because it must be based upon incomplete, abstracted, interpreted, likely unconfirmed, potentially uninformed, and possibly incorrect knowledge (of course, this very assumption, if non-tautological, entails its own <b>refutability,</b> if by [...] "refutable" [...] we mean [...] "not completely [...] irrefutable"). These systems effectively give up several logical principles in practice without rejecting them in theory.|$|E
2500|$|Discerning {{between science}} and [...] "pseudoscience" [...] was the theme of a book by Karl Popper whose summary was quoted in Daubert: [...] "the {{criterion}} of the scientific status of a theory is its falsifiability, or <b>refutability,</b> or testability." [...] The book, Conjectures and Refutations: The Growth of Scientific Knowledge (5th ed. 1989), pp.34–57, explains how psychology is more like astrology than astronomy {{because it does not}} make predictions about an individual which are falsifiable. He wrote that [...] "the impressive thing about" [...] Einstein's predictions [...] "is the risk involved...If observation shows that the predicted effect is definitely absent, then the theory is simply refuted." [...] But [...] "it was impossible to describe a human behaviour" [...] which would be accepted as proving psychology false.|$|E
2500|$|In contrast, Popper {{gave the}} example of Einstein's {{gravitational}} theory, which predicted [...] "light must be attracted by heavy bodies (such as the Sun), precisely as material bodies were attracted." [...] Following from this, stars closer to the Sun would appear to have moved a small distance away from the Sun, and away from each other. This prediction was particularly striking to Popper because it involved considerable risk. The brightness of the Sun prevented this effect from being observed under normal circumstances, so photographs {{had to be taken}} during an eclipse and compared to photographs taken at night. Popper states, [...] "If observation shows that the predicted effect is definitely absent, then the theory is simply refuted." [...] Popper summed up his criterion for the scientific status of a theory as depending on its falsifiability, <b>refutability,</b> or testability.|$|E
50|$|The {{confidentiality}} {{arises from}} the <b>refutability</b> of the individual responses.|$|E
5000|$|<b>Refutability,</b> {{enabling}} {{estimation of}} the degree of confidence in the model ...|$|E
5000|$|... "The {{criterion}} {{of the scientific}} status of a theory is its falsifiability, or <b>refutability,</b> or testability." [...] Popper cited in Klemke 1998 ...|$|E
5000|$|Herbert A. Simon called [...] "the {{sciences}} of the artificial" [...] {{these new}} sciences (cybernetics, cognitive sciences, decision and organisation sciences) that, {{because of the}} abstraction of their object (information, communication, decision), cannot match with the classical epistemology and its experimental method and <b>refutability.</b>|$|E
5000|$|Statements, hypotheses, or {{theories}} have falsifiability or <b>refutability</b> {{if there is}} the inherent possibility {{that they can be}} proven false. They are falsifiable if it is possible to conceive of an observation or an argument which could negate them. In this sense, falsify is synonymous with nullify, meaning to invalidate or [...] "show to be false".|$|E
5000|$|Approaches {{exist that}} allow for {{resolution}} of inconsistent beliefs without violating any of the intuitive logical principles. Most such systems use multi-valued logic with Bayesian inference and the Dempster-Shafer theory, allowing that no non-tautological belief is completely (100%) irrefutable because it must be based upon incomplete, abstracted, interpreted, likely unconfirmed, potentially uninformed, and possibly incorrect knowledge (of course, this very assumption, if non-tautological, entails its own <b>refutability,</b> if by [...] "refutable" [...] we mean [...] "not completely 100% irrefutable"). These systems effectively give up several logical principles in practice without rejecting them in theory.|$|E
50|$|During the mid-20th century, Karl Popper {{emphasized}} {{the criterion of}} falsifiability to distinguish science from nonscience. Statements, hypotheses, or theories have falsifiability or <b>refutability</b> if there is the inherent possibility {{that they can be}} proven false. That is, if it is possible to conceive of an observation or an argument which negates them. Popper used astrology and psychoanalysis as examples of pseudoscience and Einstein's theory of relativity as an example of science. He subdivided nonscience into philosophical, mathematical, mythological, religious and metaphysical formulations on one hand, and pseudoscientific formulations on the other, though he did not provide clear criteria for the differences.|$|E
5000|$|Popper {{summarized}} {{these statements}} {{by saying that}} the central criterion of the scientific status of a theory is its [...] "falsifiability, or <b>refutability,</b> or testability". Echoing this, Stephen Hawking states, [...] "A theory is a good theory if it satisfies two requirements: It must accurately describe a large class of observations {{on the basis of a}} model that contains only a few arbitrary elements, and it must make definite predictions about the results of future observations." [...] He also discusses the [...] "unprovable but falsifiable" [...] nature of theories, which is a necessary consequence of inductive logic, and that [...] "you can disprove a theory by finding even a single observation that disagrees with the predictions of the theory".|$|E
5000|$|Discerning {{between science}} and [...] "pseudoscience" [...] was the theme of a book by Karl Popper whose summary was quoted in Daubert: [...] "the {{criterion}} of the scientific status of a theory is its falsifiability, or <b>refutability,</b> or testability." [...] The book, Conjectures and Refutations: The Growth of Scientific Knowledge (5th ed. 1989), pp. 34-57, explains how psychology is more like astrology than astronomy {{because it does not}} make predictions about an individual which are falsifiable. He wrote that [...] "the impressive thing about" [...] Einstein's predictions [...] "is the risk involved...If observation shows that the predicted effect is definitely absent, then the theory is simply refuted." [...] But [...] "it was impossible to describe a human behaviour" [...] which would be accepted as proving psychology false.|$|E
5000|$|In contrast, Popper {{gave the}} example of Einstein's {{gravitational}} theory, which predicted [...] "light must be attracted by heavy bodies (such as the Sun), precisely as material bodies were attracted." [...] Following from this, stars closer to the Sun would appear to have moved a small distance away from the Sun, and away from each other. This prediction was particularly striking to Popper because it involved considerable risk. The brightness of the Sun prevented this effect from being observed under normal circumstances, so photographs {{had to be taken}} during an eclipse and compared to photographs taken at night. Popper states, [...] "If observation shows that the predicted effect is definitely absent, then the theory is simply refuted." [...] Popper summed up his criterion for the scientific status of a theory as depending on its falsifiability, <b>refutability,</b> or testability.|$|E
5000|$|The philosopher {{of science}} Karl Popper {{suggested}} that all scientific theories are by nature conjectures and inherently fallible, and that refutation to old theory is the paramount process of scientific discovery. According to Popper’s Philosophy the Growth of Scientific Knowledge {{is based upon}} Conjectures and Refutations.Prof. Shapiro’s doctoral studies with Prof. Dana Angluin attempted to provide an algorithmic interpretation to Karl Popper's approach to scientific discovery in particular for automating the [...] "Conjectures and Refutations" [...] method making bold conjectures and then performing experiments that seek to refute them. Prof. Shapiro generalized this into the [...] "Contradiction Backtracing Algorithm" [...] an algorithm for backtracking contradictions. This algorithm is applicable whenever a contradiction occurs between some conjectured theory and the facts. By testing {{a finite number of}} ground atoms for their truth in the model the algorithm can trace back a source for this contradiction, namely a false hypothesis, and can demonstrate its falsity by constructing a counterexample to it. The [...] "Contradiction Backtracing Algorithm" [...] is relevant both to the philosophical discussion on the <b>refutability</b> of scientific theories and in the aid for the debugging of logic programs. Prof. Shapiro laid the theoretical foundation for inductive logic programming and built its first implementation (Model Inference System): a Prolog program that inductively inferred logic programs from positive and negative examples. Inductive logic programming has nowadays bloomed as a subfield of artificial intelligence and machine learning which uses logic programming as a uniform representation for examples, background knowledge and hypotheses. Recent work in this area, combining logic programming, learning and probability, has given rise to the new field of statistical relational learning.|$|E
40|$|The paper {{discusses}} {{statements of}} experts about objects represented as the predicate formulas consistence with theory T. Techniques for introducing metrics on such statements and measure of <b>refutability</b> and offered. The research {{can be applied}} to solving the problems of the best reconciliation of expert statements, to constructing the decision functions in pattern recognition and building the expert systems. The offered functions of measure of <b>refutability</b> satisfies all requirements formulated in [1, 2]...|$|E
40|$|AbstractIt {{would thus}} appear {{that there is}} not much {{difference}} in the case of elementary number theory whether one uses for the underlying logic intuitionism, constructible falsity or <b>refutability.</b> When the domain of discourse is the species of natural numbers, mathematical induction is a very powerful principle which overcomes any deficencies which may be present. Thus if one wishes to obtain a separation of the concepts of intuitionism, constructible falsity and <b>refutability</b> then either one has to give up mathematical induction or else consider some species other than the natural numbers. The latter would probably be more interesting...|$|E
40|$|On {{the basis}} of the Suppes-Sneed {{structural}} view of sientific theories, we take a fresh look at the concept of <b>refutability,</b> which was famously proposed by K. R. Popper in 1934 as a criterion for the demarcation of scientific theories from non-scientific ones, e. g. pseudo-scientific and metaphysical theories...|$|E
40|$|We {{uncover the}} {{complete}} ordinal implications of supermodularity on finite lattices {{under the assumption}} of weak monotonicity. In this environment, we show that supermodularity is ordinally equivalent {{to the notion of}} quasisupermodularity introduced by Milgrom and Shannon. We conclude that supermodularity is a weak property, in the sense that many preferences have a supermodular representation. Complementarity Revealed preference Afriat's theorem <b>Refutability...</b>|$|E
40|$|Previous {{empirical}} {{tests have}} found that, {{contrary to the}} conclusions of the Heckscher-Ohlin model, the factor composition of traded goods fails to reveal relative factor abundance rankings. Using a nonparametric approach, this paper discusses the <b>refutability</b> {{of two of the}} assumptions of the HO model: that countries have identical homothetic preferences and identical constant returns to scale production functions. We find that, for two countries, the assumption on preferences cannot be refuted with expenditure data alone. However, the assumption on technologies is refutable even when some factor prices (such as the rental rate on capital) are unobserved. Finally, we consider the <b>refutability</b> of Deardorff's (1982) more general HO model, the main result of which is that the value of net factor exports at (intrinsically unobservable) autarky factor prices is negative. We show that, in the 2 × 2 case, this model can be refuted using just observed data, i. e. data from the observed situation under trade. ...|$|E
40|$|Even if falsificationism in {{the strict}} Popper-Lakatos sense {{may be too}} harsh for economics, {{falsifiability}} and <b>refutability</b> are eminent criteria for theory appraisal. Hausman's (1997) revision of his (1992) methodology of economics does not come sufficiently close to meeting such a methodological requirement and risks allowing the prioritising of irrefutable theories over empirical phenomena. It therefore needs further advancement. theory appraisal, methodology of neoclassical economics, falsifiability,...|$|E
40|$|When can a {{collection}} of matchings be stable, if preferences are unknown? This question lies behind the <b>refutability</b> of matching theory. A preference profile rationalizes {{a collection}} of matchings if the matchings are stable under the profile. Matching theory is refutable if there are observations of matchings that cannot be rationalized. I show that the theory is refutable, and provide a characterization of the matchings that can be rationalized...|$|E
40|$|The {{dissemination}} {{of science is}} grounded on the reliability and value of the research literature that, through an editorial process, expresses the necessary detail to meet the reproducibility and <b>refutability</b> demands that underlie the scientific method. Three roles are defined in this editorial process that the researcher may assume at differing times or simultaneously, {{from the beginning of}} his career to the end of professional life: those of author, reviewer and editor...|$|E
40|$|In this paper, I {{will argue}} that {{semantic}} holism is, inherently, a false claim. Semantic holism, here, being defined as {{the meaning of a}} word or expression in a given language (λ) is constructed by its relation to other words and expressions in said given language (λ) and its role in said given language (λ). The following premises and examples within the premises support the <b>refutability</b> of semantic holism, including linguistic relativity, culture and linguistic determinism...|$|E
40|$|Identification in the limit, {{originally}} due to Gold [Gold, Information and Control, 1967], is {{a widely}} used computation model for inductive inference and human language acquisition. We consider a nonconstructive extension to Gold 2 ̆ 7 s model. Our current topic {{is the problem of}} applying the notions of reliability and <b>refutability</b> to nonconstructive identification. Four general identification situations are defined and two of them are studied. Thus some questions left open in [Kucevalovs, 2010] are now closed...|$|E
40|$|We {{study the}} ordinal content of {{assuming}} supermodularity, including {{conditions under which}} a binary relation can be represented by a supermodular function. When applied to revealed-preference relations, our results imply that supermodularity is some times not refutable: Choices on consumption at different prices can be rationalized with a supermodular utility {{if they can be}} rationalized. Hence, supermodularity of utility has no testable implications for consumer behavior. We investigate the <b>refutability</b> of supermodularity in different contexts: assortative matching, decision under uncertainty, and production technologies. ...|$|E
40|$|Abstract. Supervaluational {{semantics}} {{have been}} applied rather successfully {{to a variety of}} phenomena involving truth-value gaps, such as vagueness, lack of reference, sortal incorrectedness. On the other hand, they have not registered a comparable fortune (if any) in connection with truth-value gluts, i. e., more generally, with semantic phenomena involving overdeterminacy or inconsistency as opposed to indeterminacy and incompleteness. In this paper I review some basic routes that are available for this purpose. The outcome is a family of semantic systems in which (i) logical truths and falsehoods retain their classical status even in the presence gaps and gluts, although (ii) the general notions of satifiability and <b>refutability</b> are radically non-classical. 1...|$|E
40|$|Formalising the {{confrontation}} of opinions (models) to observations (data) {{is the task}} of Inferential Statistics. Information Theory provides us with a basic functional, the relative entropy (or Kullback-Leibler divergence), an asymmetrical measure of dissimilarity between the empirical and the theoretical distributions. The formal properties of the relative entropy {{turn out to be}} able to capture every aspect of Inferential Statistics, as illustrated here, for simplicity, on dices (= i. i. d. process with finitely many outcomes) : <b>refutability</b> (strict or probabilistic) : the asymmetry data / models; small deviations: rejecting a single hypothesis; competition between hypotheses and model selection; maximum likelihood: model inference and its limits; maximum entropy: reconstructing partially observed data; EM-algorithm; flow data and gravity modelling; determining the order of a Markov chain. Comment: 31 pages. 2 figures...|$|E
40|$|Recently, hybrid-time flow {{systems have}} been {{introduced}} as an extension to timed transition systems, hybrid automata, continuous time evolutions of differential equations etc. Furthermore, a number of notions of bisimulation have been defined on these flow systems reflecting abstraction from certain timing properties. In this paper, we research the difference in abstraction level between this new semantic model of flow systems, and the more traditional model of real-time transition systems. We explore translations between {{the old and new}} semantic models, and we give a necessary and sufficient condition, called finite-set <b>refutability,</b> for these translations to be without loss of information. Finally, we show that differential inclusions with an upper-semicontinuous, closed and convex right-hand side, are finite-set refutable, and easily extend this result to impuls differential inclusions and hybrid automata...|$|E
40|$|Abstract: Experiments {{with the}} simple {{pendulum}} are easy, but its motion is nevertheless confounded with simple harmonic motion. However, refined theoretical models of the pendulum can, today, be easily taught using software like calcode. Similarly, the cycloidal pendulum is isochronous only in simplified theory. But what are theoretically equal intervals of time? Newton ac-cepted Barrow’s even tenor hypothesis, but conceded that ‘equal mo-tions ’ did not exist—the <b>refutability</b> of Newtonian physics is indepen-dent of time measurement. However, time measurement was the key difficulty in reconciling Newtonian physics with electrodynamics. On Poincaré’s criterion of convenience, equal intervals of time ought be so defined as to make the enunciation of physics simple. Hence he postulated constancy {{of the speed of}} light. (The Michelson-Morley experiment was not critical.) The theory of relativity followed. But does there exist a proper clock...|$|E
40|$|This paper {{proposes a}} method of {{discovering}} some kinds of differential equations with interval coefficients, which characterize or explain numerical data obtained by scientific observations and experiments. Such numerical data inevitably involve some ranges of errors, and hence they are represented by closed intervals in this paper. Based on these intervals, we design some interval inclusions which approximate integral equations equivalent to the differential equations. Interval coefficients of the differential equations are determined by solving the interval inclusions. Many combinations of interval coefficients {{can be obtained from}} numerical data. We find out a differential equation whose coefficients consist of intersections of the computed interval coefficients. The <b>refutability</b> of the differential equation is also discussed. Our discovering method is verified by some simulations. 1 Introduction By rapid progress of observation and experiment systems, it has become possible to get [...] ...|$|E
40|$|The {{method of}} {{instrumental}} variables {{was first used}} in the 1920 s to estimate supply and demand elasticities, and later used to correct for measurement error in single-equation models. Recently, instrumental variables {{have been widely used}} to reduce bias from omitted variables in estimates of causal relationships such as the effect of schooling on earnings. Intuitively, instrumental variables methods use only a portion of the variability in key variables to estimate the relationships of interest; if the instruments are valid, that portion is unrelated to the omitted variables. We discuss the mechanics of instrumental variables, and the qualities that make for a good instrument, devoting particular attention to instruments that are derived from 'natural experiments. ' A key feature of the natural experiments approach is the transparency and <b>refutability</b> of identifying assumptions. We also discuss the use of instrumental variables in randomized experiments. ...|$|E
40|$|Supervaluational {{semantics}} {{have been}} applied rather successfully {{to a variety of}} phenomena involving truth-value gaps, such as vagueness, lack of reference, sortal incorrectedness. On the other hand, they have not registered a comparable fortune (if any) in connection with truth-value gluts, i. e., more generally, with semantic phenomena involving overdeterminacy or inconsistency as opposed to indeterminacy and incompleteness. In this paper I review some basic routes that are available for this purpose. The outcome is a family of semantic systems in which (i) logical truths and falsehoods retain their classical status even in the presence gaps and gluts, although (ii) the general notions of satifiability and <b>refutability</b> are radically non-classical. 1. Introduction Since its first appearance in van Fraassen's semantics for free logic [1966 a, 1966 b], the notion of a supervaluation has been regarded by many as a powerful tool for dealing with truth-value gaps and, more generally, with [...] ...|$|E
40|$|The {{project of}} a {{full-fledged}} atmospheric aesthetics is largely {{committed to the}} descriptive tools of felt experiences, just in keeping with a phenomenological – or, better, a neo-phenomenological – approach. However, description, {{what we might call}} ‘atmospherography’, all the more one allegedly accomplished from a first-person point of view, is always a partial, if not particular, perspective. Thus a coherent aesthetics of atmospheres has also to envisage a systematic way of connecting the manifold of descriptions in a theory, call it ‘atmospherology’, which in turn should be able to account for a number of possible experiential counterfactuals. The present paper critically explores the consistency of both the epistemological and ontological consequences of an aesthetics of atmospheres, as sketched mainly by Gernot Böhme, a pioneer and a leading figure in this research field, and tries to specify at least some <b>refutability</b> conditions for such a theory. </p...|$|E
40|$|We {{define a}} general notion of <b>refutability</b> for logical calculi, similar to finite failure for logic programs, and {{we call a}} sequent {{calculus}} K for a logic L bicomplete, iff every nonderivable sequent of L is refutable by K. Thus for a bicomplete sequent calculus every sequent is either derivable or refutable. Now we show that from any bicomplete calculus for a logic L we may define a canonical semantics for L. For the semantics obtained {{in this way the}} corresponding bicomplete calculus provides {{a solution to the problem}} of constructing semantical counterexamples for nonprovable formulae of L. In particular for intuitionistic logic we are going to give three such calculi and we are thus obtaining three different types of semantics for it. One of them is the familiar Kripkean semantics. Thus the bicomplete calculus generating this semantics gives at the same time a very perspicuous solution to the notorious problem of finding Kripkean counterexamples for intuitionistically [...] ...|$|E
