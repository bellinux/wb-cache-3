34|108|Public
50|$|As {{the number}} of random splits {{approaches}} infinity, the result of repeated <b>random</b> <b>sub-sampling</b> validation tends towards that of leave-p-out cross-validation.|$|E
50|$|In k-fold cross-validation, the {{original}} sample is randomly partitioned into k equal sized subsamples.Of the k subsamples, a single subsample is retained as the validation data for testing the model, {{and the remaining}} k − 1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), {{with each of the}} k subsamples used exactly once as the validation data. The k results from the folds can then be averaged to produce a single estimation. The advantage of this method over repeated <b>random</b> <b>sub-sampling</b> (see below) is that all observations are used for both training and validation, and each observation is used for validation exactly once. 10-fold cross-validation is commonly used, but in general k remains an unfixed parameter.|$|E
30|$|In [18], a 2 D CS signal {{model has}} been {{proposed}} for inverse synthetic aperture radar (ISAR) imaging radar in which a <b>random</b> <b>sub-sampling</b> in both range and azimuth dimensions is utilized. Also, a 2 D CS image reconstruction algorithm based on iterative gradient projection is derived in [25].|$|E
30|$|Contamination is {{achieved}} by adding a constant C to the uncontaminated observations for a 5 % <b>random</b> <b>sub-sample</b> taken without replacement (i.e. 500 observations out of the total sample). We consider six contamination settings, namely C =  5, C =  10, C =  15, C =  20, C =  25 and C =  30. The R code is provided in Additional file 1.|$|R
30|$|Each MySpace profile {{was viewed}} one {{time by the}} lead investigator. Profiles were not archived, printed nor saved. A 10 % <b>random</b> <b>sub-sample</b> of {{profiles}} was coded by an additional investigator within several hours of the initial coding {{in order to assess}} inter-rater reliability. This was analyzed using the Cohen’s Kappa statistic and the degree of agreement between the two investigators for the variable of sexual references was 0.87.|$|R
40|$|A {{new method}} of {{constructing}} tests {{is presented in}} this article for the purpose of developing a test from student perception of the course [...] The Test on Understanding Science (TOUS) and the Science Process Inventory (SPI) were used as sources of items. A <b>random</b> <b>sub-sample</b> of 921 students, taking boti the pretest and posttest of TOUS and SPI during the 1967 - 68 Project Physics (PP) experimental period, served as sources of empirical data. The McNemar chi square analysis was used to select test items empirically [...] Every item was analyzed with respect to the changes in student responses between the pretest and posttest. The items showing a statistically significant change in response were combined into a single instrument called "A Measurement of Knowledge About Science and Scientists (Project Physics: Form 1) " (KASSET 1). _Another independent <b>random</b> <b>sub-sample</b> of 64 students was tested to describe the statistical attributes of KASSPPL. Findings showed that KASSPP 1 had a greater predictive validity for PP than either TOUS or SPI. _ Application pf the present method to formative evaluation was recommended,-(CC...|$|R
40|$|A {{keystroke}} {{biometric system}} {{was developed to}} accommodate touchscreen keystroke features. In addition to the usual key-press and key-transition features available from mechanical keyboards, the touchscreen features also included pressure, press location, accelerometer, and gyroscope information. Short numeric input data, ten-digit numeric strings, were collected from 52 participants on identical Android smartphones. A number of user authentication biometric experiments were performed on these data to measure overall system performance and to quantify the biometric value of various feature subsets. System performance was measured using the standard biometric techniques of receiver operating characteristic curves and equal error rates. Two validation models (repeated <b>random</b> <b>sub-sampling</b> and leave-one-out cross-validation) and two distance metrics (Euclidean and Manhattan) were compared in the study. Results were compared to previous keystroke biometric studies. The touchscreen keystroke features not available on mechanical keyboards greatly outperformed the usual mechanical-keyboard-like keystroke features with an equal error rate of 3. 9 % to 19. 7 %, respectively. Additionally, the Manhattan distance metric outperformed the Euclidean distance metric, and leave-one-out cross-validation outperformed repeated <b>random</b> <b>sub-sampling.</b> Of the various touchscreen feature subsets, the gyroscope features performed best with an equal error rate of 4. 3 %. ...|$|E
40|$|Abstract Background We {{present a}} method {{utilizing}} Healthcare Cost and Utilization Project (HCUP) dataset for predicting disease risk of individuals {{based on their}} medical diagnosis history. The presented methodology may be incorporated {{in a variety of}} applications such as risk management, tailored health communication and decision support systems in healthcare. Methods We employed the National Inpatient Sample (NIS) data, which is publicly available through Healthcare Cost and Utilization Project (HCUP), to train random forest classifiers for disease prediction. Since the HCUP data is highly imbalanced, we employed an ensemble learning approach based on repeated <b>random</b> <b>sub-sampling.</b> This technique divides the training data into multiple sub-samples, while ensuring that each sub-sample is fully balanced. We compared the performance of support vector machine (SVM), bagging, boosting and RF to predict the risk of eight chronic diseases. Results We predicted eight disease categories. Overall, the RF ensemble learning method outperformed SVM, bagging and boosting in terms of the area under the receiver operating characteristic (ROC) curve (AUC). In addition, RF has the advantage of computing the importance of each variable in the classification process. Conclusions In combining repeated <b>random</b> <b>sub-sampling</b> with RF, we were able to overcome the class imbalance problem and achieve promising results. Using the national HCUP data set, we predicted eight disease categories with an average AUC of 88. 79 %. </p...|$|E
40|$|This paper {{describes}} an efficient method for learning {{the parameters of}} a Gaussian process (GP). The parameters are learned from multiple tasks which are assumed to have been drawn independently from the same GP prior. An efficient algorithm is obtained by extending the informative vector machine (IVM) algorithm to handle the multi-task learning case. The multi-task IVM (MTIVM) saves computation by greedily selecting the most informative examples from the separate tasks. The MT-IVM is also shown to be more efficient than <b>random</b> <b>sub-sampling</b> on an artificial data-set and {{more effective than the}} traditional IVM in a speaker dependent phoneme recognition task...|$|E
50|$|In {{supervised}} learning, a <b>random</b> <b>sub-sample</b> of {{all records}} is taken and manually classified as either 'fraudulent' or 'non-fraudulent'. Relatively rare {{events such as}} fraud {{may need to be}} over sampled to get a big enough sample size. These manually classified records are then used to train a supervised machine learning algorithm. After building a model using this training data, the algorithm should be able to classify new records as either fraudulent or non-fraudulent.|$|R
40|$|Abstract. We {{present an}} {{algorithm}} for learning stable machines which {{is motivated by}} recent results in statistical learning theory. The algorithm is similar to Breiman’s bagging despite some important differences in that it computes an ensemble combination of machines trained on small <b>random</b> <b>sub-samples</b> of an initial training set. A remarkable property {{is that it is}} often possible to just use the empirical error of these combinations of machines for model selection. We report experiments using support vector machines and neural networks validating the theory...|$|R
50|$|When {{estimating}} the cross-covariance {{of a pair}} of signals that are wide-sense stationary, missing samples do not need be <b>random</b> (e.g., <b>sub-sampling</b> by an arbitrary factor is valid).|$|R
40|$|Recently, {{personalized}} recommender {{systems have}} become indispensable {{in a wide}} variety of commercial applications due to the vast amount of overloaded information. Network-based rec-ommendation algorithms for user-object link predictions have achieved significant developments. But most previous researches on network-based algorithm tend to ignore users ’ explicit ratings for objects or only select users ’ higher ratings which lead to the loss of information and even sparser data. With this understanding, we propose an improved network-based recommendation algorithm. In the process of reallocation of user’s recommendation power, this paper originally transfers users ’ explicit scores to users ’ interest similarity and user’s representativeness. Finally, we validate the proposed approach by performing large-scale <b>random</b> <b>sub-sampling</b> experiments on a widely used data set (Movielens) and compare our method with two other algorithms by two accuracy criteria. Results show that our approach significantly outperforms other algorithms...|$|E
40|$|Processing {{metabolomic}} {{liquid chromatography}} and mass spectrometry (LC/MS) data files is time consuming. Currently available R tools allow {{for only a}} limited number of processing steps and online tools are hard to use in a programmable fashion. This paper introduces the metabolite automatic identification toolkit MAIT package, which allows users to perform end-to-end LC/MS metabolomic data analysis. The package is especially focused on improving the peak annotation stage and provides tools to validate the statistical results of the analysis. This validation stage consists of a repeated <b>random</b> <b>sub-sampling</b> cross-validation procedure evaluated through the classification ratio of the sample files. MAIT also includes functions that create a set of tables and plots, such as principal component analysis (PCA) score plots, cluster heat maps or boxplots. To identify which metabolites are related to statistically significant features, MAIT includes a metabolite database for a metabolite identification stage...|$|E
40|$|Abstract. —I {{investigated}} {{the impacts of}} phylogeographic sampling decisions on species tree estimation in the Sceloporus undulatus species group, a recent radiation of small, insectivorous lizards connected by parapatric and peripatric distribu-tion across North America, {{using a variety of}} species tree inference methods (Bayesian estimation of species trees, Bayesian untangling of concordance knots, and minimize deep coalescences). Phylogenetic analyses of 16 specimens representing 4 putative species within S. “undulatus ” using complete (8 loci,> 5. 5 kb) and incomplete (29 loci,> 23. 6 kb) nuclear data sets result in species trees that share features with the mitochondrial DNA (mtDNA) genealogy at the phylogeographic level but provide new insights into the evolutionary history of the species group. The concatenated nuclear data and mtDNA data both recover 4 major clades connecting populations across North America; however, instances of discordance are localized at the contact zones between adjacent phylogeographic groups. A <b>random</b> <b>sub-sampling</b> experimen...|$|E
40|$|As a {{consequence}} of interpersonal conflicts, needs of the victimized are violated. These needs {{have to be addressed}} in order to achieve reconciliation. Due to the heterogeneity of need categories in scholarly research, we scrutinized which need categories can be empirically identified. 478 participants reported on an experienced interpersonal conflict. They responded to 109 items evaluating the perceived need violation for the conflict they reported on. By means of exploratory factor analysis with a <b>random</b> <b>sub-sample</b> (n 1 = 239), six need categories were extracted. These are the need for respect, the need for meaning, the need for acceptance, the need for pleasure, the need for self-efficacy, and the need for safety. Confirmatory factor analyses showed that these needs replicated in the second <b>random</b> <b>sub-sample</b> (n 2 = 239) as well as across sub-samples with people who had experienced an interpersonal conflict of lower severity of transgression (n A = 257) or higher severity of transgression (n B = 221). In addition, each of the need categories mediated the relationship between the severity of transgression and the desire for revenge. Yet, the results for the two need categories “pleasure ” and “safety ” have to be interpreted with caution {{due to a lack of}} scalar invariance. Among the other four need categories, respect was identified as the only independen...|$|R
40|$|BACKGROUND: Illicit {{drug misuse}} {{by people with}} mental {{disorder}} in the community {{has been associated with}} increased risk of violence but not tested among inpatients. AIMS: To evaluate screening for illicit drugs and determine the period prevalence of use in one high security hospital. METHOD: A records study six and 18 months after introduction of random ward urine testing was followed by a true random prospective study. A <b>random</b> <b>sub-sample</b> of these tests was checked in the laboratory. RESULTS: In the first period there were few tests and patchy recording; during the second, both improved, but testing was not random. In the third period there were 217 tests (65...|$|R
40|$|The use of self {{assessed}} {{health status}} {{as a measure of}} health is common in empirical research. We analyse a unique Australian survey in which a <b>random</b> <b>sub-sample</b> of respondents answer a standard self assessed health question twice – before and after an additional set of health related questions. 28 % of respondents change their reported health status. Response instability is related to age, income and occupation. We also compare the responses of these individuals to other respondents who are queried only once. The distributions of responses to both questions by the former group are statistically different from the distribution of responses by the latter group. Self assessed health status...|$|R
40|$|Let x = (X 1, 2,- [...] .,m, [...] .) be a Z-valued {{deterministic}} sequence {{such that}} Lm = m- 1 = 1 S, converge to Px weakly. Consider the following <b>random</b> <b>sub-sampling</b> scheme. Fix 6 E (0, 1), and m = m(n) such that n/m [...] , 3, generating the random variables Xl', [...] ., Xnm by sampling n values out of (xl, [...] ., xm) without replacement, i. e. X? = xj; for i = 1, [...] .,n where each choice of jl 7 j 2 ' [...] . # jn E { 1, [...] ., m} is equally likely (and {{independent of the}} sequence x). The next proposition shows that perhaps somewhat surprisingly (see Remark 1 immediately following its statement), the large deviations of the empirical measure of the resulting sample admits a rate function which is independent of the particular sequence x but different from the *Partially supported by NSF DMS 92 - 09712 grant and by a US-ISRAEL BSF grant...|$|E
40|$|Error-reduction {{sampling}} (ERS) {{is a high}} performing (but computationally expensive) query selection {{strategy for}} active learning. Subset optimisation has been proposed to reduce computational expense by applying ERS to only a subset of examples from the pool. This paper compares techniques used to construct the subset, namely <b>random</b> <b>sub-sampling</b> and pre-filtering. We focus on pre-filtering which populates the subset with more informative examples by filtering from the unlabelled pool using a query selection strategy. In this paper we establish whether pre-filtering outperforms sub-sampling optimisation, {{examine the effect of}} subset size, and propose a novel adaptive pre-filtering technique which dynamically switches between several al-ternative pre-filtering techniques using a multi-armed ban-dit algorithm. Empirical evaluations conducted on two benchmark text categorisation datasets demonstrate that pre-filtered ERS achieve higher levels of accuracy when compared to sub-sampled ERS. The proposed adaptive pre-filtering technique is also shown to be competitive with the optimal pre-filtering technique on the majority of problems and is never the worst technique. ...|$|E
40|$|In {{the vast}} amount of {{information}} in the internet, to give individual attention for each users, the personalised recommendation system is used, which uses the collaborative filtering method. By {{the result of the}} survey did with some papers, the main problems like the cold start and the sparsity which were found previously have been overcome. Filtering the users when the number is large is done by the nearest neighbour approach or by the filtration approach. Due to some popular objects the accuracy of the data's are lost. To remove this influence, the method which is proposed here is a network based collaborative filtering which will create a user similarity network, where the users having similar interests of item or movies will be grouped together forming a network. Then we calculate discriminant scores for candidate objects. Validate the proposed approach by performing <b>random</b> <b>sub-sampling</b> experiments for about 20 times to get the accurate results and evaluate the method using two accuracy criteria and two diversity measures. Results show that the approach outperforms the ordinary user-based collaborative filtering method by not only enhancing the accuracy but also improving the diversity...|$|E
40|$|Validity of {{self-reported}} {{hearing loss}} in adults: performance of three single questions Validade da perda auditiva auto-referida em adultos: desempenho de três perguntas únicas OBJECTIVE: To estimate {{the validity of}} three single questions used to assess self-reported hearing loss as compared to pure-tone audiometry in an adult population. METHODS: A validity study was performed with a <b>random</b> <b>sub-sample</b> of 188 subjects aged 30 to 65 years, drawn from the fourth wave of a population-based cohort study carried out in Salvador, Northeastern Brazil. Data were collected in household visits using questionnaires. Three questions were used to separately assess self-reported hearing loss: Q 1, “Do you feel you have...|$|R
40|$|Background This paper {{reports on}} the {{effectiveness}} of the prevention of dietary- and lifestyle-induced health effects in children and infants (IDEFICS) intervention on objectively measured physical activity (PA) and sedentary time (ST) in 2 - to 9. 9 -year-old European boys and girls. Methods The intervention was evaluated after 2 [*]years through a non-randomized cluster-controlled trial in eight European countries (one control and one intervention community per country). All children in the intervention group received a culturally adapted childhood obesity prevention programme through the community, schools/kindergartens and family. A <b>random</b> <b>sub-sample</b> of children participating in the IDEFICS study wore an accelerometer at baseline and follow-up for at least 3 [*]days (n[*]=[*] 9, 184). Of this sample, 81...|$|R
40|$|This {{paper offers}} a {{theoretical}} and empirical analysis on {{the sources of the}} narrowing gender gap in wages. Based on some existing literature, the model posits that the narrowing gender gap may be related to women 2 ̆ 7 s changing comparative advantage and the gender gap is smaller in occupations in which physical labor is less intensively used. The model implies that when an economy transforms from a manufacturing-oriented economy to service-oriented economy, a woman 2 ̆ 7 s productivity relative to a man 2 ̆ 7 s will generally increase so that the gender gap narrows. The implications of the theoretica 1 analysis are then tested based on one percent <b>random</b> <b>sub-samples</b> of two population censuses. The empirical results support the predictions of the model...|$|R
40|$|Species {{identification}} via DNA barcodes {{is contributing}} greatly to current bioinventory efforts. The initial, and widely accepted, proposal {{was to use}} the protein-coding cytochrome c oxidase subunit I (COI) region as the standard barcode for animals, but recently non-coding internal transcribed spacer (ITS) genes have been proposed as candidate barcodes for both animals and plants. However, achieving a robust alignment for non-coding regions can be problematic. Here we propose two new methods (DV-RBF and FJ-RBF) {{to address this issue}} for species assignment by both coding and non-coding sequences that take advantage of the power of machine learning and bioinformatics. We demonstrate the value of the new methods with four empirical datasets, two representing typical protein-coding COI barcode datasets (neotropical bats and marine fish) and two representing non-coding ITS barcodes (rust fungi and brown algae). Using two <b>random</b> <b>sub-sampling</b> approaches, we demonstrate that the new methods significantly outperformed existing Neighbor-joining (NJ) and Maximum likelihood (ML) methods for both coding and non-coding barcodes when there was complete species coverage in the reference dataset. The new methods also out-performed NJ and ML methods for non-coding sequences i...|$|E
30|$|While the CS {{framework}} {{requires a}} {{particular form of}} sampling (incoherent sampling), the related paradigm of low-rank matrix recovery (MC) assumes a random sampling of the matrix entries. Due to the intuitive sampling, the MC framework has been considered {{for a variety of}} signal recovery problems including collaborative spectrum sensing [19], sensor localization [20, 21], and image reconstruction problems [22, 23] among others. MC has been recently explored as a sampling scheme for WSNs [24 – 27]. In [24], the authors investigated the scenario where sensors lie on a uniform rectangular grid and <b>random</b> <b>sub-sampling</b> is taking place by each sensor. Our work bares some similarities with this line of work; however, we do not pose specific deployment constraints and we allow the sensors to occupy any location in the sensed region. Furthermore, our work differs significantly in the exploitation of prior knowledge {{in the form of a}} dictionary, which is utilized during the reconstruction stage. The utilization of the singular spectrum dictionary allows for the incorporation of prior knowledge regarding the data generation process which can significantly improve the reconstruction performance [25]. Furthermore, the proposed scheme is able to predict future measurements in addition to estimating missing past ones.|$|E
30|$|In GWAS, a low {{sample size}} {{relative}} {{to the number of}} markers and association tests can lead to inconsistency in the results. To assess the robustness of the detected associations, a sub-sampling procedure was conducted for the GBS data. The method is similar to that used by Tian et al. (2011) on maize except that our association analysis, like that of Lafarge et al. (2017), was based on a single marker model rather than a multiple marker model. A set of 80 % of the accessions (125 accessions for the indica panel and 133 for the japonica panel) was chosen at random without replacement and GWAS was conducted on this set. <b>Random</b> <b>sub-sampling</b> was repeated 100 times. The number of times that an association was detected at p <  1 e- 03 was counted to obtain a sub-sampling posterior probability for each marker. Based on the distribution of the results, a threshold that had a 95 % chance of not being overtaken was chosen. Sub-sampling was preferred to bootstrap because of the genetic structure of the panels and the simplicity in file preparation from not having several copies of a given accession. For the HDRA dataset, because of the smaller size of the panel and the much higher number of markers considerably slowing down the analyses, no robustness test was attempted.|$|E
40|$|The {{effect of}} new {{technology}} on relative demands for workers {{has been the subject}} of much research in economics. Krueger (1993) and others have studied the impact of computers on earnings in the US and elsewhere. Such studies have been criticised for ignoring the possibility of bias due to unobserved heterogeneity between computer users and non-users, resulting in computer users not being a <b>random</b> <b>sub-sample</b> of all workers. As well as looking at the effects of computers on earnings in the UK, this paper extends previous analyses by using a sample selection framework to deal with the bias problem. Results indicate not only that returns to computer use are positive but that it is important to correct for the sample selection bias. ...|$|R
40|$|The Great {{reed warbler}} (GRW) and the Seychelles warbler (SW) are {{congeners}} with markedly different demographic histories. The GRW {{is a normal}} outbred bird species while the SW population remains isolated and inbred after undergoing a severe population bottleneck. We examined variation at Major Histocompatibility Complex (MHC) class I exon 3 using restriction fragment length polymorphism, {{denaturing gradient gel electrophoresis}} and DNA sequencing. Although genetic variation was higher in the GRW, considerable variation has been maintained in the SW. The ten exon 3 sequences found in the SW were as diverged from each other as were a <b>random</b> <b>sub-sample</b> of the 67 sequences from the GRW. There was evidence for balancing selection in both species, and the phylogenetic analysis showing that the exon 3 sequences did not separate according to species, was consistent with transspecies evolution of the MHC...|$|R
30|$|NESS {{employed}} two {{strategies to}} evaluate the impact of SSLPs on children and their families. The first involved a wait-list control design, comparing 9 -and 36 -month old children (and their families) in SSLP areas with age mates growing up in reasonably similar families in communities destined to become SSLP areas. The second involved longitudinally following up a <b>random</b> <b>sub-sample</b> of the 9 -month olds in this cross-sectional study when 36 -months of age (and 5 years old) and comparing them with children growing up in disadvantaged households who were participating in another large study, the Millennium Cohort Study (MCS). Neither strategy was experimental in nature, because the government proved unwilling to randomly assign communities to the programme. This meant {{that not only was}} it necessary to implement multi-level modelling to evaluate SSLP effects, given that children were nested in communities, but that statistical controls needed to be implemented to discount effects of pre-existing differences between SSLP and comparison children, families and communities before testing SSLP effects.|$|R
40|$|Abstract—In human-computer {{interaction}} researches, {{one of the}} most interesting topics in the field of emotion recognition is to recognize human's feeling using bio-signals. According to previous researches, it is known that there is strong correlation between human emotion state and physiological reaction. Bio-signals takes noticed lately because those can be simply acquired with some sensors and are less sensitive in social and cultural difference. We have applied four algorithms, linear discriminant analysis, Naïve Bayes, decision tree and support vector machine to classify emotions, happiness, anger, surprise and stress based on bio-signals. In this study, audio-visual film clips were used to evoke each emotion and bio-signals (electrocardiograph, electrodermal activity, photoplethysmo-graph, and skin temperature) as emotional responses were measured and the features were extracted from them. For emotion recognition, the used algorithms are evaluated by only training, 10 -fold cross-validation and repeated <b>random</b> <b>sub-sampling</b> validation. We have obtained very low recognition accuracy from 28. 0 to 38. 4 % for testing. This means that it needs to apply various methodologies for the accuracy improvement of emotion recognition in the future analysis. Nevertherless, this can be helpful to provide the basis for the emotion recognition technique in human-machine interaction as well as contribute to the standardization in emotion-specific autonomic nervous system responses. Keywords-emotion classification; bio-signal; feature extraction; machine learning algorithm I...|$|E
40|$|Biometric-based identification/verification systems {{provide a}} {{solution}} to the security concerns in the modern world where machine is replacing human in every aspect of life. Fingerprints, because of their uniqueness, are the most widely used and highly accepted biometrics. Fingerprint biometric systems are either minutiae-based or pattern learning (image) based. The minutiae-based algorithm depends upon the local discontinuities in the ridge flow pattern and are used when template size is important while image-based matching algorithm uses both the micro and macro feature of a fingerprint and is used if fast response is required. In the present paper an image-based fingerprint verification system is discussed. The proposed method uses a learning phase, which is not present in conventional image-based systems. The learning phase uses pseudo <b>random</b> <b>sub-sampling,</b> which reduces the number of comparisons needed in the matching stage. This system has been developed using LabVIEW (Laboratory Virtual Instrument Engineering Workbench) toolbox version 6 i. The availability of datalog files in LabVIEW makes {{it one of the most}} promising candidates for its usage as a database. Datalog files can access and manipulate data and complex data structures quickly and easily. It makes writing and reading much faster. After extensive experimentation involving a large number of samples and different learning sizes, high accuracy with learning image size of 100 100 and a threshold value of 700 (1000 being the perfect match) has been achieved...|$|E
40|$|This paper {{presents}} an analysis conducted upon the sensor data {{gathered from the}} distribution control system of a central heating and cooling plant in Ottawa, Canada. After observing that the performance of four boilers and five chillers of this plant vary substantially in time under steady-state conditions, data-driven models were developed to explain this variability from the archived sensor data. By employing a forward stepwise regression and a repeated <b>random</b> <b>sub-sampling</b> cross-validation approach, two-layer feed-forward artificial neural network models with seven to fifteen hidden-nodes were selected for each boiler and chiller. The selected boiler models could explain 84 to 95 % of the variability in a boiler's efficiency, and the selected chiller models could explain 65 to 94 % of the variability in a chiller's coefficient of performance. Among studied nine variables, the most informative ones to predict a boiler's efficiency were identified as follows: flue gas O 2 concentration, pressure, part-load ratio, forced draft fan state, and return water flow rate. Unlike boilers, all four studied variables were found useful in predicting a chiller's coefficient of performance. These four variables were the return water flow rate, part-load ratio, outdoor temperature, and return water temperature. A residual analysis was conducted to verify {{the appropriateness of the}} selected models to the datasets. In addition, potential use cases for the selected models were discussed with illustrative examples. Peer reviewed: YesNRC publication: Ye...|$|E
40|$|ABSTRACT. Web {{surveys are}} {{frequently}} based on samples drawn from panels with {{large amounts of}} nonresponse or haphazard selection. The availability of large-scale consumer and voter databases provides large amounts of auxilliary information for both panelists and population members. Sample matching, where a conventional random sample is selected from a population frame and the clos-est matching respondent from the panel is selected for interviewing, is proposed. It is shown that under suitable assumptions (primarily ignorability of panel membership conditional upon the match-ing variables), the resulting survey estimates are consistent with an asymptotic normal distribution. Simulation {{results show that the}} matched sample estimators are superior to weighting a <b>random</b> <b>sub-sample</b> from the panel and have a similar sampling distribution to simple random sampling from the population. In an example involving the 2006 U. S. Congressional elections, estimates using sample matching from an opt-in Web panel outperformed estimates based on phone interviews with RDD samples. 1...|$|R
40|$|Distribution {{modelling}} {{of species}} {{is a topic}} {{at the core of}} ecology and conservation biology. Typically, distribution models at coarse scales are based on a set of environmental variables that are used for predicting a species 2 ̆ 7 presence/absence or abundance over geographic space. We tested the success of environmentally-based distribution models using abundance data from the North American Breeding Bird Survey. We judged model success by splitting the range of 190 bird species in east and west halves, using one for fitting the model and one for testing the predictions and vice versa. We showed that other designs for evaluating distribution models that either use the same data for fitting and evaluating the model or use a reserved evaluation data set that is a spatially <b>random</b> <b>sub-sample</b> of the total data set are overly optimistic. Accordingly, the success of environmental data-based distribution models has been overestimated in the current literature...|$|R
40|$|In this paper, we {{consider}} {{estimation of the}} causal effect of a treatment on an outcome from observational data collected in two phases. In the first phase, a simple random sample of individuals are drawn from a population. On these individuals, information is obtained on treatment, outcome, and a few low-dimensional confounders. These individuals are then stratified according to these factors. In the second phase, a <b>random</b> <b>sub-sample</b> of individuals are drawn from each stratum, with known, stratum-specific selection probabilities. On these individuals, a rich set of confounding factors are collected. In this setting, we introduce four estimators: (1) simple inverse weighted, (2) locally efficient, (3) doubly robust and (4) enriched inverse weighted. We evaluate the finite-sample performance of these estimators in a simulation study. We also use our methodology to estimate the causal effect of trauma care on in-hospital mortality {{using data from the}} National Study of Cost and Outcomes of Trauma...|$|R
