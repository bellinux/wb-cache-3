309|10000|Public
25|$|The {{elaborate}} system included automatic alarm circuitry, range-finding circuitry, and data-processing equipment; it {{was equipped}} to make 35mm photographic recordings of all signals received. A preliminary <b>reduction</b> <b>of</b> <b>data</b> was accomplished on-site, {{but the final}} processing {{was done in the}} Foreign Technology Division at Wright Patterson Air Force Base.|$|E
2500|$|M. Plešinger, The Total Least Squares Problem and <b>Reduction</b> <b>of</b> <b>Data</b> in AX ≈ B. Doctoral Thesis, TU of Liberec and Institute of Computer Science, AS CR Prague, 2008.|$|E
50|$|In computing, the <b>reduction</b> <b>of</b> <b>data</b> to {{any kind}} of {{canonical}} form is commonly called data normalization.|$|E
50|$|Sparse {{principal}} component analysis (sparse PCA) is a specialised technique used in statistical analysis and, in particular, in the analysis <b>of</b> multivariate <b>data</b> sets. It extends the classic method of {{principal component}} analysis (PCA) for the <b>reduction</b> <b>of</b> dimensionality <b>of</b> <b>data</b> by adding sparsity constraint on the input variables.|$|R
40|$|We {{propose the}} {{development}} of an expert system tool for the management and <b>reduction</b> <b>of</b> complex <b>data</b> sets. The proposed work is an extension of a successful prototype system for the calibration of CCD images developed by Dr. Johnston in 1987. The <b>reduction</b> <b>of</b> complex multi-parameter <b>data</b> sets presents severe challenges to a scientist. Not only must a particular data analysis system be mastered, (e. g. IRAF/SDAS/MIDAS), large amounts <b>of</b> <b>data</b> can require many days of tedious work and supervision by the scientist for even the most straightforward reductions. The proposed Expert Data Reduction Assistant will help the scientist overcome these obstacles by developing a reduction plan based on the data at hand and producing a script for the <b>reduction</b> <b>of</b> the <b>data</b> in a target common language...|$|R
40|$|Harnessing {{the power}} of DNA {{microarray}} technology requires the existence of analysis methods that accurately interpret microarray data. Current literature abounds with algorithms meant for the investigation <b>of</b> microarray <b>data.</b> However, there is need for an efficient approach that combines different techniques <b>of</b> microarray <b>data</b> analysis and provides a viable solution to dimensionality <b>reduction</b> <b>of</b> microarray <b>data.</b> Reducing the high dimensionality <b>of</b> microarray <b>data</b> is one approach in striving {{to better understand the}} information contained within the data. We propose a novel approach for dimensionality <b>reduction</b> <b>of</b> microarray <b>data</b> that effectively combines different techniques in the study of DNA microarrays. Our method, KAS (kernel alignment with semidefinite embedding), aids the visualization <b>of</b> microarray <b>data</b> in two dimensions and shows improvement over existing dimensionality reduction methods such as PCA, LLE and Isomap...|$|R
50|$|The {{elaborate}} system included automatic alarm circuitry, range-finding circuitry, and data-processing equipment; it {{was equipped}} to make 35 mm photographic recordings of all signals received. A preliminary <b>reduction</b> <b>of</b> <b>data</b> was accomplished on-site, {{but the final}} processing {{was done in the}} Foreign Technology Division at Wright Patterson Air Force Base.|$|E
50|$|AIPS++ {{provides}} {{facilities for}} calibration, editing, image formation, image enhancement, {{and analysis of}} images and other astronomical data. A major focus is on <b>reduction</b> <b>of</b> <b>data</b> from both single-dish and aperture synthesis radio telescopes. Although the tools provided in AIPS++ are mainly designed for processing data from varieties of radio telescopes, the package is expected to also be useful for processing other types of astronomical data and images. However, the reduction of most data from imaging array detectors is performed using IRAF instead.|$|E
50|$|The {{star was}} a test case for the HARPS-TERRA {{software}} for better <b>reduction</b> <b>of</b> <b>data</b> from the HARPS spectrometer in early 2012. Even with significantly lower margins of error on the data, less data was accessible than what was used in 2011. Still, the team reached a very similar conclusion to the previous team with {{a model of a}} planet and a trend. The residual velocities were still somewhat excessive, giving more weight to the existence of other bodies in the system, though still no conclusions could be made.|$|E
40|$|General {{results are}} {{presented}} based on nearly completed <b>reductions</b> <b>of</b> <b>data</b> for approximately 35 galaxies of all Hubble types. In general the visual and ultraviolet energy distributions are well correlated. The energy distribution in late-type galaxies appears to turn up sharply in the region around 2000 A, and this is tentatively interpreted as due {{to the presence of}} early-type stars whose energy distribution is modified by interstellar dust within these objects. Similar turnups may be present in several elliptical galaxies but the evidence at this time is not definitive...|$|R
40|$|The {{following}} topics were dealt with: Neutron {{stars and}} pulsars, the X-ray satellite ROSAT, <b>data</b> analysis techniques, <b>reduction</b> <b>of</b> ROSAT <b>data,</b> results <b>of</b> ROSAT observationsSIGLEAvailable from TIB Hannover: RA 234 :ET(260) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekDEGerman...|$|R
50|$|An example {{algorithm}} is the Monocle algorithm that carries out dimensionality <b>reduction</b> <b>of</b> the <b>data,</b> builds a minimal spanning tree using the transformed data, orders cells in pseudo-time {{by following the}} longest connected path of the tree and consequently labels cells by type.|$|R
50|$|GPS {{or other}} {{satellite}} based systems (GLONASS, GALILEO, BEIDOU, QZSS) provide {{a way to}} learn one's location, but these methods require free field conditions {{in order to receive}} the radio signal. Various satellite systems are subject to switching-off or <b>reduction</b> <b>of</b> <b>data</b> precision by the company or government that runs them. They are also prone to intentional or unintentional disturbances. Even passing through a tunnel or a garage interrupts the data flow. In situations where the signal cannot be received reliably, alternate sources of location data are needed. Combining GPS with other methods can avoid thsese limitations, but each method has its own specific limitations. A hybrid system provides fault tolerance for each underlying method and improves the overall precision of the result.|$|E
50|$|The alpha-scattering surface {{analyzer}} {{was designed to}} measure directly the abundances of the major elements of the lunar surface. The instrumentation consisted of six alpha sources (curium 242) collimated to irradiate a 100 mm diameter opening {{in the bottom of}} the instrument where the sample was located and two parallel but independent charged particle detector systems. One system, containing two sensors, detected the energy spectra of the alpha particles scattered from the lunar surface, and the other, containing four sensors, detected energy spectra of the protons produced via reactions (alpha and proton) in the surface material. Each detector assembly was connected to a pulse height analyzer. A digital electronics package, located in a compartment on the spacecraft, continuously telemetered signals to earth whenever the experiment was operating. The spectra contained quantitative information on all major elements in the samples except for hydrogen, helium, and lithium. The experiment provided 83 hours of high quality data during the first lunar day. During the second lunar day, 22 hours of data were accumulated. However, detector noise posed a problem in the <b>reduction</b> <b>of</b> <b>data</b> from this second day.|$|E
5000|$|Coding can {{be thought}} of as a means of <b>reduction</b> <b>of</b> <b>data</b> or data simplification. Using simple but broad {{analytic}} codes it is possible to reduce the data to a more manageable feat. In this stage of data analysis the analyst must focus on the identification of a more simple way of organizing data. using data reductionism researchers should include a process of indexing the data texts which could include: field notes, interview transcripts, or other documents. Data at this stage are reduced to classes or categories in which the researcher is able to identify segments of the data that share a common category or code. [...] Siedel and Kelle (1995) suggest three ways to aid with the process of data reduction and coding: (a) noticing relevant phenomena, (b) collecting examples of the phenomena, and (c) analyzing phenomena to find similarities, differences, patterns and overlying structures. This aspect of data collection is important because during this stage researchers should be attaching codes to the data to allow the researcher to think about the data in different ways. [...] Coding can not be viewed as strictly data reduction, data complication can be used as a way to open up the data to examine further. The below section addresses the process of data complication and its significance to data analysis in qualitative analysis.|$|E
40|$|Widths and {{spacings}} of conductor {{lines on}} integrated circuits measured electrically by split cross-bridge resistor technique. Speeds evaluation of integrated-circuit fabrication processes. When measurement of probe currents and voltages and <b>reduction</b> <b>of</b> measurement <b>data</b> automated, test structure probed and characterized {{in less than}} one second...|$|R
30|$|Additional {{benefit of}} this {{optimization}} is the <b>reduction</b> <b>of</b> false <b>data</b> dependencies created by register allocation when several program variables reuse the same register {{to store the}} data, effectively allowing the scheduler more scheduling freedom and increases available ILP to explore during instruction scheduling.|$|R
40|$|The {{recently}} released Reflex scientific workflow environment supports the interactive execution <b>of</b> ESO VLT <b>data</b> <b>reduction</b> pipelines. Reflex {{is based upon}} the Kepler workflow engine, and provides components for organising the data, executing pipeline recipes based on the ESO Common Pipeline Library, invoking Python scripts, and constructing interaction loops. Reflex will greatly enhance the quick validation and <b>reduction</b> <b>of</b> the scientific <b>data.</b> In this paper we summarize the main features of Reflex, and demonstrate as an example its application to the <b>reduction</b> <b>of</b> echelle UVES <b>data.</b> Comment: ADASS 2010, Submitted ASP Conference Serie...|$|R
30|$|When traffic {{congestion}} occurs, the above methods {{lead to the}} <b>reduction</b> <b>of</b> <b>data</b> sensing rates in the terminal sensor nodes. There have been some attempts to explore mechanisms for congestion avoidance in WSNs {{but not for the}} schemes based on <b>reduction</b> <b>of</b> <b>data</b> sensing rates.|$|E
40|$|Computer <b>reduction</b> <b>of</b> <b>data</b> {{acquired}} during {{cardiac catheterization}} eliminates considerable {{pencil and paper}} computation, a significantime-saver for the physician. Simultaneously, it guarantees highly sophisticated measurements while shortening a normally lengthy procedure, a /ess disquieting experience tor the patient...|$|E
30|$|Since {{wireless}} sensor networks work on {{a limited}} network bandwidth, traffic congestion frequently occurs during the relay process of sensing data. To solve this problem, several studies have been proposed based on the <b>reduction</b> <b>of</b> <b>data</b> sensing rates in terminal sensor nodes [9, 10].|$|E
30|$|When using {{a priori}} {{information}} of a PBPK model structure combined with Bayesian information about PBPK model parameter distribution, the administered activity could be determined with acceptable accuracy using only two time points (4  h, 2  d) and thus allow a considerable <b>reduction</b> <b>of</b> needed <b>data</b> for individual dosimetry.|$|R
40|$|We discuss {{here the}} meaning <b>of</b> {{discrepant}} <b>data</b> sets from an information point <b>of</b> view. <b>Reduction</b> <b>of</b> these <b>data</b> sets {{can be achieved}} by a bootstrap method, obtaining parameters that behave like the usual mean and standard deviation. The bootstrap method is also useful to define a safe bound for the standard deviation calculated by practical methods. I...|$|R
40|$|The {{uncertainty}} in relay satellite sate {{is a significant}} error source which cannot be ignored in the <b>reduction</b> <b>of</b> satellite-to-satellite tracking <b>data.</b> Based on simulations and real data reductions, it is numerically impractical to use simultaneous unconstrained solutions to determine both relay and user satellite epoch states. A Bayesian or least squares estimation technique with an a priori procedure is presented which permits the adjustment of relay satellite epoch state in the <b>reduction</b> <b>of</b> satellite-to-satellite tracking <b>data</b> without the numerical difficulties introduced by an ill-conditioned normal matrix...|$|R
30|$|A {{number of}} indoor {{experiments}} were achieved with the PDPT framework using the PDPT Client application. The main {{result of the}} use of the PDPT framework is a <b>reduction</b> <b>of</b> <b>data</b> transfer speed. The tests focused on the real use of the developed PDPT Framework and its main impact on increased data transfer.|$|E
40|$|To detect {{possible}} superficial deformations in {{the volcanic}} Caldera of Teide area (Canary Islands), {{we have set}} up a network composed by 17 points which we shall observe successively. In this paper, we present the analysis and <b>reduction</b> <b>of</b> <b>data</b> and the free tridimensional adjustment of the network for the first observation campaign. ...|$|E
40|$|Work has {{continued}} throughout the reporting period on <b>reduction</b> <b>of</b> <b>data</b> from the Fall 1976 {{flight of the}} MSFC-UAH Cosmic Ray Experiment. During this period a major effort was undertaken to improve and rebuild the experiment. This was accomplished and a successful flight was completed in September 1978 from Pierre, South Dakota with the new instrument...|$|E
40|$|A {{method is}} {{suggested}} {{for measuring the}} radioactivity of fall-out particles in atomic clouds {{as a function of}} their size. This method consists of drawing the particles through a bank of filters in series and measuring the radioactivity retained on each filter. <b>Reduction</b> <b>of</b> these <b>data</b> to desired distribution function is indicated by a matrix equation. Ntis: ad 079501. Accession No. : 59465. Misc. No. : USNRDL-TR- 61. " 19 August 1955. "Includes bibliographical references. A method is suggested for measuring the radioactivity of fall-out particles in atomic clouds as a function of their size. This method consists of drawing the particles through a bank of filters in series and measuring the radioactivity retained on each filter. <b>Reduction</b> <b>of</b> these <b>data</b> to desired distribution function is indicated by a matrix equation. Mode of access: Internet...|$|R
40|$|The {{attitude}} reconstitution (AR) {{procedure that}} will be used for the <b>reduction</b> <b>of</b> Hipparcos <b>data</b> is examined. The principles underlying the Hipparcos mission are briefly reviewed, and the AR specifications are explained. The sufficiency of the Star Mapper data to attain target accuracy is demonstrated. Encouraging results obtained from simulated experiments are reporte...|$|R
40|$|The HLA Interface Specification defines {{services}} for Data Distribution Management (DDM) and Ownership Management (OM). In addition to Declaration Management, DDM is introduced to provide value based filtering on the attribute values to be communicated. The main objectives <b>of</b> DDM are <b>reduction</b> <b>of</b> the <b>data</b> {{to be processed}} by the receiving federate (scalability) and <b>reduction</b> <b>of</b> the <b>data</b> actually sent over the network (performance). However, applying DDM leads {{to an increase in}} computational effort by the RTI, especially when many regions need to be updated regularly. To determine the conditions, under which DDM is of benefit, a predator-prey federation has been designed and implemented. The effects of applying several strategies using DDM and OM were analyzed and experiments were conducted using the available DMSO RTI. The usage of DDM and OM is evaluated and guidelines for future use of these services are presented...|$|R
40|$|We {{report an}} {{experimental}} ptychography measurement performed in fly-scan mode. With a visible-light laser source, we demonstrate a 5 -fold <b>reduction</b> <b>of</b> <b>data</b> acquisition time. By including multiple mutually incoherent modes into the incident illumination, high quality images were successfully reconstructed from blurry diffraction patterns. This approach significantly increases the throughput of ptychography, especially for three-dimensional applications and the visualization of dynamic systems...|$|E
30|$|On {{the other}} hand, there are {{algorithms}} for reducing a graph (Liu et al. 2010; Lu and Liu 2007; Sadiq and Orlowska 2000). With {{the application of}} any algorithm on the reduced graph, obviously, a lower response time is achieved. However, in this case, <b>reduction</b> <b>of</b> <b>data</b> brings loss of information. Thus, obtaining a path that is the optimal in the original graph can not be guaranteed.|$|E
40|$|Clustering {{seeks to}} group or to lump {{together}} objects or variables that share some observed qualities or, alternatively, to partition or to divide a set of objects or variables into mutually exclusive classes whose boundaries reflect differences in the observed qualities of their members. Clustering thus extracts typologies from data which in turn represent a <b>reduction</b> <b>of</b> <b>data</b> complexity and may lead to conceptual simplifications...|$|E
40|$|We {{explore the}} use of {{principal}} differential analysis {{as a tool for}} performing dimensional <b>reduction</b> <b>of</b> functional <b>data</b> sets. In particular, we compare the results provided by principal differential analysis and by functional principal component analysis in the dimensional <b>reduction</b> <b>of</b> three synthetic <b>data</b> sets, and <b>of</b> a real <b>data</b> set concerning 65 three-dimensional cerebral geometries, the AneuRisk 65 data set. The analyses show that principal differential analysis can provide an alternative and effective representation <b>of</b> functional <b>data,</b> easily interpretable in terms of exponential, sinusoidal, or damped-sinusoidal functions and providing a different insight to the functional data set under investigation. Moreover, in the analysis of the AneuRisk 65 data set, principal differential analysis is able to detect interesting features <b>of</b> the <b>data,</b> such as the rippling effect of the vessel surface, that functional principal component analysis is not able to detect...|$|R
30|$|The PGMDS {{approach}} {{operates on}} an isolated, context-free level: The {{information about the}} particular word sequence is lost by the <b>reduction</b> <b>of</b> the <b>data</b> to a DTM. In this section we analyze the words in context: as word sequences used in the self-reports. This task is accomplished by adapting approaches from social network analysis (SNA).|$|R
40|$|Abstract. It {{is argued}} that the {{relativistic}} de nitions of parallax, proper motion and radial velocity consistent with an accuracy of 1 as should be considered only within a well-de ned algorithm <b>of</b> relativistic <b>reduction</b> <b>of</b> observational <b>data.</b> Such an algorithm is formulated and the corresponding de nitions of astrometric parameters are discussed. 1...|$|R
