26|11|Public
40|$|International audienceBACKGROUND: Analyzing time-to-onset {{of adverse}} drug {{reactions}} from treatment exposure contributes to meeting pharmacovigilance objectives, i. e. identification and prevention. Post-marketing {{data are available}} from reporting systems. Times-to-onset from such databases are right-truncated because some patients {{who were exposed to}} the drug and who will eventually develop the adverse drug reaction may do it after the time of analysis and thus are not included in the data. Acknowledgment of the developments adapted to right-truncated data is not widespread and these methods have never been used in pharmacovigilance. We assess the use of appropriate methods as well as the consequences of not taking <b>right</b> <b>truncation</b> into account (naïve approach) on parametric maximum likelihood estimation of time-to-onset distribution. METHODS: Both approaches, naïve or taking <b>right</b> <b>truncation</b> into account, were compared with a simulation study. We used twelve scenarios for the exponential distribution and twenty-four for the Weibull and log-logistic distributions. These scenarios are defined by a set of parameters: the parameters of the time-to-onset distribution, the probability of this distribution falling within an observable values interval and the sample size. An application to reported lymphoma after anti TNF-¿ treatment from the French pharmacovigilance is presented. RESULTS: The simulation study shows that the bias and the mean squared error might in some instances be unacceptably large when <b>right</b> <b>truncation</b> is not considered while the truncation-based estimator shows always better and often satisfactory performances and the gap may be large. For the real dataset, the estimated expected time-to-onset leads to a minimum difference of 58 weeks between both approaches, which is not negligible. This difference is obtained for the Weibull model, under which the estimated probability of this distribution falling within an observable values interval is not far from 1. CONCLUSIONS: It is necessary to take <b>right</b> <b>truncation</b> into account for estimating time-to-onset of adverse drug reactions from spontaneous reporting databases...|$|E
40|$|Description A {{simple way}} of fitting {{detection}} functions to distance sampling data for both line and point transects. Adjustment term selection, left and <b>right</b> <b>truncation</b> {{as well as}} monotonicity constraints and binning are supported. Abundance and density estimates can also be calculated (via a Horvitz-Thompson-like estimator) if survey area information is provided. Version 0. 9. ...|$|E
40|$|Incompleteness is a {{major feature}} of time-to-event data. As one type of incompleteness, {{truncation}} refers to the unobservability of the time-to-event variable because it is smaller (or greater) than the truncation variable. A truncated sample always involves left and <b>right</b> <b>truncation.</b> Left truncation has been studied extensively while <b>right</b> <b>truncation</b> has not received {{the same level of}} attention. In one of the earliest studies on <b>right</b> <b>truncation,</b> Lagakos et al. (1988) proposed to transform a right truncated variable to a left truncated variable and then apply existing methods to the transformed variable. The reverse-time hazard function is introduced through transformation. However, this quantity does not have a natural interpretation. There exist gaps in the inferences for the regular forward-time hazard function with right truncated data. This dissertation discusses variance estimation of the cumulative hazard estimator, one-sample log-rank test, and comparison of hazard rate functions among finite independent samples under the context of <b>right</b> <b>truncation.</b> First, the relation between the reverse- and forward-time cumulative hazard functions is clarified. This relation leads to the nonparametric inference for the cumulative hazard function. Jiang (2010) recently conducted a research on this direction and proposed two variance estimators of the cumulative hazard estimator. Some revision to the variance estimators is suggested in this dissertation and evaluated in a Monte-Carlo study. Second, this dissertation studies the hypothesis testing for right truncated data. A series of tests is developed with the hazard rate function as the target quantity. A one-sample log-rank test is first discussed, followed by a family of weighted tests for comparison between finite $K$-samples. Particular weight functions lead to log-rank, Gehan, Tarone-Ware tests and these three tests are evaluated in a Monte-Carlo study. Finally, this dissertation studies the nonparametric inference for the hazard rate function for the right truncated data. The kernel smoothing technique is utilized in estimating the hazard rate function. A Monte-Carlo study investigates the uniform kernel smoothed estimator and its variance estimator. The uniform, Epanechnikov and biweight kernel estimators are implemented in the example of blood transfusion infected AIDS data...|$|E
3000|$|... -based CI depends {{dramatically}} on the <b>right</b> side <b>truncation</b> of the prior, {{which in}} this manuscript has been fixed at 10. Figure  5 (b) makes it really clear that a larger truncation point would have a huge impact in the resulting π [...]...|$|R
40|$|In this paper, {{two types}} of kernel based estimators of hazard rate under left <b>truncation</b> and <b>right</b> {{censorship}} are considered. An asymptotic representation of the integrated squared error for both estimators is obtained. Also it is shown that the bandwidth selected by the data-based {{method of least squares}} cross-validation is asymptotically optimal in a compelling sense. Left <b>truncation</b> <b>right</b> censorship Hazard rate estimation Asymptotic representation Cross-validation Optimal bandwidth...|$|R
40|$|Doubly {{truncated}} data {{appear in}} a number of applications, including astronomy and survival analysis. In this paper we review the existing methods to compute the NPMLE under double truncation, which has no explicit form and must be approximated numerically. We introduce the bootstrap as a method to estimate the finite sample distribution of the NPMLE under double truncation. The performance of the bootstrap is investigated in a simulation study. The nonstandard case in which the <b>right</b> and left <b>truncation</b> times determine each other is covered. As an illustration, nonparametric estimation and inference on the birth process and the age at diagnosis for childhood cancer in North Portugal is considered...|$|R
40|$|We show both {{analytically}} {{and through}} Monte Carlo simulations that applying standard hazard models to right-truncated data, i. e., data {{from which all}} right-censored observations are omitted, induces spurious positive duration dependence and hence can trick researchers into believing to have found evidence of social contagion when there is none. Truncation also tends to deflate the effect of time-invariant covariates. These results imply that not accounting for <b>right</b> <b>truncation</b> can lead managers to rely too much on word of mouth in generating new product adoption and to poorly identify the customers most likely to adopt early. Not accounting for <b>right</b> <b>truncation</b> {{can also lead to}} suboptimal pricing decisions and to erroneous assessments of variations in customer lifetime value. We assess the effectiveness of four possible solutions to the problem and find that only using an analytically corrected likelihood function protects one against truncation artifacts inflating coefficients of contagion and attenuating coefficients of time-invariant covariates. hazard models, duration dependence, new product diffusion, social contagion...|$|E
40|$|Recently {{some authors}} have drawn {{attention}} to the fact that there might be practical problems with the use of unbounded Pareto dis-tributions, for instance when there are natural upper bounds that truncate the probability tail. Aban, Meerschaert and Panorska (2006) derived the maximum likelihood estimator for the tail index of a trun-cated Pareto distribution with <b>right</b> <b>truncation</b> point T. The Hill (1975) estimator is then obtained from this maximum likelihood esti-mator letting T →∞. The problem of extreme value estimation under (<b>right)</b> <b>truncation</b> was also introduced in Nuyts (2010) who proposed a similar estimator for the tail index and considered trimming of the number of extreme order statistics. Given that in practice one does not always know if the distribution is truncated or not, we propose estimators for extreme quantiles and T that are consistent both under truncated and non-truncated Pareto-type distributions. In this way we extend the classical extreme value methodology adding the trun-cated Pareto-type model with truncation point T → ∞ as the sample size n→∞. Finally we present some practical examples, asymptotics and simulation results...|$|E
40|$|Anticipation, i. e. a {{decreasing}} age-at-onset {{in subsequent}} generations {{has been observed}} {{in a number of}} genetically triggered diseases. The impact of anticipation is generally studied in affected parent-child pairs. These analyses are restricted to pairs in which both individuals have been affected and are sensitive to <b>right</b> <b>truncation</b> of the data. We propose a normal random effects model that allows for right-censored observations and includes covariates, and draw statistical inference based on the likelihood function. ...|$|E
40|$|Modified {{versions}} of the life time distribution are often used in survival analysis. The modifications depend on how we choose individuals for the study and on the assumptions {{on the behavior of}} the population. A rigorous point process description of the Lexis diagram is used to make the sampling mechanisms and the preconditions transparent. The point process description gives a framework to handle all possible sampling patterns. The setup is generalized so it can handle more complicated life descriptions than just lifetimes, and the disability model is used as an example. Two setups can be used. Conditional on the birthtimes, the modications of the life time distribution are left <b>truncation</b> and either <b>right</b> censoring or <b>truncation.</b> Assuming that the birthtimes can be described by a Poisson process the modications are length bias and the recurrence time distribution known from renewal theory...|$|R
40|$|Abstract The hazard {{function}} is {{the base of}} the inferential process in survival analysis and, although relevant for describing the pattern of failures, is rarely shown in research papers, given the difficulties in nonparametric estimation. We developed the bshazard package, that facilitates the computation of a nonparametric estimate of the hazard function, with data-driven smoothing. The method accounts for left <b>truncation,</b> <b>right</b> censoring and possible covariates. B-splines are used to estimate the shape of the hazard within the generalized linear mixed models framework. Smoothness is controlled by imposing autoregressive structure on the baseline hazard coefficients. This perspective allows an ’automatic’ smoothing by avoiding the need to choose the smoothing parameter, which can be estimated from the data as a dispersion parameter. A simulation study demonstrates the capability of our software and an application to estimate the hazard of Non-Hodgkin’s lymphoma in Swedish population data shows its potential...|$|R
40|$|Interpreting the {{coefficient}} matrix {{resulting from the}} correlation between turbulent flow quantities obtained in jet-flows and far-field pressure fluctuations can be ambiguous in regions with coherent structures dominating the flow-topology [8, 34]. The aim {{of the present study}} is to contribute to the understanding of results from this so-called causality correlation approach. The sound radiated into the far-field is calculated using a free-space Green's function for correlated monopoles convecting with a constant speed along a jet axes. The amplitude of the modeled source is damped by an envelope simulating a strong activity {{at the end of the}} potential core region of the jet at x=D ¼ 5 (see Fig. 25, left). The fluctuating part of the resulting pressure-time signal is dominated by truncation effects (see Fig. 25, <b>right).</b> This <b>truncation</b> effect can be avoided by using a numerical model based on jittering wave-packets with varying amplitude and spatial extents [10] (see Fig. 26, left). Here the resulting pressure signal is clearly dominated by the intermittency of the source signal,showing high amplitude burst (see Fig. 26, right). In case of the jittering wave-packets model the correlation between the source signal and the far-field is maximal for time delays, corresponding to the sound travel time from the source to the observer (see Fig. 27). The results verify the approach documented in the literature of analyzing the spatial and temporal evolution of {{the coefficient}} matrix in order to identify flow regions which are subject to the same physical phenomenon as the aeroacoustic source [33]...|$|R
40|$|Recently {{attention}} has been drawn to practical problems {{with the use of}} unbounded Pareto distributions, for instance when there are natural upper bounds that truncate the probability tail. Aban, Meerschaert and Panorska (2006) derived the maximum likelihood estimator for the Pareto tail index of a truncated Pareto distribution with a <b>right</b> <b>truncation</b> point T. The Hill (1975) estimator is then obtained by letting T →∞. The problem of extreme value estimation under <b>right</b> <b>truncation</b> was also introduced in Nuyts (2010) who proposed a similar estimator for the tail index and considered trimming of the number of extreme order statistics. Given that in practice one does not always know whether the distribution is truncated or not, we discuss estimators for the Pareto index and extreme quantiles both under truncated and non-truncated Pareto-type distributions. We also propose a truncated Pareto QQ-plot in order to help deciding between a truncated and a non-truncated case. In this way we extend the classical extreme value methodology adding the truncated Pareto-type model with truncation point T →∞ as the sample size n →∞. Finally we present some practical examples, asymptotics and simulation results. Comment: Corrected typos. Revised arguments in Theorem 2 (b...|$|E
40|$|The {{study of}} age at onset {{anticipation}} and parent-of-origin effects on age at onset in Lynch Syndrome (LS) {{are of interest}} to both clinical medicine and research. Although several studies have suggested the presence of age at onset anticipation and parent-of-origin effects on age at onset of LS, the question remains as to whether this evidence reflects ascertainment bias rather than the phenomenon under study. The aim of this thesis is to assess decrease in age at diagnosis of LS over successive generations as well as parent-of-origin effects on age at diagnosis of LS based on the data provided by the Colon Cancer Family Registry. We first demonstrate that the variable age at diagnosis in the sample is right truncated by the closing date of the study and, as a result, the variable age at diagnosis is a biased sample of the target populations. To assess decrease in age at diagnosis of the disease over successive generations, we use the symmetry test proposed by Tsai et al. (2005) which accounts for the bias caused by the <b>right</b> <b>truncation</b> of both the parent's and child's ages at diagnosis. To test parent-of-origin effect, we examine and improve the method used by Lindor et al. (2010). Based on our preliminary analysis, {{we did not find}} sufficient statistical evidence from this sample to claim that there exists a parent-of-origin effect on age at diagnosis of LS relating to either the gender of the parent or the gender of the offspring after accounting for the sampling bias. The results given by the symmetry test suggest that there exists a decrease in age at diagnosis of LS over successive generations. This result should be free of the sampling bias caused by the <b>right</b> <b>truncation.</b> What remains uncertain is whether true genetic anticipation contributes to the decrease in age at diagnosis over successive generations observed in this disease...|$|E
40|$|A Poisson model {{typically}} {{is assumed}} for count data, but {{when there are}} so many zeroes in the response variable, because of overdispersion, a negative binomial regression is suggested as a count regression instead of Poisson regression. In this paper, a zero-inflated negative binomial regression model with <b>right</b> <b>truncation</b> count data was developed. In this model, we considered a response variable and one or more than one explanatory variables. The estimation of regression parameters using the maximum likelihood method was discussed and the goodness-of-fit for the regression model was examined. We studied the effects of truncation in terms of parameters estimation, their standard errors and the goodness-of-fit statistics via real data. The results showed a better fit by using a truncated zero-inflated negative binomial regression model when the response variable has many zeros and it was right truncated...|$|E
40|$|This thesis {{deals with}} Swedish full text {{retrieval}} {{and the problem}} of morphological variation of query terms in thedocument database. The study is an information retrieval experiment with a test collection. While no Swedish testcollection was available, such a collection was constructed. It consists of a document database containing 161, 336 news articles, and 52 topics with four-graded (0, 1, 2, 3) relevance assessments. The effects of indexing strategy-query term combination on retrieval effectiveness were studied. Three of five testedmethods involved indexing strategies that used conflation, in the form of normalization. Further, two of these threecombinations used indexing strategies that employed compound splitting. Normalization and compound splittingwere performed by SWETWOL, a morphological analyzer for the Swedish language. A fourth combinationattempted to group related terms by <b>right</b> hand <b>truncation</b> of query terms. A search expert performed the truncation. The four combinations were compared {{to each other and to}} a baseline combination, where no attempt was made tocounteract the problem of morphological variation of query terms in the document database. Two situations were examined in the evaluation: the binary relevance situation and the multiple degree relevancesituation. With regard to the binary relevance situation, where the three (positive) relevance degrees (1, 2, 3) weremerged into one, and where precision was used as evaluation measure, the four alternative combinationsoutperformed the baseline. The best performing combination was the combination that used truncation. Thiscombination performed better than or equal to a median precision value for 41 of the 52 topics. One reason for therelatively good performance of the truncation combination was the capacity of its queries to retrieve different partsof speech. In the multiple degree relevance situation, where the three (positive) relevance degrees were retained, retrievaleffectiveness was taken to be the accumulated gain the user receives by examining the retrieval result up to givenpositions. The evaluation measure used was nDCG (normalized cumulated gain with discount). This measurecredits retrieval methods that (1) rank highly relevant documents higher than less relevant ones, and (2) rankrelevant (of any degree) documents high. With respect to (2), nDCG involves a discount component: a discount withregard to the relevance score of a relevant (of any degree) document is performed, and this discount is greater andgreater, the higher position the document has in the ranked list of retrieved documents. In the multiple degree relevance situation, the five combinations were evaluated under four different user scenarios,where each scenario simulated a certain user type. Again, the four alternative combinations outperformed thebaseline, for each user scenario. The truncation combination had the best performance under each user scenario. This outcome agreed with the performance result in the binary relevance situation. However, there were alsodifferences between the two relevance situations. For 25 percent of the topics and with regard to one of the four userscenarios, the set of best performing combinations in the binary relevance situation was disjunct from the set of bestperforming combinations in the multiple degree relevance situation. The user scenario in question was such thatalmost all importance was placed on highly relevant documents, and the discount was sharp. The main conclusion of the thesis is that normalization and <b>right</b> hand <b>truncation</b> (performed by a search expert) enhanced retrieval effectiveness in comparison to the baseline, irrespective of which of the two relevance situationswe consider. Further, the three indexing strategy-query term combinations based on normalization were almost asgood as the combination that involves truncation. This holds for both relevance situations. QC 20150813 </p...|$|R
40|$|We {{present a}} {{framework}} for distributional reaction time (RT) analysis, based on maximum likelihood (ML) estimation. Given certain information relating to chosen distribution functions, one can estimate the parameters of these distributions and of finite mixtures of these distributions. In addition, left and/or <b>right</b> censoring or <b>truncation</b> may be imposed. Censoring and truncation are useful methods by which to accommodate outlying observations, which are a pervasive problem in RT research. We consider five RT distributions: the Weibull, the ex-Gaussian, the gamma, the log-normal, and the Wald. We employ quasi-Newton optimization to obtain ML estimates. Multicase distributional analyses can be carried out, which enable one to conduct detailed (across or within subjects) comparisons of RT data by means of loglikelihood difference tests. Parameters may be freely estimated, estimated subject to boundary constraints, constrained to be equal (within or over cases), or fixed. To demonstrate the feasibility of ML estimation and to illustrate some of the possibilities offered by the present approach, we present three small simulation studies. In addition, we present three illustrative analyses of real data...|$|R
40|$|The {{exceptionally}} persistent {{activity of}} Stromboli volcano has lasted {{for at least}} 1400 years. The normal strombolian activity is periodically interrupted by more energetic explosions (1 – 2 per year) and by sporadic effusive episodes (every 10 – 20 years). Normal activity and effusive episodes are characterized by crystal-rich high-K to shoshonitic basalts issuing from a volatile-poor shallow system. Crystal-poor pumice are emitted only during more violent explosions, and are thought to derive from deep pulses of volatile-rich magma. Shallow level degassing induces massive crystallization of deep pulses of feeding magma that, continuously mixing with the resident one, produces the crystal-rich shoshonite of the persistent activity. We examined the crystallization history of the crystal-rich, shallow reservoir using plagioclase Crystal Size Distribution (CSD) analysis of scoriae and lavas emitted in the past twenty years. CSDs show a linear dependence from crystal size in the size interval 0. 06 – 1. 2 mm; number density of larger crystals is biased by <b>right</b> hand <b>truncation</b> effects. CSDs slopes and intercepts are quite constant during the whole considered time span revealing {{a system that is}} close to the equilibrium also from a kinetic point of view. The linear crystal size distribution are reached by the system through episodes of growth and resorption, respectively occurring in the degassed and undegassed magma during the continuous mixing in the feeding system. Plagioclase net growth rate (2 * 10 − 11 cm/s) results from a balance of growth (10 − 10 cm/s) and resorption episodes which induce spectacular zoning and resorption textures in crystals larger than 200 μm. CSDs of mafic phases cannot be accurately acquired on each single sample due to poor counting statistics; the evaluation of pyroxene and olivine CSD on the whole data set, however, confirms the conclusions acquired from plagioclase CSDs...|$|R
40|$|A Poisson {{regression}} model is well-known for modeling the data with response variable {{in form of}} counts. However, one often encounters the situation with excess zeros occurred in the observed responses. Therefore, Poisson model is not suitable any more {{for this kind of}} data. Thus, we propose to use a hurdle negative binomial model. Furthermore, the response variable in such cases is truncated for some values. So, a truncated hurdle negative binomial model is introduced on count data with many zeros. In this model, we consider a response variable and one or more than one explanatory variables. The estimates using the maximum likelihood method are discussed and the goodness-of-fit for the {{regression model}} is examined. We study the effects of <b>right</b> <b>truncation</b> in terms of parameters estimation and their standard errors via an example and a simulation...|$|E
40|$|Modied {{versions}} of the life time distribution are often used in survival analysis. The modifications depend on how we choose individuals for the study and on the assumptions {{on the behavior of}} the population. A rigorous point process description of the Lexis diagram is used to make the sampling mechanisms and the preconditions transparent. The point process description gives a framework to handle all possible sampling patterns. The setup is generalized so it can handle more complicated life descriptions than just lifetimes, and the disability model is used as an example. Two setups can be used. Conditional on the birthtimes, the life time distribution is left truncated and subject to either right censoring or <b>right</b> <b>truncation.</b> Assuming that the birthtimes can be described by a Poisson process the modications are length bias and the recurrence time distribution known from renewal theory...|$|E
40|$|Thesis (M. Sc.) [...] Memorial University of Newfoundland, 2011. Mathematics and StatisticsBibliography: leaves 92 - 94. The {{study of}} age at onset {{anticipation}} and parent-of-origin effects on age at onset in Lynch Syndrome (LS) {{are of interest}} to both clinical medicine and research. Although several studies have suggested the presence of age at onset anticipation and parent-of-origin effects on age at onset of LS, the question remains as to whether this evidence reflects ascertainment bias rather than the phenomenon under study. The aim of this thesis is to assess decrease in age at diagnosis of LS over successive generations as well as parent-of-origin effects on age at diagnosis of LS based on the data provided by the Colon Cancer Family Registry. We first demonstrate that the variable age at diagnosis in the sample is right truncated by the closing date of the study and, as a result, the variable age at diagnosis is a biased sample of the target populations. To assess decrease in age at diagnosis of the disease over successive generations, we use the symmetry test proposed by Tsai et al. (2005) which accounts for the bias caused by the <b>right</b> <b>truncation</b> of both the parent's and child's ages at diagnosis. To test parent-of-origin effect, we examine and improve the method used by Lindor et al. (2010). Based on our preliminary analysis, {{we did not find}} sufficient statistical evidence from this sample to claim that there exists a parent-of-origin effect on age at diagnosis of LS relating to either the gender of the parent or the gender of the offspring after accounting for the sampling bias. The results given by the symmetry test suggest that there exists a decrease in age at diagnosis of LS over successive generations. This result should be free of the sampling bias caused by the <b>right</b> <b>truncation.</b> What remains uncertain is whether true genetic anticipation contributes to the decrease in age at diagnosis over successive generations observed in this disease...|$|E
40|$|There is a {{surge in}} medical {{follow-up}} studies that include longitudinal covariates in the modeling of survival data. So far, the focus has been largely on right-censored survival data. We consider survival data that are subject to both left <b>truncation</b> and <b>right</b> censoring. Left <b>truncation</b> is well known to produce biased sample. The sampling bias issue has been resolved in the literature for the case which involves baseline or time-varying covariates that are observable. The problem remains open, however, for the important case where longitudinal covariates are present in survival models. A joint likelihood approach {{has been shown in}} the literature to provide an effective way to overcome those difficulties for right-censored data, but this approach faces substantial additional challenges in the presence of left truncation. Here we thus propose an alternative likelihood to overcome these difficulties and show that the regression coefficient in the survival component can be estimated unbiasedly and efficiently. Issues about the bias for the longitudinal component are discussed. The new approach is illustrated numerically through simulations and data from a multi-center AIDS cohort study. Comment: Published in at [URL] the Annals of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
40|$|Let X 1, [...] .,Xm and Y 1, [...] .,Yn be two {{independent}} samples from two continuous distributions F and G. For testing the hypothesis H 0 : F = G, with no additional assumption on F and G, rank tests {{are used as}} {{they are easy to}} use and distribution-free under H 0. However, the problem of finding the distribution of the ranks of the Xs and Ys in the pooled sample is difficult in general when F ≠ G. Lehmann (1953) specialized a theorem of Hoeffding (1951, pages 87 - 88), and used it to obtain the distribution of the ranks under the alternatives G = Q(F), where Q is an absolutely continuous distribution function on (0, 1). However, the specialization cannot be used directly, even for the location shift problem, when the supports of the two distributions are not the same and no one of the supports contains the other. A generalized Hoeffding type theorem is given and used to obtain the distribution of the ranks under a variety of alternatives. Hence the small sample powers of the most frequently used rank tests for two-sample location and scale problems are obtained;The best precedence test (BPT) is derived for testing H 0 in a life testing experiment when two types of items are on test. The test has maximum power in the class of precedence tests at a given alternative F = 1 - (1 - G) [superscript][lambda] for some [lambda] 3 ̆e 1. The additional advantage in using the BPT is that it saves considerable time on test. We compare the power of the BPT with other tests and also obtain the average number of failures for the BPT;Consider the class of truncated populations such that G (F) is a left (<b>right</b> or left-and-right) <b>truncation</b> of F (G), or F and G are different types of truncation of some other distribution H. This class includes location-shift exponential distributions, location-shift uniform distributions and scale-change uniform distributions. The distribution of the ranks under each of the truncation models is obtained in a simple form. The ordering of the values of the probability function of the ranks is used to search for locally most powerful tests;When the observations are censored using different censoring models, e. g., censoring to the right at a fixed point, random censoring to the right, etc., the distribution of the ranks has been studied. Hence the powers of the generalized Wilcoxon test and the logrank test under the random censoring model are obtained;References: Hoeffding, W. (1951). Optimum Nonparametric Tests. Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability, 83 - 92. Lehmann, E. L. (1953). The Power of Rank Tests. Annals of Mathematical Statistics, 24, 23 - 43...|$|R
40|$|In a {{previous}} paper, the quantum-group-covariant chiral vertex operators in the spin 1 / 2 representation {{were shown to}} act, by braiding with the other covariant primaries, as generators of the well known Uq(sl(2)) quantum group symmetry (for a single screening charge). Here, this structure is transformed to the Bloch wave/Coulomb gas operator basis, thereby establishing {{for the first time}} its quantum group symmetry properties. A Uq(sl(2)) ⊗ Uq(sl(2)) symmetry of a novel type emerges: The two Cartan-generator eigenvalues are specified by the choice of matrix element (bra/ket Verma-modules); the two Casimir eigenvalues are equal and specified by the Virasoro weight of the vertex operator considered; the co-product is defined with a matching condition dictated by the Hilbert space structure of the operator product. This hidden symmetry possesses a novel Hopf like structure compatible with these conditions. At roots of unity it gives the <b>right</b> <b>truncation.</b> Its (non linear) connection with the Uq(sl(2)) previously discussed is disentangled. ...|$|E
40|$|In {{survival}} analysis, {{the data}} are often not analysed in the life time distribution. Because of the way individuals are selected for the study, the observed data must be analysed in a modied version of the lifetime distribution. We can use information on survival {{to the start of}} the study when we assume that the process of birthtimes is Poisson. In this case the relevant modications are length bias and the stationary recurrence time distribution known from renewal theory and epidemiology. On the other hand, we can condition on the birthtimes for the individuals we observe. Then the analysis does not depend on the distribution of the process of birthtimes, and we cannot use information on survival {{to the start of the}} study. In this case the relevant modications are left truncation, and <b>right</b> <b>truncation</b> or censoring. The Lexis diagram is a coordinate system with calendar time in the horizontal direction and age of individuals in the vertical direction, and it is useful as a w [...] ...|$|E
40|$|The use of {{truncated}} distributions arises {{often in}} a wide variety of scientific problems. In the literature, there are a lot of sampling schemes and proposals developed for various specific truncated distributions. So far, however, the study of the truncated multivariate t (TMVT) distribution is rarely discussed. In this paper, we first present general formulae for computing the first two moments of the TMVT distribution under the double truncation. We formulate the results as analytic matrix expressions, which can be directly computed in existing software. Results for the left and <b>right</b> <b>truncation</b> can be viewed as special cases. We then apply the slice sampling algorithm to generate random variates from the TMVT distribution by introducing auxiliary variables. This strategic approach can result in a series of full conditional densities that are of uniform distributions. Finally, several examples and practical applications are given to illustrate the effectiveness and importance of the proposed results. (C) 2011 Elsevier B. V. All rights reserved...|$|E
40|$|Anticipation, {{manifested}} through decreasing age {{of onset}} or increased severity in successive generations, {{has been noted}} in several genetic diseases. Statistical methods for genetic anticipation range from a simple use of the paired t -test for age of onset restricted to affected parent-child pairs to a recently proposed random effects model which includes extended pedigree data and unaffected family members [Larsen et al., 2009]. A naive use of the paired t -test is biased {{for the simple reason}} that age of onset has to be less than the age at ascertainment (interview) for both affected parent and child, and this <b>right</b> <b>truncation</b> effect is more pronounced in children than in parents. In this study, we first review different statistical methods for testing genetic anticipation in affected parent-child pairs that address the issue of bias due to <b>right</b> <b>truncation.</b> Using affected parent-child pair data, we compare the paired t -test with the parametric conditional maximum likelihood approach of Huang and Vieland [1997] and the nonparametric approach of Rabinowitz and Yang [1999] in terms of Type I error and power under various simulation settings and departures from the modeling assumptions. We especially investigate the issue of multiplex ascertainment and its effect on the different methods. We then focus on exploring genetic anticipation in Lynch syndrome and analyze new data on the age of onset in affected parent-child pairs from families seen at the University of Michigan Cancer Genetics clinic with a mutation in one of the three main mismatch repair (MMR) genes. In contrast to the clinic-based population, we re-analyze data on a population-based Lynch syndrome cohort, derived from the Danish HNPCC-register. Both datasets indicate evidence of genetic anticipation in Lynch syndrome. We then expand our review to incorporate recently proposed statistical methods that consider family instead of affected pairs as the sampling unit. These prospective censored regression models offer additional flexibility to incorporate unaffected family members, familial correlation and other covariates into the analysis. An expanded dataset from the Danish HNPCC-register is analyzed by this alternative set of methods. Genet. Epidemiol. 34 : 756 - 768, 2010. © 2010 Wiley-Liss, Inc...|$|E
40|$|Independent {{random samples}} {{are drawn from}} k (greater {{than or equal to}} 2) populations, having {{probability}} density functions belonging to a general truncation parameter family. The populations associated with the smallest and the largest truncation parameters are called the lower extreme population (LEP) and the upper extreme population (UEP), respectively. For the goal of selecting the LEP (UEP), we consider the natural selection rule, which selects the population corresponding to the smallest (largest) of k maximum likelihood estimates as the LEP (UEP), and study the problem of estimating the truncation parameter of the selected population. We unify some of the existing results, available in the literature for specific distributions, by deriving the uniformly minimum variance unbiased estimator (UMVUE) for the truncation parameter of the selected population. The conditional unbiasedness of the UMVUE is also checked. The cases of the left and the <b>right</b> <b>truncation</b> parameter families are dealt with separately. Finally, we consider an application to the Pareto probability model, where the performances of the UMVUE and three other natural estimators are compared with each other, under the mean squared error criterion. status: publishe...|$|E
40|$|We {{propose a}} method for fitting semiparametric models such as the {{proportional}} hazards (PH), additive risks (AR), and proportional odds (PO) models. Each of these semiparametric models implies that some transformation of the conditional cumulative hazard function (at each t) depends linearly on the covariates. The proposed method is based on nonparametric estimation of the conditional cumulative hazard function, forming a weighted average over a range of t-values, and subsequent use of least squares to estimate the parameters suggested by each model. An approximation to the optimal weight function is given. This allows semiparametric models to be fitted even in incomplete data cases where the partial likelihood fails (e. g., left censoring, <b>right</b> <b>truncation).</b> However, the main advantage of this method rests {{in the fact that}} neither the interpretation of the parameters nor the validity of the analysis depend on the appropriateness of the PH {{or any of the other}} semiparametric models. In fact, we propose an integrated method for data analysis where the role of the various semiparametric models is to suggest the best fitting transformation. A single continuous covariate and several categorical covariates (factors) are allowed. Simulation studies indicate that the test statistics and confidence intervals have good small-sample performance. A real data set is analyzed...|$|E
40|$|Bivariate {{estimation}} with {{survival data}} has received considerable attention recently; however, {{most of the}} work has focused on random censoring models. Another common feature of survival data, random truncation, is considered in this study. Truncated data may arise if the time origin of the events under study precedes the observation period. In a random right-truncation model, one observes the iid samples of (Y, T) only if (Y ≤ T), where Y is the variable of interest and T is an independent variable that prevents the complete observation of Y. Suppose that (V, X) is a bivariate vector of random variables, where Y is subject to <b>right</b> <b>truncation.</b> In this study the bivariate reverse-hazard vector is introduced, and a nonparametric estimator is suggested. An estimator for the bivariate survival function is also proposed. Weak convergence and strong consistency of this estimator are established via a representation by iid variables. An expression for the limiting covariance function is provided, and an estimator for the limiting variance is presented. Alternative methods for estimating the bivariate distribution function are discussed. Obtaining large-sample results for the bivariate distribution functions present more technical difficulties, and thus their performances are compared via simulation results. Finally, an application of the suggested estimators is presented for transfusion-related AIDS (TR-AIDS) data on the incubation time...|$|E
40|$|In {{this paper}} we build on {{previous}} work for {{estimation of the}} bivariate distribution of the time variables T 1 and T 2 when they are observable only {{on the condition that}} one of the time variables, say T 1, is greater than (lefttruncation) or less than (<b>right</b> <b>truncation)</b> some observed time variable C 1. In this paper, we introduce several results based on the Influence Curve (which we derive in this paper) of the NPMLE of the distribution F of (T 1,T 2) developed by van der Laan (van der Laan, 1996). Specifically we will: prove that the NPMLE is asymptotically equivalent to an estimator developed by Gürler (Gürler, 1997), derive the asymptotic distribution of the NPMLE based on its Influence Curve, present tests to determine the amount of dependence between T 1 and T 2, present the results of simulation studies that compare the NPMLE and Gürler’s estimator and evaluate the performance of both the above mentioned tests and confidence intervals of F based on the asymptotic distribution of the NPMLE, and finally we will apply the methods in a data analysis in which we also point out practical issues that arise in the implementation of the estimator...|$|E
40|$|In a {{previous}} paper, {{we proposed a}} construction of U_q(sl(2)) quantum group symmetry generators for 2 d gravity, where we took the chiral vertex operators of the theory to be the quantum group covariant ones established in earlier works. The basic idea was that the covariant fields in the spin 1 / 2 representation themselves {{can be viewed as}} generators, as they act, by braiding, on the other fields exactly in the required way. Here we transform this construction to the more conventional description of 2 d gravity in terms of Bloch wave/Coulomb gas vertex operators, thereby establishing for the first time its quantum group symmetry properties. A U_q(sl(2)) ⊗ U_q(sl(2)) symmetry of a novel type emerges: The two Cartan-generator eigenvalues are specified by the choice of matrix element (bra/ket Verma-modules); the two Casimir eigenvalues are equal and specified by the Virasoro weight of the vertex operator considered; the co-product is defined with a matching condition dictated by the Hilbert space structure of the operator product. This hidden symmetry possesses a novel Hopf like structure compatible with these conditions. At roots of unity it gives the <b>right</b> <b>truncation.</b> Its (non linear) connection with the U_q(sl(2)) previously discussed is disentangled. Comment: 37 pages, LaTeX, no figure...|$|E
40|$|Lifetime {{data are}} usually assumed {{to stem from}} a {{continuous}} distribution supported on [0, b) for some b ≤ ∞. The continuity assumption implies that {{the support of the}} distribution does not have atom points, particularly not at 0. Accordingly, it seems reasonable that with an accurate measurement tool all data observations will be positive. This suggests that the true support may be truncated from the left. In this work we investigate the effects of adding a left truncation parameter to a continuous lifetime data statistical model. We consider two main settings: <b>right</b> <b>truncation</b> parametric models with possible left truncation, and exponential family models with possible left truncation. We analyze the performance of some optimal estimators constructed under the assumption of no left truncation when left truncation is present, and vice versa. We investigate both asymptotic and finite-sample behavior of the estimators. We show that when left truncation is not assumed but is, in fact present, the estimators have a constant bias term, and therefore will result in inaccurate and inefficient estimation. We also show that assuming left truncation where actually there is none, typically does not result in substantial inefficiency, and some estimators in this case are asymptotically unbiased and efficient...|$|E
30|$|The bimodal AAO {{distribution}} {{found in}} both BD diagnostic subgroups {{and in the}} whole sample of 515 patients is consistent with some studies (Ortiz et al. 2011; Kennedy et al. 2005; Javaid et al. 2011). Conversely, the majority of studies on mixture analysis of AAO showed a trimodal distribution in samples comprising mainly BD 1 patients (Bellivier et al. 2001, 2003; Lin et al. 2006; Severino et al. 2009; Hamshere et al. 2009; Tozzi et al. 2011; Bellivier et al. 2014; Golmard et al. 2015). Of note, a recent study showed that bimodal and trimodal distribution fit equally well the AAO of BD (Grigoroiu-Serbanescu et al. 2014). Further {{research is needed to}} determine which distribution (bi- or tri-modal) better describes AAO in BD and which is (or which are) the best cut-off(s) before investigating clinical correlates and genetic differences between subgroups based on AAO. In fact, thresholds between subgroups found in different studies differed [e.g. thresholds between the intermediate and late AAO subgroups differed from 25 in one study (Tozzi et al. 2011) to 40  years in another (Hamshere et al. 2009)] as well as percentages of patients in each AAO subgroups [e.g. percentages of patients attributed to the early onset subgroup varied between 21.4 % (Bellivier et al. 2001) and 79.7 % (Lin et al. 2006)]. Discrepancies in the identified AAO distributions, cut-off points, and proportions of patients in each AAO subgroups may depend on diverse assessment methods, recall bias, study design (Montlahuc et al. 2016), and differences in characteristics of samples studied, including geographic location (Post et al. 2008; Bellivier et al. 2014) and birth cohort (Bauer et al. 2015; Golmard et al. 2015). Concerning study design, Montlahuc et al. (2016) tested whether cross‐sectional designs (which cause <b>right</b> <b>truncation),</b> unreliable diagnosis for individuals younger than 10  years old (which causes left truncation), and the selection criterion used for admixture analysis impacted the number of identified AAO subgroups. Importantly, a combination of left and <b>right</b> <b>truncation,</b> which is common in previously published studies of AAO admixture analysis, appeared to significantly influence the number of AAO subgroups detected (Montlahuc et al. 2016). Geographical location appears also to impact on AAO admixture analysis findings. Bellivier et al. (2014) found significant differences in the theoretical AAO functions between USA and European BD samples, mainly led by the higher proportion of patients in the EO subgroup and the lower mean AAO in the USA sample. Finally, birth cohort effect might also influence the estimation of AAO subgroups parameters. In this regard, Golmard et al. (2015) found that the proportion of EO cases increased substantially among BD cases born after 1960 compared to those born before the same year.|$|E
40|$|Cities are {{entities}} {{that are not}} “simple” but “complexly organized”. Theories about geographical structure of cities, land use patterns and cities evolution that explain how cities become spatially ordered are expanding to take in consideration this complexity. The conceptual foundation {{for the existence of}} central place hierarchies (i. e. the study of agglomeration economies in cities and trasportation and logistic costs) is now completed by the definition of emergent patterns that are not directly linked to the element of their economic processes but included in their “physic mechanisms” (i. e. the study of complex systems). This dissertation explores some of these aspects by performing empirical applications in the fields of regional and complex urban economics. The dissertation contributes to the long standing debate on the city size distribution. From the empirical standpoint, traditional studies on the distribution of cities typically rely a regularity known as Zipf’s Law. We first investigate some typical shortcomings related to the choiche of the <b>right</b> <b>truncation</b> point to discriminate between upper tail and body of the distribution (chapter 2). Secondly, we invesigate specific conditions leading to a weak form of Gibrat’s law in connection with the different typologies of rank-size distribution (Zipf’s law), by adopting parametric and non-parametric approaches (chapter 3) and, finally, we use both the laws in studying agglomeration forces whithin the European Union (chapter 4) ...|$|E
40|$|SummaryThe {{discovery}} that microsatellite repeat expansions can cause clinical disease has fostered {{renewed interest in}} testing for age-at-onset anticipation (AOA). A commonly used procedure is to sample affected parent-child pairs (APCPs) from available data sets and to test for a difference in mean age at onset between {{the parents and the}} children. However, standard statistical methods fail {{to take into account the}} <b>right</b> <b>truncation</b> of both the parent and child age-at-onset distributions under this design, with the result that type I error rates can be inflated substantially. Previously, we had introduced a new test, based on the correct, bivariate right-truncated, age-at-onset distribution. We showed that this test has the correct type I error rate for random APCPs, even for quite small samples. However, in that paper, we did not consider two key statistical complications that arise when the test is applied to realistic data. First, affected pairs usually are sampled from pedigrees preferentially selected for the presence of multiple affected individuals. In this paper, we show that this will tend to inflate the type I error rate of the test. Second, we consider the appropriate probability model under the alternative hypothesis of true AOA due to an expanding microsatellite mechanism, and we show that there is good reason to believe that the power to detect AOA may be quite small, even for substantial effect sizes. When the type I error rate of the test is high relative to the power, interpretation of test results becomes problematic. We conclude that, in many applications, AOA tests based on APCPs may not yield meaningful results...|$|E
