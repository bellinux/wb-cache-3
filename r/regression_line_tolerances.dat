0|2847|Public
50|$|The {{slopes of}} the {{different}} <b>regression</b> <b>lines</b> should be equivalent, i.e., <b>regression</b> <b>lines</b> should be parallel among groups.|$|R
3000|$|... where β′ is {{the angle}} of the <b>regression</b> <b>line</b> and b′ is the {{intercept}} of the <b>regression</b> <b>line</b> with the y-axis.|$|R
40|$|In many circumstances, {{it may be}} of {{interest}} to discover whether two or more <b>regression</b> <b>lines</b> are the same. <b>Regression</b> <b>lines</b> may differ in three properties, viz., in residual variance, in slope, and in elevation; all of which can be tested using analysis of covariance. If there are no significant differences between <b>regression</b> <b>lines,</b> an investigator may which to combine the data from different studies and fit a single <b>regression</b> <b>line</b> to the whole of the data...|$|R
30|$|For each subject, the first-order <b>regression</b> <b>line</b> was {{computed}} from nine data points for %Rec and %Det during fatiguing contractions of the bilateral MG. The {{rate of change}} in %Rec and %Det {{was defined as the}} slope of the <b>regression</b> <b>line</b> and the initial value as the intercept of the <b>regression</b> <b>line.</b>|$|R
40|$|This paper {{describes}} a machine learning method, called Regression by Selecthtg Best P~’ttllll’es (RSBF). RSBF {{consists of two}} phases: The first phase aims to find the predictive power of each feature by constructing simple linear <b>regression</b> <b>lines,</b> one per each continuous feature and number of categories pen each categorical feature. Although the predictive power of a continuous feature is constant, it varies for each distinct value of categorical features. The second phase constructs multiple linear <b>regression</b> <b>lines</b> among continuous features, each time excluding the worst feature among the current set, and constructs multiple linear <b>regression</b> <b>lines.</b> Finally, these muhiple linear <b>regression</b> <b>lines</b> and the categorical features" simple linear <b>regression</b> <b>lines</b> are sorted according to their predictive power. In the querying phase of learning, the best lineal " <b>regression</b> <b>line</b> and the features constructing that line are selected to make predictions...|$|R
40|$|This paper {{describes}} two {{machine learning}} methods, called Regression by Selecting Best Feature Projection (RSBFP) and Regression by Selecting Best Features (RSBF). RSBFP projects the training data on each feature dimension and produces exactly one simple linear <b>regression</b> <b>line</b> on each continuous feature. In {{the case of}} categorical features, exactly one simple linear <b>regression</b> <b>line</b> per each distinct value of each categorical feature is produced. Then these simple linear <b>regression</b> <b>lines</b> are analyzed to determine the feature projection that is best among all projections. The best feature projection and its corresponding simple linear <b>regression</b> <b>line</b> is then selected to make predictions. RSBF consists of two phases: The first phase uses RSBFP to sort predictive power of each feature projection. The second phase calls multiple linear least squares regression for number of features times, each time excluding the worst feature among the current set, and produces multiple linear <b>regression</b> <b>lines.</b> Finally, these <b>regression</b> <b>lines</b> are analyzed to select the best subset of features. The best subset of features and its corresponding multiple linear <b>regression</b> <b>line</b> is then selected to make predictions...|$|R
50|$|For example, when {{estimating}} the population mean, this method uses the sample mean; {{to estimate the}} population median, it uses the sample median; to estimate the population <b>regression</b> <b>line,</b> it uses the sample <b>regression</b> <b>line.</b>|$|R
3000|$|Like our <b>regression</b> <b>line,</b> <b>regression</b> <b>lines</b> {{that pass}} through the saline Kuroshio surface water were derived for the ECS shelf {{covering}} a broad sampling area for June–July (line II of Fig.  4) (Du et al. 2012), for the southern Yellow Sea in July (Kang et al. 1994), and for the area from the ECS to off the southern Japan Islands (Oba 1990) (line I of Fig.  4). Our <b>regression</b> <b>line</b> occupied an intermediate area between these <b>regression</b> <b>lines.</b> Furthermore, our equation {{is derived from the}} data obtained {{in the eastern part of}} the ECS where the eastward-flowing CDW mixes with the Kuroshio water in the summer. Therefore, our δ [...]...|$|R
40|$|BACKGROUND: The Belgian External Quality Assessment Scheme for Flow Cytometry {{evaluates the}} {{long-term}} analytical performance of participating laboratories by calculating a <b>regression</b> <b>line</b> between {{the target and}} reported values of each parameter for each laboratory during the past 3 years. This study aims to develop a method to find laboratories with aberrant variability or bias using robust techniques and to obtain robust estimates of the variability. METHODS: A method is proposed to find outliers {{with respect to the}} individual <b>regression</b> <b>line,</b> followed by a step to find <b>regression</b> <b>lines</b> with excessive variability and finally a step to find <b>regression</b> <b>lines</b> with high bias. RESULTS: The model was applied to the results obtained by 52 laboratories for CD 4 %. From the 1340 data points, 35 were determined to be regression outliers. The second step revealed one <b>regression</b> <b>line</b> with excessive variability; the third step detected three <b>regression</b> <b>lines</b> with exceeding bias. CONCLUSIONS: The methodology allows assessment of the long-term performance of laboratories, taking into account samples with different target values. Outliers in the first step indicate accidental mistakes, outliers {{in the second and third}} step point to high analytical variability or bias. Peer reviewe...|$|R
40|$|FIGURE 2. Relationship between {{arm width}} at {{half of it}} (b) and disk radious (r). Empty square: Leptychaster kerguelenensis type. Empty circle: L. mendosus type. Dotted line: <b>regression</b> <b>line</b> fitted to SW Atlantic specimens. Straight line: <b>regression</b> <b>line</b> fitted to Kerguelen and Marion islands specimens...|$|R
40|$|<b>Regression</b> <b>lines</b> were {{calculated}} for cephalothin, cephalexin, and cephaloridine by relating zone diameters of inhibition to minimal inhibitory concentrations (MIC) obtained in Mueller-Hinton agar and in Trypticase Soy Agar. A <b>regression</b> <b>line</b> was calculated for cephaloglycin by obtaining MIC values in Trypticase Soy Agar at pH 6. 6. <b>Regression</b> <b>lines</b> calculated from MIC values in Mueller-Hinton agar were practically superimposable on those based on MIC values in Trypticase Soy Agar. Organisms susceptible by disc testing to cephalothin were usually susceptible to cephalexin and cephaloridine...|$|R
30|$|This is the {{estimated}} the <b>regression</b> <b>line.</b>|$|R
3000|$|... 3 {{correlation}} between 90 -min 4 P (X-axis) and 40 -min 3 P++ (Y-axis) analyses {{in the same}} subjects. The <b>regression</b> <b>lines</b> are Y[*]=[*] 0.590 X[*]-[*] 0.005 (r 2 [*]=[*] 0.953) for the NC subject and Y[*]=[*] 0.338 X[*]+[*] 0.000 (r 2 [*]=[*] 0.907) for the AD subject. When the cerebellar data (X[*]=[*] 0.008, Y[*]=[*] 0.000) was removed from calculation for the AD subject, the <b>regression</b> <b>line</b> became Y[*]=[*] 0.295 X[*]-[*] 0.002 with slightly larger r 2 (0.935; not shown in the figure). The slopes of the <b>regression</b> <b>lines</b> show that k [...]...|$|R
40|$|This paper {{shows how}} to {{construct}} confidence bands {{for the difference}} between two simple linear <b>regression</b> <b>lines.</b> These confidence bands provide directly {{the information on the}} magnitude of the difference between the <b>regression</b> <b>lines</b> over an interval of interest and, as a by-product, {{can be used as a}} formal test of the difference between the two <b>regression</b> <b>lines.</b> Various different shapes of confidence bands are illustrated, and particular attention is paid towards confidence bands whose construction only involves critical points from standard distributions so that they are consequently easy to construct...|$|R
5000|$|Function of {{the angle}} between two {{standardized}} <b>regression</b> <b>lines</b> ...|$|R
30|$|The {{error of}} k, i.e. the {{uncertainty}} of the slope of the <b>regression</b> <b>line,</b> is, due to the number of data points, smaller than the least significant specified digit and therefore negligible. However, the standard deviation of the individual measurements from the <b>regression</b> <b>line</b> is 0.09 μm, corresponding to 2.1 μN.|$|R
40|$|This {{study was}} {{conducted}} to determine the gestational age-related reference range of the preload index [peak velocity during atrial contraction (A) /peak velocity during ventricular systole (S) ] for the inferior vena cava (IVC), the right hepatic vein, the middle hepatic vein and the left hepatic vein. The slope and the intercept of the <b>regression</b> <b>line</b> for each preload index were compared among the 4 veins using analysis of covariance. Doppler measurements were obtained for the 4 veins of 316 normal fetuses at 22 - 40 weeks of gestation. A and S values were measured from the recorded flow velocity waveform of each vein and the A/S ratio was calculated as the preload index. The <b>regression</b> <b>lines</b> for the preload index of the 4 veins decreased gradually throughout gestation. Analysis of covariance revealed no significant differences in the slopes of the <b>regression</b> <b>lines</b> for the 4 veins. However, the intercepts of the <b>regression</b> <b>lines</b> for all hepatic veins were significantly higher than that of the <b>regression</b> <b>line</b> for the IVC (P&# 60; 0. 0001), with the difference ranging from 0. 024 to 0. 033. There were no significant differences among the intercepts of the <b>regression</b> <b>lines</b> for different hepatic veins. We concluded that the relationship between the preload index and the duration of gestation was statistically similar for all hepatic veins, and strongly resembled that for the IVC. </p...|$|R
25|$|R2 is a {{statistic}} {{that will give}} some information about the goodness of fit of a model. In regression, the R2 coefficient of determination is {{a statistic}}al measure of how well the <b>regression</b> <b>line</b> approximates the real data points. An R2 of 1 indicates that the <b>regression</b> <b>line</b> perfectly fits the data.|$|R
2500|$|... (the {{point on}} the <b>regression</b> <b>line),</b> while the actual {{response}} would be ...|$|R
25|$|A test {{of whether}} the slope of a <b>regression</b> <b>line</b> differs {{significantly}} from 0.|$|R
50|$|A {{trigonometric}} {{representation of}} the orthogonal <b>regression</b> <b>line</b> was given by Coolidge in 1913.|$|R
5000|$|A test {{of whether}} the slope of a <b>regression</b> <b>line</b> differs {{significantly}} from 0.|$|R
50|$|This {{shows the}} role rxy {{plays in the}} <b>regression</b> <b>line</b> of {{standardized}} data points.|$|R
5000|$|... is {{estimated}} as {{the slope of}} the <b>regression</b> <b>line</b> for [...] versus [...] where: ...|$|R
30|$|In regression, the R 2 [*]coefficient of {{determination}} is a statistical {{measure of how}} well the <b>regression</b> <b>line</b> approximates the real data points. An R 2 [*]of 1 indicates that the <b>regression</b> <b>line</b> perfectly fits the data. The adjusted R 2 [*]is almost the same as R 2, but it penalizes the statistic as extra variables {{are included in the}} model.|$|R
3000|$|Second, the {{intercept}} of the <b>regression</b> <b>line</b> approximates {{the average}} apparent volume of distribution, [...]...|$|R
50|$|Statistical graphs {{can also}} be generated: bar graphs, line graphs, normal {{distribution}} curves, <b>regression</b> <b>lines.</b>|$|R
5000|$|The {{control limit}} lines are {{parallel}} to the <b>regression</b> <b>line</b> rather than the horizontal line.|$|R
40|$|If {{regression}} analysis {{is used for}} statistical evaluation of the data, authors must supply [...] . standard deviations of residuals (Sy|x, often called standard errors of estimates) [...] . Residuals plots [e. g., Bland-Altman] are often useful. —Extract from “Information for Authors ” (2006) The Clinical Chemistry “Information for Authors ” recom-mends that, when {{regression analysis}} is used, SDs of residuals must be supplied. (They are not always pro-vided.) As Cook and Weisberg note (1), this conceptual approach {{dates back to the}} early 1960 s, but by the late 1970 s, attention was increasingly directed to assessing the influence of individual observations on the results of regression analysis. The concept of influence (or leverage) can be illustrated by 2 simple examples. In Fig. 1 A, the <b>regression</b> <b>line</b> is shown for 4 in-line cases. When case 5 is added, the new <b>regression</b> <b>line</b> is slightly leveraged toward it (Fig. 1 C), but note that the case 5 residual is large (Fig. 1 E) and the <b>regression</b> <b>lines</b> are nearly parallel. However, when case 5 (Fig. 1 B) is added, the new <b>regression</b> <b>line</b> is much more influenced by its presence (Fig. 1 D). This case forces the <b>regression</b> <b>line</b> close to it, and its residual is correspond-ingly small (Fig. 1 F). What are the differences between these 2 cases? When an outlier is close to the mean value of x (as in case 5 in Fig. 1 A), its influence is small (Fig. 1 C), whereas when the outlier {{is a long way from}} the mean value of x and out of line of the initial <b>regression</b> <b>line</b> (as Fig. 1. The effect of an outlier on the resulting least squares <b>regression</b> <b>line.</b> Solid <b>lines</b> show the <b>regression</b> <b>line</b> with all 5 cases. (A), when case 5 outlier is near to the mean of the x values. (B), when case 5 outlier is distant from the mean of the x values. (C), leverage values for all 5 cases in A. (D), leverage values for all 5 cases in B. (E), crude residual values for all 5 cases in A. (F), crude residual values for all 5 cases in B...|$|R
30|$|Sa {{reflects}} {{the standard deviation}} of the <b>regression</b> <b>line’s</b> intercept, b quantifies the calibration curve’s slope.|$|R
5000|$|AbsScale: {{assess the}} {{deviations}} of points around the <b>regression</b> <b>line,</b> particularly around the last data points ...|$|R
30|$|At this stage, the {{students}} have not yet learned the formal background, for instance, the least squares method. They had intuitively searched for a method to draw the <b>regression</b> <b>line.</b> In fact, Paul describes here that {{he looks at the}} sum of residues. He connects mathematical techniques, such as the sum of all distances (residues) calculated, with statistical techniques, such as determining a <b>regression</b> <b>line.</b>|$|R
40|$|This paper {{describes}} a machine learning method, called Regression by Selecting Best Feature Projections (RSBFP). In the training phase, RSBFP projects the training data on each feature dimension and aims {{to find the}} predictive power of each feature attribute by constructing simple linear <b>regression</b> <b>lines,</b> one per each continuous feature and number of categories per each categorical feature. Because, although the predictive power of a continuous feature is constant, it varies for each distinct value of categorical features. Then the simple linear <b>regression</b> <b>lines</b> are sorted according to their predictive power. In the querying phase of learning, the best linear <b>regression</b> <b>line</b> and thus the best feature projection are selected to make predictions. © Springer-Verlag Berlin Heidelberg 2001...|$|R
5000|$|The mean [...] is {{estimated}} as {{the slope of}} the log-log <b>regression</b> <b>line</b> for [...] versus , where: ...|$|R
3000|$|The first {{estimated}} <b>regression</b> <b>line</b> {{indicates the}} hourly cost when the process operates in out-of-control state ([...] [...]...|$|R
5000|$|In this {{equation}} [...] for the least-squares <b>regression</b> <b>line,</b> [...] is the slope and [...] is the intercept.|$|R
