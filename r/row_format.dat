20|52|Public
5000|$|IfR is present,X,Y,C,E,K,W,N,H,G must be absent. (This sets default <b>row</b> <b>format.)</b> ...|$|E
5000|$|Oracle Database In-Memory, an in-memory, column-oriented data store, {{has been}} {{seamlessly}} {{integrated into the}} Oracle Database. This technology aims to improve the performance of analytic workloads without impacting the performance of transactions that continue to use Oracle's traditional <b>row</b> <b>format</b> in memory. Note: data is persisted on disk only in a <b>row</b> <b>format,</b> so no additional storage is required. The product's performance comes through the in-memory columnar format and {{through the use of}} SIMD vector processing (Single Instruction processing Multiple Data values). Database In-Memory features include: ...|$|E
50|$|NodeXL workbooks contain four worksheets: Edges, Vertices, Groups, and Overall Metrics. The {{relevant}} {{data about}} entities in the graph and relationships between them {{are located in}} the appropriate worksheet in <b>row</b> <b>format.</b> For example, the edges worksheet contains a minimum of two columns, and each row has a minimum of two elements corresponding to the two vertices that make up an edge in the graph. Graph metrics and edge and vertex visual properties appear as additional columns in the respective worksheets. This representation allows the user to leverage the Excel spreadsheet to quickly edit existing node properties and to generate new ones, for instance by applying Excel formulas to existing columns.|$|E
40|$|Abstract. In {{this paper}} we {{consider}} linear system Ax = b where A {{is a large}} sparse matrix. A new e ¢ cient, simple and inexpensive method for storage of coe ¢ cient matrix A was presented. The purpose of this method {{is to reduce the}} storage volume of large non-symmetric sparse matrices. The results shows that the proposed method is very inexpensive in comparison with current methods such as Coordinate <b>format,</b> Compressed Sparse <b>Row</b> (CSR) <b>format</b> and Modi…ed Sparse <b>Row</b> (MSR) <b>format...</b>|$|R
25|$|Cambridge ALGOL 68C(C) was a {{portable}} compiler that implemented {{a subset of}} ALGOL 68, restricting operator definitions and omitting garbage collection, flexible <b>rows</b> and <b>formatted</b> transput.|$|R
50|$|Hit104.9 {{recently}} {{changed from}} a 10 in a <b>row</b> music <b>format</b> to 50 Minutes Non Stop - playing 50 minute blocks of music ad free. There is no current breakfast team for Hit104.9 after Tom & Olly announced their departure from the station.|$|R
50|$|Rowstore and columnstore tables {{differ in}} {{more than just the}} storage medium used. Rowstores, as the name implies, store {{information}} in <b>row</b> <b>format,</b> which is the traditional data format used by RDBMS systems. Rowstores are optimized for singleton or small insert, update or delete queries and are most closely associated with OLTP (transactional) use cases. Columnstores are optimized for complex select queries, typically associated with OLAP (analytics) use cases. As an example, a large clinical data set for data analysis is best stored in columnar format, since queries run against it will typically be ad-hoc queries where aggregates are computed over large numbers of similar data items.|$|E
5000|$|PDF417 is a stacked barcode {{that can}} be read with a simple linear scan being swept over the symbol. Those linear scans need {{the left and right}} columns with the start and stop code words. Additionally, the scan needs to know what row it is scanning, so each row of the symbol must also encode its row number. Furthermore, the reader's line scan won't scan just a row; it will {{typically}} start scanning one row, but then cross over to a neighbor and possibly continuing on to cross successive rows. In order to minimize the effect of these crossings, the PDF417 modules are tall and narrow [...] - [...] the height is typically three times the width. Also, each code word must indicate which row it belongs to so crossovers, when they occur, can be detected. The code words are also designed to be delta-decodable, so some code words are redundant. Each PDF data code word represents about 10 bits of information (log2(900) &asymp; 9.8), but the printed code word (character) is 17 modules wide. Including a height of 3 modules, a PDF417 code word takes 51 square modules to represent 10 bits. That area does not count other overhead such as the start, stop, <b>row,</b> <b>format,</b> and ECC information.|$|E
40|$|The {{efficient}} use of multicore architectures for sparse matrix-vector multiplication (SpMV) is currently an open challenge. One algorithm which makes use of SpMV is the maximum likelihood expectation maximization (MLEM) algorithm. When using MLEM for positron emission tomography (PET) image reconstruction, one requires a particularly large matrix. We present a new storage scheme {{for this type of}} matrix which cuts the memory requirements by half, compared to the widely-used compressed sparse <b>row</b> <b>format.</b> For parallelization we combine the two partitioning techniques recursive bisection and striping. Our results show good load balancing and cache behavior. We also give speedup measurements on various modern multicore systems...|$|E
50|$|Field Hockey at the 2018 Summer Youth Olympics {{was held}} from October 6 to October 18. The events {{took place at}} the Parque Polideportivo Roca in Buenos Aires, Argentina. For the second games in a <b>row</b> the <b>format</b> for this event will be Hockey 5s, a 5-a-side tournament that is played on a smaller field size.|$|R
40|$|We {{investigate}} a new storage format for unstructured sparse ma-trices, {{based on the}} space filling Hilbert curve. Numerical tests with matrix-vector multiplication show {{the potential of the}} fractal storage format (FS) in comparison to the traditional compressed <b>row</b> storage <b>format</b> (CRS). The FS format outperforms the CRS format by up to 50 % for matrix-vector multiplications with multiple right hand sides. ...|$|R
5000|$|Super League XIII {{followed}} the top-six play-off system. It was the seventh {{year in a}} <b>row</b> the <b>format</b> had been applied, {{as well as being}} the final year before the play-offs were expanded in the 2009 season. Places were granted to the top six teams in the Super League XIII table. Following the final round of matches on the weekend of 5-7 September, all six play-off teams were set (in order of finishing place): ...|$|R
40|$|AbstractSparse matrix-vector {{multiplication}} (SpMV) is {{a fundamental}} operation for many applications. Many {{studies have been done}} to implement the SpMV on different platforms, while few work focused on the very large scale datasets with millions of dimensions. This paper addresses the challenges of implementing large scale SpMV with FPGA and GPU in the application of web link graph analysis. In the FPGA implementation, we designed the task partition and memory hierarchy according to the analysis of datasets scale and their access pattern. In the GPU implementation, we designed a fast and scalable SpMV routine with three passes, using a modified Compressed Sparse <b>Row</b> <b>format.</b> Results show that FPGA and GPU implementation achieves about 29 x and 30 x speedup on a StratixII EP 2 S 18...|$|E
40|$|International audienceThe {{implicit}} parallelism is {{an active}} domain of computer- science to hide intricate details of parallelization from the end-user. Some solutions are specific to a precise domain while others are more generic, however, the purpose is always to find the adapted level of abstraction to ease the high performance and parallel programming. We present Skel- GIS, a header-only implicit parallelism C++ library to solve mesh-based scientific simulations. In this paper is detailed the implementation of SkelGIS for the specific case of network simulations, where the space domain can be represented as a directed acyclic graph (DAG). This im- plementation {{is based on a}} modified, optimized and parallelized version of the Compressed Sparse <b>Row</b> <b>format,</b> which is completely described in this paper. Finally, experiments on different kinds of clusters and differ- ent sizes of DAGs are evaluated...|$|E
40|$|Abstract. Realistic {{modeling}} and deformation of soft tissue {{is one of}} the key technologies of virtual surgery simulation which is a challenging research field that stimulates the development of new clinical applications such as the virtual surgery simulator. In this paper we adopt the linear FEM (Finite Element Method) and sparse matrix compression stored in CSR (Compressed Sparse <b>Row)</b> <b>format</b> that enables fast {{modeling and}} deformation of soft tissue on GPU hardware with NVIDIA’s CUSPARSE (Compute Unified Device Architecture Sparse Matrix) and CUBLAS (Compute Unified Device Architecture Basic Linear Algebra Subroutines) library. We focus on the CGS (Conjugate Gradient Solver) which is the mainly time-consuming part of FEM, and transplant it onto GPU with the two libraries mentioned above. The experimental results show that the accelerating method in this paper can achieve realistic and fast modeling and deformation simulation of soft tissue...|$|E
50|$|The Sixth National Hockey League All-Star Game {{took place}} at the Detroit Olympia, home of the Detroit Red Wings, on October 5, 1952. For {{the second year in a}} <b>row,</b> the <b>format</b> had the First and Second All-Star Teams, with {{additional}} players on each team, play each other. After the game ended in a tie for the second year in a row, the NHL decided that they would continue with the previous format of the Stanley Cup winner playing an all-star team.|$|R
50|$|The {{first four}} file formats {{supported}} in Hive were plain text, sequence file, optimized <b>row</b> columnar (ORC) <b>format</b> and RCFile. Apache Parquet {{can be read}} via plugin in versions later than 0.10 and natively starting at 0.13. Additional Hive plugins support querying of the Bitcoin Blockchain.|$|R
40|$|We propose {{extensions}} of the classical <b>row</b> compressed storage <b>format</b> for sparse matrices. The extensions are designed to accomodate distributed storage of the matrix. We outline an implementation of the matrix-vector product using this distributed storage format, and give algorithms for building and using the communication structure between processors...|$|R
40|$|Abstract. In certain {{applications}} the non-zero {{elements of}} large sparse matrices are formed by adding several smaller contributions in random order {{before the final}} values of the elements are known. For some sparse matrix representations this procedure is laborious. We present an efficient method for assembling large irregular sparse matrices where the non-zero elements have to be assembled by adding together contributions and updating the individual elements in random order. A sparse matrix is stored in a hash table, which allows an efficient method to search for an element. Measurements show that for a sparse matrix with random elements the hash-based representation performs almost 7 {{times faster than the}} compressed <b>row</b> <b>format</b> (CRS) used in the PETSc library. Once the sparse matrix has been assembled we transfer the matrix to e. g. CRS for matrix manipulations. ...|$|E
40|$|As the {{importance}} of memory access delays on performance has mushroomed {{over the past few}} decades, researchers have begun exploring Processing-in-Memory (PIM) technology, which offers higher memory bandwidth, lower memory latency, and lower power consumption. In this study, we investigate whether an emerging PIM design from Sandia National Laboratories can boost performance for sparse matrix-vector product (SMVP). While SMVP is in the best-case bandwidth-bound, factors related to matrix structure and representation also limit performance. We analyze SMVP both {{in the context of an}} AMD Opteron processor and the Sandia PIM, exploring the performance limiters for each and the degree to which these can be ameliorated by data and code transformations. Over a range of sparse matrices, SMVP on the PIM outperformed the Opteron by a factor of 1. 82. On the PIM, computational kernel and data structure transformations improved performance by almost 40 % over conventional implementations using compressed-sparse <b>row</b> <b>format...</b>|$|E
40|$|In {{this paper}} {{we present a}} new {{technique}} for sparse matrix multiplication on vector multiprocessors based on the efficient implementation of a segmented sum operation. We describe how the segmented sum can be implemented on vector multiprocessors such that it both fully vectorizes within each processor and parallelizes across processors. Because of our method's insensitivity to relative row size, it is better suited than the Ellpack/Itpack or the Jagged Diagonal algorithms for matrices which have a varying number of non-zero elements in each row. Furthermore, our approach requires less preprocessing (no more time than a single sparse matrix-vector multiplication), less auxiliary storage, and uses a more convenient data representation (an augmented form of the standard compressed sparse <b>row</b> <b>format).</b> We have implemented our algorithm (SEGMV) on the Cray Y-MP C 90, and have compared its performance with other methods {{on a variety of}} sparse matrices from the Harwell-Boeing collection and in [...] ...|$|E
50|$|The {{style was}} also very flexible. The typical bay-and-gable house was made out of brick, but ones made {{completely}} out of wood were also easily produced. Bay-and-gable houses were most often built as semi-detached buildings, but the basic design could also easily be modified into a stand-alone or <b>row</b> house <b>format</b> and many examples of both exist in Toronto. There are many variations on the bay-and-gable found in Toronto. One {{of the most common}} simplifications on the style is to replace the bay window with a flat wall.|$|R
5000|$|At the 2015 awards co-founder Alison Cotes was farewelled after 25 {{years on}} the committee. She later wrote In spite of many changes over its 29 year history, and the often bitter {{political}} <b>rows</b> about <b>format</b> and judges, the Matilda Awards are still going strong, and {{with the backing of}} Arts Queensland will continue to develop, even if they do annoy many people along the way and attract plenty of criticism.At the 2015 awards the Gold Matilda was awarded posthumously to Carol Burns. She was renowned in Brisbane theatre and the audience responded with a standing ovation.|$|R
40|$|The {{implementation}} {{and performance of}} a parallel three dimensional finite element code for open domain electromagnetic problem are discussed. The whole mathematical procedure leads to a linear system with symmetric complex sparse matrix. Only the nonzero elements of the upper triangular part of this matrix are stored using the compressed <b>row</b> storage <b>format.</b> The linear system is solved with the conjugate orthogonal conjugate gradient method. The parallel code is tested for {{the case of a}} plane wave incident on a dielectric sphere. Results show that the finite element method is suited for parallelization in a massively parallel environment...|$|R
40|$|This {{work was}} also {{published}} as a Rice University thesis/dissertation: [URL] {{the importance of}} memory access delays on performance has mushroomed {{over the last few}} decades, researchers have begun exploring Processing-in-Memory (PIM) technology, which offers higher memory bandwidth, lower memory latency, and lower power consumption. In this study, we investigate whether an emerging PIM design from Sandia National Laboratories can boost performance for sparse matrix-vector product (SMVP). While SMVP is in the best-case bandwidth-bound, factors related to matrix structure and representation also limit performance. We analyze SMVP both {{in the context of an}} AMD Opteron processor and the Sandia PIM, exploring the performance limiters for each and the degree to which these can be ameliorated by data and code transformations. Over a range of sparse matrices, SMVP on the PIM outperformed the Opteron by a factor of 1. 82. On the PIM, computational kernel and data structure transformations improved performance by almost 40 % over conventional implementations using compressed-sparse <b>row</b> <b>format...</b>|$|E
40|$|The {{examination}} timetabling problem {{belongs to}} the class of combinatorial optimization problems and is of great importance for every University. In this paper, a hybrid evolutionary algorithm running on a GPU is employed to solve the examination timetabling problem. The hybrid evolutionary algorithm proposed has a genetic algorithm component and a greedy steepest descent component. The GPU computational capabilities allow the use of very large population sizes, leading to a more thorough exploration of the problem solution space. The GPU implementation, {{depending on the size}} of the problem, is up to twenty six times faster than the identical single-threaded CPU implementation of the algorithm. The algorithm is evaluated with the well known Toronto datasets and compares well with the best results found in the bibliography. Moreover, the selection of the encoding of the chromosomes and the tournament selection size as the population grows are examined and optimized. The compressed sparse <b>row</b> <b>format</b> is used for the conflict matrix and was proven essential to the process, since most of the datasets have a small conflict density, which translates into an extremely sparse matrix...|$|E
40|$|Sparse matrix–vector {{multiplication}} (SpMV) is of singular {{importance in}} sparse linear algebra, {{which is an}} important issue in scientific computing and engineering practice. Much effort has been put into accelerating SpMV, and a few parallel solutions have been proposed. This paper focuses on a special type of SpMV, namely sparse quasi-diagonal matrix–vector multiplication (SQDMV). The sparse quasi-diagonal matrix is the key to solving many differential equations, and very little research has been done in this field. This paper discusses data structures and algorithms for SQDMV that are efficiently implemented on the compute unified device architecture (CUDA) platform for the fine-grained parallel architecture of the graphics processing unit (GPU). A new diagonal storage format, a hybrid of the diagonal format (DLA) and the compressed sparse <b>row</b> <b>format</b> (CSR) (HDC) will be presented, which overcomes the inefficiency of DLA in storing irregular matrices and the imbalances of CSR in storing non-zero elements. Furthermore, HDC can adjust the storage bandwidth of the diagonal to adapt to different discrete degrees of sparse matrix, so as to get a higher compression ratio than DLA and CSR, and reduce the computational complexity. Our implementation in a GPU shows that the performance of HDC is better than that of other formats, especially for matrices with some discrete points outside the main diagonal. In addition, we combine the different parts of HDC to make a unified kernel to get a better compression ratio and a higher speedup ratio in the GPU...|$|E
40|$|We {{develop a}} {{prototype}} library for in-place (dense) matrix storage for-mat conversion between the canonical <b>row</b> and column-major <b>formats</b> {{and the four}} canonical block data layouts. Many of the fastest linear algebra routines operate on matrices in a block data layout. In-place storage for-mat conversion enables support for input/output of large matrices in the canonical <b>row</b> and column-major <b>formats.</b> The library uses algorithms associated with in-place transposition as building blocks. We investigate previous work {{on the subject of}} (in-place) transposition and the most promising algorithms are implemented and evaluated. Our results indi-cate that the Three-Stage Algorithm which only requires a small constant amount of additional memory performs well and is easy to tune. Murray Dow’s V 5 algorithm, which is a two-stage semi-in-place algorithm that re-quires a small amount of additional memory is sometimes a better choice. The write-allocate strategy of most cache-based computer architecture...|$|R
40|$|We {{improve the}} {{performance}} of sparse matrix-vector multiply (SpMV) on modern cache-based superscalar machines when the matrix structure consists of multiple, irregularly aligned rectangular blocks. Matrices from finite element modeling applications often {{have this kind of}} structure. Our technique splits the matrix, A, into a sum, A{sub 1 } + A{sub 2 } + [...] . + A{sub s}, where each term is stored in a new data structure, unaligned block compressed sparse <b>row</b> (UBCSR) <b>format.</b> The classical alternative approach of storing A in a block compressed sparse <b>row</b> (BCSR) <b>format</b> yields limited performance gains because it imposes a particular alignment of the matrix non-zero structure, leading to extra work from explicitly padded zeros. Combining splitting and UBCSR reduces this extra work while retaining the generally lower memory bandwidth requirements and register-level tiling opportunities of BCSR. Using application test matrices, we show empirically that speedups can be as high as 2. 1 x over not blocking at all, and as high as 1. 8 x over the standard BCSR implementation used in prior work. When performance does not improve, split UBCSR can still significantly reduce matrix storage. Through extensive experiments, we further show that the empirically optimal number of splittings s and the block size for each matrix term A{sub i} will in practice depend on the matrix and hardware platform. Our data lay a foundation for future development of fully automated methods for tuning these parameters...|$|R
40|$|Abstract. Sparse Matrix-Vector Multiplication (SMVM) is the {{critical}} computational kernel of many iterative solvers for systems of sparse linear equations. In this paper we propose an FPGA design for SMVM which interleaves CRS (Compressed <b>Row</b> Storage) <b>format</b> so that just a single floating point accumulator is needed, which simplifies control, avoids any idle clock cycles and sustains high throughput. For {{the evaluation of the}} proposed design we use a RASC RC 100 blade attached to a SGI Altix multiprocessor architecture. The limited memory bandwidth of this architecture heavily constraints the performance demonstrated. However, the use of FIFO buffers to stream input data makes the design portable to other FPGA-based platforms with higher memory bandwith...|$|R
40|$|This master thesis {{examines}} {{whether the}} linear module {{of the flow}} solver TRACE (Turbo machinery Research Aerodynamic Computational Environment) can be accelerated {{by the use of}} GPUs. The target platforms of the current implementation are CPU Clusters and employs MPI (Message Passing Interface) for parallelization. Different data formats are examined in this thesis. The basics of the General-Purpose Computation on Graphics Processing Unit programming (GPGPU) with the use of Open Computing Language (OpenCL) and Compute Unified Device Architecture CUDA are described. The first step is to analyze the flow solver TRACE. Which algorithm is used? Which parts of the algorithm are candidates to be executed by GPU? Which data format is used? TRACE uses the Blocked Compressed Sparse <b>Row</b> <b>format</b> (BCSR) with a block size of 5 x 5 values. It is examined if this format is suitable for processing by the GPU or if another format works better on this architecture. For testing different formats several data converters have been implemented. TRACE uses the Generalized Minimum Residual algorithm (GMRES). GMRES contains a sparse matrix-vector multiplication (SpMV); this will be ported onto the GPU. The SpMV is implemented with the use of different data formats. By trying different optimization approaches it is tested if a better performance is possible. The various implementations with OpenCL and CUDA are compared on a test system with the MPI version of TRACE. The performance of the implemented SpMV kernels is measured and compared. It is found that block variants of the ELL format give the best results...|$|E
40|$|Finding a {{suitable}} compressed representation of large-scale networks has been intensively studied in both practical and theoretical branches of data mining and network analysis [CS 09, AM 01, CKL+ 09, BV 04, SBBA 08]. In particular, {{the success of}} applying some of the recently proposed compression schemes [CKL+ 09, BV 04, AD 09] strongly depends on the “compression-friendly ” arrangement of network nodes. Usually, the goal of these arrangements is to order the nodes such that the endpoints of network links (edges) are located as close as possible. Doing so leads to a more compact representation of links and allows a better performance of compression schemes and network element access operations. In [CKL+ 09], Chierichetti et al. propose to use ordering by the minimum logarithmic arrangement problem (MLogA), that minimizes the gap encodings of edges stretched between their endpoints. This is achieved by ordering the network nodes and assigning to them unique integer values (ids) such that the endpoints of a link will obtain close values. MLogA seeks a nearly optimal information-theoretical com-pressed encoding size for all network links as it minimizes {{the total number of}} bits to spend for this purpose. The problem is NP-hard. For example, instead of the popular network/matrix compressed row representation, which contains a sorted lists of neighbors per node, the following link gap encoding can be used. Compressed <b>row</b> <b>format</b> Gap format node number sorted neighbors sorted gaps 1 i,j,k, [...] . i,j-i,k-j, [...] . 2 p,q,r, [...] . p,q-p,r-q, [...] . Problem (GMLogA) : Given a weighted graph G = (V,E), the goal of the link-weighted generalized minimum loga-rithmic arrangement problem is to minimize ij∈E wij lg |pi(i) − pi(j) | over all permutations pi. Given node volumes, v, this is equivalent to the continuous version min pi ij∈E wij lg |xi − xj | such that xi = v...|$|E
40|$|VMAT {{optimization}} is a computationally challenging problem due to {{its large}} data size, high degrees of freedom, and many hardware constraints. High-performance graphics processing units {{have been used to}} speed up the computations. However, its small memory size cannot handle cases with a large dose-deposition coefficient (DDC) matrix. This paper is to report an implementation of our column-generation based VMAT algorithm on a multi-GPU platform to solve the memory limitation problem. The column-generation approach generates apertures sequentially by solving a pricing problem (PP) and a master problem (MP) iteratively. The DDC matrix is split into four sub-matrices according to beam angles, stored on four GPUs in compressed sparse <b>row</b> <b>format.</b> Computation of beamlet price is accomplished using multi-GPU. While the remaining steps of PP and MP problems are implemented on a single GPU due to their modest computational loads. A H&N patient case was used to validate our method. We compare our multi-GPU implementation with three single GPU implementation strategies: truncating DDC matrix (S 1), repeatedly transferring DDC matrix between CPU and GPU (S 2), and porting computations involving DDC matrix to CPU (S 3). Two more H&N patient cases and three prostate cases were also used to demonstrate the advantages of our method. Our multi-GPU implementation can finish the optimization within ~ 1 minute for the H&N patient case. S 1 leads to an inferior plan quality although its total time was 10 seconds shorter than the multi-GPU implementation. S 2 and S 3 yield same plan quality as the multi-GPU implementation but take ~ 4 minutes and ~ 6 minutes, respectively. High computational efficiency was consistently achieved for the other 5 cases. The results demonstrate that the multi-GPU implementation can handle the large-scale VMAT optimization problem efficiently without sacrificing plan quality. Comment: 23 pages, 3 figures, 3 table...|$|E
40|$|Modern {{iterative}} solver packages have targeted superscalar computer architectures, {{and typically}} exhibit poor out-of-box performance on vector processor machines. In this work, we describe initial work on algorithmic {{modifications to the}} popular PETSc scientific toolkit to improve the vectorizability of its iterative linear system solvers. We describe our implementation of a PETSc matrix class that uses a simple vectorizable algorithm to perform sparse matrix-vector multiplication with compressed sparse <b>row</b> (CSR) <b>format</b> matrices. Performance tests using our kernel in codes that solve a variety of physics problems on the Cray X 1 indicate that speedups of {{an order of magnitude}} or better for matrix-vector multiplication are typical. Further work remains, however, as applications are still slowed by poor vectorization of preconditioning operations. ...|$|R
2500|$|The {{compressed}} sparse row (CSR) or compressed <b>row</b> storage (CRS) <b>format</b> {{represents a}} matrix [...] by three (one-dimensional) arrays, that respectively contain nonzero values, the extents of rows, and column indices. It {{is similar to}} COO, but compresses the row indices, hence the name. This <b>format</b> allows fast <b>row</b> access and matrix-vector multiplications (...) [...] The CSR format has been in use {{since at least the}} mid-1960s, with the first complete description appearing in 1967.|$|R
40|$|Práce se zabývá analýzou, návrhem a implementací webové aplikace pro násobení smíšených polynomů. Aplikace využívá triviální algoritmus, Strassenův algoritmus a algoritmus pro násobení řídkých smíšených polynomů ve formátu Compressed Row Storage. Tyto algoritmy jsou v práci také popsány po teoretické stránce. Výsledkem je aplikace, která dokáže vynásobit 2 smíšené polynomy o 2 neznámých pomocí zvoleného algoritmu a vykreslit grafy rychlostí různých výpočtů. This thesis {{deals with}} analysis, design and {{implementation}} of a web application for multiplication of the mixed polynomials. The application uses trivial algorithm, Strassen's algorithm and algorithm for multiplication of sparse mixed polynomials in the Compressed <b>Row</b> Storage <b>format.</b> The work contains theoretical description of these algorithms. The result is a web application, which can multiply 2 bivariate polynomials using the selected algorithm and draw plots of speeds of different algorithms...|$|R
