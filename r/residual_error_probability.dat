6|10000|Public
40|$|Abstract â€” Coding the payload {{data and}} sending the {{codeword}} as an overhead of the packet {{is a very}} common way to protect data in communication networks. The protection level of the coding technique is chosen depending {{on the importance of}} the transmitted data. While high priority, safety critical applications do not tolerate any single bit error in data packets, lower priority services can handle few bit errors. Coding techniques provide error detection and up to a certain level also error correction. Bit errors that are not detectable by coding techniques at the receiver side occur with a <b>residual</b> <b>error</b> <b>probability.</b> The better the coding technique is, the lower is its <b>residual</b> <b>error</b> <b>probability</b> for different bit error rates. Most automotive network systems use Cyclic Redundancy Codes (CRC) mainly in order to detect transmission bit errors. Instead of correcting the identified bit errors which is quite time consuming, usually a retransmission of the damaged data packet is triggered. Similar to automotive network systems, Ethernet, the most applied network technology in local area networks uses the CRC error detection technique. In this work, we present a comparative study of the error detection capabilities of automotive network systems and Ethernet as a possible network system for time critical applications in the car. We evaluate the related residual error probabilities for a reasonable range of bit error rates. Furthermore, several commercial concepts are presented from the automation field that increase the error detection capability of the standard Ethernet technology significantly. I...|$|E
40|$|Software {{engineers}} are humans {{and so they}} make lots of mistakes. Typically 1 out of 10 to 100 tasks go wrong. The only way to avoid these mistakes is to introduce redundancy in the software engineering process. This article is a plea to consciously introduce several levels of redundancy for each programming task. Depending on the required level of correctness, expressed in a <b>residual</b> <b>error</b> <b>probability</b> (typically 10 - 3 to 10 - 10), each programming task {{must be carried out}} redundantly 4 to 8 times. This number is hardly influenced by the size of a programming endeavour. Training software engineers does have some effect as non trained software engineers require a double amount of redundant tasks to deliver software of a desired quality. More compact programming, for instance by using domain specific languages, only reduces the number of redundant tasks by a small constant...|$|E
40|$|The {{problem of}} error {{correction}} for Gallager's low-density parity-check codes is famously {{equivalent to that}} of computing marginal Boltzmann probabilities for an Ising-like model with multispin interactions in a non-uniform magnetic field. Since the graph of interactions is locally a tree, the solution is very well approximated by a generalized mean-field (Bethe-Peierls) approximation. Belief propagation (BP) and similar iterative algorithms are {{an efficient way to}} perform the calculation, but they sometimes fail to converge, or converge to non-codewords, giving rise to a non-negligible <b>residual</b> <b>error</b> <b>probability</b> (error floor). On the other hand, provably-convergent algorithms are far too complex to be implemented in a real decoder. In this work we consider the application of the probability-damping technique, which can be regarded either as a variant of BP, from which it retains the property of low complexity, or as an approximation of a provably-convergent algorithm, from which it is expected to inherit better convergence properties. We investigate the algorithm behaviour on a real instance of Gallager code, and compare the results with state-of-the-art algorithms. Comment: 17 pages, 4 figures; minor revisions, results unchange...|$|E
40|$|Investigation of {{the effects}} of {{imperfect}} timing in a direct-detection (noncoherent) optical system using pulse-position-modulation bits. Special emphasis is placed on specification of timing accuracy, and an examination of system degradation when this accuracy is not attained. Bit <b>error</b> <b>probabilities</b> are shown as a function of timing errors, from which average <b>error</b> <b>probabilities</b> can be computed for specific synchronization methods. Of significant importance is shown to be the presence of a <b>residual,</b> or irreducible <b>error</b> <b>probability,</b> due entirely to the timing system, that cannot be overcome by the data channel...|$|R
40|$|The {{effects of}} {{imperfect}} timing in direct detection (non-coherent) optical binary systems using both PPM and on-off keying for bit transmission are investigated. Particular {{emphasis is placed}} on specification of timing accuracy, and an examination of system degradation when this accuracy is not attained. Bit <b>error</b> <b>probabilities</b> are shown as a function of timing errors, from which average <b>error</b> <b>probabilities</b> can be computed for specific synchronization methods. Of significant importance is the presence of a <b>residual,</b> or irreducible <b>error</b> <b>probability</b> in both systems, due entirely to the timing system, that cannot be overcome by the data channel...|$|R
40|$|The use {{of digital}} {{transmission}} with narrow light pulses appears attractive for data communications, but {{carries with it}} a stringent requirement on system bit timing. The effects of imperfect timing in direct-detection (noncoherent) optical binary systems are investigated using both pulse-position modulation and on-off keying for bit transmission. Particular {{emphasis is placed on}} specification of timing accuracy and an examination of system degradation when this accuracy is not attained. Bit <b>error</b> <b>probabilities</b> are shown as a function of timing errors from which average <b>error</b> <b>probabilities</b> can be computed for specific synchronization methods. Of significance is the presence of a <b>residual</b> or irreducible <b>error</b> <b>probability</b> in both systems, due entirely to the timing system, which cannot be overcome by the data channel...|$|R
40|$|Safety {{integrity}} level (SIL) {{verification of}} functional safety fieldbus communication {{is an essential}} part of SIL verification of safety instrumented system (SIS), and it requires quantifying <b>residual</b> <b>error</b> <b>probability</b> (RP) and residual error rate of function safety communication. The present quantification method of residual error rate uses RP of cyclic redundancy check (CRC) to approximately replace the total RP of functional safety communication. Since CRC only detects data integrity-related errors and CRC has intrinsically undetected error, some other residual errors are not being considered. This research found some residual errors of the present quantification method. Then, this research presents an extended new approach, which takes the found residual errors into account to determine more comprehensive and reasonable RP and residual error rate. From perspective of the composition of safety message, this research studies RPs of those controlling segments (sequence number, time expectation, etc.) to cover the found residual errors beyond CRC detection coverage, and the influences of insertion/masquerade errors and time window on RP are investigated. The results turn out these residual errors, especially insertion/masquerade errors, may have a great influence on quantification of residual error rate and SIL verification of functional safety communication, and they should be treated seriously...|$|E
40|$|AbstractWe {{report on}} a strong {{capacity}} boost in storing digital data in synthetic DNA. In principle, synthetic DNA is an ideal media to archive digital data for very long times because the achievable data density and longevity outperforms today's digital data storage media by far. On the other hand, neither the synthesis, nor the amplification and the sequencing of DNA strands can be performed error-free today and in the foreseeable future. In order to make synthetic DNA available as digital data storage media, specifically tailored forward error correction schemes have to be applied. For the purpose of realizing a DNA data storage, we have developed an efficient and robust forwarderror-correcting scheme adapted to the DNA channel. We based {{the design of the}} needed DNA channel model on data from a proof-of-concept conducted 2012 by a team from the Harvard Medical School [1]. Our forward error correction scheme is able to cope with all error types of today's DNA synthesis, amplification and sequencing processes, e. g. insertion, deletion, and swap errors. In a successful experiment, we were able to store and retrieve error-free 22 MByte of digital data in synthetic DNA recently. The found <b>residual</b> <b>error</b> <b>probability</b> is already in the same order as it is in hard disk drives and can be easily improved further. This proves the feasibility to use synthetic DNA as longterm digital data storage media...|$|E
40|$|This paper {{explores the}} {{relationship}} between two ideas in network information theory: edge removal and strong converses. Edge removal properties state that if an edge of small capacity is removed from a network, the capacity region does not change too much. Strong converses state that, for rates outside the capacity region, the probability of error converges to 1. Various notions of edge removal and strong converse are defined, depending on how edge capacity and <b>residual</b> <b>error</b> <b>probability</b> scale with blocklength, and relations between them are proved. In particular, each class of strong converse implies a specific class of edge removal. The opposite directions are proved for deterministic networks. Furthermore, a technique based on a novel, causal version of the blowing-up lemma is used to prove that for discrete memoryless stationary networks, the weak edge removal property [...] -that the capacity region changes continuously as the capacity of an edge vanishes [...] -is equivalent to the exponentially strong converse [...] -that outside the capacity region, the probability of error goes to 1 exponentially fast. This result is used to prove exponentially strong converses for several examples [...] -including the cut-set bound and the discrete 2 -user interference channel with strong interference [...] -with only a small variation from traditional weak converse proofs. Comment: (v 2) More consistent definitions of edge removal and strong converse variants, revised proofs, submitted to IEEE Trans. on Information Theor...|$|E
40|$|International Telemetering Conference Proceedings / September 27 - 29, 1971 / Washington Hilton Hotel, Washington, D. C. The use {{of digital}} {{transmission}} with narrow light pulses appears attractive for data communications, but {{carries with it}} a stringent requirement on system bit timing. In this paper we investigate the effects of imperfect timing in a direct detection (noncoherent) optical system using PPM bits. Particular {{emphasis is placed on}} specification of timing accuracy, and an examination of system degradation when this accuracy is not attained. Bit <b>error</b> <b>probabilities</b> are shown as a function of timing errors, from which average <b>error</b> <b>probabilities</b> can be computed for specific synchronization methods. Of significant importance is the presence of a <b>residual,</b> or irreducible <b>error</b> <b>probability,</b> due entirely to the timing system, that cannot be overcome by the data channel...|$|R
40|$|The filter {{produces}} the maximum likelihood estimates for average {{values of the}} spacecraft position and attitude errors during a single scene. The quality of the filter performance {{is characterized by the}} maximal cross and along track <b>residual</b> <b>errors</b> for which <b>probability</b> distributions can be calculated analytically for a given pattern of control points. The filter with an automatic selection of the best set of estimates provides geodetic correction at 90 % of pixels with <b>residual</b> <b>errors</b> less than 40 m for four or more control points and the mean squared measurement errors of the order of 20 - 25 m. The same accuracy can be preserved for eight or more control points and measurement errors of 30 - 35 m...|$|R
40|$|Algebraic {{decoding}} algorithms {{are commonly}} {{applied for the}} decoding of Reed-Solomon codes. Their main advantages are low computational complexity and predictable decoding capabilities. Many algorithms can be extended for correction of both errors and erasures. This enables the decoder to exploit binary quantized reliability information obtained from the transmission channel: Received symbols with high reliability are forwarded to the decoding algorithm while symbols with low reliability are erased. In this paper we investigate adaptive single-trial error/erasure decoding of Reed-Solomon codes, i. e. we derive an adaptive erasing strategy which minimizes the <b>residual</b> codeword <b>error</b> <b>probability</b> after decoding. Our result is applicable to any error/erasure decoding algorithm as long as its decoding capabilities can be expressed by a decoder capability function. Examples are Bounded Minimum Distance decoding with the Berlekamp-Massey- or the Sugiyama algorithms and the Guruswami-Sudan list decoder. Comment: Accepted for the 2011 Canadian Workshop on Information Theory, Kelowna, BC, Canada, May 17 - 20, 2011. 5 pages, 4 figure...|$|R
40|$|The symbol error {{performance}} of CD 900 -like digital {{cellular mobile radio}} systems over narrowband and urban wideband transmission channels was investigated. The basic performance is presented for Gaussian, flat-fading Rayleigh, and log-normal channels {{in the presence of}} selection and ratio combining space diversity schemes. For wideband channels having more than one resolvable fading path, a CD 900 -like system without diversity reception suffers from large <b>residual</b> symbol <b>error</b> <b>probabilities</b> PR(â‰ˆ 10 - 1). The introduction of adaptive correlation diversity (ACD) mitigates the effects of multipath, yielding a PR of 6 Ã— 10 - 5. Although this PR value is relatively low, the <b>probability</b> of symbol <b>error</b> (Pe) versus signal-to-noise ratio (SNR) is significantly poorer than for the Gaussian channel. By combining the ACD scheme with space diversity, the PR is eliminated by Pe > 10 - 5, and the channel SNR is within 5 dB of the Gaussian channel performance when Pe is 10 - 10. link_to_subscribed_fulltex...|$|R
40|$|Traditionally, multi-trial error/erasure {{decoding}} of Reed-Solomon (RS) codes {{is based}} on Bounded Minimum Distance (BMD) decoders with an erasure option. Such decoders have error/erasure tradeoff factor L= 2, which means that an error is twice as expensive as an erasure {{in terms of the}} code's minimum distance. The Guruswami-Sudan (GS) list decoder can be considered as state of the art in algebraic decoding of RS codes. Besides an erasure option, it allows to adjust L to values in the range 1 = 1 times. We show that BMD decoders with z_BMD decoding trials can result in lower <b>residual</b> codeword <b>error</b> <b>probability</b> than GS decoders with z_GS trials, if z_BMD is only slightly larger than z_GS. This is of practical interest since BMD decoders generally have lower computational complexity than GS decoders. Comment: Accepted for the 2011 IEEE International Symposium on Information Theory, St. Petersburg, Russia, July 31 - August 05, 2011. 5 pages, 2 figure...|$|R
40|$|We {{investigate}} adaptive single-trial error/erasure decoding of binary codes whose decoder is able {{to correct}} e errors and t erasures if le+t<=d- 1. Thereby, d is the minimum Hamming distance of the code and 1 <l<= 2 is the tradeoff parameter between errors and erasures. The error/erasure decoder allows to exploit soft information by treating a set of most unreliable received symbols as erasures. The obvious question here is, how this erasing should be performed, i. e. how the unreliable symbols which must be erased to obtain the smallest possible <b>residual</b> codeword <b>error</b> <b>probability</b> are determined. In a previous paper, we answer this question for the case of fixed erasing, where only the channel state and not the individual symbol reliabilities are taken into consideration. In this paper, we address the adaptive case, where the optimal erasing strategy is determined for every given received vector. Comment: Submitted to the 2010 International Symposium on Information Theory and its Applications, Taichung, Taiwan, October 17 - 20, 2010. 6 pages, 4 figure...|$|R
40|$|We {{investigate}} threshold-based multi-trial decoding of concatenated codes with {{an inner}} Maximum-Likelihood decoder and an outer error/erasure (L+ 1) /L-extended Bounded Distance decoder, i. e. a decoder which corrects e errors and t erasures if e(L+ 1) /L + t <= d - 1, where d is the minimum {{distance of the}} outer code and L is a positive integer. This is a generalization of Forney's GMD decoding, which was considered only for L = 1, i. e. outer Bounded Minimum Distance decoding. One important example for (L+ 1) /L-extended Bounded Distance decoders is decoding of L-Interleaved Reed-Solomon codes. Our main contribution is a threshold location formula, which allows to optimally erase unreliable inner decoding results, for a given number of decoding trials and parameter L. Thereby, the term optimal means that the <b>residual</b> codeword <b>error</b> <b>probability</b> of the concatenated code is minimized. We give an estimation of this probability {{for any number of}} decoding trials. Comment: Accepted for the 2010 IEEE International Symposium on Information Theory, Austin, TX, USA, June 13 - 18, 2010. 5 pages, 2 figure...|$|R
40|$|Analysis of {{transversely}} loaded cantilever shaft having multiple cracks is {{investigated in}} this paper. The behavior of cracks locations and sizes on vibration parameters is presented taking {{the advantage of}} reduction in stiffness of shaft from fracture mechanics. The shaft is subjected to static axial and bending load for given angle of twist along its longitudinal direction. Using the principle strain energy release rate and stress intensity factor (for plain stress and strain condition), the natural frequencies with their mode shapes at different crack locations with its depths are evaluated. The result obtained from theoretical method has been verified with the advantage of adaptive neuro-fuzzy inference system (ANFIS) using the modal parameter of cracked shaft. The vibration parameters such as first three non-dimensional natural frequencies with their mode shapes at different locations and depths are supplied to ANFIS to optimize the results. The crack location and size predicted from ANFIS model are verified with the theoretical data with acceptable error. The surface plot, <b>residual</b> <b>error,</b> and <b>probability</b> plot obtained from ANFIS are showing the effectiveness of theoretical method. Research work has been extended {{to set up an}} experi-mental model to strengthen both theoretical and ANFIS work. The paper basically focused on error percentage obtained from the theoretical result with ANFIS and experimental work. It is found that the error percentage in ANFIS and experimental analysis are 2 % and 7. 33 % respectively with respect to theoretical analysis. The present method is simple and can be easily extended to complex structure with different orientation of multiple cracks for any structural analysis...|$|R
30|$|After {{calculating the}} <b>residual</b> <b>errors</b> É›(t)â€² {{of the model}} AR(13), the <b>residual</b> <b>errors</b> É›(t)â€² needed to be {{verified}} whether they were the white noise. Finally, the test function of the white noise in MATLAB was used and the <b>residual</b> <b>errors</b> É›(t)â€² of the model AR(13) were verified to be the white noise. Thus the <b>residual</b> <b>errors</b> É›(t)â€² of the model AR(13) could replace the <b>residual</b> <b>errors</b> É›(t) of the model ARMA(6, 4).|$|R
40|$|In case of {{coordinate}} {{machines that}} use CAA correction matrix, {{the issue of}} kinematic errors analysis may {{be based on the}} determination of <b>residual</b> <b>error</b> distribution. Temperature changes have an impact on CMM kinematic structure, which may cause the differences in the map of <b>residual</b> <b>errors.</b> As for today, the <b>residual</b> <b>errors</b> were analysed only for the reference temperature. No research was undertaken on the <b>residual</b> <b>errors</b> changes depending on the temperature variations. This paper presents the experiment aimed at <b>residual</b> <b>errors</b> analysis and resulting errors distributions for different temperatures...|$|R
30|$|In Eq.Â (8), the <b>residual</b> <b>errors</b> É›(t) were {{unknown in}} the model ARMA (6, 4), so the time series fitting <b>residual</b> <b>errors</b> Î´ (t)^' were not {{calculated}} by the linear derivation. However, if the model order w was large enough, the model AR(w) could approximately substitute the model ARMA(p, q). And the <b>residual</b> <b>errors</b> É›(t)â€² of the model AR(w) could also replace the unknown <b>residual</b> <b>errors</b> É›(t) in the model ARMA (6, 4).|$|R
30|$|Step 3 : Update {{the weights}} {{of the student}} model by back {{propagating}} the <b>residual</b> <b>error,</b> until the <b>residual</b> <b>error</b> is small enough.|$|R
40|$|This paper {{discusses}} {{a realistic}} turbo coding system when the signal phase {{has not been}} perfectly estimated. We propose improved decoding algorithms for the situations when the <b>residual</b> phase <b>error</b> can be modelled by the Gaussian probability distribution and a Markov chain, a model {{which can be used}} in many actual phase estimators. It is shown that increasing the state space of the decoders can dramatically improve the bit <b>error</b> <b>probability</b> without the need of increasing transmitted power...|$|R
30|$|After the <b>residual</b> <b>errors</b> Î´(t)â€² of {{time series}} {{analysis}} in Eq.Â (12) were calculated, the <b>residual</b> <b>errors</b> Î´(t)â€² needed to be verified whether they were the white noise. Finally, the test function of the white noise in MATLAB was used to verify that the <b>residual</b> <b>errors</b> Î´(t)â€² of the {{time series analysis}} were the white noise. Thus, combining with the <b>residual</b> <b>errors</b> Î´(t)â€² of the time series analysis and the fitting value of the multiple linear model yM(t), the comprehensive compensation model yC(t) of the thermal errors of the wear-depth detecting system was obtained.|$|R
40|$|<b>Residual</b> <b>error</b> {{evaluation}} method for deterministic polishing of aspheric optics is studied. Two <b>residual</b> <b>error</b> {{evaluation method}}s, which are axis-direction error method and normal error method respectively, are researched theoretically. It's inferred that the <b>residual</b> <b>error</b> of aspheric surface {{should be evaluated}} by normal error method. A new approach is proposed to calculate normal direction <b>residual</b> <b>error</b> {{on the basis of}} the axis-direction <b>residual</b> <b>error</b> of the aspheric surface. There exists difference between these two kinds of error which increases from the center of the aspheric optic to the edge through the comparison of them. Taking bonnet polishing and numerical controlled small tool polishing as examples, experiments are made to quantitatively prove that using axis-direction error method to evaluate <b>residual</b> <b>error</b> in deterministic polishing would introduce different degrees of processing error. It's found that the processing error is positively correlated with the relative aperture of aspheric optics, which is the ratio of the optic's aperture and vertex's curvature radius. Therefore, it is recommended to use axis-direction error method instead of normal error method as the evaluation method of the <b>residual</b> <b>error</b> during deterministic polishing aspheric optics with relatively small relative aperture; the opposite is the other way around. ? 2014 Journal of Mechanical Engineering...|$|R
50|$|In {{order to}} use Peirce's criterion, one must first {{understand}} the input and return values. Regression analysis (or the fitting of curves to data) results in <b>residual</b> <b>errors</b> (or {{the difference between}} the fitted curve and the observation points). Therefore, each observation point has a <b>residual</b> <b>error</b> associated with a fitted curve. By taking the square (i.e., <b>residual</b> <b>error</b> raised to the power of two), <b>residual</b> <b>errors</b> are expressed as positive values. If the squared error is too large (i.e., due to a poor observation) it can cause problems with the regression parameters (e.g., slope and intercept for a linear curve) retrieved from the curve fitting.|$|R
30|$|According to this observation, it is {{concluded}} that the optimal block size can be predicted based on the MB <b>residual</b> <b>error.</b> If the <b>residual</b> <b>error</b> is small when ME for large block is performed, a large block size {{can be used as}} the optimal mode. In contrast, if the <b>residual</b> <b>error</b> is not small enough to select a large block, smaller block sizes are considered to determine the optimal block size mode.|$|R
30|$|The {{average of}} <b>residual</b> <b>errors</b> for {{different}} reconstruction algorithm was also listed in TableÂ  4. As previously indicated, TOF reconstructions could improve cold contrast by about 15 % and therefore exhibited significant smaller <b>residual</b> <b>error</b> compared with non-TOF reconstructions.|$|R
5000|$|Since [...] has at most 0.1 <b>error</b> <b>probability,</b> [...] {{can have}} at most 0.2 <b>error</b> <b>probability.</b>|$|R
40|$|A speech coding {{algorithm}} was developed {{which was based}} on a new method of selecting the excitation signal from a codebook of <b>residual</b> <b>error</b> sequences. The <b>residual</b> <b>error</b> sequences in the codebook were generated from 512 frames of real speech signals. L. P. C. inverse filtering was used to obtain the residual signal. Each <b>residual</b> <b>error</b> signal was assigned an index. The index was generated using a moments algorithm. These indices were stored on a Graded Binary Tree. A Binary Search was then used to select the correct index. The use of a Graded Binary Tree in the {{coding algorithm}} reduced the search time. The algorithm faithfully reproduced the original speech when the test <b>residual</b> <b>error</b> signal was chosen from the training data. When the test <b>residual</b> <b>error</b> signal was outside the training data, synthetic speech of a recognisable quality was produced. Finally, the fundamentals of speech coders are discussed in detail and various developments are suggested...|$|R
30|$|Once object {{images taken}} from {{different}} perspectives are imported to PhotoModeler, building the model involves human interaction, {{to pick and}} match feature points in different photos. Human error in the modeling process is accessed in terms of <b>residual</b> <b>error.</b> The computational algorithm used in PhotoModeler, called bundle adjustment, essentially applies the colliearity equations (Eqs. 1 and 2) to simultaneously fix (1) camera orientations, (2) object and image point coordinates and (3) the <b>residual</b> <b>error,</b> with an objective to minimize the <b>residual</b> <b>error.</b>|$|R
30|$|Compute the <b>residual</b> <b>error</b> {{estimator}} Î·_h_k.|$|R
3000|$|... for SA {{is still}} {{significantly}} less than the <b>error</b> <b>probability</b> of the conventional methods. The exact <b>error</b> <b>probability</b> [...]...|$|R
40|$|Introduction: In {{pharmacokinetic}} modelling, {{a combined}} proportional and additive <b>residual</b> <b>error</b> model is often preferred over a proportional or additive <b>residual</b> <b>error</b> model. Different approaches have been proposed, but {{a comparison between}} approaches is still lacking. Methods: The theoretical background of the methods is described. Method VAR assumes that the variance of the <b>residual</b> <b>error</b> {{is the sum of}} the statistically independent proportional and additive components; this method can be coded in three ways. Method SD assumes that the standard deviation of the <b>residual</b> <b>error</b> is the sum of the proportional and additive components. Using datasets from literature and simulations based on these datasets, the methods are compared using NONMEM. Results: The different coding of methods VAR yield identical results. Using method SD, the values of the parameters describing <b>residual</b> <b>error</b> are lower than for method VAR, but the values of the structural parameters and their inter-individual variability are hardly affected by the choice of the method. Conclusion: Both methods are valid approaches in combined proportional and additive <b>residual</b> <b>error</b> modelling, and selection may be based on OFV. When the result of an analysis is used for simulation purposes, it is essential that the simulation tool uses the same method as used during analysis...|$|R
3000|$|... [...]. The {{unconditional}} <b>error</b> <b>probability</b> {{is obtained}} by averaging {{over all the}} conditional <b>error</b> <b>probabilities</b> corresponding to the channel realizations.|$|R
3000|$|... -PAM. The {{probability}} {{distribution of the}} displacement vector in (19) is not circularly symmetric over given set of symbol pair x+ and xâˆ’ thus the average <b>error</b> <b>probability</b> along the horizontal and vertical directions are not exactly the same. Therefore, both horizontal and vertical PAM <b>error</b> <b>probabilities</b> need to be evaluated. The individual <b>error</b> <b>probabilities</b> along both directions depend on the marginal densities. Next, the <b>error</b> <b>probability</b> on horizontal vertical PAM can be considered.|$|R
40|$|The undetected <b>error</b> <b>probability</b> of {{a linear}} code {{used to detect}} errors on a {{symmetric}} channel {{is a function of}} the symbol <b>error</b> <b>probability</b> " of the channel and involves the weight distribution of the code. The code is proper, if the undetected <b>error</b> <b>probability</b> increases monotonously in "...|$|R
