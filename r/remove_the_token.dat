1|10000|Public
50|$|In {{that same}} year, The Charlton Press also {{published}} a stand alone catalogue of Canadian Colonial Tokens (prior to 1867). The standalone catalogue with 218 pages was researched extensively after {{the decision was}} made to <b>remove</b> <b>the</b> <b>token</b> and paper money content from the catalogue in 1976. Tokens were issued by provincial governments, chartered banks and merchants, due to the shortage of copper currency.|$|E
5000|$|... <b>removing</b> some of <b>the</b> <b>tokens</b> and {{dividing}} <b>the</b> remaining <b>tokens</b> {{into two}} nonempty heaps.|$|R
50|$|With Cross at the helm, and Charlton {{assuming}} {{an editorial}} {{position with the}} catalogues, the catalogue increased in size from 200 pages to 341 pages between the years of 1971 and 1978. In 1976, {{a decision was made}} to publish a more reader-friendly format that would split the coin catalogue by <b>removing</b> <b>the</b> <b>token</b> and paper money content to devote to two separate books, lest collectors be faced with an unnecessarily thick publication.|$|R
40|$|In {{the game}} Knock 'm Down, tokens {{are placed in}} N bins. At {{each step of the}} game, a bin is chosen at random {{according}} to a xed probability distribution. If a token remains in that bin, it is <b>removed.</b> When all <b>the</b> <b>tokens</b> have been <b>removed,</b> <b>the</b> player is done. In the solitaire version of this game, the goal is to minimize the expected number of moves needed to <b>remove</b> all <b>the</b> <b>tokens.</b> Here we present necessary conditions on <b>the</b> number of <b>tokens</b> needed for each bin in an optimal solution, leading to an asymptotic solution. MR Subject Classications: primary: 91 A 60...|$|R
50|$|The Coombe Junction {{signalman}} set up {{the route}} to the platform line, the train ran past the box and handed off <b>the</b> <b>token.</b> <b>The</b> engine ran round its train. The signalman now lowered his signals for the train to go towards Liskeard. The train pulled out until the rear was clear of the platform / Moorswater siding points, the route was reset and the train set back onto the siding. By this time the 9.45 Looe {{had arrived at the}} home signal. The route was set for the platform and the signals lowered. <b>The</b> <b>token</b> was handed off as the engine passed. The signalman put it into the instrument and gave 'Train out of Section' and at once 'asked clear' and <b>removed</b> <b>the</b> <b>token</b> for <b>the</b> 9.55 Liskeard. Once the Liskeard Looe train had gone, the engine on the Looe Liskeard train could run round its train, the signalman got out <b>the</b> <b>token</b> for <b>the</b> Liskeard line, set the road and the train left.|$|R
50|$|In 1989 the box {{was taken}} out of use, with the signal heads being <b>removed</b> and <b>the</b> <b>token</b> {{instrument}} from Tan-y-Bwlch, and the although trap points (from Minffordd) were installed at the Tan-y-Bwlch end of the loop, they were never commissioned. During this period, the ground frame was only usable as a refuge siding, and {{it was not possible to}} cross passenger trains.|$|R
50|$|When Ethernet became Fast Ethernet, it {{continued}} to use the Carrier Sense Multiple Access With Collision Detection (CSMA/CD) mechanism to manage traffic on the network cable. 100VG took advantage of <b>the</b> <b>token</b> passing concept that made ARCNET and Token Ring popular {{in order to provide}} consistent performance no matter how large the network became. It <b>removed</b> <b>the</b> <b>token</b> passing responsibility from the wiring and network nodes and placed it internal to the 100VG-AnyLAN hubs. These hubs contained <b>the</b> rotating <b>token</b> that never left the hub itself. When a node wanted to transmit data, it would raise a bit on its hub port connection that indicating to the hub that it was ready. As <b>the</b> <b>token</b> passed by a ready hub port, it would then open up traffic to that node. Because <b>the</b> <b>token</b> stayed within <b>the</b> hub, it did not have to traverse long cables going to every node as in ARCNET and Token Ring therefore becoming faster than those other deterministic networking standards and being less susceptible to cabling problems, network card failures, and line interference. Real-life load testing showed 100VG-AnyLAN reaching 95% of its theoretical network speed instead of about 45% as in Fast Ethernet when using hubs. Fast Ethernet switches were not commonplace at first because of high cost and limited availability so, initially, 100VG had a significant performance advantage.|$|R
5000|$|Now {{consider}} a special purple <b>token</b> bearing <b>the</b> number [...] "100", {{which may be}} removed by either player, who then replaces <b>the</b> purple <b>token</b> with 100 tokens of their own color. (In the notation of Conway, <b>the</b> purple <b>token</b> is <b>the</b> game {100|−100}.) <b>The</b> purple <b>token</b> is a [...] "hot" [...] component, because it is highly advantageous to be <b>the</b> player who <b>removes</b> <b>the</b> purple <b>token.</b> Indeed, {{if there are any}} purple <b>tokens</b> on <b>the</b> table, players will prefer to remove them first, leaving the red or blue tokens for last. In general, a player will always prefer to move in a hot game rather than a cold game, because moving in a hot game improves their position, while moving in a cold game injures their position.|$|R
50|$|The leaky bucket as a meter {{is exactly}} {{equivalent}} to (a mirror image of) <b>the</b> <b>token</b> bucket algorithm, i.e. {{the process of}} adding water to the leaky bucket exactly mirrors that of <b>removing</b> <b>tokens</b> from <b>the</b> <b>token</b> bucket when a conforming packet arrives, the process of leaking {{of water from the}} leaky bucket exactly mirrors that of regularly adding <b>tokens</b> to <b>the</b> <b>token</b> bucket, and <b>the</b> test that the leaky bucket will not overflow is a mirror of the test that <b>the</b> <b>token</b> bucket contains enough tokens and will not 'underflow'. Thus, given equivalent parameters, the two algorithms will see the same traffic as conforming or nonconforming. The leaky bucket as a queue {{can be seen as a}} special case of the leaky bucket as a meter.|$|R
30|$|After <b>the</b> <b>tokens</b> are classified, {{we apply}} some {{translation}} rules (defined by LIBRAS specialists) to translate these tokens (or words) for a representation in gloss notation. Initially, we simplify <b>the</b> text by <b>removing</b> some <b>tokens</b> (<b>the</b> <b>Remove</b> <b>Tokens</b> step). We chose this step because LIBRAS does not define prepositions and articles. Thus, these classes of tokens can be removed. Afterwards, some tokens (or words) are replaced (the Lexical Replacement step) {{in order to}} adapt {{the meaning of the}} sentence rewritten to LIBRAS, since the LIBRAS vocabulary is smaller than BP’s [36]. For example, the words HOME, HOUSE, HABITATION in BP have the same sign (i.e., the same visual representation) in LIBRAS, the HOME sign. Furthermore, while the BP verbs have a high degree of inflection, the LIBRAS verbs do not inflect. In this case, the BP verbs are replaced by non-inflected gloss verbs (i.e., the LIBRAS verbs). To do this replacement, we use a set of BP to LIBRAS synonyms (BP-LIBRAS dictionary). Finally, proper names and technical terms are spelled in LIBRAS [by handshapes that represent the letters of <b>the</b> <b>token</b> (word)]. Thus, we also apply a dactylology replacement to spell proper names and technical terms. The output generated is a representation in LIBRAS gloss notation.|$|R
60|$|Dick's heart sank, for {{the object}} in {{question}} was a tassel from his own girdle; and it was plain to him that this dwarfish spy, who took a malign delight in his employment, would lose no time in bearing it to his master, the baron. He was half-tempted to throw aside the arras, fall upon the scoundrel, and, {{at the risk of}} his life, <b>remove</b> <b>the</b> tell-tale <b>token.</b> And while he was still hesitating, a new cause of concern was added. A voice, hoarse and broken by drink, began to be audible from the stair; and presently after, uneven, wandering, and heavy footsteps sounded without along the passage.|$|R
5000|$|The {{staff and}} ticket system {{was still too}} {{inflexible}} for busy lines, as it did not allow for the situation where the train intended to carry <b>the</b> actual <b>token</b> was cancelled or running very late. To provide for this, <b>the</b> electric train <b>token</b> system was developed. Each single-line section is provided {{with a pair of}} token instruments, one at the signal box at each end. A supply of identical tokens is stored in the instruments, which are connected by telegraph lines. A token can be removed from one instrument only if both signalmen co-operate in agreeing to the release. Once a token has been removed, another cannot be <b>removed</b> until <b>the</b> <b>token</b> which is [...] "out" [...] is replaced in either instrument. (There are variations on this sequence of events.) By this means, it can be ensured that at any one time, only one token is available to be issued to a driver. Tokens belonging to adjacent sections have different configurations to prevent them being inserted into the wrong instrument.|$|R
40|$|In {{this paper}} we propose Instance Filtering as {{preprocessing}} step for supervised classification-based learning systems for entity recognition. The goal of Instance Filtering is to reduce both the skewed class distribution and the data set size by eliminating negative instances, while preserving positive ones as much as possible. This process is performed on both the training and test set, with the effect of reducing the learning and classification time, while maintaining or improving the prediction accuracy. We performed a comparative study on a class of Instance Filtering techniques, called Stop Word Filters, that simply <b>remove</b> all <b>the</b> <b>tokens</b> belonging {{to a list of}} stop words. We evaluated our approach on three different entity recognition tasks (i. e. Named Entity, Bio-Entity and Temporal Expression Recognition) in English and Dutch, showing that both the skewness and the data set size are drastically reduced. Consequently, we reported an impressive reduction of the computation time required for training and classification, while maintaining (and sometimes improving) the prediction accuracy. 1...|$|R
30|$|The RuleAnalyzer class {{applies the}} {{translation}} rules, {{developed by a}} human specialist, to <b>the</b> sequence of <b>tokens</b> and uses a BP to LIBRAS dictionary to do the lexical replacement step. By excluding the rules from <b>the</b> <b>Remove</b> <b>Tokens,</b> Lexical Replacement and Dactylology Replacement steps, nine high-level translation rules developed by the human specialists were taken into account. The translation rules are loaded from a file and are specified using an XML representation that allows LIBRAS specialists to easily add new rules and also modify or remove previously defined ones. Thus, {{it is possible to}} extend the set of translation rules in a simple way, just editing that file.|$|R
40|$|The 2 -player {{impartial}} game of Wythoff Nim {{is played}} on two piles of tokens. A move consists in removing {{any number of}} tokens from precisely one of the piles or {{the same number of}} tokens from both piles. The winner is <b>the</b> player who <b>removes</b> <b>the</b> last <b>token.</b> We study this game with a blocking maneuver, that is, for each move, before the next player moves the previous player may declare at most a predetermined number, k - 1 > 0, of the options as forbidden. When the next player has moved, any blocking maneuver is forgotten and does not have any further impact on the game. We resolve the winning strategy of this game for k = 2 and k = 3 and, supported by computer simulations, state conjectures of the asymptotic `behavior' of the P-positions for the respective games when 4 < k < 20. Comment: 14 pages, 1 Figur...|$|R
5000|$|When node j {{receives}} <b>the</b> <b>token</b> from k, it forwards <b>the</b> <b>token</b> to i and i is <b>removed</b> from <b>the</b> queue of j ...|$|R
40|$|Hedge {{detection}} is used {{to distinguish}} uncertain information from facts, which is of essential importance in biomedical information extraction. The task of hedge detection is often divided into two subtasks: detecting uncertain cues and their linguistic scope. Hedge scope is a sequence of <b>tokens</b> including <b>the</b> hedge cue in a sentence. Previous hedge scope detection methods usually take all tokens in a sentence as candidate boundaries, which inevitably generate {{a large number of}} negatives for classifiers. The imbalanced instances seriously mislead classifiers and result in lower performance. This paper proposes a dependency-based candidate boundary selection method (DCBS), which selects <b>the</b> most likely <b>tokens</b> as candidate boundaries and <b>removes</b> <b>the</b> exceptional <b>tokens</b> which have less potential to improve the performance based on dependency tree. In addition, we employ the composite kernel to integrate lexical and syntactic information and demonstrate the effectiveness of structured syntactic features for hedge scope detection. Experiments on the CoNLL- 2010 Shared Task corpus show that our method achieves 71. 92 % F 1 -score on the golden standard cues, which is 4. 11 % higher than the system without using DCBS. Although the candidate boundary selection method is only evaluated on hedge scope detection here, it can be popularized to other kinds of scope learning tasks...|$|R
5000|$|When {{the frame}} {{gets back to}} the originator, it sees that <b>the</b> <b>token</b> has been changed to 0 and that the message has been copied and received. It <b>removes</b> <b>the</b> message from the frame.|$|R
5000|$|<b>The</b> Book <b>Tokens</b> {{scheme was}} {{established}} in 1932, by publisher Harry Raymond. The original format of <b>the</b> <b>tokens</b> was as [...] "lick-and-stick" [...] stamp-like vouchers, which were glued into gift cards {{and had to be}} <b>removed</b> by <b>the</b> bookseller redeeming <b>the</b> <b>token.</b> In <b>the</b> 1990s, this design was changed to a [...] "currency-style" [...] voucher, available {{in a number of different}} denominations.|$|R
50|$|Seeing {{the train}} staff {{provided}} assurance {{that there could}} be no head-on collision. To ensure that the ticket is not issued incorrectly, a book of numbered tickets is kept in a locked box, the key to which is permanently fastened to <b>the</b> <b>token,</b> or is <b>the</b> <b>token.</b> In addition, <b>the</b> lock prevents <b>the</b> <b>token</b> being <b>removed</b> until <b>the</b> ticket box is closed, and it cannot be closed unless the book of tickets is in the box. Once a ticket is issued, its number is recorded in a Train Register book, and <b>the</b> <b>token</b> is locked in a secure place. This system is known as staff and ticket.|$|R
5000|$|The {{single journey}} tickets of Fuzhou Metro are red plastic RFID <b>tokens.</b> <b>The</b> logo of Fuzhou Metro {{and the words}} [...] "Metro Fuzhou" [...] are shown {{in the center of}} <b>the</b> <b>tokens.</b> <b>The</b> English and Chinese words of [...] "Fuzhou Urban Rail Transit" [...] are shown on the edges of <b>the</b> <b>tokens.</b> <b>The</b> tickets can be {{purchased}} from the automatic ticketing machines. When entering the stations, passengers tap <b>the</b> <b>token</b> against a scanner. When exiting, <b>the</b> <b>token</b> is inserted into a slot for recycling.|$|R
50|$|Transactions {{for trading}} {{needed to be}} {{accounted}} for efficiently, so <b>the</b> clay <b>tokens</b> were placed in a clay ball (bulla) to keep <b>the</b> <b>tokens</b> together. This helped with dishonesty and kept all <b>the</b> <b>tokens</b> together. In order to account for <b>the</b> <b>tokens,</b> <b>the</b> bulla {{would have to be}} crushed to reveal their content. This introduced the idea of impressing <b>the</b> <b>token</b> onto <b>the</b> wet bulla before it dried,to insure trust that <b>the</b> <b>tokens</b> hadn't been tampered with and for anyone to know what exactly was in the bulla without having to break it. Eventually seals were impressed into the clay alongside of the impression of <b>the</b> <b>tokens.</b> Each party had its own unique seal to identify them. Seals would not only identify individuals, but it would also identify their office.|$|R
50|$|As <b>the</b> <b>token</b> is {{increasingly}} transmitted or {{passed through the}} network, it becomes increasingly punctualized and also increasingly reified. When <b>the</b> <b>token</b> is decreasingly transmitted, or when an actor fails to transmit <b>the</b> <b>token</b> (e.g., <b>the</b> oil pump breaks), punctualization and reification are decreased as well.|$|R
50|$|Finally, {{note that}} guard {{conditions}} can ”peek” at <b>the</b> incoming <b>tokens</b> without actually consuming them — if the guards {{happen to be}} false or the action is not fired for some other reason, and if <b>the</b> <b>token</b> is not consumed by another action, then it remains where it is, and {{will be available for}} the next firing. (Or it will remain there forever, {{as in the case of}} <b>the</b> zero <b>token</b> in front of SplitDead, which is never <b>removed</b> because <b>the</b> actor is dead.)The Select actor below is another example of the use of guarded actions. It is similar to the NDMerge actor in the sense that it merges two streams (the ones arriving at its A and B input ports). However, it does so according to the (Boolean) values of <b>the</b> <b>tokens</b> arriving at its S input port.|$|R
50|$|Tokenization differs in that a user {{receives}} a token {{which can be}} used to directly access the back-end web/application servers. In this architecture the authentication occurs through the web access management tool but all data flows around it. This <b>removes</b> <b>the</b> network bottlenecks caused by proxy-based architectures. One of the drawbacks is that the back-end web/application server must be able to accept <b>the</b> <b>token</b> or otherwise <b>the</b> web access management tool must be designed to use common standard protocols.|$|R
30|$|The {{reliable}} transfer of tokens from one node to another {{is important for}} successful operation of EZ-AG. If a token is released by a node, but the intended recipient did not receive <b>the</b> <b>token</b> reply message, <b>the</b> <b>token</b> is lost. Reliability of token transfer can be imposed by requiring an acknowledgement from the node receiving <b>the</b> <b>token</b> and re-sending <b>the</b> <b>token</b> if an acknowledgement was not received. However, {{it is possible that}} <b>the</b> <b>token</b> was transferred correctly to a neighbor but the acknowledgement was lost or the recipient of <b>the</b> <b>token</b> moved away from the communication range of a sender. In this case, a duplicate token may be created by this process. But, since EZ-AG computes duplicate insensitive aggregates, the addition of a duplicate token will not impact the accuracy.|$|R
5000|$|The token-based {{topology}} {{works by}} using a token {{to provide access to}} the physical media. In a token-based network, there is a token that travels around the network. When a system needs to send out packets, it grabs <b>the</b> <b>token</b> off of <b>the</b> wire, attaches it to the packets that are sent, and sends it back out on the wire. As <b>the</b> <b>token</b> travels around <b>the</b> network, each system examines <b>the</b> <b>token.</b> When <b>the</b> packets arrive at the destination systems, those systems copy the information off of the wire and <b>the</b> <b>token</b> continues its journey until it gets back to the sender. When the sender receives <b>the</b> <b>token</b> back, it pulls <b>the</b> <b>token</b> off of <b>the</b> wire and sends out a new empty token to be used by the next machine.|$|R
5000|$|For every site Sj whose ID {{is not in}} <b>the</b> <b>token</b> queue, it appends its ID to <b>the</b> <b>token</b> queue if RNij =LNj+1 ...|$|R
5000|$|... if <b>the</b> <b>token</b> queue [...] is nonempty {{after this}} update, it pops a process ID [...] from [...] and sends <b>the</b> <b>token</b> to ...|$|R
5000|$|... if process [...] has <b>the</b> <b>token</b> {{and is not}} in CS, and if [...] (indicating an {{outstanding}} request), it sends <b>the</b> <b>token</b> to process ...|$|R
5000|$|... both {{processes}} use an in band {{method to}} transmit <b>the</b> <b>tokens,</b> with an {{out of band}} response mechanism whereby the account holder re-keys in <b>the</b> <b>token</b> value to a new mobile, webpage or [...]app. This mitigates {{man in the middle}} and boy in the browser attacks with regards to <b>the</b> <b>token</b> being intercepted.|$|R
40|$|The man of adamant (in Arcturus, Feb. 1842) [...] The Canterbury pilgrims (in Arcturus, March 1842) [...] The snow-image: a childish miracle (in International monthly, Oct. 1850) [...] The {{wives of}} the dead (in <b>The</b> <b>Token,</b> 1832) [...] My kinsman, Major Molineux (in <b>The</b> <b>Token,</b> 1832) [...] Roger Malvin's burial (in <b>The</b> <b>Token,</b> 1832) Photocopy. Mode of access: Internet...|$|R
50|$|<b>The</b> <b>Token</b> (1829-1842) was an annual, {{illustrated}} gift book, containing stories, {{poems and}} other light and entertaining reading. In 1833, it became <b>The</b> <b>Token</b> and Atlantic Souvenir.|$|R
5000|$|If {{the queue}} of j is not empty after {{forwarding}} <b>the</b> <b>token</b> to i, j must issue {{a request to}} i {{in order to get}} <b>the</b> <b>token</b> back ...|$|R
40|$|In this paper, {{we present}} {{monotonicity}} {{properties of the}} Leaky Bucket (LB) input rate regulation scheme. We show that the asymptotic version of the inter-departure time decreases in the convex ordering as the size of <b>the</b> <b>token</b> pool decreases or as <b>the</b> <b>token</b> generation period increase. When measured by the coefficients of variations of the inter-departure times, the burstiness of the output traffic from the LB increases with the sizes of <b>the</b> <b>token</b> pool, and decreases with <b>the</b> <b>token</b> generation period. These results may prove useful in designing the LB...|$|R
5000|$|On October 19, 2009, Phil and Mitch Margo {{filed suit}} in Manhattan {{for the rights to}} <b>the</b> <b>Tokens</b> name. They claim in their filing that Henry Medress {{suggested}} the name. In a competing suit filed in California by Siegel, he claims Siegel, Medress and Sedaka released an album named Neil Sedaka and <b>the</b> <b>Tokens</b> previously. [...] On Sedaka's own website, there is a listing in his discography catalog for a 1958 release of Neil Sedaka and <b>the</b> <b>Tokens</b> as well as a second album, also during 1958, named Neil Sedaka and <b>the</b> <b>Tokens</b> and Coins.|$|R
