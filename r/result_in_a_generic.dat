6|10000|Public
40|$|Hebbian {{changes of}} {{excitatory}} synapses {{are driven by}} and further enhance correlations between pre- and postsynaptic activities. Hence, Hebbian plasticity forms a positive feedback loop {{that can lead to}} instability in simulated neural networks. To keep activity at healthy, low levels, plasticity must therefore incorporate homeostatic control mechanisms. We find in numerical simulations of recurrent networks with a realistic triplet-based spike-timing-dependent plasticity rule (triplet STDP) that homeostasis has to detect rate changes on a timescale of seconds to minutes to keep the activity stable. We confirm this <b>result</b> <b>in</b> <b>a</b> <b>generic</b> mean-field formulation of network activity and homeostatic plasticity. Our results strongly suggest the existence of a homeostatic regulatory mechanism that reacts to firing rate changes on the order of seconds to minutes...|$|E
40|$|Lagrangian {{techniques}} {{appropriate to}} a local calculation are used {{to show that a}} weak ordered magnetic field can <b>result</b> <b>in</b> <b>a</b> <b>generic</b> condensational mode in cluster cooling flows. However, thermal instability appears possible only if the conductivity is well below its Spitzer value, for all nonradial wavenumbers. Wavenumbers not subject to conductive damping are subject to buoyant oscillations. It is shown that when instability is present, lateral magnetic confinement of high thermal pressure regions in the plasma by radial magnetic field lines is responsible in at least equal measure with radially directed magnetic tension for the suppression of oscillations and the reappearance of local condensational modes. The general importance of even very modest magnetic fields for destabilizing thermal time scale perturbations is emphasized...|$|E
40|$|Environmental, {{commercial}} and societal {{developments in the}} Netherlands stimulate the environmental improvement of the existing office building stock. In the Netherlands, about 15 % of all office area was vacant in 2012, {{and the majority of}} offices have a relative poor energy performance. To measure the improvement, different assessment tools are applied. These tools either focus on one aspect, such as operation energy, and result in a specific outcome such as MJ/m 2, or these tools combine different aspects, such as energy and materials, through a weighted system and <b>result</b> <b>in</b> <b>a</b> <b>generic</b> outcome, such as ‘excellent’. In this research, the relation between assessment outcome and actual environmental impact is investigated of both types of tools, by reflecting the outcome of the tool to the carrying capacity of a system. The relation is investigated through a comparison of the energy and material aspect of three office façade renovation solutions using four different assessment tools. Using a tool in which energy and material impact is related to the carrying capacity, current energy focused optimization might lead to a sub optimization of actual environmental impact. To illustrate this, a calculated façade solution is presented with minimal environmental impact based on carrying capacity. </p...|$|E
40|$|The paper {{presents}} the {{architecture and building}} blocks for a digital CNN, extending the previous ILVA design with packet switched communication and an OSI-compliant functional structure. Locality of operation <b>results</b> <b>in</b> <b>a</b> <b>generic</b> VHDL description {{that can be easily}} compiled for the various demands of real-time image processing. Some experiments with 2 -neighborhood operations are shown...|$|R
40|$|There {{were three}} {{objectives}} of this study: (1) {{to identify and}} document patterns in operational audits currently being used in hotel housekeeping operations in North America; (2) to analyze and summarize the most important elements of these housekeeping audits; (3) to develop <b>a</b> <b>generic</b> hotel housekeeping audit which could be self-administered by employees and management. Qualitative methodologies including document content analysis, expert panel analysis and instrument field testing were used. The study <b>resulted</b> <b>in</b> <b>a</b> <b>generic,</b> comprehensive housekeeping operational audit...|$|R
40|$|Comparison {{of digital}} library browse methods In digital {{libraries}} users can search, i. e. enter a query {{and get a}} list of results, and browse, i. e. traverse links between documents, but they find {{a combination of the}} two most useful. With an analysis of the currently developed browse techniques on digital libraries, <b>resulting</b> <b>in</b> <b>a</b> <b>generic</b> browse system, we will propose the most usable combination of browse techniques. The generic browse system provides an overview of possible techniques and needed components for browsing...|$|R
40|$|Corresponding author: Ir. Michiel Ritzen, Eindhoven University of Technology/Zuyd University of Applied Sciences, The Netherlands. E-mail: m. j. ritzen@tue. nl Environmental, {{commercial}} and societal {{developments in the}} Netherlands stimulate the environmental improvement of the existing office building stock. In the Netherlands, about 15 % of all office area was vacant in 2012, {{and the majority of}} offices have a relative poor energy performance. To measure the improvement, different assessment tools are applied. These tools either focus on one aspect, such as operational energy, and result in a specific outcome such as MJ/m 2, or these tools combine different aspects, such as energy and materials, through a weighted system and <b>result</b> <b>in</b> <b>a</b> <b>generic</b> outcome, such as ‘excellent’. In this research, the relation between assessment outcome and actual environmental impact is investigated of both types of tools, by reflecting the outcome of the tool to the carrying capacity of a system. The relation is investigated through a comparison of the energy and material aspect of three office façade renovation solutions using four different assessment tools. Using a tool in which energy and material impact is related to the carrying capacity, current energy focused optimization might lead to a sub optimization of actual environmental impact. To illustrate this, a calculated façade solution is presented with minimal environmental impact based on carrying capacity...|$|E
40|$|Due to the {{character}} of the original source materials and the nature of batch digitization, quality control issues may be present in this document. Please report any quality issues you encounter to digital@library. tamu. edu, referencing the URI of the item. Circuit boards are integral parts of almost all electronic products, which are widespread in today's society. Companies utilizing circuit board assembly processes must remain competitive by using new technologies, shortening assembly cycle times, and decreasing costs. Optimization techniques involve process planning and lead to shortened cycle times and reduced costs. No generic workload-balancing tool is available to optimize the assembly process. This study is part of a larger, ATP-sponsored project that will create a tool of this sort by choosing and configuring the placement machines, allocating parts to appropriate machines, assigning parts to machine feeders, arranging part placement in sequential order, and sequentially scheduling boards on particular lines. My part of the project is to assign circuit boards to assembly lines and to sequence the boards on those lines. The project required that the following factors be considered: the different machine types, the number of machines on each line, the number of different circuit board types, the number of circuit boards of each type to be produced, the number of different component types, and the number of each component type on each circuit board type. Incorporating these considerations, the mathematical model assigns and schedules boards on assembly lines. AMPL, Advanced Mathematical Programming Language, uses script and data files to run with the model. Testing the model and analyzing the results proved that the model is a heuristic that is capable of scheduling printed circuit boards to assembly lines. The ATP-sponsored research project will <b>result</b> <b>in</b> <b>a</b> <b>generic</b> decision support tool for circuit board assembly optimization...|$|E
40|$|Today the Internet {{is mostly}} used for {{services}} that require low or none security. The commercial and governmental applications {{have started to}} emerge but met problems since they require strong authentication, which is both difficult and costly to realize. The SIM card used in mobile phones is a tamper resistant device that contains strong authentication mechanisms. It would be very convenient and cost-efficient if Internet services could use authentication methods based on the SIM. This master thesis presents an analysis and a design of a generic authentication system based on SIM, together with {{a detailed description of}} an implemented prototype. The proposed system, called the Generic SIM Authentication System (GAS), provides a strong authentication mechanism. The GAS builds upon the existing GSM authentication infrastructure, thus allows re-use of GSM expertise from the mobile operators. New services can easily be supported, such that these can benefit from strong authentication. By gradually implementing more authentication mechanisms (e. g. OTP and PKI) on the SIM, {{it will be able to}} support several levels of security. This will <b>result</b> <b>in</b> <b>a</b> <b>generic</b> authentication system satisfying the security needs for nowadays and also for the future. In order to design the GAS, the thesis starts by giving an overview of authentication and relevant technologies, before the requirements to the system, both functional and non-functional, are defined. Then different interaction diagrams, collaboration diagrams and sequence diagrams are presented, and the necessary components and interfaces in the system are outlined. This thesis builds on two student projects finished December 2005, where tentative high-level architectures for utilizing SIM-based authentication were proposed. A Prototype has been developed in Java to demonstrate the GAS, and includes both a client (Supplicant) and a server (Authenticator) part. The communication between the Supplicant and the other components in the authentication system is based on EAP, which is a general authentication protocol supporting multiple authentication methods. When performing the GSM authentication the EAP-SIM protocol is used. The Prototype has been tested end-to-end, i. e. from the SIM to the Telenor GSM HLR/AuC, via IP-based network. Three different services have been developed to demonstrate how easily the SIM authentication can be integrated. The first demo service shows how to integrate the authentication with JSP technology and Apache Tomcat. The second service, MyService, is another example of how the authentication service could be integrated into a web portal using PHP to demonstrate that the Prototype is independent of the service implementation language. MyService also illustrates how the service provider can control the registration of new users and link up with their SIM identity. The last service, GasSpot, shows how to integrate the GAS to authenticate users to a Captive Portal. The access is controlled by the gateway, which is implemented using ChilliSpot. Based on the results of the master thesis, the authors have written the paper 9 ̆ 3 A Generic Authentication System based on SIM 9 ̆ 4, which has been submitted and accepted for publication at the ICISP 9 ̆ 206 Conference in Cap Esterel, Côte d 9 ̆ 2 Azur, France, August 26 - 29, 2006...|$|E
40|$|Developers using {{software}} components {{need to be}} confident in their selection of the most suitable component. Manual searching is time consuming and unlikely {{to be able to}} consider large numbers of components. The Context-driven Component Evaluation (CdCE) project is investigating ways to use Artificial Intelligence to assist the selection process. This paper describes our Machine Learning approach where we train a system to recognise candidates that match an ideal component specification. We utilise automated test generation techniques to create data for training the system. This <b>results</b> <b>in</b> <b>a</b> <b>generic</b> assessment system that can automatically short-list components for further investigation...|$|R
40|$|The structural, electronic, and {{magnetic}} properties of Gd@C_ 82 endohedral metallofullerene {{have been studied}} by employing on-site correlation corrected, scalar-relativistic and full-relativistic density functional theory within the local density and generalized gradient approximations. The experimentally observed reduction of the magnetic moment of Gd@C_ 82 with respect {{to that of a}} free Gd^+ 3 ion can be explained by a tiny hybridization between unoccupied Gd- 4 f states and carbon-pi states, <b>resulting</b> <b>in</b> <b>a</b> <b>generic</b> antiferromagnetic coupling of the Gd- 4 f spin with the remaining unpaired spin in the hybridized molecular orbital. Comment: 15 pages, 2 figure...|$|R
40|$|Abstract. Exposing {{data about}} {{customizable}} products is a challenging issue, {{because of the}} number of features and options a customer can choose from, and the many constraints that exist between them. These constraints are not tractable without automatic reasoning. But the configuration process, which helps a customer to make her choice, one step at a time, is a traversal of a graph of partially defined products- that is, Linked Data. This natural yet fruitful abstraction for product customization <b>results</b> <b>in</b> <b>a</b> <b>generic</b> configuration API, <b>in</b> use at Renault, who has begun publishing data about its range in this way. Current achievements and prototypes of forthcoming developments are presented...|$|R
50|$|Entrances {{were all}} built as open stairwells, {{with the panel}} above the lintel emblazoned with the scales of justice, which {{referenced}} the Superior Court of Justice at Osgoode Hall. Subsequent refurbishment <b>resulted</b> <b>in</b> <b>a</b> <b>generic</b> TTC style replacing the unique symbolism. <b>In</b> 2006 <b>a</b> new entrance, with elevator access to the concourse level, was integrated into {{the construction of the}} Four Seasons Centre, at the southeast corner of Queen and University. Along with an elevator to the platform level within the fare paid area, this makes the station fully accessible since 2007. Current plans call for Diamond and Schmitt Architects, who were responsible for the opera house, to design complementary covered entrances at the other three corners of the intersection.|$|R
40|$|Many data sources, {{including}} web sites, do {{not support}} general query interfaces. The typical solution {{is to build a}} wrapper around the source which presents a general query interface to the underlying data by translating external queries to a form the local source understands, submitting the local query, and then repackaging the <b>results</b> <b>in</b> <b>a</b> <b>generic</b> way before returning them to the caller. This approach allows heterogeneous query processors to be built on top of underlying source over which one has no control. Building wrappers is a significant amount of effort, however, and current design tools yield code that is very brittle when confronted with real world problems: i. e., errors in source documents, changing formats, inadequate training sets, etc...|$|R
40|$|This is a {{refereed}} conference paper. This paper {{describes the}} methods and tools used by the TELSCAN project to identify the requirements of elderly and disabled (E&D) travellers with various types of functional impairments. A Definition of the Travelling Task for different modes of private and public transport was used to capture the needs of E&D travellers through interviews with experts and focus groups with users. The data collection <b>resulted</b> <b>in</b> <b>a</b> <b>generic</b> specification of user requirements, in general and those specific to telematics, for elderly and disabled travellers. This methodology and data can guide the design of ITS and can provide a stepping stone to capture system-specific data {{to ensure that the}} needs of elderly and disabled people are included in the design process...|$|R
40|$|In {{this paper}} {{we present a}} method for using generic {{components}} in formal speci�cations. This approach <b>results</b> <b>in</b> <b>a</b> �exible <b>generic</b> system description that separates the concerns of structure and data types. The generic speci�cation can be extended and modi�ed <b>in</b> <b>a</b> natural manner, to track requirements a...|$|R
40|$|Abstract. A new IFLP schema is {{presented}} as a general framework for theinductionoffunctionallogicprograms(FLP). Sincenarrowing(which is the most usual operational semantics of FLP) performs a unification (mgu) followed by a replacement, we introduce two main operators in our IFLP schema: a generalisation and an inverse replacement or intrareplacement, which <b>results</b> <b>in</b> <b>a</b> <b>generic</b> inversion of the transitive property of equality. We prove that this schema is strong complete in the way that, given some evidence, it is possible to induce any program which could have generated that evidence. We outline some possible restrictions in order to improve the tractability of the schema. We also show that inverse narrowing is just a special case of our IFLP schema. Finally, a straightforward extension of the IFLP schema to function invention is illustrated...|$|R
40|$|Abstract-This paper {{presents}} {{a study of}} base station system configurations for the Universal Mobile Telecommunication System (UMTS). The base station system (BSS), being the in-terface between the mobile terminal and fixed network, should be capable of handling {{a huge amount of}} traffic, mobility, and a high signaling load which is expected in the UMTS. Several functions that should be supported by BSS are envisaged. Spe-cial attention is given to the mobility functions (e. &, hand-over). Functions that are needed to perform a handover are identified. This <b>results</b> <b>in</b> <b>a</b> <b>generic</b> handover functional model. Possible allocation scenarios are examined, taking into account the interconnection methods for the BSS and considering the efficiency and processing delay. The possible scenarios are fur-ther evaluated for given traffic and mobility models in different areas and environments...|$|R
40|$|International audienceChoreographies are {{contracts}} specifying {{interactions among}} {{a set of}} services from a global point of view. These contracts serve as reference for the further development steps of the distributed system. Therefore their specification and analysis is crucial to avoid issues (e. g., deadlock, requirement violation, faulty contract, etc.) that may induce delays and additional costs if identified lately {{in the design and}} development process. In this paper, we propose first to describe choreographies using <b>an</b> intermediate format <b>in</b> order to support various choreography specification languages as input. Second, we propose a connection to a formal verification toolbox as back-end, which includes the generation of verification scripts for fully automating the aforementioned checks. This <b>results</b> <b>in</b> <b>a</b> <b>generic,</b> modular, and extensible framework for supporting the verification of choreographies...|$|R
40|$|A new {{approach}} to error control and mesh adaptivity is described for the discretization of optimal control problems governed by elliptic partial differential equations. The Lagrangian formalism yields the first-order necessary optimality condition <b>in</b> form of <b>an</b> indefinite boundary value problem which is approximated by an adaptive Galerkin finite element method. The mesh design <b>in</b> the <b>resulting</b> reduced models is controlled by residual-based a posteriori error estimates. These are derived by duality arguments employing the cost functional of the optimization problem for controlling the discretization error. In this case, the computed state and co-state variables {{can be used as}} sensitivity factors multiplying the local cell-residuals in the error estimators. This <b>results</b> <b>in</b> <b>a</b> <b>generic</b> and simple algorithm for mesh adaptation within the optimization process. This method is developed and tested for simple boundary control problems in semi-conductor models...|$|R
40|$|We {{consider}} the hydrodynamic theory {{of an active}} fluid of self-propelled particles with nematic aligning interactions. This class of materials has polar symmetry at the microscopic level, but forms macrostates of nematic symmetry. We highlight three key features of the dynamics. First, as in polar active fluids, the control parameter for the order-disorder transition, namely the density, is dynamically convected by active currents, <b>resulting</b> <b>in</b> <b>a</b> <b>generic,</b> model independent dynamical self-regulation that destabilizes the uniform nematic state near the mean-field transition. Secondly, curvature driven currents render the system unstable deep in the nematic state, as found previously. Finally, and unique to self-propelled nematics, nematic order induces local polar order that in turn leads {{to the growth of}} density fluctuations. We propose this as a possible mechanism for the smectic order of polar clusters seen in numerical simulations. Comment: 8 pages, 3 Figure...|$|R
40|$|The {{presented}} research <b>resulted</b> <b>in</b> <b>a</b> <b>generic</b> component taxonomy, <b>a</b> <b>generic</b> code-fault taxonomy, and {{an approach}} to tailoring the generic taxonomies into domain-specific as well as project-specific taxonomies. Also, a means to identify fault links was developed. Fault links represent relationships between the types of code-faults {{and the types of}} components being developed or modified. For example, a fault link has been found to exist between Controller modules (that forms a backbone for any software via. its decision making characteristics) and Control/Logic faults (such as unreachable code). The existence of such fault links can be used to guide code reviews, walkthroughs, testing of new code development, as well as code maintenance. It {{can also be used to}} direct fault seeding. The results of these methods have been validated. Finally, we also verified the usefulness of the obtained fault links through an experiment conducted using graduate students. The results were encouraging...|$|R
40|$|Abstract — The {{activity}} rate of <b>an</b> audio clip <b>in</b> terms of three defined attributes <b>results</b> <b>in</b> <b>a</b> <b>generic,</b> quantitative measure of various acoustic sources present in it. The {{objective of this}} work is to verify if the acoustic structure {{measured in terms of}} these three attributes can be used for genre classification of music tracks. For this, we experiment on classification of full-length music tracks by using a dynamic time warping approach for time-series similarity (derived from the {{activity rate}} measure) and also a Hidden Markov Model based classifier. The performance of directly using timbral (Mel-frequency Cepstral Coefficients) features is also presented. Using only the activity rate measure we obtain classification performance that is about 35 % better than baseline chance and this compares well with other proposed systems that use musical information such as beat histogram or pitch based melody information. I...|$|R
40|$|We propose Interval Temporal Logic as a {{basis for}} {{reasoning}} about concurrent programs with fine-grained atomicity due to the generality it provides over reasoning with standard pre/post-state relations. To simplify the semantics of parallel composition over intervals, we use fractional permissions, which allows one to ensure that conflicting reads and writes to a variable do not occur simultaneously. Using non-deterministic evaluators over intervals, we enable reasoning about the apparent states over an interval, which may differ from the actual states in the interval. The combination of Interval Temporal Logic, non-deterministic evaluators and fractional permissions <b>results</b> <b>in</b> <b>a</b> <b>generic</b> framework for reasoning about concurrent programs with fine-grained atomicity. We use our logic to develop rely/guarantee-style rules for decomposing a proof of a large system into proofs of its subcomponents, where fractional permissions are used to ensure that the behaviours of a program and its environment do not conflict...|$|R
40|$|Criteria {{are needed}} {{to be able to}} judge the level of risk {{associated}} with dose rates estimated for nonhuman biota. In this paper, European guidance on the derivation of predicted no-effect chemical concentrations has been applied to appropriate radiation sensitivity data. A species sensitivity distribution fitted to the data for all species <b>resulted</b> <b>in</b> <b>a</b> <b>generic</b> predicted no-effect dose rate of 10 mGy h 1. Currently, data are inadequate to derive screening values for separate organism groups. A second, higher, benchmark could aid in decision making by putting results into context on the scale of no effect to a risk of ‘serious’ effect. The need for, meaning and use of such a value needs to be debated by the wider community. This paper explores potential approaches of deriving scientific input to this debate. The concepts proposed in this paper are broadly consistent with the framework for human protection...|$|R
40|$|In chronologic {{order of}} publication, all papers {{dealing with the}} {{systematics}} of haplochromine cichlids of Lake Victoria are analysed {{with regard to the}} generic classification of the species. Taxonomists have disputed and changed the generic classification soon after cichlids from Lake Victoria were first described. At the turn of the century, different opinions among taxonomists working on Lake Victoria haplochromines were mainly {{based on the fact that}} they studied different material. The study of more extensive collections yielded the impression that differences between the species are gradual, rendering the delimitation of genera problematic. Recently, an attempt to use cladistic methods for the unravelling of the phylogeny of the haplochromines has <b>resulted</b> <b>in</b> <b>a</b> <b>generic</b> classification which most alpha-taxonomists working on the Lake Victoria super flock consider unworkable. Pending a more clear phylogenetic picture, a new definition of the genus Haplochromis is proposed in order to create at least temporary nomenclatoral stability...|$|R
40|$|The paper {{focuses on}} {{enterprise}} business value chain modeling {{as an alternative}} to business process modeling. Well known REA methodology proposed by McCarthy and Geerts is used as the basic modeling framework. The research presented <b>in</b> the paper <b>results</b> <b>in</b> <b>a</b> <b>generic</b> semantic enterprise model using REA ontology. This rather static model is then converted into UML activity, sequence and state diagrams thus achieving dynamic view of the REA model. The dynamic REA view connects the process model and the value chain perspectives. It is shown that by using REA model transition called dynamization not only process models at task level can be achieved but also a consistency check of the REA model can be accomplished. By means of step by step value chain modeling of the enterprise a consistent process model can be reached preserving all advantages of the typical business process modeling methods...|$|R
30|$|Non-centering can, and {{probably}} will, break {{the premise of}} discorrelation between the extracted components. Rather, the first component will reflect the mean instead of the greatest variance [8]. That is, in case {{the same set of}} axes are relevant to the explanation of the variability of all clusters (high within-axes heterogeneity), non-centered PCA <b>results</b> <b>in</b> <b>a</b> single <b>generic</b> component [43].|$|R
40|$|Presently, most {{tabu search}} {{designers}} devise their applications without considering {{the potential of}} design and code reuse, which consequently prolong the development of subsequent applications. In this paper, we propose a software solution known as Tabu Search Framework (TSF), which is <b>a</b> <b>generic</b> C++ software framework for tabu search implementation. The framework excels in code recycling {{through the use of}} a well- designed set of generic abstract classes that clearly define their collaborative roles in the algorithm. Additionally, the framework incorporates a centralized process and control mechanism that enhances the search with intelligence. This <b>results</b> <b>in</b> <b>a</b> <b>generic</b> framework that is capable of solving a wide range of combinatorial optimization problems using various tabu search techniques and adaptive strategies. The applications of TSF are demonstrated on the implementation of two NP-hard problems, the Vehicle Routing Problem with Time Windows (VRPTW) and Quadratic Assignment Problem (QAP). We show that TSF is able to obtain quality solutions within reasonable implementation as well as computation time...|$|R
40|$|Many search {{algorithms}} have parameters {{that need}} to be tuned to get the best performance. Typically, the parameters are tuned offline, <b>resulting</b> <b>in</b> <b>a</b> <b>generic</b> setting that is supposed to be effective on all problem instances. For suboptimal single-agent search, problem-instance-specific parameter settings can <b>result</b> <b>in</b> substantially reduced search effort. We consider the use of dovetailing as a way {{to take advantage of this}} fact. Dovetailing is a procedure that performs search with multiple parameter settings simultaneously. Dovetailing is shown to improve the search speed of weighted IDA* by several orders of magnitude and to generally enhance the performance of weighted RBFS. This procedure is trivially parallelizable and is shown to be an effective form of parallelization for WA * and BULB. In particular, using WA * with parallel dovetailing yields good speedups in the sliding-tile puzzle domain, and increases the number of problems solved when used <b>in</b> <b>an</b> automated planning system. 1...|$|R
50|$|The {{concept of}} search {{aggregation}} {{is a relatively}} recent phenomenon with the first ones becoming available in 2006. In 2005 Amazon published the OpenSearch specification for making search <b>results</b> available <b>in</b> <b>a</b> <b>generic</b> XML format. While many sites currently publish <b>results</b> <b>in</b> OpenSearch, many simply publish in generic RSS format. However, while OpenSearch syndication allows for greater flexibility in the way Search Aggregators display results, it is generally not required.|$|R
40|$|International audienceIn this paper, {{we focus}} on {{planning}} credible walking paths <b>in</b> real-time for <b>a</b> potentially highly congested crowd of au- tonomous pedestrians. For this purpose, we exploit the prin- ciple of least effort, applied to human navigation, which pos- tulates that credible behaviours emerge {{as a function of}} the organism's propensity to minimize metabolic energy expen- diture with respect to task, environment dynamics, and or- ganism's constraints to action. We therefore propose a consistent problem formulation for the navigation task where both individual and collective dynamics are taken into account. Each pedestrian is represented as a situated agent who tries to reach its destination by following energy efficient paths. Agents are autonomous, and at the same time, sub- ject to the environment dynamics. They interact with each other through the environment in order to estimate their en- ergy expenditure relatively to their tasks. Our formulation <b>results</b> <b>in</b> <b>a</b> <b>generic</b> and scalable multi-agent model, capable of simulating individual and collective behaviours regardless of the number of agents...|$|R
40|$|Automatically calculating {{periodic}} timetables {{in public}} railway transport systems is an NP-complete problem – namely the Periodic Event Scheduling Problem (PESP). The original model {{is restricted to}} basic periodic timetabling. Extending the model by decisional transport networks with flows induces new possibilities in the timetabling and planning process. Subsequently, the given flexibility <b>results</b> <b>in</b> <b>a</b> <b>generic</b> model extension of PESP {{that can be applied}} in subsets of the timetabling process. The successful utilization of this approach is presented for distinct chain paths, duplicated chain paths and non-connected flow graphs that represent integration of routing and timetabling, planning of periodic rail freight train paths and track allocation, respectively. Furthermore, the encoding of this <b>generic</b> model into <b>a</b> binary propositional formula is introduced and the appropriate usage of several techniques like SAT solving and MaxSAT to calculate and optimize the corresponding instances will be presented accordingly. Computational results for real-world scenarios suggest the practical impact and give promising perspectives for further scientific research...|$|R
40|$|Enloe Medical Center is a {{non-profit}} community hospital in Chico, California. Among the many services they provide is a Labor and Delivery Department. While mothers are routinely admitted from 1 : 00 pm to 1 : 00 am, {{they are generally}} discharged between 10 : 00 am and 5 : 00 pm. This <b>results</b> <b>in</b> <b>a</b> <b>generic</b> bell curve behavior pattern for patient occupancy during the daytime. Hospitals are reimbursed for inpatient services in two major ways: either on a per diem basis, or by diagnosis related groups (DRG). Either way, the revenue to the hospital remains the same, regardless if the patient is discharged at 4 : 00 am or 4 : 00 pm. In California, state mandated nurse to patient ratios require hospitals to maintain a minimum level of nurse staffing for inpatient services. Thus, as the patient census rises during the day, so must the number of nurses on staff. This is the problem studied; costs expended for patient discharge delays...|$|R
40|$|We analyse the {{associated}} production of Higgs and Z boson via heavy-quark loops at the LHC in the Standard Model and beyond. We first review {{the main features}} of the Born 2 → 2 production, and in particular discuss the high-energy behaviour, angular distributions and Z boson polarisation. We then consider the effects of extra QCD radiation as described by the 2 → 3 loop matrix elements, and find that they dominate at high Higgs transverse momentum. We show how merged samples of 0 - and 1 -jet multiplicities, matched to a parton shower can provide a reliable description of differential distributions in ZH production. In addition to the Standard Model study, <b>results</b> <b>in</b> <b>a</b> <b>generic</b> two-Higgs-doublet-model are obtained and presented {{for a set of}} representative and experimentally viable benchmarks for Zh 0, ZH 0 and ZA 0 production. We observe that various interesting features appear either due to the resonant enhancement of the cross-section or to interference patterns between resonant and non-resonant contribution...|$|R
40|$|Abstract: Presently, most {{tabu search}} {{designers}} devise their applications without considering {{the potential of}} design and code reuse, which consequently prolong the development of subsequent applications. In this paper, we propose a software solution known as Tabu Search Framework (TSF), which is <b>a</b> <b>generic</b> C++ software framework for tabu search implementation. The framework excels in code recycling {{through the use of}} a welldesigned set of generic abstract classes that clearly define their collaborative roles in the algorithm. Additionally, the framework incorporates a centralized process and control mechanism that enhances the search with intelligence. This <b>results</b> <b>in</b> <b>a</b> <b>generic</b> framework that is capable of solving a wide range of combinatorial optimization problems using various tabu search techniques and adaptive strategies. The applications of TSF are demonstrated on the implementation of two NP-hard problems, the Vehicle Routing Problem with Time Windows (VRPTW) and Quadratic Assignment Problem (QAP). We show that TSF is able to obtain quality solutions within reasonable implementation as well as computation time. Key words: Tabu Search, software framework, reusability, combinatorial optimization...|$|R
