31|3|Public
5000|$|The trick can be {{exploited}} using a <b>rolling</b> <b>hash.</b> A <b>rolling</b> <b>hash</b> is a hash function specially designed to enable this operation. A trivial (but not very good) <b>rolling</b> <b>hash</b> function just adds {{the values of}} each character in the substring. This <b>rolling</b> <b>hash</b> formula can compute the next hash value from the previous value in constant time: ...|$|E
5000|$|All <b>rolling</b> <b>hash</b> {{functions}} are linear {{in the number}} of characters, but their complexity with respect to the length of the window (...) varies. Rabin-Karp <b>rolling</b> <b>hash</b> requires the multiplications of two -bit numbers, integer multiplication is in [...] Hashing ngrams by cyclic polynomials can be done in linear time.|$|E
5000|$|... rollinghashjava is an Apache-licensed Java {{implementation}} of <b>rolling</b> <b>hash</b> functions ...|$|E
40|$|New layouts for the {{assignment}} {{of a set of}} n parallel processors to perform certain tasks in several hierarchically connected layers are suggested, leading, after some initialization phase, to the full exploitation of all of the process-ing power all of the time. This framework is useful for a variety of string theoretic problems, ranging from modular arithmetic used, among others, in Karp-Rabin type <b>rolling</b> <b>hashes,</b> as well as in cryptographic applications, and up to data compression and error-correcting codes...|$|R
40|$|Virtually {{every day}} data breach {{incidents}} {{are reported in}} the news. Scammers, fraudsters, hackers and malicious insiders are raking in millions with sensitive business and personal information. Not all incidents involve cunning and astute hackers. The involvement of insiders is ever increasing. Data information leakage is a critical issue for many companies, especially nowadays where every employee has an access to high speed internet. In the past, email was the only gateway to send out information but {{with the advent of}} technologies like SaaS (e. g. Dropbox) and other similar services, possible routes have become numerous and complicated to guard for an organisation. Data is valuable, for legitimate purposes or criminal purposes alike. An intuitive approach to check data leakage is to scan the network traffic for presence of any confidential information transmitted. The existing systems use slew of techniques like keyword matching, regular expression pattern matching, cryptographic algorithms or <b>rolling</b> <b>hashes</b> to prevent data leakage. These techniques are either trivial to evade or suffer with high false alarm rate. In this thesis, 'known file content' detection in network traffic using approximate matching is presented. It performs content analysis on-the-fly. The approach is protocol agnostic and filetype independent. Compared to existing techniques, proposed approach is straight forward and does not need comprehensive configuration. It is easy to deploy and maintain, as only file fingerprint is required, instead of verbose rules. </p...|$|R
40|$|Suffix tree (and the {{closely related}} suffix array) are {{fundamental}} structures capturing all substrings {{of a given}} text essentially by storing all its suffixes in the lexicographical order. In some applications, we work with a subset of b interesting suffixes, which are stored in the so-called sparse suffix tree. Because {{the size of this}} structure is Θ(b), it is natural to seek a construction algorithm using only O(b) words of space assuming read-only random access to the text. We design a linear-time Monte Carlo algorithm for this problem, hence resolving an open question explicitly stated by Bille et al. [TALG 2016]. The best previously known algorithm by I et al. [STACS 2014] works in O(n b) time. Our solution proceeds in n/b rounds; in the r-th round, we consider all suffixes starting at positions congruent to r modulo n/b. By maintaining <b>rolling</b> <b>hashes,</b> we lexicographically sort all interesting suffixes starting at such positions, and then we merge them with the already considered suffixes. For efficient merging, we also need to answer LCE queries in small space. By plugging in the structure of Bille et al. [CPM 2015] we obtain O(n+b b) time complexity. We improve this structure, which implies a linear-time sparse suffix tree construction algorithm. We complement our Monte Carlo algorithm with a deterministic verification procedure. The verification takes O(n√(b)) time, which improves upon the bound of O(n b) obtained by I et al. [STACS 2014]. This is obtained by first observing that the pruning done inside the previous solution has a rather clean description using the notion of graph spanners with small multiplicative stretch. Then, we are able to decrease the verification time by applying difference covers twice. Combined with the Monte Carlo algorithm, this gives us an O(n√(b)) -time and O(b) -space Las Vegas algorithm...|$|R
5000|$|... rollinghashcpp is a free-software C++ {{implementation}} of several <b>rolling</b> <b>hash</b> functions ...|$|E
50|$|At best, <b>rolling</b> <b>hash</b> {{values are}} {{pairwise}} independent or strongly universal. They cannot be 3-wise independent, for example.|$|E
50|$|Another {{application}} is the Low Bandwidth Network Filesystem (LBFS), {{which uses a}} Rabin fingerprint as its <b>rolling</b> <b>hash.</b>|$|E
50|$|One of {{the main}} {{applications}} is the Rabin-Karp string search algorithm, which uses the <b>rolling</b> <b>hash</b> described below.|$|E
50|$|Another popular {{application}} is the rsync program, {{which uses a}} checksum based on Mark Adler's adler-32 as its <b>rolling</b> <b>hash.</b>|$|E
50|$|Attic uses a <b>rolling</b> <b>hash</b> to {{implement}} global data deduplication.Compression defaults to zlib, encryption is AES (via OpenSSL) authenticated by a HMAC.|$|E
50|$|In 1987, Rabin, {{together}} with Richard Karp, created {{one of the}} most well-known efficient string search algorithms, the Rabin-Karp string search algorithm, known for its <b>rolling</b> <b>hash.</b>|$|E
50|$|A <b>rolling</b> <b>hash</b> (also {{known as}} {{recursive}} hashing or rolling checksum) is a hash function where the input is hashed {{in a window}} that moves through the input.|$|E
50|$|This simple {{function}} works, {{but will}} result in statement 5 being executed more often than other more sophisticated <b>rolling</b> <b>hash</b> functions such as those discussed in the next section.|$|E
5000|$|The Rabin-Karp string search {{algorithm}} is normally {{used with a}} very simple <b>rolling</b> <b>hash</b> function that only uses multiplications and additions:where [...] is a constant, and [...] are the input characters.|$|E
50|$|Some {{specialized}} file comparison tools {{find the}} longest increasing subsequence between two files. The rsync protocol uses a <b>rolling</b> <b>hash</b> function to compare two files on two distant computers with low communication overhead.|$|E
5000|$|... zsync is a rsync-like tool {{optimized}} for many downloads per file version. zsync {{is used by}} Linux distributions such as Ubuntu for distributing fast changing beta ISO image files. zsync uses the HTTP protocol and [...]zsync files with pre-calculated <b>rolling</b> <b>hash</b> to minimize server load yet permit diff transfer for network optimization.|$|E
50|$|A few hash {{functions}} allow a <b>rolling</b> <b>hash</b> to be computed very quickly—the new {{hash value}} is rapidly calculated given only the old hash value, the old value {{removed from the}} window, and the new value added to the window—similar to the way a moving average function can be computed {{much more quickly than}} other low-pass filters.|$|E
50|$|The {{simplest}} {{approach to}} calculate the dynamic chunks is {{to calculate the}} <b>rolling</b> <b>hash</b> and if it matches a pattern (like the lower N bits are all zeroes) then it’s a chunk boundary. This approach will ensure that {{any change in the}} file will only affect its current and possibly the next chunk, but nothing else.|$|E
50|$|The <b>rolling</b> <b>hash</b> uses a 256 byte table {{containing}} the byte last seen in each possible order-1 context. The hash is updated {{by adding the}} next byte and then multiplying either by an odd constant if the byte was predicted or by an even number {{that is not a}} multiple of 4 if the byte was not predicted.|$|E
50|$|Files {{are divided}} into {{fragments}} on content-dependent boundaries. Rather than a Rabin fingerprint, ZPAQ uses a <b>rolling</b> <b>hash</b> {{that depends on the}} last 32 bytes that are not predicted by an order 1 context, plus any predicted bytes in between. If the leading 16 bits of the 32 bit hash are all 0, then a fragment boundary is marked. This gives an average fragment size of 64 KiB.|$|E
5000|$|One of the {{interesting}} use cases of the <b>rolling</b> <b>hash</b> function {{is that it can}} create dynamic, content-based chunks of a stream or file. This is especially useful when it is required to send only the changed chunks of a large file over a network and a simple byte addition {{at the front of the}} file would cause all the fixed size windows to become updated, while in reality, only the first [...] "chunk" [...] has been modified.|$|E
50|$|Being {{optimized}} for stream decompression and collaborative work with LZMA, REP has some differences {{from the original}} RZIP implementation. First, by default it finds only matches that are 512+ byte long, since benchmarking proved that this is optimal setting for overall REP+LZMA compression. Second, it uses a sliding dictionary that's about 1/2 RAM long, so decompression doesn't need to reread data from decompressed file. REP's advantage is its multiplicative <b>rolling</b> <b>hash</b> that is both quick to compute and has near-ideal distribution.|$|E
50|$|In some applications, such as {{substring}} search, {{one must}} compute a hash function h for every k-character substring {{of a given}} n-character string t; where k is a fixed integer, and n is k. The straightforward solution, which is to extract every such substring s of t and compute h(s) separately, requires a number of operations proportional to k·n. However, with the proper choice of h, one can use the technique of <b>rolling</b> <b>hash</b> to compute all those hashes with an effort proportional to k + n.|$|E
5000|$|Technically, this {{algorithm}} is only {{similar to the}} true number in a non-decimal system representation, since for example we could have the [...] "base" [...] less {{than one of the}} [...] "digits". See hash function for a much more detailed discussion. The essential benefit achieved by using a <b>rolling</b> <b>hash</b> such as the Rabin fingerprint is {{that it is possible to}} compute the hash value of the next substring from the previous one by doing only a constant number of operations, independent of the substrings lengths.|$|E
5000|$|The {{key to the}} Rabin-Karp {{algorithms}} {{performance is}} the efficient computation of hash values of the successive substrings of the text. The Rabin fingerprint is a popular and effective <b>rolling</b> <b>hash</b> function. The Rabin fingerprint treats every substring as a number in some base, the base being usually a large prime. For example, if the substring is [...] "hi" [...] and the base is 101, the hash value would be 104 &times; 1011 + 105 &times; 1010 = 10609 (ASCII of h is 104 and of i is 105).|$|E
5000|$|This Rabin-Karp <b>rolling</b> <b>hash</b> {{is based}} on a linear congruential generator.Above {{algorithm}} is also known as Multiplicative hash function. In practice, the mod operator and the parameter p can be avoided altogether by simply allowing integer to overflow because it is equivalent to mod (Max-Int-Value + 1) in many programming languages. Below table shows values chosen to initialize h and a for some of the popular implementations.Consider two strings [...] and let [...] be length of the longer one; for the analysis, the shorter string is conceptually padded with zeros up to length [...] A collision before applying [...] implies that [...] is a root of the polynomial with coefficients [...] This polynomial has at most [...] roots modulo , so the collision probability is at most [...] The probability of collision through the random [...] brings the total collision probability to [...] Thus, if the prime [...] is sufficiently large compared to the length of strings hashed, the family is very close to universal (in statistical distance).|$|E
5000|$|The Low Bandwidth Network Filesystem (LBFS) from MIT uses Rabin {{fingerprints}} {{to implement}} variable size shift-resistant blocks.The basic {{idea is that}} the filesystem computes the cryptographic hash of each block in a file. To save on transfers between the client and server,they compare their checksums and only transfer blocks whose checksums differ. But one problem with this scheme is that a single insertion {{at the beginning of the}} file will cause every checksum to change if fixed-sized (e.g. 4 KB) blocks are used. So the idea is to select blocks not based on a specific offset but rather by some property of the block contents. LBFS does this by sliding a 48 byte window over the file and computing the Rabin fingerprint of each window. When the low 13 bits of the fingerprint are zero LBFS calls those 48 bytes a breakpoint and ends the current block and begins a new one. Since the output of Rabin fingerprints are pseudo-random the probability of any given 48 bytes being a breakpoint is [...] This has the effect of shift-resistant variable size blocks. Any hash function could be used to divide a long file into blocks (as long as a cryptographic hash function is then used to find the checksum of each block): but the Rabin fingerprint is an efficient <b>rolling</b> <b>hash,</b> since the computation of the Rabin fingerprint of region B can reuse some of the computation of the Rabin fingerprint of region A when regions A and B overlap.|$|E
30|$|In this {{research}} the existing deduplication techniques fixed length, variable length block deduplication techniques are implemented and compared with proposed adaptive deduplication techniques on standard VM images dataset which {{is taken from}} open stack image registry when created with each VM configuration of RAM 2 GB and hard disk of 10 GB assuming no applications running on any VM. The Rabin-Karp <b>rolling</b> <b>hash</b> algorithm is used for variable length block deduplication. IM-Dedup uses static (fixed length) chunking procedure, and it achieves 80 % reduction in overall image storage. We achieved an 83 % reduction in the overall storage of images and 89.76 % overall reduction in migration time by using adaptive deduplication. 3 % improvement in deduplication rate by the proposed method. As the migration time is reduced obviously, downtime and application performance degradation also reduced.|$|E
40|$|The “internet” is a {{well known}} {{technology}} among students nowadays. They use the internet in finding resources for their study and enhancing their learning activity. On the other hand, internet offer big opportunity to cheat by copying others work without proper acknowledgment or using free edited wikis as a reference, which is academically prohibited. This project proposes a system to detect the use of free wiki in academic final assignment. This system adopts the well known algorithm of text based plagiarism detection system, called Winnowing Algorithm, uses fingerprinting method in detecting similarity of document. The implementation of Winnowing Algorithm involves some stages. First, parse the string into smaller form using the N-Gram method which then called as gram. Then, calculate the hash value (integer) of each gram using the <b>Rolling</b> <b>Hash</b> technique. Finally, define windows from the hash values, and choose the fingerprint from the window. In the experiment of this system, we use recall and precision value of each kind of examination to choose which value of variable {{that will be used}} for the permanent value of system variable so the system can be efficiently used. We also examine the performance of every single function of the system. The experiment result also reveals that the time needed for the plagiarism detecting process using off-line source of comparator file is far lesser than using on-line comparator file...|$|E
40|$|Abstract Background Chaos Game Representation (CGR) is an {{iterated}} {{function that}} bijectively maps discrete sequences into a continuous domain. As a result, discrete sequences can be object of statistical and topological analyses otherwise reserved to numerical systems. Characteristically, CGR coordinates of substrings sharing an L -long suffix will be located within 2 -L distance of each other. In {{the two decades}} since its original proposal, CGR has been generalized beyond its original focus on genomic sequences and has been successfully applied {{to a wide range}} of problems in bioinformatics. This report explores the possibility that it can be further extended to approach algorithms that rely on discrete, graph-based representations. Results The exploratory analysis described here consisted of selecting foundational string problems and refactoring them using CGR-based algorithms. We found that CGR can take the role of suffix trees and emulate sophisticated string algorithms, efficiently solving exact and approximate string matching problems such as finding all palindromes and tandem repeats, and matching with mismatches. The common feature of these problems is that they use longest common extension (LCE) queries as subtasks of their procedures, which we show to have a constant time solution with CGR. Additionally, we show that CGR can be used as a <b>rolling</b> <b>hash</b> function within the Rabin-Karp algorithm. Conclusions The analysis of biological sequences relies on algorithmic foundations facing mounting challenges, both logistic (performance) and analytical (lack of unifying mathematical framework). CGR is found to provide the latter and to promise the former: graph-based data structures for sequence analysis operations are entailed by numerical-based data structures produced by CGR maps, providing a unifying analytical framework for a diversity of pattern matching problems. </p...|$|E
40|$|It {{is still}} strangely {{difficult}} to backup and synchronize data. Cloud computing solves {{the problem by}} centralizing everything and letting someone else handle the backups. But what about situations with low connectivity or sensitive data? For this, software developers have an interesting distributed, decentralized, and partition-tolerant data storage system right at their fingertips: distributed version control. Inspired by distributed version control, we have researched and developed a prototype for a scalable high-availability system called Distributed Media Versioning (DMV). DMV expands Git's data model to allow files to be broken into more digestible chunks via a <b>rolling</b> <b>hash</b> algorithm. DMV will also allow data to be sharded according to data locality needs, slicing the data set in space (subset of data with full history), time (subset of history for full data set), or both. DMV repositories {{will be able to}} read and to update any subset of the data that they have locally, and then synchronize with other repositories in an ad-hoc network. We have performed experiments to probe the scalability limits of existing version control systems, specifically what happens as file sizes grow ever larger or as the number of files grow. We found that processing files whole limits maximum file size to what can fit in RAM, and that storing millions of objects loose as files with hash-based names incurs disk space overhead and write speed penalties. We have observed a system needing 24 seconds to store a 6. 8 KiB file. We conclude that the key to storing large files is the break them into many small chunks, and that the key to storing many chunks is to aggregate them into pack files. And though the current DMV prototype does only the former, we have a clear path forward as we continue our work...|$|E
30|$|Cloud Computing is a {{paradigm}} which provides resources to users from its pool based on demand {{to satisfy their}} requirements. During this process, many servers are overloaded and underloaded in the cloud environment. Thus, power consumption and load balancing are the major problems and are resolved by live virtual machine (VM) migration. Load balancing is addressed by moving virtual machines from overloaded host to under loaded host and from under loaded host to any other host which is not overloaded called VM migration. If this process is done without power off (Live) the virtual machines then it is called live VM migration. By this process, the issue of power consumption by physical hosts is also resolved. Migrating virtual machines involves virtualized components like storage disks, memory, CPU and networking, {{the entire state of}} VM is captured as a collection of data files. These data files are virtual disk files, configuration files, and log files. The virtual disk files take larger memory and take more migration time and hence the downtime. These disk files include many zero pages, similar and redundant pages. Many techniques such as compression, deduplication is used reduce the size of VM disk image file. Compression techniques are not widely used, due to the disadvantage of compression ratio and decompression time. Many researchers hence used deduplication techniques for reducing the VM disk image file in the live migration process. The significance of the research work is to design an adaptive deduplication mechanism for reducing VM disk image file size by performing fixed length and variable length block-level deduplication processes. The Rabin-Karp <b>rolling</b> <b>hash</b> algorithm is used in variable length block-level deduplication. Akka stream is used for streaming the VM disk image files as it is the bulk volume of live data transfer. To reduce the time of the deduplication process, many researchers used multithreading and multi-core technologies. We use multithreading in Akka framework to run the deduplication process concurrently without OutofMemory errors. The experiment results show that we achieved a maximum of 83 % overall reduction in image storage space and 89.76 % reduction in total migration time are achieved by adaptive deduplication method. 3 % improvement in deduplication rate when compared with the existing image management system. The results are significant because when we apply this in the storage of data centres, there are much space savings. The reduction in size is dependent on the dataset was taken and the applications running on the VM.|$|E

