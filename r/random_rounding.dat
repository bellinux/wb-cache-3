23|81|Public
5000|$|The output [...] of the <b>random</b> <b>rounding</b> scheme has {{the desired}} propertiesas long as {{none of the}} {{following}} [...] "bad" [...] events occur: ...|$|E
50|$|There {{are many}} other {{possible}} rules for tie breaking when rounding a half integer include rounding up, rounding down, rounding to or away from zero, or <b>random</b> <b>rounding</b> up or down.|$|E
5000|$|Calculate {{the maximum}} {{revenue in the}} entire population; apply a certain <b>random</b> <b>rounding</b> process that {{guarantees}} that the calculation is truthful with-high-probability. Let R be the estimated revenue; run R-profit-extractor in the entire population.|$|E
40|$|Using the decorrelation {{techniques}} {{we compare}} the randomness of three schemes {{used in the}} AES candidates. The target schemes are the original Feistel scheme and two modi#ed Feistel schemes: the MARS-like structure and the CAST 256 -like structure. As a result, the required numbers of rounds for Luby-Racko#'s randomness #which is related to resistance against chosen plaintext attacks # are 3, 5, and 7, respectively. Moreover, the required numbers of rounds for achieving the decorrelation bias of order twoof 2, 128 are 9, 25, and 35, respectively. This holds for truly <b>random</b> <b>round</b> functions. Imperfect <b>random</b> <b>round</b> functions can achieve similar decorrelation by using decorrelation modules likeinDFC, but need anumber of rounds of at least 9, 30 and 42 respectively. 1 Introduction So far, none of the comments on the AES candidates {{address the problem of}} randomness provided by the design proposals. Randomness means that no oracle circuit with polynomially many oracle gates can [...] ...|$|R
50|$|There are 3 rounds {{for each}} chosen girl (order within <b>rounds</b> are <b>random).</b> Each <b>round</b> allows {{scrolling}} at the beginning.|$|R
40|$|Abstract. We {{show that}} the Feistel {{construction}} with six <b>rounds</b> and <b>random</b> <b>round</b> functions is publicly indifferentiable from a random invertible permutation (a result that is not known to hold for full indifferentiability). Public indifferentiability (pub-indifferentiability for short) is a variant of indifferentiability introduced by Yoneyama et al. [YMO 09] and Dodis et al. [DRS 09] where the simulator knows all queries made by the distinguisher to the primitive it tries to simulate, and is useful to argue the security of cryptosystems where all the queries to the ideal primitive are public (as e. g. in many digital signature schemes). To prove the result, we introduce a new and simpler variant of indifferentiability, that we call sequential indifferentiability (seq-indifferentiability for short) and show that this notion is in fact equivalent to pub-indifferentiability for stateless ideal primitives. We then prove that the 6 -round Feistel construction is seq-indifferentiable from a random invertible permutation. We also observe that sequential indifferentiability implies correlation intractability, so that the Feistel construction with six <b>rounds</b> and <b>random</b> <b>round</b> functions yields a correlation intractable invertible permutation, a notion we define analogously to correlation intractable functions introduced by Canetti et al. [CGH 98]...|$|R
50|$|One {{approach}} would be to increase a little bit, then show that the probability of success is at least, say, 1/4.With this modification, repeating the <b>random</b> <b>rounding</b> step a few timesis enough {{to ensure a successful}} outcome with high probability.|$|E
40|$|This paper {{discusses}} the security problem for statistical data bases and examines {{in some detail}} a couple of techniques, namely rounding and <b>random</b> <b>rounding</b> which have been proposed to solve the statistical data base security problem. Analysis of their effects on data base output indicates that, with suitable choice of base, rounding {{may be a more}} secure technique than <b>random</b> <b>rounding.</b> link_to_subscribed_fulltex...|$|E
40|$|The {{security}} {{problem for}} statistical data bases is discussed {{and a couple}} of techniques, namely rounding and <b>random</b> <b>rounding,</b> which have been proposed to solve the statistical data base security problem are examined in detail. These techniques are studied in connection with a simple model of a statistical data base which produces cross-tabulations. Analysis of the effects of both techniques on data base output indicates that, with suitable choice of base, rounding may be a more secure technique than <b>random</b> <b>rounding.</b> link_to_subscribed_fulltex...|$|E
40|$|Load {{balancing}} algorithms play critical {{roles in}} systems where the workload {{has to be}} distributed across multiple resources, such as cores in multiprocessor system, computers in distributed computing, and network links. In this paper, we study and evaluate four load balancing methods: <b>random,</b> <b>round</b> robin, shortest queue, and shortest queue with stale load information. We build a simulation model and compare mean delay of the systems for the load balancing methods. We also provide a method to improve shortest queue with stale load information load balancing. A performance analysis for the improvement is also presented in this paper...|$|R
40|$|This paper {{presents}} a server selection method where the server selection is {{performed by the}} network, at the first router from the client. This method, usable in networks under single administrative control, combines network metrics (such as traffic or latency) with common server load balancing metrics (such as load, CPU utilisation or response time), allowing network conditions {{to be taken into}} account when selecting the server which will handle the request. The performance of this technique was compared, by means of simulations, to that of widely used techniques such as <b>random,</b> <b>round</b> robin and least loaded server selection. ...|$|R
50|$|Each {{show has}} three rounds, the second {{involving}} finding {{things for the}} Map of Weird, and the third called the <b>Random</b> Word <b>Round.</b> Ironically, on the first show, the random word was 'word'.|$|R
40|$|International audienceCompensated {{summation}} algorithms {{are designed}} to improve the accuracy of ill-conditioned sums. They are based on algorithms, such as FastTwoSum, which are proved to provide, with rounding to nearest, the sum of two floating-point numbers and the associated rounding error. Discrete stochastic arithmetic enables one to estimate rounding error propagation in numerical codes. It requires a <b>random</b> <b>rounding</b> mode which consists in rounding each computed result toward −∞ or +∞ with the same probability. In this paper we analyse {{the impact of this}} <b>random</b> <b>rounding</b> mode on compensated summations based on the FastTwoSum algorithm. We show the accuracy improvement obtained using such compensated summations in numerical simulations controlled with discrete stochastic arithmetic...|$|E
40|$|Government {{statistical}} agencies {{collect data}} on individuals. These data can have personal information {{that will lead to}} individual identification. The information gathered is often released and used by other agencies. In order to preserve confidentiality (people’s privacy) the data are treated in a way that prevents identification. In recent years there has been a rapid increase in the {{research in the area of}} confidentiality and statistical disclosure techniques (SDC). We focus on <b>random</b> <b>rounding</b> method, one of the SDC. In this thesis we use rounded data which have been collected by Statistics NZ. We examine the effect of <b>random</b> <b>rounding</b> in contingency tables. We simulate data, based on rounded data, and actual data and use the general log-linear model and chi-square test for analysis...|$|E
40|$|AbstractIt {{has been}} {{recently}} shown that calibration with an error less than Δ> 0 is almost surely guaranteed with a randomized forecasting algorithm, where forecasts are obtained by <b>random</b> <b>rounding</b> the deterministic forecasts up to Δ. We {{show that this}} error cannot be improved for {{a vast majority of}} sequences: we prove that, using a probabilistic algorithm, we can effectively generate with probability close to one a sequence “resistant” to any randomized rounding forecasting with an error much smaller than Δ. We also reformulate this result by means of a probabilistic game...|$|E
5000|$|... {{take out}} one <b>random</b> card each <b>round.</b> each player gets 17 cards.|$|R
40|$|In this paper, we {{consider}} efficient differentially private empirical risk minimization {{from the viewpoint}} of optimization algorithms. For strongly convex and smooth objectives, we prove that gradient descent with output perturbation not only achieves nearly optimal utility, but also significantly improves the running time of previous state-of-the-art private optimization algorithms, for both ϵ-DP and (ϵ, δ) -DP. For non-convex but smooth objectives, we propose an RRPSGD (<b>Random</b> <b>Round</b> Private Stochastic Gradient Descent) algorithm, which provably converges to a stationary point with privacy guarantee. Besides the expected utility bounds, we also provide guarantees in high probability form. Experiments demonstrate that our algorithm consistently outperforms existing method in both utility and running time...|$|R
40|$|Mechanical {{properties}} of open-cellular {{magnesium alloys with}} three types of geometric cell-structures, that is, a <b>random</b> <b>round</b> cell-structure (type A), a controlled diamond cell-structure for which the angle between the struts and the load direction is 45 degree (type B) and a controlled square cell-structure for which the angle between the struts and the loading direction is 0 degree (90 degree) (type C), are investigated by compressive tests. Results indicate that type C showed a higher collapse stress {{than the other two}} types. The collapse mechanism and the effects of the loading direction on collapse stress for the three types of magnesium alloys are discussed from the viewpoint of bending, buckling and yielding of the struts. It is suggested that collapse for the open-cellular magnesium alloys is associated with yielding of struts...|$|R
40|$|Some {{countries}} use {{forms of}} <b>random</b> <b>rounding</b> as a post-tabular method to protect Census frequency counts disseminated in tables. These methods typically result in inconsistencies between aggregated internal cells to marginal totals and across same cells in different tables. A post-tabular method for perturbing frequency counts is proposed which preserves totals and corrects {{to a large}} extent inconsistencies. The perturbation is based on invariant probability transition matrices and the use of microdata keys. This method will be compared to common pre and post-tabular methods for protecting Census frequency counts...|$|E
40|$|Sphere {{decoding}} achieves maximum-likelihood (ML) {{performance at}} the cost of exponential complexity; lattice reduction-aided decoding significantly reduces the decoding complexity, but exhibits a widening gap to ML performance as the dimension increases. To bridge the gap between them, this paper presents randomized lattice decoding based on Klein's randomized algorithm, which is a randomized version of Babai's nearest plane algorithm. The technical contribution of this paper is twofold: we analyze and optimize the performance of randomized lattice decoding resulting in reduced decoding complexity, and propose a very efficient implementation of <b>random</b> <b>rounding.</b> Simulation results demonstrate near-ML performance achieved by a moderate number of calls, when the dimension is not too large. 5 page(s...|$|E
40|$|International audienceDiscrete Stochastic Arithmetic (DSA) {{estimates}} {{round-off error}} propagation in a program. It {{is based on}} a synchronous execution of several instances of the program to control using a <b>random</b> <b>rounding</b> mode. In this paper we show how we can take advantage of multicore processors, which are nowadays widespread, to reduce the cost of DSA in terms of execution time. Several processes execute in parallel different instances of the program and exchange data when necessary. Several strategies are compared for the estimation of the result accuracy and the detection of numerical instabilities. With our parallel implementation, the cost of DSA is reduced by a factor of about 2 compared with the sequential approach. Our parallel implementation of DSA has been used successfully for the numerical validation of a real-life application...|$|E
50|$|The annulus {{tool that}} was used to mark the bubbles in the Milky Way Project phase 1 was at <b>random</b> <b>round</b> and needed improvement. This problem was solved after the {{introduction}} of the ellipse tool. This new tool was used in the phase 2 of the project, short after DR1. This changed the classification and the tool does fit the actual shape of the bubbles. The phase 2 also used different colors: 3.6 μm for blue, 4.5 μm for green and 8.0 μm for red. The same three colors as GLIMPSE 360 in Aladin Lite. Phase 3 is also called pheonix and it is now active. Phase 3 uses the same colors as phase 1 and the same ellipse tool as phase 2, combining the strength of phase 1+2.|$|R
50|$|Smith {{has taught}} at Finchley Catholic High School (1971-73), Kingsway-Princeton College (1973-84) and De Montfort University (1980-97) and {{currently}} lectures in {{music at the}} University of Hertfordshire. His work with students {{has resulted in a}} large number of arrangements, particularly for tuned percussion groups, as well as performing versions of Grainger's <b>Random</b> <b>Round,</b> Reich's Music for 18 Musicians and several works by Carla Bley. Other arrangements include a solo piano version of Holst's The Planets and reductions for violin and piano of a number of Albanian works for violin and orchestra. He is an active member of CoMA (Contemporary Music-making for All) for whom he has composed works for large, flexible ensembles such as Murdoch or Fred West - which is best? Reconsidered (2000), and Whiskies of Islay (2006).|$|R
40|$|High-speed routers rely on well-designed packet buffers {{that support}} {{multiple}} queues, large capacity and short response times. Some researchers suggested combined SRAM/DRAM hierarchical buffer architectures {{to meet these}} challenges. However, these architectures suffer from either large SRAM requirement or high time-complexity in the memory management. Our analysis indicates that they perform exactly {{the same in the}} worst case. In this paper, we present a novel packet buffer architecture which reduces the SRAM size requirement by (k- 1) / 2 k, where k denotes the number of DRAMs working in parallel. We use a fast batch load scheme and per-queue <b>Random</b> <b>Round</b> Robin memory management algorithm. Our mathematical analysis and simulation results indicate that the proposed architecture provides guaranteed performance in terms of low time complexity, short access delay and upper-bounded drop rate, when a little speedup is provided. © 2010 IEEE...|$|R
40|$|Compensated {{algorithms}} {{consist in}} computing the rounding error of individual operations and then adding them later {{on to the}} computed result. This {{makes it possible to}} increase the accuracy of the computed result efficiently. Computing the rounding error of an individual operation is possible {{through the use of a}} so-called error-free transformation. In this article, we show that it is possible to validate the result of compensated algorithms using stochastic arithmetic. We study compensated algorithms for summation, dot product and polynomial evaluation. We prove that the use of the <b>random</b> <b>rounding</b> mode inherent to stochastic arithmetic does not change the accuracy of compensated methods. This is due to the fact that error-free transformations are no more exact but still sufficiently accurate to improve the numerical quality of results...|$|E
40|$|In {{this paper}} we {{consider}} a new transportation model, called the loader problem, which is frequently encountered by third-party logistics service providers in practice. It is a tactical staff-planning {{problem with the}} objective of minimizing the total labour cost of staffing a sufficient number of loaders on a given fleet of trucks that serve a given set of customer sites. We formulate the problem as an integer program and show that it is strongly NP-hard. We then consider two special cases of the loader problem that occur in certain practical situations, and propose polynomial and pseudo-polynomial time algorithms for solving these cases. We also propose a linear programming relaxation-based <b>random</b> <b>rounding</b> algorithm for the general problem and report the computational results of the algorithm. Department of Logistics and Maritime Studie...|$|E
40|$|Statistics Centre-Abu Dhabi (SCAD) {{conducted}} a comprehensive population census for the emirate of Abu Dhabi in 2011 and released the results using online dissemination tools which allow {{the public to}} create frequency tables from census data down to small geographic areas having as few as 500 individuals. While increasing data availability, the flexibility of these tools introduced new risks of disclosing information about individual respondents. This paper presents {{a discussion of the}} risks for providing access to ad-hoc report generation and the method used by SCAD to mitigate these risks. The method presented is based on <b>random</b> <b>rounding</b> and controls for consistency across multiple output channels. While this method is applied on each output table individually, it ensures that the same frequency count in any table is always rounded to the same value...|$|E
40|$|Abstract. Holenstein et al. (STOC 2011) {{have shown}} that the Feistel {{construction}} with fourteen <b>rounds</b> and public <b>random</b> <b>round</b> functions is indifferentiable from a random permutation. In the same paper, they pointed out that a previous proof for the 10 -round Feistel construction by Seurin (PhD thesis) was flawed. However, they left open {{the question of whether the}} proof could be patched (leaving hope that the simulator described by Seurin could still be used to prove indifferentiability of the 10 -round Feistel construction). In this note, we show that the proof cannot be patched (and hence that the simulator described by Seurin cannot be used to prove the indifferentiability of the 10 -round Feistel construction) by describing a distinguishing attack that succeeds with probability close to one against this simulator. We stress that this does not imply that the 10 -round Feistel construction is not indifferentiable from a random permutation (since our attack does not exclude the existence of a different simulator that would work). ...|$|R
40|$|In {{this paper}} we study the round {{permutations}} (or S-boxes) which provide to Feistel ciphers the best resistance against differential cryptanalysis. We prove that a Feistel cipher with any round keys and {{with at least}} 5 rounds resists any differential attack if its round permutation is differentially ffi-uniform for a small ffi. This improves an earlier result due to Nyberg and Knudsen which only held for independent and uniformly <b>random</b> <b>round</b> keys. We also give some necessary conditions for a mapping to be almost perfect nonlinear (i. e. differentially 2 -uniform). 1 Introduction The underlying motivation of this work is {{the design of a}} Feistel cipher which resists all classical attacks. The DES cipher seems to have this property since no cryptanalysis is really more efficient than an exhaustive search for the key. But it would be very important to find a new secure DES-like cipher because the size of the secret-key used in DES makes a brute-force attack feasible. The main probl [...] ...|$|R
40|$|The paper {{discusses}} the Feistel cipher with a block size of n = 2 m, where {{the addition of}} a round key and a part of an incoming massage in each round is carried out modulo 2 ^m. In order to evaluate the security of such a cipher against differential and linear cryptanalyses, the new parameters of cipher s-boxes are introduced. The upper bounds of maximum average differential and linear probabilities of one round encryption transformation and the upper bounds of maximum average differential and linear characteristics probabilities of the whole cipher are obtained. The practical security of the cipher GOST (with independent and equiprobable <b>random</b> <b>round</b> keys) against differential and linear cryptanalysis is also evaluated. To the authors’ mind, the obtained results allow one to expand the basic statements concerning the practical security of Markov (Feistel and SPN) ciphers against conventionally differential and linear attacks to a cipher of the type under study...|$|R
40|$|ABSTRACT: Most modern censuses now {{accumulate}} {{a wealth}} of data {{that can be used}} to inform program and policy-making at the national, regional and even municipal level. However, in order to be of benefit, this wealth of data must be analyzed extensively. The needs of the research community have evolved in recent years towards increasing levels of detail, small area data, and micro-data. As a result, statistical agencies are placed in the position of balancing the need for detailed data with the need for confidentiality-protection. Canada bases its Census data-dissemination program on five general principles: maximize the amount of relevant analysis; protect confidentiality as a highest priority; tailor data products to user groups; produce accurate, accessible, relevant and timely data; and apply disclosure control methods without unduly restricting analytical potential. Statistics Canada improves access to detailed data by providing more census data for small areas, by increasing access to detailed tabulations and by increasing access to micro-data. Canada’s Census of Population uses two main methods of disclosure control in its tabular data, area suppression and <b>random</b> <b>rounding.</b> For its public-use micro-data files, it relies on data-reduction techniques. The 2001 Census Data Release approach features the Internet as the primary dissemination vehicle and data products designed to meet the needs of four major user groups...|$|E
40|$|Despite its reduced complexity, lattice reduction-aided {{decoding}} {{exhibits a}} widening gap to maximum-likelihood (ML) {{performance as the}} dimension increases. To improve its performance, this paper presents randomized lattice decoding based on Klein’s sampling technique, which is a randomized version of Babai’s nearest plane algorithm (i. e., successive interference cancelation (SIC)). To find the closest lattice point, Klein’s algorithm is used to sample some lattice points and the closest among those samples is chosen. Lattice reduction increases the probability of finding the closest lattice point, and only needs to be run once during pre-processing. Further, the sampling can operate very efficiently in parallel. The technical contribution {{of this paper is}} two-fold: we analyze and optimize the decoding radius of sampling decoding resulting in better error performance than Klein’s original algorithm, and propose a very efficient implementation of <b>random</b> <b>rounding.</b> Of particular interest is that a fixed gain in the decoding radius compared to Babai’s decoding can be achieved at polynomial complexity. The proposed decoder is useful for moderate dimensions where sphere decoding becomes computationally intensive, while lattice reduction-aided decoding starts to suffer considerable loss. Simulation results demonstrate near-ML performance is achieved by a moderate number of samples, even if the dimension is as hig...|$|E
40|$|Numerical {{validation}} of computed results {{is of great}} importance especially in sceintific computing. Due {{to the use of}} finite representation of real numbers, round-off errors are introduced and accumulated in arithmetic operations. Nowadays, softwares tend to run longer. This exaggerate the problem as the computed results would get severely contaminated by the propagation of the round-off errors which might, at some point, lead to obtaining unreliable results. The Discrete Stochastic Arithmetic (DSA) provides an effective and reliable approach to validate the numerical accuracy of the computed results. In DSA, a code is run N times with <b>random</b> <b>rounding</b> at every floating point operation, and the numerical accuracy information can be obtained by calculating the confidence interval of the randomly rounded results. In this work, we present a novel hardware architecture which efficiently implements the DSA. A Numerical Analysis Unit (NAU) that estimates the numerical accuracy of any intermediate result and detects numerical instabilities has been implemented based on a hardware-reduced approach. The NAU has been integrated into a high-performance FPGA system that consists of two PowerPC processors which use stochastic floating point units. Upon catching numerical instabilities, the NAU raises exceptions to the PowerPCs stopping them at the instruction that caused the exception. In contrary to the existent implementations, the proposed implementation has been developed to meet three constrains; minimal original source code modifications, minimizing hardware resource cost and exhibiting a good performance. An extension to a state of the art debugger has been developed for the debugging of numerical instabilities in a code. This extension adds functionalities which facilitate communicating with the FPGA system. Moreover, functionality specific to numerical accuracy, such as getting more details about a NAU exception or resuming execution after catching one, is provided through this extension...|$|E
40|$|In cloud, {{processing}} loads {{arrive from}} many users at random time instants {{in the form}} of task. A proper resource allocation policy attempts to assign this task to available VMs on different host so to complete the execution of the tasks in the shortest possible time with minimum power consumption. The complexity of the resource allocation problem with cloud increases with the number of hosts and becomes difficult to solve effectively. The resource allocation problem is a combinatorial problem and known to be NP-complete. The exponential solution space of the load balancing problem can be searched using heuristic techniques based on Genetic algorithms to obtain a sub- optimal solution in acceptable time. The novel ge-netic algorithm framework has been proposed for task scheduling to minimize the energy consumption in cloud computing infras-tructure. The performance of the proposed GA resource allocation strategy has been compared <b>Random</b> and <b>Round</b> Robin scheduling using in house simulator. The experimental results show that the GA based scheduling model outperforms the existing <b>Random</b> and <b>Round</b> Robin scheduling models...|$|R
5000|$|Galaxies {{and star}} {{clusters}} can be unstable, if small perturbations in the gravitational potential cause {{changes in the}} density that reinforce the original perturbation. Such instabilities usually require that the motions of stars be highly correlated, so that the perturbation is not [...] "smeared out" [...] by random motions. After the instability has run its course, the system is typically [...] "hotter" [...] (the motions are more <b>random)</b> or <b>rounder</b> than before. Instabilities in stellar systems include: ...|$|R
40|$|As the {{technology}} advances, {{the speed and}} sizes of input-queued internet switches increase dramatically. The design of the switch scheduler becomes a primary challenge, because the time interval for making scheduling decisions becomes very small. One method to resolve this problem {{is to reduce the}} scheduling frequency for a batch of packets and pipeline the switching tasks. Such method is called batch scheduling. Since computing and setting up each switch configuration incurs a notable cost, it is preferred to have minimum number of configurations for every batch. We study in this paper approximation schemes for batch scheduling with minimum configurations. We propose three algorithms together with their (expected) approximation ratios. The NAIVE ROUND ROBIN algorithm runs in O(N- 2) time and has the worst possible approximation ratio. We try to improve the approximation ratio by randomization. The RANDOMIZED ROUND ROBIN algorithm has an expected approximation ratio of O (In N). In the same fashion, we propose the BARELY <b>RANDOM</b> <b>ROUND</b> ROBIN algorithm that uses less random bits at a cost of worse approximation ratio. Finally, our simulation results indicate that a fabric speedup of 2 is sufficient for our batch scheduling algorithms to provide quality of service (QGS) ...|$|R
