11|13|Public
40|$|Estimates of intergenerational {{economic}} mobility {{that use}} {{point in time}} measures of income and earnings suffer from lifecycle and attenuation bias. They also suffer from sample selection issues and further bias driven by spells out of work. We consider these issues together for UK data, the National Child Development Study and British Cohort Study, for the first time. When all three biases are considered, our best estimate of lifetime intergenerational economic persistence in the UK is 0. 43 for children born in 1970. Whilst we argue {{that this is the}} best available estimate to date, we discuss why {{there is good reason to}} believe that this is still a lower bound, owing to <b>residual</b> <b>attenuation</b> bias...|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimitedMeasurements of the ultrasonic attenuation of 10 and 30 Mcs/sec longitudinal waves by pulsed-echo techniques {{were made on}} a 99. 999 % pure single crystal of superconducting zinc in the [0001] direction {{as a function of}} temperature from 4. 2 °K to 0. 320 °K using an open-ended type He 3 cryostat. The attenuation was found to be frequency dependent and decreased less sharply near the superconducting transition temperature, Tc, than predicted by the Bardeen-Cooper-Schrieffer (BCS) theory. Attenuation due to electron-phonon interactions only was found by subtracting from experimental points the value of <b>residual</b> <b>attenuation</b> gotten by extrapolation of the data to T = 0 °K. Using the BCS theory the zero degree superconducting energy gap was found to be 2 €(0) = (3. 36 + 0. 13) kTc with Tc = 0. 817 °K. [URL] United States Arm...|$|E
30|$|A {{template}} {{solution for}} the attenuation correction of the standard headphones is most difficult to realise because of the individual positioning of the different subjects and the complex determination {{of the position of}} the headphones. There {{have been a number of}} suggestions as to the best way to correct the attenuation caused by standard headphones. For example, Ferguson et al. [12] suggest a CT-derived attenuation map, and although the correction of the headphone attenuation was quite efficient, a <b>residual</b> <b>attenuation</b> effect of 1.9 % remained. In contrast, Heußer et al. [13] based their attenuation correction of flexible hardware components, such as headphones, on a maximum likelihood reconstruction for attenuation and activity (MLAA). In phantom measurements, the authors found an underestimation of activity concentration of 13.4 % without any correction and a maximum overestimation of 1.7 % when applying the MLAA approach. For comparison, the use of the earphones presented here, which do not require any additional data processing, resulted in a quantitation error smaller than 1 %.|$|E
40|$|This paper {{describes}} a complete fade countermeasure system, designed for thin route user-oriented and fully meshed satellite networks. The signal degradation {{due to the}} <b>residual</b> uplink <b>attenuation</b> after up-power control intervention, plus the down-link attenuation, is compensated for by varying the FEC coding and bit rates of the data. Down and up-link signal degradations are evaluated separately: the former by collecting statistics of quantised levels of the demodulated PSK signal, and the latter by using a narrow band signal level estimator. Measurement times are optimised using a model to evaluate the scintillation variance. The performance evaluation of the whole system {{in the presence of}} additive white Gaussian noise (AWGN), shows that very small link power margins can be adopted...|$|R
40|$|One of {{the biggest}} needs in network science {{research}} is access to large realistic datasets. As data analytics methods permeate a range of diverse disciplines [...] -e. g., computational epidemiology, sustainability, social media analytics, biology, and transportation [...] - network datasets that can exhibit characteristics encountered {{in each of these}} disciplines becomes paramount. The key technical issue {{is to be able to}} generate synthetic topologies with pre-specified, arbitrary, degree distributions. Existing methods are limited in their ability to faithfully reproduce macro-level characteristics of networks while at the same time respecting particular degree distributions. We present a suite of three algorithms that exploit the principle of <b>residual</b> degree <b>attenuation</b> to generate synthetic topologies that adhere to macro-level real-world characteristics. By evaluating these algorithms w. r. t. several real-world datasets we demonstrate their ability to faithfully reproduce network characteristics such as node degree, clustering coefficient, hop length, and k-core structure distributions...|$|R
40|$|In {{this paper}} the {{application}} of multirate acoustic echo canceller derived from cochlea model and psychoacustically motivated noise and <b>residual</b> echo <b>attenuation</b> system operating on the signal decomposed in cochlear spaced subbands are described. Polyphase realisation of the FIR filter bank with non-uniform frequency resolution achieved by the frequency transformation of filter characteristic using recursive allpasses {{is presented as a}} starting point to construct of the analysis and synthesis filter bank used in proposed speech enhancement system. Human auditory perception model delivered by the psychoacoustic as well cochlear model with discrete time and discrete space are used to create this allpass transformation filter bank. Performing the acoustic echo cancellation by adaptive filtering in subbands we get echo compensated signal, which is forwarded to the noise and residual echo suppression unit. Presented idea of the system combines frequency warping and acoustic echo and noise control techniques to serve in application of communication hands-free device. 1...|$|R
40|$|Patients on home {{parenteral}} nutrition {{are at risk}} for developing liver dysfunction, which is due partly to the accumulation of lipids in the liver (steatosis) and may progress to end-stage liver disease with overt liver failure. Therefore, a timely diagnosis with easy access to repeated assessment of the degree of liver steatosis is of great importance. A pilot study was performed in 14 patients on long-term home {{parenteral nutrition}} using the computer-aided ultrasound method. Ultrasound radio frequency data were acquired using a phased array transducer and were converted into conventional B-mode images. All patients were subjected to proton magnetic resonance spectroscopy measurement of liver fat content for reference. Computer-aided ultrasound parameters similar to those in a previous validation study in cows revealed significant correlations with fat content measured by magnetic resonance spectroscopy. The most significant parameters were the <b>residual</b> <b>attenuation</b> coefficient (R = 0. 95, p < 0. 001) and the lateral speckle size (R = 0. 77, p = 0. 021). These findings indicate the potential usefulness of computer-aided ultrasound for staging of hepatic steatosis...|$|E
40|$|Fatty liver (steatosis) {{occurs in}} obese patients, among others, and {{is related to}} the {{development}} of diabetes type- 2. Timely diagnosis of steatosis is therefore of great importance. Steatosis is also the most common liver disease of high-yielding dairy cattle during early lactation. This makes it a suitable animal model for studying liver steatosis. Furthermore, reference of derived ultrasound parameters against a "gold standard" is possible in cattle by taking a liver biopsy for the assessment of fat concentration. The authors undertook this pilot study to investigate the hypothesis that quantitative, computer-aided B-mode ultrasound enables the noninvasive detection of hepatic steatosis. Echographic images were obtained postpartum from dairy cows (n = 12) in transcutaneous and direct (intraoperative) applications using a convex array transducer at 4. 2 MHz. During surgery, a biopsy was taken from the caudate lobe to assess the liver fat content (fat score). A custom-designed software package for computer-aided ultrasound diagnosis (CAUS) was developed. After linearizing the post-processing look-up-table (LUT), the image gray levels were transferred into echo levels in decibels relative to the mean echo level in a tissue-mimicking phantom. The quantitative comparison of transcutaneous and intraoperative images enabled the correction for the attenuation effect of skin and subcutaneous fat layer on the mean echo level in the liver, {{as well as for the}} effects of the beam formation and attenuation of liver tissue on the echo level vs. depth. The <b>residual</b> <b>attenuation</b> coefficient (dB/cm) in fatty liver vs. normal liver was estimated and compensated for. Finally, echo level was estimated relative to the phantom used for calibration, and echo texture was characterized by the mean axial and lateral speckle size within the regions of interest. In the no fat/low fat group (n = 5) skin plus fat layer attenuation was 3. 4 dB/cm. A correlation of skin layer thickness vs. fat score of r = 0. 48 was found. The mean transcutaneous liver tissue echo level correlated well with fat score: r = 0. 80. A residual liver attenuation coefficient of 0. 76 dB/cm and 1. 19 dB/cm was found in medium and high fat liver, respectively. In transcutaneous images, correlation of <b>residual</b> <b>attenuation</b> coefficient with fat score was r = 0. 69. Axial and lateral speckle sizes were on the order of 0. 2 and 1. 0 cm, respectively, and no correlation was found with liver fat content. Results for transcutaneous and intraoperative images were similar. The authors conclude that this pilot study shows the feasibility of calibrated, computer-aided ultrasound for noninvasively diagnosing, possibly even screening, steatosis of the liver...|$|E
40|$|The aim of {{this study}} was to test the {{hypothesis}} that quantitative analysis of transcutaneous (Transc) ultrasound (US) images can predict the liver fat content with similar accuracy and precision as using intraoperative (Intraop) US. The second goal was to investigate if a tissue mimicking phantom (TMP) might be used as reference for automatic gain compensation (AGC) vs. depth instead of using the data of a set of cows without hepatic alterations. A study was performed in post partum dairy cows (N = 151), as an animal model of human nonalcoholic fatty liver disease (NAFLD), to test these hypotheses. Five Transc and five Intraop US liver images were acquired in each animal and a liver biopsy was taken. In liver tissue samples, triacylglycerol (TAG) content was measured by biochemical analysis and hepatic alterations, other than hepatic steatosis, were excluded by clinical examination. Several preprocessing steps were performed before the ultrasound tissue characteristics (UTC) parameters of B-mode images were derived. Stepwise multiple linear regression analysis was performed on a training set (N = 76) and the results were used on the test group (N = 75) to predict the TAG content in the liver. In all cases, the <b>residual</b> <b>attenuation</b> coefficient (ResAtt) was the only selected parameter. Receiver operating characteristics (ROC) analysis was applied to assess the performance and area under the curve (AUC) of predicting TAG and to compare the sensitivity and specificity of the methods used. High ROC values for AUC (95 %), sensitivity (87 %) and specificity (83 %) for both Intraop and Transc applications with control group as well as with phantom-based AGC were obtained. Consequently, it can be concluded that Transc results are equivalent to Intraop results. Furthermore, equivalent ROC values, when using TMP AGC, indicates the potential use of TMP-based corrections instead of normal group-based corrections. The high predictive values indicate that noninvasive quantitative US has a great potential for staging and screening on hepatic steatosis in cows...|$|E
40|$|Multiple-day food {{records or}} 24 -hour dietary recalls (24 HRs) are {{commonly}} used as “reference ” instruments to calibrate food frequency questionnaires (FFQs) and to adjust findings from nutritional epidemiologic studies for measurement error. Correct adjustment requires that the errors in the adopted reference instrument be independent {{of those in the}} FFQ and of true intake. The authors report data from the Observing Protein and Energy Nutrition (OPEN) Study, conducted from September 1999 to March 2000, in which valid reference biomarkers for energy (doubly labeled water) and protein (urinary nitrogen), together with a FFQ and 24 HR, were observed in 484 healthy volunteers from Montgomery County, Maryland. Accounting for the reference biomarkers, the data suggest that the FFQ leads to severe attenuation in estimated disease relative risks for absolute protein or energy intake (a true relative risk of 2 would appear as 1. 1 or smaller). For protein adjusted for energy intake by using either nutrient density or nutrient <b>residuals,</b> the <b>attenuation</b> is less severe (a relativ...|$|R
30|$|The linear {{attenuation}} coefficient of water {{was very close to}} the theoretical value (0.153 /cm at 140 keV [27]). However, for Teflon, it was lower than the theoretical value (0.301 /cm at 140 keV [27]) by about 10 % for Brightview, 14 % for Infinia, 22 % for Symbia, and 30 % for Discovery. Air data are not reported in detail; this point is further discussed in the ‘Discussion’ section. For Brightview cameras, using the CT fast protocol with low tube current or the CT slow protocol with high tube current resulted in almost identical <b>residual</b> fraction and <b>attenuation</b> coefficient values.|$|R
40|$|The {{selection}} of attenuated mutants of a virus related to A 2 /Tokyo/ 3 / 67 {{and of the}} subtype A 2 /Hong Kong/ 1 / 68 is described. By passing the former {{in the presence of}} heated horse serum it was possible to obtain a strain wholly resistant to thermostable horse-serum inhibitor. A 2 /Hong Kong/ 1 / 68 was, however, rendered only partially resistant by this technique. In subsequent volunteer trials it was clear that inhibitor-resistance was a marker of <b>attenuation.</b> <b>Residual</b> inhibitor-sensitivity was associated with some remaining pathogenicity. Inhibitor-resistant viruses were infective and antigenic...|$|R
40|$|The National Institute for Occupational Safety and Health (NIOSH) {{sponsored}} {{tests of}} three earplug fit-test systems (NIOSH HPD Well-Fit, Michael & Associates FitCheck, and Honeywell Safety Products VeriPRO). Each system {{was compared to}} laboratory-based real-ear attenuation at threshold (REAT) measurements in a sound field according to ANSI/ASA S 12. 6 - 2008 at the NIOSH, Honeywell Safety Products, and Michael & Associates testing laboratories. An identical study was conducted independently at the U. S. Army Aeromedical Research Laboratory (USAARL), which provided their data for inclusion in this article. The Howard Leight Airsoft premolded earplug was tested with twenty subjects {{at each of the}} four participating laboratories. The occluded fit of the earplug was maintained during testing with a soundfield-based laboratory REAT system as well as all three headphone-based fit-test systems. The Michael & Associates lab had the highest average A-weighted attenuations and smallest standard deviations. The NIOSH lab had the lowest average attenuations and the largest standard deviations. Differences in octave-band attenuations between each fit-test system and the American National Standards Institute (ANSI) sound field method were calculated (Attenfit-test - AttenANSI). A-weighted attenuations measured with FitCheck and HPD Well-Fit systems demonstrated approximately 21220 dB agreement with the ANSI sound field method, but A-weighted attenuations measured with the VeriPRO system underestimated the ANSI laboratory attenuations. For each of the fit-test systems, the average A-weighted attenuation across the four laboratories was not significantly greater than the average of the ANSI sound field method. Standard deviations for <b>residual</b> <b>attenuation</b> differences were about 21220 dB for FitCheck and HPD Well-Fit compared to 21420 dB for VeriPRO. Individual labs exhibited a range of agreement from less than a dB to as much as 9. 420 dB difference with ANSI and REAT estimates. Factors such as the experience of study participants and test administrators, and the fit-test psychometric tasks are suggested as possible contributors to the observed results. CC 999999 /Intramural CDC HHS/United States 2018 - 04 - 01 T 00 : 00 : 00 Z 27786602 PMC 552414...|$|E
40|$|PURPOSE: In (3 D) ultrasound, {{accurate}} discrimination {{of small}} solid masses is difficult, {{resulting in a}} high frequency of biopsies for benign lesions. In this study, we investigate whether 3 D quantitative breast ultrasound (3 DQBUS) analysis {{can be used for}} improving non-invasive discrimination between benign and malignant lesions. METHODS AND MATERIALS: 3 D US studies of 112 biopsied solid breast lesions (size < 1 cm), were included (34 fibroadenomas and 78 invasive ductal carcinomas). The lesions were manually delineated and, based on sonographic criteria used by radiologists, 3 regions of interest were defined in 3 D for analysis: ROI (ellipsoid covering the inside of the lesion), PER (peritumoural surrounding: 0. 5 mm around the lesion), and POS (posterior-tumoural acoustic phenomena: region below the lesion with the same size as delineated for the lesion). After automatic gain correction (AGC), the mean and standard deviation of the echo level within the regions were calculated. For the ROI and POS also the <b>residual</b> <b>attenuation</b> coefficient was estimated in decibel per cm [dB/cm]. The resulting eight features were used for classification of the lesions by a logistic regression analysis. The classification accuracy was evaluated by leave-one-out cross-validation. Receiver operating characteristic (ROC) curves were constructed to assess the performance of the classification. All lesions were delineated by two readers and results were compared to assess the effect of the manual delineation. RESULTS: The area under the ROC curve was 0. 86 for both readers. At 100 % sensitivity, a specificity of 26 % and 50 % was achieved for reader 1 and 2, respectively. Inter-reader variability in lesion delineation was marginal and did not affect the accuracy of the technique. The area under the ROC curve of 0. 86 was reached for the second reader when the results of the first reader were used as training set yielding a sensitivity of 100 % and a specificity of 40 %. Consequently, 3 DQBUS would have achieved a 40 % reduction in biopsies for benign lesions for reader 2, without a decrease in sensitivity. CONCLUSION: This study shows that 3 DQBUS is a promising technique to classify suspicious breast lesions as benign, potentially preventing unnecessary biopsies...|$|E
40|$|The aim of {{this study}} was to test the {{hypothesis}} that automatic segmentation of vessels in ultrasound (US) images can produce similar or better results in grading fatty livers than interactive segmentation. A study was performed in postpartum dairy cows (N= 151), as an animal model of human fatty liver disease, to test this hypothesis. Five transcutaneous and five intraoperative US liver images were acquired in each animal and a liverbiopsy was taken. In liver tissue samples, triacylglycerol (TAG) was measured by biochemical analysis and hepatic diseases other than hepatic lipidosis were excluded by histopathologic examination. Ultrasonic tissue characterization (UTC) parameters [...] Mean echo level, standard deviation (SD) of echo level, signal-to-noise ratio (SNR), <b>residual</b> <b>attenuation</b> coefficient (ResAtt) and axial and lateral speckle size [...] were derived using a computer-aided US (CAUS) protocol and software package. First, the liver tissue was interactively segmented by two observers. With increasing fat content, fewer hepatic vessels were visible in the ultrasound images and, therefore, a smaller proportion of the liver needed to be excluded from these images. Automatic-segmentation algorithms were implemented and it was investigated whether better results could be achieved than with the subjective and time-consuming interactive-segmentation procedure. The automatic-segmentation algorithms were based on both fixed and adaptive thresholding techniques in combination with a 'speckle'-shaped moving-window exclusion technique. All data were analyzed with and without postprocessing as contained in CAUS and with different automated-segmentation techniques. This enabled us to study the effect of the applied postprocessing steps on single and multiple linear regressions ofthe various UTC parameters with TAG. Improved correlations for all US parameters were found by using automatic-segmentation techniques. Stepwise multiple linear-regression formulas where derived and used to predict TAG level in the liver. Receiver-operating-characteristics (ROC) analysis was applied to assess the performance and area under the curve (AUC) of predicting TAG and to compare the sensitivity and specificity of the methods. Best speckle-size estimates and overall performance (R 2 = 0. 71, AUC = 0. 94) were achieved by using an SNR-based adaptive automatic-segmentation method (used TAG threshold: 50 mg/g liver wet weight). Automatic segmentation is thus feasible and profitable...|$|E
40|$|This paper {{describes}} a complete fade counter measure system, based on up-link transmission power control, plus data coding and bit rates variation. The {{system is designed}} for thin route TDMA user-oriented satellite networks, used for LAN interconnection. A multicarrier access to the satellite transponder is envisaged to exploit the entire transponder bandwidth with limited data rates of each carrier. The modest performance required by the earth stations means that the antennas can easily be installed on the user?s premises. In order to avoid excessive intermodulation noise, the satellite transponder in put back-off must be constant and sufficient to in operate in the linear zone. This imposes a calibrated action of the up?link power control, which is possible only by knowing the up-link attenuation with good accuracy. The total signal degradation, due to the <b>residual</b> up?link <b>attenuation</b> after up?power control intervention, and to the down-link attenuation,is compensated for by varying the coding and bit rates of the data signal to noise estimator, based on the statistics of quantised levels of the demodulated PSK signal and a narrow band signal level estimator are employed to apply the countermeasures {{that are able to}} maintain the data bit error rate required by the use...|$|R
30|$|The <b>residual</b> PET signal <b>attenuation</b> of {{rigid and}} {{stationary}} {{equipment such as}} RF coils can be compensated by straightforward AC methods. Predefined attenuation maps (templates) for the patient table and non-flexible RF coils thus are usually added to the patient tissue attenuation map prior to the PET reconstruction. These templates are based on CT transmission scans of the patient table or RF coils, providing an exact 3 D representation of the spatial distribution of attenuation factors in a virtual model of the respective hardware component [24, 25]. By linking the current patient table position during a patient examination to the known position of the hardware component on the table, template-based AC can be automatically performed during the PET data reconstruction process [7, 22]. This method for hardware component AC is an established standard in the current commercially available PET/MRI systems.|$|R
40|$|PURPOSE: The aim of {{this study}} was to compare a 1 -day with a 2 -day iodine bowel {{preparation}} for CT colonography in a positive faecal occult blood test (FOBT) screening population. MATERIALS AND METHODS: One hundred consecutive patients underwent CT colonography and colonoscopy with segmental unblinding. The first 50 patients (group 1) ingested 7 50 ml iodinated contrast starting 2 days before CT colonography. The latter 50 patients (group 2) ingested 4 50 ml iodinated contrast starting 1 day before CT colonography. Per colonic segment measurements of <b>residual</b> stool <b>attenuation</b> and homogeneity were performed, and a subjective evaluation of tagging quality (grade 1 - 5) was done. Independently, two reviewers performed polyp and carcinoma detection. RESULTS: The tagging density was 638 and 618 HU (p = 0. 458) and homogeneity 91 and 86 HU for groups 1 and 2, respectively (p = 0. 145). The tagging quality was graded 5 (excellent) in 90 % of all segments in group 1 and 91 % in group 2 (p = 0. 749). Mean per-polyp sensitivity for lesions >or= 10 mm was 86 % in group 1 and 97 % in group 2 (p = 0. 355). Patient burden from diarrhoea significantly decreased for patients in group 2. CONCLUSIONS: One-day preparation with meglumine ioxithalamate results in an improved patient acceptability compared with 2 -day preparation and has a comparable, excellent image quality and good diagnostic performanc...|$|R
40|$|The {{aim was to}} {{test the}} {{accuracy}} of calibrated digital analysis of ultrasonographic hepatic images for diagnosing fatty liver in dairy cows. Digital analysis was performed {{by means of a}} novel method, computer-aided ultrasound diagnosis (CAUS), previously published by the authors. This method implies a set of pre- and postprocessing steps to normalize and correct the transcutaneous ultrasonographic images. Transcutaneous hepatic ultrasonography was performed before surgical correction on 151 German Holstein dairy cows (mean +/- standard error of the means; body weight: 571 +/- 7 kg; age: 4. 9 +/- 0. 2 yr; DIM: 35 +/- 5) with left-sided abomasal displacement. Concentration of triacylglycerol (TAG) was biochemically determined in liver samples collected via biopsy and values were considered the gold standard to which ultrasound estimates were compared. According to histopathologic examination of biopsies, none of the cows suffered from hepatic disorders other than hepatic lipidosis. Hepatic TAG concentrations ranged from 4. 6 to 292. 4 mg/g of liver fresh weight (FW). High correlations were found between the hepatic TAG and mean echo level (r= 0. 59) and <b>residual</b> <b>attenuation</b> (ResAtt; r= 0. 80) obtained in ultrasonographic imaging. High correlation existed between ResAtt and mean echo level (r= 0. 76). The 151 studied cows were split randomly into a training set of 76 cows and a test set of 75 cows. Based on the data from the training set, ResAtt was statistically selected by means of stepwise multiple regression analysis for hepatic TAG prediction (R(2) = 0. 69). Then, using the predicted TAG data of the test set, receiver operating characteristic analysis was performed to summarize the accuracy and predictive potential of the differentiation between various measured hepatic TAG values, based on TAG predicted from the regression formula. The area under the curve values of the receiver operating characteristic based on the regression equation were 0. 94 (or= 50 mg of TAG/g of FW), 0. 83 (or= 100 mg of TAG/g of FW), and 0. 97 (or= 100 mg of TAG/g of FW). The CAUS methodology and software for digitally analyzing liver ultrasonographic images is considered feasible for noninvasive screening of fatty liver in dairy herd health programs. Using the single parameter linear regression equation might be ideal for practical applications...|$|E
40|$|In recent years, several {{attempts}} have been made to evaluate the activity of a corpus luteum by determining its sonographic echo texture. In all of these studies the values of the echo texture parameters depended on the type and settings of the ultrasound machine. Therefore, the aim of the study was to investigate if a quantitative analysis of ultrasound (US) images of the corpus luteum (CL) after calibration of the ultrasound machine enables the assessment of the peripheral plasma progesterone (P 4) level. Ten Holstein Friesian cows were examined daily at Days 4 to 8, 10 to 16, and - 5 to - 1 (Day 1 =ovulation) of the estrous cycle. B-mode sonography of the corpora lutea was performed and blood samples were taken for plasma P 4 analysis. US images were calibrated and analyzed using a software package (CAUS) developed by the authors. In addition to the area of the CL (Total Area, TotA; Tissue Area interactive, TisAi; Tissue Area Automatic, TisAa), the following US parameters were calculated from the gray level histogram and from the size of the speckles: Mean, Standard Deviation (SD) and Signal-to-Noise Ratio (SNR=Mean/SD) of echo levels, <b>Residual</b> <b>Attenuation</b> (ResAtt), Axial and Lateral speckle size (Ax and Lat, respectively). The inter-individual variability of the P 4 level was expressed by the coefficient of variability (CV), averaged over all days. It appeared that the CV of the absolute P 4 was high (0. 65) and the P 4 relative to that at Day 4 and at Day 16 was of comparable magnitude. Correlations of US parameters with P 4 were highest for the P 4 relative to Day 16 (P 4 _rel_D 16). This relative P 4 measure was then used for further analysis. The correlations of P 4 _rel_D 16 with TotA, TisAa (CL area after automatic segmentation of tissue) and ResAtt were found the highest (R= 0. 68, 0. 74, and - 0. 42, respectively). Multiple linear regression analysis, incorporating all US parameters revealed the formula: P 4 _rel_D 16 (pred) =- 0. 315 + 0. 225 TisAa- 0. 023 ResAtt, and a goodness of fit: R(2) = 0. 59 (p 0. 80 (corresponding to 0. 95 times the average_P 4 _rel_D 16 measured during the "static" phase of the luteal cycle) by ROC analysis was correctly made in 88 % of cases. In conclusion the quantitative analysis of calibrated ultrasound images may yield a good prediction of cyclic changes of P 4 levels and has potential for predicting the phase in the estrous cycle of a cow...|$|E
40|$|In this thesis, three {{different}} post-filtering algorithms for acoustic <b>residual</b> echo <b>attenuation</b> {{have been studied}} and simulated {{to see if the}} algorithms work in case of network echoes. The simulations were carried out using real recorded speech signals. The results are presented by different plots. The post-filter was implemented in the frequency domain due to the lower computation complexity. The main drawback of this is that an additional time delay will occur, when using block wise calculation. Finally, results of the post-filter and those obtained with the Non Linear Processor (NLP) were compared to see if the NLP can be replaced by the post-filter or not. The second algorithm, the overweighted wiener filter yields a very high attenuation of the Near End (NE) signal if the overweight is too great. This means that some of the NE frequencies is represented in the Far End (FE) spectrum. The third algorithm gives the lowest attenuation in case of no NE noise and modulates the NE noise mostly. The first algorithm gives the best overall performance. The simulation results shows that the NE and FE signals must be fully separated in frequency to make the post-filter working at optimum. As the post-filter modulates the NE noise, comfort noise has been injected and tested with simulations using the third algorithm. The estimation and injection of comfort noise works best when the NE noise is of a fairly stationary nature. As the main requirement is to remove the entire residual echo, the NLP can not be replaced with the post filter as it leaves some residual echo behind...|$|R
40|$|Aftershock {{recordings}} from 10 digital seismographs deployed {{along the}} axis of the San Francisco Peninsula from {{the epicenter of the}} 18 October 1989 Loma Prieta earthquake to San Francisco provide the data for a joint inversion for source, wave propagation, and site effects. The inversion procedure is a least-squares method using the singular-value decomposition algorithm and is done in two parts: The first solves only for the source and attenuation terms for all events, and the second projects the residuals from the first inversion onto the site terms. The coefficient of geometrical spreading is solved for as part of the model of <b>attenuation.</b> <b>Residuals,</b> plotted versus distance and frequency, provide a check on the appropriateness of the attenuation model [...] The earthquakes are all located near the San Andreas fault, mostly in the aftershock zone of the Loma Prieta mainshock, but some are near Daly City on the Peninsula. The data set, provided by GEOS recorders, consists of 148 digital seismograms for 33 events from 10 sites that form a linear profile from the epicentral region to the city of San Francisco, a distance of approximately 10...|$|R
40|$|Geophysical Research Abstracts, Vol. 11, EGU 2009 - 5699, 2009 EGU General Assembly 2009 © Author(s) 2009 A {{new version}} of "Boxer" code for the {{determination}} of seismic source parameters from macroseismic data. P. Gasperini (1), G. Vannucci (2), and D. Tripone (3) (1) Dipartimento di Fisica, Università di Bologna, Viale Berti-Pichat 8, I- 40127 Bologna (Italy), paolo. gasperini@unibo. it, (2) INGV-Istituto Nazionale di Geofisica e Vulcanologia, Via Donato Creti, 12, I- 40128, Bologna (Italy), (vannucci@bo. ingv. it), (3) INGV-Istituto Nazionale di Geofisica e Vulcanologia, Via Donato Creti, 12, I- 40128, Bologna (Italy), (tripone@bo. ingv. it) About {{ten years after the}} first release of the code we implemented new methods of epicentral location and magnitude computation as well as a procedure for the evaluation of uncertainties by the bootstrap technique. We also developed a user-friendly interface for parameter setup and graphical post-processing of the results. The improved code allows to locating epicenters in the sea or in uninhabited areas by minimizing the norm of the <b>residuals</b> of an <b>attenuation</b> equation. The same approach also per-mits, in the most favorable cases, the estimation of the source depth. By the geographical rendering of the bootstrap solutions we give a tool for characterizing the possible multiplicity of the seismic source of historical earthquakes...|$|R
40|$|There has {{recently}} been a growing awareness that natural processes are degrading contaminants of concern, and that the contribution these natural processes make to achieving cleanup goals needs to be formally considered during site-specific cleanup. Historical case data from {{a large number of}} releases has been used to evaluate the expectation for natural attenuation to contribute to the cleanup of petroleum hydrocarbons and chlorinated solvents. The use of historical case data has several advantages, among them: 1) sites can reduce characterization costs by sharing information on key hydrogeologic parameters controlling contaminant fate and transport, and 2) standard reference frameworks can be developed that individual sites can use as a basis of comparison regarding plume behavior. Definition of cleanup times must take into account basic constraints imposed by natural laws governing the transport and natural degradation process of petroleum hydrocarbons. The actual time to reach groundwater cleanup goals is determined by these laws and the limitations on <b>residual</b> subsurface contamination <b>attenuation</b> rates, through either active or natural biological processes. These limitations will practically constrain the time to achieve low concentration cleanup goals. Recognition is needed that sites will need to be transitioned to remediation by natural processes at some point following implementation of active remediation options. The results of an analysis of approximately 1800 California and 600 Texas fuel hydrocarbon (FHC) releases and 2. 50 chlorinated volatile organic compound (CVOC) plumes will be summarized. Plume lengths and natural biodegradation potential were evaluated. For FHC releases, 90 % of benzene groundwater plumes were less than 280 feet in length and evidence of natural biodegradation was found to be present at all sites studied in detail. For CVOC releases, source strength and groundwater flow velocity are dominant factors controlling groundwater plume lengths. After adjusting for these factors, biodegradation also appears to limit the length of CVOC plumes in many instances. The application of natural biodegradation processes as a remediation approach will depend on the time frame for anticipated beneficial use of the affected groundwater...|$|R

