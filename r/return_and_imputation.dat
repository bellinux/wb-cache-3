0|10000|Public
40|$|Features of {{census data}} make the editing <b>and</b> <b>imputation</b> phase a complex matter. Complex editing <b>and</b> <b>imputation</b> tasks can be tackled by {{dividing}} the editing <b>and</b> <b>imputation</b> process into subphases characterized by different problems, and finding appropriate solutions for each of them. An experimental application of the approach of combining different currently used methods for the editing <b>and</b> <b>imputation</b> of population census data is presented. Copyright 2004 Royal Statistical Society. ...|$|R
40|$|Data editing {{plays an}} {{important}} role in the survey process. The National Agricultural Statistics Service currently uses, in addition to some manual editing, an interactive micro-level edit system or a batch micro-level edit system, and an interactive macro-level edit system to edit reported data. Advantages of using these two edit systems are that: 1) the most complex edits can be incorporated and 2) the impact of editing at aggregate levels can be readily evaluated. There are, however, disadvantages with the use of the two edit systems: 1) a considerable amount of time and resources may be expended and 2) editing may not always be performed in a consistent manner. This paper evaluates a generalized automated edit <b>and</b> <b>imputation</b> system developed by the author called the Agricultural Generalized <b>Imputation</b> <b>and</b> Edit System (AGGIES). The AGGIES is appealing for the following reasons: 1) editing <b>and</b> <b>imputation</b> are fully automated, 2) the system provides consistency in the edit <b>and</b> <b>imputation</b> process, <b>and</b> 3) the system can be easily applied to any number of surveys, thus conserving resources to the development and maintenance of a single system. Comparisons between the AGGIES and the current edit <b>and</b> <b>imputation</b> procedures are made for expanded totals and the number and magnitude of variable changes. The data used for these comparisons are obtained from the Quarterly Hog Survey. The results reveal that the expanded totals obtained from using the AGGIES are similar to those obtained from the current edit <b>and</b> <b>imputation</b> procedures. Further testing on more applications is recommended...|$|R
40|$|In this report, we {{demonstrate}} the CANCEIS (CANadian Census Edit <b>and</b> <b>Imputation</b> System) experiments of edit <b>and</b> <b>imputation</b> with the 2006 test data. The major effort is {{to translate the}} if-then-else rules of current edit <b>and</b> <b>imputation</b> system of the decennial census into the decision logic tables (DLT) of CANCEIS. We also formulate the input files that are needed to run the CANCEIS software. The advantages of using DLT are that {{it is easy to}} understand the edit rules; and DLTs are input, not part of the software, making it easier to change when edit rules are changed. We also compare the imputation results between the CANCEIS experiments and the 2006 Census Edited File. The comparison is for our curiosity beause the constructed DLTs are not identical to the edit rules specified in the 2006 edit specs. Although the edit rules are not identical, the comparison still shows some similarities between the CEF andCANCEISresults. The CANadian Census Edit <b>and</b> <b>Imputation</b> System (CANCEIS) was designed based on the Nearest-Neighbor Imputation Methodology (NIM) developed by Mike Bankier of Statistics Canada in 1992. CANCEIS works with three sources of information provided by the user: (1) unedited input data files (2006 Census Tes...|$|R
40|$|Data editing <b>and</b> <b>imputation</b> (E&I) {{in complex}} sample {{business}} surveys {{is a task}} which is usually split into two steps to gain efficiency {{in terms of time}} and human resources: first selective editing techniques are applied to the primary target estimates variables in order to identify a potential set of influential errors that require usually manual editing and a second part of automatic identification <b>and</b> <b>imputation</b> of inconsistencies <b>and</b> missing values. Within this framework, the present paper reviews the Italian top-down data editing strategy adopted <b>and</b> automated <b>imputation</b> showing the experience applied to 2013 Farm Structure Survey livestock data. In this edition this process has been entirely carried out in the R environment by means of different R packages...|$|R
40|$|This paper {{estimates}} {{the impact of}} the debt tax shield, cash dividends <b>and</b> <b>imputation</b> tax credits on the prices of Australian stock index futures. Relative to futures payoffs, the cost of financing the set of shares of the underlying index provides a mild tax shield, cash dividends are incompletely valued <b>and</b> <b>imputation</b> credits are worth at least fifty percent of their face value. The values that investors place on cash dividends and tax credits implied by index futures prices are close to those estimated in ex-dividend-date stock-price drop-off studies of the Australian share market. 16 page(s...|$|R
40|$|This article {{introduces}} yaImpute, an R {{package for}} nearest neighbor search <b>and</b> <b>imputation.</b> Although nearest neighbor imputation {{is used in}} a host of disciplines, the methods implemented in the yaImpute package are tailored to imputation-based forest attribute estimation and mapping. The impetus to writing the yaImpute is a growing interest in nearest neighbor imputation methods for spatially explicit forest inventory, and a need within this research community for software that facilitates comparison among different nearest neighbor search algorithms <b>and</b> subsequent <b>imputation</b> techniques. yaImpute provides directives for defining the search space, subsequent distance calculation, <b>and</b> <b>imputation</b> rules for a given number of nearest neighbors. Further, the package offers a suite of diagnostics for comparison among results generated from different <b>imputation</b> analyses <b>and</b> a set of functions for mapping imputation results. ...|$|R
40|$|This study compares <b>imputation</b> methods (single <b>and</b> multiple) {{to examine}} the role of {{perceived}} stress {{in the relationship between}} social support and mood, and tested whether mediator effects influenced the relationship. The cross-sectional data reported here was collected in an experimental design with repeated measures with mothers of children who had been hospitalized in a child psychiatric unit. These methods included no <b>imputation,</b> single <b>imputation,</b> <b>and</b> multiple <b>imputation</b> for missing values. The results did not indicate any mediator effects for coping in the relationship between perceived stress and mood. These results were similar when <b>imputation</b> <b>and</b> no <b>imputations</b> methods were used. However, researchers should consider using imputation results were similar when <b>imputation</b> <b>and</b> no <b>imputations</b> methods were used. However, researchers should consider using imputation methods to help improve problems caused by missing values in the study...|$|R
40|$|Variance {{estimation}} after imputation is {{an important}} practical problem in survey sampling. When deterministic imputation or stochastic imputation is used, we show that the variance of the imputed estimator can be consistently estimated by a unifying linearize and reverse approach. We provide some applications of the approach to regression imputation, fractional categorical <b>imputation,</b> multiple <b>imputation</b> <b>and</b> composite <b>imputation.</b> Results from a simulation study, under a factorial structure for the sampling, response <b>and</b> <b>imputation</b> mechanisms, show that the proposed linearization variance estimator performs well in terms of relative bias, assuming a missing at random response mechanism. Copyright 2009, Oxford University Press. ...|$|R
30|$|NEPS {{data are}} also methodologically sophisticated. Given the large sample size on {{individual}} {{as well as}} institutional level, even multilevel structural equation modeling can be applied without any problems. Special {{attention is paid to}} issues of weighting <b>and</b> <b>imputation.</b>|$|R
40|$|In National Statistical Offices (NSOs) the {{analysis}} of the effects on data of any processing activity has progressively assumed a central role. In this paper we concentrate on the problem of evaluating and documenting data editing <b>and</b> <b>imputation</b> processes (E&I), <b>and</b> in particular the E&I activities performe...|$|R
5000|$|List of {{haplotype}} estimation <b>and</b> genotype <b>imputation</b> software ...|$|R
40|$|We {{describe}} an algorithm for the fitting of multivariate responses using classification and regression trees, named the intersection-seeking algorithm. Although motivated by problems of record linkage <b>and</b> <b>imputation</b> of missing values in surveys, the algorithm {{may be used}} in other contexts. File completion <b>Imputation</b> Regression <b>and</b> classification trees...|$|R
40|$|In 2006 - 2007, the Economic Directorate of the United States Census Bureau {{conducted}} {{a series of}} studies to assess processing procedures of several of its Economics surveys and censuses in the hope of targeting areas of improvement. A subgroup was assigned to focus specifically on editing <b>and</b> <b>imputation</b> procedures. This subgrou...|$|R
40|$|We discuss {{variance}} estimation by resampling in {{surveys in}} which data are missing. We derive {{a formula for}} jackknife linearization {{in the case of}} calibrated estimation with deterministic regression <b>imputation,</b> <b>and</b> compare the resulting variance estimates with balanced repeated replication with and without grouping, the bootstrap, the block jackknife, <b>and</b> multiple <b>imputation,</b> for simulated data based on the Swiss Household Budget Survey. Jackknife linearisation, the bootstrap, <b>and</b> multiple <b>imputation</b> perform best in terms of relative bias and mean square error...|$|R
40|$|Abstract. Missing {{data are}} quite common in {{practical}} applications of statistical methods <b>and</b> <b>imputation</b> {{is a general}} statistical method {{for the analysis of}} incomplete data sets. Stekhoven and Bühlmann (2012) proposed an iterative imputation method (called “missForest”) based on Random Forests (Breiman 2001) to cope with missing values. In the paper a short description of “missForest ” is presented and some selected missing data techniques are compared with “missForest ” by artificially simulating different proportions and mechanisms of missing data using complete data sets from the UCI repository of machine learning databases. Key words: missing values, single <b>and</b> multiple <b>imputation,</b> random forests, missForest...|$|R
40|$|Abstract: Editing of data {{collected}} for preparation of statistics {{is a time}} and resource consuming process. This paper presents experiments with artificial neural networks as a potential tool for increasing the effectiveness of statistical editing <b>and</b> <b>imputation.</b> To maintain accuracy in resulting statistics, the possibility of deriving reliable accuracy predictions is also discussed. 1...|$|R
40|$|Cooperative {{interval}} {{games are}} a generalized model of cooperative {{games in which}} the worth of every coalition corresponds to a closed interval representing the possible outcomes of its cooperation. Selections are all possible outcomes of the interval game with no additional uncertainty. We introduce new selection-based classes of interval games and prove their characterization theorems and relations to existing classes based on the interval weakly better operator. We show new results regarding the core <b>and</b> <b>imputations</b> <b>and</b> examine a problem of equivalence between two {{different versions of the}} core, which is the main stability solution of cooperative games. Then we introduce the definition of strong <b>imputation</b> <b>and</b> strong core as a universal solution concept of interval games...|$|R
40|$|Abstract. Missing {{data are}} common in surveys {{regardless}} of research field, undermining statistical analyses and biasing results. One solution is to use an imputation method, which recovers missing data by estimating replacement values. Previously, we have evaluated the hot-deck k-Nearest Neighbour (k-NN) method with Likert data in a software engineering context. In this paper, we extend the evaluation by benchmarking the method against four other imputation methods: Random Draw Substitution, Random <b>Imputation,</b> Median <b>Imputation</b> <b>and</b> Mode <b>Imputation.</b> By simulating both non-response <b>and</b> <b>imputation,</b> we obtain comparable performance measures for all methods. We discuss the performance of k-NN {{in the light of}} the other methods, but also for different values of k, different proportions of missing data, different neighbour selection strategies and different numbers of data attributes. Our results show that the k-NN method performs well, even when much data are missing, but has strong competition from both Median <b>Imputation</b> <b>and</b> Mode <b>Imputation</b> for our particular data. However, unlike these methods, k-NN has better performance with more data attributes. We suggest that a suitable value of k is approximately the square root of the number of complete cases, and that letting certain incomplete cases qualify as neighbours boosts the imputation ability of the method. ...|$|R
40|$|More than 100 {{programs}} are conducted within the Economic Directorate at the U. S. Census Bureau. Although there are common editing <b>and</b> <b>imputation</b> procedures used throughout, each program has developed procedures that best suit their data. On the surface, {{this appears to}} conflict with the directoratewide movement towards generalized processing systems. Designing flexible systems that offer the mos...|$|R
40|$|In {{the present}} investigation, {{a hybrid of}} {{calibration}} <b>and</b> <b>imputation</b> has been considered {{in the presence of}} a random non-response in survey sampling. Estimators of population mean associated with the proposed hybrid remain unbiased under design based approach. A simulation study is carried out to show the performance of the proposed methods compared to the ratio method of imputation...|$|R
40|$|The U. S. Census Bureau's Annual Capital Expenditures Survey (ACES) {{collects}} {{data about}} domestic capital expenditures in non-farm businesses operating within the United States. Analysts manually edit the ACES data using a specified set of editing rules. Although individual edits are straightforward, the hierarchical combination of edits are complicated with several nested levels of simultaneous balance requirements. We investigate {{the feasibility of}} replacing the current ACES editing procedures with an automated system based on National Agricultural Statistics Service's generalized edit <b>and</b> <b>imputation</b> system (AGGIES). The AGGIES system solves simultaneous linear-inequality edits using Chernikova-type algorithms for determining the minimum number of fields to change so that a record satisfies all edits. These algorithms can simultaneously deal {{with a large number}} of mathematical constraints and have been successfully applied in Statistics Canada's Generalized Edit <b>and</b> <b>Imputation</b> System <b>and</b> Statistics Netherlands' CherryPI system. Key Words: data editing, Fellegi-Holt model, error localization 1...|$|R
40|$|The paper describes, {{on the one}} hand, the {{procedures}} implemented to maximise the response rates and prevent response errors and, on the other hand, those for editing <b>and</b> <b>imputation</b> on the collected data in the quarterly Istat survey on job vacancies and hours worked. The strategy on which {{the design of the}}se procedures is based is analysed. In particular, the three main ideas behind the design of the entire sequence of the above mentioned procedures are described. First of all, the need to organise the data collection phase so as to maximise the response rates and minimise response errors, which informs the choice of the (mainly CATI) data collection technique and the sequence of procedures implemented in all its stages for the error prevention purpose. In the second place, data integration with two other statistical sources, which allows to contain the response burden for enterprises, to rationalise the productive processes and to produce coherent indicators for the three sources. In the third place, the phase of editing <b>and</b> <b>imputation</b> on the collected data. The paper illustrates {{the procedures}} implemented so far, which: concern occupied posts and job vacancies; are based on a careful analysis of the target parameters and the characteristics of the target variable; and use the data of the two auxiliary sources. They include: preliminary steps to automatically edit systematic errors; identification and initial editing of outliers on job vacancies <b>and</b> occupied posts; <b>imputation</b> of these two variables; identification and editing of influential errors, which have a substantial effect on the target parameter estimates on the study domains. Key words: monitoring systems, data integration, editing <b>and</b> <b>imputation</b> techniqu...|$|R
40|$|Abstract: This paper studies {{modeling}} of nonignorable nonresponse in panel surveys. A class of sequential conditional logistic models for nonresponse is considered. Model-based maximum likelihood estimation <b>and</b> <b>imputation</b> {{are used for}} estimating population proportions. Various models are evaluated, and comparisons are made with traditional methods of weighting <b>and</b> direct data <b>imputation.</b> Two cases are considered, (i) the population rate {{of participation in the}} 1989 Norwegian Storting election and (ii) estimation of car ownership in Norway in 1989 and 1990. Keywords: Nonignorable nonresponse, logistic modeling, imputation, election survey, consumer expenditure surve...|$|R
40|$|An {{advanced}} {{knowledge of}} the river condition helps for better source management. This information can be gathered via estimation using DA methods. The DA methods blend the system model with the observation data to obtain the estimated river flow and stage. However, the observation data may contain some missing data due to the hardware power limitations, unreliable channel, sensor failure and etc. This problem limits {{the ability of the}} standard method such as EKF, EnKF and PF. The Multi Imputation Particle Filter (MIPF) able to deal with this problem since it allows for new input data to replace the missing data. The result shows that the performance of the river flow and stage estimation is depending on the number of particles <b>and</b> <b>imputation</b> used. The performance is evaluated by comparing the estimated velocity obtained using the estimated flow and stage, with the measured velocity. The result shows that higher number of particles <b>and</b> <b>imputation</b> ensure better estimation result...|$|R
40|$|Programme of the European Commission. The {{objectives}} of EUREDIT are quite ambitious, and include {{the development of}} new methods for editing <b>and</b> <b>imputation,</b> the comparative evaluation of these methods together with "standard " methods and the dissemination of these results via a software product as well as publications. 2. The main focus of EUREDIT is editing <b>and</b> <b>imputation</b> for the types of data collected by national statistical agencies. Reflecting this, the participants in EURDEDIT include the UK Office for National Statistics, the Netherlands Central Bureau of Statistics, the Italian National Statistical Institute, Statistics Denmark, Statistic Finland and the Swiss Federal Statistics Office. From a methodological point of view, the main "new " methods that will be investigated within EUREDIT will be those based on the application of neural net and related computationally intensive methods, as well as the application of modern outlier robust statistical methods. The neural net and computationally intensive methods will be developed by th...|$|R
40|$|Missing data is {{very common}} in survey research. However, {{currently}} few guidelines exist {{with regard to the}} diagnosis and remedy to missing data in survey research. The goal of this thesis was to investigate properties and effects of three selected missing data techniques (listwise deletion, hot deck <b>imputation,</b> <b>and</b> multiple <b>imputation)</b> via a simulation study, and apply the three methods to address the missing race problem in a real data set extracted from teh National Hospital Discharge Survey. The results of this study showed that multiple <b>imputation</b> <b>and</b> hot deck <b>imputation</b> procedures provided more reliable parameter estimates than did listwise deletion. A similar outcome was observed with respect to the standard errors of the parameter estimates, with the multiple <b>imputation</b> <b>and</b> hot deck <b>imputation</b> producing parameter estimates with smaller standard errors. Multiple imputation outperformed the hot deck imputation by using larger significant levels for variables with missing data and reflecting uncertainty with missing values. In summary, our study showed that employing an appropriate imputation technique to handling missing data in public use surveys is better than ignoring it...|$|R
40|$|The {{development}} of an Integrated System for Editing and Estimation (ISEE) {{is an important part}} in Statistics Norway's strategic plans for improvement of statistical production processes and more efficient use of available data sources. ISEE is organized in applications for different processing functions, and implemented in a service-oriented IT structure. Two of the applications are DYNAREV (for editing <b>and</b> <b>imputation)</b> <b>and</b> STRUKTUR (for estimation of population aggregates). With the two processes of editing and estimation being fully integrated in ISEE, a producer of statistics is now in a much better position to implement the so-called top-down approach to editing, because the effect on the estimates of the population totals due to any changes made to the data can be examined instantly. In this paper we provide an overview of the various tools for prediction <b>and</b> <b>imputation</b> in ISEE. Some of these are well in place whereas others are still being developed. Our main focus is on the construction of a statistical register. We propose and discuss a triple-goal criterion, and assess the alternative imputation methods in accordance, and finally outline a method that is potentially capable of satisfying these needs...|$|R
40|$|Editing <b>and</b> <b>imputation</b> are {{undertaken}} to improve quality in data and statistics {{in virtually all}} National Statistics Offices. Currently imputation methods are typically based on simple statistical ideas such as nearest neighbour, but {{little is known about}} the comparative performance of each method across the wide variety of data sources used. For the purposes of this paper we define “edit” as error localization, i. e. identifying doubtful or erroneous data values. Once incorrect (or missing) values have been identified they will need to be corrected. Imputing new values is often preferred over the alternative of re-weighting because of its simplicity <b>and</b> because <b>imputation</b> provides a dataset that can easily be used by many different future users. Research and development is needed to identify edit <b>and</b> <b>imputation</b> (E&I) methods: • That improve the efficiency of the edit / imputation process compared with current methods • That improve the quality of the edit / imputation process compared with current methods • Are faster than current methods • Can cope with complex data structures that are difficult to specify in terms of simple edit rules • Can deal with mixtures of discrete and continuous data • That ensure consistency with all edit rules specifie...|$|R
40|$|This paper {{addresses}} {{the problem of}} text-independent speaker verification {{in the presence of}} unreliable (masked by noise) features. It presents and assesses several integration <b>and</b> <b>imputation</b> approaches used for unreliable feature compensation in the framework of Gaussian mixture models (GMMs) of speakers. These approaches include marginalisation, bounded integration, mean imputations, integrated speech-background model and Wiener filtering dependent on the most probable Gaussian component. 1...|$|R
40|$|Weighting {{methods are}} {{commonly}} used in situations of unit nonresponse with linked register data. However, several arguments in terms of valid inference and practical usability can be made {{against the use of}} weighting methods in these situations. Imputation methods such as sample <b>and</b> mass <b>imputation</b> may be suitable alternatives, as they lead to valid inference in situations of item nonresponse and have some practical advantages. In a simulation study, sample <b>and</b> mass <b>imputation</b> were compared to traditional weighting when dealing with unit nonresponse in linked register data. Methods were compared on their bias and coverage in different scenarios. Both, sample <b>and</b> mass <b>imputation,</b> had better coverage than traditional weighting in all scenarios...|$|R
40|$|Semicontinuous {{variables}} have {{a proportion}} of responses at some fixed value and a continuous distribution among the remaining responses. Variables of this type occur in economic surveys of individuals or establishments (e. g. specific types of income or expenditures) where distributions are frequently characterized by a mixture of zeros and continuously distributed positive numbers. In this paper, we review strategies for joint statistical modeling <b>and</b> <b>imputation</b> of semicontinuous survey variables. Algorithms are presented for parameter estimation {{in the presence of}} unit and item nonresponse <b>and</b> for the <b>imputation</b> of missing values. Methods and software are demonstrated on data {{from a variety of sources}} including the Consumer Expenditures Survey...|$|R
30|$|The PIRLS 2011 Abu Dhabi {{data was}} {{analyzed}} using the IEA International Database (IDB) Analyzer software (version 4.0. 20). Used {{in conjunction with}} SPSS, the IEA IDB Analyzer applies the sampling weights, implements the jackknife repeated replication method to compute appropriate sampling errors, performs the computations five times for each plausible value, and aggregates the results to produce accurate estimates of average achievement and standard errors that account for both sampling <b>and</b> <b>imputation</b> errors.|$|R
40|$|We derive an {{estimator}} of the asymptotic {{variance of}} both single <b>and</b> multiple <b>imputation</b> estimators. We assume a parametric imputation model but allow for non- and semiparametric analysis models. Our variance estimator, {{in contrast to}} the estimator proposed by Rubin (1987), is consistent even when the <b>imputation</b> <b>and</b> analysis models are misspecified and incompatible with one another...|$|R
40|$|Some nonparametric {{imputation}} techniques, {{including two}} categories: single <b>imputation</b> <b>and</b> multiple <b>imputation,</b> are in- troduced and studied. Some {{properties of the}} estimators such as the bias, the variance, and the mean squared error are pre- sented. Finally, some imputation techniques are applied to a real case. These methods are compared {{in order to assess}} their advantages, disadvantages, and applicabilities...|$|R
30|$|The {{second step}} is to impute the CQ missing data. For each matrix {{sampling}} design, we implement SMI and MMI for the CQ missing data using predictive mean matching (PMM) via the R package MICE (van Buuren and Groothuis-Oudshoorn 2010). Previous research (Kaplan and Su 2016; Kaplan and McCarty 2013) has found predictive mean matching to be quite good with respect to meeting the requirements for the validity of statistical matching <b>and</b> <b>imputation</b> set down by Räassler (2002).|$|R
40|$|Human-disease {{etiology}} can {{be better}} understood with phase information about diploid sequences. We present a method for estimating haplotypes, using genotype data from unrelated samples or small nuclear families, that leads to improved accuracy and speed compared to several widely used methods. The method, segmented haplotype estimation <b>and</b> <b>imputation</b> tool (SHAPEIT), scales linearly {{with the number of}} haplotypes used in each iteration and can be run efficiently on whole chromosomes. © 2012 Nature America, Inc. All rights reserved...|$|R
