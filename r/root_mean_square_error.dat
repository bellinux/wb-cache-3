5765|10000|Public
25|$|Mean squared {{error is}} used for obtaining {{efficient}} estimators, a widely used class of estimators. <b>Root</b> <b>mean</b> <b>square</b> <b>error</b> is simply the square root of mean squared error.|$|E
50|$|The running {{times for}} author {{recognition}} are in seconds and the error rates are <b>root</b> <b>mean</b> <b>square</b> <b>error</b> (RMSE) and relative absolute error (RAE).|$|E
50|$|Mean squared {{error is}} used for obtaining {{efficient}} estimators, a widely used class of estimators. <b>Root</b> <b>mean</b> <b>square</b> <b>error</b> is simply the square root of mean squared error.|$|E
3000|$|... {{coefficient}} {{was estimated}} minimizing the <b>root</b> <b>mean</b> <b>squared</b> <b>error</b> (RMSE) between model and experimental data.|$|R
30|$|A)). Extensions to this {{approach}} have been proposed, such as taking the average, <b>root</b> <b>mean</b> <b>squared</b> <b>error,</b> or combinations [34].|$|R
30|$|Table  3 shows (averaged) <b>root</b> <b>mean</b> <b>squared</b> <b>error</b> {{estimates}} for on-site and cross-city prediction in Linz using these methods. The latter measures are {{calculated from the}} same prediction errors as used for Table  1 to counter doubts regarding the results shown therein. More specifically, least-squares estimation implicitly optimizes the <b>root</b> <b>mean</b> <b>squared</b> <b>error,</b> and the on-site performance of b), c), and d) in Linz shown in Table  3 surpasses the respective cross-city result.|$|R
5000|$|... 'Best Fit' {{automatically}} selects {{the optimal}} model equation and weighting algorithm with different parameters to optimize for: <b>Root</b> <b>mean</b> <b>square</b> <b>error</b> (RMSE), R squared, and Standard Deviation of % Recovery ...|$|E
50|$|Compared with {{published}} values, this calculation has a <b>root</b> <b>mean</b> <b>square</b> <b>error</b> of only 3.7 s. The greatest {{error is}} 6.0 s. This {{is much more}} accurate than the approximation described above, but not as accurate as the elaborate calculation.|$|E
50|$|Mean square error or {{mean squared error}} (abbreviated MSE) and <b>root</b> <b>mean</b> <b>square</b> <b>error</b> (RMSE) {{refer to}} the amount by which the values {{predicted}} by an estimator differ from the quantities being estimated (typically outside the sample from which the model was estimated).|$|E
30|$|The ultra-short-term {{wind power}} {{forecasting}} accuracy is further improved by using error correction {{in terms of}} normalized <b>root</b> <b>mean</b> <b>squared</b> <b>error</b> (NRMSE).|$|R
3000|$|This method exaggerates the {{prediction}} error—the difference between prediction value and actual value. The <b>root</b> <b>mean</b> <b>squared</b> <b>error</b> (RMSE) is evaluated by [...]...|$|R
30|$|<b>Root</b> <b>Mean</b> <b>Squared</b> <b>Error</b> (RMSE), {{which is}} the <b>square</b> of the <b>Mean</b> <b>Square</b> <b>Error</b> and expresses the {{standard}} deviation {{of the differences between}} forecasted and actual values.|$|R
50|$|For a {{cyclically}} alternating electric current, RMS {{is equal}} to the value of the direct current that would produce the same average power dissipation in a resistive load.In Estimation theory the <b>root</b> <b>mean</b> <b>square</b> <b>error</b> of an estimator {{is a measure of the}} imperfection of the fit of the estimator to the data.|$|E
50|$|To {{measure the}} {{performance}} of a model, some frequently used metrics are the linear correlation coefficient, Spearman's rank correlation coefficient, and the <b>root</b> <b>mean</b> <b>square</b> <b>error</b> (RMSE). Other metrics are the kappa coefficient and the outliers ratio. ITU-T Rec. P.1401 gives an overview of statistical procedures to evaluate and compare objective models.|$|E
5000|$|The <b>root</b> <b>mean</b> <b>square</b> <b>error</b> of {{approximation}} (RMSEA) avoids {{issues of}} sample size {{by analyzing the}} discrepancy between the hypothesized model, with optimally chosen parameter estimates, and the population covariance matrix. The RMSEA ranges from 0 to 1, with smaller values indicating better model fit. A value of [...]06 or less is indicative of acceptable model fit.|$|E
30|$|Throughout {{this review}} paper {{we will use}} {{normalised}} <b>root</b> <b>mean</b> <b>squared</b> <b>error</b> (NRMSE), providing point-to-point CED plots to compare the performance of different landmarking methodologies.|$|R
3000|$|The various {{sampling}} methods were compared {{in terms of}} the relative bias, and the relative <b>root</b> <b>mean</b> <b>squared</b> <b>error</b> (RMSE%). The relative bias is defined as [...]...|$|R
50|$|Forecast errors can be {{evaluated}} {{using a variety}} of methods namely <b>mean</b> percentage <b>error,</b> <b>root</b> <b>mean</b> <b>squared</b> <b>error,</b> <b>mean</b> absolute percentage <b>error,</b> <b>mean</b> <b>squared</b> <b>error.</b> Other methods include tracking signal and forecast bias.|$|R
5000|$|... where η is the {{learning}} rate (typically 0.002 to 0.01), y is the predicted bit, and (y − P(1)) is the prediction error. The weight update algorithm differs from backpropagation {{in that the}} terms P(1)P(0) are dropped. This is because {{the goal of the}} neural network is to minimize coding cost, not <b>root</b> <b>mean</b> <b>square</b> <b>error.</b>|$|E
5000|$|When {{applied to}} facial recognition, CNNs {{achieved}} a large decrease in error rate. Another paper reported a 97.6 percent recognition rate on [...] "5,600 still images {{of more than}} 10 subjects". CNNs {{were used to assess}} video quality in an objective way after manual training; the resulting system had a very low <b>root</b> <b>mean</b> <b>square</b> <b>error.</b>|$|E
5000|$|Even {{the models}} with very high climate {{sensitivity}} {{were found to}} be [...] "as realistic as other state-of-the-art climate models". The test of realism was done with a <b>root</b> <b>mean</b> <b>square</b> <b>error</b> test. This does not check on realism of seasonal changes and it is possible that more diagnostic measures may place stronger constraints on what is realistic. Improved realism tests are being developed.|$|E
30|$|The fixed lag smoothing-based {{tracking}} algorithms {{proposed in}} this work also provide a significant improvement over existing online algorithms in terms of false track discrimination and <b>root</b> <b>mean</b> <b>square</b> <b>errors</b> (RMSEs).|$|R
5000|$|Forecast {{skill for}} single-value {{forecasts}} is commonly represented {{in terms of}} metrics such as correlation, <b>root</b> <b>mean</b> <b>squared</b> <b>error,</b> <b>mean</b> absolute error, relative mean absolute error, bias, and the Brier score, among others.|$|R
3000|$|... p[*]=[*] 1, 000. The {{rest of the}} {{parameters}} is same as in Section 6.2. The performance measure is taken as the <b>root</b> <b>mean</b> <b>squared</b> <b>error</b> (RMSE) of the moving source position estimate given by [...]...|$|R
5000|$|In particular, simply summing n {{numbers in}} {{sequence}} has a worst-case error that grows proportional to n, and a <b>root</b> <b>mean</b> <b>square</b> <b>error</b> that grows as [...] for random inputs (the roundoff errors form a random walk). [...] With compensated summation, the worst-case error bound {{is independent of}} n, so {{a large number of}} values can be summed with an error that only depends on the floating-point precision.|$|E
5000|$|The Degree (D) {{value is}} almost linearly {{dependent}} {{to the square}} root of the average wave Height (H) above, i.e., [...] Using linear regression on the table above, the coefficients can be calculated for the low Height values (...) and for the high Height values (...) [...] Then the Degree can be approximated as the average between the low and high estimations, i.e.:where [...] is the optional rounding to the closest integer value. Without the rounding to integer, the <b>root</b> <b>mean</b> <b>square</b> <b>error</b> of this approximation is: [...]|$|E
5000|$|<b>Root</b> <b>mean</b> <b>square</b> <b>error</b> of {{approximation}} (RMSEA) fit index: RMSEA is {{an estimate}} of the discrepancy between the model and the data per degree of freedom for the model. Values less that [...]05 constitute good fit, values between 0.05 and 0.08 constitute acceptable fit, a values between 0.08 and 0.10 constitute marginal fit and values greater than 0.10 indicate poor fit [...] An advantage of the RMSEA fit index is that it provides confidence intervals which allow researchers to compare a series of models with varying numbers of factors.|$|E
50|$|The {{forecast}} error {{needs to}} be calculated using actual sales as a base. There are several forms of forecast error calculation methods used, namely <b>Mean</b> Percent <b>Error,</b> <b>Root</b> <b>Mean</b> <b>Squared</b> <b>Error,</b> Tracking Signal and Forecast Bias.|$|R
40|$|This papers {{studies and}} compares the {{asymptotic}} bias of GMM and generalized empirical likelihood (GEL) estimators {{in the presence}} of estimated nuisance parameters. We consider cases in which the nuisance parameter is estimated from independent and identical samples. A simulation experiment is conducted for covariance structure models. Empirical likelihood offers much reduced mean and median bias, <b>root</b> <b>mean</b> <b>squared</b> <b>error</b> and <b>mean</b> absolute error, as compared with two-step GMM and other GEL methods. Both analytical and bootstrap bias-adjusted two-step GMM estimators are compared. Analytical bias-adjustment appears to be a serious competitor to bootstrap methods in terms of finite sample bias, <b>root</b> <b>mean</b> <b>squared</b> <b>error</b> and <b>mean</b> absolute error. Finite sample variance seems to be little affected. ...|$|R
40|$|Three-phase {{sampling}} {{can be a}} {{very effective}} design for the estimation of regional and national forest cover type frequencies. Simultaneous estimation of frequencies and sampling variances require estimation {{of a large number of}} parameters; often so many that consistency and robustness of results becomes an issue. A new stepwise estimation model, in which bias in phase one and two is corrected sequentially instead of simultaneously, requires fewer parameters. Simulated three-phase sampling tested the new model with 144 settings of sample sizes, the number of classes and classification accuracy. Relative mean absolute deviations and <b>root</b> <b>mean</b> <b>square</b> <b>errors</b> were, in most cases, about 8 % lower with the stepwise method than with a simultaneous approach. Differences were a function of design parameters. Average expected relative <b>root</b> <b>mean</b> <b>square</b> <b>errors,</b> derived from the assumption of a Dirichlet distribution of cover-type frequencies, tracked the empirical <b>root</b> <b>mean</b> <b>square</b> <b>errors</b> obtained from repeated sampling with - 10 %. Resampling results indicate that the relative bias of the most frequent cover types was slightly inflated by the stepwise method. For the least common cover type, the simultaneous method produced the largest relative bias. ...|$|R
5000|$|A 2001 {{paper by}} Roy Batchelor of City University Business School, London {{compared}} the Consensus Forecasts with forecasts {{made by the}} International Monetary Fund (IMF) and Organisation for Economic Co-operation and Development (OECD). The study found: [...] "With few exceptions, the private sector forecasts are less biased and more accurate in terms of mean absolute error and <b>root</b> <b>mean</b> <b>square</b> <b>error.</b> Formal tests show these differences are statistically significant for forecasts of real growth and production, less so for forecasts of inflation and unemployment. Overall, there appears little information in the OECD and IMF forecasts {{that could be used}} to reduce significantly the error in the private sector forecasts." ...|$|E
5000|$|A 2001 {{paper by}} Roy Batchelor of City University Business School, London {{compared}} the Consensus Forecasts with forecasts {{made by the}} International Monetary Fund (IMF) and the OECD. The study found: [...] "With few exceptions, the private sector forecasts are less biased and more accurate in terms of mean absolute error and <b>root</b> <b>mean</b> <b>square</b> <b>error.</b> Formal tests show these differences are statistically significant for forecasts of real growth and production, less so for forecasts of inflation and unemployment. Overall, there appears little information in the OECD and IMF forecasts {{that could be used}} to reduce significantly the error in the private sector forecasts." ...|$|E
5000|$|Steps 1 and 2 of the {{algorithm}} yield a matrix [...] {{very close to}} the true matrix [...] (as measured by the <b>root</b> <b>mean</b> <b>square</b> <b>error</b> (RMSE) with high probability. In particular, with probability , [...] for some constant [...] [...] denotes the Frobenius norm. Note that the full suite of assumptions is not needed for this result to hold. The incoherence condition, for example, only comes into play in exact reconstruction. Finally, although trimming may seem counter intuitive as it involves throwing out information, it ensures projecting [...] onto its first [...] principal components gives more information about the underlying matrix [...] than about the observed entries.|$|E
40|$|This paper {{explores the}} {{forecasting}} performances of several non-linear models, namely GARCH, EGARCH, APARCH used with three distributions, namely the Gaussian normal, the Student-t and Generalized Error Distribution (GED). In order {{to evaluate the}} performance of the competing models we used the standard loss functions that is the <b>Root</b> <b>Mean</b> <b>Squared</b> <b>Error,</b> <b>Mean</b> Absolute Error, Mean Absolute Percentage Error and the Theil Inequality Coefficient. Our result show that the asymmetric GARCH family models are generally the best for forecasting NICs indices. We also find that both <b>Root</b> <b>Mean</b> <b>Squared</b> <b>Error</b> and <b>Mean</b> Absolute Error forecast statistic measures tend to choose models that were estimated assuming the normal distribution, while the other two remaining forecast measures privilege models with t-student and GED distribution. ...|$|R
30|$|Median {{scores of}} SROCC, LCC, Kendall {{correlation}} constant (KCC), and <b>root</b> <b>mean</b> <b>squared</b> <b>error</b> (RMSE) are {{reported for the}} performance evaluation of the proposed approach. The SROCC, LCC, and KCC scores measure the similarity between mean observer score and predicted quality score, whereas RMSE measure the error.|$|R
5000|$|MAE {{has a clear}} {{interpretation}} as {{the average}} absolute difference between yi and xi. Many researchers want to know this average difference because its interpretation is clear, but researchers frequently compute and misinterpret the <b>Root</b> <b>Mean</b> <b>Squared</b> <b>Error</b> (RMSE), {{which is not the}} average absolute error.|$|R
