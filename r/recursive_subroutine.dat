13|11|Public
500|$|By {{a result}} of , every -vertex graph has at most [...] maximal cliques. They can be listed by the Bron–Kerbosch algorithm, a {{recursive}} backtracking procedure of [...] The main <b>recursive</b> <b>subroutine</b> of this procedure has three arguments: a partially constructed (non-maximal) clique, a set of candidate vertices that could {{be added to the}} clique, and another set of vertices that should not be added (because doing so would lead to a clique that has already been found). The algorithm tries adding the candidate vertices one by one to the partial clique, making a recursive call for each one. After trying each of these vertices, it moves it to the set of vertices that should not be added again. Variants of this algorithm can be shown to have worst-case running time , matching the number of cliques that might need to be listed. Therefore, this provides a worst-case-optimal {{solution to the problem of}} listing all maximal cliques. Further, the Bron–Kerbosch algorithm has been widely reported as being faster in practice than its alternatives.|$|E
5000|$|Each syntax {{equation}} is {{translated into a}} <b>recursive</b> <b>subroutine</b> which tests the input string for a particular phrase structure, and deletes it if found.|$|E
50|$|However, another {{advantage}} of the call stack method {{is that it allows}} <b>recursive</b> <b>subroutine</b> calls, since each nested call to the same procedure gets a separate instance of its private data.|$|E
40|$|AbstractWe {{characterize}} the Least Fixed Point logic (FO + LFP) {{as a class}} of pebble game programs. Such a program can be characterized unambiguously as a structure of <b>recursive</b> <b>subroutines.</b> These programs {{can be described as}} flowchart-like digraphs. Concentrating on fixed points of a bounded number of quantifier alternations, we partition FO + LFP into an infinite nonlinear hierarchy of queries, composed of expressibility classes characterized by alternations within <b>recursive</b> <b>subroutines...</b>|$|R
40|$|Fortran (FORmula TRANslation) was {{introduced}} in 1957 and remains the language of choice for most scientific programming. The latest standard, Fortran 90, includes extensions that are familiar to users of C. Some {{of the most important}} features of Fortran 90 include <b>recursive</b> <b>subroutines,</b> dynamic storage allocation and pointers, user defined data structures, modules, and the ability t...|$|R
40|$|It {{is shown}} that MS Fortran- 77 compilers allow to {{construct}} <b>recursive</b> <b>subroutines.</b> The <b>recursive</b> one-dimensional adaptive quadrature subroutine is considered in particular. Despite its extremely short body (only eleven executable statements) the subroutine {{proved to be}} very effective and competitive. It was tested on various rather complex integrands. The possibility of function calls number minimization by choosing the optimal number of Gaussian abscissas is considered. The proposed recursive procedure can be effectively applied for creating more sophisticated quadrature codes (one- or multi-dimensional) and easily incorporated into existing programs. ...|$|R
5000|$|... (In principle, {{one could}} {{continue}} dividing the matrices until a base case of size 1&times;1 is reached, {{but in practice}} one uses a larger base case (e.g. 16&times;16) in order to amortize the overhead of the <b>recursive</b> <b>subroutine</b> calls.) ...|$|E
50|$|While {{there exists}} an {{unsatisfied}} clause C, call a <b>recursive</b> <b>subroutine</b> fix with C as its argument. This subroutine chooses a new random truth assignment for the variables in C, and then recursively calls the same subroutine on all unsatisfied clauses (possibly including C itself) that share a variable with C.|$|E
50|$|In an {{algorithm}} of {{this sort}} (as for divide and conquer algorithms in general), it is desirable to use a larger base case in order to amortize the overhead of the recursion. If N = 1, then there is roughly one <b>recursive</b> <b>subroutine</b> call for every input, but more generally there is one recursive call for (roughly) every N/2 inputs if the recursion stops at exactly n = N. By making N sufficiently large, the overhead of recursion can be made negligible (precisely this technique of a large base case for recursive summation is employed by high-performance FFT implementations).|$|E
50|$|Using coroutines {{for state}} {{machines}} or concurrency {{is similar to}} using mutual recursion with tail calls, as {{in both cases the}} control changes to a different one of a set of routines. However, coroutines are more flexible and generally more efficient. Since coroutines yield rather than return, and then resume execution rather than restarting from the beginning, they are able to hold state, both variables (as in a closure) and execution point, and yields are not limited to being in tail position; mutually <b>recursive</b> <b>subroutines</b> must either use shared variables or pass state as parameters. Further, each mutually recursive call of a subroutine requires a new stack frame (unless tail call elimination is implemented), while passing control between coroutines uses the existing contexts and can be implemented simply by a jump.|$|R
40|$|Introduction The four {{sections}} {{in this chapter}} deal with four distinct topics: systems of recursions, exponential generating functions, P'olya's Theorem and asymptotics estimates. These sections can be read independently of one another. The section on &quot;asymptotic &quot; estimates refers to formulas in earlier sections of the chapter, {{but there is no}} need to read the section containing the formula. &quot;Systems of recursions, &quot; as you might guess, deals with the creation and solution of sets of simultaneous recursions. These can arise in a variety of ways. One source of them is algorithms that involve interrelated <b>recursive</b> <b>subroutines.</b> Another source is situations that can be associated with grammars. General context free grammars can lead to complicated recursions, but regular grammars (which are equivalent to finite state automata) lead to simple systems of recursions. We limit our attention to these simple systems...|$|R
5000|$|Most {{programming}} environments with <b>recursive</b> <b>subroutines</b> use a stack {{for control}} flow. This structure typically also stores local variables, including subroutine parameters (in {{call by value}} system such as C). Forth often does not have local variables, however, nor is it call-by-value. Instead, intermediate values are kept in another stack, {{different from the one}} it uses for return addresses, loop counters, etc. Words operate directly on the topmost values in the first of these two stacks. It may therefore be called the [...] "parameter" [...] or [...] "data" [...] stack, but most often simply [...] "the" [...] stack. The second, function-call stack is then called the [...] "linkage" [...] or [...] "return" [...] stack, abbreviated rstack. Special rstack manipulation functions provided by the kernel allow it to be used for temporary storage within a word, and it is often used by counted loops, but otherwise it cannot be used to pass parameters or manipulate data.|$|R
5000|$|A {{subprogram}} {{may have}} any number {{and nature of}} call sites. If recursion is supported, a subprogram may even call itself, causing its execution to suspend while another nested execution of the same subprogram occurs. Recursion is a useful means to simplify some complex algorithms and break down complex problems. Recursive languages generally provide a new copy of local variables on each call. If the programmer desires the value of local variables to stay the same between calls, they can be declared static in some languages, or global values or common areas can be used. Here {{is an example of}} <b>recursive</b> <b>subroutine</b> in C/C++ to find Fibonacci numbers: ...|$|E
50|$|There is {{no concept}} of locally scoped {{variables}} in non-structured programming (although for assembly programs, general purpose registers may serve the same purpose after saving on entry), but labels and variables can have a limited area of effect (for example, a group of lines). This means there is no (automatic) context refresh when calling a subroutine, so all variables might retain their values from the previous call. This makes general recursion difficult, but some cases of recursion—where no subroutine state values are needed after the recursive call—are possible if variables dedicated to the <b>recursive</b> <b>subroutine</b> are explicitly cleared (or re-initialized to their original value) on entry to the subroutine. The depth of nesting also may be limited {{to one or two}} levels.|$|E
5000|$|By {{a result}} of , every -vertex graph has at most [...] maximal cliques. They can be listed by the Bron-Kerbosch algorithm, a {{recursive}} backtracking procedure of [...] The main <b>recursive</b> <b>subroutine</b> of this procedure has three arguments: a partially constructed (non-maximal) clique, a set of candidate vertices that could {{be added to the}} clique, and another set of vertices that should not be added (because doing so would lead to a clique that has already been found). The algorithm tries adding the candidate vertices one by one to the partial clique, making a recursive call for each one. After trying each of these vertices, it moves it to the set of vertices that should not be added again. Variants of this algorithm can be shown to have worst-case running time , matching the number of cliques that might need to be listed. Therefore, this provides a worst-case-optimal {{solution to the problem of}} listing all maximal cliques. Further, the Bron-Kerbosch algorithm has been widely reported as being faster in practice than its alternatives.|$|E
40|$|Verilog is a {{hardware}} description language (HDL) used to model electronic systems. The language (sometimes called Verilog HDL) supports the design, verification, {{and implementation of}} analog, digital, and mixed-signal circuits at various levels of abstraction. The designers of Verilog wanted a language with syntax similar to the C programming language {{so that it would}} be familiar to engineers and readily accepted. The language has a pre-processor like C, and the major control keywords such as "if", "while", etc are similar. The formatting mechanism in the printing routines and language operators (and their precedence) are also similar. The language differs in some fundamental ways. Verilog uses Begin/End instead of curly braces to define a block of code. The definition of constants in verilog require a bit width along with their base, consequently these differ. Verilog 95 and 2001 don't have structures, pointers, or <b>recursive</b> <b>subroutines</b> either. (However, System Verilog now includes these capibilities) Finally, the concept of time —so important to a HDL — won't be found in C...|$|R
50|$|The {{idea of a}} {{subroutine}} was {{worked out}} after computing machines had already existed for some time.The arithmetic and conditional jump instructions were planned {{ahead of time and}} have changed relatively little; but the special instructions used for procedure calls have changed greatly over the years.The earliest computers and microprocessors, such as the Small-Scale Experimental Machine and the RCA 1802, did not have a single subroutine call instruction.Subroutines could be implemented, but they required programmers to use the call sequence—a series of instructions—at each call site.Some very early computers and microprocessors, such as the IBM 1620, the Intel 8008, and the PIC microcontrollers, have a single-instruction subroutine call that uses dedicated hardware stack to store return addresses—such hardware supports only a few levels of subroutine nesting, but can support recursive subroutines.Machines before the mid 1960s—such as the UNIVAC I, the PDP-1, and the IBM 1130—typically use a calling convention which saved the instruction counter in the first memory location of the called subroutine. This allows arbitrarily deep levels of subroutine nesting, but does not support recursive subroutines.The PDP-11 (1970) {{is one of the first}} computers with a stack-pushing subroutine call instruction; this feature supports both arbitrarily deep subroutine nesting and also supports <b>recursive</b> <b>subroutines.</b>|$|R
50|$|True BASIC {{provides}} statements for matrix arithmetic, {{a feature}} that had been present in Dartmouth BASIC since early times, but had been dropped in almost all microcomputer versions of BASIC interpreters due to memory limitations. It also supports global and local variables, which permits <b>recursive</b> functions and <b>subroutines</b> to be written.|$|R
40|$|This paper briefly {{describes}} a programming language, its implementation on a microprocessor via a compiler and link-assembler, and the mechanically checked {{proof of the}} correctness of the implementation. The programming language, called Piton, is a high-level assembly language designed for verified applications and as the target language for high-level language compilers. It provides execute-only programs, <b>recursive</b> <b>subroutine</b> call and return, stack based parameter passing, local variables, global variables and arrays, a user-visible stack for intermediate results, and seven abstract data types including integers, data addresses, program addresses and subroutine names. Piton is formally specified by an interpreter written {{for it in the}} computational logic of Boyer and Moore. Piton has been implemented on the FM 8502, a general purpose microprocessor whose gate-level design has been mechanically proved to implement its machine code interpreter. The FM 8502 implementation of Piton is via a [...] ...|$|E
40|$|META II is a {{compiler}} writing language {{which consists}} of syntax equations resembling Eackus normal form and into which instructions to output assembly language cnmm-uds are inserted. Com-pilers have been written in this language for VALGOL I and VALGOL II. The former is a simple algebraic language designed {{for the purpose of}} illustrating META II. The latter contains a fairly large subset of ALGOL 60. The method of writing cempilers which is given in detail in the paper may be explained briefly as follows. Each syntax equation is trans-lated into a <b>recursive</b> <b>subroutine</b> which tests the input string for a particular phrase structure, and deletes it if found. Backup is avoided by the extensive use of factoring in the syntax equatiorm. For each source language, an interpreter is writ-ten and programs are compiled into that interpret-ive language • META II is not intended as a standard lan-guage which everyone will use to write compilers. Rather, it {{is an example of a}} simple working lan-guage which can give one a good start in design-ing a compiler-writing compiler suited to his own needs. Indeed, the META II compiler is written in its own language, thus lending itself to modi-fication. History The basic ideas behind META llwere described in a series of three papers by Schaidt, I Met-calf, 2 and Schorre. 3 These papers were present...|$|E
40|$|FMG 3 D (full multigrid 3 dimensions) is a pilot {{computer}} program that solves equations of fluid flow using a finite difference representation on a structured grid. Infrastructure exists for three dimensions {{but the current}} implementation treats only two dimensions. Written in Fortran 90, FMG 3 D {{takes advantage of the}} <b>recursive</b> <b>subroutine</b> feature, dynamic memory allocation, and structured-programming constructs of that language. FMG 3 D supports multi-block grids with three types of block-to-block interfaces: periodic, C-zero, and C-infinity. For all three types, grid points must match at interfaces. For periodic and C-infinity types, derivatives of grid metrics must be continuous at interfaces. The available equation sets are as follows: scalar elliptic equations, scalar convection equations, and the pressure-Poisson formulation of the Navier-Stokes equations for an incompressible fluid. All the equation sets are implemented with nonzero forcing functions to enable the use of user-specified solutions to assist in verification and validation. The equations are solved with a full multigrid scheme using a full approximation scheme to converge the solution on each succeeding grid level. Restriction to the next coarser mesh uses direct injection for variables and full weighting for residual quantities; prolongation of the coarse grid correction from the coarse mesh to the fine mesh uses bilinear interpolation; and prolongation of the coarse grid solution uses bicubic interpolation...|$|E
40|$|Profiling is a {{technique}} for identifying performance bottlenecks in programs by measuring the time spent in each subroutine as the program runs. In call graph profiling, time used by each subroutine is also charged to its callers {{in order to give}} a better idea of how the time is divided among major tasks in the program. In order to do this with acceptable performance impact, a common implementation technique (used by GNU gprof among others) is to count the number of calls of each subroutine, and charge its time to the callers in proportion to the number of calls they make. Whilst this gives acceptably accurate results for simple programs, it is inaccurate for subroutines whose running time depends on the values of their arguments, and becomes almost useless for programs that exploit higher-order functions or dynamic method binding. Programs containing mutually <b>recursive</b> <b>subroutines</b> cause additional problems with this approach. In this article, we discuss a way of improving on gprof by collecting more information during execution, without significantly increasing the overhead of profiling. The method is based on the idea of keeping track of the set of subroutines that are active at each moment during the execution of the program being analysed and the calling arcs between these subroutines. By considering only the most recent activation of each subroutine, we arrange that even recursive programs give rise to a finite number of these contexts that is usually fairly small. The information can be collected e#ciently by dynamically constructing a finite state machine whose states correspond to execution contexts in the program. 1...|$|R
40|$|This report {{describes}} a programming language, its implementation on a microprocessor via a compiler, an assembler, and a linker, and the mechanically checked {{proof of the}} correctness of the implementation. The programming language, called Piton, is a high-level assembly language designed for verified applications and as the target language for high-level language compilers. It provides execute-only programs, <b>recursive</b> <b>subroutine</b> call and return, stack based parameter passing, local variables, global variables and arrays, a user-visible stack for intermediate results, and seven abstract data types including integers, data addresses, program addresses and subroutine names. Piton is formally specified by an interpreter written {{for it in the}} computational logic of Boyer and Moore. Piton has been implemented on the FM 8502, a general purpose microprocessor whose gate-level design has been mechanically proved to implement its machine code interpreter. The FM 8502 implementation of Piton is via a function in the Boyer-Moore logic which maps a Piton initial state into an FM 8502 binary core image. The compiler, assembler and linker are all defined as functions in the logic. The implementation requires approximately 36 K bytes and 1, 400 lines of prettyprinted source code in the Pure Lisp-like syntax of the logic. The implementation has been mechanically proved correct. In particular, if a Piton state can be run to completion without error, then th...|$|E

