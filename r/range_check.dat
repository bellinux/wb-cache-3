17|221|Public
5000|$|The {{server-based}} diagnostics {{that result}} with an XML/JSON solution if incorrect data is attempted to be inserted (e.g., <b>range</b> <b>check</b> or regular-expression pattern violations) are cryptic to the end-user: {{to convey the}} error accurately, one would, at the least, need to associate a detailed and user-friendly error diagnostic with each attribute.|$|E
5000|$|Safety {{guarantees}} {{come at a}} run-time cost. For example, the compiler {{is required}} to put appropriate range checks in the code. Guarding each array access with a <b>range</b> <b>check</b> is not efficient, so most JIT compilers will try to eliminate them statically or by moving them out of inner loops (although most native compilers for C++ {{will do the same}} when range-checks are optionally used).|$|E
50|$|A <b>range</b> <b>check</b> is a {{check to}} make sure a number is within a certain range; for example, to ensure that a value about to be {{assigned}} to a 16-bit integer is within the capacity of a 16-bit integer (i.e. checking against wrap-around). This is not quite the same as type checking. Other range checks may be more restrictive; for example, a variable to hold the number of a calendar month may be declared to accept only the range 1 to 12.|$|E
5000|$|Automatic <b>range</b> <b>checking</b> {{for basic}} {{properties}} (int, double, string) {{based on the}} property-annotations ...|$|R
5000|$|... b := boolean(i); // Will raise proper rangecheck errors for {{undefined}} {{values with}} <b>range</b> <b>checks</b> on.|$|R
50|$|Unlike <b>range</b> <b>checks,</b> {{data are}} checked for one limit only, upper OR lower, e.g., data {{should not be}} greater than 2 (<=2).|$|R
40|$|This article {{introduces}} a high-performance packet filter design {{in which we}} propose the partial parallel <b>range</b> <b>check</b> (PPRC) technique for speeding up port <b>range</b> <b>check.</b> Unlike the conventional serial design that uses cascading cells to perform the serial check, PPRC divides the single path into several segments. All PPRC segments perform the range compare simultaneously, that is, parallel check, and then the results of each segment are serialized to generate the final check result. Besides theoretical analyses, we also use UMC 90 nm CMOS process to implement the PPRC design and verify {{its effect on the}} check performance. Compared to state-of-the-art <b>range</b> <b>check</b> techniques, the results show that the PPRC design with the best configuration can improve check performance by 28 %, at least. In addition, the PPRC design is more stable and energy efficient than related designs, even though it requires more transistors to implement the peripheral circuitry. The range of energy improvement achieved by the PPRC design is about 35 % [...] 70 %...|$|E
40|$|Abstract- This paper {{describes}} {{techniques for}} optimizing range checks performed to detect array bound violations. In {{addition to the}} elimination of range check:s, the optimizations discussed in this paper also reduce the overhead due to range checks that cannot be eliminated by compile-time analysis. The optimizations reduce the program execution time and the object code size through elimination of redundant checks, propagation of checks out of loops, and combination of multiple checks into a single check. A minimal control flow graph (MCFG) is constructed using which the minimal amount of data flow information required for <b>range</b> <b>check</b> optimizations is com-puted. The <b>range</b> <b>check</b> optimizations are per-formed using the MCFG rather the CFG for the entire program. This allows the global <b>range</b> <b>check</b> optimizations to be performed efficiently since the MCFG is significantly smaller than the CFG. Any array bound violation that is detected by a program with all range checks included, will also be detel:ted by the program after <b>range</b> <b>check</b> optimization and vice versa. Even though the above optimizations {{may appear to be}} similar to traditional code optimizations, similar {{reduction in the number of}} range checks executed can not be achieved by a traditional code optimizer. Experi-mental results indicate that the number of range checks performed in executing a program is greatly reduced using the above techniques...|$|E
40|$|Abstract: Class {{discovery}} {{is one of}} the most important tasks in cancer classification using biomolecular data. To perform this, a multiple clustering approach called Hierarchical clustering is used. It uses one of the metrics called Manhattan Distance which measures the distance between the values of the data set and builds a hierarchy of clusters after analysing it. The clustering result enables to classify the cancer types and it is further evaluated by <b>Range</b> <b>check</b> and Delta check. The various test results are compared with the known initial range of values using <b>Range</b> <b>check.</b> Delta check is performed on the current test result and the immediate previous test result for better results. These techniques are used to improve the diagnosis of cancer...|$|E
5000|$|Corollary: if [...] and [...] are 2-dimensional vectors, the {{difference}} formula is frequently used in practice {{to compute the}} angle between those vectors {{with the help of}} , since the resulting computation behaves benign in the range [...] and can thus be used without <b>range</b> <b>checks</b> in many practical situations.|$|R
5000|$|SQL {{constraints}} (e.g., <b>range</b> <b>checks,</b> {{regular expression}} checks) cannot {{be applied to}} sparse columns. The only check that is applied is for correct data type. Constraints would have to be implemented in metadata tables and middle-tier code, as is done in production EAV systems. (This consideration also applies to business applications as well.) ...|$|R
50|$|IBM {{recognized}} {{the problems with}} the Macro Assembler and created an automated program generator named DMS. DMS later became Cross System Product (CSP) on the 8100. DMS was essentially a screen painter; it could do simple edits such as field <b>range</b> <b>checking</b> or numeric tests but more complex logic still had to be coded using the Macro Assembler.|$|R
40|$|Migrating from T 89 C 51 CC 01 to AT 89 C 51 CC 03 This {{application}} note is {{a guide to}} assist T 89 C 51 CC 01 users in converting existing designs to the AT 89 C 51 CC 03 devices. In addition to the functional changes, the electrical characteristics of the AT 89 C 51 CC 03 are different including an increase in operating power supply <b>range.</b> <b>Check</b> the datasheet for detailed information. To ease the product migration, this {{application note}} compares the memory organization/accesses, the new features and electrical parameters...|$|E
40|$|Fine-grained {{hardware}} protection, {{if it can}} be {{done without}} slowing down the processor, could deliver significant benefits to software, enabling the implementation of strongly encapsulated light-weight objects. In this paper we introduce Legba, a new caching architecture that aims at supporting fine-grained memory protection and protected procedure calls without slowing down the processor's clock speed. This is achieved by separating translation from protection, which allows the use of virtually-addressed caches and moving the TLB off-core. Protection is implemented in two stages. We add protection information {{in the form of an}} object ID to each cache line. This object ID is combined with a per-protection context identifier, and the result is used to index into a protection cache, which delivers the access rights. As no <b>range</b> <b>check</b> is required on the protection cache, it can be set-associative, allowing it to be made large, fast and low-power, compared to a fully associative TLB. On a cache miss, the object ID is retrieved in parallel to the cache line fetch, performing the protection <b>range</b> <b>check</b> off-core. A new switch permission enables Legba to implement protected procedure calls, where the new context identifier is taken from the instruction cache liner's object ID. This mechanism is similar to call gates but more flexible. The paper compares Legba with approaches based on the idea of a protection look-aside buffer, in particular with respect to coverage...|$|E
40|$|The total column {{ozone data}} from SCIAMACHY were {{monitored}} {{in the ozone}} data assimilation system at NASA’s Data Assimilation office (DAO). The ozone data from NOAA− 16 Solar Backscatter UltraViolet/ 2 (SBUV/ 2) instrument are assimilated in the DAO’s system. The monitoring consists of computing the differences between SCIAMACHY observations and the ozone system forecast, i. e. observed−minus−forecast (O−F) residuals. The O−F residuals are then analyzed using simple statistical methods. Two monitoring experiments were performed for SCIAMACHY data for the period form October 25 to December 1, 2002. In the first experiment no quality control other than the quality flag provided with the data was used. In the second experiment a simple <b>range</b> <b>check</b> (total column ozone values must be greater that 100 and less than 1000 Dobson units) was used to screen the data before monitoring. In both experiments large O−F residuals were concentrated in {{the northern and southern}} high latitudes at what appear to be high solar zenith angles near the terminator. In the experiment with the <b>range</b> <b>check</b> the zonal mean of SCIAMACHY observations is 20 to 30 Dobson units lower that that of the DAO forecast throughout the Tropics and mid−latitudes. At high latitudes where the magnitude of the O−F residuals is particularly large, however, the SCHIAMACHY total ozone is higher then the DAO forecast...|$|E
40|$|One of {{the main}} {{objectives}} of engineering design {{is to find a}} design that is the cheapest among all designs that satisfy given constraints. Most of the constraints must be satisfied under all possible values within certain <b>ranges.</b> <b>Checking</b> all possible combinations of values is often very time-consuming. In this paper, we propose a faster algorithm for checking such constraints...|$|R
50|$|Level 4 inmates occupy similarly-designed cells {{but have}} {{additional}} freedom {{to move about}} within specific cell blocks. Inmates classified as Level 4B may also exercise within their specific cell block, but are also required to lock down before security staff enter the cell block to perform <b>range</b> <b>checks,</b> serve food, etc. Inmates classified as Level 4A {{are not subject to}} this restriction.|$|R
50|$|The greater {{horizontal}} acceleration caused a data conversion from a 64-bit floating point number to a 16-bit signed integer value to overflow and cause a hardware exception. Efficiency considerations had omitted <b>range</b> <b>checks</b> {{for this particular}} variable, though conversions of other variables in the code were protected. The exception halted the reference platforms, resulting {{in the destruction of}} the flight.|$|R
40|$|At the pre-aggregation {{stage of}} micro-editing in the National Statistics Center (NSC), an optimal editing of {{individual}} data has been implemented by consistency check and <b>range</b> <b>check</b> for each survey. In particular, some errors in numeric data items are logically detected {{based on a}} probability distribution of fixed parameters of the sample mean and standard deviation. However, when an acceptance region (range) for the error detection is narrow, a lot of correct data are detected. On the other hand, when the range is wide, many errors are not detected. In general, the problem associated with the range is revealed during a data review process at the post-aggregation stage. In the past, it was customary to edit manually. In order to solve this problem, {{it is necessary to}} extract the errors in the cell of the statistical table after aggregation. Therefore, we propose the introduction and systematization of naive Bayes that can treat a parameter as a random variable. In this method, the errors can be filtered by subjective probability. Thus, it is not a strict <b>range</b> <b>check</b> compared with objective probability, such as Smirnov-Grubbs ' test. The filtering in subjective probability is dependent on a combination of several items and prior probability by unit of records. In addition, the validation test of the records may also include individual data with missing values. In this paper, we assess the error extraction method that focuses on sales variable in aggregated cells of the statistical table. As a result, it is possible to extract the errors in the point of view that is different from micro editing, and that the data editing process is automated...|$|E
40|$|Abstract. Speech-model-based phoneme {{enhancement}} or replacement {{can be used}} {{to compensate}} losses of auditory selectivity that typically occur in people suffering from sensory hearing deficits. A dedicated phonetic spotter cam serve as control unit for selectivity-restoring speech processing; it offers effective control of phonemespecific enhancement and replacement. The future implementation in wearable lowpower DSPs imposes stringent limitation on processing costs. To find the numerical optimum of a predefined “low-cost ” configuration, software tools have been developed in C++, which let us define a parametric model. Starting from labelling of speech material, prototypes of the phonemes to be spotted are established {{on the basis of a}} mixed statistical approach, which contains <b>range</b> <b>check</b> and a likelihoodbased distance measure in the feature space chosen. ...|$|E
40|$|IBAMar ([URL] is a {{regional}} database that puts together all physical and biochemical {{data obtained by}} multiparametric probes (CTDs equipped with different sensors), during the cruises managed by the Balearic Center of the Spanish Institute of Oceanography (COB-IEO). It has been recently extended to include data obtained with classical hydro casts using oceanographic Niskin or Nansen bottles. The result is a database that includes a main core of hydrographic data: temperature (T), salinity (S), dissolved oxygen (DO), fluorescence and turbidity; complemented by bio-chemical data: dissolved inorganic nutrients (phosphate, nitrate, nitrite and silicate) and chlorophyll-a. In IBAMar Database, different technologies and methodologies were used by different teams along the four decades of data sampling in the COB-IEO. Despite of this fact, data have been reprocessed using the same protocols, and a standard QC {{has been applied to}} each variable. Therefore it provides {{a regional}} database of homogeneous, good quality data. Data acquisition and quality control (QC) : 94 % of the data are CTDs Sbe 911 and Sbe 25. S and DO were calibrated on board using water samples, whenever a Rossetta was available (70 % of the cases). All CTD data from Seabird CTDs were reviewed and post processed with the software provided by Sea-Bird Electronics. Data were averaged to get 1 dbar vertical resolution. General sampling methodology and pre processing are described in [URL] Manual QC include visual checks of metadata, duplicate data and outliers. Automatic QC include <b>range</b> <b>check</b> of variables by area (north of Balearic Islands, south of BI and Alboran Sea) and depth (27 standard levels), check for spikes and check for density inversions. Nutrients QC includes a preliminary control and a <b>range</b> <b>check</b> on the observed level of the data to detect outliers around objectively analyzed data fields. A quality flag is assigned as an integer number, depending on the result of the QC check...|$|E
50|$|ThumbEE is {{a variant}} of the Thumb2 16/32-bit {{instruction}} set. It integrates null pointer checking; defines some new fault mechanisms; and repurposes the 16-bit LDM and STM opcode space to support a few instructions such as <b>range</b> <b>checking,</b> a new handler invocation scheme, and more. Accordingly, compilers that produce Thumb or Thumb2 code can be modified to work with ThumbEE-based runtime environments.|$|R
40|$|When {{connecting}} one {{master to}} one slave, the AXI Interconnect can optionally perform address <b>range</b> <b>checking.</b> It can also perform {{any of the}} normal data-width, clock-rate, or protocol conversions and pipelining. When connecting one master to one slave, and not performing any conversions or address <b>range</b> <b>checking,</b> the AXI Interconnect is implemented as wires, with no resources, no delay, and no latency. Built-in data-width conversion: Each master and slave connection can independently use data widths of 32, 64, 128, or 256 bits wide:- The internal crossbar can be configured to have a native data-width of 32, 64, 128, or 256 bits. - Data-width conversion is performed for each master and slave connection that does not match the crossbar native data-width. When converting to a wider interface (upsizing), data is packed (merged) when permitted by address channel control signals (CACHE modifiable bit is asserted). When converting to a narrower interface (downsizing), burst transactions can be split into multiple transactions if the maximum burst length would otherwise be exceeded...|$|R
40|$|We {{propose a}} new method of {{eliminating}} <b>range</b> <b>checks</b> {{in connection with}} array index expressions and assignments to variables of subrange types. Contrary to the approaches documented in the literature, we work on an extended static single assignment form (XSA) very near the target level. This gives significant advantages over previous attempts since many questions don't occur at all or else in a simpler form in the XSA. The technique has been implemented {{in a family of}} Modula- 2 and Oberon- 2 compilers. Introduction Implementation of safe programming languages requires <b>range</b> <b>checks</b> in connection with array index expressions and, where applicable, with assignments to variables of subrange types. Also, it calls for overflow checks in connection with arithmetic operations. Of course, these checks cause run time overhead, which is why they are frequently turned off for the production version of a program. This inherently bad practice has been compared by Hoare to a pilot who wears a parachute [...] ...|$|R
40|$|This paper {{presents}} a compiler optimization algorithm {{to reduce the}} run time overhead of array subscript range checks in programs without compromising safety. The algorithm is based on partial redundancy elimination and it incorporates previously developed algorithms for <b>range</b> <b>check</b> optimization. We implemented the algorithm in our research compiler, Nascent, and conducted experiments on a suite of 10 benchmark programs to obtain four results: (1) the execution overhead of naive range checking is high enough to merit optimization, (2) there are substantial differences between various optimizations, (3) loop-based optimizations that hoist checks out of loops are effective in eliminating about 98 % of the range checks, and (4) more sophisticated analysis and optimization algorithms produce very marginal benefits. 1 Introduction Program statements that access elements of an array outside the declared array ranges introduce errors which {{can be difficult to}} detect. Since compile-time check [...] ...|$|E
30|$|The CRA attack {{model is}} {{applicable}} only to encryption schemes that allow range predicates {{to be evaluated}} in the encrypted domain. In this model, the adversary A has access to a <b>Range</b> <b>Check</b> Algorithm, RCA(x^*,RI). RCA takes two inputs, x^* being a ciphertext and RI being the encrypted Range Information corresponding to a plaintext range. RCA outputs 1 if the plaintext value of x^* belongs to the underlying range specified by RI, else it outputs 0. RCA corresponds to the SP observing the query processing at the Cloud server. A also has access to a Transformation Oracle (TO). The TO, on input of a plaintext pair (lvalue, hvalue), returns the encrypted range information RI such that RCA(x^*,RI) outputs 1 {{if and only if}} x ∈ [lvalue,hvalue], where x^* ← Enc(x). TO corresponds to the SA rewriting the plaintext query into the equivalent query over the encrypted database. The adversary A is given a set M^* consisting of m ciphertexts and the interval constraint size H. A selects a challenge ciphertext x^* ∈ M^* and his objective is to identify a plaintext interval (a, b) for x^* such that |b-a| < H. Also, A is allowed to issue a polynomial (in λ, the security parameter) number of range queries to the TO and observe their computations and results.|$|E
40|$|We compare {{a number}} of {{radiative}} transfer models for atmospheric sounding in the millimeter and submillimeter wavelength <b>range,</b> <b>check</b> their consistency, and investigate their deviations from each other. This intercomparison deals with three different aspects of radiative transfer models: (1) the inherent physics of gaseous absorption lines {{and how they are}} modeled, (2) the calculation of absorption coefficients, and (3) the full calculation of radiative transfer for different geometries, i. e., up-looking, down-looking, and limb-looking. The correctness and consistency of the implementations are tested by comparing calculations with predefined input such as spectroscopic data, line shape, continuum absorption model, and frequency grid. The absorption coefficients and brightness temperatures calculated by the different models are generally within about 1 % of each other. Furthermore, the variability or uncertainty of the model results is estimated if (except for the atmospheric scenario) the input such as spectroscopic data, line shape, and continuum absorption model could be chosen freely. Here the models deviate from each other by about 10 % around the center of major absorption lines. The main cause of such discrepancies is the variability of reported spectroscopic data for line absorption and of the continuum absorption model. Further possible causes of discrepancies are different frequency and pressure grids and differences in the corresponding interpolation routines, as well as differences in the line shape functions used, namely a prefactor of (í/í 0) or (í/í 0) 2 of the Van-Vleck-Weisskopf line shape function. Whether or not the discrepancies affect retrieval results remains to be investigated for each application individually...|$|E
40|$|John Gough, Queensland University of Technology Herbert Klaeren, University of Tubingen y November 1, 1994 Abstract We {{propose a}} new method of {{eliminating}} <b>range</b> <b>checks</b> {{in connection with}} array index expressions and assignments to variables of subrange types. Contrary to the approaches documented in the literature, we work on an extended static single assignment form (XSA) very near the target level. This gives significant advantages over previous attempts since many questions don't occur at all or else in a simpler form in the XSA. The technique has been implemented {{in a family of}} Modula- 2 and Oberon- 2 compilers. Introduction Implementation of safe programming languages requires <b>range</b> <b>checks</b> in connection with array index expressions and, where applicable, with assignments to variables of subrange types. Also, it calls for overflow checks in connection with arithmetic operations. Of course, these checks cause run time overhead; it is therefore a good idea to eliminate at compile t [...] ...|$|R
40|$|Abstract. One of {{the main}} {{objectives}} of engineering design {{is to find a}} design that is the cheapest among all designs that satisfy given con-straints. Most of the constraints must be satisfied under all possible val-ues within certain <b>ranges.</b> <b>Checking</b> all possible combinations of values is often very time-consuming. In this paper, we propose a faster algorithm for checking such constraints. 1 Formulation of the Proble...|$|R
50|$|In {{computer}} programming, {{bounds checking}} is any method of detecting whether a variable is within some bounds {{before it is}} used. It is usually used {{to ensure that a}} number fits into a given type (<b>range</b> <b>checking),</b> or that a variable being used as an array index is within the bounds of the array (index checking). A failed bounds check usually results in the generation of some sort of exception signal.|$|R
30|$|Given {{the simple}} graph in Fig.  4 a, the method {{identifies}} the cliques of largest size shown in Fig.  4 b. Figure  4 c shows an intuitive {{example of a}} possible graph encoding scheme using the proposed framework. To store the graph compactly (e.g., to disk), notice that one can simply write the cliques to a file such that each line contains the vertices of a clique C ∈C, followed by the remaining edges (between nodes that do not share a clique). This can be reduced further by first recoding the vertex ids such that the vertex ids in the cliques are C_ 1 ={v_ 1,..., v_k}, C_ 2 ={v_k+ 1,..., }, and so on. Now, {{it is possible to}} encode the vertices in each clique by simply storing the first (min) vertex id. For instance, in-memory or on disk, the first vertex id of each clique is stored one after the other, for instance, storing v_ 1, v_ 6, v_ 10 (in memory as an array, or to disk/a single line of a file) is enough since the clique C_i is retrieved by examining the ith position. That is, to retrieve the first clique in Fig.  4 b, we retrieve the first vertex v_ 1 and can compute the last vertex id by the i+ 1 position, which corresponds to the first vertex id of the clique i+ 1. Thus, using this, we immediately get the last vertex id v_ 5. Furthermore, fundamental graph primitives (such as “checking if a node is a neighbor”) may benefit from this encoding as many graph operations can be answered in o(1) with a simple <b>range</b> <b>check.</b>|$|E
40|$|A {{methodology}} for quantifying {{the accuracy of}} snow depth measurement are demonstrated in this study by using the equation of error propagation for the same type sensors and by compariong autimatic measurement with manual observation. Snow depth was measured at the Centre for Atmospheric Research Experiments (CARE) site of the Environment Canada (EC) during the 2013 – 2014 winter experiment. The snow depth measurement system at the CARE site was comprised of three bases. Three ultrasonic and one laser snow depth sensors and twelve snow stakes were placed on each base. Data from snow depth sensors are quality-controlled by <b>range</b> <b>check</b> and step test to eliminate erroneous data such as outliers and discontinuities. In comparison with manual observations, bias errors were calculated to show the spatial distribution of snow depth by considering snow depth measured from four snow stakes located on the easternmost side of the site as reference. The bias error of snow stakes {{on the west side}} of the site was largest. The uncertainty of all pairs of stakes and the average uncertainty for each base were 1. 81 and 1. 52 cm, respectively. The bias error and normalized bias removed root mean square error (NBRRMSE) for each snow depth sensor were calculated to quantify the systematic error and random error in comparison of snow depth sensors with manual observations that share the same snow depth target. The snow depth sensors on base 12 A (11 A) measured snow depth larger (less) than manual observation up to 10. 8 cm (5. 21 cm), and the NBRRMSEs ranged from 5. 10 to 16. 5 %. Finally, the instrumental uncertainties of each snow depth sensor were calculated by comparing three sensors of the same type installed at the different bases. The instrumental uncertainties ranged from 0. 62 to 3. 08 cm...|$|E
50|$|Some methods {{do all the}} {{additions}} {{first and}} then cast out sevens, whereas others cast them out at each step, as in Lewis Carroll's method. Either way is quite viable: the former is easier for calculators and computer programs; the latter for mental calculation (it is quite possible {{to do all the}} calculations in one's head with a little practice). None of the methods given here perform <b>range</b> <b>checks,</b> so unreasonable dates will produce erroneous results.|$|R
40|$|Abstract. In {{the present}} work the total {{radiative}} losses {{and the total}} emissivity curve are calculated for an optically thin plasma using the Arcetri Spectral Code in the 10 4 – 10 8 temperature <b>range.</b> <b>Checks</b> have been made {{on the effects of}} changes in the parameters involved in the calculations, such as element abundances, ion fractions, electron density and transition probabilities on the resulting curves. Parametric fits are given for the resulting radiative losses curves and comparison is made with previous results...|$|R
40|$|This paper {{presents}} {{comparative analysis}} betweentwo devices {{for the design}} of a radix 4 wallace tree multiplier that performs interval multiplication. This 64 bit multiplier requires Booth partial product selection logic and [27, 2] compressor. This interval arithmetic gives accurate result as rounding off error of floating point multiplier is eliminated. It requires slightly more area than conventional floating point unit. There is definite performance improvement over softwareapproach as function calls, error and <b>range</b> <b>checking</b> etc are notpresent due to dedicated hardwar...|$|R
50|$|The {{concept of}} slicing was surely known {{even before the}} {{invention}} of compilers. Slicing as a language feature probably started with FORTRAN (1957), more {{as a consequence of}} non-existent type and <b>range</b> <b>checking</b> than by design. The concept was also alluded to in the preliminary report for the IAL (ALGOL 58) in that the syntax allowed one or more indices of an array element (or, for that matter, of a procedure call) to be omitted when used as an actual parameter.|$|R
