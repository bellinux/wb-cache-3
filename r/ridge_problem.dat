2|57|Public
50|$|That {{excellent}} fit was more convincing a proof of continental drift than previous attempts, which left a large gap between Africa and South America. For his reconstruction Boris Choubert dared {{for the first}} time the idea that the Iberian Peninsula suffered, after the Triassic, a rotation with respect to the rest of Europe. He also swept the Mid-Atlantic <b>Ridge</b> <b>problem</b> by explaining that it formed posteriorly to Atlantic Ocean opening. The Caribbean Sea does not fit as well as the rest but, as it is known today, it was seriously affected by Cenozoic tectonics.|$|E
40|$|Problems {{that are}} not aligned with the {{coordinate}} system can present difficulties to many optimization algorithms, including evolutionary algorithms, by trapping the search on a ridge. The <b>ridge</b> <b>problem</b> in single-objective optimization is understood, but until now little {{work has been done}} on understanding this issue in the multi-objective domain. Multi-objective problems with parameter interactions present difficulties to an optimization algorithm, which are not present in the single-objective domain. In this work, we have explained the nature of these difficulties, and investigated the behavior of the NSGA-II, which has difficulties with problems not aligned with the principle coordinate system. This study has investigated Simplex Crossover (SPX), Unimodal Normally Distributed Crossover (UNDX), Parent-Centric Crossover (PCX), and Differential Evolution (DE), as possible alternatives to the Simulated Binary Crossover (SBX) operator within the NSGA-II, on problems exhibiting parameter interactions through a rotation of the coordinate system. An analysis of these operators on three rotated bi-objective test problems, and a four-and eight-objective problem is provided. New observations on the behavior of rotationally invariant crossover operators in the multi-objective problem domain have been reported...|$|E
5000|$|Driving on {{dirt roads}} {{requires}} great attention to {{variations in the}} surface and {{it is easier to}} lose control than on a gravel road. In addition to the hazards already mentioned, and potholes, ruts and <b>ridges,</b> <b>problems</b> associated with driving on gravel roads include: ...|$|R
40|$|The <b>ridging</b> <b>problem</b> in {{ferritic}} {{stainless steel}} is well known and unsolved {{for more than two}} decades. Ferritic stainless steel (FSS) sheets exhibit ridging parallel to the rolling direction when subjected to tension or deep drawing. The origin of ridging has not been clearly explained yet. Most models suggested before are too simplified and underestimate the influence of neighboring grains. In this study, we simulate the ridging phenomenon using the crystal plasticity finite element method (CPFEM). We test the previous models with CPFEM and investigate the relations between orientations and ridging by simulating a more realistic case using EBSD results. open 0...|$|R
5000|$|Given [...] {{training}} data, {{where the}} [...] bag contains samples from a probability distribution [...] and the [...] output label is , one can tackle the distribution regression task {{by taking the}} embeddings of the distributions, and learning the regressor from the embeddings to the outputs. In other words, one can consider the following kernel <b>ridge</b> regression <b>problem</b> ...|$|R
40|$|International audiencePenalized {{selection}} criteria like AIC or BIC {{are among the}} most popular methods for variable selection. Their theoretical properties have been studied intensively and are well understood, but making use of them in case of high-dimensional data is difficult due to the non-convex optimization problem induced by L 0 penalties. In this paper we introduce an adaptive ridge procedure (AR), where iteratively weighted <b>ridge</b> <b>problems</b> are solved whose weights are updated {{in such a way that}} the procedure converges towards selection with L 0 penalties. After introducing AR its specific shrinkage properties are studied in the particular case of orthogonal linear regression. Based on extensive simulations for the non-orthogonal case as well as for Poisson regression the performance of AR is studied and compared with SCAD and adaptive LASSO. Furthermore an efficient implementation of AR in the context of least-squares segmentation is presented. The paper ends with an illustrative example of applying AR to analyze GWAS data...|$|R
30|$|The PCFT is {{proposed}} {{to pick up}} weak components under heavy noise based on the advantages of FT and PCT. As a global transform, FT can avoid <b>ridge</b> extraction <b>problem</b> in low-SNR environment. As a parameterized method, PCT provides an energy-concentrated time–frequency ridge by optimizing the way of signal accumulation. It is these two merits that the PCFT integrates to obtain an energy-concentrated polynomial chirping spectrum utilized in component extraction.|$|R
40|$|Penalized {{selection}} criteria like AIC or BIC {{are among the}} most popular methods for variable selection. Their theoretical properties have been studied intensively and are well understood, but making use of them in case of high-dimensional data is difficult due to the non-convex optimization problem induced by L 0 penalties. An elegant solution to this problem is provided by the multi-step adaptive lasso, where iteratively weighted lasso problems are solved, whose weights are updated {{in such a way that}} the procedure converges towards selection with L 0 penalties. In this paper we introduce an adaptive ridge procedure (AR) which mimics the adaptive lasso, but is based on weighted <b>Ridge</b> <b>problems.</b> After introducing AR its theoretical properties are studied in the particular case of orthogonal linear regression. For the non-orthogonal case extensive simulations are performed to assess the performance of AR. In case of Poisson regression and logistic regression it is illustrated how the iterative procedure of AR can be combined with iterative maximization procedures. The paper ends with an efficient implementation of AR in the context of least-squares segmentation...|$|R
40|$|We {{revisit the}} {{stochastic}} limited-memory BFGS (L-BFGS) algorithm. By proposing a new {{framework for the}} convergence analysis, we prove improved convergence rates and computational complexities of the stochastic L-BFGS algorithms compared to previous works. In addition, we propose several practical acceleration strategies {{to speed up the}} empirical performance of such algorithms. We also provide theoretical analyses for most of the strategies. Experiments on large-scale logistic and <b>ridge</b> regression <b>problems</b> demonstrate that our proposed strategies yield significant improvements vis-à-vis competing state-of-the-art algorithms...|$|R
40|$|AbstractInitially, it is {{proved that}} the EM (estimate, maximize) and OSL (one-step-late) algorithms, when applied to <b>ridge</b> {{regression}} <b>problems,</b> are special cases of the so-called linear stationary methods of the first degree for the underlying system of linear equations. It is shown that, although the EM and OSL algorithms converge, their optimum extrapolated counterparts have faster convergence. Using an incomplete data argument, an alternative interpretation of the extrapolated methods is given, which allows the full potential of optimum extrapolated methods to be exploited...|$|R
40|$|Abstract. There {{has been}} growing recent {{interest}} in probabilistic interpretations of kernel-based methods as well as learning in Banach spaces. The absence of a useful Lebesgue measure on an infinite-dimensional reproducing kernel Hilbert space is a serious obstacle for such stochastic models. We propose an estimation model for the <b>ridge</b> regression <b>problem</b> {{within the framework of}} abstract Wiener spaces and show how the support vector machine solution to such problems can be interpreted in terms of the Gaussian Radon transform. 1...|$|R
40|$|Show {{that the}} {{solution}} for the <b>ridge</b> regression <b>problem</b> ˆwridge = argmax w is given by ˆwridge = (X T X + λI) − 1 X T y. N∑ (yi − w T xi) 2 − λ i= 1 d∑ w 2 j (1) j= 0 This proof is conceptually {{similar to the one}} presented in class on 9 / 8 / 06 for unregularized linear least-squares regression in d dimensions. Let us begin by defining the following quantities: 1 x X...|$|R
40|$|There {{has been}} growing recent {{interest}} in probabilistic interpretations of kernel-based methods as well as learning in Banach spaces. The absence of a useful Lebesgue measure on an infinite-dimensional reproducing kernel Hilbert space is a serious obstacle for such stochastic models. We propose an estimation model for the <b>ridge</b> regression <b>problem</b> {{within the framework of}} abstract Wiener spaces and show how the support vector machine solution to such problems can be interpreted in terms of the Gaussian Radon transform. Comment: 28 pages, 4 figure...|$|R
40|$|Asymmetric rolling, {{in which}} the {{circumferential}} velocities of {{the upper and lower}} rolls are different, can give rise to intense plastic shear strains and in turn shear deformation textures through the sheet thickness. The ideal shear deformation texture of fcc metals can be approximated by the // ND and { 001 } orientations, among which the former improves the deep drawability. The ideal shear deformation texture for bcc metals can be approximated by the Goss { 110 } and { 112 } orientations, among which the former improves the magnetic permeability along the directions and is the prime orientation in grain oriented silicon steels. The intense shear strains can result in the grain refinement and hence improve echanical properties. Steel sheets, especially ferritic stainless steel sheets, and luminum alloy sheets may exhibit an undesirable surface roughening known as ridging or roping, when elongated along RD and TD, respectively. The ridging or roping is caused by differently oriented colonies, which are resulted from the oriented columnar structure in ingots or billets, especially for ferritic stainless steels, that is not easily destroyed by the conventional rolling. The breakdown of columnar structure and the grain refinement can be achieved by asymmetric rolling, resulting in a decrease in the <b>ridging</b> <b>problem...</b>|$|R
40|$|A {{conceptual}} framework for the three-dimensional lee wave is given, and the severe limits of mathematical analysis as applied to the problem are described. It is shown that numerical simulation is the only feasible approach and some simulation results are displayed. These results are then compared with those from corresponding simulations of the much more familiar case of two-dimensional flow over a <b>ridge.</b> The <b>problem</b> of predicting in which portions of the lee wave clear air turbulence encounters are most probable is addressed. Here, idealized linear models are employed to establish a few tentative guidelines...|$|R
40|$|Nowadays {{there is}} an {{increasing}} demand for implant-supported prosthetic rehabilitation of the edentulous <b>ridges.</b> However <b>problems</b> occur regarding adequate bone support for implants. Loss of alveolar bone may be gradual due to age resorption or due to previous local pathology. Alveolar bone augmentation may be achieved by {{using a variety of}} different techniques. The aim of the present paper is to compare two methods for bone augmentation: distraction osteogenesis and bone grafting. Advantages and disadvantages of each one are presented and discussed together with their ability to reconstruct the deficient edentulous ridges with ultimate purpose the placement of implants upported prostheses...|$|R
40|$|In several {{supervised}} learning applications, it happens that reconstruction methods {{have to be}} applied repeatedly before being able to achieve the final solution. In these situations, the availability of learning algorithms able to provide effective predictors {{in a very short}} time may lead to remarkable improvements in the overall computational requirement. Here we consider the kernel <b>ridge</b> regression <b>problem</b> and we look for predictors given by a linear combination of kernel functions plus a constant term, showing that an effective solution can be obtained very fastly by applying specific regularization algorithms directly to the linear system arising from the Empirical Risk Minimization problem...|$|R
40|$|Excessive {{alveolar}} bone resorption is commonly found when teeth are extracted. This {{is a problem}} in anterior part of mouth because it will result in an unaesthetic pontic on a narrow hollowed out alveolar <b>ridge.</b> Yet, another <b>problem</b> is gingival recession and root exposure in adjacent teeth which represent a therapeutic problem to the clinician...|$|R
40|$|In {{this paper}} a simple {{derivation}} of duality is presented for convex quadratic programs with a convex quadratic constraint. This problem arises {{in a number}} of applications including trust region subproblems of nonlinear programming, regularized solution of ill-posed least squares <b>problems,</b> and <b>ridge</b> regression <b>problems</b> in statistical analysis. In general, the dual problem is a concave maximization problem with a linear equality constraint. We apply the duality result to: (1) the trust region subproblem, (2) the smoothing of empirical functions, and (3) to piecewise quadratic trust region subproblems arising in nonlinear robust Huber M-estimation problems in statistics. The results are obtained from a straightforward application of Lagrange duality...|$|R
40|$|A partial ridge {{estimator}} {{is proposed}} as {{a modification of}} the Hoerl and Kennard ridge regression estimator. It is shown that the proposed estimator has certain advantages over the <b>ridge</b> estimator. The <b>problem</b> of taking an additional observation to meet certain optimality criteria is also discussed. 1 This work was supported by NSF Grants GU- 2059 and GU- 19568 and b...|$|R
40|$|EnThe Principal Component Analysis onto References Subspaces is {{a multivariate}} method to analyze {{two sets of}} {{quantitative}} variables when between the two sets exists a directional relationship. When the explicative variables are affected by multicollinearity this technique is not recommended. In literature exist many methods to resolve this <b>problem</b> (<b>Ridge</b> Regression, Principal Component Regression, Partial Least Square, Latent Root Regression), this work shows an alternative method based on simple linear regression...|$|R
40|$|Cataloged from PDF {{version of}} article. In this paper a simple {{derivation}} of duality is presented for convex quadratic programs with a convex quadratic constraint. This problem arises {{in a number}} of applications including trust region subproblems of nonlinear programming, regularized solution of ill-posed least squares <b>problems,</b> and <b>ridge</b> regression <b>problems</b> in statistical analysis. In general, the dual problem is a concave maximization problem with a linear equality constraint. We apply the duality result to: (1) the trust region subproblem, (2) the smoothing of empirical functions, and (3) to piecewise quadratic trust region subproblems arising in nonlinear robust Huber M-estimation problems in statistics. The results are obtained from a straightforward application of Lagrange duality. Ó 2000 Elsevier Science B. V. All rights reserved...|$|R
40|$|Abstract: The Principal Component Analysis onto References Subspaces is {{a multivariate}} method to analyze {{two sets of}} {{quantitative}} variables when between the two sets exists a directional relationship. When the explicative variables are affected by multicollinearity this technique is not recommended. In literature exist many methods to resolve this <b>problem</b> (<b>Ridge</b> Regression, Principal Component Regression, Partial Least Square, Latent Root Regression), this work shows an alternative method based on simple linear regression...|$|R
40|$|We are {{concerned}} with an approximation problem for a symmetric positive semidefinite matrix due to motivation from a class of nonlinear machine learning methods. We discuss an approximation approach that we call {matrix ridge approximation}. In particular, we define the matrix ridge approximation as an incomplete matrix factorization plus a ridge term. Moreover, we present probabilistic interpretations using a normal latent variable model and a Wishart model for this approximation approach. The idea behind the latent variable model in turn leads us to an efficient EM iterative method for handling the matrix <b>ridge</b> approximation <b>problem.</b> Finally, we illustrate the applications of the approximation approach in multivariate data analysis. Empirical studies in spectral clustering and Gaussian process regression show that the matrix ridge approximation with the EM iteration is potentially useful...|$|R
40|$|Multicolinear is a {{case that}} occurs in multi-linear {{regression}} analysis. Using multicolinear, {{it will be difficult}} to separate the influence of each independent variable towards the response variables. It also occurs in a farm production like cabbage. To solve this <b>problem,</b> <b>Ridge</b> regression method is used. This research aims to obtain a Ridge regression model to solve the multicolinear case. By using this method, the alleged regression coefficient is obtained by variance inflation factor less than ten for six free variables.  </p...|$|R
40|$|Abstraction and {{reformulation}} are fundamental, powerful {{ideas in}} artificial intelligence, {{but they have}} not {{had a great deal of}} application in the area of constraint satisfaction. The obvious way to implement abstraction in a constraint satisfaction context is to simplify the problem by removing constraints, and then use the solutions to the simplified problem to guide the search for a solution to the original problem. In a sense, local search, hill climbing methods use this approach, while giving up completeness guarantees. The problem for a complete method is that a simplified problem may be trivial (recent work on locating hard problems suggests that hard problems may cluster on narrow <b>ridges</b> in <b>problem</b> space). Thus there may be too many solutions to the simplified problem to be useful. The Cartesian product representation and interchangeability techniques provide ways of working with compact representations of large sets of solutions...|$|R
40|$|The {{need for}} solving {{weighted}} <b>ridge</b> regression (WRR) <b>problems</b> arises {{in a number}} of collaborative filtering (CF) algorithms. Often, there is not enough time to calculate the exact solution of the WRR problem, or it is not required. The conjugate gradient (CG) method is a state-of-the-art approach for the approximate solution of WRR problems. In this paper, we investigate some applications of the CG method for new and existing implicit feedback CF models. We demonstrate through experiments on the Netflix dataset that CG can be an efficient tool for training implicit feedback CF models...|$|R
40|$|We {{investigate}} {{the application of}} sufficient dimension reduction (SDR) to a noiseless data set derived from a deterministic function of several variables. In this context, SDR provides a framework for ridge recovery. A ridge function {{is a function of}} a few linear combinations of the variables [...] -i. e., a composition of a low-dimensional linear transformation and a nonlinear function. The goal of ridge recovery is: using only point evaluations of the function, estimate the subspace that is the span of the ridge vectors that define the linear combinations. SDR provides the foundation for algorithms that search for this ridge subspace. We study two inverse regression methods for SDR [...] -sliced inverse regression (SIR) and sliced average variance estimation (SAVE) [...] -that approximate the ridge subspace using point evaluations of the function at samples drawn from a given probability density function. We provide convergence results for these algorithms for solving the <b>ridge</b> recovery <b>problem,</b> and we demonstrate the methods on simple numerical test problems...|$|R
40|$|Joint {{sparsity}} regularization in multi-task {{learning has}} {{attracted much attention}} in recent years. The traditional convex formulation employs the group Lasso relaxation to achieve joint sparsity across tasks. Although this approach leads to a simple convex formulation, it suffers from several issues due to the looseness of the relaxation. To remedy this problem, we view jointly sparse multi-task learning as a specialized random effects model, and derive a convex relaxation approach that involves two steps. The first step learns the covariance matrix of the coefficients using a convex formulation which we refer to as sparse covariance coding; the second step solves a <b>ridge</b> regression <b>problem</b> with a sparse quadratic regularizer based on the covariance matrix obtained in the first step. It is shown that this approach produces an asymptotically optimal quadratic regularizer in the multitask learning setting {{when the number of}} tasks approaches infinity. Experimental results demonstrate that the convex formulation obtained via the proposed model significantly outperforms group Lasso (and related multi-stage formulations). ...|$|R
40|$|AbstractIn {{multiple}} {{linear regression}} analysis, multicollinearity is an important <b>problem.</b> <b>Ridge</b> regression {{is one of the}} most commonly used methods to overcome this problem. There are many proposed ridge parameters in the literature. In this paper, we propose some new modifications to choose the ridge parameter. A Monte Carlo simulation is used to evaluate parameters. Also, biases of the estimators are considered. The mean squared error is used to compare the performance of the proposed estimators with others in the literature. According to the results, all the proposed estimators are superior to ordinary least squared estimator (OLS) ...|$|R
40|$|Linear fitting {{techniques}} by implicit algebraic models usually suffer from global stability <b>problems.</b> <b>Ridge</b> regression regularization {{can be used}} to improve the stability of algebraic surface fits. In this paper a Euclidean Invariant 3 D ridge regression matrix is developed and applied to a particular linear algebraic surface fitting method. Utilization of such a regularization in fitting process makes it possible to globally stabilize 3 D object fits with surfaces of any degree. Robustness to noise and moderate levels of occlusion has also been enhanced significantly. Experimental results are presented to verify the improvements in global stability and robustness of the resulting fits...|$|R
40|$|In value {{function}} approximation in RL, however, {{the objective is}} not to recover a target function given its noisy observations, but is instead to approximate the fixed-point of the Bellman operator given sample trajectories. This creates some difficulties in applying Lasso and <b>ridge</b> to this <b>problem.</b> Despite these difficulties, both ℓ 1 and ℓ 2 regularizations have been previously studied in {{value function}} approximation in RL. Farahmand et al. presented several such algorithms wherein ℓ 2 -regularization was added to LSTD and modified Bellman residual minimization (Farahmand et al., 2008), to fitted Q-iteration (Farahmand et al., 2009), and finite-sample performance bounds for these algorithms were proved. There has also been alhal- 00830149...|$|R
40|$|Principal {{component}} analysis (PCA) {{is a popular}} dimensionality reduction and data visualization method. Sparse PCA (SPCA) is its extensively studied and NP-hard-to-solve modifcation. In the past decade, many diferent algorithms were proposed to perform SPCA. We build upon the work of Zou et al. (2006) who recast the SPCA problem into the regression framework and proposed to induce sparsity with the l 1 penalty. Instead, we propose to drop the l 1 penalty and promote sparsity by re-weighting the l 2 -norm. Our algorithm thus consists mainly of solving weighted <b>ridge</b> regression <b>problems.</b> We show that the algorithm basically attempts to fnd a solution to a penalized least squares problem with a non-convex penalty that resembles the l 0 -norm more closely. We also apply the algorithm to analyze the voting records of the Chamber of Deputies of the Parliament of the Czech Republic. We show not only why the SPCA is more appropriate to analyze this type of data, but we also discuss whether the variable selection property can be utilized as an additional piece of information, for example to create voting calculators automatically...|$|R
40|$|Abstract. In the multi-view {{regression}} problem, we have {{a regression}} problem where the input variable (which is a real vector) can be partitioned into two different views, where {{it is assumed that}} either view of the input is sufficient to make accurate predictions — this is essentially (a significantly weaker version of) the co-training assumption for the regression problem. We provide a semi-supervised algorithm which first uses unlabeled data to learn a norm (or, equivalently, a kernel) and then uses labeled data in a ridge regression algorithm (with this induced norm) to provide the predictor. The unlabeled data is used via canonical correlation analysis (CCA, which is a closely related to PCA for two random variables) to derive an appropriate norm over functions. We are able to characterize the intrinsic dimensionality of the subsequent <b>ridge</b> regression <b>problem</b> (which uses this norm) by the correlation coefficients provided by CCA in a rather simple expression. Interestingly, the norm used by the ridge regression algorithm is derived from CCA, unlike in standard kernel methods where a special apriori norm is assumed (i. e. a Banach space is assumed). We discuss how this result shows that unlabeled data can decrease the sample complexity. ...|$|R
25|$|Increasing {{nitrogen}} emission is {{a problem}} facing the delicate balance within the ecosystems that contain the Bay checkerspot in many areas. At Coyote <b>Ridge</b> the <b>problem</b> is well documented by conservation biologist Stuart Weiss. Faced with a declining population of Bay checkerspots at Coyote Ridge, Weiss searched for a cause. He found a link {{to a combination of}} pollution from the freeway below the ridge and, again, a cutback in cattle grazing locally. Weiss documented how nitrogen oxide emissions from cars enriched the nutrient-poor serpentine soil. This {{is a prime example of}} the nitrogen effect explained above. Aside from helping decrease nitrogen, the cattle also help to control invasive grasses by eating them. Any question about whether nitrogen emissions from cars traversing U.S. Route 101, 110,000 vehicles daily, is significant evaporates when faced with monitoring statistics from Weiss. His monitoring equipment has confirmed that 15 to 20 pounds of nitrogen per acre is deposited on Coyote Ridge annually. Some of the particles stick to the plant and ground and are washed into the soil, and others are directly absorbed by the plants themselves. By contrast, pollution from power plants and vehicles drops only about four to five pounds of nitrogen per acre per year on the Jasper Ridge preserve.|$|R
40|$|A novel {{unsupervised}} {{method for}} automatic music structure analysis is proposed. Three types of audio features, namely the mel-frequency cepstral coefficients, the chroma features, and the auditory temporal modulations {{are employed in}} order to form beat-synchronous feature sequences modeling the audio signal. Assume that the feature vectors from each segment lie in a subspace and the song as a whole occupies the union of several subspaces. Then any feature vector can be represented as a linear combination of the feature vectors stemming from the same subspace. The coefficients of such a linear combination are found by solving an appropriate <b>ridge</b> regression <b>problem,</b> resulting to the ridge representation (RR) of the audio features. The RR yields an affinity matrix with nonzero within-subspace affinities and zero between-subspace ones, revealing {{the structure of the}} music recording. The segmentation of the feature sequence into music segments is found by applying the normalized cuts algorithm to the RR-based affinity matrix. In the same context, the combination of multiple audio features is investigated as well. The proposed method is referred to as ridge regression-based music structure analysis (RRMSA). State-of-the-art performance is reported for the RRMSA by conducting experiments on the manually annotated Beatles benchmark dataset. 1...|$|R
