16|84|Public
40|$|A {{relatively}} simple method is proposed {{in this study}} for using accumulated relevance feedback information in an automatic information retrieval system to improve precision and recall. This method involves the dynamic modification of relevant document vectors after each <b>retrieval</b> <b>run.</b> Experiments on a collection of 425 documents indicate that this method does improve precision and recall significantly. More experiments must be conducted to perfect the method for large scale practical applications. 1...|$|E
40|$|Hundreds of {{millions}} of users each day use web search engines to meet their information needs. Advances in web search e#ectiveness are therefore {{perhaps the most significant}} public outcomes of IR research. Query expansion is one such method for improving the e#ectiveness of ranked retrieval by adding additional terms to a query. In previous approaches to query expansion, the additional terms are selected from highly ranked documents returned from an initial <b>retrieval</b> <b>run.</b> We propose a new method of obtaining expansion terms, based on selecting terms from past user queries that are associated with documents in the collection. Ou...|$|E
40|$|Abstract. Evaluation has {{a crucial}} role in Information Retrieval (IR) since it allows for {{identifying}} possible points of failure of an IR approach, thus addressing them to improve its effectiveness. Developing tools to support researchers and analysts when analyzing results and investigat-ing strategies to improve IR system performance can help make the analysis easier and more effective. In this paper we discuss a Visual Analytics-based approach to support the analyst when deciding whether or not to investigate re-ranking to improve the system effectiveness mea-sured after a <b>retrieval</b> <b>run.</b> Our approach is based on effectiveness mea-sures that exploit graded relevance judgements and it provides both a principled and intuitive way to support analysis. A prototype is described and exploited to discuss some case studies based on TREC data. ...|$|E
40|$|The {{notion of}} {{relevance}} differs between assessors, thus {{giving rise to}} assessor disagreement. Although assessor disagreement has been frequently observed, the factors leading to disagreement are still an open problem. In this paper we study the relationship between assessor disagreement and various topic independent factors such as readability and cohesiveness. We build a logistic model using reading level and other simple document features to predict assessor disagreement and rank documents by decreasing probability of disagreement. We compare the predictive power of these document-level features {{with that of a}} meta-search feature that aggregates a document’s ranking across multiple <b>retrieval</b> <b>runs.</b> Our features are shown to be {{on a par with the}} meta-search feature, without requiring a large and diverse set of <b>retrieval</b> <b>runs</b> to calculate. Surprisingly, however, we find that the reading level features are negatively correlated with disagreement, suggesting that they are detecting some other aspect of document content...|$|R
40|$|This {{year the}} Eurospider team, {{with help from}} Columbia, focused on trying {{different}} combinations of translation approaches. We investigated the use and integration of pseudo-relevance feedback, multilingual similarity thesauri and machine translation. We also looked at different ways of merging individual crosslanguage <b>retrieval</b> <b>runs</b> to produce multilingual result lists. We participated in both the CLIR main task and the GIRT sub task. 1...|$|R
40|$|Traditional {{retrieval}} evaluation uses explicit relevance judgments {{which are}} expensive to collect. Relevance assessments inferred from implicit feedback such as click-through {{data can be}} collected inexpensively, but may be less reliable. We compare assessments derived from click-through data to another source of implicit feedback that we assume to be highly indicative of relevance: purchase decisions. Evaluating <b>retrieval</b> <b>runs</b> based on a log of an audio-visual archive, we find agreement between system rankings and purchase decisions to be surprisingly high...|$|R
40|$|Evaluation has {{a crucial}} role in Information Retrieval (IR) since it allows {{possible}} point of failures of an IR approach to be identi ed and addressed thus improving the predictive capability of such approach. Developing tools to support users when analyzing results and investigating strategies to improve IR system performance can help make the analysis easier and more eective. In this paper we discuss a Visual Analytics-based approach to support the user when deciding whether or not to perform re-ranking to improve the system eectiveness measured after a <b>retrieval</b> <b>run.</b> The proposed approach is based on eectiveness measures that exploit graded relevance judgements and provide both a principled and intuitive way to support the user. A prototype is described and exploited to discuss some case studies based on TREC data...|$|E
40|$|Abstract. This paper {{presents}} our submitted {{experiments in}} the Concept annotation and Concept Retrieval tasks using Flickr photos at ImageCLEF 2012. This edition we applied new strategies for both the textual and the visual subsystems included in our multimodal retrieval system. The visual subsystem has focus on extending the low-level features vector with concept features. These concept features have been calculated {{by means of a}} logistic regression model. The textual subsystem has focus on expanding the query information using external resources. Our best concept <b>retrieval</b> <b>run,</b> a multimodal one, is at the ninth position with a MnAP of 0. 0295, being the second best group of the contest for the multimodal modality. This is also our best run in the global ordered list (where eleven textual runs are also better than it). We have adapted our multimodal retrieval process for the annotation task obtaining non-very good results for this first participation, with a MiAP of 0. 1020...|$|E
40|$|Abstract: In this paper, we {{document}} {{our efforts}} in participating to the TREC 2007 Legal track. We had multiple aims: First, {{to experiment with}} using different query formulations, trying to exploit the verbose topic statements. Second, to analyse how ranked retrieval methods can be fruitfully combined with traditional Boolean queries. Our main findings can be summarized as follows: First, we got mixed results trying to combine the original search request with terms extracted from the verbose topic statement. Second, by combining the Boolean reference run wit our ranked <b>retrieval</b> <b>run</b> allows us to get the high recall of the Boolean retrieval, whilst precision scores show an improvement over both the Boolean and the ranked retrieval runs. Third, {{we found out that}} if we treat the Boolean query as free text with varying degrees of interpretation of the original operator, we get competitive results. Moreover, both types of queries seem to capture different relevant documents, and the combination between the request text and the Boolean query leads to substantial gain in precision and recall. ...|$|E
40|$|This poster {{describes}} {{a potential problem}} with a relatively well used measure in Information Retrieval research: Kendall&# 039;s Tau rank correlation coefficient. The coefficient {{is best known for}} its use in determining the similarity of test collections when ranking sets of <b>retrieval</b> <b>runs.</b> Threshold values for the coefficient have been defined and used in a number of published studies in information retrieval. However, this poster presents results showing that basing decisions on such thresholds is not as reliable as has been assumed...|$|R
40|$|We {{describe}} evaluation experiments {{conducted by}} submitting <b>retrieval</b> <b>runs</b> for the monolingual Bulgarian, Czech and Hungarian information retrieval tasks of the Ad-Hoc Track of the Cross-Language Evaluation Forum (CLEF) 2007. In {{the ad hoc}} retrieval tasks, the system was given 50 natural language queries, and {{the goal was to}} find all of the relevant documents (with high precision) in a particular document set. We conducted diagnostic experiments with different techniques for matching word variations and handling stopwords, comparing the performance on the robust Gener-alize...|$|R
40|$|Collection fusion is a {{data fusion}} problem {{in which the}} results of <b>retrieval</b> <b>runs</b> on separate, {{autonomous}} document collections must be merged to produce a single, effective re-sult. This paper explores two collection fusion techniques that learn the rmrnber of documents to retrieve from each collection using only the ranked lists of documents returned in response to past queries and those documents! relevance judgments. Retrieval experiments using the TREC test co]lection demonstrate that {{the effectiveness of the}} fusion techniques is within 10 ’?%of the effectiveness of a run in which the entire set of documents is treated as a single collection. ...|$|R
40|$|In text-based image {{retrieval}}, the Incomplete Annotation Problem (IAP) {{can greatly}} degrade retrieval effectiveness. A standard method used {{to address this}} problem is pseudo relevance feedback (PRF) which updates user queries by adding feedback terms selected automatically from top ranked documents in a prior <b>retrieval</b> <b>run.</b> PRF assumes that the target collection provides enough feedback information to select effective expansion terms. This is often not the case in image retrieval since images often only have short metadata annotations leading to the IAP. Our work proposes the use of an external knowledge resource (Wikipedia) in the process of refining user queries. In our method, Wikipedia documents strongly related to the terms in user query (" definition documents") are first identified by title matching between the query and titles of Wikipedia articles. These definition documents are used as indicators to re-weight the feedback documents from an initial search run on a Wikipedia abstract collection using the Jaccard coefficient. The new weights of the feedback documents are combined with the scores rated by different indicators. Query-expansion terms are then selected based on these new weights for the feedback documents. Our method is evaluated on the ImageCLEF WikipediaMM image retrieval task using text-based retrieval on the document metadata fields. The results show significant improvement compared to standard PRF methods...|$|E
40|$|AbstractSeveral {{wind turbine}} {{simulation}} packages use reduced models for computationally efficient load case simulations of offshore wind turbines. These models capture the global wind turbine behaviour, where the support structure behaviour is expressed using {{a limited number}} of eigenmode amplitudes involving the lower frequencies. However, the disregarded higher eigenmodes are significant for the detailed behaviour of support structure members and contribute significantly to the fatigue damage and maximum stresses under extreme loads. To get detailed member load information, fully integrated simulations can be performed {{at the expense of the}} computational efficiency gained by using reduced models. Alternatively, load case simulations may be performed sequentially. This involves water flow load evaluation at a stage where the tower motion is not yet known. This paper presents a new sequential approach in which the sensitivities of the water flow loads with respect to support structure motion are conveyed to the dynamic simulation stage, and in which the parts of the water flow loads that are disregarded in the reduced model space are recovered in the <b>retrieval</b> <b>run,</b> allowing evaluation of the contribution of these forces to the fatigue damage and maximum stresses. Application of the new approach confirms that these contributions are significant. Furthermore, the new method is reasonably efficient, requiring about 80 % extra calculation time compared to the traditional method...|$|E
40|$|Choosing {{the correct}} target words is a {{difficult}} problem for machine translation. In cross-language information retrieval, this problem of choice is mitigated since more than one translation alternative can be retained in the target query. Between choosing just one word as a translation and keeping all the possible translations for each source word, one can apply a range of filtering techniques for eliminating some words and keeping others. In the bilingual track of CLEF 2002, focusing on word translation ambiguity, we experimented with several techniques for choosing the best target translation for each source query word by using co-occurrence statistics in a reference corpus consisting of documents in the target language. One of two distinct corpora was used, the target-language test corpus or the World Wide Web. Our techniques give one best translation per source query word. We also experimented with combining these word choice results (providing up to three translations for each word) in the final translated query. The source query languages were Spanish and Chinese; the target language documents were in English. We submitted four automatic runs for each language pair. When the methods were combined, mixing results obtained with different reference corpora, the recall and average precision of Spanish-to-English retrieval reached 95 % and 97 %, respectively, of the recall and average precision of an English monolingual <b>retrieval</b> <b>run.</b> For Chinese-to-English text retrieval, the recall and average precision reached 89 % and 60 %, respectively, of the English run. 1...|$|E
40|$|Abstract: In this paper, we {{document}} our official submissions to the TREC 2007 Legal track. Our main aims were two-fold: First, we {{experimented with}} using different query formulations trying {{to exploit the}} verbose topic statements. Second, we analysed how ranked retrieval methods can be fruitfully combined with traditional Boolean queries. Our main findings can be summarized as follows. First, we got mixed results trying to combine the original search request with terms extracted from the verbose topic statement. Second, by combining Boolean and ranked retrieval allows us to get the high recall of the Boolean retrieval, whilst precision scores show an improvement over both the Boolean and the ranked <b>retrieval</b> <b>runs...</b>|$|R
40|$|In this notebook, we {{describe}} the automatic <b>retrieval</b> <b>runs</b> from NEC Laboratories America (NECLA) for the Text REtrieval Conference (TREC) 2012 Medical Records track. Our approach {{is based on a}} combination of UMLS medical concept detection and a set of simple retrieval models. Our best run, sennamed 2, has achieved the best inferred average pre-cision (infAP) score on 5 of the 47 test topics, and obtained a higher score than the median of all submission runs on 27 other topics. Over-all, sennamed 2 ranks at the second place amongst all the 82 automatic runs submitted for this track, and obtains the third place amongst both automatic and manual submissions. ...|$|R
40|$|This paper {{examines}} {{the feasibility of}} merging the results of <b>retrieval</b> <b>runs</b> on separate, autonomous document collections into an effective combined result. In particular, we examine two collection fusion techniques that use the results of past queries to compute the number of documents to retrieve from each {{of a set of}} subcollections such that the total number of retrieved documents is equal to N, the number of documents to be returned to the user. The fusion techniques are independent of the particular weighting schemes, similarity measures, and retrieval models used by the component collections. Our official TREC- 3 runs are fusion runs in which N = 100...|$|R
40|$|The use of {{feedback}} information {{is an effective}} approach to address the vocabulary gap between a user's query and the relevant documents. It {{has been shown that}} some relevant documents act like "poison pills," i. e. they hurt the performance {{of feedback}} systems {{despite the fact that they}} are relevant. In this paper, we study the positive counterpart of this by investigating the helpfulness of nonrelevant documents in feedback. In general, we find that although documents that are explicitly judged as non-relevant are normally assumed to be poisonous for feedback systems, sometimes considering high-scored non-relevant documents as a positive feedback helps to improve the performance of retrieval. In our experimental data, we observe a considerable fraction of non-relevant documents in higher ranked positions of the initial <b>retrieval</b> <b>run,</b> for most of the topics. Hence, by ignoring the potential value of non-relevant documents, we may loose a lot of useful information. We investigate the potential contribution of non-relevant documents using existing state-of-the-art feedback methods. Our main findings are the following. First, we find that some of the nonrelevant documents are exclusively helpful, they improve retrieval on their own, and others are complementary helpful, they lead to further improvement when added to a set of relevant documents. Second, we discover that, on average, exclusively helpful non-relevant documents have a higher contribution to the performance improvement, compared to the complementary ones. Third, we show that non-relevant documents in topics with poor average precision in the initial retrieval are more likely to help in the feedback...|$|E
40|$|University of Minnesota M. S. thesis. September 2013. Major: Computer science. Advisor: Carolyn B. Donald. 1 {{computer}} file (PDF); v, 32 pages. Information retrieval is {{the science of}} returning data from a corpus (a large collection of documents) matching the user's informational need. It identifies the data (originally in document form) by matching the terms in the query with terms contained in the documents of the collection. Representing documents and queries for effective retrieval is best accomplished by defining a model. Among the various models, the one most frequently used is Salton's Vector Space Model [9]. In this model, documents and queries are represented as vectors. The similarity between the query and a document is found by using a similarity measure (e. g., cosine). Extensible Markup Language (XML) is a simple, flexible text format derived from Standard Generalized Markup Language (SGML) [3], {{designed to meet the}} challenges of large-scale electronic publishing, XML plays an important role in the exchange of a wide variety of data on the Web and elsewhere. INEX (The Initiative for evaluation of XML retrieval) [1] sponsors a competition that promotes the development of XML-based retrieval. It provides a Wikipedia collection in the form of XML files and each of these XML files are well defined and documented. We are interested in building a reference run for a given set of queries without first performing a separate <b>retrieval</b> <b>run</b> to produce it. To that end, we perform some basic experiments which are based on the terminal nodes of the document set. The experiments depend upon the content of these nodes. Analysis of these early results, conclusions, and suggestions for future research are included...|$|E
40|$|We {{propose a}} novel method of query {{expansion}} for Language Modeling (LM) in Information Retrieval (IR) {{based on the}} similarity of the query with sentences in the top ranked documents from an initial <b>retrieval</b> <b>run.</b> In justiﬁcation of our approach, we argue that the terms in the expanded query obtained by the proposed method roughly follow a Dirichlet distribution which, being the conjugate prior of the multinomial distribution used in the LM retrieval model, helps the feedback step. IR experiments on the TREC ad-hoc retrieval test collections using the sentence based query expansion (SBQE) show a signiﬁcant increase in Mean Average Precision (MAP) compared to baselines obtained using standard term-based query expansion using LM selection score and the Relevance Model (RLM). The proposed approach to query expansion for LM {{increases the likelihood of}} generation of the pseudo-relevant documents by adding sentences with maximum term overlap with the query sentences for each top ranked pseudorelevant document thus making the query look more like these documents. A per topic analysis shows that the new method hurts less queries compared to the baseline feedback methods, and improves average precision (AP) over a broad range of queries ranging from easy to difﬁcult in terms of the initial retrieval AP. We also show that the new method is able to add a higher number of good feedback terms (the golden standard of good terms being the set of terms added by True Relevance Feedback). Additional experiments on the challenging search topics of the TREC- 2004 Robust track show that the new method is able to improve MAP by 5. 7 % without the use of external resources and query hardness prediction typically used for these topics...|$|E
40|$|DCU {{participated in}} the ImageCLEF 2008 photo <b>retrieval</b> task, {{submitting}} <b>runs</b> for both the English and Random language annotation conditions. Our approaches used text-based and image-based retrieval approaches to give baseline <b>retrieval</b> <b>runs,</b> with the highest-ranked images from these baseline runs clustered using K-Means clustering of the text annotations. Finally, each cluster was represented by its most relevant image and these images were ranked for the final submission. For random annotation language runs, we used TextCat 1 to identify German annotation documents, which were then translated into English using Systran Version: 3. 0 Machine Translator. We also compared results from these translated runs with untranslated runs. Our results showed that, as expected, runs that combine image and text outperform text alone and image alone. Our baseline image+text runs (i. e. without clustering) give our best MAP score, and these runs also outperformed the mean and median ImageCLEFPhoto submissions for CR@ 20. Clustering approaches consistently gave a large improvement i...|$|R
40|$|York University {{participated in}} the {{terabyte}} track this year. Using the GOV 2 collection, we used filtering techniques to shorten the amount of data to be indexed before indexing into eight partitions. As there were several different subsections of the terabyte track, we chose {{to participate in the}} ad hoc and named page <b>retrieval</b> <b>runs.</b> Our technique involved partitioned indexes across a single machine. We combined our results by first calculating the document frequency of a term across all the indexes, calculating the weight, then using the same weight in retrieving the top results from each index. This approach effectively tried to mimic the results that would be obtained if there were only one large index...|$|R
40|$|The {{relative}} {{performance of}} retrieval systems when evaluated on {{one part of}} a test collection may bear little or no similarity to the relative performance measured on {{a different part of the}} collection. In this paper we report the results of a detailed study of the impact that different sub-collections have on retrieval effectiveness, analyzing the effect over many collections, and with different approaches to sub-dividing the collections. The effect is shown to be substantial, impacting on comparisons between <b>retrieval</b> <b>runs</b> that are statistically significant. Some possible causes for the effect are investigated, and the implications of this work are examined for test collection design and for the strength of conclusions one can draw from experimental results...|$|R
40|$|Hundreds of {{millions}} of users each day search the web and other repositories to meet their information needs. However, queries can fail to find documents due to a mismatch in terminology. Query expansion seeks {{to address this problem}} by automatically adding terms from highly ranked documents to the query. While query expansion {{has been shown to be}} effective at improving query performance, the gain in effectiveness comes at a cost: expansion is slow and resource-intensive. Current techniques for query expansion use fixed values for key parameters, determined by tuning on test collections. We show that these parameters may not be generally applicable, and, more significantly, that the assumption that the same parameter settings can be used for all queries is invalid. Using detailed experiments, we demonstrate that new methods for choosing parameters must be found. In conventional approaches to query expansion, the additional terms are selected from highly ranked documents returned from an initial <b>retrieval</b> <b>run.</b> We demonstrate a new method of obtaining expansion terms, based on past user queries that are associated with documents in the collection. The most effective query expansion methods rely on costly retrieval and processing of feedback documents. We explore alternative methods for reducing query-evaluation costs, and propose a new method based on keeping a brief summary of each document in memory. This method allows query expansion to proceed three times faster than previously, while approximating the effectiveness of standard expansion. We investigate the use of document expansion, in which documents are augmented with related terms extracted from the corpus during indexing, as an alternative to query expansion. The overheads at query time are small. We propose and explore a range of corpus-based document expansion techniques and compare them to corpus-based query expansion on TREC data. These experiments show that document expansion delivers at best limited benefits, while query expansion, including standard techniques and efficient approaches described in recent work, usually delivers good gains. We conclude that document expansion is unpromising, but it is likely that the efficiency of query expansion can be further improved...|$|E
40|$|Abstract. The {{paper for}} the CHiC pilot lab {{describes}} the motivation, tasks, Europeana collections and topics, evaluation measures {{as well as}} the submitted and analyzed information <b>retrieval</b> <b>runs.</b> In its first year, CHiC offered three tasks: ad-hoc, which measured retrieval effectiveness according to relevance of the ranked retrieval results (standard 1000 document TREC output), variability, which required participants to present a list of 12 records that represent diverse information contexts and semantic enrichment, which asked participants to provide a list of 10 semantically related concepts to the one in the query to be used in query expansion experiments. All tasks were offered in monolingual, bilingual and multilingual modes. 126 different experiments from 6 participants were evaluated using the DIRECT system...|$|R
40|$|This paper {{describes}} {{our opinion}} retrieval system for TREC 2008 blog track. We focused on five {{different aspects of}} the system. The first module is focussed on extracting the blog content out from junk html and thereby decreasing the noise in the indexed content. The second module aims at removing various kind of spam content from real blogs. The third module aimed at retrieving the relevant documents. The fourth module filters out opinionated documents and the fifth one calculated the polarity of the sentiments in the document. The final ranked <b>retrieval</b> <b>runs</b> were based on various combination of settings in each module so as to study the effect of each. For classification of subjectivity and polarity, the predictions we done using a complementary naive bayes classifier...|$|R
40|$|This paper {{describes}} {{the participation of}} the Human Interface Laboratory of Meiji University in the ImageCLEF 2008 photo retrieval task. We submitted eight <b>retrieval</b> <b>runs</b> taking two main approaches. The first approach combined Text-Based Image Retrieval (TBIR) and Context-Based Image Retrieval (CBIR). The second approach applied query expansion using conceptual fuzzy sets (CFS). A CFS is a method that uses the expression of meaning depending on the context, which an ordinary fuzzy set does not recognize. A conceptual dictionary is necessary to perform query expansion using CFS, and this is constructed by clustering. We propose here the use of query expansion with CFS, pseudo relevance feedback (PRF), and other techniques, for image retrieval that integrates different media, and we verify the utility of the system by explaining our experimental results...|$|R
40|$|Table 1 : Kendall’s Tau rank {{correlation}} coefficient between the automatic RS approach and statAP/MTC respectively. All correlations are significant (p < 0. 01). Column 2 contains {{the average number}} of sampled documents from the pool. For the automatic evaluation, we implemented the random sampling approach [7]: first, the top p retrieved documents of all <b>retrieval</b> <b>runs</b> for a particular query are pooled together such that a document that is retrieved by x runs, appears x times in the pool. Then, a number m of documents are drawn at random from the pool; those are now considered to be the pseudo relevant documents. This process is performed for each query and the subsequent evaluation of each system is performed with pseudo relevance judgments instead of relevance judgments. Due to the randomnes...|$|R
3000|$|Retrieve. The data <b>retrieval</b> {{algorithm}} is <b>run</b> by CSC. Taken as input the system global parameter GP, the keyword secure index I [...]...|$|R
40|$|Information {{retrieval}} (IR) researchers commonly use {{three tests}} of statistical significance: the Student’s paired t-test, the Wilcoxon signed rank test, and the sign test. Other researchers have previously proposed using both the bootstrap and Fisher’s randomization (permutation) test as nonparametric significance tests for IR but these tests have seen little use. For {{each of these}} five tests, we took the ad-hoc <b>retrieval</b> <b>runs</b> submitted to TRECs 3 and 5 - 8, and for each pair of runs, we measured the statistical significance {{of the difference in}} their mean average precision. We discovered that there is little practical difference between the randomization, bootstrap, and t tests. Both the Wilcoxon and sign test have a poor ability to detect significance and have the potential to lead to false detections of significance. The Wilcoxon and sign tests are simplified variants of the randomization test and their use should be discontinued for measuring the significance of a difference between means...|$|R
40|$|In TREC 2007 Blog Track, we {{developed}} a three-step algorithm for the opinion retrieval task. An information retrieval step retrieves the query-relevant documents. A following opinion identification step identifies the opinionative texts in these documents. A ranking step identifies the query-related opinions in the documents and ranks them by calculating their opinion similarity scores. For the polarity task, our strategy {{is to find the}} positive and negative documents respectively, and then find the mixed opinionative documents in the intersection of the positive and negative document sets. We implemented our opinion retrieval algorithm in two special cases, one to retrieve the positive documents, and the other to retrieve the negative documents. A judging function labeled a subset of the documents, which were in the intersection of the positive and negative documents, as the mixed opinionative documents. We studied two parameters in our opinion retrieval algorithm, each of which had two values to compare. This resulted in four submitted opinion <b>retrieval</b> <b>runs</b> and their corresponding polarity runs. 1...|$|R
40|$|The TREC- 3 {{project at}} Virginia Tech focused on methods for {{combining}} {{the evidence from}} multiple <b>retrieval</b> <b>runs</b> and queries to improve retrieval performance over any single retrieval method or query. The largest improvements result from the combination of retrieval paradigms {{rather than from the}} use of multiple similar queries. 1 Overview The primary focus of our experiments at Virginia Tech involved methods of combining the results from various divergent search schemes and document collections. In performing our TREC- 3 ad-hoc retrieval experiments on the provided test collections, the results from both vector and P-norm [3] queries were considered in estimating the similarity for each document in an individual collection. The results for each collection were then merged to create a single final set of documents that would be presented to the user. Our TREC- 3 experiments built upon our TREC- 2 experiments and focused more on determining where the improvements in combination were derived [...] ...|$|R
40|$|This book {{constitutes}} the thoroughly refereed {{proceedings of the}} 7 th International Workshop of the Initiative for the Evaluation of XML Retrieval, INEX 2008, held at Dagstuhl Castle, Germany, in December 2008. The aim of the INEX 2008 workshop was to bring together researchers {{who participated in the}} INEX 2008 campaign. Over the year leading up to the event, participating organizations contributed to the building of a large-scale XML test collection by creating topics, performing <b>retrieval</b> <b>runs,</b> and providing relevance assessments. The workshop concluded the results of this large-scale effort, summarized and addressed the issues encountered, and devised a work plan for the future evaluation of XML retrieval systems. The 49 papers included in this volume report the final results of INEX 2008. They have been divided into sections according to the seven tracks of the workshop, investigating various aspects of XML retrieval, from book search to entity ranking, including interaction aspects...|$|R
40|$|The {{existence}} {{and use of}} standard test collections in information retrieval experimentation allows results to be compared between research groups and over time. Such comparisons, however, are rarely made. Most researchers only report results from their own experiments, a practice that allows lack of overall improvement to go unnoticed. In this paper, we analyze results achieved on the TREC Ad-Hoc, Web, Terabyte, and Robust collections as reported in SIGIR (1998 – 2008) and CIKM (2004 – 2008). Dozens of individual published experiments report effectiveness improvements, and often claim statistical significance. However, {{there is little evidence}} of improvement in ad-hoc retrieval technology over the past decade. Baselines are generally weak, often being below the median original TREC system. And in only a handful of experiments is the score of the best TREC automatic run exceeded. Given this finding, we question the value of achieving even a statistically significant result over a weak baseline. We propose that the community adopt a practice of regular longitudinal comparison to ensure measurable progress, or at least prevent the lack of it from going unnoticed. We describe an online database of <b>retrieval</b> <b>runs</b> that facilitates such a practice...|$|R
40|$|We {{describe}} evaluation experiments {{conducted by}} submitting <b>retrieval</b> <b>runs</b> for the monolingual German, French, English and Persian (Farsi) information retrieval tasks of the Ad Hoc Track of the Cross-Language Evaluation Forum (CLEF) 2009. In {{the ad hoc}} retrieval tasks, the system was given 50 natural language queries, and {{the goal was to}} find all of the relevant records or documents (with high precision) in a par-ticular document set. We conducted diagnostic experiments with different techniques for matching word variations, comparing the performance on the robust Generalized Success@ 10 measure and the non-robust Mean Average Precision measure. The mea-sures generally agreed on the mean benefits of morphological techniques such as de-compounding and stemming, but generally disagreed on the blind feedback technique. Also, for each language, we submitted a sample of the first 10000 retrieved items to in-vestigate the frequency of relevant items at deeper ranks than the official judging depth of 60 for German, French and English and 80 for Persian. The results suggest that, on average, the percentage of relevant items assessed was less than 62 % for German, 27 % for French, 35 % for English and 22 % for Persian...|$|R
