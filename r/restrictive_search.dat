11|27|Public
40|$|Background: A major {{challenge}} in updating clinical guidelines is to efficiently identify new, relevant evidence. We evaluated {{the efficiency and}} feasibility of two new approaches: the development of <b>restrictive</b> <b>search</b> strategies using PubMed Clinical Queries for MEDLINE {{and the use of}} the PLUS (McMaster Premium Literature Service) database. Methods: We evaluated a random sample of recommendations from a national guideline development program and identified the references that would potentially trigger an update (key references) using an exhaustive approach. We designed <b>restrictive</b> <b>search</b> strategies using the minimum number of Medical Subject Headings (MeSH) terms and text words required from the original exhaustive search strategies and applying broad and narrow filters. We developed PLUS search strategies, matching Medical Subject Headings (MeSH) and Systematized Nomenclature of Medicine (SNOMED) terms with guideline topics. We compared the number of key references retrieved by these approaches with those retrieved by the exhaustive approach. Results: The restrictive approach retrieved 68. 1 % fewer references than the exhaustive approach (12, 486 versus 39, 136), and identified 89. 9 % (62 / 69) of key references and 88 % (22 / 25) of recommendation updates. The use of PLU...|$|E
40|$|Nowadays, many web {{databases}} "hidden" {{behind their}} <b>restrictive</b> <b>search</b> interfaces (e. g., Amazon, eBay) contain rich and valuable {{information that is}} of significant interests to various third parties. Recent studies have demonstrated the possibility of estimating/tracking certain aggregate queries over dynamic hidden web databases. Nonetheless, tracking all possible aggregate query answers to report interesting findings (i. e., exceptions), while still adhering to the stringent query-count limitations enforced by many hidden web databases providers, is very challenging. In this paper, we develop a novel technique for tracking and discovering exceptions (in terms of sudden changes of aggregates) over dynamic hidden web databases. Extensive real-world experiments demonstrate the superiority of our proposed algorithms over baseline solutions...|$|E
40|$|During {{the recent}} years the Internet {{has evolved into a}} very {{effective}} medium for storing information. Providing database search facility on the Internet using different options, categories etc is common. Searchable aviation accident related databases also follow similar approach and provide <b>restrictive</b> <b>search</b> facility. This report is on the development of an interactive Web based query engine for Aircraft Accident Statistics and Knowledge (AASK) database. The type of search facilities provided by the query engine are different from other search engines and searchable database web sites. The aim of AASK database query engine is to provide better and more flexible database search facility to people concerned with aviation safety all over the world...|$|E
40|$|For online {{searches}} in patents it {{is important}} to select a suitable database depending on the <b>search</b> topic. A <b>restrictive</b> but unerring <b>search</b> strategy has proved to be the best form for statistical analysis. Taking the example of neural networks the search strategy developed in the article shows that patents can well be used as a source of technical information in computer technology...|$|R
40|$|More {{and more}} {{systematic}} reviews of diagnostic test accuracy studies are being published, {{but they can}} be methodologically challenging. In this paper, the authors present some of the recent developments in the methodology for conducting systematic reviews of diagnostic test accuracy studies. <b>Restrictive</b> electronic <b>search</b> filters are discouraged, as is the use of summary quality scores. Methods for meta-analysis should take into account the paired nature of the estimates and their dependence on threshold. Authors of these reviews are advised to use the hierarchical summary receiver-operating characteristic or the bivariate model for the data analysis. Challenges that remain are the poor reporting of original diagnostic test accuracy studies and difficulties with the interpretation of the results of diagnostic test accuracy researc...|$|R
40|$|Standardised MedDRA Queries (SMQs) {{have been}} {{developed}} since the early 2000 's and used by academia, industry, public health, and government sectors for detecting safety signals in adverse event safety databases. The {{purpose of the present}} study is to characterize how SMQs are used and the impact in safety analyses for New Drug Application (NDA) and Biologics License Application (BLA) submissions to the United States Food and Drug Administration (USFDA). We used the PharmaPendium database to capture SMQ use in Summary Basis of Approvals (SBoAs) of drugs and biologics approved by the USFDA. Characteristics of the drugs and the SMQ use were employed to evaluate the role of SMQ safety analyses in regulatory decisions and the veracity of signals they revealed. A comprehensive search of the SBoAs yielded 184 regulatory submissions approved from 2006 to 2015. Search strategies more frequently utilized <b>restrictive</b> <b>searches</b> with "narrow terms" to enhance specificity over strategies using "broad terms" to increase sensitivity, while some involved modification of search terms. A majority (59 %) of 1290 searches used descriptive statistics, however inferential statistics were utilized in 35 % of them. Commentary from reviewers and supervisory staff suggested that a small, yet notable percentage (18 %) of 1290 searches supported regulatory decisions. The searches with regulatory impact were found in 73 submissions (40 % of the submissions investigated). Most searches (75 % of 227 searches) with regulatory implications described how the searches were confirmed, indicating prudence in the decision-making process. SMQs have an increasing role in the presentation and review of safety analysis for NDAs/BLAs and their regulatory reviews. This study suggests that SMQs are best used for screening process, with descriptive statistics, description of SMQ modifications, and systematic verification of cases which is crucial for drawing regulatory conclusions...|$|R
40|$|Artículo de publicación ISIBackground: A major {{challenge}} in updating clinical guidelines is to efficiently identify new, relevant evidence. We evaluated {{the efficiency and}} feasibility of two new approaches: the development of <b>restrictive</b> <b>search</b> strategies using PubMed Clinical Queries for MEDLINE {{and the use of}} the PLUS (McMaster Premium Literature Service) database. Methods: We evaluated a random sample of recommendations from a national guideline development program and identified the references that would potentially trigger an update (key references) using an exhaustive approach. We designed <b>restrictive</b> <b>search</b> strategies using the minimum number of Medical Subject Headings (MeSH) terms and text words required from the original exhaustive search strategies and applying broad and narrow filters. We developed PLUS search strategies, matching Medical Subject Headings (MeSH) and Systematized Nomenclature of Medicine (SNOMED) terms with guideline topics. We compared the number of key references retrieved by these approaches with those retrieved by the exhaustive approach. Results: The restrictive approach retrieved 68. 1 % fewer references than the exhaustive approach (12, 486 versus 39, 136), and identified 89. 9 % (62 / 69) of key references and 88 % (22 / 25) of recommendation updates. The use of PLUS retrieved 88. 5 % fewer references than the exhaustive approach (4, 486 versus 39, 136) and identified substantially fewer key references (18 / 69, 26. 1 %) and fewer recommendation updates (10 / 25, 40 %). Conclusions: The proposed restrictive approach is a highly efficient and feasible method to identify new evidence that triggers a recommendation update. Searching only in the PLUS database proved to be a suboptimal approach and suggests the need for topic-specific tailoring. Canadian Institutes of Health Research 	 Instituto de Salud Carlos III (FIS) 	 CM 11 / 00035 CM 12 / 00168 CP 09 / 00137 FIS PI 10 / 0034...|$|E
40|$|Numerous web databases, e. g., amazon. com, eBay. com, are “hid-den ” behind (i. e., {{accessible}} only through) their <b>restrictive</b> <b>search</b> and browsing interfaces. This demonstration showcases HDBTrack-er, a web-based system that reveals and tracks (the changes of) user-specified aggregate queries over such hidden web databases, espe-cially {{those that are}} frequently updated, by issuing a small num-ber of search queries through the public web interfaces of these databases. The ability to track and monitor aggregates has appli-cations over {{a wide variety of}} domains- e. g., government agen-cies can track COUNT of openings at online job hunting websites to understand key economic indicators, while businesses can track the AVG price of a product over a basket of e-commerce websites to understand the competitive landscape and/or material costs. A key technique used in HDBTracker is RS-ESTIMATOR, the first algorithm that can efficiently monitor changes to aggregate query answers over a hidden web database. 1...|$|E
40|$|Artículo de publicación ISIBackground: Clinical {{practice}} guidelines (CPGs) become quickly outdated {{and require}} a periodic reassessment of evidence research {{to maintain their}} validity. However, there is little research about this topic. Our project will provide evidence {{for some of the}} most pressing questions in this field: 1) what is the average time for recommendations to become out of date?; 2) what is the comparative performance of two restricted search strategies to evaluate the need to update recommendations?; and 3) what is the feasibility of a more regular monitoring and updating strategy compared to usual practice?. In this protocol we will focus on questions one and two. Methods: The CPG Development Programme of the Spanish Ministry of Health developed 14 CPGs between 2008 and 2009. We will stratify guidelines by topic and by publication year, and include one CPG by strata. We will develop a strategy to assess the validity of CPG recommendations, which includes a baseline survey of clinical experts, an update of the original exhaustive literature searches, the identification of key references (reference that trigger a potential recommendation update), and the assessment of the potential changes in each recommendation. We will run two alternative search strategies to efficiently identify important new evidence: 1) PLUS search based in McMaster Premium LiteratUre Service (PLUS) database; and 2) a <b>Restrictive</b> <b>Search</b> (ReSe) based on the least number of MeSH terms and free text words needed to locate all the references of each original recommendation. We will perform a survival analysis of recommendations using the Kaplan-Meier method and we will use the log-rank test to analyse differences between survival curves according to the topic, the purpose, the strength of recommendations and the turnover. We will retrieve key references from the exhaustive search and evaluate their presence in the PLUS and ReSe search results. Discussion: Our project, using a highly structured and transparent methodology, will provide guidance of when recommendations are likely to be at risk of being out of date. We will also assess two novel <b>restrictive</b> <b>search</b> strategies which could reduce the workload without compromising rigour when CPGs developers check for the need of updating...|$|E
40|$|Many {{databases}} {{on the web}} are "hidden" behind (i. e., {{accessible only}} through) their <b>restrictive,</b> form-like, <b>search</b> interfaces. Recent {{studies have shown that}} it is possible to estimate aggregate query answers over such hidden web databases by issuing a small number of carefully designed search queries through the restrictive web interface. A problem with these existing work, however, is that they all assume the underlying database to be static, while most real-world web databases (e. g., Amazon, eBay) are frequently updated. In this paper, we study the novel problem of estimating/tracking aggregates over dynamic hidden web databases while adhering to the stringent query-cost limitation they enforce (e. g., at most 1, 000 search queries per day). Theoretical analysis and extensive real-world experiments demonstrate the effectiveness of our proposed algorithms and their superiority over baseline solutions (e. g., the repeated execution of algorithms designed for static web databases) ...|$|R
40|$|This paper {{introduces}} a general {{framework for the}} use of translation probabilities in cross-language information retrieval based on the notion that information retrieval fundamentally requires matching what the searcher means with what the author of a document meant. That perspective yields a computational formulation that provides a natural way of combining what have been known as query and document translation. Two well-recognized techniques are shown to be a special case of this model under <b>restrictive</b> assumptions. Cross-language <b>search</b> results are reported that are statistically indistinguishable from strong monolingual baselines for both French and Chinese documents...|$|R
40|$|The Szekeres metric is an inhomogeneous cosmological model {{without any}} symmetries. The {{standard}} Riemann-type coordinates {{can be transformed}} into spherical-type coordinates, but the metric is no longer diagonal, and the constant "radius" 2 -spheres, 2 -hyperboloids or 2 -planes {{are known to be}} "non-concentric". Since the transformation into spherical-type coordinates is "radius" dependent, we question whether these coordinates have the same orientation on each 2 -surface. To answer this question, we set up an orthonormal tetrad (ONT), and investigate its variation. We find that a relative rotation of the tetrad is generic, and it can increase systematically under conditions that are not very <b>restrictive.</b> We <b>search</b> for paths along which the tetrad is constant, and find they only exist under very restrictive conditions. In the process, we create a systematic method for defining an ONT with chosen properties from a given metric. Comment: 21 pages, no figures. V 2 has a few minor correction...|$|R
40|$|Post-translational {{modifications}} (PTMs) are {{of great}} biological importance. Most existing approaches perform a <b>restrictive</b> <b>search</b> {{that can only}} take into account a few types of PTMs and ignore all others. We describe an unrestrictive PTM search algorithm that searches {{for all types of}} PTMs at once in a blind mode, i. e., without knowing which PTMs exist in a sample. The blind PTM identification opens a possibility to study the extent and frequencies of different types of PTMs, still an open problem in proteomics. Using our new algorithm, we were able to construct a twodimensional PTM frequency matrix that reflects the number of MS/MS spectra in a sample for each putative PTM type and each amino acid. Application of this approach to a large IKKb dataset resulted in the largest set of PTMs reported for a single MS/MS sample so far. We demonstrate an excellent correlation between high values in the PTM frequency matrix and known PTMs thus validating our approach. We further argue that the PTM frequency matrix may reveal some still unknown modifications that warrant further experimental validation. ...|$|E
40|$|While {{identifying}} post-translational modifications (PTMs) {{is undoubtedly}} {{the next big}} step for proteomics, most MS/MS database search algorithms perform a <b>restrictive</b> <b>search</b> that can only take into account a few types of PTMs and ignore all others. We describe an unrestrictive PTM search algorithm (MS-Alignment) that searches {{for all types of}} PTMs at once in a blind mode, i. e., without knowing which PTMs exist in nature. The blind PTM identification opens a possibility to study the extent and frequency of different types of PTMs, still an open problem in proteomics. We use MS-Alignment to construct a two-dimensional PTM frequency matrix that reflects the number of MS/MS spectra in a sample for each putative PTM type and each amino acid. Application of this approach to lens proteins resulted in a largest set of PTMs reported in human crystallins so far. Our analysis of various MS/MS datasets implies that the biological phenomenon of modification is much more widespread than previously thought. We further argue that MS-Alignment reveals some still unknown types of modifications that warrant further experimental validation. ...|$|E
40|$|Background: Clinical {{practice}} guidelines (CPGs) become quickly outdated {{and require}} a periodic reassessment of evidence research {{to maintain their}} validity. However, there is little research about this topic. Our project will provide evidence {{for some of the}} most pressing questions in this field: 1) what is the average time for recommendations to become out of date?; 2) what is the comparative performance of two restricted search strategies to evaluate the need to update recommendations?; and 3) what is the feasibility of a more regular monitoring and updating strategy compared to usual practice?. In this protocol we will focus on questions one and two. Methods: The CPG Development Programme of the Spanish Ministry of Health developed 14 CPGs between 2008 and 2009. We will stratify guidelines by topic and by publication year, and include one CPG by strata. We will develop a strategy to assess the validity of CPG recommendations, which includes a baseline survey of clinical experts, an update of the original exhaustive literature searches, the identification of key references (reference that trigger a potential recommendation update), and the assessment of the potential changes in each recommendation. We will run two alternative search strategies to efficiently identify important new evidence: 1) PLUS search based in McMaster Premium LiteratUre Service (PLUS) database; and 2) a <b>Restrictive</b> <b>Search</b> (ReSe) based on the least number o...|$|E
40|$|Abstract- The core-based {{approach}} in multipoint communication broadens the solution space {{in terms of}} QoSefficiency of solutions in inter and intra-domain routing. In an earlier work [KH 03 a], we showed that the constrained cost minimization solutions in core-based approach proposed to date are <b>restrictive</b> in their <b>search</b> to a subrange of solutions, and we proposed SPAN, a generic framework to process in the extended solution space. In this paper, we study the core selection component of SPAN and propose two novel algorithms, SPAN/COST and SPAN/ADJUST. Our algorithms consistently outperform their counterparts proposed to date and can be considered pioneering in their optimization range of multiple metrics and processing in the extended solution space. 1...|$|R
40|$|A {{search for}} neutral heavy leptons (NHLs) has been {{performed}} using an instrumented decay channel at the NuTeV (E 815) experiment at Fermilab. The decay channel {{was composed of}} helium bags interspersed with drift chambers, and was {{used in conjunction with}} the NuTeV neutrino detector to search for NHL decays. Our search was sensitive to isosinglet type neutral leptons with masses between 0. 25 and 2. 0 GeV. No evidence of NHLs was found. The experiment took place during the 1996 – 1997 Fermilab fixed-target run. Collected data were examined for NHLs decaying into muonic final states (μμν, μ eν, μπ and μρ). This analysis places limits on the mixing of NHLs with standard light neutrinos at a level up to an order of magnitude more <b>restrictive</b> than previous <b>search</b> limits in this mass range...|$|R
40|$|A {{search for}} neutral heavy leptons (NHLs) has been {{performed}} using an instrumented decay channel at the NuTeV (E- 815) experiment at Fermilab. The decay channel {{was composed of}} helium bags interspersed with drift chambers, and was {{used in conjunction with}} the NuTeV neutrino detector to search for NHL decays. The data were examined for NHLs decaying into muonic final states (mu mu nu, mu e nu, mu pi, and mu rho); no evidence has been found for NHLs in the 0. 25 - 2. 0 GeV mass range. This analysis places limits on the mixing of NHLs with standard light neutrinos at a level up to an order of magnitude more <b>restrictive</b> than previous <b>search</b> limits in this mass range. Comment: 5 pages; 4 figures; submitted to Physical Review Letter...|$|R
40|$|The European Union {{and other}} {{countries}} on the Balkan route for migrants have recorded a large {{increase in the number}} of asylum seekers. In parallel with the increased number of refugees trying to enter the territory of the EU, measures for migration management have tightened, and the right to asylum at the level of the Member States has been interpreted more and more <b>restrictive.</b> <b>Search</b> for protection from persecution has become a reason for closing borders and disabling access to territory and asylum system. However, access to asylum system is the first step in the realization of the right to asylum as guaranteed by international, European and national law. In addition to allowing access to territory and asylum system, which implies an obligation of states to accept refugees in order to confirm the need for international protection in a fair and efficient procedure, the states are obliged to respect the principle of non-refoulement. The aim of this paper is to clarify the connection between providing access to asylum system and respect for the principle of non-refoulement. Analysis in the paper was done by legal-dogmatic method of research and interpretation of legal acts and other authorities, as well as of UNHCR relevant recommendations and documents. The assumption is that without the provision of access to territory and asylum system the principle of non-refoulement cannot be respected. Apart from the international refugee law and doctrinal interpretations, it derives from the practice of the European Court of Human Rights regarding the prohibition of torture or other inhuman treatment or punishment guaranteed by the Convention for the Protection of Human Rights and Fundamental Freedoms. The conclusion is that the states must take into account international and European standards regarding the protection of the principle of non-refoulement when considering the introduction of new measures to manage migration movements...|$|E
40|$|This {{research}} studies {{the feasibility of}} applying heuristic learning algorithm in artificial intelligence to address the traveling salesman problem. The research focuses on tour construction and seeks to overcome the weakness of traditional tour construction heuristics, which are of greedy and myopic nature. The advantage of tour construction heuristic is its simplicity in concept. However, the greedy and myopic nature of tour construction heuristic result in a sub-optimal solution, and the tour needs to be improved with much effort after it is built. The improvement is made using tour improvement heuristics, which improves tour by changing the tour configuration until a better solution is found. Traditional tour construction heuristics were not designed to modify {{the configuration of the}} tour, which is an important feature in tour improvement heuristics, during the tour construction process. This research investigates the application of a real time admissible heuristic learning algorithm that allows the tour configuration to change as the tour is built. The heuristic evaluation function of the algorithm considers both local and global estimated distance information. The search engine of the algorithm incorporates Delaunay triangulations of computational geometry as part of the search strategy. This helps to improve the search efficiency because computational geometry provides information about the geometric properties of nodes distributed in Euclidean plane, so that only promising nodes that are likely to be in the optimal tour are considered during the search process. A state space transformation process that defines state, state transition operator and state transition cost has been developed. The heuristic estimation of a state is computed using minimal spanning tree, and the set of relevant states for consideration at each state selection is identified through the application of Delaunay triangulations. Computational results show that the geometric distribution of nodes in the Euclidean plane influences the heuristic estimation, because it influences the computation of minimal spanning tree. Problems that exhibit distinct and well-separated clusters are advantageous to this approach because it is capable of producing good quality heuristic estimate. A <b>restrictive</b> <b>search</b> approach that could further reduce the search space of the heuristic learning algorithm has also been investigated. It is based on the characteristics of optimal tour in which nodes located on the convex hull are visited in the order in which they appear on the convex hull boundary. Using this characteristic together with the proximity concept of Voronoi diagram in computational geometry, some of the nodes that are unlikely to travel {{in the same direction as}} the optimal tour can be pruned. This approach could identify promising candidate edge set using only edges from one triangle selected from Delaunay triangulations, and the triangle is selected based on the direction the tour travels. The examples used in this research show that the saving in heuristic updates can be quite significant...|$|E
40|$|Advanced {{database}} systems face a {{great challenge}} raised {{by the emergence of}} massive, complex structural data in bioinformatics, chem-informatics, and many other applications. The most fundamental support needed in these applications is the efficient search of complex structured data. Since exact matching is often too <b>restrictive,</b> similarity <b>search</b> of complex structures becomes a vital operation that must be supported efficiently. In this paper, we investigate the issues of substructure similarity search using indexed features in graph databases. By transforming the edge relaxation ratio of a query graph into the maximum allowed missing features, our structural filtering algorithm, called Grafil, can filter many graphs without performing pairwise similarity computations. It is further shown that using either too few or too many features can result in poor filtering performance. Thus the challenge is to design an effective feature set selection strategy for filtering. By examining the effect of different feature selection mechanisms, we develop a multi-filter composition strategy, where each filter uses a distinct and complementary subset of the features. We identify the criteria to form effective feature sets for filtering, and demonstrate that combining features with similar size and selectivity can improve the filtering and search performance significantly. Moreover, the concept presented in Grafil can be applied to searching approximate non-consecutive sequences, trees, and other complicated structures as well. 1...|$|R
40|$|AbstractThis Letter {{presents}} {{a search for}} a hidden-beauty counterpart of the X(3872) in the mass ranges of 10. 05 – 10. 31 GeV and 10. 40 – 11. 00 GeV, in the channel Xb→π+π−ϒ(1 S) (→μ+μ−), using 16. 2 fb− 1 of s= 8 TeV pp collision data collected by the ATLAS detector at the LHC. No evidence for new narrow states is found, and upper limits are set on the product of the Xb cross section and branching fraction, relative to those of the ϒ(2 S), at the 95 % confidence level using the CLS approach. These limits range from 0. 8 % to 4. 0 %, depending on mass. For masses above 10. 1 GeV, the expected upper limits from this analysis are the most <b>restrictive</b> to date. <b>Searches</b> for production of the ϒ(13 DJ), ϒ(10860), and ϒ(11020) states also reveal no significant signals...|$|R
40|$|Çetin, Serkant Ali (Dogus Author) This Letter {{presents}} {{a search for}} a hidden-beauty counterpart of the X(3872) in the mass ranges of 10. 05 - 10. 31 GeV and 10. 40 - 11. 00 GeV, in the channel X-b -> pi(+) pi(-) gamma(1 S) (-> mu(+) mu(-)), using 16. 2 fb(- 1) of root s = 8 TeV pp collision data collected by the ATLAS detector at the LHC. No evidence for new narrow states is found, and upper limits are set on the product of the Xb cross section and branching fraction, relative to those of the gamma(25), at the 95 % confidence level using the CLs approach. These limits range from 0. 8 % to 4. 0 %, depending on mass. For masses above 10. 1 GeV, the expected upper limits from this analysis are the most <b>restrictive</b> to date. <b>Searches</b> for production of the gamma(1 (3) D(J)), gamma(10860), and gamma(11 020) states also reveal no significant signals...|$|R
40|$|In modern {{cartography}} of {{the late}} twentieth century the Internet offers an ideal plat-form for making communication with maps more feasible – on one hand. But on the other hand {{there has to be an}} understanding of the process and the methods of how to generate cartographic models and then how to communicate spatial data, which means geo-information accurately and efficiently. Due to this focus, cartography has to fulfil the obligation to achieve both – the creation of cartographic presentation forms for the new media and to accept the responsibility on understanding the deep relations within the whole cartographic communication process, which includes the user, the models in different cartographic media and the transmission. In this article, the importance of interactive and multimedia atlas information system as cartographic geo-communication platforms will be presented, where the user expands his knowledge by <b>restrictive</b> but flexible <b>searching</b> for spatial informa-tion. An atlas information system subdivides all functionalities into a cartographi-cally conceptional and structured order...|$|R
40|$|This Letter {{presents}} {{a search for}} a hidden-beauty counterpart of the X(3872) in the mass ranges 10. 05 [...] 10. 31 GeV and 10. 40 [...] 11. 00 GeV, in the channel X b →π + π − Υ(1 S) (→μ + μ −), using 16. 2 fb⁻¹ of √s= 8 TeV pp collision data collected by the ATLAS detector at the LHC. No evidence for new narrow states is found, and upper limits are set on the product of the X b cross section and branching fraction, relative to those of the Υ(2 S), at the 95 % confidence level using the CL S approach. These limits range from 0. 8 % to 4. 0 %, depending on mass. For masses above 10. 1 GeV, the expected upper limits from this analysis are the most <b>restrictive</b> to date. <b>Searches</b> for production of the Υ(1 3 D J), Υ(10860), and Υ(11020) states also reveal no significant signals...|$|R
40|$|This Letter {{presents}} {{a search for}} a hidden-beauty counterpart of the X(3872) in the mass ranges of 10. 05 - 10. 31 GeV and 10. 40 - 11. 00 GeV, in the channel X-b -> pi(+) pi(-) gamma(1 S) (-> mu(+) mu(-)), using 16. 2 fb(- 1) of root s = 8 TeV pp collision data collected by the ATLAS detector at the LHC. No evidence for new narrow states is found, and upper limits are set on the product of the Xb cross section and branching fraction, relative to those of the gamma(25), at the 95 % confidence level using the CLs approach. These limits range from 0. 8 % to 4. 0 %, depending on mass. For masses above 10. 1 GeV, the expected upper limits from this analysis are the most <b>restrictive</b> to date. <b>Searches</b> for production of the gamma(1 (3) D(J)), gamma(10860), and gamma(11 020) states also reveal no significant signals. ATLAS Collaboration, for complete list of authors see [URL]...|$|R
40|$|The core-based {{approach}} in multipoint communication enhances the solution space {{in terms of}} QoS-efficiency of solutions in inter and intra-domain routing. In an earlier work [KH 04], we showed that the constrained cost minimization solutions in core-based approach proposed to date are <b>restrictive</b> in their <b>search</b> to a subrange of solutions, and we proposed SPAN, a generic framework to process in our identified extended solution space. In this paper, we study the core selection component of SPAN and propose two novel algorithms, SPAN/COST and SPAN/ADJUST, which define the core-selection component of SPAN. SPAN/COST mainly optimizes the cost distances to be traveled between the source-core and core-receiver pairs on the multicast trees, while SPAN/ADJUST selects the cores based on the numbers of nodes they dominate and adjusting the set based on cost. Our algorithms consistently outperform their counterparts proposed to date and can be considered pioneering in their optimization range of multiple metrics and processing in the extended solution spacePublisher's Versio...|$|R
40|$|The future Internet is {{expected}} to support multicast applications with quality of service (QoS) requirements. To facilitate this, QoS multicast routing protocols are pivotal in enabling new receivers to join a multicast group. However, current routing protocols are either too <b>restrictive</b> in their <b>search</b> for a feasible path between a new receiver and the multicast tree, or burden the network with excessive overhead. We propose QMRP, a new Qos-aware Multicast Routing Protocol. QMRP achieves scalability by signi cantly reducing the communication overhead of constructing a multicast tree, yet it retains a high chance of success. This is achieved by switching between single-path routing and multiple-path routing according to the current network conditions. The high level design of QMRP makes it operable on top of any unicast routing algorithm in both intra-domain and inter-domain. Its responsiveness is improved by using a termination mechanism which detects the failure {{as well as the}} succe [...] ...|$|R
40|$|See {{paper for}} full list of authors –International audienceThis Letter {{presents}} {{a search for}} a hidden-beauty counterpart of the X(3872) in the mass ranges 10. 05 [...] 10. 31 GeV and 10. 40 [...] 11. 00 GeV, in the channel Xb→π+π−Υ(1 S) (→μ+μ−), using 16. 2 fb− 1 of s√= 8 TeV pp collision data collected by the ATLAS detector at the LHC. No evidence for new narrow states is found, and upper limits are set on the product of the Xb cross section and branching fraction, relative to those of the Υ(2 S), at the 95 % confidence level using the CLS approach. These limits range from 0. 8 % to 4. 0 %, depending on mass. For masses above 10. 1 GeV, the expected upper limits from this analysis are the most <b>restrictive</b> to date. <b>Searches</b> for production of the Υ(13 DJ), Υ(10860), and Υ(11020) states also reveal no significant signals...|$|R
5000|$|In 2015, {{responding}} to news articles {{in which the}} term orthorexia is applied to people who merely follow a non-mainstream theory of healthy eating, Bratman specified the following: [...] "A theory may be conventional or unconventional, extreme or lax, sensible or totally wacky, but, regardless of the details, followers of the theory do not necessarily have orthorexia. They are simply adherents of a dietary theory. The term 'orthorexia' only applies when an eating disorder develops around that theory." [...] Bratman elsewhere clarifies that with a few exceptions, most common theories of healthy eating are followed safely {{by the majority of}} their adherents; however, [...] "for some people, going down the path of a <b>restrictive</b> diet in <b>search</b> of health may escalate into dietary perfectionism." [...] Karin Kratina, PhD, writing for the National Eating Disorders Association, summarizes this process as follows: [...] "Eventually food choices become so restrictive, in both variety and calories, that health suffers - an ironic twist for a person so completely dedicated to healthy eating." ...|$|R
40|$|A {{search for}} neutral heavy leptons (NHLs) has been {{performed}} using an instrumented decay channel at the NuTeV (E- 815) experiment at Fermilab. The decay channel {{was composed of}} helium bags interspersed with drift chambers, and was {{used in conjunction with}} the NuTeV neutrino detector to search for NHL decays. The data were examined for NHLs decaying into muonic final states (µµν, µeν, µπ, and µρ); no evidence has been found for NHLs in the 0. 25 - 2. 0 GeV mass range. This analysis places limits on the mixing of NHLs with standard light neutrinos at a level up to an order of magnitude more <b>restrictive</b> than previous <b>search</b> limits in this mass range. PACS numbers: 13. 15. +g, 13. 35. Hb, 14. 60. Pq 1 Various extensions [1, 2] to the Standard Model predict neutral heavy leptons (NHLs) which can mix with the standard light neutrinos. In these extensions, the NHLs are weak isosinglets that do not couple directly to the Z and W bosons, but can decay via mixing with the Standard Model neutrinos. Figure 1 shows one possible set of tree-level diagrams for NHL production and decay...|$|R
40|$|Search for the Xb {{and other}} hidden-beauty {{states in the}} pi+pi−Υ(1 S) channel at ATLAS The ATLAS Collaboration This Letter {{presents}} {{a search for a}} hidden-beauty counterpart of the X(3872) in the mass ranges 10. 05 – 10. 31 GeV and 10. 40 – 11. 00 GeV, in the channel Xb → pi+pi−Υ(1 S) (→ µ+µ−), using 16. 2 fb− 1 of√ s = 8 TeV pp collision data collected by the ATLAS detector at the LHC. No evidence for new narrow states is found, and upper limits are set on the product of the Xb cross section and branching fraction, relative to those of the Υ(2 S), at the 95 % confidence level using the CLS approach. These limits range from 0. 8 % to 4. 0 %, depending on mass. For masses above 10. 1 GeV, the expected upper limits from this analysis are the most <b>restrictive</b> to date. <b>Searches</b> for production of the Υ(13 DJ), Υ(10860), and Υ(11020) states also reveal no significant signals. c © 2014 CERN {{for the benefit of the}} ATLAS Collaboration. Reproduction of this article or parts of it is allowed as specified in the CC-BY- 3. 0 license. a...|$|R
40|$|International audiencePURPOSE: To {{analyze the}} {{literature}} comparing two different policies for episotomy practice: liberal versus <b>restrictive</b> use. To <b>search</b> and discuss specific indications for episiotomy. METHODS: The Medline base was analyzed from 1970 to 2005. The articles where selected {{by using the}} key word episiotomy and selective or restrictive and routine or liberal. Every potential indications was crossed with episotomy. RESULTS: A policy implying a liberal practice of episiotomy is not better compared to a restrictive policy. The evidence-based medical literature favors avoiding routine episiotomy in low risk deliveries. Data are quite scarce concerning the different specific indications for episiotomy, and finally we can retain only one specific indication which is the short perineum when {{the distance between the}} fourchette and the center of the anus is less than 3 cm. Nevertheless, in order to improve delivery conditions obstetricians can advisably use episiotomy in accordance with their clinical assessment. CONCLUSION: There is no evidence in the literature favoring a liberal policy over a restrictive policy for the use of episiotomy, both in terms of fetal (Grade C) and maternal (Grade A) indications. A number of obstetrical situations considered as at risk do not systematically indicate an episiotomy. There are however circumstances in which a pertinent and prudent clinical assessment will lead the obstetrician to use an episiotomy...|$|R
30|$|Previous {{studies on}} the {{etiology}} of ADHD are mostly based on structural or functional neuroimaging research of group level (ADHD vs. control) differences. Some informative features extracted are blood oxygenation level-dependent (BOLD) signals from {{functional magnetic resonance imaging}} (fMRI) data [2], wavelet synchronization likelihoods extracted from electroencephalography (EEG) data [3], rolandic spikes from EEG data [4], brain volume measure extracted from magnetic resonance imaging (MRI) data [5]. The pursuit of neuroanatomical biomarkers has a great potential to facilitate new discriminative methods that are etiologically informed and validated by neuropsychological theories. However, due to high cost of neuroimaging data acquisition, most current ADHD studies are based on relatively small sample sizes, which reduce the statistical power needed to validate meaningful discriminative variable from {{a very large number of}} features extracted from structural MRI [6]. A limited sample size with equivalent number of features raises new challenges to traditional machine learning algorithms, such as logistic regression or support vector machines (SVM), as they tend to overfit and lack a generalization power when training on a dataset containing the number of features far larger than the sample size (p ≫ n problem). In previous work, some models either use hundreds of features as an input or exhaustively search on a preselected smaller subset of features. SVM is mostly favored [7] and some variant of feedforward neural networks [8] is also used. We believe that those methods are either susceptible to overfitting or too <b>restrictive</b> in the <b>search</b> space. The interpretation of the final models is very difficult to validate by existing neuropsychological theories.|$|R
40|$|This Letter {{presents}} {{a search for}} a hidden-beauty counterpart of the X(3872) in the mass ranges 10. 05 [...] 10. 31 GeV and 10. 40 [...] 11. 00 GeV, in the channel Xb→π+π−Υ(1 S) (→μ+μ−), using 16. 2 fb− 1 of s√= 8 TeV pp collision data collected by the ATLAS detector at the LHC. No evidence for new narrow states is found, and upper limits are set on the product of the Xb cross section and branching fraction, relative to those of the Υ(2 S), at the 95 % confidence level using the CLS approach. These limits range from 0. 8 % to 4. 0 %, depending on mass. For masses above 10. 1 GeV, the expected upper limits from this analysis are the most <b>restrictive</b> to date. <b>Searches</b> for production of the Υ(13 DJ), Υ(10860), and Υ(11020) states also reveal no significant signals. We acknowledge the support of ANPCyT, Argentina; YerPhI, Armenia; ARC, Australia; BMWFW and FWF, Austria; ANAS, Azerbaijan; SSTC, Belarus; CNPq and FAPESP, Brazil; NSERC, NRC and CFI, Canada; CERN; CONICYT, Chile; CAS, MOST and NSFC, China; COLCIENCIAS, Colombia; MSMT CR, MPO CR and VSC CR, Czech Republic; DNRF, DNSRC and Lundbeck Foundation, Denmark; EPLANET, ERC and NSRF, European Union; IN 2 P 3 -CNRS, CEA-DSM/IRFU, France; GNSF, Georgia; BMBF, DFG, HGF, MPG and AvH Foundation, Germany; GSRT and NSRF, Greece; ISF, MINERVA, GIF, I-CORE and Benoziyo Center, Israel; INFN, Italy; MEXT and JSPS, Japan; CNRST, Morocco; FOM and NWO, Netherlands; BRF and RCN, Norway; MNiSW and NCN, Poland; GRICES and FCT, Portugal; MNE/IFA, Romania; MES of Russia and ROSATOM, Russian Federation; JINR; MSTD, Serbia; MSSR, Slovakia; ARRS and MIZS, Slovenia; DST/NRF, South Africa; MINECO, Spain; SRC and Wallenberg Foundation, Sweden; SER, SNSF and Cantons of Bern and Geneva, Switzerland; NSC, Taiwan; TAEK, Turkey; STFC, the Royal Society and Leverhulme Trust, United Kingdom; DOE and NSF, United States of America...|$|R
40|$|Abstract Background Quantification {{of protein}} {{expression}} {{by means of}} mass spectrometry (MS) has been introduced in various proteomics studies. In particular, two label-free quantification methods, such as spectral counting and spectra feature analysis have been extensively investigated {{in a wide variety}} of proteomic studies. The cornerstone of both methods is peptide identification based on a proteomic database search and subsequent estimation of peptide retention time. However, they often suffer from <b>restrictive</b> database <b>search</b> and inaccurate estimation of the liquid chromatography (LC) retention time. Furthermore, conventional peptide identification methods based on the spectral library search algorithms such as SEQUEST or SpectraST have been found to provide neither the best match nor high-scored matches. Lastly, these methods are limited in the sense that target peptides cannot be identified unless they have been previously generated and stored into the database or spectral libraries. To overcome these limitations, we propose a novel method, namely Quantification method based on Finding the Identical Spectral set for a Homogenous peptide (Q-FISH) to estimate the peptide's abundance from its tandem mass spectrometry (MS/MS) spectra through the direct comparison of experimental spectra. Intuitively, our Q-FISH method compares all possible pairs of experimental spectra in order to identify both known and novel proteins, significantly enhancing identification accuracy by grouping replicated spectra from the same peptide targets. Results We applied Q-FISH to Nano-LC-MS/MS data obtained from human hepatocellular carcinoma (HCC) and normal liver tissue samples to identify differentially expressed peptides between the normal and disease samples. For a total of 44, 318 spectra obtained through MS/MS analysis, Q-FISH yielded 14, 747 clusters. Among these, 5, 777 clusters were identified only in the HCC sample, 6, 648 clusters only in the normal tissue sample, and 2, 323 clusters both in the HCC and normal tissue samples. While it will be interesting to investigate peptide clusters only found from one sample, further examined spectral clusters identified both in the HCC and normal samples since our goal is to identify and assess differentially expressed peptides quantitatively. The next step was to perform a beta-binomial test to isolate differentially expressed peptides between the HCC and normal tissue samples. This test resulted in 84 peptides with significantly differential spectral counts between the HCC and normal tissue samples. We independently identified 50 and 95 peptides by SEQUEST, of which 24 and 56 peptides, respectively, were found to be known biomarkers for the human liver cancer. Comparing Q-FISH and SEQUEST results, we found 22 of the differentially expressed 84 peptides by Q-FISH were also identified by SEQUEST. Remarkably, of these 22 peptides discovered both by Q-FISH and SEQUEST, 13 peptides are known for human liver cancer and the remaining 9 peptides are known to be associated with other cancers. Conclusions We proposed a novel statistical method, Q-FISH, for accurately identifying protein species and simultaneously quantifying the expression levels of identified peptides from mass spectrometry data. Q-FISH analysis on human HCC and liver tissue samples identified many protein biomarkers that are highly relevant to HCC. Q-FISH can be a useful tool both for peptide identification and quantification on mass spectrometry data analysis. It may also prove to be more effective in discovering novel protein biomarkers than SEQUEST and other standard methods. </p...|$|R
