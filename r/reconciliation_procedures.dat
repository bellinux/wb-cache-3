24|44|Public
50|$|While each act {{has its own}} {{legislative}} history and effect on the tax code, the JGTRRA amplified and accelerated aspects of the EGTRRA. Since 2003, the two acts have often been spoken of together, {{especially in terms of}} analyzing their effect on the U.S. economy and population and in discussing their political ramifications. Both laws were passed using controversial Congressional <b>reconciliation</b> <b>procedures.</b>|$|E
50|$|Congress {{passed its}} budget {{resolution}} in May 2015. It {{was the first}} budget resolution successfully passed by Congress in over five years. Congressional budget resolutions are non-binding and largely symbolic, in that the actual spending levels are specified in much greater detail in the appropriations bills to be passed later in the year. The bill envisioned increasing military spending and decreasing social programs, {{with the goal of}} having a balanced budget by 2025. The passage of a budget resolution opened the way for budget <b>reconciliation</b> <b>procedures</b> to be used to repeal the Affordable Care Act on a simple majority vote, although a presidential veto of such legislation was expected.|$|E
40|$|The {{reconciliation}} {{of a system}} of time series is known in the literature as the statistical process of adjusting preliminary values of the series to satisfy both temporal and contemporaneous constraints. In this paper we propose new <b>reconciliation</b> <b>procedures</b> based on the Growth Rates Preservation (GRP) principle, which explicitly preserves the period-to-period growth rates of the preliminary series. A non-linear constrained minimization problem is solved through a Newton’s optimization method, which exploits the analytical gradient and Hessian of the GRP objective function. We apply these procedures to two real-life applications and compare them with the state-of-the-art <b>reconciliation</b> <b>procedures.</b> The results show that simultaneous and two-step <b>reconciliation</b> <b>procedures</b> based on the GRP criterion are worthy candidates in terms of both quality of results and computational time, even for large systems with many constraints...|$|E
5000|$|In {{case of a}} {{conflict}} between a constituent country and the Kingdom, Article 12 of the Charter prescribes an administrative <b>reconciliation</b> <b>procedure.</b> This was often deemed a democratic deficit of the Kingdom, leading {{to the adoption of}} an amendment to the Charter, which entered into force on 10 October 2010. The new Article 12a specifies that in addition to the administrative <b>reconciliation</b> <b>procedure,</b> [...] "by Kingdom Act measures shall be made allowing for the arbitration of certain conflicts, as specified by Kingdom Act, between the Kingdom and the countries." [...] The imperative formulation was the result of an amendment in the Chamber of Representatives by special delegates Evelyna Wever-Croes and J.E. Thijsen of Aruba; the original formulation was [...] "by Kingdom Act measures can be made".|$|R
50|$|Frumin began {{receiving}} significant {{media coverage}} and notice in his usually quiet role during the 2010 healthcare reform debate for {{the critical role}} he plays in determining {{the validity of the}} <b>reconciliation</b> <b>procedure</b> being employed to apply changes desired by the House to portions of the Patient Protection and Affordable Care Act passed by both houses.|$|R
40|$|We {{present the}} case for {{allowing}} independent updates on replicated databases. In autonomous, heterogeneous, or large scale systems, using two-phase commit for updates may be infeasible. Instead, we propose that a site may perform updates independently. Sites that are available can receive these updates immediately. But sites that are unavailable, or otherwise {{do not participate in}} the update transaction, receive these updates later through propagation, rather than preventing the execution of the update transaction until sufficient sites can participate. Two or more sites come to agreement using an <b>reconciliation</b> <b>procedure</b> that uses reception vectors to determine how much of the history log should be transferred from one site to another. We also consider what events can initiate a <b>reconciliation</b> <b>procedure.</b> 1 Introduction Many recent papers have studied the applicability of replicated databases, and many strategies have been developed to deal with updates in such an environment [1, 6, 7 [...] ...|$|R
40|$|International audienceThis paper {{present a}} method to {{identify}} and estimate gross errors for linear dynamic systems. This method {{is applied to the}} doubly fed induction generator (DFIG) of a wind turbine. Gross error detection is used not only to improve the estimation accuracy of data <b>reconciliation</b> <b>procedures</b> but also to identify instrumentation problems by using statistical tests...|$|E
40|$|A {{mechanism}} for handling inconsistencies created when replicated objects are updated during disconnected operations on a mobile host is described. By exploiting the encapsulation property of objects, analysis of object behaviours is employed to derive procedures &quot; so con icting concurrent updates made on di erent copies of objects may be rendered mutually consistent. The process of generating and applying <b>reconciliation</b> <b>procedures</b> is discussed...|$|E
40|$|For physical-layer security, key <b>reconciliation</b> <b>procedures</b> {{are needed}} to correct key {{differences}} that can arise {{as a consequence of}} independent noise at the two ends of a reciprocal link. We assume either a random link in a mobile environment or use reconfigurable antenna elements to randomize the channel, such that it allows for frequent key generation. We apply LDPC codes to reconcile the keys on both sides. In here, we derive the LLRs taking into account the underlying quantization...|$|E
40|$|We give an {{achievable}} {{secret key}} rate of a binary modulated continuous variable {{quantum key distribution}} schemes in the collective attack scenario considering quantum channels that impose arbitrary noise on the exchanged signals. Bob performs homodyne measurements on the received states and the two honest parties employ a reverse <b>reconciliation</b> <b>procedure</b> in the classical post-processing step of the protocol. Comment: 16 pages, 2 figure...|$|R
40|$|We {{estimate}} a {{lower bound}} to the secret key rate of a binary modulated continuous variable quantum key distribution scheme. We consider the collective attack scenario with quantum channels that impose arbitrary noise on the exchanged signals. The analysis {{is done in the}} infinite key limit. Bob performs ideal homodyne measurements on the received states and the two honest parties employ a reverse <b>reconciliation</b> <b>procedure</b> in the classical postprocessing step of the protocol...|$|R
40|$|The {{reconciliation}} of systems of time series subject to both temporal and contemporaneous constraints {{can be solved}} {{in such a way}} that the temporal profiles of the original series be preserved ‘at best’ (the movement preservation principle). A new feasible simultaneous <b>reconciliation</b> <b>procedure</b> is presented, which exploits the sparsity of the linear system to be solved. A two-step reconciliation strategy might be more suitable in the case of large systems. We compare the results of the simultaneous and two-step approaches for two data sets from real life...|$|R
40|$|Correspondence {{issued by}} the Government Accountability Office with an {{abstract}} that begins "We performed the agreed-upon procedures requested by the Secretary of the Senate related to receipt and disbursement processing and related procedures applicable to the Office of Public Records Revolving Fund's (the Fund's) fiscal years 2003 - 2005. In summary, the procedures we agreed to perform involved inspecting supporting documentation for Fund-related receipt and disbursement activities processed through the Office of Public Records (OPR) and Senate Disbursing Office (SDO) and <b>reconciliation</b> <b>procedures</b> performed by OPR. ...|$|E
40|$|The budget {{reconciliation}} {{process is}} an optional procedure that operates {{as an adjunct}} to the budget resolution process established by the Congressional Budget Act of 1974. The chief purpose of the reconciliation process is to enhance Congress's ability to change current law in order to bring revenue, spending, and debt-limit levels into conformity with the policies of the annual budget resolution. This report identifies and briefly summarizes the 20 budget reconciliation measures enacted into law during the period covering 1980, when <b>reconciliation</b> <b>procedures</b> first were used by both chambers, through 2010, the last year of the 111 th Congress...|$|E
40|$|Key <b>reconciliation</b> <b>procedures</b> {{are needed}} to correct key {{differences}} that can arise {{as a consequence of}} independent noise at the two ends of a reciprocal link. We assume a line-of-sight channel and use reconfigurable antenna elements to randomize it, such that it allows for key generation. The Linde-Buzo-Gray algorithm is employed to quantize the complex channel transfer characteristic, and adaptive guard bands, symmetric to the quantization thresholds, are further constructed. To limit the number of key errors, we ensure that only the points that fall outside the guard band interval are accepted for key generation. The steps for constructing the guard bands are presented...|$|E
40|$|We {{investigate}} {{the performance of}} a continuous-variable quantum key distribution scheme in a practical setting. More specifically, we take a nonideal error <b>reconciliation</b> <b>procedure</b> into account. The quantum channel connecting the two honest parties is assumed to be lossy but noiseless. Secret key rates are given for the case that the measurement outcomes are postselected or a reverse reconciliation scheme is applied. The reverse reconciliation scheme loses its initial advantage in the practical setting. If one combines postselection with reverse reconciliation, however, much of this advantage can be recovered...|$|R
40|$|The {{reconciliation}} of flow rates of fluids entraining and reacting with solid particles is considered in this article. It is shown that, {{in case the}} amount of solid particles is not high, the reconciliation problem can be addressed formally as a rectification that includes additional components and solved using the techniques proposed by earlier research [Crowe, C. M., Garcia Campos, Y. A, Hrymak, A. AIChE J. 1983, 29, 881 - 888]. Furthermore, it is shown that {{the introduction of the}} interaction law between the fluid and the solid phase can improve the overall reliability of the <b>reconciliation</b> <b>procedure...</b>|$|R
40|$|In this paper, {{we present}} methods for {{supporting}} autonomous updates in replicated databases. Autonomous updates {{are of particular}} importance to applications that cannot tolerate the delay and vulnerability due to synchronous update methods (2 PC). We separate the notion of replication consistency, meaning that all copies have the same value and re ect the same update transactions, from behavior consistency, meaning that transaction execution re ects all integrity constraints. The method proposed in this paper supports independent updates during network partitioning, and achieves a consistent nal database state on recovery of partitions that re ects all actions that were executed during network partitioning. To this purpose, we describe a <b>reconciliation</b> <b>procedure</b> that applies all actions to each updated data item in {{the order in which}} they were originally performed, possibly independently; therefore, reconciliation may require the undo and redo of actions. We formally de ne the properties that need to hold for our approach towork, and we prove that our <b>reconciliation</b> <b>procedure</b> respects these properties. Our approach is incremental, as it can be applied to any sequence of partitionings and recoveries; reconciliation occurs whenever possible or at the user's desire. However, we trade consistent behavior for update availability: in general, {{there is no guarantee that}} the execution will re ect all global consistency constraints. Localization techniques for constraints can be used to support consistent behavior for This work was performed in the context of the Fauve-project and started while some of the author...|$|R
40|$|We propose new {{simultaneous}} and two-step {{procedures for}} reconciling systems of time series subject to temporal and contemporaneous constraints {{according to a}} growth rates preservation (GRP) principle. The techniques exploit the analytic gradient and Hessian of the GRP objective function, {{making full use of}} all the derivative information at disposal. We apply the new GRP procedures to two systems of economic series, and compare the results with those of <b>reconciliation</b> <b>procedures</b> based on the proportional first differences (PFD) principle, widely used by data-producing agencies. Our experiments show that (1) the nonlinear GRP problem can be efficiently solved through an interior-point optimization algorithm, and (2) GRP-based procedures preserve better the growth rates than PFD solutions, especially for series with high temporal discrepancy and high volatility...|$|E
40|$|This chapter aims {{to examine}} how far {{collective}} rituals instigated at a socio-political level are able to modify the social attitudes that prevail in populations following conflict, violations of human rights, or massacres. This examination relies on two studies conducted {{in the framework of}} the Truth and Reconciliation procedure that was developed in the post-genocide era in Rwanda, under the name of "Gacaca" (pronounced gatchatcha). First, we briefly recall what a Truth and Reconciliation procedure represents, and describe the situation facing Rwanda after the 1994 genocide. Next, we discuss the expected effects of Truth and <b>Reconciliation</b> <b>procedures</b> and examine a theoretical model we have adopted in this regard. Finally, we consider the findings from studies designed to test this model...|$|E
40|$|Most of {{the data}} {{obtained}} by statistical agencies have to be adjusted, corrected or somehow processed by statisticians in order to arrive at useful, consistent and publishable values. When temporally and contemporaneously aggregated series are known, temporal (e. g., between quarterly and annual data) and contemporaneous (between the quarterly aggregate and the sum of its component series) discrepancies can be eliminated using various <b>reconciliation</b> <b>procedures.</b> In this paper we consider (i) an extension of the univariate benchmarking approach by Denton (1971), founded on a well known movement preservation principle, and (ii) a data-based benchmarking procedure (Guerrero and Nieto, 1999) which exploits the autoregressive features of the preliminary series to be adjusted. In order to evaluate their performance in practical situations, both procedures are applied to simulated and real world data. ...|$|E
40|$|We {{show that}} {{replacing}} the usual sifting {{step of the}} standard quantum-key-distribution protocol BB 84 by a one-way reverse <b>reconciliation</b> <b>procedure</b> increases its robustness against photon-number-splitting (PNS) attacks {{to the level of}} the SARG 04 protocol while keeping the raw key-rate of BB 84. This protocol, which uses the same state and detection than BB 84, is the m= 4 member of a protocol-family using m polarization states which we introduce here. We show that the robustness of these protocols against PNS attacks increases exponentially with m, and that the effective keyrate of optimized weak coherent pulses decreases with the transmission T like T^{ 1 + 1 /(m- 2) }...|$|R
40|$|International audienceWe {{designed}} high-efficiency error correcting codes allowing {{to extract}} an errorless secret key in a continuous-variable {{quantum key distribution}} protocol using a Gaussian modulation of coherent states and a homodyne detection. These codes are available {{for a wide range}} of signal-to-noise ratios on an AWGN channel with a binary modulation and can be combined with a multidimensional reconciliation method proven secure against arbitrary collective attacks. This improved <b>reconciliation</b> <b>procedure</b> considerably extends the secure range of a continuous-variable quantum key distribution with a Gaussian modulation, giving a secret key rate of about 10 ^{- 3 } bit per pulse at a distance of 120 km for reasonable physical parameters...|$|R
40|$|In this work, we {{investigate}} a physical-layer key reconciliation protocol for a reciprocal, {{flat fading channel}} between two legitimate users. We consider the scenario when the n bits of the secret key are measured independently by Alice and Bob without a transmission over the channel. Due to reciprocity, the generated keys are identical except for noise at both ends. We assume Gaussian noise and ignore non-ideal behavior of circuitry and alike. Redundancy information required to reconciliate the key is transmitted from one legitimate user to the other. LDPC codes are employed for the <b>reconciliation</b> <b>procedure.</b> The main focus of this work lies in designing the code structure through density evolution for a multi-edge-type description...|$|R
40|$|A {{mechanism}} for handling inconsistencies created when replicated objects are updated during disconnected operations on a mobile host is described. By exploiting the encapsulation property of objects, analysis of object behaviours is employed to derive "reconciliation procedures" so conflicting concurrent updates made on different copies of objects may be rendered mutually consistent. The process of generating and applying <b>reconciliation</b> <b>procedures</b> is discussed. Keywords: Mobile Computing, Disconnected Operation, Reconciliation, Nested Transactions. 1 Introduction When mobile machines {{are forced to}} operate in disconnected mode (i. e. without radio or other connection to a base network [8, 9]) {{it is necessary to}} make copies of the objects required by the mobile host (MH). This permits processing to continue on the MH despite disconnection. Unfortunately, the availability of multiple copies of an object introduces the potential for inconsistency. In connected systems, this is managed by [...] ...|$|E
40|$|Some {{recent work}} on {{enhancing}} concurrency in database systems {{has focused on}} using semantic information [Wei 91]. One problem with {{this approach is that}} application designers must be sufficiently knowledgeable to determine, 'a priori, how the application semantics interact with the transaction model. Our approach exploits an object-oriented world and places this load on the transaction system which must be able to determine by static analysis how to maintain database consistency. We adopt an optimistic approach whereby each transaction is given its own copy (version) of all of the objects it needs to execute to completion without interruption from other processes. Within the context of concurrency, this paper uses the information from static analysis to develop algorithms which allow the compiler to generate <b>reconciliation</b> <b>procedures</b> automatically from the initial transaction specification. 1 Introduction Multiversioning for the purpose of enhancing concurrency and reliability is not [...] ...|$|E
40|$|Correspondence {{issued by}} the Government Accountability Office with an {{abstract}} that begins "GAO performed the agreed-upon procedures Congress requested related to receipt and disbursement processing and related procedures applicable to the Office of Public Records Revolving Fund (the Fund) for fiscal years 2006 and 2007. In summary, the procedures we agreed with Congress to perform related to supporting documentation for Fund-related receipt and disbursement activities processed through the Office of Public Records (OPR) and Senate Disbursing Office (SDO) and <b>reconciliation</b> <b>procedures</b> performed by OPR. We conducted our work in accordance with U. S. generally accepted government auditing standards, which incorporate the attestation standards established by the American Institute of Certified Public Accountants. These standards also provide guidance on performing and reporting {{on the results of}} agreed-upon procedures. By specifying the procedures we agreed to perform, OPR, SDO, and the Office of the Secretary of the Senate were responsible for ensuring that the procedures were sufficient to meet Congressional purposes, and we make no representation in that respect. The enclosure contains the agreed-upon procedures we performed and the results we obtained. ...|$|E
40|$|We {{designed}} high-efficiency error correcting codes allowing {{to extract}} an errorless secret key in a continuous-variable {{quantum key distribution}} protocol using a Gaussian modulation of coherent states and a homodyne detection. These codes are available {{for a wide range}} of signal-to-noise ratios on an AWGN channel with a binary modulation and can be combined with a multidimensional reconciliation method proven secure against arbitrary collective attacks. This improved <b>reconciliation</b> <b>procedure</b> considerably extends the secure range of a continuous-variable quantum key distribution with a Gaussian modulation, giving a secret key rate of about 10 ^{- 3 } bit per pulse at a distance of 120 km for reasonable physical parameters. Comment: 8 pages, 5 figures, 5 table...|$|R
40|$|The {{problem of}} {{hermeneutic}} {{is an important}} component in Averroes’s thought and it is a key to resolve the contradiction between religion and philosophy, and to reject the claim that philosophers are pagan, as well as to understand religious texts and manuscripts. According to him, hermeneutic is to move from the exoteric meaning to esoteric one and to conceive internal target of the text. His hermeneutical method is based on the <b>reconciliation</b> <b>procedure.</b> He tries to reconcile reason and revealing, or religion and philosophy. Therefore, Averroes introduces certain rules for hermeneutic and asserts that as philosophers benefit of rational method, they have priority and allowance to hermeneutic, but they are not permitted to issue it...|$|R
40|$|The authors {{present the}} case for {{allowing}} independent updates on replicated databases. In autonomous, heterogeneous, or large scale systems, using two-phase commit for updates may be infeasible. Instead, the authors propose that a site may perform updates independently. Sites that are available can receive these updates immediately. But sites that are unavailable, or otherwise {{do not participate in}} the update transaction receive these updates later through propagation, rather than preventing the execution of the update transaction until sufficient sites can participate. Two or more sites come to agreement using a <b>reconciliation</b> <b>procedure</b> that uses reception vectors to determine how much of the history log should be transferred from one site to another. They also consider what events can initiate a reconciliation procedur...|$|R
40|$|The Listening Experience Database (LED) is {{a project}} that gathers {{documented}} evidence of listening to music across cultural and historical contexts. Its underlying information system relies on the principles and practices of Linked Data, including a knowledge base that is itself a linked dataset structured according to common vocabularies for media and bibliographies such as Bibo and the Music Ontology. The data management workflow fully supports crowd-sourced input and incorporates data reuse from various sources right {{from the point of}} data collection, including the British National Bibliography dataset. The LED system gives contributors and moderators the tools to enhance these data by modelling divisions of bibliographc entries, providing information beyond the BNB schema, or aligning with other datasets by simple data <b>reconciliation</b> <b>procedures.</b> Vocabularies for music genre and instrument classes are also crowd-sourced on top of a baseline taxonomy from the DBpedia dataset. The Web frontend uses the Drupal content management system to encapsulate listening experiences into digital objects that incorporate textual information with references to literary, musical and other classes of related entities...|$|E
40|$|Abstract – This paper {{describes}} an architectural model to facilitate multiversion {{objects that are}} explicitly designed to enhance concurrency. The reader {{should be aware that}} version management has been used in the object literature in several ways, most commonly dealing with design issues. Our goal here is related to concurrency control and reliability, so care must be taken to ensure the reader is not misled by this overloading of terminology found in the literature. Within the context of concurrency the key aspects addressed by this paper are: 1) An architectural model is developed to support multiversioning that provides the well-known ACID transaction properties; 2) An optimistic concurrency control algorithm that functions on this architecture is described and demonstrated to be correct with respect to a correctness criterion; 3) The algorithm is enhanced to examine the history of past versions with the goal of inserting a committing transaction at a time earlier in the sequence when it would have been valid if other, later transactions had not been completed before this one attempted to commit; and 4) Based on static analysis information, other algorithms are developed to optimize the compiler in order to generate <b>reconciliation</b> <b>procedures</b> automatically from the initial transaction specification...|$|E
40|$|With limited {{resources}} and reduced funding for Naval forces, {{there is a}} need to standardize accounting ashore for all afloat activities. The purpose of this thesis was to review the framework for standardization of inventory reporting afloat under one stores (inventory) accounting system, referred to as the Material Financial Control System-Retail (MFCS-Retail). Additional analysis was conducted on general funds obligational reporting for afloat Operating Targets (OPTARS) and the conversion to the Standard Accounting and Reporting System, Field Level (STARS-FL) system. Empirical research was conducted at DFAS Operating Locations in San Diego, California and Norfolk, Virginia to review existing stores accounting and general funds management procedures. Additionally, financial <b>reconciliation</b> <b>procedures</b> were reviewed for inventory and financial accounting, with the goal of using artificial intelligence to reduce unmatched receipts and expenditures. Emphasis was placed on areas that could be streamlined and automated to provide timeliness in reporting, while reducing workload afloat. The major finding of this research was that standardizing accounting for inventories afloat under MFCS-Retail and STARS-FL for OPTAR management allows for streamlining detailed inventory management and financial reporting ashore. A major benefit is the reduction of workload afloat through the standardization of re ortin across the fleet. NAU. S. Navy (U. S. N.) autho...|$|E
40|$|Transaction Reordering in Replicated Databases F. Pedone, R. Guerraoui and A. Schiper This paper {{presents}} a fault-tolerant lazy replication protocol that ensures 1 -copy serializability {{at a relatively}} low cost. Unlike eager replication approaches, our protocol enables local transaction execution and {{does not lead to}} any deadlock situation. Compared to previous lazy replication approaches, we significantly reduce the abort rate of transactions and we do not require any <b>reconciliation</b> <b>procedure.</b> Our protocol first executes transactions locally, then broadcasts a transaction certification message to all replica managers, and finally employs a certification procedure to ensure 1 -copy serializability. Certification messages are broadcast using a non-blocking atomic broadcast primitive, which alleviates the need for a more expensive non-blocking atomic commitment algorithm. The certification procedure uses a reordering technique to reduce the probability of transaction aborts...|$|R
40|$|International audienceThe paper {{establishes}} a rigorous probabilistic {{framework for the}} reconciliation of apparently conflicting data from various physical and chemical measurements related to the key biological variables of alcoholic fermentation: the ethanol and the residual sugar concentrations. The analysis is carried out on a database consisting of 15 beer fermentation experiments, for which off-line determinations of ethanol concentration, fermentable sugar concentration, wort density and refractive index are available, as well as on-line records of evolved CO 2. The basic reconciliation method uses mass balance and monotonicity constraints derived from the biological knowledge of the fermentation process. In order to provide interpolated values and rate estimates, smoothness requirements are added. The <b>reconciliation</b> <b>procedure</b> gives more reliable estimates than any given measurement, detects outliers, helps fixing problems in the experimental setting and is also applicable on line...|$|R
40|$|International audienceIn {{the steel}} industry, the {{determination}} of the control system set-points of batch processes is a common problem. It consists in adjusting the set-points in order to reach the given product specifications thanks to a process model. Small changes in operating conditions may impact final product quality. This is particularly true for the Basic Oxygen Furnace (BOF) where the information collected during a specific batch serves to adjust the set-points of the next batch. For being able to control that type of process, measurements must be made coherent and it may be convenient to use data <b>reconciliation</b> <b>procedure.</b> The proposed paper describes a method allowing simultaneous data reconciliation and model parameter estimation. Parameter estimation results can either be used to update the process model or to detect abnormal parameter variations due, e. g. to fouling, corrosion, degradation of parts of the process...|$|R
