73|292|Public
5000|$|... s2: greater scatter in {{the data}} around the <b>regression</b> <b>surface</b> leads to proportionately more {{variance}} in the coefficient estimates ...|$|E
50|$|In statistics, an {{additive}} model (AM) is a nonparametric regression method. It was suggested by Jerome H. Friedman and Werner Stuetzle (1981) {{and is an}} essential part of the ACE algorithm. The AM uses a one-dimensional smoother to build a restricted class of nonparametric regression models. Because of this, it is less affected by the curse of dimensionality than e.g. a p-dimensional smoother. Furthermore, the AM is more flexible than a standard linear model, while being more interpretable than a general <b>regression</b> <b>surface</b> at the cost of approximation errors. Problems with AM include model selection, overfitting, and multicollinearity.|$|E
50|$|There exist other unique {{properties}} of the least absolute deviations line. In {{the case of a}} set of (x,y) data, the least absolute deviations line will always pass through {{at least two of the}} data points, unless there are multiple solutions. If multiple solutions exist, then the region of valid least absolute deviations solutions will be bounded by at least two lines, each of which passes through at least two data points. More generally, if there are k regressors (including the constant), then at least one optimal <b>regression</b> <b>surface</b> will pass through k of the data points.|$|E
40|$|We {{consider}} {{the problem of}} detecting jump location curves of <b>regression</b> <b>surfaces.</b> In the literature, most existing methods detect jumps in <b>regression</b> <b>surfaces</b> based on estimation of either the first-order or the second-order derivatives of <b>regression</b> <b>surfaces.</b> Methods based on the first-order derivatives are usually better in removing noise, whereas methods based on the second-order derivatives are often superior in localization of the detected jumps. There {{are a number of}} existing jump detection procedures using both the first-order and second-order derivatives. But they often use the two types of derivatives in separate steps, and estimation of the derivatives is usually based on intuition. In this paper, we suggest a new procedure for jump detection in <b>regression</b> <b>surfaces,</b> which combines the two types of derivatives in a single criterion. Estimation of the derivatives is based on local quadratic kernel smoothing. Theoretical justifications and numerical studies show that it works well in applications...|$|R
40|$|Signficant {{changes in}} the RBFInterpolant. Users need to update their code Added RBF <b>regression</b> <b>surfaces</b> Added version {{information}} in the module. pySOT. version gives the version of the current pySOT installation The Gutmann strategy has been temporarily removed due to the RBF redesign, but will be added back soon Check out test_rbf. py to see {{how to use the}} new RB...|$|R
40|$|We {{consider}} {{the problem of}} locating jumps in <b>regression</b> <b>surfaces.</b> A jump detection algorithm is suggested based on local least squares estimation. This method requires O(Nk) computations, where N is the sample size and k is the window width of the neighborhood. This property {{makes it possible to}} handle large data sets. The conditions imposed on the jump location curves, the jump surfaces and the noise are mild. 1 Introduction In computer image analysis, a very important problem involves detecting the edges of objects, or equivalently, detecting the discontinuities of the underlying "intensity function" (the brightness of each point in the image is expressed by this function). In meteorology and oceanography, the equi-temperature surfaces of the high sky and the deep ocean are usually discontinuous. From a statistical view point, all of these problems could be regarded as applications of estimation of two dimensional (2 -D) jump <b>regression</b> <b>surfaces</b> (JRS). The {{purpose of this paper is}} t [...] ...|$|R
40|$|<b>Regression</b> <b>surface</b> {{analysis}} was used to examine relations between the family environment and measures of academic achievement at different levels of school-related attitudes for 800 l l-year-old children from different Australian social groups. The sample included lower social-status families from th...|$|E
40|$|Our {{goal is to}} {{accurately}} estimate the error in any prediction of a regression model. We propose a probabilistic regression framework for basis function regression models, which includes widely used kernel methods such as support vector machines and nonlinear ridge regression. The framework outputs a point specific estimate of {{the probability that the}} true <b>regression</b> <b>surface</b> lies between two user specified values, denoted by y 1 and y 2. More formally, given any y 2 > y 1, we estimate the Pr(y 1 f(x)), where y is a true <b>regression</b> <b>surface,</b> x is the input, and f(x) is the basis function model. Thus the framework encompasses the less general standard error bar approach used in regression...|$|E
40|$|Minimax L_ 2 {{risks for}} high-dimensional nonparametric {{regression}} are derived under two sparsity assumptions: (1) the true <b>regression</b> <b>surface</b> is a sparse function that depends only on d=O(n) important predictors among {{a list of}} p predictors, with p=o(n); (2) the true <b>regression</b> <b>surface</b> depends on O(n) predictors but is an additive function where each additive component is sparse but may contain two or more interacting predictors and may have a smoothness level different from other components. For either modeling assumption, a practicable extension of the widely used Bayesian Gaussian process regression method is shown to adaptively attain the optimal minimax rate (up to n terms) asymptotically as both n,p→∞ with p=o(n). Comment: Published at [URL] in the Annals of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
40|$|The aim of {{this work}} {{is to get the}} best {{detailed}} knowledge about hardness distribution in first 60 mm below the surface of backing roll. To this end, a method for obtaining multi-dimensional polynomial regression was developed and then a computer program for its processing was written. Way of finding suitable <b>regression</b> <b>surfaces</b> and their subsequent interpretation, is a pivotal part of this work...|$|R
40|$|A local {{smoothing}} {{procedure is}} proposed for detecting jump location curves of <b>regression</b> <b>surfaces.</b> This procedure simpli es the computation of some existing jump detectors in the statistical literature. It also generalizes the Sobel edge detector {{in the image}} processing literature such that more observations {{can be used to}} smooth away random noise in the data. The problem to evaluate the performance of jump detectors is discussed and a new measurement of jump detection performance is suggested...|$|R
40|$|The paper {{explores the}} {{potential}} {{application of the}} filtered backprojection algorithm of Computerized Tomography to statistical smoothing of multivariate densities and <b>regression</b> <b>surfaces.</b> The key attraction {{of this approach is}} that the implementation only makes use of automated one-dimensional smoothers to get an automated smooth of a multi-dimensional target function. In addition the structure is well suited to parallel processing. Various practical details are discussed. The method is illustrated with some two-dimensional examples. AsYIIlPtoticmeane~ror characteristics. of them(i!thod are shown to match those of more familarmultivariate kernel- or spline-type smoothing techniques. A Statistical ApplicationofFiltered Backprojectio...|$|R
40|$|Abstract—Nonparametric {{estimation}} {{capabilities of}} fuzzy systems in stochastic environments are analyzed in this paper. By using ideas from sieve estimation, increasing sequences of fuzzy rule-based systems, capable of consistently estimating regression surfaces in different settings, are constructed. Results include least squares learning of a mapping perturbed by additive random noise in a static-regression context and least squares learning of a <b>regression</b> <b>surface</b> from data {{generated by a}} bounded stationary ergodic random process. L 1 estimation is also studied, and the consistency of fuzzy rule-based sieve estimators for the L 1 -optimal <b>regression</b> <b>surface</b> is shown, thus giving additional theoretical support to the robust filtering capabilities of fuzzy systems and their adequacy for modeling, prediction and control of systems affected by impulsive noise. Index terms—fuzzy systems, nonparametric sieve estimation, consistency, least squares learning, robust learning 1 I. INTRODUCTION AND MOTIV...|$|E
40|$|A {{new method}} for nonparametric {{multiple}} regression is presented. The procedure models the <b>regression</b> <b>surface</b> as a sum of general- smooth functions of linear combinations of {{the predictor variables}} in an iterative manner. It is more general than standard stepwise and stagewise regression procedures, {{does not require the}} definition of a metric in the predictor space, and lends itself to graphi-cal interpretation...|$|E
40|$|In a {{regression}} problem, one {{is given a}} d- dimensional random vector X, the components of which are called predictor variables, and a random variable, Y, called response. A <b>regression</b> <b>surface</b> describes a general relationship between variables X and Y. One nonparametric regression technique that has been successfully applied to highdimensional data is projection pursuit regression (PPR). In this method, the <b>regression</b> <b>surface</b> is approximated by a sum of empirically determined univariate functions of linear combinations of the predictors. Projection pursuit learning (PPL) proposed by Hwang et al. formulates PPR using a two-layer feedforward neural network. One of the main differences between PPR and PPL is that the smoothers in PPR are nonparametric, whereas those in PPL are based on Hermite functions of some predefined highest order R. While the convergence property of PPR is already known, that for PPL has not been thoroughly studied. In this paper, we demonstrate that PPL networks i [...] ...|$|E
40|$|We {{investigate}} {{the relationship between}} focal surfaces and surfaces at a constant distance {{from the edge of}} <b>regression</b> on a <b>surface.</b> We show that focal surfaces F 1 and F 2 of the surface M can be obtained by means of some special surfaces at a constant distance from the edge of <b>regression</b> on the <b>surface</b> M...|$|R
40|$|Economic {{conditions}} such as convexity, homogeneity, homotheticity, and monotonicity are all important assumptions or consequences of assumptions of economic functionals to be estimated. Recent research has seen {{a renewed interest in}} imposing constraints in nonparametric regression. We survey the available methods in the literature, discuss the challenges that present themselves when empirically implementing these methods and extend an existing method to handle general nonlinear constraints. A heuristic discussion on the empirical implementation for methods that use sequential quadratic programming is provided for the reader and simulated and empirical evidence on the distinction between constrained and unconstrained nonparametric <b>regression</b> <b>surfaces</b> is covered. identification, concavity, Hessian, constraint weighted bootstrapping, earnings function...|$|R
40|$|International audienceApparent molar volumes Vφand {{apparent}} molar heat capacitiesCp, φ {{were determined}} for aqueous solutions of magnesium chloride (MgCl 2) and cadmium chloride (CdCl 2) at molalities m = (0. 01 to 1. 0) mol · kg − 1, at T = 278. 15 K to T = 393. 15 K, {{and at the}} pressure 0. 35 MPa. Apparent molar volumes were calculated from densities obtained using a vibrating-tube densimeter. Apparent molar heat capacities were obtained using a twin fixed-cell, power-compensation, differential-output, temperature-scanning calorimeter. These results were used to create the surfaces (Vφ, m, T) and (Cp, φ,m, T) which were fitted by regression. Differences in ion–ion interactions for MgCl 2 (aq) and CdCl 2 (aq) are observed by comparison of the <b>regression</b> <b>surfaces...</b>|$|R
30|$|Locally {{weighted}} regression yi=g(xi) + εi, where i= 1,…, n index of observations, g is the regression function and εi are residual errors, provides an estimate g(x) of each <b>regression</b> <b>surface</b> at any value x in the d-dimensional {{space of the}} independent variables. Correlations between observations of the response variable yi and the vector with the observations d-tuples xi of d predictor variables are identified. Local regression provides an estimation of function g(x) near x=x 0 according to its value in a particular parametric class. This estimation could be achieved by adapting a <b>regression</b> <b>surface</b> to the data points within a neighborhood of the point x 0, which is bounded by a smoothing parameter: span. The span determines the percentage of data that are considered for each local fit and hence the smoothness of the estimated surface is influenced [27]. The span ranges from 0 (wavy curve) to 1 (smooth curve). Each local regression uses either a first or a second degree polynomial that it is specified by {{the value of the}} “degree” parameter of the method (degree = 1 or degree = 2).|$|E
40|$|In $SF_ 6 $-filled {{electrical}} equipment, {{the electric}} field distribution is kept rather uniform. However in practice, the electric {{field in the}} gas gap is distorted by nonuniformities. For this reason, the inhomogeneous field breakdown in $SF_ 6 $ has been extensively studied by various researchers and the breakdown characteristics of compressed $SF_ 6 $ have been reported. Obtaining experimental data under all conditions is not possible. Therefore, an attempt {{has been made in}} the present work to apply an artificial neural network (ANN) to obtain such data. The projection pursuit learning network (PPLN) has been used as the ANN model. Breakdown data for four different voltage waveforms were used to train the network for $SF_ 6 $ pressures of 1 - 5 bar and rod diameters of 1 - 12 mm in a rod-plane geometry. The ANN was first trained with these data so as to obtain a smooth <b>regression</b> <b>surface</b> interpolating the training data. The <b>regression</b> <b>surface</b> thus obtained, was thereafter used to generate the breakdown and corona inception voltages with in the range of gas pressures and nonuniformities studied, where no data is available...|$|E
40|$|Among {{those who}} use {{multiple}} regression analysis or its offshoots, the dominant method of modeling an interaction effect of two independent variables on a dependent variable is to include a product variable in a linear estimation equation. Textbook writers and researchers almost always interpret the coefficient for the product variable as describing both how the first independent variable influences {{the effect of the}} second independent variable, and vice versa. As a result, writers often claim that interaction effects are “symmetrical. ” In this paper I distinguish between the <b>regression</b> <b>surface</b> produced by estimating a product-variable model of an interaction effect, and the causal mechanisms that produce the <b>regression</b> <b>surface.</b> I discuss four types of interaction effects, and show that the meaning of the product variable’s coefficient differs across types. I also show that one should use the same estimation equation for different types of interaction effects. These considerations imply that the usual interpretation of the product variable’s coefficient is almost always wrong, and that researchers need strong theory or knowledge before they can interpret the results of a product variable model as giving information about the causal mechanisms that constitute an interaction effect. 2 Interpreting Product-Variable Models of Interaction Effects 1 Many interesting findings in the social sciences involve “interaction ” o...|$|E
40|$|We {{propose a}} simple and {{convenient}} method for analyzing nonstationary spatiotemporal data. To account for spatial variability, we write the mean function as a locally-weighted mixture of linear regressions. To capture temporal variation, we allow the <b>regression</b> <b>surfaces</b> to change over time. A linear state-space framework is proposed for the model, which allows us to explore temporal factors such as trends, cyclical patterns, and autoregressive components. The main feature of the proposed method is its computational simplicity: using the Kalman filtering and smoothing algorithms, we can obtain posterior and predictive distributions in closed form. This allows quick implementation of the model, and provides full probabilistic inference for the parameters, interpolations and forecasts. To illustrate the method, we analyze two large datasets: tropical rainfall levels and Atlantic ocean temperatures...|$|R
40|$|In this contribution, multivariate {{regression}} {{was applied}} to surface channel rock and borehole geochemical data from the world-class Sari Gunay epithermal gold deposit, in northwest Iran, to model subsurface mineralization for further drilling. Multiple, factorial, polynomial and response <b>surface</b> <b>regression</b> models were applied to the geochemical data sets from a training mineralized area to evaluate the accuracy of these models using separate geochemical data from a test area. Geochemical data of 31 elements in surface channel rock samples were used as independent variables, and three parameters namely average grade, sum and productivity in individual 25 m by 25 m grid cells, obtained by kriging of borehole data, were used as dependent variables. All the multivariate regression models revealed high determination coefficients for three parameters, among which the response <b>surface</b> <b>regression</b> model yielded the highest values. The response <b>surface</b> <b>regression</b> yielded the best result, followed by the multiple regression, in modeling the geochemical data from the test area. Therefore, {{the result of the}} response <b>surface</b> <b>regression</b> was used to model subsurface gold mineralization at the Sari Gunay gold deposit in order to design additional drillings...|$|R
40|$|We {{describe}} a system, written in Lisp-Stat, for designing and conducting {{experiments in the}} perception of three-dimensional dynamic scatterplots. The system includes tools for designing stimuli that systematically vary aspects of the display (e. g., the presence of graphical elements such as axes, <b>regression</b> <b>surfaces,</b> and residuals) and for conveniently generating datasets that can incorporate features such as nonlinearity and outliers. The system is designed to present stimuli to subjects automatically, to interact with the subjects, and to collect information about their responses and response-latencies. It is hoped that this software will contribute to the investigation of the properties of threedimensional statistical graphs, and, ultimately, to their improvement. 1. Introduction Three-dimensional scatterplots have been promoted as geometrical tools for understanding statistical concepts (e. g., by Monette, 1990), and as tools for analyzing data (e. g., Cook and Weisberg, 1989, 1994 [...] ...|$|R
40|$|We {{formulate}} regression as {{maximizing the}} minimum probability (#) {{that the true}} regression function is within of the regression model. Our framework starts by posing regression as a binary classification problem, such that {{a solution to this}} single classification problem directly solves the original regression problem. Minimax probability machine classification (Lanckriet et al., 2002 a) is used to solve the binary classification problem, resulting in a direct bound on the minimum probability # that the true regression function is within of the regression model. This minimax probability machine regression (MPMR) model assumes only that the mean and covariance matrix of the distribution that generated the regression data are known; no further assumptions on conditional distributions are required. Theory is formulated for determining when estimates of mean and covariance are accurate, thus implying robust estimates of the probability bound #. Conditions under which the MPMR <b>regression</b> <b>surface</b> is identical to a standard least squares <b>regression</b> <b>surface</b> are given, allowing direct generalization bounds to be easily calculated for any least squares regression model (which was previously possible only under very specific, often unrealistic, distributional assumptions). We further generalize these theoretical bounds to any superposition of basis functions regression model. Experimental evidence is given supporting these theoretical results...|$|E
40|$|Abstract-This paper {{describes}} a memory-based network that provides estimates of continuous variables and converges {{to the underlying}} (linear or nonlinear) <b>regression</b> <b>surface.</b> This gen-eral regression neural network (GRNN) is a one-pass learning algorithm with a highly parallel structure. Even with sparse data in a multidimensional measurement space, the algorithm provides smooth transitions from one observed value to an-other. The algorithmic form {{can be used for}} any regression problem in which an assumption of linearity is not justified. The parallel network form should find use in applications such as learning the dynamics of a plant model for prediction or control. I...|$|E
40|$|In the {{graduation}} thesis we compared geoidal heights interpolated from Slovenian national geoid model SLOG 2000, preliminary calculations of new model SLOG 2010 and global geopotential model EGM 2008, {{on the southern}} slope of Krvavec. Also compared are geoidal heights calculated from GPS measurements and trigonometric levelling. In the thesis we also present theoretical basics of height systems, geoid and different methods of geoid determination. We described the specific geoid models used and calculated deviations of those from »measured« values, then evaluated them by quality. On the basis of measurements we calculated the local geoid plane as a <b>regression</b> <b>surface...</b>|$|E
40|$|Geographically Weighted Regression (GWR) is {{a method}} of spatial {{statistical}} analysis used to explore geographical differences in the effect {{of one or more}} predictor variables upon a response variable. However, as a form of local analysis, it does not scale well to (especially) large data sets because of the repeated processes of fitting and then comparing multiple <b>regression</b> <b>surfaces.</b> A solution is to make use of developing grid infrastructures, such as that provided by the National Grid Service (NGS) in the UK, treating GWR as an “embarrassing parallel” problem and building on existing software platforms to provide a bridge between an open source implementation of GWR (in R) and the grid system. To demonstrate the approach, we apply it to a case study of participation in Higher Education, using GWR to detect spatial variation in social, cultural and demographic indicators of participation...|$|R
40|$|The {{computation}} {{of grain}} burning <b>surface</b> <b>regression</b> plays {{a very important}} role in the internal ballistic performance evaluation of solid rocket motor, however, the traditional methods such as geometry-based one could not handle the self-intersection and characteristic geometric element disappearing problems. This paper presents an effective and efficient framework to simulate 3 D grain burning <b>surface</b> <b>regression</b> with level set method which is combined with Fast Marching technique to constrain the calculation area only around the burning surface. At last, a typical grain example is given by our framework to verify our method's effectiveness and efficiency. © (2012) Trans Tech Publications. The computation of grain burning <b>surface</b> <b>regression</b> plays {{a very important role}} in the internal ballistic performance evaluation of solid rocket motor, however, the traditional methods such as geometry-based one could not handle the self-intersection and characteristic geometric element disappearing problems. This paper presents an effective and efficient framework to simulate 3 D grain burning <b>surface</b> <b>regression</b> with level set method which is combined with Fast Marching technique to constrain the calculation area only around the burning surface. At last, a typical grain example is given by our framework to verify our method's effectiveness and efficiency. © (2012) Trans Tech Publications...|$|R
40|$|We {{suggest a}} three-stage {{procedure}} to recover discontinuous <b>regression</b> <b>surfaces</b> when noisy data are present. In the first stage, jump candidate points are detected using a jump detection criterion. A local principal component line is then fitted through these {{points in a}} neighborhood of a design point. This line provides a first-order approximation to the true jump location curve in that neighborhood. In the third stage, observations {{on the same side}} of the line as the given point are combined using a weighted average procedure to fit the surface at that point. If there are no jump candidate points in the neighborhood, then all observations in that neighborhood are used in the surface fitting. If, however, the center of the neighborhood is on a jump location curve, only those observations on one side of the line are used. Thus blurring is automatically avoided around the jump locations. This methodology requires O(N(k) 2) computation, where N is the sample size and k is the w [...] ...|$|R
40|$|This paper {{proposes a}} {{constrained}} nonparametric {{method of estimating}} an input distance function. A regression function is estimated via kernel methods without functional form assumptions. To guarantee that the estimated input distance function satisfies its properties, monotonicity constraints are imposed on the <b>regression</b> <b>surface</b> via the constraint weighted bootstrapping method borrowed from statistics literature. The first, second, and cross partial analytical derivatives of the estimated input distance function are derived, and thus the elasticities measuring input substitutability can be computed from them. The method is then applied to a cross-section of 3, 249 Norwegian timber producers...|$|E
40|$|The paper {{presents}} an econometric application of generalized additive Logit regression models (GALRMs) for brand choice. Our semi-parametric models are flexible and robust {{extensions of the}} Logit model. The GALRMs are fit to binary response data by maximizing a penalized log likelihood or a penalized log partial-likelihood. The GAMs allow us to build a <b>regression</b> <b>surface</b> as a sum of lower-dimensional nonparametric terms circumventing the curse of dimensionality: the slow convergence of an estimator to the true value in high dimensions. Four GALRMs are compared with a Logit model for brand choice and the best model is selected using various model selection criteria...|$|E
40|$|Abstract. Many {{applications}} in computer vision require robust linear regression on photogrammetrically reconstructed point clouds. Due to the modeling process from perspective images {{the uncertainty of}} an object point depends heavily on its location in object space w. r. t. the cameras. Standard algorithms for robust regression are based on distance measures from the <b>regression</b> <b>surface</b> to the points, but these distances are biased by varying uncertainties. In this paper {{a description of the}} local object point precision is given and the Mahalanobis distance to a plane is derived to allow unbiased regression. Illustrative examples are presented to demonstrate the effect of the statistically motivated distance measure. ...|$|E
40|$|In kernel-based {{regression}} learning, optimizing each kernel individually {{is useful}} when the data density, curvature of <b>regression</b> <b>surfaces</b> (or decision boundaries) or magnitude of output noise varies spatially. Previous work has suggested gradient descent techniques or complex statistical hypothesis methods for local kernel shaping, typically requiring some amount of manual tuning of meta parameters. We introduce a Bayesian formulation of nonparametric regression that, {{with the help}} of variational approximations, results in an EM-like algorithm for simultaneous estimation of regression and kernel parameters. The algorithm is computationally efficient, requires no sampling, automatically rejects outliers and has only one prior to be specified. It can be used for nonparametric regression with local polynomials or as a novel method to achieve nonstationary regression with Gaussian processes. Our methods are particularly useful for learning control, where reliable estimation of local tangent planes is essential for adaptive controllers and reinforcement learning. We evaluate our methods on several synthetic data sets and on an actual robot which learns a task-level control law. ...|$|R
50|$|After the Cretaceous sea {{had retreated}} (marine <b>regression),</b> the <b>surface</b> {{of the land}} was shaped by {{weathering}} influences and watercourses, of which the Elbe made the deepest incision. Later the Lusatian granodiorite was uplifted over the 600 metre thick sandstone slab along the Lusatian Fault and pushed it downwards until it fractured. This northern boundary of the sandstone deposit lies roughly along the line Pillnitz-Hohnstein-Hinterhermsdorf-Krásná Lípa (Schönlinde).|$|R
40|$|A novel {{method is}} {{proposed}} {{in this paper}} for automatic acquisition of three-dimensional models of unknown objects by an active vision system, in which the vision sensor is to be moved from one viewpoint to the next around the target to obtain its complete model. In each step, sensing parameters are determined automatically for incrementally building the 3 D target models. The method is developed by analyzing the target’s trend surface, which is the regional feature of a surface for describing the global tendency of change. Whilst previous approaches to trend analysis are usually focused on generating polynomial equations for interpreting <b>regression</b> <b>surfaces</b> in three dimensions, this paper proposes a new mathematical model for predicting the unknown area of the object surface. A uniform surface model is established by analyzing the surface curvatures. Furthermore, a criterion is defined to determine the exploration direction and an algorithm is developed for determining {{the parameters of the}} next view. Implementation of the method is carried out to validate the proposed method...|$|R
