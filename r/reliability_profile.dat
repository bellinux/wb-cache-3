14|63|Public
40|$|Methods {{validation}} {{is mandatory}} {{in order to}} assess the fitness of purpose of the developed analytical method. Of core importance {{at the end of the}} validation is the evaluation of the reliability of the individual results that will be generated during the routine application of the method. Regulatory guidelines provide a general framework to assess the validity of a method, but none address the issue of results reliability. In this study, a Bayesian approach is proposed to address this concern. Results reliability is defined here as "the probability (π) of an analytical method to provide analytical results (X) within predefined acceptance limits (±λ) around their reference or conventional true concentration values (μ(T)) over a defined concentration range and under given environmental and operating conditions. " By providing the minimum reliability probability (π(min)) needed for the subsequent routine application of the method, as well as specifications or acceptance limits (±λ), the proposed Bayesian approach provides the effective probability of obtaining reliable future analytical results over the whole concentration range investigated. This is summarised in a single graph: the <b>reliability</b> <b>profile.</b> This Bayesian <b>reliability</b> <b>profile</b> is also compared to two frequentist approaches, the first one derived from the work of Dewé et al. [W. Dewé, B. Govaerts, B. Boulanger, E. Rozet, P. Chiap, Ph. Hubert, Chemometr. Intell. Lab. Syst. 85 (2007) 262 - 268] and the second proposed by Govaerts et al. [B. Govaerts, W. Dewé, M. Maumy, B. Boulanger, Qual. Reliab. Eng. Int. 24 (2008) 667 - 680]. Furthermore, to illustrate the applicability of the Bayesian <b>reliability</b> <b>profile,</b> this approach is also applied here to a bioanalytical method dedicated to the determination of ketoglutaric acid (KG) and hydroxymethylfurfural (HMF) in human plasma by SPE-HPLC-UV...|$|E
40|$|Time-variant {{reliability}} analysis of RC highway bridges strengthened with carbon fibre reinforced polymer CFRP laminates under four possible competing damage modes (concrete crushing, steel rupture after yielding, CFRP rupture and FRP plate debonding) and three degradation factors is analyzed {{in terms of}} reliability index β using FORM. The first degradation factor is chloride-attack corrosion which induces reduction in steel area and concrete cover cracking at characteristic key times (corrosion initiation, severe surface cover cracking). The second degradation factor considered is fatigue which leads to damage in concrete and steel rebar. Interaction between corrosion and fatigue crack growth in steel reinforcing bars is implemented. The third degradation phenomenon is the CFRP properties deterioration due to aging. Considering these three degradation factors, the time-dependent flexural <b>reliability</b> <b>profile</b> of a typical simple 15  m-span intermediate girder of a RC highway bridge is constructed under various traffic volumes and under different corrosion environments. The bridge design options follow AASHTO-LRFD specifications. Results of the study {{have shown that the}} reliability is very sensitive to factors governing the corrosion. Concrete damage due to fatigue slightly affects <b>reliability</b> <b>profile</b> of non-strengthened section, while service life after strengthening is strongly related to fatigue damage in concrete...|$|E
40|$|Semiconductor {{transient}} faults (soft errors) {{are becoming}} an increasingly critical threat to reliable software execution. With {{the advent of}} the billion transistor chip era, it is impractical to protect the entire hardware. As a result, it is crucial that the tradeoffs between reliability and performance be made at the architecture design stage. To achieve this goal, researchers need a framework to evaluate software vulnerability to transient errors at a high level. This paper describes Sim-SODA (SOftware Dependability Analysis), a unified framework for estimating microprocessor reliability in the presence of soft errors at the architectural level. Compared with previous studies, Sim-SODA covers more hardware structures and provides fine-grained reliability analysis. We present a detailed architectural <b>reliability</b> <b>profile</b> of an Alpha- 21264 -like superscalar microprocessor running workloads from various application domains. Additionally, we obtain program vulnerability phases and correlate them with microprocessor performance metrics. 1...|$|E
40|$|In this paper, the {{lifetime}} performance of existing steel bridges under preventive and essential maintenance actions is analyzed. Performance {{is measured by}} the reliability index (resulting from structural analysis) and the condition index (resulting from visual inspections). The analysis of performance using these two indicators allows the incorporation of the results of visual inspections in the assessment of the safety of a structure, resulting in a more accurate prediction of the need to maintenance. Due to the uncertainty in the deterioration process and time of application and effect of maintenance actions, a probabilistic framework is used. The reduction in structural performance due to corrosion of the steel profiles is simulated using an extension of the model proposed in 1998 by Frangopol [1], considering all parameters defining the profiles the condition and <b>reliability</b> <b>profiles</b> as random variables. The effect of the maintenance actions on the condition and <b>reliability</b> <b>profiles</b> is defined in terms of the improvement in performance and the reduction or suppression of deterioration for a period time after application of maintenance. Monte-Carlo simulation is used to compute the probabilistic properties of the condition, reliability, and life-cycle cost profiles associated with each maintenance strategy. The model described is applied to steel bridges located in Colorado. The results show the importance, in terms of life-cycle cost, of considering both preventive and essential maintenance in a rational lifetime maintenance strategy. U. S. National Science Foundation - CMS 9912525, CMS 0217290. Fundação para a Ciência e a Tecnologia (FCT) ...|$|R
40|$|A novel {{methodology}} {{is presented}} for probabilistic fatigue life prediction of welded joints in orthotropic bridge steel decks. Monitoring data {{were used to}} specify time-series model parameters for the main drivers of fatigue damage in such structures, namely pavement temperatures and heavy traffic intensities, which influence the stress range distributions at critical locations. Polynomial regression models were developed to quantify the relationship between fatigue loading, derived using S-N principles from strain measurements at welded joints, with pavement temperatures and heavy traffic counts. The different models were integrated within a fatigue reliability framework, in which the uncertainties arising from material properties and fatigue damage at failure were modelled via random variables. A Monte Carlo scheme was then deployed to predict S-N fatigue damage using the fatigue loading regression models and simulated time-series of heavy traffic and pavement temperatures. Thus, fatigue <b>reliability</b> <b>profiles</b> were generated, which account for different scenarios in terms of future changes in traffic and pavement temperature. The proposed methodology was illustrated considering actual monitoring outcomes from the Great Belt Bridge (Denmark) with <b>reliability</b> <b>profiles</b> developed for both ‘baseline’ and ‘adverse’ scenarios {{in the context of}} asset integrity management. The combined effect of higher temperature and heavy traffic levels was shown to result in considerable reductions in fatigue reliability, with a commonly used threshold being reached up to 40 years earlier compared to the baseline ‘no change’ scenario. However, this reduction was not uniform for all the fatigue details considered, emphasizing the importance of monitoring different locations, based on a thorough understanding of the fatigue behaviour of the orthotropic steel deck...|$|R
40|$|Reliability {{evaluation}} of systems composed of non-identical multi-state components is a challenging task {{due to the}} highly combinatorial nature of the problem. The outcome {{is a lack of}} understanding regarding the reliability behavior of a system when components with different magnitudes of failure probabilities are considered, especially when extra components are added. The present work addresses this problem by presenting a simplified approach to analyze variations on <b>reliability</b> <b>profiles</b> of series and parallel systems composed of non-identical three-state components. In addition, this work presents an optimization model for allocation of non-identical components under financial and physical restrictions. The model performed satisfactorily in terms of robustness when different magnitudes of failure probabilities were tested...|$|R
40|$|Abstract. Our {{research}} involves {{application of}} methods well-studied in virtual multiagent systems (MAS) but less well-understood in physical multi-robot systems (MRS). This paper investigates {{the relationship between}} performance results collected in parallel simulated (multiagent) and physical (multi-robot) environments. Our hypothesis is that some performance metrics established in simulation will predict results in the physical environment. Experiments show that some performance metrics can predict actual values, because data collected in both simulated and physical settings fall within the same numeric range. Other performance metrics predict relative values, because patterns found in data collected in the simulated setting are similar to patterns found in the physical setting. The long term aim {{is to establish a}} <b>reliability</b> <b>profile</b> for comparing different types of performance metrics in simulated versus physical environments. The work presented here demonstrates a first step, in which experiments were conducted and assessed within one parallel simulatedphysical setup. ...|$|E
40|$|Abstract: In {{this paper}} {{a variety of}} battery {{configuration}} topologies for electrified vehicles are investigated with regard to reliability and expected lifetime along {{with the possibility of}} applying active fault detection to provide early warnings for the driver. Different configurations are investigated ranging from a simple single serial string of battery cells providing only the lowest level of fault tolerance, to a highly elaborate and still practically relevant triple string configuration providing fault detection and reconfiguration possibilities as well as repair. All configurations are analyzed with regard to the associated <b>reliability</b> <b>profile</b> assuming non-ageing cell failure model. A novel method for active early fault detection is presented based on encoding faults into a parametric dynamic cell model, where parameters are continuously estimated under the influence of an auxiliary test signal designed to optimize parametric sensitivity. Finally reliability profiles for all investigated configurations are compared mutually and with standard requirements on the basis of mean time to failure statistics...|$|E
40|$|Faculty of Engineering and the Built Environment; Master of Science in Engineering; Research ReportThe primary {{objective}} {{of this study was}} to develop a methodology for evaluating how the <b>reliability</b> <b>profile</b> of the typical South African Platinum concentrator plant is affected by firstly the size of the primary milling units incorporated in the circuit and secondly by the way that the primary milling units are configured. A methodology, together with a set of general expressions is presented which considers the Platinum concentrator as a stochastic process where the behaviour of the primary mill is a direct measure of the failure pattern of the overall concentrator. The reliability, availability and maintainability (RAM) of the primary mill, and hence the overall concentrator, is then determined by a combination of three different Markov models where each Markov model is used to evaluate and measure a separate set of reliability parameters. This approach effectively overcomes the computational complexity associated with large Markov models. The results of two case studies used to validate the methodology do indicate that the reliability, availability and maintainability profiles of large single stream Platinum concentrators could be fundamentally different from the conventional multiple stream primary mill configurations...|$|E
40|$|Habitat {{monitoring}} is {{an important}} driving application for wireless sensor networks (WSNs). Although researchers anticipate some challenges arising in the real-world deployments of sensor networks, {{a number of problems}} can be discovered only through experience. This paper evaluates a sensor network system described in an earlier work and presents a set of experiences from a four month long deployment on a remote island o# the coast of Maine. We present an in-depth analysis of the environmental and node health data. The close integration of WSNs with their environment provides biological data at densities previous impossible; however, we show that the sensor data is also useful for predicting system operation and network failures. Based on over one million data and health readings, we analyze the node and network design and develop network <b>reliability</b> <b>profiles</b> and failure models...|$|R
40|$|Current genomic screens for non-coding RNAs (ncRNAs) {{predict a}} large num-ber of genomic regions {{containing}} potential structural ncRNAs. The analy-sis of this data requires highly accurate prediction of ncRNA boundaries and discrimination of promising candidate ncRNAs from weak predictions. Exist-S. Will. LocARNA-P 2 ing methods struggle with these goals because such comparative analysis {{is based on}} multiple sequence alignments of orthologous regions and does not revise these alignments based on sequence and structural similarity. To over-come this limitation, we systematically fulfill both requirements by efficiently computing the reliabilities of sequence-structure alignments. The <b>reliability</b> <b>profiles</b> of alignments provide a versatile tool for the manual and automatic analysis of ncRNAs. In particular, we improve the boundary prediction of the widely used non-coding RNA gene finder RNAz {{by a factor of}} three from a median deviation of 47 to 13 nucleotides. Post-processing RNAz prediction...|$|R
40|$|We {{provide an}} {{in-depth}} study of applying {{wireless sensor networks}} (WSNs) to real-world habitat monitoring. A set of system design requirements were developed that cover the hardware design of the nodes, the sensor network software, protective enclosures, and system architecture {{to meet the requirements}} of biologists. In the summer of 2002, 43 nodes were deployed on a small island off the coast of Maine streaming useful live data onto the web. Although researchers anticipate some challenges arising in real-world deployments of WSNs, many problems can only be discovered through experience. We present a set of experiences from a four month long deployment on a remote island. We analyze the environmental and node health data to evaluate system performance. The close integration of WSNs with their environment provides environmental data at densities previously impossible. We show that the sensor data is also useful for predicting system operation and network failures. Based on over one million 2 Polastre et. al. data readings, we analyze the node and network design and develop network <b>reliability</b> <b>profiles</b> and failure models...|$|R
40|$|Semiconductor {{transient}} faults (i. e. soft errors) {{have become}} an increasingly important threat to microprocessor reliability. Simultaneous multithreaded (SMT) architectures exploit thread-level parallelism to improve overall processor throughput. A great amount of {{research has been conducted}} in the past to investigate performance and power issues of SMT architectures. Nevertheless, the effect of multithreaded execution on a microarchitecture’s vulnerability to soft error remains largely unexplored. To address this issue, we have developed a microarchitecture level soft error vulnerability analysis framework for SMT architectures. Using a mixed set of SPEC CPU 2000 benchmarks, we quantify the impact of multithreading {{on a wide range of}} microarchitecture structures. We examine how the baseline SMT microarchitecture <b>reliability</b> <b>profile</b> varies with workload behavior, the number of threads and fetch policies. Our experimental results show that the overall vulnerability rises in multithreading architectures, while each individual thread shows less vulnerability. By considering both performance and reliability, SMT outperforms superscalar architectures. The SMT reliability and its tradeoff with performance vary across different fetch policies. With a detailed analysis of the experimental results, we point out a set of potential opportunities to reduce SMT microarchitecture vulnerability, which can serve as guidance to exploiting thread-aware reliability optimization techniques in the near future. To our knowledge, this paper presents the first effort to characterize microarchitecture vulnerability to soft error on SMT processors. 1...|$|E
40|$|University of Minnesota Ph. D. dissertation. November 2013. Major: Electrical Engineering. Advisor: David J. Lilja. 1 {{computer}} file (PDF); xi, 110 pages. The underlying technologies for storing digital bits {{have become more}} diverse in last decade. There is no fundamental differences in their functionality yet their behaviors can be quite different and no single management technique seems to fit them all. The differences can be categorized based on the metric of interest such as the performance profile, the <b>reliability</b> <b>profile</b> and the power profile. These profiles are {{a function of the}} system and the workload assuming that the systems are exposed only to a pre-specified environment. Near infinite workload space makes it infeasible to obtain the complete profiles for any storage systems unless the system enforces a discrete and finite profile internally. The thesis of this work is that an acceptable approximation of the profiles may be achieved by proper characterization of the workloads. A set of statistical tools as well as understanding of system behavior were used to evaluate and design such characterizations. The correctness of the characterization cannot be fully proved except by showing that the resulting profile can correctly predict any workload and storage system interactions. While this is not possible, we show that we can provide a reasonable confidence in our characterization by statistical evaluation of results. The characterizations of this work were applied to compression ratio for backup data deduplication and load balancing of heterogeneous storage systems in a virtualized environments. The validation of our characterization is validated through hundreds of real world test cases as well as reasonable deductions based on our understanding of the storage systems. In both cases, the goodness of characterizations were rigorously evaluated using statistical techniques. The findings along the validations were both confirming and contradicting of many previous beliefs...|$|E
40|$|The {{continued}} {{safe and}} reliable operation of the ATR is critical to the Department of Energy (DOE) Office of Nuclear Energy (NE) mission. While ATR is safely fulfilling current mission requirements, a variety of aging and obsolescence issues challenge ATR engineering and maintenance personnel’s capability to sustain ATR over the long term. First documented in a series of independent assessments, beginning with an OA Environmental Safety and Health Assessment conducted in 2003, the issues were validated in a detailed Material Condition Assessment (MCA) conducted {{as a part of the}} ATR Life Extension Program in 2007. Accordingly, near term replacement of aging and obsolescent original ATR equipment has become important to ensure ATR capability in support of NE’s long term national missions. To that end, a mission needs statement has been prepared for a non-major system acquisition which is comprised of three interdependent sub-projects. The first project will replace the existent diesel-electrical bus (E- 3), switchgear, and the fifty year old antiquated marine diesels with commercial power that is backed with safety-related emergency diesel generators (EDGs), switchgear, and uninterruptible power supply. The second project will replace the four, obsolete, original primary coolant pumps and motors. The third project, the subject of this major modification determination, will replace the current emergency firewater injection system (EFIS). The replacement water injection system will function as the primary emergency water injection system with the EFIS being retained as a defense-in-depth backup. Completion of this and the two other age-related projects (replacement of the ATR diesel bus (E- 3) and switchgear and replacement of the existent aged primary coolant pumps and motors) will resolve major age-related operational issues plus make a significant contribution in sustaining the ATR safety and <b>reliability</b> <b>profile.</b> The major modification criteria evaluation of the project pre-conceptual design identified several issues that lead to the conclusion that the project is a major modification...|$|E
40|$|Every {{organisation}} depends critically on reliable high-performance storage. Driven by {{the high}} costs of maintaining and managing multiple local storage systems, there is a trend towards virtualised multi-tier storage infrastructures. The main limitation of such centralised solutions is their inability to guarantee application-level Quality of Service (QoS) without extensive and ongoing human intervention. This intervention is necessary since delivered QoS can vary extensively both across and within storage tiers, and also depends on the access profile of the data. This paper presents the first steps towards the concrete realisation of a self-managing virtualised storage system which automatically allocates and migrates data throughout its lifecycle guided by user-provided QoS hints. Specifically, we use the Logical Volume Manager (LVM) to create a virtualised multi-tier storage infrastructure with variable performance and <b>reliability</b> <b>profiles.</b> On to that, we place an enhanced (but backwardscompatible) Linux Extended 3 Filesystem which we call ext 3 ipods and which supports QoS metadata. We describe the kernel modifications necessary to quantify the QoS provided by a given data layout, thus enabling the subsequent development of intelligent data placement and migration algorithms. ...|$|R
40|$|This thesis {{deals with}} voids {{formation}} in lead-free soldered joint. In theoretical part are described types of voids, voids formation, effect on join <b>reliability,</b> effect reflow <b>profile</b> on voids formation and effect of surface finish on voids formation. In practical part is investigated a effect of thermal stress on voids growth...|$|R
40|$|Dynamic program {{analysis}} frameworks greatly improve {{software quality}} as they enable {{a wide range}} of powerful analysis tools (e. g., <b>reliability,</b> <b>profiling,</b> and logging) at runtime. However, because existing frameworks run only one actual execution for each software application, the execution is fully or partially coupled with an analysis tool in order to transfer execution states (e. g., accessed memory and thread interleavings) to the analysis tool, easily causing a prohibitive slowdown for the execution. To reduce the portions of execution states that require transfer, many frameworks require significantly carving analysis tools as well as the frameworks themselves. Thus, these frameworks significantly trade off transparency with analysis tools and allow only one type of tools to run within one execution. This paper presents REPFRAME, an efficient and transparent framework that fully decouples execution and analysis by constructing multiple equivalent executions. To do so, REPFRAME leverages a recent faulttolerant technique: transparent state machine replication, which runs the same software application on a set of machines (or replicas), and ensures that all replicas see the same sequence of inputs and process these inputs with the same efficient thread interleavings automatically. In addition, this paper discusses potential directions in which REPFRAME can further strengthen existing analyses. Evaluation shows that REPFRAME is easy to run two asynchronous analysis tools together and has reasonable overhead...|$|R
40|$|In {{pharmaceutical}} industries, quantitative {{analytical methods}} such as HPLC play a key role. Indeed, the analytical results obtained from them are used to make crucial decisions such as the release of batches of drugs, the evaluation of safety and efficacy of new drug candidates or the monitoring of patients health. Prior to their routine use, analytical methods are submitted to a stringent validation study {{where they have to}} demonstrate that they are fit for their final purpose, i. e. providing accurate result. Typically this demonstration is made by either providing point estimates of systematic error (bias) and random error (variance) or sometimes by providing interval estimates of these statistical parameters at several well defined concentration levels of the target analyte. They are then compared to maximum acceptable levels. More recently, tolerance intervals approaches have been proposed that are evaluated in a similar way at these key concentration levels. However none of these decision approaches allow knowing the probability to obtain accurate results over the whole concentration range of interest. Frequentist approximations have been proposed to estimate this probability but only at the concentration levels experimentally tested and not for the whole range of interest. In this work, a linear hierarchical Bayesian approach is proposed. It takes into account the potential random characteristic of the slope and intercept observed from one analytical run to the other, and also integrates the possible covariance between the parameters. Additionally, heteroscedasticity of the residual variance over the concentration range investigated is taken into account. A situation regularly observed in practice. Finally a <b>reliability</b> <b>profile</b> for the whole concentration range studied is obtained using MCMC sampling. This profile provides the probability (Prel) to obtain accurate results over the full concentration range investigated. This profile is then compared to a minimum reliability probability (Pmin) that will define the valid concentration range of the analytical method. The usefulness of this approach is illustrated through the validation of a bioanalytical method and also compared with a one concentration level at a time frequentist approach derived from tolerance intervals. Peer reviewe...|$|E
40|$|Near term {{replacement}} {{of aging and}} obsolescent original ATR equipment has become important to ensure ATR capability in support of NE’s long term national missions. To that end, a mission needs statement has been prepared for a non-major system acquisition which is comprised of three interdependent subprojects. The first project, subject of this determination, will replace the existent diesel-electrical bus (E- 3) and associated switchgear. More specifically, INL proposes transitioning ATR to 100 % commercial power with appropriate emergency backup to include: • Provide commercial power as the normal source of power to the ATR loads currently supplied by diesel-electric power. • Provide backup power to the critical ATR loads {{in the event of}} a loss of commercial power. • Replace obsolescent critical ATR power distribution equipment, e. g., switchgear, transformers, motor control centers, distribution panels. Completion of this and two other age-related projects (primary coolant pump and motor replacement and emergency firewater injection system replacement) will resolve major age related operational issues plus make a significant contribution in sustaining the ATR safety and <b>reliability</b> <b>profile.</b> The major modification criteria evaluation of the project pre-conceptual design identified several issues make the project a major modification: 1. Evaluation Criteria # 2 (Footprint change). The addition of a new PC- 4 structure to the ATR Facility to house safety-related SSCs requires careful attention to maintaining adherence to applicable engineering and nuclear safety design criteria (e. g., structural qualification, fire suppression) to ensure no adverse impacts to the safety-related functions of the housed equipment. 2. Evaluation Criteria # 3 (Change of existing process). The change to the strategy for providing continuous reliable power to the safety-related emergency coolant pumps requires careful attention and analysis to ensure it meets a project primary object to maintain or reduce CDF and does not negatively affect the efficacy of the currently approved strategy. 3. Evaluation Criteria # 5 (Create the need for new or revised safety SSCs). The change to the strategy for providing continuous reliable power to the safety-related emergency coolant pumps, based on the pre-conceptual design, will require the addition of two quick start diesel generators, their associated power coordination/distribution controls, and a UPS to the list of safety-related SSCs. Similarly to item 1 above, the addition of these active SSCs to the list of safety-related SSCs and {{replacement of}} the E- 3 bus requires careful attention to maintaining adherence to applicable engineering and nuclear safety design criteria (e. g., seismic qualification, isolation of redundant trains from common fault failures) to ensure no adverse impacts to the safety-related functions...|$|E
40|$|In {{pharmaceutical}} and biomedical industries, quantitative {{analytical methods}} such as HPLC play a key role. Indeed, the analytical results obtained from them are used to make crucial decisions such as the release of batches of drugs, the evaluation of safety and efficacy of new drug candidates or the monitoring of patients health. Prior to their routine use, analytical methods are submitted to a stringent validation study [1] {{where they have to}} demonstrate that they are fit for their final purpose, i. e. providing accurate results: where is the analytical result, is the theoretical unknown true concentration of analyte in the sample analyzed and a regulatory acceptance limit. Typically this demonstration is made by either providing point estimates of systematic error (bias) and random error (variance) or sometimes by providing interval estimates of these statistical parameters at several well defined concentration levels of the target analyte [2]. They are then compared to maximum acceptable levels. More recently, tolerance intervals approaches have been proposed that are evaluated in a similar way at these key concentration levels [3]. However none of these decision approaches allow knowing the probability to obtain accurate results over the whole concentration range of interest: is a vector of parameters and Pmin is a minimum reliability probability. Frequentist approximations have been proposed to estimate this probability but only at the concentration levels experimentally tested [4, 5]. In this work, a linear hierarchical Bayesian approach is proposed. It takes into account the potential random characteristic of the slope and intercept observed from one analytical run to the other, but it also integrates the possible covariance between the parameters. Additionally, heteroscedasticity of the residual variance over the concentration range investigated is taken into account. A situation regularly observed in practice. Finally a <b>reliability</b> <b>profile</b> for the whole concentration range studied is obtained using MCMC sampling. This profile provides the probability (Prel) to obtain accurate results over the full concentration range investigated. This profile is then compared to a minimum reliability probability (Pmin) that will define the valid concentration range of the analytical method. The usefulness of this approach is illustrated through the validation of a bioanalytical method and also compared with one concentration level at a time frequentist approaches [4, 5]. [1] International Conference on Harmonization (ICH) of Technical Requirements for registration of Pharmaceuticals for Human Use Topic Q 2 (R 1) : Validation of Analytical Procedures: Text and Methodology, Geneva, 2005. [2] A. Bouabidi and al., J. Chromatogr. A, 1217 (2010) 3180. [3] Ph. Hubert and al., J. Pharm. Biomed. Anal., 36 (2004) 579. [4] W. Dewé and al., Chemometr. Intell. Lab. Syst. 85 (2007) 262. [5] B. Govaerts and al., Qual. Reliab. Engng. Int. 24 (2008) 667. Peer reviewe...|$|E
40|$|The {{study of}} {{microarray}} experiments allows description of genome-wide expression changes {{in health and}} disease. Many different gene expression profile analysis methods have been applied to identify the significant genes in the microarray experiments. This report attempts to evaluate the reliability of p-values provided by two popular permutation involved analytical technics. Two main methods were designed to check the <b>reliability</b> of each <b>profile</b> technic individually. 1...|$|R
40|$|Work Keys Job <b>Profiles</b> 2 The <b>reliability</b> of job <b>profiles</b> {{collected}} for ACT*s Work Keys system, which {{was first introduced}} to IPMAAC in 1993, is reviewed. The Work Keys job profiling system, the job analysis system used to establish content validity of the Work Keys assessments, is discussed. Profiles showed strong interrater agreement, with skill levels differing depending {{on the job and}} company in which the data was collected. Implications for selection, training, and educational skill standards are discussed...|$|R
40|$|Low {{temperature}} {{tape and}} thick film materials {{for use on}} stainless steel substrates in domestic appliance applications were developed. The insulating glass-ceramic coatings were applied to stainless steel bases using screen printing and tape processing. Heaters and heat regulators using PTC and other 850 íC firing thick film materials were designed and built. Properties of the coated metal substrates and the heater devices are presented including initial and aged data demonstrating properties and <b>reliability</b> in low <b>profile</b> heater applications...|$|R
40|$|The {{continued}} {{safe and}} reliable operation of the ATR is critical to the Department of Energy (DOE) Office of Nuclear Energy (NE) mission. While ATR is safely fulfilling current mission requirements, a variety of aging and obsolescence issues challenge ATR engineering and maintenance personnel’s capability to sustain ATR over the long term. First documented in a series of independent assessments, beginning with an OA Environmental Safety and Health Assessment conducted in 2003, the issues were validated in a detailed Material Condition Assessment (MCA) conducted {{as a part of the}} ATR Life Extension Program in 2007. Accordingly, near term replacement of aging and obsolescent original ATR equipment has become important to ensure ATR capability in support of NE’s long term national missions. To that end, a mission needs statement has been prepared for a non-major system acquisition which is comprised of three interdependent subprojects. The first project will replace the existent diesel-electrical bus (E- 3), switchgear, and the 50 -year-old obsolescent marine diesels with commercial power that is backed with safety related emergency diesel generators, switchgear, and uninterruptible power supply (UPS). The second project, the subject of this major modification determination, will replace the four, obsolete, original primary coolant pumps (PCPs) and motors. Completion of this and the two other age-related projects (replacement of the ATR diesel bus [E- 3] and switchgear and replacement of the existent emergency firewater injection system) will resolve major age-related operational issues plus make a significant contribution in sustaining the ATR safety and <b>reliability</b> <b>profile.</b> The major modification criteria evaluation of the project pre-conceptual design identified several issues that lead to the conclusion that the project is a major modification: 1. Evaluation Criteria # 3 (Change of existing process). The proposed strategy for equipping the replacement PCPs with VFDs and having the PCPs also function as ECPs will require significant safety basis changes requiring DOE approval. 2. Evaluation Criteria # 4 (Use of new technology). The use of VFD and VFD “pump catcher” technology for the PCPs is not currently in use and has not been previously formally reviewed/approved by DOE for ATR. It is noted that VFD technology has several decades of commercial use and experience. However, the ATR probabilistic risk assessment will have to be updated, reflecting the changes for supplying ECP flows including VFD reliability, to confirm that the proposed activity maintains or reduces the CDF for the ATR. 3. Evaluation Criteria # 5 (Create the need for new or revised safety SSCs). It is expected that the proposed activity will result in a revised list of safety-related SSCs. Specifically, as currently proposed, the existing ECPs will be deleted from the list. The PCPs and their associated components, picking up the ECP function, will be classified as safety-related active Seismic Category I...|$|E
40|$|Reliability is {{considered}} the most important attribute of transit service by passengers. There are congested transit environments wherein even if a transit service is perfectly on schedule it is termed unreliable from a passenger’s perspective when {{they are unable to}} board the first service of their choice set. The phenomenon ‘failure to board’ arises in congested transit networks having strict physical capacity constraints wherein the transit service cannot take in passengers beyond its capacity. This results in some of the passengers being left waiting for the next service at the transit stops. The existing transit assignment models; be it hyperpath based effective frequency models, Bureau of Public Roads (BPR) based route section models or aggregate stochastic process models with strict capacity constraints, all assume that the passengers have perfect knowledge of the network seldom discussing the sources of such information. In the current thesis this assumption is renounced and a reliability based disaggregate stochastic process model with strict capacity constraints (R-DSPM) using route section approach is proposed such that each passenger in the absence of information updates his/her route choice based on their individual experience. Though the aggregate stochastic process model implements the strict capacity constraint for each transit service generated; the model along with the assumption of perfect knowledge of the network assumes that the passengers are risk neutral. The proposed R-DSPM implements a strict capacity constraint for each transit service generated thereby accounting for failure to board situation in congested network. The proposed model differs from the existing aggregate stochastic process model in its assumption of risk averse passengers. Risk aversion in R-DSPM considers variance associated with:- interarrival times of transit service at the transit stop; the waiting time of passengers due to the ‘failure to board’ condition; the in-vehicle travel times of routes comprising of route sections containing more than one attractive line section and the variable demand generated for each day’s travel. The risk aversion of each passenger is accounted for in R-DSPM through the linear combination of mean total travel time and total travel variance (mean-variance) and a linear combination of mean total travel time and expected lateness (mean-lateness). A generic day to day framework is developed with markovian properties such that it enables the integration of both mean-variance and mean-lateness costs with ease and results in a unique stationary distribution of costs and flows for each route. The proposed R-DSPM thus accounts for: strict capacity constraints of transit vehicle, passengers learning process, risk aversion of passengers, differences in passenger perceptions, day to day variability in demand and supply of transit network. The micro simulation framework shows through implementation on example networks that while accounting for passenger’s risk aversion the R-DSPM is able to arrive at a unique stationary distribution irrespective of its initial conditions. The sensitivity of the proposed R-DSPM with strict capacity constraint under different parameter assumptions has been carried out. A calibration of the parameters involved in the route section based BPR styled cost function and the hyperpath based effective frequency cost function using the proposed R-DSPM indicates that different congestion function parameters are required for different sections of a transit network. It is also shown through implementation on an example network that the proposed R-DSPM framework enables the passengers to learn about the reliability of routes and strategies. At higher dispersion values R-DSPM assign risk averse passengers such that the standard deviation of flows and experienced total travel time on various routes and strategies are lesser than that obtained by accounting for risk aversion using the aggregate stochastic process models. The impact of accounting for risk aversion on various policy measures that could be carried out by the operators to improve the waiting time reliability of passengers is also assessed using the proposed R-DSPM with strict capacity constraints. It is shown that for certain parameter assumptions and for certain policy measures the assumption of risk aversion in transit assignment could result in an entirely different <b>reliability</b> <b>profile</b> from that of an assignment process assuming risk neutral passengers. The implementation of the proposed R-DSPM with strict capacity constraints on a real network has been carried out on a section of London underground and several possible policy measures have been evaluated. The evaluation of policies has further emphasised the need to consider the risk aversion in passengers especially to account for the number of passengers preferring to make a transfer (in absence of transfer penalty) at the transfer stops to minimise their risk aversion costs...|$|E
40|$|New {{materials}} can provide indisputably important {{answers to the}} challenges of the 21 st century. They help us conserve resources and assure sustainable and affordable energy supply, mobility and medical care. With their special structural and functional properties, high-performance ceramics {{play a key role in}} applications involving strong thermal, mechanical and chemical stresses. Requirements in respect of design, property <b>profile,</b> <b>reliability</b> and cost are constantly rising. Germany plays a leading role in technical ceramics in international competition and can draw on a close-knit network of university and non-university teaching and research institutes...|$|R
40|$|Research has {{determined}} that organizational culture is related to employee turnover, job commitment, and job satisfaction. Assessment of this culture requires an instrument that befits the type of organization under examination. Using exploratory factor analysis, Stohr and her colleagues were able {{to demonstrate that the}} Organizational Culture Instrument (OCI) had a solid <b>reliability</b> and validity <b>profile.</b> The current study reanalyzes these data, using confirmatory factor analysis and structural equation modeling. The findings indicate that there is statistical evidence to claim validation of the OCI and its seven theoretically based dimensions...|$|R
40|$|Large cohorts of archival {{samples are}} stored in tissue banks {{worldwide}} yet their contribution to biomarker discovery is limited. Proteomic profiling technologies have potential for early screening and diagnosis of cancer, and data from such samples can be the answer for many clinical questions. Here we introduce the notion of archival samples proteomics. Using SELDI-TOF MS analysis, we compared 30 -year-old archival serum samples of healthy volunteers and patients diagnosed with non metastatic breast cancer. To validate the reproducibility of our results, analysis of the same samples was repeated in a different centre under standardised settings. Plausible differentially expressed protein peaks between the breast cancer and control groups were repeatedly detected. Our pilot study showed highly reproducible and concordant results between two independent analyses conducted in different centres. The feasibility and <b>reliability</b> of <b>profiling</b> serum archives of women with breast cancer was tested in this pilot study. Our results imply that proteomic profiling of serum may {{have an important role}} in biomarkers discovery regardless of the storage period. Clearly, multicentre validation of larger archival cohorts is vita...|$|R
40|$|X-ray {{photoelectron}} spectroscopy {{combined with}} Ar+ ion etching {{is a powerful}} concept to identify different chemical states of compounds in depth profiles, important for obtaining information underneath surfaces or at layer interfaces. The possibility of occurring sputter damage is known but insufficiently investigated for corrosion products of Zn-based steel coatings like ZnCr. Hence, in this work reference materials are studied according to stability against ion sputtering. Indeed some investigated compounds reveal a very unstable chemical nature. On {{the basis of these}} findings the <b>reliability</b> of depth <b>profiles</b> of real samples can be rated to avoid misinterpretations of observed chemical species. Comment: 8 figures, in press, accepted manuscript, Corrosion Science, online availabl...|$|R
40|$|The crystal {{structure}} of optically active microbial poly(D(-) -beta-hydroxybutyrate) has been refined with the Rietveld whole-fitting method applied to powder X-ray diffraction data. This natually occurring polymer {{gives rise to}} a very crystalline phase and therefore to very detailed and resolved powder <b>profiles.</b> <b>Reliability</b> of the refinement is discussed in relation to results obtained from previous studies on oriented-fiber diffraction patterns. The conclusion is that well-detailed powder profile data are highly discriminatory toward structural models not very dissimilar from each other. This result is considered an encouraging step toward a more general assessment of the accuracy of structural parameters obtained from best fitting of powder X-ray diffraction profiles...|$|R
40|$|University of Minnesota Ph. D. dissertation. June 2013. Major: Educational Psychology. Advisor: Michael C. Rodriguez. 1 {{computer}} file (PDF); vii, 197 pages, appendices A-C. The importance of subscores in educational and psychological assessments is undeniable. Subscores yield diagnostic {{information that can}} be used for determining how each examinee's abilities/skills vary over different content domains. One of the most common criticisms about reporting and using subscores is insufficient reliability of subscores. This study employs a new reliability approach that allows the evaluation of between-person subscore reliability as well as within-person subscore reliability. Using this approach, the unidimensional IRT (UIRT) and multidimensional IRT (MIRT) models are compared in terms of subscore reliability in simulation and real data studies. Simulation conditions in the simulation study are subtest length, correlations among subscores, and number of subtests. Both unidimensional and multidimensional subscores are estimated with the maximum a posteriori probability (MAP) method. Subscore reliability of ability estimates are evaluated in light of between-person reliability, within-person <b>reliability,</b> and total <b>profile</b> <b>reliability.</b> The results of this study suggest that the MIRT model performs better than the UIRT model under all simulation conditions. Multidimensional subscore estimation benefits from correlations among subscores as ancillary information, and it yields more reliable subscore estimates than unidimensional subscore estimation. The subtest length is positively associated with both between-person and within-person reliability. Higher correlations among subscores improve between-person reliability, while they substantially decrease within-person reliability. The number of subtests seems to influence between-person reliability slightly but it has no effect on within-person reliability. The two estimation methods provide similar results with real data as well...|$|R
40|$|In this paper, {{software}} reliability measurement is studied {{from the perspective}} of usage <b>profile.</b> <b>Reliability</b> parameters on each usage profile are expressed with unascertained rational numbers. Since failure process of each usage profile can be affected by multiple factors such as software attributes and users imports etc., GEP (gene expression programming) is introduced into {{software reliability}} modeling, to study comprehensive force among the factors. Finally, a software reliability GEP model based on usage profile is constructed. With the new model, users can confirm software reliability according to its usage, thus enabling software reliability to a more objective assessment. The new model is experimented to measure reliability of SYS in Musa data, and proved with a satisfactory performance...|$|R
40|$|Users of multi-scale tests {{like the}} MMPI- 2 {{tend not to}} {{interpret}} scales {{one at a time}} {{in a way that would}} correspond to standard scale-level reliability information. Instead, clinicians integrate inferences from a multitude of scales simultaneously, producing a descriptive narrative that is thought to characterize the examinee. This study was an attempt to measure the reliability of such integrated interpretations using a q-sort research methodology. Participants were 20 MMPI- 2 users who responded to E-mail solicitations on professional listservs and in personal emails. Each participant interpreted one of two common MMPI- 2 profiles using a q-set of 100 statements designed for MMPI- 2 interpretation. To measure the “interpretive reliability” of the MMPI- 2 profile interpretations, q-sort descriptions were intercorrelated. Mean pairwise interpretive reliability was. 39, lower than expected, and there was no significant difference in <b>reliability</b> between <b>profiles.</b> There was also not a significant difference between within-profile and cross-profile correlations. Q-set item analysis was conducted to determine which individual statements had the most impact on interpretive reliability. Although sampling in this study was limited, implications for the field reliability of MMPI- 2 interpretation are sobering...|$|R
40|$|In this study, we used {{survey and}} observation, {{questionnaire}} techniques and others secondary data such as reports, database {{and so on}} for data collection. Based on <b>reliability</b> test, respondent <b>profile,</b> frequency, correlations, cross tab tabulation and Chisquared test, a clear findings and result is observed. Job satisfaction {{is one of the}} evaluation aspects in work behavior, which has been emphasized by all of the organizations to ensure the successful of organization’s goal. This research is focus on job satisfaction among employees at TH Travel & Services Sdn Bhd. The main objective is to identify satisfaction factors among employees at TH Travel & Services Sdn Bhd, to determine relationship between job satisfaction element and overall job satisfaction among employees of TH Travel & Services Sdn Bhd, to determine the demography of the respondent towards job satisfaction on TH Travel & Services Sdn Bhd and to recommend solutions on how to improve satisfaction among TH Travel & Services Sdn Bhd. Besides, the feedback from the questionnaire that the researcher distributed also gives an impact to the company. All this feedback can upgrade the performance of the company. The findings showed that most of the staffs have moderate of satisfaction with the payments, rewards, working condition and working itself. The researcher is also able to give some recommendations and suggestion on how to improve and increase job satisfaction of the staffs after analysis, findings and interpretation. In this study, we used survey and observation, questionnaire techniques and others secondary data such as reports, database and so on for data collection. Based on <b>reliability</b> test, respondent <b>profile,</b> frequency, correlations, cross tab tabulation and Chi-squared test, a clear findings and result is observed...|$|R
40|$|Power {{loss of a}} {{distribution}} system can be reduced significantly by using optimum size and location of distributed generation (DG). Proper allocation of DG with appropriate size maximizes overall system efficiency. Moreover it improves the <b>reliability</b> and voltage <b>profile</b> of the distribution system. In this paper, IEEE 123 node test feeder has been considered to determine the optimum size and location of a synchronous machine based DG for loss reduction of the system. This paper also investigates the steady-state and dynamic voltage profile of that three phase unbalance distribution network in presence of DG with optimum size. This analysis shows that optimum size of DG at proper location minimizes the power loss as well as improves the dynamic voltage profile of the distribution system...|$|R
