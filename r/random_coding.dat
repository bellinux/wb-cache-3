479|1555|Public
5000|$|In {{the style}} of the <b>random</b> <b>coding</b> argument, we {{randomly}} generate [...] codewords of length n from a probability distribution Q.|$|E
5000|$|For any [...] "small enough" [...] {{function}} family , {{there exists}} a (possibly inefficient) coding scheme which is non-malleable w.r.t. F. Moreover, for a fixed [...] "small enough" [...] function family , a <b>random</b> <b>coding</b> scheme {{is likely to be}} non-malleable w.r.t. F with overwhelming probability. Unfortunately, <b>random</b> <b>coding</b> schemes cannot be efficiently represented, nor is the encoding/decoding function likely to be efficient. Therefore, this result should merely be thought of as showing [...] "possibility" [...] and providing a target that we should then strive to match constructively. Moreover, this result also highlights the difference between [...] "error-correction/detection" [...] and [...] "non-malleability" [...] since a result of this form could not be true for the former notions.|$|E
50|$|Both {{types of}} proofs {{make use of}} a <b>random</b> <b>coding</b> {{argument}} where the codebook used across a channel is randomly constructed - this serves to make the analysis simpler while still proving {{the existence of a}} code satisfying a desired low probability of error at any data rate below the channel capacity.|$|E
40|$|Abstract—Minimum {{distance}}s, distance distributions, {{and error}} exponents on a binary-symmetric channel (BSC) are given for typical <b>codes</b> from Shannon’s <b>random</b> <b>code</b> ensemble and for typical <b>codes</b> from a <b>random</b> linear <b>code</b> ensemble. A typical <b>random</b> <b>code</b> of length and rate {{is shown to}} have minimum distance @P A, where @ A is the Gilbert–Varshamov (GV) relative distance at rate, whereas a typical linear code (TLC) has minimum distance @ A. Consequently, a TLC has a better error exponent on a BSC at low rates, namely, the expurgated error exponent. Index Terms—Distance distributions, exponential error bounds, minimum distance, <b>random</b> <b>codes,</b> <b>random</b> linear <b>codes,</b> typical linear <b>codes,</b> typical <b>random</b> <b>codes.</b> I...|$|R
40|$|Abstract- The minimum {{probability}} of error achievable by <b>random</b> <b>codes</b> on the arbitrarily varying channel (AVC) is in-vestigated. New exponential error bounds are found and applied to the AVC with and without input and state constraints. Also considered is a simple subclass of <b>random</b> <b>codes,</b> called randomly modulated codes, in which encoding and decoding operations are separate from code randomization. A universal coding theorem is proved which shows the existence of randomly modulated codes that achieve the same error bounds as “fully ” <b>random</b> <b>codes</b> for all AVC’s. Index Terms- Arbitrarily varying channels, error exponents, <b>random</b> <b>codes,</b> jamming. T I...|$|R
40|$|An erasure channel with a fixed {{alphabet}} size q, where q ≫ 1, is studied. It is {{proved that}} over any erasure channel (with or without memory), Maximum Distance Separable (MDS) codes achieve the minimum {{probability of error}} (assuming maximum likelihood decoding). Assuming a memoryless erasure channel, the error exponent of MDS codes are {{compared with that of}} <b>random</b> <b>codes</b> and linear <b>random</b> <b>codes.</b> It is shown that the envelopes of all these exponents are identical for rates above the critical rate. Noting the optimality of MDS codes, it is concluded that both <b>random</b> <b>codes</b> and linear <b>random</b> <b>codes</b> are exponentially optimal whether the block sizes is larger or smaller than the alphabet size...|$|R
50|$|The proof {{runs through}} in {{almost the same}} way as that of channel coding theorem. Achievability follows from <b>random</b> <b>coding</b> with each symbol chosen {{randomly}} from the capacity achieving distribution for that particular channel. Typicality arguments use the definition of typical sets for non-stationary sources defined in the asymptotic equipartition property article.|$|E
5000|$|In {{information}} theory, typical set decoding is used {{in conjunction}} with <b>random</b> <b>coding</b> to estimate the transmitted message as the one with a codeword that is jointly ε-typical with the observation. i.e.where [...] are the message estimate, codeword of message [...] and the observation respectively. [...] is defined with respect to the joint distribution [...] where [...] is the transition probability that characterizes the channel statistics, and [...] is some input distribution used to generate the codewords in the random codebook.|$|E
5000|$|The proof runs as follows. Suppose [...] and [...] are fixed. First we show, for a fixed [...] and [...] chosen randomly, the {{probability}} of failure over [...] noise is exponentially small in n. At this point, the proof works for a fixed message [...] Next we extend this result to work for all [...] We achieve this by eliminating half of the codewords from the code with {{the argument that the}} proof for the decoding error probability holds for {{at least half of the}} codewords. The latter method is called expurgation. This gives the total process the name <b>random</b> <b>coding</b> with expurgation.|$|E
40|$|We {{study the}} {{arbitrarily}} varying relay channel, and establish the cutset bound and partial decode-forward bound on the <b>random</b> <b>code</b> capacity. We further determine the <b>random</b> <b>code</b> capacity for special cases. Then, we consider {{conditions under which}} the deterministic code capacity is determined as well. In addition, we consider the arbitrarily varying Gaussian relay channel with sender frequency division under input and state constraints. We determine the <b>random</b> <b>code</b> capacity and establish lower and upper bounds on the deterministic code capacity...|$|R
3000|$|..., {{the most}} {{protected}} {{class of the}} PEG and PEG-ACE code has only slightly worse performance than the <b>random</b> <b>code,</b> while the average BER is much higher for the <b>random</b> <b>code.</b> This paper shows that we can improve both the average BER performance and the UEP capability by optimizing the threshold offset.|$|R
40|$|Hi-Tech Research and Development Program of China [2013 AA 122901]; Natural Science Foundation of China [61571427]; Youth Innovation Promotion Association CAS [2013162]According to the {{reconstruction}} feature of fluctuation-correlation ghost imaging (GI), we define a normalized characteristic matrix {{and the influence}} of the property of <b>random</b> <b>coded</b> patterns on GI is investigated based on the theory of matrix analysis. Both simulative and experimental results demonstrate that for different <b>random</b> <b>coded</b> patterns, the quality of fluctuation-correlation GI can be predicted by some parameters extracted from the normalized characteristic matrix, which suggests its potential application in the optimization of <b>random</b> <b>coded</b> patterns for GI system...|$|R
50|$|In {{the theory}} of quantum communication, the entanglement-assisted {{classical}} capacity of a quantum channel is the highest rate at which classical information can be transmitted from a sender to receiver when they share an unlimited amount of noiseless entanglement. It is given by the quantum mutual information of the channel, which is the input-output quantum mutual information maximized over all pure bipartite quantum states with one system transmitted through the channel. This formula is the natural generalization of Shannon's noisy channel coding theorem, {{in the sense that}} this formula is equal to the capacity, and {{there is no need to}} regularize it. An additional feature that it shares with Shannon's formula is that a noiseless classical or quantum feedback channel cannot increase the entanglement-assisted classical capacity. The entanglement-assisted classical capacity theorem is proved in two parts: the direct coding theorem and the converse theorem. The direct coding theorem demonstrates that the quantum mutual information of the channel is an achievable rate, by a <b>random</b> <b>coding</b> strategy that is effectively a noisy version of the super-dense coding protocol. The converse theorem demonstrates that this rate is optimal by making use of the strong subadditivity of quantum entropy.|$|E
40|$|A unified {{framework}} to obtain all known lower bounds (<b>random</b> <b>coding,</b> typical <b>random</b> <b>coding</b> and expurgated bound) on the reliability {{function of a}} point-to-point discrete memoryless channel (DMC) is presented. By using a similar idea for a two-user discrete memoryless (DM) multiple-access channel (MAC), three lower bounds on the reliability function are derived. The first one (<b>random</b> <b>coding)</b> {{is identical to the}} best known lower bound on the reliability function of DM-MAC. It is shown that the <b>random</b> <b>coding</b> bound is the performance of the average code in the constant composition code ensemble. The second bound (Typical <b>random</b> <b>coding)</b> is the typical performance of the constant composition code ensemble. To derive the third bound (expurgated), we eliminate some of the codewords from the codebook with larger rate. This is the first bound of this type that explicitly uses the method of expurgation for MACs. It is shown that the exponent of the typical <b>random</b> <b>coding</b> and the expurgated bounds are {{greater than or equal to}} the exponent of the known <b>random</b> <b>coding</b> bounds for all rate pairs. Moreover, an example is given where the exponent of the expurgated bound is strictly larger. All these bounds can be universally obtained for all discrete memoryless MACs with given input and output alphabets. Comment: 46 pages, 2 figure...|$|E
3000|$|... {{satisfies}} (9). The <b>random</b> <b>coding</b> used in {{this section}} {{is a combination of}} Gel'fand-Pinsker coding [16] and coding for MAC [31]. This <b>random</b> <b>coding</b> is not a new technique but it is included for completeness. Fix [...]...|$|E
30|$|By expurgating the <b>random</b> <b>code</b> ensemble, we {{obtain the}} {{following}} lemma.|$|R
30|$|Heap Spraying A {{technique}} used in exploits to assist <b>random</b> <b>code</b> execution.|$|R
40|$|In this work, {{we study}} two models of {{arbitrarily}} varying channels, when causal side {{information is available}} at the encoder in a causal manner. First, we study the arbitrarily varying channel (AVC) with input and state constraints, when the encoder has state information in a causal manner. Lower and upper bounds on the <b>random</b> <b>code</b> capacity are developed. A lower bound on the deterministic code capacity is established {{in the case of a}} message-averaged input constraint. In the setting where a state constraint is imposed on the jammer, while the user is under no constraints, the <b>random</b> <b>code</b> bounds coincide, and the <b>random</b> <b>code</b> capacity is determined. Furthermore, for this scenario, a generalized non-symmetrizability condition is stated, under which the deterministic code capacity coincides with the <b>random</b> <b>code</b> capacity. A second model considered in our work is the arbitrarily varying degraded broadcast channel with causal side information at the encoder (without constraints). We establish inner and outer bounds on both the <b>random</b> <b>code</b> capacity region and the deterministic code capacity region. The capacity region is then determined for a class of channels satisfying a condition on the mutual informations between the strategy variables and the channel outputs. As an example, we show that the condition holds for the arbitrarily varying binary symmetric broadcast channel, and we find the corresponding capacity region...|$|R
3000|$|In this Section, we {{evaluate}} achievable rate regions by <b>random</b> <b>coding</b> for the G-CICS in {{the regime}} of high state power. In [16], by using <b>random</b> <b>coding,</b> two inner bounds for the G-CICS are provided when |a|≤ 1. By evaluating the inner bound 1 of Proposition 3 in [16] (and replacing [...]...|$|E
30|$|The {{paper is}} {{organized}} as follows: We present the channel model in Section 2. The achievable rate region by <b>random</b> <b>coding</b> is presented at Section 3. Section 4, by using lattice codes, establishes an achievable rate region for the G-CICS. Using numerical examples, achievable rate regions of our proposed scheme and <b>random</b> <b>coding</b> are compared in Section 5. Section 6 concludes the paper.|$|E
40|$|We {{consider}} ensembles of channel {{codes that}} are partitioned into bins, {{and focus on}} analysis of exact <b>random</b> <b>coding</b> error exponents associated with optimum decoding of the index of the bin to which the transmitted codeword belongs. Two main conclusions arise from this analysis: (i) for independent random selection of codewords within a given type class, the <b>random</b> <b>coding</b> exponent of optimal bin index decoding is given by the ordinary <b>random</b> <b>coding</b> exponent function, computed {{at the rate of}} the entire code, independently of the exponential rate {{of the size of the}} bin. (ii) for this ensemble of codes, sub-optimal bin index decoding, that is based on ordinary maximum likelihood (ML) decoding, is as good as the optimal bin index decoding in terms of the <b>random</b> <b>coding</b> error exponent achieved. Finally, for the sake of completeness, we also outline how our analysis of exact <b>random</b> <b>coding</b> exponents extends to the hierarchical ensemble that correspond to superposition coding and optimal decoding, where for each bin, first, a cloud center is drawn at random, and then the codewords of this bin are drawn conditionally independently given the cloud center. For this ensemble, conclusions (i) and (ii), mentioned above, no longer hold necessarily in general. Comment: 19 pages; submitted to IEEE Trans. on Information Theor...|$|E
3000|$|... is the j th {{element of}} the k th user's <b>random</b> {{polarity}} <b>code</b> (or <b>random</b> DS spreading <b>code).</b> To reduce the effect of MAI, it can be well designed to assign the specific <b>random</b> TH <b>code</b> [...]...|$|R
3000|$|... after around 50 iterations. Thereby it can {{be assumed}} that the non-UEP <b>random</b> <b>code</b> will perform better at low [...]...|$|R
40|$|This paper calculates new bounds on {{the size}} of the {{performance}} gap between <b>random</b> <b>codes</b> and the best possible codes. The first result shows that, for large block sizes, the ratio of the error probability of a <b>random</b> <b>code</b> to the sphere-packing lower bound on the error probability of every code on the binary symmetric channel (BSC) is small for a wide range of useful crossover probabilities. Thus even far from capacity, <b>random</b> <b>codes</b> have nearly the same error performance as the best possible long codes. The paper also demonstrates that a small reduction k 0 ~ k in the number of information bits conveyed by a codeword will make the error performance of an (n; ~ k) <b>random</b> <b>code</b> better than the sphere-packing lower bound for an (n; k) code as long as the channel crossover probability is somewhat greater than a critical probability. For example, the sphere-packing lower bound for a long (n; k), rate 1 = 2, code will exceed the error probability of an (n; ~ k) <b>random</b> <b>code</b> if k 0 ~ k? 10 and the crossover probability is between 0 : 035 and 0 : 11 = H 01 (1 = 2). Analogous results are presented for the binary erasure channel (BEC) and the additive white Gaussian noise (AWGN) channel. The paper also presents substantial numerical evaluation of the performance of <b>random</b> <b>codes</b> and existing standard lower bounds for the BEC, BSC, and the AWGN channel. These last results provide a useful standard against which to measure many popular codes including turbo codes, e. g., there exist turbo codes that perform within 0. 6 dB of the bounds over a wide range of block lengths...|$|R
40|$|We {{compute the}} <b>random</b> <b>coding</b> error {{exponent}} for linear multihop amplify-and-forward (AF) relay channels. Instead of considering only the achievable rate or the error probability as a performance measure separately, the error exponent results {{can give us}} insight into the fundamental tradeoff between the information rate and communication reliability in these channels. This measure enables us to determine what codeword length that is required to achieve a given level of communication reliability at a rate below the channel capacity. We first derive a general formula for the <b>random</b> <b>coding</b> exponent of general multihop AF relay channels. Then we present a closed-form expression of a tight upper bound on the <b>random</b> <b>coding</b> error exponent for the case of Rayleigh fading. From the exponent expression, the capacity of these channels is also deduced. The effect {{of the number of}} hops on the performance of linear multihop AF relay channels from theerror exponent point of view is studied. As an application of the <b>random</b> <b>coding</b> error exponent analysis, we then find the optimal number of hops which maximizes the communication reliability (i. e., the <b>random</b> <b>coding</b> error exponent) for a given data rate. Numerical results verify our analysis, and show the tightness of the proposed bound. Funding agencies|Swedish Research Council (VR) ||ELLIIT||Knut and Alice Wallenberg Foundation|...|$|E
40|$|Non-asymptotically bounds for the {{probability}} of optimal messages decoding in the wiretap channel of a information transmission system with <b>random</b> <b>coding</b> by a arbitrary resilient function over an Abelian group, are obtained. Algorithms of random messages coding and decoding in main channel which use non-linear systhematic codes, are described. An example of the information transmission system with <b>random</b> <b>coding</b> by Preparata codes is considered. ???????? ????????????????? ??????? ??????????? ???????????? ?????? ????????? ? ???????? ?????? ??????? ???????? ?????????? ?? ????????? ????????????, ??????????? ?? ?????? ???????????? ?????????? ??????? ??? ???????? ???????? ???????. ??????? ????????? ?????????? ??????????? ? ????????????? ????????? ? ???????? ?????? ? ?????????????? ?????????? ??????????????? ?????. ?????????? ?????? ??????? ???????? ?? ????????? ????????????, ??????????? ?? ?????? ????? ?????????...|$|E
40|$|Abstract—Random coding of {{a channel}} with an erasure option is studied. By {{analyzing}} the large deviations {{behavior of the}} code ensemble, we obtain exact single-letter formulas for the error exponents in lieu of Forney’s lower bounds. The analysis technique we use {{is based on an}} enhancement and specialization of tools for assessing the moments of certain distance enumerators, that were recently used for determining the exponential behavior of other communication systems. We specialize our results to the binary symmetric channel case with uniform <b>random</b> <b>coding</b> distribution and derive an explicit expression for the error exponent which, unlike Forney’s bounds, does not involve optimization over two parameters. We also establish the fact that for the binary symmetric channel case with uniform <b>random</b> <b>coding</b> distribution, the difference between the exact error exponent corresponding to the probability of undetected decoding error and the error exponent corresponding to the erasure event is equal to the threshold parameter. Numerical calculations for the binary symmetric channel with uniform <b>random</b> <b>coding</b> distribution indicate that in this case Forney’s bound coincides with the exact <b>random</b> <b>coding</b> exponent. I...|$|E
40|$|We {{study the}} {{capacity}} region C_L of an arbitrarily varying multiple-access channel (AVMAC) for deterministic codes with decoding into {{a list of}} a fixed size L and for the average error probability criterion. Motivated by known results {{in the study of}} fixed size list decoding for a point-to-point arbitrarily varying channel, we define for every AVMAC whose capacity region for <b>random</b> <b>codes</b> has a nonempty interior, a nonnegative integer Ω called its symmetrizability. It is shown that for every L ≤Ω, C_L has an empty interior, and for every L ≥ (Ω+ 1) ^ 2, C_L equals the nondegenerate capacity region of the AVMAC for <b>random</b> <b>codes</b> with a known single-letter characterization. For a binary AVMAC with a nondegenerate <b>random</b> <b>code</b> capacity region, it is shown that the symmetrizability is always finite. Comment: Accepted to the IEEE Transactions on Information Theory, January 201...|$|R
50|$|When the {{original}} worm tried to infect other computers at <b>random,</b> <b>Code</b> Red II tried to infect machines {{on the same}} subnet as the infected machine.|$|R
40|$|Abstract—In {{this paper}} we analyze {{a class of}} {{systematic}} fountain/rateless codes constructed using Bernoulli(1 / 2) random variables. Using simple bounds we then show that this class of codes stochastically minimizes the number of coded packets receptions needed to successfully decode all the information packets. This optimality holds over a large class of <b>random</b> <b>codes</b> that includes Bernoulli(q) <b>random</b> <b>codes</b> with q ≤ 1 / 2 and LT codes. We then conclude by demonstrating asymptotic optimality for intermediate decoding of the same codes. I...|$|R
40|$|An {{improved}} pre-factor for the <b>random</b> <b>coding</b> bound is proved. Specifically, for channels with {{critical rate}} not equal to capacity, if a regularity condition is satisfied (resp. not satisfied), then for any ϵ > 0 a pre-factor of O(N^- 1 / 2 (1 - ϵ + ρ̅^∗_R)) (resp. O(N^- 1 / 2)) is achievable for rates above the critical rate, where N and R is the blocklength and rate, respectively. The extra term ρ̅^∗_R {{is related to}} the slope of the <b>random</b> <b>coding</b> exponent. Further, the relation of these bounds with the authors' recent refinement of the sphere-packing bound, as well as the pre-factor for the <b>random</b> <b>coding</b> bound below the critical rate, is discussed. Comment: Submitted to IEEE Trans. Inform. Theor...|$|E
40|$|A multiple-descriptions (MD) coding {{strategy}} is proposed and an inner {{bound to the}} achievable rate-distortion region is derived. The scheme utilizes linear codes. It is shown in two different MD set-ups that the linear coding scheme achieves a larger rate-distortion region than previously known <b>random</b> <b>coding</b> strategies. Furthermore, it is shown via an example that the best known <b>random</b> <b>coding</b> scheme for the set-up can be improved by including additional randomly generated codebooks...|$|E
3000|$|We use a <b>random</b> <b>coding</b> {{argument}} {{to establish the}} existence of a code with rates given by (46) such that [...]...|$|E
40|$|Abstract — <b>Random</b> {{linear network}} <b>coding</b> is a {{particularly}} decentralized approach to the multicast problem. Use of <b>random</b> network <b>codes</b> introduces a non-zero probability however that some sinks {{will not be able}} to successfully decode the required sources. One of the main theoretical motivations for <b>random</b> network <b>codes</b> stems from the lower bound on the probability of successful decoding reported by Ho et. al. (2003). This result demonstrates that all sinks in a linearly solvable network can successfully decode all sources provided that the <b>random</b> <b>code</b> field size is large enough. This paper develops a new bound on the probability of successful decoding. I...|$|R
50|$|To {{create new}} codes, it is {{possible}} to enter <b>random</b> <b>codes</b> into a Game Genie. This evolutionary approach is equivalent to using random POKE operations. Usually, entering <b>random</b> <b>codes</b> will result in no noticeable change in the game or freezing the game and possibly corrupting save data, but a useful difference may appear in the game if this process is repeated many times. One must write down the <b>random</b> <b>codes</b> for each attempt because there is no method to view the codes after starting the game. Once a useful code is discovered, making slight modifications to this code has a much higher probability of producing additional useful codes. With ROM files, emulators, and decompilers for these games and systems, it has become possible to reverse engineer games to find specific ROM data to modify. This information can be directly converted into Game Genie codes.|$|R
40|$|International audienceIn this paper, we rst recall {{some basic}} facts about rank metric. We then derive an {{asymptotic}} {{equivalent of the}} minimum rank distance of codes that reach the rank metric GilbertVarshamov bound. We then derive an asymptotic equivalent of the average minimum rank distance of <b>random</b> <b>codes.</b> We show that <b>random</b> <b>codes</b> reach GV bound. Finally, we show that optimal codes in rank metric have a packing density which is bounded by functions depending only on the base eld and the minimum distance and show the potential interest in cryptographic applications...|$|R
