10|95|Public
5000|$|For {{effective}} implementations of Tamil Nadu Water <b>Resources</b> <b>consolidation</b> Project and {{to achieve}} functional specialization, the Public Works Department has been bifurcated as Water Resources Organisation and Building Organisation on 01.10.95 ...|$|E
5000|$|An Integrated Management Plan was {{implemented}} with financial support of Rs 570 million (US$12.7 million) out of [...] "special problem grants" [...] {{recommended by the}} Finance Commissions. Hydrobiological monitoring was supported under the Odisha Water <b>Resources</b> <b>Consolidation</b> Project of the World Bank, {{to the extent of}} Rs 10 million (US$220,000). A strong support network was created with 7 state government organizations, 33 NGOs, 3 national government ministries, 6 other organizations, 11 international organizations, 13 research institutions and 55 different categories of community groups.|$|E
5000|$|The lake {{is formed}} below the Palani hill ranges at {{the origin of}} the Varahanadhi (Varaha River) which is used for {{purposes}} of water supply and irrigation.As a part of the World Bank funded Water <b>Resources</b> <b>Consolidation</b> Project (WRCP) of Tamil Nadu, sedimentation condition was assessed for a large number of reservoirs including Berijam reservoir.This indicated that the [...] gross storage capacity of the reservoir in 1911 decreased to [...] in 1987.The percentage reduction of storage due to siltation is 23%, over a period of 76 years.|$|E
30|$|In this section, we {{demonstrate}} {{both the}} necessity {{and benefits of}} <b>resource</b> <b>consolidation</b> in a map service system.|$|R
40|$|Cloud Computing {{offers a}} variety of {{benefits}} including cost-saving, agility, efficiency, <b>resource</b> <b>consolidation,</b> business opportunities and green IT. Cloud Computing is used to demonstrate scientific research challenges, and provides alternative solutions for computing-intensive simulations with ease, which include tsunami and seismic simulations to study impacts to Japan...|$|R
5000|$|Protection of the {{population}} and <b>resources</b> and <b>consolidation</b> of territorial control.|$|R
50|$|In 1981, Chilika Lake was {{designated}} the first Indian {{wetland of international importance}} under the Ramsar Convention {{due to its}} rich biodiversity. Over a million migratory waterfowl and shorebirds winter here including many rare and endangered species. The lake is of great value in preserving genetic diversity and over 400 vertebrate species have been recorded. However conflicts have arisen over the ecosystem of the lake such as Siltation, and disagreements between fisherman, resulting in an overall loss of biodiversity. As a result, the Odisha State Government with support from the Government of India adopt adaptive conservation and management actions. In 1992, the Government of Odisha, concerned by the degradation of the lake's ecosystem, established the Chilika Development Authority (CDA) for the restoration and overall development of the lake under the Indian Societies Registration Act. An Integrated Management Plan was later implemented with financial support of Rs570 million (US $12.7 million) and Hydrobiological monitoring was supported under the Odisha Water <b>Resources</b> <b>Consolidation</b> Project of the World Bank, to the extent of Rs10 million (US $220,000). A strong support network was created with 7 state government organisations, 33 NGOs, 3 National Government Ministries, 6 other organisations, 11 International organisations, 13 research institutions and 55 different categories of community groups established good international contacts for protection in the area.|$|E
40|$|Abstract. Along {{with the}} network {{technology}} {{in the application of}} modern education, building network teaching environment of marine diesel engine become an inevitable trend. The network teaching environment design from the teaching goal analysis in the first place, secondly the network learners were analyzed in three aspects, the network information <b>resources</b> <b>consolidation,</b> analysis and application of teaching strategies, then the design of the whole teaching process, and finally the evaluation on teaching effect...|$|E
40|$|AbstractBased on {{the basic}} {{principle}} of management entropy theory, this paper constructs a metric scale and entropy-based quantization model for evaluating the complexity of resource consolidation system engineering. Under the transformed structure of coal resource consolidation system of Shanxi Province and the constructed relation mode, the proposed model is put in practice to analyze and evaluate the complexity of Shanxi coal <b>resources</b> <b>consolidation</b> system. The results show that by importing material, energy and information, the complex system evolves to the direction of order and dynamic balance...|$|E
40|$|Abstract—Virtualization {{has become}} a very {{important}} technology which has been adopted in many enterprise computing systems and data centers. Virtualization makes resource management and maintenance easier, and can decrease energy consumption through <b>resource</b> <b>consolidation.</b> To develop and employ sophisticated resource management, accurate power and performance models of the hardware resources in a virtualized environment are needed. Based on extensive experiments and measurements, this paper presents accurate power and performance models for a high performance multi-core server system with virtualization. Keywords- modeling; energy efficiency, virtulization, <b>consolidation,</b> <b>resource</b> sharing, scheduling. I...|$|R
50|$|Clusters are {{primarily}} designed with performance in mind, but installations {{are based on}} many other factors; fault tolerance (the ability for a system to continue working with a malfunctioning node) also allows for simpler scalability, and in high performance situations, low frequency of maintenance routines, <b>resource</b> <b>consolidation,</b> and centralized management. Other advantages include enabling data recovery {{in the event of}} a disaster and providing parallel data processing and high processing capacity.|$|R
40|$|Virtualization {{is often}} used {{as a tool for}} <b>resource</b> <b>consolidation</b> in the server market. Virtualization is also used to simplify {{management}} tasks and provide high availability. However, the ultimate high availability feature, fault-tolerance, has been limited to special and costly hardware and software. This thesis will give an overview of how one can use virtualization tech- nologies to build a fault tolerant system, and show what would be the cost, in the sense of performance degradation when compared to a non-fault-tolerant system. ...|$|R
40|$|It was {{a simple}} question. A {{community}} leader of a small rural town in Maine asked, “How can the humanities help us with challenges like outmigration, strained <b>resources,</b> <b>consolidation,</b> and loss of identity?” Answering the question {{may not be so}} simple, but striving to do so is essential. Maine is considered the nation’s most rural state with close to two-thirds of its population of 1. 3 million living in rural areas. 1 Rural living can be immensely satisfying, but it would not be an exaggeration to say that many of Maine’s rural communities face a slow erosion of a way of life, their knowledge and wisdom, and sustainable means to build wealth. Parts of Maine share what other rural areas in North America face: stagnant economy, dwindling population, inability to retain and attract youth and talent, and diminishing resources. Ad...|$|E
40|$|Chagas {{disease was}} an {{important}} medical and social problem in almost all of Latin America throughout the twentieth century. It has been combated over a broad swath of this continent over recent decades, with very satisfactory results in terms of vector and transfusional transmission. Today, a surveillance stage still remains to be consolidated, in parallel with appropriate care required for some millions of infected individuals who are today living in endemic and non-endemic areas. Contradictorily, the good results attained have generated excessive optimism and even disregard among health authorities, in relation to this disease and its control. The loss of visibility and priority may be a logical consequence, particularly in Latin American healthcare systems that are still disorganized and overburdened due to insufficiencies of financial and human <b>resources.</b> <b>Consolidation</b> of the victories against Chagas disease is attainable but depends on political will and continual attention from the most consequential protagonists in this struggle, especially the Latin American scientific community...|$|E
40|$|The cloud {{virtualization}} technology {{improves the}} economy of scale for data centers through server consolidation, application consolidation and <b>resources</b> <b>consolidation.</b> Virtualization allows the provider to move Virtual Images from more congested host to less-congested hosts, as required. Enterprises also get improved server reliability, which in turn increases application performance. Despite these benefits, it includes major security challenges with the portability of Virtual Images between different cloud providers. The security and integrity of Virtual images is {{the foundation for the}} overall security of the cloud. Many of the Virtual images are intended to be shared by diverse and unrelated users. Unfortunately, existing approaches to cloud security built by cloud practitioners fall short when dealing with Virtual images. Secure transmission of virtual Images can bepossible by providing authentication using Blind Authentication protocol (BAP). The proposed approach authenticates the allocation of virtual images using Blind authentication protocol. It provides provable protection against replay and client side attacks even if the keys of the user are compromised. The encryption also provides template protection, revocability and alleviates the concerns on privacy in widespread use of biometrics. Carrying out the authentication in the encrypted domain is a secure process, while the encryption key acts as an additional layer of security...|$|E
40|$|Waste {{of energy}} due to over-provisioning and overdimensioning of network {{infrastructures}} has recently stimulated {{the interest on}} energy consumption reduction by Internet Service Providers (ISPs). By means of <b>resource</b> <b>consolidation,</b> network virtualization based architectures will enable energy saving. In this letter, we extend the well-known virtual network embedding problem (VNE) to energy awareness and propose a mixed integer program (MIP) which provides optimal energy efficient embeddings. Simulation results show the energy gains of the proposed MIP over the existing cost-based VNE approach. Peer ReviewedPostprint (published version...|$|R
40|$|Click on the DOI link {{to access}} the article (may not be free). Server {{virtualization}} has enabled <b>resource</b> <b>consolidation</b> and has minimized {{the need for additional}} and expensive hardware. Server virtualization has been widely deployed in a lot of organizations, because of the attractive benefits it offers like minimal hardware, cost effective <b>resource</b> <b>consolidation,</b> easier management and maintenance of hardware resources and so on. Many cloud service providers have offloaded processing capabilities to virtual servers in their data centers instead of using multiple physical servers which are very expensive these days. When such a virtualized server setup is connected to disk arrays, latency of data access becomes prominent. In order to minimize the response time of application requests on the virtual servers and make efficient utilization of cache, a dynamic caching mechanism is proposed by the authors on the hypervisor. The caching mechanism proposed in the research is different, in the sense that, it is an isolated, unshared and dynamic cache, where caching is done on the hypervisor instead of the virtual machines. The dynamic cache allocation algorithm enables thin provisioning of the hypervisor cache and leads to efficient cache utilization. Experimental results obtained using the thin provisioning algorithm look promising in terms of reducing the response time and efficiently utilizing the available cache...|$|R
30|$|Most of the {{existing}} energy-efficient strategies usually search the subset of resources in the whole SNs for the VNs. Whereas, <b>resource</b> <b>consolidation</b> achieves the minimization of energy consumption by switching off or hibernating as many physical infrastructures as possible like physical servers and fiber optic links. However, this process {{may lead to the}} hotspots among SNs. Thus, it can result in a sharp decrease in request acceptance rate. Here, these objectives are conflicting in nature. Therefore, comprehensive consideration of these conflicting objectives has become a critical problem that requires immediate attention for resolving it.|$|R
40|$|Web {{applications}} {{are getting more}} and more complicated with the extensive growth of the Internet. In order to cope with user demands, that are constantly increasing, a specialattention should be paid to performance optimizations. While a lot of attention is devoted to back-end optimization, front-end is often overlooked and therefore is a fertileground for performance bottlenecks. This thesis is destined to investigate a set of well-established front-end optimization techniques in order to find out those, that are the most efficient. The thesis primarily focuses on an examination of a limited set of techniques, that can be applied to static web resources. Some of the techniques are: <b>resources</b> <b>consolidation,</b> minification, compression and caching. The measurements used during the examination are based on four metrics, such as the Page Size, the Page Load Time, the Page Start Render Time and the Number of Requests the page made. The results show which methods impact performance most. In particular, the results revealed, that the resource compression technique alone brings significant performance improvements, the page size was reduced by 79 % and the page load time by 72 %, respectively. Despite that, it is evident that the best results can be achieved by a combination of different techniques. All optimization techniques combined made a serious difference, helping us reduce the page load time from 24 seconds down to just one second...|$|E
40|$|Cloud Computing (CC) has {{transformed}} the way many organisations work. It {{offers a variety of}} benefits including cost-saving, agility, efficiency, <b>resource</b> <b>consolidation,</b> business opportunities and green IT. This brings technical and business challenges in many organisations. To address increasing requirements from Industry and Academia, there are two major areas need to be addressed: (i) the organisational sustainability and measurement of Return on Investment (ROI); and (ii) enterprise portability, so that the entire applications and services can moved from desktops to clouds, and between different clouds, with ease and convenience...|$|R
40|$|AbstractOne of the {{greatest}} challenges in computational chemistry is the design of enzymes to catalyze non-natural chemical reactions. We focus on harnessing the distributed parallel computational power of the Grid to automate the inside-out process of enzyme design using scientific workflow systems. This paper presents a scientific workflow based approach to facilitate the inside-out enzyme design process in the Grid execution environment by providing features such as <b>resource</b> <b>consolidation,</b> task parallelism, provenance tracking, fault tolerance and workflow reuse, which results in an automated, pipelined, efficient, extensible, stable, and easy-to-use computational process for enzyme design...|$|R
5000|$|Web browsers open {{separate}} Transmission Control Protocol (TCP) connections {{for each}} Hypertext Transfer Protocol (HTTP) request submitted when downloading a web page. These requests total {{the number of}} page elements required for download. However, a browser is limited to opening only {{a certain number of}} simultaneous connections to a single host. To prevent bottlenecks, the number of individual page elements are reduced using <b>resource</b> <b>consolidation</b> whereby smaller files (such as images) are bundled together into one file. This reduces HTTP requests and the number of [...] "round trips" [...] required to load a web page.|$|R
40|$|This paper uses a unique, {{longitudinal}} data set of UK and German new technology-based firms (NTBFs) {{to investigate the}} determinants of internationalization and firm survival. Specifically, it tests the influence of absorptive capacity, inter-firm specific relationships and international exposure on survival. Its key findings are that high absorptive capacity increases survival probabilities; specific customer-supplier relationships enhance survival; and the greater the firm's exposure to internationalization activity, the higher its subsequent chance of survival. Thus, the paper provides evidence that young firms {{are more likely to}} survive when they pursue an internationalization strategy based on <b>resource</b> <b>consolidation.</b> © The Author(s) 2010...|$|R
50|$|FEO {{focuses on}} {{changing}} the actual content, where as DSA focuses on improving content delivery without touching content (i.e. DSA has verbatim delivery of content). DSA focuses on optimizing bit delivery across the network, without changing the content. FEO aims {{to decrease the}} number of objects required to download websites, and to decrease {{the total amount of}} traffic. This can be done by device-aware content serving (e.g. dropping the quality of images), minification, <b>resource</b> <b>consolidation</b> and inlining Because FEO changes the actual traffic, configuration tends to be more difficult, as there is a risk of affecting the user-experience, by serving content that was incorrectly changed.|$|R
40|$|International audienceThis paper {{addresses}} {{energy efficient}} VNF placement and chaining over NFV enabled infrastructures. VNF placement and chaining are formulated as a decision tree search {{to overcome this}} NP-Hard problem complexity. The proposed approach {{is an extension of}} the Monte Carlo Tree Search (MCTS) method to achieve energy savings using physical <b>resource</b> <b>consolidation</b> and sharing VNFs between multiple tenants. A real cloud testbed and extensive simulations are used to assess performance and ability to scale with problem size. Evaluation results show significant reduction in energy consumption of the proposed placement solution compared to related work. The polynomial complexity of our proposal is highlighted by the simulation result...|$|R
40|$|Cloud {{computing}} {{has emerged}} as a flexible and efficient paradigm to provide IT resources on-demand. However, it has also raised new challenges for infrastructure providers to manage large-scale deployments in an efficient and effective way. In this paper, we present the trade-off between energy consumption and performance. We outline a novel framework for efficient and effective <b>resource</b> <b>consolidation</b> in data centers, building on latest trends in software development practice and recent standards for energy efficiency. In particular, we consider the usage of code annotations from software developers and the adoption of a "green abstraction layer" to model the trade-off between performance and energy consumption...|$|R
40|$|This paper {{explores the}} {{background}} of a proposed revision to the Mineral Resources Law of China, why and how the law was amended in the past, its salient features and objectives. Of equal importance is an analysis of how this national law, with its attendant regulations and policies, {{formed the basis for}} the growth and continued development of China's small-scale mining industry. The Xiaoqingling Gold Mountain case study is shown to justify the necessity and feasibility for formalizing and consolidating small-scale mines in China, and to some extent, the success of the nation-wide ASM <b>resource</b> <b>consolidation</b> policy at a local level. © 2009 Elsevier Ltd. All rights reserved...|$|R
30|$|Green cloud {{computing}}: Green {{cloud computing}} demands divergence from conventional computing techniques, hence, increased operational and infrastructural costs. For example, renewable energy {{has a higher}} cost than conventional grid energy. Similarly, waste heat utilization measures in data centers also demand costly thermal heat exchange materials. Incorporating green measures with cost-efficient business operations is a challenging task in cloud data centers. The efficiency of renewable energy generation and storage mediums needs to be rigorously increased {{in order to provide}} comparable business incentives. The cost of VM migrations for <b>resource</b> <b>consolidation</b> over long-haul networks is also a highly debated research issue [14, 65]. Moreover, government policies need to be devised that provide incentives to green cloud computing business providers and users.|$|R
40|$|Managed hosting and {{enterprise}} wide <b>resource</b> <b>consolidation</b> {{trends are}} increasingly leading to sharing of storage resources across multiple classes, corresponding to di#erent applications/customers, {{each with a}} possibly di#erent Quality of Service (QoS) requirement. To enable a storage system to meet diverse QoS requirements, we present two algorithms for dynamically allocating cache space among multiple classes of workloads. Our algorithms dynamically adapt the cache space allocated to each class {{in response to the}} observed response time, the temporal locality of reference, and the arrival pattern for each class. Using trace driven simulations collected from large storage system installations, we experimentally demonstrate that the algorithms not only meet the QoS requirements, but also increase the throughput by achieving a higher hit rate whenever feasible...|$|R
40|$|Abstract—From specific-purpose storage {{appliances}} to the {{new generation}} of cloud storage services, the industry has different solutions for the data storage needs of home and small office users, whereas for backup, <b>resource</b> <b>consolidation</b> or file sharing purposes. Although dedicated appliances and cloud services offer a different balance between capacity, accessibility, and cost, the two paradigms are complementary, rather than incompatible. This paper proposes an operator-managed hybrid storage service which explores the existing complementarity between the appliance-based and the cloud-based storage service models. By making use of home gateways/routers as storage service appliances, it aims to provide an integrated solution eliminating the need for dedicated storage devices. The proposed approach fully integrates with the prevailing management frameworks already used by broadband access network operators, namely th...|$|R
40|$|A {{data center}} is a {{facility}} {{with a group of}} networked servers used by an organization for storage, management and dissemination of its data. The increase in data center energy consumption {{over the past several years}} is staggering, therefore efforts are being initiated to achieve energy efficiency of various components of data centers. One of the main reasons data centers have high energy inefficiency is largely due to the fact that most organizations run their data centers at full capacity 24 / 7. This results into a number of servers and switches being underutilized or even unutilized, yet working and consuming electricity around the clock. In this paper, we present Adaptive TrimTree; a mechanism that employs a combination of <b>resource</b> <b>consolidation,</b> selective connectedness and energy proportional computing for optimizing energy consumption in a Data Center Network (DCN). Adaptive TrimTree adopts a simple traffic-and-topology-based heuristic to find a minimum power network subset called ‘active network subset’ that satisfies the existing network traffic conditions while switching off the residual unused network components. A ‘passive network subset’ is also identified for redundancy which consists of links and switches that can be required in future and this subset is toggled to sleep state. An energy proportional computing technique is applied to the active network subset for adapting link data rates to workload thus maximizing energy optimization. We have compared our proposed mechanism with fat-tree topology and ElasticTree; a scheme based on <b>resource</b> <b>consolidation.</b> Our simulation results show that our mechanism saves 50 %– 70 % more energy as compared to fat-tree and 19. 6 % as compared to ElasticTree, with minimal impact on packet loss percentage and delay. Additionally, our mechanism copes better with traffic anomalies and surges due to passive network provision...|$|R
40|$|To meet an ever-growing {{demand for}} {{advanced}} multimedia services {{and to support}} electronic connectivity anywhere on the planet, development of ubiquitous broadband multimedia systems is gaining a huge interest at both academic and industry levels. Satellite networks in general and LEO satellite constellations in particular will play {{an essential role in}} the deployment of such systems. Therefore, as LEO satellite constellations like Iridium or IridiumNEXT are extremely expensive to deploy and maintain, extending their service lifetimes is of crucial importance. In the main part of this thesis, we propose different techniques for extending satellite service life in LEO satellite constellations. Satellites in such constellations can spend over 30 % of their time under the earth’s umbra, time during which they are powered by batteries. While the batteries are recharged by solar energy, the Depth of Discharge (DoD) they reach during eclipse significantly affects their lifetime – and by extension, the service life of the satellites themselves. For batteries of the type that power Iridium and Iridium-NEXT satellites, a 15 % increase to the DoD can practically cut their service lives in half. We first focus on routing and propose two new routing metrics – LASER and SLIM – that try {{to strike a balance between}} performance and battery DoD in LEO satellite constellations. Our basic approach is to leverage the deterministic movement of satellites for favoring routing traffic over satellites exposed to the sun as opposed to the eclipsed satellites, thereby decreasing the average battery DoD– all without taking a significant penalty in performance. Then, we deal with <b>resource</b> <b>consolidation</b> – a new paradigm for the reduction of the power consumption. It consists in having a carefully selected subset of network links entering a sleep state, and use the rest to transport the required amount of traffic. This possible without causing major disruptions to network activities. Since communication networks are designed over the peak traffic periods, and with redundancy and over-provisioned in mind. As a remedy to these issues, we propose two different methods to perform <b>resource</b> <b>consolidation</b> in LEO networks. First, we propose trafficaware metric for quantifiying the quality of a frugal topology, the Maximum Link Utilization (MLU). With the problem being NP-hard subject to a given MLU threshold, we introduce two heuristics, BASIC and SNAP, which represent different tradeoffs in terms of performance and simplicity. Second, we propose a new lightweight traffic-agnostic metric for quantifiying the quality of a frugal topology, the Adequacy Index (ADI). After showing that the problem of minimizing the power consumption of a LEO network subject to a given ADI threshold is NP-hard, we propose a heuristc named AvOId to solve it. We evaluate both forms of <b>resource</b> <b>consolidation</b> using realistic LEO topologies and traffic requests. The results show that the simple schemes we develop are almost double the satellite batteries lifetime. Following the green networking in LEO systems, the second part of this thesis focuses on extending the <b>resource</b> <b>consolidation</b> schemes to current wired networks. Indeed, the energy consumption of wired networks has been traditionally overlooked. Several studies exhibit that the traffic load of the routers only has a small influence on their energy consumption. Hence, the power consumption in networks is strongly related to the number of active network elements. In this context, we extend the traffic-agnostic metric, ADI, to the wired networks. We model the problem subject to ADI threshold as NP-hard. Then, we propose two polynomial time heuristics – ABStAIn and CuTBAck. Although ABStAIn and CuTBAck are traffic unaware, we assess their behavior under real traffic loads from 3 networks, demonstrating that their performance are comparable to the more complex traffic-aware solutions proposed in the literature...|$|R
40|$|Abstract—Live {{migration}} {{is a widely}} used technique for <b>resource</b> <b>consolidation</b> and fault tolerance. KVM and Xen use iterative pre-copy approaches which work well in practice for commercial applications. In this paper, we study pre-copy live migration of MPI and OpenMP scientific applications running on KVM and present a detailed performance analysis of the migration process. We show that due to {{a high rate of}} memory changes, the current KVM rate control and target downtime heuristics do not cope well with HPC applications: statically choosing rate limits and downtimes is infeasible and current mechanisms sometimes provide sub-optimal performance. We present a novel on-line algorithm able to provide minimal downtime and minimal impact on end-to-end application performance. At the core of this algorithm is controlling migration based on the application memory rate of change. I...|$|R
40|$|Abstract e-Science {{has been}} greatly {{enhanced}} from the developing capability and usability of cyberinfrastructure. This chapter explains how scientific workflow systems can facilitate e-Science discovery in Grid environments by providing features including scientific process automation, <b>resource</b> <b>consolidation,</b> parallelism, provenance tracking, fault tolerance, and workflow reuse. We first overview the core services to support e-Science discovery. To demonstrate how these services can be seamlessly assembled, an open source scientific workflow system, called Kepler, is integrated into the University of California Grid. This architecture is being applied to a computational enzyme design process, which is a formidable and collaborative problem in computational chemistry that challenges our knowledge of protein chemistry. Our implementation and experiments validate how the Kepler workflow system can make the scientific computation process automated, pipe-lined, efficient, extensible, stable, and easy-to-use. 13. ...|$|R
40|$|This paper {{discusses}} {{the evolution of}} the concept of public service at land grant universities, pointing out the challenges confronted by these universities today and tomorrow and current challenges experienced in the areas of leadership and volunteer development. First, {{the evolution of the}} concept of public service at land grant institutions is traced from the 1862 Morrill Act establishing these schools through a 1985 survey of the National Association of State Universities of State Universities and Land Grant Colleges that showed 100 respondents agreeing on a single definition of "public service " (later cited by P. H. Crosson at a conference held in November, 1988). Specifically, the paper identifies challenges in the areas of: global interdependence and change, <b>resource</b> <b>consolidation,</b> the re-definition and re-commitment of ethics and values, the need for cooperative energy an...|$|R
30|$|The {{software}} and virtualization techniques {{have led to}} current advancements in the energy efficiency of networking technologies. Software Defined Networks (SDN) separate the data and control plane of network routers {{with the help of}} a central controller. SDN do not have a direct impact on the energy consumption of a network. However, the pervasive programmable interface of SDN supports energy efficient network operations indirectly through <b>resource</b> <b>consolidation</b> [56]. A minimum energy efficient subset of network resources can be calculated through a resource optimization technique and implemented through SDN as demonstrated in [57]. Hence, server and network resource management techniques can be utilized in parallel with the virtualization and SDN enabling technologies. SDN can help implement green computing policies at the network level based on their programmable control plane. Similarly, security policies can be implemented with the help of SDN while eliminating the need for stand-alone security devices. Consequently, SDN-enabled network devices can also implement security functions, lowering the total operational costs and energy bill [58].|$|R
