0|36|Public
50|$|The terms <b>Remote</b> <b>Batch,</b> <b>Remote</b> Job System and Remote Job Processing {{are also}} used for RJE facilities.|$|R
50|$|<b>Remote</b> <b>Batch</b> Processing (RBP) was a {{capability}} {{that existed}} in VS/9, although it was never completely exploited, probably due to limited demand. RBP allowed remote users to submit batch jobs for execution on the mainframe and receive the results back at their offsite printer. Typically, a <b>remote</b> <b>batch</b> device consisted of a card reader and a printer connected to a communication line which interfaced with the <b>remote</b> <b>batch</b> services in the operating system. Like a local batch job, operators could receive requests for tape or disk mounts/dismounts and program prompts for responses to questions.|$|R
50|$|Like CP-V, CP-6 {{had five}} access modes, which {{operated}} concurrently: <b>batch</b> processing, <b>remote</b> <b>batch,</b> timesharing, transaction processing, and real-time processing. It included multiprogramming and operated on multiple CPUs.|$|R
50|$|In 1971, Barr also {{implemented}} the HASP workstation for M & M Computer Industries, Orange, California. Implemented on the Data General Nova minicomputer, the program became the Singer Corporation <b>Remote</b> <b>Batch</b> Terminal. Both Singer and UCC sold their terminal divisions to Harris Corporation, which continued {{to market the}} products.|$|R
50|$|A {{terminal}} concentrator {{was also}} developed that year, since time-sharing {{was still a}} prevalent mode of computer use. In 1975, the network shrank slightly due to budgetary constraints, but the setback was only temporary. At that point, the network provided <b>remote</b> login, <b>remote</b> <b>batch</b> and file transfer user application services.|$|R
40|$|Abstract. Because Remote Procedure Calls do not compose efficiently, {{designers}} of distributed object systems use Data Transfer and Remote Façade patterns to create large-granularity interfaces, hard-coded for particular client use cases. As {{an alternative to}} RPC-based distributed objects, this paper presents <b>Remote</b> <b>Batch</b> Invocation (RBI), language support for explicit client-defined <b>batches.</b> A <b>Remote</b> <b>Batch</b> statement combines <b>remote</b> and local execution: all the remote code is executed in a single round-trip to the server, where all data sent to the server and results from the batch are communicated in bulk. RBI supports remote blocks, iteration and conditionals, and local handling of remote exceptions. RBI is efficient even for fine-grained interfaces, {{eliminating the need for}} hand-optimized server interfaces. We demonstrate RBI with an extension to Java, using RMI internally as the transport layer. RBI supports large-granularity, stateless server interactions, characteristic of service-oriented computing. ...|$|R
40|$|The {{workload}} of the CERN central {{computer system}} is dominated, {{in terms of}} number of jobs, by short jobs submitted from, terminals and <b>remote</b> <b>batch</b> stations. The arrival of the CDC 7600, which {{will be even more}} easily accessible and will give the user his results more quickly, is likely to accentuate this domination...|$|R
40|$|Abstract. When {{building}} distributed object systems, designers use Data Transfer and Remote Façade {{patterns to}} create large-granularity interfaces. They must hard-code support for particular client use cases because Remote Procedure Calls (RPC) do not compose efficiently. As {{an alternative to}} RPC-based distributed objects, this paper presents <b>Remote</b> <b>Batch</b> Invocation (RBI), language support for explicit client-defined <b>batches.</b> A <b>Remote</b> <b>Batch</b> statement combines <b>remote</b> and local execution: all the remote code is executed in a single round-trip to the server, where all data sent to the server and results from the batch are communicated in bulk. RBI supports remote blocks, iteration and conditionals, and local handling of remote exceptions. RBI is efficient even for fine-grained interfaces, {{eliminating the need for}} hand-optimized server interfaces. We demonstrate RBI with an extension to Java, using RMI internally as the transport layer. RBI supports largegranularity, stateless server interactions, characteristic of service-oriented computing. ...|$|R
5000|$|<b>Remote</b> <b>batch</b> {{rendering}} enables resource-intensive graphics processing {{to be done}} {{using either}} public or private cloud resources, or a hybrid combination of the two. This approach is particularly valuable for on-demand usages that may have large peak workloads, {{such as in the}} final stages of production for animated films. Render farms operated by Pixar and LucasFilm are established examples of this usage model.|$|R
40|$|The {{fundamental}} {{ideas of}} distributed objects have changed {{little in the}} last 20 years. Existing languages are retrofitted with transparent distribution mechanisms based on proxies. Experiments with mobile code demonstrate its power but have little impact on practice. The problems with transparency and mobile code have been well known since at least 1994. But {{in the absence of}} any fundamental new ideas, the same problematic approaches are used, for example in the design of Java RMI. In this essay we discuss a new programming construct called <b>Remote</b> <b>Batch</b> Invocation (RBI). A batch is a code block that combines remote and local execution over fine-grained object interfaces, but is executed by partitioning and <b>remote</b> evaluation. <b>Remote</b> <b>Batch</b> Invocation effectively addresses the shortcomings of transparent distribution with a controlled form of mobile code. Our experience leads us to believe that distribution cannot be implemented as a library, but requires specific language support. Viewing distribution as a language design problem represents a revolutionary step in the development of distributed objects...|$|R
50|$|The {{unofficial}} MACE {{software was}} later chosen over the official SCOPE product {{as the basis}} of the next CDC operating system, Kronos, named after the Greek god of time. The main marketing reason for its adoption was the development of its TELEX time sharing feature and its BATCHIO <b>remote</b> <b>batch</b> feature. Kronos continued to use the COS/SCOPE 1 file system {{with the addition of a}} permanent file feature.|$|R
50|$|The Sigma 9 was {{announced}} in 1970 and the first delivery was made in 1971. There were 3 models built, the Sigma 9, the Sigma 9 Model 2 and the Sigma 9 Model 3. The original was {{the most powerful and}} was universally applicable to all data processing applications at the time. The Model 2 was able to process in multi-programmed <b>batch,</b> <b>remote</b> <b>batch,</b> conversational time-sharing, real-time, and transaction processing modes. The Model 3 was designed for the scientific real-time community.|$|R
5000|$|Operating system {{development}} then {{split into two}} camps. The CDC-sanctioned evolution of COS was undertaken at the Sunnyvale, California software development lab. Many customers eventually took delivery of their systems with this software, then known as SCOPE (Supervisory Control Of Program Execution). SCOPE version 1 was, essentially, dis-assembled COS; SCOPE version 2 included new device and file system support; SCOPE version 3 included permanent file support, EI/200 <b>remote</b> <b>batch</b> support, and INTERCOM time sharing support. SCOPE always had significant reliability and maintainability issues.|$|R
50|$|MTS can {{and does}} use {{communication}} controllers {{such as the}} IBM 2703 and the Memorex 1270 to support dial-in terminals and <b>remote</b> <b>batch</b> stations over dial-in and dedicated data circuits, but these controllers proved to be fairly inflexible and unsatisfactory for connecting large numbers of diverse terminals and later personal computers running terminal emulation software at ever higher data rates. Most MTS sites choose {{to build their own}} front-end processors or to use a front-end processor developed by one of the other MTS sites to provide terminal support.|$|R
50|$|The 1700 series found use as {{communications}} concentrators, Digigraphics workstations, <b>remote</b> <b>batch</b> {{job entry}} stations, and industrial process controllers. One application, running the AUTRAN program, controlled {{water and wastewater}} treatment plants for many years. Another was used as Maintenance and Diagnostic SubSystem (M&DSS) for the AN/FPQ-16 Perimeter Acquisition Radar Attack Characterization System (PARCS), located at Cavalier Air Force Station (CAFS) in North Dakota; this CDC 1700 is still being used {{as of this writing}} (2016). The system was also used by Ticketron as central servers for their wagering systems and ticketing services.|$|R
40|$|A {{dedicated}} microbiology {{data processing}} system with <b>remote</b> <b>batched</b> job entry to an obsolete computer, has been superseded by the inclusion of bacteriology in an on-line interactive clinical pathology system which had previously incorporated chemical pathology and haematology. The original Phoenix system has been adapted {{to allow for the}} entry of bacteriology data using mnemonic codes and to deal with the problems caused by the longer processing time of bacteriology specimens. Particular advantages of the new system include the immediate linkage of all specimens for each patient and an easy recall and display of results in the laboratories and on the wards...|$|R
50|$|The {{remaining}} models initially ran the Batch Processing Monitor (BPM), later augmented with a timesharing option (BTM); {{the combined}} system was usually {{referred to as}} BPM/BTM. The Universal Time-Sharing System (UTS) became available in 1971, supporting much enhanced time-sharing facilities. A compatible upgrade (or renaming) of UTS, Control Program V (CP-V) became available starting in 1973 and added real-time, <b>remote</b> <b>batch,</b> and transaction processing. A dedicated real-time OS, Control Program for Real-Time (CP-R) was also available for Sigma 9 systems. The Xerox Operating System (XOS), intended as an IBM DOS replacement, also runs on Sigma 6/7/9 systems, but never gained real popularity.|$|R
50|$|The varying {{requirements}} for the scale of performance and density among these workloads have implications for the cloud resources that optimally support them. For example, a visual cloud infrastructure to support remote desktops would likely be configured {{with the goal of}} supporting the greatest practical number of desktop instances per server. Cloud game streaming, on the other hand, requires far greater attention to meeting peak graphics performance, likely requiring lower density per server. While both those interactive usages are also highly latency sensitive, <b>remote</b> <b>batch</b> rendering values time to completion, with latency playing a far less important role.|$|R
5000|$|The MTS job {{program is}} the one with which most users {{interact}} and provides command interpretation, execution control, file and device management, and accounting services. Other job programs assist the supervisor (the Paging Device Processor or PDP, the OPERATOR console job, the Disk Manager or DMGR, ...), provide common or shared services (spooled local and <b>remote</b> <b>batch</b> services via HASP and the HASPlings or later the Resource Manager or RM which was developed at the University of British Columbia to replace HASP), or allow the system operators to display status and otherwise control the system (JOBS, UNITS, STOP, BLAST, GOOSE, STARTUP, SHUTDOWN, REW, WTM, ...).|$|R
40|$|Cluster {{computing}} {{has come}} to prominence as a cost-effective parallel processing tool for solving many complex computational problems. In this paper, we propose a new timesharing opportunistic scheduling policy to support <b>remote</b> <b>batch</b> job executions over networked clusters {{to be used in}} conjunction with the Condor Up-Down scheduling algorithm. We show that timesharing approaches can be used in an opportunistic setting to improve both mean job slowdowns and mean response times with little or no throughput reduction. We also show that the proposed algorithm achieves significant improvement in job response time and slowdown as compared to exiting approaches and some recently proposed new approaches...|$|R
40|$|The Web Service Description Language {{defines a}} {{service as a}} {{procedure}} whose inputs and outputs are arbitrarily structured data values, sometimes called documents. In this paper we argue that document-oriented interfaces {{can be viewed as}} batches of calls to finer-grained object-oriented interfaces. Turning this correspondence around, we show that flexible documents can be specified by converting a block of finegrained object-oriented invocations into a batch document. The statements in the block operate directly on virtual service objects, freeing the programmer from the need to explicitly construct invocation objects and then manually correlate them to results in the response. Batch blocks can also include conditionals and loops. Our system, <b>Remote</b> <b>Batch</b> Invocation for Web Services, translates object-oriented interface...|$|R
40|$|The growing {{interest}} for on-line computer service and process control at CERN decentralises certain computer activities. Small process computers, <b>remote</b> <b>batch</b> stations and user terminals {{are to be}} backed by a powerful central computer. The present data network is principally star shaped. At the centre {{of it is a}} CDC 6600 - 6500 computer combination. It has a front end CDC 3100 computer with a Hewlett Packard 2116 as multiplexer. Some details about the fast parallel connections between the CDC 3100 and the HP 2116 B are given in the paper, as well as descriptions of some computer simulation techniques used to test the present systems. Finally some plans on a future network are given. (12 refs) ...|$|R
50|$|Service in Informatics and Analysis (SIA Ltd.) {{was one of}} the {{pioneering}} time-sharing service bureau companies in the late 1960s, later known as SIA Computer Services. Its head office was located at Lower Belgrave Street, close to Victoria Station in London, and the company had branch offices in Edinburgh, Manchester, the West End, Paris and (much later) in Hong Kong. SIA offered terminal services via the Post Office telephone network at speeds of 10, 15, 30, 60 and 120 characters per second for Teletype-style terminals and of 1200 baud, 2400 baud and 4800 baud for Remote Job Entry terminals. Later with the release of the IBM PC, systems were developed to emulate the <b>Remote</b> <b>Batch</b> and interactive terminals.Clients could also visit the head or branch offices to submit their jobs personally or have them accepted and supervised by the production department.|$|R
40|$|The National Science Foundation is {{supporting}} {{an investigation into}} {{the costs and benefits of}} coupling a small stand-alone timesharing system (such as TSS- 8 or RSTS- 11) to a large multiprogrammed system (such a s OS/ 370 or 1108 EXEC 8). A system is being designed which will support local limited resource time-sharing, <b>remote</b> <b>batch</b> operations, on-line <b>remote</b> job entry, and line concentration, as well as allow the small system to act as an "intelligent. " terminal. The system will allow the migration of programs and data between the two systems as well as allow the user of the small system to access all physical resources of the larger one. The system will be initally implemented on a Datacraft 6024 / 1108 pairing but a primary goal of the design is to generalize to other machine pairs. These loosely coupled systems will increase the practical modes of access to computing services and hardware resources...|$|R
40|$|Abstract Due to {{the shift}} from software-as-a-product (SaaP) to {{software-as-a-service}} (SaaS), software components that were developed to run in a single address space must increasingly be accessed remotely across the network. Distribution middleware is frequently used to facilitate this transition. Yet a range of middleware platforms exist, and there are few existing guidelines to help the programmer choose an appropriate middleware platform to achieve desired goals for performance, conciseness, intuitiveness, and reliability. To address this limitation, in this article, we describe {{a case study of}} transitioning an Open Service Gateway Initiative (OSGi) service from local to remote access. In our case study, we evaluate five remote versions of this service, constructed using different distribution middleware platforms. These platforms are implemented by widely-used commercial technologies or have been proposed as improvements {{on the state of the}} art. In particular, we implemented a service-oriented version of our own <b>Remote</b> <b>Batch</b> Invocation abstraction. We compare and contrast these implementations in terms of their respective performance, conciseness, complexity, and reliability. Our results can help remote servic...|$|R
40|$|Due to {{the shift}} from software-as-a-product (SaaP) to {{software-as-a-service}} (SaaS), software components that were developed to run in a single address space must in-creasingly be accessed remotely across the network. Dis-tribution middleware is frequently used to facilitate this transition. Yet a range of middleware platforms exist, and there are few existing guidelines to help the programmer choose an appropriate middleware platform to achieve de-sired goals for performance, expressiveness, and reliabil-ity. To address this limitation, {{in this paper we}} describe a case study of transitioning an Open Service Gateway Initia-tive (OSGi) service from local to remote access. Our case study compares five remote versions of this service, con-structed using different distribution middleware platforms. These platforms are implemented by widely-used commer-cial technologies or have been proposed as improvements {{on the state of the}} art. In particular, we implemented a service-oriented version of our own <b>Remote</b> <b>Batch</b> Invoca-tion abstraction. We compare and contrast these implemen-tations in terms of their respective performance, expressive-ness, and reliability. Our results can help remote service programmers make informed decisions when choosing mid-dleware platforms for their applications. ...|$|R
5000|$|At the University of Michigan (UM) and Wayne State University (WSU) {{there was}} a {{parallel}} development effort by the Merit Network to develop network support. The Merit nodes were PDP-11 based and used custom hardware and software to provide host to host interactive connections between MTS systems and between MTS and the CDC SCOPE/HUSTLER system at Michigan State University (MSU). The Merit nodes were known as Communication Computers (CCs) and acted as IBM Control Units {{on the one side}} while providing links to other CCs on the other side. The initial host to host interactive connections were supplemented a bit later by terminal to host (TL) connections, and later still by host to host batch connections which allowed remote jobs submitted from one system to be executed (EX) on another with printed (PR) and punched card output (PU) returned to the submitting system or to another host on the network. The <b>remote</b> <b>batch</b> jobs could be submitted from a real card reader or via *BATCH* using a #NET [...] "card" [...] {{at the front of the}} job.|$|R
50|$|The first network {{connections}} were host to host interactive connections {{which allowed}} person to remote computer or local computer to remote computer interactions. To this, terminal to host connections, <b>batch</b> connections (<b>remote</b> job submission, <b>remote</b> printing, <b>batch</b> file transfer), and interactive file copy were added. And, {{in addition to}} connecting to host computers over custom hardware interfaces, the ability to connect to hosts or other networks over groups of asynchronous ports and via X.25 were added.|$|R
40|$|This paper {{presents}} a new {{solution to the}} expression problem (EP) that works in OO languages with simple generics (including Java or C#). A key novelty of this solution is that advanced typing features, including F-bounded quantification, wildcards and variance annotations, are not needed. The solution is based on object algebras, which are an abstraction closely related to algebraic datatypes and Church encodings. Object algebras also {{have much in common}} with the traditional forms of the Visitor pattern, but without many of its drawbacks: they are extensible, remove the need for accept methods, and do not compromise encapsulation. We show applications of object algebras that go beyond toy examples usually presented in solutions for the expression problem. In the paper we develop an increasingly more complex set of features for a mini-imperative language, and we discuss a real-world application of object algebras in an implementation of <b>remote</b> <b>batches.</b> We believe that object algebras bring extensibility to the masses: object algebras work in mainstream OO languages, and they significantly reduce the conceptual overhead by using only features that are used by everyday programmers. © 2012 Springer-Verlag Berlin Heidelberg. link_to_subscribed_fulltex...|$|R
40|$|Grid-control is a {{lightweight}} and highly portable open source submission tool that supports virtually all workflows in high energy physics (HEP). Since 2007 {{it has been}} used by a sizeable number of HEP analyses to process tasks that sometimes consist of up 100 k jobs. grid-control is built around a powerful plugin and configuration system, that allows users to easily specify all aspects of the desired workflow. Job submission {{to a wide range of}} local or <b>remote</b> <b>batch</b> systems or grid middleware is supported. Tasks can be conveniently specified through the parameter space that will be processed, which can consist of any number of variables and data sources with complex dependencies on each other. Dataset information is processed through a configurable pipeline of dataset filters, partition plugins and partition filters. The partition plugins can take the number of files, size of the work units, metadata or combinations thereof into account. All changes to the input datasets or variables are propagated through the processing pipeline and can transparently trigger adjustments to the parameter space and the job submission. While the core functionality is completely experiment independent, integration with the CMS computing environment is provided by a small set of plugins. Comment: 8 pages, 7 figures, Proceedings for the 22 nd International Conference on Computing in High Energy and Nuclear Physic...|$|R
50|$|The Merit Network {{was formed}} in 1966 as the Michigan Educational Research Information Triad to explore {{computer}} networking between three of Michigan's public universities {{as a means to}} help the state's educational and economic development. With initial support from the State of Michigan and the National Science Foundation (NSF), the packet-switched network was first demonstrated in December 1971 when an interactive host to host connection was made between the IBM mainframe computer systems at the University of Michigan in Ann Arbor and Wayne State University in Detroit. In October 1972 connections to the CDC mainframe at Michigan State University in East Lansing completed the triad. Over the next several years in addition to host to host interactive connections the network was enhanced to support terminal to host connections, host to host <b>batch</b> connections (<b>remote</b> job submission, <b>remote</b> printing, <b>batch</b> file transfer), interactive file transfer, gateways to the Tymnet and Telenet public data networks, X.25 host attachments, gateways to X.25 data networks, Ethernet attached hosts, and eventually TCP/IP and additional public universities in Michigan join the network. All of this set the stage for Merit's role in the NSFNET project starting in the mid-1980s.|$|R
50|$|Merit Network, Inc., an {{independent}} non-profit 501(c)(3) corporation governed by Michigan's public universities, {{was formed in}} 1966 as the Michigan Educational Research Information Triad to explore computer networking between three of Michigan's public universities {{as a means to}} help the state's educational and economic development. With initial support from the State of Michigan and the National Science Foundation (NSF), the packet-switched network was first demonstrated in December 1971 when an interactive host to host connection was made between the IBM mainframe computer systems at the University of Michigan in Ann Arbor and Wayne State University in Detroit. In October 1972 connections to the CDC mainframe at Michigan State University in East Lansing completed the triad. Over the next several years in addition to host to host interactive connections the network was enhanced to support terminal to host connections, host to host <b>batch</b> connections (<b>remote</b> job submission, <b>remote</b> printing, <b>batch</b> file transfer), interactive file transfer, gateways to the Tymnet and Telenet public data networks, X.25 host attachments, gateways to X.25 data networks, Ethernet attached hosts, and eventually TCP/IP and additional public universities in Michigan join the network. All of this set the stage for Merit's role in the NSFNET project starting in the mid-1980s.|$|R
40|$|Abstract. Although {{distributed}} object systems, like RMI and CORBA, enable object-oriented {{programs to}} be easily distributed across a network, achieving acceptable performance usually requires client-specific optimization of server interfaces, making such systems difficult to maintain and evolve. Automatic optimization techniques, including Batched Futures and Communication Restructuring, are weaker and more unpredictable. This paper presents <b>Batched</b> <b>Remote</b> Method Invocation (BRMI) for Java, a system for explicit <b>batching</b> of <b>remote</b> operations on multiple objects, with support for array cursors, custom exception handling, and chained batches. With BRMI, clients can optimize their access to servers without custom server changes. The applicability of BRMI is demonstrated by rewriting several RMI client applications to use BRMI. Benchmarks of these applications and several micro-benchmarks demonstrate that BRMI outperforms RMI and has significantly better scalability when clients make multiple remote calls. ...|$|R
40|$|Norden's {{expanded}} and interactive version of GPSS/ 360 {{has been extensively}} improved through new data entry and storage capabilities, a a new report generator, selective output display, and HELP blocks to enable model manipulation {{and to provide a}} a "window " to view the simulation during its progress. The data entry and manipulation system allows the user to input and display matrix savevalues through an IBM 2250 Display Unit. Titles may be placed on rows and columns to simplify the location of data items. A A less elaborate system was developed for <b>remote</b> terminal and <b>batch</b> mode data entry. The user can witness the progress of the simulation during the actual model execution • He then has the option to stop the simulation at any time, change parameters controlling operation of the model, and resume running of the model • This interactive feature opens vast new horizon...|$|R
40|$|Although {{distributed}} object systems, {{for example}} RMI and CORBA, enable object-oriented programs {{to be easily}} distributed across a network, achieving acceptable performance usually requires client-specific optimization of server interfaces, making such systems difficult to maintain and evolve. Automatic optimization techniques, including Batched Futures and Communication Restructuring, do not {{work as well as}} hand optimization. This paper presents <b>Batched</b> <b>Remote</b> Method Invocation (BRMI), a languagelevel technique for clients to specify explicit batches of operations on remote objects. We have implemented BRMI for Java as an extension of RMI, with support for batches with array cursors, custom exception handling, conditionals and loops. BRMI allows common design patterns, including Data Transfer Objects and Remote Object Facade, to be constructed on the fly by clients without special server support. The performance benefits of batching operations are well known; our evaluation focuses on the usability of explicit batches, but we also confirm that BRMI outperforms RMI and scales significantly better when clients make multiple remote calls. The applicability of BRMI is demonstrated by rewriting several RMI client applications to use BRMI...|$|R
40|$|Abstract. This paper {{focuses on}} searching the best k objects with more {{attributes}} according to user preferences in the Web environment. Attributes {{of an object}} type are distributed on servers in a disjunctive way, i. e. values of one attribute are stored in only one remote server on the Internet. In our work, every user can express his/her preferences for each attribute by a fuzzy function and mutual relations between the attributes by an aggregation function. We use client/server architecture and communication via Web Services. We deal with the usage of Fagin’s NRA algorithm, which can find the best k objects without accessing all the objects. Because of support of sorting objects according to a fuzzy function, an indexing method based on B+-tree is used in each remote server. Moreover, each server is stateless, i. e. independent from any previous request. Our solution is based on cache memory, which loads objects from <b>remote</b> servers in <b>batches</b> and thus reduces the amount of network communication. In this paper we present a system TOPKNET, which can efficiently find the best k objects for various users with data on remote servers. ...|$|R
