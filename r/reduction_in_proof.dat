5|10000|Public
40|$|Abstract. Following a brief {{discussion}} of uniprocessor scheduling in which we argue the case for formal analysis, we describe a distributed Event B model of interrupt driven scheduling. We first consider a model with two executing tasks, presented {{with the aid of}} state machine diagrams. We then present a faulty variant of this model which, under particular event timings, may ”drop ” an interrupt. We show how the failure to discharge a particular proof obligation leads us to the conceptual error in this model. Finally we generalise the correct model to n tasks, leading to a <b>reduction</b> <b>in</b> <b>proof</b> effort...|$|E
40|$|In this paper, we {{show the}} {{correspondence}} existing between normalization in calculi with explicit substitution and cut elimination in sequent calculus for Linear Logic, via Proof Nets. This correspondence {{allows us to}} prove that a typed version of the #x-calculus [30, 29] is strongly normalizing, {{as well as of}} all the calculi isomorphic to it such as # # [24], # s [19], # d [21], and # f [11]. In order to achieve this result, we introduce a new notion of <b>reduction</b> <b>in</b> <b>Proof</b> Nets: this extended reduction is still confluent and strongly normalizing, and is of interest of its own, as it correspond to more identifications of proofs in Linear Logic that differ by inessential details. These results show that calculi with explicit substitutions are really an intermediate formalism between lambda calculus and proof nets, and suggest a completely new way to look at the problems still open in the field of explicit substitutions...|$|E
40|$|Roberto Di Cosmo DMI-LIENS (CNRS URA 1347) Ecole Normale Superieure 45, Rue d'Ulm 75230 Paris Cedex, France Email:dicosmo@ens. fr Delia Kesner LRI (CNRS URA 410) Bat 490 Universite de Paris-Sud 91405 Orsay Cedex, France Email:kesner@lri. fr Abstract In this paper, we {{show the}} {{correspondence}} existing between normalization in calculi with explicit substitution and cut elimination in sequent calculus for Linear Logic,via Proof Nets. This correspondence {{allows us to}} prove that a typed version of the #x-calculus [34, 5] is strongly normalizing, {{as well as of}} all the calculi that can be translated to it keeping normalization properties such as # # [27], # s [22], # d [24], and # f [14]. In order to achieve this result, we introduce a new notion of <b>reduction</b> <b>in</b> <b>Proof</b> Nets: this extended reduction is still confluent and strongly normalizing, and is of interest of its own, as it corresponds to more identifications of proofs in Linear Logic that differ by inessential details. These [...] ...|$|E
40|$|Interpolants are the {{cornerstone}} of several approximate verification techniques. Current interpolation techniques restrict the search heuristics of the underlying decision procedure to compute interpolants, incurring {{a negative impact on}} performance, and apply primarily to the lazy proof explication framework. We bridge the gap between fast decision procedures that aggressively use propositional reasoning and slower interpolating decision procedures by extending the scope of the latter to non-lazy approaches and relaxing the restrictions on search heuristics. Both are achieved by combining a simple set of transformations on resolution refutations. Our experiments show that this method leads to speedups when computing interpolants and to <b>reductions</b> <b>in</b> <b>proof</b> size...|$|R
3000|$|... holds. The latter fact will be {{employed}} <b>in</b> our <b>reduction</b> <b>proof</b> <b>in</b> {{order to be}} able to show that an optimal mass flow through the network must have certain properties.|$|R
40|$|Abstract. Zerocoin {{proposed}} adding decentralized cryptographically anonymous e-cash to Bitcoin. Given {{the increasing}} popularity of Bitcoin and {{its reliance on}} a distributed pseudononymous public ledger, this anonymity is important if only to provide the same minimal privacy protections from nosy neighbors offered by conventional banking. Unfor-tunately, at 25 KB, the non-interactive zero-knowledge proofs for spending a zerocoin are nearly prohibitively large. In this paper, we consider several improvements. First, we strengthen Zerocoin’s anonymity guarantees, making them independent {{of the size of}} these proofs. Given this freedom, we explore several techniques for drastically reducing proof size while ensuring that forging a single zerocoin is more difficult than the block mining process used to maintain Bitcoin’s distributed ledger. Provided a zerocoin is worth less than the reward for a Bitcoin block, forging a coin is not an economically rational action. Hence we preserve Zerocoin’s absolute anonymity guarantees while achieving drastic <b>reductions</b> <b>in</b> <b>proof</b> size by limiting ourselves to security against rational attackers...|$|R
40|$|In the {{simplest}} setting, one represents a boolean function using expressions over variables, where each variable {{corresponds to a}} function input. So-called parametric represen-tations, used to represent a function in some restricted subspace of its domain, break this correspondence by allowing inputs {{to be associated with}} functions. This can lead to more succinct representations, for example when using binary decision dia-grams (BDDs). Here we introduce Universal Boolean Functional Vectors (UBFVs), which also break the correspondence, but done so such that all input vectors are accounted for. Intelligent choice of a UBFV can have a dramatic impact on BDD size; for instance we show how the hidden weighted bit function can be efficiently represented using UBFVs, whereas without UBFVs BDDs are known to be exponential for any variable order. We show several industrial examples where the UBFV approach has a huge impact on proof performance, the “killer app” being floating point addition, wherein the wide case-split used in the state-of-the-art approach is entirely done away with, resulting in 70 -fold <b>reduction</b> <b>in</b> <b>proof</b> runtime. We give other theoretical and experimental results, and also provide two approaches to verifying the crucial “universality” aspect of a proposed UBFV. Finally, we suggest several interesting avenues of future research stemming from this program...|$|E
50|$|A carbon project {{refers to}} a {{business}} initiative that receives funding because of the cut the emission of greenhouse gases (GHGs) will result. To prove that the project will result in real, permanent, verifiable <b>reductions</b> <b>in</b> Greenhouse Gases, <b>proof</b> must be provided {{in the form of}} a project design document and activity reports validated by an approved third party in the case of Clean Development Mechanism (CDM) or Joint Implementation (JI) projects.|$|R
40|$|Nonuniform (or “nested” or “heterogeneous”) datatypes are recursively defined {{types in}} which the type {{arguments}} vary recursively. They arise {{in the implementation of}} finger trees and other efficient functional data structures. We show how to reduce a large class of nonuniform datatypes and codatatypes to uniform types in higher-order logic. We programmed this <b>reduction</b> <b>in</b> the Isabelle/HOL <b>proof</b> assistant, thereby enriching its specification language. Moreover, we derive (co) recusion and (co) induction principles based on a weak variant of parametricity...|$|R
40|$|McCullagh and Barreto {{presented}} an identity-based authenticated key agreement protocol in CT-RSA 2005. Their protocol {{was found to}} be vulnerable to a key-compromise impersonation attack. In order to recover the weakness, McCullagh and Barreto, and Xie proposed two variants of the protocol respectively. In each of these works, a security proof of the proposed protocol was presented. In this paper, we revisit these three security proofs and show that all the <b>reductions</b> <b>in</b> these <b>proofs</b> are invalid, because the property of indistinguishability between their simulation and the real world was not held. As a replacement, we present a new reduction for the McCullagh and Barreto modified protocol in the weaker Bellare-Rogaway key agreement model. Our reduction is based on a new assumption, which is at least as weak as some well-explored assumptions in the literature...|$|R
40|$|International audienceNonuniform (or " nested " or " {{heterogeneous}} ") data-types are recursively defined {{types in}} which the type arguments vary recursively. They arise {{in the implementation of}} finger trees and other efficient functional data structures. We show how to reduce a large class of nonuniform datatypes and codatatypes to uniform types in higher-order logic. We programmed this <b>reduction</b> <b>in</b> the Isabelle/HOL <b>proof</b> assistant, thereby enriching its specification language. Moreover, we derive (co) induction and (co) recursion principles based on a weak variant of parametricity...|$|R
40|$|SUMMARY There {{are three}} {{well-known}} identification schemes: the Fiat-Shamir, GQ and Schnorr identification schemes. All {{of them are}} proven secure against the passive or active attacks under some number-theoretic assumptions. How-ever, efficiencies of the <b>reductions</b> <b>in</b> those <b>proofs</b> of security are not tight, because they require “rewinding ” a cheating prover. We show an identification scheme IDKEA 1, which is an enhanced version of the Schnorr scheme. Although it needs the four ex-changes of messages and slightly more exponentiations, the ID-KEA 1 is proved to be secure under the KEA 1 and DLA assump-tions with tight reduction. The idea underlying the IDKEA 1 is to use an extractable commitment for prover’s commitment. <b>In</b> the <b>proof</b> of security, the simulator can open the commitment in two different ways: one by the non-black-box extractor of the KEA 1 assumption and the other through the simulated transcript. This means that we don’t need to rewind a cheating prover and can prove the security without loss of the efficiency of reduction. key words: identification scheme, rewinding, KEA 1 assump-tion, tight reduction...|$|R
40|$|We {{illustrate}} {{the benefits of}} using Natural Deduction in combination with weak Higher-Order Abstract Syntax for formalizing an object-based calculus with objects, cloning, method-update, types with subtyping, and side-effects, in inductive type theories such as the Calculus of Inductive Constructions. This setting suggests a clean and compact formalization of the syntax and semantics of the calculus, with an efficient management of method closures. Using our formalization and the Theory of Contexts, we can prove formally the Subject <b>Reduction</b> Theorem <b>in</b> the <b>proof</b> assistant Coq, with a relatively small overhead...|$|R
40|$|A {{detailed}} quantitative {{model for}} the strengthening of monolithic alloys and composites due to precipitation strengthening, solution strengthening, grain and subgrain strengthening, strengthening by dislocations and load transfer to ceramic inclusions is presented. The model includes a newly derived description {{of the effect of}} a precipitate free zone (PFZ) around the reinforcing phase incorporating strain hardening of the PFZ. The model is successfully applied to model the experimental data for the proof strengths of four Al Li Cu Mg type alloys and composites aged to obtain a wide range of microstructures and all strengthening contributions are quantified. It is shown that PFZ formation in the 8090 MMC causes a drastic <b>reduction</b> <b>in</b> the <b>proof</b> strength (about 100 MPa), but it has little influence on the time required for peak ageing. In all alloys strengthening due to GPB zones is more important than strengthening due to delta' (Al 3 Li) phase...|$|R
40|$|This paper {{describes}} {{a framework for}} flow analysis of programs with higher-order functions with normal-order reduction. The framework {{is based on an}} abstract machine derived from the Geometry of Interaction semantics for <b>reduction</b> <b>in</b> linear logic <b>proof</b> nets. By standard methods from abstract interpretation the transition system defined by the machine induces a set of equations defining the flow between the program points. This set of equations defines a collecting semantics for the program and is amenable to further analysis by abstraction-based approximation. As examples of its application we show how to obtain information about strictness, control-flow and usage of data...|$|R
40|$|In Eurocrypt 2009, Hohenberger and Waters {{pointed out}} that a {{complexity}} assumption, which restricts the adversary to a single correct response, seems inherently more reliable than their flexible counterparts. The q-SDH assumption is less reliable than standard assumptions because its solution allows exponential answers. On the other hand, the q-SDH assumption exhibits the nice feature of tight <b>reduction</b> <b>in</b> security <b>proof.</b> <b>In</b> this paper, we propose {{a variant of the}} q-SDH assumption, so that its correct answers are polynomial and no longer exponentially many. The new assumption is much more reliable and weaker than the original q-SDH assumption. We propose a new digital signature scheme that can tightly reduce the security to the proposed assumption in the standard model. We show that our signature scheme shares most properties with the q-SDH based signature schemes. We also propose a new approach to construct fully secure signatures from weakly secure signature against known-message attacks. Although our security transformation is conditional and not completely generic, it offers another efficient approach to construct fully secure signatures...|$|R
40|$|A theorem of Green, Tao, and Ziegler can {{be stated}} (roughly) as follows: if R is a {{pseudorandom}} set, and D is a dense subset of R, then D may be modeled {{by a set}} M that is dense in the entire domain such that D and M are indistinguishable. (The precise statement refers to“measures ” or distributions rather than sets.) The proof of this theorem is very general, and it applies to notions of pseudorandomness and indistinguishability {{defined in terms of}} any family of distinguishers with some mild closure properties. The proof proceeds via iterative partitioning and an energy increment argument, {{in the spirit of the}} proof of the weak Szemerédi regularity lemma. The “reduction” involved <b>in</b> the <b>proof</b> has exponential complexity in the distinguishing probability. We present a new proof inspired by Nisan’s proof of Impagliazzo’s hardcore set theorem. The <b>reduction</b> <b>in</b> our <b>proof</b> has polynomial complexity in the distinguishing probability and provides a new characterization of the notion of “pseudoentropy” of a distribution. We also follow the connection between the two theorems and obtain a new proof of Impagliazzo’s hardcore set theorem via iterative partitioning and energy increment. While our reductio...|$|R
40|$|In Asiacrypt’ 08, Green and Hohenberger {{presented}} an adaptive oblivious transfer (OT) scheme which {{makes use of}} a signature built from the Boneh-Boyen Identity Based Encryption. In this note, we show that the signature scheme is vulnerable to known-message attacks and the <b>reduction</b> used <b>in</b> the <b>proof</b> of Lemma A. 6 is flawed. We also remark that the paradigm of “encryption and proof of knowledge” adopted in the OT scheme is unnecessary because the transferred message must be “recognizable” in practice, otherwise the receiver cannot decide which message to retrieve. However, {{we would like to}} stress that this work does not break the OT scheme itself. SCOPUS: cp. kinfo:eu-repo/semantics/publishe...|$|R
40|$|Syntax Alberto Ciaffaglione # ciaffagl@dimi. uniud. it Luigi Liquori Luigi. Liquori@inria. fr Marino Miculan miculan@dimi. uniud. it ABSTRACT We {{illustrate}} {{the benefits of}} using Natural Deduction in combination with weak Higher-Order Abstract Syntax for formalizing an object-based calculus with objects, cloning, method-update, types with subtyping, and side-e#ects, in inductive type theories such as the Calculus of Inductive Constructions. This setting suggests a clean and compact formalization of the syntax and semantics of the calculus, with an e#cient management of method closures. Using our formalization and the Theory of Contexts, we can prove formally the Subject <b>Reduction</b> Theorem <b>in</b> the <b>proof</b> assistant Coq, with a relatively small overhead...|$|R
40|$|Abstract. In this paper, we {{extend the}} results of [Tama] on the Grothendieck Conjecture for affine {{hyperbolic}} curves over finite fields to obtain a Grothendieck Conjecture-type result for singular, proper, stable log-curves over finite fields. Using this result, we derive a strong Grothendieck Conjecture-type result for smooth, proper hyperbolic curves over number fields, and a weak Grothendieck Conjecture-type result for smooth, proper, hyperbolic curves over local fields with ordi-nary <b>reduction.</b> <b>In</b> [Tama], a <b>proof</b> of the Grothendieck Conjecture (reviewed below) was given for smooth affine hyperbolic curves over finite fields (and over number fields). The {{purpose of this paper}} is to show how one can derive the Grothendieck Conjecture for arbitrary (i. e., not necessarily affine) smoot...|$|R
40|$|Ken-etsu Fujita Kyushu Institute of Technology, Iizuka, 820 - 8502, Japan fujiken@dumbo. ai. kyutech. ac. jp Abstract. We {{introduce}} a polymorphic call-by-value calculus, # v exc, based on 2 nd order classical logic. The call-by-value computation rules are defined based on <b>proof</b> <b>reductions,</b> <b>in</b> which classical <b>proof</b> <b>reductions</b> {{are regarded as}} a logical permutative <b>reduction</b> <b>in</b> the sense of Prawitz and a dual permutative reduction. It is shown that the CPS-translation from the core # v exc to the intuitionistic fragment, i. e., the Damas-Milner type system is sound. We discuss {{that the use of}} the dual permutative <b>reduction</b> is, <b>in</b> general, uncorrected in polymorphic calculi. We also show the Church-Rosser property of # v exc, and the soundness and completeness of the type inference algorithm W. From the subject reduction property, it is obtained that a program whose type is inferred by W never leads to a type-error under the rewriting semantics. Finally, we give a brief comparison wit [...] ...|$|R
40|$|We {{investigate}} the computational complexity of testing dominance and consistency in CP-nets. Previously, {{the complexity of}} dominance has been determined for restricted classes in which the dependency graph of the CP-net is acyclic. However, there are preferences of interest that define cyclic dependency graphs; these are modeled with general CP-nets. In our main results, we show here that both dominance and consistency for general CP-nets are PSPACE-complete. We then consider the concept of strong dominance, dominance equivalence and dominance incomparability, and several notions of optimality, and identify {{the complexity of the}} corresponding decision problems. The <b>reductions</b> used <b>in</b> the <b>proofs</b> are from STRIPS planning, and thus reinforce the earlier established connections between both areas. ...|$|R
50|$|One {{application}} of gadgets is in proving hardness of approximation results, by reducing {{a problem that}} {{is known to be}} hard to approximate to another problem whose hardness is to be proven. In this application, one typically has a family of instances of the first problem {{in which there is a}} gap in the objective function values, and in which it is hard to determine whether a given instance has an objective function that is on the low side or on the high side of the gap. The <b>reductions</b> used <b>in</b> these <b>proofs,</b> and the gadgets used <b>in</b> the <b>reductions,</b> must preserve the existence of this gap, and the strength of the inapproximability result derived from the reduction will depend on how well the gap is preserved.|$|R
40|$|Abstract In a {{distributed}} ring signature scheme, {{a subset}} of users cooperate to compute a distributed anonymous signature on a message, {{on behalf of a}} family of possible signing subsets. The receiver can verify that the signature comes from {{a subset of}} the ring, but he cannot know which subset has actually signed. In this work we use the concept of dual access structures to construct a distributed ring signature scheme which works with general families of possible signing subsets. The length of each signature is linear on the number of involved users, which is desirable for some families with many possible signing subsets. The scheme achieves the desired properties of correctness, anonymity and unforgeability. The <b>reduction</b> <b>in</b> the <b>proof</b> of unforgeability is tighter than the <b>reduction</b> <b>in</b> the previous proposals which work with general families. We analyze the case in which our scheme runs in an identity-based scenario, where public keys of the users can be derived from their identities. This fact avoids the necessity of digital certificates, and therefore allows more efficient implementations of such systems. But our scheme can be extended to work in more general scenarios, where users can have different types of keys. 1 Introduction In standard public key cryptosystems, the public keys of the users must be authenticated via a Public Key Infrastructure (PKI) based on digital certificates, which link the identities of the users with their public keys. This fact makes the use of cryptographic protocols less efficient in the real life...|$|R
40|$|The Schnorr {{signature}} {{scheme is}} the most efficient signature scheme based on the discrete loga-rithm problem and {{a long line of}} research investigates the existence of a tight security reduction for this scheme. Almost all recent works present lower tightness bounds and most recently Seurin (Euro-crypt 2012) showed that under certain assumptions the non-tight security proof for Schnorr signatures by Pointcheval and Stern (Eurocrypt 1996) is essentially optimal. All previous works in this direction rule out tight reductions from the (one-more) discrete logarithm problem. In this paper we introduce a new meta-reduction technique, which shows lower bounds for the large and very natural class of generic reductions. A generic reduction is independent of a particular representation of group elements and most <b>reductions</b> <b>in</b> state-of-the-art security <b>proofs</b> have this desirable property. Our approach shows uncondi-tionally that there is no tight generic reduction from any natural computational problem Π defined over algebraic groups (including even interactive problems) to breaking Schnorr signatures, unless solving Π is easy...|$|R
40|$|A {{long line}} of {{research}} investigates the existence of tight security reductions for the Schnorr signature scheme. Most of these works presented lower tightness bounds, most recently Seurin (Eurocrypt 2012) showed that under certain assumptions the non-tight security proof for Schnorr signatures by Pointcheval and Stern (Eurocrypt 1996) is essentially optimal. All previous works in this direction share the same restrictions: The results hold only under the interactive one-more discrete logarithm assumption, they only consider algebraic reductions, and they only rule out tight reductions from the (one-more) discrete logarithm problem. The existence of a tight reduction from weaker computational problems, like CDH or DDH, remained open. In this paper we introduce a new meta-reduction technique, which allows to prove lower bounds for the large and very natural class of generic reductions. A generic reduction is independent of a particular representation of group elements. Most <b>reductions</b> <b>in</b> state-of-the-art security <b>proofs</b> have this desirable property. This new approach allows to show unconditionally {{that there is no}} tight generic reduction from any natural computational problem Π defined over algebraic groups (including even interactive problems) to breaking Schnorr signatures, unless solving Π is easy. ...|$|R
40|$|AbstractThe {{notion of}} {{parallel}} reduction is {{extracted from the}} simple proof of the Church-Rosser theorem by Tait and Martin-Löf. Intuitively, this means to reduce a number of redexes (existing in a λ-term) simultaneously. Thus {{in the case of}} β-reduction the effect of a parallel reduction is same as that of a "complete development" which is defined by using "residuals" of β-redexes. A nice feature of parallel reduction, however, is that it can be defined directly by induction on the structure of λ-terms (without referring to residuals or other auxiliary notions), and the inductive definition provides us exactly what we need in proving the theorem inductively. Moreover, the notion can be easily extended to other reduction systems such as Girard′s second-order system F and Gödel′s system T. In this paper, after reevaluating the significance of the notion of parallel <b>reduction</b> <b>in</b> Tait-and-Martin-Löf type <b>proofs</b> of the Church-Rosser theorems, we show that the notion of parallel reduction is also useful in giving short and direct proofs of some other fundamental theorems <b>in</b> <b>reduction</b> theory of λ-calculus; among others, we give such simple proofs of the standardization theorem for β-reduction (a special case of which is known as the leftmost reduction theorem for β-reduction), the quasi-leftmost reduction theorem for β-reduction, the postponement theorem of η-reduction (in βη-reduction), and the leftmost reduction theorem for βη-reduction...|$|R
40|$|Tensile {{tests on}} high purity Fe-P alloys with 0, 0. 05 and 0. 1 mass%P {{were carried out}} at {{temperatures}} between 300 K and 1073 K to clarify the intrinsic effect of phosphorus on the mechanical properties of iron at elevated temperatures. Microstructures of as-quenched, interrupted and ruptured specimens were observed. Experimental {{results show that the}} addition of phosphorus causes a remarkable increase <b>in</b> <b>proof</b> stress of high purity iron at 300 K, but the increase <b>in</b> <b>proof</b> stress by phosphorus decreases with increasing test temperature. The strengthening effect of phosphorus reduces to zero at 1073 K. High purity iron and Fe-P alloys rupture at almost 100 % <b>reduction</b> <b>in</b> area at the whole test temperatures. However, Fe-P alloys show much larger elongation at test temperatures above 773 K than high purity iron. The increased elongation of high purity iron by addition of phosphorus was shown {{to be related to the}} effect of phosphorus on dynamic recovery and recrystallization of iron as its intrinsic effect...|$|R
5000|$|Selected Papers <b>in</b> <b>Proof</b> Theory (North-Holland), August 1992, , Studies <b>in</b> <b>Proof</b> Theory series) ...|$|R
40|$|Abstract. The {{existence}} of tight <b>reductions</b> <b>in</b> cryptographic security <b>proofs</b> {{is an important}} question, motivated by the theoretical search for cryptosystems whose security guarantees are truly independent of adversarial behavior and the practi-cal necessity of concrete security bounds for the theoretically-sound selection of cryptographic parameters. At Eurocrypt 2002, Coron described a meta-reduction technique that allows to prove the impossibility of tight reductions for certain digital signature schemes. This seminal result has found many further interesting applications. However, due to a technical subtlety in the argument, the applica-bility of this technique beyond digital signatures in the single-user setting {{has turned out to}} be rather limited. We describe a new meta-reduction technique for proving such impossibility re-sults, which improves on known ones in several ways. First, it enables interesting novel applications. This includes a formal proof that for certain cryptographic primitives (including public-key encryption/key encapsulation mechanisms and digital signatures), the security loss incurred when the primitive is transferred from an idealized single-user setting to the more realistic multi-user setting is impossible to avoid, and a lower tightness bound for non-interactive key ex-change protocols. Second, the technique allows to rule out tight reductions from a very general class of non-interactive complexity assumptions. Third, the provided bounds are quantitatively and qualitatively better, yet simpler, than the bounds de-rived from Coron’s technique and its extensions. ...|$|R
40|$|Abstract. Efficient {{signature}} scheme whose {{security is}} relying on reliable assumptions is important. There are few schemes based on the standard assumptions such as the Diffie-Hellman (DH) in the standard model. We present a new approach for (hash-and-sign) DH-based signature scheme in the standard model. First, we combine two known techniques, programmable hashes and a tag-based signature scheme so that we obtain a short signature scheme with somewhat short public key of Θ (λ log λ) group elements. Then, we developed a new technique for asymmetric trade between the public key and random tags, which are part of signatures. Roughly speaking, we can dramatically reduce the public key size by adding one field element in each signature. More precisely, our proposal produces public key of Θ(λ log λ) group elements, where λ is the security parameter. The signature size is still short, requiring two elements {{in a group of}} order p and two integers in Zp. In our approach, we can guarantee the security against adversaries that make an a-priori bounded number of queries to signing oracle (we call bounded CMA). i. e., the maximum number q of allowable signing queries is prescribed at the parameter generating time. Note that for polynomial q, we limit ourselves to dealing with only polynomial-time <b>reductions</b> <b>in</b> all security <b>proofs.</b> ...|$|R
40|$|Abstract. Tensile {{tests on}} high purity Fe-P alloys with 0, 0. 05 and 0. 1 mass%P {{were carried out}} at {{temperatures}} between 300 K and 1073 K to clarify the i n m i c effect of phosphorus on the mechanical properties of iron at elevated temperatures. Microstructures of as-quenched, interrupted and ruptured specimens were observed. Experimental {{results show that the}} addition of phosphorus causes a remarkable increase <b>in</b> <b>proof</b> stress of high purity iron at 300 K, but the increase <b>in</b> <b>proof</b> stress by phosphorus decreases with increasing test temperature. The strengthening effect of phosphorus reduces to zero at 1073 K High purity iron and Fe-P alloys rupture at almost 100 % <b>reduction</b> <b>in</b> area at the whole test temperatures. However, Fe-P alloys show much larger elongation at test temperatures above 773 K than high purity iron. The increased elongation of high purity iron by addition of phosphorus was shown {{to be related to the}} effect of phosphorus on dynamic recovery and recrystallization of iron as its intrinsic effect. 1...|$|R
40|$|From whole globe {{the demand}} {{of oil and}} gas is {{increasing}} so increase production is most important. To increase oil production from existing wells in an oil field we have to focus much on optimizing oil production. One way to optimize oil production is to find an alternative to existing intermittent gas lift technology. As we study big oil field currently with huge oil and gas operating companies, the gas injection line network is very old and new intermittent gas lift installations on new wells has been connected to the existing network. Now this creates gas starving for the new installations and resulting in inadequate amount of gas production from the well. By introducing Plunger lift technology of artificial gas lift along with existing intermittent gas lift can provide a fool proof solution to the problem and also <b>reduction</b> <b>in</b> injected gas requirement also. Thus, reducing the compressor duty and overall <b>reduction</b> <b>in</b> environmental emission of harmful gases by reducing the diesel engine load required previously for compression. This paper discusses <b>in</b> detail with <b>proof</b> <b>in</b> form of tables, increase in oil production and <b>reduction</b> <b>in</b> injection gas requirement and <b>reduction</b> <b>in</b> emission of harmful gases to the environment...|$|R
5000|$|... 10 000 Kc Pragergroschen. Mintage is 1000 in BU {{from year}} 1995 and 1256 in BU and 744 <b>in</b> <b>proof</b> from year 1996 and 1500 <b>in</b> <b>proof</b> from year 1997.|$|R
3000|$|... is an NP-complete problem. It is {{pertinent}} {{therefore to}} ask whether the addition of condition (b) alters this result, or affects the <b>proof.</b> <b>In</b> fact, it can be shown that it does not, since the <b>reduction</b> <b>in</b> [13] involves {{the construction of an}} RAF that is automatically closed.|$|R
40|$|We provide two {{necessary}} {{conditions on}} hash functions for the Schnorr signature scheme to be secure, assuming compact group representations {{such as those}} which occur in elliptic curve groups. We also show, via an argument in the generic group model, that these conditions are sufficient. Our hash function security requirements are variants of the standard notions of preimage and second preimage resistance. One of them is in fact equivalent to the Nostradamus attack by Kelsey and Kohno (Eurocrypt, Lecture Notes in Computer Science 4004 : 183 - 200, 2006), and, when considering keyed compression functions, both {{are closely related to}} the ePre and eSec notions by Rogaway and Shrimpton (FSE, Lecture Notes in Computer Science 3017 : 371 - 388, 2004). Our results have a number of interesting implications in practice. First, since security does not rely on the hash function being collision resistant, Schnorr signatures can still be securely instantiated with SHA- 1 /SHA- 256, unlike DSA signatures. Second, we conjecture that our properties require O(2 n) work to solve for a hash function with n-bit output, thereby allowing the use of shorter hashes and saving twenty-five percent in signature size. And third, our analysis does not reveal any significant difference in hardness between forging signatures and computing discrete logarithms, which plays down the importance of the loose <b>reductions</b> <b>in</b> existing random-oracle <b>proofs,</b> and seems to support the use of "normal-size” group...|$|R
