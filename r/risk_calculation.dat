271|507|Public
25|$|The EPA has {{established}} guidance {{for assessing the}} cumulative risk for multiroute exposures to pesticide groups determined to have a common mechanism of toxicity. The Agency has also published guidance on determining whether chemicals share a common mechanism of toxicity, i.e., whether they can be assessed as a common mechanism group. The <b>risk</b> <b>calculation</b> is based on dose addition, where individual pesticide exposure levels are scaled by their relative potency and then summed. Some key uncertainties in this approach include the possibility that relative potency changes with dose, {{and the potential for}} toxicological interactions (dose addition assumes no toxicological interactions). Another caution is that the resulting risk assessments are not complete environmental health evaluations of all chemical exposures, but only represent risks from those common mechanism pesticides.|$|E
50|$|A {{health risk}} {{assessment}} (HRA) is a health questionnaire, {{used to provide}} individuals with an evaluation of their health risks and quality of life. Commonly a HRA incorporates three key elementsan extended questionnaire, a <b>risk</b> <b>calculation</b> or score, and some form of feedback i.e. face-to-face with a health advisor or an automatic online report.|$|E
50|$|Algorithmics {{was voted}} {{as the leading}} {{enterprise}} risk firm for market risk, economic capital <b>risk</b> <b>calculation,</b> risk dashboards and collateral management in Risk magazine's 2010 Technology Rankings. Algorithmics was also selected as the best Risk Analytics Solution Provider in Waters magazine's annual financial technology rankings. In 2011, Algorithmics won the Life and Pension Risk award for Best Solvency II provider. In 2007, Algorithmics was selected as one of Canada's Top 100 Employers, as published in Maclean's magazine.|$|E
50|$|When it was {{acquired}} by NASDAQ OMX, FTEN’s technology was processing over $150 billion in daily <b>risk</b> <b>calculations</b> globally, including {{over a third of}} U.S. equities volume.|$|R
50|$|Skewness {{risk and}} {{kurtosis}} risk also have technical implications in calculation of value at risk. If either are ignored, the Value at <b>Risk</b> <b>calculations</b> will be flawed.|$|R
40|$|Recent {{studies showed}} that {{one third of the}} Belgian {{coastline}} is not sufficiently protected against severe storm events. Therefore coastal protection plans are set up to assure a minimum safety standard for the entire coastline. Flood <b>risk</b> <b>calculations</b> constitute the main input parameter for the concept and planning phases. Since 100 % safety can never be guaranteed, contingency plans are constructed to reduce the remaining flood <b>risks.</b> Flood <b>risk</b> <b>calculations</b> are a powerful communicative and operational instrument to use between engineers and experts on the field, thus forming the link between coastal management and disaster planning...|$|R
50|$|Unfortunately, Qishan’s <b>risk</b> <b>calculation</b> did {{not involve}} being conned by his partner cum girlfriend, Lu Huiting (May Phua). Saddled with a $2 million debt, he is not only {{declared}} a bankrupt but has to serve a jail term. Qishan’s father, devastated {{that he has been}} deceived into mortgaging his house, falls ill and dies in hospital. As a result, Qishan’s mother does not forgive her son. After he is released from prison, Qishan goes back to see his mother but is driven out of the house. He is given shelter by Wang Li’an (Zhang Yao Dong), son of Chicken King (Chen Guo Hua), a stall vendor in the market.|$|E
50|$|The EPA has {{established}} guidance {{for assessing the}} cumulative risk for multiroute exposures to pesticide groups determined to have a common mechanism of toxicity. The Agency has also published guidance on determining whether chemicals share a common mechanism of toxicity, i.e., whether they can be assessed as a common mechanism group. The <b>risk</b> <b>calculation</b> is based on dose addition, where individual pesticide exposure levels are scaled by their relative potency and then summed. Some key uncertainties in this approach include the possibility that relative potency changes with dose, {{and the potential for}} toxicological interactions (dose addition assumes no toxicological interactions). Another caution is that the resulting risk assessments are not complete environmental health evaluations of all chemical exposures, but only represent risks from those common mechanism pesticides.|$|E
50|$|László Szombatfalvy (born 1927 in Budapest, Hungary) is a {{businessman}} and author living in Stockholm, Sweden. Szombatfalvy fled to Sweden in 1956 following the Hungarian uprising of that year. He arrived with two empty hands and worked initially {{in a variety}} of jobs including as a magician in the refugee camps, before gradually becoming interested in the stock market. His interest in stock led to his development of a method of <b>risk</b> <b>calculation</b> for investments for which he became well known on the Swedish stock market. In the late 1980s his interests turned toward entirely different matters and he withdrew from the market. Over {{the past couple of years}} his attention has focused on the application of his risk assessment method to new fields.|$|E
30|$|In this {{functional}} architecture the ITS station {{in each layer}} {{is regarded as the}} component for the cooperative applications responsible for sensor data fusion, situation management / situation awareness and e.g. collision <b>risk</b> <b>calculations.</b>|$|R
40|$|The article {{describes}} the method of <b>risks</b> <b>calculation</b> on all stages of project life cycle {{on the basis of}} application of the Bayes networks. Programmatic realization of this method is carried out by the Hugin software package. ? ?????? ??????? ???????? ??????? ?????? ?? ???? ?????? ?????????? ????? ??????? ?? ?????? ?????????? ??????????? ?????. ??????????? ?????????? ??????? ?????? ?????????????? ? ??????? ???????????? ?????? Hugin...|$|R
50|$|Computer aided call {{handling}} (CACH) {{is built on}} the premise that effective {{call handling}} is the foundation for an efficient dispatch response. By using structured call handling and a series of <b>risk</b> <b>calculations,</b> such systems can make objective dispatch recommendations based on information provided by the caller.|$|R
5000|$|The {{destruction}} of BORAX-I caused the [...] "aerial distribution of contaminants {{resulting from the}} final experiment of the BORAX-I reactor" [...] and the likely contamination of the topmost 1 foot of soil over about 2 acres in the vicinity. The site needed to be cleaned up prior to being used for subsequent experiments. The 84,000-square foot (7,800 m2) area was covered with 6 inches of gravel in 1954, but grass, sagebrush, and other plants reseeded the area since then. The BORAX-I burial ground is located about 2730 feet m northwest of the Experimental Breeder Reactor-1, a publicly accessible national monument. Since 1987, the United States Environmental Protection Agency has classified the burial ground as Superfund site Operable Unit 6-01, one of two such sites (along with SL-1) at the Idaho National Laboratory. In 1995, the EPA ordered the primary remedy of the burial ground should be: [...] "Containment by capping with an engineered barrier constructed primarily of native materials." [...] The site is expected to produce {{no more than a}} 2 in 10,000 increase in cancer risk for long term residential use after 320 years, with no significant decrease after that time. This <b>risk</b> <b>calculation</b> ignores the shielding provided by the soil cover, which {{at the time of the}} EPA decision had reduced exposure to little more than background level, and makes very pessimistic modeling assumptions that greatly increase the projected risk, to deliberately focus on the high rather than low effect side.|$|E
40|$|How to {{reproduce}} the <b>risk</b> <b>calculation?</b> Requirements For performing the <b>risk</b> <b>calculation</b> {{you need to make}} sure to have R (>= 3. 3. 1, www. r-project. org) installed on your computer and at least 16 GB of RAM (required to perform the <b>risk</b> <b>calculation</b> for the “toilet flushing scenario”). An integrated development environment (IDE) like RStudio is helpful but is not required. However, the workflow described below assumes that you are using RStudio. Workflow To perform the <b>risk</b> <b>calculation</b> for the Old Ford case study you need to: 	 	Open the R script run_risk_calculation. R in RStudio, 	 	 	Run the whole script by clicking on the Source button in RStudío 	 	 	Wait until <b>risk</b> <b>calculation</b> & plotting is completed for all 3 scenarios 	 Important note: In case the R script crashes because of insufficient RAM memory for the 3 rd scenario you are still able {{to reproduce}} the results and plots for the first two. If this occurs just close RStudio, reopen the R script run_risk_calculation. R and just perform the lines 32 until 132. That’s all! Happy reproducing...|$|E
40|$|AbstractThis paper {{presents}} a rapid and simple <b>risk</b> <b>calculation</b> method for large and complex engineering systems, the simulated maximum entropy method (SMEM), {{which is based}} on integration of the advantages of the Monte Carlo and maximum entropy methods, thus avoiding the shortcoming of the slow convergence rate of the Monte Carlo method in <b>risk</b> <b>calculation.</b> Application of SMEM in the calculation of reservoir flood discharge risk shows that this method can make full use of the known information under the same conditions and obtain the corresponding probability distribution and the risk value. It not only greatly improves the speed, compared with the Monte Carlo method, but also provides a new approach for the <b>risk</b> <b>calculation</b> in large and complex engineering systems...|$|E
40|$|On {{recent years}} many {{countries}} {{have tried to}} reactivate their economies with a monetary policy that has established low interest rate levels as an effect of 2007 financial crisis. This rate reduction generates misleading results on Value at <b>Risk</b> <b>calculations</b> on certain instruments when rates tend to negative values...|$|R
40|$|This slide {{presentation}} {{provides an}} overview of the attempt to develop and demonstrate a methodology for the comparative assessment of risks across the entire portfolio of NASA projects and assets. It includes information about strategic risk identification, normalizing strategic <b>risks,</b> <b>calculation</b> of relative <b>risk</b> score, and implementation options...|$|R
40|$|The {{estimation}} of multivariate GARCH models remains a challenging task, even in modern computer environments. This manuscript shows how Independent Component Analysis {{can be used}} to estimate the Generalized Orthogonal GARCH model in a fraction of the time otherwise required. The proposed method is a two-step procedure, separating the {{estimation of}} the correlation structure from that of the univariate dynamics, thus facilitating the incorporation of non-Gaussian innovations distributions in a straightforward manner. The generalized hyperbolic distribution provides an excellent parametric description of financial returns data and is used for the univariate fits, but its convolutions, necessary for portfolio <b>risk</b> <b>calculations,</b> are intractable. This restriction is overcome by a saddlepoint approximation to the required distribution function, which is computationally cheap and extremely accurate — most notably in the tail, which is crucial for <b>risk</b> <b>calculations.</b> A simulation study and an application to stock returns demonstrate the validity of the procedure...|$|R
30|$|A Margin {{specifies}} {{how close}} values {{in a continuous}} field are required to be to be judged to be equal {{for the purpose of}} <b>risk</b> <b>calculation.</b>|$|E
40|$|Copyright © 2012 B. Mikat et al. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Preeclampsia {{is one of the}} leading causes of maternal and fetal morbidity and mortality. New molecular insights offer new possibilities of early diagnosis of elevated maternal risk. Maternal risk factors, biophysical parameters like Doppler examination of the uterine arteries and biochemical parameters allow early <b>risk</b> <b>calculation.</b> Preventive and effective therapeutic agents like acetylsalicylacid can be started in the early second trimester. This article reviews the diagnostic possibilities of early <b>risk</b> <b>calculation</b> to detect women having high risk for preeclampsia and the potential benefits for them, the offspring and health care systems. We provide <b>risk</b> <b>calculation</b> for preeclampsia as an important and sensible part of first trimester screening. 1...|$|E
30|$|It {{also needs}} to be {{acknowledged}} that an estimated mortality <b>risk</b> <b>calculation</b> by combining APACHE II and SAPS II (as available) may be less accurate than having a full set of both scores.|$|E
40|$|In this {{contribution}} {{we consider}} the overall risk given as the sum of random subrisks X_j {{in the context of}} value-at-risk (VaR) based <b>risk</b> <b>calculations.</b> If we assume that the undertaking knows the parametric distribution family subrisk X_j=X_j(θ_j), but does not know the true parameter vectors θ_j, the undertaking faces parameter uncertainty. To assess the appropriateness of methods to model parameter uncertainty for <b>risk</b> capital <b>calculation</b> we consider a criterion introduced in the recent literature. According to this criterion, we demonstrate that, in general, appropriateness of a risk capital model for each subrisk does not imply appropriateness of the model on the aggregate level of the overall risk.|$|R
50|$|Historically, {{public health}} {{regulations}} {{have been based}} on theoretical <b>risk</b> <b>calculations</b> according to known levels of chemical substances in air, water, soil, food, other consumer products and other sources of potential exposure. Human biomonitoring offers the opportunity to analyze the actual internal levels of bodily substances from all potential routes of exposure at one time, which may contribute to improving risk assessments.|$|R
40|$|Flood {{defence systems}} {{can be seen as}} {{multiple}} interdependent flood defences. This paper advances an approach for finding an optimal configuration for flood defence systems based on an economic cost–benefit analysis with an arbitrary number of interdependent flood defences. The proposed approach is based on a graph algorithm and is, thanks to some beneficial properties of the application, able to represent large graphs with strongly reduced memory requirements. Furthermore, computational efficiency is achieved by delaying cost calculations until they are actually needed by the graph algorithm. This significantly reduces the required number of computationally expensive flood <b>risk</b> <b>calculations.</b> In this paper, we conduct a number of case studies to compare the optimal paths found by the proposed approach with the results of competing methods that generate identical results. The proposed approach is set up in a generic way and implements the shortest-path approach for optimising cost–benefit analyses of interdependent flood defences with computationally expensive flood <b>risk</b> <b>calculations...</b>|$|R
40|$|Interventions {{to modify}} {{cardiovascular}} risk factors {{have the greatest}} bene t in individuals at greatest risk of a cardiovascular event, not necessarily those with the most extreme levels of any particular risk factor. Patients with vascular disease are readily identied, but <b>risk</b> <b>calculation</b> in those without disease provides a way of integrating several risk factors to identify individuals at greatest risk. Equations have been derived from different epidemiological studies (Framingham, PROCAM, etc.) to estimate the risk of different clinical end-points [coronary heart disease (CHD) or cardiovascular disease (CVD) ]. Some algorithms have been adapted to produce risk ’charts ’ or ’tables’; others have been modied to incorporate additional risk factors. Current UK guidelines all endorse calculation of CHD risk using the Framingham algorithm. This predicts risk well for some North European populations but less reliably in low-risk populations. It does not incorporate some risk factors, including serum triglycerides, family history, C-reactive protein, or homocysteine. <b>Risk</b> <b>calculation</b> {{should not be used}} in those with evidence of vascular disease, genetic hyperlipidaemia or a strong family history of CHD, and should be interpreted with caution in non-Caucasians. Measurement of high-density lipoprotein (HDL) -cholesterol is essential for accurate <b>risk</b> <b>calculation.</b> This has signicant cost and workload implications for the laboratory. The way a laboratory reports results should be designed {{in such a way as}} to facilitate simple and accurate <b>risk</b> <b>calculation.</b> Ann Clin Biochem 2002; 39 : 12 - 2...|$|E
40|$|Aims Current {{guidelines}} for lipid lowering as primaryprevention advocate a treatment threshold 10 -year {{coronary heart disease}} (CHD) risk of> 30 %. There is {{a variety of methods}} of calculating CHD risk, of varying complexity, which incorporate different factors. We aimed to determine if different methods of CHD <b>risk</b> <b>calculation</b> give different results and if a simpler method of CHD <b>risk</b> <b>calculation</b> could be developed for type 2 diabetes. Methods Using three recognised methods based on the Framingham equation, CHD risk was calculated for 200 consecutive patients with type 2 diabetes attending the clinic. A simple method of establishing those who should be treated with lipid-lowering agents wa...|$|E
40|$|In {{this article}} we {{described}} algorithm of <b>risk</b> <b>calculation</b> for technological equipment. This algorithm uses two different methods: methods of index and probability risk assessment. Using of both methods allows to avoid issues connected with data uncertainty in comparing with existing methods. ? ?????? ?????? ?? ???????????? ???????? ??????? ???????????? ????? ???????????? ????????. ???????????? ???????? ??????? ?? ?????????? ?????????? ????????? ?????? ? ?????? ?????-?????, ??? ????????? ???????? ???????????? ?????? ????? ? ??????????? ????????? ??????? ???????? ???????????????? ???????? ?????? ?? ????????? ? ????????????? ????????...|$|E
50|$|Coastal Risk uses {{data from}} the National Oceanic and Atmospheric Administration, the Federal Emergency Management Agency, the National Flood Insurance Program, the U.S. Army Corps of Engineers, {{high-resolution}} LIDAR, local tide gauges, and various other geographic information systems to create their cloud-based climate impact risk analyses. However, Coastal Risk's risk scores differ from FEMA flood maps by including sea level rise projections into their flood <b>risk</b> <b>calculations.</b>|$|R
40|$|Journalists {{are often}} blamed for {{producing}} scare stories. It {{seems to have}} been forgotten that many, perhaps most, modern scare stories are based on scientific <b>risk</b> <b>calculations,</b> and that journalists are not trained in scaring the wits out of people in that particular way. A more precise accusation might be that journalists are eager, unthinking and unquestioning conveyors of results from scientific <b>risk</b> <b>calculations.</b> <b>Calculation</b> of <b>risk</b> has become an important research product; a product fitting nicely into conventional journalistic storytelling, but the concept of risk tends to dilute value disagreement and conflict of interests into seemingly purely factual issues, leaving little room for political debate. Moreover, the cargo attitude of journalism is in conflict with the journalistic ideal of critical investigation and analysis on behalf of the public to stimulate common deliberation in the public sphere. Apparently, the production of scientific knowledge is excluded from the public sphere. Regarding discussions on science and technology, journalists will have to enquire into aspects of facts, values and social interests {{to live up to the}} ideal of investigation on behalf of the public. Several obstacles along this path can be identified, one of them being the commercialization of journalism in the media-industry and of scientific research in the knowledge-industry. Universities, in the search for a meaning of life, might consider providing a home for independent, reflexive journalism on science in a social context. ...|$|R
40|$|For {{determination}} {{of the risk of}} failure of ship structures during the operation, the method of calculation based on results of ultrasonic measurements of residual thickness has been presented. The result of the calculation is an indicator of risk for the items and groups of the hull construction bonds over time. As an example, <b>risks</b> <b>calculation</b> for elements and groups of elements structural tanker oil product depending on time is given...|$|R
30|$|Instability <b>risk</b> <b>calculation</b> {{of gravity}} dam based on Monte Carlo {{simulation}} {{can be divided}} into four steps: (1) determine the instability model function of gravity dam; (2) identify and quantify uncertainty of model factors; (3) simulate; (4) analyze results and calculate instability risk ratio.|$|E
40|$|BM&FBOVESPA {{is a world}} {{reference}} in risk and collateral management the fragility of risk management systems while highlighting stronger monitoring of the systemically important payment, All of these measures were adopted in Brazil back in March of 2002 when through {{the implementation of the}} new Brazilian Payment System, the nation’s clearinghouses, under the administration of the private sector and the supervision of the Central Bank of Brazil, assumed a fundamental role in risk management and the prevention of systemic crises. As a result the BM&FBOVESPA clearinghouses, which act as central counterparties to the equity, derivatives, advanced risk management structures that surpass those of many other countries. In the Brazilian exchange it is mandatory to register all transactions of derivatives (including OTC), foreign INDIVIDUALIZED <b>RISK</b> <b>CALCULATION</b> AND methodology. <b>Risk</b> <b>calculation</b> is based on the stress testing model which is conducted in quasi-real time (calculated several times throughout the day), and additional collateral is required whenever necessary. <b>Risk</b> <b>calculation</b> and collateral requirement are Collateral is pledged in the Clearinghouse custody the credit risk of commercial banks. REGULATION AND SUPERVISION robust regulation and supervision structure based on the self-regulation structure of the Exchanges, the assessment and ongoing supervision of risk and of Brazil, and the supervision of securities markets (including the derivatives market) carried out b...|$|E
3000|$|... “That is what {{government}} principals would like…But {{let me give}} you an example from the Health Council. We always explicitly made the point that there had to be interaction (between government and experts, rh), during problem structuring, in the models for <b>risk</b> <b>calculation</b> and risk assessment…” [...]...|$|E
50|$|To enable {{consideration}} of stochastic health <b>risk,</b> <b>calculations</b> are performed {{to convert the}} physical quantity absorbed dose into equivalent and effective doses, the details of which depend on the radiation type and biological context. For applications in radiation protection and dosimetry assessment the International Committee on Radiation Protection (ICRP) and International Commission on Radiation Units and Measurements (ICRU) have published recommendations and data which are used to calculate these.|$|R
40|$|This paper {{provides}} a new framework for modeling {{uncertainty in the}} input data for power system <b>risk</b> <b>calculations,</b> and the error bars that this places on the results. Differently from previous work, systematic error in unit availability probabilities is considered as well as random error, and a closed-form expression is supplied for the error bars on the results. This closed-form expression reveals the relative contribution of different sources of error much more transparently than iterative methods. The new approach is demonstrated using the thermal units connected to the Great Britain transmission system. The availability probabilities used are generic type availabilities, published rounded to the nearest 5 % by the system operator. Very wide error bars {{on the results of}} <b>risk</b> <b>calculations</b> result from the use of these probabilities; however, this is only revealed by modeling of the systematic error caused by the rounding. The approach is also used to investigate quantitatively the widely acknowledged view that comparing relative risks is a more robust use of simulated risk indices than stating absolute risk levels...|$|R
40|$|The {{deployment}} of next generation access networks (NGAs) is investigated from technical, regulatory and investment perspectives. A {{brief review of}} the possible network architectures and deployment scenarios of NGAs is provided. <b>Risk</b> <b>calculations</b> of these NGA scenarios are performed based on a fully detailed techno-economic model. Furthermore, {{the effectiveness of the}} European Commission's Recommendation on regulated access to NGAs, aiming to tackle the regulatory trade-off between encouraging investments and promoting competition, is discussed...|$|R
