473|6878|Public
5|$|<b>Resample</b> {{the current}} curve by placing new sample points at a uniform spacing, as {{measured}} by normalized arc length.|$|E
25|$|From 1950 to 1996, all the {{publications}} on Sequential Monte Carlo methodologies including the pruning and <b>resample</b> Monte Carlo methods introduced in computational physics and molecular chemistry, present natural and heuristic-like algorithms applied to different situations {{without a single}} proof of their consistency, nor a discussion on the bias of the estimates and on genealogical and ancestral tree based algorithms. The mathematical foundations and the first rigorous analysis of these particle algorithms are due to Pierre Del Moral in 1996. Branching type particle methodologies with varying population sizes were also developed {{in the end of}} the 1990s by Dan Crisan, Jessica Gaines and Terry Lyons, and by Dan Crisan, Pierre Del Moral and Terry Lyons. Further developments in this field were developed in 2000 by P. Del Moral, A. Guionnet and L. Miclo.|$|E
2500|$|In {{imbalanced}} datasets, {{where the}} sampling ratio {{does not follow}} the population statistics, one can <b>resample</b> the dataset in a conservative manner called minimax sampling. The minimax sampling has its origin in Anderson minimax ratio whose value is proved to be 0.5: in a binary classification, the class-sample sizes should be chosen equally. This ratio can be proved to be minimax ratio only under the assumption of LDA classifier with Gaussian distributions. The notion of minimax sampling is recently developed for a general class of classification rules, called class-wise smart classifiers. In this case, the sampling ratio of classes is selected so that the worst case classifier error over all the possible population statistics for class prior probabilities, would be the ...|$|E
30|$|The {{previous}} section {{talked about the}} general AMSSDR. Its solution {{is made up of}} three main components: (1) <b>resampling</b> selector; (2) systematic important resampling; and (3) rounding copy <b>resampling.</b> This current section talks about the AMSSDR selector that was taken note of in the {{previous section}}. The <b>resampling</b> selector’s main purpose is to alter the <b>resampling</b> operation between traditional variation <b>resampling</b> and traditional <b>resampling,</b> based on memory adaptation. This allows the operation of <b>resampling</b> in an optimum manner, based on the computing devices’ physical memory requirements. Code 1 illustrates the pseudocode that was used for the <b>resampling</b> selector. Primarily, the pseudocode determines the total quantity of physical memory that is presently used by a computing device. If it is determined to be beyond 1536  MB, the <b>resampling</b> selector will select the <b>resampling</b> rounding. On the other hand, the <b>resampling</b> selector will select the systematic <b>resampling</b> algorithm to serve as its <b>resampling</b> operation. The next section talks about the operation systematic <b>resampling.</b>|$|R
3000|$|There {{are several}} kinds of <b>resampling</b> methods, and the basic <b>resampling</b> method is called the single {{distribution}} <b>resampling.</b> Furthermore, single distribution <b>resampling</b> is subdivided into two categories called: traditional variation <b>resampling</b> and traditional <b>resampling.</b> Using traditional <b>resampling</b> benefits computing devices that need a single sampling process for every [...] j [...] cycle (for instance, computing devices that only have low memory requirements). On the other hand, the utilisation of traditional variation <b>resampling</b> is beneficial for computing devices that require {{more than a single}} sampling process for every j cycle (for instance, computing devices that have a high memory requirement). Although computing devices that have high memory requirement do not normally have problems with memory consumption, traditional variation <b>resampling</b> is often thought to be more appropriate for usage in this environment because it is faster than traditional <b>resampling</b> [58].|$|R
40|$|We {{describe}} an algorithm for perfect weighted-random <b>resampling</b> {{of a population}} with time complexity O(m + n) for <b>resampling</b> m inputs to produce n outputs. This algorithm is an incremental improvement over standard <b>resampling</b> algorithms. Our <b>resampling</b> algorithm is parallelizable, with linear speedup. Linear-time <b>resampling</b> yields notable performance improvements in our motivating example of Sequentia...|$|R
2500|$|In {{computational}} physics {{and more specifically}} in quantum mechanics, the ground state energies of quantum systems {{is associated with the}} top of the spectrum of Schrödinger's operators. The Schrödinger equation is the quantum mechanics version of the Newton's second law of motion of classical mechanics (the mass times the acceleration is the sum of the forces). This equation represents the wave function (a.k.a. the quantum state) evolution of some physical system, including molecular, atomic of subatomic systems, as well as macroscopic systems like the universe. The solution of the imaginary time Schrödinger equation (a.k.a. the heat equation) is given by a Feynman-Kac distribution associated with a free evolution [...] Markov process (often represented by Brownian motions) in the set of electronic or macromolecular configurations and some potential energy function. The long time behavior of these nonlinear semigroups is related to top eigenvalues and ground state energies of [...] Schrödinger's operators. [...] The genetic type mean field interpretation of these Feynman-Kac models are termed <b>Resample</b> Monte Carlo, or Diffusion Monte Carlo methods. These branching type evolutionary algorithms are based on mutation and selection transitions. During the mutation transition, the walkers evolve randomly and independently in a potential energy landscape on particle configurations. [...] The mean field selection process (a.k.a. quantum teleportation, population reconfiguration, resampled transition) is associated with a fitness function that [...] reflects the particle absorption in an energy well. Configurations with low relative energy are more likely to duplicate. In molecular chemistry, and statistical physics Mean field particle methods are also used to sample Boltzmann-Gibbs measures associated with some cooling schedule, and to compute their normalizing constants (a.k.a. free energies, or partition functions).|$|E
5000|$|Instead, we use bootstrap, {{specifically}} case resampling, {{to derive}} {{the distribution of}} [...] We first <b>resample</b> the data to obtain a bootstrap <b>resample.</b> An example of the first <b>resample</b> might look like this X1* [...] x2, x1, x10, x10, x3, x4, x6, x7, x1, x9. Note {{that there are some}} duplicates since a bootstrap <b>resample</b> comes from sampling with replacement from the data. Note also that the number of data points in a bootstrap <b>resample</b> is equal to the number of data points in our original observations. Then we compute the mean of this <b>resample</b> and obtain the first bootstrap mean: μ1*. We repeat this process to obtain the second <b>resample</b> X2* and compute the second bootstrap mean μ2*. If we repeat this 100 times, then we have μ1*, μ2*, …, μ100*. This represents an empirical bootstrap distribution of sample mean. From this empirical distribution, one can derive a bootstrap confidence interval for the purpose of hypothesis testing.|$|E
5000|$|The Monte Carlo {{algorithm}} for case resampling {{is quite}} simple. First, we <b>resample</b> the data with replacement, {{and the size}} of the <b>resample</b> must be equal to the size of the original data set. Then the statistic of interest is computed from the <b>resample</b> from the first step. We repeat this routine many times to get a more precise estimate of the Bootstrap distribution of the statistic.|$|E
3000|$|... {{specific}} computing devices’ memory gives developers several difficulties as {{a result}} of the increased effort and time needed for the development of a particle filter. Thus, one needs a new sequential <b>resampling</b> algorithm that is flexible enough to allow it to be used with various computing devices. Therefore, this paper formulated a new single distribution <b>resampling</b> called the adaptive memory size-based single distribution <b>resampling</b> (AMSSDR). This <b>resampling</b> method integrates traditional variation <b>resampling</b> and traditional <b>resampling</b> in one architecture. The algorithm changes the <b>resampling</b> algorithm using the memory in a computing device. This helps the developer formulate a particle filter without over considering the computing devices’ memory utilisation during the development of different particle filters. At the start of the operational process, it uses the AMSSDR selector to choose an appropriate <b>resampling</b> algorithm (for example, rounding copy <b>resampling</b> or systematic <b>resampling),</b> based on the current computing devices’ physical memory. If one chooses systematic <b>resampling,</b> the <b>resampling</b> will sample every particle for every cycle. On the other hand, if it chooses the rounding copy <b>resampling,</b> the <b>resampling</b> will sample more than one of each cycle’s particle. This illustrates that the method (AMSSDR) being proposed is capable of switching <b>resampling</b> algorithms based on various physical memory requirements. The aim of the authors is to extend this research in the future by applying their proposed method in various emerging applications such as real-time locator systems or medical applications.|$|R
3000|$|The {{preceding}} section {{talked about}} the AMSSDR selector’s operation, which was utilised in the switching of the <b>resampling</b> operation between traditional variation <b>resampling</b> and traditional <b>resampling,</b> the basis of which is memory adaptation. Systematic <b>resampling</b> is the traditional <b>resampling</b> algorithm used, while rounding copy <b>resampling</b> is the traditional variation <b>resampling</b> algorithm implemented. This section {{will talk about the}} operation of systematic <b>resampling</b> (Code 2 can be used to refer to the pseudocode), which may be utilised the physical memory falls below 1.5  GB (1536  MB). First, it will make a sample [...] u_ 1 ∼ U([...] 0, 1 /N) and give a definition to [...] u_i = u_ 1 + ([...] i - 1)/N for i =  2,…N. Lastly, it utilises u [...]...|$|R
40|$|The <b>resampling</b> of discrete-time signals {{where the}} {{underlying}} analog signal is non-bandlimited is considered in this paper. We extend the generalized sampling theory developed {{based on the}} principle of consistency to <b>resampling.</b> Realizing the <b>resampling</b> system has both discrete input and output, the performance of the <b>resampling</b> filter is considered in l 2 instead of the traditionally used L 2. We show that the performance of the <b>resampling</b> system depends on the <b>resampling</b> rate instead of the actual interpolating kernels. The theory can be applied to image processing applications like zooming to provide better response to high frequency components. Since the <b>resampling</b> process is discrete in nature, our filter designed to optimize <b>resampling</b> in l 2 is shown to outperform other techniques designed in L 2. 1...|$|R
5000|$|... resamp <b>Resample</b> the 1st {{dimension}} of a 2-dimensional function f(x1,x2) ...|$|E
50|$|Another {{approach}} to bootstrapping in regression problems is to <b>resample</b> residuals. The method proceeds as follows.|$|E
5000|$|<b>Resample</b> {{the current}} curve by placing new sample points at a uniform spacing, as {{measured}} by normalized arc length.|$|E
40|$|The {{asymptotic}} {{distribution of the}} bootstrap sample mean depends on the <b>resampling</b> intensity. This paper explores the sensitivity of that distribution against different <b>resampling</b> intensities. It is generally assumed that small <b>resampling</b> sizes make the bootstrap work. However, we will show that the bootstrap mean can only be highly unstable for small <b>resampling</b> intensities. Our setup considers <b>resampling</b> from a triangular array of row-wise independent and identically distributed random variables satisfying the Central Limit Theorem. Bootstrap sample mean Asymptotic stability Asymptotic distribution Triangular arrays <b>Resampling</b> intensity...|$|R
3000|$|... is more {{effective}} than X̃_t. It was also observed that the amount of <b>resampled</b> particles Nis not equal to the amount of propagated particles all the time. Traditional <b>resampling</b> methods help maintain their value, and, generally, M = N. Lastly, for most <b>resampling</b> methods, the particle weights after <b>resampling</b> become equal. However, <b>resampling</b> may produce undesired effects, such as sample impoverishment. During <b>resampling,</b> it is likely for low weighted particles to be removed. Thus, the diversity of the particles is reduced [32, 44 – 48]. For instance, if a small amount of particles of X [...]...|$|R
30|$|The {{simulation}} {{makes use}} of various <b>resampling</b> methods. However, identical observation data is retained at every time step, and there will the same starting quantity of particles N for every filter. However, it will omit the selective <b>resampling</b> strategy that only implements <b>resampling</b> at certain steps. The filter estimate that is provided by all the particles’ weighted mean is obtained after each filter is <b>resampled</b> {{so that it can}} directly reflect the effect that <b>resampling</b> has. The results of these filters utilising various <b>resampling</b> methods are plotted in the following figures.|$|R
5000|$|... 12-voice {{polyphony}} (<b>resample</b> polyphony: 4 mono voices OR 1 stereo {{voice and}} 2 mono voices OR 2 stereo voices) ...|$|E
5000|$|From , {{sample a}} {{reconstruction}} [...] {{of the visible}} units, then <b>resample</b> the hidden activations [...] from this. (Gibbs sampling step) ...|$|E
5000|$|CCDPACK : A {{package of}} {{programs}} for reducing CCD-like data. They allow you to debias, remove dark current, pre-flash, flatfield, register, <b>resample,</b> normalize and combine your data.|$|E
40|$|AbstractUniform <b>resampling</b> is {{the easiest}} to apply and is a general recipe for all problems, but it may require a large {{replication}} size B. To save computational effort in uniform <b>resampling,</b> balanced bootstrap <b>resampling</b> is proposed to change the bootstrap <b>resampling</b> plan. This <b>resampling</b> plan is effective for approximating {{the center of the}} bootstrap distribution. Therefore, this paper applies it to neural model selection. Numerical experiments indicate {{that it is possible to}} considerably reduce the replication size B. Moreover, the efficiency of balanced bootstrap <b>resampling</b> is also discussed in this paper...|$|R
40|$|<b>Resampling</b> {{methods are}} popular tools for {{exploring}} the statistical structure of neural spike trains. In many applications it is desirable to have <b>resamples</b> that preserve certain non-Poisson properties, like refractory periods and bursting, and that are also robust to trial-to-trial variability. “Pattern jitter ” is a <b>resampling</b> technique that accomplishes this by preserving the recent spiking history of all spikes and constraining <b>resampled</b> spikes to remain close to their original positions. The <b>resampled</b> spike times are maximally random up to these constraints. Dynamic programming is used to create an efficient <b>resampling</b> algorithm. ...|$|R
3000|$|... has the {{greatest}} weights, numerous <b>resampled</b> particles will {{end up being the}} same (there will be lesser distinct particles in X̃_t). The next effect is on the particle filter’s speed of implementation. Often, particle filter is used to process signals when {{there is a need for}} the real-time processing of observations. An effective solution is to parallelise the particle filter. Later, it will be demonstrated how the process of parallelising the <b>resampling</b> can be a challenging one. Resampling’s undesired effects have encouraged researchers to develop advanced <b>resampling</b> methods. These methods have a vast range of features, which include a variable amount of particles, the avoidance of rejecting low weighted particles, the removal of the restriction of the <b>resamples</b> needing equal weights, and the introduction of parallel frameworks that can be used during <b>resampling.</b> Several decisions are needed when conducting <b>resampling,</b> including: specifying the sampling strategy; choosing the distribution for <b>resampling,</b> determining the <b>resampled</b> size; and choosing the <b>resampling</b> frequency.|$|R
5000|$|In {{univariate}} problems, it {{is usually}} acceptable to <b>resample</b> the individual observations with replacement ("case resampling" [...] below). In small samples, a parametric bootstrap approach might be preferred. For other problems, a smooth bootstrap will likely be preferred.|$|E
5000|$|The 'exact' {{version for}} case {{resampling}} is similar, but we exhaustively enumerate every possible <b>resample</b> {{of the data}} set. This can be computationally expensive as there are a total of [...] different resamples, where n {{is the size of}} the data set.|$|E
50|$|The {{algorithm}} {{described above}} lends itself well to parallelization, since resampling two independent events , i.e. , in parallel {{is equivalent to}} resampling A, B sequentially. Hence, at each iteration of the main loop one can determine the maximal set of independent and satisfied events S and <b>resample</b> all events in S in parallel.|$|E
3000|$|..., {{indicates}} that estimates {{are likely to}} be inaccurate. The key to addressing this is to introduce <b>resampling.</b> The basic idea of <b>resampling</b> is to eliminate samples with low importance weights and replicate samples with larger weights 12. While {{there are a number of}} variants of the <b>resampling</b> algorithm, they all consist of two core stages: calculating how many copies of each sample to generate and generating that number of copies of each sample. The different <b>resampling</b> variants differ in terms of how they calculate the number of copies to generate. We focus here on minimum variance <b>resampling</b> (also known as systematic <b>resampling)</b> which minimises the errors inevitably introduced by the <b>resampling</b> process (and is discussed in more detail in Section 4.6). The use of <b>resampling</b> with SIS is often known as the sampling importance <b>resampling</b> (SIR) filter and has been at the heart of particle filters since their invention [1, 24, 25].|$|R
40|$|Part 1 : Research PapersInternational audienceTo create convincing forged images, {{manipulated}} images {{or parts}} of them are usually exposed to some geometric operations which require a <b>resampling</b> step. Therefore, detecting traces of <b>resampling</b> became an important approach {{in the field of}} image forensics. In this paper, we revisit existing techniques for <b>resampling</b> detection and design some targeted attacks in order to assess their reliability. We show that the combination of multiple <b>resampling</b> and hybrid median filtering works well for hiding traces of <b>resampling.</b> Moreover, we propose an improved technique for detecting <b>resampling</b> using image forensic tools. Experimental evaluations show that the proposed technique is good for <b>resampling</b> detection and more robust against some targeted attacks...|$|R
40|$|This paper {{considers}} {{the effect of}} the <b>Resampling</b> schemes in the behavior of Particle Filter (PF) based robot localizer. The investigated schemes are Multinomial <b>Resampling,</b> Residual <b>Resampling,</b> Residual Systematic <b>Resampling,</b> Stratified <b>Resampling</b> and Systematic <b>Resampling.</b> An algorithm is built in Matlab environment to host these schemes. The performances are evaluated in terms of computational complexity and error from ground truth and the results are reported. The results showed that the localization plan which adopts the Systematic or Stratified <b>Resampling</b> scheme achieves higher accuracy localization while decreasing consumed computational time. However, the difference is not significant. Moreover, a particle excitation strategy is proposed. This strategy achieved significant improvement in the behavior of PF based robot localization...|$|R
5000|$|The Wild bootstrap, {{proposed}} originally by Wu (1986), is suited {{when the}} model exhibits heteroskedasticity. The idea is, like the residual bootstrap, {{to leave the}} regressors at their sample value, but to <b>resample</b> the response variable based on the residuals values. That is, for each replicate, one computes a new [...] based on ...|$|E
50|$|In {{regression}} problems, {{the explanatory}} variables are often fixed, {{or at least}} observed with more control than the response variable. Also, {{the range of the}} explanatory variables defines the information available from them. Therefore, to <b>resample</b> cases means that each bootstrap sample will lose some information. As such, alternative bootstrap procedures should be considered.|$|E
50|$|From 1950 to 1996, all the {{publications}} on particle filters, genetic algorithms, including the pruning and <b>resample</b> Monte Carlo methods introduced in computational physics and molecular chemistry, present natural and heuristic-like algorithms applied to different situations {{without a single}} proof of their consistency, nor a discussion on the bias of the estimates and on genealogical and ancestral tree based algorithms.|$|E
40|$|AbstractThe {{asymptotic}} {{distribution of the}} bootstrap sample mean depends on the <b>resampling</b> intensity. This paper explores the sensitivity of that distribution against different <b>resampling</b> intensities. It is generally assumed that small <b>resampling</b> sizes make the bootstrap work. However, we will show that the bootstrap mean can only be highly unstable for small <b>resampling</b> intensities. Our setup considers <b>resampling</b> from a triangular array of row-wise independent and identically distributed random variables satisfying the Central Limit Theorem...|$|R
40|$|In {{this report}} a {{comparison}} is made between four frequently encountered <b>resampling</b> algorithms for particle filters. A theoretical framework is introduced {{to be able}} to understand and explain the differences between the <b>resampling</b> algorithms. This facilitates a comparison of the algorithms based on <b>resampling</b> quality and on computational complexity. Using extensive Monte Carlo simulations the theoretical results are verified. It is found that systematic <b>resampling</b> is favourable, both in <b>resampling</b> quality and computational complexity...|$|R
30|$|The {{preceding}} section {{considered the}} used {{memory of the}} AMSSDR technique. This present section reviews {{the conclusion of the}} suggested method (AMSSDR). Restricting the use of single distribution <b>resampling</b> in case of the memory of specific computing devices causes difficulties for the developer, because of the additional time and effort needed to develop a particle filter. Hence, a new sequential <b>resampling</b> technique is needed, one with an amount which requires memory size depending on the memory requirement of computing devices. In this research, a new single distribution <b>resampling</b> technique has been created, known as AMSSDR, which is based on the combination of traditional <b>resampling</b> technique and traditional variation <b>resampling</b> technique in a <b>resampling</b> architecture. The technique will switch the <b>resampling</b> algorithm which is based on memory in a computer. The execution of this algorithm will facilitate developers to more effortlessly develop a particle filter, with no need to give a big degree of consideration to memory usage in a computing device when including different particle filter development. Initially, in the operational process, the AMSSDR selector will be utilised to select an appropriate <b>resampling</b> algorithm (for instance, systematic <b>resampling</b> technique or rounding copy <b>resampling</b> technique), as per the physical memory available in present computing devices. Following that, the outcomes show that, if systematic <b>resampling</b> is chosen, the <b>resampling</b> will take sample for each particle for each j cycle, while if the rounding copy <b>resampling</b> is selected, the <b>resampling</b> will take samples for more than one unit of each j cycle. Hence, this demonstrates that the suggested method (AMSSDR) is capable of switching and <b>resampling</b> algorithms in various physical memory requirements. The authors of this paper wish to extend this work gradually by implementing their suggested method {{in a number of different}} promising applications (for instance, in medical software [66] or real-time locator systems [67].|$|R
