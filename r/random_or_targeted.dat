27|10000|Public
5000|$|... (3) Uniform {{standards}} for operations. In cities with populations of 20,000 or more, minor decoy operations must be conducted on either a <b>random</b> <b>or</b> <b>targeted</b> basis.|$|E
30|$|Previous {{studies have}} looked at the {{differences}} in brain network robustness and its tolerance to removal of nodes (either targeted or random) [18], including the removal of rich-club regions [19, 20]. To test if dimensionality-reduction techniques capture visually meaningful and interpretable information in a 3 D environment, we used similar removal strategies to understand the structural connectome’s topology after <b>random</b> <b>or</b> <b>targeted</b> node removal.|$|E
40|$|Most mouse {{genetics}} laboratories maintain mouse strains {{that require}} genotyping {{in order to}} identify the genetically modified animals. The plethora of mutagenesis strategies and publicly available mouse alleles means that any one laboratory may maintain alleles with <b>random</b> <b>or</b> <b>targeted</b> insertions of orthologous or unrelated sequences as well as <b>random</b> <b>or</b> <b>targeted</b> deletions and point mutants. Many experiments require that different strains be cross bred conferring the need to genotype progeny at more than one locus. In contrast to the range of new technologies for mouse mutagenesis, genotyping methods have remained relatively static with alleles typically discriminated by agarose gel electrophoresis of PCR products. This requires a large amount of researcher time. Additionally it is susceptible to contamination of future genotyping experiments because it requires that tubes containing PCR products be opened for analysis. Progress has been made with the genotyping of mouse point mutants because a range of new high-throughput techniques have been developed for the detection of Single Nucleotide Polymorphisms. Some of these techniques are suitable for genotyping point mutants but do not detect insertion or deletion alleles. Ideally, mouse genetics laboratories would use a single, high-throughput platform that enables closed-tube analysis to genotype the entire range of possible insertion and deletion alleles and point mutants. Here we show that High Resolution Melt Analysis meets these criteria, it is suitable for closed-tube genotyping of all allele types and current genotyping assays can be converted to this technolog...|$|E
50|$|Pizza places may {{be subject}} to prank orders for {{numerous}} pizzas <b>or</b> to <b>random</b> houses <b>or</b> a <b>target</b> house. A prank order may cost businesses money and aggravation, resulting in the restaurant throwing away the unpaid pizzas. For example, in November 2010 in Amherst, Massachusetts, a man claiming to be part of Bob Dylan's crew placed an order for 148 pizzas which cost nearly $4,000. Prank callers have been fined in Singapore for placing false orders.|$|R
30|$|Dendrochronology and fire-scar dating {{has been}} used {{extensively}} in {{other regions of the}} western United States to evaluate long-term changes in fire frequency and extent in relation to land use history and climate (e.g., Swetnam 1993, Veblen et al. 1999, Grissino-Mayer and Swetnam 2000). Recent studies have demonstrated the utility of spatially distributed fire-scar data (collected via <b>random,</b> systematic, <b>or</b> <b>targeted</b> sampling) to reconstruct spatial extent of past fires within sampled areas (e.g., Heyerdahl et al. 2001, van Horne and Fulé 2006, Hessl et al. 2007). Furthermore, well-distributed spatial networks of fire-scar samples have been shown to accurately reconstruct the spatial extent and fire frequency patterns in independently mapped records of fires from the twentieth century (Collins and Stephens 2007, Shapiro-Miller et al. 2007, Farris et al. 2010).|$|R
40|$|Metros (heavy {{rail transit}} systems) are {{integral}} parts of urban transportation systems. Failures in their operations can have serious impacts on urban mobility, and measuring their robustness is therefore critical. Moreover, as physical networks, metros {{can be viewed}} as network topological entities, and as such they possess measurable network properties. In this paper, by using network science and graph theoretical concepts, we investigate both theoretical and experimental robustness metrics (i. e., the robustness indicator, the effective graph conductance, and the critical thresholds) and their performance in quantifying the robustness of metro networks under <b>random</b> failures <b>or</b> <b>targeted</b> attacks. We find that the theoretical metrics quantify different aspects of the robustness of metro networks. In particular, the robustness indicator captures the number of alternative paths and the effective graph conductance focuses on the length of each path. Moreover, the high positive correlation between the theoretical metrics and experimental metrics and the negative correlation within the theoretical metrics provide significant insights for planners to design more robust system while accommodating for transit specificities (e. g., alternative paths, fast transferring) ...|$|R
40|$|We {{introduce}} a graph generating model aimed at representing {{the evolution of}} protein interaction networks. The model {{is based on the}} hypotesis of evolution by duplications and divergence of the genes which produce proteins. The obtained graphs shows multifractal properties recovering the absence of a characteristic connectivity as found in real data of protein interaction networks. The error tolerance of the model to <b>random</b> <b>or</b> <b>targeted</b> damage is in very good agreement with the behavior obtained in real protein networks analysis. The proposed model is a first step in the identification of the evolutionary dynamics leading to the development of protein functions and interactions. Comment: 9 pages, 3 figure...|$|E
40|$|Various {{strategies}} {{have been used}} to isolate genes that participate in the regulation of mouse development. Gene families that have been identified {{on the basis of their}} homology to motifs within Drosophila control genes or human transcription factor genes, namely homeobox (Hox), paired-box (Pax), and POU genes, can be compared with respect to gene organization, structure, and expression patterns. The functions of these genes can be analyzed molecularly in vitro and in vivo with the use of available mouse mutants or transgenic mice. In addition, it has been possible to generate gain- or loss-of-function mutations by <b>random</b> <b>or</b> <b>targeted</b> introduction of transgenes. Models derived from these studies can reveal the successive steps of developmental control on a genetic level...|$|E
30|$|An {{effective}} and independent monitoring system is required. It must {{be possible to}} determine whether an individual PPP user is, indeed, working to minimise the amounts applied, and checks {{must be carried out}} frequently enough to ensure effectiveness. The legal obligation of farmers to document their use of pesticides (in application logs) provides a suitable basis for traceability. The responsibility for checking compliance with the minimisation requirement could again lay with the plant protection services at the federal state level. Their remit would be to define criteria for good farming practice for plant protection in accordance with the minimisation requirement, taking into account the regional conditions and “pest pressure”, and to check compliance. This calls for <b>random</b> <b>or</b> <b>targeted</b> inspections of the application records.|$|E
40|$|The {{recent article}} by Dalkvist, Mossbridge, and Westerlund (2014) on {{expectation}} bias in presentiment studies discussed an important methodological problem, but included a controversial recommendation and two key comments {{that are not}} correct. Presentiment studies investigate whether physiological measures indicate that a person can unconsciously and precognitively anticipate a random stimulus. The most common strategy for analyzing the data has been to compare the average values of the observed physiological measures preceding {{the different types of}} random stimuli. This analysis strategy reverses the traditional analysis for a typical ESP experiment, such as a participant pushing a button to predict which light will be randomly selected. The traditional analysis uses the button press or response to predict the <b>random</b> light <b>or</b> <b>target.</b> As described by Burdick and Kelly (1977, p. 93), The response array is taken as fixed (in fact, it is immaterial where it came from, and this underlies the great generality of the method). The statistical problem is to evaluate the probability of obtaining a number of hits as large or larger than that observed, given the response array...|$|R
40|$|Two Extended Kalman filter routines, one using a {{one-step}} estimation/prediction and {{the other}} a sequential approach, were developed and compared to provide real time estimates of target positions on the three dimensional underwater tracking range at Naval Underwater Weapons Engineering Station, Keyport, Washington. Inputs to the routines were acoustic pulse transit times from the target to receiving array elements which are non-linear functions of the position coordinates. These inputs were linearized and the filter gains calculated on-line. Simulated runs were conducted for tracks in the area of one hydrophone array and for tracks that transited through multiple arrays. It was found that the sequential estimate routine exhibited better performance in recovering from transients caused by <b>random</b> measurement noise <b>or</b> <b>target</b> movement. [URL] United States Nav...|$|R
40|$|Active disease {{surveillance}} during epidemics is {{of utmost}} importance in detecting and eliminating new cases quickly, and targeting such surveillance to high-risk individuals is considered more efficient than applying a random strategy. Contact tracing {{has been used as}} a form of at-risk targeting, and a variety of mathematical models have indicated that it is likely to be highly efficient. However, for fast-moving epidemics, resource constraints limit the ability of the authorities to perform, and follow up, contact tracing effectively. As an alternative, we present a novel real-time Bayesian statistical methodology to determine currently undetected (occult) infections. For the UK foot-and-mouth disease (FMD) epidemic of 2007, we use real-time epidemic data synthesized with previous knowledge of FMD outbreaks in the UK to predict which premises might have been infected, but remained undetected, at any point during the outbreak. This provides both a framework for targeting surveillance in the face of limited resources and an indicator of the current severity and spatial extent of the epidemic. We anticipate that this methodology will be of substantial benefit in future outbreaks, providing a compromise between targeted manual surveillance and <b>random</b> <b>or</b> spatially <b>targeted</b> strategies. ...|$|R
40|$|Central to {{the study}} of {{chromosome}} biology are techniques that permit the purification of small chromatin sections for analysis of associated DNA and proteins, including histones. Chromatin purification protocols vary greatly in the extent of chemical cross-linking used to prevent protein dissociation/re-association during isolation. Particularly for genome-wide analyses, chromatin purification requires a balanced level of fixation that captures native protein-protein and protein/ DNA interactions, yet leaving chromatin sections soluble and accessible to affinity reagents. We have applied a relative quantification methodology called I-DIRT (isotopic differentiation of interactions as <b>random</b> <b>or</b> <b>targeted)</b> for optimizing levels of chemical cross-linking for affinity purification of cognate chromatin sections. We show that fine-tuning of chemical cross-linking is necessary for isolation of chromatin sections when minimal histone/protein exchange is required...|$|E
40|$|There {{are only}} 7 cases {{reported}} in the literature of squamous cell cancer of the bladder in patients performing intermittent self-catheterization (ISC). We report on an eighth case, and the first case described in a patient with a Mitrofanoff continent appendicovesicostomy. A description of the case and review of the literature are presented. Risk factors for squamous cell cancer include recurrent urinary tract infections, keratinising squamous metaplasia (leukoplakia) and local mucosal trauma from intermittent self-catheterization. There is no recognized or validated monitoring program for patients performing ISC who may also have these risk factors. Reasonable protocols may include regular urinary cytology and cystoscopy with <b>random</b> <b>or</b> <b>targeted</b> bladder biopsies. Squamous cell cancer may present late in this cohort of patients and is associated with a dismal prognosis...|$|E
40|$|Protein–protein {{interactions}} modulate cellular functions {{ranging from}} the activity of enzymes to signal transduction cascades. A technology termed transient isotopic differentiation of interactions as <b>random</b> <b>or</b> <b>targeted</b> (transient I-DIRT) is described for the identification of stable and transient protein–protein interactions in vivo. The procedure combines mild in vivo chemical cross-linking and non-stringent affinity purification to isolate low abundance chromatin-associated protein complexes. Using isotopic labeling and mass spectrometric readout, purified proteins are categorized {{with respect to the}} protein ‘bait’ as stable, transient, or contaminant. Here we characterize the local interactome of the chromatin-associated NuA 3 histone lysine-acetyltransferase protein complex. We describe transient associations with the yFACT nucleosome assembly complex, RSC chromatin remodeling complex and a nucleosome assembly protein. These novel, physical associations with yFACT, RSC, and Nap 1 provide insight into the mechanism of NuA 3 -associated transcription and chromatin regulation...|$|E
40|$|The pivotal {{quality of}} {{proximity}} graphs is connectivity, i. e. all nodes in the graph {{are connected to}} one another either directly or via intermediate nodes. These types of graphs are robust, i. e., {{they are able to}} function well even if they are subject to limited removal of elementary building blocks, as it may occur for <b>random</b> failures <b>or</b> <b>targeted</b> attacks. Here, we study how the structure of these graphs is affected when nodes get removed successively until an extensive fraction is removed such that the graphs fragment. We study different types of proximity graphs for various node removal strategies. We use different types of observables to monitor the fragmentation process, simple ones like number and sizes of connected components, and more complex ones like the hop diameter and the backup capacity, which is needed to make a network N- 1 resilient. The actual fragmentation turns out to be described by a second order phase transition. Using finite-size scaling analyses we numerically assess the threshold fraction of removed nodes, which is characteristic for the particular graph type and node deletion scheme, that suffices to decompose the underlying graphs. Comment: 12 pages, 9 figure...|$|R
40|$|Spread of {{computer}} viruses can be modeled as the SIS (susceptible-infected-susceptible) epidemic propagation. We {{show that in}} order to ensure the <b>random</b> immunization <b>or</b> the <b>targeted</b> immunization effectively prevent computer viruses propagation on homogeneous networks, we should install antivirus programs in every computer node and frequently update those programs. This may produce large work and cost to install and update antivirus programs. Then we propose a new policy called "network monitors" to tackle this problem. In this policy, we only install and update antivirus programs for small number {{of computer}} nodes, namely the "network monitors". Further, the "network monitors" can monitor their neighboring nodes' behavior. This mechanism incur relative small cost to install and update antivirus programs. We also indicate that the policy of the "network monitors" is efficient to protect the network's safety. Numerical simulations confirm our analysis...|$|R
30|$|Moreover, all {{of these}} {{solutions}} lack the ability to properly identify which applications are truly outdated. Since this information {{is a prerequisite for}} the actual update process, it is a crucial step in the process of keeping (dormant) virtual machines in a Cloud or a virtualized Grid computing environment up-to-date. While such a check is easy to perform for running virtual machines, because of the commonly used package management systems on Linux platforms and automatic update facilities on Windows platforms, it is again a problem with dormant virtual machines. Even if virtual machines are kept up to date, the installed software might still contain design flaws or software vulnerabilities not fixed with the latest update. Thus, only checking for updates alone is not sufficient. Furthermore, machines used in a public IaaS environment are subject to external attacks, i.e., they might be a selected <b>or</b> <b>random</b> <b>target</b> chosen by scripts. Therefore, it is indispensable to continuously analyze the used virtual machines and take proactive countermeasures such as patching the revealed flaws.|$|R
40|$|Recent {{work on the}} internet, social networks, and {{the power}} grid has {{addressed}} the resilience of these networks to either <b>random</b> <b>or</b> <b>targeted</b> deletion of network nodes. Such deletions include, for example, the failure of internet routers or power transmission lines. Percolation models on random graphs provide a simple representation of this process, but have typically been limited to graphs with Poisson degree distribution at their vertices. Such graphs are quite unlike real world networks, which often possess power-law or other highly skewed degree distributions. In this paper we study percolation on graphs with completely general degree distribution, giving exact solutions {{for a variety of}} cases, including site percolation, bond percolation, and models in which occupation probabilities depend on vertex degree. We discuss the application of our theory to the understanding of network resilience. Comment: 4 pages, 2 figure...|$|E
40|$|We {{introduce}} a graph generating model aimed at representing {{the evolution of}} protein interaction networks. The model {{is based on the}} hypotesis of evolution by duplications and divergence of the genes which produce proteins. The obtained graphs shows multifractal properties recovering the absence of a characteristic connectivity as found in real data of protein interaction networks. The error tolerance of the model to <b>random</b> <b>or</b> <b>targeted</b> damage is in very good agreement with the behavior obtained in real protein networks analysis. The proposed model is a first step in the identification of the evolutionary dynamics leading to the development of protein functions and interactions. PACS numbers: 89. 75. -k, 87. 15. Kg, 87. 23. Kg Typeset using REVTEX 1 The complete genome sequencing gives {{for the first time the}} means to analyze organisms on a genomic scale. This implies the understanding of the role of a huge number of gene products and their interactions. For instance, it becomes a fundamental task to assig...|$|E
40|$|We {{study the}} {{robustness}} properties of multiplex networks consisting of {{multiple layers of}} distinct types of links, focusing {{on the role of}} correlations between degrees of a node in different layers. We use generating function formalism to address various notions of the network robustness relevant to multiplex networks such as the resilience of ordinary- and mutual connectivity under <b>random</b> <b>or</b> <b>targeted</b> node removals as well as the biconnectivity. We found that correlated coupling can affect the structural robustness of multiplex networks in diverse fashion. For example, for maximally-correlated duplex networks, all pairs of nodes in the giant component are connected via at least two independent paths and network structure is highly resilient to random failure. In contrast, anti-correlated duplex networks are on one hand robust against targeted attack on high-degree nodes, {{but on the other hand}} they can be vulnerable to random failure. Comment: 9 pages, 9 figures, accepted for publication in Phys. Rev. ...|$|E
40|$|Abstract. This paper {{investigates the}} {{symmetry}} of polarimetric scattering and emission coefficients of media with reflection symmetry. A reflection operator is defined {{and is used}} to create the images of electromagnetic fields and sources. The image fields satisfy Maxwell's equations, meaning that Maxwell's equations are invariant under the described reflection operations. By applying the reflection operations to media with reflection symmetry, the symmetry properties of the Stokes parameters, characterizing the polarization state of thermal emissions, are shown to agree with existing experimental data. The first two Stokes parameters are symmetric {{with respect to the}} reflection plane, while the third and fourth Stokes parameters have odd symmetry. In active remote sensing, the symmetry properties of the polarimetric scattering matrix elements of deterministic targets and the polarimetric covariance matrix elements of <b>random</b> media <b>or</b> distributed <b>targets</b> are examined. For deterministic targets, the cross-polarized responses are odd functions with respect to the symmetry direction, whereas the copolarized responses are even functions. For distributed <b>targets</b> <b>or</b> <b>random</b> media, it is found that the correlations of copolarized and cross-polarized responses are antisymmetric with respect to the reflection plane, while the other covariance matrix elements are symmetric. Consequently, in the cases of backscatter, the copolarized and cross-polarized components are completely uncorrelated when the incidence direction is on the symmetry plane. The derived symmetry properties of polarimetric backscattering coefficients agree with the predictions of a two-scale surface scattering model and existing sea surface HH and VV backscatter data. Finally, the conditions for a general type of media, i. e., bianisotropic media, to be reflection symmetric are examined. 1...|$|R
40|$|Preimplantation genetic {{analysis}} allows testing embryos produced by {{in vitro fertilisation}} before their transfer into the uterus. Preimplantation {{genetic analysis}} can be performed as screening of <b>random</b> aneuploidies (PGS) <b>or</b> <b>targeted</b> diagnosis of familial chromosomal aberrations or monogenic diseases (PGD) or combination of both approaches (PGD/PGS). Preimplantation genetic testing is appropriate for wide spectre of patients undregoing human assisted reproduction treatment. Array comparative genomic hybridization (aCGH) is sensitive and high throughput method for detection of chromosomal copy number changes on whole embryonal genome, therefore allows performing screening of random aneuploidies (PGS) in combination with diagnosis of unbalanced chromosomal familial aberrations (PGD/PGS). The {{data for this study}} were obtained from Genetic Laboratory IVF Zentren Prof. Zech in Pilsen during 2014. The results of 469 examined embryonal samples resulting from 98 clients were analysed. All biopsies of trophectoderm cells were performed on blastocyst stage. All embryonal samples underwent whole genome amplification (WGA) and were processed using 24 sure microarrays (BlueGnome). PGS was performed for 366 embryos in total and combined analysis including PGS and PGD was performed for 103 embryos. An average maternal age at the time of analysis was 40, 5. This study demonstrates that there is significantly higher rate of aneuploidy in patients of higher maternal age. A suitability of PGS for older patients was confirmed. In other groups of patients containing IVF cycles with donated oocytes or patients carrying familial chromosomal aberrations a higher profit of fertility treatment was observed...|$|R
40|$|We {{investigated}} {{the role of}} the human right Supra-Marginal Gyrus (SMG) in the generation of learned eye movement sequences. Using MRI-guided transcranial magnetic stimulation (TMS) we disrupted neural activity in the SMG whilst human observers performed saccadic eye movements to multiple presentations of either predictable <b>or</b> <b>random</b> <b>target</b> sequences. For the predictable sequences we observed shorter saccadic latencies from the second presentation of the sequence. However, these anticipatory improvements in performance were significantly reduced when TMS was delivered to the right SMG during the inter-trial retention periods. No deficits were induced when TMS was delivered concurrently with the onset of the target visual stimuli. For the random version of the task, neither delivery of TMS to the SMG during the inter-trial period nor during the presentation of the target visual stimuli produced any deficit in performance that was significantly different from the no-TMS or control conditions. These findings demonstrate that neural activity within the right SMG is causally linked to the ability to perform short latency predictive saccades resulting from sequence learning. We conclude that neural activity in rSMG constitutes an instruction set with spatial and temporal directives that are retained and subsequently released for predictive motor planning and responses...|$|R
40|$|Objectives: The authors utilize a {{model of}} activity-dependent {{neuronal}} plasticity to study the interplay between synaptogenesis, neuronal death, and neurogenesis on the resulting pattern of neuronal connectivity. Design: A mathematical model of neuronal network activity was employed, with plasticity instantiated by an activity-dependent rewiring rule. In particular, the authors modeled a neural system {{as a collection of}} “nodes ” (neural subsystems) connected by “links ” (anatomical connectivity). Neuronal damage was simulated by deletion of nodes in this evolving network through either <b>random</b> <b>or</b> <b>targeted</b> attack. Neurogenesis was likewise simulated by insertion of new nodes with random connections. Measurements: Local and global structural network properties were characterized using the metrics of local and global “efficiency, ” and network “reachability. ” Results: Activity-dependent plasticity yields a network that is robust to random node deletion, with preservation of a “small-world ” architecture, characterized by high local and global efficiency. In contrast, targeted deletion of central nodes leads to a drop in reachability and global efficiency, with a consequent loss of small-world properties. Simulated neurogenesis is able to compensate for this targeted cel...|$|E
40|$|Metabolic {{networks}} {{are composed of}} several functional modules, reproducing metabolic pathways and describing the entire cellular metabolism of an organism. In the last decade, an enormous interest has grown {{for the study of}} tolerance to errors and attacks in metabolic networks. Studies on their robustness have suggested that metabolic {{networks are}} tolerant to errors, but very vulnerable to targeted attacks against highly connected nodes. However, many findings on metabolic networks suggest that the above classification is too simple and imprecise, since hub node attacks can be by-passed if alternative metabolic paths can be exploited. On the contrary, non-hub nodes attacks can affect cell survival when the node is the only path within a functional module. In this paper an integrated approach for metabolic networks robustness analysis is presented. With more details, statistical, topological, and functional analysis are used together to evaluate metabolic network behavior under normal operation conditions and under <b>random</b> <b>or</b> <b>targeted</b> attacks. Two real biological metabolic networks have been used to test the effectiveness of the proposed approach...|$|E
40|$|AbstractAutophagic {{cell death}} (ACD) can be operationally {{described}} as cell death with an autophagic component. While most molecular bases of this autophagic component are known, in ACD {{the mechanism of}} cell death proper is not well defined, in particular because in animal cells there is poor experimental distinction between what triggers autophagy and what triggers ACD. Perhaps as a consequence, it is often thought that in animal cells a little autophagy is protective while a lot is destructive and leads to ACD, thus that the shift from autophagy to ACD is quantitative. The aim {{of this article is}} to review current knowledge on ACD in Dictyostelium, a very favorable model, with emphasis on (1) the qualitative, not quantitative nature of the shift from autophagy to ACD, in contrast to the above, and (2) <b>random</b> <b>or</b> <b>targeted</b> mutations of in particular the following genes: iplA (IP 3 R), TalB (talinB), DcsA (cellulose synthase), GbfA, ugpB, glcS (glycogen synthase) and atg 1. These mutations allowed the genetic dissection of ACD features, dissociating in particular vacuolisation from cell death...|$|E
5000|$|... "Hoge Podge": Merril {{shares a}} <b>random</b> <b>or</b> off-the-wall {{story from the}} NFL.|$|R
50|$|Colloidal {{particles}} in a sol are continuously bombarded by the molecules of the dispersion medium on all sides. The impacts are however not equal in every direction. As a result, the sol particles show <b>random</b> <b>or</b> zig-zag movements. This <b>random</b> <b>or</b> zig-zag {{motion of the}} colloidal {{particles in}} a sol is called Brownian motion or Brownian movement.|$|R
40|$|An {{attempt is}} made {{to carry out a}} program (outlined in a {{previous}} paper) for defining the concept of a <b>random</b> <b>or</b> patternless, finite binary sequence, and for subsequently defining a <b>random</b> <b>or</b> patternless, infinite binary sequence to be a sequence whose initial segments are all <b>random</b> <b>or</b> patternless finite binary sequences. A definition based on 2 G. J. Chaitin the bounded-transfer Turing machine is given detailed study, but insufficient understanding of this computing machine precludes a complete treatment. A computing machine is introduced which avoids these difficulties. Key Words and Phrases: computational complexity, sequences, random sequences, Turing machines CR Categories: 5. 22, 5. 5, 5. 6 1. Introduction In this section a definition is presented of the concept of a <b>random</b> <b>or</b> patternless binary sequence based on 3 -tape-symbol bounded-transfer Turing machines. 2 These computing machines have been introduced and studied in [1], where a proposal to apply them in this manne [...] ...|$|R
30|$|The {{supply chain}} {{literature}} emphasizes that specific measures {{are required for}} evaluating topological resilience of SCNs by incorporating the role of various nodes in the network. Analytical measures of resilience commonly used in the network science literature (Rubinov and Sporns 2010; Costa et al. 2005) are unable to account for node heterogeneity, which is a critical aspect in SCN modeling. For example, (Zhao et al. 2011 b, 2011 a) point {{out that in the}} context of SCNs, the distance between two supply nodes or two demand nodes are not as important as that between a supply and a demand node. To tackle this issue, researchers rely on simulation based metrics to analyze topological resilience through customized metrics. The usual approach consists of simulating <b>random</b> <b>or</b> <b>targeted</b> disruptions by removing nodes from the network. (Perera et al. 2017) provides an outline of the methodological framework that is typically used for analysis of topological resilience of SCNs. This procedure consists of sequentially repeating the following steps:(i) simulate node removal, and (ii) measure the relevant resilience metrics. This can provide general insights into the topological aspects of SCN resilience.|$|E
40|$|In {{the past}} few years, several studies have {{explored}} the topology of interactions in different complex systems. Areas of investigation span from biology to engineering, physics and the social sciences. Although having different microscopic dynamics, the results demonstrate that most systems under consideration tend to self-organize into structures that share common features. In particular, the networks of interaction are characterized by a power law distribution, P(k) ∼ k −α, {{in the number of}} connections per node, k, over several orders of magnitude. Networks that fulfill this propriety of scale-invariance are referred to as “scale-free”. In the present work we explore the implication of scale-free topologies in the antiferromagnetic (AF) Ising model and in a stochastic model of opinion formation. In the first case we show that the implicit disorder and frustration lead to a spinglass phase transition not observed for the AF Ising model on standard lattices. We further illustrate that the opinion formation model produces a coherent, turbulent-like dynamics for a certain range of parameters. The influence, of <b>random</b> <b>or</b> <b>targeted</b> exclusion of nodes is studied. Keywords: Scale-free networks, spin glass, complex systems, sociophysics 1...|$|E
40|$|Mutagenesis can be <b>random</b> <b>or</b> <b>targeted</b> and occur {{by nature}} or artificially by humans. However, {{the bulk of}} {{mutagenesis}} employed in plants are random and caused by physical agents such as x-ray and gamma-ray or chemicals such as ethyl-methane sulfonate (EMS). Researchers are interested in first identifying these mutations and/or polymorphisms in the genome followed by investigating their effects in the plant function {{as well as their}} application in crop improvement. The high-throughput technique called TILLING (Targeting Induced Local Lesion IN Genomes) has been already established and become popular for identifying candidate mutant individuals harboring mutations in the gene of interest. TILLING is a non-transgenic and reverse genetics method of identifying a single nucleotide changes. The procedure of TILLING comprises traditional mutagenesis using optimum type and concentration of mutagen, development of a non-chimeric population, DNA extraction and pooling, mutation detection as well as validation of results. In general, TILLING has proved to be robust in identifying useful mutant lines in diverse economically important crops of the world. The main goal of the current mini-review is to show the significance role played by mutagenesis and TILLING in the discovery of DNA lesions which are {{to be used in the}} improvement of crops for the trait of interest...|$|E
50|$|Initialization vectors may be {{referred}} to as nonces, as they are typically <b>random</b> <b>or</b> pseudo-random.|$|R
5000|$|Inappropriate words. (<b>Random</b> <b>or</b> exclamatory {{articulated}} speech, but no conversational exchange. Speaks {{words but}} no sentences.) ...|$|R
5000|$|... non predictable: the {{gateways}} use an algorithm that {{is either}} <b>random</b> <b>or</b> too impractical to predict.|$|R
