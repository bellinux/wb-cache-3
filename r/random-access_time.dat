3|2|Public
25|$|Tries can be slower in {{some cases}} than hash tables for looking up data, {{especially}} if the data is directly accessed on a hard disk drive or some other secondary storage device where the <b>random-access</b> <b>time</b> is high compared to main memory.|$|E
30|$|Finally, NVM {{technologies}} have {{a bright future}} since every end-use application needs to store some parameters or some amount of an application program in the on-board NVM to enable it to function. The upcoming NVMs are the big hope for a semiconductor memory market, which provides memories for systems to run with flexibility, reliability, high performance, and low power consumption in a tiny footprint in nearly every electronic application. Recent market trends have indicated that commercialized or near-commercialized circuits are optimized across speed, density, power efficiency, and manufacturability. Flash memory is not suited to all applications, having its own problems with <b>random-access</b> <b>time,</b> bit alterability, and write cycles. With the increasing need to lower power consumption with zero-power standby systems, observers are predicting {{that the time has}} come for alternative technologies to capture at least some share in specific markets such as automotive smart airbags, high-end mobile phones, and RFID tags. An embedded nonvolatile memory with superior performance to Flash could see widespread adoption in system-on-chip (SoC) applications such as smart cards and microcontrollers.|$|E
40|$|Relative Lempel-Ziv (RLZ) is {{a popular}} {{algorithm}} for compressing databases of genomes from individuals {{of the same species}} when fast random access is desired. With Kuruppu et al. ’s (SPIRE 2010) original implementation, a reference genome is selected and then the other genomes are greedily parsed into phrases exactly matching substrings of the reference. Deorowicz and Grabowski (Bioinformatics, 2011) pointed out that letting each phrase end with a mismatch character usually gives better compression because many of the differences between individuals’ genomes are single-nucleotide substitutions. Ferrada et al. (SPIRE 2014) then pointed out that also using relative pointers and run-length compressing them usually gives even better compression. In this paper we generalize Ferrada et al. ’s idea to handle well also short insertions, deletions and multi-character substitutions. We show experimentally that our generalization achieves better compression than Ferrada et al. ’s implementation with comparable <b>random-access</b> <b>times...</b>|$|R
50|$|NTFS can {{compress}} files using LZNT1 algorithm (a {{variant of}} LZ77) Files are compressed in 16 cluster chunks. With 4 kB clusters, files are compressed in 64 kB chunks. The compression algorithms in NTFS {{are designed to}} support cluster sizes of up to 4 kB. When the cluster size is greater than 4 kB on an NTFS volume, NTFS compression is not available. If the compression reduces 64 kB of data to 60 kB or less, NTFS treats the unneeded 4 kB pages like empty sparse file clusters—they are not written. This allows for reasonable <b>random-access</b> <b>times</b> as the OS just has to follow the chain of fragments. However, large compressible files become highly fragmented since every chunk < 64KB becomes a fragment. According to Microsoft's NTFS Development team's research, 50-60 GB is a reasonable maximum size for a compressed file on an NTFS volume with a 4 kB (default) cluster (block) size. This reasonable maximum size decreases sharply for volumes with smaller cluster sizes. Single-user systems with limited hard disk space can benefit from NTFS compression for small files, from 4 kB to 64 kB or more, depending on compressibility. Files smaller than approximately 900 bytes are stored within the directory entry at the MFT.|$|R

