166|10000|Public
5000|$|Cell, a {{cellular}} architecture containing 9 cores, is the processor {{used in the}} PlayStation 3. Another prominent cellular architecture is Cyclops64, a <b>massively</b> <b>parallel</b> <b>architecture</b> currently under development by IBM.|$|E
50|$|One of {{the major}} {{research}} facilities at the centre is the IBM Blue Gene/P Supercomputer. Blue Gene/P has a <b>massively</b> <b>parallel</b> <b>architecture</b> consisting of 1024 independent units of microprocessors that are connected {{together to form a}} very high computational capacity. The computational power of Blue Gene/P is being utilized for high resolution weather forecasting which can predict the weather 48 hours ahead at a high resolution of 1.5 km x 1.5 km. These weather forecasts are further used for various applications like analyzing the effects of climate change, flood forecasting, solar and wind power forecasts and agro-technologies like mobile applications for rice farmers.|$|E
40|$|An SIMD massively {{parallel}} computer has to compete with workstations and parallel systems based on off-the-shelf microprocessor. This report motivate the choice {{for the development of}} a SIMD <b>massively</b> <b>parallel</b> <b>architecture</b> for image processing focusing on the constraints under which the architecture can succesfully compete with available sequential systems. The correct design methodology is also pointed out. Contents 1 Introduction 2 Motivations {{for the development of a}} SIMD <b>massively</b> <b>parallel</b> <b>architecture</b> 3 Design Methodology SM-IMP/DIST/ 02 Motivations and Design Methodology [...] . February 1994 1 Introduction This report corresponds to the presentation given by DIST at the second Esprit SMIMP project meeting held in London. The presentation focused mainly on the motivations for the development of a SIMD <b>massively</b> <b>parallel</b> <b>architecture</b> (the SIMD part of the SM-IMP project) and on the constraints to satisfy in order to design a competitive architecture. This discussion was followed by [...] ...|$|E
40|$|Scientific {{applications}} {{rely heavily}} on floating point data types. Floating point operations are complex and require complicated hardware that is both area and power intensive. The emergence of <b>massively</b> <b>parallel</b> <b>architectures</b> like Rigel creates new challenges and poses new questions with respect to floating point support. The <b>massively</b> <b>parallel</b> aspect of Rigel places great emphasis on area efficient, low power designs. At the same time, Rigel is a general purpose accelerator and must provide high performance for a wide class of applications. This thesis presents an analysis of various floating point unit (FPU) components with respect to Rigel, and attempts to present a candidate design of an FPU that balances performance, area, and power and is suitable for <b>massively</b> <b>parallel</b> <b>architectures</b> like Rigel...|$|R
40|$|The parareal algorithm, {{recalled}} in section 2 below, {{allows one to}} solve evolution equations on (possibly <b>massively)</b> <b>parallel</b> <b>architectures.</b> The two building blocks of the algorithms are a coarse-discretization predictor (solved sequentially) and a fine-discretization corrector (solved in parallel). First developed in [9] and slightl...|$|R
40|$|This project {{supports}} {{the development of}} software using terascale computers to carry out molecular simulations of protein function and macromolecular interactions. We are building on the existing CHARMM and Amber simulation packages, adapting them in novel ways to <b>massively</b> <b>parallel</b> <b>architectures</b> and high-performance CPUs. Three principal avenues being pursued are: (1) Improvements in load-balancing and communication for large-scale particle-mesh Ewald (PME) simulations of solvated biomolecules. (2) Modern techniques for accelerating convergence of sampling of configuration space offer promise for further exploitation of <b>massively</b> <b>parallel</b> <b>architectures.</b> These methods include parallel tempering and ''lambda dynamics'' procedures that connect multiple, synchronized results from PME simulations like those described in part [1]. (3) The implementation of efficient and scalable algorithms that move towards lower-resolution models in ways that can be carefully calibrated against atomic-level solvated simulations...|$|R
40|$|HTMT is {{an ambitious}} new {{architecture}} combining cutting edge technologies to reach petaflop performance sooner than current technology trends allow. It is a <b>massively</b> <b>parallel</b> <b>architecture</b> with multi-threaded hardware and a multi-level memory hierarchy. Microservers provide {{a new perspective}} for viewing this memory hierarchy whereby memory is actively involved in process execution...|$|E
40|$|The paper {{presents}} the mechanisms for dynamic load distribution implemented within {{the support for}} the Parallel Objects (PO for short) programming environment. PO applications evolve depending on their dynamic need of resources, enhancing application performance. The goal is to show how dynamic load distribution can be successfully applied on a <b>massively</b> <b>parallel</b> <b>architecture...</b>|$|E
40|$|A finite {{difference}} {{version of}} the equations governing two-dimensional, non-divergent flow on a sphere is implemented and integrated on the Massively Parallel Processor (MPP). The MPP's performance is then compared with the Cyber's. The feasibility of using a <b>massively</b> <b>parallel</b> <b>architecture</b> to solve the hydrodynamic equations as they are used in numerical weather prediction (NWP) are described...|$|E
40|$|Reconfiguration {{is largely}} an {{unexplored}} {{property in the}} context of parallel models of computation. However, it is a powerful concept as far as <b>massively</b> <b>parallel</b> <b>architectures</b> are concerned, because it overcomes the constraints due to the bissection width arising in most of distributed memory machines. In this paper, we show how to use reconfiguration in order to improve communication operations that are widely used in parallel applications. We propose quasi-optimal algorithms for broadcasting, scattering, gossiping and multi-scattering. Keywords: Reconfiguration, broadcast, scattering, gossiping, communications, distributed memory parallel computers 1 Introduction For <b>massively</b> <b>parallel</b> <b>architectures,</b> the hardware complexity of the interconnection network is much higher than that of the processing units: "the interconnection network employs 99 % of the hardware involved" [JMM 92]. Moreover, due to the communication-intensive nature of most computational tasks, their performance depen [...] ...|$|R
40|$|Particle {{simulation}} {{as applied}} in the direct simulation Monte Carlo (DSMC) method is a technique for analyzing low density flows and is used extensively for engineering analysis of aerospace vehicles. This work compares two implementations of this method on outwardly similar <b>massively</b> <b>parallel</b> <b>architectures.</b> The Mas-Par MP- 2 and the Connection Machine CM- 2 are both <b>massively</b> <b>parallel</b> SIMD <b>architectures</b> with a data parallel paradigm. Inter-nally, however, the two machines differ substantially. The more recent, and more balanced architectural approach of the MasPar MP- 2 results in better performance of the MP- 2 for direct particle simulation...|$|R
40|$|Low-cost <b>massively</b> <b>parallel</b> <b>architectures</b> are {{generally}} {{characterized by a}} limited amount of memory owned by each Processing Element. As a consequence, low-cost mesh-connected architectures can utilize only a specific processor virtualization mechanism which is based on the sequential scanning of the data set stored in an external memory. As a consequence of this virtualization mechanism, applications must be developed according to some precise criteria. This paper presents the optimization of some key parameters for the improvement of system performance. These optimizations are validated through an image processing case study. 1 Introduction Many specific tasks in the early vision phase of high-level applications, such as obstacle avoidance in automotive applications and object recognition in robotics, are very well suited to <b>massively</b> <b>parallel</b> <b>architectures</b> because of their fine grain parallelism and to the peak processing power demanded by real-time constraints. These processing st [...] ...|$|R
40|$|Parallel Objects is a {{powerful}} model for distributed/parallel Object-Oriented program-ming. Goal {{of this paper is}} to present the approach adopted in porting the support of the Parallel Objects environment, originally implemented for a <b>massively</b> <b>parallel</b> <b>architecture,</b> onto the PVM environment, which is nowadays a de-facto standard in the design of distributed applications on heterogeneous networks of computers...|$|E
40|$|The {{performance}} of an interconnection network in a <b>massively</b> <b>parallel</b> <b>architecture</b> {{is subject to}} physical constraints whose impact needs to be re-evaluated from time to time. Fat-trees and low dimensional cubes have raised {{a great interest in}} the scientific community {{in the last few years}} and are emerging standards in the design of interconnection networks for massively parallel computers...|$|E
40|$|The {{option to}} {{automatically}} model {{the behavior of}} different actors during live exercise training would increase {{the value of the}} after-action-review (AAR) process. If a simulated model of the actors is available right after the live exercise training, the evaluation of their behavior would be more timely and alternative actions could also be evaluated at the same time. The CxBR Diffusion Engine merges technologies to establish a tool for automatic, on-line behavior modeling. Context Based Reasoning (CxBR) is a proven methodology to build simulated agents with human behavior. Genetic Programming (GP) provides the CxBR framework with learning capabilities to automatically create simulated agents with human behavior. The final piece in the CxBR Diffusion Engine is to provide an efficient, flexible, scaleable and mobile platform to evolve the agents’ behavior. This platform is the newly developed <b>massively</b> <b>parallel</b> <b>architecture</b> for distributed GP. The <b>massively</b> <b>parallel</b> <b>architecture</b> has the potential to execute the GP linear machine code representation at a rate of up to 50, 000 generations per second. Implemented in an FPGA, this architecture is highly portable and applicable to mobile, on-line applications. This paper will present a theory on how the CxBR + GP can evolve simulated agents with human behavior by observation in a <b>massively</b> <b>parallel</b> <b>architecture.</b> These pieces will introduce all the necessary elements to build the CxBR Diffusion Engine that could model human behavior to enable individual AAR of trainees in the training field...|$|E
40|$|In {{this paper}} {{we present a}} {{light-weight}} modification of bi-directional path tracing algorithm that is optimized for <b>massively</b> <b>parallel</b> <b>architectures</b> with limited memory, like GPU. The amount of computations performed by the algorithm is still comparable to unidirectional path tracing. Though modified algorithm preserves some benefits of general bi-directional path tracing and handles indirect illumination and caustics quite efficiently...|$|R
40|$|The paper {{focuses on}} local load {{balancing}} policies for <b>massively</b> <b>parallel</b> <b>architectures</b> and introduces a new scheme for load information exchange between neighbor nodes. The {{idea is to}} distort the exchanged load information to let the policy keep into account a more global view {{of the system and}} overcome the limits of the local scope. The presented scheme has been integrated into two variants of a direct-neighbor policy and evaluated in dependence of the characteristics of the system load. Experimental results show that the transmission of distorted load information provides high efficiency unless the dynamicity of the load becomes too high, in which case it is preferable to exploit non-distorted load information. Keywords: <b>Massively</b> <b>Parallel</b> <b>Architectures,</b> Dynamic Load Balancing, DirectNeighbor Policies, Load Information Exchange, Performance Evaluation 1. Introduction Dynamic load balancing is required for parallel applications characterized by non-predictable patterns in the accesse [...] ...|$|R
25|$|Connectionist replies:Closely {{related to}} the brain {{simulator}} reply, this claims that a <b>massively</b> <b>parallel</b> connectionist <b>architecture</b> {{would be capable of}} understanding.|$|R
40|$|This paper {{describes}} a new parallel algorithm for Euclidean Distance Transform on the Polymorphic Processor Array, a <b>massively</b> <b>parallel</b> <b>architecture</b> {{based on a}} reconfigurable mesh interconnection network. The proposed algorithm has been implemented using the Polymorphic Parallel C language and has been validated through simulation, its computational complexity is O(N) (worst case) for pictures of NxN pixels on a Polymorphic Processor Array of NxN processing elements...|$|E
40|$|We {{present a}} new, <b>massively</b> <b>parallel</b> <b>architecture</b> for {{accelerating}} machine learning algorithms, based on arrays of vector processing elements (VPEs) with variable-resolution arithmetic. Groups of VPEs operate in SIMD (single instruction multiple data) mode, and each group {{is connected to}} an independent memory bank. The memory bandwidth thus scales {{with the number of}} VPEs, while the main data flows are local, keeping power dissipation low. With 256 VPEs, implemented on two FPGAs (field programmable gate array) chips, we obtain a sustained speed of 19 GMACS (billion multiplyaccumulate per sec.) for SVM training, and 86 GMACS for SVM classification. This performance is more than an order of magnitude higher than that of any FPGA implementation reported so far. The speed on one FPGA is similar to the fastest speeds published on a Graphics Processor for the MNIST problem, despite a clock rate that is an order of magnitude lower. Tests with Convolutional Neural Networks show similar compute performances. This <b>massively</b> <b>parallel</b> <b>architecture</b> is particularly attractive for embedded applications, where low power dissipation is critical. ...|$|E
40|$|The {{emergence}} of semiconductor fabrication technology allowing a tight coupling between high-density DRAM and CMOS logic {{on the same}} chip {{has led to the}} important new class of Processor-In-Memory (PIM) architectures. Newer developments provide powerful parallel processing capabilities on the chip, exploiting the facility to load wide words in single memory accesses, and supporting complex address manipulations in the memory. Furthermore, large arrays of PIMs can be arranged into a <b>massively</b> <b>parallel</b> <b>architecture...</b>|$|E
40|$|Simulators {{are still}} the primary tools for {{development}} and performance evaluation of applications running on <b>massively</b> <b>parallel</b> <b>architectures.</b> However, current virtual platforms {{are not able to}} tackle the complexity issues introduced by 1000 -core future scenarios. We present a fast and accurate simulation framework targeting extremely large parallel systems by specifically taking advantage of the inherent potential processing parallelism available in modern GPGPUs...|$|R
50|$|Netezza’s {{proprietary}} AMPP (Asymmetric <b>Massively</b> <b>Parallel</b> Processing) <b>architecture</b> is a two-tiered {{system designed}} to quickly handle very large queries from multiple users.|$|R
40|$|Abstract—Computing Nash equilibria {{is a very}} {{important}} problem in strategic analysis of markets, conflicts and resource allocation. Unfortunately, computing these equilibria even for moderately sized games is computationally expensive. To obtain faster execution times it is essential to exploit the available parallelism offered by the currently available <b>massively</b> <b>parallel</b> <b>architectures.</b> To address this issue, we design a GPU-based parallel support enumeration algorithm for computing Nash equilibria in bimatrix games. The algorithm is based on a new parallelization method which achieves high degrees of parallelism suitable for <b>massively</b> <b>parallel</b> GPU <b>architectures.</b> We perform extensive experiments to characterize the performance of the proposed algorithm. The algorithm achieves significant speedups relative to the OpenMP-based parallel implementation of the support enumeration method running on conventional multicore machines. I...|$|R
3000|$|A {{high-speed}} analog VLSI {{image acquisition}} and low-level image processing system are presented. The {{architecture of the}} chip {{is based on a}} dynamically reconfigurable SIMD processor array. The chip features a <b>massively</b> <b>parallel</b> <b>architecture</b> enabling the computation of programmable mask-based image processing in each pixel. Extraction of spatial gradients and convolutions such as Sobel operators are implemented on the circuit. Each pixel includes a photodiode, an amplifier, two storage capacitors, and an analog arithmetic unit based on a four-quadrant multiplier architecture. A [...]...|$|E
40|$|QPACE {{is a novel}} <b>massively</b> <b>parallel</b> <b>architecture</b> {{optimized}} for lattice QCD simulations. Each node comprises an IBM PowerXCell- 8 i processor. The nodes are interconnected by a custom 3 -dimensional torus network implemented on an FPGA. The architecture was systematically optimized with respect to power consumption. This put QPACE in the number one spot on the Green 500 List published in November 2009. In this paper we give {{an overview of the}} architecture and highlight the steps taken to improve power efficiency...|$|E
40|$|This paper {{describes}} a new parallel algorithm tbr Minimum Cost Path computation on the Polymorphic Processor Array, a <b>massively</b> <b>parallel</b> <b>architecture</b> {{based on a}} reconfigurable mesh interconnection network. The proposed algorithm has been implemented using the Polymorphic Parallel C language and has been validated through simulation. The proposed algorithm for the Polymorphic Processor Array, delivers the same performance, in terms of computational complexity, as the hypercube interconnection network of the Connection Machine, and as the Gated Connection Network. © Springer-Verlag Berlin Heidelberg 1998...|$|E
40|$|International audienceUltrasonic imaging is a {{commonly}} used method {{to detect and}} identify defects in a mechanical part in nuclear applications. Nowadays <b>massively</b> <b>parallel</b> <b>architectures</b> enable the simulation of ultrasonic field emitted by a phased array transducer inspecting a part across a coupling medium. In this paper, regular field computation model will be discussed along its implementations on General Purpose Processors (GPP) and Graphic Processing Units (GPU) ...|$|R
5000|$|The LBM {{method was}} {{designed}} from scratch to run efficiently on <b>massively</b> <b>parallel</b> <b>architectures,</b> ranging from inexpensive embedded FPGAs and DSPs up to GPUs and heterogeneous clusters and supercomputers (even {{with a slow}} interconnection network). It enables complex physics and sophisticated algorithms. Efficiency leads to a qualitatively new level of understanding, since it allows solving problems that previously could not be approached (or only with insufficient accuracy).|$|R
40|$|In {{this paper}} <b>massively</b> <b>parallel</b> {{algorithms}} and <b>architectures</b> for real-time wavefront {{control of a}} dense adaptive optic system (’SXLENE) are presented. We have already shown that the computation of a near optimal control algorithm for SELENE {{can be reduced to}} the solution c}f a discrete Poisson equation on a regular domain. Although, this represents an optimal computation, due the large size of the system and the high sampling rate requirement, the implementation of this control algorithm pc,ses a computationally challenging pxoblern since it demands a sustained computational throughput of the order of 10 GFlops. We develop a novel algorithm, designated as Fast Invariant Imbedding algorithm, which offers a massive degree of parallelism with simple communication and synchronization requirements. Due to these features, our algorithm is signif~cantly more efficient than other Fast Poisson Solvers for implementation on <b>massively</b> <b>parallel</b> <b>architectures.</b> We also discuss two <b>massively</b> <b>parallel,</b> algorithmically special~zed, <b>architectures</b> for low-cost and optimal implementation of the Fast Invariant Imbedding algorithm. 1...|$|R
40|$|A Lagrangian {{approach}} for the simulation of reactive flows {{has been developed}} {{during the course of}} this project, and has been applied to a number of significant and challenging problems including the transverse jet simulations. An efficient strategy for parallel domain decomposition has also been developed to enable the implementation of the approach on <b>massively</b> <b>parallel</b> <b>architecture.</b> Since 2005, we focused our efforts on the development of a semi-Lagrangian treatment of diffusion, and fast and accurate Lagrangian simulation tools for multiphysics problems including combustion...|$|E
40|$|AbstractQPACE {{is a novel}} <b>massively</b> <b>parallel</b> <b>architecture</b> {{optimized}} for lattice QCD simulations. A single QPACE node {{is based on the}} IBM PowerXCell 8 i processor. The nodes are interconnected by a custom 3 -dimensional torus network implemented on an FPGA. The compute power of the processor is provided by 8 Synergistic Processing Units. Making effcient use of these accelerator cores in scientific applications is challenging. In this paper we describe our strategies for porting applications to the QPACE architecture and report on performance numbers...|$|E
40|$|Abstract. We {{describe}} Janus, an application-driven architecture for Monte Carlo {{simulations of}} spin glasses. Janus is a <b>massively</b> <b>parallel</b> <b>architecture,</b> based on reconfigurable FPGA nodes; it offers two {{orders of magnitude}} better performance than commodity systems for spin glass applications. The first generation Janus machine has been operational since early 2008; we are currently developing a new generation, that will be on line in early 2013. In this paper we present the Janus archi-tecture, describe both implementations and compare their performances with those of commodity systems. ...|$|E
40|$|We {{present a}} highly {{parallelizable}} and flexible computational method to solve high-dimensional stochastic dynamic economic models. Solving such models often requires {{the use of}} iterative methods, like time iteration or dynamic programming. By exploiting the generic iterative structure of this broad class of economic problems, we propose a parallelization scheme that favors hybrid <b>massively</b> <b>parallel</b> computer <b>architectures.</b> Within a <b>parallel</b> nonlinear time iteration framework, we interpolate policy functions partially on GPUs using an adaptive sparse grid algorithm with piecewise linear hierarchical basis functions. GPUs accelerate this part of the computation one order of magnitude thus reducing overall computation time by 50 %. The developments in this paper include the use of a fully adaptive sparse grid algorithm and the use of a mixed MPI-Intel TBB-CUDA/Thrust implementation to improve the interprocess communication strategy on <b>massively</b> <b>parallel</b> <b>architectures.</b> Numerical experiments on “Piz Daint” (Cray XC 30) at the Swiss National Supercomputing Centre show that high-dimensional international real business cycle models can be efficiently solved in parallel. To our knowledge, this performance on a <b>massively</b> <b>parallel</b> petascale <b>architecture</b> for such nonlinear high-dimensional economic models has not been possible prior to present work...|$|R
40|$|In {{this paper}} we present {{the design of}} a new hybrid system based on {{commodity}} components to gain supercomputer power at low cost. The architecture is built around a MIMD PC-cluster linked by a high-speed network and <b>massively</b> <b>parallel</b> SIMD processor boards connected to each node. We present {{the design of a}} volume rendering algorithm in order to derive an efficient mapping onto the new architecture. This results in a volume visualisation implementation, which is faster than all previously published implementations on <b>massively</b> <b>parallel</b> <b>architectures...</b>|$|R
40|$|Abstract. This paper proposes {{the design}} and {{implementation}} of a dynamic pro-gramming based algorithm for (distributed) constraint optimization, which ex-ploits modern <b>massively</b> <b>parallel</b> <b>architectures,</b> such as those found in modern Graphical Processing Units (GPUs). The paper studies the proposed algorithm in both centralized and distributed optimization contexts. The experimental analy-sis, performed on unstructured and structured graphs, shows the advantages of employing GPUs, resulting in enhanced performances and scalability. ...|$|R
