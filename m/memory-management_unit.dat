9|1|Public
50|$|A Translation lookaside buffer (TLB) is {{a memory}} cache {{that is used}} to reduce the time taken to access a user memory location. It {{is a part of the}} chip’s <b>memory-management</b> <b>unit</b> (MMU). The TLB stores the recent {{translations}} of virtual memory to physical memory and can be called an address-translation cache. A TLB may reside between the CPU and the CPU cache, between CPU cache and the main memory or between the different levels of the multi-level cache. The majority of desktop, laptop, and server processors include one or more TLBs in the memory management hardware, and it is nearly always present in any processor that utilizes paged or segmented virtual memory.|$|E
40|$|This paper {{describes}} {{the design and}} implementation of memory protection in the Roadrunner/pk operating system. The design is portable between various CPUs that provide page-level protection using <b>Memory-Management</b> <b>Unit</b> (MMU) hardware. The approach maps logical addresses one-to-one to physical addresses and uses the MMU to overlay protection domains over regions of memory that are in use by application threads and the kernel. An analysis of code size shows that this design and implementation can be executed with {{an order of magnitude}} less code than similar function in the Linux 2. 2. 14 kernel while providing comparable performance for basic memory protection operations...|$|E
40|$|Abstract—CHERI extends a {{conventional}} RISC Instruction-Set Architecture, compiler, and operating system to support fine-grained, capability-based memory protection to mitigate memory-related vulnerabilities in C-language TCBs. We describe how CHERI capabilities can also underpin a hardware-software object-capability model for application compartmentalization that can mitigate broader classes of attack. Prototyped {{as an extension}} to the open-source 64 -bit BERI RISC FPGA soft-core processor, FreeBSD operating system, and LLVM compiler, we demonstrate multiple orders-of-magnitude improvement in scalability, simplified programmability, and resulting tangible security benefits as compared to compartmentalization based on pure <b>Memory-Management</b> <b>Unit</b> (MMU) designs. We evaluate incrementally deployable CHERI-based compartmentalization using several real-world UNIX libraries and applications. I...|$|E
40|$|This report {{provides}} a simple multi-core MIPS model we call MIPS- 86. It aims at providing an overall specification for the reverse-engineered hardware models provided in [Pau 12]. We take the simple sequential MIPS processor model from [Pau 12] and extend {{it with a}} memory and device model that resembles the one of modern x 86 architectures. The model has the following features: Sequential processor core abstraction with atomic execute-and fetch transitions, <b>memory-management</b> <b>units</b> (MMUs) with translation-lookaside buffers (TLBs), store buffers (SBs), processor-local advanced programmable interrupt controllers (local APICs), I/O APICs, and devices. Open issues: • adaption of this specification to fit to the pipelined hardware implementation. E. g. by introduction of pipelining abstractions (e. g. delayed PC, delayed visibil-ity of multiplication results in HI, LO) • addition of emode special purpose registe...|$|R
40|$|When {{we want to}} {{make things}} fast, the OS needs some help. And help usually comes from one place: the hardware. Here, to speed address translation, we are going to add what is called (for {{historical}} reasons [CP 78]) a translation-lookaside buffer, or TLB. A TLB (a part of the chip’s MMU, or <b>memory-management</b> <b>unit)</b> is simply a hardware cache of popular virtual-to-physical address translations; thus, a better name would have simply been an address-translation cache. On any memory reference, the hardware will look first in the TLB to see if the desired translation is held therein; if it is, the translation is performed (quickly, in the processor) without having to consult the page table (which has all the translations for a given process). Thus, the hardware (and the OS) follows the approach as seen in Figure 13. 1 when servicing a memory reference by a process to a given virtual address. Note that the left column tells u...|$|E
40|$|This paper {{provides}} {{a brief overview}} of that effort, sample results indicating the performance of the current implementation, and a few comments about future work. The CM- 5 port of our simulator has been operational since June 1992 and has proved invaluable, especially for running simulations of large Alewife systems (64 to 512 nodes). 2 Alewife Overview Alewife is an experimental distributed-memory multiprocessor under construction at the MIT Laboratory for Computer Science [1]. As shown in Figure 1, each Alewife node consists of a Sparcle processor [2], a floating-point coprocessor, several megabytes of DRAM, a 64 -kilobyte unified instruction/data cache, the local portion of a two-dimensional mesh interconnect, and a Communications and <b>Memory-Management</b> <b>Unit</b> (CMMU) that serves as both memory and network interface. We have implemented a detailed sequential simulator for Alewife that runs on SPARC- and MIPS-based workstations; this simulator is called NWO [3]. In addition to providing cycle-by-cycle simulation of the processing elements and flit-by-flit modeling of the network interfaces, NWO strives to model internal workings of the actual Alewife CMMU [4] as closely as possible...|$|E
40|$|International audienceManycore {{processors}} {{are a way}} to {{face the}} always growing demand in digital data processing. However, by putting closer distinct and possibly private data, they open up new security breaches. Splitting the architecture into several partitions managed by a hypervisor {{is a way to}} enforce isolation between the running virtual machines. Thanks to their high number of cores, these architectures can mitigate the impact of dedicating cores both to the virtual machines and the hypervisor, while allowing an efficient execution of the virtualized operating systems. We present such an architecture allowing the execution of fully virtualized multicore operating systems benefiting of hardware cache coherence. The physical isolation is made by the means of address space via the introduction of a light hardware module similar to a <b>memory-management</b> <b>unit</b> at the network-on-chip entrance, but without the drawback of relying on a page table. We designed a cycle-accurate virtual prototype of the architecture, controlled by a light blind hypervisor with minimum rights, only able to start and stop virtual machines. Experiments made on our virtual prototype shows that our solution has a low time overhead – typically 3 % on average...|$|E
40|$|When {{we want to}} {{make things}} fast, the OS needs some help. And help usually comes from one place: the hardware. To speed address translation, we are going to add what is called (for {{historical}} reasons [CP 78]) a translation-lookaside buffer, or TLB [C 68,C 95]. A TLB is part of the chip’s <b>memory-management</b> <b>unit</b> (MMU), and is simply a hardware cache of popular virtual-to-physical address translations; thus, a better name would be an address-translation cache. Upon each virtual memory reference, the hardware first checks the TLB to see if the desired translation is held therein; if so, the translation is performed (quickly) without having to consult the page table (which has all translations). Because of their tremendous performance im-pact, TLBs in a real sense make virtual memory possible [C 95]. Figure 18. 1 shows how hardware might handle a virtual address translation (assuming a simple linear page table and a hardware-managed TLB). In the common case (lines 3 – 9), we are hoping that a translation will be found in the TLB (a TLB hit) and thus the trans-lation will be quite fast (done in hardware near the processing core). In the less common case (lines 10 – 19), the translation won’t be in the cache (a TLB miss), and the system will have to consult the page table in main memory, update the TLB, and retry the memory refer-ence...|$|E
40|$|When {{we want to}} {{make things}} fast, the OS needs some help. And help usually comes from one place: the hardware. To speed address translation, we are going to add what is called (for {{historical}} reasons [CP 78]) a translation-lookaside buffer, or TLB. A TLB (part of the chip’s MMU, or <b>memory-management</b> <b>unit)</b> is simply a hardware cache of popular virtual-to-physical address translations; thus, a better name would be an addresstranslation cache. Upon a virtual memory reference, the hardware will first check the TLB to see if the desired translation is held therein; if so, the translation is performed (quickly) without having to consult the page table (which has all translations). Thus, the hardware (and the OS) follows the approach as seen in Figure 13. 1 when servicing a memory reference by a process to a given virtual address. Note that the left column tells us whether it is the hardware (hw) or the OS (os) that performs the given action; in some cases (depending on the system), it could be one or the other, and hence we see hw/os. In the common case, we are hoping that most translations will be found in the TLB (a TLB hit) and thus the translation will be quite fast (done in hardware near the processing core). In the less common case, the translation won’t be in the cache (a TLB miss), and the system will have to consult the page table in main memory, update the TLB, and retry the memory reference...|$|E
40|$|Using paging as {{the core}} {{mechanism}} to support virtual memory can lead to high performance overheads. By chopping the address space into small, fixed-sized units (i. e., pages), paging requires {{a large amount of}} mapping information. Because that mapping information is generally stored in physical memory, paging logically requires an extra memory lookup for each virtual address generated by the program. Going to memory for translation information before every instruction fetch or explicit load or store is prohibitively slow. And thus our problem: THE CRUX: HOW TO SPEED UP ADDRESS TRANSLATION How can we speed up address translation, and generally avoid the extra memory reference that paging seems to require? What hardware support is required? What OS involvement is needed? When we want to make things fast, the OS usually needs some help. And help often comes from the OS’s old friend: the hardware. To speed address translation, we are going to add what is called (for historical rea-sons [CP 78]) a translation-lookaside buffer, or TLB [C 68, C 95]. A TLB is part of the chip’s <b>memory-management</b> <b>unit</b> (MMU), and is simply a hardware cache of popular virtual-to-physical address translations; thus, a better name would be an address-translation cache. Upon each virtual memory reference, the hardware first checks the TLB to see if the desired translation is held therein; if so, the translation is performed (quickly) without having to consult the page table (which has all translations). Be-cause of their tremendous performance impact, TLBs in a real sense make virtual memory possible [C 95]. 19. 1 TLB Basic Algorithm Figure 19. 1 shows a rough sketch of how hardware might handle a virtual address translation, assuming a simple linear page table (i. e., the page table is an array) and a hardware-managed TLB (i. e., the hardware handles much of the responsibility of page table accesses; we’ll explain more about this below) ...|$|E

