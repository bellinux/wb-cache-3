0|31|Public
40|$|Time-tag-generating circuit {{designed}} for use in LAN monitor, monitors frames of data transmitted among computers on local-area network (LAN). To each frame of data that LAN monitor receives from LAN, time-tag generator appends <b>ancillary</b> data on <b>time</b> of arrival of frame, precise to within 1 microsecond of centrally generated <b>time</b> signal. Inserts <b>ancillary</b> <b>time</b> data in place of already used frame-check data before frames of data stored in memory of LAN monitor...|$|R
5000|$|Outdoors, Lewis jumped 14 of the 20 furthest {{ancillary}} jumps of all <b>time.</b> (<b>Ancillary</b> marks {{are those}} which are valid, but were not the furthest in a series.) ...|$|R
2500|$|In 1925 the division's roll {{shifted from}} design and {{building}} of to acquiring and evaluating aircraft prototypes {{submitted by the}} commercial aircraft industry. [...] This left division engineers were left free to concentrate on developing standards unique to military aircraft, reviewing designs, modifying and testing procured <b>machines,</b> and developing <b>ancillary</b> equipment to enhance military aircraft.|$|R
50|$|At {{the same}} <b>time,</b> <b>ancillary</b> {{structures}} were added, including a two-storey office building, lodge, weighbridge and boundary walls. Constructed in brick, these all survive and are Grade II listed, and also {{form part of}} the Scheduled Ancient Monument.|$|R
40|$|Smart' sensors onboard NASA space {{missions}} {{will require}} variable data output bandwidth as {{they respond to}} phenomena of interest. An Instrument Telemetry Packet (ITP) approach has been developed which encodes experimental instrument data into an autonomous data package, along with pertinent engineering parameters and <b>ancillary</b> data (<b>time,</b> position, attitude, etc.). New requirements for onboard concentration and buffering, {{as well as for}} end-to-end error control, arise from this approach. Emphasis is placed on packet protocols compatible with the data link standard ADCCP, to enable one set of ground support equipment to readily support instrument development, launch site checkout and mission operations phases...|$|R
40|$|Fletcher Challenge Canada Limited (FCCL) {{is a large}} {{pulp and}} paper {{producer}} in British Columbia. FCCL has traditionally been a newsprint producer and has kept pace with increasingly stringent quality requirements by continually rebuilding existing paper <b>machines</b> and <b>ancillary</b> plant. The company is also considering other options, including converting machines to different grades of paper. Because of the complexity associated with {{the large number of}} possible options available, the company decided to develop an optimisation model to assist with this strategic decision making. Market forecasts, capital requirements, production and other pertinent data were collated for a ten year planning horizon, and incorporated into a multi-period optimisation model [...] Initially this model proved to be extremely difficult to solve. Based on knowledge of the business a number of extra constraints were added that improved its performance and allowed optimal solutions to be obtained. The model has proved to be successful in challenging entrenched views within FCCL regarding strategic direction, and stimulating wide ranging thought and discussion...|$|R
40|$|Competitive {{abstract}} {{machines for}} Prolog are usually large, intricate, and incorpórate sophisticated optimizations. This makes them difñcult to code, optimize, and, especially, maintain and extend. This is {{partly due to}} the fact that efñciency considerations make it necessary to use low-level languages in their implementation. Writing the abstract <b>machine</b> (and <b>ancillary</b> code) in a higher-level language can help harness this inherent complexity. In this paper we show how the semantics of basic components of an efficient virtual machine for Prolog can be described using (a variant of) Prolog which retains much of its semantics. These descriptions are then compiled to C and assembled to build a complete bytecode emulator. Thanks to the high level of the language used and its closeness to Prolog the abstract machine descriptions can be manipulated using standard Prolog compilation and optimization techniques with relative ease. We also show how, by applying program transformations selectively, we obtain abstract machine implementations whose performance can match and even exceed that of highly-tuned, hand-crafted emulators...|$|R
40|$|In this presentation, we {{describe}} our experiences with building and using large ontologies, with application to locating NASA Earth science data. We use OWL {{to represent the}} mutual relationships of scientific concepts and their <b>ancillary</b> space, <b>time,</b> and environmental descriptors. Background NASA’s Earth science mission is to improve {{our understanding of the}} integrated Earth system and its components, through the use of satellite data products. NASA makes its data and information products available at no charge to scientists and non-scientists. The motivation of our task is to improve the discovery of these products using tools that incorporate semantic understanding. In support of this effort, we developed a collection of ontologies for describing Earth science data and knowledge. An ontology-aided search tool was developed to demonstrate the use of these ontologies. Ontologie...|$|R
50|$|Many modern {{military}} vehicles, airplanes, {{and weapons}} {{are equipped with}} ancillary weapons, including tanks (a large cannon is the main weapon, but the tank also has a heavy machine gun as an ancillary weapon), jet fighters (air-to-air missiles and/or bombs are the main armament, but most fighters also have a autocannon as an ancillary weapon), bombers (a World War II-era B-17 Flying Fortress' main armament was up to 4800 lb of bombs, but it also had up to thirteen 0.50 in <b>machine</b> guns as <b>ancillary</b> weapons, for defense against enemy fighter planes) and even assault rifles (the bayonet is the ancillary weapon; as well, even the wooden rifle butt {{can be used as}} an improvised, last resort weapon).|$|R
40|$|In {{order to}} achieve {{competitive}} performance, abstract machines for Prolog and related languages end up being large and intricate, and incorporate sophisticated optimizations, both at the design and at the implementation levels. At the same time, efficiency considerations make it necessary to use low-level languages in their implementation. This makes them laborious to code, optimize, and, especially, maintain and extend. Writing the abstract <b>machine</b> (and <b>ancillary</b> code) in a higher-level language can help tame this inherent complexity. We show how the semantics of most basic components of an efficient virtual machine for Prolog can be described using (a variant of) Prolog. These descriptions are then compiled to C and assembled to build a complete bytecode emulator. Thanks to {{the high level of}} the language used and its closeness to Prolog, the abstract machine description can be manipulated using standard Prolog compilation and optimization techniques with relative ease. We also show how, by applying program transformations selectively, we obtain abstract machine implementations whose performance can match and even exceed that of state-of-the-art, highly-tuned, hand-crafted emulators. Comment: 56 pages, 46 figures, 5 tables, To appear in Theory and Practice of Logic Programming (TPLP...|$|R
40|$|In {{automatic}} production controls a fully {{knowledge of}} all {{the state of the}} different system is often mandatory. In particular the measurement of many parameters is necessary in order to obtain the desired specification in terms of both safety requirements and protective measures for operator’s safety and in terms of system reliability in compliance with both the international standards and the correct working conditions. In the field of Injection Moulding Machine (IMM) the European Standard EN 201 is the reference standard. In particular, the mentioned standard specifies ‘‘the essential safety requirements for injection moulding machines for the processing of plastics and/or rubber’’ and ‘‘the safety requirements for the interaction between injection moulding <b>machines</b> and <b>ancillary</b> equipment’’. If a magnetic clamping system is used in order to attach the mould to the platens the measure of the temperature of permanent-electro magnetic platens is mandatory. In this paper, after a brief introduction concerning the theory of operation of the permanent-electro magnetic platens, an example of industrial low-cost temperature measurement for permanent- electro magnetic platens is presented and discussed in each aspect. At this aim a theoretical introduction has been presented, and in a second time, actual implementation of the proposed approach is finally depicted and discussed...|$|R
40|$|In many {{electricity}} grids {{around the}} world, wind power {{is becoming a}} preferred generation option over conventional generation. Long term environmental and economic benefits are {{the main reason for}} this trend. However, the intermittency of wind resources and limitations linked with the associated power electronic interfaces are seen to be barriers for adoption of this sustainable technology. The main concern is the adverse impact wind power could have on the stability of the power system. Despite the low marginal cost of operation, unpredictability of generation and the inability to assist the grid with <b>ancillary</b> services, at <b>times,</b> makes wind power a burden to the power system...|$|R
40|$|We {{consider}} {{the problem of}} providing network access to hosts whose physical location changes with time. Such hosts cannot depend on traditional forms of network connectivity and routing because their location, and hence the route to reach them, cannot be deduced from their IP address. We present protocols that seamlessly integrate mobile hosts into the current IP networking infrastructure. They are primarily targeted at supporting a campus environment with mobile computers, but also extend gracefully to accomodate hosts moving between different networks. The key feature is the dependence on <b>ancillary</b> <b>machines</b> to track {{the location of the}} mobile hosts. Our protocols are designed to react quickly to changing topologies, to scale well, and not to place an overwhelming burden on the network. 1 Motivation In recent years we have observed a proliferation of portable computers. There is a strong trend toward the production of smaller and more powerful such units. A serious drawback of curren [...] ...|$|R
40|$|We {{consider}} {{the problem of}} providing network access to hosts whose physical location changes with time. Such hosts cannot depend on traditional forms of network connectivity and routing because their location, and hence the route to reach them, cannot be deduced from their network address. In this paper, we explore the concept of providing continuous network access to mobile computers, and present a set of IP-based protocols that achieve that goal. They are primarily targeted at supporting a campus environment with mobile computers, but also extend gracefully to accommodate hosts moving between different networks. The key feature is the dependence on <b>ancillary</b> <b>machines,</b> the Mobile Support Stations (MSSs), to track {{the location of the}} Mobile Hosts. Using a combination of caching, forwarding pointers, and timeouts, a minimal amount of state is kept in each MSS. The state information is kept in a distributed fashion; the system scales well, reacts quickly to changing topologies, and does [...] ...|$|R
40|$|We {{present an}} {{architecture}} for detecting "zero-day" worms and viruses in incoming email. Our main {{idea is to}} intercept every incoming message, pre-scan it for potentially dangerous attachments, and only deliver messages that are deemed safe. Unlike traditional scanning techniques that rely on some form of pattern matching (signatures), we use behavior-based anomaly detection. Under our approach, we "open" all suspicious attachments inside an instrumented virtual machine looking for dangerous actions, such as writing to the Windows registry, and flag suspicious messages. The attachment processing can be offloaded to a cluster of <b>ancillary</b> <b>machines</b> (as many as are needed {{to keep up with}} a site's email load), thus not imposing any computational load on the mail server. Messages flagged are put in a "quarantine" area for further, more labor-intensive processing. Our implementation shows that we can use a large number of malware-checking VMs operating in parallel to cope with high loads. Finally, we show that we are able to detect the actions of all malicious software we tested, while keeping the false positive rate to under 5 %...|$|R
40|$|Abstract. We {{present an}} {{architecture}} for detecting “zero-day ” worms and viruses in incoming email. Our main {{idea is to}} intercept every incoming message, prescan it for potentially dangerous attachments, and only deliver messages that are deemed safe. Unlike traditional scanning techniques that rely on some form of pattern matching (signatures), we use behavior-based anomaly detection. Under our approach, we “open ” all suspicious attachments inside an instrumented virtual machine looking for dangerous actions, such as writing to the Windows registry, and flag suspicious messages. The attachment processing can be offloaded to a cluster of <b>ancillary</b> <b>machines</b> (as many as are needed {{to keep up with}} a site’s email load), thus not imposing any computational load on the mail server. Messages flagged are put in a “quarantine ” area for further, more labor-intensive processing. Our implementation shows that we can use a large number of malwarechecking VMs operating in parallel to cope with high loads. Finally, we show that we are able to detect the actions of all malicious software we tested, while keeping the false positive rate to under 5 %. ...|$|R
40|$|A summer field {{campaign}} {{was conducted at}} the forested background site of K-puszta in Hungary. The main aim {{was to assess the}} contribution of terpene-derived particulate organic compounds to the PM 2. 5 organic carbon (OC) and of the secondary organic carbon (SOC) from α-pinene to the OC. The study lasted from 24 May to 29 June 2006; the first half the weather was cold, while the second half was warm. Separate daytime and night-time PM 2. 5 samples were collected with a high-volume sampler and the samples were analysed by several analytical techniques, including ion chromatography (IC) and liquid chromatography–mass spectrometry (LC/MS). The latter technique was used for measuring the terpene-derived species. <b>Ancillary</b> high <b>time</b> resolution measurements of volatile organic compounds (VOCs) were made with proton-transfer reaction–mass spectrometry. The temporal and diurnal variability of the particulate compounds and VOCs and interrelationships were examined. It was found that the monoterpenes and a number of terpene-derived particulate compounds, such as cis-pinic and cis-caric acid, exhibited a strong day/night difference during the warm period, with about 10 times higher levels during the night-time. During the warm period, the IC compounds and LC/MS compounds accounted, on average, for 3. 1 % and 2. 0 %, respectively, of the OC, whereas the contribution of SOC from α-pinene to the OC was estimated at a minimum of 7. 1 %...|$|R
40|$|Emergency Department (ED) {{crowding}} and bottle necks are {{the reality}} of hospitals across the country. Patients seeking care and needing inpatient beds via the emergency rooms are facing delays with attaining the right level of care. Orchestrating a patient through an ED admission requires a multidisciplinary effort to provide safe, effective and efficient care. This quality improvement project conducted in a tertiary acute care hospital focused on Centers for Medicare and Medicaid metrics to measure Emergency Department (ED) throughput. This multidisciplinary initiative focused on reducing time stamps for patient arrival to the ED through departure to hospital or home. Outcomes showed {{a significant decrease in}} the time frame for patient arrival to being seen by a qualified provider, left without being seen rates, ED diversion, and <b>ancillary</b> department turnaround <b>times.</b> The interventions can be applied at other hospital based emergency departments...|$|R
40|$|This {{thesis is}} {{concerned}} with flows relating to the continuous coating of multiple layers on moving webs using the slide bead process. The lowermost layer is generally known as a carrier layer when the viscosity and flow rate are both small compared with the corresponding properties of the other layers. This study is predominantly experimental in nature and broad in scope as addressing issues relating to an industrial slide coating process used for the manufacture of photographic products and inkjet media. Novel specialist pieces of equipment have been designed and built for visualizing such flows {{as part of this}} work. The studies have been carried out using a pilot coating <b>machine</b> and <b>ancillary</b> flow control facilities currently owned by HARMAN technology Limited. The new techniques enable fresh insight into the interaction between the carrier layer and the surface properties of the substrate, including roughness, surface free energy, electric charge and porosity - an area of investigation that has hitherto been largely ignored. The behaviour of the bead when coating embossed webs showing a "stippled" finish is of particular interest when compared with apparently equally rough substrates of equivalent surface energy. Increasing slide angle is shown to be advantageous to expanding the coating window for difficult substrates. The results show that the widely perceived criteria for a carrier layer needs to be redefined when coating rough surfaces of low surface energy using this process. Charge assisted coating is shown likely to be superior to conventional slide bead coating for minimising waste due to streaks. The studies include the visualisation of flows at the slot exit and on the slide. The methods allow the profile of the interface as well as the free surface to be monitored and give new insight into two major unreported effects limiting the use of a thin low viscosity carrier layer. The scope also extends to the study of waves induced in the surface of wet multi-layer coatings when subjected to the impact of air from an impingement dryer - an area of considerable interest to the coating technologist yet largely ignored by the equipment supplier. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|DSN Requirement Scheduler is a {{computer}} program that automatically schedules, reschedules, and resolves conflicts for allocations of resources of NASA s Deep Space Network (DSN) {{on the basis of}} ever-changing project requirements for DSN services. As used here, resources signifies, primarily, DSN antennas, <b>ancillary</b> equipment, and <b>times</b> during which they are available. Examples of project-required DSN services include arraying, segmentation, very-long-baseline interferometry, and multiple spacecraft per aperture. Requirements can include periodic reservations of specific or optional resources during specific time intervals or within ranges specified in terms of starting times and durations. This program is built on the Automated Scheduling and Planning Environment (ASPEN) software system (aspects of which have been described in previous NASA Tech Briefs articles), with customization to reflect requirements and constraints involved in allocation of DSN resources. Unlike prior DSN-resource- scheduling programs that make single passes through the requirements and require human intervention to resolve conflicts, this program makes repeated passes in a continuing search for all possible allocations, provides a best-effort solution at any time, and presents alternative solutions among which users can choose...|$|R
40|$|Once-daily {{aminoglycoside}} (ODA) regimens {{have been}} instituted to maximize bacterial killing by optimizing the peak concentration/MIC ratio {{and to reduce}} the potential for toxicity. We initiated an ODA program at our institution that utilizes a fixed 7 -mg/kg intravenous dose with a drug administration interval based on estimated creatinine clearance: > or = 60 ml/min every 24 h (q 24 h), 59 to 40 ml/min q 36 h, and 39 to 20 ml/min q 48 h. Subsequent interval adjustments are made by using a single concentration in serum and a nomogram designed for monitoring of ODA therapy. Since initiation of the program, 2, 184 patients have received this ODA regimen. The median dose was 450 (range, 200 to 925) mg, while the median length of therapy was 3 (range, 1 to 26) days. The median age {{of the population was}} 46 (range, 13 to 97) years. Gentamicin accounted for 94 % of the aminoglycoside use, and the majority (77 %) of patients received the drug q 24 h. The 36 -, 48 -, and > 48 -h intervals were used for 15, 6, and 2 % of this population, respectively. Three patients exhibited clinically apparent ototoxicity. Twenty-seven patients (1. 2 %) developed nephrotoxicity (the Hartford Hospital historical rate is approximately 3 to 5 %) after a median of 7 (range, 3 to 19) days of therapy. On the basis of a prospective evaluation of 58 patients and follow-up of additional patients via clinician reports, we have noted no apparent alterations in clinical response with our ODA program. This ODA program appears to be clinically effective, reduces the incidence of nephrotoxicity, and provides a cost-effective method for administration of aminoglycosides by reducing <b>ancillary</b> service <b>time</b> and serum aminoglycoside determinations...|$|R
40|$|During 1980 s and 1990 s, the {{resources}} {{on which the}} Chilean artisanal fishery is based suffered a severe crisis. Since the crisis was driven principally {{by the lack of}} compliance with the central fishery agency, a new fisheries act introduced in 1991 the Management and Exploitation Areas (MEA), a form of co-management based on territorial use rights. This law restricts the co-management to one-by-one arrangements between a particular community and the fisheries authority, which results in the lack of any formal strategy for coordinating managements above the spatial scale of the individual fishers community. Since many important resources for the artisanal fishery form metapopulations, MEAs of different populations could rely on larval transport from other MEAs or fishing grounds outside of their control, making the coordination of managements at each community unavoidable. However, the connectivities among local populations needed to implement this coordination are still unknown for the Chilean coast. Moreover, as connectivity is an emergent property of the natural-human system, what is really needed is a system that delivers information about the metapopulation to co-management institutions and that is sensitive to actions derived from co-management negotiations. The aim {{of the present study was}} to develop a preliminary tool for this information system based on available information. Three major factors influencing connectivity were studied: 1) the coastal circulation, 2) the size and location of patches in which the adult individuals are distributed, and 3) the size of stocks at each management unit. <b>Ancillary</b> <b>time</b> series of satellite sea surface temperature were used to track thermal features in successive days, obtaining some spatial patterns of currents. These patterns were summarized in simple rules that, applied to a realistic geometry of the coast, were utilized to generate idealized circulation fields. These fields were used to force the larvae movement in a simple Lagrangean scheme. Literature records were combined with geographic information to delimitate location and size of suitable patches for local populations. The resulting larval transport was included into a matrix model of the metapopulation dynamics. Simulations with the model were used to inform the discussion about the proposal of transforming the entire coast of central Chile in MEAs. The results confirm the reported daily variability in the coastal circulation associated to changes in the alongshore wind. Furthermore, besides offshore dynamics associated to headlands in Lengua de Vaca and Punta Choros, the observed variability close to the coast is principally along the shore, and includes reversion of coastal currents. A single general regime of circulation applied to a realistic coastal geometry will produce a rich variety of larval transport patterns, which cannot be adequately reproduced by a single dispersion function for the whole coast. Current variability associated to coastal trapped waves are much more important for connectivity than those associated to the upwelling. Since the energy of coastal-trapped waves changes during El NiÃ ï¿½Ã Â±o events, the NiÃ ï¿½Ã Â±o/NiÃ ï¿½Ã Â±a cycle will change the connectivities. The way in which this cycle modifies the connectivities will depend on the annual mean of the currents. Based on the available information about alongshore mean current, the two largest local populations are sources of larvae for the others, and are, therefore, the only ones that could be set aside as MEA under the present co-management system. This also shows that under the current fisheries act both, the TAC and the co-management systems are compatible provided that the source/sink structure is known. Given the dependence of populations from imported larvae, the transformation of the entire coast of the IV region in MEAs requires the modification of the present co-management institutions to promote the coordination of management among the fishers communities. The system is highly sensitive to management options, being the effects of local changes in this management difficult to visualize a priori, without simulations such as done in this study...|$|R
40|$|Aims: The multi-center SPEED {{registry}} {{evaluated the}} procedural success and in-hospital clinical outcomes of direct stenting with the Svelte 'all-in-one' coronary stent Integrated Delivery System (IDS) through diagnostic catheters {{to identify the}} clinical indications for which this approach is appropriately suited. Methods & results: Forty-eight (48) patients with 54 lesions of lengths ≤ 20 mm and RVD 2. 5 - 3. 5 mm were targeted for direct stenting through diagnostic catheters (4 - 6 F) via radial or femoral approach. Procedural characteristics early in an investigator's experience (28 lesions) were compared with outcomes following experience (26 lesions). Procedure, device and strategy success were realized in 54 (100 %), 50 (93 %) and 46 (85 %) lesions, respectively, with strategy success significantly related to RVD (P = 0. 05), lesion location (P = 0. 01), and diagnostic catheter size (P = 0. 05). Significant improvement in crossing and intervention time and trends toward improvement in device and strategy success, reductions in procedure and radiation time and contrast use were observed. Conclusions: Direct stenting through diagnostic catheters via radial or femoral approach using the Svelte IDS is feasible and associated with good in-hospital outcomes. This approach offers the attractive option of assessing lesions via diagnostic catheter and, depending upon vessel anatomy and lesion morphology, continuing with ad-hoc interventional treatment using the same diagnostic catheter. Improvements in strategy success and procedural efficiencies, based on operator experience, facilitate catheter downsizing and reduce intervention <b>time,</b> <b>ancillary</b> product use and overall procedure costs...|$|R
40|$|Graduate Entry Medicine (GEM) {{provides}} both {{challenges and}} opportunities for students and teachers alike. These are global issues for Ireland and the UK which are gaining in momentum and synergy with medical education in the USA. They provide an important chance to revisit the tenets of a good doctor {{and how best to}} select students for training from an enormous pool of potential applicants. In the UK and Ireland there is a move to Graduate Entry Medicine (GEM) courses to supplement the number of doctors being trained from traditional undergraduate courses mainly populated by school leavers. This is in alignment with the USA where all students are graduate entry and so come to the course with learning in another subject {{which may or may not}} be directly related and also life skills and perhaps the experience of working in another profession. For the researcher the big question is; âDo GEM courses lead to better doctors or not?â An <b>ancillary</b> question at <b>time</b> of a global shortage of doctors with its increasing health economy is; âWill GEM aid recruitment to the profession, but more importantly retention?â Generic Challenges With a shortened fast-track GEM course the curriculum will to a certain extent be ânewâ and so changed with an emphasis on self-directed learning (SDL). For directors of GEM courses there is a need to acknowledge the array of different learning styles as it may be hard for some students to adapt being a student again particularly if they ar...|$|R
40|$|In a {{deregulated}} electricity market, auction {{serves as}} a primary pricing tool in various segments of the market including day-ahead, real <b>time,</b> <b>ancillary</b> services markets, and Financial Transmission Rights (FTRs) market. Deregulated power markets around the world use different auction strategies {{that exist in the}} literature, since very little comparative guidelines exist as to the relative merits of these strategies. In this thesis, a computational methodology and its solution framework are developed to evaluate the impact of an auction strategy on the equilibrium prices in a constrained network with multiple generators at nodes, and where transactions are settled using the optimal power flow (OPF) program. The methodology is tested on a power market represented by a sample 12 -bus IEEE network available in the MATPOWER software, which is reconfigured to allow multiple generators to supply power at a bus. The network is used as a platform to comprehensively assess the performance of uniform price auction, discriminatory auction, and second-price uniform auction. Auction rules are used to update generator costs, which are then introduced into the OPF program for obtaining optimal price and quantity allocations. This Auction-OPF procedure is embedded within a game theoretic model that obtains the equilibrium bidding strategies and the corresponding prices and quantities for the network. A detailed comparison of the auction mechanisms is carried out using different measures of performance such as revenue, average prices, and quantity weighted average prices. The comparison shows that there is, perhaps, an appreciable difference among the auction mechanisms...|$|R
40|$|Emerging {{standards}} such as OpenADR enable Demand Response (DR) Resources {{to interact}} directly with Utilities and Independent System Operators {{to allow their}} facility automation equipment {{to respond to a}} variety of DR signals ranging from day ahead to real <b>time</b> <b>ancillary</b> services. In addition, there are Aggregators in today’s markets who are capable of bringing together collections of aggregated DR assets and selling them to the grid as a single resource. However, in most cases these aggregated resources are not automated and when they are, they typically use proprietary technologies. There is a need for a framework for dealing with aggregated resources that supports the following requirements: • Allows demand-side resources to participate in multiple DR markets ranging from wholesale ancillary services to retail tariffs without being completely committed to a single entity like an Aggregator; • Allow aggregated groups of demand-side resources to be formed in an ad hoc fashion to address specific grid-side issues and support the optimization of the collective response of an aggregated group along a number of different dimensions. This is important in order to taylor the aggregated performance envelope to the needs to of the grid; • Allow aggregated groups to be formed in a hierarchical fashion so that each group can participate in variety of markets from wholesale ancillary services to distribution level retail tariffs. This paper explores the issues of aggregated groups of DR resources as described above especially within the context of emerging smart grid standards and the role they will play in both the management and interaction of various grid-side entities with those resources...|$|R
40|$|International Telemetering Conference Proceedings / November 14 - 16, 1978 / Hyatt House Hotel, Los Angeles, CaliforniaThe {{emergence}} of "smart" sensors onboard space missions is forcing a reexamination {{of the procedures}} by which NASA acquires, multiplexes, transmits, annotates, and distributes sensor data to the user community. Increasingly we find that "smart" sensors are being planned for future space missions which will search for specific unusual phenomena and, when present, record these phenomena in great detail. This {{gives rise to the}} need for a widely varying bandwidth requirement from each instrument in response to the occurrence of phenomena that cannot be anticipated in advance. An asynchronously multiplexed packet telemetry concept is described which, within broad limits, permits instruments to acquire and transmit information at the rate appropriate for the experimental phenomena being observed. Data from a single instrument, along with the necessary <b>ancillary</b> data (typically <b>time,</b> position, and attitude), will be assembled into self-contained packets and will be subsequently transmitted over various communications links (i. e., space telemetry channel, ground communications circuits, etc.) to the experimenter's facility in near real time. Reliable error control coding will be included in each link transmission to protect the integrity of the data packets. A major objective is to make the entire data acquisition and distribution process completely transparent to the experimenter {{in the sense that the}} output terminal of the distribution system will be physically, logically, and electrically identical to that of the experiment output channel. To provide greater inter-mission portability of instruments and to reduce the instrument interfacing costs, the emerging national and international telecommunications standards (ADCCP/HDLC/SDLC, X. 25, etc.) will be utilized as the instrument interface standards wherever practical. Except for the time delay imposed by propagation and nominal queueing considerations, the experimenters will observe an interface identical to that which would occur if the instrument were physically located at their facilities...|$|R
40|$|Due to the {{character}} of the original source materials and the nature of batch digitization, quality control issues may be present in this document. Please report any quality issues you encounter to digital@library. tamu. edu, referencing the URI of the item. Includes bibliographical references (leaves 92). Issued also on microfiche from Lange Micrographics. Metal mesh is a commercially available material used in many applications including seals, heat shields, filters, gaskets, aircraft engine mounts, and vibration absorbers. This material has been tested in the Turbomachinery Laboratory at Texas A&M University (TAMU) as a bearing damper in a rotordynamic test rig. The test facility was originally used to support the design of a turboprop engine at TAMU, developing squirrel cage bearing supports and squeeze film dampers for both the gas generator and power turbine rotors. To design the metal mesh damper, static stiffness and dynamic rap test measurements were first made on metal mesh samples in a specially designed non-rotating test fixture. These property tests were performed on samples of various densities and press fits. One sample was also tested in an Instron <b>machine</b> as an <b>ancillary</b> and redundant way to determine the stiffness. Using the stiffness test results and equations derived by a previous investigator, a spreadsheet program was written and used to size metal mesh donuts that have the radial stiffness value required to replace the squirrel cage in the power turbine. The squirrel cage and squeeze-film bearing damper developed for the power turbine rotor was then replaced by a metal mesh donut sized by the computer code. Coast-down tests were conducted through the first critical speed of the power turbine. The results of the metal mesh tests were compared with those obtained from previous testing with the squeeze film damper. The results show that the metal mesh damper has the same damping as the squeeze film at room temperature but does not lose its damping at elevated temperatures up to 210 F?. Experiments were run under several different conditions including balanced rotor hot oil soaked, unbalanced rotor hot oil soaked, balanced dry, and unbalanced dry. Over all, metal mesh dampers appear to be a viable and attractive substitute for squeeze film dampers in gas turbine engines. The advantages shown to date include less variation of damping with temperature, ability to handle large rotor unbalance, and the ability (if required) to operate effectively in an oil free environment...|$|R
40|$|In {{this paper}} a cloud {{detection}} algorithm {{applied to the}} MSG-SEVIRI (Metcosat Second Generation-Spinning Enhanced Visible and Infrared Imager) data is described. In order to obtain a good performance in cloud detection, physical, statistical and temporal approaches have been used. In the statistical algorithm, the spectral and textural features of the MSG-SEVIRI images {{have been used as}} input, while, in the physical tests, a set of dynamic thresholds has been used. The physical algorithm does not use real <b>time</b> <b>ancillary</b> data- such as sea surface temperature map and NWP temperature and humidity profiles. A further test is applied to that pixels having low confidence to be clear or cloudy. This test takes advantage of the best MSG-SEVIRI temporal resolution and it applies the K-Nearest Neighbour classifier to the spectral and textural features calculated in "temporal" boxes 3 x 3 pixels, defined "temporal" because their elements belong to three subsequent MSG-SEVIRI images. The MACSP (cloud MAsk Coupling of Statistical and Physical methods) algorithm has been validated against the MODIS cloud mask and compared with CPR (Cloud Profiling Radar) and SAFNWC cloud masks. The outcomes show that the MACSP detects 91. 8 % {{of the total number of}} the pixels used for validation against MODIS cloud mask correctly, while the SAFNWC cloud mask detects 89. 2 % of them correctly. In particular, the MACSP classifies as cloudy 8. 8 % of the pixels classified by the MODIS cloud mask as clear, while the SAFNWC cloud mask classifies as cloudy 12. 1 % of them. The MACSP detects 91. 2 % of the cloudy CPR pixels and 90. 8 % of the cloud-free CPR pixels, considered for comparison, correctly. On the other hand, the SAFNWC and CPR cloud masks agree in the detection of 90. 7 % of the cloudy pixels and of 90. 2 % of the cloud-free pixels. (C) 2008 Elsevier Inc. All rights reserved...|$|R
40|$|The Oak Ridge National Laboratory (ORNL), at {{the request}} of the California Energy Commission and the U. S. Department of Energy, is {{investigating}} opportunities for electrical load to provide the ancillary service of spinning reserve to the electric grid. The load would provide this service by stopping for a short time when there is a contingency on the grid such as a transmission line or generator outage. There is a possibility that a significant portion of the California Independent System Operator's (CAISO's) spinning reserve requirement could be supplied from the California Department of Water Resources (CDWR) pumping load. Spinning reserve has never been supplied from load before, and rule changes would be needed to allow it. In this report, we are presenting technical findings on the possibility of supplying spinning reserve from pumping system load. In parallel, we are pursuing the needed rule changes with the North American Electric Reliability Council (NERC), the Federal Energy Regulatory Commission (FERC), the Western Electricity Coordinating Council (WECC), and the CAISO. NERC and FERC have agreed that they have no prohibition against supplying spinning reserve from load. The WECC Minimum Operability Reliability Criteria working group has agreed that the concept should be considered, and they are presently discussing the needed tariff and rule changes. Presently, spinning reserve is provided by generation that is actually spinning but is operating at low power levels and can be ramped up quickly to provide reserve power. In a sense, this is an inefficient and environmentally unfriendly way of providing reserves because it requires the generator to operate at a low power level that may be inefficient and may discharge more pollutants per kW than operating at rated power. It would be better if this generation capacity were in a position to bid into the energy market. Providing an additional supply of spinning reserve would tend to reduce prices for both reserves and the regular electric energy market. The CAISO is presently in the process of redesigning its market rules for <b>ancillary</b> services. The <b>time</b> is right to pursue this opportunity to supply spinning reserve from load. It is our hope that the CDWR will endorse this recommendation. ORNL will then work with FERC, NERC, WECC, and the CAISO to obtain the needed rule changes. This project would provide the CDWR with another option in the complex process of obtaining its energy at the lowest possible cost, while at the same time providing more flexibility to the ISO and relief to the energy market. After this project is implemented in California, we hope that the practice spreads across the nation, allowing much more flexibility in energy markets and increasing the availability of reserve services...|$|R

