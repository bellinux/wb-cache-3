0|38|Public
40|$|The particle-scale {{dynamics}} of granular materials have commonly {{been characterized by}} the self-diffusion coefficient D. However, this <b>measure</b> <b>discards</b> the collective and topological information known {{to be an important}} characteristic of particle trajectories in dense systems. Direct measurement of the entanglement of particle space-time trajectories can be obtained via the topological braid entropy, which has previously been used to quantify mixing efficiency in fluid systems. Here, we investigate the utility of in characterizing the {{dynamics of}} a dense, driven granular material at packing densities near the static jamming point ϕ_J. From particle trajectories measured within a two-dimensional granular material, we typically observe that is well-defined and extensive. However, for systems where ϕ≳ 0. 79, we find that (like D) is not well-defined, signifying that these systems are not ergodic on the experimental timescale. Both and D decrease with either increasing packing density or confining pressure, independent of the applied boundary condition. The related braiding factor provides a means to identify multi-particle phenomena such as collective rearrangements. We discuss possible uses for this measure in characterizing granular systems...|$|R
40|$|Abstract. The particle-scale {{dynamics}} of granular materials have commonly {{been characterized by}} the self-diffusion coefficient D. However, this <b>measure</b> <b>discards</b> the collective and topological information known {{to be an important}} characteristic of particle trajectories in dense systems. Direct measurement of the entanglement of particle space-time trajectories can be obtained via the topological braid entropy Sbraid, which has previously been used to quantify mixing efficiency in fluid systems. Here, we investigate the utility of Sbraid in characterizing the {{dynamics of}} a dense, driven granular material at packing densities near the static jamming point φJ. From particle trajectories measured within a two-dimensional granular material, we typically observe that Sbraid is well-defined and extensive. However, for systems where φ & 0. 79, we find that Sbraid (like D) is not well-defined, signifying that these systems are not ergodic on the experimental timescale. Both Sbraid and D decrease with either increasing packing density or confining pressure, independent of the applied boundary condition. The related braiding factor provides a means to identify multi-particle phenomena such as collective rearrangements. We discuss possible uses for this measure in characterizing granular systems. PACS numbers: 45. 70. Mg; 45. 70. -n; 81. 05. R...|$|R
30|$|Overall, our {{proposed}} {{method is}} {{fast as it}} only involves accumulating the pixel-wise values of the images and then taking the mean. Furthermore, not all LR images from a given set are used because some images that did not pass our sharpness <b>measure</b> test are <b>discarded.</b>|$|R
50|$|In the BQSM, one can {{construct}} {{commitment and}} oblivious transferprotocols. The underlying idea is the following: The protocol parties exchange more than Q quantum bits (qubits). Since even a dishonest party cannot store all that information (the quantum {{memory of the}} adversary is limited to Q qubits), {{a large part of}} the data will have to be either <b>measured</b> or <b>discarded.</b> Forcing dishonest parties to measure {{a large part of the}} data allows to circumvent the impossibility result by Mayers; commitment and oblivious transfer protocols can now be implemented.|$|R
40|$|Research PaperThe Enhanced Data Collection Project (EDCP), {{administered}} by the Oregon Department of Fish and Wildlife, collected data on discard rates for groundfish species and bycatch rates of prohibited species. From late 1995 to early 1999 EDCP observers collected discard data from 235 fishing trips by 25 trawl vessels that voluntarily participated in the program. Besides these observer data, vessels in the program kept enhanced logbooks that recorded retained and discarded catch. These logbooks had data from 866 trips by 44 vessels, including most of the trips with observers. Provided logbook data are acceptably accurate, collecting discard data using logbooks could be a cost-effective supplement to an observer program for <b>measuring</b> <b>discard</b> rates and total discards. Comparisons of tow-by-tow logbook discards with the corresponding observer discards indicated substantial inaccuracies in the logbook information. However, when averaged across tows, trips, and vessels, the discard rates (discard/catch) from the logbooks were lower than the observer discard rates, but predictably so. Generalized linear models {{were used to determine}} the major factors contributing to variability in the discard rates. The models indicated tremendous vessel-to-vessel variability in discard rates. Principal components analysis (PCA) was applied to trip-level landings and species compositions from the entire groundfish trawl fleet to summarize the fleet-wide characteristics of fishing trips. Analyses of the PCA scores from the trips that were in the EDCP compared to the scores from all other trips indicated that the EDCP trips probably were not representative of the fleet at large...|$|R
40|$|Societal {{demands to}} reduce {{discarding}} and other impacts associated with fishing are growing. Pressure is increasing on policy makers, fishermen and scientists to ‘do {{something about the}} discard problem’. Discarding is high on the agenda in the upcoming review of the Common Fisheries Policy (CFP) and within the Commission, Member States and the fishing industry there is considerable discussion on appropriate management <b>measures</b> to mitigate <b>discarding.</b> In order to address the issue, fisheries should first be evaluated (audited) to identify the specific discard problems and to reference these against the available mitigation tools i. e. <b>measures</b> to reduce <b>discards.</b> This Atlas represents a first attempt at auditing Irish fisheries and proposes some options to mitigate discards. It should be emphasised that discarding occurs in all international fleets operating in the waters around Ireland and that mitigation measures must be applied to all these fleets {{if we are to}} implement a successful discard reduction policy...|$|R
40|$|This report {{presents}} the results of laboratory developments, field experiments and data analyses carried out in the contract MARE/ 2011 / 07 - "Studies on the Common Fisheries Policy" - Lot 1 "Reduction of gear impact and discards in deep sea fisheries". The contractual objectives were "(I) to identify and study trawl modification and alternative gear that aim at reducing the impact of the gear on the sea bottom when engaged in deep-sea fisheries, and (II) to identify and study a <b>measure</b> for <b>discard</b> reduction in deep-sea fisheries or fisheries having deep-sea species as a by-catch, pursued with trawls or nets. The measure could consist in gear modifications or catch purification based on the skipper’s strategy to avoid unwanted fish. ...|$|R
5000|$|If {{the vehicle}} {{does not belong}} to the driver and is quickly abandoned, a trace may not be {{possible}} without examination of forensic evidence. In some cases, the offender may go to extreme <b>measures</b> to <b>discard</b> the getaway vehicle in order to hide his 'tracks' by dumping it in a river or secluded park, and/or setting it on fire; while this may not make solving the crime impossible, it can make the effort more difficult for law enforcement. [...] The criminal investigation can be further complicated by the use of multiple getaway vehicles, which can confuse eyewitnesses, as well as creating multiple places to investigate: each vehicle is a new crime scene. [...] In Forensics for Dummies, the rookie is reminded: [...] "At a minimum, the crime scene includes ... Areas from which the site can be entered, exited, or even escaped...." ...|$|R
50|$|As a {{cost-cutting}} <b>measure,</b> ABC Records <b>discarded</b> many master tapes in the 1970s to save storage space. When these recordings were reissued on {{compact disc}} in the 1980s, CD versions were often taken from master copies which {{had less than}} optimal sound quality. The company's last president, Steve Diener, was named president in 1977 after serving as head of ABC Records' international division. Because of financial problems, ABC Records was sold on January 31, 1979 to MCA Records, which discontinued the ABC label on March 5, 1979. The bestselling albums in the ABC catalog were reissued on MCA.|$|R
5000|$|Wilkins {{considered}} the earth's meridian, atmospheric pressure and, following a suggestion by Christopher Wren and demonstrations by Christiaan Huygens, the pendulum {{as the source}} for his universal <b>measure.</b> He <b>discarded</b> atmospheric pressure as a candidate - it was described by Torricelli in 1643 as being susceptible to variation (the link between atmospheric pressure and weather was not understood at the time) and he discarded a meridian as being too difficult to measure; leaving the pendulum as his preferred choice. He proposed that {{the length of a}} [...] "seconds pendulum" [...] (approximately 993 mm) which he named the [...] "standard" [...] should be the basis of length. He proposed further that the [...] "measure of capacity" [...] (base unit of volume) should be defined as a cubic standard and that the [...] "measure of weight" [...] (base unit of weight mass) should be the weight of a cubic standard of rainwater. All multiples and sub-multiples of each of these measures would be related to the base measure in a decimal manner. In short, Wilkins [...] "proposed essentially what became ... the French decimal metric system".|$|R
40|$|We {{introduce}} {{an approach}} {{for the detection}} of approximately rectangular structures in gray scale images. Our research is motivated by the Silvretta Historica project that aims at automated detection of remains of livestock enclosures in remotely sensed images of alpine regions. The approach allows detection of enclosures with linear sides of various sizes and proportions. It is robust to incomplete or fragmented rectangles and tolerates deviations from a perfect rectangular shape. Morphological operators are used to extract linear features. They are grouped into parameterized linear segments by means of a local Hough transform. To identify appropriate configurations of linear segments we define convexity and angle constraints. Configurations meeting these constraints are rated by a proposed rectangularity <b>measure</b> that <b>discards</b> overly fragmented configurations and configurations with more than one side completely missing. The search for appropriate configurations is efficiently performed on a graph. Its nodes represent linear segments and edges encode the above constraints. We tested our approach on a set of aerial and GeoEye- 1 satellite images of 0. 5 m resolution that contain ruined livestock enclosures of approximately rectangular shape. The approach showed encouraging results in finding configurations of linear segments originating from the objects of our interest...|$|R
40|$|Since {{they can}} provide a natural and {{flexible}} description of nonlinear dynamic behavior of complex system, Agent-based models (ABM) have been commonly used for immune system simulation. However, it is crucial for ABM to obtain an appropriate estimation for the key parameters of the model by incorporating experimental data. In this paper, a systematic procedure for immune system simulation by integrating the ABM and regression method under the framework of history matching is developed. A novel parameter estimation method by incorporating the experiment data for the simulator ABM during the procedure is proposed. First, we employ ABM as simulator to simulate the immune system. Then, the dimension-reduced type generalized additive model (GAM) is employed to train a statistical regression model by using the input and output data of ABM and play a role as an emulator during history matching. Next, we reduce the input space of parameters by introducing an implausible <b>measure</b> to <b>discard</b> the implausible input values. At last, the estimation of model parameters is obtained using the particle swarm optimization algorithm (PSO) by fitting the experiment data among the non-implausible input values. The real Influeza A Virus (IAV) data set is employed to demonstrate the performance of our proposed method, and {{the results show that}} the proposed method not only has good fitting and predicting accuracy, but it also owns favorable computational efficiency...|$|R
40|$|The {{phenomenon}} of discarding was studied using a multidisciplinary approach {{to allow the}} integration of biological, social and economic data. The research aimed to evaluate {{the strength of the}} case to reduce discarding in a single case study fishery, the English Nephrops fishery; to identify specific objectives for discard reduction; to identify factors that inhibit discard reduction and to determine the best means of achieving those objectives. Changes in trawl structure offered the best solution while having the least impact on fishing opportunities. However, the inadequate level of incentive was identified as the main impediment to discard reduction rather than the lack of technical ability. To implement effective <b>measures</b> to reduce <b>discarding</b> in this fishery, an increased level of incentive is required. Three measures are identified as a means to achieve this. Nephrops Discards Multidisciplinary Incentive Fishery...|$|R
40|$|International audienceDispersion {{effects in}} {{perfusion}} MRI data have a relevant {{influence on the}} residue function computed from deconvolution of the measured arterial and tissular concentration time-curves. Their characterization allows reliable estimation of hemody-namic parameters and can reveal pathological tissue conditions. However, the time-delay between the measured concentration time-curves is a confounding factor. We perform deconvolution by means of dispersion-compliant bases, separating the effects of dispersion and delay. In order to characterize dispersion, we introduce shape parameters, such as the dispersion time and index. We propose a new formulation for the dispersed residue function and perform in silico experiments that validate the reliability of our approach against the block-circulant Singular Value Decomposition. We successfully apply the approach to stroke MRI data and show that the calculated parameters are coherent with physiological considerations, highlighting the importance of dispersion as an effect to be <b>measured</b> rather than <b>discarded...</b>|$|R
40|$|Incomplete data present serious {{problems}} when integrating largescale brain imaging data sets from different imaging modalities. In the Alzheimer’s Disease Neuroimaging Initiative (ADNI), for example, {{over half of}} the subjects lack cerebrospinal fluid (CSF) measurements; an independent half of the subjects do not have fluorodeoxyglucose positron emission tomography (FDG-PET) scans; many lack proteomics measurements. Traditionally, subjects with missing <b>measures</b> are <b>discarded,</b> resulting in a severe loss of available information. We address this problem by proposing two novel learning methods where all the samples (with at least one available data source) can be used. In the first method, we divide our samples according to the availability of data sources, and we learn shared sets of features with state-of-the-art sparse learning methods. Our second method learns a base classifier for each data source independently, based on which we represent each source using a single column of prediction scores; we then estimate the missing prediction scores, which, combined with the existing prediction scores, are used to build a multi-source fusion model. To illustrate the proposed approaches, we classify patients from the ADNI study into groups with Alzheimer’s disease (AD), mild cognitive impairment (MCI) and normal controls, based on the multi-modality data. At baseline, ADNI’s 780 participants (172 AD, 397 MCI, 211 Normal), have at least one of four data types: magnetic resonance imaging (MRI), FDG-PET, CSF and proteomics. These data are used to test our algorithms. Comprehensive experiments show that our proposed methods yield stable and promising results...|$|R
40|$|Analysis of {{incomplete}} data {{is a big}} challenge when integrating large-scale brain imaging datasets from different imaging modalities. In the Alzheimer’s Disease Neuroimaging Initiative (ADNI), for example, {{over half of the}} subjects lack cerebrospinal fluid (CSF) measurements; an independent half of the subjects do not have fluorodeoxyglucose positron emission tomography (FDG-PET) scans; many lack proteomics measurements. Traditionally, subjects with missing <b>measures</b> are <b>discarded,</b> resulting in a severe loss of available information. In this paper, we address this problem by proposing an incomplete Multi-Source Feature (iMSF) learning method where all the samples (with at least one available data source) can be used. To illustrate the proposed approach, we classify patients from the ADNI study into groups with Alzheimer’s disease (AD), mild cognitive impairment (MCI) and normal controls, based on the multi-modality data. At baseline, ADNI’s 780 participants (172 AD, 397 MCI, 211 NC), have at least one of four data types: magnetic resonance imaging (MRI), FDG-PET, CSF and proteomics. These data are used to test our algorithm. Depending on the problem being solved, we divide our samples according to the availability of data sources, and we learn shared sets of features with state-of-the-art sparse learning methods. To build a practical and robust system, we construct a classifier ensemble by combining our method with four other methods for missing value estimation. Comprehensive experiments with various parameters show that our proposed iMSF method and the ensemble model yield stable and promising results...|$|R
40|$|Summary The {{information}} in animal colour patterns {{plays a key}} role in many ecological interactions; quantification would help us to study them, but this is problematic. Comparing patterns using human judgement is subjective and inconsistent. Traditional shape analysis is unsuitable as patterns do not usually contain conserved landmarks. Alternative statistical approaches also have weaknesses, particularly as they are generally based on summary <b>measures</b> that <b>discard</b> most or all of the spatial {{information in}} a pattern. We present a method for quantifying the similarity of a pair of patterns based on the distance transform of a binary image. The method compares the whole pattern, pixel by pixel, while being robust to small spatial variations among images. We demonstrate the utility of the distance transform method using three ecological examples. We generate a measure of mimetic accuracy between hoverflies (Diptera: Syrphidae) and wasps (Hymenoptera) based on abdominal pattern and show that this correlates strongly with the perception of a model predator (humans). We calculate similarity values within a group of mimetic butterflies and compare this with proposed pairings of Müllerian comimics. Finally, we characterise variation in clypeal badges of a paper wasp (Polistes dominula) and compare this with previous measures of variation. While our results generally support the findings of existing studies that have used simpler ad hoc methods for measuring differences between patterns, our method is able to detect more subtle variation and hence reveal previously overlooked trends...|$|R
40|$|We {{present a}} general {{mathematical}} {{framework for the}} fusion of noisy sensor data. On {{the basis of the}} mathematical theory of dynamical systems we couple the outputs of the sensors to obtain a nonlinearly averaged overall estimate of the physical quantity to <b>measure</b> which automatically <b>discards</b> outliers from the averaging process. Drifts within the time series of single sensors can be compensated through a recalibration by the method of time scale inversion. By means of a unified way of representing information as stable states of a dynamical system it is possible to integrate di#erent sorts of information such as expert knowledge and sensor information smoothly within the data fusion system. We verify the feasibility of our approach on the basis of simulated stochastic data sets and on the basis of data from a study in which the brightness temperature of oil films on sea water was measured. The proposed self-calibrating sensor fusion architecture extends the work we presented at IGARSS [...] ...|$|R
40|$|OBJECTIVES: This study {{examined}} the effects of a school-based intervention designed to promote the consumption of low-fat white milk at lunchtime in 6 elementary schools in an inner-city, primarily Latino neighborhood. METHODS: A multifaceted intervention based on social marketing techniques was delivered at 3 randomly selected schools. The school was the unit of assignment and analysis; 6902 children were involved in the study. Milk selection and consumption were <b>measured</b> by sampling <b>discarded</b> milk and/or tallying milk carton disappearance at baseline, immediately postintervention, and at 3 to 4 months follow-up. RESULTS: Immediately postintervention, the mean proportion of sampled milk cartons that contained low-fat milk increased in the intervention schools, from 25 % to 57 %, but remained constant at 28 % in the control schools. Differences between intervention and control schools remained significant at 3 to 4 months follow-up. The intervention was not associated with a decrease in overall milk consumption. CONCLUSIONS: A school-based intervention can lead to significant increases in student consumption of low-fat milk...|$|R
40|$|WFD {{prescribe}} {{a strong}} public participation {{in decision making}} processes. One common experience among the NOLIMP project partners {{is the importance of}} involving stakeholders at an early stage. A well established contact network will ease the process of solving controversies and agreeing on measures to meet environmental targets. The development of demonstrations sites create interests from other areas and exchange of knowledge/experience on the national level is stimulated. WFD supply some general rules on monitoring. It has been experienced that monitoring needs to be relatively comprehensive to enable proper abatement planning {{and to be able to}} follow the environmental effects/improvements of implemented measures. The use of numerical models to better understand the dynamics and interactions between different factors as well as climate is beneficial for water management. Another aspect of monitoring is to enable transparency and objectivity regarding the quantification of pollution load from different sources/sectors. The use of cost efficiency analysis is recommended in WFD, but its use has different status and history among the partners. The ranking of measures according to cost and the possibility to <b>discard</b> <b>measures</b> that obviously have a low efficiency is experienced as very usefu...|$|R
30|$|However, {{some aspects}} of the system could still be {{evaluated}} in more detail, while others could be improved or extended. An obvious direction of further evaluation is to adapt the proposed system to different EDM genres. For some genres, this can be as straightforward as adapting the tempo range, and potentially retraining the downbeat tracker on some annotated songs in the target genre. However, as different genres might require different styles of mixing [2], for some genres, more modifications might be required (e.g., implementing additional transition types). Additionally, comparing the different transition types or even the DJ system as a whole with for example transitions performed by a human expert or a commercial automatic DJ system would also be very informative, and offers a great direction of further research. Several extensions and augmentations are also possible. Firstly, the annotation modules would benefit from a built-in reliability <b>measure</b> to <b>discard</b> songs with uncertain annotations. Furthermore, the used vocal activity detection and key estimation algorithms are simple baseline versions and subject to improvement. To improve the cue point selection, a finer distinction between section types could be implemented, instead of only considering “low” and “high” sections. This could lead to more precise cue point positions or a more diverse set of transition types. It would furthermore be very interesting to make the volume and equalization progressions throughout the transitions adapt to the songs being mixed, instead of following a fixed pattern as in the current implementation. Finally, even though the track selection method yields pleasing results, it might be an oversimplification of how professional DJs compose their sets and create a deliberate progression of energy and mood throughout the mix. A model that better understands the long-term structures and composition of a good Drum and Bass mix, e.g., learned using the abundance of professional Drum and Bass mixes that can be found online, might therefore lead to a better result.|$|R
40|$|Due to its blood microcirculation, {{the retina}} {{is one of}} the first organs {{affected}} by hypertension and diabetes: retinal damages can lead to serious visual loss, that can be avoided by an early diagnosis. The most distinctive sign of diabetic retinopathy or severe hypertensive retinopathy are dark lesions such as haemorrhages and microaneurysms (HM), and bright lesions such as hard exudates (HE) and cotton wool spots (CWS). Automatic detection of their presence in the retina is thus of paramount importance for assessing the presence of retinopathy, and therefore relieve the burden of image examination by retinal experts. The first step for the automatic detection of retinal lesions has to identify candidate lesions, not losing any of them, and providing with accurate outlines, so to allow the extraction of meaningful features for a possible subsequent classification. To accomplish this, we propose a two stage approach. The first stage identifies the rough location of candidate lesions with one or more seed points, evaluating a measure of the spatial density of pixels selected by a local thresholding. The second stage has the objective of outlining as accurately as possible the lesions surrounding each seed. Due to the high variability of lesions and background appearance, classical region growing approaches often fail and are difficult to calibrate, since in retinal imaging the noisy and highly variable background hides the small homogenous regions representing lesions. To tackle this, we rely on a stochastic modelling of a region of interest around a seed as a Markov random field: This is particularly suited to separate objects with different textures, since it combines feature distribution and spatial connectivity. Responses to a Gabor filters bank spanning different orientations and scales provide the description of the local texture, and the final classification is obtained via a simulated annealing optimization. Along with the classification, we propose a simple post-optimization <b>measure</b> to <b>discard</b> regions where no lesions are present despite the presence of seeds. We show results of the proposed method on a data set of manually segmented 60 images, 6 of which containing retinal lesions...|$|R
40|$|Årsliste 2007 WFD {{prescribe}} {{a strong}} public participation {{in decision making}} processes. One common experience among the NOLIMP project partners {{is the importance of}} involving stakeholders at an early stage. A well established contact network will ease the process of solving controversies and agreeing on measures to meet environmental targets. The development of demonstrations sites create interests from other areas and exchange of knowledge/experience on the national level is stimulated. WFD supply some general rules on monitoring. It has been experienced that monitoring needs to be relatively comprehensive to enable proper abatement planning {{and to be able to}} follow the environmental effects/improvements of implemented measures. The use of numerical models to better understand the dynamics and interactions between different factors as well as climate is beneficial for water management. Another aspect of monitoring is to enable transparency and objectivity regarding the quantification of pollution load from different sources/sectors. The use of cost efficiency analysis is recommended in WFD, but its use has different status and history among the partners. The ranking of measures according to cost and the possibility to <b>discard</b> <b>measures</b> that obviously have a low efficiency is experienced as very usefulNorth Sea Programme of the European Regional Development Fun...|$|R
40|$|The Northwest Fisheries Science Center (NWFSC) has {{produced}} reports documenting total fishing mortality for groundfish {{for the years}} 2004 - 2006, which {{are available on the}} NWFSC’s website (Hastie 2006, Hastie and Bellman 2006, 2007). A central element of these analyses is the estimation of discard occurring in the non-hake groundfish bottom trawl fishery. This estimation process incorporates data from at-sea observations, logbooks, and fish tickets. During the past year, questions have been raised regarding the methods used to expand amounts of <b>measured</b> or estimated <b>discard</b> on observed vessels into total discard estimates for the groundfish bottom trawl fleet. As part of the Groundfish Science Report at the September 2007 Council meeting, the NWFSC presented a statement addressing several questions relating to the estimation of groundfish bycatch (Supplemental Agenda Item G. 1. b). We noted that the use of tow duration as a measure of fleet effort in this expansion process could be expected to yield higher discard estimates for some species and lower estimates for others. This report documents the differences between discard amounts estimated with the method currently used to assess total mortality and estimates derived using an alternative method that utilizes tow duration, for the years 2004 - 2006...|$|R
40|$|Solutions to {{the problem}} of {{discarding}} in fisheries have been debated for decades. Despite this attention, <b>measures</b> to ameliorate <b>discarding</b> have had limited success. Regulators, researchers, and industry continue to struggle with fisheries management and foregone yield {{in the face of the}} continued wastage of valuable resources due to discarding. Waste minimization and by-product utilization are powerful imperatives in other sectors that are also reliant on the harvest of natural resources. This paper considers the performance of these sectors in waste minimization and by-product utilization, with the aim of dentifying practices and processes that may be applied to ameliorate discarding in fisheries. This paper describes the handling, utilization, and mitigation of discards and waste in the livestock farming, agriculture, mining, and waste management industries, and in particular, in forestry. In terms of biological impact, economic objectives, and management approaches the harvesting of trees has substantial similarities to industrialized fishing. However, the forestry sector has found ways to utilize almost 100 % of the natural product harvest by establishing markets and new products. Analogous developments within the fishing industry could substantially improve sustainability through reduced levels of discarding and wastage. Based on the experiences of these sectors it is suggested that evaluations of potential Management Strategies are developed to specifically examine discard mitigation approaches on a broader scale than previously conducted. JRC. G. 4 -Maritime affair...|$|R
40|$|In this thesis, {{our work}} {{builds on the}} future hedging {{strategy}} presented by Barbi and Romagnoli (2014). The authors propose the optimal hedge ratio as the minimizer of a generic quantile risk measure (QRM), which includes Value-at-Risk (VaR) and Expected Shortfall (ES). Moreover, the quantiles of the hedged portfolio can be represented {{in terms of a}} copula function, so that the dependence structure between the spot and futures could be better captured and hedging performance improved. In that paper, {{it has been shown that}} the empirical performance of the model is in general superior compared to some of the existing future hedging models that only consider limited risk <b>measures</b> or <b>discard</b> copula method. However, the model suggests that we use the static copula to fit observations during a previous long period and represent the spot-futures dependence structure. It may result in a poor representation as the dependence between the spot and futures is always characterized as time-varying. Moreover, as a consequence, it may yield a less accurate optimal hedge ratio and inefficient hedging performance. Motivated by this drawback, this thesis starts with the discussion of the robustness of the model in Barbi and Romagnoli (2014), where we use simulated data to conduct sensitivity analysis and performance test. Then an extension is proposed in which we allow the copula parameter to be dynamic and switch between different regimes. We consider two regimes and they correspond to relatively strong and weak dependence between the spot and futures return series. With such extension, we propose an hedging strategy to calculate the approximate optimal hedge ratio, which we call the extended regime-switching hedging strategy or the extended model. Monte Carlo simulations are followed to compare its new hedging performance with that of the original model without regime switching. The extended regime-switching model shows good advantage in capturing the dynamic dependence, but it dominates the original model in hedging effectiveness only when there are significant regime shifts in the spot-futures dependence and the difference of dependence level in two regimes is more dramatic. Finally, our proposed extended model methodology is applied to empirical data, where we use FTSE 100 stock index and its corresponding futures contract. The empirical results reconfirm our conclusions getting from simulated data...|$|R
40|$|Cuba's Transfusion Medicine Program (TMP) is a {{subsystem}} of the country's National Health System. The TMP's {{objective is}} to ensure hemotherapy with blood that is safe and sufficient for all the individuals who need it. The TMP subsystem {{is made up of}} the National Commission on Transfusion Medicine, the Institute of Hematology and Immunology, 37 clinical services, 44 blood banks, 120 collection centers, 19 mobile units, and 37 blood certification laboratories. Additional facilities include a laboratory for plasma separation, a laboratory that produces leukocyte interferon and transfer factor, and two laboratories that produce reagents for blood classification and blood diagnosis systems. In Cuba, blood donation is voluntary. Since 1997 approximately 5 % of the population per year has donated blood, thus meeting the goal recommended by the Pan American Health Organization of one voluntary blood donation annually for every 20 persons. During 2002, 563 204 blood donations were received, and there were 445 898 transfusions of blood or blood components. All donations are individually screened for HIV 1 and 2, hepatitis B, hepatitis C, and syphilis, thus meeting the country's current regulations. In 2002 these screening <b>measures</b> led to <b>discarding,</b> respectively, 0. 12 %, 0. 60 %, 0. 71 %, and 1. 8 % of the blood donations. Although the prevalence of human T-cell lymphotropic virus I and II in Cuba is very low, this test will soon be added to the screening process...|$|R
40|$|Like {{many school}} districts in the southwest, the Arlington Indement School District (AISD) felt a need to upgrade their schools to {{maximize}} their value for the gas and electricity being used. They felt they needed to make sure that energy was not being wasted and that any energy conservation measure implemented would provide good payback for the investment made by the school district. After conducting an extensive search to determine the best way to identify and implement energy conservation strategies, the school district selected Johnson Controls, Inc. as the company they felt could best help them achieve the goals that they had set. AISD also requested help in identifying the best way for the school district to finance the measures taken and for the tracking of the measures to make sure the desired energy conservation goals were met. Finally, the school district requested, a guarantee that savings goal be attained by an agreement that the contractor pay for any "shortfall" in savings that might occur. The contractor was initially assigned the ten schools which seemed to be the most wasteful within the school district. With their personnel, loads and energy users within the schools were identified and quantified. From this information, areas where energy was being wasted were identified and ways of eliminating waste were examined. If a reasonable payback and a means to measure the payback were identified, then it was included in the list of measures to be presented for consideration; if payback was high, or could not be effectively measured, then that conservation <b>measure</b> was <b>discarded.</b> After all energy conservation measure ideas were gathered, they were presented to the school board with a suggested method of payment, so the best return on investment for the district could be achieved. Also, a written guarantee was made available, so the school district could feel certain that in whatever conservation measures selected, desired results would be achieved. During the construction phase, special attention was given to ensure classes would be disrupted as little as possible and the work would be completed on schedule. After construction, the contractor monitored savings results and has shown that this project, which represented an investment of well over a million dollars, will provide payback on time, or in advance, of the original estimate...|$|R
40|$|Since {{its first}} use in human in 1929, the {{electroencephalogram}} (EEG) {{has become one}} of the most important diagnostic tool in clinical neurophysiology. However, their use in clinical studies is limited because the huge quantity of collected information is complicated to treat. Indeed, it is very difficult to have an overall picture of this multivariate problem. In addition to the impressive quantity of data to be treated, an intrinsic problem with electroencephalograms is that the signals are "contaminated" by body signals not directly related to cerebral activity. However, these signals do not interest us directly to evaluate treatment effect on the brain. Removing these signals known as "parasitic noise" from electroencephalograms is a difficult task. We use clinical data kindly made available by the pharmaceutical company Eli Lilly (Lilly Clinical Operations S. A., Louvain-la-Neuve, Belgium). Particular types of analyses were already carried out on these data, most based on frequency bands. They mainly confirmed the enormous potential of EEG in clinical studies without much insight in the understanding of treatment effect on the brain. The aim of this thesis is to propose and evaluate a panel of statistical techniques to clean and to analyze electroencephalograms. The first presented tool enables to align curves such as selected parts of EEGs before any further statistical treatment. Indeed, when monitoring some continuous process on similar units (like patients in a clinical study), one often notices a typical pattern common to all curves but with variation both in amplitude and dynamics across curves. In particular, typical peaks could be shifted from unit to unit. This complicates the statistical analysis of sample of curves. For example, the cross-sectional average usually does not reflect a typical curve pattern: due to shifts, the signal structure is smeared or might even disappear. Another of the presented tools is based on the preliminary linear decomposition of EEGs into statistically independent signals. This decomposition provides on the one hand an effective cleaning method and on the other hand a considerable reduction of the quantity of data to be analyzed. The technique of decomposition of our signals in statistically independent signals is a well-known technique in physics primarily used to unmix sound signals. This technique is named Independent Component Analysis or ICA. The last studied tool is functional ANOVA. The analysis of longitudinal curve data is a methodological and computational challenge for statisticians. Such data are often generated in biomedical studies. Most of the time, the statistical analysis focuses on simple summary <b>measures,</b> thereby <b>discarding</b> potentially important information. We propose to model these curves using non parametric regression techniques based on splines. (STAT 3) [...] UCL, 200...|$|R
40|$|Madsen, A. T., Duller, G. A. T., Donnelly, J. P., Roberts, H. M., Wintle, A. G. (2009). A {{chronology}} of hurricane landfalls at Little Sippewissett Marsh, Massachusetts, USA, using optical dating. Geomorphology, 109 (1 - 2), 36 - 45. Sponsorship: National Science FoundationOptical dating {{has been applied}} to sediments preserved in Little Sippewissett Marsh, Massachusetts, USA, which are associated with overwashing of the beach barrier during hurricane strikes on the coast. The aims were to determine the hurricane landfall frequency, and make comparisons with independent age control and the historical record. Written sources of hurricane activity along the American east coast are only considered reliable back to the mid 19 th century, but the sedimentary record is potentially much longer. Optical dating was applied to quartz grains extracted from thirteen samples within a sediment core from the salt-marsh. Variability in the luminescence characteristics between aliquots was observed and ~ 33 % of the <b>measured</b> aliquots were <b>discarded</b> based upon the ratio of the fast component to the medium component. The majority of the samples gave normal dose distributions implying homogeneous resetting of the luminescence signal at the time of deposition, but three of the samples required application of the minimum age model (MAM). Ages ranging between 20 ? 2 and 594 ? 38 years were obtained and are broadly in agreement with independent chronologies, thus demonstrating the potential of optical dating in this setting. The hurricane record based upon optical dating extends approximately 300 years further back in time than the official National Oceanic Atmospheric Administration (NOAA) record. The localised nature of hurricane landfalls means that {{it will be necessary to}} collect multiple cores from a number of different sites in order to build up a complete hurricane record for this part of the coast. Peer reviewe...|$|R
40|$|An optimistic {{computation}} is a computation {{that makes}} guesses about its future behavior, then proceeds with execution {{based on these}} guesses {{before they can be}} verified. Optimistic computations guess data values before they are produced and guess the control flow of computations before it is known. The performance of optimistic computations is determined by the number of idle resources available for optimistic execution, the percentage of guesses that are correct, the bookkeeping costs of managing optimistic execution, and the overhead of preventing optimistic computations from interfering with their execution environments until the guesses that they are based on are verified. We model computations by their program dependence graphs, then present a series of application-independent transformations on the dependence graphs that convert pessimistic computations into semantically equivalent optimistic computations. We demonstrate the viability of this approach by applying our transformations to the make program, fault tolerance based on message logging and checkpointing, distributed simulation, database concurrency control, and bulk data transfer. Our derived optimistic computations are similar to those presented in the literature, but typically require additional application-dependent transformations to produce viable implementations. We investigate, in detail, the implementation and performance of optimistic make, an optimistic variant of conventional pessimistic make. Optimistic make executes the commands necessary to bring makefile targets up-to-date prior to the time the user types a make request. The side effects produced by these optimistically executed commands are masked using encapsulations until the commands are known to be needed, at which time the side effects are committed to the external world. The side effects of unneeded commands and commands executed with inputs that have changed are <b>discarded.</b> <b>Measured</b> and simulated results from our implementation of optimistic make show a median response time improvement of 1. 72 and a mean improvement of 8. 28 over pessimistic make...|$|R
40|$|The landing {{obligation}} of the reformed European Union Common Fisheries Policy {{is designed to}} encourage more selective fishing strategies and improve recording of catches. There are allowable exemptions to this {{landing obligation}} including for species with high post-release survivability. Discarding patterns of prawns (Nephrops norvegicus) were evaluated in a trawl fishery in the Firth of Clyde, West Scotland which supplies the live-catch market. Around 30 % of the Nephrops caught were discarded but the reasons for discarding changed seasonally. Using visual indices, physiological biomarkers and video recordings this study evaluated the physiological condition linked to short-term survival and predator avoidance behaviour of the discarded animals. Although short-term survival after 48 h recovery was high (around 90 %) and physiological <b>measures</b> indicated that <b>discarded</b> Nephrops can recover from trawling, survival was negatively affected by levels of physical damage and Hematodinium infection. Taking into consideration these factors a conservative estimate for discard survival was 63 - 88 %. Underwater video showed that Nephrops discarded in good condition rapidly recovered normal behaviour when placed on the seabed. Moribund animals, however, took up to 10 minutes to return to an upright posture and this time was sufficient for predators to be attracted. Since around 20 % of Nephrops were in a moribund condition immediately after trawling, the survival estimates based upon enclosed recovery experiments may need correcting by up to this amount to account for potential interactions with predators on the seabed. The post-release survival rates in discarded Nephrops suggested for this fishery are relatively high compared with other Nephrops trawl-fisheries which have been studied. This could be explained because this fishery targets the live-market, prioritises product quality over volume and uses short-duration tows leading to relatively low levels of physical damage to the Nephrops...|$|R
40|$|In general, in NMR {{spectroscopy}} {{a sample}} {{is prepared for}} the NMR measurement, positioned in the magnet, <b>measured,</b> and <b>discarded.</b> But when non-destructiveness of the sample is more important, it becomes important to rely on relaxation analysis at low resolution NMR. If the samples are big in size the unilateral or single-sided NMR in inhomogeneous fields is an important choice. Unilateral NMR or inside-out NMR is one recognized technique {{in the field of}} low resolution NMR. The NMR-MOUSE (Mobile Universal Surface Explorer) belongs to this single-sided NMR concept. It is a palm-size NMR device which is built from two permanent magnets. They are mounted on an iron yoke with anti-parallel polarization. The main direction of the polarization field B 0 is across the gap. The rf field B 1 is generated by a surface coil which is mounted in the gap. Together with the equation of motion for the nuclear magnetization the profiles of the B 0 and B 1 fields define the sensitive volume in which the NMR conditions are satisfied. The sensitive volume is restricted to regions near the surface of the examined object. A particularly interesting development in this context is the use of NMR for process and quality control. It implies the lack of invasive sample preparation. Applications of low-resolution NMR are in the food industry, the paper and pulp industry, and in the analysis of building materials and polymer materials. Following research areas has been covered in the Ph. D. thesis: (a) Investigation of Silicone and synthetic rubber samples: Different NMR-MOUSEs have been used for the analysis of industrial and institute made rubber samples (multi-layered PDMS cable shells and cured natural rubber samples). High energy cable shells made of silicone rubber (aged and stretched at different temperature) were investigated successfully with relaxometric analysis with Hahn, CPMG and MQ pulse sequences. Besides the detection of voids in the sample, agglomerates have also been discovered with the NMR imaging technique in silicone as well as natural rubber. The results obtained with the NMR imaging experiments confirm the results obtained with NMR-MOUSE. (b) Investigation of porous building materials: Different NMR-MOUSEs have been used to characterize the size distribution of water filled pores in building materials with the CPMG pulse sequence even in the case of ferromagnetic contaminations. The main idea was to investigate the state of historical building materials which can be subjected to chemical treatment with stone strengtheners for preservation. The shapes and the positions of the relaxation time distributions {{are similar to those of}} the pore size distributions obtained by mercury intrusion porosimetry unless the samples exhibit very strong ferromagnetic contaminations (historical brick samples). (c) Degradation of historical papers: In the present work historical books from the 17 th century were subjected to the non-destructive relaxometric analysis with the NMR-MOUSE using the Hahn echo pulse sequence. The distribution of relaxation times obtained from inverse Laplace transformation of the Hahn echo envelope data of differently degraded paper samples clearly discriminate the paper types investigated...|$|R
40|$|The {{research}} project TOETS was initiated {{as a response}} to a number of specific management questions concerning the discard problems in the Belgian beam trawl fishery, which is characterized by particularly high discard percentages of commercially unimportant organisms (invertebrates, fish species with low or no commercial interest, and individuals of commercially important species below minimum landing size). Several issues concerning the development of a suitable protocol for collecting discard data on board of commercial beam trawl vessels, the quantification of the amount and the composition of discards produced by the Belgian beam trawlers, and the evaluation of technical adaptations to the existing trawling gear resulting in a substantial reduction in the amount of discards, were the main research topics of this project. Informing the fisheries sector about the results of the project also belonged to the objectives. Based on the results of the project TOETS there is concluded that about one fourth of the Belgian catches currently consists of organisms that are discarded (with an extensive variation in discard weights, composition and length-frequency distributions). Including noncommercial organisms in the existing discard monitoring programmes implicates an enormous added value for the insights we build up with respect to the impact of beam trawling on the marine ecosystem, but requires some adaptations to the protocol, and implies as well a significantly higher physical demand of the on-board observers. However, technical adaptations to the trawling gear offer great potentials for reducing discards. (with the best results, in this study, when using a benthos release panel). However, the reduction mainly affected discarded numbers rather than weights. Finally, it is concluded that the fisheries industry is very well aware of the discard problems in the Belgian beam trawl fisheries, as is illustrated by some own initiatives and the willingness to cooperate with scientists from the Institute of Agricultural and Fisheries Research. Additionally, the resultats of the project TOETS can help our fishermen to react pro-actively to the possible future introduction of European <b>measures</b> to achieve <b>discard</b> reductions...|$|R
40|$|In {{order to}} {{understand}} in more depth and on a genome wide scale the behavior of transcription factors (TFs), novel quantitative experiments with high-throughput are needed. Recently, HiTS-FLIP (High-Throughput Sequencing-Fluorescent Ligand Interaction Profiling) was invented by the Burge lab at the MIT (Nutiu et al. (2011)). Based on an Illumina GA-IIx machine for next-generation sequencing, HiTS-FLIP allows to measure the affinity of fluorescent labeled proteins to millions of DNA clusters at equilibrium in an unbiased and untargeted way examining the entire sequence space by Determination of dissociation constants (Kds) for all 12 -mer DNA motifs. During my PhD I helped to improve the experimental design of this method to allow measuring the protein-DNA binding events at equilibrium omitting any washing step by utilizing the TIRF (Total Internal Reflection Fluorescence) based optics of the GA-IIx. In addition, I developed the first versions of XML based controlling software that automates the measurement procedure. Meeting the needs for processing the vast amount of data produced by each run, I developed a sophisticated, high performance software pipeline that locates DNA clusters, normalizes and extracts the fluorescent signals. Moreover, cluster contained k-mer motifs are ranked and their DNA binding affinities are quantified with high accuracy. My approach of applying phase-correlation to estimate the relative translative Offset between the observed tile images and the template images omits resequencing and thus allows to reuse the flow cell for several HiTS-FLIP experiments, which greatly reduces cost and time. Instead of using information from the sequencing images like Nutiu et al. (2011) for normalizing the cluster intensities which introduces a nucleotide specific bias, I estimate the cluster related normalization factors directly from the protein Images which captures the non-even illumination bias more accurately and leads to an improved correction for each tile image. My analysis of the ranking algorithm by Nutiu et al. (2011) has revealed that it is unable to rank all <b>measured</b> k-mers. <b>Discarding</b> all the clusters related to previously ranked k-mers has the side effect of eliminating any clusters on which k-mers could be ranked that share submotifs with previously ranked k-mers. This shortcoming affects even strong binding k-mers with only one mutation away from the top ranked k-mer. My findings show that omitting the cluster deletion step in the ranking process overcomes this limitation and allows to rank {{the full spectrum of}} all possible k-mers. In addition, the performance of the ranking algorithm is drastically reduced by my insight from a quadratic to a linear run time. The experimental improvements combined with the sophisticated processing of the data has led to a very high accuracy of the HiTS-FLIP dissociation constants (Kds) comparable to the Kds measured by the very sensitive HiP-FA assay (Jung et al. (2015)). However, experimentally HiTS-FLIP is a very challenging assay. In total, eight HiTS-FLIP experiments were performed but only one showed saturation, the others exhibited Protein aggregation occurring at the amplified DNA clusters. This biochemical issue could not be remedied. As example TF for studying the details of HiTS-FLIP, GCN 4 was chosen which is a dimeric, basic leucine zipper TF and which acts as the master regulator of the amino acid starvation Response in Saccharomyces cerevisiae (Natarajan et al. (2001)). The fluorescent dye was mOrange. The HiTS-FLIP Kds for the TF GCN 4 were validated by the HiP-FA assay and a Pearson correlation coefficient of R= 0. 99 and a relative error of delta= 30. 91...|$|R
40|$|Signal {{compression}} algorithms {{are required}} in speech processing system {{to support the}} limited bandwidth requirements. And while compressing the signal, speech quality must not be degraded and modern speech methods makes use of perceptual information present in the signal to distinguish which part of the signal needs to be sent and which needs to be <b>discarded.</b> <b>Measures</b> like Signal to Noise Ratio (SNR) do not provide accurate measurement of speech quality of these modern communication systems. Traditionally, Mean opinion score(MOS) is an subjective quality measure with panel of listeners and conducting these tests is very expensive and time consuming making it impractical for all test conditions. An objective quality measure Perceptual evaluation of speech quality was recommended by ITU-T as an enhanced speech quality measure, especially in a laptop microphone system. This research includes the elimination of fan noise, keystroke noise and measurement of speech quality for different noises like factory noise, engine noise etc using PESQ at different SNRs varies from 0 db to 45 db. The speech enhancement algorithms like Wiener Beamformer, spectral subtraction and combination of Wiener Beamformer and Spectral Subtraction is implemented in Matlab environment, PESQ measures the speech quality range from 1 to 4. 5, 1 means poor quality and 4. 5 means excellent. In this thesis simulation results shows that the PESQ values for different noise (fan noise, keystroke, factory noise and engine noise) signals using different speech enhancement algorithms (systems) for various SNRs like 0 db to 45 db. Using these speech enhancement algorithms reduce the fan noise, keystroke noise, factory noise and engine noise for various SNRs with an SNRI and maintaining speech quality. In the simulation results of evaluation method- 1, in which the pure speech signal is compared with itself, then PESQ score is observed as 4. 5 {{on the scale of}} 4. 5. In the evaluation method- 2, the different noise signals with different sampling frequencies i. e fs= 8 K and 16 K are tested with pure speech signal, to analyze the PESQ variation over different sampling frequencies. With the obtained results it shows that for fs= 8 kHz, the PESQ score is higher when compared to fs= 16 KHz as the SNR is gradually increasing from 0 to 45 dB. In the evaluation method- 3, the Wiener Beamformer system itself preserves the speech quality while attenuating different noises i. e. PESQ score is higher, when compared to SS system and combined system. Lingapuram Praveen Utridarevägen 3 A c/o Rakeshreddy polamreddy 37140, Karlskrona...|$|R
