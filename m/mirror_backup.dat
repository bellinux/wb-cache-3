5|26|Public
2500|$|Further {{equipment}} upgrades {{arrived on}} 20 July 2009 when the facelifted Australian range went on sale. All variants now offered six airbags, Bluetooth, an auxiliary audio-input jack {{for the sound}} system, sun visor lamps and seatback pockets. The [...] "Ateva" [...] grade received a new audio system with 4.3-inch color LCD incorporating a rear-view camera, while the [...] "Sportivo" [...] variant gained the same LCD and dual-zone climate control. Upgrades for the [...] "Grande" [...] comprised keyless entry and starting system, rear parking sensors, an electrochromatic rear-view <b>mirror,</b> <b>backup</b> camera, and an upgraded sound system. From March 2010 production, alloy wheels were made as standard fitment to the base [...] "Altise". The limited edition [...] "Touring" [...] was re-launched the following June, this time adding a reversing camera, a rear lip spoiler, and manual transmission availability over the previous release. Due to lack of demand, Australian manufacture of Camrys fitted with manual transmissions ended {{at the end of}} May 2011.|$|E
5000|$|Dual write capabilities. (Redundant <b>mirror</b> <b>backup)</b> (filePro 5.8.01.03) ...|$|E
5000|$|Backup4all is {{a backup}} {{software}} for Microsoft Windows developed by Softland. It allows files {{to be backed}} up to any local or network drive, FTP or SFTP server, CD/DVD/Blu-ray, or other removable media. The term [...] "backing up" [...] means to save data on an external hard drive in case something happens to you personal computer. This is useful to safeguard sensitive data from deletion. Four different types of backups are supported: incremental backup, differential backup, full backup and <b>mirror</b> <b>backup.</b>|$|E
5000|$|... #Caption: 2013-2014 Ridgeline {{rearview}} <b>mirror</b> with <b>backup</b> camera (2014 RTL) ...|$|R
5000|$|Redundant {{systems for}} {{emergency}} <b>backup</b> (<b>Mirrored</b> or n+1 redundancy available) ...|$|R
5000|$|... #Caption: The <b>backup</b> <b>mirror,</b> by Kodak; its {{inner support}} {{structure}} {{can be seen}} {{because it is not}} coated with a reflective surface ...|$|R
5000|$|Further {{equipment}} upgrades {{arrived on}} 20 July 2009 when the facelifted Australian range went on sale. All variants now offered six airbags, Bluetooth, an auxiliary audio-input jack {{for the sound}} system, sun visor lamps and seatback pockets. The [...] "Ateva" [...] grade received a new audio system with 4.3-inch color LCD incorporating a rear-view camera, while the [...] "Sportivo" [...] variant gained the same LCD and dual-zone climate control. Upgrades for the [...] "Grande" [...] comprised keyless entry and starting system, rear parking sensors, an electrochromatic rear-view <b>mirror,</b> <b>backup</b> camera, and an upgraded sound system. From March 2010 production, alloy wheels were made as standard fitment to the base [...] "Altise". The limited edition [...] "Touring" [...] was re-launched the following June, this time adding a reversing camera, a rear lip spoiler, and manual transmission availability over the previous release. Due to lack of demand, Australian manufacture of Camrys fitted with manual transmissions ended {{at the end of}} May 2011.|$|E
40|$|Development of {{software}} {{is one of}} the most expensive projects undertaken in practice. Traditionally, the rate of failure in software development projects is higher compared to other kinds of projects. This is partly due to the failure in determining software users’ requirements. By using Quality Function Deployment (QFD), this research focuses on identification and prioritization of users’ requirements in the context of developing quality health-care software system for Sultan Qaboos University Hospital (SQUH) in Oman. A total of 95 staff working at eight departments of SQUH were contacted and they were requested to provide their requirements in using hospital information systems. Analytic Hierarchy Process has been integrated with QFD for prioritizing those user requirements. Then, in consultation with a number {{of software}} engineers, a list consisting of 30 technical requirements was generated. These requirements are divided into seven categories and all of them are purported to satisfy the user needs. At the end of QFD exercise, continuous <b>mirror</b> <b>backup</b> from backup category, multi-level access from the security and confidentiality category, linkage to databases from application category emerge as technical requirements having higher weights. These technical requirements should receive considerable attention when designing the health-care software system for SQUH. ...|$|E
5000|$|... #Caption: One Hubble <b>backup</b> <b>mirror</b> {{is used by}} Magdalena Observatory's 2.4-meter SINGLE Telescope, and {{the other}} is an exhibit in the Smithsonian Museum ...|$|R
25|$|New grilles and four-segmented {{taillights}} {{highlighted the}} facelifted 1966 LeSabre models. Also new was a revised instrument panel with a horizontal sweep speedometer replacing the round pod instruments and new interior door handles. Both base and Custom level series were continued. New standard safety features included a padded instrument panel, outside driver-side rear view <b>mirror</b> and <b>backup</b> lights.|$|R
50|$|In 2008 (for the 2009 model year) RAV4 {{was given}} a {{mid-cycle}} refresh in some markets, featuring a number of changes, including an all-new 4-cylinder engine, and a redesigned front end and tweaked rear end. The Limited model gets a different front grille and bumper cover from other models. The Sport model features a bigger spoiler and red badging along with an option on the V6 model to have a rear door without the externally mounted spare tire (run-flat tires are used on this model). New features/options include turn signals integrated into the side <b>mirrors,</b> <b>backup</b> camera (with monitor built into rear-view mirror), satellite navigation, smart keyless entry, a push button starter, a multi-function instrument cluster display, etc. Much of the interior remains the same. In 2009, {{it was also the}} first time for the Canadian market to have a 2WD model sold in Canada.|$|R
5000|$|A {{recovery}} point objective, or “RPO”, {{is defined by}} business continuity planning. It is the maximum targeted period in which data might be lost from an IT service due to a major incident. [...] The RPO gives systems designers a limit to work to. For instance, if the RPO is set to four hours, then in practice, off-site <b>mirrored</b> <b>backups</b> must be continuously maintained - a daily off-site backup on tape will not suffice. Care {{must be taken to}} avoid two common mistakes around the use and definition of RPO. Firstly, business continuity staff use business impact analysis to determine RPO for each service - RPO is not determined by the existent backup regime. Secondly, when any level of preparation of off-site data is required, rather than at the time the backups are offsited, the period during which data is lost very often starts near the time of the beginning of the work to prepare backups which are eventually offsited.|$|R
5000|$|... rdiff-backup {{maintains}} a <b>backup</b> <b>mirror</b> of a file or directory either locally or remotely over the network, on another server. rdiff-backup stores incremental rdiff deltas with the backup, {{with which it}} is possible to recreate any backup point.|$|R
5000|$|The BA/IA {{partnership}} is built with the aims to preserve heritage {{for future generations}} and to provide universal access to human knowledge. The BA maintains the only <b>mirror</b> and external <b>backup</b> of the Internet Archive. The Internet Archive donated five million USD to the BA, including: ...|$|R
5000|$|There is no native {{operating}} system {{support for the}} Cloud Files API {{so it is not}} yet possible to [...] "map" [...] or [...] "mount" [...] it as a virtual drive without third-party software like JungleDisk that translates to a supported standard such as WebDAV. There are no concepts of [...] "appending" [...] or [...] "locking" [...] data within Cloud Files (which may affect some disk <b>mirroring</b> or <b>backup</b> solutions), nor support for permissions or transcoding. Data is organised into [...] "containers" [...] but {{it is not possible to}} create nested folders without a translation layer.|$|R
40|$|Abstract—Data {{outsourcing}} offers cost-effective {{computing power}} to manage massive data streams and reliable access to data. For example, data owners can forward their data to clouds, {{and the clouds}} provide data <b>mirroring,</b> <b>backup,</b> and online access services to end users. However, outsourcing data to untrusted clouds requires data authentication and query integrity {{to remain in the}} control of the data owners and users. In this paper, we address this problem specifically for multi-version key-value data that is subject to continuous updates under the constraints of data integrity, data authenticity, and “freshness ” (i. e., ensuring that the value returned for a key is the latest version). We detail this problem and propose INCBM-TREE, a novel construct delivering freshness and authenticity. Compared to existing work, we provide a solution that offers (i) lightweight signing and verification on massive data update streams for data owners and users (e. g., allowing for small memory footprint and CPU usage on mobile user devices), (ii) integrity of both real-time and historic data, and (iii) support for both real-time and periodic data publication. Extensive benchmark evaluations demonstrate that INCBM-TREE achieves more throughput (in an order of magnitude) for data stream authentication than existing work. For data owners and end users that have limited computing power, INCBM-TREE can be a practical solution to authenticate the freshness of outsourced data while reaping the benefits of broadly available cloud services. I...|$|R
500|$|On October 10, 2008, Cyrus, {{dressed as}} Montana, premiered [...] "Supergirl", along with eight other songs, at the concert taping {{for the third}} season of Hannah Montana, which was set on October 10 in Irvine, California at the Verizon Wireless Amphitheatre. The {{performance}} began with Montana, {{dressed in a white}} and pink tie-dye tee shirt, white pants, pink leather jacket, and large sunglasses. The performance begins with Cyrus sitting in a bench, facing a <b>mirror,</b> as <b>backup</b> dancers apply makeup on her and attend her. She stands up and roams around the stage to sing afterward. Peter Larsen of The Orange County Register called it one of [...] "hits of the night". The performance was later premiered as the song's music video on July 2, 2009, on Disney Channel to promote Hannah Montana 3.|$|R
40|$|This paper {{addresses}} the major challenges that large organizations face in protecting their valuable data. Some of these challenges include recovery objectives, data explosion, cost {{and the nature}} of data. The paper explores multiple methods of data protection at different storage levels. RAID disk arrays, snapshot technology, storage <b>mirroring,</b> and <b>backup</b> and archive strategies all are methods used by many large organizations to protect their data. The paper surveys several different enterprise-level backup and archive solutions in the market today and evaluates each solution based on certain criteria. The evaluation criteria cover all business needs and help to tackle the key issues related to data protection. Finally, this paper provides insight on data protection mechanisms and proposes guidelines that help organizations to choose the best backup and archive solutions...|$|R
5000|$|On October 10, 2008, Cyrus, {{dressed as}} Montana, premiered [...] "Supergirl", along with eight other songs, at the concert taping {{for the third}} season of Hannah Montana, which was set on October 10 in Irvine, California at the Verizon Wireless Amphitheatre. The {{performance}} began with Montana, {{dressed in a white}} and pink tie-dye tee shirt, white pants, pink leather jacket, and large sunglasses. The performance begins with Cyrus sitting in a bench, facing a <b>mirror,</b> as <b>backup</b> dancers apply makeup on her and attend her. She stands up and roams around the stage to sing afterward. Peter Larsen of The Orange County Register called it one of [...] "hits of the night". The performance was later premiered as the song's music video on July 2, 2009, on Disney Channel to promote Hannah Montana 3.|$|R
50|$|The museum {{received}} COSTAR, the corrective optics instrument {{installed in}} the Hubble Space Telescope during its first servicing mission (STS-61), when it was removed and returned to Earth after Space Shuttle mission STS-125. The museum also holds the <b>backup</b> <b>mirror</b> for the Hubble which, unlike {{the one that was}} launched, was ground to the correct shape. There were once plans for it to be installed to the Hubble itself, but plans to return the satellite to Earth were scrapped after the Space Shuttle Columbia disaster in 2003; the mission was re-considered as too risky.|$|R
40|$|Abstract. A {{tape library}} is seldom {{considered}} as a viable place for constructing a file system for a sequential write/read device. Storage virtualization technology has become a buzzword in technology circles lately, {{in this paper we}} propose a tape library file system, called TLFS. The purpose of TLFS is to maintain a consistent view of mass storage so that the user can effectively manage it. Like disk file system, TLFS provides some file system operations, such as create, delete, open, close, read and write files/directories. It supports remote data access, <b>backup,</b> <b>mirroring,</b> replication by iSCSI protocol and facilitates fast data backup and archive. Moreover, TLFS supports large-scale storage management, provides file system fragment management, defenses virus and has transparent file movement. The prototype system, which is built by a SCSI disk and a tape library, is present and some implementation details are shown. Also, the experiment results are analyzed. ...|$|R
40|$|The {{costs of}} data loss and {{unavailability}} can be large, so businesses use many data protection techniques, such as re-mote <b>mirroring,</b> snapshots and <b>backups,</b> {{to guard against}} failures. Choosing an appropriate combination of tech-niques is difficult because there are numerous approaches for protecting data and allocating resources. Storage system designers typically use ad hoc techniques, often resulting in over-engineered, expensive solutions or under-provisioned, inadequate ones. In contrast, this paper presents a princi-pled, automated approach for designing dependable storage solutions for multiple applications in shared environments. Our contributions include search heuristics for intelligently exploring the large design space and modeling techniques for capturing interactions between applications during re-covery. Using realistic storage system requirements, we show that our design tool can produce designs that cost up to 3 X less in initial outlays and expected data penalties than the designs produced by an emulated human design process. ...|$|R
40|$|<b>Mirroring,</b> data replication, <b>backup,</b> {{and more}} recently, {{redundant}} arrays of independent disks (RAID) are all technologies {{used to protect}} and ensure access to critical company data. A new set of problems has arisen as data {{becomes more and more}} geographically distributed. Each of the technologies listed above provides important benefits; but each has failed to adapt fully to the realities of distributed computing. The key to data high availability and protection is to take the technologies' strengths and 'virtualize' them across a distributed network. RAID and mirroring offer high data availability, which data replication and backup provide strong data protection. If we take these concepts at a very granular level (defining user, record, block, file, or directory types) and them liberate them from the physical subsystems with which they have traditionally been associated, we have the opportunity to create a highly scalable network wide storage fault tolerance. The network becomes the virtual storage space in which the traditional concepts of data high availability and protection are implemented without their corresponding physical constraints...|$|R
40|$|Subversion, an {{open-source}} centralized {{version control}} system, developed by CoallabNet, {{is currently the}} second most popular version control system, after the ever popular CVS. Like CVS, Subversion uses a client-server architecture, but has a cleaner, modular architecture. One set of subversion modules, are the filesystem backends modules of subversion. Two "official" backends are currently supplied with subversion, a berkleyDB based backend(bdb), and a custom filebased filesystem implementation (fsfs). At least another un-official backend module using an SQL-database exists. Pesto is a secure, decentralized, distributed peer-to-peer storage system, implemented both as a NetBSD filesystem, and as middelware, a portable C-library (libpesto). Currently two applications using libpesto have been written, a C#. Net windows client, and a highly scalable serverfarm for Pesto, written in Java. In this project we integrate subversion with pesto, by creating a new filesystem backend for subversion using libpesto. The result is a version-control system, that works like a centralized version-control system, but has decentralized storage. We show that this system {{can be used for}} <b>backup,</b> <b>mirroring</b> of repositories, and as a decentralized version-control system...|$|R
40|$|Abstract—The {{costs of}} data loss and {{unavailability}} can be large, so businesses use many data protection {{techniques such as}} remote <b>mirroring,</b> snapshots, and <b>backups</b> to guard against failures. Choosing an appropriate combination of techniques is difficult because there are numerous approaches for protecting data and allocating resources. Storage system architects typically use ad hoc techniques, often resulting in overengineered expensive solutions or underprovisioned inadequate ones. In contrast, this paper presents a principled automated approach for designing dependable storage solutions for multiple applications in shared environments. Our contributions include search heuristics for intelligent exploration of the large design space and modeling techniques for capturing interactions between applications during recovery. Using realistic storage system requirements, we show that our design tool produces designs that cost up to two times less in initial outlays and expected data penalties than the designs produced by an emulated human design process. Additionally, we compare our design tool to a random search heuristic and a genetic algorithm metaheuristic, and show that our approach consistently produces better designs for the cases we have studied. Finally, we study the sensitivity of our design tool to several input parameters. Index Terms—Data protection techniques, design space exploration, discrete-event simulation, genetic algorithm, search heuristic, storage system design. Ç...|$|R
40|$|In {{this paper}} we analyze {{alternatives}} to centralized storage for {{a network of}} client workstations. Centralized storage can be expensive and unreliable. A {{single point of failure}} can halt the operation of a network of computers. Most of the client workstations distributed around our campus have excess storage on the order of 10 - 20 Gigabytes and the data that they to manage and maintain is redundant with other systems within the same building. If user home directories are located on storage other than local media, up to 98 % of the local storage is a duplicate copy of storage on an adjacent system. By using a hash index, we are proposing that this data can be shared and mirrored across systems. This <b>mirroring</b> provides a <b>backup</b> and restore mechanism by redundantly indexing data across multiple systems. This replication mechanism can then be extended to include user home directories {{to take advantage of the}} 20 Terabytes of unused storage distributed across our campus resulting in hundreds of thousands of dollar savings while increasing reliability and availability of user data. If the central server fails or does not respond to a client request, the network of workstations can act as a failover agent and respond to the user request for data. ...|$|R
40|$|The remote file {{synchronization}} {{problem is}} how to update an outdated version of a file located on one machine to the current version located on another machine with a minimal amount of network communication. It arises in many scenarios including web site <b>mirroring,</b> file system <b>backup</b> and replication, or web access over slow links. A widely used open-source tool called rsync uses a single round of messages between the two machines {{to solve this problem}} (plus an initial round for exchanging meta information). While research has shown that significant additional savings in bandwidth are possible by using multiple rounds, such approaches are often not desirable due to network latencies, increased protocol complexity, or other overheads at the endpoints. In this paper, we study single-round synchronization techniques that offer significant benefits in bandwidth consumption while preserving many of the advantages of the rsync approach. In particular, we propose a new and simple algorithm for file synchronization based on set reconciliation techniques. We then show how to integrate sampling techniques into our approach in order to adaptively select the most suitable algorithm and parameter setting for a given data set. Experimental results on several benchmark data sets show that the resulting protocol gives significant benefits over rsync, particularly on data sets with high degrees of redundancy between versions. ...|$|R
40|$|Abstract — Given two {{versions}} of a file, a current version located on one machine and an outdated version known only to another machine, the remote file synchronization problem is how to update the outdated version over a network with a minimal amount of communication. In particular, when the versions are very similar, the total data transmitted should be significantly smaller than the file size. File synchronization problems arise in many application scenarios such as web site <b>mirroring,</b> file system <b>backup</b> and replication, and web access over slow links. An open source tool for this problem, called rsync and included in many Linux distributions, is widely used in such scenarios. rsync uses a single round of messages between the two machines. While recent {{research has shown that}} significant additional savings in bandwidth consumption are possible through the use of optimized multi-round protocols, there are many scenarios where multiple rounds are undesirable. In this paper, we study single-round protocols for file synchronization that offer significant improvements over rsync. Our main contribution is a new approach to file synchronization based on the use of erasure codes. Using this approach, we design a single-round protocol that is provably efficient with respect to common measures of file distance, and another optimized practical protocol that shows promising improvements over rsync on our data sets. In addition, we show how to obtain moderate improvements by engineering the rsync approach. I...|$|R
40|$|Abstract—Given two {{versions}} of a file, a current version located on one machine and an outdated version known only to another machine, the remote file synchronization prob-lem is how to update the outdated version over a network with a minimal amount of communication. In particular, when the versions are very similar, the total data transmit-ted should be significantly smaller than the file size. File synchronization problems arise in many application scenar-ios such as web site <b>mirroring,</b> file system <b>backup</b> and repli-cation, and web access over slow links. An open source tool for this problem, called rsync and included in many Linux distributions, is widely used in such scenarios. rsync uses a single round of messages between the two machines. While recent {{research has shown that}} significant additional sav-ings in bandwidth consumption are possible through the use of optimized multi-round protocols, there are many scenar-ios where multiple rounds are undesirable. In this paper, we study single-round protocols for file synchronization that offer significant improvements over rsync. Our main contribution is a new approach to file syn-chronization based on the use of erasure codes. Using this approach, we design a single-round protocol that is prov-ably efficient with respect to common measures of file dis-tance, and another optimized practical protocol that shows promising improvements over rsync on our data sets. In addition, we show how to obtain some improvements by en-gineering the rsync approach, and study the use of recent set reconciliation techniques in file synchronization protocols. I...|$|R
40|$|The National e-Governance Plan (NeGP) proposes citizen service {{delivery}} {{up to the}} village level through various channels including village kiosks. The citizen services to be delivered {{are going to be}} web services (as against the present simply web enabled services) based on the Service Oriented Architecture paradigm. These Web Services expect adequate networking and computing resources for effective and efficient {{service delivery}}. Grid computing is the new computing paradigm. According to Gartner, computing (scientific, business and e-governance based) will be completely transformed in this decade by using grid enabled web services to integrate across the Internet to share not only information and application but also computing power. The latest grid computing standard OGSA (Open Grid Services Architecture) integrates the power of the grid with that of the web services – both stateless and stateful, based on Service Oriented Architecture (SOA). Leveraging the power of grid computing for e-governance takes us towards an e-governance grid for India. Towards this objective, the existing computing networks such as NICNET with all its SAN Data Centres connected with each other and also the other state owned Data Centres and SWANs (State Wide Area Networks) are required to be connected with each other to ultimately form the e-Governance Grid of India (e-GGI). Once this is achieved, the web services which offer citizen services will be effectively supported by the powerful resources of this e-ggI, ensuring nonstop, fast and efficient delivery, with all the due <b>backup,</b> <b>mirroring</b> and recovery features in place. Then we can successfully operationalise Web Services Repositories at the District, State and National levels on the e-governance grid of India, thereby delivering citizen services across the country. An architectural framework for citizen services delivery is also proposed based on e-GGI...|$|R

