5|26|Public
40|$|In this paper, an {{emulation}} {{environment for}} approximate memory architectures is presented. In {{the context of}} error tolerant applications, in which energy is saved {{at the expense of}} the occurrence of errors in data processing, approximate memories play a relevant part. Approximate memories are memories where read/write errors are allowed with controlled probability. In general these errors are the result of circuital or architectural techniques (i. e. voltage scaling, refresh rate reduction) introduced to save energy. The ability to simulate these systems is particularly important since the amount of tolerated error is application dependent. Simulation allows to analyze the behavior of an application and explore its tolerance to actual error rates, determining the trade-off between saved energy and output quality. We have developed an emulation environment for such architectures, based on QEmu, which allows the execution of programs that can allocate some of their data in a <b>memory</b> <b>zone</b> subject to faults. We present the emulated architecture, the fault injection model and a case of study showing results that can be obtained by our emulator...|$|E
40|$|We {{propose a}} memory {{abstraction}} able to lift existing numerical static analyses to C programs containing union types, pointer casts, and arbitrary pointer arithmetics. Our framework {{is that of}} a combined points-to and data-value analysis. We abstract the contents of compound variables in a field-sensitive way, whether these fields contain numeric or pointer values, and use stock numerical abstract domains to find an overapproximation of all possible memory states— with the ability to discover relationships between variables. A main novelty of our approach is the dynamic mapping scheme we use to associate a flat collection of abstract cells of scalar type to the set of accessed memory locations, while taking care of byte-level aliases—i. e., C variables with incompatible types allocated in overlapping memory locations. We do not rely on static type information which can be misleading in C programs as it does not account for all the uses a <b>memory</b> <b>zone</b> may be put to. Our work was incorporated within the Astrée static analyzer that checks for the absence of run-time-errors in embedded, safety-critical, numerical-intensive software. It replaces the former memory domain limited to well-typed, union-free, pointer-cast free data-structures. Early results demonstrate that this abstraction allows analyzing a larger class of C programs, without much cost overhead...|$|E
40|$|A long-time quantum memory {{capable of}} storing and {{measuring}} quantum {{information at the}} single-qubit level is an essential ingredient for practical quantum computation and com-munication. Recently, there have been remarkable progresses of increasing coherence time for ensemble-based quantum memories of trapped ions, nuclear spins of ionized donors or nuclear spins in a solid. Until now, however, the record of coherence time of a single qubit is {{on the order of}} a few tens of seconds demonstrated in trapped ion systems. The qubit coherence time in a trapped ion is mainly limited by the increasing magnetic field fluctuation and the decreasing state-detection efficiency associated with the motional heating of the ion without laser cooling. Here we report the coherence time of a single qubit over 10 minutes in the hyperfine states of a ion sympathetically cooled by a ion in the same Paul trap, which eliminates the heating of the qubit ion even at room temperature. To reach such coherence time, we apply a few thousands of dynamical decoupling pulses to suppress the field fluctuation noise. A long-time quantum memory demonstrated in this experiment makes an important step for construction of the <b>memory</b> <b>zone</b> in scalable quantum computer architectures or for ion-trap-based quantum networks. With further improvement of the coherence time by techniques such as magnetic field shielding and increase of the number of qubits in the quantum memory, our demonstration also makes a basis for other applications including quantum money. Comment: 6 pages, 4 figure...|$|E
5000|$|... 2012 {{included}} seven haunts, two new; two new scare {{zones and}} one show. The Faded <b>Memories</b> <b>zone</b> was a retrospective {{of the last}} 13 years of Howl-O-Scream.|$|R
40|$|AbsnactProhlems occur lor users {{when they}} attempt to {{navigate}} or purchase items from a virtual shopping environment that has to correlate in the physical world. These problems arise in part, hecause online shoppers {{do not have the}} benefit of external memory provided by the physical world. The <b>memory</b> <b>zones</b> idea attempts to solve these problems by providing an online parallel to this external memory. The system reported uses a profile of previous purchases and a representation of the physical environment of the actual shop to recommend Items to shoppers, that they may have otherwise forgotten {{because of the lack of}} suitable external memor...|$|R
40|$|The {{soundness}} of device drivers generally cannot be verified in isolation, but {{has to take}} into account the reactions of the hardware devices. In critical embedded systems, interfaces often were simple “volatile ” variables, and the interface specification typically a list of bounds on these variables. Some newer systems use “intelligent ” controllers that handle dynamic worklists in shared memory and perform direct memory accesses, all asynchronously from the main processor. Thus, it is impossible to truly verify the device driver without taking the intelligent device into account, because incorrect programming of the device can lead to dire consequences, such as <b>memory</b> <b>zones</b> being erased. We have successfully verified a device driver extracted from a critical industrial system, asynchronously combined with a model for a USB OHCI controller. This paper studies this case, as well as introduces a model and analysis techniques for this asynchronous composition...|$|R
40|$|Can we, today, {{address the}} citizens' {{healthcare}} without {{talking about his}} territory ? This seems unbelievable, and the French government has already understood this and to {{answer to this question}} has designed and launched the "ARS", which stand for "Healthcare Regional Agency", in order to define and manage healthcare strategy at its own territory level (region). But, before arriving to this conclusion, the healthcare system had to reform itself along four centuries moving from healthcare concept to healthcare system concept. This has been conducted through several reforms from decision makers that had to use quantifiable elements to perform so. Therefore, some specific indicators, called "health deterministic indicators", have been put in place. Then, the sum of all these along the years bring the French government to launch the ARS which are defined within the law called "HPST" standing for Hospital Patient Healthcare and Territory). Now, if we are looking closer to the ARS mission, it clearly appears that they are closely linked to the notion of "sustainable performance" for which it is important to define underneath concepts as "Performance", "sustainable performance" and of course "healthcare sustainable performance". Then, to applied those healthcare sustainable performance principles, the ARS would have to learn how this performance could be increased or decreased based on some specific mechanisms. Therefore, the notions of "risk" and "vulnerability" became key components of such an approach. However, looking forward it appears that the vulnerability has a close link to risk and that the risk is essentially linked to a mismatch between the "offering" and the "demand" or the "needs" of the citizens. Then, by integrating all those aspects, the ARS would became a healthcare sustainable performance for a territory vector, thanks to new governances, a better risk management politic and the usage of performance measures as healthcare expenses for a territory, regulation tool. Based on this it is easy to understand that the ARS will be {{at the heart of the}} reform and will have to drive the regional health system. But, and it is true for all sectors (private and public), the management of such a system cannot be done without an accurate information system. Then, the next question is "what is an information system for a health system?" but also, "how to feed this information system in order to let it provide relevant information?" Therefore, to understand this, it is necessary to understand the definition of all existing information system types and their role in the healthcare environment and how the citizen and the patient will take benefits of that. Based on this theory, it is now interesting to better understand the status of the current health information system and why he has failed in his pupil service missions, and that in order to better understand how, through a new ARS governance, it will be possible to have health information system able to reach the sustainable performance goals. The failure notification and the willingness to get the French healthcare system out of this trap is real. However, it is important to notice that to succeed in the mission regarding the information system, it will be necessary to adopt an accurate methodology. But, it is also important to understand that after investigation, no such methodology has been found and it appears necessary to build one in order to satisfy the needs of health professionals, citizens, patients. The final goal being to allows the French health system to reach the health sustainable performance goals by the understanding and the resolution of business pains coming from the citizen requests known and understood at the right time and the right location. It is easy to understand that cannot be done without a better understanding of the human, geographical, politic, social and health environment that could be better handled through new technologies. Therefore, it will be possible to put in place certain of these technologies that will enable this approach by allowing a global access to data or simplifying application integration into complex business processes or providing analysis and permanent traceability tools allowing to take "preventive" decisions (real time) or "corrective" decisions (after past facts analysis). Everything, of course, being at maximum secured and with maximum integrity guarantee. This thought has bring to the definition of MAEVA methodology which seat in the middle of a particular context composed by actors (health professionals, citizens, politicians, patients, etc.), stimulus (known or unknown meaning handled or not), the health system itself (on which the methodology is applied) and results (benefits for the health system increasing the sustainable performance of the health system). To do so, I has been necessary to build the method in two "layers". The first layer is made of components called "fundamentals" which permit to define the project foundations that has to be implemented by following MAEVA rules. These fundamentals, counted as four (plus one) allow the definition of a global consensus for the project and for the associated community of practice in order to successfully deliver the related project. The "fifth" fundamental allows the rationale to pursue on a new version or to stop the project in its current stage if the necessary condition to continue it are not met. Once these basis have been setup, the method offer the capability to define five actions which will allow to manage the project from the beginning until the final delivery. These actions are: "Integration", allowing the integration of the data sources needed to the project implementation; "Detection", allowing the definition of "actuators" components bringing to a risky situation; "Anticipation", allowing the definition of self defense mechanisms against the previously mentioned "actuators"; "Action", allowing the realization of the project mission; and at the end, "Evaluation", offering factual elements to analyze project outcomes and provide therefore facts to the fundamental "Decide". All these components are linked within the methodology through two "tools": Iteration, allowing to complete a phases with results coming from an upfront phase; and the Active <b>Memory</b> <b>Zone</b> (ZAM) which is used to be the project memory in order to store traceability data, but also "keep in mind" all taken decisions. This methodology, as defined, hasn't been designed theoretically in few months, but has been the result of real life projects analyze done during the past five years in the area of health system performance. It is also good to know that the MAEVA approach based on consensus will be a great help for the ARS, as they will have, since the beginning, work with people, processes and information coming from diverse horizons that were, until now, more in competition rather than in collaboration mood. But all of this is just about the first version of the methodology and already, due to read publications, it appears that some research works, like "Design Thinking" should be integrated partially or totally in a future "release" of MAEVA. Est-il possible, de nos jours, de parler de santé des citoyens sans faire référence au territoire qu'ils occupent ? Ceci est fort peu probable, et le gouvernement français en a pris conscience et pour cela a créé et mis en place les " Agences Régionales de Santé " (ARS) permettant de définir et gérer la stratégie de la santé sur son territoire régional. Mais avant d'en arriver là, le système de santé a dû se réformer, pendant près de quatre siècles, afin d'évoluer du concept de santé au concept de système de santé. De plus, pour mener à bien ces réformes, il a fallut que les différents organismes décideurs puissent s'appuyer sur des éléments concrets, et, de fait, des indicateurs particuliers, appelés " déterminants de santé " ont été mis en place. Ainsi, tout ceci a amené le gouvernement français à créer les ARS organisés autour d'un territoire de santé et dont le cadre légal est défini à travers la loi HPST (Hôpital Patient Santé Territoire). Dès lors, en s'intéressant de plus près à la mission des ARS, il apparait qu'elle est clairement liée à la notion de performance durable, d'où l'importance de définir les concepts sous jacents à savoir : " Performance ", " Performance durable " et bien sur " Performance durable en santé ". Ainsi, pour pouvoir appliquer des principes de performance durable en santé, les ARS vont devoir s'appliquer à comprendre les mécanismes favorisant ou pénalisant une telle performance. Dès lors, les notions de " risque " et de " vulnérabilité " sont devenus des éléments clés d'une telle approche. De fait, en regardant, une fois de plus, d'un peu plus près, il apparait que la vulnérabilité est liée au risque, et que le risque est essentiellement lié à une inadéquation, ou déséquilibre, entre " l'offre " la " demande " et les " besoins " du citoyen. De fait, en intégrant ces éléments, les ARS vont devenir un véritable vecteur de performance durable en santé au niveau d'un territoire, grâce à un nouveau pilotage, à une meilleure gestion des risques et l'utilisation de la mesure de performance comme d'un outil de régulation des dépenses de santé sur le dit territoire. De part ces propos, on comprend bien que les ARS vont être au cœur de la réforme et du pilotage du système de santé en région. Or, et cela est vrai dans tous les secteurs (aussi bien privés que publics), la gestion d'un tel système ne peux s'effectuer sans un système d'information adapté. Ainsi va se poser la question de ce qu'est un système d'information pour un système de santé, mais aussi comment faire en sorte que ce système d'information consomme et produise des informations de qualité. De fait, et pour mieux comprendre tout cela, il est nécessaire d'appréhender la définition de tous les types de systèmes d'information existants, mais aussi leurs rôles dans le domaine de la santé et comment cela va bénéficier au citoyen et patient. Fort de cette approche théorique, il est maintenant intéressant de s'attarder sur l'état du système d'information en santé, et de comprendre pourquoi, en l'état actuel, il a échoué dans sa mission de service public, et ce afin de mieux comprendre comment à travers la nouvelle gouvernance des ARS, il sera possible d'avoir un système d'information en santé permettant d'atteindre des objectifs de performance durable. Le constat d'échec, ainsi que la volonté de sortir le système de santé français de l'ornière dans laquelle elle s'est enfoncée est réel. Cependant, force est de constater que pour mener à bien cette mission, et notamment en regard des systèmes d'information, il sera nécessaire d'utiliser une méthodologie adaptée. Or, après investigation, il apparait qu'une telle méthode ou approche n'existe pas, d'où la nécessité d'en définir une nouvelle, afin de satisfaire les besoins des professionnels de la santé, des citoyens et patients, et ce pour permettre au système de santé français d'atteindre des objectifs de performance durable en santé par la compréhension et la résolution de problèmes métiers issue de la demande, connue et comprise en temps et en lieu, des citoyens. Il est clair que ceci ne peut se réaliser que par une meilleure compréhension de l'environnement humain, géographique, politique, social et médical qui peut mieux être appréhendé au travers des nouvelles technologies. Ainsi, il sera possible de mettre en œuvre un certain nombre de ces technologies facilitant cette approche, soit en garantissant un accès global aux données, soit en simplifiant l'intégration d'applicatifs au sein d'un processus métier complexe, soit en fournissant des outils d'analyse et d'audit permettant des prises de décision " préventives " (temps réel) ou " correctives " (après analyse des faits passés). Le tout, évidemment, en garantissant un niveau de sécurité et d'intégrité maximum. Ainsi, cette réflexion a amené à la définition de la méthode MAEVA qui se situe au milieu d'un contexte bien particulier, composé d'acteurs (professionnels, citoyens, politiques, patients, etc.), de perturbants (connus ou inconnus, donc maitrisés ou non), du système de santé en soit (sur lequel la méthode s'applique) et des résultantes (résultat de la méthode appliquée sur l'accroissement de la performance du système de santé). Pour se faire, il a été nécessaire de bâtir la méthode en deux " couches ". La première couche, composée d'éléments appelés " fondamentaux ", permet de définir les fondations du projet à implémenter selon MAEVA. Ces fondamentaux, au nombre de quatre (plus un), permettent de définir un consensus global, pour le projet et pour la communauté de pratique associée afin de mener à bien ce projet. Le " cinquième " fondamental permettant lui de définir les éléments de poursuite vers une nouvelle version ou d'arrêt du projet si les conditions nécessaires ne sont pas atteintes pour reconduire une nouvelle version du projet. Une fois ces fondements posés, la méthode offre la possibilité de définir cinq actions qui vont permettre de gérer le projet du début à la fin. Il s'agit de : " l'Intégration ", qui permet d'intégrer les sources de données nécessaires à la réalisation du projet; la " Détection ", qui permet de définir les éléments " déclencheurs " amenant à des situations à risque; " l'Anticipation ", qui permet de définir des mécanismes d'auto défense vis-à-vis des " déclencheurs "; " l'Action ", qui permet de réaliser la mission pour laquelle le projet a été défini, et enfin " l'Evaluation " qui va offrir les éléments pour analyser le projet et de fait fournir des éléments factuels au fondamental " Décider ". Tout ces éléments, étant relié au sein de la méthode à travers deux " outils " : l'itération, qui permet de compléter une phase amont avec des éléments issus d'une phase aval; et la Zone Active de Mémorisation (ZAM) qui sert de mémoire au projet et permet non seulement d'entreposer des données de traçabilité, mais aussi de garder la mémoire des décisions prises. Cette méthode ainsi définie n'a pas été établie de façon théorique en quelques mois, mais a été issu de l'analyse d'expériences vécues au fil de cinq ans de labeur dans le domaine de la performance du système de santé. Il est à noter que l'approche consensuelle proposée par MAEVA sera d'une grande utilité pour les ARS qui vont devoir, dès le début de leur existence, travailler avec du personnel, des processus et des informations issus de divers horizons jusqu'à présent plutôt compétitifs que collaboratifs. Mais tout ceci ne constitue que la première version de la méthode, et déjà, compte tenu des publications lues ces derniers temps, il est apparut que certains travaux de recherche tels que le " Design Thinking " devraient pouvoir être intégrés partiellement ou totalement dans une prochaine " release " de MAEVA...|$|E
40|$|ACM, 2007. This is the author's {{version of}} the work. It is posted here by {{permission}} of ACM for your personal use. Not for redistribution. The definitive version was published in EMSOFT 2007. International audienceThe soundness of device drivers generally cannot be verified in isolation, but has {{to take into account}} the reactions of the hardware devices. In critical embedded systems, interfaces often were simple "volatile" variables, and the interface specification typically a list of bounds on these variables. Some newer systems use "intelligent" controllers that handle dynamic worklists in shared memory and perform direct memory accesses, all asynchronously from the main processor. Thus, it is impossible to truly verify the device driver without taking the intelligent device into account, because incorrect programming of the device can lead to dire consequences, such as <b>memory</b> <b>zones</b> being erased. We have successfully verified a device driver extracted from a critical industrial system, asynchronously combined with a model for a USB OHCI controller. This paper studies this case, as well as introduces a model and analysis techniques for this asynchronous composition...|$|R
40|$|International audienceIn data {{security}} systems, general purpose processors (GPPs) are often extended by a cryptographic accelerator. The paper presents three ways of extending GPPs for {{symmetric key cryptography}} applications. Proposed extensions guarantee secure key storage and management even if the system is facing protocol, software and cache memory attacks. The system is partitioned into processor, cipher, and key <b>memory</b> <b>zones.</b> The three security zones are separated at protocol, system, architecture and physical levels. The proposed principle was validated on Altera NIOS II, Xilinx MicroBlaze and Microsemi Cortex M 1 soft core processor extensions. We show that stringent separation of the cipher zone is helpful for partial reconfiguration of the security module, if the enciphering algorithm needs to be dynamically changed. However, the key zone including reconfiguration controller must remain static {{in order to maintain}} the high level of security required. We demonstrate that the principle is feasible in partially reconfigurable field programmable gate arrays (FPGAs) such as Altera Stratix V or Xilinx Virtex 6 and also to some extent in FPGAs featuring hardwired general purpose processors such as Cortex M 3 in Microsemi SmartFusion FPGA. Although the three GPPs feature different data interfaces, we show that the processors with their extensions reach the required high security level while maintaining partial reconfiguration capability...|$|R
40|$|When Charge Coupled Devices {{are used}} for {{scientific}} observations, their dark signal is a hindrance. In their pristine state, most CCD pixels are `cool'; they exhibit low, quasi uniform dark current, which can be estimated and corrected for. In space, after having been hit by an energetic particle, pixels can turn `hot'. They start delivering excessive, less predictable, dark current. The hot pixels need therefore to be flagged so that subsequent analysis may ignore them. The image data of the PICARD SODISM solar telescope (Meftah et al. 2013) require dark signal correction and hot pixel identification. Its frame transfer E 2 V 42 - 80 CCD operates at - 7 C. Both image and <b>memory</b> <b>zones</b> thus accumulate dark current during, respectively, integration and readout time. These two components must be separated to estimate the dark signal for any observation. This {{is the purpose of}} the Dark Signal Model presented in this paper. The dark signal time series of every pixel is processed by the Unbalanced Haar Technique (Fryzlewicz 2007) in order to timestamp when its dark signal is expected to change. In-between those instants, both components are assumed constant and a robust linear regression vs. integration time provides first estimates and a quality coefficient. The latter serves to assign definitive estimates. Our model is part of the SODISM Level 1 data production scheme. To check its reliability, we verify on dark frames that it leaves a negligible residual bias (5 e-), and generates a small RMS error (25 e- rms). The cool pixel level is found to be 4 e-/pxl/s, in agreement with the predicted value. The emergence rate of hot pixels is investigated too. It legitimates a threshold criterion at 50 e-/pxl/s. The growth rate is found to be 4...|$|R
40|$|We {{analyze the}} {{performance}} of several copying garbage collection algorithms in a large address space offered by modern architectures. In particular, we describe the design {{and implementation of the}} RealOF garbage collector, an algorithm explicitly designed to exploit the features of 64 -bit environments. This collector maintains a correspondence between object age and object placement in the address space of the heap. It allocates and copies objects within designated regions of <b>memory</b> called <b>zones</b> and performs garbage collection incrementally by collecting one or more ranges of memory called windows. The address-ordered heap allows us to use the same inexpensive write barrier that works for traditional generational collectors. We show that for server applications this algorithm improves throughput and reduces heap size requirements over the best-throughput generational copying algorithms such as the Appel-style generational collector. ...|$|R
30|$|Fabrication of {{nanoscale}} {{structures and}} {{devices such as}} nanoimprint lithography templates, dynamic random-access <b>memory</b> capacitors, <b>zone</b> plates (X-ray lenses), etc. requires a high-aspect-ratio (AR) and high-resolution patterning capability. Utilizing electron beam lithography (EBL) to fabricate such nanostructures further requires that the patterning be performed {{as rapidly as possible}} (high throughput) due to the serial writing nature of EBL. The requirement of high throughput often imposes a trade-off between the selection of processing conditions and performance. As an example, using a higher voltage in EBL enables the fabrication of higher AR nanostructures; however, the electron dose increases in proportion to the voltage, thus increasing the time of exposure. Careful selection of other processing parameters such as using a higher performance developer solution can decrease the electron dose requirement (increase the process sensitivity) and, to a certain extent, compensate for such trade-offs.|$|R
40|$|It is {{difficult}} to achieve high performance while programming in the large. In particular, maintaining locality hinders portability and modularity. Existing methodologies are not sufficient: explicit communication and coding for locality require the programmer to vio-late encapsulation and compositionality of software modules, while automated compiler analysis remains unreliable. This thesis presents a performance model that makes thread and object locality explicit. Zones form a runtime hierarchy that reflects the intended clustering of threads and objects, which are dynamically mapped onto hardware units such as processor clusters, pages, or cache lines. This conceptual indirection allows programmers to reason in the abstract about locality without committing to the hardware of a specific <b>memory</b> system. <b>Zones</b> comple-ment conventional coding for locality and may be added to existing code to improve per-formance without affecting correctness. The integration of zones into the Sather language is described, including an implementa-tion of memory management customized to parameters of the memory system...|$|R
5000|$|The Legacy {{is offered}} as a sedan with black interior; {{specifications}} are similar to European [...] "Comfort" [...] trim package. The Legacy 2.0i Premium comes with {{the choice of a}} six-speed manual transmission or Lineartronic CVT, leather trim, power driver’s front seat with <b>memory</b> function, dual <b>zone</b> climate control, electric sunroof and 16 inch alloy wheels. The Legacy 2.5i Sport Premium CVT adds dual front electric seats, carbon-fibre dash inserts, alloy pedals, sports front bumper and grille; Bilstein suspension, 18-inch alloy wheels and Xenon headlights with headlight washers to the Premium package. Satellite navigation is not offered along with the premium Harmon/Kardon sound system.|$|R
40|$|Cities are {{spaces of}} <b>memory</b> with several <b>zones</b> (parts of cities) {{with their own}} history and {{cultural}} events. Today, cities are also marked by a form of intangible cultural heritage like street art, which creates a visual culture based {{on the process of}} reflection about the city and the world. To link these realities and create a personal user interaction with this cultural heritage it is important to capture the story and aesthetics, and find alternatives to immerse the user in these spaces of memory. To that end, this article presents a project which combines Augmented Reality technologies and concepts of Transmedia Storytelling applied to Lisbon City, using Street Art artifacts as markers in a framework of digital media-art...|$|R
5000|$|A three {{character}} {{memory address}} in an instruction was an encoding of a five digit memory address. The three low order digits {{of the five}} digit address, 000 to 999, were specified by the numeric bits of the three characters. The zone bits of the high-order character specified an increment as follows: A 1000, B 2000, B and A together 3000, giving an addressability of 4,000 <b>memory</b> locations. The <b>zone</b> bits of the low-order character specified increments of 4000, 8000, or 12000, to address 16,000 memory locations (with an IBM 1406 Storage Unit). For example, the three character address [...] "I99" [...] was a reference to memory location 3000 + 999, or 3999.|$|R
40|$|Context. Astrophysical {{observations}} must {{be corrected}} for their imperfections of instrumental origin. When Charge Coupled Devices (CCDs) are used, their dark signal {{is one such}} hindrance. In their pristine state, most CCD pixels are ‘cool’, i. e. they exhibit a low, quasi uniform dark current, which can be estimated and corrected for. In space, after having been hit by an energetic particle, pixels can turn ‘hot’, viz. they start delivering excessive, less predictable, dark current. The hot pixels need therefore to be flagged so that subsequent analysis may ignore them. Aims. The image data of the PICARD SODISM solar telescope (Meftah et al. 2013) require dark signal correction and hot pixel identification. Its E 2 V 42 - 80 CCD operates at - 7. 2 °C and has a frame transfer architecture. Both image and <b>memory</b> <b>zones</b> thus accumulate dark current during, respectively, integration and readout time. These two components must be separated in order to estimate the dark signal for any observation. This is {{the main purpose of}} the Dark Signal Model presented in this paper. Methods. The dark signal time series of every pixel is processed by the ‘unbalanced Haar technique’ (Fryzlewicz 2007) in order to timestamp the instants when its dark signal is expected to change significantly. In-between those, both components are assumed constant, and a robust linear regression vs. integration time provides first estimates and a quality coecient. The latter serves to assign definitive estimates for this pixel and for that period. Results. Our model is part of the SODISM Level 1 data production scheme. To check its reliability, we verify on dark frames that it leaves a negligible residual bias (5 e^{-}), and generates a small RMS error (25 e^{-}rms). We also analyze the distribution of the image zone dark current. The cool pixel level is found to be 4. 1 e^{-}. pxl^{- 1 }. s^{- 1 }, in agreement with the predicted value. The emergence rate of hot pixels is investigated too. It legitimates a threshold criterion at 50 e^{-}. pxl^{- 1 }. s^{- 1 }. The growth rate is found to be on average ~ 500 new hot pixels per day, i. e. 4. 2 % of the image zone area per year. Conclusions. A new method for dark signal correction of a frame transfer CCD operating at only ca. - 10 °C is demonstrated. It allows making recommendations about the scientific usage of such CCDs in space. Independently, aspects of the method (adaptation of the unbalanced Haar technique, dedicated robust linear regression) have a generic interest...|$|R
40|$|Splenic {{marginal}} zone lymphomas (SMZLs) {{have been}} proposed to originate from postgerminal center memory B cells that usually have mutated immunoglobulin heavy-chain variable (VH) genes. However, the majority of SMZLs are thought to express both IgD and IgM, which is more typical of naïve B cells that have unmutated VH genes. To better define the SMZL cell of origin and pathogenesis, we studied the histological and immunophenotypic features of eight cases and also sequenced their rearranged VH genes. Half of the cases had unmutated VH genes consistent with a naïve B-cell origin and half had mutated VH genes consistent with a memory B-cell origin. Most of the unmutated cases (three of four) were positive for IgD, which further supports a naïve B-cell origin, whereas the others were negative. In addition, VH gene segment use seems to be nonrandom because seven of eight cases used genes from the VH 1 or VH 4 families and repetitive use of the V 1 - 2, V 1 - 69, and V 4 - 34 gene segments was observed. Our results suggest {{there are two types}} of SMZLs, one that originates from naïve marginal zone B cells in addition to one that originates from <b>memory</b> marginal <b>zone</b> B cells, and that antigen selection may be occurring during lymphomagenesis...|$|R
30|$|MALT {{lymphoma}} {{arises in}} a number of epithelial tissues, including the stomach, salivary gland, lung, small bowel and thyroid. MALT lymphoma constitutes about 5  % of all NHLs and almost 50  % of all gastric lymphomas (Armitage and Weisenburger 1998). Persistent antigenic stimulation due to autoimmune processes or chronic infectious conditions, such as H pylori gastritis, Sjögren syndrome and Hashimoto thyroiditis predispose to development of MALT (Isaacson and Du 2004). Data from epidemiological and molecular studies favor a multistage theory (Zucca et al. 1998; Seydel et al. 2003). In gastric MALT lymphomas which have been extensively studied, it is considered that the molecular mechanism involves the stimulation of antigen receptor by autoantigen and co-stimulatory molecule CD 40 by H pylori-specific T cells. MALT lymphoma develops as marginal <b>zone</b> <b>memory</b> B cells undergo somatic mutation and efface the normal B cell population (Hamoudi et al. 2010).|$|R
40|$|Effects {{of various}} preintrinsic and {{phosphorus}} diffusion gettering treatments upon quality of near‐surface region in Czochralski silicon wafers are studied during a simulated 4 Mb dynamic random access <b>memory</b> process. Denuded <b>zone</b> depth and bulk microdefect density {{are determined by}} synchrotron radiation section topography. Minority carrier lifetime and junction characteristics from test device structures were measured to determine overall gettering efficiency. A two‐step thermal anneal cycle before actual device processing resulted in formation of precipitates, dislocations, stacking faults, and a well‐defined denuded zone in samples with medium oxygen content only when a low‐temperature cycle at 775 °C {{was followed by a}} high‐temperature one at 1150 °C. The longest minority carrier lifetimes are observed in samples where very few or no defects are visible in the bulk of the wafer in the section topographs. Phosphorus gettering, however, was found to be effective for improving both minority carrier lifetime and junction properties...|$|R
40|$|The {{hippocampus}} {{is thought}} to be an associative <b>memory</b> “convergence <b>zone,</b> ” binding together the multimodal elements of an experienced event into a single engram. This predicts a degree of dependency between the retrieval of the different elements comprising an event. We present data from a series of studies designed to address this prediction. Participants vividly imagined a series of person–location– object events, and memory for these events was assessed across multiple trials of cued retrieval. Consistent with the prediction, a significant level of dependency was found between the retrieval of different elements from the same event. Furthermore, the level of dependency was sensitive both to retrieval task, with higher dependency during cued recall than cued recognition, and to subjective confidence. We propose a simple model, in which events are stored as multiple pairwise associations between individual event elements, and dependency is captured by a common factor that varies across events. This factor may relate to between-events modulation of the strength of encoding, or to a process of within-event “pattern completion ” at retrieval. The model predicts the quantitative pattern of depen-dency in the data when changes in the level of guessing with retrieval task and confidence are taken into account. Thus, we find direct behavioral support for the idea that memory for complex multimodal events depends on the pairwise associations of their constituent elements and that retrieval of the various elements corresponding to the same event reflects a common factor that varies from event to event...|$|R
40|$|Experiments {{to study}} the {{behavior}} of various materials point to the relation that exists between elastic properties {{and the type of}} stress. The influence of the state of stress on the elasticity of a fractured material will be discussed for a physically non-linear model of an elastic solid. The strain-dependent moduli model of material, presented in this paper, makes it possible to describe this feature of a solid. It also permits to simulate a dilatancy of rocks. A damage parameter, introduced into the model using a thermodynamical approach, allows to describe a rheological transition from the ductile regime to the brittle one, and to simulate the rock's <b>memory,</b> narrow fracture <b>zone</b> creation and strain rate localization. Additionally, the model enables the investigation of the final geometry of fracture zones, and also to simulate their creation process, taking into account pre-existing fracture zones. The process of narrow fracture zone creation and strain rate localization was simulated numerically for single axis compression and shear flow...|$|R
40|$|B cell lymphomas and leukemias are {{heterogeneous}} tumors {{with different}} cellular origins. Analysis of immunoglobulin (Ig) genes enables {{insight into the}} B cell progenitor, as Ig somatic hypermutation correlates with antigen-related B cell transit through the germinal center (GC). Also, restricted Ig variable heavy chain (VH) gene repertoires in B cell malignancies could imply antigen selection during tumorigenesis. The length of telomeres {{has been shown to}} differ between GC B cells and pre/post-GC B cells, possibly representing an alternative angle to investigate B cell tumor origin. Mantle cell lymphoma (MCL), previously postulated to derive from a naïve, pre-GC B cell, was shown to have an Ig-mutated subset (18 / 110 MCLs, 16 %), suggestive of divergent cellular origin and GC exposure. Another subset of MCL (16 / 110, 15 %), characterized by VH 3 - 21 /Vλ 3 - 19 gene usage, alludes to a role for antigen(s) in pathogenesis, also possible for hairy cell leukemia (HCL) in which the VH 3 - 30 gene (6 / 32, 19 %) was overused. HCL consisted mainly of Ig-mutated cases (27 / 32, 84 %) with low level intraclonal heterogeneity, contrasting with the proposed post-GC origin, for both Ig-mutated and Ig-unmutated HCLs. For MCL and HCL, derivation from naïve or <b>memory</b> marginal <b>zone</b> B cells which may acquire mutations without GC transit are tempting speculations, but currently little is known about this alternative immunological pathway. Heavily mutated Ig genes without intraclonal heterogeneity were demonstrated in lymphoplasmacytic lymphoma/Waldenström’s macroglobulinemia (13 / 14, 93 %), confirming that the precursor cell was transformed after GC affinity maturation. Telomere length analysis within 304 B cell tumors revealed variable lengths; shortest in the Ig-unmutated subset of chronic lymphocytic leukemia, longest in the GC-like subtype of diffuse large B cell lymphoma, and homogeneous in MCL regardless of Ig mutation status. However, telomere length is complex with regard to GC-related origin. In summary, this thesis has provided grounds for speculation that antigens play a role in MCL and HCL pathogenesis, although the potential antigens involved are currently unknown. It has also enabled a more informed postulation about the cellular origin of B cell tumors, which will ultimately enhance understanding of the biological background of the diseases...|$|R
40|$|JMCT is {{a general}} purpose Mont Carlo neutron-photon-electron or coupled neutron/photon/electron {{transport}} code with a continuous energy and multigroup. The code has almost all functions of a general Monte Carlo code which include the various variance reduction techniques, the multi-level parallel computation of MPI and OpenMP, the domain decomposition and on-fly Doppler broadening, etc. Especially, JMCT supports the depletion calculation with TTA and CRAM methods. The input uses the CAD modelling and the calculated results use the visual output. The geometry zones, materials, tallies, depletion <b>zones,</b> <b>memories</b> and the period of random number are enough big for suit of various problems. This paper describes {{the application of the}} JMCT Monte Carlo code to the simulation of BEAVRS and SG-III shielding model. For BEAVRS model, the JMCT results of HZP status are almost the same with MC 21, OpenMC and experiment. Also, we performed the coupled calculation of neutron transport and depletion in full power. The results of ten depletion steps are obtained, where the depletion regions exceed 1. 5 million and 120 thousand processors to be used. Due to no coupled with thermal hydraulics, the result is only for reference. Finally, we performed the detail modelling for Chinese SG-III laser facility, where the anomalistic geometry bodies exceed 10 thousands. The flux distribution of the radiation shielding is obtain based on the mesh tally in case of Deuterium-Tritium fusion reaction. The high fidelity of JMCT has been shown...|$|R
40|$|Toll-like receptors (TLRs) are pattern {{recognition}} receptors that recognize pathogen associated molecular patterns and trigger innate immunity leading to initiation of adaptive immunity. TLR-mediated activation of dendritic cells (DCs) {{is known to}} be a critical event in the initiation of cellular and humoral immune responses. Recent work however suggests that B cells also express TLRs, and that they can be activated via TLR ligands. However, whether such B cell activation occurs only on memory B cells, or whether it can also occur on truly naïve B cells remains controversial. Furthermore, the expression and functional relevance of TLRs on distinct subsets of B cells, which are known to play differential roles in humoral responses is not known. In this study, we investigated the expression pattern of different TLRs in distinct subsets of murine B cells (naïve, <b>memory,</b> follicular, marginal <b>zone,</b> B- 1 and peyer's patch). In contrast to the reported restricted expression pattern of TLRs in human peripheral blood naïve B cells, murine splenic naïve B cells express a variety of TLRs with the exception of TLR 5 and 8. Consistent with this relatively broad expression pattern, murine naive B cells proliferate and secrete antibody to a variety of TLR agonists in vitro, in the absence of B-cell receptor cross-linking. In addition, we observed subtle differences in the antibody secretion pattern of follicular, marginal zone, B- 1 and peyer's patch B-cell subsets. Thus various B cell subsets, including truly naïve B cells, express multiple TLRs, and signaling via such TLRs results in their robust proliferation and antibody secretion, {{even in the absence of}} dendritic cell activation, or T-cell help...|$|R
50|$|Returning to the agency, Shotaro fumes over Philip's {{behavior}} when Kamen Rider Skull {{appears in}} the office who helps Shotaro realize Philip's feelings about wanting a family before leaving his Lost Driver and disappearing. Shotaro walks over to inspect the Lost Driver when Reika {{appears in the}} office to kill him. After learning that Daido has Philip, Shotaro tries to fight back before she transforms into the Heat Dopant and sends flying over his desk when he finds {{a hole in the}} floor and finds the object that made the hole in his ceiling earlier: The T2 Joker Memory. Using it, Shotaro transforms into Kamen Rider Joker as they take the fight to a nearby creek where the Heat Dopant can no longer use her fire-based attacks effectively. Joker uses the opportunity to use his Rider Kick Maximum Drive on the Heat Dopant, knocking her out. With Ryu's support, Shotaro heads to Futo Tower to save Philip and defeat NEVER, attracting Luna Dopant's attention as he sends his Masquerade Dopant illusions to attack Shotaro as he transforms into Joker and gets into the tower, while Ryu manages to use the Engine Blade to hold off the Dopants as well as Izumi and Ashihara. Facing the Metal Dopant, Joker manages to use his Rider Punch Maximum Drive to defeat him before arriving to NEVER's control center with Philip hooked up to the X-Bicker as an amplifier. Eternal overwhelms Joker and takes the final T2 Joker Gaia Memory as he uses T2 <b>Zone</b> <b>Memory</b> to summon all T2 Gaia Memory into the X-Bicker to activate the device which will send out a wave of energy that will turn everyone into Necro-Overs. By then, Reika arrives, begging for help as the body is falling apart with Daido pushing her aside, revealing that when a Necro-Over is hit by a Maximum Drive they will crumble away into dust, as Domoto dissolves away outside with Reika following after realizing how heartless Daido is.|$|R
50|$|Refusing to let Daido {{have his}} way and with Shotaro {{covering}} him, Philip deactivates the X-Bicker by force and disable T2 Eternal Memory's Eternal Requiem. Though Shotaro is unable to hold him back for long, Daido is disabled when Maria injects him with an anti-Necro-Over solution. Enraged, Daido shot her in the stomach as Philip destroys the X-Bicker while Ryu becomes Accel to defeat the Trigger Dopant in Trial form. As Daido limps off to inject himself with the antidote, Maria apologizes to Philip for using him and causing all the chaos as he assures her that he'll stop Daido. Managing to find Daido, Shotaro and Philip fight Eternal before Transforming into Double. However, the Luna Dopant arrives to drag Double off. However, before the fight could start, a strange red coin rolls up to Double's foot. Double picks it up {{as a young man}} named Eiji Hino appears and decides to fight the Luna Dopant, using two other coins to transform into Kamen Rider OOO and hold the Luna Dopant at bay as Double goes after Eternal. Switching from Tatoba Combo to his Takakiriba form, OOO destroys the Dopant with his Medajaribur. Using the HardTurbuler, Double catches up to Eternal at the top of Futo Tower {{as he was about to}} use the X-Bicker energy to destroy the city, fighting the evil Kamen Rider in his 9 basic forms before transforming into CycloneJokerXtreme to deliver a final blow. However, Eternal uses the <b>Zone</b> <b>Memory</b> once more to summon every T2 Gaia Memory onto his Memory Slots, increasing his power. Eternal activates his Never Ending Hell Maximum Drive, absorbing the X-Bicker energy as he sends the blades of the Futo Tower onto Double, sending him to his death. But at the last second, with the city's support of Double, a gale blows through Futo and into the Xtreme Memory, transforming CycloneJokerXtreme into CycloneJokerGoldXtreme. With this power boost, Double flies through the debris towards Eternal, hitting him with the Golden Xtreme Maximum Drive Rider Kick. In his last breath, Katsumi Daido laughs, stating that he hadn't been feeling his own death for quite some time before exploding in a massive fireball, with the Eternal Memory breaking soon after. Later, everyone celebrates the defeat of NEVER, watching the fireworks as repairs with Futo Tower begin.|$|R
40|$|During {{the last}} {{years there has been}} an {{enormous}} advance in FPGAs. Traditionally, FPGAs have been used mainly for prototyping as they offer significant advantages at a suitable low cost: flexibility and verification easiness. Their flexibility allows the implementation of different generations of a given application and provides space to designers to modify implementations until the very last moment, or even correct mistakes once the product has been released. Second, the verification of a design mapped into an FPGA is easier and simpler than in ASICs which require a huge verification effort. Additionally to these advantages, the technological advances have added great capabilities and per- formance to FPGAs, and even though FPGAs are not as efficient as ASICs in terms of performance, area or power, it is true that nowadays they can provide better performance than standard or digital signal processor (DSP) based systems. This fact, in conjunction with the enormous logic capacity allowed by today’s technologies, makes FPGAs an attractive choice for implementation of complex digital systems. Furthermore, with their newly acquired digital signal processing capabilities, FPGAs are now expanding their traditional prototyping roles to help offload computationally intensive functions from standard processors. This Thesis is focused on the last point, the use of FPGAs to accelerate computationally intensive applications. The use of FPGAs for hardware acceleration is an active research field. However, there are still several challenges concerning the use of FPGAs as accelerators: • Availability of Cores. • Capability and performance of FPGAs. • Methods, algorithms and techniques suited for FPGAs. • Design tools. • Hardware-Software co-design and integration. Studying in depth each one of these five challenges related to hardware acceleration is not feasible in just one Thesis. The great variety of applications that can be accelerated and the different features among them imply that the complexity of each task is high. Therefore, in this Thesis we have chosen one subset of applications to be studied, dealing with the implementation of a real application of this subset. Selecting a complex subset of applications, in our case Monte Carlo simulations, allows us to make a general analysis of the main topic, hardware acceleration, from the study, analysis and design of a particular application. This subset of applications has several features shared with other applications and allows us to make a general analysis of the main topic, hardware acceleration, from the study, analysis and design of a given application. Specifically, we have selected a financial application, the Monte Carlo based LIBOR Market Model. Developing an FPGA application from scratch is almost impossible and availability of cores is a must for shorten development time. Following this idea, one of the main objectives is to study the common elements that {{play a key role in}} Monte Carlo simulations and in our target application (and shared with many other applications). Two common elements have been outstood: • The random number generators that are required for the underlying random variables, • Floating-point operators, which are the base elements for implementing the mathematical models that are evaluated. In this way, the first objective of this Ph. D. Thesis is the study, design and implementation of random number generators. In particular, we have focused on Gaussian random number generation and the implementation of a complete generator compatible with variance reduction techniques that can be used for our target application and for other applications. In this field we have developed a high-quality high-performance Gaussian random number generator which is parameterizable and compatible with the also developed parameterizable Latin Hypercube core and a high performance Mersenne Twister generator. Research results in this field demonstrate that random number generation is ideal for hardware acceleration, as an isolated core or within bigger accelerators. Meanwhile, the second objective has dealt with the implementation of efficient and FPGA-oriented mathematical operators (both basic and complex and using floating-point arithmetic). We focused on the design, development and characterization of libraries of components. Instead of focusing on the algorithms of the operators, our approach has been to study how the format can be simplified to obtain operators that are better suited for FPGAs and present better performance. One important goal searched here was to achieve libraries of general purpose components that can be reused in several applications and not just in a particular target application. Different design decisions have been studied and analyzed, and from this analysis, the impact of the overhead due to some of the floating-point standard features has been determined. The format overhead implies a major use of resources and reducing it is a must to obtain operators, independently of what underlying calculation algorithm, that are better suited for FPGAs while present better performances. In particular, the handling of denormalized numbers has a major impact on the FPGA operators. Following the results obtained in that studied, we have discussed and selected a set of features that implies improved performance and reduced resources. This set, has been chosen to design two additional hardware FPGAs-oriented libraries that ensure (or even improve) the accuracy and resolution given by the standard. The operators of these libraries are the base components for the implementation of target application. Additionally, a second analysis has been carried out to study the capabilities of FPGAs to implement complex datapaths. This analysis shows the huge capabilities of current FPGAs which allow up to hundreds of single floating-point operators. Although this capacity, this second analysis has also demonstrate how the working frequency of the operators is severely affected by the routing of their elements when the operators are not isolated and a high percentage of the resources of an FPGA are used. Related to the target application, a third objective of this work was to deepen on the implementation of a particular operator, the exponentiation function. This operator is required in many scientific and financial simulations. Its complexity and the lack of previous general purpose implementations have deserved special attention. We have developed and presented an accurate exponentiation operator for FPGAs based on the straightforward translation of xy into a chain of sub-operators and on the FPGA flexibility which allows tailored precisions. Taking advantage of this flexibility, the provided error analysis focused on determining which precisions are needed in the partial results and in the internal architectures of the sub-operators to obtain an accurate operator with a maximum error of one u. Finally, the integration of this error analysis and the development of the operator within the FloPoCo project have allowed to automatize the generation of exponentiation operators with variable precisions. The next objective we tackle was related to the global purpose of the Thesis of validating all the previously developed elements for the implementation of a complex Monte Carlo simulation which involves all the features that can be found in Monte Carlo simulations. In this way, we have deal with the implementation of the target application, the LIBOR Market Model (LMM). Special attention was devoted to all the features, requirements and circumstances that affect to the performance of the accelerator. A complete LMM hardware core has been developed and its results validated against the original software implementation. Three main features were analyzed: • Correctness of the results obtained. • Accuracy. • Speedup factors obtained by the global application and by each of the main components. Finally, the last objective was the integration of the hardware accelerator within the original software application. All issues related to the communication mechanism are studied putting special focus on how performance is affected by data transfers and by the hardware-software partitioning policy implemented. Following the partitioning policy selected, we have developed the infrastructure (both hardware and software) required to make possible the integration of our accelerator within a software application. A mechanism, based on the use of two RAM <b>memory</b> <b>zones</b> and a PCI-E core with Bus Master capabilities in the FPGA, has been proposed and implemented. And it has allowed us to extend the intrinsic parallelism of Monte Carlo simulations to how the CPU and the FPGA work together. In this way, we exploit the CPU to work in parallel with the FPGA, overlapping their execution times. Hence, the software execution time affecting the performance is reduced to the initial and final processing and to the product valuation in case it is slower that LMM plus the random generator in the FPGA. With this scheme we have achieved high speedups, around 18 times, and close to the theoretical limit for our cases: when there is no software not ported to Hardware or which execution is overlapped with the FPGA execution (the LMM plus RNG achievable speedup). In this case, the speedup achieved could be considerably improved using new FPGAs and several LMM cores in parallel. Durante los últimos años ha habido un enorme avance en la tecnología y capacidades de las FPGAs. Tradicionalmente, las FPGAs se han utilizado principalmente para el desarrollo de prototipos, ya que ofrecen importantes ventajas a un bajo coste: flexibilidad y facilidad de verificación. Su flexibilidad permite la implementación de las diferentes versiones de una aplicación determinada y permite a los diseñadores modificar las implementaciones hasta el último momento, o incluso corregir errores una vez que el producto esta siendo|$|R

