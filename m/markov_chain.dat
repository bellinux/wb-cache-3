10000|5531|Public
5|$|A <b>Markov</b> <b>chain</b> {{is a type}} of Markov {{process that}} has either {{discrete}} state space or discrete index set (often representing time), but the precise definition of a <b>Markov</b> <b>chain</b> varies. For example, it is common to define a <b>Markov</b> <b>chain</b> as a Markov process in either discrete or continuous time with a countable state space (thus regardless of the nature of time), but it is also common to define a <b>Markov</b> <b>chain</b> as having discrete time in either countable or continuous state space (thus regardless of the state space).|$|E
5|$|Stochastic {{matrices}} are square matrices whose rows are probability vectors, that is, whose {{entries are}} non-negative and sum up to one. Stochastic matrices {{are used to}} define Markov chains with finitely many states. A row of the stochastic matrix gives the probability distribution for the next position of some particle currently in the state that corresponds to the row. Properties of the <b>Markov</b> <b>chain</b> like absorbing states, that is, states that any particle attains eventually, can be read off the eigenvectors of the transition matrices.|$|E
5|$|Markov {{processes}} and Markov chains are named after Andrey Markov who studied Markov chains {{in the early}} 20th century. Markov was interested in studying an extension of independent random sequences. In his first paper on Markov chains, published in 1906, Markov showed that under certain conditions the average outcomes of the <b>Markov</b> <b>chain</b> would converge to a fixed vector of values, so proving a weak law of large numbers without the independence assumption, which had been commonly regarded as a requirement for such mathematical laws to hold. Markov later used Markov chains to study the distribution of vowels in Eugene Onegin, written by Alexander Pushkin, and proved a central limit theorem for such chains.|$|E
40|$|In this {{research}} paper, weighted / unweighted, directed / undirected graphs {{are associated with}} interesting Discrete Time <b>Markov</b> <b>Chains</b> (DTMCs) as well as Continuous Time <b>Markov</b> <b>Chains</b> (CTMCs). The equilibrium / transient behaviour of such <b>Markov</b> <b>chains</b> is studied. Also entropy dynamics (Shannon entropy) of certain structured <b>Markov</b> <b>chains</b> is investigated. Finally certain structured graphs and the associated <b>Markov</b> <b>chains</b> are studied...|$|R
2500|$|Time-homogeneous <b>Markov</b> <b>chains</b> (or {{stationary}} <b>Markov</b> <b>chains)</b> are processes where ...|$|R
40|$|We extend {{probabilistic}} computational tree logic {{for expressing}} properties of <b>Markov</b> <b>chains</b> to imprecise <b>Markov</b> <b>chains,</b> {{and provide an}} efficient algorithm for model checking of imprecise <b>Markov</b> <b>chains.</b> Thereby, we provide a formal framework to answer a very wide range of questions about imprecise <b>Markov</b> <b>chains,</b> in a systematic and computationally efficient way...|$|R
25|$|If {{every state}} can reach an {{absorbing}} state, then the <b>Markov</b> <b>chain</b> is an absorbing <b>Markov</b> <b>chain.</b>|$|E
25|$|A <b>Markov</b> <b>chain</b> {{is a type}} of Markov {{process that}} has either {{discrete}} state space or discrete index set (often representing time), but the precise definition of a <b>Markov</b> <b>chain</b> varies. For example, it is common to define a <b>Markov</b> <b>chain</b> as a Markov process in either discrete or continuous time with a countable state space (thus regardless of the nature of time), but it is also common to define a <b>Markov</b> <b>chain</b> as having discrete time in either countable or continuous state space (thus regardless of the state space).|$|E
25|$|<b>Markov</b> <b>chain</b> {{models have}} been used in {{advanced}} baseball analysis since 1960, although their use is still rare. Each half-inning of a baseball game fits the <b>Markov</b> <b>chain</b> state when the number of runners and outs are considered. During any at-bat, there are 24 possible combinations of number of outs and position of the runners. Mark Pankin shows that <b>Markov</b> <b>chain</b> models can be used to evaluate runs created for both individual players as well as a team.|$|E
40|$|We {{consider}} finite <b>Markov</b> <b>chains</b> {{where there}} are uncertainties {{in some of the}} transition probabilities. These uncertainties are modeled by fuzzy numbers. Using a restricted fuzzy matrix multiplication we investigate the properties of regular, and absorbing, fuzzy <b>Markov</b> <b>chains</b> and show that the basic properties of these classical <b>Markov</b> <b>chains</b> generalize to fuzzy <b>Markov</b> <b>chains...</b>|$|R
40|$|This Bachelor Thesis tackles {{the basics}} of the <b>Markov</b> <b>chains</b> theory. The first four {{chapters}} describe fundamental definitions and theorems {{of the theory of}} <b>Markov</b> <b>chains,</b> both in continuous and discrete time and both with discrete and general state space. The last chapter contains examples of each type of <b>Markov</b> <b>chains.</b> The conclusion describes the relation between all four types of <b>Markov</b> <b>chains...</b>|$|R
40|$|<b>Markov</b> <b>chains</b> {{have been}} widely applied in {{information}} theory and various areas of its applications. Quantum <b>Markov</b> <b>chains</b> are a natural quantum extension of <b>Markov</b> <b>chains.</b> This paper studies irreducibility and periodicity of (discrete time) quantum <b>Markov</b> <b>chains.</b> In particular, we present a periodic decomposition and establish limit theorems for them. Using these mathematical tools, we give a new characterization of (one-shot) zero-error capacity of quantum channels...|$|R
25|$|Note {{that this}} example {{illustrates}} a periodic <b>Markov</b> <b>chain.</b>|$|E
25|$|<b>Markov</b> <b>chain</b> {{methods have}} also become very {{important}} for generating sequences of random numbers to accurately reflect very complicated desired probability distributions, via a process called <b>Markov</b> <b>chain</b> Monte Carlo (MCMC). In recent years this has revolutionized the practicability of Bayesian inference methods, allowing {{a wide range of}} posterior distributions to be simulated and their parameters found numerically.|$|E
25|$|Markov {{chains are}} used in finance and {{economics}} to model {{a variety of different}} phenomena, including asset prices and market crashes. The first financial model to use a <b>Markov</b> <b>chain</b> was from Prasad et al. in 1974. Another was the regime-switching model of James D. Hamilton (1989), in which a <b>Markov</b> <b>chain</b> is used to model switches between periods high and low GDP growth (or alternatively, economic expansions and recessions). A more recent example is the Markov Switching Multifractal model of Laurent E. Calvet and Adlai J. Fisher, which builds upon the convenience of earlier regime-switching models. It uses an arbitrarily large <b>Markov</b> <b>chain</b> to drive the level of volatility of asset returns.|$|E
40|$|Abstract. This {{paper will}} explore {{the basics of}} discrete-time <b>Markov</b> <b>chains</b> used to prove the Ergodic Theorem. Definitions and basic theorems {{will allow us to}} prove the Ergodic Theorem without any prior {{knowledge}} of <b>Markov</b> <b>chains,</b> although some knowledge about <b>Markov</b> <b>chains</b> will allow the reader better insight about the intuitions behind the provided theorems. Even for those familiar with <b>Markov</b> <b>chains,</b> the provided definitions will be importan...|$|R
40|$|In the paper, {{the law of}} the {{iterated}} logarithm for additive functionals of <b>Markov</b> <b>chains</b> {{is obtained}} under some weak conditions, which are weaker than the conditions of invariance principle of additive functionals of <b>Markov</b> <b>chains</b> in Maxwell and Woodroofe [2000. Central limit theorems for additive functionals of <b>Markov</b> <b>chains.</b> Ann. Probab. 28 (2), 713 - 724]. <b>Markov</b> <b>chains</b> The law of the iterated logarithm Additive functionals Dunford-Schwartz operator Shift processes...|$|R
40|$|Brand loyalty, <b>Markov</b> <b>chains,</b> {{stochastic}} process, {{market share}} <b>Markov</b> <b>chains,</b> applied in marketing problems, are principally used for Brand Loyalty studies. Especially, <b>Markov</b> <b>chains</b> are strong techniques for forecasting long term market shares in oligopolistic markets. The concepts of marketing studies are thought as discrete {{from the time}} and place viewpoint and so finite <b>Markov</b> <b>chains</b> are applicable for this kind of process. The {{purpose of this study is}} to examine the consumer Brand Loyalty of sportshoes with <b>markov</b> <b>chains</b> method. In this study, the data to examine the Brand Loyalty have been obtained from 531 undergraduate students in Istanbul, Turkey...|$|R
25|$|The process {{described}} {{here is a}} <b>Markov</b> <b>chain</b> on a countable state space that follows a random walk.|$|E
25|$|Using a <b>Markov</b> <b>chain</b> Monte Carlo method, the Tutte {{polynomial}} can be arbitrarily well approximated {{along the}} positive branch of , equivalently, the partition {{function of the}} ferromagnetic Ising model. This exploits the close connection between the Ising model {{and the problem of}} counting matchings in a graph. The idea behind this celebrated result of Jerrum and Sinclair is to set up a <b>Markov</b> <b>chain</b> whose states are the matchings of the input graph. The transitions are defined by choosing edges at random and modifying the matching accordingly. The resulting <b>Markov</b> <b>chain</b> is rapidly mixing and leads to “sufficiently random” matchings, which can be used to recover the partition function using random sampling. The resulting algorithm is a fully polynomial-time randomized approximation scheme (fpras).|$|E
25|$|The use of Markov chains in <b>Markov</b> <b>chain</b> Monte Carlo methods covers {{cases where}} the process follows a {{continuous}} state space.|$|E
3000|$|... } are <b>Markov</b> <b>chains</b> for a fixed channel, {{equalizer}} pair at (Z,Θ). These two <b>Markov</b> <b>chains</b> take {{values in}} [...]...|$|R
30|$|Benjamini and Peres [1] {{have given}} the {{definition}} of the tree-indexed homogeneous <b>Markov</b> <b>chains.</b> Here we improve their definition and give {{the definition of the}} tree-indexed second-order nonhomogeneous <b>Markov</b> <b>chains</b> in a similar way. We also give the following definition (Definition 2.3) of tree-indexed nonhomogeneous <b>Markov</b> <b>chains.</b>|$|R
40|$|AbstractWe {{study the}} {{relaxation}} time of product-type <b>Markov</b> <b>chains</b> approaching a product distribution. We bound {{the approach to}} stationarity for such <b>Markov</b> <b>chains</b> {{in terms of the}} mixing times of the component <b>Markov</b> <b>chains.</b> In cases where the component mixing times differ considerably we propose an optimized visiting scheme which makes such product-type <b>Markov</b> <b>chains</b> comparable to Gibbs-type samplers. We conclude the paper by discussing the relaxation of Metropolis-type samplers for separable energy functions...|$|R
25|$|Stochastic models {{allow for}} the direct {{modeling}} of the random perturbations that underlie real world ecological systems. <b>Markov</b> <b>chain</b> models are stochastic.|$|E
25|$|Optimized <b>Markov</b> <b>chain</b> {{algorithms}} {{which use}} local searching heuristic sub-algorithms {{can find a}} route extremely close to the optimal route for 700 to 800 cities.|$|E
25|$|A <b>Markov</b> <b>chain</b> {{with more}} than one state and just one out-going {{transition}} per state is either not irreducible or not aperiodic, hence cannot be ergodic.|$|E
40|$|We {{study the}} {{continuous}} one-dimensional hard-sphere model and present irreversible local <b>Markov</b> <b>chains</b> that mix on faster time scales than the reversible heatbath or Metropolis algorithms. The mixing time scales appear {{to fall into}} two distinct universality classes, both faster than for reversible local <b>Markov</b> <b>chains.</b> The event-chain algorithm, the infinitesimal limit {{of one of these}} <b>Markov</b> <b>chains,</b> belongs to the class presenting the fastest decay. For the lattice-gas limit of the hard-sphere model, reversible local <b>Markov</b> <b>chains</b> correspond to the symmetric simple exclusion process (SEP) with periodic boundary conditions. The two universality classes for irreversible <b>Markov</b> <b>chains</b> are realized by the totally asymmetric simple exclusion process (TASEP), and by a faster variant (lifted TASEP) that we propose here. Lifted <b>Markov</b> <b>chains</b> and the recently introduced factorized Metropolis acceptance rule extend the irreversible <b>Markov</b> <b>chains</b> discussed here to general pair interactions and to higher dimensions. Comment: 5 pages + supplements; v 3 corrects definition in the supplement...|$|R
25|$|For an {{overview}} of <b>Markov</b> <b>chains</b> on a general state space, see the article <b>Markov</b> <b>chains</b> on a measurable state space.|$|R
40|$|<b>Markov</b> <b>chains</b> with block-structured {{transition}} matrices {{find many}} applications in various areas. Such <b>Markov</b> <b>chains</b> {{are characterized by}} partitioning the state space into subsets called levels, each level consisting {{of a number of}} stages. Examples include <b>Markov</b> <b>chains</b> of GI/M/ 1 type and M/G/ 1 type, and, more generally, <b>Markov</b> <b>chains</b> of Toeplitz type, or GI/G/ 1 type. In the analysis of such <b>Markov</b> <b>chains,</b> a number of properties and measures which relate to transitions among levels play a dominant role, while transitions between stages within the same level are less important. The censoring technique has been frequently used in the literature in studying these measures and properties. In this paper, we use this same technique to study block-structured <b>Markov</b> <b>chains.</b> New results and new proofs on factorizations and convergence of algorithms will be provided...|$|R
25|$|Constraints on many cosmological {{parameters}} can {{be obtained}} from their effects on the power spectrum, and results are often calculated using <b>Markov</b> <b>Chain</b> Monte Carlo sampling techniques.|$|E
25|$|A <b>Markov</b> <b>chain</b> {{is said to}} be {{irreducible}} if it {{is possible}} to get to any state from any state. The following explains this definition more formally.|$|E
25|$|This <b>Markov</b> <b>chain</b> is irreducible, {{because the}} ghosts can fly from every state to {{every state in}} a finite amount of time. Due to the secret passageway, the <b>Markov</b> <b>chain</b> is also aperiodic, because the monsters can move from any state to any state both in an even and in an uneven number of state transitions. Therefore, a unique {{stationary}} distribution exists {{and can be found}} by solving π'Q=0 subject to the constraint that elements must sum to 1. The solution of this linear equation subject to the constraint is %.|$|E
2500|$|Considering a {{collection}} of <b>Markov</b> <b>chains</b> whose evolution takes in account the state of other <b>Markov</b> <b>chains,</b> {{is related to the}} notion ...|$|R
30|$|There {{has been}} {{considerable}} recent {{work on the}} problem of computable bounds for convergence rates of <b>Markov</b> <b>chains.</b> Recently, the authors (see [1 – 4]) gave the bounds of convergence rates for <b>Markov</b> <b>chains.</b> Their main methods are based on renewal theory and coupling theory. And in [5 – 7], the authors gave the convergence rates of stochastically monotone <b>Markov</b> <b>chains.</b> Their results and methods have the advantages of being applicable to some <b>Markov</b> <b>chains</b> or processes.|$|R
40|$|An {{introduction}} {{to the theory of}} mixing of random processes is presented. The aim of this introduction is to be eventually able to separate general random processes, <b>markov</b> <b>chains</b> and <b>markov</b> <b>chains</b> with finite alphabet into groups which mix differently. The introduction is made complete by examples. We show, that for general processes those groups are separate, for <b>markov</b> <b>chains</b> some coincide, and for <b>markov</b> <b>chains</b> with finite alphabet all coincide. Powered by TCPDF (www. tcpdf. org...|$|R
