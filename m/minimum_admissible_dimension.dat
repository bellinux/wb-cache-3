0|33|Public
50|$|To be {{eligible}} {{to get to the}} next level, i.e. the second stage, it is necessary that a student scores at-least a <b>Minimum</b> <b>Admissible</b> Score (MAS) which is 40% of the maximum score.|$|R
30|$|All {{aforementioned}} allows {{to recommend}} conducting {{the process of}} lignite desulphurization with <b>minimum</b> <b>admissible</b> values of OFR (close to 0.6  m 3 /(h kg)), which provide maximally possible RDPS, TSC {{and the content of}} H 2 S in desulphurization gases and these are sufficient for keeping up the process temperature due to burning out the part of COM.|$|R
40|$|The paper {{considers}} {{an identification}} of binary sequences which {{is necessary for}} formation of a general cryptographic key in perspective information technologies with the purpose to select sequences containing <b>minimum</b> <b>admissible</b> quantity of discrepancies at comprehensible level of the disclosed information. One-stage plan of selective quality assurance on an alternative sign is proposed for solution of the problem. </p...|$|R
5000|$|In {{probability}} theory, a Hunt {{process is}} a strong Markov process which is quasi-left continuous {{with respect to the}} <b>minimum</b> completed <b>admissible</b> filtration [...]|$|R
5000|$|Students must satisfy {{appropriate}} {{eligibility criteria}} in NSEP.They must score more than 50% {{of the average}} of the top ten scores. This is the <b>Minimum</b> <b>Admissible</b> Score (MAS in short). If they score more than 80% of the above-mentioned average, they're selected for INPhO.This score is the Merit Index (MI). After selection of these students, more are selected from below the MI, but above the MAS, till the quota of seats allowed to their respective state is filled, completely, or to as large an extent as possible.|$|R
40|$|We {{construct}} an optimal quantum universal variable-length code that achieves the <b>admissible</b> <b>minimum</b> rate, i. e., our code {{is used for}} any probability distribution of quantum states. Its probability of exceeding the <b>admissible</b> <b>minimum</b> rate exponentially goes to 0. Our code is optimal {{in the sense of}} its exponent. In addition, its average error asymptotically tends to 0. ...|$|R
40|$|Abstract — The {{objective}} of the paper is to provide qualitative insight into the global effects of distributed mechanisms, such as {{carrier sense multiple access}} (CSMA) and rate control, on the performance and stability of multi-hop wireless networks. Toward this end, we introduce a linear queueing network model where the service capacity of each node is modulated by the transmission state of its neighbor. We derive lower bounds on the steady-state utilization at each queue of such networks and demonstrate the existence of a phase transition phenomenon, whereby infinitesimal traffic increase at a single node in the network can suddenly render the entire network instable. We also present NS simulation results that show how this phenomenon can actually take place in IEEE 802. 11 multi-hop wireless networks. Our results have direct bearing on rate control schemes, in that they indicate a <b>minimum</b> <b>admissible</b> threshold rate required to prevent network instability. I...|$|R
40|$|ABSTRACT:. The {{softening}} postpeak load-deflection relation for cracking reinfor:ced ~oncrete {{beams and}} frames is analyzed by layered finite elements. Concrete {{is assumed to}} exhibit strain softening in both tension and compression, and the steel reinforcement is elastic-plastic. The bending theory assumptions are used and bond slip of reinforcement is neglected·. It is shown that the model can satisfactorily approximate the existing test resurts for softening beams and frames. At the same time, the constitutive laws with strain softening, including those of continuum damage mechanics, are shown to lead to spurious sensitivity of results to the chosen finite element size, similar to that documented before for other strain-softening problems. In analogy to the finite element crack-band model, this problem can be overcome if the <b>minimum</b> <b>admissible</b> element size is specified as a cross section propertYl its suitable value appears to be equal to the beam depth. INTRoDucnO...|$|R
40|$|In this brief, we {{consider}} the problem of 3 -D path generation and tracking for unmanned air vehicles (UAVs). The proposed path generation algorithm allows us to find a path satisfying arbitrary initial and final conditions, specified in terms of position and velocity. Our method assumes that aircraft structural and dynamic limitations can be translated in a turn radius constraint; therefore, the generated paths satisfy a constraint on the <b>minimum</b> <b>admissible</b> turning radius. The proposed algorithm for the path tracking guarantees, under specified assumptions, that the tracking error, both in position and in attitude, asymptotically tends to zero. The work {{has been carried out}} with reference to the UAV of the Italian Aerospace Research Center (CIRA). Simulation results for both the path generation and the tracking algorithms are presented; the latter have been obtained using a detailed 6 -degree-of-freedom model of the CIRA UAV in the presence of wind and turbulence...|$|R
60|$|I {{never felt}} the {{advantage}} of this mode of travelling, and I believe we have now tried nearly all the others, or {{the advantages of the}} Parisian plan of living, so strongly as on the present occasion. Up to the last moment, I was undecided by what route to travel. The furniture of the apartment was my own, and it was our intention to return to Paris, to pass the winter. The luggage had been stowed early in the morning, the carriage was in the court ready to hook on, and at ten we sat down quietly to breakfast, as usual, with scarcely a sign of movement about us. Like old campaigners, the baggage had been knowingly reduced to the very <b>minimum</b> <b>admissible,</b> no part of the furniture was deranged, but everything was in order, and you may form some idea of the facilities, when you remember that this was the condition of a family of strangers, that in half an hour was to start on a journey of several months' duration, to go--they knew not whither.|$|R
40|$|From {{the total}} {{electricity}} generated in South Africa, about 85 % is coal based. Coal powder {{is a solid}} fuel used in injectors for electricity production. The coal powder is extremely abrasive therefore {{it is important to}} maintain a <b>minimum</b> <b>admissible</b> flow speed. As the most severe pipe wall erosion takes place in bends, the most significant factor influencing wall erosion is the solid particle velocity. The aim of this research was to reduce the wall erosion through flow velocity control. Computational fluid dynamics (CFD) simulations, showed that for the same velocity, the erosion is dependent on flow direction change and the ratio between the radius of the bend and pipe diameter. The reduction of flow velocity was achieved by subtracting a spiral volume, from the inner volume of a horizontal pipe, just before an upward or downward bend. The modified volume geometry generated a vortex flow in the bend, achieving a more even distribution of maximum erosion points over the bend surface. The wall erosion, expressed in mm–wall thickness loss/hour, in the modified volume decreased by about 28, 8 % while the installation life expectancy increased significantly. Furthermore the bend radius was proved to influence the erosion magnitude and location...|$|R
40|$|International audienceCellular {{manufacturing}} is {{an effective}} alternative to batch-type production systems where different products are intermittently produced in small lot sizes with frequent setups, large in-process storage quantities, long production lead times, decreasing throughputs, and complex planning and control functions. An effective approach to forming manufacturing cells and introducing families of similar parts, consequently increasing production volumes and machine utilization, {{is the use of}} similarity coefficients in conjunction with clustering procedures. In a similarity coefficients based approach, the results of the clustering analysis depend on the <b>minimum</b> <b>admissible</b> level of similarity adopted for the generic group of clustered items. This is the so-called threshold value of group similarity. The aim {{of this paper is to}} identify effective values of the threshold value of group similarity to help practitioners and managers of manufacturing systems form machine groups and related part families. The proposed threshold values for a given similarity coefficient are based on calculation of the percentile of aggregations generated by the adopted clustering algorithm. The importance of the proposed measure of group similarity has been demonstrated by experimental analysis conducted on a large set of significant instances of the cell formation problem in the literature. This analysis also supports the best determination of this percentile-based cut value...|$|R
40|$|Purpose. Experimental {{definition}} of {{values of the}} dynamic parameters characterizing traffic safety of the rail autobus in tangent and curved track sections and switches, and conformity check to their demands of normative documents. Methodology. Test methods are based on comparison of experimentally determined dynamic qualities of the autobus with their admitted values. As the parameters defining traffic safety, the numerical value of which is resulted further, are used the following ones: the derailment stability coefficient; vertical dynamics coefficients {{in the first and}} second steps of spring suspension; the frame forces; smoothness of movement. Determination of the derailment stability coefficient is performed by a known technique. Vertical dynamics coefficients in the each step of spring suspension are defined as the relation of dynamic vertical bending flexures to magnitudes of their static values corresponding to the set occupancy of the autobus. Findings. Coefficient values of the vertical dynamics of train carriages in the first and second steps of spring suspension, as well as the value of frame forces and parameters of movement smoothness did not exceed the admissible standard values, and was higher than the <b>minimum</b> <b>admissible</b> value. Originality. Values of the parameters characterizing dynamic qualities of new type of the rail autobus are obtained, and possibility of its operation on the main ways of the railways of Ukraine is shown. Practical value. Admissible speeds of the rail autobus traffic on various railway track sections are defined...|$|R
40|$|We discuss {{coloring}} and partitioning {{questions related}} to Sperner's Lemma, originally motivated by an application in hardness of approximation. Informally, we call a partitioning of the (k- 1) -dimensional simplex into k parts, or a labeling of a lattice inside the simplex by k colors, "Sperner-admissible" if color i avoids the face opposite to vertex i. The questions we study are of the following flavor: What is the Sperner-admissible labeling/partitioning that makes the total area of the boundary between different colors/parts as small as possible? First, for a natural arrangement of "cells" in the simplex, we prove an optimal lower bound {{on the number of}} cells that must be non-monochromatic in any Sperner-admissible labeling. This lower bound is matched by a simple labeling where each vertex receives the <b>minimum</b> <b>admissible</b> color. Second, we show for this arrangement that in contrast to Sperner's Lemma, there is a Sperner-admissible labeling such that every cell contains at most 4 colors. Finally, we prove a geometric variant of the first result: For any Sperner-admissible partition of the regular simplex, the total surface area of the boundary shared by at least two different parts is minimized by the Voronoi partition (A^*_ 1, [...] .,A^*_k) where A^*_i contains all the points whose closest vertex is i. We also discuss possible extensions of this result to general polytopes and some open questions. Comment: To appear in "A Journey through Discrete Mathematics. A Tribute to Jiri Matousek", edited by Martin Loebl, Jaroslav Nesetril and Robin Thomas, due to be published by Springe...|$|R
40|$|Social {{needs and}} {{instruments}} for their satisfaction — public goods — have been studied. Attention {{is drawn to}} the fact that social needs are composed of individual and collective needs of economic entities. They are interrelated and complementary. An important difference between individual and collective needs is in their personification (individualization) and divisibility. Proper consideration has been paid to concepts related to the needs of society — the public interest (as a form of social needs), public goods (all the goods, services that can satisfy social needs). For studying social needs and benefits in this work there was taken {{one of the most popular}} of their classification, which lies in their division into primary and secondary ones. Emphasis is placed on importance of its use in determining the priority of financing public goods. The focus is on the approach to calculating the cost of public goods. It is regarded as one of the stages in satisfying social needs (along with planning and analysis of the degree for their satisfaction). An approach to determining the structure of financing the total volume of public goods, calculating <b>minimum</b> <b>admissible</b> and desired volume of financing has been presented. The current interest in the European experience prompted the authors to analyze the cost and structure of financing public goods in France, the available statistical base allowing it to be implemented. The determined dependence between the indicators enabled conducting calculations of the desired volumes of financing the public needs per person according to three scenarios: optimistic, satisfactory and critical relevant to phases of the economic cycle. This simulation is aimed at being used in management decisionmaking, development of social and economic policy, etc. There have been noted serious shortcomings related to the lack of quality and reliable statistical information on Ukraine, which hamper the calculation and consideration of foreign experienc...|$|R
40|$|The {{conventional}} channel resolvability problem {{refers to}} {{the determination of the}} minimum rate needed for an input process to approximate the output distribution of a channel in either the total variation distance or the relative entropy. In contrast to previous works, in this paper, we use the (normalized or unnormalized) Rényi divergence (with the Rényi parameter in [0, 2]) to measure the level of approximation. We also provide asymptotic expressions for normalized Rényi divergence when the Rényi parameter is larger than or equal to 1 as well as (lower and upper) bounds for the case when the same parameter is smaller than 1. We characterize the minimum rate needed to ensure that the Rényi resolvability vanishes asymptotically. The optimal rates are the same for both the normalized and unnormalized cases. In addition, the minimum rate when the Rényi parameter no larger than 1 equals the minimum mutual information over all input distributions that induce the target output distribution similarly to the traditional case. When the Rényi parameter is larger than 1 the minimum rate is, in general, larger than the mutual information. The optimal Rényi resolvability is proven to vanish at least exponentially fast for both of these two cases, as long as the code rate is larger than the <b>minimum</b> <b>admissible</b> one. The optimal exponential rate of decay for i. i. d. random codes is also characterized exactly. We apply these results to the wiretap channel, and completely characterize the optimal tradeoff between the rates of the secret and non-secret messages when the leakage measure is given by the (unnormalized) Rényi divergence. This tradeoff differs from the conventional setting when the leakage is measured by the traditional mutual information. Comment: 49 pages. An error in the exponent in Theorem 6 has been fixe...|$|R
40|$|The paper {{presents}} a computer procedure {{to evaluate the}} collapse load and the collapse mechanism of three-dimensional structures made of rigid blocks with no-tension and frictional interfaces based on mathematical programming. Using the theorems of non-standard limit analysis, the algorithm searches the <b>minimum</b> statically <b>admissible</b> load factor, which ensures a possible mechanism. The presence of friction makes the problem non-linear and difficult to solve. The possibility to detect the optimal solution using a quasi-feasible initial estimate of the unknowns is shown. This is obtained as the solution of a linear programming problem corresponding {{to a system of}} blocks with dilatancy in the joints instead of friction. Various analyses show the validity of the procedure and the effectiveness of the non-standard limit analysis approach for studying the collapse behaviour of real stone masonry structures...|$|R
40|$|This {{article is}} devoted to the local {{geometry}} of 2 -nondegenerate CR manifolds M of hypersurface type. An absolute parallelism for such structures was recently constructed independently by Isaev-Zaitsev, Medori-Spiro, and Pocchiola in the minimal possible dimension (M= 5), and for M= 7 in certain cases by the first author. In this paper, we develop a bigraded analog of Tanaka's prolongation procedure to construct a canonical absolute parallelism for these CR structures in arbitrary (odd) dimension with Levi kernel of arbitrary <b>admissible</b> <b>dimension.</b> We introduce the notion of a bigraded Tanaka symbol [...] a complex bigraded vector space [...] containing all essential information about the CR structure. Under the additional regularity assumption that the symbol is a Lie algebra, we define a bigraded analog of the Tanaka universal algebraic prolongation, endowed with an anti-linear involution, and prove that for any CR structure with a given regular symbol there exists a canonical absolute parallelism on a bundle whose dimension is that of the bigraded universal algebraic prolongation. Moreover, we show that there is a unique (up to local equivalence) such CR structure whose algebra of infinitesimal symmetries has maximal possible dimension, and the latter algebra is isomorphic to the real part of the bigraded universal algebraic prolongation of the symbol. In the case of 1 -dimensional Levi kernel we classify all regular symbols and calculate their bigraded universal algebraic prolongations. In particular, we show for the most interesting class of the regular CR symbols, the universal prolongation is isomorphic to the complex orthogonal algebra so(m, C) where m= 12 (M+ 5). For each such symbol we specify the real form of this algebra corresponding to the algebra of infinitesimal symmetries of the maximally symmetric model. Comment: 32 page...|$|R
40|$|We {{introduce}} {{a new class of}} admissible pairs of triangular sequences and prove a bijection between the set of admissible pairs of triangular sequences of length n and the set of parking functions of length n. For all u and v = 0, 1, 2, 3 andall 7 we describe in terms of <b>admissible</b> pairs the <b>dimensions</b> of the bi-graded components h u,v of diagonal harmonics 1, [...] .,x n; y 1, [...] .,y n]/S n, i. e., polynomials in two groups of n variables modulo the diagonal action of symmetric group S n...|$|R
40|$|We {{prove the}} {{continuity}} of the value function of the sparse optimal control problem. The sparse optimal control is a control whose support is <b>minimum</b> among all <b>admissible</b> controls. Under the normality assumption, it is known that a sparse optimal control is given by L^ 1 optimal control. Furthermore, the value function of the sparse optimal control problem is identical with that of the L 1 -optimal control problem. From these properties, we prove {{the continuity of}} the value function of the sparse optimal control problem by verifying that of the L 1 -optimal control problem. Comment: Submitted to ASCC 201...|$|R
40|$|In this paper, the bending waves {{propagating}} {{along the}} edge of a semi-infinite Kirchhoff plate resting on a two-parameter Pasternak elastic foundation are studied. Two geometries of the foundation are considered: either it is infinite or it is semi-infinite with the edges of the plate and of the foundation coinciding. Dispersion relations along with phase and group velocity expressions are obtained. It is shown that the semi-infinite foundation setup exhibits a cut-off frequency which is the same as for a Winkler foundation. The phase velocity possesses a minimum which corresponds to the critical velocity of a moving load. The infinite foundation exhibits a cut-off frequency which depends on its relative stiffness and occurs at a nonzero wavenumber, which is in fact hardly observed in elastodynamics. As a result, the associated phase velocity <b>minimum</b> is <b>admissible</b> only up to a limiting value of the stiffness. In the case of a foundation with small stiffness, asymptotic expansions are derived and beam-like one-dimensional equivalent models are deduced accordingly. It is demonstrated that for the infinite foundation the related nonclassical beam-like model comprises a pseudo-differential operator...|$|R
40|$|This {{research}} {{focus on}} the sand wave field along the canyon axis in the upper 4 km of the Monterey Submarine Canyon revealed by high-resolution multibeam bathymetry collected by MBARI/Mapping Lab of the California State University ([URL] spanning six years period (spring 2003 to fall 2008). The goal of the research is to understand submarine processes linking erosion, gravitative processes, sediment liquefaction by storm events, sediment transport by tidal currents, and deposition of the reworked sediment in distal basins with quasi-horizontal floors. Morphometric analysis has been conducted on the sand wave on the canyon head and main axes comparing the waves shape of nearest surveys by means of smoothing filtering and geostatistical techniques. The analysis has allowed to classify the canyon floor in upstream migration zones, downstream migration zones, and completely reworked zones. In the first two zones a sand migration velocity has been inferred, {{and in the last}} ones, where the wave field is completely reworked between each surveys, a <b>minimum</b> <b>admissible</b> migration velocity has been deducted. A simple mathematical model has then permitted to reproduce the main features of sand wave inception and growth. In particular the model {{focus on the}} prediction of the migration rates that sand waves undergo because of tidal currents. The model output has been compared versus the morphometric analysis results and match and mismatch are discussed. Results of the research show that the sand waves migrate in a predominantly up-canyon direction with tidal and internal tidal currents, despite different behaviour along the canyon. However the research shows that these are not the dominating flows within the canyon. Seismic profiles interpretation, other morphometric analysis results like local channel widening causing lateral erosion of older channel and extension of gully head on canyon walls and rim, point out high velocity transport processes in the canyon main axes. This mechanism is possibly triggered by seismic events and storm events, as suggested by the slope stability and liquefaction potential analyses. Results of these latter in fact shows that earthquakes with peak ground acceleration (PGA) > 0. 18 (about 50 % of the PGA related to the 1989 M 7. 1 Loma Prieta earthquake) can lead to mass movements involving large volumes of materials and/or to seismic liquefaction of the fluvial, estuarine and beach shallow sediments at the canyon’s head and in the nearshore region...|$|R
40|$|Generalizing {{the work}} of [5, 41], we give a general {{solution}} to the following prob-lem: describe the triplets (Ω, g, µ) where g = (gij(x)) is the (co) metric associated to the symmetric second order differential operator L(f) = 1 ρ ij ∂i(g ijρ∂jf), de-fined on a domain Ω of Rd and such that L is expandable on a basis of orthogonal polynomials on L 2 (µ), and dµ = ρ(x) dx is some <b>admissible</b> measure. In <b>dimension</b> d = 2, and up to affine transformations, we find 10 compact domains Ω plus a one parameter family. We also give some description of the non-compact cases in thi...|$|R
40|$|This paper {{examines}} one paradigm used {{to develop}} admissible heuristics: problem relaxation [10, 11, 32]. This consists of three steps: simplify (or relax) a problem, solve the simplified problem, and use that solution as advice to guide {{the search for a}} solution to the original problem. We introduce an extension to this methodology which exploits the simplicity of relaxed models. By criticizing the feasibility of a relaxed solution, we arrive at a closer approximation of the solution to the original problem. This solution-criticism process recovers some of the information lost by relaxation. and yields more powerful admissible heuristics than by relaxation alone. We apply our methodology to the Traveling-Salesman problem and the N Puzzle. For the Traveling-Salesman Problem, it yields the well known. <b>admissible</b> <b>minimum</b> spanning tree heuristic. For the Eight and Fifteen Puzzles (in general the N puzzle), it yields a new heuristic which performs significantly better than all previously known heuristics...|$|R
40|$|Voltage {{regulator}} modules (VRMs) used {{in modern}} computers {{can work in}} “extreme” conditions since they supply high currents at a very low voltage and keep voltage swing inside a small tolerance interval against large variations of load currents. This is done by designing a digital controller that allows the VRM to behave {{as much as possible}} as an ideal voltage source with a very small series resistor. This can be achieved through different possible choices of controlling schema. We show that, considering also effects due to parasitics, controlling schema reflect on stability and on the <b>admissible</b> <b>minimum</b> size of the capacitor bank filtering the VRM output voltage. Moreover we show how the output impedance of the VRM degrades versus lowering of the bank size. This is done by resorting to a general and efficient technique, already presented in the literature, allowing to (i) determine the periodic steady state behaviour of these circuits together with their stability and (ii) derive time varying transfer functions such as, for example, the output impedance of VRMs...|$|R
40|$|In Roman Baths (e. g. the Baths of Diocletian in Rome) the Romans {{employed}} groin vaults {{of great}} dimension. The maximum span {{is more than}} 20 m. The central body of the structure is made {{of a series of}} seven aisles with semicircular barrel vaults intersecting three aisles; outer groin vaults of minor dimensions provide counteraction of thrust of central vaults. In the aim of structural conservation of ancient wide span vaulted halls, simple tools of analysis are still lacking, despite the many sophisticated computational methods now available; due to geometrical and material complexity the theme is not commonly faced in technical lit-erature. In this paper, we study the collapse behavior of cross vaults, damaged or undamaged, under horizon-tal static action; the effects of foundation settlements are also analyzed. In the present modelling, masonry is discretized as a system of interacting rigid blocks in no-tension and frictional contact. The computational code used consists of an optimization algorithm which, based on the theorems of non-standard Limit Analysis (in the presence of non-associative rules), searches for the <b>minimum</b> statically <b>admissible</b> load factor ensuring a kinematically admissible collapse mechanism. Although the presence of friction makes the problem non-convex, non-linear and deprived of the uniqueness of solution, the possibility to detect a ‘safe' optimal solu-tion is guaranteed by a quasi-feasible initial estimate of the unknowns, corresponding to the solution of a sys-tem with dilatancy instead of friction (Linear Programming – LP). To simplify the study only (LP) approach is here used. The main difficult consists in a suitable description of the 3 D geometry and of the proper geom-etry of the joints, varying from a joint to the other. The effectiveness of the proposed approach is shown through several examples...|$|R
40|$|Abstract: The Main Boundary Thrust {{in northwestern}} Pakistan is a floor thrust along which a thrust system {{incorporating}} Precambrian and Phanerozoic rocks of the Kala Chitta and Attock-Cherat Ranges was emplaced over Cenozoic strata {{of the northern}} Kohat and Potwar Plateaus. The MBT and successive thrusts toward the foreland are interpreted as low angle to fiat decollement thrusts at 8 - 10 km depths that bound thrust sheets with large lateral <b>dimensions.</b> <b>Admissible</b> cross sections indicate over 60 km of shortening along the largely pre-Palaeocene Khairabad, Cherat, and Hissartang thrusts within the MBT allochthon. In the toe of the MBT allochthon, contraction ismanifest at the surface as tight folds in largely Mesozoic and lower Tertiary strata, decoupled in the upper third of the allochthon. Displacement of the MBT is estimated at over 40 km {{and south of the}} MBT, shortening is interpreted along blind thrusts accompanied by shallow (0 - 3 km deep) backthrusting. The MBT allochthon was deformed prior to its low angle displacement over the northern Kohat and Potwar Plateaus, and folded and eroded during or after its emplacement. The MBT thrust front may participate in its own later deformation, but not as an emergent low angle thrust hrough the Quaternary...|$|R
40|$|The {{subject of}} this master thesis is {{shrinkage}} estimators for the location parameter of an elliptically symmetric distribution in a general linear model. The theory regarding these type of estimators has been introduced by James and Stein (1961), while Stein (1956) proved that the usual least square estimator (lse) is not <b>admissible</b> when the <b>dimension</b> of the parameter space is greater than 2. Ever since, manifold generalizations have taken place that refer to domination over lse and performance of this class of estimators. This project covers part of this theory from 1985 to 1994 and is divided in 5 sections. In section 1, we study elliptically symmetric distributions, give characterizations and prove many of their interesting properties. In section 2, we present a class of shrinkage estimators for the location parameter of a multidimensional normal distribution. We offer a condition of domination over lse, submitting any analytical properties of the shrinkage function. In section 3, we generalize our previous results under elliptical symmetry for a general quadratic loss. Section 4 refers to shrinkage estimators for the location parameter of an elliptically symmetric distribution where the shrinkage function {{is replaced by a}} differentiable vector function. Finally, in section 5 we present two applications of shrinkage estimators (particularly Steins estimator) on rainfall data in co-operation with IACM-FORTH...|$|R
40|$|AbstractLet X be a Tychonoff space, H(X) {{the group}} of all self-homeomorphisms of X with the usual {{composition}} and e:(f,x) ∈H(X) ×X→f(x) ∈X the evaluation function. Topologies on H(X) providing continuity of the evaluation function are called admissible. Topologies on H(X) compatible with the group operations are called group topologies. Whenever X is locally compact T 2, there is the <b>minimum</b> among all <b>admissible</b> group topologies on H(X). That can be described simply as a set-open topology, further agreeing with the compact-open topology if X is also locally connected. We show the same result in two essentially different cases of rim-compactness. The former one, where X is rim-compact T 2 and locally connected. The latter one, where X agrees with the rational number space Q equipped with the euclidean topology. In the first case the minimal admissible group topology on H(X) is the closed-open topology determined by all closed sets with compact boundaries contained in some component of X. Moreover, whenever X is also separable metric, it is Polish. In the rational case the minimal admissible group topology on H(Q) is just the closed-open topology. In both cases the minimal admissible group topology on H(X) is {{closely linked to the}} Freudenthal compactification of X. The Freudenthal compactification in rim-compactness plays a key role as the one-point compactification does in local compactness. In the rational case we investigate whether the fine or Whitney topology on H(Q) induces an admissible group topology on H(Q) stronger than the closed-open topology...|$|R
40|$|Let X be a Tychonoff space, H(X) {{the group}} of all self-homeomorphisms of X with the usual {{composition}} and e : (f, x) ∈ H(X) × X→f (x) ∈ X the evaluation function. Topologies on H(X) providing continuity of the evaluation function are called admissible. Topologies on H(X) compatible with the group operations are called group topologies. Whenever X is locally compact T 2, there is the <b>minimum</b> among all <b>admissible</b> group topologies on H(X). That can be described simply as a set-open topology, further agreeing with the compact-open topology if X is also locally connected. We show the same result in two essentially different cases of rim-compactness. The former one, where X is rim-compact T 2 and locally connected. The latter one, where X agrees with the rational number space Q equipped with the euclidean topology. In the first case the minimal admissible group topology on H(X) is the closed-open topology determined by all closed sets with compact boundaries contained in some component of X. Moreover, whenever X is also separable metric, it is Polish. In the rational case the minimal admissible group topology on H(Q) is just the closed-open topology. In both cases the minimal admissible group topology on H(X) is {{closely linked to the}} Freudenthal compactification of X. The Freudenthal compactification in rim-compactness plays a key role as the one-point compactification does in local compactness. In the rational case we investigate whether the fine or Whitney topology on H(Q) induces an admissible group topology on H(Q) stronger than the closed-open topology...|$|R
40|$|The {{location}} and identification of human settlements {{is a key}} information in any assessment related to human security and safety decision process, from the preparedness to and mitigation of natural hazards to postdisaster response and reconstruction. The human settlements {{can be defined as}} infrastructures build on earth surface for living, trading or any social activity. In order to cover and analyze large areas where scarce and heterogeneous data may exist, the automatic information extraction from Earth Observation is the only sustainable strategy. Several studies about the detection of built-up presence from high resolution satellite were performed during the past few years. Among the various methods, a built-up index aggregating anisotropic textural co-occurence measures (PanTex) was demonstrated to be a robust indicator, where 54 high resolution optical images representing cities distributed around the world are processed. The validation of the built-up presence index emphasized its robustness against the type of constructions, the sun angles and the seasons of acquisitions. In this paper, we propose an alternative procedure for the calculation of built-up presence index from panchromatic high resolution satellite images. While radiometric descriptors of human settlements are highly variable across the world and with illumination conditions, the shape of settlements often contains right angles. This property being more stable, we propose a built-up index based on the density of corners. The index is obtained in two steps: - corners are detected by multi scale Harris detector based on differential morphological decompositions; - corners are spatially aggregated to form a density of corners, which is the built-up index. The differential morphological decomposition is a scale-space representation of the image, where image elements are separated by their scales. Then, the Harris corner indicator, which is highly dependent on a scale parameter, can be adapted to a set of scales. The output of the corner detection is a set of points associated to a scale, which represents the right angles in the image. The corners associated to a scale in the range of <b>admissible</b> settlements <b>dimensions</b> are selected and spatially aggregated to derive a density of corners. The proposed index is extracted from various high resolution panchromatic images and it is compared to the PanTex. In the experimental section, the high correlation between both indicators proves the suitability of the proposed method for consistently detecting built-up presence. Moreover, it gives a new interpretation of the PanTex which is close to a density of corners. Such an observation is critical for understanding the variablity of the PanTex index in between dense urban and industrial areas (industrial areas being composed of large buildings mechanically contain less corners). JRC. G. 2 -Global security and crisis managemen...|$|R
40|$|The offered {{project of}} storage {{facility}} {{allows us to}} simplify and unitise the ground-based infrastructure objects. The storage facility implements a full preparatory cycle of the propellant components (PC) in all parameters. Another problem the developers of complexes of groundbased equipment face now is bulk receipt of PC from manufacturer. The tanks of launch complexes cannot accept such volumes of propellant. It proves {{that there is a}} need to create a storage facility. The facility solves problems concerning the components receipt, temperature preparation, moisture content (drying), gas content, and supply to consumers. For preparation the perspective technologies with low power consumption are used. Receiving the propellant from the dispensing platform is carried out via filters of rough cleaning. Transfer from transport tankage goes using a pump. The received product passes through a gas separator to clean technological gas impurity. To prepare propellant temperature, a technology of cryogenic bubbling by boiling nitrogen is chosen. To improve efficiency of cryogenic bubbling it is advised to use the specialized capacities. Railway <b>dimensions,</b> <b>admissible</b> for the trainload goods across the railroads of Siberia and the Far East, define their sizes. As a drying technology and a gas content preparation the preliminary propellant filtration using vertical electro-separators is chosen to save a space. The chamber vertical electroseparators allow 2 — 3 times increase of dehydration capacity. The article presents calculations to prove that using the chosen cooling and drying technologies is efficient. Prepared PC can be supplied: • to transport-fueling containers (TFC) with the subsequent transportation to the launch complexes either by the railway or by road; • to mobile fuelling tanks, which feed rocket-carrier tanks on arrival at the blast-off; • to transport capacities for transportation to the object outside the cosmodrome (spaceport); • directly to tanks of the rocket-carriers through the pipeline. A direct supply of the prepared component to the rocket-carrier tanks allows a significantly decreasing equipment demand at the launch complex (LC) owing to almost full abandonment of the near-launch storehouses and propellant-feed systems. The pipeline fuel remnants are discharged through the branch in the sump from which fuel can be directed to the storage capacities to have its future preparation again. Existing LC versus LC with the storage facility: - Each LC has a separate near-launch storehouse; - Each near-launch storehouse contains several charges to feed fuel tanks of rocket-carrier; - Each LC has the unique systems of propellant preparation. Application of storage facility: - storage facility allows bulk receipt and storage of high-boiling propellant to meet needs of all LC of the spaceport; - there are all means at the storage facility to prepare high-boiling propellant in all required parameters; - high-boiling propellant can be supplied from the storage facility using both transport capacities, and pipelines directly to the rocket-carrier tanks. Advantages: + possibility to receive and store the bulk high-boiling propellant + decreasing total demand of technological systems + simplified spaceport infrastructure Disadvantages: - decreasing reliability rates caused by a lack of reservation of technological systems</p...|$|R
40|$|The flow-shop {{problem has}} been a problem for {{production}} management ever since the manufacturing industry began. This problem involves the scheduling of production processes for manufacturing orders (referred to as jobs), where each job requires the same processing sequence and thus the same processing facilities (referred to as machines), although the processing time for a job on a machine may be different from that of a different job. The objective is to find a schedule that can complete all jobs in the shortest possible time under the constraints of machine availability. The fact that more than one job may be chosen to be processed on a machine at any one time has made this problem combinatorial in nature. The most unwanted characteristic of a combinatorial problem is the explosive growth of the solution space and hence the computation time. In mathematical terms, the number of possible schedules for n jobs and m machines is (n!) m. Early research in this area is mainly based on Johnson 2 ̆ 7 s theorem (Johnson, 1954). For flow-shop problems with only two machines or three machines with certain characteristics and an arbitrary number of jobs, Johnson 2 ̆ 7 s theorem provides simple procedures for finding an optimal solution. The optimal solution is the sequence of arbitrary number of jobs that will minimise the maximum flow-time (makespan or schedule time). Unfortunately, this theorem does not extend to general three-machine problems or problems involving more than three machines. The aim of this research is to define a new solution procedure (algorithm) for finding an optimal solution for the three-machine flow-shop problem. In this research, a significant contribution has been made by developing an algorithm that can solve flow-shop scheduling problems beyond two machines. The three-machine flow-shop heuristic algorithm we have developed will always produce an optimal sequence. One characteristic of the algorithm developed in this research is its ability to learn past history to improve on the future search processes. The process of learning from past history reduces the disadvantage of the large computation time associated with most flow-shop scheduling algorithms. In the process of developing the algorithm, we have introduced two important improvements related to the way we estimate the initial distance to the goal {{at the start of the}} search and during the search. In with minor modifications the algorithm we have developed can find efficiently multiple optimal solutions when they exist. We envisage that the algorithm developed in this research will contribute significantly to the manufacturing industry, computer industry, digital data communication and transportation industries. The results of research will be presented in five chapters. In chapter 1 we discuss the basic concepts related to scheduling and we define the special terms used in the field. Then, we discuss the two-machine flow-shop problem and its solution as well as developments to date relating to the three-machine flowshop problem. Finally we consider the development of modern heuristic algorithms for searching, especially the A* algorithm and modifications to A* which make it more efficient. In chapter 2 we gradually develop our new heuristic algorithm for solving the threemachine problem. The new algorithm is based on the Search and Learning A* algorithm and is developed to ensure that the algorithm is optimal and complete. begin with the development of a heuristic algorithm for the two-machine problem and after introducing an important improvement to that algorithm w e are able to extend the algorithm to use with problems involving three machines. As is usually the case with algorithms using heuristic functions, we can improve the performance of the algorithm by choosing a high quality heuristic function which underestimates the <b>minimum</b> makespan (is <b>admissible)</b> but is very close to it. In chapter 3 we analyse and compare a set of heuristic functions suitable for use with three-machine problem. We establish the admissibility of the functions and describe conditions which enable us to choose from amongst them a heuristic function which closest to the minimum makespan. Chapter 3 concludes with a statement of the new heuristic algorithm incorporating improvements developed in chapter 2 as well as guidelines for choosing a high quality heuristic function. In chapter 4 we compare the use of the flow-shop heuristic algorithm developed in chapter 3 with the use of the genetic algorithm described in chapter 1. We discuss other improvements we can apply to the algorithm in terms of memory usage. We modify the flow-shop heuristic algorithm to enable us to find multiple optimal solutions if they exist. We discuss the possibility of applying the algorithm to fourmachine and five-machine flow-shop problems and some possible applications of the algorithm to practical industrial problems. Chapter 5 presents the conclusions and the findings of this research as well as identifying areas for further related research...|$|R

