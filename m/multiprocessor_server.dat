19|77|Public
50|$|Sun-4u1: Sometimes used to {{identify}} the Sun Enterprise 10000 (Starfire) 64-way <b>multiprocessor</b> <b>server</b> architecture. The Starfire is supported by Solaris 2.5.1 onwards.|$|E
50|$|The Cray Superserver 6400, or CS6400, is a {{discontinued}} <b>multiprocessor</b> <b>server</b> {{computer system}} produced by Cray Research Superservers, Inc., {{a subsidiary of}} Cray Research, and launched in 1993. The CS6400 was also sold as the Amdahl SPARCsummit 6400Ehttp://www.spikynorman.dsl.pipex.com/CrayWWWStuff/prfoldercomp/CRAY_AMDAHL_AGMT.950228.txt.|$|E
5000|$|Later on, the POWER5 {{processor}} added enhanced DLPAR capabilities, including micro-partitioning: up to 10 LPARs can be configured per processor, with {{a single}} <b>multiprocessor</b> <b>server</b> supporting a maximum of 254 LPARs (and thus up to 254 independent operating system instances).|$|E
5000|$|Digital Equipment Corporation (DEC) {{for their}} DECstation {{workstations}} and <b>multiprocessor</b> DECsystem <b>servers</b> ...|$|R
50|$|In 1996, Sun {{replaced}} the SPARCserver 1000E and SPARCcenter 2000E models with the Ultra Enterprise 3000, 4000, 5000 and 6000 <b>servers.</b> These are <b>multiprocessor</b> <b>servers</b> {{based on a}} common hardware architecture incorporating the Gigaplane packet-switched processor/memory bus and UltraSPARC-I or II processors. High availability and fault-tolerance features {{are included in the}} X000 systems which are intended for mission-critical applications.|$|R
50|$|One {{technique}} {{of how this}} parallelism is achieved is through multiprocessing systems, computer systems with multiple CPUs. Once reserved for high-end mainframes and supercomputers, small-scale (2-8) <b>multiprocessors</b> <b>servers</b> have become commonplace for the small business market. For large corporations, large scale (16-256) multiprocessors are common. Even personal computers with multiple CPUs have appeared since the 1990s.|$|R
50|$|The Cray S-MP was a <b>multiprocessor</b> <b>server</b> {{computer}} sold by Cray Research from 1992 to 1993. It {{was based}} on the Sun SPARC microprocessor architecture and could be configured with up to eight 66 MHz BIT B5000 processors. Optionally, a Cray APP matrix co-processor cluster could be added to an S-MP system.|$|E
5000|$|The S-MP was a SPARC-based <b>multiprocessor</b> <b>server</b> (based on the Model 500); the APP an i860-based matrix co-processor array. After [...] CRI {{purchased}} FPS, {{it changed}} the group's direction by making them Cray Research Superservers, Inc., later becoming the Cray Business Systems Division; however the S-MP architecture was not developed further, instead it {{was replaced by the}} Cray Superserver 6400, (CS6400) which was derived indirectly from a collaboration between Sun Microsystems and Xerox PARC.|$|E
50|$|The Sun Enterprise 450 is a {{rack-mountable}} entry-level <b>multiprocessor</b> <b>server</b> {{launched in}} 1997, capable {{of up to}} four UltraSPARC II processors. The Sun Enterprise 250 is a two-processor version launched in 1998. These were later joined by the Enterprise 220R and Enterprise 420R rack-mount servers in 1999. The 220R and 420R models are respectively based on the motherboards of the Ultra 60 and Ultra 80 workstations. The 250 {{was replaced by the}} Sun Fire V250, the 450 by the Sun Fire V880. The 220R was superseded by the Sun Fire 280R and the 420R by the Sun Fire V480.|$|E
40|$|Database {{applications}} such as online transaction processing (OLTP) and decision support systems (DSS) constitute the largest and fastest-growing segment {{of the market for}} <b>multiprocessor</b> <b>servers.</b> However, most current system designs have been optimized to perform well on scientific and engineering workloads. Given the radically different behavior of database workloads (especially OLTP), it is important to re-evaluate key system design decisions {{in the context of this}} important class of applications...|$|R
50|$|After Silicon Graphics {{acquired}} Cray Research in 1996, {{this business}} unit {{along with the}} CS6400 product line were sold to Sun Microsystems. This was a great strategic mistake by SGI, as Cray were developing the Starfire system at the time, this being launched by Sun as the very successful Ultra Enterprise 10000 <b>multiprocessor</b> <b>servers.</b> These systems allowed Sun to become a first tier vendor in the large server market which Silicon Graphics never achieved.|$|R
40|$|In this paper, {{we present}} Deadline Fair Scheduling (DFS), a proportionate-fair CPU {{scheduling}} algorithm for <b>multiprocessor</b> <b>servers.</b> A particular {{focus of our}} work is to investigate practical issues in instantiating proportionatefair (P-fair) schedulers into conventional operating systems. We show via a simulation study that characteristics of conventional operating systems such as the asynchrony in scheduling multiple processors, frequent arrivals and departures of tasks, and variable quantum durations can cause proportionate-fair schedulers to become non-workconserving...|$|R
50|$|The DEC 7000 AXP and DEC 10000 AXP are {{a series}} of {{high-end}} <b>multiprocessor</b> <b>server</b> computers developed and manufactured by Digital Equipment Corporation, introduced on 10 November 1992 (although the DEC 10000 AXP was not available until the following year). These systems formed part of the first generation of systems based on the 64-bit Alpha AXP architecture and at the time of introduction, ran Digital's OpenVMS AXP operating system, with DEC OSF/1 AXP available in March 1993. They were designed in parallel with the VAX 7000 and VAX 10000 minicomputers, and are identical except for the processor module(s) and supported bus interfaces. A field upgrade from a VAX 7000/10000 to a DEC 7000/10000 AXP was possible by means of swapping the processor boards.|$|E
40|$|Published by the IEEE Computer Society Simulating {{a single}} CPU is {{typically}} {{thousands of times}} slower than the actual CPU. Full-system multiprocessor simulation, which involves simulating many CPUs, peripherals, and other system components on a single host, compounds the slowdown by another factor of 10 to 100. In other words, multiprocessor simulation is up to a million times slower than real hardware. This speed difference leads to prohibitively long turnaround times for simulating complete computer benchmarks—in particular, <b>multiprocessor</b> <b>server</b> benchmarks. (These benchmarks are often longer than thei...|$|E
40|$|Abstract. Existing network testbeds can enable {{developers}} {{to evaluate the}} performance of different routing protocols in a network and help students to concepts of routing protocols by allowing them carrying out projects on it, but they are either limited in features or expensive to establish and manage. To address this problem, this paper presents ARTNet- A Real-Time Testbed for Routing Network – which supports almost all the popular routing protocols for typical applications in a cost-effective manner. ARTNet has been implemented on a <b>multiprocessor</b> <b>server</b> for users to create and manage their routing networks. Performance and functionality evaluations on the ARTNet platform show {{that it is a}} promising approach...|$|E
40|$|Many future shared-memory <b>multiprocessor</b> <b>servers</b> {{will both}} target {{commercial}} workloads and use highly-integrated "glueless" designs. Implementing low-latency cache coherence in these systems is difficult, because traditional approaches either add indirection for common cache-to-cache misses (directory protocols) or require a totally-ordered interconnect (traditional snooping protocols). Unfortunately, totally-ordered interconnects {{are difficult to}} implement in glueless designs. An ideal coherence protocol would avoid indirections and interconnect ordering; however, such an approach introduces numerous protocol races {{that are difficult to}} resolve...|$|R
40|$|The {{proliferation}} of <b>multiprocessor</b> <b>servers</b> and multithreaded applications {{has increased the}} demand for high-performance synchronization. Traditional scheduler-based locks incur the overhead of a full context switch between threads and are thus unacceptably slow for many applications. Spin locks offer low overhead, but they either scale poorly (test-and-set style locks) or handle preemption badly (queue-based locks). Previous work has shown how to build preemption-tolerant locks using an extended kernel interface, but such locks are neither portable to nor even compatible with most operating systems...|$|R
40|$|Brazos {{is a third}} {{generation}} distributed shared memory (DSM) system designed for x 86 machines running Microsoft Windows NT 4. 0. Brazos is unique among existing systems in its use of selective multicast, a software-only implementation of scope consistency, and several adaptive runtime performance tuning mechanisms. The Brazos runtime system is multithreaded, allowing the overlap of computation with the long communication latencies typically associated with software DSM systems. Brazos also supports multithreaded user-code execution, allowing programs {{to take advantage of}} the local tightly-coupled shared memory available on <b>multiprocessor</b> PC <b>servers,</b> while transparently interacting with remote "virtual" shared memory. Brazos currently runs on a cluster of Compaq Proliant 1500 <b>multiprocessor</b> <b>servers</b> connected by a 100 Mbps FastEthernet. This paper describes the Brazos design and implementation, and compares its performance running five scientific applications to the performance of Solari [...] ...|$|R
40|$|WWW-based {{information}} service has grown enormously {{during the last}} few years, and major performance bottlenecks have been caused by WWW server and Internet bandwidth inadequacies. Augmenting a server with multiprocessor support and shifting computation to client-site machines can substantially improve system response times and for some applications, it may also reduce network bandwidth requirements. In this paper, we propose adaptive scheduling techniques that optimize the use of a <b>multiprocessor</b> <b>server</b> with client resources by predicting demands of requests on I/O, CPU and network capabilities. We also provide a performance analysis under simplified assumptions for understanding the impact of system loads and network bandwidth when using our scheduling strategy. Finally we report preliminary experimental results to examine the system performance and verify the usefulness of the analytic model...|$|E
40|$|During the DAΦNE {{commissioning}} and run operations the Control System {{has been}} continuously evolving {{in order to}} fulfill the user requirements and the needs of a complete accelerator management. The original structure of distributed CPUs relaying on a central shared memory proved to be scalable and suitable for adding functionality "on the fly". Also the choice of a commercial software environment for all the control tasks demonstrated to be valid and allowed redesigning the user level with no worries for porting all the developed software. Console applications have been moved from personal computers to Force ® VME embedded processors and user interfaces now runs on a Sun ® <b>multiprocessor</b> <b>server</b> connected to many lightweight SunRay ® terminals. A comprehensive Control System evolution history is reported...|$|E
40|$|This paper {{presents}} the task model, instruction set, reasoning scheme, software infrastructure, {{as well as}} the experimental results, of a new distributed semantic network system. Unlike the synchronous and static marker passing algorithm previously used for parallel semantic network design, our system operates asynchronously, supporting knowledge sharing, dynamic load balancing and duplicate checking. To better the performance in distributed environments, the system has two collaborating components: the slave module, which performs task execution; and the host module, which interacts with the user and processes the information for the slaves. Our current implementation focuses on path-based knowledge inferences, using ANSI C and the MPICH-G 2 with flex lexical analyzer and the yacc parser generator. Tests of individual components have been performed on a SUN <b>multiprocessor</b> <b>server.</b> The experiments demonstrate promising speedups...|$|E
40|$|We {{describe}} {{how we have}} parallelized Python, an interpreted object oriented scripting language, {{and used it to}} build an extensible message-passing molecular dynamics application for the CM- 5, Cray T 3 D, and Sun <b>multiprocessor</b> <b>servers</b> running MPI. This allows us to interact with large-scale message-passing applications, rapidly prototype new features, and perform application specific debugging. It is even possible to write message passing programs in Python itself. We describe some of the tools we have developed to extend Python and results of this approach. ...|$|R
40|$|The authors {{describe}} {{how they have}} parallelized Python, an interpreted object oriented scripting language, {{and used it to}} build an extensible message-passing C/C++ applications for the CM- 5, Cray T 3 D, and Sun <b>multiprocessor</b> <b>servers</b> running MPI. Using a parallelized Python interpreter, it is possible to interact with large-scale parallel applications, rapidly prototype new features, and perform application specific debugging. It is even possible to write message passing programs in Python itself. The authors describe some of the tools they have developed to extend Python and applications of this approach...|$|R
50|$|Pentium 4 CPUs {{introduced}} the SSE2 and, in the Prescott-based Pentium 4s, SSE3 instruction sets to accelerate calculations, transactions, media processing, 3D graphics, and games. Later versions featured Hyper-Threading Technology (HTT), a feature {{to make one}} physical CPU work as two logical CPUs. Intel also marketed a version of their low-end Celeron processors based on the NetBurst microarchitecture (often referred to as Celeron 4), and a high-end derivative, Xeon, intended for <b>multiprocessor</b> <b>servers</b> and workstations. In 2005, the Pentium 4 was complemented by the Pentium D and Pentium Extreme Edition dual-core CPUs.|$|R
40|$|The Host Identity Protocol (HIP) was {{introduced}} {{almost a decade}} ago. There are three interoperating software implementations. HIP provides mobility and multi-homing in a secure way to the Internet hosts. Several studies evaluated the duration of HIP association establishment and mobility updates. In this paper, we perform stress testing of available HIP implementations on a <b>multiprocessor</b> <b>server.</b> When we started, the best result was only 12 connections per second, but after providing feedback to the developers, the stability of HIP implementations has improved and a new version is able to accept 112 connections per second. Moreover the number should increase up to 800 on an eight-core server by introducing more efficient multithreading support to the HIP implementations. This would make HIP performance comparable with current commercial SSL/TLS implementations...|$|E
40|$|Abstract — We {{present a}} fast {{parallel}} simulator {{to evaluate the}} impact of different error control methods {{on the performance of}} networks-on-chip (NoCs). The simulator, implemented with message passing interface (MPI) language to exploit <b>multiprocessor</b> <b>server</b> environments, models characteristics of intellectual property (IP) cores, network interfaces (NIs), and routers. Moreover, different error control schemes can be inserted to the simulator in a plug-and-play manner for evaluation. A highly tunable fault injection feature allows estimation of the latency, throughout and energy consumption of an error control scheme against different fault types, fault injection locations and traffic injections. Case NoC studies are presented to demonstrate how the simulator assists in NoC design space exploration, evaluating the impact of different error control methods on NoC performance, and providing design guidelines for NoCs with error control capabilities. Index Terms—Fault tolerance, reliability, performance analysis and design aid, simulator, error control, networks-on-chip. I...|$|E
40|$|One of {{the main}} reasons for the {{difficulty}} of hardware verification is that hardware platforms are typically nondeterministic at clock-cycle granularity. Uninitialized state elements, I/O, and timing variations on high-speed buses all introduce nondeterminism that causes different behavior on different runs starting from the same initial state. To improve our ability to debug hardware, we would like to completely eliminate nondeterminism. This paper introduces the Cycle-Accurate Deterministic REplay (CADRE) architecture, which cost-effectively makes a boardlevel computer cycle-accurate deterministic. We characterize the sources of nondeterminism in computers and show how to address them. In particular, we introduce a novel scheme to ensure deterministic communication on source-synchronous buses that cross clock-domain boundaries. Experiments show that CADRE on a 4 -way <b>multiprocessor</b> <b>server</b> enables cycle-accurate deterministic execution of one-second intervals with modest buffering requirements (around 200 MB) and minimal performance loss (around 1 %). Moreover, CADRE has modest hardware requirements. 1...|$|E
50|$|Sun Enterprise is a {{range of}} UNIX server {{computers}} produced by Sun Microsystems from 1996 to 2001. The line was launched as the Sun Ultra Enterprise series; the Ultra prefix was dropped around 1998. These systems {{are based on the}} 64-bit UltraSPARC microprocessor architecture and related to the contemporary Ultra series of computer workstations. Like the Ultra series, they run Solaris. Various models, from single-processor entry-level servers to large high-end <b>multiprocessor</b> <b>servers</b> were produced. The Enterprise brand was phased out in favor of the Sun Fire model line from 2001 onwards.|$|R
5000|$|<b>Multiprocessor</b> {{workstations}} and <b>servers</b> {{using the}} KBus 64-bit inter-processor bus: ...|$|R
40|$|The {{design of}} high {{performance}} computing systems requires many design {{decisions based on}} performance, cost, power consumption, and possibly other criteria. Decisions made in the early, high-level speci cation phase are critical to developing a successful product. We describe a methodology which allows the architect to explore alternatives at all design levels for di erent technology options. We use our approach to explore tradeo s {{in the design of}} high performance <b>multiprocessor</b> <b>servers</b> using next-generation VLSI technology. The performance {{of a wide range of}} machine con gurations varying in architectural options and technology parameters is explored for several SPEC benchmarks. ...|$|R
40|$|This paper {{presents}} {{the design and}} performance of a new parallel graphics renderer for 3 -D images. This renderer {{is based on an}} adaptive supersampling approach that works for time/space-efficient execution on two classes of parallel computers. Our rendering scheme takes sub-pixel supersamples only along polygon edges. This leads to a significant reduction in rendering time and in buffer memory requirement. Furthermore, we offer a balanced rasterization of all transformed polygons. Experimental results prove these advantages on both a shared-memory SGI <b>multiprocessor</b> <b>server</b> and a Unix cluster of Sun workstations. We reveal performance effects of the new rendering scheme on subpixel resolution, polygon number, scene complexity, and memory requirements. The balanced parallel renderer demonstrates scalable performance with respect to increase in graphics complexity and in machine size. Our parallel renderer outperforms Crow’s scheme in benchmark experiments performed. The improvements are made in three fronts: (i) reduction in rendering time, (ii) higher efficiency with balanced workload, and (iii) adaptive to available buffer memory size. The balanced renderer can be more cost-effectively imbedded within many 3 -D graphics algorithms, such as those for edge smoothing and 3 -D visualization. Our parallel renderer is MPI-coded, offering high portability and cross-platform performance. These advantages can greatly improve the QoS in 3 -D imaging and in real-time interactive graphics...|$|E
40|$|With {{the latest}} {{high-end}} computing nodes combining shared-memory multiprocessing with hardware multithreading, new scheduling policies {{are necessary for}} workloads consisting of multithreaded applications. The use of hybrid multiprocessors presents schedulers {{with the problem of}} job pairing, i. e. deciding which specific jobs can share each processor with minimum performance penalty, by running on different execution contexts. Therefore, scheduling policies are expected to decide not only which job mix will execute simultaneously across the processors, but also which jobs can be combined within each processor. This paper addresses the problem by introducing new scheduling policies that use run-time performance information to identify the best mix of threads to run across processors and within each processor. Scheduling of threads across processors is driven by the memory bandwidth utilization of the threads, whereas scheduling of threads within processors is driven by one of three metrics: bus transaction rate per thread, stall cycle rate per thread, or outermost level cache miss rate per thread. We have implemented and experimentally evaluated these policies on a real <b>multiprocessor</b> <b>server</b> with Intel Hyperthreaded processors. The policy using bus transaction rate for thread pairing achieves an average 13. 4 % and a maximum 28. 7 % performance improvement over the Linux scheduler. The policy using stall cycle rate for thread pairing achieves an average 9. 5 % and a maximum 18. 8 % performance improvement. The average and maximum performance gains of the policy using cache miss rate for thread pairing are 7. 2 % and 23. 6 % respectively. 1...|$|E
40|$|The second {{generation}} of the Digital Equipment Corp. (DEC) DECchip Alpha AXP microprocessor {{is referred to as}} the 21164. From the viewpoint of numerically-intensive computing, the primary difference between it and its predecessor, the 21064, is that the 21164 has twice the multiply/add throughput per clock period (CP), a maximum of two floating point operations (FLOPS) per CP vs. one for 21064. The AlphaServer 8400 is a shared-memory <b>multiprocessor</b> <b>server</b> system that can accommodate up to 12 CPUs and up to 14 GB of memory. In this report we will compare single processor performance of the 8400 system with that of the International Business Machines Corp. (IBM) RISC System/ 6000 POWER- 2 microprocessor running at 66 MHz, the Silicon Graphics, Inc. (SGI) MIPS R 8000 microprocessor running at 75 MHz, and the Cray Research, Inc. CRAY J 90. The performance comparison is based on a set of Fortran benchmark codes that represent a portion of the Los Alamos National Laboratory supercomputer workload. The advantage of using these codes, is that the codes also span a wide range of computational characteristics, such as vectorizability, problem size, and memory access pattern. The primary disadvantage of using them is that detailed, quantitative analysis of performance behavior of all codes on all machines is difficult. One important addition to the benchmark set appears for the first time in this report. Whereas the older version was written for a vector processor, the newer version is more optimized for microprocessor architectures. Therefore, we have for the first time, an opportunity to measure performance on a single application using implementations that expose the respective strengths of vector and superscalar architecture. All results in this report are from single processors. A subsequent article will explore shared-memory multiprocessing performance of the 8400 system...|$|E
40|$|Java-based middleware, and {{application}} servers in particular, are rapidly gaining importance {{as a new}} class of workload for commercial <b>multiprocessor</b> <b>servers.</b> SPEC has recognized this trend with its adoption of SPECjbb 2000 and the new SPECjAppServer 2001 (ECperf) as standard benchmarks. Middleware, by definition, connects other tiers of server software. SPECjbb is a simple benchmark that combines middleware services, a simple database server, and client drivers into a single Java program. ECperf more closely models commercial middleware by using a commercial application server and separate machines for the different tiers. Because it is a distributed benchmark, ECperf provides an opportunity for architects to isolate the behavior of middleware...|$|R
40|$|The Internet {{has made}} {{database}} management systems and Web servers integral parts of today’s business and communications infrastructure. These and other commercial transaction-processing applications work with critical personal and business data—storing it, {{providing access to}} it, and manipulating it. As dependence on these applications increases, so does the need for them to run reliably and efficiently. Our group at the University of Wisconsin (www. cs. wisc. edu/multifacet/) researches innovative {{ways to improve the}} performance of the <b>multiprocessor</b> <b>servers</b> that run these important commercial applications. Execution-driven simulation is a design evaluation tool that models system hardware. These simulations capture actual program behavior and detailed system interactions. They are more flexible and less expensive than hardware prototypes, and they model important system details more accurately than analytic modeling does. However, the combination of large systems and demanding workloads is difficult to simulate, especially on the inexpensive machines available to most researchers. Commercial workloads, unlike simpler workloads, rely heavily on operating system services such as input/output, process scheduling, and interprocess communication. To run commercial workloads correctly, simulators must model these services. In addition, <b>multiprocessor</b> <b>servers</b> introduce the challenges of interactions among processors, large main memories, and many disks. To make effective use of limited simulation resources, researchers must balance three goals: • developing a representative approximation of large workloads, • achieving tractable simulation times, and • simulating a sufficient level of timing detail. We developed a simulation methodology to achieve these goals. Our methodology uses multiple simulations, pays careful attention to scaling effects on workload behavior, and extends VirtutechAB’s Simics full-system functional simulator with detailed timing models...|$|R
40|$|Abstract [...] -The paper {{presents}} {{an approach to}} performance analysis of heterogeneous parallel algorithms. As a typical heterogeneous parallel algorithm is just a modification of some homogeneous one, {{the idea is to}} compare the heterogeneous algorithm with its homogeneous prototype, and to assess the heterogeneous modification rather than analyse the algorithm as an isolated entity. A criterion of optimality of heterogeneous parallel algorithms is suggested. A parallel algorithm of matrix multiplication on heterogeneous clusters is used to illustrate the proposed approach. 1. Introduction. Heterogeneous networks of computers are a promising distributed-memory parallel architecture. In the most general case, a heterogeneous network includes PCs, workstations, <b>multiprocessor</b> <b>servers,</b> clusters of workstations, and even supercomputers. Unlike traditional homogeneous parallel platforms, the heterogeneous paralle...|$|R
