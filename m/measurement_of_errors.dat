7|10000|Public
40|$|Corners play {{important}} roles in high level image understanding. They are the main features in many 2 D or 3 D image models associated with image understanding algorithms. The Bayesian corner detection method inputs a sequence of row-column pairs along an arc and outputs the corner positions and the corner included angles that maximize the a posteriori probability. Experiments on artificially generated sequences permit the <b>measurement</b> <b>of</b> <b>errors</b> of the estimated corner positions and included angles versus different noise perturbations, angles and line lengths respectively. ...|$|E
40|$|A novel {{scheme to}} obtain the optimum tissue heating {{condition}} during hyperthermia treatment is proposed. To do this, {{the effect of the}} controllable overall heat transfer coefficient of the cooling system is investigated. An inverse problem by a conjugated gradient with adjoint equation is used in our model. We apply the finite difference time domain method to numerically solve the tissue temperature distribution using Pennes bioheat transfer equation. In order to provide a quantitative <b>measurement</b> <b>of</b> <b>errors,</b> convergence history of the method and root mean square of errors are also calculated. The effects of heat convection coefficient of water and thermal conductivity of casing layer on the control parameter are also discussed separately...|$|E
40|$|An {{established}} {{method of}} room-temperature interferometric null testing of mirrors having simple shapes (e. g., flat, spherical, or spheroidal) has been augmented to enable <b>measurement</b> <b>of</b> <b>errors</b> {{in the surface}} figures of off-axis, non-axisymmetric, aspherical mirrors when the mirrors are located inside cryogenic vacuum chambers. The established method {{involves the use of}} a computer-generated hologram (CGH), functionally equivalent to a traditional null lens, to modify the laser beam of an imaging interferometer to obtain a reference wavefront that matches the ideal surface figure of a mirror under test. The CGH is inserted at the appropriate position and orientation in the optical path of the imaging interferometer, which, in turn, is appropriately positioned and oriented with respect to the mirror under test. Deviations of the surface figure of the mirror from the ideal surface figure manifest themselves as interference fringes. Interferograms are recorded and analyzed to deduce figure errors...|$|E
40|$|Deficits in {{cervical}} proprioception {{have been}} identified in subjects With neck pain through the Measure of head repositioning accuracy (HRA). Nevertheless there appears to be 110 general Consensus regarding the construct <b>of</b> <b>measurement</b> <b>of</b> <b>error</b> used for calculating HRA. This study investigated four different mathematical methods <b>of</b> <b>measurement</b> <b>of</b> <b>error</b> to determine if there were any differences in their ability to discriminate between a control group and subjects with a whiplash associated disorder...|$|R
40|$|Measurement {{error is}} an {{essential}} part <b>of</b> nonsampling <b>error.</b> Mod-eling <b>of</b> <b>measurement</b> <b>error</b> has many applications such as to assess accuracy of survey results. Modeling <b>of</b> <b>measurement</b> <b>error</b> should be assessed based on types of data sets. In the previous investigations, modeling <b>of</b> <b>measurement</b> <b>error</b> for continues data is studied. In this paper, a model <b>of</b> <b>measurement</b> <b>error</b> for categorical data in surveys is proposed...|$|R
40|$|The {{value of}} {{research}} in any topic area turns on its validity. Patient safety research has revealed—or, at least, given renewed urgency to—a raft of methodological issues. The meaning and thus the value of empirical {{research in this field}} is contingent on getting the methodology right. The need for good methods for the <b>measurement</b> <b>of</b> <b>error</b> is necessary whenever an inference is intended and, since inferences {{lie at the heart of}} research and management, there is a huge need to understand better how to make measurements that are meaningful, precise, and accurate. In this paper we consider issues relating to the <b>measurement</b> <b>of</b> <b>error</b> and the need for more research...|$|R
40|$|Golf putting {{accuracy}} is often evaluated {{by measuring the}} distance that the ball finishes from the hole. However, {{accuracy is}} a function of line and length, and distance-from-hole measures confound these two factors. A scoring system for evaluating putting accuracy is described that enables the efficient <b>measurement</b> <b>of</b> <b>errors</b> in line and length. A camera placed above the hole takes digital photographs of the final position of the ball. A custom-developed program written in the National Instruments LabVIEW graphical programming language derives a variety of accuracy measures from these photographs, including distance from the hole, angle of error, distance short or long from the hole, and distance left or right from the hole. Evaluation of the system indicated that the measures were as accurate as manual measurements and were reliable when rescored on separate occasions. The camera-based scoring system presents a number of advantages in the evaluation of putting accuracy and may be extended to examine performance in other sports. Griffith Health, School of Applied PsychologyFull Tex...|$|E
40|$|Abstract. [Purpose] Concurrent {{feedback}} is more detrimental for long-term retention of motor skills because learners depend on accessible visual information provided {{in parallel with}} movements. However, visual informa-tion is not always accessible. Furthermore, the effects of concurrent feedback vary with aspects of the task being performed. We investigated the effects of inaccessible visual feedback used concurrently or terminally, focusing on aspects of movement. [Subjects and Methods] Fourteen subjects were quasi-randomly assigned to either a concur-rent feedback group or a terminal feedback group. They practiced a task that involved right shoulder flexion with a specific acceleration. Learning achievements were assessed by <b>measurement</b> <b>of</b> <b>errors</b> in movement duration, peak timing, and strength. [Results] Regarding errors in movement duration, the concurrent feedback group was superior to the terminal feedback group during the midterm and final sessions. Regarding errors in peak timing, learning occurred in the concurrent feedback group, {{but not in the}} terminal feedback group because the improvement in performance during practice was inadequate. Regarding errors in peak strength, learning occurred in both groups. [Conclusion] Concurrent visual feedback that is used inaccessibly has learning effects that either equal or surpass those of terminal feedback that is used with inaccessible visual information for all parameters...|$|E
40|$|Thermal {{effects on}} the {{accuracy}} of {{numerically controlled machine tool}} are specially important in the context of unmanned manufacture or under conditions of precision metal cutting. Removal of the operator from the direct control of the metal cutting process has created problems in terms of maintaining accuracy. The objective of this thesis is to study thermal {{effects on the}} accuracy of numerically controlled machine tools. ^ The initial part of the thesis is concerned with the analysis of a hypothetical machine. The thermal characteristics of this machine are studied. Numerical methods for evaluating the errors exhibited by the slides of the machine are proposed and the possibility of predicting thermally induced errors by the use of regresssion equations is investigated. A method for computing the workspace error is also presented. ^ Machine tools are generally made of box type structures. Based on analysis of box type structures, theoretical evidence for the prediction of machine tool errors is presented. The problem of heat flow in a box made of thin plates is studied and analytic solutions are derived. ^ The final part is concerned with the actual <b>measurement</b> <b>of</b> <b>errors</b> on a modern CNC machining center. Thermal influences on the errors is the main objective of the experimental work. Based on experimental evidence, it is shown that thermal effects on the accuracy of machine tools can be predicted by the use of simple regression equations. Finally, numerical methods are used to predict thermal influences on the errors of machine tools. These methods make use of measured values of temperatures for prediction of errors. ^ Based on the evidence presented in this thesis, it is shown conclusively that thermal influences on the errors of machine tools are predictable. Techniques for determining thermal effects on machine tools at a design stage are also presented. ...|$|E
40|$|This bachelor’s thesis {{describes}} {{structure of}} the transport stream MPEG- 2 TS of the DVB-T standard and subjective and objective measuring methods of the video quality. The work also includes the description <b>of</b> the <b>measurement</b> devices and software Quality Analyzer R&S DVQ, measuring decoder ¸ DVMD, MPEG- 2 Realtime Monitor, MPEG- 2 Quality Monitor, which {{were used for the}} measuring the video quality and <b>errors</b> <b>of</b> the MPEG- 2 TS in DVB-T. In the next part the work deals with the <b>measurement</b> and analysis <b>of</b> <b>errors</b> and quality <b>of</b> digital video stream in MPEG- 2 TS. The obtained results from the measurement are presented in tables and graphically evaluated. In this work a twentyfour hour <b>measurements</b> <b>of</b> <b>errors</b> and quality <b>of</b> digital video stream in MPEG- 2 TS were made. The obtained results from these measurements are evaluated and discussed. Finally, a new laboratoy work for the simultaneously <b>measurement</b> <b>of</b> the <b>errors</b> in video stream and video quality of the MPEG- 2 TS stream was proposed...|$|R
50|$|In digital {{transmission}}, the Viterbi error rate, {{also known}} as the Viterbi bit error rate (VBER), is a <b>measurement</b> <b>of</b> <b>error</b> in the transmission of digital Information after the original signal has been corrected for errors and aberrations, usually noise and/or distortion. The value of the VBER is directly linked {{to the quality of the}} Channel Bit Error Rate (CBER), a <b>measurement</b> <b>of</b> the strength & quality of the original signal.|$|R
40|$|Considering {{that the}} absence <b>of</b> <b>measurement</b> <b>error</b> in {{research}} is a rare phenomenon and its effects can be dramatic, we examine the impact <b>of</b> <b>measurement</b> <b>error</b> on propensity score (PS) analysis used to minimize selection bias in behavioral and social nonexperimental studies. A Monte Carlo {{study was conducted to}} explore the effects <b>of</b> <b>measurement</b> <b>error</b> on treatment effect and balance estimates in PS analysis across seven different PS conditioning methods. In general, the results indicate that even low levels <b>of</b> <b>measurement</b> <b>error</b> in the covariates lead to substantial statistical bias in estimates of treatment effects and concomitant reduction in confidence interval coverage across all methods of conditioning on the PS...|$|R
40|$|In modern {{manufacturing}} industry, {{precision components}} are typically produced on Computer Numerical Controlled (CNC) machine tools which translate their accuracy onto machined parts. This accuracy {{is affected by}} a set of different motion errors caused by inherent imperfections in the design and build of the machine, variations in the local environment such as temperature, the cutting process itself and human factors. The reduction of these effects is achieved primarily through design improvements and error compensation techniques. The latter requires detailed knowledge about the existing errors {{in order to deal with}} them effectively. This thesis describes a novel sensor system for <b>measurement</b> <b>of</b> <b>errors</b> caused by deviation in the straightness of Cartesian axes present in the structural loop of most machine tools. Currently there are very few methods available to measure straightness directly, each having advantages and disadvantages when considering simplicity, accuracy and affordability. The proposed system uses a taut wire reference with a novel sensor, a two-point technique for reference error cancellation and software to enable fast and accurate measurement of straightness between any two points of the measured machine’s working volume. The standout features of the sensing system include ultra-low cost and high performance when compared with existing state-of-the-art systems. It is capable of measuring a straightness error as low as 3 μm and takes only 2 s of dwell time between readings, while laser interferometer requires 4 s to perform averaging when measuring the same error. Existing taut wire microscopy is limited by 10 - 20 μm of measured error depending on optics quality and manual reading takes at least 5 s to minimise the human error. Setup time is also different – the new system saves 15 minutes time on 2 m axis and more on longer lengths compared the laser due to simpler reference alignment procedure. Theoretical analysis and practical implementation are followed by detailed performance evaluation experiments carried out under typical manufacturing conditions comprising different machine tools, different axes, measured errors, environmental effects and alternative measuring equipment. Tests cover aspects of accuracy, repeatability and overall system stability providing a complete picture of the system’s capability and the method’s potential which is also supported by uncertainty analysis. In addition to defining setup and measuring procedures, a user-friendly software interface is described and its main units are explained with respect to overall measurement efficiency and setup fault detection...|$|E
3000|$|In this case, {{due to the}} dataset composition, the <b>measurement</b> <b>of</b> the <b>error</b> for age prediciton was {{slightly}} modified as follows: let N [...]...|$|R
30|$|Remark 2 Either {{empirical}} model or survey <b>measurements</b> <b>of</b> NLOS <b>errors</b> {{must be provided}} since the corresponding CRLB is derived based on the PDF <b>of</b> NLOS <b>errors.</b> Many {{empirical model}}s and survey <b>measurements</b> <b>of</b> the PDF <b>of</b> NLOS <b>errors</b> are reported in [31, 32].|$|R
40|$|International audienceAssessing that a {{probability}} of false alarm is below a given significance level {{is a crucial}} issue in watermarking. We pro- pose an iterative and self-adapting algorithm which estimates very low probabilities <b>of</b> <b>error.</b> Some experimental investiga- tions validates its performance for a rare detection scenario where there exists a close form formula of the probability of false alarm. Our algorithm appears to be much quicker and more accurate than a classical Monte Carlo estimator. It even allows the experimental <b>measurement</b> <b>of</b> <b>error</b> exponents...|$|R
3000|$|... hComputation <b>of</b> <b>measurement</b> <b>error</b> <b>of</b> the {{dependent}} variable is done with this formulae: [(MP-P 1) 2 [*]+[*](MP-P 2) 2 [*]+[*](MP-P 3) 2 [*]+[*](MP-P 4) 2 [*]+[*](MP-P 5) 2]/ 5. MP[*]=[*]average 5 plausible values and P 1 to P 5 are the plausible values.|$|R
40|$|AbstractLinear {{regression}} models are studied when {{variables of interest}} are observed in the presence <b>of</b> <b>measurement</b> <b>error.</b> Techniques involving Fourier transforms that lead to simple differential equations with unique solutions {{are used in the}} context of multiple regression. Necessary and sufficient conditions are proven for a random vector <b>of</b> <b>measurement</b> <b>error</b> <b>of</b> the independent variable to be multivariate normal. One characterization involves the Fisher score of the observed vector. A second characterization involves the Hessian matrix of the observed density...|$|R
40|$|Assessing that a {{probability}} of false alarm is below a given significance level {{is a crucial}} issue in watermarking. We propose an iterative and self-adapting algorithm which estimates very low probabilities <b>of</b> <b>error.</b> Some experimental investigations validates its performance for a rare detection scenario where there exists a close form formula of the probability of false alarm. Our algorithm appears to be much quicker and more accurate than a classical Monte Carlo estimator. It even allows the experimental <b>measurement</b> <b>of</b> <b>error</b> exponents. Index Terms — Watermarking, False alarm, Rare event analysis...|$|R
40|$|Measurement is a {{fundamental}} part of all scientific research, and the introduction <b>of</b> <b>errors</b> <b>of</b> different sorts is an inevitable part <b>of</b> the <b>measurement</b> process in epidemiological and clinical research. Despite the ubiquity <b>of</b> <b>measurement</b> <b>error</b> in research, the substantial impacts which measurement error can have on data and subsequent study inferences are frequently overlooked. This review introduces the basic concepts <b>of</b> <b>measurement</b> <b>error</b> that are most relevant {{to the study of}} sexually transmitted infections, and demonstrates the impacts of several of the most common forms <b>of</b> <b>measurement</b> <b>error</b> on study results. A self assessment test and MCQs follow this paper...|$|R
40|$|Linear {{regression}} models are studied when {{variables of interest}} are observed in the presence <b>of</b> <b>measurement</b> <b>error.</b> Techniques involving Fourier transforms that lead to simple differential equations with unique solutions {{are used in the}} context of multiple regression. Necessary and sufficient conditions are proven for a random vector <b>of</b> <b>measurement</b> <b>error</b> <b>of</b> the independent variable to be multivariate normal. One characterization involves the Fisher score of the observed vector. A second characterization involves the Hessian matrix of the observed density. Measurement error model conditional expectation conditional variance...|$|R
40|$|Abstract: This paper {{outlines}} {{a methodology}} for quantitative and objective <b>measurements</b> <b>of</b> <b>errors</b> generated from hydrological modelling algorithms. A number of artificial mathematical surfaces were generated {{and used to}} test geomorphological and hydrological feature modelling algorithms which were found and implemented from literature and available GIS software. The actual output values from these algorithms were compared with the theoretical expectations on the mathematical surfaces so that the standard errors can be computed and their spatial distribution can be mapped...|$|R
40|$|The effects <b>of</b> <b>measurement</b> <b>errors</b> on usual linear {{regression}} estimator are exam-ined. A comparative study is made among the {{linear regression}} estimator, the mean per unit estimator and the ratio estimator {{in the presence}} <b>of</b> <b>measurement</b> <b>errors.</b> Key words: Bias; efficiency; mean square error; observational error. ...|$|R
40|$|This work {{presents}} an automated and dedicated system aiming at the <b>measurement</b> <b>of</b> straightness <b>errors</b> <b>of</b> mechanical components, using an industrial robot. A multi-probe error separation technique {{was used to}} make measurements independent from the coordinate system of the robot. A mathematical model {{that takes into account}} the readings from three sensors was specifically designed for the proposed measurements and produces inspection results by means of the solution of a system of linear equations, in only one operation. Also in this work, a new approach was developed to minimize the influence <b>of</b> the zero-adjustment <b>errors</b> <b>of</b> the sensors, which represent the major source <b>of</b> <b>errors</b> in the separation process. Experimental tests applied to the <b>measurement</b> <b>of</b> straightness <b>errors</b> <b>of</b> mechanical components were accomplished, which demonstrated the effectiveness of the employed methodology...|$|R
40|$|This paper {{discusses}} {{the benefits of}} using generalizabilty theory in lieu of classical test theory. Generalizability theory subsumes and extends the precepts of classical test theory by estimating the magnitude of multiple sources <b>of</b> <b>measurement</b> <b>error</b> and their interactions simultaneously in a single analysis. Since classical test theory examines only one source <b>of</b> <b>measurement</b> <b>error</b> at a time (e. g. occasions, forms, or internal consistency), {{it is not possible}} to estimate the magnitudes of all sources <b>of</b> <b>measurement</b> <b>error</b> and the magnitude <b>of</b> <b>measurement</b> <b>error</b> interaction effects concurrently. As this paper explores the shortcomings of classical test theory and the strengths afforded by using generalizability theory, a small heuristic data set is used to make the discussion concrete. (Contains 9 tables and 28 references.) (SLD) Reproductions supplied by EDRS are the best that can be made from the original document...|$|R
40|$|This paper {{investigates the}} effect that {{covariate}} measurement error has on a conventional treatment effect analysis built on an unconfoundedness restriction that embodies conditional independence restrictions {{in which there is}} conditioning on error free covariates. The approach uses small parameter asymptotic methods to obtain the approximate generic effects <b>of</b> <b>measurement</b> <b>error.</b> The approximations can be estimated using data on observed outcomes, the treatment indicator and error contaminated covariates providing an indication of the nature and size <b>of</b> <b>measurement</b> <b>error</b> effects. The approximations {{can be used in a}} sensitivity analysis to probe the potential effects <b>of</b> <b>measurement</b> <b>error</b> on the evaluation of treatment effects. ...|$|R
30|$|Work time {{measurements}} are {{the starting point}} for any calculation of unit costs of machine exploitation; therefore, the accuracy of evaluating these costs determines the economic effectiveness of technological solutions employed in forest work. The research aimed to determine the level <b>of</b> <b>measurement</b> <b>error</b> <b>of</b> harvester operation times by means of a chronometric method.|$|R
40|$|Abstract. This paper {{suggests}} that the <b>measurement</b> <b>of</b> <b>error</b> between mesh surfaces that are similar can be performed more time-efficiently than the existing methods offer. We presume that evaluation <b>of</b> <b>error</b> between similar surfaces does not require so many sample points and two-way distance measurement. Also we apply a different technique for sample point picking that involves barycentric coordinates. Tests versus other methods show that our method performs faster in cases of identical and similar surfaces, but also produces a less precise result. Thus, it {{is considered to be}} effective in applications that do not need precise measurements, like fast detection of flaws, deformation or overlapping areas...|$|R
40|$|In {{order to}} reduce the <b>measurement</b> <b>error</b> <b>of</b> heat {{transfer}} in water and air side for finned-tube heat-exchanger as little as possible, and design a heat-exchanger test-board measurement system economically, based on the principle <b>of</b> test-board system <b>error</b> analyses and design, the equation <b>of</b> <b>measurement</b> <b>error</b> <b>of</b> heat transfer in air side and water side about orifice meter for the finned-tube heat-exchanger was obtained. This paper studies the major factors that may influence the largest admitted <b>measurement</b> <b>error</b> <b>of</b> <b>measurement</b> instruments for heat transfer in water and air side, and analyzes the degree that water temperature and pressure measurement influence heat transfer in water side, and the degree that wet bulb temperature difference measurement influences heat transfer in air side. Finally, this paper indicates that the key problem of reducing heat transfer in water side is water temperature <b>measurement</b> <b>of</b> the in-out pipe of heat-exchanger, and wet bulb temperature difference is a key to decrease the heat transfer in air side for finned-tube heat-exchanger. This paper gives a theoretical instruction for designing the <b>measurement</b> system <b>of</b> a finned-tube heat-exchanger test-boar...|$|R
40|$|Technology and voltage scaling {{is making}} {{integrated}} circuits increasingly susceptible to failures caused by soft <b>errors.</b> The source <b>of</b> soft <b>errors</b> are temporary hardware faults that alter data and signals in digital circuits. Soft errors are predominately caused by ionizing particles, electrical noise and wear-out effects, {{but may also}} occur {{as a result of}} marginal circuit designs and manufacturing process variations. Modern computers are equipped with a range of hardware and software based mechanisms for detecting and correcting soft errors, as well as other types <b>of</b> hardware <b>errors.</b> While these mechanisms can handle a variety <b>of</b> <b>errors</b> and error types, protecting a computer completely from the effects <b>of</b> soft <b>errors</b> is technically and economically infeasible. Hence, in applications where reliability and data integrity is of primary concern, it is desirable to assess and measure the system's ability to detect and correct soft errors. This thesis is devoted to the problem <b>of</b> measuring hardware <b>error</b> sensitivity  of computer systems. We define hardware error sensitivity as the probability that a hardware error results in an undetected erroneous output. Since the complexity of computer systems makes it extremely demanding to assess the effectiveness <b>of</b> <b>error</b> handling mechanisms analytically, error sensitivity and related measures, e. g., error coverage, are in practice determined experimentally by means of fault injection experiments. The <b>error</b> sensitivity <b>of</b> a computer system depends not only on the design <b>of</b> its <b>error</b> handling mechanism, but also on the program executed by the computer. In addition, <b>measurements</b> <b>of</b> <b>error</b> sensitivity is affected by the experimental set-up, including how and where the errors are injected, and the assumptions about how soft errors are manifested, i. e., the error model. This thesis identifies and investigates six parameters, or sources of variation, that affect <b>measurements</b> <b>of</b> <b>error</b> sensitivity. These parameters consist of two subgroups, those that deal with systems characteristics, namely, (i) the input processed by a program, (ii) the program's source code implementation, (iii) the level of compiler optimization; and those that deal with measurement setup, namely, (iv) the number of bits that are targeted in each experiment, (v) the target location in which faults are injected, (vi) the time of injection. To accurately measure the <b>error</b> sensitivity <b>of</b> a system, one needs to conduct several sets of fault injection experiments by varying different sources of variations. As these experiments are quite time-consuming, it is desirable to improve the efficiency <b>of</b> fault injection-based <b>measurement</b> <b>of</b> <b>error</b> sensitivity. To this end, the thesis proposes and evaluates different error space optimization and error space pruning techniques to reduce the time and effort needed to measure the error sensitivity...|$|R
40|$|The paper {{discusses}} {{the development of}} a device for directional characteristics <b>measurements</b> <b>of</b> RFID tags marked objects. Current procedures for carrying out laboratory tests of UHF RFID devices rely on relatively high involvement of human labour associated with higher risk <b>of</b> <b>measurements</b> <b>errors.</b> The newly developed system that performs great part of manual operations autonomously reduces the risk <b>of</b> <b>measurement</b> <b>errors</b> and enables experiments to be performed faster, more accurate and in repeatable manner...|$|R
40|$|This article {{presents}} {{the problem of}} estimating the population mean using auxiliary information in the presence <b>of</b> <b>measurement</b> <b>errors.</b> A numerical study is made among the proposed estimator, the exponential ratio estimator, Singh and Solanki (2012) estimator and the mean per unit estimator in the presence <b>of</b> <b>measurement</b> <b>errors.</b> Key words: Population mean, Study variate, Auxiliary variates, Mean squared error, Measurement errors, Efficienc...|$|R
40|$|First {{published}} online 15 September 2015. We {{discuss the}} problem <b>of</b> random <b>measurement</b> <b>error</b> in two variables when using a cross-lagged panel design. We apply {{the problem to}} the question of the causal direction between socio-economic status and subjective health, known also as health selection versus social causation. We plot the bias of the ratio between the social causation and the health selection coefficient as a function of the degree <b>of</b> <b>measurement</b> <b>error</b> in subjective health and socio-economic status for different scenarios which might occur in practice. Using simulated data we give an example of a Bayesian model for the treatment <b>of</b> <b>measurement</b> <b>error</b> that relies on external information about the degree <b>of</b> <b>measurement</b> <b>error...</b>|$|R
40|$|This work {{presents}} an automated {{system for the}} <b>measurement</b> <b>of</b> form <b>errors</b> <b>of</b> mechanical components using an industrial robot. A three-probe error separation technique was employed to allow decoupling between the measured form error and errors introduced by the robotic system. A mathematical model of the measuring system was developed to provide inspection results {{by means of the}} solution of a system of linear equations. A new self-calibration procedure, which employs redundant data from several runs, minimizes the influence of probes zero-adjustment on the final result. Experimental tests applied to the <b>measurement</b> <b>of</b> straightness <b>errors</b> <b>of</b> mechanical components were accomplished and demonstrated the effectiveness of the employed methodology. (C) 2007 Elsevier Ltd. All rights reserved. Fundacao de Amparo a Pesquisa do Estado de Sao Paulo - FAPESPConselho Nacional de Desenvolvimento Cientifico e Tecnologico-CNP...|$|R
40|$|Synthetic Aperture Radar Interferometry (InSAR) {{techniques}} are increasingly applied for monitoring land subsidence. The advantages of InSAR include high accuracy {{and the ability}} to cover large areas; nevertheless, research validating the use of InSAR on building deformation is limited. In this paper, we test the monitoring capability of the InSAR in experiments using two landmark buildings; the Bohai Building and the China Theater, located in Tianjin, China. They were selected as real examples to compare InSAR and leveling approaches for building deformation. Ten TerraSAR-X images spanning half a year were used in Permanent Scatterer InSAR processing. These extracted InSAR results were processed considering the diversity in both direction and spatial distribution, and were compared with true leveling values in both Ordinary Least Squares (OLS) regression and <b>measurement</b> <b>of</b> <b>error</b> analyses. The detailed experimental results for the Bohai Building and the China Theater showed a high correlation between InSAR results and the leveling values. At the same time, the two Root Mean Square Error (RMSE) indexes had values of approximately 1 mm. These analyses show that a millimeter level of accuracy can be achieved by means of InSAR technique when measuring building deformation. We discuss the differences in accuracy between OLS regression and <b>measurement</b> <b>of</b> <b>error</b> analyses, and compare the accuracy index of leveling in order to propose InSAR accuracy levels appropriate for monitoring buildings deformation. After assessing the advantages and limitations of InSAR techniques in monitoring buildings, further applications are evaluated...|$|R
40|$|MiscLassification of {{exposure}} is a well-recognized inherent limitation of epideniiologic studies {{of disease and}} the environment. For many agents of interest, exosures take place over time and in multiple locations; accurately es g the relevant exposures for an individual participant in epidemiologic studies is often daunting, particularly within the limits set by feasibility, participant burden, and cost. Researchers have taken steps {{to deal with the}} consequences <b>of</b> <b>measurement</b> <b>error</b> by limiting the degree <b>of</b> <b>error</b> through a study's design, estimating the degee <b>of</b> <b>error</b> using a nested validation study, and by adjuting for measurement error in sutistical analyses. In thi paper, we address measurement error in obsemvtional studies of air pollution and health. Because measurement error may have substantial implications for interpreting epidemiologic studies on air pollution, particularly the time-series analyses, we developed a sytematic conceptual formulaton of the problem <b>of</b> <b>measurement</b> <b>error</b> in epidemiologic studies of air pollution and then considered the consequences witiin this formulation. When possible, we used available relevant data to make simple estimates <b>of</b> <b>measurement</b> <b>error</b> efec. This. paper provides an overview <b>of</b> <b>measurement</b> <b>errors</b> in linear regression, distinguishing two e es of a continumn-Berkson from clasical typ...|$|R
