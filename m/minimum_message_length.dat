247|863|Public
25|$|The <b>minimum</b> <b>message</b> <b>length</b> {{principle}} of statistical and inductive inference and machine learning {{was developed by}} C.S. Wallace and D.M. Boulton in 1968. MML is Bayesian (i.e. it incorporates prior beliefs) and information-theoretic. It has the desirable properties of statistical invariance (i.e. the inference transforms with a re-parametrisation, such as from polar coordinates to Cartesian coordinates), statistical consistency (i.e. even for very hard problems, MML will converge to any underlying model) and efficiency (i.e. the MML model will converge to any true underlying model about as quickly as is possible). C.S. Wallace and D.L. Dowe (1999) showed a formal connection between MML and algorithmic information theory (or Kolmogorov complexity).|$|E
2500|$|Tan, Peter Jing; and [...] (2004); , Lecture Notes in Artificial Intelligence (LNAI) 3339, Springer-Verlag, [...] (This paper uses <b>minimum</b> <b>message</b> <b>length</b> (MML) and {{actually}} incorporates probabilistic support vector {{machines in the}} leaves of decision trees.) ...|$|E
2500|$|The minimum {{instruction}} set {{of a universal}} Turing machine requires approximately the same length description across different formulations, and is small compared to the Kolmogorov complexity of most practical theories. Marcus Hutter has used this consistency to define a [...] "natural" [...] Turing machine of small size as the proper basis for excluding arbitrarily complex {{instruction set}}s {{in the formulation of}} razors. Describing the program for the universal program as the [...] "hypothesis", and the representation of the evidence as program data, it has been formally proven under Zermelo–Fraenkel set theory that [...] "the sum of the log universal probability of the model plus the log of the probability of the data given the model should be minimized." [...] Interpreting this as minimising the total length of a two-part message encoding model followed by data given model gives us the <b>minimum</b> <b>message</b> <b>length</b> (MML) principle.|$|E
40|$|Abstract. Solomonoff {{induction}} {{is known}} to be universal, but incomputable. Its approximations, namely, the <b>Minimum</b> Description (or <b>Message)</b> <b>Length</b> (MDL) principles, are adopted in practice in the efficient, but non-universal form. Recent attempts to bridge this gap leaded to development of the Repre-sentational MDL principle that originates from formal decomposition of the task of induction. In this paper, possible extension of the RMDL principle in the context of universal intelligence agents is considered, for which introduction of representations is shown to be an unavoidable meta-heuristic and a step toward efficient general intelligence. Hierarchical representations and model optimiza-tion with the use of information-theoretic interpretation of the adaptive reson-ance are also discussed...|$|R
40|$|We {{consider}} a two party network where each party wishes to compute {{a function of}} two correlated sources. Each source is observed {{by one of the}} parties. The true joint distribution of the sources is known to one party. The other party, on the other hand, assumes a distribution for which the set of source pairs that have a positive probability is only a subset of those that may appear in the true distribution. In that sense, this party has only partial information about the true distribution from which the sources are generated. We study the impact of this asymmetry on the worst-case <b>message</b> <b>length</b> for zero-error function computation, by identifying the conditions under which reconciling the missing information prior to communication is better than not reconciling it but instead using an interactive protocol that ensures zero-error communication without reconciliation. Accordingly, we provide upper and lower bounds on the <b>minimum</b> worst-case <b>message</b> <b>length</b> for the communication strategies with and without reconciliation. Through specializing the proposed model to certain distribution classes, we show that partially reconciling the true distribution by allowing a certain degree of ambiguity can perform better than the strategies with perfect reconciliation as well as strategies that do not start with an explicit reconciliation step. As such, our results demonstrate a tradeoff between the reconciliation and communication rates, and that the worst-case <b>message</b> <b>length</b> {{is a result of the}} interplay between the two factors...|$|R
40|$|Solomonoff {{induction}} {{is known}} to be universal, but incomputable. Its approximations, namely, the <b>Minimum</b> Description (or <b>Message)</b> <b>Length</b> (MDL) principles, are adopted in practice in the efficient, but non-universal form. Recent attempts to bridge this gap leaded to development of the Representational MDL principle that originates from formal decomposition of the task of induction. In this paper, possible extension of the RMDL principle in the context of universal intelligence agents is considered, for which introduction of representations is shown to be an unavoidable meta-heuristic and a step toward efficient general intelligence. Hierarchical representations and model optimization with the use of information-theoretic interpretation of the adaptive resonance are also discussed. Comment: proceedings of AGI 2012, Lecture Notes in Artificial Intelligence, Vol. 7716, pp. 242 - 251, Springer-Verlag, 2012. The final publication is available at link. springer. co...|$|R
5000|$|<b>Minimum</b> <b>message</b> <b>length</b> ({{decision}} trees, decision graphs, etc.) ...|$|E
50|$|The <b>Minimum</b> <b>Message</b> <b>Length</b> (MML) point {{estimator}} {{is based}} in Bayesian information theory and is not so {{directly related to the}} posterior distribution.|$|E
50|$|The {{program with}} the {{shortest}} length that matches the data {{is the most likely}} to predict future data. This is the thesis behind the <b>Minimum</b> <b>message</b> <b>length</b> and Minimum description length methods.|$|E
5000|$|Timing {{requirements}} such as {{length of}} the break condition used to awaken sensors, <b>minimum</b> time between <b>messages</b> and <b>length</b> of time before a sensor enters a low-power state.|$|R
30|$|Communication {{messages}} {{should be}} of low redundancy rate and <b>minimum</b> <b>message</b> exchange between the nodes.|$|R
5000|$|A <b>message</b> of <b>length</b> [...] {{obviously}} {{takes longer}} to send than {{a message of}} size 1. However, the BSP model {{does not make a}} distinction between a <b>message</b> <b>length</b> of [...] or [...] <b>messages</b> of <b>length</b> 1. In either case the cost is said to be [...]|$|R
50|$|Alan Turing {{used the}} natural ban. Boulton and Wallace {{used the term}} nit in {{conjunction}} with <b>minimum</b> <b>message</b> <b>length</b> which was subsequently changed by the minimum description length community to nat to avoid confusion with the nit used as a unit of luminance.|$|E
5000|$|The <b>minimum</b> <b>message</b> <b>length</b> {{principle}} (Wallace and Boulton, 1968, WB1968) [...] - [...] an information-theoretic {{principle in}} statistics, econometrics, machine learning, inductive inference and knowledge discovery {{which can be}} seen both as a mathematical formalisation of Occams Razor and as an invariant Bayesian method of model selection and point estimation, ...|$|E
5000|$|MDL was not {{the first}} information-theoretic {{approach}} to learning; as early as 1968 Wallace and Boulton pioneered a related concept called <b>minimum</b> <b>message</b> <b>length</b> (MML). The difference between MDL and MML is a source of ongoing confusion. Superficially, the methods appear mostly equivalent, but there are some significant differences, especially in interpretation: ...|$|E
50|$|To avoid ambiguity, the <b>message</b> <b>length</b> value must be itself {{resistant}} to length extension attacks. Most common implementations use a fixed bit-size (generally 64 or 128 bits in modern algorithms) and a fixed position {{at end of}} the last block for encoding the <b>message</b> <b>length</b> value.|$|R
3000|$|... {{denotes the}} random amount for the preponed {{transmission}} with at most the maximum, respectively, <b>minimum</b> <b>message</b> staggering delay [...]...|$|R
25|$|<b>Message</b> <b>length</b> is Max 260Bytes. Data field MAX 255.|$|R
50|$|The {{principle}} of grammar induction {{has been applied}} to other aspects of natural language processing, and has been applied (among many other problems) to morpheme analysis, and place name derivations. Grammar induction has also been used for lossless data compression and statistical inference via <b>minimum</b> <b>message</b> <b>length</b> (MML) and minimum description length (MDL) principles.|$|E
50|$|Chris Wallace and D. M. Boulton {{developed}} <b>minimum</b> <b>message</b> <b>length</b> circa 1968. Later Jorma Rissanen {{developed the}} minimum description length circa 1978. These methods allow information theory {{to be related}} to probability, in a way that can be compared to the application of Bayes' theorem, but which give a source and explanation for the role of prior probabilities.|$|E
5000|$|<b>Minimum</b> <b>message</b> <b>length</b> (MML) is {{a formal}} {{information}} theory restatement of Occam's Razor: even when models are not equal in {{goodness of fit}} accuracy to the observed data, the one generating the shortest overall message {{is more likely to}} be correct (where the message consists of a statement of the model, followed by a statement of data encoded concisely using that model). MML was invented by Chris Wallace, first appearing in the seminal paper [...] "An information measure for classification" [...]|$|E
5000|$|Probability {{is related}} to {{information}} content described by <b>message</b> <b>length</b> L, ...|$|R
40|$|We {{consider}} {{the transmission of}} a message from a source node to a terminal node in a network with n nodes and m links where the message is divided into parts and each part is transmitted over a different path {{in a set of}} paths from the source node to the terminal node. Here each link is characterized by a bandwidth and delay. The set of paths together with their transmission rates used for the message is referred to as a multipath. We present two algorithms that produce a minimum-end-to-end message delay multipath path table that, for every <b>message</b> <b>length,</b> specifies a multipath that will achieve the minimum end-to-end delay. The algorithms also generate a function that maps the <b>minimum</b> end-to-end <b>message</b> delay to the <b>message</b> <b>length.</b> The time complexities of the algorithms are O(n{sup 2 }((n{sup 2 }/logn) + m) min(D{sub max}, C{sub max})) and O(nm(C{sub max} + nmin(D{sub max}, C{sub max}))) when the link delays and bandwidths are non-negative integers. Here D{sub max} and C{sub max} are respectively the maximum link delay and maximum link bandwidth and C{sub max} and D{sub max} are greater than zero...|$|R
50|$|The <b>message</b> <b>length</b> {{was limited}} to less than 2128 for SMASH-256 and 2256 for SMASH-512.|$|R
5000|$|He was {{appointed}} Foundation Chair of Information Science at Monash University in 1968 {{at the age}} of 34 (before the Department was later re-named Computer Science), and Professor Emeritus in 1996. Wallace was a fellow of the Australian Computer Society and in 1995 he {{was appointed}} a fellow of the ACM [...] "For research {{in a number of areas}} in Computer Science including fast multiplication algorithm, <b>minimum</b> <b>message</b> <b>length</b> principle and its applications, random number generation, computer architecture, numerical solution of ODEs, and contribution to Australian Computer Science."http://fellows.acm.org/fellow_citation.cfm?id=1058015&srt=alpha&alpha=W ...|$|E
5000|$|In a {{decision}} tree, all paths from the root node to the leaf node proceed {{by way of}} conjunction, or AND.In {{a decision}} graph, {{it is possible to}} use disjunctions (ORs) to join two more paths together using <b>Minimum</b> <b>message</b> <b>length</b> (MML). [...] Decision graphs have been further extended to allow for previously unstated new attributes to be learnt dynamically and used at different places within the graph. [...] The more general coding scheme results in better predictive accuracy and log-loss probabilistic scoring. In general, decision graphs infer models with fewer leaves than decision trees.|$|E
5000|$|These {{conditions}} {{were difficult to}} achieve, since {{as soon as one}} requirement was achieved the others would go wrong. The purpose of making all from the same field was to avoid special cases. However, they were eventually required to be made from 20 to 40 master fields. As regards changing keys, the printer was given lead strips bearing the pattern of each of the 36 rows, of which 24 were chosen for each raster. The <b>minimum</b> <b>message</b> <b>length</b> was initially set to 60 characters but was lowered to 45 by the army after some use. Fricke asked the TICOM interrogators ...|$|E
40|$|PLAYTHROUGH ring {{performance}} is studied with self-similar traffic patterns. Measurements made by others on local networks {{connected to the}} Internet have shown that TCP traffic is self-similar. Self-similar traffic can be generated using heavy-tailed distributions. In particular, others {{have shown that the}} Weibull distribution provides a good fit for TCP connection interarrival times. The Weibull distribution, with specific parameters measured in real networks, is used to simulate the operation of the PLAYTHROUGH ring under self-similar traffic. Simulation results reveal that the mean waiting time performance of the PLAYTHROUGH ring under self-similar traffic is markedly worse than that of hitherto assumed traffic patterns using the exponentially distributed interarrival times and geometrically distributed <b>message</b> <b>lengths.</b> Furthemore, it appears that, in general, mean waiting times are significantly greater for PLAYTHROUGH ring under exponential interarrival times and Weibull-distributed <b>message</b> <b>lengths</b> than in the case when message interarrival times and <b>message</b> <b>lengths</b> are assumed to be Weibull-distributed and geometrically distributed, respectively. An analytical model is derived for various PLAYTHROUGH ring performance metrics under the assumption of exponential interarrivals and Weibull-distributed <b>message</b> <b>lengths,</b> including the moments of the number of minipackets, control frame round trip time, transmission time, service time, blocking duration, and waiting time. When Weibull interarrival times are assumed, finding an analytical model for waiting times is a seemingly intractable problem because the Laplace transform of the Weibull distribution does not have a closed form. However, it is shown that, under heavy loads, mean waiting times under the assumption of exponentially distributed interarrival times and geometrically distributed <b>message</b> <b>lengths</b> are, in general, a lower bound on mean waiting times under the assumption of Weibull interarrivals and geometrically distributed <b>message</b> <b>lengths.</b> Moreover, under heavy loads, mean waiting times under the assumption of exponentially distributed interarrival times and Weibull <b>message</b> <b>lengths</b> are, in general, upper bounds on mean waiting times under the assumption of Weibull interarrivals and geometrically distributed <b>message</b> <b>lengths.</b> This work provides the first analytical approxiamtion that predicts the performance of PLAYTHROUGH ring under self-similar traffic. In fact, no prior analytical model exists for any ring network under self-similar traffic, including TOKEN ring...|$|R
30|$|Embedding {{capacity}} {{is defined as}} the maximum mean <b>message</b> <b>length</b> which could be embedded in an image.|$|R
5000|$|Let [...] with [...] be {{the binary}} {{representation}} of the <b>message</b> <b>length</b> [...] and define [...] for [...]|$|R
50|$|The <b>minimum</b> <b>message</b> <b>length</b> {{principle}} of statistical and inductive inference and machine learning {{was developed by}} C.S. Wallace and D.M. Boulton in 1968. MML is Bayesian (i.e. it incorporates prior beliefs) and information-theoretic. It has the desirable properties of statistical invariance (i.e. the inference transforms with a re-parametrisation, such as from polar coordinates to Cartesian coordinates), statistical consistency (i.e. even for very hard problems, MML will converge to any underlying model) and efficiency (i.e. the MML model will converge to any true underlying model about as quickly as is possible). C.S. Wallace and D.L. Dowe (1999) showed a formal connection between MML and algorithmic information theory (or Kolmogorov complexity).|$|E
5000|$|Figueiredo and Jain [...] {{note that}} {{convergence}} to 'meaningless' parameter values obtained at the boundary (where regularity conditions breakdown, e.g., Ghosh and Sen (1985)) is frequently observed {{when the number}} of model components exceeds the optimal/true one. On this basis they suggest a unified approach to estimation and identification in which the initial n is chosen to greatly exceed the expected optimal value. Their optimization routine is constructed via a <b>minimum</b> <b>message</b> <b>length</b> (MML) criterion that effectively eliminates a candidate component if there is insufficient information to support it. In this way it is possible to systematize reductions in n and consider estimation and identification jointly.|$|E
5000|$|A {{variety of}} {{approaches}} {{to the problem of}} mixture decomposition have been proposed, many of which focus on maximum likelihood methods such as expectation maximization (EM) or maximum a posteriori estimation (MAP). Generally these methods consider separately the questions of system identification and parameter estimation; methods to determine the number and functional form of components within a mixture are distinguished from methods to estimate the corresponding parameter values. Some notable departures are the graphical methods as outlined in Tarter and Lock [...] and more recently <b>minimum</b> <b>message</b> <b>length</b> (MML) techniques such as Figueiredo and Jain [...] and to some extent the moment matching pattern analysis routines suggested by McWilliam and Loh (2009).|$|E
40|$|Abstract: We {{provide a}} brief {{overview}} ofMinimum <b>Message</b> <b>Length</b> (MML) inductive inference (Wallace and Boulton (1968), Wallace and Freeman (1987)). We then outline how MML is used for statistical parameter estimation, and how the MML intrinsic classification program, Snob (Wallace and Boulton (1968), Wallace (1986), Wallace (1990)) uses the <b>message</b> <b>lengths</b> from various parameter estimates to enable it to combine parameter estimation with model selection in intrinsic classification. We mention here the most recent extensions to Snob, permitting Poisson and von Mises circular distributions. We also survey some applications of Snob (albeit briefly), and further provide some documentation on how the user can guide Snob’s search through various models of the given data to try to obtain that model whose <b>message</b> <b>length</b> is a <b>minimum...</b>|$|R
50|$|The <b>Message</b> <b>Length</b> field {{indicates}} {{the length of}} the Diameter message including the header fields and the padded AVPs.|$|R
5000|$|The rate of a {{block code}} {{is defined as}} the ratio between its <b>message</b> <b>length</b> and its block length: ...|$|R
