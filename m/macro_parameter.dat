13|84|Public
25|$|In MASM 80x86 Assembly Language, & is the Substitution Operator, {{which tells}} the {{assembler}} {{to replace a}} <b>macro</b> <b>parameter</b> or text macro name with its actual value.|$|E
25|$|In this {{particular}} example, the macro {{is no more}} complex than a C-style textual substitution, but because parsing of the <b>macro</b> <b>parameter</b> occurs before the macro operates on the calling code, diagnostic messages {{would be far more}} informative. However, because the body of a macro is executed at compile time each time it is used, many techniques of optimization can be employed. It is even possible to entirely eliminate complex computations from resulting programs by performing the work at compile-time.|$|E
2500|$|<b>Macro</b> <b>parameter</b> {{substitution}} {{is strictly}} by name: at macro processing time, {{the value of}} a parameter is textually substituted for its name. The most famous class of bugs resulting was the use of a parameter that itself was an expression and not a simple name when the macro writer expected a name. In the macro: ...|$|E
40|$|The major task of {{this study}} is to provide general {{analyses}} of solubility problems. The important focus of this work is to study the quantitative relationship between the thermodynamical parameters in such a way that it will be possible to relate the <b>macro</b> <b>parameters</b> characterizing the system to the microstructure of the materialsSummary in EnglishAvailable from STL Prague, CZ / NTK - National Technical LibrarySIGLECZCzech Republi...|$|R
40|$|This paper {{considers}} the aggregation of production functions within a stochastic framework where both the technological parameters and the factor inputs {{are thought to}} be probabilistically distributed across firms. The corresponding "true" aggregate production function expressed in moments form is compared with the macro function of choice. This comparison either yields an aggregation bias in total output, if the macro function is fully specified, or allows the relation between exact aggregation <b>macro</b> <b>parameters</b> and their micro foundations to be established, if not. ...|$|R
40|$|Creating {{reusable}} models typically {{requires that}} general-purpose models be written with re-definable parameters such as SIZE, WIDTH and DEPTH. With respect to coding parameterized Verilog models, two Verilog constructs that are over-used and abused are the global macro definition (`define) and the infinitely abusable parameter redefinition statement (defparam). This paper will detail techniques for coding proper parameterized models, detail {{the differences between}} <b>parameters</b> and <b>macro</b> definitions, present guidelines for using <b>macros,</b> <b>parameters</b> and parameter definitions, discourage the use of defparams, and detail Verilog- 2001 enhancements to enhance coding and usage of parameterized models. 1...|$|R
5000|$|An {{identifier}} can denote an object; a function; a tag or {{a member}} of a structure, union, or enumeration; a typedef name; a label name; a macro name; or a <b>macro</b> <b>parameter.</b> The same identifier can denote different entities at different points in the program. ... For each different entity that an identifier designates, the identifier is visible (i.e., can be used) only within a region of program text called its scope.|$|E
50|$|In this {{particular}} example, the macro {{is no more}} complex than a C-style textual substitution, but because parsing of the <b>macro</b> <b>parameter</b> occurs before the macro operates on the calling code, diagnostic messages {{would be far more}} informative. However, because the body of a macro is executed at compile time each time it is used, many techniques of optimization can be employed. It is even possible to entirely eliminate complex computations from resulting programs by performing the work at compile-time.|$|E
5000|$|The HLA CTL {{includes}} many control {{statements such as}} #IF, #WHILE, #FOR, #PRINT, an assignment statement and so on. One can also create compile-time variables and constants (including structured data types such as records and unions). The HLA CTL also provides hundreds of built-in functions (including a very rich set of string and pattern-matching functions). The HLA CTL allows programmers to create CTL programs that scan and parse strings, allowing those programmers to create embedded domain specific languages (EDSLs, also termed mini-languages). The [...] macro appearing earlier {{is an example of}} such an EDSL. The put macro (in the stdout namespace, hence the name stdout.put) parses its <b>macro</b> <b>parameter</b> list and emits the code that will print its operands.|$|E
40|$|Weakly wetted {{granular}} material {{is the subject}} of many studies. Several formulations were proposed to calculate the capillary forces between wet particles. In this paper some of such models have been implemented in a DEM-framework, and simulation results were compared to experimental measurements. Also, the influence of capillary model type on <b>macro</b> <b>parameters</b> like local shear viscosity and cohesive parameters of sheared material have been investigated through the simulation of spherical beads using a DEM-model of a split-bottom shear-cell. It was concluded that the water content, simulated with the help of capillary bridge models, changes the macro-properties of the simulated {{granular material}}. Different capillary bridge models do not influence the macroscopic results visibly...|$|R
40|$|This paper {{presents}} the broad <b>macro</b> <b>parameters</b> {{of the growth}} of the Indian economy since the nation's independence and a cross-country evaluation of where India stands, drawing out the patterns discernible in these aggregative statistics. The paper gives an overview of the on-going debate on the components of the Indian growth and the relative importance of the different policies in the 1980 s and 1990 s. It contributes to this debate by identifying the landmark years, and analysing the politics behind some of the economics. The paper also analyses the factors behind the changes in India's savings rate and the relation between growth and development, on the one hand, and the nature of labour market regulation, on the other...|$|R
40|$|The paper {{considers}} <b>macro</b> <b>parameters</b> of corporation innovation {{activity in}} the BRICS countries. The authors determine transnational corporation behavior strategies {{in the context of}} creating and transferring new knowledge, where developed countries (the USA, European countries and Japan) play an important role and take a leading position in this process. Companies from emerging economies focus on using and adopting innovations. The {{reason for this is that}} knowledge “is coded” specifically, consequently the participants of its exchange have to be in similar intellectual space. Nevertheless, the market-leading corporations from the BRICS countries join the world chains of innovation creation, building their networks to satisfy their branches needs concerning technological decisions and personnel trainin...|$|R
40|$|This {{application}} helps programmers {{learn and}} use SAS ® macro definitions {{in a more}} efficient and structured manner than reading text from the macro program header. From one master file, the macro developer customizes one application file to each SAS macro, by defining each <b>macro</b> <b>parameter</b> and its attributes (e. g. mandatory/optional, default value …etc.). Then it’s used by any programmer to view the documentation associated with each <b>macro</b> <b>parameter,</b> input its value and verify the validity of all entered values. A warning list of potential problems is immediately displayed (e. g. invalid numeric value where a numeric value is expected, embedded comma …etc.). Thus, this application identifies the exact source of potential errors prior to submitting the macro. The application writes out the complete SAS code to execute the macro, which can be cut and pasted into any program. It can also launch a SAS session and directly submit the macro...|$|E
40|$|Using lunar seismological data, {{constraints}} {{have been}} proposed on the available parameter space of macroscopic dark matter (macros). We show that actual limits are considerably weaker by considering in greater detail the mechanism through which macro impacts generate detectable seismic waves, which have wavelengths considerably longer than {{the diameter of the}} macro. We show that the portion of the <b>macro</b> <b>parameter</b> space that can be ruled out by current seismological evidence is considerably smaller than previously reported, and specifically that candidates with {{greater than or equal to}} nuclear density are not excluded by lunar seismology. Comment: 9 pages, 3 color figure...|$|E
40|$|We report our {{investigations}} on {{measuring the}} energy transfer upconversion (ETU) parameter in various neodymium-doped laser crystals (YAG, YVO 4, GdVO 4, KGW, and YLF) via the z-scan technique. Starting {{with a simple}} two-level macro-parameter spatially dependent rate equation model we obtain a good correlation for Nd:YAG at different concentrations and crystal temperatures, however the other crystals illustrate significant deviation between simulation and measurement. Currently we attribute this difference to additional ion-ion interactions in the respective samples, for which a more detailed model is currently being considered. Of the tested materials Nd:YAG appears to have the lowest ETU <b>macro</b> <b>parameter,</b> at around 0. 35 x 10 - 16 cm 3 /s for a 0. 6 at. % doping concentration, compared with nominally thrice this for 0. 5 at% Nd:YLF and almost {{an order of magnitude}} higher for the 0. 5 at. % vanadates (YVO 4 and GdVO 4). These values are significant for determining additional heat load in the respective gain media, especially when trying to increase the output power/energy from lasers employing these crystals, typically achieved by increasing the pump and cavity mode size...|$|E
40|$|Since July 1984 the New Zealand Government {{has been}} {{following}} a policy of disengagement and deregulation throughout the New Zealand economy. The impacts of the new policy have been felt at both the macro and the micro level. For the agricultural sector, a closer relationship to international prices and costs has been sought, with less industry assistance and direct support from the Government. With the removal of special privileges formerly enjoyed, the agricultural sector has also become more closely aligned with and vulnerable to changes in major <b>macro</b> <b>parameters</b> in the economy such as exchange rates, interest rates, inflation control and budget economies. The alignment of the agricultural economy with international prices and costs is likely {{to bring about a}} smaller and more competitive sector than was previously the case. Agricultural and Food Policy,...|$|R
40|$|Although a {{significant}} portion of conditions encountered in geotechnical engineering, for investigating engineering behavior of soil, involves unsaturated soils; the traditional analysis and design approach has been to assume the limiting conditions of soils being either completely dry or completely saturated. In unsaturated soils the capillary force produce attractive forces between particles. Discrete Element Method (DEM) is an appropriate tool to consider the capillary effects. The calculations performed in DEM is based on iterative application of Newton’s second law to the particles and force-displacement law at the contacts. In the present study, the behavior of unsaturated soils in pendular regime is simulated utilizing DEM. Triaxial  compression tests were modeled as two-dimensional, considering capillary force effects. Finally, capillary effects on <b>Macro</b> <b>parameters</b> of a simulated granular soil (stress, axial strain, volumetric strain and void ratio) and Mohr Coulomb failure criteria parameters were studied...|$|R
40|$|This paper {{presents}} {{theory for}} compartmental models used in positron emission tomography. Both plasma input models and reference tissue input models are considered. General theory is derived and the systems are characterised {{in terms of}} their impulse response functions. The theory shows that the <b>macro</b> <b>parameters</b> of the system may be determined simply from the coefficients of the impulse response functions. These results are discussed in the context of radioligand binding studies. It is shown that binding potential is simply related to the integral of the impulse response functions for all plasma and reference tissue input models currently used in positron emission tomography. The paper also introduces a general compartmental description for the behaviour of the tracer in blood, which then allows for the blood volume induced bias in reference tissue input models to be assessed...|$|R
40|$|Cloud base height (CBH) is an {{important}} cloud <b>macro</b> <b>parameter</b> that {{plays a key role}} in global radiation balance and aviation flight. Building on a previous algorithm, CBH is estimated by combining measurements from CloudSat/CALIPSO and MODIS based on the International Satellite Cloud Climatology Project (ISCCP) cloud-type classification and a weighted distance algorithm. Additional constraints on cloud water path (CWP) and cloud top height (CTH) are introduced. The combined algorithm takes advantage of active and passive remote sensing to effectively estimate CBH in a wide-swath imagery where the cloud vertical structure details are known only along the curtain slice of the nonscanning active sensors. Comparisons between the estimated and observed CBHs show high correlation. The coefficient of association (R 2) is 0. 8602 with separation distance between donor and recipient points in the range of 0 to 100 [*]km and falls off to 0. 5856 when the separation distance increases to the range of 401 to 600 [*]km. Also, differences are mainly within 1 [*]km when separation distance ranges from 0 [*]km to 600 [*]km. The CBH estimation method was applied to the 3 D cloud structure of Tropical Cyclone Bill, and the method is further assessed by comparing CTH estimated by the algorithm with the MODIS CTH product...|$|E
40|$|Dark {{matter is}} a vital {{component}} of the current best model of our universe, ΛCDM. There are leading candidates for what the dark matter could be (e. g. weakly-interacting massive particles, or axions), but no compelling observational or experimental evidence exists to support these particular candidates, nor any beyond-the-Standard-Model physics that might produce such candidates. This suggests that other dark matter candidates, including ones that might arise in the Standard Model, should receive increased attention. Here we consider a general class of dark matter candidates with characteristic masses and interaction cross-sections characterized in units of grams and cm^ 2, respectively [...] we therefore dub these macroscopic objects as Macros. Such dark matter candidates could potentially be assembled out of Standard Model particles (quarks and leptons) in the early universe. A combination of Earth-based, astrophysical, and cosmological observations constrain {{a portion of the}} <b>Macro</b> <b>parameter</b> space. A large region of parameter space remains, most notably for nuclear-dense objects with masses in the range 55 - 10 ^ 17 g and 2 × 10 ^ 20 - 4 × 10 ^ 24 g, although the lower mass window is closed for Macros that destabilize ordinary matter. Comment: 13 pages, 1 table, 4 figures. Submitted to MNRAS. v 3 : corrected small errors and a few points were made more clear, v 4 : included CMB bounds on dark matter-photon coupling from Wilkinson et al. (2014) and references added. Final revision matches published versio...|$|E
40|$|Within the {{framework}} of the “Sel-tag imaging project”, a novel method was used to rapidly label protein tracers and the in vivo targeting abilities of these tracers were studied in animal models of cancer using a preclinical positron emission tomography (PET) camera. To first evaluate and optimize preclinically the use of PET tracers can facilitate their translation to and implementation in human patient studies. The ultimate goal of the different projects within the Sel-tag imaging project was to find imaging biomarkers that could potentially be used for individualizing cancer treatment and thereby improve the therapeutic results. This thesis focuses on methods employed to describe the distribution of these protein-based tracers in human xenografts. Many of the techniques used had been developed for other imaging circumstances. Therefore verification for these imaging applications was an important aspect of these papers. Paper I examined the distribution in a tumour of a medium-sized AnnexinA 5 -based tracer that targeted phosphatidylserine externalised during cell death in tumours in two cases; first, with no pre-treatment (baseline) and, second, after pre-treatment with a chemotherapeutic agent. Small differences between tracer uptakes in the two cases required a <b>macro</b> <b>parameter</b> analysis method for quantifications. Evaluations of the influence of the enhanced permeability and retention effect by using a size-matched control were introduced. The AnnexinA 5 results were compared to those of the metabolic tracer [18 F]FDG and complemented with circulating serum markers to increase sensitivity. Paper II extended the analysis in paper I to incorporate more verifications that were also more thorough. The choice of input (blood or reference tissue) and the statistical significance of intergroup comparisons when using conventional uptake measurements and the more involved <b>macro</b> <b>parameter</b> analyses like in paper I were compared. We also proposed that distribution volume ratio was a more appropriate quantification parameter concept for these protein-based tracers with relatively large non-specific uptake. Paper III assessed the smaller Affibody™ tracer ZHER 2 : 342 as an imaging biomarker for human epidermal growth factor 2 (HER 2), whose overexpressions are associated with a poor prognosis for breast cancer patients. In order to demonstrate specific binding to HER 2, pre-treatment of the tumour with unlabelled protein and uptake in xenografts with low HER 2 expression was evaluated. Ex vivo immunohistochemistry of expression levels supported the imaging results. Paper IV examined a radiopharmaceutical that targeted the epidermal growth factor receptor (EGFR), whose overexposure in tumours is associated with a negative prognosis. Again an Affibody™ molecule, (ZEGFR: 2377), was used and, as in in paper I, a size-matched control was also used to estimate the non-specific uptake. Uptakes, quantified by conventional uptake methods, varied in tumours with different EGFR expression levels. Ex vivo analyses of expression levels were also performed. Paper V addressed the non-uniform (heterogeneous) uptake of different tracers in a tumour tissue. An algorithm was written that aimed at incorporating all relevant aspects that will influence non-uniformity. Histograms were generated that visualized how the frequency and spread of deviations contributed to the heterogeneity. These aspects could not always be attended in a direct manner, but instead had to be handled in an indirect way. The effect of varying imaging parameters was examined as part of the validation procedure. The method developed is a robust, user-friendly tool for comparing heterogeneity in similar volume preclinical tumor tissues...|$|E
40|$|AbstractFirstly, {{compressed}} air foam system (CAFS) was briefly introduced. The difference of foams produced from low expansion foam extinguishing system and CAFS were compared according to <b>macro</b> <b>parameters</b> and micro parameters. Secondly, thickness of tank shell, thickness of tank bottom, intensity of {{water supply and}} intensity of foam solution supply were determined according to codes. Then oil tank for model test was designed {{with the help of}} Auto Computer Aided Design (Auto CAD) and completed using Q 345 R steel plates. Finally, small-scale model tests were carried out to examine extinguishing effect of CAFS onto oil tank fire. Three conclusions were as follows. The oil tank designed according to codes could endure the flame radiation. CAFS could be applied in extinguishing oil tank fire. The torch-like flame when the fuel level was high and ball-like flame when the fuel level was lower could be explained using theory of middle layer effect...|$|R
40|$|Continuum {{mechanics}} and kinetic theory are two mathematical theories with fundamentally {{different approaches to}} the same physical phenomenon. Continuum mechanics together with thermodynamics treat a substance (a gas or a fluid) as a continuous medium and describes the evolution of its macro characteristics via application of the conservation laws to small packets of the substance. Kinetic theory attempts to describe {{the evolution of the}} <b>macro</b> <b>parameters</b> by treating a substance as a family of colliding objects. The number of objects must be large enough so a statistical approach can be taken. In this work we introduce a numerical scheme to solve 1 -D Bhatnagar-Gross-Krook model equations and examine the formation of a stationary viscous shock. Obtained results are compared to a stationary numerical solution of 1 -D Navier-Stokes equation with a similar set of shock forming conditions. MathematicsDoctoralUniversity of New Mexico. Dept. of Mathematics and StatisticsLorenz, JensEmbid, PedroHayat, MajeedPereyra, Cristin...|$|R
40|$|A {{numerical}} analysis of combustion {{in the secondary}} chamber (thermoreactor) ofa two-stage pilot scale incinerator using computational fluid dynamics (CFD) is presented in detail. Various versions of the CFD program package CFX were used, which offer different combustion models for specific types and forms of combustion processes. The present study was focused on those physicalconditions that assure complete combustion, that is, temperature, residence time, and turbulent mixing. The selection of an appropriate combustion model {{was based on a}} comparison of the numerical results and experimental values of some combustion <b>macro</b> <b>parameters</b> in a thermoreactor. Combustion models based on one-step bimolecular chemical reaction and models based on multistep reactions were used. These models enabled a more detailed prediction of the combustion process in the secondary chamber of a pilot-scaleincinerator. The products of incomplete combustion that are significantly important for the designing and optimization of combustion devices can be predicted by applying multistep reaction models more accurately, especially in a transient regime of combustion...|$|R
40|$|In {{studying}} individual consumption behavior, {{an important}} issue is {{the analysis of the}} relation between commodity expenditure and income (or total expenditure). In this paper we firstly review the more recent theoretical and empirical literature attempting to: (i) derive theory-consistent demand systems models which are able to account for empirically observed non-linearities in total expenditure; (ii) find out whether there exist necessary and sufficient conditions on the across-households distributions such that empirically obtained demand functions still preserve a strong consistency between micro and <b>macro</b> <b>parameters</b> (e. g. consumption-income elasticities). We then apply the techniques discussed {{in the first part of}} the paper to the data generated by a computer-simulated model of consumption dynamics presented in Aversi et al. (1999). We find that the model, under a large range of parametrizations, is pretty well equipped to replicate most of the stylized facts displayed by empirically observed consumption patterns, both cross-section and across time. Household Behavior, Consumption, Demand, Engel Curves, Socially Evolving Preferences, Dynamics. ...|$|R
40|$|This paper {{reports the}} effect of {{confining}} pressure on the mechanical behavior of granular materials from micromechanical considerations starting from the grain scale level, {{based on the results}} of numerically simulated tests on disc assemblages using discrete element modeling (DEM). The two <b>macro</b> <b>parameters</b> which are influenced by the increase in confining pressure are stiffness (increases) and volume change (decreases). The lateral strain coefficient (Poisson's ratio) at the beginning of the test is more or less constant. The angle of internal friction slightly decreases with increase in confining pressure. The numerical results of disc assemblages indicate very clearly a non-linear Mohr-Coulomb failure envelope with increase in confining pressure. The increase in average coordination number and accompanying decrease of fabric anisotropy reduce the shear strength at higher confining pressures. Micromechanical explanations of the macroscopic behavior are presented in terms of the force and fabric anisotropy coefficients. (C) 1999 Elsevier Science Ltd. AII rights reserved...|$|R
40|$|This chapter {{tries to}} give an {{overview}} of the more traditional drinking water treatment from ground and surface waters. Water is treated to meet the objectives of drinking water quality and standards. Water treatment and water quality are therefore closely connected. The objectives for water treatment are to prevent acute diseases by exposure to pathogens, to prevent long-term adverse health effects by exposure to chemicals and micropollutants, and finally to create a drinking water that is palatable and is conditioned {{in such a way that}} transport from the treatment works to the customer will not lead to quality deterioration. Traditional treatment technologies as described in this chapter are mainly designed to remove <b>macro</b> <b>parameters</b> such as suspended solids, natural organic matter, dissolved iron and manganese etc. The technologies have however only limited performance for removal of micropollutants. Advancing analytical technologies and increased and changing use of compounds however show strong evidence of new and emerging threats to drinking water quality. Therefore, more advanced treatment technologies are required...|$|R
40|$|Abstract: Molecular {{dynamics}} (MD) simulation and {{finite element}} (FE) method {{have been successfully}} applied in the simulation of the machining process, but the two methods have their own limitations. For example, the MD simulation can only explain the phenomena occurring at nanometric scale because of the computational cost and nanoscale, while the FE method is suited to model meso-macroscale machining and to simulate <b>macro</b> <b>parameters</b> such as the temperature in cutting zone, the stress/strain distribution, and cutting forces. With the successful application of multiscale simulation in many research fields, the multiscale simulation of the machining process is becoming possible {{in relation to the}} machined surface generation including the surface roughness, residual stress, microhardness, microstructure, and fatigue. Based on the quasicontinuum (QC) method, this paper presents the multiscale simulation of nanometric cutting of crystal copper to demonstrate that a combined MD–FE technique can be applied to a multiscale simulation of the machining process. The study shows that the multiscale simulation is feasible, not withstanding that there is still more work needing to be done to make the multiscale simulation more practical...|$|R
40|$|The {{quality and}} {{condition}} {{of a road}} surface is of great importance for convenience and safety of driving. So the investigations of the behaviour of road materials in laboratory conditions and monitoring of existing roads are widely fulfilled for controlling a geometric parameters and detecting defects in the road surface. Photogrammetry as accurate non-contact measuring method provides powerful means for solving different tasks in road surface reconstruction and analysis. The range of dimensions concerned in road surface analysis can have great variation from tenths of millimetre to hundreds meters and more. So a set of techniques is needed to meet all requirements of road parameters estimation. Two photogrammetric techniques for road surface analysis are presented: for accurate measuring of road pavement and for road surface reconstruction based on imagery obtained from unmanned aerial vehicle. The first technique uses photogrammetric system based on structured light for fast and accurate surface 3 D reconstruction and it allows analysing the characteristics of road texture and monitoring the pavement behaviour. The second technique provides dense 3 D model road suitable for road <b>macro</b> <b>parameters</b> estimation...|$|R
40|$|A structural, fault-model based {{methodology}} for {{the generation of}} compact high-quality test sets for analog macros is presented. Results are shown for an IV-converter <b>macro</b> design. <b>Parameters</b> of so-called test configurations are optimized for detection of faults in a fault-list and an optimal selection algorithm results in determining the best test set. The distribution of the results along the parameter-ares of the test configurations is investigated to identi 3 a collapsed high-quality test set. ...|$|R
40|$|An {{investigation}} on the ballistic impact behaviour of tungsten blunt projectiles on Kevlar® 29 plain-woven fabrics with an epoxy matrix {{has been carried}} out for the purpose of gaining an insight in the phenomena {{at the level of the}} composite components. Numerical models have been developed and have then been compared with experimental results. The numerical models have been developed in LS-DYNA adopting two different approaches: a Macro-homogeneous model, in which each layer is modelled as an orthotropic equivalent, and a Meso-heterogeneous one, based on the definition of all the details of the fabric architecture in the area of impact of the projectile's tip (in this case, yarns and matrix have their own properties). In both models, the projectile is considered a rigid body. An in-depth analysis of the <b>macro</b> <b>parameters,</b> such as the residual velocities, their dependency upon the yaw impact angle and delamination has been performed. Also the stress state inside the layers and the morphological features of the damaged plates has been studied and interesting results regarding the behaviour of different layers of Kevlar® 29 plain-woven fabrics during impact have been obtained. Performances and the pros/cons of the different numerical approaches have also been discussed...|$|R
40|$|Tipping {{points are}} a common {{occurrence}} in complex adaptive systems. In such systems feedback dynamics strongly influence equilibrium points and {{they are one of}} the principal concerns of research in this area. Tipping points occur as small changes in system parameters result in disproportionately large changes in the global properties of the system. In order to show how tipping points might be managed we use the Maximum Entropy (MaxEnt) method developed by Jaynes to find the fixed points of an economic system in two different ways. In the first, economic agents optimise their choices based solely on their personal benefits. In the second they optimise the total benefits of the system, taking into account the effects of all agent’s actions. The effect is to move the game from a recently introduced dual localised Lagrangian problem to that of a single global Lagrangian. This leads to two distinctly different but related solutions where localised optimisation provides more flexibility than global optimisation. This added flexibility allows an economic system to be managed by adjusting the relationship between <b>macro</b> <b>parameters,</b> in this sense such manipulations provide for the possibility of “steering” an economy around potential disasters...|$|R
40|$|The {{interest}} to small and media size enterprises’ (SMEs) internationalization process is increasing with a growth of SMEs’ contribution to GDP. Internet gives {{an opportunity to}} provide variety of services online and reach market niche worldwide. The overlapping of SMEs’ internationalization and online services is {{the main issue of}} the research. The most SMEs internationalize according to intuitive decisions of CEO of the company and lose limited resources to worthless attempts. The purpose of this research is to define effective approaches to online service internationalization and selection of the first international market. The research represents single holistic case study of local massive open online courses (MOOCs) platform going global. It considers internationalization costs and internationalization theories applicable to online services. The research includes preliminary screening of the markets and in-depth analysis based on <b>macro</b> <b>parameters</b> of the market and specific characteristics of the customers and expert evaluation of the results. The specific issues as GILT (Globalization, Internationalization, Localization and Translation) approach and Internet-enabled internationalization are considered. The research results include recommendations on international market selection methodology for online services and for effective internationalization strategy development...|$|R
30|$|Introduction: The {{management}} of tissue perfusion in cardiac surgery with fluid therapy is a challenging task as the parameters {{used to assess}} it do not reflect microcirculatory dysfunction. Heterogeneity in blood flow perfusion and abnormalities in capillary density characterize microcirculation dysfunction and can be assessed with incident dark-field illumination (IDF) imaging of the sublingual tissue. The restoration of abnormalities for capillaries can become a target parameter for the administering of fluids in the future {{in addition to the}} restoration of the <b>macro</b> circulatory <b>parameters</b> solely used nowadays.|$|R
40|$|This article {{addresses}} {{a modification of}} local time for stochastic processes, {{to be referred to}} as `natural local time'. It is prompted by theoretical developments arising in mathematical treatments of recent experiments and observations of phenomena in the geophysical and biological sciences pertaining to dispersion in the presence of an interface of discontinuity in dispersion coefficients. The results illustrate new ways in which to use the theory of stochastic processes to infer <b>macro</b> scale <b>parameters</b> and behavior from micro scale observations in particular heterogeneous environments...|$|R
40|$|A kinetic {{modelling}} {{approach for}} the quantification of in vivo tracer studies with dynamic {{positron emission tomography}} (PET) is presented. The approach {{is based on a}} general compartmental description of the tracer’s fate in vivo and determines a parsimonious model consistent with the measured data. The technique involves the determination of a sparse selection of kinetic basis functions from an overcomplete dictionary using the method of basis pursuit denoising. This enables the characterization of the systems impulse response function from which values of the systems <b>macro</b> <b>parameters</b> can be estimated. These parameter estimates can be obtained from a region of interest analysis or as parametric images from a voxel based analysis. In addition, model order estimates are returned which correspond to the number of compartments in the estimated compartmental model. Validation studies evaluate the methods performance against two pre-existing data led techniques, namely graphical analysis and spectral analysis. Application of this technique to measured PET data is demonstrated using [11 C]diprenorphine (opiate receptor) and [11 C]WAY- 100635 (5 -HT 1 A receptor). Whilst, the method is presented in the context of PET neuroreceptor binding studies, it has general applicability to the quantification of PET/SPET radiotracer studies in neurology, oncology and cardiology...|$|R
