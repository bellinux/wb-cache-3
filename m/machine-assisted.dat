162|0|Public
2500|$|The {{concept of}} {{compiling}} the world's knowledge {{in a single}} location {{dates back to the}} ancient Libraries of Alexandria and Pergamum, but the modern concept of a general-purpose, widely distributed, printed encyclopedia originated with Denis Diderot and the 18th-century French encyclopedists. The idea of using automated machinery beyond the printing press to build a more useful encyclopedia can be traced to Paul Otlet's 1934 book Traité de documentation; Otlet also founded the Mundaneum, an institution dedicated to indexing the world's knowledge, in 1910. This concept of a <b>machine-assisted</b> encyclopedia was further expanded in H. G. Wells' book of essays World Brain (1938) and Vannevar Bush's future vision of the microfilm-based Memex in his essay [...] "As We May Think" [...] (1945). Another milestone was Ted Nelson's hypertext design Project Xanadu, which was begun in 1960.|$|E
5000|$|The {{translation}} editor provides {{various features}} for <b>machine-assisted</b> translation, such as ...|$|E
50|$|Computer-assisted {{translation}} {{is sometimes called}} <b>machine-assisted,</b> or machine-aided, translation (not {{to be confused with}} machine translation).|$|E
50|$|He {{obtained}} BA in Mathematics and Computation (1986-1989) and DPhil in <b>Machine-Assisted</b> Theorem Proving for Software Engineering (1991-1994) {{from the}} University of Oxford.|$|E
50|$|The {{web site}} was {{named one of}} the best free {{reference}} web sites in 2003 by the <b>Machine-Assisted</b> Reference Section of the American Library Association.|$|E
5000|$|... The {{center has}} also {{collaborated with the}} Department of Homeland Security and other {{agencies}} in developing algorithms for <b>machine-assisted</b> baggage searches, weapons detection and identification, and emergency communications.|$|E
50|$|The Science Commons Open Access Data Protocol was {{a method}} for {{ensuring}} that scientific databases can be legally integrated with one another. The protocol was not a license or legal tool, but instead a methodology and best practices document for creating such legal tools in the future, and marking data {{in the public domain}} for <b>machine-assisted</b> discovery.|$|E
5000|$|James Parry (born July 13, 1967), {{commonly}} known by his nickname and username Kibo , is a Usenetter {{known for his}} sense of humor, various surrealist net pranks, an absurdly long [...]signature, and a <b>machine-assisted</b> knack for [...] "kibozing": joining any thread in which [...] "kibo" [...] was mentioned. His exploits have earned him a multitude of enthusiasts, who celebrate him as the head deity of the parody religion kibology, centered on the humor newsgroup alt.religion.kibology.|$|E
5000|$|An Arm Slave is {{operated}} in a semi-master-slave system. This is an operating method {{which has the}} slave arm (machine motion) trace {{the movements of the}} master arm (pilot motion). The Arm Slave is not a complete master-slave piloting arrangements, as full motion tracing is impractical within the limited space of the cockpit. This limitation is solved by the Arm Slave amplifying the motion of the pilot. The [...] "semi" [...] designation of the piloting system refers to this <b>machine-assisted</b> motion management.|$|E
50|$|There are {{automated}} equivalents of all {{of these}} functions, and each analyst will have a personal balance between manual and <b>machine-assisted</b> methods. Unquestionably, when quantitative methods such as modeling and simulation are appropriate, the analyst will want computer assistance, and possibly consultation from experts in methodology. When combining maps and imagery, especially different kinds of imagery, a geographic information system is usually needed to normalize coordinate systems, scale and magnification, and the ability to suppress certain details and add others.|$|E
50|$|Schmieder {{continues}} {{research into}} nanologic {{as a new}} paradigm for <b>machine-assisted</b> problem solving. In particular, he develops the rigorous mathematical basis of applications of nanologic, and performs experiments with systems to demonstrate {{the principles of the}} technology.He emphasizes that a nanologic machine is not a computer in the sense of performing computations, but a machine for abstracting the meaning from incomplete or imperfect information and making “intelligent” conclusions or predictions. In this sense, nanologic is closer to human cognition and analysis than to computation.|$|E
5000|$|The coffee {{cherries}} are sorted by {{immersion in}} water. Bad or unripe fruit will float {{and the good}} ripe fruit will sink. The skin of the cherry {{and some of the}} pulp is removed by pressing the fruit by machine in water through a screen. The bean will still have a significant amount of the pulp clinging to it that needs to be removed. This is done either by the classic ferment-and-wash method or a newer procedure variously called <b>machine-assisted</b> wet processing, aquapulping or mechanical demucilaging: ...|$|E
5000|$|Alberti was an {{accomplished}} cryptographer {{by the standard}} of his day, and invented the first polyalphabetic cipher, which {{is now known as}} the Alberti cipher, and <b>machine-assisted</b> encryption using his Cipher Disk. The polyalphabetic cipher was, at least in principle, for it was not properly used for several hundred years, the most significant advance in cryptography since before Julius Caesar's time. Cryptography historian David Kahn titles him the [...] "Father of Western Cryptography", pointing to three significant advances in the field which can be attributed to Alberti: [...] "the earliest Western exposition of cryptanalysis, the invention of polyalphabetic substitution, and the invention of enciphered code." ...|$|E
50|$|In <b>machine-assisted</b> wet processing, {{fermentation}} is {{not used}} to separate the bean from {{the remainder of the}} pulp; rather, this is done through mechanical scrubbing. This process can cut down on water use and pollution since ferment and wash water stinks. In addition, removing mucilage by machine is easier and more predictable than removing it by fermenting and washing. However, by eliminating the fermentation step and prematurely separating fruit and bean, mechanical demucilaging can remove an important tool that mill operators have of influencing coffee flavor. Furthermore, the ecological criticism of the ferment-and-wash method increasingly has become moot, since a combination of low-water equipment plus settling tanks allows conscientious mill operators to carry out fermentation with limited pollution.|$|E
5000|$|Threat hunting can be {{a manual}} process, in which a {{security}} analyst sifts through various data information using their own knowledge and familiarity with the network to create hypotheses about potential threats. To be even more effective and efficient, however, threat hunting can be partially automated, or <b>machine-assisted,</b> as well. In this case, the analyst utilizes a software that harnesses machine learning and user and entity behavior analytics (UEBA) to inform the analyst of potential risks. The analyst then investigates these potential risks, tracking suspicious behavior in the network. Thus hunting is an iterative process, meaning {{that it must be}} continuously carried out in a loop, beginning with a hypothesis. There are three types of hypotheses: ...|$|E
5000|$|Threat hunting can be {{a manual}} process, in which a {{security}} analyst sifts through various data information using their own knowledge and familiarity with the network to create hypotheses about potential threats, such as, but not limited to, Lateral Movement by Threat Actors. To be even more effective and efficient, however, threat hunting can be partially automated, or <b>machine-assisted,</b> as well. In this case, the analyst utilizes software that leverages machine learning and user and entity behavior analytics (UEBA) to inform the analyst of potential risks. The analyst then investigates these potential risks, tracking suspicious behavior in the network. Thus hunting is an iterative process, meaning {{that it must be}} continuously carried out in a loop, beginning with a hypothesis. There are three types of hypotheses: ...|$|E
5000|$|The {{concept of}} {{compiling}} the world's knowledge {{in a single}} location dates to the ancient Libraries of Alexandria and Pergamum, but the modern concept of a general-purpose, widely distributed, printed encyclopedia originated with Denis Diderot and the 18th-century French encyclopedists. The idea of using automated machinery beyond the printing press to build a more useful encyclopedia {{can be traced to}} Paul Otlet's 1934 book Traité de documentation; Otlet also founded the Mundaneum, an institution dedicated to indexing the world's knowledge, in 1910. This concept of a <b>machine-assisted</b> encyclopedia was further expanded in H. G. Wells' book of essays World Brain (1938) and Vannevar Bush's future vision of the microfilm-based Memex in his essay [...] "As We May Think" [...] (1945). Another milestone was Ted Nelson's hypertext design Project Xanadu, which was begun in 1960.|$|E
5000|$|The movie {{starts at}} {{what seems like}} a small house in a natural setting. Mickey walks out the door and says, [...] "Oh boy! What a day!" [...] Then, he pulls a lever and walks inside. The house is {{converted}} into a trailer (with the natural setting {{in the shape of a}} giant hand fan revealed to be a city dump) and Goofy's car is released from the side. Then, Goofy starts driving through the countryside while Mickey makes dinner-like breakfast (corn on the cob, baked potatoes, watermelon, coffee, and milk). Meanwhile, Donald can't wake up, even when his alarm clock rings and pulls off his blanket. Thanks to a secret controlboard, Mickey manages to rouse him for a <b>machine-assisted</b> bath, but he saw birds and tried to swat them with the towel. Later, the bath is converted into a dining area.|$|E
50|$|Practitioners {{within the}} field of {{technical}} translation often employ what is called machine translation (MT), or <b>machine-assisted</b> translation. This method of translation uses various types of computer software to generate translations from a source language to a target language without the assistance of a human. There are different methods of machine translation. A plethora of machine translators in the form of free search engines are available online. However, {{within the field}} of technical communication, there are two basic types of machine translators, which are able to translate massive amounts of text at a time. There are transfer-based and data-driven machine translators. Transfer-based machine translation systems, which are quite costly to develop, are built by linguists who determine the grammar rules for the source and target languages. The machine works within the rules and guidelines developed by the linguist. Due to the nature of developing rules for the system, this can be very time-consuming and requires an extensive knowledge base about the structures of the languages {{on the part of the}} linguist; nonetheless, the majority of commercial machine translators are transfer-based machines. Yahoo! BabelFish is a common example of a platform that uses this type of translation technology.|$|E
5000|$|Paleoethnobotanists use {{a variety}} of methods to recover and {{identify}} plant remains. Charred plant remains are usually recovered by flotation. The matrix (the soil from a suspected archaeological feature) is slowly added to agitated water. The soil, sand, and other heavy material, known as heavy fraction, will sink to the bottom. The less dense organic material such as charred seeds, grains and charcoal will tend to float to the surface. The material that floats to the top, called light fraction, is poured into a sieve (usually 250-500 µm). The light fraction is then dried and later examined under a low power microscope. Samples of the heavy fraction are also gathered for later analysis. Flotation can be undertaken manually with buckets, [...] or by <b>machine-assisted</b> flotation where water is circulated through a series of tanks by a pump.Waterlogged plant remains are separated from the matrix by a combination of wet-sieving and/or small-scale flotation in a laboratory. [...] Desiccated plant remains are usually recovered by dry-sieving, using a stack of different sieves to separate larger items such as cereal straw and fruit stones from smaller items such as weed seeds.|$|E
40|$|There is a {{consensus}} between many linguists that half of all languages risk disappearing {{by the end of}} the century. Documentation is agreed to be a priority. This includes the process of phonemic analysis to discover the contrastive sounds of a language with the resulting benefits of further linguistic analysis, literacy, and access to speech technology. A <b>machine-assisted</b> approach to phonemic analysis has the potential to greatly speed up the process and make the analysis more objective. It is demonstrated that a <b>machine-assisted</b> approach can make a measurable contribution to a phonemic analysis for all the procedures investigated; phonetic similarity, complementary distribution, and minimal pairs. The evaluation measures introduced in this paper allows a comprehensive quantitative comparison between these phonemic analysis procedures. Given the best available data and the <b>machine-assisted</b> procedures described, there is a strong indication that phonetic similarity is the most important piece of evidence in a phonemic analysis...|$|E
40|$|This paper {{focuses on}} two {{binomial}} identities. The proofs illustrate {{the power and}} elegance in enumerative/algebraic combinatorial arguments, modern <b>machine-assisted</b> techniques of Wilf-Zeilberger and the classical tools of generatingfunctionol-ogy. Key Words and Phrases: recurrence equations, combinatorial identities, Zeilberger Algo-rithm, WZ, generatingfunctionology...|$|E
40|$|This paper {{focuses on}} two {{binomial}} identities. The proofs illustrate {{the power and}} elegance in enumerative/algebraic combinatorial arguments, modern <b>machine-assisted</b> techniques of Wilf-Zeilberger and the classical tools of generatingfunctionology. United States. National Security Agency (Grant H 98230 - 10 - 1 - 0222...|$|E
40|$|A {{categorization}} {{was made}} of independent variables previously found to be potent in simple perceptual-motor tasks. A computer was then used to generate hypothetical factorial designs. These were evaluated in terms of literature trends and pragmatic criteria. Potential side-effects of <b>machine-assisted</b> research strategy were discussed...|$|E
30|$|The {{synthesis}} of 5 '-amino- 3 '-phosphate dinucleotides is reported elsewhere [5]. Tetra- and hexanucleotides were produced by <b>machine-assisted</b> synthesis; dinucleotides were synthesized in solution. All {{the building blocks}} were purified by preparative HPLC, lyophilized, diluted with double distilled water, filtered and stored at - 20 °C.|$|E
40|$|A {{large body}} of work exists for <b>machine-assisted</b> {{analysis}} of cryptographic protocols in the formal (Dolev-Yao) model, i. e., by abstracting cryptographic operators as a free algebra. In particular, proving secrecy by typing has {{shown to be a}} salient technique as it allowed for elegant and fully automated proofs, ofte...|$|E
40|$|Manually annotating {{clinical}} document corpora {{to generate}} reference standards for Natural Language Processing (NLP) systems or Machine Learning (ML) is a timeconsuming and labor-intensive endeavor. Although {{a variety of}} open source annotation tools currently exist, {{there is a clear}} opportunity to develop new tools and assess functionalities that introduce efficiencies into the process of generating reference standards. These features include: management of document corpora and batch assignment, integration of <b>machine-assisted</b> verification functions, semi-automated curation of annotated information, and support of <b>machine-assisted</b> pre-annotation. The goals of reducing annotator workload and improving the quality of reference standards are important considerations for development of new tools. An infrastructure is also needed that will support largescale but secure annotation of sensitive clinical data as well as crowdsourcing which has proven successful for a variety of annotation tasks. We introduce the Extensible Human Oracle Suite of Tools (eHOST...|$|E
40|$|This paper {{focuses on}} the design {{methodology}} of the MultiLingual Dictionary-System (MLDS), which is a human-oriented tool for assisting in the task of translating lexical units, oriented to translators and conceived from {{studies carried out with}} translators. We describe the model adopted for the representation of multilingual dictionary-knowledge. Such a model allows an enriched exploitation of the lexical-semantic relations extracted from dictionaries. As well, MLDS is supplied with knowledge about the use of the dictionaries in the process of lexical translation, which was elicitated by means of empirical methods and specied in a formal language. The dictionary-knowledge along with the task-oriented knowledge are used to oer the translator active, anticipative and intelligent assistance. 1 Introduction: <b>Machine-Assisted</b> Translation and Dictionaries Translation support tools have become a novel and promising branch in the eld of <b>machine-assisted</b> translation {{in the last few years}} [...] . ...|$|E
40|$|We {{describe}} {{our experiences}} verifying real communications hardware using <b>machine-assisted</b> proof. In particular we {{reflect on the}} errors found, problems encountered and the bottlenecks that slowed {{the progress of the}} proofs. We also note techniques which would alleviate the problems. Most of the problems we discuss only become significant when large designs are verified...|$|E
40|$|We {{provide an}} axiomatisation of the Timed Interval Calculus, a {{set-theoretic}} notation for expressing properties of time intervals. We implement the axiomatisation in the Ergo theorem prover {{in order to}} allow the machine-checked proof of laws for reasoning about predicates expressed using interval operators. These laws can be then used in the <b>machine-assisted</b> verification of real-time applications...|$|E
40|$|Here we {{describe}} {{the use of a}} new open-source software package and a Raspberry Pi ® computer for the simultaneous control of multiple flow chemistry devices and its application to a <b>machine-assisted,</b> multi-step flow preparation of pyrazine- 2 -carboxamide – a component of Rifater ®, used in the treatment of tuberculosis – and its reduced derivative piperazine- 2 -carboxamide...|$|E
40|$|AbstractThe Health Insurance Portability and Accountability Act (HIPAA) Safe Harbor method {{requires}} {{removal of}} 18 types of protected health information (PHI) from clinical documents {{to be considered}} “de-identified” prior to use for research purposes. Human review of PHI elements from a large corpus of clinical documents can be tedious and error-prone. Indeed, multiple annotators {{may be required to}} consistently redact information that represents each PHI class. Automated de-identification has the potential to improve annotation quality and reduce annotation time. For instance, using <b>machine-assisted</b> annotation by combining de-identification system outputs used as pre-annotations and an interactive annotation interface to provide annotators with PHI annotations for “curation” rather than manual annotation from “scratch” on raw clinical documents. In order to assess whether <b>machine-assisted</b> annotation improves the reliability and accuracy of the reference standard quality and reduces annotation effort, we conducted an annotation experiment. In this annotation study, we assessed the generalizability of the VA Consortium for Healthcare Informatics Research (CHIR) annotation schema and guidelines applied to a corpus of publicly available clinical documents called MTSamples. Specifically, our goals were to (1) characterize a heterogeneous corpus of clinical documents manually annotated for risk-ranked PHI and other annotation types (clinical eponyms and person relations), (2) evaluate how well annotators apply the CHIR schema to the heterogeneous corpus, (3) compare whether <b>machine-assisted</b> annotation (experiment) improves annotation quality and reduces annotation time compared to manual annotation (control), and (4) assess the change in quality of reference standard coverage with each added annotator’s annotations...|$|E
40|$|We {{present a}} {{detailed}} description of a <b>machine-assisted</b> verification of an algorithm for self-stabilizing mutual exclusion that is due to Dijkstra [Dij 74]. This verification was constructed using PVS. We compare the mechanical verification to the informal proof sketch on which it is based. This comparison yields several observations regarding the challenges of formalizing and mechanically verifying distributed algorithms in general...|$|E
40|$|The presentations {{document}} current {{activities in}} the field of remote sensing. Papers include those concerned with data collection, processing, and analysis hardware and methodology, as well as the application of this technology to monitoring and managing the earth's resources and man's global environment. Ground-based, airborne, and spaceborne sensor systems and both manual and <b>machine-assisted</b> data analysis and interpretation are considered...|$|E
40|$|A {{course in}} <b>machine-assisted</b> {{translation}} at final-year undergraduate level {{is the subject}} of the paper. The course includes a workshop session during which students compile a list of post-editing guidelines to make a text suitable for use in a clearly defined situation, and the paper describes this workshop and considers its place in the course and its future development. Issues of teaching MT to language learners are discussed. ...|$|E
40|$|In {{this paper}} {{we show that}} the {{automated}} reasoning technique of deductive synthesis {{can be applied to}} address the problem of <b>machine-assisted</b> composition of e-Science workflows according to users ’ specifications. We encode formal specifications of e-Science data, services and workflows, constructed from their descriptions, in the generic theorem prover Isabelle. Workflows meeting this specification are then synthesised as a side-effect of proving that these specifications can be met. ...|$|E
