691|19|Public
25|$|The {{mathematical}} {{basis of}} the operations enabled high precision <b>multiword</b> arithmetic subroutines to be built relatively easily.|$|E
25|$|A hyphen is {{also used}} if an {{adjective}} is formed from a <b>multiword</b> name (e.g. Victor Hugó-i ‘typical of V.H.’, San Franciscó-i ‘S.F.-based’). The last vowel is lengthened even in writing if it is pronounced and it is required by phonological rules. If the suffix begins with the same letter as a word-final double letter (e.g. Grimm-mel ‘with Grimm’)., a hyphen is used again.|$|E
25|$|Charles Simonyi, {{who worked}} at Xerox PARC in the 1970s and later oversaw the {{creation}} of Microsoft's Office suite of applications, invented and taught the use of Hungarian Notation, one version of which uses the lower case letter(s) {{at the start of}} a (capitalized) variable name to denote its type. One account claims that the camel case style first became popular at Xerox PARC around 1978, with the Mesa programming language developed for the Xerox Alto computer. This machine lacked an underscore key, and the hyphen and space characters were not permitted in identifiers, leaving camel case as the only viable scheme for readable <b>multiword</b> names. The PARC Mesa Language Manual (1979) included a coding standard with specific rules for upper and lower camel case that was strictly followed by the Mesa libraries and the Alto operating system.|$|E
40|$|Dictionaries are a {{valuable}} {{source of information}} about <b>multiwords.</b> Unfortunately, only few <b>multiwords</b> are explicitly marked as such in dictionaries: most of them are presented without being distinguished from free combinations of words. In this paper we present a methodology for detecting hidden <b>multiwords</b> in bilingual dictionaries, along with their translation in another language. The methodology is based on a number of automatic procedures which exploit regularities in the different kinds of expressions that {{can be found in the}} Collins English-Italian bilingual dictionary to select those phrases that are most likely to contain <b>multiwords.</b> The quantitative results of the experiment are provided. ...|$|R
40|$|Abstract: In this paper, a {{supervised}} {{neural network}} {{has been used}} to classify pairs of terms as being <b>multiwords</b> or non-multiwords. Classification is based on the values yielded by different estimators, currently available in literature, used as inputs for the neural network. Lists of <b>multiwords</b> and nonmultiwords have been built to train the net. Afterward, many other pairs of terms have been classified using the trained net. Results obtained in this classification have been used to perform information retrieval tasks. Experiments show that detecting <b>multiwords</b> results in better performance of the IR methods. 1...|$|R
40|$|Abstract. Previous {{approaches}} {{on automatic}} extraction of lexical similarities have considered as semantic unit of text the word. However, the theoretical perspective of contextual lexical semantics suggests that larger segments of text, specifically non-compositional <b>multiwords,</b> are {{more appropriate for}} this role. We experimentally tested the applicability of this notion, applying automatic collocation extraction to identify and merge such <b>multiwords</b> prior to the similarity estimation process. Employing an automatic comparative evaluation scheme we ascertain improvement of the extracted lexico-semantic knowledge. ...|$|R
2500|$|Acronyms {{are used}} most often to abbreviate names of {{organizations}} and long or frequently referenced terms. The {{armed forces and}} government agencies frequently employ acronyms; some well-known examples from the United States are among the [...] "alphabet agencies" [...] (also jokingly referred to as [...] "alphabet soup") created by Franklin D. Roosevelt (also of course known as FDR) under the New Deal. Business and industry also are prolific coiners of acronyms. The rapid advance {{of science and technology}} in recent centuries seems to be an underlying force driving the usage, as new inventions and concepts with <b>multiword</b> names create a demand for shorter, more manageable names. One representative example, from the U.S. Navy, is COMCRUDESPAC, which stands for commander, cruisers destroyers Pacific; it's also seen as [...] "ComCruDesPac". [...] "YABA-compatible" [...] (where YABA stands for [...] "yet another bloody acronym") is used to mean that a term's acronym can be pronounced but is not an offensive word, e.g., [...] "When choosing a new name, be sure it is 'YABA-compatible'." ...|$|E
50|$|Two {{additional}} Advanced Timing modes {{have been}} {{defined in the}} CompactFlash specification 2.1. Those are <b>Multiword</b> DMA mode 3 and <b>Multiword</b> DMA mode 4. They are specific to CompactFlash. <b>Multiword</b> DMA is only permitted for CompactFlash devices configured in True IDE mode.|$|E
5000|$|... #Subtitle level 2: Machine Translation (MT) of <b>Multiword</b> Expressions ...|$|E
40|$|Previous {{studies on}} {{automatic}} extraction of lexical similarities have considered as semantic unit of text the word. However, {{the theory of}} contextual lexical semantics implies that larger segments of text, namely non-compositional <b>multiwords,</b> are more appropriate for this role. We experimentally tested the applicability of this notion applying automatic collocation extraction to identify and merge such <b>multiwords</b> prior to the similarity estimation process. Employing an automatic WordNet-based comparative evaluation scheme along with a manual evaluation procedure, we ascertain improvement of the extracted similarity relations...|$|R
40|$|In this paper, {{we propose}} a multiwordenhanced author topic model that {{clusters}} authors with similar interests and expertise, {{and apply it}} to an information retrieval system that returns a ranked list of authors related to a keyword. For example, we can retrieve Eugene Charniak via search for statistical parsing. The existing works on author topic modeling assume a “bag-of-words ” representation. However, many semantic atomic concepts are represented by <b>multiwords</b> in text documents. This paper presents a pre-computation step as a way to discover these <b>multiwords</b> in the corpus automatically and tags them in the termdocument matrix. The key advantage of this method is that it retains the simplicity and the computational efficiency of the unigram model. In addition to a qualitative evaluation, we evaluate the results by using the topic models as a component in a search engine. We exhibit improved retrieval scores when the documents are represented via sets of latent topics and authors. ...|$|R
30|$|During this period, the named {{entities}} such {{as people}} names, surnames, and geographical items were assigned into the word classes in recognition dictionary. The vocabulary has been continually updated {{with the new}} words, checked, and corrected. We have introduced filled pauses into the language modeling as transparent words and model some geographically named entities as <b>multiwords.</b> We have also tested a number of methods for language model adaptation to the ted domain and algorithms for text classification and clustering.|$|R
5000|$|In Go, the {{convention}} {{is to use}} [...] or [...] rather than underscores to write <b>multiword</b> names.|$|E
5000|$|The {{mathematical}} {{basis of}} the operations enabled high precision <b>multiword</b> arithmetic subroutines to be built relatively easily.|$|E
50|$|In <b>multiword</b> {{transfer}} mode (Block mode) once transfer has begun {{it will continue}} until all words are transferred.|$|E
40|$|This article {{describes}} a strategy {{based on a}} naive-bayes classifier for detecting the po-larity of English tweets. The experiments {{have shown that the}} best performance is achieved by using a binary classifier be-tween just two sharp polarity categories: positive and negative. In addition, in or-der to detect tweets with and without po-larity, the system makes use of a very basic rule that searchs for polarity words within the analysed tweets/texts. When the clas-sifier is provided with a polarity lexicon and <b>multiwords</b> it achieves 63 % F-score. ...|$|R
40|$|Computational and {{statistical}} techniques for identifying, 4 normalizing, clustering, classifying, selecting, and defining <b>multiwords</b> can revolutionize lexicographical processes. These techniques rely on two basic ideas: (a) the basic statistical insight that a wellselected sample {{can serve as}} a surrogate for the larger universe from which it is drawn; and (b) the fact that, given such a well-selected sample, it is possible to derive other data that can guide the organization of both lexicographical processes and data itself. Almost no one questions the fundamental contribution of corpora {{and statistical}} techniques to contemporary lexicographical practice, as presented in Armstrong (1994), Biber, Conrad, and Reppen (1998), Kennedy (1998) and Thomas and Short (1996). Recent improvements in programming languages, computing power, and storage capacity now permit the use of hitherto prohibitively expensive and complex computational and statistical methods on ordinary desktop computers. The techniques presented in this article are not unique and are hardly specific to lexicography, {{as can be seen in}} Charniak (1993), Manning and Schutze (1999), and Woods, Fletcher, and Hughes (1986). What is novel is an approach to lexicography framed and guided by these techniques. In diis article we focus on their use for dictionaries of contemporary business English; these techniques, however, can be used in any dictionary work. This article will cover the very large classes of <b>multiwords,</b> mostiy in fact multi-part proper and common nouns. These techniques can be applied to idioms and phrasal verbs as well, but diese classes are less likely to produce new terms in business English dian multi-part nouns...|$|R
40|$|This paper aims {{to propose}} an {{alternative}} method for retrieving documents using <b>Multiwords</b> Expressions (MWE) extracted from a document base {{to be used}} as descriptors in search of an Information Retrieval System (IRS). In this sense, unlike methods that consider the text as a set of words, bag of words, we propose a method that takes into account the characteristics of the physical structure of the document in the extraction process of MWE. From this set of terms comparing pre-processed using an exhaustive algorithmic technique proposed by the authors with the results obtained for thirteen different measures of association statistics generated by the software Ngram Statistics Package (NSP). To perform this experiment was set up with a corpus of documents in digital forma...|$|R
50|$|The {{extensions}} {{are specifically}} dedicated to morphology, MRD, NLP syntax, NLP semantics, NLP multilingual notations, NLP morphological patterns, <b>multiword</b> expression patterns, and constraint expression patterns.|$|E
50|$|According to Sag et al. (2002) <b>Multiword</b> Expressions are, {{apart from}} Disambiguation, {{one of the}} two key {{problems}} for Natural Language Processing (NLP) and especially for machine translation (MT).|$|E
5000|$|Loss {{of final}} [...] Loss of [...] {{elsewhere}} unless {{a sequence of}} three consonants would be produced (such constraints operate over <b>multiword</b> sequences of words that are syntactically connected).|$|E
40|$|The paper aims {{to come up}} with {{a system}} that {{examines}} the degree of semantic equivalence between two sentences. At the core of the paper is the attempt to grade the similarity of two sentences by finding the maximal weighted bipartite match between the tokens of the two sentences. The tokens include single words, or <b>multiwords</b> in case of Named Entitites, adjectivally and numerically modified words. Two token similarity measures are used for the task- WordNet based similarity, and a statistical word similarity measure which overcomes the shortcomings of WordNet based similarity. As part of three systems created for the task, we explore a simple bag of words tokenization scheme, a more careful tokenization scheme which captures named entities, times, dates, monetary entities etc., and finally try to capture context around tokens using grammatical dependencies. ...|$|R
40|$|International audienceWords {{interacting}} {{in a text}} may be compared, to {{a certain}} extent, to molecules interacting and building “complexes”, i. e. <b>multiwords,</b> named entities, or longer-range semantic or syntactic associations. We will call them “k-itemsets” in the sequel, k being their interaction level. We have shown (Cadot 06) than an adequately built subset of these k-itemsets is enough for describing {{the entirety of the}} relations at work in a corpus, whatever the level k of these relations. Experimental assessment: we have shown, on a subset of 120, 000 abstracts of Web of Science database in the domain of genomics that a small proportion of these itemsets suffices for discriminating with a measurable precision, fifty sub-categories of genomic research. These ones are issued from an unsupervised categorization process involving the whole 230, 000 1 -itemsets, i. e. individuals words...|$|R
40|$|In {{this paper}} {{we present a}} new, {{multilingual}} data-driven method for coreference resolution as implemented in the SWIZZLE system. The results obtained after training this system on a bilingual corpus of English and Romanian tagged texts, outperformed coreference resolution {{in each of the}} individual languages. 1 Introduction The recent availability of large bilingual corpora has spawned interest in several areas of multilingual text processing. Most of the research has focused on bilingual terminology identication, either as parallel <b>multiwords</b> forms (e. g. the Champollion system (Smadja et al. 1996)), technical terminology (e. g. the Termight system (Dagan and Church, 1994) or broad-coverage translation lexicons (e. g. the SABLE system (Resnik and Melamed, 1997)). In addition, the Multilingual Entity Task (MET) from the TIPSTER program 1 ([URL] projects/tipster/met. htm) challenged the participants in the Message Understanding Conference (MUC) to extr [...] ...|$|R
50|$|Such usage {{is similar}} to <b>multiword</b> file names written for {{operating}} systems and applications that are confused by embedded space codes - such file names instead use an underscore (_) as a word separator, as_in_this_phrase.|$|E
5000|$|A <b>multiword</b> {{expression}} (MWE), {{also called}} phraseme, is a lexeme {{made up of}} a sequence of two or more lexemes that has properties that are not predictable from the properties of the individual lexemes or their normal mode of combination.|$|E
50|$|A <b>multiword</b> {{expression}} {{can be a}} compound, {{a fragment}} of a sentence, or a sentence. The group of lexemes which makup up a MWE can be continuous or discontinuous. It is not always possible to mark a MWE with a part of speech.|$|E
40|$|We are {{presenting}} a new, hybrid alignment architecture for aligning bilingual, linguistically annotated parallel corpora. It {{is able to}} align simultaneously at paragraph, sentence, phrase and word level, using statistical and heuristic cues, along with linguistics-based rules. The system currently aligns English and German texts, and the linguistic annotation used covers POS-tags, lemmas and syntactic constitutents. However, as the system is highly modular, we can easily adapt it to new language pairs {{and other types of}} annotation. The hybrid nature of the system allows experiments with a variety of alignment cues to find solutions to word alignment problems like the correct alignment of rare words and <b>multiwords,</b> or how to align despite syntactic differences between two languages. First performance tests are promising, and we are setting up a gold standard for a thorough evaluation of the system. ...|$|R
40|$|ATLAS – a new text {{alignment}} architecture We are {{presenting a}} new, hybrid alignment architecture for aligning bilingual, linguistically annotated parallel corpora. It {{is able to}} align simultaneously at paragraph, sentence, phrase and word level, using statistical and heuristic cues, along with linguistics-based rules. The system currently aligns English and German texts, and the linguistic annotation used covers POS-tags, lemmas and syntactic constitutents. However, as the system is highly modular, we can easily adapt it to new language pairs {{and other types of}} annotation. The hybrid nature of the system allows experiments with a variety of alignment cues to find solutions to word alignment problems like the correct alignment of rare words and <b>multiwords,</b> or how to align despite syntactic differences between two languages. First performance tests are promising, and we are setting up a gold standard for a thorough evaluation of the system. ...|$|R
40|$|This paper {{presents}} the 'techniques of correcting for spelling errors, orthographical errors, and grammatical errors in computer-based text. And this paper addresses an extension {{that goes beyond}} normal checking of isolated single word by taking <b>multiwords</b> {{as well as a}} sentence. The candidate words for spelling errors are created by applying function of rules and correction rule table that contains heuristic information of collocation. To prevent excessive creation of candidate words and improve accuracy, we use the high frequency word dictionary that contains 300, 000 words derived from corpus. For constituent errors, by applying grammar based partial parsing rules, collocation words errors between the words can be found. We make an experiment with correction techniques on corpora that are the final result of SERI's research, texts, newspaper materials, and public materials. The system has 98 % accuracy rate when the 8. 5 % errors caused by unregistered words were excluded. The average number of prospective candidates suggested by the system is 1. 12. I...|$|R
50|$|The {{number of}} {{annotated}} tokens is 99,480 (the {{difference in the}} number of tokens compared to the initial corpus {{is due to the fact}} that some of them are not linguistic items). The simple word count is 86,842 and <b>multiword</b> expressions (MWE) are 5,797 (12,638 tokens).|$|E
50|$|In the UK, her {{research}} has been funded by the Engineering and Physical Sciences Research Council (EPSRC) and Arts and Humanities Research Council (AHRC). According to Google Scholar and Scopus her most cited publications include papers on minimal recursion semantics, <b>multiword</b> expressions, polysemy, named-entity recognition and feature structure grammars.|$|E
50|$|The {{explanation}} for the difference between single and <b>multiword</b> DMA {{can be found in}} how the Intel 8237 DMA chip works.In single transfer mode, only one word (16-bit) will be transferred between the device and the computer before returning control to the CPU, and later it will repeat this cycle, allowing the CPU to process data while data is transferred.|$|E
40|$|Abstract—We {{report on}} our recent efforts toward a large {{vocabulary}} Vietnamese speech recognition system. In particular, {{we describe the}} Vietnamese text and speech database recently collected {{as part of our}} GlobalPhone corpus. The data was complemented by a large collection of text data crawled from various Vietnamese websites. To bootstrap the Vietnamese speech recognition system we used our Rapid Language Adaptation scheme applying a multilingual phone inventory. After initialization we investigated the peculiarities of the Vietnamese language and achieved significant improvements by implementing different tone modeling schemes, extended by pitch extraction, handling <b>multiwords</b> to address the monosyllable structure of Vietnamese, and featuring language modeling based on 5 -grams. Furthermore, we addressed the issue of dialectal variations between South and North Vietnam by creating dialect dependent pronunciations and including dialect in the context decision tree of the recognizer. Our currently best recognition system achieves a word error rate of 11. 7 % on read newspaper speech. I...|$|R
40|$|TaLTaC 2 (T 2) is a ‘metric’ {{software}} for the automatic analysis of texts of various types (collections of documents and fragments, short texts from surveys, websites etc.). T 2 allows for both a lexical and a textual treatment, and for the full integration of (unstructured) textual data with (structured) codified data - the latter associated with the units of context. The lexical unit of the analysis is mixed, both words and <b>multiwords.</b> The search and extraction of information exploits statistico-linguistic resources – both standard and tailor-made – as well as weighing criteria of a statistical and quantitative nature. Linguistic and semantic annotations are also employed. This allows the user to individuate keywords and elements of terminology {{at the level of}} the whole corpus, of parts of it and of single units of context. The results can be exported via matrices, and this makes multi-dimensional analyses possible that express associations between words and latent sense models...|$|R
40|$|The MEANING Italian Corpus (MIC) is a {{large size}} corpus of written {{contemporary}} Italian, which is being created at ITC-irst, {{in the framework of}} the EU-funded MEANING project. Its novelty consists in the fact that domain-representativeness has been chosen as the fundamental criterion for the selection of the texts {{to be included in the}} corpus. A core set of 42 basic domains, broadly representative of all the branches of knowledge, has been chosen to be represented in the corpus. The MEANING Italian corpus will be encoded using XML and taking into account, whenever possible according to the requirements of our NLP applications, the XML version of the Corpus Encoding Standard (XCES) and the new standard ISO/TC 37 /SC 4 for language resources. A multi-level annotation is planned in order to encode seven different kinds of information: orthographic features, the structure of the text, morphosyntactic information, <b>multiwords,</b> syntactic information, named entities, and word senses. 1...|$|R
