68|0|Public
25|$|Following {{the release}} of the Radeon R520 and GeForce G70 GPU cores with {{programmable}} shaders, the large floating-point throughput drew attention from academic and commercial groups, experimenting with using then for non-graphics work. The interest led ATI (and Nvidia) to create GPGPU products — able to calculate general purpose mathematical formulas in a massively parallel way — to process heavy calculations traditionally done on CPUs and specialized floating-point math co-processors. GPGPUs were projected to have immediate performance gains of a factor of 10 or more, over compared to contemporary <b>multi-socket</b> CPU-only calculation.|$|E
2500|$|Plug {{adaptors}} permit {{two or more}} plugs {{to share}} one socket-outlet, or allow {{the use of a}} plug of different type. There are several common types, including double- and triple-socket blocks, shaver adaptors, and <b>multi-socket</b> strips. [...] Adaptors which allow the use of non-BS1363 plugs, or more than two BS1363 plugs, must be fused. Appliances are designed not to draw more power than their plug is rated for; the use of such adaptors, and also multi-socketed extension leads, makes it possible for several appliances to be connected through a single outlet, with the potential to cause dangerous overloads.|$|E
50|$|Overall {{single socket}} and <b>multi-socket</b> {{throughput}} increased with the T3 processor in systems, providing superior throughput {{with half the}} CPU socket requirements to its predecessor.|$|E
50|$|The PowerCube {{was created}} by two {{graduates}} of industrial design engineering at the Delft University of Technology (TU) in the Netherlands. They noticed that people had problems using conventional <b>multi-socket</b> adaptors and started a company, Allocacoc in 2011, to produce an alternative.|$|E
50|$|The {{first digit}} {{of the model}} number designates the largest {{supported}} <b>multi-socket</b> configuration; thus, E5-26xx v3 models support up to dual-socket configurations, while the E7-48xx v3 and E7-88xx v3 models support up to quad- and eight-socket configurations, respectively. Also, E5-16xx/26xx v3 and E7-48xx/88xx v3 models have no integrated GPU.|$|E
50|$|Introduced in May 2015, Xeon E7-48xx v3 and Xeon E7-88xx v3 series provide higher core counts, higher per-core {{performance}} and improved reliability features, {{compared to the}} previous Xeon E7 v2 generation. Following the usual SKU nomenclature, Xeon E7-48xx v3 and E7-88xx v3 series allow <b>multi-socket</b> operation, supporting up to quad- and eight-socket configurations, respectively. These processors use the LGA 2011 (R1) socket.|$|E
50|$|In March 2010 AMD {{released}} the Magny-Cours Opteron 6100 series CPUs for Socket G34. These are 8- and 12-core multi-chip module CPUs {{consisting of two}} four or six-core dies with a HyperTransport 3.1 link connecting the two dies. These CPUs updated the <b>multi-socket</b> Opteron platform to use DDR3 memory and increased the maximum HyperTransport link speed from 2.40 GHz (4.80 GT/s) for the Istanbul CPUs to 3.20 GHz (6.40 GT/s).|$|E
50|$|The {{first model}} in this line, the Unisite-40, {{featured}} a removable module {{with a single}} 40-pin DIP ZIF socket, called the SITE-40 and space to install optional programming adapters {{to the right of}} this DIP module. Such modules included the 'SetSite,' a module containing eight 40-pin ZIF sockets to allow gang programming of up to eight identical memory devices, and the 'ChipSite,' an early <b>multi-socket</b> module accommodating several sizes of PLCC and SOIC DIP packages with 'clamshell' ZIF sockets.|$|E
50|$|Westmere-EP is {{the first}} six-core dual-socket {{processor}} from Intel, following the quad-core Bloomfield and Gainestown (also known as Nehalem-EP) processors using the same LGA 1366 package, while the earlier Dunnington six-core processor is a Socket 604 based <b>multi-socket</b> processor. The CPUID extended model number is 44 (2Ch) and two product codes are used, 80613 for the UP desktop/server models and 80614 for the Xeon 5600-series DP server models. In some models, only {{four of the six}} cores are enabled.|$|E
50|$|The Xeon E5-16xx {{processors}} {{follow the}} previous Xeon 3500/3600-series products as the high-end single-socket platform, using the LGA 2011 package introduced with this processor. They share the Sandy Bridge-E platform with the single-socket Core i7-38xx and i7-39xx processors. The CPU chips have no integrated GPU but eight CPU cores, {{some of which}} are disabled in the entry-level products. The Xeon E5-26xx line has the same features but also enables <b>multi-socket</b> operation like the earlier Xeon 5000-series and Xeon 7000-series processors.|$|E
5000|$|Unlike the front-side bus (FSB), QPI is a {{point-to-point}} interface {{and supports}} not only processor-chipset interface, but also processor-to-processor connection and chip-to-chip connection. The X58 has two QPIs and can directly connect to two processors on a <b>multi-socket</b> motherboard or form a ring-like connection (processor 1 to X58 to processor 2 back to processor 1). When {{used with the}} Intel Core i7, the second QPI is usually unused (though, in principle, the second X58 might be daisy-chained on the board).|$|E
50|$|Following {{the release}} of the Radeon R520 and GeForce G70 GPU cores with {{programmable}} shaders, the large floating-point throughput drew attention from academic and commercial groups, experimenting with using then for non-graphics work. The interest led ATI (and Nvidia) to create GPGPU products — able to calculate general purpose mathematical formulas in a massively parallel way — to process heavy calculations traditionally done on CPUs and specialized floating-point math co-processors. GPGPUs were projected to have immediate performance gains of a factor of 10 or more, over compared to contemporary <b>multi-socket</b> CPU-only calculation.|$|E
5000|$|The Xeon [...] is a {{brand of}} x86 {{microprocessors}} designed, manufactured, and marketed by Intel, targeted at the non-consumer workstation, server, and embedded system markets. The primary advantages of the Xeon CPUs, {{when compared to the}} majority of Intel's desktop-grade consumer CPUs, are their <b>multi-socket</b> capabilities, higher core counts, larger cache memory, and support for ECC memory. A Xeon Phi co-processor, or co-processors, can be used alongside an existing Intel Xeon processor to provide increased/improved computing power, requiring an available PCI Express 3.0 ×16 slot in ×16, or ×8 mode.|$|E
50|$|A typical TAE {{installation}} is a <b>multi-socket</b> {{junction box}} {{with at least}} one N connector and one F connector socket in the box, but having usually two N and one F connector. Up to three N connectors are possible. Network service enters the box and connects to pins 1 and 2 on the right-most N connector. Service is daisy-chained from pins 5 and 6 on the N connector to pins 1 and 2 on the next N connector to the left. The F connector is attached to pins 5 and 6 on the last N connector {{at the end of the}} chain.|$|E
50|$|Plug {{adaptors}} permit {{two or more}} plugs {{to share}} one socket-outlet, or allow {{the use of a}} plug of different type. There are several common types, including double- and triple-socket blocks, shaver adaptors, and <b>multi-socket</b> strips. Adaptors which allow the use of non-BS 1363 plugs, or more than two BS 1363 plugs, must be fused. Appliances are designed not to draw more power than their plug is rated for; the use of such adaptors, and also multi-socketed extension leads, makes it possible for several appliances to be connected through a single outlet, with the potential to cause dangerous overloads.|$|E
50|$|The first {{processor}} to implement Intel 64 was the <b>multi-socket</b> processor Xeon code-named Nocona in June 2004. In contrast, the initial Prescott chips (February 2004) did not enable this feature. Intel subsequently began selling Intel 64-enabled Pentium 4s using the E0 {{revision of the}} Prescott core, being sold on the OEM market as the Pentium 4, model F. The E0 revision also adds eXecute Disable (XD) (Intel's name for the NX bit) to Intel 64, and has been included in then current Xeon code-named Irwindale. Intel's official launch of Intel 64 (under the name EM64T at that time) in mainstream desktop processors was the N0 stepping Prescott-2M.|$|E
50|$|A {{power strip}} (also {{known as an}} {{extension}} block, power board, power bar, plug board, trailing gang, trailing socket, plug bar, trailer lead, <b>multi-socket,</b> multiple socket, multiple outlet, polysocket and by many other variations) is a block of electrical sockets that attaches {{to the end of}} a flexible cable (typically with a mains plug on the other end), allowing multiple electrical devices to be powered from a single electrical socket. Power strips are often used when many electrical devices are in proximity, such as for audio, video, computer systems, appliances, power tools, and lighting. Power strips often include a circuit breaker to interrupt the electric current in case of an overload or a short circuit. Some power strips provide protection against electrical power surges. Typical housing styles may include strip, rack-mount, under-monitor and direct plug-in.|$|E
5000|$|Although some {{high-end}} Core i7 processors expose QPI, other [...] "mainstream" [...] Nehalem desktop {{and mobile}} processors intended for single-socket boards (e.g. LGA 1156 Core i3, Core i5, and other Core i7 processors from the Lynnfield/Clarksfield and successor families) do not expose QPI externally, because these processors {{are not intended}} to participate in <b>multi-socket</b> systems. However, QPI is used internally on these chips {{to communicate with the}} [...] "uncore", {{which is part of the}} chip containing memory controllers, CPU-side PCI Express and GPU, if present; the uncore may or may not be on the same die as the CPU core, for instance it is on a separate die in the Westmere-based Clarkdale/Arrandale. These post-2009 single-socket chips communicate externally via the slower DMI and PCI Express interfaces, because the functions of the traditional northbridge are actually integrated into these processors, starting with Lynnfield, Clarksfield, Clarkdale and Arrandale; thus, there is no need to incur the expense of exposing the (former) front-side bus interface via the processor socket. Although the core-uncore QPI link is not present in desktop and mobile Sandy Bridge processors (as it was on Clarkdale, for example), the internal ring interconnect between on-die cores is also based on the principles behind QPI, at least as far as cache coherency is concerned.|$|E
40|$|Abstract — Traditionally CPU {{workload}} {{scheduling and}} fan control in <b>multi-socket</b> {{systems have been}} designed sep-arately leading to less efficient solutions. In this paper we present Cool and Save, a cooling aware dynamic workload management strategy that is significantly more energy ef-ficient than state-of-the art solutions in <b>multi-socket</b> CPU systems because it performs workload scheduling in tan-dem with controlling socket fan speeds. Our experimental results indicate that applying our scheme gives average fan energy savings of 73 % concurrently with reducing the max-imum fan speed by 53 %, thus leading to lower vibrations and noise levels. I...|$|E
40|$|Part 2 : AlgorithmsInternational audienceThis paper {{describes}} {{the acceleration of}} the most computationally intensive kernels of the Blender rendering engine, Blender Cycles, using Intel Many Integrated Core architecture (MIC). The proposed parallelization, which uses OpenMP technology, also improves {{the performance of the}} rendering engine when running on multi-core CPUs and <b>multi-socket</b> servers. Although the GPU acceleration is already implemented in Cycles, its functionality is limited. Our proposed implementation for MIC architecture contains all features of the engine with improved performance. The paper presents performance evaluation for three architectures: <b>multi-socket</b> server, server with MIC (Intel Xeon Phi 5100 p) accelerator and server with GPU accelerator (NVIDIA Tesla K 20 m) ...|$|E
40|$|<b>Multi-socket,</b> {{multi-core}} {{computers are}} becoming ubiquitous, especially as nodes in compute clusters of all sizes. Common memory benchmarks and memory performance models treat memory as characterized by well-defined maximum bandwidth and average latency parameters. In contrast, {{current and future}} systems are based on deep hierarchies and NUMA memory systems, which are not easily described this simply. Memory performance characterization of <b>multi-socket,</b> multi-core systems require measurements and models more sophisticated than than simple peak bandwidth/minimum latency models. To investigate this issue, we performed a detailed experimental study of the memory performance {{of a variety of}} AMD <b>multi-socket</b> quad-core systems. We used the pChase benchmark to generate memory system loads with a variable number of concurrent memory operations in the system across a variable number of threads pinned to specific chips in the system. While processor differences had minor but measurable impact on bandwidth, the make-up and structure of the memory has major impact on achievable bandwidth. Our experiments exposed 3 different bottlenecks at different levels of the hardware architecture: limits on the number of references outstanding per thread; limits to the memory requests serviced by a single memory channel; and limits on the total global memory references outstanding wer...|$|E
40|$|With {{the rise}} of <b>multi-socket</b> {{multi-core}} CPUs {{a lot of effort}} is being put into how to best exploit their abundant CPU power. In a shared memory setting the <b>multi-socket</b> CPUs are equipped with their own memory module, and access memory modules across sockets in a non-uniform access pattern (NUMA). Memory access across socket is relatively expensive compared to memory access within a socket. One of the common solutions to minimize across socket memory access is to partition the data, such that the data affinity is maintained per socket. In this paper we explore the role of memory mapped storage to provide transparent data access in a NUMA environment, without the need of explicit data partitioning. We compare the performance of a database engine in a distributed setting in a <b>multi-socket</b> environment, with a database engine in a NUMA oblivious setting. We show that though the operating system tries to keep the data affinity to local sockets, a significant remote memory access still occurs, as the number of threads increase. Hence, setting explicit process and memory affinity results into a robust execution in NUMA oblivious plans. We use micro-experiments and SQL queries from the TPC-H benchmark to provide an in-depth experimental exploration of the landscape, in a four socket Intel machine...|$|E
40|$|Multi-core {{computers}} are ubiquitous and <b>multi-socket</b> versions dominate as nodes in compute clusters. Given {{the high level}} of parallelism inherent in processor chips, the ability of memory systems to serve a large number of concurrent memory access operations is becoming a critical performance problem. The most common model of memory performance uses just two numbers, peak bandwidth and typical access latency. We introduce concurrency as an explicit parameter of the measurement and modeling processes to characterize more accurately the complexity of memory behavior of <b>multi-socket,</b> multi-core systems. We present a detailed experimental <b>multi-socket,</b> multi-core memory study based on the PCHASE benchmark, which can vary memory loads by controlling the number of concurrent memory references per thread. The make-up and structure of the memory {{have a major impact on}} achievable bandwidth. Three discrete bottlenecks were observed at different levels of the hardware architecture: limits on the number of references outstanding per core; limits to the memory requests serviced by a single memory controller; and limits on the global memory concurrency. We use these results to build a memory performance model that ties concurrency, latency and bandwidth together to create a more accurate model of overall performance. We show that current commodity memory sub-systems cannot handle the load offered by high-end processor chips. I...|$|E
40|$|Abstract – As {{we enter}} the era of {{large-scale}} Chip Multi-Processing (CMP) systems, evaluating architectures and projecting performance for commercial workloads on such systems is becoming increasingly important. One of the major areas of concern for <b>Multi-Socket</b> SMP systems has been the detrimental effects of Locking and Synchronization (L&S) overheads. However, the lower on-die interconnect latency and higher available bandwidth in CMP systems can change the effects of L&S dramatically. We wanted to analyze and study these effects. Towards this goal, we built a flexible, fast and accurate platform simulation framework called ManySim, and used this to study effects of locking and synchronization. We demonstrate that the CMP architecture outperforms the <b>multi-socket</b> architecture due to reduced L&S overheads, making the CMP architecture highly scalable and allowing it to almost reach the limits of Amdahl’s law. I...|$|E
40|$|In several {{experiments}} using NASA's Advanced Communications Technology Satellite (ACTS), investigators have reported disappointing throughput using the transmission control protocol/Internet protocol (TCP/IP) protocol suite over 1. 536 Mbit/sec (T 1) satellite circuits. A {{detailed analysis of}} file transfer protocol (FTP) file transfers reveals that both the TCP window size and the TCP 'slow starter' algorithm contribute to the observed limits in throughput. In this paper we summarize the experimental and and theoretical analysis of the throughput limit imposed by TCP on the satellite circuit. We then discuss in detail {{the implementation of a}} <b>multi-socket</b> FTP, XFTP client and server. XFTP has been tested using the ACTS system. Finally, we discuss a preliminary set of tests on a link with non-zero bit error rates. XFTP shows promising performance under these conditions, suggesting the possibility that a <b>multi-socket</b> application may be less effected by bit errors than a single, large-window TCP connection...|$|E
40|$|GPUs achieve high {{throughput}} {{and power}} efficiency by employing many small single instruction multiple thread (SIMT) cores. To minimize scheduling logic and performance variance they utilize a uniform memory system and leverage strong data parallelism exposed via the programming model. With Moore's law slowing, for GPUs to continue scaling performance (which largely depends on SIMT core count) {{they are likely}} to embrace <b>multi-socket</b> designs where transistors are more readily available. However when moving to such designs, maintaining the illusion of a uniform memory system is increasingly difficult. In this work we investigate <b>multi-socket</b> non-uniform memory access (NUMA) GPU designs and show that significant changes are needed to both the GPU interconnect and cache architectures to achieve performance scalability. We show that application phase effects can be exploited allowing GPU sockets to dynamically optimize their individual interconnect and cache policies, minimizing the impact of NUMA effects. Our NUMA-aware GPU outperforms a single GPU by 1. 5 ×, 2. 3 ×, and 3. 2 × while achieving 89...|$|E
40|$|This paper {{presents}} the most exhaustive study of syn-chronization to date. We span multiple layers, from hardware cache-coherence protocols up to high-level concurrent software. We {{do so on}} different types of architectures, from single-socket – uniform and non-uniform – to <b>multi-socket</b> – directory and broadcast-based – many-cores. We draw a set of observations that, roughly speaking, imply that scalability of synchroniza-tion is mainly a property of the hardware. ...|$|E
40|$|To fuel an {{increasing}} need for parallel performance, system designers have resulted to using multiple sockets {{to provide more}} hardware parallelism. These multisocket systems have limited off-chip bandwidth due to their electrical interconnect which is both power and pin limited. Current systems often use of a Non-Uniform Memory Architecture (NUMA) {{to get the most}} system memory bandwidth from limited off-chip bandwidth. A NUMA system complicates the work of a performance programmer or operating system, because they must maintain data locality to maintain performance. Silicon photonics is an emerging technology that promises great off-chip bandwidth density and energy efficiency when compared to electrical signaling. With this abundance of bandwidth, {{it will be possible to}} build a relatively flat, high bandwidth memory interconnect. Because this interconnect has uniform bandwidth, NUMA optimizations will be unnecessary, which increases performance programmer productivity. If the penalties to making a <b>multi-socket</b> system are negated by the use of silicon photonics, there is less incentive to integrate, and economic incentives to disintegrate. In this thesis, we present this scalable and coherent <b>multi-socket</b> design along with discussing the tradeoffs facing an architect when incorporating silicon photonics technology. Chapter...|$|E
40|$|Future single-board <b>multi-socket</b> {{systems may}} be unable to deliver the needed memory {{bandwidth}} electrically due to power limitations, which will hurt their ability to drive performance improvements. Energy efficient o ff-chip silicon photonics could be used to deliver the needed bandwidth, and it could be extended on-chip to create a relatively flat network topology. That flat network may make it possible to implement the same number of cores with a greater number of small dies for a cost advantage with negligible performance degradation...|$|E
40|$|The {{dataflow}} {{model is}} gaining popularity as a paradigm for programming multicore processors and <b>multi-socket</b> systems of such processors. This work proposes a programming interface and an implementation for a dataflow-based scheduler, which dispatches tasks dynamically at runtime. The scheduler relies on data dependency analysis between tasks in a sequential representation of an algorithm, which provides productivity and facilitates rapid prototyping for developers. Also, {{through the application}} of dataflow principles, it ensures efficient scheduling with provisions for data reuse. Although designed with generalit...|$|E
40|$|Memory {{bandwidth}} {{has become}} a major performance bottleneck as more and more cores are integrated onto a single die, demand-ing more and more data from the system memory. Several prior studies have demonstrated that this memory bandwidth problem can be addressed by employing a 3 D-stacked memory architec-ture, which provides a wide, high frequency memory-bus interface. Although previous 3 D proposals already provide as much band-width as a traditional L 2 cache can consume, the dense through-silicon-vias (TSVs) of 3 D chip stacks can provide still more band-width. In this paper, we contest that we need to re-architect our memory hierarchy, including the L 2 cache and DRAM interface, so that it can take full advantage of this massive bandwidth. Our technique, SMART- 3 D, is a new 3 D-stacked memory architecture with a vertical L 2 fetch/write-back network using a large array of TSVs. Simply stated, we leverage the TSV bandwidth to hide latency behind very large data transfers. We analyze the design trade-offs for the DRAM arrays, careful enough to avoid compro-mising the DRAM density because of TSV placement. Moreover, we propose an efcient mechanism to manage the false sharing prob-lem when implementing SMART- 3 D in a <b>multi-socket</b> system. For single-threaded memory-intensive applications, the SMART- 3 D ar-chitecture achieves speedups from 1. 53 to 2. 14 over planar designs and from 1. 27 to 1. 72 over prior 3 D designs. We achieve simi-lar speedups for multi-program and multi-threaded workloads on multi-core and <b>multi-socket</b> processors. Furthermore, SMART- 3 D can even lower the energy consumption in the L 2 cache and 3 D DRAM for it reduces the total number of row buffer misses. 1...|$|E
40|$|We {{present a}} {{computational}} modeling framework for data-driven simulations {{and analysis of}} infectious disease spread in large populations. For the purpose of efficient simulations, we devise a parallel solution algorithm targeting <b>multi-socket</b> shared memory architectures. The model integrates infectious dynamics as continuous-time Markov chains and available data such as animal movements or aging are incorporated as externally defined events. To bring out parallelism and accelerate the computations, we decompose the spatial domain and optimize cross-boundary communication using dependency-aware task scheduling. Using registered livestock data at a high spatio-temporal resolution, we demonstrate that our approach not only is resilient to varying model configurations, but also scales on all physical cores at realistic work loads. Finally, we show that these very features enable the solution of inverse problems on national scales. Comment: 27 pages, 5 figure...|$|E
40|$|Breadth-First Search is an {{important}} kernel used by many graph-processing applications. In many of these emerging applications of BFS, such as analyzing social networks, the input graphs are low-diameter and scale-free. We propose a hybrid approach that is advantageous for low-diameter graphs, which combines a conventional top-down algorithm along with a novel bottom-up algorithm. The bottom-up algorithm can dramatically {{reduce the number of}} edges examined, which in turn accelerates the search as a whole. On a <b>multi-socket</b> server, our hybrid approach demonstrates speedups of 3. 3 – 7. 8 on a range of standard synthetic graphs and speedups of 2. 4 – 4. 6 on graphs from real social networks when compared to a strong baseline. We also typically double the performance of prior leading shared memory (multicore and GPU) implementations...|$|E
40|$|This paper {{explores the}} {{potential}} of utilizing approximate system load information to enhance work stealing for dynamic load balancing in hierarchical multicore systems. Maintaining information about the load of a system has not been extensively researched since it is assumed to introduce performance overheads. We propose SWAS, a lightweight approximate scheme for retrieving and using such information, based on compact bit vector structures and lightweight update operations. This approximate information is used to enhance the effectiveness of work stealing decisions. Evaluating SWAS {{for a number of}} representative scenarios on a <b>multi-socket</b> multi-core platform showed that work stealing guided by approximate system load information achieves considerable performance improvements: up to 18. 5 % for dynamic, severely imbalanced workloads; and up to 34. 4 % for workloads with complex task dependencies, when compared with random work stealing...|$|E
40|$|Abstract — On a <b>multi-socket</b> {{architecture}} {{with load}} below peak, {{as is often}} the case in a server installation, it is common to consolidate load onto fewer sockets to save processor power. However, this can increase main memory power consumption due to the decreased total cache space. This paper describes inter-socket victim cacheing, a technique that enables such a system to do both load consolidation and cache aggregation at the same time. It uses the last level cache of an idle processor in a connected socket as a victim cache, holding evicted data from the active processor. This enables expensive main memory accesses to be replaced by cheaper cache hits. This work examines both static and dynamic victim cache management policies. Energy savings is as high as 32. 5 %, and averages 5. 8 %. I...|$|E
40|$|We {{introduce}} “asynchronized concurrency (ASCY), ” {{a paradigm}} consisting of four complementary programming patterns. ASCY {{calls for the}} design of concurrent search data structures (CSDSs) to resemble that of their sequen-tial counterparts. We argue that ASCY leads to implemen-tations which are portably scalable: they scale across differ-ent types of hardware platforms, including single and <b>multi-socket</b> ones, for various classes of workloads, such as read-only and read-write, and according to different performance metrics, including throughput, latency, and energy. We sub-stantiate our thesis through the most exhaustive evaluation of CSDSs to date, involving 6 platforms, 22 state-of-the-art CSDS algorithms, 10 re-engineered state-of-the-art CSDS algorithms following the ASCY patterns, and 2 new CSDS algorithms designed with ASCY in mind. We observe up to 30 % improvements in throughput in the re-engineered algo-rithms, while our new algorithms out-perform the state-of-the-art alternatives...|$|E
