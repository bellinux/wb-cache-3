23|19|Public
50|$|A {{compound}} traverse {{is where}} an open traverse is linked at its ends to an existing traverse {{to form a}} closed traverse. The closing line may be defined by coordinates at the end points which have been determined by previous survey. The difficulty is, where there is linear <b>misclosure,</b> {{it is not known}} whether the error is in the new survey or the previous survey.|$|E
5000|$|In the {{parametric}} adjustment, {{the second}} design matrix is an identity, B=-I, and the <b>misclosure</b> vector {{can be interpreted}} as the pre-fit residuals, , so the system simplifies to:which {{is in the form of}} ordinary least squares. In the conditional adjustment, the first design matrix is null, A=0.For the more general cases, Lagrange multipliers are introduced to relate the two Jacobian matrices and transform the constrained least squares problem into an unconstrained one (albeit a larger one). In any case, their manipulation leads to the [...] and [...] vectors as well as the respective parameters and observations a posteriori covariance matrices.|$|E
40|$|As {{access to}} {{information}} changes with increased use of technology, privacy becomes an increasingly prominent issue among technology users. Privacy concerns should be taken seriously because they influence system adoption, the way a system is used, and may even lead to system disuse. Threats to privacy are not only due to traditional security and privacy issues; human factors issues such as unintentional disclosure of information also influence the preservation of privacy in technology systems. A dual pronged approach {{was used to examine}} privacy. First, a broad investigation of younger and older adults' privacy behaviors was conducted. The goal {{of this study was to}} gain a better understanding of privacy across technologies, to discover the similarities, and identify the differences in what privacy means across contexts as well as provide a means to evaluate current theories of privacy. This investigation resulted in a categorization of privacy behaviors associated with technology. There were three high level privacy behavior categories identified: avoidance, modification, and alleviatory behavior. This categorization furthers our understanding about the psychological underpinnings of privacy concerns and suggests that 1) common privacy feelings and behaviors exist across people and technologies and 2) alternative designs which consider these commonalities may increase privacy. Second, I examined one specific human factors issue associated with privacy: disclosure error. This investigation focused on gaining an understanding of how to support privacy by preventing <b>misclosure.</b> A <b>misclosure</b> is an error in disclosure. When information is disclosed in error, or misclosed, privacy is violated in that information not intended for a specific person(s) is nevertheless revealed to that person. The goal of this study was to provide a psychological basis for design suggestions for improving privacy in technology which was grounded in empirical findings. The study furthers our understanding about privacy errors in the following ways: First, it demonstrates for the first time that both younger and older adults experience misclosures. Second, it suggests that misclosures occur even when technology is very familiar to the user. Third, it revealed that some <b>misclosure</b> experiences result in negative consequences, suggesting <b>misclosure</b> is a potential threat to privacy. Finally, by exploring the context surrounding each reported <b>misclosure,</b> I was able to propose potential design suggestions that may decrease the likelihood of <b>misclosure.</b> Ph. D. Committee Chair: Fisk, Arthur; Committee Member: Catrambone, Richard; Committee Member: Foley, Jim; Committee Member: Jeffries, Robin; Committee Member: Rogers, Wend...|$|E
40|$|Traversing is a {{fundamental}} operation in surveying and {{the assessment of the}} quality of a traverse is a skill that every surveyor develops. Acceptable traverses have angular and linear <b>misclosures</b> that fall within acceptable bounds; which would permit the adjustment of the traverse measurements to remove mathematical inconsistencies. Unfortunately these <b>misclosures</b> tell very little about the precision of the location of the traverse stations - although large <b>misclosures</b> are good indicators of gross errors - and more sophisticated mathematical techniques are required for proper traverse analysis. This paper presents some relatively simple techniques that can be employed to give reliable estimations of the precision of traverse stations that allows a simple assessment of the quality of a traverse...|$|R
40|$|K e y w o r d s: {{levelling}} networks, random errors, systematic errors. A b s t r a c t Paper presents {{statistical evaluation}} of accuracy of levelling network measured in Poland in years 1999 – 2001. The analysis was done using 16 150 <b>misclosures</b> from the double levelling of the sections, 382 <b>misclosures</b> from the double levelling {{of the lines}} and 133 loops <b>misclosures.</b> The statistical analysis was conducted by the regression method, correlation method and the analysis of variance. It results that the measured height differences have various accuracy (analysis of variance), and that systematic errors are changing according to the value and sign. The existence of systematic errors causes that the successive neighboring sections of some levelling lines are correlated. The correlation {{in the majority of}} the lines is not statistically essential...|$|R
40|$|The aim of {{the present}} paper is to {{investigate}} the accuracy of levelling {{along the lines of}} the First Order Levelling Network of Greece as deduced from the <b>misclosures</b> of its loops. Actual standard deviation of the standardized circuit <b>misclosures</b> significantly larger than 1 reveals correlation of the individual height differences between consecutive bench marks along a given levelling line accompanied by various other systematic effects. Correlation also exists between forward and backward levelling runs. The detection of correlations neither leads to their source nor eliminates them. However, especially for a high degree of correlation, the investigation of their causes, and their inclusion in the network adjustment may be helpful for the analysis and an improvement of the results...|$|R
40|$|Synthesis {{methods of}} control system {{regulators}} with the given properties of a closed system are {{considered in the}} paper aiming at the development of synthesis methods of automatic control system regulators {{on the base of}} the minimization of the <b>misclosure</b> functional between the source dynamics and the desired dynamics of a closed system (standard). As a result the methodology of the automatic regulation system synthesis in a time region on the base of the <b>misclosure</b> functional minimization has been developed as well as the method of the optimum regulator synthesis. The construction asymptotical, non-asymptotical and algebraic methodologies for the regulators with a priori properties of a closed system are the results of the paper. Construction methodologies for the regulators with a priori properties of a closed system have been introduced in to operationAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|E
40|$|Various {{notions of}} the {{projection}} method optimality are investigated in the paper aiming at the search of new criteria of the projection method optimality, {{the creation of new}} methods of the error estimation obtaining and the construction of new optimum projection methods. As a result formulae for the approximate solution have been obtained. New criteria of the projection method optimality have been found. Optimality numbers have been introduced and investigated. Some known earlier results have been generalized. Methods of the construction of optimum on accuracy and asymptotically optimum in the <b>misclosure</b> sense projection methods have been suggestedAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|E
40|$|The lifelogging {{activity}} enables users, the lifeloggers, to passively capture images using wearable cameras from a {{first person}} perspective and ultimately create a visual diary encoding every possible {{aspect of their}} life with unprecedented details. This growing phenomenon, has posed several privacy concerns for the lifeloggers (people wearing the device), and bystanders (any person who is captured in the images). In this paper, we present a user- study to understand the sharing preferences of the lifeloggers for the images captured in difference scenarios with different audience groups. Our findings motivate the need to design privacy preserving techniques, which will automatically recommend sharing decisions which will help the lifeloggers avoid <b>misclosure,</b> i. e. wrongly sharing a sensitive image {{with one or more}} sharing groups...|$|E
30|$|A “preslip” {{hypothesis}} is proposed {{in terms of}} earthquake prediction (e.g., Kamigaichi and Tsukada, 2006). The preslip {{is thought to be}} a quasi-static preseismic slip that would lead to the main earthquake rupture (e.g., Yoshida and Kato, 2005). A controversial observational example of preslip is that large <b>misclosures</b> in the leveling survey just {{a few days before the}} 1944 Tonankai earthquake are interpreted as the accelerating tilt deformation caused by the preslip of the earthquake (Mogi, 1984; Linde and Sacks, 2002). However, there is a possibility that the observed <b>misclosures</b> are observational errors (Sagiya, 2004). Measurements with borehole strainmeters and other high-precision instruments for inland earthquakes have also shown no indication of clear precursory deformation (Johnston et al., 1987, 2006). Thus it should be noticed that there is no clear and widely accepted observational evidence of preslip.|$|R
40|$|From {{the end of}} {{the twentieth}} century, {{commercial}} vendors have introduced airborne three line scanners, instead of more traditional single line scanners. While ADS 40 (Leica Geosystems) and TLS (STARLABO) placed CCD arrays on the focal plane in a single optical system, 3 -DAS- 1 and 3 -OC (Wehrli & Associates) use three optical systems but rigidly fixed to each other. For this reason, we need to develop a photogrammetric model for three different cameras but moving together along a single flight trajectory. In this paper, we present a sensor model for such a three line scanner and a piece-wise polynomial trajectory model. Preliminary triangulation results show that when geometric constraints on the trajectory model are loosely weighted, the <b>misclosures</b> in the image space are around 3 pixels while <b>misclosures</b> were increased up to 9 pixels if the geometric constraints are strictly enforced...|$|R
40|$|To extract {{different}} types of roads such as arterial roads and local roads and to update the existing road database, a voxel-based skewness and kurtosis balancing algorithm is proposed in this thesis. Vector data is applied to divide lidar points into various tiles. For each tile, a voxel-based skewness and kurtosis balancing algorithm is utilised to extract roads. The initial extraction results are refined by width constraint, spatial interpolation and curve fitting which remove the false positives and join up <b>misclosures.</b> This method {{is based on the}} assumption of a normal distribution of the lidar points, and takes little time in parameter-tuning. The test results show an acceptable accuracy and completeness. To deal with areas where vector data is not available, an edge-clustering algorithm is proposed to extract elongated road segments from airborne lidar data and aerial images. Multiple criteria such as height, elevation difference, intensity, band-ratio and length constraint are applied to extract road segments. After the initial extraction, a series of refinements are conducted, including k-Nearest-Neighbouring clustering, searching intersection nodes, spatial interpolation and curve fitting which connect the <b>misclosures</b> and remove the open areas (e. g. parking lots). The results indicate that the proposed approach is practical to extract roads from airborne lidar data and aerial images at an acceptable accuracy. Edge-clustering algorithms often fail to extract complete roads in occluded areas because of the minimum length constraint, which is also validated by the statistics of the undetected road length. To tackle this problem, a colour-space based algorithm is proposed. The main idea of this algorithm is to explore the contextual road information in different colour spaces. Red/Green/Blue channels are transformed into Hue/Saturation/Intensity and Brightness/Blue Difference/Red Difference colour spaces. Various constraints are applied to detect road points. After the extraction, refinements consisting of k-Nearest-Neighbouring clustering, spatial interpolation and curve fitting are applied to connect the <b>misclosures</b> and remove the open areas. The statistics of the tests show better results than the edge-clustering method in terms of the detected road length...|$|R
40|$|The DIA {{method for}} the detection, {{identification}} and adaptation of model misspecifications combines estimation with testing. The {{aim of the}} present contribution is to introduce a unifying framework for the rigorous capture of this combination. By using a canonical model formulation and a partitioning of <b>misclosure</b> space, we show that the whole estimation–testing scheme can be captured in one single DIA estimator. We study the characteristics of this estimator and discuss some of its distributional properties. With {{the distribution of the}} DIA estimator provided, one can then study all the characteristics of the combined estimation and testing scheme, as well as analyse how they propagate into final outcomes. Examples are given, as well as a discussion on how the distributional properties compare with their usage in practice. </p...|$|E
40|$|An {{investigation}} of further possible vertical crustal {{movements in the}} Western United States made with circuit microclosure analysis is presented. The San Andreas fault in Cal., the Nevada seismic zone in Nev., and the Sierra Nevada in Calif. were studied based on supposition that in areas undergoing crustal movement the <b>misclosure</b> for a particular circuit should have the smallest value when the circuit is formed from the most temporarily homogeneous survey data; it should have larger, predictable values when the circuit is closed with surveys conducted at other times. Leveling surveys along the San Andreas fault and the Nevada seismic zone are discussed, noting the possibility of regional tilting in the Great Basin between 1934 and 1955, and of elevation changes in the Northern Nevada Range using results of leveling surveys between Roseville, Cal. and Reno, Nev...|$|E
40|$|Abstract: The static {{calibration}} {{and analysis of}} the Velodyne HDL- 64 E S 2 scanning LiDAR system is presented and analyzed. The mathematical model for measurements for the HDL- 64 E S 2 scanner is derived and discussed. A planar feature based least squares adjustment approach is presented and utilized in a minimally constrained network in order to derive an optimal solution for the laser’s internal calibration parameters. Finally, the results of the adjustment along with a detailed examination of the adjustment residuals are given. A three-fold improvement in the planar <b>misclosure</b> residual RMSE over the standard factory calibration model was achieved by the proposed calibration. Results also suggest that there may still be some unmodelled distortions in the range measurements from the scanner. However, despite this, the overall precision of the adjusted laser scanner data appears to make it a viable choice for high accuracy mobile scanning applications...|$|E
40|$|Copyright © by the paper's authors. In this paper, we {{proposed}} a road extraction algorithm that utilises voxel-based skewness and kurtosis balancing in order to update existing road networks accurately and reliably from airborne lidar data. The {{proposed a}}lgorithm consists of initial road extraction followed {{by a series of}} refinements including width constraints, spatial interpolations and curve fitting which are successfully implemented to reduce false-positives, join up <b>misclosures</b> and determine centrelines of the extracted roads. A numerous sample lidar datasets are tested in order to assess the proposed algorithm. The quantified completeness and correctness of the classification results are 98. 20 % and 98. 54 %, respectively; hence it is concluded that the proposed algorithm works effectively in acquisition of road features from airborne lidar data...|$|R
40|$|One of the {{promising}} ways for improving precision of {{estimation of the}} targeted parameters of geodetic VLBI is to use phase delay measurements which are roughly 40 -fold more precise than group delay observables. However phase delay ambiguities should be resolved before using these observables in LSQ adjustments. This problem is solved almost always for the case of relatively short baselines (1 [...] 10 km) where {{we are able to}} ignore an atmosphere contribution to time delay, but it is still a challenge for the case of longer baselines. A procedure for resolving phase delay ambiguity is presented. It exploits the fact that phase <b>misclosures</b> are usually a small fraction of phase ambiguity spacings. Phase delay measurements made in the network consisting of three or more stations provide some redundancy. It is shown that the proposed approach allows to resolve phase delay ambiguities for a subset of observations. Some results of phase delay solutions of EUROPE, IRIS-S and VLBA networks are [...] ...|$|R
40|$|The 3 -OC {{is one of}} {{the newest}} digital three line {{scanners}} on the market. Unlike other three line scanners using a single optical system, the 3 -OC uses three different optical systems moving together. Therefore, this thesis aimed to develop a photogrammetric model for the 3 -OC. To precisely relate ground space and the corresponding image space, all the exterior orientation (E. O.) parameters of image lines need to be estimated using a bundle block adjustment. The biggest hurdle in this problem is the large number of exterior orientation parameters because one image strip of the 3 -OC usually contains tens of thousands of lines. To reduce the number of unknown E. O. parameters, the E. O. parameters of all the three cameras at an instant imaging time were represented by transformed parameters with respect to the gimbal rotation center. As a result, the unknown E. O. parameters were reduced to one third of original number of parameters. However, the number of E. O. parameters is still too big and estimating these E. O. parameters requires enough observations which are practically very difficult to obtain. To resolve this problem, there have been two kinds of approaches. One is reducing the number of unknown parameters and the other is providing fictitious observations using a stochastic model. As the title of this thesis implies, a stochastic trajectory model was implemented in this thesis. The stochastic relationships between two adjacent lines, as described in previous work, were expanded to the stochastic relationships between two adjacent image observations, so that the E. O. parameters of the lines between two adjacent observations can be recovered by interpolation. By providing enough pass points, it was possible to recover all the E. O. parameters accurately. In addition, the number of unknown E. O. parameters was drastically reduced as well. In this thesis, aerial triangulations of the suggested photogrammetric model were performed with self-calibrating some of the system parameters. As a result, the exterior orientation parameters were successfully estimated and the system parameters were calibrated as well. The RMSE of image <b>misclosures</b> on check points was less than 1. 2 pixel and the RMSE of ground <b>misclosures</b> at check points was less than 0. 6 ft (nominal GSD is 0. 5 ft). ...|$|R
30|$|Another {{method of}} {{combination}} {{is to use}} the VCE and re-weight the observations to obtain optimal value for the geopotential coefficients. The CAM is not sensitive to an a priori variance factor. The only important matter in the CAM is to have different accuracies. Each CAM has a <b>misclosure</b> vector which can be used to determine the variance components (VCs) according to a pre-described stochastic model for the observations error. In fact, in the VCE, an a posteriori variance factor is estimated for each set of observations. The observation weights are re-scaled in order to obtain a better match between the residuals and misclosures. In the following sections we summarize wellknown methods of the VCE, namely the best quadratic unbiased estimation (BQUE) and best quadratic unbiased non-negative estimation (BQUNE) (Sjöberg, 1984 b) and the modified best quadratic unbiased non-negative estimator (MBQUNE) (Eshagh and Sjöberg, 2008). The discussion is opened with the Gauss-Helmert model, followed by the special case of the CAM. Degree-order VCs (DOVC) are defined as a VC that is estimated according to the degree and order of the solution.|$|E
40|$|The Iterative Closest Point (ICP) {{algorithm}} is prevalent for the automatic fine registration of overlapping pairs of terrestrial laser scanning (TLS) data. This method {{along with its}} vast number of variants, obtains the least squares parameters {{that are necessary to}} align the TLS data by minimizing some distance metric between the scans. The ICP algorithm uses a "model-data" concept in which the scans obtain differential treatment in the registration process depending on whether they were assigned to be the "model" or "data". For each of the "data" points, corresponding points from the "model" are sought. Another concept of "symmetric correspondence" was proposed in the Point-to-Plane (P 2 P) algorithm, where both scans are treated equally in the registration process. The P 2 P method establishes correspondences on both scans and minimizes the point-to-plane distances between the scans by simultaneously considering the stochastic properties of both scans. This paper studies both the ICP and P 2 P algorithms in terms of their consistency in registration parameters for pairs of TLS data. The question being investigated in this paper is, should scan A be registered to scan B, will the parameters be the same if scan B were registered to scan A? Experiments were conducted with eight pairs of real TLS data which were registered by the two algorithms in the forward (scan A to scan B) and backward (scan B to scan A) modes and the results were compared. The P 2 P algorithm was found to be more consistent than the ICP algorithm. The differences in registration accuracy between the forward and backward modes were negligible when using the P 2 P algorithm (mean difference of 0. 03 mm). However, the ICP had a mean difference of 4. 26 mm. Each scan was also transformed by the forward and backward parameters of the two algorithms and the <b>misclosure</b> computed. The mean <b>misclosure</b> for the P 2 P algorithm was 0. 80 mm while that for the ICP algorithm was 5. 39 mm. The conclusion from this study is that the symmetric correspondence of the P 2 P algorithm provides more consistent registration results between a given pair of scans. The basis for this improvement is that symmetric correspondence better deals with the disparity between scans in terms of point density and point precision...|$|E
40|$|Abstract: The {{temporal}} {{stability and}} static calibration {{and analysis of}} the Velodyne HDL- 64 E S 2 scanning LiDAR system is discussed and analyzed. The mathematical model for measurements for the HDL- 64 E S 2 scanner is updated to include misalignments between the angular encoder and scanner axis of rotation, which are found to be a marginally significant source of error. It is reported that the horizontal and vertical laser offsets cannot reliably be obtained with the current calibration model due to their high correlation with the horizontal and vertical offsets. By analyzing observations from two separate HDL- 64 E S 2 scanners {{it was found that the}} temporal stability of the horizontal angle offset is near the quantization level of the encoder, but the vertical angular offset, distance offset and distance scale are slightly larger than expected. This is felt to be due to long term variations in the scanner range, whose root cause is as of yet unidentified. Nevertheless, a temporally averaged calibration dataset for each of the scanners resulted in a 25 % improvement in the 3 D planar <b>misclosure</b> residual RMSE over the standard factory calibration model...|$|E
40|$|In this contribution, we extend ‘Kalman-filter’ theory by {{introducing}} a new BLUE–BLUP recursion of the partitioned measurement and dynamic models. Instead {{of working with}} known state-vector means, we relax the model and assume these means to be unknown. The recursive BLUP is derived from first principles, in which a prominent role is played by the model’s <b>misclosures.</b> As {{a consequence of the}} mean state-vector relaxing assumption, the recursion does away with the usual need of having to specify the initial state-vector variance matrix. Next to the recursive BLUP, we introduce, for the same model, the recursive BLUE. This extension is another consequence of assuming the state-vector means unknown. In the standard Kalman filter set-up with known state-vector means, such difference between estimation and prediction does not occur. It is shown how the two intertwined recursions can be combined into one general BLUE–BLUP recursion, the outputs of which produce for every epoch, in parallel, the BLUP for the random state-vector and the BLUE for the mean of the state-vector...|$|R
40|$|This paper {{investigates the}} normal-orthometric {{correction}} {{used in the}} definition of the Australian Height Datum, and also computes and evaluates normal and Helmert orthometric corrections for the Australian National Levelling Network (ANLN). Testing these corrections in Australia is important to establish which height system is most appropriate for any new Australian vertical datum. An approximate approach to assigning gravity values to ANLN benchmarks (BMs) is used, where the EGM 2008 -modelled gravity field is used to "re-construct" observed gravity at the BMs. Network loop closures (for first- and second-order levelling) indicate reduced <b>misclosures</b> for all height corrections considered, particularly in the mountainous regions of south eastern Australia. Differences between Helmert orthometric and normal-orthometric heights reach 44 cm in the Australian Alps, and differences between Helmert orthometric and normal heights are about 26 cm in the same region. Normal orthometric heights differ from normal heights by up to 18 cm in mountainous regions > 2, 000 m. This indicates that the quasigeoid is not compatible with normal-orthometric heights in Australia...|$|R
40|$|This study {{provides}} new {{estimates for the}} orientation of a geometrically best fitting lunar triaxial ellipsoid {{with respect to the}} mean Earth/polar axis reference frame calculated from the footprint positions of the Chang'E- 1 (CE- 1), SELenological and ENgineering Explorer (SELENE) laser altimetry measurements and Unified Lunar Control Networks 2005, (ULCN 2005) station coordinates. The semi-principal axes of the triaxial ellipsoid and the coordinates of its geometric center are also calculated simultaneously. All the estimated parameters from all three data sets are found to be consistent. In particular, the RMS differences of the semi-principal axes of the triaxial ellipsoids and the locations of their geometric centers from solutions with and without modeling Euler angles (orientation of the triaxial ellipsoid) using uniformly distributed laser altimetry (LAL) footprints are 29 and 31 m respectively. The <b>misclosures</b> of all the solutions indicate a better fit for the triaxial ellipsoid to the footprint and station coordinates if the Euler angles are included in the models. Department of Land Surveying and Geo-InformaticsAuthor name used in this publication: X. L. Din...|$|R
40|$|Abstract. In {{real time}} {{kinematic}} (RTK) GPS positioning the reference station(s) is (are) static, and the moving receivers {{must not be}} far from the reference station(s). But in some cases, such as formation flying, satellite-to-satellite orbit determination, etc, {{it is difficult to}} establish a static reference station. GPS kinematic-to-kinematic positioning (KINRTK) will meet such requirements. The key work of ambiguity resolution for KINRTK is to obtain an ambiguity float solution rapidly. The float solution can be estimated using either the “Geometry-based”(GB) or “Geometry-free”(GF) approach, requiring the construction of a “GB ” or “GF ” ambiguity search space. These two spaces are different but have the same true integer ambiguity result. Searching in two spaces at the same time, referred to here as Dual-space Ambiguity Resolution Approach (DARA), will be faster than in the individual spaces because only a few ambiguity candidates meet the conditions of both spaces simultaneously. It can be shown that DARA can dramatically reduce ambiguity candidates even if the C/A-code pseudo-range observables are used. The results of a vehicle test confirm that our approach is promising, resulting in millimeter-level <b>misclosure</b> of the KINRTK run...|$|E
40|$|Identifying errors (blunders and {{systematic}} errors) in coastal geodetic levelling networks {{has often been}} problematic. This is because (1) mean sea level (MSL) at tide gauges cannot be directly compared to height differences from levelling because the geoid/quasigeoid and MSL are not parallel, being separated by the ocean’s mean dynamic topography (MDT) and (2) the lack of redundancy {{at the edge of}} the levelling network. This paper sets out a methodology to independently identify blunders and/or systematic errors (over long distances) in geodetic levelling using MDT models to account for the separation between the geoid/quasigeoid and MSL at tide gauges. This method is then tested in a case study using an oceanographic MDT model, MSL observations, GNSS data and a quasigeoid model. The results are significant because the errors found could not be detected by standard levelling <b>misclosure</b> checks alone, with supplementary data from an MDT model, with cross-validation from GNSS-quasigeoid allowing their detection. In addition, it appears that an oceanographic-only MDT is as effective as GNSS and a quasigeoid model for detecting levelling errors, which could be particularly useful for countries with coastal levelling errors in their levelling networks that cannot be identified by conventional levelling closure checks...|$|E
40|$|Currently, Global Positioning System (GPS) {{techniques}} {{are becoming a}} much larger part of the surveying industry. Many companies are now using GPS in their everyday work activities. The Real Time Kinematic (RTK) positioning {{is an integral part}} of topographic surveys, road surveying, con-structions and most civil engineering applications. Normally, RTK can be used to collect the posi-tioning data successfully and quickly. The civil and construction projects are designed in ground distances while RTK measurements are done in grid coordinate system, in which the distances between points are different from ground. The RTK measurements should be converted to ground for compatibility with the designed. In this paper, the accuracy of three alternatives for converting RTK measurements to ground was studied. These alternatives are, using scale factor, using two ground reference points and using Low Distortion Projection (LDP) surface. For the accuracy in-vestigation purpose, a traverse of 14 points elongated for a distance of about 1400 m was con-structed. Its coordinates were measured using total station, then the <b>misclosure</b> error was com-puted and the coordinates were adjusted. The traverse points coordinates were measured again using RTK_GPS considering one of them as base point. The three studied alternatives were applie...|$|E
40|$|The Empirical CODE Orbit Model (ECOM) of the Center for Orbit Determination in Europe (CODE), {{which was}} {{developed}} in the early 1990 s, is widely used in the International GNSS Service (IGS) community. For a rather long time, spurious spectral lines are known to exist in geophysical parameters, in particular in the Earth Rotation Parameters (ERPs) and in the estimated geocenter coordinates, which could recently be attributed to the ECOM. These effects grew creepingly with the increasing influence of the GLONASS system in recent years in the CODE analysis, which is based on a rigorous combination of GPS and GLONASS since May 2003. In a first step we show that the problems associated with the ECOM are to the largest extent caused by the GLONASS, which was reaching full deployment by the end of 2011. GPS-only, GLONASS-only, and combined GPS/GLONASS solutions using the observations in the years 2009 – 2011 of a global network of 92 combined GPS/GLONASS receivers were analyzed for this purpose. In a second step we review direct solar radiation pressure (SRP) models for GNSS satellites. We demonstrate that only even-order short-period harmonic perturbations acting along the direction Sun-satellite occur for GPS and GLONASS satellites, and only odd-order perturbations acting along the direction perpendicular to both, the vector Sun-satellite and the spacecraft’s solar panel axis. Based on this insight we assess in the third step the performance of four candidate orbit models for the future ECOM. The geocenter coordinates, the ERP differences w. r. t. the IERS 08 C 04 series of ERPs, the <b>misclosures</b> for the midnight epochs of the daily orbital arcs, and scale parameters of Helmert transformations for station coordinates serve as quality criteria. The old and updated ECOM are validated in addition with satellite laser ranging (SLR) observations and by comparing the orbits to those of the IGS and other analysis centers. Based on all tests, we present a new extended ECOM which substantially reduces the spurious signals in the geocenter coordinate z (by about a factor of 2 – 6), reduces the orbit <b>misclosures</b> at the day boundaries by about 10 %, slightly improves the consistency of the estimated ERPs with those of the IERS 08 C 04 Earth rotation series, and substantially reduces the systematics in the SLR validation of the GNSS orbits...|$|R
40|$|Transformations between {{different}} geodetic reference frames are often performed such that first the transformation parameters are determined from control points. If {{in the first}} place we do not know which of the numerous transformation models is appropriate then we can set up a multiple hypotheses test. The paper extends the common method of testing transformation parameters for significance, to the case that also constraints for such parameters are tested. This provides more flexibility when setting up such a test. One can formulate a general model with a maximum number of transformation parameters and specialize it by adding constraints to those parameters, which need to be tested. The proper test statistic in a multiple test is shown to be either the extreme normalized or the extreme studentized Lagrange multiplier. They are shown to perform superior to the more intuitive test statistics derived from <b>misclosures.</b> It is shown how model selection by multiple hypotheses testing relates to the use of information criteria like AICc and Mallows’ Cp, which are based on an information theoretic approach. Nevertheless, whenever comparable, the results of an exemplary computation almost coincide...|$|R
40|$|The Center for Orbit Determination in Europe (CODE) is {{contributing}} {{as a global}} analysis center to the International GNSS Service (IGS). Since 2012 CODE also contributes to the Multi-GNSS-EXperiment (MGEX) of the IGS. The list of satellite systems included in the CODE MGEX (COM) orbit and clock solution has been extended step-by-step in recent years. Today, it includes five satellite systems, namely GPS, GLONASS, Galileo, BeiDou, and QZSS. The COM orbit and clock products are regularly updated at the IGS MGEX products directory of the CDDIS data center and at the ftp server of the AIUB. CODE's experimental MGEX solution is subject to frequent updates and improvements. The introduction of an improved solar radiation pressure (SRP) model in early 2015 significantly improved the orbits and clock corrections of satellites with elongated bodies (in particular GLONASS, Galileo, and QZSS) {{as long as the}} satellite's attitude is maintained by yaw-steering. Currently we focus on improving the orbits of QZSS and BeiDou satellites, while moving in the orbit normal mode. The COM orbits are validated by computing orbit <b>misclosures</b> at the day boundaries and by SLR residuals. The COM clocks are validated using the Allan deviations and linear fits through the time series of epoch-wise clock corrections. We present the current status of the COM products and the validation results...|$|R
40|$|The {{temporal}} {{stability and}} static calibration {{and analysis of}} the Velodyne HDL‑ 64 E S 2 scanning LiDAR system is discussed and analyzed. The mathematical model for measurements for the HDL- 64 E S 2 scanner is updated to include misalignments between the angular encoder and scanner axis of rotation, which are found to be a marginally significant source of error. It is reported that the horizontal and vertical laser offsets cannot reliably be obtained with the current calibration model due to their high correlation with the horizontal and vertical offsets. By analyzing observations from two separate HDL- 64 E S 2 scanners {{it was found that the}} temporal stability of the horizontal angle offset is near the quantization level of the encoder, but the vertical angular offset, distance offset and distance scale are slightly larger than expected. This is felt to be due to long term variations in the scanner range, whose root cause is as of yet unidentified. Nevertheless, a temporally averaged calibration dataset for each of the scanners resulted in a 25 % improvement in the 3 D planar <b>misclosure</b> residual RMSE over the standard factory calibration model...|$|E
40|$|Progress {{has been}} made {{enabling}} expensive, high-end inertial measurement units (IMUs) {{to be used as}} tracking sensors. However, the cost of these IMUs is prohibitive to their widespread use, and hence the potential of low-cost IMUs is investigated in this study. A wearable low-cost sensing system consisting of IMUs and ultrasound sensors was developed. Core to this system is an extended Kalman filter (EKF), which provides both zero-velocity updates (ZUPTs) and Heuristic Drift Reduction (HDR). The IMU data was combined with ultrasound range measurements to improve accuracy. When a map of the environment was available, a particle filter was used to impose constraints on the possible user motions. The system was therefore composed of three subsystems: IMUs, ultrasound sensors, and a particle filter. A Vicon motion capture system was used to provide ground truth information, enabling validation of the sensing system. Using only the IMU, the system showed loop <b>misclosure</b> errors of 1 % with a maximum error of 4 – 5 % during walking. The addition of the ultrasound sensors resulted in a 15 % reduction in the total accumulated error. Lastly, the particle filter was capable of providing noticeable corrections, which could keep the tracking error below 2 % after the first few steps...|$|E
40|$|The {{measurement}} of Second Precise Levelling Network (PLN) for the Peninsular Malaysia which {{was completed in}} 2000 by Department of Surveying and Mapping Malaysia (DSMM) is set to replace the First Order Levelling Network of 1967. The new network consists of 113 levelling lines with more than 5000 bench marks and covers a total distance of over 5000 km. Precise levelling technique is used to establish the network where the allowable <b>misclosure</b> between fore and back levelling is less than 3 mm per root kilometre of length along a line. Its configuration is predominantly dictated by the land transportation pattern. The mean sea level (MSL) at Port Kelang, based upon a 10 -year tidal observation (1984 - 93), was later being adopted as the new Peninsular Malaysia Geodetic Vertical Datum (PMGVD). A consistent and accurate set of adjusted heights of benchmarks has been achieved in the adjustment of the Precise Levelling Network of Peninsular Malaysia on the datum defined by MSL height at Port Klang. These adjusted heights {{are based on the}} Helmert orthometric height system. By fixing Port Kelang, the precision of the PLN can be expressed as 1. 14 mmvkm. This implies that for any of the 5, 295 first-order levelling bench mark across the nation, a height precision of better than 3 cm can be expected...|$|E
40|$|Homogeneously {{reprocessed}} combined GPS/GLONASS 1 - and 3 -day solutions from 1994 to 2013, {{generated by}} the Center for Orbit Determination in Europe (CODE) {{in the frame of}} the second reprocessing campaign REPRO- 2 of the International GNSS Service, as well as GPS and GLONASS-only 1 - and 3 -day solutions for the years 2009 to 2011 are analyzed {{to assess the impact of}} the arc length on the estimated Earth Orientation Parameters (EOP, namely polar motion and length of day), on the geocenter, and on the orbits. The conventional CODE 3 -day solutions assume continuity of orbits, polar motion components, and of other parameters at the day boundaries. An experimental 3 -day solution, which assumes continuity of the orbits, but independence from day to day for all other parameters, as well as a non-overlapping 3 -day solution, is included into our analysis. The time series of EOPs, geocenter coordinates, and orbit <b>misclosures,</b> are analyzed. The long-arc solutions were found to be superior to the 1 -day solutions: the RMS values of EOP and geocenter series are typically reduced between 10 and 40 %, except for the polar motion rates, where RMS reductions by factors of 2 – 3 with respect to the 1 -day solutions are achieved for the overlapping and the nonoverlapping 3 -day solutions. In the low-frequency part of the spectrum, the reduction is even more important. The better performance of the orbits of 3 -day solutions with respect to 1 -day solutions is also confirmed by the validation with satellite laser ranging...|$|R
40|$|Semantic 3 D city {{models are}} {{increasingly}} {{used as a}} data source in planning and analyzing processes of cities. They represent a virtual copy of the reality and are a common information base and source of information for examining urban questions. A significant advantage of virtual city models is that important indicators such as the volume of buildings, topological relationships between objects and other geometric as well as thematic information can be derived. Knowledge about the exact building volume is an essential base for estimating the building energy demand. In {{order to determine the}} volume of buildings with conventional algorithms and tools, the buildings may not contain any topological and geometrical errors. The reality, however, shows that city models very often contain errors such as missing surfaces, duplicated faces and <b>misclosures.</b> To overcome these errors (Steuer et al., 2015) have presented a robust method for approximating the volume of building models. For this purpose, a bounding box of the building is divided into a regular grid of voxels and it is determined which voxels are inside the building. The regular arrangement of the voxels leads to a high number of topological tests and prevents the application of this method using very high resolutions. In this paper we present an extension of the algorithm using an octree approach limiting the subdivision of space to regions around surfaces of the building models and to regions where, in the case of defective models, the topological tests are inconclusive. We show that the computation time can be significantly reduced, while preserving the robustness against geometrical and topological errors...|$|R
40|$|This project aims at the {{consolidation}} of the data from integrated fieldwork in Swabian Alb test area since 1996 to 2013 {{as well as the}} height systems computations. Reliable data were checked by height differences and gravity values, after that they were grouped into 5 closed loops with 56 out of 121 observed points. Potential differences were computed from height differences which acquired from spirit levelling and gravity value, then least square adjustment was adopted. Observation equation (A-matrix) and condition equation (B-matrix) were applied in the adjustment, a weight matrix was also assigned in the adjustment. Geopotential differences were computed based on the Helmert orthometric height at point 580, then geopotenial numbers of the other points were computed by adding the geopotential number with the adjusted potential differences, then height systems could be determined as well as height corrections. In the closed loop adjustment especially adjustment of several loops with many data, condition equation adjustment is preferred because of the smaller size of design matrix compared to the observation equations and the advantage of condition equations over observation equations is that loop <b>misclosures</b> can be determined by condition equation. The results from both equations are the same. The difference of the geopotential numbers between unweighted and weighted adjustment is up to 0. 0129 m 2 /s 2 and the difference of height systems between unweighted and weighted adjustment is up to 0. 0013 meter or 1. 3 millimeter, so the height system computations were not significantly affected by the assigned weight. The difference of height corrections between unweighted and weighted adjustment is up to 10 - 8 m so the assigned weights did not affect height correction results. Normal corrections give the smallest values while dynamic corrections give the largest values because the test area is located at latitude 48. 485 ° instead of latitude 45 ° so the correction values are quite large...|$|R
