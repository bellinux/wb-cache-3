3|156|Public
5000|$|Angell {{was born}} in Providence, Rhode Island, to Henry and Mae (née Cooney) Angell. He {{received}} a bachelor's degree in English Literature from Providence College. [...] He married Lynn Angell on August 14, 1971. Soon after Angell entered the U.S. Army upon graduation and served at the Pentagon until 1972. He then moved to Boston {{and worked as a}} <b>methods</b> <b>analyst</b> at an engineering company and later at an insurance firm in Rhode Island. His brother, the late Most Rev. Kenneth Angell, was a Roman Catholic prelate and former Bishop of Burlington, Vermont.|$|E
5000|$|Keith Oliver {{was educated}} in the United Kingdom at Monmouth School and holds an honours degree from Birmingham University. He is {{currently}} {{on the staff of}} the management consulting firms, Booz & Company / Booz Allen Hamilton and worked previously as Senior Organizations and <b>Methods</b> <b>Analyst</b> to the West Midlands Gas Board, and then as a consultant for Business Operations Research (Systems) Limited.According to Damon Schechter, Oliver played a critical role in ushering in the third significant evolution of logistical thought in the 1970s and 1980s [...] and has contributed as author and co-author of numerous articles [...] and the chapter entitled: [...] "Distribution: the total cost-to serve" [...] in The Gower Handbook of Management (1983 - 1998).|$|E
50|$|Hudson {{worked in}} the oil fields and {{mechanic}} shops during the summer while in college, and since graduating {{has been involved in}} a wide variety of jobs in farming, timber harvesting, home building, heavy construction, law enforcement, real estate development, soldiering, and even crop dusting with an airplane. Upon completing basic training in the Mississippi Army National Guard, he worked for the Boeing Company on the Saturn Project, starting as a <b>methods</b> <b>analyst</b> and rising to be the administrative assistant to the manager of Industrial Engineering/Production Control. When the Saturn program ended, he enrolled in OCS at Fort Benning, Georgia, graduating as a 2nd Lt. Today, besides being a county supervisor, he is involved in the restoration of our wetlands.|$|E
40|$|Characterizing a firm’s true (but unobservable) {{expected}} {{returns as}} the normative benchmark, we develop a two-dimensional framework {{for evaluating the}} relative performance of implied cost-of-capital (ICC) estimates. First, in time-series, variations in ICC estimates should reflect changes in true expected returns rather than changes in measurement errors. Second, cross-sectionally, ICC estimates should predict future realized returns. Using this framework, we compare seven alternative ICC measures and show that several perform quite well along both dimensions, and all do much better than Beta-based estimates. In addition, we provide evidence {{on the importance of}} appropriate matching between the earnings forecasting <b>method</b> (<b>analyst</b> vs. mechanical) and the valuation model. Overall, our evidence provides significant support for the broader adoption of ICCs as firm-level expected return proxies...|$|R
5000|$|To {{develop new}} {{environmental}} policies, {{it is important}} first to evaluate those {{that have already been}} adopted. However, this intuitively simple idea is difficult to apply in practice, no more so than in the EU where the complex system of multi-level governance adds considerably to the practical difficulty of evaluating policies. [...] Assessing impacts and finding side-effects of policies is best achieved by a plurality of data, <b>methods,</b> <b>analysts</b> and theories, as well as evaluation criteria. In recent years the demand for evaluations of EU policies and programmes has increased as the importance of evaluation has become more widely recognised. Many actors have become involved in commissioning, producing and using evaluations (including the European Environment Agency), but the role of evaluation is often still quite weak.|$|R
40|$|Mellor and Shlaer [17] assert: “The {{ability to}} execute the {{application}} analysis models is a sine qua non for any industrial-strength <b>method,</b> because <b>analysts</b> {{need to be able}} to verify the behaviour of the model with both clients and domain experts. ” On the other hand, a recent assessment of CASE tools [19] has noted: “The biggest mistake was to overestimate the eas...|$|R
50|$|Glaser {{originated}} {{the basic}} process of Grounded theory method {{described as the}} constant comparative <b>method</b> where the <b>analyst</b> begins analysis with the first data collected and constantly compares indicators, concepts and categories as the theory emerges.|$|R
30|$|In the {{original}} version of the Ueda’s <b>method,</b> the <b>analyst</b> sets the number of expected outliers in the data set in advance, i.e. the value of s following the notation considered herein. Carling (2000) showed that, for several probability distributions and various sample sizes, approximately 5 % to 50 % {{of the total number}} of observations N can be, in general, set as the minimum and maximum reasonably expected number of potential outliers, respectively.|$|R
40|$|In {{studies of}} {{educational}} comparative effectiveness, analysts are often {{interested in the}} relative performance of school types. The analyst hopes to draw causal inferences {{about the effects of}} schools on various outcomes. One classic example in the study of school effects is the comparison across Catholic and public schools in terms of scholastic achievement. Matching estimators are one <b>method</b> <b>analysts</b> use to adjust for observed confounders across treated and control schools. These matches focus on balancing differences in student level distributions. We propose that matching in studies of school effects should mimic the group randomized trial. This implies that imbalances in school level covariates must also be eliminated. Once schools are matched, students can be matched within school pairs. We present a case study on the effect of Catholic schools on mathematics test scores. We demonstrate that large imbalances exist in school level covariates which confound the effect of interest. We find that once schools are balanced, there is little evidence of a Catholic school effect. Specifically, we find that the Catholic school effect decreases in magnitude as imbalances in school level covariates decrease...|$|R
40|$|This paper {{provides}} an annotated bibliography of over 100 articles concerning methods for analyzing correlated categorical response data. Most {{of the papers}} listed here concern categorical regression models and estimation, with particular emphasis on binary responses. The papers are classified by several characteristics which group them according to common themes. The bibliography serves as a reference of <b>methods</b> for <b>analysts</b> of correlated categorical data, {{as well as for}} persons interested in methodologic work in this active area of statistical research...|$|R
40|$|The assay of {{pharmaceutical}} preparations containing {{more than one}} active ingredient often represents difficulties that face <b>analysts.</b> <b>Methods</b> that allow the easy determination of each ingredient in presence of the others are needed. Such methods should be selective, sensitive, easy to perform and not costly...|$|R
40|$|Germany, {{initiated}} {{development of}} a spectra library to help clinical and forensic toxicologists identify poisons as quickly as possible. The library contains more than 1, 600 UV spectra of compounds relevant to pharmacology and toxicology. The spectra were measured at the Forschungsgesellschaft für Lungen- und Thoraxerkrankungen (FILT) in Berlin and tested with extracts of human serum, spiked full blood or urine samples and samples taken during the investigation of many real cases of poisoning. The library is divided into sublibraries that enable faster peak identification. Problem-oriented preselection of the match-ing sublibrary allows identification of an analyte with {{a higher degree of}} accuracy. Using this library and a diode-array detector software, ana-lysts can compare the spectrum of an unknown peak with those in the library in a very short time. With relatively simple sample preparation <b>methods,</b> <b>analysts</b> can use this library to identify poisons and their metabolites fast and confidently. The library also offers a link to a data-base that gives each compound's nonproprietary name, CAS number, effect or use, retention time and spectroscopic maxima, minima and shoulders. An easy-to-use manual provides the same data, a printout of each spectrum and the structural formula of each compound. All of this makes the library a valuable tool in forensic, clinical and toxicological analysis...|$|R
40|$|In capital {{budgeting}} processes, {{there are}} some crucial problem related to infation and the criteria in making the decision. Among the two discounting <b>methods,</b> <b>analysts</b> may use NPV or IRR. If NPV is used in making capital investment decision, some still doubt that project having zero NPV is still a good project. In this paper we try to explore how important infation factor {{to be considered in}} capital badgeting. The fnancial professional literatures dealing with investment decisions, state in general that the net present value shows objective picture for the decision maker. The net present value shows the amount of wealth growth that have been accumulated by the investment during the life time of the project, but the investmentâ€™s internal rate of return informs the decision maker that how works the real yield of long capital investment. The NPV method is theoritically superior to the IRR method in dealing with mutually exclusive project. The Project having zero NPV is considered to be proftable. In general terms, a sound capital investment will earn its original investment and cover the cost of capital invested. In the countres with high rate of infation, the efect on capital investment can be dramatic, so that cash fow adjustment is very critical...|$|R
40|$|The {{article of}} record as {{published}} {{may be found}} at [URL] date, most social network analyses (SNAs) of terrorist groups have used network data that provide snap-shots of the groups at a single point in time. Seldom have they used network data that take into account how the groups have changed over time. In this article, a unique longitudinal network data set, the Noordin Top terrorist network from 2001 to 2010, is examined in order to explore whether a recently developed method – social network change detection (SNCD) – can help analysts monitor a dark network’s topography (e. g. centralization, density, degree of fragmentation) in order to detect significant changes in its structure and identify possible causes. The application of change detection to this historical data set illustrates the method’s potential usefulness, including its ability to detect {{significant changes in the}} network in response to a series of exogenous factors, such as the acquisition of bombing materials, the capture of key leaders and groups, and the death of Noordin himself. The method’s inability to detect other significant events, however, highlights important limitations when working with it. While SNCD should not be the only <b>method</b> <b>analysts</b> have at their disposal, the results detailed in this article suggest that it should be included in their toolkit...|$|R
5000|$|Analytic Strengths of State-Centered Approaches to Revolution [...] Goodwin {{argues that}} his statist {{approach}} is imperative when solving key {{problems of the}} study of revolutions. This state-centered analysis has much strength that makes it the best tools for analyzing revolutions. Revolutions are phenomenons that are fairly modern and have occurred with much more frequency in the twentieth century. Many have asked themselves why revolutions became more frequent in the twentieth century and the state-centered perspective answers this question. The answer is that revolutions need states to occur and this international state system that is present now thus makes revolutions much more frequent. This is not the only question that this statist perspective is able to answer. The state-centered perspective is able to answer why radical movements are concerned with destroying and seizing state power. Following this is also the question as to why it is necessary for the state to break down. This statist perspective also solves the question of why, when, and where revolutions occur. Lastly through this <b>method</b> <b>analysts</b> can figure out why some groups are able to attract much support and others are not. Through the state-centered perspectives all of these questions are answered in the analysis of revolutions.|$|R
40|$|Yield curve {{forecasting}} is {{an important}} problem in finance. In this work we explore the use of Gaussian Processes {{in conjunction with a}} dynamic modeling strategy, much like the Kalman Filter, to model the yield curve. Gaussian Processes have been successfully applied to model functional data in a variety of applications. A Gaussian Process is used to model the yield curve. The hyper-parameters of the Gaussian Process model are updated as the algorithm receives yield curve data. Yield curve data is typically available as a time series with a frequency of one day. We compare existing methods to forecast the yield curve with the proposed method. The results of this study showed that while a competing method (a multivariate time series method) performed well in forecasting the yields at the short term structure region of the yield curve, Gaussian Processes perform well in the medium and long term structure regions of the yield curve. Accuracy in the long term structure region of the yield curve has important practical implications. The Gaussian Process framework yields uncertainty and probability estimates directly in contrast to other competing <b>methods.</b> <b>Analysts</b> are frequently interested in this information. In this study the proposed method has been applied to yield curve forecasting, however it can be applied to model high frequency time series data or data streams in other domains...|$|R
30|$|In this experiment, we varied {{the number}} of {{features}} selected. We generated datasets with the instances described by all attributes (features) and datasets with attributes selected by Relief and manual selection, which is an appropriate selection <b>method</b> if the <b>analyst</b> has knowledge about the problem domain. The most important criterion in selecting the features manually was their mutual correlation.|$|R
30|$|Finally, we {{analyzed}} {{the sections of}} the interviews coded as ‘effects of professional development’ to identify {{whether or not the}} USE project or other similar initiatives had any demonstrable impact on organizational memory or retrieval functions at this institution. Using the inductive analytic techniques described earlier (e.g., open-coding and constant comparative <b>method),</b> the <b>analysts</b> first identified instances where respondents clearly attributed a change in their course planning activities to the USE project. Thus, the analysts did not infer effects but instead relied on respondent accounts. These statements of attribution were mapped onto the bins of the retention structure used in previous analyses (e.g., individual memory and curricular artifacts).|$|R
40|$|Issues {{regarding}} black box, large systems verification are explored. It {{begins by}} collecting data from several testing teams. An integrated database containing test, fault, repair, and source file information is generated. Intuitive effectiveness measures are generated using conventional {{black box testing}} results analysis <b>methods.</b> Conventional <b>analysts</b> <b>methods</b> indicate that the testing was effective {{in the sense that}} as more tests were run, more faults were found. Average behavior and individual data points are analyzed. The data is categorized and average behavior shows a very wide variation in number of tests run and in pass rates (pass rates ranged from 71 percent to 98 percent). The 'white box' data contained in the integrated database is studied in detail. Conservative measures of effectiveness are discussed. Testing efficiency (ratio of repairs to number of tests) is measured at 3 percent, fault record effectiveness (ratio of repairs to fault records) is measured at 55 percent, and test script redundancy (ratio of number of failed tests to minimum number of tests needed to find the faults) ranges from 4. 2 to 15. 8. Error prone source files and subsystems are identified. A correlational mapping of test functional area to product subsystem is completed. A new adaptive testing process based on real-time generation of the integrated database is proposed...|$|R
40|$|This {{article to}} desrcibe {{multiculturalism}} {{in the novel}} Kusut author by Ismet Fanany. To get the purpose use the study of kualitative with descriptive <b>analyst</b> <b>method.</b> A data {{of this study is}} the data multiculturalism solidarityandbrotherhood, open trading, family values, respect for ethics, was enough in life, and sharing and control of power gainedfromthe studyobjectordata source, that is novel Kusut author by Ismet Fanany...|$|R
40|$|Data Shop, a {{department}} of Cityscape, presents short articles or notes on the uses of data in housing and urban research. Through this department, PD&R introduces readers to new and overlooked data sources and to improved techniques in using wellknown data. The emphasis is on sources and <b>methods</b> that <b>analysts</b> can use in their own work. Researchers often run into knotty data problems involving data interpretation or manipulation that must be solved before a project can proceed, but they seldom get to focus in detail on the solutions to such problems. If {{you have an idea}} for an applied, data-centric note of no more than 3, 000 words, please send a one-paragraph abstract t...|$|R
40|$|In vivo {{repeatability}} and reproducibility of the quantita-tive light-induced fluorescence (QLF) method {{were tested}} {{with respect to}} three variables: lesion area, and average and maximum changes in lesion fluorescence. To test the image-capturing stages, three analysts each captured images of 15 incipient smooth surface lesions in vivo, and the images were analysed {{by one of the}} ana-lysts. To test the analytical stage of the <b>method,</b> three <b>analysts</b> analysed the images of 15 in vivo incipient smooth surface lesions. For the image-capturing stage, inter-examiner reliability showed an intra-class correla-tion coefficient (r) between 0. 95 and 0. 98. For the analyti-cal stage, intra-examiner reliability for all three analysts showed a value of r between 0. 93 and 0. 99. Inter-examin...|$|R
30|$|Automated {{analysis}} {{refers to}} the use of automated analysis <b>methods,</b> for supporting <b>analysts</b> in the analysis process. Automated analysis methods are algorithms that allow extracting and calculating information from the model, for (1) obtaining facts that are results based on the information placed in the model or (2) enriching the model augmenting it with elements, relations, elements’ attributes, or relations’ attributes in order to provide new useful information.|$|R
40|$|From the DuPont Identity, {{this paper}} derives a formula {{relating}} the percent {{changes in the}} return on equity (ROE) to the percent changes in the DuPont components. This formula is useful in determining the primary reasons why the ROE changed from one period to the next. While the periodic percent change relationship is nonlinear, a simple and intuitive additive formula is an approximation, albeit at times a poor approximation. We also convert the periodic percent changes in the ROE and its DuPont components into their equivalent instantaneous rates of change. These instantaneous rates {{of changes in the}} DuPont components do precisely sum to the instantaneous rate of change in the ROE, providing a <b>method</b> for <b>analysts</b> to both intuitively and accurately present their analysis...|$|R
40|$|In {{a recent}} paper, Lanjouw and Ravallion {{proposed}} an attractive and simple method to conduct marginal benefit incidence analysis {{with a single}} cross-section of data. Their <b>method</b> enables the <b>analyst</b> to test whether the poor benefit {{more or less than}} the non-poor from program expansion. In this note, we propose an alternative to the method proposed by Lanjouw and Ravallion. An application to access to basic infrastructure services in Latin America illustrates the approach...|$|R
50|$|The {{different}} Advanced Level exams {{are more}} practical and require deeper knowledge in special areas. Test Manager deals with planning {{and control of}} the test process. Test Analyst concerns, amongst other things, reviews and black box testing <b>methods.</b> Technical Test <b>Analyst</b> includes component tests (also called unit test), requiring knowledge of white box testing and non-functional testing methods - this section also includes test tools. The Expert Level is still in preparation.|$|R
40|$|This report {{documents}} {{establishment of}} bias, bias trends and uncertainty for {{validation of the}} CSAS 25 control module from the SCALE 4. 4 a computer code system for use in evaluating criticality safety of uranium systems. The 27 -group ENDF/B-IV, 44 -group ENDF/B-V, and 238 -group ENDF/B-V cross-section libraries were used. The criticality validation calculations were performed using over 500 benchmark cases from Volumes II and IV of the &#x 27;&#x 27;International Handbook of Evaluated Criticality Safety Benchmark Experiments,&#x 27;&#x 27; published by the Nuclear Energy Agency Organization for Economic Cooperation and Development (NEA/OECD). Based on statistical analysis of the calculation results, the bias, bias trends and uncertainty of the benchmark calculations have been established for these benchmark experiments. Numerical methods for applying margins are briefly described, but the determination of appropriate correlating parameter and values for additional margin, applicable to a particular analysis, must be determined as part of process analysis. As such, this document does not specify upper subcritical limits as {{has been done in}} the past. A follow-on report will be written to assess the methods for determination of an upper safety limit in more detail, provide comparisons, and recommend a preferred <b>method.</b> <b>Analysts</b> using these results are responsible for exercising sound engineering judgment using strong technical arguments to develop a margin in k{sub eff} or other correlating parameter that is sufficiently large to ensure that conditions (calculated by this method to be subcritical by this margin) will actually be subcritical. Documentation of determination and justification of the appropriate margin in the analyst&#x 27;s evaluation, in conjunction with this report, will constitute the complete Validation Report in accordance with ANSI/ANS- 8. 1 - 1998, Section 4. 3. 6 (4) ...|$|R
40|$|This {{thesis is}} {{concerned}} with exploring the equity market price discovery process, the translation and incorporation of new information into stock prices, by studying both what information is included in this process and which valuation methods are used to translate that information into a value. The overarching research question posed in this thesis is: How is equity valued? The overarching question is broad and has been divided into the following sub questions: What valuation methods do companies use when valuing takeover targets? What valuation <b>methods</b> do sell-side <b>analysts</b> use when valuing equity? What factors explain the variation {{in the use of}} valuation <b>methods</b> by sell-side <b>analysts?</b> To what extent do sell-side analysts utilize non-financial information in their reports? These questions are addressed in four separate essays. Findings of the thesis emphasized that valuation behavior is contextual to several specific circumstances. Findings showed that companies valuing takeover targets used sophisticated valuation methods to a higher extent than did sell-side analysts. Findings also showed systematic differences in the choice of valuation <b>methods</b> among sell-side <b>analysts.</b> With regards to the use of non-financial information and information on Intellectual Capital this thesis showed that the context of the target firm dictates which information is relevant for predicting future performance, and hence is used by analysts. Additionally, the accessibility of information is an important factor affecting what information is used in the valuation process. Understanding the valuation behavior of the different actors on the capital market is to understand the pricing process of the market, and as such the contribution of this thesis has been to shed more light on the cornerstone of market efficiency- the ability of market actors to identify and buy (sell) under priced (over priced) stocks...|$|R
40|$|The {{clinical}} work of psychoanalysts can {{be thought}} of in both a narrow and a broad sense. In the narrow sense, it refers to what is commonly thought of as psychoanalytic technique, the <b>methods</b> <b>analysts</b> use to understand their patients and to convey that understanding to them. In the broad sense, it refers to the entirety of their work as clinicians: the content of understanding, as well as such things as diagnosing, estimating analyzability, recommending therapy, and prescribing medication. The current enthusiastic expectation that neuroscience will have an immediate and direct impact on clinical work in the narrow sense is misguided, but neuroscientific discoveries, it is argued, will {{have a major impact on}} psychoanalytic theory in the not too distant future. The resulting changes in metapsychology will ultimately have reverberations on clinical work in the broad sense, although psychoanalytic technique, the analyst’s basic approach to patients, will remain essentially the same. L et me begin with a word about my title. Judging from the laugh-ter and rolled eyes I encounter when I read the title to my friends, I suspect that it might be a little controversial. However, I like contro-versy, and, having had long experience with the soporif ic effect of plenary addresses, I wanted to be sure that at least in the beginning I had everybody’s attention. Those of you who are hoping to hear a diatribe against neuroscience are, I’m afraid, going to be disappointed. This is a pro-neuroscience paper. My overall aim is threefold. First, I want to temper some of the overexpectation that tends to be stirred up by the neuroscientif ic papers appearing these days in the analytic literature. These papers often imply that neuroscience will in some way change our daily psychoanalytic technique, an implication I conside...|$|R
40|$|Security {{prices in}} {{efficient}} markets reflect all relevant information. Past price formations and even fundamental analysis cannot guarantee abnormal returns consistently to any pre-identified strategy or market participant, be they novice or expert traders. There have been various studies done in past to test market efficiency in emerging markets. However, in this study, {{we take the}} approach of surveying the professional investment community and study their stated actions in making investments. Our results indicate prevalence of herding and overconfidence in professional analysts. We also find that analysts extrapolate past into the future forecasts. We also find association between demographic characteristics and choice of security valuation <b>methods</b> that <b>analysts</b> use. In line with Chevalier & Ellison (1998), we find that young people herd less in our sample than the old people. ...|$|R
40|$|Analyst {{motivation}} {{is to develop}} team work with forwarderand his principal in order to realise effective communicationbetween forwarder and principal, according to the set problemsand limitations directed by the surroundings in the system of internationalforwarding agency, foreign trade, traffic and informationsyscems. The analyst cares for personal and collective motivationdevelopment, creating convenient conditions of team work offorwarder and his principal in order to model the process anddata in the most creative way using the Stntctured SystemsAnalysis method. By applying this <b>method,</b> the <b>analyst</b> createscomplete documentation of the process model: Functional DecompositionDiagram, Context Diagram, Data Flow Diagramof other levels of abstraction, logic process definition, and descriptionof data flow structure and description of data bufferstntcture. The team tries to achieve the optimal diversity with definitegoodwill of analyst, forwarder and principal, and diverse characteristicsof personality, professional knowledge and balanceof all members...|$|R
40|$|Virtually every {{virus and}} worm {{that circulates the}} Internet today is ""protected"" by some form of {{obfuscation}} that hides the code's true intent. In the Window's world where worms prevail, the use of tools such as UPX, ASPack, and teLock has become standard. Protection of malicious code {{is not the only}} goal of binary obfuscators however which can be used to protect intellectual property. In the Linux world, tools such as Burneye and Shiva exist which can be used in ways similar to any Window's obfuscation tool. To fight such <b>methods,</b> <b>analysts</b> have created specific tools or techniques for unraveling these code obfuscators in order to reveal the software within. To date, in the fight against malware, anti-virus vendors have had the luxury of focusing on signature development since obfuscation of malware has presented little challenge. To combat this, malware authors are rapidly morphing their code in order to evade quickly developed and deployed signature-matching routines. What will happen when malware authors begin to morph their obfuscation techniques as rapidly as they morph their worms? While not designed specifically as a malware protection tool, one program, Shiva, aims to do exactly that. Shiva forces analysis of malicious code to be delayed while analysts fight through each novel mutation of Shiva's obfuscation mechanism. This, in effect, provides the malware {{a longer period of time}} to wreak havoc before countermeasures can be developed. This talk will focus on the use of emulated execution within IDA Pro to provide a generic means for rapidly deobfuscating protected code. Capabilities of the emulation engine will be discussed and the removal of several types of obfuscation will be demonstrated. Finally, the development of standalone deobfuscation tools based on the emulation engine will be discussed. Chris Eagle is the Associate Chairman of the Computer Science Department at the Naval Postgraduate School (NPS) in Monterey, CA. A computer engineer/scientist for 18 years, his research interests include computer network operations, computer forensics and reverse/anti-reverse engineering...|$|R
40|$|Purpose of {{the study}} The aim of my thesis is study how M&A {{announcements}} affect the valuations of the merging firms and the peers of the acquirer. In addition, I analyze if the method to determine the peer group causes the possible peer effect to differ. To study the peer effects, prior literature has relied on industry classifications whereas I, {{in addition to the}} traditional method, employ the new common <b>analyst</b> based <b>method</b> (Kaustia & Rantala, 2013). Furthermore, I also examine how the single, closest peer of the acquirer, also determined by the common <b>analyst</b> <b>method</b> is affected by the merger announcement. Data and methodology The final sample consists of 563 M&A announcements of public, US companies between January 1999 to December 2013 by US based firms listed on the New York Stock Exchange. The data on the mergers was obtained from SDC Platinum. The stock price data was obtained from CRSP. The common <b>analyst</b> <b>method</b> constructs the peer groups based on joint analyst coverage and the analyst data is available in IBES. The industry classification based peer groups I formed by matching the acquirer’s 4 -digit SIC code to all other firms sharing the same industry classification. Finally, I used the event study methodology to determine the abnormal returns on and around the announcement day. Findings {{of the study}} The abnormal returns to the acquiring firm were slightly negative, while the target firm encountered highly positive abnormal returns; on average the mergers were value-creating with a positive combined wealth effect. The peers, on average, irrespective of the methodology used, did not encounter significant abnormal returns. When further splitting the sample based on the CWE of the merger, the peers faced a similar signed effect as the CWE; this result provides support for the contagion effect for peers in mergers. The results are robust to the peer group method used and both methods returned extremely similar results. Furthermore, in support of the common <b>analyst</b> <b>method,</b> the closest peer determined by it showed the most prominent effect in the merger announcements...|$|R
40|$|A {{simplified}} <b>analysts</b> <b>method</b> for {{the examination}} of complex corrosion data is presented in terms of data from slurry corrosion toroid experiments. This method facilitates the assignment of average effects to specific imposed experimental variables in a test series. It is based on statistical analysis of variance procedures but emphasizes the use of averages and presents results relative to a chosen reference experimental condition. The use of the logarithm of attack rate results in {{the expression of the}} effects of variables as ratios or multiplicative terms. (auth...|$|R
30|$|User {{interface}} prototyping is {{an effective}} method for users to validate the requirements defined by analysts {{at an early stage}} of a software development. However, a user interface prototype system offers weak support for the analysts to verify the consistency of the specifications about internal aspects of a system such as business logic. As the result, the inconsistency causes a lot of rework costs because the inconsistency often makes the developers impossible to actualize the system based on the specifications. For verifying such consistency, functional prototyping {{is an effective}} <b>method</b> for the <b>analysts,</b> but it needs a lot of costs and more detailed specifications. In this paper, we propose a review <b>method</b> so that <b>analysts</b> can verify the consistency among several different kinds of diagrams in UML efficiently by employing system-side prototyping without the detailed model. The system-side prototype system does not have any functions to achieve business logic, but visualizes the results of the integration among the diagrams in UML as Web pages. The usefulness of our proposal was evaluated by applying our proposal into a development of Library Management System (LMS) for a laboratory. This development was conducted by a group. As the result, our proposal was useful for discovering the serious inconsistency caused by the misunderstanding among the members of the group.|$|R
40|$|Abstract. This paper {{compares the}} {{performance}} of alternative models of east Asian exchange rates at different data frequencies. Selected models employ different specifications of the conditional variance and the condi-tional error distribution. Conditional variance specifications include: homoscedasticity, GARCH, LGARCH, and EGARCH. Conditional error distribution specifications include normal and Student t. The best exchange rate model specification is clearly conditional on data frequency. Higher frequency (daily, weekly) data commonly ex-hibit characteristics that demand more sophisticated estimation <b>methods</b> than <b>analysts</b> commonly employ. These characteristics generally vanish at lower (monthly, quarterly) frequencies. Overall we find significant benefit from accommodating heteroscedasticity and leptokurtic properties of the conditional distribution as data fre-quency increases. Using a likelihood ratio test we compare the relative gain from addressing heteroscedasticity (through use of GARCH models) versus accommodation of leptokurtosis. This comparison suggests that the gains from correct specification of the conditional distribution dominate those obtained from addressing problems of heteroscedasticity...|$|R
