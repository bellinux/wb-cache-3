5694|259|Public
25|$|ThreadWeaver is a {{programming}} {{library to}} help applications {{take advantage of}} <b>multicore</b> processors and is included with kdelibs.|$|E
25|$|<b>Multicore</b> scaling on NUMA hardware: IIS 8.0 {{provides}} several configuration {{options that}} optimize performance on systems that run NUMA, such as running several worker processes under one application pool, using soft or hard affinity and more.|$|E
25|$|Cell is a {{multi-core}} microprocessor microarchitecture {{that combines}} a general-purpose Power Architecture core of modest performance with streamlined coprocessing elementss <b>Multicore</b> Architecture|publisher=IEEE|accessdate=2007-03-22|format=PDF}} which greatly accelerate multimedia and vector processing applications, {{as well as}} many other forms of dedicated computation.|$|E
40|$|Do {{applications}} programmers need {{to write}} explicitly parallel programs? Most people believe that the current method of parallel programming is impeding the exploitation of <b>multicores.</b> In other words, the number of cores in a microprocessor is likely to track Moore’s law in the near future, but the programming of <b>multicores</b> might remain the biggest obstacle in the forward march of performance. Let’s assume that this premise is true. Now, the real question becomes: how should applications programmers exploit the potential of <b>multicores?</b> There have been two main ideas in exploiting parallelism: implicitly an...|$|R
40|$|In {{this paper}} we present an {{architectural}} and on-line resource management solution to optimize lifetime reliability of asymmetric <b>multicores</b> while minimizing the system energy consumption, targeting both single nodes (<b>multicores)</b> {{as well as}} multiple ones (cluster of <b>multicores).</b> The solution exploits the different characteristics of the computing resources to achieve the desired performance while optimizing the lifetime/energy trade-off. The experimental results show that a combined optimization of energy and lifetime allows for achieving an extended lifetime (similar to the one pursued by lifetime-only optimization solutions) with a marginal energy consumption detriment (less than 2 %) with respect to energy-aware but aging-unaware systems...|$|R
40|$|Abstract. The whole {{computer}} hardware industry embraced <b>multicores.</b> For these machines, the extreme optimisation of sequential algorithms {{is no longer}} sufficient to squeeze the real machine power, which can be only exploited via thread-level parallelism. Decision tree algorithms exhibit natural concurrency that makes them suitable to be parallelised. This paper presents an approach for easy-yet-efficient porting of an implementation of the C 4. 5 algorithm on <b>multicores.</b> The parallel porting requires minimal changes to the original sequential code, and {{it is able to}} exploit up to 7 × speedup on an Intel dual-quad core machine. Keywords: parallel classification, C 4. 5, <b>multicores,</b> structured parallel programming, streaming...|$|R
25|$|The El Centro, CA MSA is a <b>multicore</b> {{metropolitan}} region containing several {{urban areas}}. While most urban areas {{are located in}} the metropolitan area, the Yuma, AZ CA-AZ urban area spills over into Winterhaven. Populations are from the U.S. Census Bureau.|$|E
25|$|Intel's Larrabee <b>multicore</b> {{architecture}} project uses {{a processor}} core {{derived from a}} P5 core (P54C), augmented by multithreading, 64-bit instructions, and a 16-wide vector processing unit. Intel's low-powered Bonnell microarchitecture employed in early Atom processor cores also uses an in-order dual pipeline similar to P5.|$|E
25|$|So much {{data was}} {{produced}} {{in the course of}} the creation of the movie, the studio was forced to upgrade all of its processors to <b>multicore</b> versions, which run quicker and more efficiently. The creation of additional rendering nodes throughout Culver City, California was necessitated by the movie's production.|$|E
50|$|The cable {{harnesses}} used {{in sound}} engineering (stage and studio) for carrying audio signals are also called <b>multicores.</b>|$|R
50|$|Composite <b>multicores</b> combine {{different}} types of signals in the one cable. They may contain coaxial cores for video, twisted pair for data or low voltage cores for mains power. Composite <b>multicores</b> are usually used to connect professional video cameras, but they are now gaining usage in live event support {{with the introduction of}} the Yamaha PM1D which uses a composite cable to connect it to the stage box.|$|R
5000|$|First Virtual School on Computational Science and Engineering: GPUs and <b>Multicores</b> - led by Wen-mei Hwu and David Kirk (Summer 2008) ...|$|R
25|$|Alpha {{was also}} {{implemented}} in the Piranha, a research prototype developed by Compaq's Corporate Research and Nonstop Hardware Development groups at the Western Research Laboratory and Systems Research Center. Piranha was a <b>multicore</b> design for transaction processing workloads that contained eight simple cores. It was described at the 27th Annual International Symposium on Computer Architecture in June 2000.|$|E
25|$|The {{breakdown}} of Dennard scaling prompted a switch among some chip manufacturers {{to a greater}} focus on <b>multicore</b> processors, but the gains offered by switching to more cores are lower than the gains that would be achieved had Dennard scaling continued. In another departure from Dennard scaling, Intel microprocessors adopted a non-planar tri-gate FinFET at 22 nm in 2012 that is faster and consumes less power than a conventional planar transistor.|$|E
25|$|Bader is {{an expert}} in the design and {{analysis}} of parallel and <b>multicore</b> algorithms for real-world applications such as those in cybersecurity and computational biology. He has won highly competitive awards from the National Science Foundation (NSF), IBM, Microsoft Research, Sony, and Sun Microsystems. Bader is also a Golden Core Member of the IEEE Computer Society (2010), and a recipient of the IEEE Computer Society Meritorious Service Award (2010). He has co-chaired a series of meetings, the IEEE International Workshop on High-Performance Computational Biology (HiCOMB), written several book chapters, and co-edited a special issue of the Journal of Parallel and Distributed Computing on High-Performance Computational Biology. He has co-authored over 210 articles in peer-reviewed journals and conferences.|$|E
40|$|For {{power and}} {{performance}} reasons, <b>multicores</b> {{have become the}} dominant microprocessor architecture. However, <b>multicores</b> have many components (e. g., caches and cores) whose interactions are timing dependent. As {{a result of this}} non-determinism, <b>multicores</b> exhibit both software and hardware bugs. On the software side, <b>multicores</b> run shared-memory parallel programs which suffer from non-deterministic bugs. Such bugs are difficult to reliably replicate during debug executions. Further, writing parallel programs is prone to errors such as unintended data races, livelocks, and deadlocks. ^ On the hardware side, <b>multicores</b> employ coherence protocols to facilitate communication between cores. However, the many interacting components lead to complex, error-prone protocols. Verifying a protocol involves checking each of the system 2 ̆ 7 s states, the count of which increases exponentially with the number of components (e. g., cores). Thus, the verification effort increases with system scale. Further, every time the system size increases, the protocol must be re-verified. Existing protocols are either not scalably verifiable (most existing protocols) or are verifiable but do not scale well in performance (e. g., fractal coherence). ^ In my thesis I propose new memory system architectures to address the above software programmability and hardware verifiability problems. Specifically, to address the difficulty of debugging and writing parallel programs, I propose a low-overhead hardware record-and-replay system and architectural support for a high-performance transactional memory system, respectively. To address the difficulty of scalably verifying coherence protocols, I propose a high-performance fractal coherence protocol. ...|$|R
40|$|With the {{emergence}} of accelerator devices such as <b>multicores,</b> graphics-processing units (GPUs), and field-programmable gate arrays (FPGAs), application designers are confronted {{with the problem of}} searching a huge design space that has been shown to have widely varying performance and energy metrics for different accelerators, different application domains, and different use cases. To address this problem, numerous studies have evaluated specific applications across different accelerators. In this paper, we analyze an important domain of applications, referred to as sliding-window applications, when executing on FPGAs, GPUs, and <b>multicores.</b> For each device, we present optimization strategies and analyze use cases where each device is most effective. The results show that FPGAs can achieve speedup of up to 11 x and 57 x compared to GPUs and <b>multicores,</b> respectively, while also using orders of magnitude less energy...|$|R
40|$|Management {{of shared}} {{resources}} in emerging <b>multicores</b> for achiev-ing predictable performance has received considerable attention in recent times. In general, almost all these approaches attempt {{to guarantee a}} certain level of performance QoS (weighted IPC, har-monic speedup, etc) by managing a single shared resource or at most a couple of interacting resources. A fundamental shortcoming of these approaches is the lack of coordination between these shared resources to satisfy a system level QoS. This is undesirable because providing end-to-end QoS in future <b>multicores</b> is essential for sup-porting wide-spread adoption of these architectures in virtualized servers and cloud computing systems. An initial step towards such an end-to-end QoS support in <b>multicores</b> is to ensure that at least the major computational and memory resources on-chip are man-aged efficiently in a coordinated fashion...|$|R
25|$|Open Virtual Platforms (OVP) {{includes}} the freely available for non-commercial use simulator OVPsim, {{a library of}} models of processors, peripherals and platforms, and APIs which enable users {{to develop their own}} models. The models in the library are open source, written in C, and include the MIPS 4K, 24K, 34K, 74K, 1004K, 1074K, M14K, microAptiv, interAptiv, proAptiv 32 bit cores and the MIPS 64bit 5K range of cores. These models are created and maintained by Imperas and in partnership with MIPS Technologies have been tested and assigned the MIPS-Verified (tm) mark. Sample MIPS-based platforms include both bare metal environments and platforms for booting unmodified Linux binary images. These platforms–emulators are available as source or binaries and are fast, free for non-commercial usage, and are easy to use. OVPsim is developed and maintained by Imperas and is very fast (hundreds of million of instructions per second), and built to handle <b>multicore</b> homogeneous and heterogeneous architectures and systems.|$|E
500|$|NIX is a fork of Plan9 {{aimed at}} <b>multicore</b> systems and cloud computing. The [...] {{contains}} more information.|$|E
500|$|Rumors of a {{successor}} to Poulson (code named Kittson) began to circulate in 2012–2013. This was at first associated with a forthcoming 22nm shrink, and later revised {{in the face of}} declining Itanium sales to a less-ambitious 32nm node. In April 2015, [...] Intel, although it had not yet confirmed formal specifications, did confirm that it continued to work on the project. [...] Meanwhile, the aggressively <b>multicore</b> Xeon E7 platform displaced Itanium-based solutions in the Intel roadmap.|$|E
40|$|Temperature {{has become}} an {{important}} constraint in high-performance processors, especially <b>multicores.</b> Thread migration will be essential to exploit the full potential of future thermally con-strained <b>multicores.</b> We propose and study a thread migration method that maximizes performance under a temperature constraint, while minimizing the number of migrations and ensuring fairness between threads. We show that thread migration brings important performance gains {{and that it is}} most effective during the first tens of seconds following a decrease of the number of running threads...|$|R
40|$|Embedded {{computing}} {{is one of}} {{the most}} important areas in computer science today, witnessed by the fact that 98 % of all computers are embedded. Given that many embedded systems have to interact “promptly ” with their physical environment, the scientific community has invested signifi-cant efforts in developing algorithms for scheduling the workload, which is generally implemented as a set of tasks, at the right time and in proving before run-time that all the timing requirements will be satisfied at run-time. This field of study is referred to as the real-time scheduling theory. The scheduling theory for a unicore processor is well-developed; the scientific results are taught at all major universities world-wide and the results are widely-used in industry. Scheduling theory for <b>multicores</b> is emerging but the focus so far has been for <b>multicores</b> with identical pro-cessing units. This is unfortunate because the computer industry is moving towards heterogeneous <b>multicores</b> with a constant number of distinct processor types — AMD Fusion, Intel Atom and NVIDIA Tegra are some of the examples of such <b>multicores.</b> This work deals with the problem of scheduling a set of tasks to meet their deadlines on het-erogeneous multiprocessors with a constant number of distinct processor types. On heterogeneou...|$|R
40|$|The whole {{computer}} hardware industry embraced <b>multicores.</b> For these machines, the extreme optimisation of sequential algorithms {{is no longer}} sufficient to squeeze the real machine power, which can be only exploited via thread-level parallelism. Decision tree algorithms exhibit natural concurrency that makes them suitable to be parallelised. This paper presents an approach for easy-yet-efficient porting of an implementation of the C 4. 5 algorithm on <b>multicores.</b> The approach {{is based on the}} FastFlow parallel programming environment. The strength of our porting consists in minimal changes to the original sequential code. In addition to the tree building algorithm, we consider also the so far unaddressed problem of parallelising the error-based pruning with grafting algorithm of C 4. 5. We devise lower bounds for the forms of parallelisations adopted, and achieve performances close to such bounds. Keywords Parallel classification, <b>multicores,</b> C 4. 5, error-based pruning, structured parallel programming, streaming parallelism. ...|$|R
2500|$|IIS 8.0 is only {{available}} in Windows Server 2012 and Windows 8. [...] IIS 8.0 includes SNI (binding SSL to hostnames rather than IP addresses), Application Initialization, centralized SSL certificate support, and <b>multicore</b> scaling on NUMA hardware, among other new features.|$|E
2500|$|PARbars are PARs {{that are}} {{permanently}} affixed to and circuited through an aluminium pipe. [...] Four PARs on a bar are called a four-bar. [...] Six PARs are called six-bars. Typically mains <b>multicore</b> (AKA Socapex) cabling {{will be used}} to supply power to the bars, so that only one connection has to be made, thus saving assembly time and giving a neater appearance.|$|E
2500|$|Many {{operating}} systems support multitasking which enables many computer programs {{to appear to}} run simultaneously on one computer. [...] Operating systems may run multiple programs through process scheduling– a software mechanism to switch the CPU among processes often so users can interact with each program while it runs. Within hardware, modern day multiprocessor computers or computers with <b>multicore</b> processors may run multiple programs.|$|E
40|$|<b>Multicores</b> {{have become}} the {{platform}} of choice across all market segments. Cost-eective protection against soft er-rors is important in these environments, due {{to the need to}} move to lower technology generations and the exploding number of transistors on a chip. While <b>multicores</b> oer the exibility of varying the number of application threads and the number of cores on which they run, the reliability im-pact of choosing one conguration over another is unclear. Our study reveals that the reliability costs vary dramatically between congurations and being unaware could lead to a sub-optimal choice...|$|R
40|$|The whole {{computer}} hardware industry embraced <b>multicores.</b> For these machines, the extreme optimisation of sequential algorithms {{is no longer}} sufficient to squeeze the real machine power, which can be only exploited via thread-level parallelism. Decision tree algorithms exhibit natural concurrency that makes them suitable to be parallelised. This paper presents an approach for easy-yet-efficient porting of an implementation of the C 4. 5 algorithm on <b>multicores.</b> The parallel porting requires minimal changes to the original sequential code, and {{it is able to}} exploit up to 7 × speedup on an Intel dual-quad core machine...|$|R
40|$|In recent years, {{a variety}} of {{concerns}} in power and thermal issues, instruction-level parallelism (ILP) limits, and design complexity, among others have driven a shift in focus away from uniprocessor systems to chip multiprocessor (CMP) designs, or more popularly known as <b>multicores</b> or manycores. <b>Multicores</b> alleviate many of these uniprocessor scaling barriers, by providing the potential for throughput and performance gains with {{an increasing number of}} on-chip processor cores. Ideally, designers would like to extract performance and throughput gains proportional to the increase in cores at each technology generation. Unfortunately, a major challenge, the programming wall, needs to be addressed before such a goal can be achieved. It is the obstacle for programmers to efficiently parallelize their applications to exploit <b>multicores</b> and manycores. My current and future research goal is to overcome the multicore/manycore programming wall by means of compilers and programming tools, runtime systems, and architectures. To achieve this goal, I am currently leading the Center fo...|$|R
2500|$|Sound {{reinforcement}} {{in a large}} format system typically {{involves a}} signal path that starts with the signal inputs, which may be instrument pickups (on an electric guitar or electric bass) or a microphone that a vocalist is singing into or a microphone {{placed in front of}} an instrument or guitar amplifier. These signal inputs are plugged into the input jacks of a thick <b>multicore</b> cable (often called a snake). [...] The snake then delivers the signals of all of the inputs to either one or more mixing consoles.|$|E
2500|$|Another {{challenge}} with designing sound systems for live music clubs {{is that the}} sound system {{may need to be}} used for both prerecorded music played by DJs and live music. If the sound system is optimized for prerecorded DJ music, then it will not provide the appropriate sound qualities (or mixing equipment and monitoring equipment) needed for live music, and vice versa. A club system designed for DJs needs a DJ mixer and space for record players. Clubs tend to focus on either live music or DJ shows. However, clubs which feature both types of shows may face challenges providing the desired equipment and set-up for both uses. In contrast, a live music club needs a mixing board designed for live sound, an onstage monitor system, and a <b>multicore</b> [...] "snake" [...] cable running from the stage to the mixer. Lastly, live music clubs can be a hostile environment for sound gear, in that the air may be hot, humid, and smoky; in some clubs, keeping racks of power amplifiers cool may be a challenge. Often an air conditioned room just for the amplifiers is utilised.|$|E
5000|$|<b>Multicore</b> {{embedded}} applications design Strategies {{for designing}} a <b>multicore</b> application; Regular vs. irregular applications. Lab on heterogeneous <b>multicore</b> architectures (with GPUs).|$|E
40|$|Abstract—Elementary {{functions}} are extensively used in com-puter graphics, signal and image processing, and communication systems. This paper presents a special-purpose compiler that automatically generates customized look-up tables and imple-mentations for elementary functions under user given constraints. The generated implementations include a C/C++ code {{that can be}} used directly by applications running on <b>multicores,</b> as well as a MATLAB-like code that can be translated directly to a hardware module on FPGA platforms. The experimental results show that our solutions for function evaluation bring significant performance improvements to applications on <b>multicores</b> as well as significant resource savings to designs on FPGAs. I...|$|R
40|$|Concurrentlyrunningapplicationsonmultiprocessors may desire {{different}} CPU frequency/voltage {{settings in}} order to achieve performance, power, or thermal objectives. Today’s <b>multicores</b> typically require that all sibling cores on a single chip run at the same frequency/voltagelevelwhile differentCPU chipscanhave non-uniform settings. This paper targets multicorebased symmetric platforms and demonstrates the benefits of per-chip adaptive frequency scaling on <b>multicores.</b> Specifically,bygroupingapplicationswithsimilar frequency-to-performance effects, we create the opportunity for setting a chip-wide desirable frequency level. Werunexperimentswith 12 SPECCPU 2000 benchmarks andtwo server-style applicationsonamachinewith two dual-core Intel “Woodcrest ” processors. Results show that per-chip frequency scaling can save ∼ 20 watts of CPU power while maintaining performance within a specifiedboundoftheoriginalsystem. ...|$|R
30|$|For parallelization, {{we chose}} a Message Passing Interface (MPI) library called MPICH 2 [38]. The MPICH 2 {{is a message}} passing {{interface}} library with tested and proved scalability on <b>multicores</b> [39] and proved performance of message passing on shared memory [40].|$|R
