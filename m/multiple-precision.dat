96|0|Public
50|$|RISC-V has no {{condition}} code register or carry bit. The designers believed that {{condition code}}s make fast CPUs more complex by forcing interactions between instructions in {{different stages of}} execution. This choice makes <b>multiple-precision</b> arithmetic more complex. Also, a few numerical tasks need more energy.|$|E
5000|$|Twelve bit {{arithmetic}} {{may seem}} limited compared to modern computers with 32 or 64-bit words. But, twelve bits could handle unsigned integers from 0 to 4095 and signed numbers from -2048 to +2047. This can control machinery {{to more than}} three decimal digits of precision. It also was higher precision than a slide rule or most analog computers. The PDP-8 also had a carry bit, [...] "link," [...] so that software could do <b>multiple-precision</b> arithmetic. A calculation using two 12-bit words has more that seven decimal digits of precision. Even when using <b>multiple-precision</b> arithmetic, the PDP-8 was thousands of times faster than affordable electromechanical calculators such as a Friden calculator, and substantially less expensive than many contemporary electronic computers, such as the IBM 701.|$|E
50|$|Pure, {{successor}} to the equational language Q, is a dynamically typed, functional programming language based on term rewriting. It has facilities for user-defined operator syntax, macros, arbitrary-precision arithmetic (<b>multiple-precision</b> numbers), and compiling to native code through the LLVM. Pure is free and open-source software distributed (mostly) under the GNU Lesser General Public License version 3 or later.|$|E
5000|$|In {{a monumental}} {{breakthrough}} publication (Electronics Letters, 1966), {{he explained the}} [...] "secret" [...] behind the low passband sensitivity of doubly loaded reactance two-ports and showed how to design active two-ports that retain this key attribute. Among Professor Orchard's key contributions was {{the development of a}} systematic process for the computer-aided design of filters. During the early years of computer-aided filter design, when the synthesis of a single circuit required several days of <b>multiple-precision</b> computation, his method had an important beneficial effect. He was instrumental in introducing into switched capacitor filter design the bilinear s-z mapping, previously used solely in digital filter design, and in developing a methodology that allowed the use of arbitrary active-RC models for switched-capacitor filter syntheses.|$|E
50|$|We {{could try}} to use the {{available}} arithmetic of the computer more efficiently. A simple escalation would be to use base 100 (with corresponding changes to the translation process for output), or, with sufficiently wide computer variables (such as 32-bit integers) we could use larger bases, such as 10,000. Working in a power-of-2 base closer to the computer's built-in integer operations offers advantages, although conversion to a decimal base for output becomes more difficult. On typical modern computers, additions and multiplications take constant time independent {{of the values of}} the operands (so long as the operands fit in single machine words), so there are large gains in packing as much of a bignumber as possible into each element of the digit array. The computer may also offer facilities for splitting a product into a digit and carry without requiring the two operations of mod and div as in the example, and nearly all arithmetic units provide a carry flag which can be exploited in <b>multiple-precision</b> addition and subtraction. This sort of detail is the grist of machine-code programmers, and a suitable assembly-language bignumber routine can run much faster than the result of the compilation of a high-level language, which does not provide access to such facilities.|$|E
40|$|<b>Multiple-precision</b> modular multiplications {{are the key}} {{components}} in security applications, like public-key cryptography for encrypting and signing digital data. But unfortunately they are computationally expensive for contemporary CPUs. By exploiting the computing power of the many-core GPUs, we implemented a <b>multiple-precision</b> integer library with CUDA. In this paper, we will investigate the implementation of two approaches of <b>multiple-precision</b> modular multiplications on GPU. We will analyze the detail of the instructions of <b>multiple-precision</b> modular multiplication on the GPU and find the hit issues, and then we propose to use the inline ASM to improve the implementation of this function. Our experimental {{results show that the}} performance of <b>multiple-precision</b> modular multiplication has been improved by 20 %...|$|E
40|$|We present {{efficient}} parallel algorithms for <b>multiple-precision</b> arithmetic {{operations of}} more than several million decimal digits on distributed-memory parallel computers. A parallel implementation of floating-point real FFT-based multiplication is used, since the key operation for fast <b>multiple-precision</b> arithmetic is multiplication. The operation for releasing propagated carries and borrows in <b>multiple-precision</b> addition, subtraction and multiplication was also parallelized. More than 2. 576 trillion decimal digits of π were computed on 640 nodes of Appro Xtreme-X 3 (648 nodes, 147. 2 GFlops/node, 95. 4 TFlops peak performance) with a computing elapsed time of 73 h 36 min which includes the time required for verification...|$|E
40|$|Abstract. We present {{efficient}} parallel algorithms for <b>multiple-precision</b> arithmetic {{operations of}} more than several million decimal digits on distributed-memory parallel computers. A parallel implementation of floating-point real FFT-based multiplication is used because a key operation in fast <b>multiple-precision</b> arithmetic is multiplication. We also parallelized an operation of releasing propagated carries and borrows in <b>multiple-precision</b> addition, subtraction and multiplication. More than 1. 6 trillion decimal digits of π were computed on 256 nodes of Appro Xtreme-X 3 (648 nodes, 147. 2 GFlops/node, 95. 4 TFlops peak performance) with a computing elapsed time of 137 hours 42 minutes which includes the time for verification. ...|$|E
40|$|This paper {{presents}} a heuristic method {{to perform the}} high-level synthesis of <b>multiple-precision</b> specifications. The scheduling {{is based on the}} balance of the number of bits calculated per cycle, and the allocation on the bit-level reuse of the hardware resources. The implementations obtained are <b>multiple-precision</b> datapaths independent of the number and widths of the specification operations. As a result impressive area savings are achieved in comparison with conventional algorithms implementations...|$|E
40|$|This is the CODE-RADE Foundation Release 3 of The GNU MPFR Library. MPFR is a C {{library for}} <b>multiple-precision</b> floating-point {{computations}} with correct rounding. Laurent Fousse, Guillaume Hanrot, Vincent Lefèvre, Patrick Pélissier, and Paul Zimmermann. 2007. MPFR: A <b>multiple-precision</b> binary floating-point library with correct rounding. ACM Trans. Math. Softw. 33, 2, Article 13 (June 2007). DOI=[URL] This patch release includes the updates to the build containers, and has passed build tests. See [URL]...|$|E
40|$|International audienceWe {{consider}} {{the problem of}} short division [...] - division without remainder [...] - of <b>multiple-precision</b> integers. We present ready-to-be-implemented algorithms that yield an approximation of the quotient, with tight and rigorous error bounds. We exhibit speedups of up to 30 % with respect to GMP division with remainder, and up to 10 % with respect to GMP short division, with room for further improvements. This work enables one to implement fast correctly rounded division routines in <b>multiple-precision</b> software tools...|$|E
40|$|In {{studying}} {{the complexity of}} iterative processes it is usually assumed that the arithmetic operations of addition, multiplication, and division can be performed in certain constant times. This assumption is invalid if the precision required increases as the computation proceeds. We give {{upper and lower bounds}} on the number of single-precision operations required to perform various <b>multiple-precision</b> operations, and deduce some interesting consequences concerning the relative efficiencies of methods for solving nonlinear equations using variable-length <b>multiple-precision</b> arithmetic. A postscript describes more recent developments. Comment: An old (1976) paper with a postscript (1999) describing more recent developments. 30 pages. For further details, see [URL]...|$|E
40|$|This {{research}} {{reveals the}} dependency of floating point computation in nonlinear dynamical systems on machine precision and step-size by applying a <b>multiple-precision</b> {{approach in the}} Lorenz nonlinear equations. The paper also demonstrates the procedures for obtaining a real numerical solution in the Lorenz system with long-time integration and a new multiple-precision-based approach used to identify the maximum effective computation time (MECT) and optimal step-size (OS). In addition, the authors introduce how to analyze round-off error in a long-time integration in some typical cases of nonlinear systems and present its approximate estimate expression. Key words: <b>multiple-precision</b> numerical calculation, round-off error, nonlinear dynamical system doi: 10. 1007 /s 00376 - 006 - 0758 -y 1...|$|E
40|$|<b>Multiple-precision</b> {{multiplication}} algorithms are {{of fundamental}} interest for both {{theoretical and practical}} reasons. The conventional method requires 0 (n 2) bit operations whereas the fastest known multiplication algorithm is of order 0 (n log n log log n). The price {{that has to be}} paid for the increase in speed is a much more sophisticated theory and programming code. This work presents an extensive study of the best known <b>multiple-precision</b> multiplication algorithms. Different algorithms are implemented in C, their performance is analyzed in detail and compared to each other. The break even points, which are essential for the selection of the fastest algorithm for a particular task, are determined for a given hardware software combination...|$|E
40|$|AbstractWe {{study the}} problem of {{computing}} middle products of <b>multiple-precision</b> integers. In particular we adapt the Karatsuba polynomial middle product algorithm to the integer case, showing how to efficiently mitigate the failure of bilinearity of the integer middle product noted by Hanrot, Quercia and Zimmermann...|$|E
40|$|Thesis (Master) [...] İzmir Institute of Technology, Computer Engineering, İzmir, 2005 Includes bibliographical {{references}} (leaves: 61 - 63) Text in English; Abstract: Turkish and Englishxi, 63 leavesCryptographic schemes require {{specialized software}} libraries {{to work with}} large numbers on fixed-precision processors. The concept is known as <b>multiple-precision</b> computation. In this thesis, we aim to review the <b>multiple-precision</b> algorithms with the contemporary modifications. With this motivation, we develop a new multiprecision library named CRYMPIX and we carefully benchmark CRYMPIX {{in comparison with the}} fastest alternatives. We also develop a distributed wrapper for computationally expensive functions. Hence, we provide an abstraction method for the higher level cryptographic implementations by allowing them run in a distributed environment without containing any specialized code for distribution...|$|E
40|$|INTRODUCTION Many exact integer computations, {{rather than}} being {{performed}} using <b>multiple-precision</b> arithmetic, are performed instead {{over a number of}} single-precision modular rings or fields, with the results being then combined using the Chinese Remainder Theorem [Moses 71] [Knuth 81] [Gregory 84] [Schroeder 86]. Organizing the computation in this way avoids many <b>multiple-precision</b> arithmetic operations and the storage management overheads that these operations entail. Unfortunately, unless the hardware architecture and computer language offers double-precision multiplication and double-precision Euclidean division-with-remainder operations, the cost of simulating these double-precision operations can eliminate any savings from the modular approach. In order to profit from modular techniques, therefore, one must either restrict the program to use only halfprecision integer operations (e. g., 15 bits), or seek another way to avoid double-precision operations. The popularity of ANS...|$|E
40|$|Author’s summary: “A {{technique}} is described for the non-tentative computer {{determination of the}} Galois groups of irreducible polynomials with integer coefficients. The technique for a given polynomial involves finding high-precision approximations {{to the roots of}} the polynomial, and fixing an ordering for these roots. The roots are then used to create resolvent polynomials of relatively small degree, the linear factors of which determine new orderings for the roots. Sequences of these resolvents isolate the Galois group of the polynomial. Machine implementation of the technique requires the use of <b>multiple-precision</b> integer and <b>multiple-precision</b> real and complex floatingpoint arithmetic. Using this technique, the author has developed programs for the determination of the Galois groups of polynomials of degree N ≤ 7. Two illustrative calculations are given. ” Reviewed by P. Abellana...|$|E
40|$|Conference URL: [URL] {{study the}} <b>multiple-precision</b> {{addition}} of two positive floating-point numbers in base 2, with exact rounding, as {{specified in the}} MPFR library, i. e. where each number has its own precision. We show how the best possible complexity (up to a constant factor {{that depends on the}} implementation) can be obtain...|$|E
40|$|Introduction: There {{has been}} {{significant}} recent research into multiplewordlength or <b>multiple-precision</b> systems, where datapaths are constructed from operators with different bit-width [1 [...] 3]. However, {{little research has}} been conducted [3, 4] into high-level synthesis for these systems. The use of multiple wordlengths has {{a significant impact on the}} traditional problems of high-level synthesis: scheduling, resource binding, and module selection [5]. This is the result of two factors. First, each computational unit of a specific type, for example `multiplier', cannot be assumed to have an equal cost in a <b>multiple-precision</b> system [4]. Secondly, the choice of wordlength for an operation can impact on the latency of that operation. The existence of multiple wordlengths therefore complicates the resource binding problem, and also increases the interaction between operation binding and scheduling. To our knowledge, this Letter is the first formulation of the combined scheduling...|$|E
40|$|Abstract. We {{present a}} {{parallel}} implementation of Schönhage’s inte-ger GCD algorithm on distributed memory architectures. Results are generalized for the extended GCD algorithm. Experiments on sequential architectures show that Schönhage’s algo-rithm overcomes other GCD algorithms implemented in two well known <b>multiple-precision</b> packages for input sizes {{larger than about}} 50000 bytes. In the extended case this threshold drops to 10000 bytes. In these input ranges a parallel implementation provides additional speed-up. Paral-lelization is achieved by distributing matrix operations and by using parallel implementations of the <b>multiple-precision</b> integer multiplication algorithms. We use parallel Karatsuba’s and parallel 3 -primes FFT mul-tiplication algorithms implemented in CALYPSO, a computer algebra library for parallel symbolic computation we have developed. Schönhage’s parallel algorithm is analyzed by using a message-passing model of computation. Experimental results on distributed memory ar-chitectures, such as the Intel Paragon, conrm the analysis. ...|$|E
40|$|This paper {{presents}} an heuristic method {{to solve the}} combined resource selection and binding problems for the high-level synthesis of <b>multiple-precision</b> specifications. Traditionally, the number of functional (and storage) units in a datapath {{is determined by the}} maximum number of operations scheduled in the same cycle, with their respective widths depending on the number of bits of the wider operations. When these wider operations are not scheduled in such “busy ” cycle, this way of acting could produce a considerable waste of area. To overcome this problem, we propose the selection of the set of resources taking into account the only truly relevant aspect: the maximum number of bits calculated and stored simultaneously in a cycle. The implementation obtained is a <b>multiple-precision</b> datapath, where the number and widths of the resources are independent of the specification operations and data objects. 1...|$|E
40|$|We {{present an}} {{algorithm}} allowing to perform integer multiplications by constants. This algorithm {{is compared to}} existing algorithms. Such algorithms are useful, as they occur in several problems, such as the Toom-Cook-like algorithms to multiply large <b>multiple-precision</b> integers, the approximate computation of consecutive values of a polynomial, and the generation of integer multiplications by compilers...|$|E
40|$|Abstract. The {{classical}} algorithm for <b>multiple-precision</b> division normalizes digits {{during each}} step and sometimes makes correction steps when the initial guess for the quotient digit {{turns out to}} be wrong. A method is presented that runs faster by skipping most of the intermediate normalization and recovers from wrong guesses without separate correction steps. 1...|$|E
40|$|We {{present and}} compare various algorithms, {{including}} a new one, allowing to perform multiplications by integer constants using elementary operations. Such algorithms are useful, as they occur in several problems, such as the Toom-Cook-like algorithms to multiply large <b>multiple-precision</b> integers, the approximate computation of consecutive values of a polynomial, and the generation of integer multiplications by compilers...|$|E
40|$|FM is a {{collection}} of Fortran- 77 routines which performs floating-point <b>multiple-precision</b> arithmetic and elementary functions. Results are almost always correctly rounded, and due to improved algorithms used for the elementary functions, reasonable efficiency is obtained. Categories and Subject Descriptors: G. 1. 0 [Numerical Analysis]: General – computer arithmetic; G. 1. 2 [Numerical Analysis]: Approximation – elementary function approximation...|$|E
40|$|We {{study the}} <b>multiple-precision</b> {{addition}} of two positive floating-point numbers in base 2, with exact rounding, as {{specified in the}} MPFR library, i. e. where each number has its own precision. We show how the best possible complexity (up to a constant factor {{that depends on the}} implementation) can be obtain. Comment: Conference website at [URL]...|$|E
40|$|INTRODUCTION The FMLIB {{package of}} Fortran subroutines for floating-point <b>multiple-precision</b> {{computation}} now includes routines {{to evaluate the}} Gamma function and related functions. These routines use the basic FM operations and derived types [Smith 1991; 1998] for <b>multiple-precision</b> arithmetic, constants, and elementary functions. The new functions available are essentially those in the chapter on the Gamma function in a reference such as Abramowitz and Stegun [1965]. The FM routines almost always return correctly rounded results. Extensive testing has found no cases where the error before rounding the final value was more than 0. 001 unit in the last place of the returned result. This means that in rare cases the returned result di#ered from the correctly rounded value by a maximum of one unit in the last place for a given precision. Most of these routines gain speed by storing constants such as Bernoulli numbers and Euler's constant so {{they do not have}} to be computed again on...|$|E
40|$|We {{describe}} {{two techniques}} for fast <b>multiple-precision</b> evaluation of linearly convergent series, including power series and Ramanujan series. The computation time for N bits is O((log N) &sup 2;M(N)), where M(N) {{is the time}} needed to multiply two N-bit numbers. Applications include fast algorithms for elementary functions, &pi;, hypergeometric functions at rational points, Euler's, Catalan's and Apéry's constant. The algorithms are suitable for parallel computation...|$|E
40|$|The exact {{computation}} of {{orbits of}} discrete dynamical systems on the interval is considered. There-fore, a <b>multiple-precision</b> floating point approach based on error analysis is chosen {{and a general}} algorithm is presented. The correctness of the algorithm is shown and the computational complexity is analyzed. As a main result, the computational complexity measure considered here {{is related to the}} Ljapunow exponent of the dynamical system under consideration. ...|$|E
40|$|Abstract: We {{present and}} compare various algorithms, {{including}} a new one, allowing to perform multiplications by integer constants using elementary operations. Such algorithms are useful, as they occur in several problems, such as the Toom-Cook-like algorithms to multiply large <b>multiple-precision</b> integers, the approximate computation of consecutive values of a polynomial, and the generation of integer multiplications by compilers. Key-words: integer multiplication, addition chains Unité de recherche INRIA Lorrain...|$|E
40|$|International audienceMany {{scientific}} computing applications demand massive numerical computations on parallel architectures such as Graphics Processing Units (GPUs). Usually, either floating-point {{single or}} double precision arithmetic is used. Higher precision {{is generally not}} available in hardware, and software extended precision libraries are much slower and rarely supported on GPUs. We develop CAMPARY: a <b>multiple-precision</b> arithmetic library, using the CUDA programming language for the NVidia GPU platform. In our approach, the precision is extended by representing real numbers as the unevaluated sum of several standard machine precision floating-point numbers. We make use of error-free transforms algorithms, which are based only on native precision operations, but keep track of all rounding errors that occur when performing a sequence of additions and multiplications. This offers the simplicity of using hardware highly optimized floating-point operations, while also allowing for rigorously proven rounding error bounds. This also allows for easy implementation of an interval arithmetic. Currently, all basic <b>multiple-precision</b> arithmetic operations are supported. Our target applications are in chaotic dynamical systems or automatic control...|$|E
40|$|International audienceThis paper {{presents}} a <b>multiple-precision</b> binary floating-point library, {{written in the}} ISO C language, {{and based on the}} GNU MP library. Its particularity is to extend to arbitrary-precision ideas from the IEEE 754 standard, by providing correct rounding and exceptions. We demonstrate how these strong semantics are achieved [...] with no significant slowdown with respect to other arbitrary-precision tools [...] and discuss a few applications where such a library can be useful...|$|E
40|$|A paraîtreInternational audienceWe {{describe}} the mechanisms {{and implementation of}} a library that defines a decimal <b>multiple-precision</b> interval arithmetic. The aim of such a library is to provide guaranteed and accurate results in decimal. This library contains correctly rounded (for decimal arbitrary precision), fast and reliable basic operations and some elementary functions. Furthermore the decimal representation is IEEE 754 - 2008 compatible, and the interval arithmetic is compliant with the new IEEE 1788 - 2015 Standard for Interval Arithmetic [2]...|$|E
40|$|Foremost, I {{would like}} to express my {{gratitude}} to my advisor, Assoc. Prof. Dr. Ahmet Koltuksuz, for his guidance, patience, and encouragement. He {{was the one who}} uplifted me when I was in trouble with critical decisions. His valuable support, and confidence have been the driving force of this thesis work. Furthermore, I had the pleasure of working with Serap Atay who helped me in understanding the mathematical background of many algorithms. I should also thank to my room mates Selma Tekir and S¸ükran Asarcıklı for their patience and support. I {{would also like to thank}} Asst. Prof. Murat Atmaca, Sultan Eylem Toksoy and Gök¸sen Bacak for their help in writing this thesis with LATEX 2 e. Finally, I should thank to my parents who always supported me throughout my Cryptographic schemes require specialized software libraries to work with large numbers on fixed-precision processors. The concept is known as <b>multiple-precision</b> computation. In this thesis, we aim to review the <b>multiple-precision</b> algorithms with the contemporary modifications. With this motivation, we develop a new multiprecision library named CRYMPIX and we carefully benchmark CRYMPIX in comparison with th...|$|E
40|$|A {{collection}} of ANSI Standard Fortran subroutines for performing <b>multiple-precision</b> floating-point arithmetic and evaluating elementary and special functions is described. The subrou-tines are machine independent and the precision is arbitrary, subject to storage limitations. The {{design of the}} package is discussed, some of the algomthms are described, and test results are given. Key Words and Phrases ' arithmetic, multiple precision, extended precision, floating point, elementary function evaluation, Euler's constant, gamma function, polyalgorithm, software package, Fortran, machine-independent software, special function evaluation, Bessel func...|$|E
