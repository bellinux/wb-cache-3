10000|10000|Public
25|$|Because of {{the motion}} of the Earth around the Sun, the <b>measured</b> <b>data</b> were also {{expected}} to show annual variations.|$|E
25|$|Model {{ships are}} {{important}} {{in the field of}} engineering, where analytical modeling of a new design needs to be verified. Principals of similitude are used to apply <b>measured</b> <b>data</b> from a scaled model to the full scale design. Models are often tested in special facilities known as model basins.|$|E
25|$|Scientific {{visualization}} using {{computer graphics}} gained in popularity as graphics matured. Primary applications were scalar fields and vector fields from computer simulations and also <b>measured</b> <b>data.</b> The primary methods for visualizing two-dimensional (2D) scalar fields are color mapping and drawing contour lines. 2D vector fields are visualized using glyphs and streamlines or line integral convolution methods. 2D tensor fields are often resolved to a vector field by {{using one of}} the two eigenvectors to represent the tensor each point in the field and then visualized using vector field visualization methods.|$|E
50|$|One {{application}} of multilevel modeling (MLM) is {{the analysis of}} repeated <b>measures</b> <b>data.</b> Multilevel modeling for repeated <b>measures</b> <b>data</b> is most often discussed {{in the context of}} modeling change over time (i.e. growth curve modeling for longitudinal designs); however, it may also be used for repeated <b>measures</b> <b>data</b> in which time is not a factor.|$|R
5000|$|... #Subtitle level 3: Applications to {{longitudinal}} (repeated <b>measures)</b> <b>data</b> ...|$|R
40|$|Hierarchical Linear Models {{are widely}} used in {{psychiatric}} research to model repeated <b>measures</b> <b>data.</b> The Mixed Procedure in SAS can be used implement HLM and model the covariance structure of repeated <b>measures</b> <b>data.</b> This paper investigates the use of Proc Mixed and other SAS procedures to model diurnal cortisol slopes...|$|R
25|$|The Cassini {{spacecraft}} has directly sampled the plumes {{escaping from}} Enceladus. <b>Measured</b> <b>data</b> indicates that these geysers are made primarily of salt rich particles with an 'ocean-like' composition, which {{is believed to}} originate from a subsurface ocean of liquid saltwater, {{rather than from the}} moon's icy surface. Data from the geyser flythroughs also indicate the presence of basic organic chemicals in the plumes. Heat scans of Enceladus' surface also indicate warmer temperatures around the fissures where the geysers originate from, with temperatures reaching -93°C (-135°F), which is 115°C (207°F) warmer than the other regions on the moon's surface.|$|E
2500|$|This is an {{electronic}} or fiberoptic sensor to provide continuous monitoring of downhole pressure and temperature. Gauges either use a 1/4" [...] control line clamped onto {{the outside of}} the tubing string to provide an electrical or fiberoptic communication to surface, or transmit <b>measured</b> <b>data</b> to surface by acoustic signal in the tubing wall.|$|E
2500|$|Where A and P are {{the angles}} of the {{analyzer}} and polarizer under null conditions respectively. [...] By rotating the analyzer and polarizer and measuring {{the change in}} intensities of light over the image, analysis of the <b>measured</b> <b>data</b> by use of computerized optical modeling {{can lead to a}} deduction of spatially resolved film thickness and complex refractive index values.|$|E
40|$|We give a {{definition}} for Obstacle Problems with <b>measure</b> <b>data</b> and general obstacles. For such problems we prove existence and uniqueness of solutions and consistency with the classical theory of Variational Inequalities. Continuous dependence {{with respect to}} data is discussed. Ref. S. I. S. S. A. 147 / 97 /M (November 97) Obstacles problems with <b>measure</b> <b>data</b> 1 1...|$|R
5000|$|Collect current benefit <b>measure</b> <b>data</b> {{to have a}} {{quantitative}} basis for decision making ...|$|R
25|$|Most {{certifications}} <b>measure</b> <b>data</b> {{from the}} original version and the remix featuring Justin Bieber.|$|R
2500|$|... where β1 {{determines the}} initial {{velocity}} of the ball, β2 {{is proportional to the}} standard gravity, and ε'i is due to measurement errors. Linear regression can be used to estimate the values of β1 and β2 from the <b>measured</b> <b>data.</b> This model is non-linear in the time variable, but it is linear in the parameters β1 and β2; if we take regressors x'i=(x'i1, x'i2) =(t'i, t'i2), the model takes on the standard form ...|$|E
2500|$|It is {{important}} to remember that every experiment has associated errors. Random errors will affect the reproducibility and precision of the resulting structures. If the errors are systematic, the accuracy of the model will be affected. The precision indicates the degree of reproducibility of the measurement and is often expressed as the variance of the <b>measured</b> <b>data</b> set under the same conditions. The accuracy, however, indicates the degree to which a measurement approaches its [...] "true" [...] value.|$|E
2500|$|Typically this {{equation}} {{is used to}} find the average T and S values near a pumping well, from drawdown data collected during an aquifer test. [...] This is a simple form of inverse modeling, since the result (s) is measured in the well, r, t, and Q are observed, and values of T and S which best reproduce the <b>measured</b> <b>data</b> are put into the equation until a best fit between the observed data and the analytic solution is found.|$|E
40|$|In {{order to}} show {{existence}} of solutions for linear elliptic problems with <b>measure</b> <b>data,</b> a first classical method, due to Stampacchia, {{is to use a}} duality argument (and a regularity result for elliptic problems). Another classical method is to pass to the limit on approximate solutions obtained with regular data (converging towards the <b>measure</b> <b>data).</b> A third method is presented. It consists to pass to the limit on approximate solutions obtained with numerical schemes such that Finite Element schemes or Finite Volume schemes. This method also works for convection-diffusion problems which lead to non coercive elliptic problems with <b>measure</b> <b>data.</b> Thanks to a uniqueness result, the convergence of the approximate solutions as the mesh size vanishes is also achieved...|$|R
40|$|The device (10) has {{a safety}} device (40) forming {{encrypted}} test statement (VHC) by using keys (PS) {{of a key}} pair with <b>measuring</b> <b>data</b> (M). The test data is encrypted by a complementary key of the key pair outside the measuring device, so that authenticity of the <b>measuring</b> <b>data</b> is verified. The safety device forms a measurement value-individual encrypted test statement with the measurement value by using the keys. The safety device forms a hash code as a test code, which uses a hash algorithm, based on the <b>measuring</b> <b>data,</b> and encrypts the hash code with the keys. An independent claim is also included for a method for producing and outputting measurement data...|$|R
30|$|When {{performance}} <b>measure</b> <b>data</b> {{is missing}} from Wind, we hand-collect the data from firms’ performance-vested stock option plans.|$|R
2500|$|An {{alternative}} to a direct measurement, is considering a mathematical model that resembles formation of a real-world fractal object. In this case, a validation can also be done by comparing other than fractal properties implied by the model, with <b>measured</b> <b>data.</b> In colloidal physics, systems composed of particles with various fractal dimensions arise. To describe these systems, it is convenient [...] to speak about a distribution of fractal dimensions, and eventually, a time evolution of the latter: {{a process that is}} driven by a complex interplay between aggregation and coalescence.|$|E
2500|$|The {{challenge}} {{posed by}} MEG {{is to determine}} the location of electric activity within the brain from the induced magnetic fields outside the head. Problems such as this, where model parameters (the location of the activity) have to be estimated from <b>measured</b> <b>data</b> (the SQUID signals) are referred to as inverse problems (in contrast to forward problems where the model parameters (e.g. source location) are known and the data (e.g. the field at a given distance) is to be estimated.) The primary difficulty is that the inverse problem does not have a unique solution (i.e., there are infinite possible [...] "correct" [...] answers), and the problem of defining the [...] "best" [...] solution is itself the subject of intensive research. Possible solutions can be derived using models involving prior knowledge of brain activity.|$|E
2500|$|... {{are now the}} {{preferred}} method of reconstruction. [...] These algorithms compute {{an estimate of the}} likely distribution of annihilation events that led to the <b>measured</b> <b>data,</b> based on statistical principles. [...] The advantage is a better noise profile and resistance to the streak artifacts common with FBP, but the disadvantage is higher computer resource requirements. A further advantage of statistical image reconstruction techniques is that the physical effects that would need to be pre-corrected for when using an analytical reconstruction algorithm, such as scattered photons, random coincidences, attenuation and detector dead-time, can be incorporated into the likelihood model being used in the reconstruction, allowing for additional noise reduction. Iterative reconstruction has also been shown to result in improvements in the resolution of the reconstructed images, since more sophisticated models of the scanner Physics can be incorporated into the likelihood model than those used by analytical reconstruction methods, allowing for improved quantification of the radioactivity distribution.|$|E
3000|$|... [...]. This {{establishes}} {{a connection with}} another class of solutions to <b>measure</b> <b>data</b> problem. These are called [...]...|$|R
40|$|Abstract. In {{order to}} show {{existence}} of solutions for linear elliptic problems with <b>measure</b> <b>data,</b> a first classical method, due to Stampacchia, {{is to use a}} duality argument (and a regularity result for elliptic problems). Another classical method is to pass to the limit on approximate solutions obtained with regular data (converging towards the <b>measure</b> <b>data).</b> A third method is presented. It consists to pass to the limit on approximate solutions obtained with numerical schemes such that Finite Element schemes or Finite Volume schemes. This method also works for convection-diffusion problems which lead to non coercive elliptic problems with <b>measure</b> <b>data.</b> Thanks to a uniqueness result, the convergence of the approximate solutions as the mesh size vanishes is also achieved...|$|R
40|$|In {{this paper}} we study the {{continuous}} dependence {{with respect to}} obstacles for obstacle problems with <b>measure</b> <b>data.</b> This is deeply investigated introducing a suitable type of convergence, which gives stability under very general hypotheses. Moreover stability with respect to H 1 and uniform convergent obstacles is proved. Ref. S. I. S. S. A. 142 / 99 /M (December 1999) Stability results for solutions of obstacle problems with <b>measure</b> <b>data</b> 1 1...|$|R
2500|$|The {{mass balance}} of Hintereisferner and Kesselwandferner glaciers in Austria have been {{continuously}} monitored since 1952 and 1965 respectively. [...] Having been continuously measured for 55 years, Hintereisferner {{has one of}} the longest periods of continuous study of any glacier in the world, based on <b>measured</b> <b>data</b> and a consistent method of evaluation. Currently this measurement network comprises about 10 snow pits and about 50 ablation stakes distributed across the glacier. [...] In terms of the cumulative specific balances, Hintereisferner experienced a net loss of mass between 1952 and 1964, followed by a period of recovery to 1968. Hintereisferner reached an intermittent minimum in 1976, briefly recovered in 1977 and 1978 and has continuously lost mass in the 30 years since then. [...] Total mass loss has been 26 m since 1952 Sonnblickkees Glacier has been measured since 1957 and the glacier has lost 12 m of mass, an average annual loss of −0.23 m per year.|$|E
2500|$|The Venetia Burney Student Dust Counter (VBSDC), {{built by}} {{students}} at the University of Colorado Boulder, is operating continuously to make dust measurements. It consists of a detector panel, about , mounted on the anti-solar face of the spacecraft (the ram direction), and an electronics box within the spacecraft. The detector contains fourteen polyvinylidene difluoride (PVDF) panels, twelve science and two reference, which generate voltage when impacted. Effective collecting area is [...] No dust counter has operated past the orbit of Uranus; models of dust in the outer Solar System, especially the Kuiper belt, are speculative. The VBSDC is always turned on measuring the masses of the interplanetary and interstellar dust particles (in the range of nano- and picograms) as they collide with the PVDF panels mounted on the New Horizons spacecraft. The <b>measured</b> <b>data</b> is expected to greatly contribute {{to the understanding of}} the dust spectra of the Solar System. The dust spectra can then be compared with those from observations of other stars, giving new clues as to where Earth-like planets can be found in the universe. The dust counter is named for Venetia Burney, who first suggested the name [...] "Pluto" [...] at the age of 11. A thirteen-minute short film about the VBSDC garnered an Emmy Award for student achievement in 2006.|$|E
50|$|The Model Calibration {{option is}} based on a process where <b>measured</b> <b>data</b> from a real device is used to tune parameterssuch that the {{simulation}} results are in good agreement with the <b>measured</b> <b>data.</b>|$|E
25|$|Two {{examples}} are hertz (Hz), {{which is used}} to measure the clock rates of electronic components, and bit/s, used to <b>measure</b> <b>data</b> transmission speed.|$|R
40|$|We state pointwise {{estimate}} for the positive subsolutions associated to a p-homogeneous form and nonnegative Radon <b>measures</b> <b>data.</b> As a by-product we establish an oscillation’s {{estimate for}} the solutions relative to Kato <b>measures</b> <b>data.</b> 1. Introduction. The {{necessary part of}} the Wiener criterion for the regularity of boundary points {{in the case of}} nonlinear elliptic problems has been proved in a recent paper by Malỳ, [15] [16], using an estimate on positive subsolutions of the problem. The estimate has been generalize...|$|R
5000|$|... "Computational {{epistemology}} is {{an interdisciplinary}} field that concerns {{itself with the}} relationships and constraints between reality, <b>measure,</b> <b>data,</b> information, knowledge, and wisdom" [...] (Rugai, 2013) ...|$|R
50|$|Besides {{the modest}} {{measurement}} effort, {{the processing of}} the <b>measured</b> <b>data</b> can also be done efficiently:It is possible {{to carry out the}} fitting of a physical density matrix on the <b>measured</b> <b>data</b> even for large systems.|$|E
50|$|Statistical {{databases}} typically contain parameter {{data and}} the <b>measured</b> <b>data</b> for these parameters. For example, parameter data consists of the different values for varying conditions in an experiment (e.g., temperature, time). The <b>measured</b> <b>data</b> (or variables) are the measurements taken in the experiment under these varying conditions.|$|E
50|$|Differences in {{real-world}} <b>measured</b> <b>data</b> {{from the}} true values come about from by multiple factors affecting the measurement.|$|E
3000|$|Several {{variants}} {{of the basic}} linear regression model of (2) are widely used in various areas of science. One such variant is the mixed-effect model. These models include additional random-effect terms and are appropriate in representing clustered, and therefore, dependent data arising when data are collected over time on the same entities; that is, these repeated <b>measures</b> <b>data</b> are generated by observing a number of entities repeatedly under differing experimental conditions, where the entities are assumed to constitute a random sample from a population of interest. Longitudinal data constitute a common type of repeated <b>measures</b> <b>data,</b> where the observations are ordered by time or position in space. In general, longitudinal data {{can be defined as}} repeated <b>measures</b> <b>data</b> where the observations within entities could not have been randomly assigned to the levels of a [...] "treatment" [...] of interest (usually time or position in space); hence, serial correlation results.|$|R
5000|$|Bauer, D.J., Gottfredson, N.C., Dean, D., & Zucker, R.A. (2013). Analyzing {{repeated}} <b>measures</b> <b>data</b> {{on individuals}} nested within groups: accounting for dynamic group effects. Psychological Methods, 18, 1-14[...]|$|R
40|$|Abstract: The {{development}} of control systems and automatic storing of <b>measuring</b> <b>data</b> permits a better supervision of a heat-and-power station. Traditional procedures of calculating the energy indices cannot check the obtained <b>measuring</b> <b>data.</b> The present paper {{deals with the}} application of the least squares adjustment method in order to improve the reliability of energy measurements of a heat-and-power unit. A mathematical model of a heat-and-power unit equipped with an extraction-condensing steam turbine and fluidizedbed boiler has been presented. The set of mathematical model equations is built up basing on physical laws and existing <b>measuring</b> <b>data.</b> In the case of energy problems they are mainly balance equations of substance and energy. The formulated mathematical model consists of substance and energy balances of the turbine, boiler, main and regenerative heat exchangers. The verified results of measurements and calculations were used to determine the energy indices of the operation of the heat and power unit. Key words: heat-and-power unit, least squares adjustment method, energy measurements...|$|R
