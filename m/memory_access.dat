4510|5229|Public
5|$|Computer {{architectures}} {{in which}} each element of main memory can be accessed with equal latency and bandwidth are known as uniform <b>memory</b> <b>access</b> (UMA) systems. Typically, {{that can be achieved}} only by a shared memory system, in which the memory is not physically distributed. A system that does not have this property is known as a non-uniform <b>memory</b> <b>access</b> (NUMA) architecture. Distributed memory systems have non-uniform <b>memory</b> <b>access.</b>|$|E
5|$|Super Mario RPG: Legend of the Seven Stars {{is one of}} {{only seven}} SNES games {{released}} outside Japan to use the Nintendo SA-1 chip. Compared with standard SNES games, the additional microprocessor allows higher clock speeds; faster access to the random-access memory (RAM); greater memory mapping capabilities, data storage, and compression; new direct <b>memory</b> <b>access</b> (DMA) modes, such as bitmap to bit plane transfer; and built-in CIC lockout for piracy protection and regional marketing control.|$|E
5|$|These {{and other}} {{differences}} reflect the differing design {{goals of the}} two buses: USB was designed for simplicity and low cost, while FireWire was designed for high performance, particularly in time-sensitive applications such as audio and video. Although similar in theoretical maximum transfer rate, FireWire400 is faster than USB2.0 high-bandwidth in real-use, especially in high-bandwidth use such as external hard drives. The newer FireWire800 standard is {{twice as fast as}} FireWire400 and faster than USB2.0 high-bandwidth both theoretically and practically. However, FireWire's speed advantages rely on low-level techniques such as direct <b>memory</b> <b>access</b> (DMA), which in turn have created opportunities for security exploits such as the DMA attack.|$|E
50|$|Most modern CPUs reorder <b>memory</b> <b>accesses</b> {{to improve}} {{execution}} efficiency (see memory ordering for types of reordering allowed). Such processors invariably give {{some way to}} force ordering in a stream of <b>memory</b> <b>accesses,</b> typically through a memory barrier instruction. Implementation of Peterson's and related algorithms on processors which reorder <b>memory</b> <b>accesses</b> generally requires use of such operations to work correctly to keep sequential operations from happening in an incorrect order. Note that reordering of <b>memory</b> <b>accesses</b> can happen even on processors that don't reorder instructions (such as the PowerPC processor in the Xbox 360).|$|R
50|$|These prefetches are {{non-blocking}} memory operations, i.e. these <b>memory</b> <b>accesses</b> do {{not interfere}} with actual <b>memory</b> <b>accesses.</b> They do not change {{the state of the}} processor or cause page faults.|$|R
40|$|Shared memory {{applications}} {{running on}} NUMA machines {{suffer from the}} overhead caused by excessive remote <b>memory</b> <b>accesses</b> since references to remote memories normally exhibit a significant higher latency. Common approaches for reducing remote <b>memory</b> <b>accesses</b> are either compiler-based optimizations or OS-supporting data and/or computation migrations. This pape...|$|R
25|$|Although {{this meant}} fewer clock cycles per instruction, {{compared}} to the Z80 for instance, the latter's higher resolution state machine allowed clock frequencies 3-5 times as high without demanding faster memory chips, which was often the limiting factor. This is because the Z80 combines two full (but short) clock cycles into a relatively long <b>memory</b> <b>access</b> period {{compared to the}} clock, while the more asynchronous 6809 instead has relatively short <b>memory</b> <b>access</b> times: depending on version and speed grade, approximately 60% of a single clock cycle was typically available for <b>memory</b> <b>access</b> in a 6809 (see data sheets).|$|E
25|$|A set of AArch64 {{load and}} store {{instructions}} {{that can provide}} <b>memory</b> <b>access</b> order that is limited to configurable address regions.|$|E
25|$|The {{official}} Acorn RISC Machine project {{started in}} October 1983. They chose VLSI Technology as the silicon partner, {{as they were}} a source of ROMs and custom chips for Acorn. Wilson and Furber led the design. They implemented it with a similar efficiency ethos as the 6502. A key design goal was achieving low-latency input/output (interrupt) handling like the 6502. The 6502's <b>memory</b> <b>access</b> architecture had let developers produce fast machines without costly direct <b>memory</b> <b>access</b> (DMA) hardware.|$|E
5000|$|Stride <b>memory</b> <b>accesses</b> of high-{{dimensional}} data: High {{dimensional data}} {{are stored in}} non-continuous <b>memory</b> locations. <b>Accesses</b> along high dimensions are thus strided and uncoalesced, wasting available memory bandwidth.|$|R
40|$|Abstractâ€”H. 264 /AVC video {{compression}} standard uses multiple reference frame motion estimation (MRF-ME) {{to enhance the}} coding performance. The required frame <b>memory</b> <b>accesses</b> and the computational cost of the MRF-ME, however, are very high. This paper proposes an approach which exploits the characteristic of a constant luminance macroblock (L-MB) to avoid unnecessary frame <b>memory</b> <b>accesses</b> and MRF-ME computation. Simulation {{results show that the}} proposed scheme can reduce about 43 % of the frame <b>memory</b> <b>accesses</b> and 33 % of the MRF-ME computation for low motion video sequences without any PSNR degradation and bit-rate increase. I...|$|R
40|$|A major {{challenge}} of Transactional memory implementations {{is dealing with}} <b>memory</b> <b>accesses</b> that occur outside of transactions. In previous work we showed how to specify transactional memory in terms of admissible interchanges of transaction operations, and gave proof rules for showing that an implementation satisfies its specification. However, we did not capture non-transactional <b>memory</b> <b>accesses.</b> In this work we show how to extend our previous model to handle non-transactional accesses. We apply our proof rules using a PVS-based theorem prover and produce a machine checkable, deductive proof for the correctness of implementations of transactional memory systems that handle non-transactional <b>memory</b> <b>accesses...</b>|$|R
25|$|Includes {{enhanced}} {{support for}} Non-Uniform <b>Memory</b> <b>Access</b> (NUMA) and systems with large memory pages. Windows Vista also exposes APIs for accessing the NUMA features.|$|E
25|$|The {{relatively}} small number of processors in early systems, allowed them to easily use a shared memory architecture, which allows processors to access a common pool of memory. In the early days a common approach was the use of uniform <b>memory</b> <b>access</b> (UMA), in which access time to a memory location was similar between processors. The use of non-uniform <b>memory</b> <b>access</b> (NUMA) allowed a processor to access its own local memory faster than other memory locations, while cache-only memory architectures (COMA) allowed for the local memory of each processor to be used as cache, thus requiring coordination as memory values changed.|$|E
25|$|An {{efficient}} and {{simple way to}} provide hardware support of capabilities is to delegate the MMU the responsibility of checking access-rights for every <b>memory</b> <b>access,</b> a mechanism called capability-based addressing. Most commercial computer architectures lack such MMU support for capabilities.|$|E
50|$|It is {{noticeable}} that correctness is {{not affected}} if <b>memory</b> <b>accesses</b> following the unlock issue before the unlock complete or <b>memory</b> <b>accesses</b> {{prior to a}} lock issue after the lock acquisition. However, the code in critical section can not be issued prior to the lock acquisition is complete because mutual exclusion may not be guaranteed.|$|R
5000|$|Minimize host <b>memory</b> <b>accesses,</b> fully {{eliminating}} {{them when}} USB devices are idle ...|$|R
5000|$|RISC-V {{supports}} {{computers that}} share memory between multiple CPUs and threads. RISC-V's standard memory consistency model is release consistency. That is, loads and stores may generally be reordered, but some loads are [...] "acquire" [...] operations which must precede later <b>memory</b> <b>accesses,</b> and some stores are [...] "release" [...] operations which must follow earlier <b>memory</b> <b>accesses.</b>|$|R
25|$|Channel {{controllers}} {{are making}} a comeback {{in the form of}} bus mastering peripheral devices, such as PCI direct <b>memory</b> <b>access</b> (DMA) devices. The rationale for these devices is the same as for the original channel controllers, namely off-loading transfer, interrupts, and context switching from the main CPU.|$|E
25|$|QuickTime 7.5.5 {{and earlier}} {{are known to}} have a list of {{significant}} vulnerabilities that allow a remote attacker to execute arbitrary code or cause a denial of service (out-of-bounds <b>memory</b> <b>access</b> and application crash) on a targeted system. The list includes six types of buffer overflow, data conversion, signed vs unsigned integer mismatch, and uninitialized memory pointer.|$|E
25|$|Typed lambda calculi are {{foundational}} programming {{languages and}} are the base of typed functional programming languages such as ML and Haskell and, more indirectly, typed imperative programming languages. Typed lambda calculi {{play an important role}} in the design of type systems for programming languages; here typability usually captures desirable properties of the program, e.g. the program will not cause a <b>memory</b> <b>access</b> violation.|$|E
40|$|The growing processor/memory {{performance}} gap causes {{the performance of}} many codes to be limited by <b>memory</b> <b>accesses.</b> If known to exist in an application, strided <b>memory</b> <b>accesses</b> forming streams can be targeted by optimizations such as prefetching, relocation, remapping, and vector loads. Undetected, {{they can be a}} significant source of memory stalls in loops. Existing stream-detection mechanisms either require special hardware, which may not gather statistics for subsequent analysis, or are limited to compile-time detection of array accesses in loops. Formally, little treatment has been accorded to the subject; the concept of locality fails to capture the existence of streams in a program's <b>memory</b> <b>accesses...</b>|$|R
5000|$|Distributed refresh - refresh cycles are {{performed}} at regular intervals, interspersed with <b>memory</b> <b>accesses.</b>|$|R
40|$|Wire-exposed, {{programmable}} microarchitectures including Trips [11], Smart Memories [8], and Raw [13] {{offer an}} opportunity to schedule instruction execution and data movement explicitly. This paper proposes stream algorithms, which, along with a decoupled systolic architecture, provide an excellent match for the physical and technological constraints of single-chip tiled architectures. Stream algorithms enable programmed systolic computations for different problem sizes, without incurring the cost of <b>memory</b> <b>accesses.</b> To that end, we decouple <b>memory</b> <b>accesses</b> from computation and move the <b>memory</b> <b>accesses</b> off the critical path. By structuring computations in systolic phases, and deferring <b>memory</b> <b>accesses</b> to dedicated <b>memory</b> processors, stream algorithms can solve many regular problems with varying sizes on a constant-sized tiled array. Contrary to common sense, the compute efficiency of stream algorithms increases as we {{increase the number of}} processing elements. In particular, we show that the compute efficiency of stream algorithms can approach 100 % asymptotically, that is for large numbers of processors and appropriate problem size. ...|$|R
25|$|Though the {{predicate}} {{takes up}} {{four of the}} 32bits in an instruction code, and thus cuts down significantly on the encoding bits available for displacements in <b>memory</b> <b>access</b> instructions, it avoids branch instructions when generating code for small if statements. Apart from eliminating the branch instructions themselves, this preserves the fetch/decode/execute pipeline {{at the cost of}} only one cycle per skipped instruction.|$|E
25|$|When I/O {{transfer}} is complete or an error is detected, the controller communicates with the CPU through the channel using an interrupt. Since the channel has {{direct access to}} the main memory, it is also often referred to as DMA controller (where DMA stands for direct <b>memory</b> <b>access),</b> although that term is looser in definition and is often applied to non-programmable devices as well.|$|E
25|$|The <b>memory</b> <b>access</b> {{pattern of}} AI {{calculations}} differs from graphics: a more predictable but deeper dataflow, benefiting {{more from the}} ability to keep more temporary variables on-chip (e.g. in scratchpad memory rather than caches); GPUs by contrast devote silicon to efficiently dealing with highly non-linear gather-scatter addressing between texture maps and frame-buffers, and texture filtering, as is needed for their primary role in 3D rendering.|$|E
40|$|Many {{parallel}} architectures {{support a}} memory model where some <b>memory</b> <b>accesses</b> are local, and thus inexpensive, while other <b>memory</b> <b>accesses</b> are remote, and potentially quite expensive. In {{order to achieve}} good parallel performance, it is often necessary {{to reduce the number}} of remote <b>memory</b> <b>accesses.</b> This can be done by the programmer, the compiler, or a combination of both. The overall goal is to minimize the work required by the programmer, and have the compiler automate the process as much as possible. This paper reports on compiler techniques for decreasing the number of remote <b>memory</b> <b>accesses</b> using locality analysis for a parallel dialect of C called EARTHC. The locality analysis uses an algorithm inspired by type inference algorithms for fast points-to analysis. The algorithm estimates when an indirect reference via a pointer can be safely assumed to be a local access. The locality inference algorithm is also used to guide the automatic specialization of functions in order to ta [...] ...|$|R
30|$|The load of {{a bucket}} can {{potentially}} {{be larger than}} one without increasing <b>memory</b> <b>accesses.</b>|$|R
40|$|Power {{savings that}} can be {{achieved}} by data-reuse decisions targeting at a custom memory hierarchy for multimedia applications executing on embedded cores are examined in this paper. Exploiting the temporal locality of <b>memory</b> <b>accesses</b> in data-intensive applications a set of data-reuse transformations on a typical motion estimation algorithm is determined. The aim is to reduce data related power consumption by moving background <b>memory</b> <b>accesses</b> to smaller foreground memories, which are less power costly. The impact of these transformations on power, performance and area is evaluated both for application specific circuits and general purpose processors. The number of data and instruction <b>memory</b> <b>accesses</b> is analytically calculated, enabling a fast exploration of the design space by varying algorithmic parameters...|$|R
25|$|SCO {{has claimed}} {{a number of}} {{instances}} of IBM Linux code as breaches of contract. These examples include code related to Symmetric multiprocessing (SMP), Journaled File System (JFS), Read-copy-update (RCU) and Non-Uniform <b>Memory</b> <b>Access</b> (NUMA). This code is questionably in the Linux kernel, and may have been added by IBM through the normal kernel submission process. This code was developed and copyrighted by IBM. IBM added features to AIX and Dynix.|$|E
25|$|With {{the aid of}} the {{firmware}} and device drivers, the kernel {{provides the}} most basic level of control over all of the computer's hardware devices. It manages <b>memory</b> <b>access</b> for programs in the RAM, it determines which programs get access to which hardware resources, it sets up or resets the CPU's operating states for optimal operation at all times, and it organizes the data for long-term non-volatile storage with file systems on such media as disks, tapes, flash memory, etc.|$|E
25|$|EIDE was an {{unofficial}} update (by Western Digital) {{to the original}} IDE standard, with the key improvement being the use of direct <b>memory</b> <b>access</b> (DMA) to transfer data between the disk and the computer without {{the involvement of the}} CPU, an improvement later adopted by the official ATA standards. By directly transferring data between memory and disk, DMA eliminates the need for the CPU to copy byte per byte, therefore allowing it to process other tasks while the data transfer occurs.|$|E
40|$|We {{study the}} {{fundamental}} problem of approximate nearest neighbor search in d-dimensional Hamming space { 0, 1 }^d. We study {{the complexity of the}} problem in the famous cell-probe model, a classic model for data structures. We consider algorithms in the cell-probe model with limited adaptivity, where the algorithm makes k rounds of parallel accesses to the data structure for a given k. For any k> 1, we give a simple randomized algorithm solving the approximate nearest neighbor search using k rounds of parallel <b>memory</b> <b>accesses,</b> with O(k(d) ^ 1 /k) accesses in total. We also give a more sophisticated randomized algorithm using O(k+(1 /k d) ^O(1 /k)) <b>memory</b> <b>accesses</b> in k rounds for large enough k. Both algorithms use data structures of size polynomial in n, the number of points in the database. For the lower bound, we prove an Î©(1 /k(d) ^ 1 /k) lower bound for the total number of <b>memory</b> <b>accesses</b> required by any randomized algorithm solving the approximate nearest neighbor search within k< d/ 2 d rounds of parallel <b>memory</b> <b>accesses</b> on any data structures of polynomial size. This lower bound shows that our first algorithm is asymptotically optimal for any constant round k. And our second algorithm approaches the asymptotically optimal tradeoff between rounds and <b>memory</b> <b>accesses,</b> in a sense that the lower bound of <b>memory</b> <b>accesses</b> for any k_ 1 rounds can be matched by the algorithm within k_ 2 =O(k_ 1) rounds. In the extreme, for some large enough k=Î˜(d/ d), our second algorithm matches the Î˜(d/ d) tight bound for fully adaptive algorithms for approximate nearest neighbor search due to Chakrabarti and Regev...|$|R
30|$|ZM-SPECK [25] {{can remove}} state-maps and dynamic lists in the {{existing}} SPIHT algorithm using linear indexing property of wavelet tree and merged refinement technique. This method can reduce both computational complexity and <b>memory</b> <b>accesses</b> related to dynamic lists. However, it still {{has a lot of}} <b>memory</b> <b>accesses</b> due to its bit-plane coding and recursive set-partition technique. The compression ratio of ZM-SPECK is slightly higher than that of the existing SPIHT algorithm.|$|R
40|$|Abstract. The IA- 64 {{architecture}} is {{significantly different from}} previous IA- 32, and it offers to the operating system and applications a set of features that can improve efficiency in code execution by reducing <b>memory</b> <b>accesses.</b> A quick overview on some IA- 64 new features is presented, {{with a focus on}} the registers set and on the register stack engine, which are the main features that mostly reduce the <b>memory</b> <b>accesses...</b>|$|R
