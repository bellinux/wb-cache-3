2|29|Public
40|$|We are {{currently}} working on a project that provides an objective and automated method for classification and reconstruction of archaeological pottery. The purpose of classification is to get a systematic view of the material found, to recognize types, and to add labels for additional information as a measure of quantity. Traditional archaeological classification is based on the so-called profile of the object, which is the cross-section of the fragment {{in the direction of the}} rotational axis of symmetry. This two-dimensional plot holds all the information needed to perform archaeological research. Therefore, our method of classifying and reconstructing archaeological pottery is also based on this profile. This paper shows how a two-dimensional profile can be formulated as functions (<b>multiple</b> <b>splines)</b> which segment the complete profile into relevant sub-parts used for classification...|$|E
40|$|Probabilistic {{reasoning}} typically {{suffers from}} the explosive amount of information it must maintain. There {{are a variety of}} methods available for curbing this explosion. However, in doing so, it is important to avoid oversimplifying the given domain through injudicious use of assumptions such as independence. Multiple splining is an approach for compressing and approximating the probabilistic information. Instead of positing additional independence conditions, it attempts to identify patterns in the information. While the data explosion is multiplicative in nature, O(n 1 n 2 ΔΔ k), <b>multiple</b> <b>splines</b> reduces it to an additive one, O(n 1 + n 2 + ΔΔΔ + n k). We consider how these splines can be found and used. Since splines exploit patterns in the data, we can also use them to help in filling in missing data. As it turns out, our splining method is quite general and may be applied to other domains besides probabilistic reasoning which c [...] ...|$|E
40|$|Prism: <b>Multiple</b> <b>spline</b> {{regression}} with regularization, dimensionality reduction, {{and feature}} selection Prism uses {{a combination of}} statistical methods to conduct spline-based multiple regression. Prism conducts this regression using regularization, dimensionality reduction, and feature selection, {{through a combination of}} smoothing spline regression, PCA, and RVR/LASSO...|$|R
30|$|Before {{using the}} {{classical}} EMD {{one needs to}} consider whether <b>multiple</b> <b>spline</b> interpolation does not take too much time. In particular this applies to analysis of long signals or acquisition systems with high sampling frequency or {{a large number of}} channels. Proposed algorithm, Sliding Window EMD, can speed up computation about 10 times. Examples of using SWEMD presented in this article show that it has similar quality as classical EMD - boundary effects in typical windowing are nearly completely reduced in SWEMD. Proposed algorithm is suitable also in real time analysis of data.|$|R
40|$|AbstractThe problem {{considered}} {{in this paper}} is best Lp approximation with multiple constraints for 1 ⩽ p < ∞. Characterizations of best Lp approximations from <b>multiple</b> n-convex <b>splines</b> and functions are established {{and the relationship between}} them is investigated. Applications to best monotone convex approximation are studied...|$|R
50|$|The {{software}} uses {{a proprietary}} spline mesh technology to perform modeling and animation, {{and it is}} different in this sense from polygon mesh or NURBS-based programs. The system used is called patch-based modeling. It uses <b>multiple</b> intersecting <b>splines</b> to create surfaces, called patches. Patches present an efficiency in that one patch can describe a complex curved surface that would require many facets to approximate in flat polygons.|$|R
30|$|This study uses {{a variety}} of methods to conduct the mass {{appraisal}} of the English rental market. One is a quassi Poisson generalised linear model (GLM) to account for the skewed distribution of the rental price and its possible over-dispersion. Then, a number of machine learning algorithms are used, primarily tree based (gradient boost (GB) [48], Cubist [49]) or specialist non-linear models (support vector machines (SVM) [50], <b>multiple</b> adaptive <b>splines</b> (MARS) [51]).|$|R
40|$|Problem statement: Research on Smooth Support Vector Machine (SSVM) is {{an active}} field in data mining. Many {{researchers}} developed the method to improve accuracy of the result. This study proposed a new SSVM for classification problems. It is called <b>Multiple</b> Knot <b>Spline</b> SSVM (MKS-SSVM). To {{evaluate the effectiveness of}} our method, we carried out an experiment on Pima Indian diabetes dataset. The accuracy of previous results of this data still under 80 % so far. Approach: First, theoretical of MKS-SSVM was presented. Then, application of MKS-SSVM and comparison with SSVM in diabetes disease diagnosis were given. Results: Compared to the SSVM, the proposed MKS-SSVM showed better performance in classifying diabetes disease diagnosis with accuracy 93. 2 %. Conclusion: The results of this study showed that the MKS-SSVM was effective to detect diabetes disease diagnosis and this is very promising compared to the previously reported results...|$|R
40|$|Prediction of 305 -day first {{lactation}} {{milk yield}} in cows with selected regression models In the present study, the prognostic values of <b>multiple</b> and <b>spline</b> regression models were tested for 305 -day lactation milk yield of cows. The predictors were: HF genes proportion in cow’s genotype; average milk yield for 305 -day lactation from 4 first milkings of all cows in a barn, {{in which a}} cow was used in a given year; month of calving; and average daily milk yield from first four test-day milkings. Models were developed basing on 628 first lactations of BW cows with average 71 % HF genes proportion. Subsequently, the predictive values of the models examined were verified {{on the grounds of}} next 105 first lactations. Prognostic differences of the models examined were determined, finding the prognosis obtained with the spline regression more accurate (smaller prediction error, higher coefficient of correlation for prognosis and real values in the model containing information from first three test-day milkings). These models are easy to construct and may be useful in practical estimation of cow lactation yields. They may be used for predicting the actual lactation yield in order t...|$|R
40|$|The {{introduction}} of video objects (VOs) {{is one of}} the innovations of MPEG- 4. The-plane of a VO defines its shape at a given instance in time and hence determines the boundary of its texture. In packet-based networks, shape, motion, and texture are subject to loss. While there has been considerable attention paid to the concealment of texture and motion errors, little has been done in the field of shape error concealment. In this paper, we propose a post-processing shape error-concealment technique that uses geometric boundary information of the received-plane. Second-order Hermite splines are used to model the received boundary in the neighboring blocks, while third order Hermite splines are used to model the missing boundary. The velocities of these splines are matched at the boundary point closest to the missing block. There exists the possibility of <b>multiple</b> concealing <b>splines</b> per group of lost boundary parts. Therefore, we draw every concealment spline combination that does not self-intersect and keep all possible results until the end. At the end, we select the concealment solution that results in one closed boundary. Experimental results demonstrating the performance of the proposed method and comparisons with prior proposed methods are presented...|$|R
40|$|AbstractOur {{study of}} perfect spline {{approximation}} reveals: (i) it {{is closely related}} to ΣΔ modulation used in one-bit quantization of bandlimited signals. In fact, they share the same recursive formulae, although in different contexts; (ii) the best rate of approximation by perfect splines of order r with equidistant knots of mesh size h is hr− 1. This rate is optimal in the sense that a function can be approximated with a better rate if and only if it is a polynomial of degree <r. The uniqueness of best approximation is studied, too. Along the way, we also give a result on an extremal problem, that is, among all perfect splines with integer knots on R, (<b>multiples</b> of) Euler <b>splines</b> have the smallest possible norms...|$|R
40|$|In {{quantitative}} {{estimates from}} radioimmunoassay, {{one of four}} variance of u {{is important to the}} estimation, it is irrelevant types of response curves is usually used: a freehand curve, a to the choice of response function. The <b>multiple</b> binding-site <b>spline</b> function, an equation based upon mass-action consid- equation (1, 2; symbols slightly altered to aid systematic erations, or a logistic equation. This paper comments briefly presentation) is: on the subjectivity and labor of the first and on the overpara-metnzationof the second. It is chieflyconcernedto compare (2) the single binding-siteequation with a simple or modified _______ logistic. Whatever the theoretical merits of the binding-site approach (these are not under discussion), estimation of parametersis difficult. The paper showsthat undermany but R = (T- U) /U. (3) not all circumstances a four- or five-parameter logistic will fit data at least as well over a wide range of doses. This is Summation is over s binding-sites, T is the total count, N particularly so when both the binding-site concentration and the nonspecific binding term, z 0 the concentration of labelled the equilibrium constant are small, antigen, and Q 1 and K, the concentration and the equilibri-um constant for binding site i. The parameter T is easily an...|$|R
40|$|Abstract. Our {{study of}} perfect spline {{approximation}} reveals: (i) it {{is closely related}} to Σ ∆ modulation used in one-bit quantization of bandlimited signals. In fact, they share the same recursive formulae, although in different contexts; (ii) the best rate of approximation by perfect splines of order r with equidistant knots of mesh size h is h r− 1. This rate is optimal in the sense that a function can be approximated with a better rate if and only if it is a polynomial of degree < r. The uniqueness of best approximation is studied, too. Along the way, we also give a result on an extremal problem, that is, among all perfect splines with integer knots on R, (<b>multiples</b> of) Euler <b>splines</b> have the smallest possible norms...|$|R
40|$|Seismological images {{represent}} {{maps of the}} earth's structure. Apparent bandwidth {{limitation of}} seismic data prevents successful estimation of transition sharpness by the multiscale wavelet transform. We discuss the application of two recently developed techniques for (non-linear) singularity analysis designed for bandwidth limited data, such as imaged seismic reflectivity. The first method is a generalization of Mallat's modulus maxima approach to a method capable of estimating coarse-grained local scaling/sharpness/Holder regularity of edges/transitions from data residing at essentially one single scale. The method {{is based on a}} non-linear criterion predicting the (dis) appearance of local maxima {{as a function of the}} data's fractional integrations/di#erentiations. The second method is an extension of an atomic decomposition technique based on the greedy Matching Pursuit Algorithm. Instead of the ordinary Spline Wavelet Packet Basis, our method uses <b>multiple</b> Fractional <b>Spline</b> Wavelet Packet Bases, especially designed for seismic reflectivity data. The first method excels in pinpointing the location of the singularities (the stratigraphy). The second method improves the singularity characterization by providing information on the transition's location, magnitude, scale, order and direction (anti-/causal/symmetric). Moreover, the atomic decomposition entails data compression, denoising and deconvolution. The output of both methods produces a map of the earth's singularity structure. These maps can be overlayed with seismic data, thus providing us with a means to more precisely characterize the seismic reflectivity's litho-stratigraphical information content...|$|R
40|$|A cash-in-advance, {{endogenous}} growth, economy defines financial development {{within a}} banking sector production {{function as the}} degree of scale economies for normalized capital and labor. Less financially developed economies have smaller such returns to scale, and can be credit constrained endogenously by a steeply sloping marginal cost of credit supply. The degree of scale economies uniquely determines the marginal cost curvature and the unit cost of financial intermedition, which is {{expressed in terms of}} an interest differential. The interest differential result allows for calibration of the finance production function using industry data. A hypothesis of how financial development interacts with inflation and growth is tested, using fixed effects panel estimation with endogeneity tests, dynamic panel estimation, and an extended use of <b>multiple</b> inflation rate <b>splines</b> in estimation of the growth rateInflation, financial development, growth, panel data...|$|R
40|$|Accurate {{software}} metrics-based maintainability prediction can {{not only}} enable developers to better identify the determinants of software quality and thus help them improve design or coding, it can also provide managers with useful information to help them plan the use of valuable resources. In this paper, we employ a novel exploratory modeling technique, <b>multiple</b> adaptive regression <b>splines</b> (MARS), to build software maintainability prediction models using the metric data collected from two different object-oriented systems. The prediction accuracy of the MARS models are evaluated and compared using multivariate linear regression models, artificial neural network models, regression tree models, and support vector models. The results suggest that for one system MARS can predict maintainability more accurately than the other four typical modeling techniques, and that for the other system MARS is as accurate as the best modeling technique. Department of Computin...|$|R
40|$|With Emphasis on greater connectivity, {{there is}} a need of unpaved road to achieve economy. In this study a large scale {{laboratory}} plate load test was conducted on a circular footing resting on with and without geogrid reinforced bed. Sand and granular materials are used as subgrade and subbase layer. The experiments were conducted for both static and dynamic loading. Test result reveals that with the addition of geogrid the settlement has reduced up to 40 - 60 % as compared to unreinforced section. The experimental static results have validated with numerical modelling using both Finite element method and Finite difference method (Plaxis 2 D and FLAC 2 D) and dynamic results have validated by using empirically by Giroud and Han‟s equation. Based on the experimental and numerical studies, predictive models are proposed using two recently developed artificial intelligent techniques, Genetic Programming (GP) and <b>Multiple</b> adoptive Regression <b>Spline</b> (MARS) ...|$|R
40|$|Inspections {{have been}} shown to be an {{effective}} means of detecting defects early on in the software development life cycle. However, they are not always successful or beneficial as they are affected by a number of technical and managerial factors. One important aspect is to understand what are the factors that affect inspection effectiveness (the rate of detected defects) in a given environment, based on project data. In this paper we look at management factors such as the effort assigned, the inspection rate, and so forth. We collected data on a number of analysis and code inspections, and performed a multivariate statistical analysis. Because the functional form of effectiveness models is a priori unknown, we use a novel exploratory analysis technique: <b>Multiple</b> Adaptive Regression <b>Splines</b> (MARS). We compare the MARS model with more classical regression models and show how it can help understand the complex trends and interactions in the data, without requiring the analyst to rely on s [...] ...|$|R
40|$|This {{paper is}} argues that {{increasing}} the number of private labels products in grocery stores is not always an efficient strategy for a retailer wishing to enhance her unit sales. Although private labels can be effective for providing higher margins or developing customer loyalty program, their increased number of Stock Keeping Unit (SKU) assumes item deletions for other types of brand - especially national brands. For this study, we choose to focus on the composition of the whole brand assortment rather than the competitive advantage of this or that brand type. This paper proposes an econometric approach to this assortment reshaping issue. Using data drawn from a retailer panel (provided by IRI Inc), we construct a <b>Multiple</b> Adaptive Regression <b>Spline</b> (MARS) model to investigate the relation between both private label's and national brand's SKU and store-level unit sales. Results are given with respect to store size. We find that only in biggest stores higher levels of private labels leads to enhanced sales...|$|R
40|$|The role of {{functional}} limitations and self-reported chronic disease on CES-D- 8 depression scores {{was investigated in}} the Health and Retirement Study's 2008 data. The sample included 5835 respondent's {{ranging in age from}} 50 - 83. Multivariate adaptive regression splines (MARS) modeling was employed to identify <b>multiple</b> linear <b>splines.</b> Possible predictors included functional limitations (ADL, IADL), chronic diseases (back pain, stroke, arthritis, cancer, high blood pressure, heart disease, lung disease, & diabetes) and demographic measures (age, education, gender, being white, being Hispanic). The MARS modeling process resulted in selection of the following measures in the final model: ADL limitations, IADL limitations, back pain, stroke, education, being female, and being Hispanic. The results indicated that ADL limitations included two splines with a large increase in depression score from zero to one limitation, and a more shallow increase from one to three limitations. IADL limitations included two splines with a linear increase in depression score from zero to two limitations and a decrease in score from two to three limitations. Reporting back pain or stroke resulted in higher depression scores compared to absence of disease. Two splines were identified for education in which the highest depression scores were associated with zero to 7 years of education; there was a linear decline in depression from seven to 17 years of education. Females and Hispanics were associated with higher depression scores compared to males and non-Hispanics. Results emphasize the importance {{of functional}} status and selected chronic health conditions on depression scores and provide more detailed descriptions of change than traditional regression-based models...|$|R
40|$|International audienceWe {{investigate}} {{the relationship between}} landscape heterogeneity and the spatial distribution of small mammals in two areas of Western Sichuan, China. Given a large diversity of species trapped within {{a large number of}} habitats,we first classified small mammal assemblages and then modelled the habitat of each in the space of quantitative environmental descriptors. Our original two step “classify then model” procedure is appropriate for the frequently encountered study scenario: trapping data collected in remote areas with sampling guided by expert field knowledge. In the classification step, we defined assemblages by grouping sites of similar species composition and relative densities using an expert-class-merging procedure which reduced redundancy in the habitat factor used within a multinomial logistic regression predicting species trapping probabilities. Assemblages were thus defined as mixtures of small mammal frequency distributions in discrete groups of sampled sites. In the modelling step, assemblages' habitats and environments of the two sampled areas were discriminated in the space of remotely sensed environmental descriptors. First, we compared the discrimination of assemblage/study areas by linear and non-linear forms of discriminant analysis (linear discriminant analysis versus mixture discriminant analysis) and of multiple regression (generalized linear models versus <b>multiple</b> adaptive regression <b>splines).</b> The “best” predictive modelling technique was then used to quantify the contribution of each environmental variable in discriminations of assemblages and areas. Mixtures of Gaussians provided a more efficient model of assemblage coverage in environmental space than a single Gaussian cluster model. However, non-linearity in assemblage response to environmental gradients was consistently predicted with lower deviance and misclassification error by <b>multiple</b> adaptive regression <b>splines.</b> The two study areas were mainly discriminated along vegetation indices. However, although the normalized difference vegetation index (NDVI) could discriminate forested from non-forested habitats, its power to discriminate assemblages in Maerkang, where a greater diversity of forest habitat was observed, was seen to be limited, and in this case NDVI was outperformed by the enhanced vegetation index (EVI). Our analyses highlight previously unobserved differences between the environments and smallmammalcommunities oftwo fringe areas of the Tibetan plateauand suggests that a biogeographical approach is required to elucidate ecological processes in small mammal communities and to reduce extrapolation uncertainty in distribution mappin...|$|R
40|$|We {{evaluate}} 179 classifiers {{arising from}} 17 families (discriminant analysis, Bayesian, neural networks, support vector machines, decision trees, rule-based classifiers, boosting, bagging, stacking, random forests and other ensembles, generalized linear models, nearest-neighbors, partial least squares and principal component regression, logistic and multino-mial regression, <b>multiple</b> adaptive regression <b>splines</b> and other methods), implemented in Weka, R (with {{and without the}} caret package), C and Matlab, including all the relevant classifiers available today. We use 121 data sets, which represent the whole UCI data base (excluding the large-scale problems) and other own real problems, {{in order to achieve}} significant conclusions about the classifier behavior, not dependent on the data set col-lection. The classifiers most likely to be the bests are the random forest (RF) versions, the best of which (implemented in R and accessed via caret) achieves 94. 1 % of the maximum accuracy overcoming 90 % in the 84. 3 % of the data sets. However, the dif-ference is not statistically significant with the second best, the SVM with Gaussian kernel implemented in C using LibSVM, which achieves 92. 3 % of the maximum accuracy. A fe...|$|R
40|$|Linking host spatial {{distributions}} {{to environmental}} variables can provide key information for understanding and predicting {{the transmission of}} a parasite in space. When {{a large number of}} potential intermediate hosts co-occur within a diversity of habitats, community level modelling helps to summarize such complex data set by defining groups of species/sites, i. e assemblages. We built a predictive model for the niches of small mammal assemblages including potential Echinococcus multilocularis intermediate hosts, in two areas of western China (Sichuan) by a three step modelling procedure. First, 8 assemblages were defined using a multinomial logistic model associated with a redundancy reduction procedure. Then, niches of these assemblages were modelled w. r. t. the environmental space of each sampled area using a <b>Multiple</b> Adaptative Regression <b>Spline.</b> Elevation and ETM band 6 (land surface temperature) were one of the main factors influencing assemblage distributions in both study areas. The importances of vegetation indices (NDVI and EVI) effects, which are corelated to the amount of vegetation, differed between the two locations. Finally, the model providing the lowest predictive classification error was chosen to map assemblage occurence probabilities beyond the sampled locations. We could thus discuss the predictive error component induced by extrapolation of model predictions on non trained locations...|$|R
40|$|Dynamic {{balance in}} human {{locomotion}} {{can be assessed}} through the local dynamic stability (LDS) method. Whereas gait LDS has been used successfully in many settings and applications, {{little is known about}} its sensitivity to individual characteristics of healthy adults. Therefore, we reanalyzed a large dataset of accelerometric data measured for 100 healthy adults from 20 to 70 years of age performing 10 min. treadmill walking. We sought to assess {{the extent to which the}} variations of age, body mass and height, sex, and preferred walking speed (PWS) could influence gait LDS. The random forest (RF) and <b>multiple</b> adaptive regression <b>splines</b> (MARS) algorithms were selected for their good bias-variance tradeoff and their capabilities to handle nonlinear associations. First, through variable importance measure (VIM), we used RF to evaluate which individual characteristics had the highest influence on gait LDS. Second, we used MARS to detect potential interactions among individual characteristics that may influence LDS. The VIM and MARS results indicated that PWS and age correlated with LDS, whereas no associations were found for sex, body height, and body mass. Further, the MARS model detected an age by PWS interaction: on one hand, at high PWS, gait stability is constant across age while, on the other hand, at low PWS, gait instability increases substantially with age. We conclude that it is advisable to consider the participants' age as well as their PWS to avoid potential biases in evaluating dynamic balance through LDS. Comment: This is the author's version of a manuscript published in the Journal of Biomechanic...|$|R
40|$|This release {{includes}} {{a complete and}} long awaited overhaul of the HoloViews documentation and website, with a new gallery, getting-started section, and logo. In the process, we have also improved and made small fixes {{to all of the}} major new functionality that appeared in 1. 7. 0 but was not properly documented until now. We want to thank all our old and new contributors for providing feedback, bug reports, and pull requests. Major features and improvements: Completely overhauled the documentation and website (PR # 1384, # 1473, # 1476, # 1473, # 1537, # 1585, # 1628, # 1636) Replaced dependency on bkcharts with new Bokeh bar plot (# 1416) and bokeh BoxWhisker plot (# 1604) Added support for drawing the Arrow annotation in bokeh (# 1608) Added periodic method to DynamicMap to schedule recurring events (# 1429) Cleaned up the API for deploying to bokeh server (# 1444, # 1469, # 1486) Validation of backend-specific options (# 1465) Added utilities and entry points to convert notebooks to scripts including magics (# 1491) Added support for rendering to PNG in bokeh backend (# 1493) Made matplotlib and bokeh styling more consistent and dropped custom matplotlib rc file (# 1518) Added iloc and ndloc method to allow integer-based indexing on tabular and gridded datasets (# 1435) Added option to restore case-sensitive completion order by setting hv. extension. case_sensitive_completion=True in python or via holoviews. rc file (# 1613) Other new features and improvements: Optimized datashading of NdOverlay (# 1430) Expose last DynamicMap args and kwargs on Callable (# 1453) Allow colormapping Contours Element (# 1499) Add support for fixed ticks with labels in bokeh backend (# 1503) Added a clim parameter to datashade controlling the color range (# 1508) Add support for wrapping xarray DataArrays containing dask arrays (# 1512) Added support for aggregating to target Image dimensions in datashader aggregate operation (# 1513) Added top-level hv. extension and hv. renderer utilities (# 1517) Added support for <b>Splines</b> defining <b>multiple</b> cubic <b>splines</b> in bokeh (# 1529) Add support for redim. label to quickly define dimension labels (# 1541) Add BoundsX and BoundsY streams (# 1554) Added support for adjoining empty plots (# 1561) Handle zero-values correctly when using logz colormapping option in matplotlib (# 1576) Define a number of Cycle and Palette defaults across backends (# 1605) Many other small improvements and fixes (# 1399, 1400, 1405, 1412, # 1413, 1418, 1439, 1442, 1443, 1467, 1485, 1505, 1493, 1509, 1524, # 1543, 1547, 1560, 1603) Changes affecting backwards compatibility: Renamed ElementOperation to Operation (# 1421) Removed stack_area operation in favor of Area. stack classmethod (# 1515) Removed all mpld 3 support (# 1516) Added opts method on all types, replacing the now-deprecated __call__ syntax to set options (# 1589) Styling changes for both matplotlib and bokeh, which can be reverted for a notebook with the config option of hv. extension. For instance, hv. extension('bokeh', config=dict(style_ 17 =True)) (# 1518...|$|R
40|$|Wildfires {{can cause}} {{significant}} damage {{to an area}} by destroying forested and agricultural areas, homes, businesses, and leading to the potential loss of life. Climate change may further increase the frequency of wildfires. Thus, developing a quick, simple, and accurate method for identifying key drivers that cause wildfires and modeling and predicting their occurrence becomes very important and urgent. Various modeling methods have been developed and applied for this purpose. The objective {{of this study was}} to identify key drivers and search for an appropriate method for modeling and predicting natural wildfire occurrence for the United States. In this thesis, various vegetation, topographic and climate variables were examined and key drivers were identified based on their spatial distributions and using their correlations with natural wildfire occurrence. Five models including General Linearized Models (GLM) with Binomial and Poisson distribution, MaxEnt, Random Forests, Artificial Neural Networks, and <b>Multiple</b> Adaptive Regression <b>Splines,</b> were compared to predict natural wildfire occurring for seven different climate regions across the United States. The comparisons were conducted using three datasets including LANDFIRE consisting of thirteen variables including characteristics of vegetation, topography and disturbance, BIOCLIM containing climate variables such as temperature and precipitation, and composite data that combine the most important variables from LANDFIRE and BIOCLIM after the multicollinearity test of the variables done using variance inflation factor (VIF). This results of this study showed that niche modeling techniques such as MaxEnt, GLM with logistic regression (LR), and binomial distribution were an appropriate choice for modeling natural wildfire occurrence. MaxEnt provided highly accurate predictions of natural wildfire occurrence for most of seven different climate regions across the United States. This implied that MaxEnt offered a powerful solution for modeling natural wildfire occurrence for complex and highly specialized systems. This study also showed that although MaxEnt and GLM were quite similar, both models produced very different spatial distributions of probability for natural wildfire occurrence in some regions. Moreover, it was found that natural wildfire occurrence in the western regions was more influenced by precipitation and drought conditions while in the eastern regions the natural wildfire occurrence was more affected by extreme temperature...|$|R
40|$|Introduction: As the {{statistical}} time series {{are in short}} period and the meteorological station are not distributed well in mountainous area determining of climatic criteria are complex. Therefore, in recent years interpolation methods for establishment of continuous climatic data have been considered. Continuous daily maximum temperature data are a key factor for climate-crop modeling which is fundamental for water resources management, drought, and optimal use from climatic potentials of different regions. The main objective {{of this study is}} to evaluate different interpolation methods for estimation of regional maximum temperature in the Isfahan province. Materials and Methods: Isfahan province has about 937, 105 square kilometers, between 30 degree and 43 minutes to 34 degree and 27 minutes North latitude equator line and 49 degree and 36 minutes to 55 degree and 31 minutes east longitude Greenwich. It is located in the center of Iran and it's western part extend to eastern footage of the Zagros mountain range. It should be mentioned that elevation range of meteorological stations are between 845 to 2490 in the study area. This study was done using daily maximum temperature data of 1992 and 2007 years of synoptic and climatology stations of I. R. of Iran meteorological organization (IRIMO). In order to interpolate temperature data, two years including 1992 and 2007 with different number of meteorological stations have been selected the temperature data of thirty meteorological stations (17 synoptic and 13 climatologically stations) for 1992 year and fifty four meteorological stations (31 synoptic and 23 climatologically stations) for 2007 year were used from Isfahan province and neighboring provinces. In order to regionalize the point data of daily maximum temperature, the interpolation methods, including inverse distance weighted (IDW), Kriging, Co-Kriging, Kriging-Regression, <b>multiple</b> regression and <b>Spline</b> were used. Therefore, for this allocated data (24 days for each year and 2 days for each month) were used for different interpolation methods. Using difference measures viz. Root Mean Square Error (RMSE), Mean Bias Error (MBE), Mean Absolute Error (MAE) and Correlation Coefficient (r), the performance and accuracy of each model were tested to select the best method. Results and Discussion: The assessment of normalizing condition of data was done using Kolmogrov-Smirnov test at ninety five percent (95...|$|R
40|$|Interfaces play a {{dominant}} role in governing {{the response of}} many biological systems and they pose many challenges to traditional finite element. For sharp-interface model, traditional finite element methods necessitate the finite element mesh to align with surfaces of discontinuities. Diffuse-interface model replaces the sharp interface with continuous variations of an order parameter resulting in significant computational effort. To overcome these difficulties, we focus on developing a computationally efficient spline-based finite element method for interface problems. A key challenge while employing B-spline basis functions in finite-element methods is the robust imposition of Dirichlet boundary conditions. We begin by examining weak enforcement of such conditions for B-spline basis functions, with application to both second- and fourth-order problems based on Nitsche's approach. The use of spline-based finite elements is further examined along with a Nitsche technique for enforcing constraints on an embedded interface. We show that how the choice of weights and stabilization parameters in the Nitsche consistency terms has a great influence on the accuracy and robustness of the method. In the presence of curved interface, to obtain optimal rates of convergence we employ a hierarchical local refinement approach to improve the geometrical representation of interface. In <b>multiple</b> dimensions, a <b>spline</b> basis is obtained as a tensor product of the one-dimensional basis. This necessitates a rectangular grid that cannot be refined locally in regions of embedded interfaces. To address this issue, we develop an adaptive spline-based finite element method that employs hierarchical refinement and coarsening techniques. The process of refinement and coarsening guarantees linear independence and remains the regularity of the basis functions. We further propose an efficient data transfer algorithm during both refinement and coarsening which yields to accurate results. The adaptive approach is applied to vesicle modeling which allows three-dimensional simulation to proceed efficiently. In this work, we employ a continuum approach to model the evolution of microdomains {{on the surface of}} Giant Unilamellar Vesicles. The chemical energy is described by a Cahn-Hilliard type density functional that characterizes the line energy between domains of different species. The generalized Canham-Helfrich-Evans model provides a description of the mechanical energy of the vesicle membrane. This coupled model is cast in a diffuse-interface form using the phase-field framework. The effect of coupling is seen through several numerical examples of domain formation coupled to vesicle shape changes. Dissertatio...|$|R
40|$|International audienceAlthough Mexico's central arid and {{semiarid}} lands face numerous conservation problems, {{they have}} largely been neglected from conservation, despite their apparent biological importance. We studied the semiarid Llanos de Ojuelos, where the original plant communities of grasslands, shrublands, and patches of oaks have been modified strongly by cattle grazing and for cultivation, affecting animal populations and communities, {{as well as the}} landscape-level ecological dynamics. Very little about the populations of wild animals in the area and the processes underlying their distribution and dynamics is known. We focused on whether satellite information was useful to explain rodent distributions and abundances in this area with a complex landscape of diffuse plant communities. In the spring of 2008 we inventoried the rodents at 74 locations in the area, through the use of Sherman live-traps (40 traps during two nights, per site; total= 5920 night-traps). We covered the major perennial vegetation types. The existence of a complex landscape of intergrading grasslands, Opuntia comunities and shrublands (dominated by Acacia, Mimosa, Dodonaea, and Quercus) of different types and composition creates rich habitat mosaic that promotes a rich rodent fauna. We captured 20 species of rodents (458 individuals), while richness was estimated at 21 - 25 species. Reithrodontomys fulvescens, Chaetodipus nelsoni, Dipodomys phillipsii, Peromyscus melanophrys, Peromyscus gratus, and Peromyscus difficilis were very common at our sites, while Chaetodipus pennicillatus, Liomys irroratus, Perognathus flavus, Dipodomys merriami, Reithrodontomys megalotis, Peromyscus boylii, Neotoma leucodon, Peromyscus eremicus, Dipodomys ordii, and Peromyscus manicultatus were common, and Neotoma goldmani, Chaetodipus hispidus, Onychomys arenarius, and Baiomys taylori were rare. To explore the value of satellite information to understand and predict the rodents' distributions, we derived reflectance data from all bands of a Landsat 5 TM Image (5 March 2008) for each trapline's midpoint. We will compare each rodent species' abundance with this data {{as well as with the}} normalized difference vegetation index (NDVI) and tasseled cap transformations (TCs: brightness, greenness, and wetness). We will link rodent presence/absences to reflectance values through linear and non-linear discriminant analysis, as well as through general linear models and <b>multiple</b> adaptive regression <b>splines.</b> Satellite images classified following the best models will then be interpreted according to field expertise. As end results we expect to have a better understanding on the rodent species' ecology, an adequate image classification procedure, and potential distribution maps for the rodent species in the study area...|$|R
40|$|ADVISOR is {{a machine}} {{learning}} architecture for constructing intelligent tutoring systems (ITS). ADVISOR {{is able to}} automate some of the reasoning about how the student will probably perform, {{and all of the}} reasoning about which teaching action should be made in a particular context. The benefit of this approach is that it works by observing students using an ITS. By observing students, ADVISOR constructs a model of how a student will respond to a particular teaching action in a given situation. With this model, ADVISOR is able to experiment and determine a policy for presenting teaching actions that tries to achieve a customizable teaching goal. ^ We experimented with a variety of approaches for constructing a model of how students behave, and we found that sophisticated approaches such as <b>Multiple</b> Adaptive Regression <b>Splines</b> (MARS) are only slightly better than linear regression. ^ We also examined a variety of ways ADVISOR can reason with the model of student performance and determine how to teach. We used including temporal difference learning, heuristic search, and the use of rollouts. If little is known a prior about the teaching goal, rollouts are a strong choice as they require little prior knowledge and are robust. Given prior knowledge of the teaching goal, some type of temporal difference learning is a good option since this requires less computation time than using heuristic search or rollouts. ^ ADVISOR was tested {{in the context of the}} AnimalWatch tutor for grade school arithmetic. However, the architecture is generic and applicable to a variety of ITS. As part of AnimalWatch, ADVISOR was tested in a grade school and achieved the specified teaching goal of minimizing the amount of time per problem. ^ The ADVISOR architecture is also useful for evaluating what components of the tutoring system are responsible for performance, and what components of ADVISOR are constraining performance. In this way, engineering effort can be directed to where it is most profitable. ^ Thus, the ADVISOR architecture has the potential to benefit a wide range of ITS (and possibly other adaptive systems) in several ways. In addition to determining which components limit performance, our hope is ADVISOR 2 ̆ 7 s ability to automate the construction of the knowledge of how to teach will result in a decreased cost to construct ITS. ...|$|R
40|$|The {{discrete}} {{wavelet transform}} using adaptive wavelet bases were investigated in classification, regression and experimental design applications for spectroscopic data. Adaptive wavelets {{have been used}} previously in near infrared spectroscopy fields for classification and regression; however methods to select the parameters required in the adaptive wavelet algorithm have been largely influenced by human interaction. Methods are developed within this thesis to select parameters for adaptive wavelets along with investigating the hypothesis of using multiple wavelet bases to improve the predictability of classification and regression models. Use of the adaptive discrete wavelet transform (ADWT) is illustrated using a repeated measures experiment. Near infrared (NIR) spectra of wine grape homogenates, from the Australian viticulture industry, underwent feature extraction via the ADWT and then modelled using penalised discriminate analysis, random forests and <b>multiple</b> adaptive regression <b>splines.</b> The correct classification rates of all three methods were substantially improved when the ADWT was applied. Scores from the ADWT penalised discriminate analysis (PDA) were analysed via {{multivariate analysis of variance}} (MANOVA) where it is reported that all main and interaction effects were significant. A bi-plot of the PDA scores illustrated the ease of which the ADWT extracted useful features from the spectra which were pertinent to the experimental design. A method of ADWT parameter selection was derived using the Bayes' information criteria (BIC) and demonstrated in an unsupervised classification problem. Using the BIC to select ADWT parameters removed the need to for human interaction to select good, optimised, adaptive wavelets. This outcome highlighted an advantage over standard wavelet types, which gave similar unsupervised classification performances, where adaptive wavelets only need to span a relatively small set of parameters to give good models while a prohibitively large number of standard wavelet types need to be trialled. Investigation of using multiple wavelet transforms to improve model performance - a new hypothesis in the field of chemometrics – was demonstrated in supervised classification and regression applications. In the classification example, SELDI-TOF mass spectra from a cancer study were analysed by pre-processing the spectra with a variety of standard wavelet types prior to variable elimination via a t-static and random forest approach. The retained variables were subsequently model using Treeboost where the specificity and sensitivity of the modelling process was improved by using multiple standard wavelet types compared to model using only one wavelet type alone. Models derived from wavelet processing were superior to models without preprocessing. Further evidence supporting the multiple wavelet feature extraction hypothesis was gained in the regression application. Using a publically available and well documented NIR dataset, a Bayes Metropolis regression was modified to incorporate multiple wavelet transforms by using constrained stacking rather than Bayes model averaging as the model ensemble method. Multiple adaptive wavelets and multiple standard wavelets were trialled with the multiple adaptive wavelet approach resulting in a superior predictive regression model when compared to: all single standard wavelet models, single adaptive wavelet models, multiple wavelet standard wavelet models and models cited previously in literature for the same data set. Methods for using adaptive wavelets, both multiple and singular wavelet bases, are outlined in this thesis with the general conclusion that the modelling process of NIR data (or juxta-positional data) can be substantially improved by the use of these wavelet transforms...|$|R

