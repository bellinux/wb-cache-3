89|0|Public
50|$|In 2006, the NICTA group {{commenced}} a from-scratch {{design of}} a third-generation microkernel, called seL4, {{with the aim of}} providing a basis for highly secure and reliable systems, suitable for satisfying security requirements such as those of Common Criteria and beyond. From the beginning, development aimed for formal verification of the kernel. To ease meeting the sometimes conflicting requirements of performance and verification, the team used a <b>middle-out</b> software process starting from an executable specification written in Haskell.seL4 uses capability-based access control to enable formal reasoning about object accessibility.|$|E
40|$|We develop two {{applications}} of <b>middle-out</b> reasoning in inductive proofs: Logic program synthesis and {{the selection of}} induction schemes. <b>Middle-out</b> reasoning as part of proof planning was first suggested by Bundy et al [Bundy et al 90 a]. <b>Middle-out</b> reasoning uses variables to represent unknown terms and formulae. Unification instantiates the variables in the subsequent planning, while proof planning provides the necessary search control. <b>Middle-out</b> reasoning is used for synthesis by planning the verification of an unknown logic program: The program body is represented with a meta-variable. The planning results both in an instantiation of the program body and {{a plan for the}} verification of that program. If the plan executes successfully, the synthesized program is partially correct and complete. <b>Middle-out</b> reasoning is also used to select induction schemes. Finding an appropriate induction scheme during synthesis is difficult, because the recursion of the program, which is un [...] ...|$|E
40|$|We develop two {{applications}} of <b>middle-out</b> reasoning in inductive proofs: the logic program synthesis and {{the selection of}} induction schemes. <b>Middle-out</b> reasoning uses variables to represent unknown terms and formulae. Unification instantiates the variables in the subsequent planning, while proof planning provides the necessary search control. <b>Middle-out</b> reasoning is used for synthesis by planning the verification of an unknown logic program: the program body is represented with a meta-variable. The planning results both in an instantiation of the program body and {{a plan for the}} verification of that program. If the plan executes successfully, the synthesized program is partially correct and complete. <b>Middle-out</b> reasoning is also used to select induction schemes. In <b>middle-out</b> induction, we set up a schematic step case by representing the constructors that are applied to induction variables with meta-variables. Once the step case is complete, the instantiated variables correspond to an induction appropriate to the recursion of the program. We have implemented these techniques {{as an extension of the}} proof planning system CLAM, called Periwinkle, and synthesized a variety of programs fully automatically...|$|E
40|$|Abstract. Lemma {{speculation}} {{has long}} been considered a promising technique to automate the discovery of missing lemmas for inductive proofs. This technique involves speculating a schematic lemma that becomes incrementally instantiated by unification as the proof continues. This synthesis process is known as <b>middle-out</b> reasoning. We have extended lemma speculation, and more generally <b>middle-out</b> reasoning, to dynamic rippling for higher-order domains, implemented it in the Isa-Planner system and improved the technique to ensure termination. This provides a practical basis for exploring the applications of <b>middle-out</b> reasoning. We demonstrate such an application by performing a critical and comparative evaluation of lemma speculation. This shows that when lemma speculation is applied it often finds the needed lemmas to complete the proof, {{but it is not}} applicable as often as initially expected. In comparison, we show that simpler proof methods combined with theory formation methods offer an effective alternative. ...|$|E
30|$|The {{results also}} cite other {{concepts}} associated with leadership {{which are not}} necessarily theories in their own right: top-down, bottom-up, <b>middle-out,</b> formal, informal, structured, supportive, passive (the latter coming {{under the umbrella of}} FRLT).|$|E
40|$|This paper {{describes}} several {{methods that}} are currently available in the hts package, for forecasting hierarchical time series. The methods included are: top-down, buttom-up, <b>middle-out</b> and optimal combination. The implementation of these methods is illustrated by using regional infant mortality counts in Australia...|$|E
40|$|Because of its scale, the GEOSS {{design has}} the {{challenges}} that come with desigining Ultra-Large-Scale (ULS) systems. Traditional methods (top down and bottom up) are not adequate to handle these challenges. We describe the method of <b>middle-out</b> design: a suggested best practice for designing the GEOSS...|$|E
40|$|A {{description}} is given of {{a technique called}} <b>middle-out</b> reasoning for the control of search in automatic theorem proving. The authors illustrate it use {{in the domain of}} automatic program synthesis. Programs can be synthesised from proofs that their logical specifications are satisfiable. Each proof step is also a program construction step. Unfortunately, a naive use of this technique requires a human or computer to produce proof steps which provide the essential structure of the desired program. It is hard to see the justification for these steps at the time that they are made; the reason for them emerges only later in the proof. Such proof steps are often call eureka steps. <b>Middle-out</b> reasoning enables these eureka steps to be produced, automatically, as a side effect of non-eureka steps...|$|E
40|$|We {{present a}} succinct account of dynamic rippling, a {{technique}} used {{to guide the}} automation of inductive proofs. This simplifies termination proofs for rippling and hence facilitates extending the technique in ways that preserve termination. We illustrate this by extending rippling with a terminating version of <b>middle-out</b> reasoning for lemma speculation. This supports automatic speculation of schematic lemmas which are incrementally instantiated by unification as the rippling proof progresses. <b>Middle-out</b> reasoning and lemma speculation have been implemented in higher-order logic and evaluated on typical libraries of formalised mathematics. This reveals that, when applied, the technique often finds the needed lemmas to complete the proof, {{but it is not}} as frequently applicable as initially expected. In comparison, we show that theory formation methods, combined with simpler proof methods, offer an effective alternative...|$|E
40|$|Introduction Proof {{planning}} [4] is {{an approach}} to theorem proving which encodes heuristics for constructing mathematical proofs in a meta-theory of methods. The Clam system, developed at Edinburgh [3], {{has been used for}} several years to develop proof planning, in particular proof plans for induction. It has become clear that many of the theorem-proving tasks that we would like to perform are naturally higher-order. For example, an important technique called <b>middle-out</b> reasoning [6] uses meta-variables to stand for some unknown objects in a proof, to be instantiated as the proof proceeds. Domains such as the synthesis and verification of software and hardware systems, and techniques such as proof critics [7], benefit greatly from such <b>middle-out</b> reasoning. Since in these domains the meta-variables often become instantiated with terms of function type, reasoning with them is naturally higher-order, and higher-order unification is...|$|E
40|$|As governments {{commit to}} {{national}} electronic health record (EHR) systems, {{there is increasing}} international interest in identifying effective implementation strategies. We draw on Coiera's typology of national programmes - ‘top-down’, ‘bottom-up’ and ‘middle-out’ - to review EHR implementation strategies in three exemplar countries: England, the USA and Australia. In comparing and contrasting three approaches, we show how different healthcare systems, national policy contexts and anticipated benefits have shaped initial strategies. We reflect on progress and likely developments {{in the face of}} continually changing circumstances. Our review shows that irrespective of the initial strategy, over time there is likely to be convergence on the negotiated, devolved <b>middle-out</b> approach, which aims to balance the interests and responsibilities of local healthcare constituencies and national government to achieve national connectivity. We conclude that, accepting the current lack of empirical evidence, the flexibility offered by the <b>middle-out</b> approach may make this the best initial national strategy...|$|E
30|$|While {{modularity}} {{represents the}} horizontal {{organization of the}} cell, living systems also present vertical organization (Cheng and Hu 2010). Molecules, cells, tissues, organs, organisms, populations and ecosystems reflect the hierarchical organization of life. A modeling formalism that supports hierarchical models and different levels of abstraction will cope with models that connect vertical organization layers using top-down, bottom-up or <b>middle-out</b> approaches (Noble 2002).|$|E
30|$|The <b>middle-out</b> {{approach}} of Uschold et al. (Uschold 1996): {{the most frequently}} recurring concepts in the domain are determined first, and then both generalized and specialized; in other words they are merged into an intermediate approach between the two previous ones. Following the logic of <b>middle-out,</b> some authors have established a general procedure based on 4 fixed stages, applied {{to the field of}} knowledge: (1) Obtain the central concepts from the terms of a domain; (2) Organize concepts hierarchically, finding connections with respect to the central concepts of the domain through the basic relationship “is a”; (3) Study the types of relationships associated with the general concepts; (4) Organize the results into a visualization system ontology to allow a partial sharing of the process logic from the user (logical trees). Lately, new approaches have been developed substantially derived from the three described above, which make use of resources already defined to identify the concepts.|$|E
40|$|We {{describe}} a novel technique for the automatic synthesis of tail-recursive programs. The technique is {{to specify the}} required program using the standard equations and then synthesise the tail-recursive program using the proofs as programs technique. This requires the specification to be proved realisable in a constructive logic. Restrictions on {{the form of the}} proof ensure that the synthesised program is tail-recursive. The automatic search for a synthesis proof is controlled by proof plans, which are descriptions of the high-level structure of proofs of this kind. We have extended the known proof plans for inductive proofs by adding a new form of generalisation and by making greater use of <b>middle-out</b> reasoning. In <b>middle-out</b> reasoning we postpone decisions {{in the early part of}} the proof by the use of meta-variables which are instantiated, by unification, during later parts of the proof. Higher order unification is required, since these meta-variables can represent higher o [...] ...|$|E
40|$|The Clam system, {{developed}} at Edinburgh [4], {{has been used}} for several years to develop proof planning, in particular proof plans for induction. An important technique called <b>middle-out</b> reasoning [6] uses meta-variables to stand for some unknown objects in a proof, to be instantiated as the proof proceeds. It has become clear that some domains such as the synthesis and verification of software and hardware systems, and techniques such as proof critics, benefit greatly from <b>middle-out</b> reasoning. Since in these domains the meta-variables often become instantiated with terms of function type, the reasoning is naturally higher-order, and higher-order unification is a vital tool. While some ability to perform higherorder reasoning had been added to Clam in an ad hoc way, it has become clear {{that it is time to}} move Clam from first-order to higher-order logic. This system description outlines the Clam system for proof planning in higher-order logic. Clam is written in Prolog [1...|$|E
40|$|Top-down, bottom-up, <b>middle-out</b> and {{abiotic factors}} are usually viewed as main forces {{structuring}} biological communities, although {{assessment of their}} relative importance, in a single study, is rarely done. We quantified, using multivariate methods, associations between abiotic and biotic (top-down, bottom-up and <b>middle-out)</b> variables and infaunal population/community variation on intertidal mudflats in the Bay of Fundy, Canada, over two years. Our analysis indicated that spatial structural factors like site and plot accounted {{for most of the}} community and population variation. Although we observed a significant relationship between the community/populations and the biotic and abiotic variables, most were of minor importance relative to the structural factors. We suggest that community and population structure were relatively uncoupled from the structuring influences of biotic and abiotic factors in this system because of high concentrations of resources that sustain high densities of infauna and limit exploitative competition. Furthermore, we hypothesize that the infaunal community primarily reflects stochastic spatial events, namely a "first come, first served" process...|$|E
40|$|International audienceThis article {{presents}} a <b>middle-out</b> approach to build legal domain reference ontology for a Legal Knowledge Based System (LKBS). The proposed {{approach is a}} combination of top-down and bottom-up strategies. In particular, we propose to develop legal domain reference ontology, splitted into modules or fragments, based on merging two processes: Conceptual Modeling Process, by reusing foundational ontologies (top-down strategy) and Ontology Learning Process from textual resources (bottom-up strategy) ...|$|E
40|$|International audienceThe {{paper by}} Hannah et al. (this volume) invokes foodweb {{theory and the}} ideas of {{complexity}} theory to guide the construction of models of intermediate complexity, which sacrifice explicit process detail {{to increase the number}} of interacting components of the system and simulate the web of feedback loops. This approach has its merits, if best practice modelling guidelines are followed and the method is used well. However, if this is not the case then the fundamental weakness of the intermediate model approach is that it may end up producing models that are over general and therefore not useful. To make the most of the intermediate complexity approach it is essential to keep the tenets of the <b>middle-out</b> approach in mind. Under the <b>middle-out</b> approach computational models are constructed and tested at the levels where we have the most detailed information, building on our existing knowledge and data and coupling a hierarchy of models rather than being swamped in a morass of detail or missing key feedbacks through over-generality...|$|E
40|$|The {{pedagogical}} {{value of}} graphical representations and analyses (GR and GA) in economics education is examined in {{a framework of}} top-down and bottom-up processes of thinking. We argue, {{with the support of}} two illustrative examples, that they are useful {{to the extent that they}} provide bridges between economic theories and facts. We also note that over-reliance on GR and GA may lead to misconceptions on the students ’ part. Hence, the challenge for educators and students of economics is to connect GR and GA upwardly with theories and downwardly with the empirical world. Top-down, <b>middle-out,</b> and bottom-up processes: A cognitive perspective of teaching and learning economics Graphical representations (GR) and graphical analyses (GA) are often used by teachers of economics and authors of economics textbooks to help students understand economic theories, models and concepts. For example, GR and GA are commonly used to explain the concepts of quantity demanded, quantity supplied, and price equilibrium, as well as the law of demand, or the relationship between price 60 Top-down, <b>middle-out,</b> and bottom-up processes and quantity demanded. Most students find GR easier to understand than written explanations (MacDonald-Ross, 1977). Recently, Dickinson (2003), Lai, Chang and Ka...|$|E
40|$|Abstract. We {{describe}} version 2 of IsaPlanner, a proof planner for the Isabelle proof {{assistant and}} present the central design decisions and their motivations. The major advances are {{the support for}} a declarative presentation of the proof plans, reasoning with meta-variables to support <b>middle-out</b> reasoning, new proof critics for lemma speculation and case analysis, the ability to mix search strategies, and {{the inclusion of a}} higher-order version of rippling that can use best-first search. The result is a more flexible and powerful proof planner for exploring proof automation in Isabelle. ...|$|E
40|$|Motivation: After ten-year investigations, {{the folding}} {{mechanisms}} of beta-hairpins {{are still under}} debate. Experiments strongly support zip-out pathway, while most simulations prefer the hydrophobic col-lapse model (including <b>middle-out</b> and zip-in pathways). In this pa-per, we show that all pathways can occur during the folding of beta-hairpins but with different probabilities. The zip-out pathway is the most probable one. This is {{in agreement with the}} experimental re-sults. We came to our conclusions by thirty-eight 100 -ns room-temperature all-atom molecular dynamics simulations of the beta-hairpin trpzip 2. Our results may help to clarify the inconsistencies in the current pictures of β-hairpin folding mechanisms. ...|$|E
40|$|We {{describe}} novel computational {{techniques for}} constructing induction rules for deductive synthesis proofs. Deductive synthesis {{holds out the}} promise of automated construction of correct computer programs from specifications of their desired behaviour. Synthesis of programs with iteration or recursion requires inductive proof, but standard techniques {{for the construction of}} appropriate induction rules are restricted to recycling the recursive structure of the specifications. What is needed is induction rule construction techniques that can introduce novel recursive structures. We show that a combination of rippling and the use of meta-variables as a least-commitment device can provide such novelty. Key words: deductive synthesis, proof planning, induction, theorem proving, <b>middle-out</b> reasoning. ...|$|E
40|$|To {{complement}} the traditional linear and reductionism approaches, we are considering holism and the <b>middle-out</b> approach to better {{deal with the}} ever-growing complexity of modern command and control systems. In this article, four “complex conditions ” (called modalities) are proposed for studying complex systems (CxS) and to characterize their evolution toward superior behaviors. Modalities address multiple notions such as features, properties, complex mechanisms that can be linked together into “interaction diagrams”, which help understand complex cause-effect interrelationships in more holistic perspectives. The proposed modalities and approaches form {{the first version of}} a complexity framework (CxF). Its use is illustrated with an example involving the NATO C 2 Network Centric Operation taxonomy...|$|E
40|$|With the {{changing}} face {{of higher education}} comes a demand to include new technological tools. Universities need to build their capacity to respond to new technology-related challenges. The introduction of ePortfolios is a significant strategy in this response. A number of organizational change management models are used to analyze the incorporation of new technologies, such as ePortfolios, into university culture, including Kotter’s Model of Change, the LASO Model, and the <b>middle-out</b> approach. This article offers a case study of using a <b>middle-out</b> approach to technology adoption in the context of change management. It argues that such an approach provides links between university faculty values and upper institutional management decision-making that results in a positive and collegial transition to introducing ePortfolios. This study used a staged methodological process, based on faculty and professional staff feedback, literature in the field, benchmarking with similar universities, and external reports of best practices to develop functional criteria customized to the institution’s context, an analysis of available and appropriate ePortfolio software, and finally, recommendations to the institution’s decision-makers. The distinction is made throughout the article between faculty, who are staff members with teaching and research responsibilities, and professional staff, who provide a range of support to faculty, including teaching support and technical services. Where a particular sub-group is identified, they are named in terms of their primary function. Findings reflect the importance of the individual context and available resources of the institution when assessing new technology implementation and the value of the middle-agent role in facilitating a seamless shift towards change inclusive of both “top” and “bottom” stakeholder groups...|$|E
40|$|This paper {{presents}} a collaborative approach for designing, implementing and deploying situated urban HCI interventions. It draws on field studies that use HCI technologies for collecting feedback from citizens. Based on {{an analysis of}} these field studies and a discussion of top-down and bottom-up initiatives currently used in community engagement we propose that both decision makers and local communities {{should be involved in}} the city making process. We relate our approach, which we refer to as <b>middle-out</b> design, to other co-design and participatory design movements in HCI and conclude on a discussion on how our work can contribute to the discourse around urban HCI particularly for the purpose of community engagement to inform change...|$|E
40|$|Abstract—Consolidating {{currently}} fragmented {{health information}} systems in low and middle-income countries (LMIC) {{into a coherent}} national information system will increase operational efficiencies, improve decision-making and will lead to better health outcomes. However, engineering an enterprise information system of the scale and complexity of a national health information system (NHIS) pose unique and complex challenges in LMICs. In this paper, we review current approaches to NHIS development and discuss challenges faced by LMICs to develop their NHIS. Based on current LMIC systems we identify three stages of system evolution and propose that LMICs should follow an evolutionary, <b>middle-out</b> approach to NHIS development supported by appropriate architectural frameworks. Keywords—national health information systems, low and medium income countries, architectural frameworks, architectural approaches I...|$|E
40|$|The {{transition}} to a low-carbon society is imperative to climate change mitigation and requires cross-sectoral action at multiple levels. A growing literature emphasizes local action, but less is written about scaling up action at a county level. Combining three analytical perspectives – transition theory, strategic niche management and the <b>middle-out</b> – we examine the evolution and scaling up of local community-scale carbon action in Oxfordshire county. Our analysis is based on four local-level research projects. By identifying the roles and strategies of local actors {{in the development and}} scaling up of low carbon innovation and action, we conclude that local meso-level actors are crucial for catalyzing initial stages of county-level transitions, but limited in their capacity to scale up low-carbon innovation...|$|E
40|$|Some {{issues in}} {{designing}} computers for artificial intelligence (AI) processing are discussed. These issues {{are divided into}} three levels: the representation level, the control level, and the processor level. The representation level deals with the knowledge and methods used {{to solve the problem}} and the means to represent it. The control level is concerned with the detection of dependencies and parallelism in the algorithmic and program representations of the problem, and with the synchronization and sheduling of concurrent tasks. The processor level addresses the hardware and architectural components needed to evaluate the algorithmic and program representations. Solutions for the problems of each level are illustrated by a number of representative systems. Design decisions in existing projects on AI computers are classed into top-down, bottom-up, and <b>middle-out</b> approaches...|$|E
40|$|We {{present the}} design of a {{computer-aided}} environment, RMCase, to support the design and construction of hypermedia applications. The environment is based upon the Relationship Management methodology. MCase supports hypermedia design and development activities. Support for cognitive design processes is achieved through three fundamental premises that form the foundation of RMCase: (1) fluid feedback loops between the various methodological stages, (2) manipulation of objects at the instance level, and (3) lightweight prototyping. To achieve this, RMCase itself is designed as a hypermedia application, where hypertextual navigation implements feedback loops. Instance objects can be cloned and abstraction/instantiation mechanisms are envisioned to facilitate designers back and forth movements between the abstract and the concrete layers of an application. As a result, RMCase will support bottom-up, top-down and <b>middle-out</b> software development styles. ...|$|E
40|$|We {{propose a}} novel {{approach}} to automating the synthesis of logic programs: Logic programs are synthesized as a by-product of the planning of a verification proof. The approach is a two-level one: At the object level, we prove program verification conjectures in a sorted, first-order theory. The conjectures are of the form ∀a⃗r⃗g⃗s⃗. prog(a⃗r⃗g⃗s⃗) spec(a⃗r⃗g⃗s⃗). At the meta-level, we plan the object-level verification with an unspecified program definition. The definition is represented with a (second-order) meta-level variable, which becomes instantiated {{in the course of}} the planning. This technique is an application of the Clam proof planning system. Clam is currently powerful enough to plan verification proofs for given programs. We show that, if Clam's use of <b>middle-out</b> reasoning is extended, it will also be able to synthesize programs...|$|E
40|$|Introduction: To {{expand our}} ability to test current {{concepts}} about system and organ function within organisms in normal and disease states {{we need a new}} class of discrete, event-driven simulation models that achieve a higher level of biological realism across multiple scales, while being sufficiently flexible to represent different aspects of the biology. Here we provide the first description of such models, one that is focused on the rat liver. We use a <b>middle-out</b> design strategy that begins with primary parenchymal units. The models are sufficiently flexible to represent different aspects of hepatic biology, including heterogeneous microenvironments. Model components are designed to be easily joined and disconnected, and to be replaceable and reusable. The models function within a multitier, in silico apparatus designed to support iterative experimentation on models that will have extended life cycles...|$|E
40|$|Abstract. Rippling is a {{technique}} developed for inductive theorem proving which uses syntactic differences of terms to guide the proof search. Annotations (like colors) to terms are used to maintain this information. This technique has several advantages, e. g. it is highly goal oriented and involves little search. In this paper we give a general formalisation of coloring terms in a higher-order setting. We introduce a simply-typed lambda calculus with color annotations and present algorithms for unification, pre-unification and pattern unification. Our work is a formal basis {{to the implementation of}} rippling in a higher-order setting which is required e. g. in case of <b>middle-out</b> reasoning. Another application is in the construction of natural language semantics, where the color annotations rule out linguistically invalid readings that are possible using standard higher-order unification. ...|$|E
40|$|The paper defines {{weighted}} head transducers,finite-state {{machines that}} perform <b>middle-out</b> string transduction. These transducers are strictly more expressive than the special case of standard leftto-right finite-state transducers. Dependency transduction models are then defined as collections of weighted head transducers that are applied hierarchically. A dynamic programming search algorithm is described for finding the optimal transduction of an input string {{with respect to}} a dependency transduction model. A method for automatically training a dependency transduction model from a set of input-output example strings is presented. The method first searches for hierarchical alignments of the training examples guided by correlation statistics, and then constructs the transitions of head transducers that are consistent with these alignments. Experimental results are given for applying the training method to translation from English to Spanish and Japanese. 1...|$|E
40|$|We {{present the}} {{prototype}} of a computer-aided environment, RMCase, to support the design and construction of hypermedia applications. The environment {{is based upon the}} Relationship Management Methodology. RMCase supports hypermedia design and development activities. Support for cognitive design processes is achieved through three fundamental premises that form the foundation of RMCase: (1) fluid feedback loops between the various methodological stages, (2) manipulation of objets at the instance level, and (3) lightweight prototyping. To achieve this, RMCase itself is designed as a hypermedia application, where hypertextual navigation implements feedback loops. As a result, RMCase will support bottom-up, top-down and <b>middle-out</b> software development styles. 1 1. INTRODUCTION Hypermedia application design and development is a complex task that involves a variety of activities, at the storage, access and presentation levels. As a consequence, the constituencies participating in hypermed [...] ...|$|E
40|$|One of the {{prominent}} debates {{of the current}} era of increasingly networked economy and sociality relates {{to the concept of}} ‘connectedness’ (Price, 2013; Siemens, 2006). On the one hand, technologies extend, mediate, and reduce the need for physical spaces for social interaction; on the other hand, there has also been a resurgence in community hubs as existing and new forms of critical resources for diverse individuals and communities. This paper provides a comparative analysis of three creative community hubs in South East Queensland, each representing a case of bottom-up, <b>middle-out,</b> and top-down driven initiatives. By applying the transdisciplinary lens of Urban Informatics, it explores the juxtaposed values of physical space and digital technologies for connectedness within the context of creative communities. It then opens questions about how Human-Computer Interaction (HCI) might add to tactics of connection for meaningful and impactful community engagement...|$|E
40|$|Coloring terms (rippling) is a {{technique}} developed for inductive theorem proving which uses syntactic dierences of terms to guide the proof search. Annotations (colors) to symbol occurrences in terms are used to maintain this information. This technique has several advantages, e. g. it is highly goal oriented and involves little search. In this paper we give a general formalization of coloring terms in a higher-order setting. We introduce a simply-typed calculus with color annotations and present appropriate algorithms for the general, pre- and pattern unification problems. Our work is a formal basis {{to the implementation of}} rippling in a higher-order setting which is required e. g. in case of <b>middle-out</b> reasoning. Another application is in the construction of natural language semantics, where the color annotations rule out linguistically invalid readings that are possible using standard higher-order unification...|$|E
