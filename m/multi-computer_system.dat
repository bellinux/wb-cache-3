21|31|Public
50|$|The Suprenum-1 was {{designed}} as a massively parallel MIMD <b>multi-computer</b> <b>system</b> and it was based on a distributed hardware architecture.|$|E
50|$|HAMMONDsoftware {{distributed}} {{a number}} of RT-11 compatible operating systems including STAReleven, an early <b>multi-computer</b> <b>system</b> and SHAREplus, a multi-process/multi-user implementation of RT-11 which borrowed some architectural concepts from the VAX/VMS operating system. RT-11 device drivers were required for operation. Transparent device access to other PDP-11s and VAX/VMS were supported with a network option. Limited RSX-11 application compatibility was also available. SHAREplus had its strongest user base in Europe.|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimitedTechniques are {{developed for the}} design of a monitor of a real-time <b>multi-computer</b> <b>system</b> that is under heavy loading. The first portion relates to the requirements of partitioning to aid in fault recognition and diagnostic routines. The dynamic reallocation of system time to the system tasks and fault monitoring in developed secondly. System reconfiguration of the partitioned subsystems restores the system to operation at a degraded level until faults are corrected. the paper discusses a Ship Combat Weapon System {{as an example of a}} large scale <b>multi-computer</b> <b>system</b> monitor. [URL] United States Nav...|$|E
5000|$|Tandem's NonStop systems use {{a number}} of {{independent}} identical processors and redundant storage devices and controllers to provide automatic high-speed [...] "failover" [...] {{in the case of}} a hardware or software failure. To contain the scope of failures and of corrupted data, these <b>multi-computer</b> <b>systems</b> have no shared central components, not even main memory. Conventional <b>multi-computer</b> <b>systems</b> all use shared memories and work directly on shared data objects. Instead, NonStop processors cooperate by exchanging messages across a reliable fabric, and software takes periodic snapshots for possible rollback of program memory state.|$|R
40|$|The major aims of {{this work}} is to give a {{comparative}} survey of static interconnection topologies, and to discuss their properties {{with respect to their}} use as interconnection topologies in message passing <b>multi-computer</b> <b>systems,</b> i. e. each processing element has its own local memory, there is no common memory, and the processing elements communicate via message-passing. To this end it was necessary to recall relevant measures on graphs from graph theory, like for example the average distance or the network diameter, and requirements from the parallel processing area, like the reliability or extensibility. Special emphasis has been given to present the construction rules for various graphs, because these seemed [...] along with the network characteristics [...] most relevant for interconnecting processing elements in reconfigurable <b>multi-computer</b> <b>systems.</b> Critical to applications in these kind of parallel systems is the possibility of exchanging local data among cooperating processing element [...] ...|$|R
40|$|The major aims of this workistogive a comparativesurvey {{of static}} {{interconnection}} topologies, {{and to discuss}} their properties {{with respect to their}} use as interconnection topologies in message passing <b>multi-computer</b> <b>systems,</b> i. e. each processing element has its own local memory, there is no common memory, and the processing elements communicate via message-passing. To this end it was necessary to recall relevant measures on graphs from graph theory,like for example the average distance or the network diameter, and requirements from the parallel processing area, likethereliability or extensibility. Special emphasis has been given to present the construction rules for various graphs, because these seemed [...] along with the network characteristics [...] most relevant for interconnecting processing elements in reconfigurable <b>multi-computer</b> <b>systems.</b> Critical to applications in these kind of parallel systems is the possibilityofexchanging local data among cooperating processing elements, so some space has also been devoted to the representation of the communication behaviour and the resulting routing demands...|$|R
40|$|This paper {{presents}} three complementary {{components of}} the flow control solution adopted for the Fed-X fabric: high-speed scalable interconnect for a <b>multi-computer</b> <b>system.</b> Each of the three addresses performance problems caused by a particular characteristic of realistic network workloads. The three flow-control techniques introduced in this paper are backpressure flow control, alpha scheduling...|$|E
40|$|The PRISMA {{project is}} a {{large-scale}} research effort {{in the design and}} implementation of a highly parallel machine for data and knowledge processing. The PRISMA database machine is a distributed, main-memory database management system implemented in an object-oriented language that runs on top of a <b>multi-computer</b> <b>system.</b> A prototype that is envisioned consists of 64 processing elements...|$|E
40|$|The {{implementation}} of existing {{software for the}} numerical treatment of partial differential equations on a heterogeneous <b>multi-computer</b> <b>system</b> is discussed. The underlying concept {{is that of a}} user-friendly problem solving environment integrating various software packages from different vendors into one complex tool. Special attention is paid to improvements in user's efficiency. Enhancements leading from problem solving environments toward expert systems are outlined...|$|E
40|$|Abstract: 2 ̆ 2 This paper {{considers}} the coupled design problems of task allocation and processor specification for embedded <b>multi-computer</b> <b>systems.</b> Two unique problem representations are proposed. The first representation involves multi-dimensional bin packing {{while the second}} is based on graph partitioning. Automated solution strategies are developed and evaluated for both representations. The paper concludes {{with a discussion of}} the results, pending research, and areas of future work. 2 ̆...|$|R
40|$|The {{production}} system {{is an effective}} method for knowledge representation. With the scale of its knowledgebase become larger. its inference efficiency turn lower. The paper analysizes the inference mechanism of the {{production system}}. and design a partition algorithm and parallel inference model called GPIM for the parallel processing of the production systemon {{the architecture of the}} distributed <b>multi-computer</b> <b>systems.</b> they are more suited to the processing of large scale knowledgebase国家自然科学基金和航天基金资助项 目...|$|R
40|$|Abstract This paper {{describes}} {{a new language}} for <b>multi-computer</b> <b>systems</b> programming. Processes in the language communicate by sending messages over communication channels called links. Links may be created, manipulated, and destroyed dynamically to provide complete runtime control over the interconnections among processes. Message type checking is performed at run time. Messages may be received explicitly or may trigger the execution of entry procedures. A control-flow mechanism similar to coroutines simplifies the ordering of entry procedures...|$|R
40|$|The PRISMA {{project is}} a {{large-scale}} research effort {{in the design and}} implementation of a highly parallel machine for data and knowledge processing. ThePRISMA database machine is a distributed, main-memory database management system implemented in an object-oriented language that runs on top of a <b>multi-computer</b> <b>system.</b> A prototype that is envisioned consists of 64 processing elements. 1 PRISMA Project The long term objective of the PRISMA project is to obtain a flexible architecture for a ma-chine that effectively stores and manipulates both data and knowledge. This long term objective translates into the following goals: * The construction of a <b>multi-computer</b> <b>system,</b> consisting {{of a large number of}} processing elements connected via a message-passing network; * The definition and efficient implementation f a parallel object-oriented language, c lled POOL-X;. The design and implementation f a main-memory datab le system in POOL-X; o The design and implementation f an expert system shell in POOL-X that exploits paral-lelism for inferencing; • The investigation f using medium to coarse grain parallelism for data and knowledge pro-cessing applications, the int gration of data and knowledge processing, and the evaluation of the prototype multl-computer, among other things...|$|E
40|$|This paper {{presents}} the eighteen year history {{leading to the}} development of a computerized medical information system and discusses the factors which influenced its philosophy, design and implementation. This system, now called TMR, began as a single-user, tape-oriented minicomputer package and now exists as a multi-user, multi-database, <b>multi-computer</b> <b>system</b> capable of supporting a full range of users in both the inpatient and outpatient settings. The paper discusses why we did what we did, what worked, and what didn't work. Current projects are emphasized including networking and the integration of inpatient and outpatient functions into a single system. A theme of the paper is how hardware and software technological advancements, increasing sophistication of our users, our increasing experience, and just plain luck contributed to the success of TMR...|$|E
40|$|Many modern {{systems are}} {{controlled}} by ComputerBased Systems (CBS). Examples include commercial management information systems such as airline reservations, payroll information, stock control, and electronic banking systems, real-time embedded computer systems such as process control and computer integrated manufacturing systems, space systems, telephone and communications systems, transportation systems (automotive control, train control, ship control, traffic control), medical instruments, avionics systems, missile control systems, microcomputer controlled domestic appliances and point of sale systems. The CBS controlling these systems, typically consist of many networked, geographically distributed subsystems. Each subsystem may be or may contain a <b>multi-computer</b> <b>system.</b> They are intensively dependent on software, and frequently depend on data communication networks, human-computer interaction, and special hardware. Engineers developing such systems {{have to work with}} CBS engineers re [...] ...|$|E
40|$|The {{distributed}} computational {{systems and}} the distribution algorithms are thoroughly studied. In this paper we focus on data-intensive distributed problems including of task distributions, network delay times, join of the part task results, {{and the effect of}} non-execution. First we make a study of dedicated <b>multi-computer</b> <b>systems</b> in which the number of computers and the capacity of computers are almost constant. Then we analyze non-dedicated systems where the number and the free capacity of computers are frequently changed. The solution time of the full task depends on a large number of complex parameters. The effects of different parameters are demonstrated by numerical analysis based on behavior of distributed data-intensive super-computing systems...|$|R
5000|$|The {{specification}} {{discussed the}} architecture of <b>multi-computer</b> <b>systems,</b> preferring peer-to-peer rather than master-slave. Each member of such an interconnected group of separate computers is free at any time to initiate and dispatch special control orders to any of its partners in the system. As a consequence, the supervisory control over the common task may initially be loosely distributed throughout the system and then temporarily concentrated in one computer, or even passed rapidly from one machine to the other as the need arises. …it {{should be noted that}} the various interruption facilities which have been described are based on mutual cooperation between the computer and the external devices subsidiary to it, and do not reflect merely a simple master-slave relationship. ALAN L. LEINER ...|$|R
40|$|Abstract. Dynamic data {{redistribution}} enhances data locality {{and improves}} algorithm performance for numerous scientific problems on distributed memory <b>multi-computers</b> <b>systems.</b> Previous results focus on reducing index computational cost, schedule computational cost, and message packing/unpacking cost. In irregular redistribution, however, messages with varying sizes are transmitted {{in the same}} communication step. Therefore, the largest sized messages in the same communication step dominate the data transfer time required for this communication step. This work presents an efficient algorithm to partition large messages into multiple small ones and schedules them by using the minimum number of steps without communication contention and, in doing so, reducing the overall redistribution time. When the number of processors or the maximum degree of the redistribution graph increases or the selected size of messages is medium, the proposed algorithm can significantly reduce the overall redistribution time to 52 %. ...|$|R
40|$|Design of a {{parallel}} computation of iterative solution for electromagnetic fields formulation {{obtained from the}} multidimensional Cauchy integral equations based on Clifford algebra on a <b>multi-computer</b> <b>system</b> is presented. The presented parallel algorithm enhances the Cauchy integral equations whose kernel of integration is simple and contains weakly singular integral by saving time consuming and increasing the memory capacity in the calculation. To implement and demonstrate the computation, the domain decomposition techniques and the Message Passing Interface (MPI) programming are respectively exploited. The performance of the presented algorithm is investigated in two aspects: the speed and the efficiency of calculation. The speed linearly increases {{as a number of}} processors are added while the efficiency is always near 1. 0. No Full Tex...|$|E
40|$|The Tandem NonStop is a loosely-coupled <b>multi-computer</b> <b>system</b> {{managed by}} Guardian 90, a message-based {{distributed}} operating {{system designed to}} provide an environment for online transaction processing. One {{of the benefits of}} a loosely-coupled architecture is its inherently distributed character. A distributed architecture allows many components to be applied scalably in parallel to a large data-intensive task, such as batch or query processing, or to many small independent tasks, such as online transaction processing. However, achieving good performance in a loosely-coupled architecture presents some challenges relative to a tightly-coupled architecture. These challenges include: • Overcoming the high cost of inter-process communication. • Performing load-balancing without shared memory. • Solving the client-server priority inversion problem. Overcoming these challenges has required complex performance-oriented optimizations in Guardian. These optimizations- aimed at reducing message traffic, avoiding softwar...|$|E
40|$|A Cluster of Workstations (COW) is {{network based}} <b>multi-computer</b> <b>system,</b> {{which is the}} most {{prominent}} distributed memory system aimed to replace supercomputers. A cluster of workstations {{can be viewed as a}} single machine in which one job is divided into n subtasks and delegated to n workstations in the COW architecture. To get the job completed, all subtasks assigned to component workstations must be completed. Therefore, for satisfactory job completion, all workstations must be functional. However, a faulty node can suspend the over all job completion task until. Therefore, a job can not be completed until a faulty node is recovered from fault. This paper presents a fault tolerant architecture for COW, which will allow a normally working workstation to perform the tasks of the faulty workstation in addition to its original assignments. The Markov models are basic tools applied for availability modeling. This paper presents a Markov Availability model for estimating the availability of component workstations as a function of workstation failure rates...|$|E
40|$|This paper {{presents}} a new simulation methodology for scheduling and coordinating decentralized job shops using <b>multi-computer</b> <b>systems.</b> The parallel working subsystems {{of an integrated}} production system are distributed across a network of computers. Utilizing the special structure of parallel processor architectures new coordination and scheduling rules for job shops are designed and evaluated. The simulation experiments show that the performance of conventional scheduling rules is significantly improved using the developed coordination approach. Furthermore the run-time of the distributed simulation model comes closer to realtime response of job shop simulations. Keywords: Job Shop Scheduling, Dispatching Rules, Coordination Rules, Distributed Simulation, MultiComputer Systems. 1 Introduction The scheduling or sequencing problem can be stated as follows: N jobs must be processed by M machines or work stations within a given time period to optimize some objectives. Each job consists of [...] ...|$|R
40|$|Computer {{networks}} can {{be classified}} into two broad categories: wired networks and wireless networks, according to the hardware and software technologies used to interconnect the individual devices. Wired interconnection networks are hardware fabrics supporting communications between individual processors in highperformance computing <b>systems</b> (e. g., <b>multi-computer</b> <b>systems</b> and cluster systems). On the other hand, due to the rapid development of wireless technologies, wireless networks have emerged and become an indispensable part for people¿s lives. The integration of different wireless technologies is an effective approach to accommodate the increasing demand of the users {{to communicate with each}} other and access the Internet. This thesis aims to investigate the performance of wired interconnection networks and integrated wireless networks under the realistic working conditions. Traffic patterns have a significant impact on network performance. A number of recent measurement studies have convincingly demonstrated that the traffic generated by many real-world applications in communication networks exhibits bursty arrival nature and the message destinations are non-uniformly distributed. Analytical models for the performance evaluation of wired interconnection networks and integrated wireless networks have been widely reported. However, most of these models are developed under the simplified assumption of non-bursty Poisson process with uniformly distributed message destinations. To fill this gap, this thesis first presents an analytical model to investigate the performance of wired interconnection networks in <b>multi-computer</b> <b>systems.</b> Secondly, the analytical models for wired interconnection networks in multi-cluster systems are developed. Finally, this thesis proposes analytical models to evaluate the end-to-end delay and throughput of integrated wireless local area networks and wireless mesh networks. These models are derived when the networks are subject to bursty traffic with non-uniformly distributed message destinations which can capture the burstiness of real-world network traffic in the both temporal domain and spatial domain. Extensive simulation experiments are conducted to validate the accuracy of the analytical models. The models are then used as practical and cost-effective tools to investigate the performance of heterogeneous wired or wireless networks under the traffic patterns exhibited by real-world applications...|$|R
40|$|Practical {{implementations}} of the Finite Element method on {{distributed memory}} <b>multi-computer</b> <b>systems</b> necessitate {{the use of}} partitioning tools to subdivide the mesh into sub-meshes of roughly equal size. Graph partitioning algorithms are mandatory when implementing distributed sparse matrix methods or domain decomposition techniques for irregularly structured problems, on parallel computers. We propose a class of algorithms {{which are based on}} level set expansions from a number of center nodes. A critical component of these methods is the location of these centers. We present a number of different strategies for finding centers which lead to good-quality partitionings. Work supported in part by ARPA under grant NIST 60 NANB 2 D 1272, in part by NSF grant CCR- 9214116, and by the Minnesota Supercomputer Institute. y Department of Computer Science, University of Minnesota, Minneapolis 55455 z Department of Computer Science, and Minnesota Supercomputer Institute, University of Minnesota, Mi [...] ...|$|R
40|$|MulitSat WebService is a space borne service concept {{primarily}} to support, extend or substitute information services for mobility and traffic purposes. It allows {{the determination of}} traffic data from space on a global and near-real-time scale. Main objective {{is to provide a}} profitable service for mobility and traffic management. A market survey being made shows that space borne online information services may be viable. The service provides the possibility to receive preprocessed, near-real-time Earth surface data with E-commerce compatible methods. The sysem design features a satellite constellation with imaging Synthetic Aperture Radar /SAR) and optical payloads combined with low-rate communication especially established to support this service. Also included is a scalable, fault tolerant, <b>multi-computer</b> <b>system.</b> The development cycle focuses on an airborne demonstration of the service idea as a first milestone. The MulitSat WebService concept is being created and designed by a consortium consisting of German Aerospace Center (DLR); Technische Universität Dresden and Fraunhofer Gesellschaft FIRST and presented here as a visionary feasibility study...|$|E
40|$|When {{working with}} Multi-Agent Systems (MAS), {{small-scale}} experiments limit the conclusions {{that can be}} drawn from a model. Therefore implementing MAS in a distributed <b>multi-computer</b> <b>system</b> will allow us to run larger parameter experiments that will result in more reliable data and conclusions. Converting previous implementations of social behavior into the Scala computing language will allow us to work in a distributive environment via a previously established framework. Specifically, we will focus on translating Yu Wu’s C++ code that simulates his Highest Weighted Reward (HWR) social strategy. With the HWR methodology, agents evaluate their neighbors based on previous actions (Agents can cooperate or defect), weighting their most recent action the most and actions in the past correspondingly less. Based on this reward system, agents can choose to disconnect or add new friends. The same experiments held by Yu Wu will be run with the translated code to verify the validity and integrity of the new Scala code. This code will then be expanded upon to test a different type of hierarchal social strategy among the agents...|$|E
40|$|In this paper, {{there is}} an {{investigation}} into the possibility of executing a Morphological Scene Change Detection (MSCD) system on a Field Programmable Gate Array (FPGA), which would allow its set up in virtually any location, with its purpose to detect intruders and raise an alarm to call security personal, and a signal to initial a lockdown of the local area. This paper will include how the system was scaled down from the full building <b>multi-computer</b> <b>system,</b> to an FPGA without losing any functionality using Altera’s DSP Builder development tool. Also included is the analysis of the different situations which the system would encounter in the field, and their respective alarm triggering levels, these include indoors, outdoors, close-up, distance, high-brightness, low-light, bad weather, etc. The triggering mechanism is a pixel counter and threshold system, and its adaptive design will be included. All the results shown in this paper, will also be verified by MATLAB m-files running on a full desktop PC, to show that the results obtained from the FPGA based system are accurate...|$|E
40|$|POEMS is a Parallel Object-oriented Environment for <b>Multi-computer</b> <b>Systems.</b> In {{order to}} support dynamic load balancing, its runtime {{execution}} model is based on object replication. Method invocation in POEMS is asynchronous and threads are created to execute methods. Inter-object, intra-object as well as intra-method parallelism are all supported. Programs in POEMS are written using two classes of objects, i. e. parallel object replication (POR) and parallel object collection (POC) classes. They are used to support programming in MPMD and SPMD styles, respectively. This paper {{will focus on the}} object models and programming facilities of POEMS and presents some preliminary performance studies. The major features and execution models of POR and POC classes are described in detail. In addition, some typical applications are also presented to illustrate the usage of these two classes. The implementation issues of a POEMS prototype runtime system are also discussed...|$|R
40|$|Process {{allocation}} {{is important}} for fault-tolerant <b>multi-computer</b> <b>systems</b> since it directly affects the performance and dependability of the system. In this paper, we consider load-balancing process allocation for fault-tolerant systems that balances the load before as well as after faults start to degrade {{the performance of the}} system. We show two schemes to tolerate multiple faults in the passive replica process model. The first scheme is running multiple backup processes for each process. The second scheme is running only one backup process for each process, but re-generate backup processes for processes that do not have a backup process after a fault occurrence to keep the primarybackup process pair available. In both schemes, we propose heuristic process allocation methods for balancing loads inspite of the occurrence of a fault. Simulation is used to compare the performance of both schemes. In each scheme, we are able to keep the load balanced reasonably among nodes after a fault [...] ...|$|R
40|$|Web hosting is an {{infrastructure}} service that allows to design, integrate, {{operate and maintain}} all of the infrastructure components required to run web-based applications. It includes Web server farms, network access, data staging tools and security rewalls. Web server farms are used in a Web hosting infrastructure {{as a way to}} create scalable and highly available solutions. One of the main problems in web server farm management iscontent management and load balancing. In this paper, we analyze several hardware/software solutions on the market and demonstrate their scalability problems. We outline a new scalable solution FLEX for design and management of an e cient Web hosting service. This solution can be applied to a Web hosting service implemented on di erent architecture platforms such asweb server farms with replicated disk content, web server clusters having access to a shared le <b>system</b> or <b>multi-computer</b> <b>systems</b> using a global (shared) le system. A preliminary performance analysis provides a comparison of the current solutions and FLEX using a synthetic workload generator based on SpecWeb' 96 benchmark...|$|R
40|$|There are {{now over}} one million UNIX sites and the pace at which new {{installations}} are added is steadily increasing. Along with this increase, comes a need to develop simple efficient, effective and adaptable ways of simultaneously collecting real-time diagnostic and performance data. This need exists because distributed systems can give rise to complex failure situations that are often un-identifiable with single-machine diagnostic software. The simultaneous collection of error and performance data is also important for research in failure prediction and error/performance studies. This paper introduces a portable method to concurrently collect real-time diagnostic and performance data on a distributed UNIX system. The combined diagnostic/performance data collection is implemented on a distributed <b>multi-computer</b> <b>system</b> using SUN 4 's as servers. The approach uses existing UNIX system facilities to gather system dependability information such as error and crash reports. In addition, performance data such as CPU utilization, disk usage, I/O transfer rate and network contention is also collected. In the future, the collected data {{will be used to}} identify dependability bottlenecks and to analyze the impact of failures on system performance...|$|E
40|$|MultiSat WebService is a {{spaceborne}} service concept {{primarily to}} support, extend or substitute information services for mobility and traffic purposes. It allows {{the determination of}} traffic data from space on a global and near-real-time scale. Main objective {{is to provide a}} profitable service for mobility and traffic management. A market survey being made shows that spaceborne online information services may be viable. The service provides the possibility to receive pre-processed, near-real-time Earth surface data with E-commerce compatible methods. The system design gives the opportunity to freely configure the space system according to customers needs. The MultiSat infrastructure design features a satellite constellation with imaging Synthetic Aperture Radar (SAR) and optical payloads combined with low-rate communication especially established to support this service. Also included is a scalable, fault tolerant, <b>multi-computer</b> <b>system.</b> The development cycle focuses on an airborne demonstration of the service idea as a first milestone. The MultiSat WebService concept is being created and designed by a consortium consisting of German Aerospace Center (DLR), Technische Universität Dresden and Fraunhofer Gesellschaft FIRST and presented here as a visionary feasibility study. 1...|$|E
40|$|Abstract- OmniRPC is a Grid RPC {{system for}} {{parallel}} programming in cluster and grid environments. To support typical master-worker grid {{applications such as}} a parametric execution efficiently, OmniRPC provides an automaticinitializable remote module to send and store data to a remote executable invoked in the remote host. Since it may accept several requests for subsequent calls by keeping the connection alive, the data set by the initialization is re-used, resulting in efficient execution by {{reducing the amount of}} communication. It allows to use several authentication systems including “ssh ” and “globus GRAM”. For a cluster over a private network, an agent process running the server host functions as a proxy to relay communications between the client and the remote executables by multiplexing the communications into one connection to the client. Several algorithms and applications for a grid are designed or re-targeted by using OmniRPC. In this paper, we present a parallel method for large sparse generalized eigenvalue problems, a gridenabled Molecular Conformational Space Search Program (CONFLEX-G), and a grid-enabled Heterogeneous <b>Multi-Computer</b> <b>System</b> (HMCS-G) for computational astrophysics in order to demonstrate the effectiveness of OmniRPC. Keywords—Grid computing, remote procedure call, parametric execution I...|$|E
40|$|Abstract: Today’s <b>multi-computer</b> <b>systems</b> are {{heterogeneous}} in nature, i. e., {{the machines}} they are composed of, have varying processing capabilities and are interconnected through high speed networks, thus, making them suitable for performing diverse set of computing-intensive applications. In order {{to exploit the}} high performance of such a distributed system, efficient mapping of the tasks on available machines is necessary. This is an active research topic and different strategies have been adopted in literature for the mapping problem. A novel approach has been introduced in the paper for the efficient mapping of the DAG-based applications. The approach {{that takes into account}} the lower and upper bounds for the start time of the tasks. The algorithm is based on list scheduling approach and has been compared with the well known list scheduling algorithms existing in the literature. The comparison results for the randomly synthesized graphs as well as the graphs from the real world elucidate that the proposed algorithm significantly outperforms the existing ones on the basis of different cost and performance metrics...|$|R
40|$|Roscoe is a <b>multi-computer</b> {{operating}} <b>system</b> {{running on}} a network of LSI- 11 computers at the University of Wisconsin. This document describes {{the implementation of the}} Roscoe kernel at the level of detail necessary for a programmer who intends to add a module or modify the existing code. Companion reports describe the purposes and concepts underlying the Roscoe project, present the implementation details of the utility processes, and display Roscoe fromthe {{point of view of the}} user program...|$|R
40|$|Mitigating {{the impact}} of {{computer}} failure is possible if accurate failure predictions are provided. Resources, and services can be scheduled around predicted failure and limit the impact. Such strategies are especially important for <b>multi-computer</b> <b>systems,</b> such as compute clusters, that experience {{a higher rate of}} failure due to the large number of components. However providing accurate predictions with sufficient lead time remains a challenging problem. This research uses a new spectrum-kernel Support Vector Machine (SVM) approach to predict failure events based on system log files. These files contain messages that represent a change of system state. While a single message in the file may not be sufficient for predicting failure, a sequence or pattern of messages may be. This approach uses a sliding window (sub-sequence) of messages to predict the likelihood of failure. Then, a frequency representation of the message sub-sequences observed are used as input to the SVM. The SVM associates the messages to a class of failed or non-failed system. Experimental results using actual system log files from a Linux-based compute cluster indicate the proposed spectrum-kernel SVM approach can predict hard disk failure with an accuracy of 80 % about one day in advance. ...|$|R
