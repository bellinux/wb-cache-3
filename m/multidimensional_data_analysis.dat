105|10000|Public
5000|$|Madagascar is a {{software}} package for <b>multidimensional</b> <b>data</b> <b>analysis</b> and reproducible computational experiments. Its {{mission is to}} provide ...|$|E
5000|$|Besides the {{standard}} functions for one- and <b>multidimensional</b> <b>data</b> <b>analysis</b> {{the focus was}} on non- and semiparametric modelling and the statistics of financial markets.|$|E
5000|$|High-dimensional spaces {{frequently}} {{occur in}} mathematics and the sciences, in example N-dimensional feature space, which presents input signals of neural network or collection of N-dimensional parameters for <b>multidimensional</b> <b>data</b> <b>analysis.</b> Rotation is one of rigid transformations in geometrical space, that preserves length of vectors and can be presented using matrix operation like Y = M.X , where X and Y are input and output vector respectively, and M - rotation matrix.|$|E
40|$|This paper {{introduces}} {{an innovative}} methodological approach for European ANSP-Assessment. An evaluation of potential methodologies identified {{the application of}} the <b>multidimensional</b> <b>Data</b> Envelopment <b>Analysis</b> combined with a Regression Analysis as an appropriate procedure in ANSP assessment activities. In addition, the paper introduces an innovative modular benchmarking scheme for an in-depth investigation of ANSP-Efficiency...|$|R
40|$|The paper {{describes}} an innovative methodological approach for European ANSP-assessment. An evaluation of potential methodologies identified {{the application of}} the <b>multidimensional</b> <b>Data</b> Envelopment <b>Analysis</b> combined with a regression analysis as an appropriate procedure in ANSP assessment activities. Furthermore, the paper introduces an innovative modular benchmarking scheme for an in-depth investigation, extending the described methodology by including an application on different operational levels and the utilization of manifold data sources...|$|R
40|$|We develop various {{statistical}} methods important for <b>multidimensional</b> genetic <b>data</b> <b>analysis.</b> Theorems justifying {{application of these}} methods are established. We concentrate on the multifactor dimensionality reduction, logic regression, random forests, stochastic gradient boosting along with their new modifications. We use complementary approaches to study the risk of complex diseases such as cardiovascular ones. The roles of certain combinations of single nucleotide polymorphisms and non-genetic risk factors are examined. To perform the <b>data</b> <b>analysis</b> concerning the coronary heart disease and myocardial infarction the Lomonosov Moscow State University supercomputer “Chebyshev” was employed...|$|R
50|$|Decision support {{applications}} and research concentrates on identified data-oriented systems, management expert systems, <b>multidimensional</b> <b>data</b> <b>analysis,</b> query and reporting tools, online analytical processing (OLAP), business Intelligence, group DSS, conferencing and groupware, document management, spatial DSS and executive Information Systems as the technologies rise, meet and wander. The investigation of decision support systems is a connected train that utilizes learning and particularly hypothesis from different disciplines. Consequently, numerous DSS scientists look into inquiries {{that have been}} analyzed {{on the grounds that}} they were of worry to individuals who were building and utilizing particular DSS. Subsequently, a great part of the wide DSS information base gives speculations and headings to building more powerful DSS.|$|E
40|$|The main {{objective}} of my diploma thesis is <b>multidimensional</b> <b>data</b> <b>analysis.</b> Analyzed data {{come from the}} comparative research Město pro byznys 2013 (Eng. translation: The city for business 2013). Another goal is to propose some changes that could improve the project. Used methods for <b>multidimensional</b> <b>data</b> <b>analysis</b> are exploratory analysis, principal component analysis, factor analysis and cluster analysis. Among others, for proposing some changes I use multi-criteria decision analysis...|$|E
40|$|The article {{deals with}} the {{problematic}} aspects of the information technologies deployment in the university educational process, defines the tasks of the integrated monitoring of student's academic achievement. In the article it has been analyzed the modern technologies of <b>multidimensional</b> <b>data</b> <b>analysis,</b> Data Mining and OLAP-technologies, determined directions and possibilities of using <b>multidimensional</b> <b>data</b> <b>analysis</b> in monitoring of student's academic achievement, performed an object design of software systems using UML, substantiated the choice of platform development and described a prototype of the system...|$|E
40|$|International audienceData {{warehouses}} store {{large volumes}} of consolidated and historized <b>multidimensional</b> <b>data</b> for <b>analysis</b> and exploration by decision-makers. Exploring data is an incremental OLAP (On-Line Analytical Processing) query process for searching relevant information in a dataset. In order to ease user exploration, recommender systems are used. However when facing a new system, such recommendations do not operate anymore. This {{is known as the}} cold-start problem. In this paper, we provide recommendations to the user while facing this cold-start problem in a new system. This is done by patternizing OLAP queries. Our process is composed of four steps: patternizing queries, predicting candidate operations, computing candidate recommendations and ranking these recommendations...|$|R
40|$|In the <b>analysis</b> of <b>multidimensional</b> <b>data</b> sets {{questions}} involving {{detection of}} extremal events, correlations, patterns and trends play {{an increasingly important}} role {{in a variety of}} applications. Axesbased visualizations like Parallel or Star Coordinates are useful tools for the <b>analysis</b> of <b>multidimensional</b> <b>data</b> sets. In this paper, we present several interactive axes, which can be used to analyze data in an intuitive manner. Furthermore, we present two novel radial visual arrangements of such axes- the TimeWheel and the MultiComb. They focus on data sets with one variable of reference. TimeWheel and MultiComb in combination with interactive axes are part of an interactive framework called VisAxes, which can be used for enhanced <b>multidimensional</b> <b>data</b> browsing and <b>analysis...</b>|$|R
40|$|This work {{is related}} to Business Intelligence {{technology}}, Data Warehouses, Data Mining technology and query languages. Using modern <b>data</b> <b>analysis</b> functions we will show an approach {{to increase the efficiency}} of <b>data</b> <b>analysis</b> by using <b>multidimensional</b> <b>data</b> query and <b>analysis</b> languages like MDX and DMX. These languages are poorly known and have great potential. For example we can enhance data warehouse queries with Data Mining functions, which can give us very quick and accurate results. We will also discuss possibilities to use DMX patterns for data prediction. We will use Data Mining prediction functions to solve the lack o data in data warehouses problem. It is very unusual usage of Data Mining functions, because such functions are usually used for <b>data</b> <b>analysis,</b> but not for data renewal...|$|R
40|$|Accurate {{calculation}} of energy losses, technical {{as well as}} commercial, in real power systems is a complex issue that requires consideration of many various factors. The analysis of electric energy losses in different distribution companies with respect to chosen characteristics (area size, number of recipients, load density, recipients density, network density, and others) is presented in this paper. In order {{to carry out a}} detailed analysis of losses characteristics and to examine the influence of different factors on the level of losses several classical methods of <b>multidimensional</b> <b>data</b> <b>analysis</b> have been applied. Additionally, a new method of graphic representation of the full structure of multidimensional data is proposed by the authors. The carried out analysis can be a meaningful voice in the ongoing discussion on shaping tariffs for electric energy. It is especially important in the context of deregulation of electric energy markets in Poland and Europe because losses reduce economic efficiency of a company thus worsen its competitiveness on the market. KEY WORDS Power distribution, electric energy market, electric energy losses, <b>multidimensional</b> <b>data</b> <b>analysis,</b> visual methods of <b>multidimensional</b> <b>data</b> <b>analysis</b> 1...|$|E
40|$|Abstract. This paper {{designed}} a data warehouse model of agricultural production. And it built {{an effective and}} viable agricultural production data warehouse, by using some key technologies: <b>multidimensional</b> <b>data</b> <b>analysis,</b> cube, materialized view selection, materialized view maintenance. Finally, it provided a solution for the effective management and maintenance problems about high-capacity heterogeneous data...|$|E
40|$|<b>Multidimensional</b> <b>data</b> <b>analysis</b> is {{currently}} being discussed in terms like OLAP, data warehousing, or decision support, mainly concentrating on business applications. Numerous OLAP-tools providing flexible query facilities for datacubes are being designed and distributed. Typical analysis sessions with these kind of systems comprise long and branching sequences of exploratory analysis steps which base upon each other. While concentrating on single functions and processing steps, management of this analysis process {{as a whole is}} scarcely supported. This paper proposes a dataflow-based visual programming environment for <b>multidimensional</b> <b>data</b> <b>analysis</b> (VIOLA) as an approach to deal with this problem. Providing a foundation of basic operations, data processing, navigation, and user interaction, an appropriate data model (MADEIRA) is developed. Epidemiological studies, i. e. investigations of aggregate data on populations, their state of health, and potential risk factors, will serve as a [...] ...|$|E
40|$|It is {{proved that}} the {{internal}} path length of a d [...] dimensional quad tree after normalization converges in distribution. The limiting distribution is characterized as a fixed point of a random affine operator. We obtain convergence of all moments and of the Laplace transforms. The moments of the limiting distribution can be evaluated from the recursion and lead to first order asymptotics for the moments of the internal path lengths. The analysis {{is based on the}} contraction method. Key words: quad trees, contraction method, <b>multidimensional</b> <b>data</b> structure, <b>analysis</b> of algorithms AMS subject classification: 68 Q 25, 60 F 05 1 Introduction Quad trees are a classical data structure introduced by Finkel and Bently [4] to store and retrieve <b>data</b> from some <b>multidimensional</b> <b>data</b> space that extends the familiar binary search tree for one dimensional data. Several characteristics of quad trees have been analysed in the standard random model which assumes that the data points are independent and [...] ...|$|R
40|$|Automatic target {{detection}} and recognition (ATR) requires {{the ability to}} optimally extract the essential features of an object from (usually) cluttered environments. In this regard, efficient data representation domains are required in which the important target features are both compactly and clearly represented, enhancing ATR. Since both {{detection and}} identification are important, <b>multidimensional</b> <b>data</b> representations and <b>analysis</b> techniques, such as the continuous wavelet transform (CWT), are highly desirable. First we review some relevant properties of two 2 D CWT. Then we propose a two-step algorithm based on the 2 D CWT and discuss its adequacy for solving the ATR problem. Finally we apply the algorithm to various images...|$|R
40|$|The volume {{presents}} {{new developments}} in <b>data</b> <b>analysis</b> and classification, and gives {{a state of the}} art impression of these scientific fields {{at the turn of the}} Millenium. Areas that receive considerable attention in this book are Cluster <b>Analysis,</b> <b>Data</b> Mining, <b>Multidimensional</b> and Symbolic <b>Data</b> <b>Analysis,</b> Decision and Regression Trees. The volume contains a refereed selection of original papers, overview papers, and innovative applications presented at the 7 th Conference of the International Federation of Classification Societies (IFCS- 2000), with contributions from eminent scientists all over the world. The reader finds introductory material into various areas and kaleidoscopic views of recent technical and methodological developments in widely different areas within <b>data</b> <b>analysis</b> and classification. The presence of a large number of application papers demonstrates the usefulness of the recently developed techniques...|$|R
40|$|Important {{time-budget}} methodological {{issues are}} concerned with analysing time use tables, obtainable from time-budget diaries to face the multipurpose nature, the size and the complexity of time-budget data. After a brief introduction to the main time use analysis the paper focuses on the cross-sectional analysis using the explorative <b>multidimensional</b> <b>data</b> <b>analysis.</b> The paper deals with the multiway methods suitable for comparing statistical studies (i. e. countries) when each of them has many variables (i. e. activities) observed on many cases (i. e. categories of population). This article examines an example of application to cross-national differences in time use in six European countries {{at different stages of}} life. The results are exemplary of the applicational steps and statistical aspects of the methods proposed rather than definitive findings. Cross-national, cross-sectional, explorative <b>multidimensional</b> <b>data</b> <b>analysis,</b> multiway analysis, statis method, multiple factor analysis method...|$|E
40|$|Deterministic and {{stochastic}} {{forms of}} linear and non-linear ``prior'' models {{were used to}} develop a new <b>multidimensional</b> <b>data</b> <b>analysis</b> within the classical canonical analysis. Detection of outliers with the new model is discussed. While the new model opens up a variety of research problem, it has potential straightforward applications in data mining in science, economics, commerce and industry...|$|E
40|$|The factors {{motivating}} {{students to}} take part in overseas study programs are instrumental in understanding the phe-nomenon of visiting students and other participants in edu-cational tours to Israel. In this study, the reasons why Ameri-can Jewish students come to study in Israel are examined. <b>Multidimensional</b> <b>data</b> <b>analysis</b> reveals four motivational categories: religion, tourism, religion and tourism com-bined, and other...|$|E
30|$|Principal {{component}} analysis {{is a well-known}} multivariate analysis used to summarize <b>multidimensional</b> correlated <b>data.</b> The <b>analysis</b> finds principal components (PC) from several explanatory variables. The PCs are created such that the first PC accounts for the maximum variation, the second PC accounts for {{as much of the}} remaining variation as possible, and so forth. Since the PCs are not correlated with each other, irrelevant and unstable information is discarded. Only the most relevant and stable part of the variation is used. Standardized climate factors were analyzed using principal {{component analysis}} (Excel Tokei 2008, Social Survey Research Information Co., Ltd.).|$|R
40|$|This paper {{deals with}} a {{performance}} assessment of European Air Navigation Providers on Area Control Center level. Based on a requirement analysis considering tasks and functionality of an ANSP, an evaluation of potential methodologies determined {{the application of the}} <b>multidimensional</b> <b>Data</b> Envelopment <b>Analysis</b> as an appropriate procedure for assessment activities. Official reports like the ATM Cost Effectiveness Report published by EUROCONTROL as well as several scientific studies provide a first approach to ANSP benchmarking. However, since an ANSP is represented by several operational levels this paper is aims to dig deeper. Therefore, we analyzed 14 ACCs of six European ANSPs, organized in the conglomerate of the Functional Airspace Block European Central (FABEC). Furthermore, we ran a Malmquist analysis in order to gain information about the efficiency and productivity developments over time. Four models were created in order to cover different ANSP requirements, to test the influence of ATFM-delay as a proxy for service quality and to check the DEA model for robustness. The paper shows that the application of DEA lead to plausible and stable results...|$|R
40|$|This paper {{focuses on}} the use of {{graphical}} display of <b>multidimensional</b> <b>data</b> using an improved version of parallel coordinates. It shows how visualization can be used effectively for exploratory <b>data</b> <b>analysis</b> and describes WinViz, a tool that realizes it. Keywords Graphical display of information, exploratory <b>data</b> <b>analysis,</b> <b>data</b> mining 1...|$|R
40|$|The aim of {{this work}} is to propose a new {{approach}} for dealing with histogram data in symbolic data analysis framework. The idea is to approximate histogram data using B-spline functions in order to synthetize the information within data trough some characteristic function parameters. This parameters {{will be the new}} data that could be, subsequently, analyzed with methodologies of <b>multidimensional</b> <b>data</b> <b>analysis...</b>|$|E
40|$|EditorialThis {{special issue}} {{presents}} {{a selection of}} nine papers dealing with various topics of <b>multidimensional</b> <b>data</b> <b>analysis,</b> coming from very different fields. Some papers are proposing new Stochastic Modeling, while others {{are more concerned with}} the improvement of exploratory Data Analysis techniques. Most of them combine Stochastic Modeling with techniques of approximation that are either combinatorial or geometrical. Their common feature is the strong relation with applications...|$|E
40|$|<b>Multidimensional</b> <b>data</b> <b>analysis</b> is {{considerably}} important {{when dealing with}} such large and complex datasets. Among the possibilities when analyzing such kind of data, applying visualization techniques can help the user find and understand patters, trends and establish new goals. This thesis aims to present several visualization methods to interactively explore multidimensional datasets aimed from specialized to casual users, by making use of both static and dynamic representations created by multidimensional projections...|$|E
50|$|Most {{important}} {{applications of}} the method and free software are in bioinformatics for exploratory <b>data</b> <b>analysis</b> and visualisation of <b>multidimensional</b> <b>data,</b> for data visualisation in economics, social and political sciences, as an auxiliary tool for data mapping in geographic informational systems and for visualisation of data of various nature.|$|R
40|$|High-content {{screening}} {{is a part}} of {{the drug}} discovery pipeline dealing with the identification of substances that affect cells in a desired manner. Biological assays with a large set of compounds are developed and screened and the output is generated with a <b>multidimensional</b> structure. <b>Data</b> <b>analysis</b> is performed manually by an expert with a set of tools and this is considered to be too time consuming and unmanageable when the amount of data grows large. This thesis therefore investigates and proposes a way of automating the <b>data</b> <b>analysis</b> phase through a set of machine learning algorithms. The resulting implementation is a cloud based application that can support the user with the selection of which features that are relevant for further analysis. It also provides techniques for automated processing of the dataset and training of classification models which can be utilised for predicting sample labels. An investigation of the workflow for analysing data was conducted before this thesis. It resulted in a pipeline that maps the different tools and software to what goal they fulfil and which purpose they have for the user. This pipeline was then compared with a similar pipeline but with the implemented application included. This comparison demonstrates clear advantages in contrast to previous methodologies in that the application will provide support to work in a more automated way of performing <b>data</b> <b>analysis...</b>|$|R
50|$|Data {{drilling}} (also drilldown) {{refers to}} any of various operations and transformations on tabular, relational, and <b>multidimensional</b> <b>data.</b> The term has widespread use in various contexts, but is primarily associated with specialized software designed specifically for <b>data</b> <b>analysis.</b>|$|R
40|$|Gaia will observe {{more than}} one billion objects {{brighter}} than ¢¡¤£¦ ¥, including stars, asteroids, galaxies and quasars. As Gaia performs real time detection (i. e. without an input catalogue) the intrinsic properties of most of these objects will not be known a priori. An {{integral part of the}} Gaia data processing is therefore to classify everything observed. This will be based primarily on multiband photometry provided by Gaia, but should also make optimal use of the high resolution spectroscopy (for brighter stars) and the parallaxes. In addition to a broad classification, we can also determine fundamental stellar parameters, in particular effective temperature, metallicity and the line-of-sight interstellar extinction. Such information will be essential for fully exploiting the astrometric part of the Gaia catalogue for stellar population studies. However, extracting this information is a significant challenge, and will need to make use of appropriate <b>multidimensional</b> <b>data</b> <b>analysis</b> techniques. I outline some of the problems and the strategies being developed to tackle them. Key words: classification – stellar parameters – data processing – <b>multidimensional</b> <b>data</b> <b>analysis.</b> 1...|$|E
40|$|Abstract: The paper {{provides}} {{a contribution to}} factorial methods in <b>multidimensional</b> <b>data</b> <b>analysis</b> covering the gap of graphical representations of statistical units on which a multiple set of response variables {{as well as a}} common set of explanatory variables are observed. By joining the features of multiple Co-Inertia analysis with those of a geometrical non-symmetrical approach, the proposed technique gains remarkable advantages in identifying a typology of statistical units generated by the mentioned dependence structure...|$|E
40|$|Aim of {{this paper}} is to {{underline}} the main contributions in the context of Factorial Conjoint Analysis. The integration of Conjoint Analysis with the exploratory tools of <b>Multidimensional</b> <b>Data</b> <b>Analysis</b> is the basis of different research strategies, proposed by the authors, combining the common estimation method with its geometrical representation. Here we present a systematic and unitary review of some of these methodologies by taking into account their contribution to several open ended problems...|$|E
40|$|Physics of {{high energy}} {{elementary}} particles is {{considered in the}} paper {{with the aim of}} the development of a physical experiment of the <b>multidimensional</b> correlated <b>data</b> statistic <b>analysis</b> universal methodology and its introduction into operation. As a result the determination model-independent methodology for a mass composition and energetic spectrums of a primary space radiation has been created. An upper Bound of an iron nucleus part in a space radiation primary flow, when energy is more than 10 " 1 " 5 eV, has been determined for the first time according to roentgen-emulsion camera <b>data.</b> New <b>analysis</b> methods of experiment data with roentgen-emulsion cameras and Cherenkov's telescopes have been introduced into operation. The authenticity of physical conclusions has been increased considerablyAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
40|$|On-line {{analytical}} processing (OLAP) systems considerably improve <b>data</b> <b>analysis</b> and {{are finding}} wide-spread use. OLAP systems typically employ <b>multidimensional</b> <b>data</b> models to structure their data. This paper identifies 11 modeling requirements for <b>multidimensional</b> <b>data</b> models. These requirements {{are derived from}} an assessment of complexdata found in real-world applications. A survey of 14 <b>multidimensional</b> <b>data</b> models reveals shortcomings in meeting some of the requirements. Existing models do not support many-to-many relationships between facts and dimensions, lack built-in mechanisms for handling change and time, lack support for imprecision, and are generally unable to insert data with varying granularities. This paper defines an extended <b>multidimensional</b> <b>data</b> model and algebraic query language that address all 11 requirements. The model reuses the common multidimensional concepts of dimension hierarchies and granularities to capture imprecise data. For queries that cannot be answere [...] ...|$|R
40|$|AbstractIt is {{estimated}} that approximately $ 700 billion is lost due to fraud, waste, and abuse in the US healthcare system. Medicaid has been particularly susceptible target for fraud in recent years, with a distributed management model, limited cross- program communications, and a difficult-to-track patient population of low-income adults, their children, and people with certain disabilities. For effective fraud detection, one {{has to look at}} the data beyond the transaction-level. This paper builds upon Sparrow's fraud type classifications and the Medicaid environment and to develop a Medicaid multidimensional schema and provide a set of <b>multidimensional</b> <b>data</b> models and <b>analysis</b> techniques that help to predict the likelihood of fraudulent activities. These data views address the most prevalent known fraud types and should prove useful in discovering the unknown unknowns. The model is evaluated by functionally testing against known fraud cases...|$|R
