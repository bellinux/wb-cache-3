26|1428|Public
25|$|Because some {{radioactivity}} {{is present}} everywhere (i.e., background radiation), the spectrum should be analyzed when no source is present. The background radiation must then be subtracted {{from the actual}} <b>measurement.</b> <b>Lead</b> absorbers can be placed around the measurement apparatus to reduce background radiation.|$|E
50|$|In science, a fact is a {{repeatable}} careful observation or measurement (by experimentation {{or other}} means), also called empirical evidence. Facts {{are central to}} building scientific theories. Various forms of observation and <b>measurement</b> <b>lead</b> to fundamental questions about the scientific method, and the scope and validity of scientific reasoning.|$|E
40|$|Large scale {{computer}} simulations are presented {{to investigate the}} avalanche statistics of sand piles using molecular dynamics. We could show that different methods of <b>measurement</b> <b>lead</b> to contradicting conclusions, presumably due to avalanches not reaching {{the end of the}} experimental table. Comment: 6 pages, 4 figure...|$|E
40|$|Roughness {{measurements}} on smooth samples of scattering and profilometric measurements are compared. Scattering <b>measurements</b> <b>lead</b> {{to the power}} spectral density of the surface while profilometric <b>measurements</b> <b>lead</b> to the surface profile. Usually from these functions surface roughness parameter were calculated and compared with each other. It is shown, {{that because of the}} different spatial frequency bandwidths of these measuring processes this procedure leads to dramatically errors. Another possibility is to describe smooth surfaces as fractals. The fractal parameters are calculated in three different ways and compared with each other...|$|R
3000|$|At {{the first}} rotation, all <b>{{measurements}}</b> <b>lead</b> to new track roots, but at later rotations, only the measurements which are {{far away from}} any current track prediction are added (row 10). The distance to other track threshold is given by [...]...|$|R
3000|$|As for the {{measurement}} configuration, the {{attention has been}} mainly focused {{on the impact of}} additional voltage measurements, since they can significantly affect numerical properties and efficiency of the branch current-based formulation of DSSE. In fact, voltage <b>measurements</b> <b>lead</b> to non-zero terms in the Jacobian H [...]...|$|R
40|$|The {{entanglement}} {{of clouds}} of atoms recently experimentally verified {{is expressed in}} terms of the fluctuation algebra introduced by Goderis, Verbeure and Vets. A mean field Hamiltonian describing the coupling to a laser beam leads to different time evolutions if considered on microscopic or mesoscopic operators. Only the latter creates non trivial correlations that finally after a <b>measurement</b> <b>lead</b> to entanglement between the clouds...|$|E
40|$|Nuclear {{dependence}} of phi and ω − ρ production is studied from heavy nuclei and deuteron-induced reactions. In {{addition to the}} ratio phi/(ω + ρ), {{closely related to the}} strangeness saturation factor γS in transverse mass (MT) domains, absolute cross-sections and multiplicities are also presented. The phi multiplicities for central Pb–Pb collisions, as obtained in the most recent NA 50 <b>measurement,</b> <b>lead</b> to an updated version of the phi puzzle, where reduced discrepancy between experiments opens a new space for a systematic difference between the leptonic and hadronic decay channels...|$|E
40|$|We {{present a}} {{theoretical}} analysis of inequivalent classes of interference experiments with non-abelian anyons using an idealized Mach-Zender type interferometer. Because of the non-abelian {{nature of the}} braid group action one has to distinguish the different possibilities in which the experiment can be repeated, which lead to different interference patterns. We show that each setup will, after repeated <b>measurement,</b> <b>lead</b> to {{a situation where the}} two-particle (or multi-particle) state gets locked into an eigenstate of some well defined operator. Also the probability to end up in such an eigenstate is calculated. Some representative examples are worked out in detail. Comment: 27 pages, 9 figure...|$|E
40|$|The {{study of}} binary {{diagrams}} for which some {{features of the}} nematic-smectic A transitions are not in agreement with McMillan's theories is presented. A comparison of microscopic observations with enthalpic <b>measurements</b> <b>leads</b> to assume the occurrence of SA-SA transitions, the structure of smectic layers being either monomolecular or bimolecular...|$|R
50|$|Since around 1997-2003, {{the problem}} is {{believed}} to be solved by most cosmologists: modern <b>measurements</b> <b>lead</b> to an estimate of {{the age of the universe}} of 13.8 billion years, and recent age estimates for the oldest objects are either younger than this, or consistent allowing for measurement uncertainties.|$|R
40|$|Due to a {{restrictive}} {{scenario of}} sequential projective measurements, multi-point temporal quantum correlations {{were thought to}} factorize into two-point ones. We show that sequential generalized <b>measurements</b> <b>lead</b> to genuinely multi-point temporal correlations, which enable us to translate typical spatial genuinely multipartite nonlocal phenomena, like Greenberger-Horne-Zeilinger paradox, into the temporal scenario...|$|R
40|$|This paper {{considers}} {{the use of}} imputation and weighting to correct for measurement error in the estimation of a distribution function. The paper is motivated by the problem of estimating the distribution of hourly pay in the United Kingdom, {{using data from the}} Labour Force Survey. Errors in <b>measurement</b> <b>lead</b> to bias and the aim is to use auxiliary data, measured accurately for a subsample, to correct for this bias. Alternative point estimators are considered, based upon a variety of imputation and weighting approaches, including fractional imputation, nearest neighbour imputation, predictive mean matching and propensity score weighting. Properties of these point estimators are then compared both theoretically and by simulation. A fractional predictive mean matching imputation approach is advocated. It performs similarly to propensity score weighting, but displays slight advantages of robustness and efficiency...|$|E
40|$|Indicators {{of success}} in {{oncology}} traditionally include cure, survival, and tumor response. In advanced stage however, Quality of life (QOL) has become an important outcomes. 202 ̆ 712 Despite the broad {{use of the term}} QOL, it is difficult precisely define. 102 ̆ 712 It is multidimensional, dynamic and subjective concept. 32 ̆ 752 ̆ 710 The numerous questionnaire for QOL <b>measurement</b> <b>lead</b> to a challenge for its application. 12 Most of QOL instrument include physical symptoms, functioning, psychological and social well-being. 12 In advanced stage existential, meaning, fulfillment, purpose and grief become more prominent. 112 ̆ 712 Besides the contain of the questionnaire, validity and reliability need to be considered in deciding which instrument will be applied. 12 This paper discusses the definition of QOL, the purpose of measuring QOL, various QOL instruments, the McGill Quality of Life Questionnaire as a measure suggested in clinical practice and the reasons for its application. Key words; Quality of Life, Palliative, Cancer Patient...|$|E
40|$|Large scale {{computer}} simulations are presented {{to investigate the}} avalanche statistics of sand piles using molecular dynamics. We could show that different methods of <b>measurement</b> <b>lead</b> to contradicting conclusions, presumably due to avalanches not reaching {{the end of the}} experimental table. Keywords: The physics of an evolving sandpile has been of large interest to physicists and engineers and there has been done much work in this field. One of the most popular (or sometimes unpopular) ideas is the concept of self organized criticality (SOC) [1]. It has been argued by many physicists that sandpiles can be described by cellular automata in two or three dimensions (e. g. [2]) and by stochastic cellular automata (e. g. [3]) which in simulations might show SOCbehavior. There are many effects in nature which are supposed to reveal SOC, and hence there is a variety of articles investigating the theory of SOC (e. g. [4]). When particles are dropped one after the other onto the top of a sand hea [...] ...|$|E
40|$|Unpassivated GaN/AlGaN/GaN HEMTs on SiC {{substrate}} {{both with}} intentionally undoped and doped structures with an extremely weak current {{collapse in the}} 50 ns range have been presented. The characterization with gate-lag techniques did not shown relevant DC to RF dispersion and preliminary RF output power <b>measurements</b> <b>lead</b> to expect good RF performances...|$|R
40|$|Application of the path-integral {{approach}} to continuous <b>measurements</b> <b>leads</b> to effective Lagrangians or Hamiltonians {{in which the}} effect of the measurement is taken into account through an imaginary term. We apply these considerations to nonlinear oscillators with use of numerical computations to evaluate quantum limitations for monitoring position in such a class of systems...|$|R
50|$|Note {{that the}} voltage shift {{expressed}} by the Seebeck effect cannot be measured directly, since the measured voltage (by attaching a voltmeter) contains an additional voltage contribution, due to the temperature gradient and Seebeck effect in the <b>measurement</b> <b>leads.</b> The voltmeter voltage is always dependent on relative Seebeck coefficients among the various materials involved.|$|R
40|$|The {{supposedly}} missing {{dark energy}} of the cosmos is found quantitatively in a direct analysis without involving ordi nary energy. The analysis relies on five dimensional Kaluza-Klein spacetime and a Lagrangian constrained by an auxil iary condition. Employing the Lagrangian multiplier method, {{it is found that}} this multiplier is equal to the dark {{energy of the}} cosmos and is given by where E is energy, m is mass, c is the speed of light, and λ is the Lagrangian multiplier. The result is in full agreement with cosmic measurements which were awarded the 2011 Nobel Prize in Physics {{as well as with the}} interpretation that dark energy is the energy of the quantum wave while ordinary energy is the energy of the quantum particle. Conse quently dark energy could not be found directly using our current measurement methods because <b>measurement</b> <b>lead</b> s to wave collapse leaving only the quantum particle and its ordinary energy intact. </p...|$|E
40|$|This work {{presents}} {{an important step}} towards the realisation of sub-millikelvin temperatures in nanoelectronic devices. The ability to reach low millikelvin or even microkelvin temperatures in nanoelectronic devices would open up the chance to discover new physics in various systems. We layout a new approach aimed at cooling nanostructures to microkelvin temperature based on the well established technique of adiabatic nuclear demagnetisation: each device <b>measurement</b> <b>lead</b> incorporates its own, individual nuclear refrigerator, allowing efficient thermal contact to a microkelvin bath. This scheme short-circuits two main bottlenecks of cooling electrons: thermal boundary resistance and electron-phonon coupling. We have addressed the technical challenges and constructed a parallel network of nuclear refrigerators, yielding a prototype which proved to achieve temperatures of � 1 mK simultaneously on ten measurement leads upon demagnetisation. Thus, we have accomplished the first step towards ultracold nanoscale samples. It was also found that a field and ramp rate dependent heat leak limited the performance and hindered us to reach for lower temperatures...|$|E
40|$|Supersymmetric (SUSY) {{explanation}} of {{the discrepancy between the}} measurement of (g- 2) _μ and its SM prediction puts strong upper bounds on the chargino and smuon masses. At the same time, lower experimental limits on the chargino and smuon masses, combined with the Higgs mass <b>measurement,</b> <b>lead</b> to an upper bound on the stop masses. The current LHC limits on the chargino and smuon masses (for not too compressed spectrum) set the upper bound on the stop masses of about 10 TeV. The discovery potential of the future lepton and hadron colliders should lead to the discovery of SUSY if it is responsible for the {{explanation of}} the (g- 2) _μ anomaly. This conclusion follows {{from the fact that the}} upper bound on the stop masses decreases with the increase of the lower experimental limit on the chargino and smuon masses. Comment: 14 pages, 4 figures; v 2 : fig. 2, comments and references added, matches published versio...|$|E
30|$|Assuming a {{density of}} 1.3 g/cm³ (as in Polyaniline) and a {{transfer}} of 2 electrons per 4 aniline monomer units, the electrochemical <b>measurements</b> <b>lead</b> to a nominal average thickness of the Polyaniline-Ag layer of around 50 nm. XPS measurements show that the Silver within these 50 nm only has a nominal average thickness of about 4 nm.|$|R
40|$|Dr. Eric Turkheimer {{focused on}} the nonshared {{environment}} project {{and pointed to the}} nonshared environment and its components as the central problem of human scientific psychology. He described three causal models, applying them to each genetic and environmental causation, and concluded that rough <b>measurements</b> <b>lead</b> to detectable causations, but more precise measurements make the effects harder to detect. |$|R
30|$|The {{presence}} of outliers in CS <b>measurements</b> <b>leads</b> {{to the study}} of robust estimators since the recovered sparse signal is highly affected by the {{presence of}} the large errors in the data. Robust M-estimators bring substantial benefits in this scenario because, rather than relying on classical Gaussian ML estimation, they are based on modeling the contamination noise of the measurements as heavy-tailed process.|$|R
40|$|Abstract: Large scale {{computer}} simulations are presented {{to investigate the}} avalanche statistics of sand piles using molecular dynamics. We could show that different methods of <b>measurement</b> <b>lead</b> to contradicting conclusions, presumably due to avalanches not reaching {{the end of the}} experimental table. Keywords: The physics of an evolving sandpile has been of large interest to physicists and engineers and there has been done much work in this field. One of the most popular (or sometimes unpopular) ideas is the concept of self organized criticality (SOC) [1]. It has been argued by many physicists that sandpiles can be described by cellular automata in two or three dimensions (e. g. [2]) and by stochastic cellular automata (e. g. [3]) which in simulations might show SOCbehavior. There are many effects in nature which are supposed to reveal SOC, and hence there is a variety of articles investigating the theory of SOC (e. g. [4]). When particles are dropped one after the other onto the top of a sand heap one observes avalanches. The time intervals in between successive avalanche...|$|E
40|$|Following Borcherding and Deacon [1972] and Bergstrom and Goodman [1973], a {{sizeable}} empirical literature {{has used the}} median voter model to analyze local spending behaviors. The main innovation in this paper is methodological as the median voter ?s satisfaction is described by a Stone-Geary utility function. Then, {{we are able to}} specify the public spending function in a simplified linear expenditure system, which can be estimated without price measurement. Then, the median voter model offers a price variable which the voter uses to define her demand for local public goods. So, we are able to estimate the specification with a price measurement. We provide new evidence using French municipalities of more than  20, 000  inhabitants. The main results suggest that the minimum public spending value is dominant but decreases with the population size. Estimates oblained with price <b>measurement</b> <b>lead</b> to an overestimation of the preferences parameter and to an underestimalion of the minimum part. Price and income elasticities are greater than values obtained with the traditional method. Demand for local public goods, Median voter, Linear expenditure system...|$|E
40|$|Because of the {{penetrating}} {{ability of}} the radiation used in nuclear medicine, metallic lead is widely used as radiation shielding. However, this shielding may present an insidious health hazard because of the dust that is readily removed from the surfaces of lead objects. The lead dust may become airborne, contami-nate floors and other nearby surfaces, and be inadvertently in-haled or ingested by patients. We determined if the quantity of lead dust encountered within nuclear medicine departments exceeded Environmental Protection Agency (EPA) standards. Methods: For lead dust quantification, professional lead test kits were used to sample fifteen 1 -ft 2 sections of different sur-faces within the department. Four samples were collected once per week from each site. The samples were then submitted to a National Lead Laboratory–accredited program for a total lead <b>measurement.</b> <b>Lead</b> contamination (mg/ft 2) {{for each of the}} 60 samples was compared with the EPA standards for lead dust. Results: Lead contamination was present at 6 of the 15 sites, and of 60 samples, 18 exceeded the EPA standard of 50 mg/ft 2. Conclusion: Lead contamination is present within nu-clear medicine departments, and corrective measures should be considered when dealing with pediatric patients. A larger se-ries needs to be conducted to confirm these findings...|$|E
40|$|Non-normal {{variation}} across repeated <b>measurements</b> <b>leads</b> to nonlinear and heteroskedastic regression to {{the mean}} unlike the simple linear and homoskedastic regression to the mean found in normal models. This paper investigates {{the nature of}} the regression to the mean phenomenon in non-normal settings using (a) small variance approximations and (b) exact results obtained using normal mixtures to approximate non-normal distributions. ...|$|R
40|$|Application of the path-integral {{approach}} to continuous <b>measurements</b> <b>leads</b> to effective Lagrangians or Hamiltonians {{in which the}} effect of the measurement is taken into account through an imaginary term. We apply these considerations to nonlinear oscillators with use of numerical computations to evaluate quantum limitations for monitoring position in such a class of systems. Comment: 10 pages, REVTeX 3. 0, 4 PostScript figure...|$|R
40|$|Abstract—A {{discussion}} is given {{on the use}} of electrical double layer measurements as a tool for the study of polymer conformations in the adsorbed state. The emphasis is on polymers at solid particles. Combination of data from electrophoresis and direct surface charge <b>measurements</b> <b>leads</b> to information on the polymer segment distribution between trains and loops. The silver iodide-polyvinylalcohol system is chosen as the main example...|$|R
40|$|It is {{important}} to maintain a posture via equilibrial and protective reactions in all activities of daily living. There is normal neurological finding in upper extremities in patients with a spinal cord injuries in the area from Th 1 to Th 7, muscles of the thorax are paralysed under {{the level of the}} lesion and when there is a complete 2 damage of the upper pectoral spinal cord, abdominal muscles are always paralysed. There is an important part in a therapy of patients with a spinal cord lesion, which consists of an activation of the postural system by means of sensomotory stimulation exercise according to Janda called sitting on a cylinder. The aim {{of this study is to}} objectively evaluate the impact of the exercise on a cylinder on postural system of patients with lesion of postural muscles. Surface electromyography was chosen as the most suitable objective method. Special set of testing movements on a deckchair or on a cylinder was created. Selected patients were tested by exercises on a deckchair or on a cylinder before a stay in the centre. After a ten-day stay the patients are tested by the same set of exercises and evaluation of their muscular activities is performed. Results of our <b>measurement</b> <b>lead</b> to a conclusion, that sensomotory stimulation is valuable part of kinesiotherapy in patients with SCI in [...] ...|$|E
40|$|The {{dynamics}} of excited-state intramolecular enol-keto proton-transfer tautomerism in 10 -hydroxybenzo-quinoline (HBQ) and its deuterated analogue (DBQ) {{have been investigated}} by steady-state absorption and fluorescence spectroscopy, femtosecond fluorescence upconversion in combination with pump-probe transient absorption experiments in nonpolar solvents. In cyclohexane, the time scale for both proton and deuterium transfer in the excited state cannot be resolved under the response limit of ca. 160 and 200 fs, respectively, of our current upconversion and transient absorption systems. The initially prepared keto tautomer is in a higher lying excited state, possibly the S′ 2 state (prime indicates the keto-tautomer form) which then undergoes a 330 fs S′ 2 f S′ 1 internal conversion, resulting in a highly vibrationally excited S′ 1 state. Subsequently, a solvent-induced vibrational relaxation {{takes place in a}} time scale of 8 - 10 ps followed by a relatively much longer, thermally cooled S′ 1 f S′ 0 decay rate of 3. 3 109 s- 1 (ôf 300 ps- 1). The results in combination with extremely weak enol fluorescence resolved from the steady-state <b>measurement</b> <b>lead</b> us to conclude that excited-state intramolecular proton transfer (ESIPT) is essentially barrierless. The rate of ESIPT upon 385 - 405 nm excitation may be determined within the period of low-frequency, large-amplitude vibrations incorporating the motion of atoms associated with the hydrogen bond...|$|E
40|$|The {{harmonic}} impedance spectra (HIS) of a time-varying bioimpedance Z(ω, t) is a {{new tool}} to better understand and describe complex time-varying biological systems with a distinctive periodic character as, for example,cardiovascular and respiratory systems. In this paper, {{the relationship between the}} experimental setup and the identification framework for estimating Z(ω, t) is set up. The theory developed applies to frequency response based impedance measurements from noisy current–voltage observations. We prove theoretically and experimentally that a voltage source (VS) and a current source (CS) analogue front end-based <b>measurement</b> <b>lead,</b> respectively, to a closed-loop and an open-loop HIS identification problem. Next, we delve into the estimation of the HIS by treating Z(ω, t), on the one hand, as a linear time-invariant (LTI) system within a short time window; and, on the other hand, as a linear periodically time-varying (PTV) system within the entiremeasurement interval. The LTI approach is based on the short-time Fourier transform (STFT), while the PTV approach relies on the information that is present in the skirts of the voltage and/or current spectra. In addition, direct and indirect methods are developed for estimating the HIS by using simple as well as more sophisticated techniques. Ultimately, theHIS and their uncertainty bounds are estimated from real measurements conducted on a periodically varying dummy impedance. Postprint (published version...|$|E
50|$|Moderate {{exercise}} before BIA <b>measurements</b> <b>lead</b> to an overestimation of fat-free {{mass and}} an underestimation {{of body fat}} percentage due to reduced impedance. For example, moderate intensity exercise for 90-120 minutes before BIA measurements causes nearly a 12 kg overestimation of fat-free mass, i.e. body fat is significantly underestimated. Therefore, it is recommended not to perform BIA for several hours after moderate or high intensity exercise.|$|R
40|$|We {{propose a}} new scheme aimed at cooling {{nanostructures}} to microkelvin temperature based on thewell established technique of adiabatic nuclear demagnetization: we attach each devicemeasurement {{lead to an}} individual nuclear refrigerator, allowing efficient thermal contact to amicrokelvin bath. On a prototype consisting of a parallel network of nuclear refrigerators,temperatures of 1 mK simultaneously on ten <b>measurement</b> <b>leads</b> have been reached upondemagnetization, thus completing the first steps toward ultracold nanostructures...|$|R
40|$|Misra and Sudarshan pointed out, {{based on}} the quantum {{measurement}} theory, that repeated <b>measurements</b> <b>lead</b> to a slowing down of the transition, which they called the quantum Zeno effect. Recently, Itano, Heinzen, Bollinger and Wineland have reported that they succeeded in observing that effect. We show {{that the results of}} Itano et al. can be recovered through conventional quantum mechanics and do not involve a repeated reduction of the wave functioninfo:eu-repo/semantics/publishe...|$|R
