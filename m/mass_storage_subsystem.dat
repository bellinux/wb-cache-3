7|1883|Public
5000|$|Designed to {{minimize}} the amount of CPU involvement, the protocol depends upon two queues. Into one queue are placed packets which fully describe the commands to be executed by the <b>mass</b> <b>storage</b> <b>subsystem.</b> To initiate an I/O request, the CPU has only to create a small data structure in memory, append it to a [...] "send" [...] queue, {{and if that is}} the first packet in the send queue, wake the MSCP controller. After the command has been executed, an appropriate status packet is placed into the second queue to be read by the CPU.|$|E
5000|$|Each CI cable {{connected}} to its computer via a CI Port, which could {{send and receive}} packets without any CPU involvement. To send a packet, a CPU had only to create a small data structure in memory and append it to a [...] "send" [...] queue; similarly, the CI Port would append each incoming message to a [...] "receive" [...] queue. Tests showed that a VAX-11/780 could send and receive 3000 messages per second, {{even though it was}} nominally a 1-MIPS machine. The closely related Mass Storage Control Protocol (MSCP) allowed similarly high performance from the <b>mass</b> <b>storage</b> <b>subsystem.</b> In addition, MSCP packets were very easily transported over the CI allowing remote access to storage devices.|$|E
40|$|The authors {{present the}} design, implementation, and {{utilization}} {{of a large}} <b>mass</b> <b>storage</b> <b>subsystem</b> (MSS) for the numerical aerodynamics simulation. The MSS supports a large networked, multivendor Unix-based supercomputing facility. The MSS at Ames Research Center provides all processors on the numerical aerodynamics system processing network, from workstations to supercomputers, the ability to store large amounts of data in a highly accessible, long-term repository. The MSS uses Unix System V and is capable of storing {{hundreds of thousands of}} files ranging from a few bytes to 2 Gb in size...|$|E
40|$|IBM Storage Tank ™ {{provides}} a complete storage management solution in a heterogeneous distributed environment. IBM Storage Tank {{is designed to}} provide performance that is comparable to that of file systems built on bus-attached, high-performance storage. In addition to high performance, the goal of IBM Storage Tank is to provide high availability, increased scalability, and centralized, automated storage and data management. Storage Area Network Technology Storage Area Network (SAN) technology allows an enterprise to connect large numbers of devices, including clients, servers, and <b>mass</b> <b>storage</b> <b>subsystems,</b> to a high-performance network. On a SAN, clients can access large volumes of data directly from storage devices, using high-speed, low-latency connections. IBM Storage Tank is designed to be independent of the actual SAN fabric technology. IBM Storage Tank will work with Fibre Channel networks as well as new emerging storage networking technologies such as Gigabit Ethernet (iSCSI) and Infiniband. By using SAN technology, IBM Storage Tank can meet the needs of general data sharing in a distributed environment, as well as the needs of special, data-intensive applications, such as imaging, animation, digital video, and large-scale distributed applications...|$|R
40|$|ATLAS is {{a general}} purpose High Energy Physics {{experiment}} at the Large Hadron Collider (LHC). The trigger/data acquisition (TDAQ) system is designed to handle the extremely high data rates. The three level ATLAS trigger system (Level 1, Level 2 and Event Filter) reduces the data rate from 40 MHz bunch crossing down to ~ 200 Hz. The DAQ system is designed to transport data at different trigger levels to the <b>mass</b> <b>storage,</b> with main <b>subsystems</b> including the dataflow, a combination of custom design components and commodity processors running multithread software applications and connected by gigabit Ethernet, the online software responsible for the configuration, control and information sharing of the system, and the monitoring software responsible for the data quality assurance. The system architecture will be overviewed. The commissioning in situ with detectors will be discussed. Results on system functionality and performance based on the cosmic data and simulated events will be presented. The early experience on LHC beam in 2008 will also be shown...|$|R
40|$|NASDA's new Advanced Earth Observing Satellite (ADEOS) is {{scheduled}} for launch in August, 1996. ADEOS carries 8 sensors to observe earth environmental phenomena and sends their data to NASDA, NASA, and other foreign ground stations around the world. The downlink data bit rate for ADEOS is 126 MB/s and the total volume of data is about 100 GB per day. To archive and manage such a large quantity of data with high reliability and easy accessibility {{it was necessary to}} develop a new <b>mass</b> <b>storage</b> system with a catalogue information database using advanced database management technology. The data will be archived and maintained in the Master Data <b>Storage</b> <b>Subsystem</b> (MDSS) which is one subsystem in NASDA's new Earth Observation data and Information System (EOIS). The MDSS is based on a SONY ID 1 digital tape robotics system. This paper provides an overview of the EOIS system, with a focus on the Master Data <b>Storage</b> <b>Subsystem</b> and the NASDA Earth Observation Center (EOC) archive policy for earth observation satellite data...|$|R
40|$|As {{a result}} of {{advances}} related to the combination of computer system technology and numerical modeling, computational aerodynamics has emerged as an essential element in aerospace vehicle design methodology. NASA has, therefore, initiated the Numerical Aerodynamic Simulation (NAS) Program with the objective to {{provide a basis for}} further advances in the modeling of aerodynamic flowfields. The Program is concerned with the development of a leading-edge, large-scale computer facility. This facility is to be made available to Government agencies, industry, and universities as a necessary element in ensuring continuing leadership in computational aerodynamics and related disciplines. Attention is given to the requirements for computational aerodynamics, the principal specific goals of the NAS Program, the high-speed processor subsystem, the workstation subsystem, the support processing subsystem, the graphics subsystem, the <b>mass</b> <b>storage</b> <b>subsystem,</b> the long-haul communication subsystem, the high-speed data-network subsystem, and software...|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimited. This paper documents {{the development of}} a statistical analysis package for the TRS- 80 microcoraputer. The package is comprised of six interactive programs which are generally divided into topical areas. The major emphasis is on exploratory data analysis and statistical inference, however, probability and inverse probability distributions are also included. The programming language is TRS- 80 Level II BASIC enhanced by the input/output commands available through the ESF- 80 (Exatron Stringy Floppy) <b>mass</b> <b>storage</b> <b>subsystem.</b> With the modification of these few commands, the package is compatible with most floppy disk operating systems designed for the TRS- 80 Model I or Model III microcomputers. This statistical analysis capability implemented on a relatively inexpensive system provides a useful tool to the student or the trained analyst without ready access to a mainframe computer system. Major, United States Marine Corp...|$|E
40|$|With {{the growing}} {{popularity}} of storage area networks (SANs) and clustered, shared file systems, the file system is becoming a distinct and critical part of a system environment. Because the file system mitigates access to data on a <b>mass</b> <b>storage</b> <b>subsystem,</b> it has certain behavioral and functional characteristics that affect I/O performance from an application and/or system point of view. Measuring file system performance is significantly more complicated than that of the underlying disk subsystem because of the many types of higher-level operations that can be performed (allocations, deletions, directory searches, [...] . etc.). The tasks of measuring and characterizing the performance of a file system is further complicated by SANs and emerging clustering technologies that add a distributed aspect to the file systems themselves. Similarly, as the cluster/SAN grows in size, so does the task of performance measurement. The objective {{of this study is to}} identify some of the more significant issues involved with file system benchmarking in a highly scalable clustered environment...|$|E
40|$|Nickel-cadmium batteries, bipolar nickel-hydrogen batteries, and {{regenerative}} {{fuel cell}} <b>storage</b> <b>subsystems</b> were evaluated for use as the <b>storage</b> <b>subsystem</b> in a 37. 5 kW power system for space station. Design requirements were set {{in order to establish}} a common baseline for comparison purposes. The <b>storage</b> <b>subsystems</b> were compared on the basis of effective energy density, round trip electrical efficiency, total subsystem weight and volume, and life...|$|R
40|$|Building {{reliable}} storage systems {{becomes increasingly}} challenging as {{the complexity of}} modern storage systems continues to grow. Understanding storage failure characteristics is crucially important for designing and building a reliable storage system. While several recent {{studies have been conducted}} on understanding storage failures, almost all of them focus on the failure characteristics of one component – disks – and do not study other storage component failures. This paper analyzes the failure characteristics of <b>storage</b> <b>subsystems.</b> More specifically, we analyzed the storage logs collected from about 39, 000 storage systems commercially deployed at various customer sites. The data set covers a period of 44 months and includes about 1, 800, 000 disks hosted in about 155, 000 storage shelf enclosures. Our study reveals many interesting findings, providing useful guideline for designing reliable storage systems. Some of our major findings include: (1) In addition to disk failures that contribute to 20 - 55 % of <b>storage</b> <b>subsystem</b> failures, other components such as physical interconnects and protocol stacks also account for significant percentages of <b>storage</b> <b>subsystem</b> failures. (2) Each individual <b>storage</b> <b>subsystem</b> failure type and <b>storage</b> <b>subsystem</b> failure as a whole exhibit strong selfcorrelations. In addition, these failures exhibit “bursty” patterns. (3) <b>Storage</b> <b>subsystems</b> configured with redundant interconnects experience 30 - 40 % lower failure rates than those with a single interconnect. (4) Spanning disks of a RAID group across multiple shelves provides a more resilient solution for <b>storage</b> <b>subsystems</b> than within a single shelf...|$|R
5000|$|Keep data on {{multiple}} related volumes consistent across <b>storage</b> <b>subsystems</b> ...|$|R
40|$|Approved {{for public}} release; {{distribution}} is unlimitedWith {{the shift from}} batch applications to online systems supporting the strategic role of information, corporate or institutional goals tie directly to the information management functions. This has been true at the Naval Postgraduate School (NPS). Like many other Government installations, the NPS Computer Center has to meet its objectives with less than state-of-the-art hardware. In the early 1980 's, the Center employed IBM's 3850 <b>Mass</b> <b>Storage</b> <b>Subsystem</b> (MSS) for online storage of student and faculty data sets. It was installed in December 1980 and performed well for over six years. Faced with IBM's announcement (in February 1985) of the limited future connectivity and compatibility and the increasing maintenance costs, {{the decision was made}} to replace the MSS with hardware/software alternative that would use a more modern and reliable architecture. The objective of this thesis is to define the solution, and data set migration process, and describe the early experience with a multi-level, software-managed, storage system. [URL]...|$|E
40|$|The {{performance}} of <b>storage</b> <b>subsystems</b> has not followed the rapid improvements in processors technology, despite the increased capacity and density in storage medium. Here, we {{introduce a new}} model {{based on the idea}} of enhancing the I/O subsystem controller capabilities by dynamic load balancing on a <b>storage</b> <b>subsystem</b> of multiple disk drives...|$|R
40|$|This paper {{describes}} the functionality of a point-in-time copy facility and describes both {{the benefits and}} drawbacks of providing this facility on the <b>storage</b> <b>subsystem.</b> While there are other benefits, the biggest benefit of providing this facility on the <b>storage</b> <b>subsystem</b> is performance; we do not needlessly add load to the storage network or host as part of making the copy. The biggest drawback is that the <b>storage</b> <b>subsystem</b> in today's world is only aware of data {{at the level of}} logical units and blocks; this makes it hard to meaningfully perform copies at a granularity of less than an entire logical uni...|$|R
25|$|In 2004, a new <b>storage</b> <b>subsystem</b> was {{developed}} and named FSFS.|$|R
50|$|Monitor {{performance}} metrics {{across multiple}} <b>storage</b> <b>subsystems</b> {{from a single}} console.|$|R
5000|$|... 2004 Announced VTrak, the world's first dual-port, HW iSCSI RAID <b>storage</b> <b>subsystem.</b>|$|R
40|$|The Data <b>Storage</b> <b>Subsystem</b> Simulator (DSSSIM) {{simulating}} (by ground software) {{occurrence of}} discrete {{events in the}} Voyager mission is described. Functional requirements for Data <b>Storage</b> <b>Subsystems</b> (DSS) simulation are discussed, and discrete event simulation/DSSSIM processing is covered. Four types of outputs associated with a typical DSSSIM run are presented, and DSSSIM limitations and constraints are outlined...|$|R
40|$|Due to {{the growing}} {{popularity}} of emerging applications such as digital libraries, Video-On Demand, distance learning, and Internet World-Wide Web, multimedia servers with a large capacity and high performance <b>storage</b> <b>subsystem</b> are in high demand. Serial storage interfaces are emerging technologies designed to improve the performance of such <b>storage</b> <b>subsystems.</b> They provide high bandwidth, fault tolerance, fair bandwidth sharing and long distance connection capability. All of these issues are critical in designing a scalable and high performance <b>storage</b> <b>subsystem.</b> Some of the serial storage interfaces provide the spatial reuse feature which allows multiple concurrent transmissions. That is, multiple hosts can access disks concurrently with full link bandwidth if their access paths are disjoint. Spatial reuse provides a way to build a <b>storage</b> <b>subsystem</b> whose aggregate bandwidth may be scaled up with the number of hosts. However, {{it is not clear how}} much the performance of a storage subsy [...] ...|$|R
5000|$|Function {{to allow}} PowerHA/XD PPRC to support {{multiple}} <b>storage</b> <b>subsystems</b> {{at each site}} ...|$|R
40|$|In this paper, we {{describe}} a multimedia system architecture consisting of: (1) an information management <b>subsystem,</b> (2) a <b>storage</b> <b>subsystem,</b> and (3) a network subsystem. Whereas the information management subsystem provides means for identifying {{the set of}} multimedia objects that may be pertinent to a client's query, the <b>storage</b> <b>subsystem</b> ensures that multimedia objects are efficiently stored and retrieved from secondary storage devices. The network subsystem, on the other hand, guarantees timely delivery of the multimedia objects accessed by the <b>storage</b> <b>subsystem</b> {{to each of the}} client sites. The main goal {{of this paper is to}} identify and discuss the research issues involved in designing each of these three subsystems...|$|R
50|$|There are 3 {{subsystems}} in the BluOnyx: The <b>Storage</b> <b>Subsystem,</b> The Application Processing Subsystem and the Connectivity Subsystem.|$|R
5000|$|... 2007 Announced the VTrak E-Class, {{enterprise-class}} external <b>storage</b> <b>subsystem</b> and {{the first}} DLNA Network Attached Storage - SmartStor (NS4300).|$|R
40|$|This report {{describes}} {{the results of}} a study that investigated the synergy between electrochemical capacitors (ECs) and flywheels, in combination {{with each other and with}} batteries, as energy <b>storage</b> <b>subsystems</b> in photovoltaic (PV) systems. EC and flywheel technologies are described and the potential advantages and disadvantages of each in PV energy <b>storage</b> <b>subsystems</b> are discussed. Seven applications for PV energy <b>storage</b> <b>subsystems</b> are described along with the potential market for each of these applications. A spreadsheet model, which used the net present value method, was used to analyze and compare the costs over time of various system configurations based on flywheel models. It appears that a synergistic relationship exists between ECS and flywheels. Further investigation is recommended to quantify the performance and economic tradeoffs of this synergy and its effect on overall system costs...|$|R
40|$|The InTENsity PowerWall is {{a display}} system used for {{high-resolution}} visualization of very large volumetric data sets. The display {{is linked to}} two separate computing environments consisting {{of more than a}} dozen computer systems. Linking these systems is a common shared <b>storage</b> <b>subsystem</b> that allows a great deal of flexibility in the way visualization data can be generated and displayed. These visualization applications demand very high bandwidth performance from the <b>storage</b> <b>subsystem</b> and associated file system...|$|R
40|$|In {{computer}} systems today, speed and responsiveness is often determined by network and <b>storage</b> <b>subsystem</b> performance. Faster, more scalable networking interfaces like Fibre Channel and Gigabit Ethernet provide the scaffolding from which higher performance {{computer systems}} implementations may be constructed, but new thinking is required about how machines interact with network-enabled storage devices. In this paper we describe how we implemented journaling in the Global File System (GFS), a shared-disk, cluster file system for Linux. Our previous three papers on GFS at the <b>Mass</b> <b>Storage</b> Symposium discussed our first three GFS implementations, their performance, {{and the lessons}} learned. Our fourth paper describes, appropriately enough, the evolution of GFS version 3 to version 4, which supports journaling and recovery from client failures. In addition, GFS scalability tests extending to 8 machines accessing 8 4 -disk enclosures were conducted: these tests showed good scaling. W [...] ...|$|R
40|$|To {{address the}} {{limitations}} of OpenNebula <b>storage</b> <b>subsystems,</b> we have designed and developed an extension {{that is capable of}} achieving higher I/O throughput than the prior <b>subsystems.</b> The semi-shared <b>storage</b> <b>subsystem</b> uses central and distributed resources at the same time. Virtual machine instances with high availability requirements can run directly from central storage while other virtual machines can use local resources. As I/O performance measurements show, this technique can decrease I/O load on central storage by using local resources of host machines...|$|R
50|$|Allow {{administrators to}} monitor metrics, such as I/O rates and cache utilization, and support {{optimization}} of storage through {{the identification of}} the best LUNs across multiple <b>storage</b> <b>subsystems.</b>|$|R
50|$|The first {{implementation}} of FlashCopy, Version 1 allowed entire volumes to be instantaneously “copied” to another volume {{by using the}} facilities of the newer Enterprise <b>Storage</b> <b>Subsystems</b> (ESS).|$|R
50|$|TPC for Replication is {{designed}} to control and monitor copy services operations in storage environments. It also provides advanced copy services functions for supported <b>storage</b> <b>subsystems</b> on the SAN.|$|R
50|$|The <b>Storage</b> <b>Subsystem</b> {{can be made}} {{of either}} Hard Disk Drives or Flash chips. The BluOnyx is {{expected}} to come in 1GB, 2GB, 4GB, 10GB and 40GB storage sizes.|$|R
50|$|He joined Google in October 2005 {{and was a}} {{founding}} member of the Google Gears team, responsible for designing the client side browser local <b>storage</b> <b>subsystem</b> and JavaScript interface bindings.|$|R
50|$|Vess <b>storage</b> <b>subsystems</b> are {{designed}} for small to medium business and organizations of all sizes. The Vess storage system is used in surveillance applications and can handle unstructured data, disk-to-disk backup.|$|R
50|$|This {{component}} discovers, gathers information from, analyzes performance of, {{and controls}} <b>storage</b> <b>subsystems</b> and SAN fabrics. It coordinates communication with {{and data collection}} from agents that scan SAN fabrics and storage devices.|$|R
5000|$|DSS270 disk <b>storage</b> <b>subsystem</b> {{provided}} up to 20 modules of head-per-track disk. Capacity per module was 15.3 million characters. Average {{access time}} was 26 ms, and maximum transfer rate was 333,000 cps.|$|R
