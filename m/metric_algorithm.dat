41|475|Public
40|$|Accurate {{estimation}} of evapotranspiration (ET) {{is essential for}} hydrological modeling and efficient crop water management in hyper-arid climates. In this study, we applied the <b>METRIC</b> <b>algorithm</b> on Landsat- 8 images, acquired from June to October  2013, for the mapping of ET of a 50  ha center-pivot irrigated alfalfa field in the eastern region of Saudi Arabia. The METRIC-estimated energy balance components and ET were evaluated against the data provided by an eddy covariance (EC) flux tower installed in the field. Results indicated that the <b>METRIC</b> <b>algorithm</b> provided accurate ET estimates over the study area, with RMSE values of 0. 13 and 4. 15  mm d − 1. The <b>METRIC</b> <b>algorithm</b> was observed to perform better in full canopy conditions compared to partial canopy conditions. On average, the <b>METRIC</b> <b>algorithm</b> overestimated the hourly ET by 6. 6...|$|E
3000|$|Confidence <b>Metric</b> <b>Algorithm</b> 1 {{uses the}} {{difference}} between the channel power and the predefined threshold, [...]...|$|E
30|$|Confidence <b>Metric</b> <b>Algorithm</b> 2 {{uses the}} square of the {{difference}} between the channel power and the predefined threshold.|$|E
40|$|A new {{criterion}} {{is introduced}} for comparing the convergence properties of variable <b>metric</b> <b>algorithms,</b> focusing on stepwise descent properties. This criterion is a bound on {{the rate of}} decrease in the function value at each iterative step (single-step con-vergence rate). Using this criterion {{as a basis for}} algorithm development leads to the introduction of variable coefficients to rescale the objective function at each itera-tion, and, correspondingly, to a new class of variable <b>metric</b> <b>algorithms.</b> Effective scaling can be implemented by restricting the parameters in a two-parameter family of variable <b>metric</b> <b>algorithms.</b> Conditioins are derived for these parameters that guarantee monotonic improvement in the single-step convergence rate. These conditions are obtained by analyzing the eigenvalue structure of the associated inverse Hessian approximations. 1...|$|R
40|$|The {{parallel}} variable <b>metric</b> optimization <b>algorithms</b> of Straeter (1973) and van Laarhoven (1985) are reviewed, and {{the possible}} drawbacks of the algorithms are noted. By including Davidon (1975) projections in the variable metric updating, researchers can generalize Straeter's algorithm to a family of parallel projected variable <b>metric</b> <b>algorithms</b> which do not suffer the above drawbacks and which retain quadratic termination. Finally researchers consider the numerical performance of {{one member of the}} family on several standard example problems and illustrate how the choice of the displacement vectors affects the performance of the algorithm...|$|R
40|$|Auxiliary problem {{principle}} and inexact variable <b>metric</b> forward-backward <b>algorithm</b> for minimizing {{the sum of}} a differentiable function and a convex function Jean-Philippe Chancelier To cite this version: Jean-Philippe Chancelier. Auxiliary problem {{principle and}} inexact variable <b>metric</b> forward-backward <b>algorithm</b> for minimizing the sum of a differentiable function and a convex function...|$|R
30|$|For higher Pd, the {{confidence}} <b>metric</b> <b>algorithm</b> gives better results. In case of spectrum sensing, higher Pd {{is more important}} than lower Pf, as in case of a missed detection this will lead to collision with the primary user, which is unacceptable for CR systems.|$|E
40|$|Variable <b>Metric</b> <b>Algorithm</b> for Constrained Optimization (VMACO) is {{nonlinear}} {{computer program}} developed to calculate least value of function of n variables subject to general constraints, both equality and inequality. First set of constraints equality and remaining constraints inequalities. Program utilizes iterative method in seeking optimal solution. Written in ANSI Standard FORTRAN 77...|$|E
40|$|This paper {{deals with}} a new {{variable}} <b>metric</b> <b>algorithm</b> for stochastic optimization problems. The essence of this is as follows: there exist two stochastic quasigradient algorithms working simultaneously - {{the first in the}} main space, the second with respect to the matrices that modify the space variables. Almost sure convergence of the algorithm is proved for the case of the convex (possibly nonsmooth) objective functio...|$|E
40|$|Abstract. This paper {{gives some}} {{of the history of}} the {{conjugate}} gradient and Lanczos algorithms and an annotated bibliography for the period 1948 - 1976. Key words, conjugate gradient algorithm, Lanczos <b>algorithm,</b> variable <b>metric</b> <b>algorithms</b> AMS(MOS) subject classifications. 65 F 10, 65 F 15, 65 H 10, 65 F 03 1. Introduction. Th...|$|R
40|$|We {{initiate}} a study comparing {{effectiveness of the}} transformed spaces learned by recently proposed supervised, and semisupervised <b>metric</b> learning <b>algorithms</b> to those generated by previously proposed unsupervised dimensionality reduction methods (e. g., PCA). Through a variety of experiments on different realworld datasets, we find IDML-IT, a semisupervised <b>metric</b> learning <b>algorithm</b> {{to be the most}} effective. ...|$|R
50|$|The K-nearest {{neighbor}} classification {{performance can}} often be significantly improved through (supervised) <b>metric</b> learning. Popular <b>algorithms</b> are neighbourhood components analysis and large margin nearest neighbor. Supervised <b>metric</b> learning <b>algorithms</b> use the label information {{to learn a new}} metric or pseudo-metric.|$|R
40|$|In {{the search}} {{technique}} presented, {{the number of}} function evaluations is reduced by making use of the derivative of the function. The search method is combined with the variable <b>metric</b> <b>algorithm</b> reported by Davidon (1959) to solve a 17 -parameter optimal atmospheric flight of a space shuttle vehicle. In {{the case of the}} problem, the proposed method requires one or two function evaluations per iteration less than the standard cubic fit-golden section method...|$|E
40|$|The Constrained Variable <b>Metric</b> <b>Algorithm</b> {{is chosen}} to {{minimize}} the objective function (cost) {{in the design of}} a natural draft dry cooling tower. An existing cooling system design that has specific performance characteristics under prescribed operating conditions is selected as a reference unit. By changing design variables, but not exceeding prescribed constraints, a more cost-effective design is achieved. The influence of various parameters, and the sensitivity of the objective function to these parameters, are evaluated...|$|E
30|$|In case {{of lower}} {{probability}} of false alarm, using confidence <b>metric</b> <b>algorithm</b> gives a worse performance than the normal algorithm. This observation may {{vary according to}} the values of the chosen thresholds. In case we choose different threshold values, we could end up with the algorithm being better in case of lower probability of false alarm. The optimal calculation of the thresholds is out of scope of this study and could be added in the future study.|$|E
40|$|This paper {{deals with}} new {{variable}} <b>metric</b> <b>algorithms</b> for nonsmooth optimization problems, so-called “adaptive algorithms”. The essence {{of these are}} as follows: there are two simultaneously working gradient algorithms, {{the first in the}} main space, the second with respect to the matrices that modify the space variables. The convergence theorems for these algorithms are given for different cases...|$|R
50|$|The SEBAL and <b>METRIC</b> <b>algorithms</b> {{solve the}} energy balance at the earth's surface using {{satellite}} imagery. This allows for both actual and potential evapotranspiration {{to be calculated}} on a pixel-by-pixel basis. Evapotranspiration is a key indicator for water management and irrigation performance. SEBAL and METRIC can map these key indicators in time and space, for days, weeks or years.|$|R
40|$|International audienceThe {{notion of}} quasi-Fejér {{monotonicity}} {{has proven to}} be an efficient tool to simplify and unify the convergence analysis of various algorithms arising in applied nonlinear analysis. In this paper, we extend this notion in the context of variable <b>metric</b> <b>algorithms,</b> whereby the underlying norm is allowed to vary at each iteration. Applications to convex optimization and inverse problems are demonstrated...|$|R
40|$|This paper {{presents}} Differential Evolution with... jective Optimization algorithm (DECMOSA-SQP), {{which uses}} the self-adaptation mechanism from DEMOwSA algorithm presented at CEC 2007 and a SQP local search. The constrained handling mechanism is also {{incorporated in the}} new algorithm. Assessment of the algorithm using CEC 2009 special session and competition on constrained multiobjective optimization test functions is presented. The functions are composed of unconstrained and constrained problems. Their results are assessed using the IGD metric. Based on this <b>metric,</b> <b>algorithm</b> strengths and weaknesses are discussed...|$|E
30|$|Different sets of {{research}} mainly {{focus on one}} variable time series now, while researches involving multivariate time series have been insufficient. In this paper, combined linear segments and fitting error for multivariate time series, we present a new method to reduce the time complexity of DTW distance <b>metric</b> <b>algorithm.</b> Based on the shape feature and the tilt angle, we propose a new approach for similarity matching of DTW multivariate time series. Experimental results demonstrate that this method is helpful for ensuring accuracy and for reducing the time complexity of similarity matching.|$|E
40|$|Mesh is {{a network}} {{topology}} that achieves high throughput and stable intercommunication. With great potential, {{it is expected}} to be the key architecture of future networks. Wireless sensor networks are an active research area with numerous workshops and conferences arranged each year. The overall performance of a WSN highly depends on the energy consumption of the network. This paper designs a new routing metric for wireless mesh sensor networks. Results from simulation experiments reveal that the new <b>metric</b> <b>algorithm</b> improves the energy balance of the whole network and extends the lifetime of wireless mesh sensor networks (WMSNs) ...|$|E
40|$|Many of the {{problems}} in mathematical economics and game theory may be reduced to the investigation of a generalized equation with a multivalued right-hand side. This paper deals with methods for solving generalized equations. The author has developed a new approach to the construction of variable <b>metric</b> <b>algorithms</b> for these equations. The convergence of the suggested algorithm is proved for X*-antimonotone multivalued maps...|$|R
40|$|The {{notion of}} quasi-Fejér {{monotonicity}} {{has proven to}} be an efficient tool to simplify and unify the convergence analysis of various algorithms arising in applied nonlinear analysis. In this paper, we extend this notion in the context of variable <b>metric</b> <b>algorithms,</b> whereby the underlying norm is allowed to vary at each iteration. Applications to convex optimization and inverse problems are demonstrated...|$|R
40|$|Abstract: This paper {{introduces}} a supervised <b>metric</b> learning <b>algorithm,</b> called kernel density metric learning (KDML), which {{is easy to}} use and provides nonlinear, probability-based distance measures. KDML constructs a direct nonlinear mapping from the original input space into a feature space based on kernel density estimation. The nonlinear mapping in KDML embodies established distance measures between probability density functions, and leads to correct classification on datasets for which linear metric learning methods would fail. Existing <b>metric</b> learning <b>algorithms,</b> such as large margin nearest neighbors (LMNN), can then be applied to the KDML features to learn a Mahalanobis distance. We also propose an integrated optimization algorithm that learns not only the Mahalanobis matrix but also kernel bandwidths, the only hyper-parameters in the nonlinear mapping. KDML can naturally handle not only numerical features, but also categorical ones, which is rarely found in previous <b>metric</b> learning <b>algorithms.</b> Extensive experimental results on various benchmark datasets show that KDML significantly improves existing <b>metric</b> learning <b>algorithms</b> in terms of kNN classification accuracy...|$|R
40|$|Powell (1977, 1978), Biggs (1972, 1975), and Han (1976, 1977) have {{developed}} a class of variable metric methods which create an explicit, quadratic, subproblem {{which is to be}} solved for finding a search direction for design improvement. A one-dimensional search is then performed. The present paper has the objective to present this variable metric approach in the context of structural synthesis. The variable <b>metric</b> <b>algorithm</b> is modified for application to the structural synthesis problem. The application of the new procedure is illustrated with the aid of examples, taking into account a 10 -bar planar truss, a 17 -bar planar tower, and a cantilever beam...|$|E
40|$|The {{similarity}} metric {{in traditional}} content based 3 D model retrieval method mainly refers the distance <b>metric</b> <b>algorithm</b> used in 2 D image retrieval. But this method will limit the matching breadth. This paper proposes a new retrieval matching method based on case learning {{to enlarge the}} retrieval matching scope. In this method, the shortest path in Graph theory is {{used to analyze the}} similarity how the nodes on the path between query model and matched model effect. Then, the label propagation method and k nearest-neighbor method based on case learning is studied and used to improve the retrieval efficiency based on the existing feature extraction...|$|E
40|$|Abstract: The triad {{sequence}} distributions of-EEE-,-EEP-,-EPE-,-PEP-,-PPE- and-PPP- of Ethylene-Propylene Rubber (EPR) {{has been}} determined by infrared spectroscopy (IR) combined with 13 C NMR and partial least squares (PLS) chemo <b>metric</b> <b>algorithm.</b> The triad sequence distributions obtained by 13 C NMR was used as reference date. PLS regression models of the sequence distributions of EPR have been developed by MIR range. The wave number range of IR was 1300 ~ 600 cm- 1 and the spectra were pretreated through smoothing and second derivation. The results showed the sequence distributions predicted by models were consisted with the data from 13 C NMR. The triad sequence distributions of EPR could be fastly analyzed by IR models. 1...|$|E
40|$|NLP {{datasets}} {{are high}} dimensional. • Reliable parameter estimation from high dimensional data is hard. • Better data representation {{is needed for}} effective learning! Existing Solution: 1. Project data into a lower dimensional space using unsupervised dimensionality reduction methods (e. g. PCA, NMF). 2. Perform learning in the lower dimensional space. • However, {{a limited amount of}} labeled data, along with vast amounts of unlabeled data are also available. • Recently proposed <b>metric</b> learning <b>algorithms</b> [1, 2] make use of such resources to learn a (Mahalanobis) distance metric. • These <b>metric</b> learning <b>algorithms</b> learn a representation of the data (more below). We explore the effectiveness of data representations learned by <b>metric</b> learning <b>algorithms</b> for NLP tasks...|$|R
5000|$|There is a {{range of}} {{sequential}} decoding approaches based on choice of <b>metric</b> and <b>algorithm.</b> Metrics include: ...|$|R
40|$|This paper {{explores the}} issues {{involved}} in using symbolic <b>metric</b> <b>algorithms</b> for automatic speech recognition (ASR), via a structural representation of speech. This representation {{is based on a}} set of phonological distinctive features which is a linguistically well-motivated alternative to the "beads-on-a-string" view of speech that is standard in current ASR systems. We report the promising results of phoneme classication experiments conducted on a standard continuous speech task...|$|R
40|$|In {{this paper}} {{variable}} metric algorithms are extended to solve general nonlinear programming problems. In the algorithm we iteratively solve a linearly constrained quadratic program which contains {{an estimate of}} the Hessian of the Lagrangian. We suggest the variable metric updates for the estimates of the Hessians and justify our suggestion by showing that, when some well known update such as the Davidon-Fletcher-Powell update are so employed, the algorithm converges locally with a superlinear rate. Our algorithm is in a sense a natural extension of the variable <b>metric</b> <b>algorithm</b> to the constrained optimization and this extension offers us not only a class of effective algorithms in nonlinear programming but also a unified treatment of constrained and unconstrained optimization in the variable metric approach...|$|E
40|$|An algorithm, {{designed}} {{to exploit the}} parallel computing or vector streaming (pipeline) capabilities of computers is presented. When p is the degree of parallelism, then one cycle of the parallel variable <b>metric</b> <b>algorithm</b> is defined as follows: first, the function and its gradient are computed in parallel at p different values of the independent variable; then the metric is modified by p rank-one corrections; and finally, a single univariant minimization is {{carried out in the}} Newton-like direction. Several properties of this algorithm are established. The convergence of the iterates to the solution is proved for a quadratic functional on a real separable Hilbert space. For a finite-dimensional space the convergence is in one cycle when p equals the dimension of the space. Results of numerical experiments indicate that the new algorithm will exploit parallel or pipeline computing capabilities to effect faster convergence than serial techniques. Prepared at Langley Research Center. Cover title. Bibliography: p. 20 - 21. An algorithm, {{designed to}} exploit the parallel computing or vector streaming (pipeline) capabilities of computers is presented. When p is the degree of parallelism, then one cycle of the parallel variable <b>metric</b> <b>algorithm</b> is defined as follows: first, the function and its gradient are computed in parallel at p different values of the independent variable; then the metric is modified by p rank-one corrections; and finally, a single univariant minimization is carried out in the Newton-like direction. Several properties of this algorithm are established. The convergence of the iterates to the solution is proved for a quadratic functional on a real separable Hilbert space. For a finite-dimensional space the convergence is in one cycle when p equals the dimension of the space. Results of numerical experiments indicate that the new algorithm will exploit parallel or pipeline computing capabilities to effect faster convergence than serial techniques. Mode of access: Internet...|$|E
40|$|It is {{an equal}} failing to trust {{everybody}} and to trust nobody. English proverb We describe a distributed and scalable trust metric for networks where transactions occur under {{a model of}} preferential attachment. Our trust <b>metric</b> <b>algorithm,</b> which we call expert voting is very simple. For a network over nodes, the algorithm always considers only {{the opinions of the}} first nodes to join the network; we call these nodes experts. For any node, the algorithm evaluates the trustworthiness of based on the opinions of those experts which have had transactions with. Empirical results suggest that this simple algorithm is surprisingly robust for large scale networks where transactions occur under a model of preferential attachment. To the best of our knowledge, this is the first algorithm that exploits a model of preferential attachment. ...|$|E
40|$|Abstract—This paper {{introduces}} a supervised <b>metric</b> learn-ing <b>algorithm,</b> called kernel density metric learning (KDML), which {{is easy to}} use and provides nonlinear, probability-based distance measures. KDML constructs a direct nonlinear mapping from the original input space into a feature space based on kernel density estimation. The nonlinear mapping in KDML embodies established distance measures between probability density functions, and leads to correct classification on datasets for which linear metric learning methods would fail. It addresses the severe challenge to kNN when features are from heterogeneous domains and, as a result, the Euclidean or Mahalanobis distance between original feature vectors is not meaningful. Existing <b>metric</b> learning <b>algorithms</b> can then be applied to the KDML features. We also propose an integrated optimization algorithm that learns not only the Mahalanobis matrix but also kernel bandwidths, the only hyper-parameters in the nonlinear mapping. KDML can naturally handle not only numerical features, but also categorical ones, which is rarely found in previous <b>metric</b> learning <b>algorithms.</b> Extensive experimental results on various datasets show that KDML significantly improves existing <b>metric</b> learning <b>algorithms</b> in terms of kNN classification accuracy. I...|$|R
40|$|<b>Metric</b> {{learning}} <b>algorithms</b> {{can provide}} useful distance functions {{for a variety}} of domains, and recent work has shown good accuracy for problems where the learner can access all distance constraints at once. However, in many real applications, constraints are only available incrementally, thus necessitating methods that can perform online updates to the learned <b>metric.</b> Existing online <b>algorithms</b> offer bounds on worst-case performance, but typically do not perform well in practice as compared to their offline counterparts. We present a new online <b>metric</b> learning <b>algorithm</b> that updates a learned Mahalanobis metric based on LogDet regularization and gradient descent. We prove theoretical worst-case performance bounds, and empirically compare the proposed method against existing online <b>metric</b> learning <b>algorithms.</b> To further boost the practicality of our approach, we develop an online locality-sensitive hashing scheme which leads to efficient updates to data structures used for fast approximate similarity search. We demonstrate our algorithm on multiple datasets and show that it outperforms relevant baselines. ...|$|R
40|$|<b>Metric</b> {{learning}} <b>algorithms</b> {{produce a}} linear transformation of data which is optimized for a prediction task, such as nearest-neighbor classification or ranking. However, when the input data contains {{a large portion}} of noninformative features, existing methods fail to identify the relevant features, and performance degrades accordingly. In this paper, we present an efficient and robust structural <b>metric</b> learning <b>algorithm</b> which enforces group sparsity on the learned transformation, while optimizing for structured ranking output prediction. Experiments on synthetic and real datasets demonstrate that the proposed method outperforms previous methods in both high- and low-noise settings. 1...|$|R
