20|8|Public
40|$|DREAM, or Dynamic Research Evaluation for Management, is a <b>menu-driven</b> <b>software</b> {{package for}} {{evaluating}} the economic impacts of agricultural research and development (R&D). Users can simulate a range of market, technology adoption, research spillover, and trade policy scenarios based on a flexible, multi-market, partial equilibrium model. BookNon-PRIFPRI 1; IFPRI 1 training; TCSPEPT...|$|E
3000|$|... [*]A small {{handheld}} controller {{which houses}} the microprocessor, the <b>menu-driven</b> <b>software,</b> and the LCD screen. The controller is programmable by the {{investigator for the}} number of treatment sessions and the session duration. The user interface indicates to the patient the number of sessions completed and the remaining time in each session. The controller plugs into the power mains via a medically approved, UL-certified power supply.|$|E
40|$|Abstract-The {{computer}} code FEMTR (Finite Element Method-TRaining) for IBM compatible personal computers is presented. The code {{has been developed}} as educational software for learning finite element methbd in structural mechanics. FEMTR is fully <b>menu-driven</b> <b>software</b> with easy to use graphic display of data and results files. There are two ways of using FEMTR for teaching students: (I) data preparation, problem solving and analysis of resuits; (2) de;eloping Pascal mod & for some‘specific requirement in FEM and incorporating it in FEMTR. 1...|$|E
40|$|Mapping Analysis and Planning System (MAPS) is <b>menu-driven</b> {{interactive}} <b>software</b> system {{providing information}} {{to aid in}} land-use planning. Assists user in taking account of multiple requirements, including environmental regulations, affecting types and locations of facilities and structures. Adaptable to municipal, commercial, and industrial land-use planning...|$|R
40|$|SIMBAD {{has been}} {{developed}} to facilitate the tasks involved {{in the design of}} a concatenative speech synthesis system: 1) building a dictionary of parameterized speech units, and 2) obtaining a set of rules to concatenate these units and to model prosodics. It is an interactive, <b>menu-driven</b> and graphical <b>software</b> tool that allows both automatic processing and graphical editing of speech with high flexibility. Peer ReviewedPostprint (published version...|$|R
30|$|Methods: A {{dedicated}} {{spreadsheet software}} to calculate pharmacokinetic parameters such as tracer-biological half-life (T 1 / 2), mean residence time (MRT) for each compartment, {{as well as}} the fraction of the tracer leaving the compartments per unit time (transfer rate constant, TRC), has been developed. The <b>menu-driven</b> spreadsheet <b>software</b> is based on five-Compartment Kinetic Model (CoKiMo). CoKiMo was used to study pharmacokinetic parameters of 11 99 mTc-nitrido complexes, reported as potential myocardial perfusion-imaging agents (MPIAs), of general formula [99 mTcN(DTC-Ln)(PNP)]+ (DTC-Ln= alicyclic dithiocarbamates; PNP= diphosphinoamine) 2, and the results were compared with those of 99 mTc-Sestamibi and 99 mTc-Tetrofosmin. Biodistribution studies for all MPIAs have been performed in Sprague-Dawley rats (n = 312) to determine organ uptake.|$|R
40|$|Low {{software}} development productivity {{is a major}} concern for corporations. This paper describes one approach to improve productivity by allowing end users to build their own business data processing systems. The approach uses Orion, a <b>menu-driven</b> <b>software</b> development tool. Orion generates source level COBOL programs that compile and run without modification. The tool does not require end users to know COBOL or high-level query languages. Orion was used for nearly two years to build real business systems. The approach resulted in a significant improvement in {{software development}} productivity...|$|E
40|$|The {{use of an}} inexpensive, commercially {{available}} audio digitizer {{in conjunction with a}} PC to digitize Doppler bubble signals for visual and electronic evaluation is reported. This device can be operated simultaneously with Doppler audio monitoring. Precordial and arterial Doppler recordings of gas bubbles were obtained from anesthetized dogs after intravascular infusion or following decompression. Additional evaluations were conducted on Doppler bubble recordings obtained from human decompression studies. The device can be used in real-time or for later signal analysis. Accompanying <b>menu-driven</b> <b>software</b> provides for numerous signal modification options and visual displays. This device can provide a simultaneous visual display of Doppler signals normally available only for audio evaluation...|$|E
40|$|The Department of Energy is {{developing}} {{an estimate of}} the undeveloped hydropower potential in the United States. The Hydropower Evaluation Software (HES) is a computer model that was developed by the Idaho National Engineering Laboratory for this purpose. The software measures the undeveloped hydropower resources available in the United States, using uniform criteria for measurement. The software was developed and tested using hydropower information and data provided by the Southwestern Power Administration. It is a <b>menu-driven</b> <b>software</b> program that allows the personal computer user to assign environmental attributes to potential hydropower sites, calculate development suitability factors for each site based on the environmental attributes present, and generate reports based on these suitability factors. This report details the resource assessment results for the Commonwealth of Massachusetts...|$|E
40|$|Radcalc for Windows is a {{user-friendly}} <b>menu-driven</b> Windows-compatible <b>software</b> {{program with}} {{applications in the}} transportation of radioactive materials. It calculates the radiolytic generation of hydrogen gas in the matrix of low-level and high-level radioactive waste using NRC-accepted methodology. It computes the quantity of a radionuclide and its associated products for a given period of time. In addition, the code categorizes shipment quantities as radioactive, Type A or Type B, limited quantity, low specific activity, highway route controlled, and fissile excepted using DOT definitions and methodologies, as outlined in 49 CFR Subchapter C. The code has undergone extensive testing and validation. Volume I is a User`s Guide, and Volume II is the Technical Manual for Radcalc for Window...|$|R
40|$|The minimisation {{of complex}} logic gates is {{important}} to simplify the hardware design of programmable logic arrays (PLAs) and programmable array logic (PALs). The result of minimisation is a considerable reduction of production cost of these digital systems. Quine-McCluskey (Q-M) is an attractive algorithm for simplifying Boolean expressions because it can handle any number of variables. A <b>menu-driven</b> keyboard-based <b>software</b> package has been developed, in C language under MS Windows, to implement the Q-M algorithm for logic gate minimisation. Based on user input (i. e. logic expression), the system displays the sum of product (SOP) functions, as well as minimised logic gates diagram. Q-M algorithm and its software implementation are described. The experimental results demonstrate successful implementation, and {{the simplicity of the}} user interface, makes the package a useful teaching and learning tool for both students and tutors...|$|R
40|$|Elevated plasma {{levels of}} apolipoproteins A 1 (apoA 1) and B (apoB) are {{important}} protective factors and risk factors, respectively, for atherosclerosis and coronary heart disease. It {{is well known}} that both apoA 1 and apoB reveal strong familial aggregation. Our goal was to investigate whether exogenous variables influence these associations. We used marginal regression models for the mean and association structure (Generalised Estimating Equations 2; GEE 2) to analyse data from 1435 family members within 469 families of different sizes included in the Donolo-Tel Aviv Three-Generation Offspring Study. The usual robust variance matrix was approximated by extensions of jackknife estimators of variance to GEE 2 models. Upon use of this approach estimation of standard errors in models with quite complex correlation structures was possible. All analyses were easily carried out using a <b>menu-driven</b> stand-alone <b>software</b> tool for marginal regression modelling. We demonstrate that a var [...] ...|$|R
40|$|Portable {{geophysical}} loggers {{that are}} specifically designed for ground-water applications (fig. 1) are now available. The geophysical loggers are PC-based and have <b>menu-driven</b> <b>software</b> for the collection, real-time display, and subsequent analysis of digital log data. Drawworks for shallow investigations are highly portable, and some have plastic-coated logging cables for easy decontamination. Logging probes are available {{that can be used}} in boreholes with a diameter as small as 5 centimeters. Many probes are capable of collecting multiple parameters with a single logging run, thereby greatly increasing the efficiency of the logging operation and taking full advantage of the synergistic nature of geophysical log data. Figure 2. Electromagnetic-induction log delineates a leachate plume in a sand-and-gravel aquifer downgradient of a municipal landfill. The most highly contaminated part of th...|$|E
40|$|The {{second phase}} {{development}} of the multi-chamber plasma etching system MPE 3003 is described herein. After {{a brief review of}} the design criteria presented last year, we now describe the full implementation of the system into the semiconductor fabrication manufacturing environment. In particular the higher level functions necessary for integration into fully automated production lines have been critically assessed and are now incorporated as standard features. We report on modular installation concepts, clean room compatibility, random-access robotic wafer manipulation for all-vacuum-chamber processing and minimisation of particle generation. The user/operator interface is also defined by <b>menu-driven</b> <b>software</b> with special attention paid to the various options for end-point monitoring and control. In addition, proprietary multi-step etch processes have been developed and characterised in order {{to make use of the}} full range of facilities and advanced options currently available. (IMT...|$|E
40|$|With {{the burgeoning}} number of online files {{provided}} by {{an increasing number}} of vendors, information specialists will need more sophisticated capabilities to retrieve online information. Integrated databases provide a means for online searchers to retrieve information holistically. Three online developments by vendors will improve information retrieval: (1) simultaneous multi-file searchers; (2) crossover between separate files; (3) tagged retrievals that correlate to like records in other files. Simplicity in software design tends to engender rigid systems, such as <b>menu-driven</b> <b>software,</b> while flexibility for searching encourages complexity in software design. Vendors will need to balance these elements in integrating databases. Where should the information industry be going to make life easier for the information specialist and end user? Combined full-text, bibliographic, and numeric files; dictionary and thesaurus files for term and strategy generation; and artificial intelligence interfaces {{will be part of the}} new systems that facilitate searching in the future. ...|$|E
40|$|Radclac for Windows is a user {{friendly}} <b>menu-driven</b> Windows compatible <b>software</b> program with {{applications in the}} transportation of radioactive materials. It calculates the radiolytic generation of hydrogen gas in the matrix of low-level and high-level radioactive wastes. It also calculates pressure buildup due to hydrogen and the decay heat generated in a package at seal time. It computes the quantity of a radionuclide and its associated products for a given period of time. In addition, the code categorizes shipment quantities as reportable quantity (RQ), radioactive Type A or Type B, limited quality (LQ), low specific activity (LSA), highway road controlled quality (HRCQ), and fissile excepted using US Department of Transportation (DOT) definitions and methodologies...|$|R
40|$|The Idaho National Laboratory (INL) has {{developed}} a suite of systems that rapidly scan, analyze, and characterize radiological contamination in soil. These systems have been successfully deployed at several Department of Energy (DOE) laboratories and Cold War Legacy closure sites. Traditionally, these systems have been used during the characterization and remediation of radiologically contaminated soils and surfaces; however, subsequent to {{the terrorist attacks of}} September 11, 2001, the applications of these systems have expanded to include homeland security operations for first response, continuing assessment and verification of cleanup activities in the event of the detonation of a radiological dispersal device. The core system components are a detector, a spectral analyzer, and a global positioning system (GPS). The system is computer controlled by <b>menu-driven,</b> user-friendly custom <b>software</b> designed for a technician-level operator. A wide variety of detectors have been used including several configurations of sodium iodide (NaI) and high-purity germanium (HPGe) detectors, and a large area proportional counter designed for the detection of x-rays from actinides such as Am- 241 and Pu- 238. Systems have been deployed from several platforms including a small all-terrain vehicle (ATV), hand-pushed carts, a backpack mounted unit, and an excavator mounted unit used where personnel safety considerations are paramount. The INL has advanced this concept, and expanded the system functionality to create an integrated, field-deployed analytical system through the use of tailored analysis and operations software. Customized, site specific software is assembled from a supporting toolbox of algorithms that streamline the data acquisition, analysis and reporting process. These algorithms include region specific spectral stripping, automated energy calibration, background subtraction, activity calculations based on measured detector efficiencies, and on-line data quality checks and measures. These analyses are combined to provide real-time areal activity and coverage maps that are displayed to the operator as the survey progresses. The flexible functionality of the INL systems are well suited to multiple roles supporting homeland security needs...|$|R
40|$|Contract DE-AC 07 - 94 ID 13223 iii The Department of Energy is {{developing}} {{an estimate of}} the undeveloped hydro-power potential in this country. The Hydropower Evaluation Software is a com-puter model that was developed by the Idaho National Engineering Laboratory for this purpose. The software measures the undeveloped hydropower resources avail-able in the United States, using uniform criteria for measurement. The software was developed and tested using hydropower information and data provided by the Southwestern Power Administration. It is a <b>menu-driven</b> <b>software</b> program that allows the personal computer user to assign environmental attributes to potential hydropower sites, calculate development suitability factors for each site based on the environmental attributes present, and generate reports based on these suitability factors. This report details the resource assessment results for the State of Indiana. vACKNOWLEDGMENTS The author thanks Peggy A. M. Brookshier, John V. Flynn and Robert MacDo-nald of the Department of Energy for their active participation and timely com-ments. vi...|$|E
40|$|This {{research}} program {{focused on the}} construction of several key radio wave diagnostics in support of the HF Active Auroral Ionospheric Research Program (HAARP). Project activities led to the design, development, and fabrication of a variety of hardware units and to the development of several <b>menu-driven</b> <b>software</b> packages for data acquisition and analysis. The principal instrumentation includes an HF (28 MHz) radar system, a VHF (50 MHz) radar system, and a high-speed radar processor consisting of three separable processing units. The processor system supports the HF and VHF radars and is capable of acquiring very detailed data with large incoherent scatter radars. In addition, a tunable HF receiver system having high dynamic range was developed primarily for measurements of stimulated electromagnetic emissions (SEE). A separate processor unit was constructed for the SEE receiver. Finally, a large amount of support instrumentation was developed to accommodate complex field experiments. Overall, the HAARP diagnostics are powerful tools for studying diverse ionospheric modification phenomena. They are also flexible enough to support a host of other missions beyond the scope of HAARP. Many new {{research program}}s have been initiated by applying the HAARP diagnostics to studies of natural atmospheric processes...|$|E
40|$|This paper {{introduces}} a flexible Bayesian nonparametric Item Response Theory (IRT) model, which applies to dichotomous or polytomous item responses, and which {{can apply to}} either unidimensional or multidimensional scaling. This is an infinite-mixture IRT model, with person ability and item difficulty parameters, and with a random intercept parameter that is assigned a mixing distribution, with mixing weights a probit function of other person and item parameters. As {{a result of its}} flexibility, the Bayesian nonparametric IRT model can provide outlier-robust estimation of the person ability parameters and the item difficulty parameters in the posterior distribution. The estimation of the posterior distribution of the model is undertaken by standard Markov chain Monte Carlo (MCMC) methods based on slice sampling. This mixture IRT model is illustrated through the analysis of real data obtained from a teacher preparation questionnaire, consisting of polytomous items, and consisting of other covariates that describe the examinees (teachers). For these data, the model obtains zero outliers and an R-squared of one. The paper concludes with a short discussion of how to apply the IRT model for the analysis of item response data, using <b>menu-driven</b> <b>software</b> that was developed by the author...|$|E
40|$|The US Department of Energy is {{estimating}} the undeveloped hydropower {{potential in the}} US. The Hydropower Evaluation software is a computer model that {{was developed by the}} Idaho National Engineering Laboratory for this purpose. The Hydropower Evaluation Software estimates the undeveloped hydropower resources available in the US, using uniform criteria for measurement. The software was developed and tested using hydropower information and data provided by the Southwestern Power Administration. It is a <b>menu-driven</b> <b>software</b> application. Hydropower Evaluation Software allows the personal computer user to assign environmental attributes to potential hydropower sites, calculate development suitability factors for each site based on the environmental attributes present, and generate reports based on these suitability factors. This status report describes Hydropower Evaluation Software`s development, its data requirements, and its application to the 20 states assessed to date. This report does not discuss or present the various user-friendly menus of the Hydropower Evaluation Software. The reader is referred to the User`s Manual for specifics. This report focuses on data derivation, summarization of the 20 states (Arkansas, Missouri, Montana, New Hampshire, North Dakota, Oklahoma, Rhode Island, South Dakota, Texas, Utah, Vermont, and Wyoming) assessed to date, and plans for future assessments...|$|E
40|$|In {{this pilot}} study, {{a group of}} 41 "gifted and talented " juniors in high school English and social {{sciences}} classes used the menu-driven Wilsearch software to gather leads to information sources in order to complete their first major term paper. The study demonstrates that: (1) students had little difficulty in using the <b>menu-driven</b> <b>software</b> to combine search terms or to request specific subject areas for books through the Dewey Decimal option; (2) students were selective in the titles requested for printout from Wilsonline and narrowed their choices even more when requesting materials through interlibrary loan; (3) students received over 80 % of the journal articles requested but cited only 24 % of those received; (4) students received 39 % of the book titles requested and cited 72 % of the books received; and (5) attitudes toward the use of online searching were positive. Suggestions for expanding the time period for the assignment and raising the information use expectations are outlined. Four appendices include the student assignment sheet, handouts for introducing Wilsearch, the student worksheet for topic identification and subject heading verification, and the student questionnaire. (Author/KM) Reproductions supplied by EDRS are the best {{that can be made}} from the original document. re...|$|E
40|$|Most {{of applied}} {{statistics}} involves regression analysis of data. This paper presents a stand-alone and <b>menu-driven</b> <b>software</b> package, Bayesian Regression: Nonparametric and Parametric Models. Currently, this package gives the user a choice from 83 Bayesian models for data analysis. They include 47 Bayesian nonparametric (BNP) infinite-mixture regression models; 5 BNP infinite-mixture models for density estimation; and 31 normal random effects models (HLMs), including normal linear models. Each of the 78 regression models handles either a continuous, binary, or ordinal dependent variable, and can handle multi-level (grouped) data. All 83 Bayesian models {{can handle the}} analysis of weighted observations (e. g., for meta-analysis), and the analysis of left-censored, right-censored, and/or interval-censored data. Each BNP infinite-mixture model has a mixture distribution assigned one of various BNP prior distributions, including priors defined by either the Dirichlet process, Pitman-Yor process (including the normalized stable process), beta (two-parameter) process, normalized inverse-Gaussian process, geometric weights prior, dependent Dirichlet process, or the dependent infinite-probits prior. The software user can mouse-click to select a Bayesian model and perform data analysis via Markov chain Monte Carlo (MCMC) sampling. After the sampling completes, the software automatically opens text output that reports MCMC-based estimates of the model's posterior distribution and model predictive fit to the data. Additional text and/or graphical output can be generated by mouse-clicking other menu options. This includes output of MCMC convergence analyses, and estimates of the model's posterior predictive distribution, for selected functionals and values of covariates. The software, constructed from MATLAB Compiler, is illustrated through the BNP regression analysis of real data...|$|E
40|$|The design, fabication, and {{performance}} of an apparatus for measurement of nonresonant rf power absorption (NRRA) in superconducting and CMR samples are described. The system consists of an effective self-resonant LC tank circuit driven by a NOT gate (Logic gate). The samples under investigation are placed {{in the core of}} an inductive coil and nonresonant power absorption is determined from the measured shift in total current supplies to the whole oscillator circuit. A customized low temperature insert is used to integrate the experiment with a commercial oxford cryostat and temperature controller. The system makes use of a sensitive digital multimeter (Keithley 2002 model) and is capable of measuring NRRA in superconducting and colossal magnetoresistance samples of volume as small as $ 1 X 10 ^-^ 3 $ $cm^ 3 $ with a signal to noise ratio of 10. Further increase in the sensitivity of the experimental setup can be obtained by summing the results of repeated measurements obtained in the same temperature interval. The system has been tested for an IC 74 LS 04 oscillator at frequencies between 1 MHz and 25 MHz in the temperature range from 4. 2 K to 400 K and in magnetic field from 0 to 1. 4 T. The system performance is evaluated by measuring the NRRA in $YBa_ 2 Cu_ 3 O_ 7 $ (YBCO) superconducting sample and $La_ 0 _. _ 7 Sr_ 0 _. _ 3 MnO_ 3 $ (LSMO) colossal magnetoresistive (CMR) manganite samples at different rf frequencies. During a measurement all operation are controlled automatically by computer from a <b>menu-driven</b> <b>software</b> system, with user input required only on initiation of measurement sequence...|$|E
40|$|The design, {{fabrication}} {{and performance}} of an apparatus for the measurement of direct rf power absorption in colossal magnetoresistive (CMR) and superconducting samples are described. The system consists of a self-resonant LC tank circuit of an oscillator driven by a NOT logic gate. The samples under investigation are placed {{in the core of}} the coil forming the inductance L and the absorbed power is determined from the measured change in the current supplied to the oscillator circuit. A customized low temperature insert is used to integrate the experiment with a commercial Oxford Instruments cryostat and temperature controller. The oscillator working in the rf range between 1 MHz to 25 MHz is built around an IC 74 LS 04. The temperature can be varied from 4. 2 to 400 K and the magnetic field from 0 to 1. 4 T. The apparatus is capable of measuring direct power absorption in CMR and superconducting samples of volume as small as 1 / 1000 cm^ 3 with a signal to noise ratio of 10 : 1. Further increase in the sensitivity can be obtained by summing the results of repeated measurements obtained at a given temperature. The system performance is evaluated by measuring the absorbed power in La_ 0. 7 Sr_ 0. 3 MnO_ 3 (LSMO) CMR manganite samples and superconducting Y Ba_ 2 Cu_ 3 O_ 7 (YBCO) samples at different rf frequencies. All operations during the measurements are automated using a computer with a <b>menu-driven</b> <b>software</b> system, user input being required only for the initiation of the measurement sequence. Comment: 32 pages including 14 figure...|$|E
40|$|Detailed {{information}} on subsurface conditions {{is essential for}} the development and management of ground-water resources and the characterization and remediation of contaminated sites. Borehole geophysics provides a highly efficient means for the collection of such information. Recent advances in methods and equipment have greatly increased the ability of geoscientists to obtain subsurface information in ground-water investigations through the use of borehole-geophysical techniques. Portable geophysical loggers that are specifically designed for ground-water applications (fig. 1) are now available. The geophysical loggers are PC-based and have <b>menu-driven</b> <b>software</b> for the collection, real-time display, and subsequent analysis of digital log data. Drawworks for shallow investigations are highly portable, and some have plastic-coated logging cables for easy decontamination. Logging probes are available {{that can be used in}} boreholes with a diameter as small as 5 centimeters. Many probes are capable of collecting multiple parameters with a single logging run, thereby greatly increasing the efficiency of the logging operation and taking full advantage of the synergistic nature of geophysical log data. Figure 1. A PC-based geophysical logger with portable drawworks and plastic-coated logging cable in use at a contaminated ground-water site. Electromagnetic-induction logging replaced normal-resistivity logging in the oil industry many years ago. Induction probes recently have been designed for small-diameter monitoring wells. Induction logs can be collected in water-, air-, and mud-filled holes and through PVC casing. Major factors that affect induction-log response in sand-and-gravel aquifers are the concentration of dissolved solids in the ground water and the silt and clay content of the aquifer. Induction logs, which are commonly run in combinal ion with gamma logs, can be used to identify lithology and zones of electrically conductive contamination such as landfill leachate (fig. 2) and saltwater intrusion...|$|E
40|$|DREAM, or Dynamic Research EvaluAtion for Management, is a <b>menu-driven</b> <b>software</b> {{package for}} {{evaluating}} the economic impacts of agricultural research and development (R&D). Users can simulate a range of market, technology adoption, research spillover, and trade policy scenarios based on a flexible, multi-market, partial equilibrium model. With DREAM you can define a range of technology investment, development, and adoption scenarios and save them in an integrated database. Scenarios are described using market, R&D, and adoption information {{for any number of}} separate “regions. ” Some factors, such as taxes, subsidies, growth rates, and price elasticities, can be specified as constant or as changing over the analysis period. Each region in which production takes place may have its own pattern of technology adoption. After specifying the initial conditions for each region, you can simulate the likely effects of technology development and adoption on prices; on quantities produced, consumed, and traded; and on the flow of economic benefits to producers, consumers, and government. DREAM handles simple to relatively complex evaluation problems using a standardized interface. A number of market assumptions are possible: small open economy, closed economy, vertically integrated farm and post-harvest sectors in a single economy, or multiple trading regions. The software also accommodates technology-driven shifts in supply or demand, and users may specify constant or variable shift effects over time in farmers fields. Importantly, DREAM’s multiple region specification can simulate various technology “spillover” scenarios wherein a technology may be adopted in more than one region. Changes in the pattern of technology spillovers can significantly alter the size and distribution of R&D benefits. DREAM has been applied to the evaluation of individual projects in a national context as well as to entire commodity sectors at a subcontinental or continental scale. And while it was designed primarily to evaluate options for R&D that is yet to be undertaken (ex ante assessments), DREAM has also been successfully applied to analyzing the effect of past research (ex post assessments). Knowledge productNon-P...|$|E
40|$|This manual is {{designed}} to teach people to use the statistical software S-Plus and to support {{the process of learning}} statistical concepts and methods. It is most useful as a workbook to accompany Whitlock and Schluter's The Analysis of Biological Data, published by Roberts & Company, Colorado. Although we include enough statistical background to put the procedures being demonstrated in context, we assume that readers will be acquiring most of their understanding of statistical concepts elsewhere. Several of the authors of this manual have been teaching introductory biostatistics to undergraduate and postgraduate students on two campuses in Australia {{for more than a decade}} (in fact one of us, who would prefer not to be identified, taught a biostatistics course for the first time more than three decades ago). In 2008 we discovered the textbook The Analysis of Biological Data (referred to in this manual as ABD). We liked everything about the book: its explanations were beautifully clear and aimed at students much like our own; it used a wide variety of real biological examples; it emphasized concepts and procedures important to biologists and explained how they worked; and it introduced some newer computer-intensive techniques that almost all beginning researchers find themselves needing sooner rather than later. We immediately adopted the book as a text for our own introductory biostatistics course. But this adoption acted as a trigger for making some other changes to our teaching—and in particular, to the way we introduced students to statistical software. To statistical novices, no statistical software is 'user-friendly', and its use needs to be introduced in a structured way which runs in parallel with their acquisition of statistical understanding. At the same time, teaching effort needs to stay focused on statistics rather than software, so that students do not come to see learning to use the software as their primary goal. This manual is intended to allow users to learn to use the software on their own, while keeping a focus on the concepts and procedures which it supports. We have followed the ABD approach and layout very closely—indeed, we started out with the intention of simply demonstrating in S-Plus every example used in the body of that text. In the end, because everyone has a slightly different view of what should be included in a first statistics course, we added a number of other examples, mostly using our own data, to demonstrate software capabilities that would not otherwise have been covered. Also we did not include material associated with ABD Chapter 20 (Likelihood) or with Chapter 21 (Meta-analysis) which are largely conceptual. Most of the computational procedures in Chapter 20 are covered elsewhere in the manual. Why S-Plus? There are a lot of statistical software options, and most of them will execute all the procedures needed in an introductory course. In choosing a software package, we had four criteria beyond its ability to execute procedures taught in the course. * It should have little or no cost to students, and should run on operating systems that students are likely to use on their own machines. Some of us (OK, one of us) remembered etching statistics in the days when the only computing aid available to students was a hand calculator; the rest of us at least remember being taught that way. While we did not wish to return to those days, they had one huge advantage—students could work on the material anywhere and anytime, and not just in computer laboratories provided by the university. Many of our students are part-time, and some are in remote locations. While we can now reasonably expect that students will have access to a computer at home, we cannot reasonably expect them to buy expensive software for themselves. That meant that if we wanted students to work off-campus, we needed to choose software which was either free or very cheap, or which gave students access on their own machines as part of the university's site licence. * It should be useful beyond the course. We wanted students to use professional-quality software that they would not 'grow out of' providing access to all or most of the techniques they were likely to use throughout their careers; and able to import and export data in a wide range of formats (including text files, databases, spreadsheets and other statistical software). * It should have a very strong graphics capability. We wanted students to realise as quickly as possible that nothing substitutes for an intimate familiarity with the data they are analysing—and easily usable graphics allow the data to be explored more quickly and thoroughly than anything else. We wanted the graphics capability to cover the whole range from quick-and-dirty exploratory plots to presentation and publication-quality graphs. * It should reinforce the statistical concepts we wanted students to grasp, and not get in the way of learning them. We wanted to avoid both excessive or inappropriate output, and too much difficulty in using the software itself. Excessive output is often a problem with <b>menu-driven</b> <b>software,</b> which may be relatively easy to use, but often provides pages of output that users neither asked for nor know what to do with. Especially for novices, our preference was for software that gives users exactly what they request and offers warnings (or refuses to perform) when that request is questionable. We believe that someone learning to use statistical procedures should also learn to think about what they are doing and work out exactly what it is they want, rather than making guesses about what button to click in the hope that something useful will happen. On the other hand, if software is too difficult to use, students will inevitably concentrate on learning the mechanics of how to use it rather than developing more fundamental understanding. In the end we chose S-Plus as the best fit to our needs. That choice committed us to producing this manual: there are some excellent introductory books available for S-Plus, but none that we investigated is targeted at undergraduates who begin as complete statistical novices. S-Plus is very powerful and flexible, has superb editable graphics, and its site licence for universities gives enrolled students permission to use the software on their own computers. While it provides a professional-quality graphical user interface (GUI), it also provide a parallel introduction to the command line and to writing basic scripts. Since we made this decision, a final issue has become more important. That is, the increasing importance of the open-source statistical software R in the biological research community. Learning the R language as a complete statistical novice is a hard ask for students, who are often having quite enough difficulty with statistical concepts. But S-Plus shares its command language with R—and our experience has been that students make the transition to R quite easily by the end of an introductory course based on S-Plus. How to use the manual If you are a student using ABD as a text, and you have access to S-Plus, you can use S-Plus to work through each chapter of the manual independently. Every example is demonstrated in enough detail for you to carry it out on your own after reading the ABD chapter and/or covering the statistical concepts in class. You should execute every example yourself to make sure that you can carry out the procedures correctly and get to the right result. A set of exercises is provided at the end of each chapter for you to test your skills. You should make sure that you can do all those marked essential—you may require assistance from your instructor to successfully complete some of them, possibly the advanced exercises. All the data and scripts required for each chapter are available in the resource material provided with the manual. The first chapter of the manual is a basic introduction to S-Plus, and is one of a of a few chapters whose content is not linked to ABD. The second chapter introduces you to S-Plus graphics. The remaining chapters can be covered in several different orders, but you need to work through these two first. Not all the material in later chapters will necessarily be included in an introductory course. In most chapters, we show how to execute statistical procedures using both the GUI and the command line. A few procedures require the use of scripts (short programs written in the S language). Where this is the case, we provide the scripts in the resource material, with them being reproduced in Appendix 2 — and we show you how to load and use them (but we also encourage you to learn to write your own). In many cases, there are more efficient and elegant ways to write scripts than we have used here—in general, we have tried to produce scripts whose logic can be easily understood by beginners, rather than trying for maximum computational efficiency. Appendix 1 provides a more extensive summary of the S language and S-Plus functions relevant to each chapter of the manual. To the instructor We believe that learning statistics is like learning to play the piano—there is no substitute for practice. Consequently, in our own teaching, we provide a lot of incentives for students to practice. In the introductory course that we teach, we expect students to have worked through the appropriate chapter(s) in the manual and attempted the exercises before they arrive at the relevant practical class or tutorial - and the first 20 minutes of each 2 -hour practical class includes a simple open-book practical test, marked in class, which requires them to analyse some new data using techniques covered in the chapter. (By the end of the course, most students score marks on most of these tests). We also run formal (but also open-book) practical exams twice during the course, where the emphasis is on demonstrating that students can make sensible decisions about what to do as well as demonstrating that they can do it. These are also graded immediately. Because students can take this manual—or anything else—into practical tests and exams, we are explicitly not testing how well they remember what buttons to click. When we first changed to this very assessment-orientated approach to the acquisition of practical skills, one unexpected result was that the average grade on the theory exam at the end of the course (which was in the same format and covered the same material as previously) was significantly higher than that achieved by any previous class; it has remained at this level in subsequent years. Perhaps the development of practical skills really does improve theoretical understanding. Acknowledgements: As noted above, the structure and content of this manual owes a huge debt to Whitlock and Schluter's text, which provides the best introduction we know of to statistical methods for biology students. We are also very grateful to the students in our 2009 biometrics class, and especially to the practical class tutors (Clwedd Burns, Gavin Coombes, Rie Hagihara, and Philip Newey) whose combined input and feedback improved the manual immensely. Finally, for all his help our thanks to Kris Angelovski of Solution Metrics Pty Ltd, the Australian distributor of S-Plus...|$|E

