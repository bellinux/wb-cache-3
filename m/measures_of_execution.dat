12|10000|Public
2500|$|Construction of {{the ghetto}} began on July 18, 1941, as {{confirmed}} by Stahlecker himself: [...] "Apart from organizing and carrying out <b>measures</b> <b>of</b> <b>execution,</b> the creation of Ghettos was begun in the larger towns at once during {{the first days of}} operations.". Jewish forced labor was used to build the ghetto, which was not an actual housing district, but rather a decrepit fortress {{on the west side of}} the Daugava river, a short distance just to the northwest of the suburb of Griva, across from the main city of Daugavpils.: [...] "In July, as the initial wave of shootings subsided, the local Germans and their Latvian counterparts rounded up some 14,000 Jews from Daugavpils and the outlying areas of Latgale and crammed them into the old Daugavpils fort, 'the Citadel'." ...|$|E
40|$|This paper {{describes}} an exploratory {{study of how}} software profilers {{can be used to}} help design, evaluate, and index reusable components. Testing of a program using a sampling profiler, gprof, showed that longer execution times are needed to obtain reliable <b>measures</b> <b>of</b> <b>execution</b> speed. We discuss how to use profiling data for indexing reusable components...|$|E
40|$|Abstract — In {{the context}} of a design space {{exploration}} frame-work for supporting the platform-based design approach, we address the problem of robustness with respect to manufacturing process variations. First, we introduce response surface modeling techniques to enable an efficient evaluation of the statistical <b>measures</b> <b>of</b> <b>execution</b> time and energy consumption for each system configuration. We then introduce a robust design space exploration framework to afford the problem of the impact of manufacturing process variations onto the system-level metrics and consequently onto the application-level constraints. We finally provide a comparison of our design space exploration technique with conventional approaches. 1 I...|$|E
30|$|We use {{the number}} of {{instructions}} executed as the <b>measure</b> <b>of</b> <b>execution</b> costs instead <b>of</b> <b>execution</b> times since times counted by interval timer interrupts from a clock device are not reliable on system emulators. When QEMU is invoked with the -icount 0 option, it enables the time stamp counter (TSC) register to count {{the number of}} instructions executed. The RDTSC instruction is used to read the TSC value.|$|R
40|$|A <b>measure</b> <b>of</b> <b>execution</b> {{on market}} impact cost is developed; {{it is the}} {{difference}} between a transaction price and th e volume weighted average price for that day. Fourteen thousand insti tutional trades are examined. Market impact costs average five basis points. Commission costs average eighteen basis points. Total costs a verage twenty-three basis points. Total costs vary only slightly acro ss brokers and vary greatly across money managers. There is no trade- off between commission costs and market impact costs. Copyright 1988 by American Finance Association. ...|$|R
40|$|Prior {{drafts of}} this paper were {{circulated}} under the title “An Order-Based Comparison <b>of</b> <b>Execution</b> Quality across Markets. ” The Impact <b>of</b> Preferencing on <b>Execution</b> Quality We examine the impact <b>of</b> preferencing on <b>execution</b> quality for Nasdaq and NYSE-listed stocks. Our theoretical model demonstrates that realized spreads are more reliable than effective spreads {{in the presence of}} preferencing, but even realized spreads are a poor <b>measure</b> <b>of</b> <b>execution</b> quality if the stocks being compared have different degrees of information asymmetry. We provide a new <b>measure</b> <b>of</b> the costs of preferencing that is independent of asymmetric information. Using data from the SEC 11 Ac 1 - 5 reports for marketable orders of up to 2000 shares, we find that both realized spreads and our preferencing measure are lower for NYSE-listed stock...|$|R
40|$|By {{comparing}} execution {{costs of}} trades handled by Amex floor brokers with trades entered through its automated post execution reporting (PER) system, {{this article provides}} evidence that floor brokers have value. Because they can opportunistically seize liquidity, using a floor broker is equivalent to placing a "smart" limit order. Overall, floor trades have a lower realized half-spread than PER trades (- 3. 06 bps vs. 4. 43 bps). This finding holds for other <b>measures</b> <b>of</b> <b>execution</b> costs and is consistent across all order-size categories. The analysis {{of the value of}} intermediation in a securities market has implications for automated trading systems. ...|$|E
40|$|In {{the context}} of a design space {{exploration}} framework for supporting the platform-based design approach, we address the problem of robustness with respect to manufacturing process variations. First, we introduce response surface modeling techniques to enable an efficient evaluation of the statistical <b>measures</b> <b>of</b> <b>execution</b> time and energy consumption for each system configuration. We then introduce a robust design space exploration framework to afford the problem of the impact of manufacturing process variations onto the system-level metrics and consequently onto the application-level constraints. We finally provide a comparison of our design space exploration technique with conventional approaches...|$|E
40|$|In a {{world with}} private {{information}} and learning {{on the part of}} the market participants, transaction costs should be defined as the (positive) differences between transaction prices and full-information prices, i. e., the prices that reflect all information, private and public, about the asset of interest. While current approaches to the estimation of execution costs largely focus on measuring the differences between transaction prices and efficient prices, i. e., the prices that embed all publicly available information about the asset, this work provides a simple and robust methodology to identify full-information transaction costs based on high-frequency transaction price data. Our estimator is defined in terms of sample moments and is model-free in nature. Specifically, our measure of transaction costs is robust to unrestricted temporary and permanent market microstructure frictions as induced by operating costs (order-processing and inventory-keeping, among others) and adverse selection. Using a sample of S&P 100 stocks we provide support for both the operating cost and the asymmetric information theory of transaction cost determination but show that conventional <b>measures</b> <b>of</b> <b>execution</b> costs have the potential to understate considerably the true cost of trade...|$|E
40|$|This paper {{provides}} {{a detailed analysis}} of the call auction procedure on the Frankfurt Stock Exchange. We develop a direct <b>measure</b> <b>of</b> <b>execution</b> costs in a call auction that is comparable to the bid–ask spread in a continuous market. We find that transaction costs for small transactions in the call market are lower than the quoted spread in the continuous market, whereas transaction costs for large transactions in the call market are higher than the spread in the continuous market. An analysis of specialist (Makler) participation shows that Maklers provide a valuable service to the market. On average, their compensation is restricted to commission income rather than trading profits. Journal of Economic Literatur...|$|R
40|$|International audienceQuantitative {{properties}} of stochastic systems are usually specified in logics that allow one {{to compare the}} <b>measure</b> <b>of</b> <b>executions</b> satisfying certain temporal properties with thresholds. The model checking problem for stochastic systems with respect to such logics is typically solved by a numerical approach [31, 8, 35, 22, 21, 5] that iteratively computes (or approximates) the exact <b>measure</b> <b>of</b> paths satisfying relevant subformulas; the algorithms themselves depend on the class of systems being analyzed {{as well as the}} logic used for specifying the properties. Another approach to solve the model checking problem is to simulate the system for finitely many executions, and use hypothesis testing to infer whether the samples provide a statistical evidence for the satisfaction or violation of the specification. In this tutorial, we survey the statistical approach, and outline its main advantages in terms of efficiency, uniformity, and simplicity...|$|R
50|$|Latency {{is often}} {{measured}} in seconds per unit <b>of</b> <b>execution</b> workload. Throughput is often <b>measured</b> in units <b>of</b> <b>execution</b> workload per second. Another unit of throughput is instructions per cycle (IPC) and its reciprocal, cycles per instruction (CPI) {{is another of}} unit of latency.|$|R
40|$|Financial {{markets are}} {{considered}} to be liquid if a large quantity can be traded quickly and with minimal price impact. Although the idea of a liquid market involves both a cost as well as a time component, most <b>measures</b> <b>of</b> <b>execution</b> costs tend to focus on only a single number reflecting average costs and do not explicitly account for the temporal dimension of liquidity. In reality, trading takes time since larger orders are often broken up into smaller transactions or when limit orders are used. Recent work shows that the time taken to transact introduces a risk component in execution costs. In this setting, the decision {{can be viewed as a}} risk/reward tradeoff faced by the investor who can solve for a mean variance utility maximizing trading strategy. We introduce an econometric method to jointly model the expected cost and the risk of the trade thereby characterizing the mean variance tradeoffs associated of different trading approaches given market and order characteristics. We apply our methodology to a novel data set and show that the risk component is a non-trivial part of the transaction decision. The conditional distribution of transaction costs is also used to construct a new measure of liquidation risk that we refer to as liquidation value at risk (LVaR) ...|$|E
40|$|This paper {{addresses}} {{the issue of}} the optimal trading system for less actively traded stocks. Several studies have examined the quality and performance of alternative market structures for actively traded stocks. However, very few empirical studies compare the performance of different market structures for less actively traded (i. e., “thinly-traded”) stocks. In 1997 the Italian Stock Exchange (ISE) implemented the Thin Stocks Project to improve market quality of “thinly-traded ” stocks. Under this project, stocks defined as “thinly-traded ” by the ISE were given the option to trade either on a pure order driven market with limit order book (POD), or a hybrid order driven market with specialist and limit order book (HOD). In this paper we compare the relative performance of the two trading systems for “thinly-traded ” stocks using several metrics of market quality. We find that the hybrid order driven system offers superior performance along several quality metrics. In particular, the hybrid order driven system offers lower execution costs, greater depth, significant increase in the depth-to-spread ratio, and a decrease in adverse selection cost. Very illiquid stocks benefit more from adopting a hybrid system than moderately illiquid ones. The results are robust to different <b>measures</b> <b>of</b> <b>execution</b> costs, sampling periods, and modeling approaches. Our findings support the analysis by Grossman and Miller (1988) that a specialist can enhance liquidity of “thinly-traded” stocks...|$|E
40|$|Manufacturing process {{variation}} is dramatically {{becoming one of}} the most important challenges related to power and performance optimization for sub- 90 nm CMOS technologies. Process variability impacts the optimization of the target system metrics, that is, performance and energy consumption by introducing fluctuations and unpredictability. Besides, it impacts the parametric yield of the chip with respect to application level constraints by reducing the number of devices working within normal operating conditions. The impact of variability on systems with stringent application-specific requirements (such as portable multimedia and critical embedded systems) is much greater than on general-purpose systems given the emphasis on predictability and reduced operating margins. In this market segment, failing to address such a problem within the early design stages of the chip may lead to missing market deadlines and suffering greater economic losses. In the context of a design space exploration framework for supporting the platform-based design approach, we address the problem of robustness with respect to manufacturing process variations. First, we apply Response Surface Modeling (RSM) techniques to enable an efficient evaluation of the statistical <b>measures</b> <b>of</b> <b>execution</b> time and energy consumption for each system configuration. Then, we apply a robust design space exploration framework to afford the problem of the impact of manufacturing process variations onto the system-level metrics and consequently onto the application-level constraints. We finally provide a comparison of our design space exploration technique with conventional approaches on two different case studies...|$|E
40|$|International audienceWe give a {{semantic}} account <b>of</b> the <b>execution</b> time (i. e. {{the number of}} cut elimination steps leading to the normal form) of an untyped MELL net. We first prove that: 1) a net is head-normalizable (i. e. normalizable at depth 0) {{if and only if}} its interpretation in the multiset based relational semantics is not empty and 2) a net is normalizable if and only if its exhaustive interpretation (a suitable restriction of its interpretation) is not empty. We then give {{a semantic}} <b>measure</b> <b>of</b> <b>execution</b> time: we prove that we can compute the number of cut elimination steps leading to a cut free normal form of the net obtained by connecting two cut free nets by means of a cut-link, from the interpretations of the two cut free nets. These results are inspired by similar ones obtained by the first author for the untyped lambda-calculus...|$|R
30|$|On {{traditional}} computer performance evaluation, Hennessy and Patterson [27] {{consider the}} <b>execution</b> time <b>of</b> real programs {{as the only}} consistent and reliable <b>measure</b> <b>of</b> performance. <b>Execution</b> times have been continuously hampered by the variability of computer performance, specially for parallel programs which are affected by data races, thread scheduling, synchronization order, and contention of shared resources. In [28], it is shown that multicore processors bring even more variability to execution times.|$|R
40|$|Electronic brokerages on the Internet {{represent}} one of {{the most}} successful examples of electronic commerce, having captured over 20 % of retail stock trades. According to economic theory, prices of commodities like securities should converge to one price in a market with the transparency of the Internet. A review of published commissions for online brokers shows that this "law of one price" does not appear to hold for the commissions charged by retail brokers. In this paper we explore one possible explanation for these differences in commissions. Specifically, we test whether the total cost of trading, including commissions and savings based on the quality <b>of</b> <b>execution,</b> obeys the law of one price. In a carefully designed experiment, we simultaneously purchased or sold 100 share lots of stock using a voice-broker, an expensive online broker and an inexpensive online broker in each trial. We found relatively few price improvements, which are a <b>measure</b> <b>of</b> <b>execution</b> quality. The difference among brokers in obtaining price improvements was not statistically significant. The brokers do exhibit statistically significant differences in total trading costs; at a volume of 100 shares commission costs dominate execution quality. We explore the implications of the findings for larger lot sizes, choosing a broker, and electronic commerce in the brokerage industry. ...|$|R
40|$|The {{constitutional}} principle of a proper utilization of land {{as a national}} property is reflected, among others, in a duty of agricultural utilization of farmlands and land re-cultivated f'or agricultural purposes. Two structures of subjects can be defined upon characterizing this regulation: the State — owners of agricultural estates, and owners of these estates — all non-owners. The statutory duty of agricultural utilization of farmlands and lands re-cultivated for agricultural purposes {{is a part of}} the structure: the State — owners of agricultural estates. According to the valid legal rules, this duty does not have to be substantiated in a form of administrative decision but directly creates the obligation for owners (possessors) to perform generally defined real actions on specified cultivable lands. Fulfillment of this duty is a subject of control perfomed by a primary State's administrative agency in a period of plants' vegetation. The statutory duty on owners (possessors) of specified cultivable lands- of utilizing them in agricultural way is related to the right of State administrative agencies to control its fulfillment. Thus the duty becomes an element of potential relation under administrative law of the basic character in the aspect of substantive law. Content of the duty being an order of running effective vegetable production imposes itself in a way on that part of allowed behavior of owners (possessors) which consists in taking up vegetable production. In case of observing any action contrary to the ordered activities State administrative agencies are competent to levy <b>measures</b> <b>of</b> <b>execution.</b> In this way a potential relation under law and execution can foe defined, it is one of the types of a relation under administrative law. Digitalizacja i deponowanie archiwalnych zeszytów RPEiS sfinansowane przez MNiSW w ramach realizacji umowy nr 541 /P-DUN/ 201...|$|E
40|$|The Recognition and Enforcement of Debts {{under the}} {{statutes}} of Acton Burnell (1283) and Merchants (1285), 1283 - 1307 Christopher McNall (Magdalen College) Thesis submitted for {{the degree of}} D. Phil Trinity Term 2000 This thesis is about the statutes of Acton Burnell (1283) and Merchants (1285) which provided for the voluntary registration of debts before specially established registries, and sophisticated <b>measures</b> <b>of</b> <b>execution</b> against the defaulting debtor's person, goods, and lands. The introduction describes the sources for this thesis; the London Recognisance rolls; the certificates of statute merchant into the Chancery; the Plea Rolls of the Royal courts and of local - principally, borough - courts. Chapter 1 describes the background to the statutes, in particular the recoverability of debts before Royal, local, and mercantile courts before 1283. Chapter 2 explores the immediate legal and political contexts of Acton Burnell. A draft of the statute is discussed and compared with the statute. The need for reform in 1285 is assessed, setting Merchants alongside Westminster II n c. 39. The provisions under both statutory schemes for recognition and enrolment of the debt, and the initiation of execution are described. Chapter 4 examines execution against the debtor's movable property. The statutory appraisal, sale and delivery of the debtor's goods are examined and compared both with the draft provisions and common law modes of execution. Appraisers' liability under the statutes is examined. Competing execution against the same debtor is investigated. Chapter 5 examines the debtor's arrest and detention, gaolers' statutory liability, statutory costs and damages. It investigates {{the operation of the}} statutes once the debt had been satisfied, the mechanisms for obtaining the debtor's release, and challenges to unlawful imprisonment via the writ audita querela. Chapter 6 examines execution against the debtor's immovable property. The chapter discusses the 'extent' by which the debtor's lands were to be delivered to the creditor under Merchants, {{and the nature of the}} creditor's holding of his debtor's immovables (the tenancy 'by statute merchant'). This thesis is 92, 800 words in length, excluding tables and appendices</p...|$|E
40|$|Abstract. Formal proofs of {{functional}} correctness and rigorous analyses of fault tolerance have, traditionally, been separate processes. In the former a programming logic (proof) or computational model (model checking) {{is used to}} establish that all the system’s behaviours satisfy some (specification) criteria. In the latter, techniques derived from engineering are used to determine quantitative properties such as probability of failure (given failure of some component) or expected performance (an average <b>measure</b> <b>of</b> <b>execution</b> time, for example). To combine the formality and the rigour requires a quantitative approach within which functional correctness can be embedded. Programming logics for probability are capable in principle of doing so, and {{in this article we}} illustrate the use of the probabilistic guarded-command language (pGCL) and its logic for that purpose. We take self-stabilisation as an example of fault tolerance, and present program-logical techniques for determining, on the one hand, that termination occurs with probability one and, on the other, the the expected time to termination is bounded above by some value. An interesting technical novelty required for this is the recognition of both “angelic” and “demonic ” refinement, reflecting our simultaneous interest in both upper- and lower bounds. ...|$|R
5000|$|... 58-14 (added on June 6, 1937) [...] "Counter-revolutionary sabotage", i.e., {{conscious}} non-execution or deliberately careless <b>execution</b> <b>of</b> [...] "defined duties", {{aimed at}} the weakening {{of the power of}} the government and of the functioning of the state apparatus is subject to at least one year of freedom deprivation, and under especially aggravating circumstances, up to the highest <b>measure</b> <b>of</b> social protection: <b>execution</b> by shooting with confiscation of property.|$|R
30|$|To <b>measure</b> {{the level}} <b>of</b> {{dependency}} {{we used the}} Barthel index (Spanish version); this index provided us with quantitative information about the level <b>of</b> dependency, <b>measuring</b> the <b>execution</b> <b>of</b> ten daily life activities (Reliability of original version 0.86 - 0.92) (Cid-Ruzafa and Damián-Moreno 1997). Translated from the original version (Mahoney and Barthel 1965).|$|R
50|$|For this reason, MIPS {{has become}} not a <b>measure</b> <b>of</b> {{instruction}} <b>execution</b> speed, but task performance speed {{compared to a}} reference. In the late 1970s, minicomputer performance was compared using VAX MIPS, where computers were measured on a task and their performance rated against the VAX 11/780 that was marketed as a 1 MIPS machine. (The measure was {{also known as the}} VAX Unit of Performance or VUP.) This was chosen because the 11/780 was roughly equivalent in performance to an IBM System/370 model 158-3, which was commonly accepted in the computing industry as running at 1 MIPS.|$|R
500|$|His orchestral players affectionately nicknamed him [...] "Timber" [...] – {{more than}} a play on his name, since it seemed to {{represent}} his reliability too. His tally of first performances, or first performances in Britain, was heroic: at least 717works by 357composers. Greatness as <b>measured</b> by finesse <b>of</b> <b>execution</b> may not be his, particularly in his limited legacy of recordings, but he {{remains one of the}} most remarkable musicians Britain has produced.|$|R
40|$|Electronic {{commerce}} {{has enjoyed}} great {{success in the}} retail brokerage industry. Attracted by commission savings, consumers' use of on-line brokerage firms has grown. However, brokerage customers may have difficulty comparing total trading costs, which consist of both the commission the broker charges {{and the cost of}} executing a trade. This paper reports on an experiment to examine whether order handling practices by traditional voice brokers and on-line brokerage firms lead to differences in the quality <b>of</b> trade <b>execution.</b> We test two hypotheses; the first is that execution quality differs among brokers and is positively related to commission rates, and the second is that total trading costs are converging as might be expected in a stable market. In the experiment, we conducted 196 trades, simultaneously purchasing or selling 100 share lots of stock using a voice-based broker, a "brand-name" online broker and a deep discount online broker in each trial. We found 36 percent of our orders received price improvement, a <b>measure</b> <b>of</b> <b>execution</b> quality. The differences among brokers in obtaining price improvements were (weakly) statistically significant for NYSE-listed shares only. The brokers do exhibit statistically significant differences in total trading costs; at a volume of 100 shares commission costs dominate execution quality. We discuss implications for larger lot sizes and speculate on the ability of full-service brokerage firms to maintain high commission charges. The paper concludes that electronic commerce is having a major impact on the brokerage industry, and has the potential to affect pricing in other industries with bundled products and services...|$|R
40|$|This study <b>measures</b> the {{magnitude}} <b>of</b> <b>execution</b> costs <b>of</b> outright options and options which constitute strategies ("strategy-linked options"), and examines if any differences in execution costs {{between these two}} groups is attributable to differences in market making costs on the Australian Options Market. Results reveal that execution costs for tailor-made strategy-linked options are greater than outright options. Also, this study provides evidence that the disadvantage of tailor-made strategy-linked options over outright options is driven by market makers' hedging costs. 15 page(s...|$|R
5000|$|His biographer Arthur Jacobs {{wrote of}} Wood:His orchestral players affectionately nicknamed him [...] "Timber" [...] - {{more than a}} play on his name, since it seemed to {{represent}} his reliability too. His tally of first performances, or first performances in Britain, was heroic: at least 717 works by 357 composers. Greatness as <b>measured</b> by finesse <b>of</b> <b>execution</b> may not be his, particularly in his limited legacy of recordings, but he {{remains one of the}} most remarkable musicians Britain has produced.|$|R
40|$|Motor {{planning}} {{was assessed}} by performance of 10 male children (age range 9 - 12 years) with ASD on a simple obstacle course of horizontal barriers. The primary <b>measures</b> <b>of</b> motor planning were acts of hesitation and hesitation time. These measures, along with executive functioning scores from the BRIEF, and <b>measures</b> <b>of</b> movement <b>execution</b> were correlated to assess the validity of this obstacle course. Results of these correlations supported {{the validity of the}} motor planning inferences from the performance of the obstacle course since motor planning correlated in the expected directions with the BRIEF scores and movement execution measures. Internal consistency (Cronbach's alpha) was sufficiently high to support the reliability of this obstacle course, with the item analysis providing direction for the most reliable barrier heights. Therefore, the use of this obstacle course task provides both valid inferences and reliable <b>measures</b> <b>of</b> motor planning, although further development is warranted...|$|R
2500|$|However, after {{resistance}} {{started in}} Province of Ljubljana, Italian {{violence against the}} Slovene civil population easily matched that of the Germans. The province saw the deportation of 25,000 people — which equated to 7.5% {{of the total population}} of the province [...] — {{in one of the most}} drastic operations in the Europe that filled up many Italian concentration camps, such as Rab concentration camp, in Gonars concentration camp, Monigo (Treviso), Renicci d'Anghiari, Chiesanuova and elsewhere. To suppress the mounting resistance by the Slovene Partisans, Mario Roatta adopted draconian <b>measures</b> <b>of</b> summary <b>executions,</b> hostage-taking, reprisals, internments, and the burning of houses and whole villages. The [...] "3C" [...] pamphlet, tantamount to a declaration of war on civilians, involved him in Italian war crimes.|$|R
40|$|The present paper {{investigates the}} {{influence}} <b>of</b> the <b>execution</b> history on the precision <b>of</b> measurement-based <b>execution</b> time estimates for embedded software. A {{new approach to}} timing analysis is presented {{which was designed to}} overcome the problems of existing static and dynamic methods. By partitioning the analyzed programs into easily traceable segments and by precisely controlling run-time measurements with on-chip tracing facilities, the new method is able to preserve information about the <b>execution</b> context <b>of</b> <b>measured</b> <b>execution</b> times. After an adequate number of measurements have been taken, this information can be used to precisely estimate the Worst-Case <b>Execution</b> Time <b>of</b> a program without being overly pessimistic...|$|R
40|$|Abstract — Performance of {{parallel}} programs {{is one of}} the reasons of their development. The process of designing and programming a parallel application is a very hard task that requires the necessary knowledge for the detection of performance bottlenecks, and the corresponding changes in the source code of the application to eliminate those bottlenecks. Current approaches to this analysis require a certain level of expertise from the developers part in locating and understanding the performance details <b>of</b> the application <b>execution.</b> For these reasons, we present an automatic performance analysis tool with the objective of alleviating the developers of this hard task: Kappa Pi. The most important limitation of KappaPi approach is the important amount of gathered information needed for the analysis. For this reason, we present a dynamic tuning system that takes <b>measures</b> <b>of</b> the <b>execution</b> on-line. This new design is focused to improve the performance {{of parallel}} programs during runtime...|$|R
30|$|Table 4 {{shows the}} <b>measured</b> results <b>of</b> the <b>execution</b> <b>of</b> the {{compiled}} MATLAB code. By {{increasing the number}} of cores, the calculation time is reduced, but the theoretical speed-up value cannot be reached (e.g. 1.87 instead of 3). The {{reason for this is that}} the worker node needs a certain amount of time to start the MATLAB Component Runtime Environment (MCR). To reduce the MCR overhead and the Web service, overhead each sub-calculation should be a long running calculation. If a sub-calculation is completed fast enough, it is possible to send multiple sub-calculations within one Web service call.|$|R
40|$|Performance of {{parallel}} programs {{is one of}} the reasons of their development. The process of designing and programming a parallel application is a very hard task that requires the necessary knowledge for the detection of performance bottlenecks, and the corresponding changes in the source code of the application to eliminate those bottlenecks. Current approaches to this analysis require a certain level of expertise from the developers part in locating and understanding the performance details <b>of</b> the application <b>execution.</b> For these reasons, we present an automatic performance analysis tool with the objective of alleviating the developers of this hard task: Kappa Pi. The most important limitation of KappaPi approach is the important amount of gathered information needed for the analysis. For this reason, we present a dynamic tuning system that takes <b>measures</b> <b>of</b> the <b>execution</b> on-line. This new design is focused to improve the performance {{of parallel}} programs during runtime. I Workshop de Procesamiento Distribuido y Paralelo (WPDP...|$|R
40|$|Pipeline {{execution}} {{is a form}} of parallelism {{in which}} subcomputations of a repeated computation, such as statements in the body of a loop, are executed in parallel. A <b>measure</b> <b>of</b> the <b>execution</b> time <b>of</b> a pipeline is needed to determine if pipelining is an effective form of parallelism for a loop, and to evaluate alternative scheduling choices. We derive a formula for precisely determining the asynchronous pipeline <b>execution</b> time <b>of</b> a loop modeled as iterated <b>execution</b> <b>of</b> an acyclic task graph. The formula can be evaluated in time linear in the number of tasks and edges in the graph. We assume that computation and communication times are fixed and known, interprocessor communication and buffering capability are unbounded, and each task is assigned to a distinct processor. 1. Introduction Pipelining is an "assembly line" form of parallelism in which subcomputations of a repeated computation are executed concurrently. Pipeline parallelism {{has a long history of}} application in specific domain [...] ...|$|R
40|$|Objectives: We {{assessed}} feasibility, {{validity and}} diagnostic accuracy of a non-invasive dynamic ambulatory test with Near-Infrared Spectroscopy (NIRS) evaluating foot perfusion in Peripheral Arterial Disease (PAD). Design: Prospective observational study. Materials and Methods: Eighty PAD patients (63 males, 71 ± 9 y) including 41 patients with coexisting diabetes), participated. Thirteen healthy subjects (8 males, 26 ± 8 y) were also studied by Echo-Color-Doppler providing 160 diseased and 26 non-diseased limbs. Under clinostatic conditions, participants performed a 10 -repetition toe-flexion test with NIRS probes on the dorsum of each foot; the area-under-curve of the oxygenated hemoglobin trace (‘toflex-area’) was calculated and the ankle brachial index (ABI) was <b>measured.</b> Time <b>of</b> <b>execution,</b> rate <b>of</b> wrong tests and adverse reactions were recorded. Within-session reliability {{was assessed by}} administering the test twice after five-minute interval. Validity was assessed determining whether the toflex-area was: i) dependent on the oxygen delivery from the lower limb arteries simulating PAD conditions by a progressive blood flow restriction (40 ‒ 120...|$|R
