1|9790|Public
40|$|Upper bounds on {{worst-case}} execution times, {{which are}} commonly called WCET, are {{a prerequisite for}} validating the temporal correctness of tasks in a real-time system. Due to the execution history sensitive behavior of components like caches, pipelines, buffers and periphery, the static determination of safe upper execution-time bounds is a challenging task. A successful timing analysis approach developed at Saarland University/AbsInt GmbH uses abstract interpretation to derive safe WCET bounds based on timing models of the processor and periphery in a system. So far, WCET {{research has focused on}} processor timing behavior. System performance depends heavily on the performance of the periphery, namely the system controller, which includes the <b>memory</b> <b>access</b> <b>logic.</b> This paper is the first to describe experience in deriving a timing model for such a system controller. The starting point is the VHDL description from which the controllers FPGA implementation is synthesized. By a sequence of simplifications and abstractions we obtain an abstract VHDL model which can be translated easily into a timing model. The evaluation of the derived WCET tool shows that the approach leads to a precise and efficient analysis. This opens up the perspective of automatically deriving timing models from VHDL descriptions also for processors. Categories and Subject Descriptors B. 4. 4 [Hardware]: Input/Output and data communications...|$|E
40|$|Smart cards {{have been}} {{commercial}} products for 15 years, {{but they have}} only found use in {{a broad range of}} applications in the past few years. Most early applications, such as prepaid phone cards, used <b>memory</b> cards with <b>access</b> <b>logic.</b> Adding a CPU to the card enhanced the security and paved the way for more possibilities: subscriber identification module (SIM) cards in cellular phones, banking cards, and electronic social-security cards. New services that require secure transactions will reach the market soon; these include payment systems for online transactions and subscription TV. The huge progress in both VLSI technolog...|$|R
40|$|In {{this paper}} we present our design and {{implementation}} of Network-on-Chip (NoC) support into our TACO protocol processor architecture. Our signaling scheme is Virtual Component Interface standard (VCI) compliant. Due to the data and I/O-intensive nature of protocol processing, <b>memory</b> <b>access</b> from I/O <b>logic</b> {{plays a key role}} in NoC interface design. We have addressed this problem by using dual-port memory for protocol data units (PDUs) and a separate single port memory for other user data. We evaluated our NoC interface with VHDL synthesis of a case study in IPv 6 processing. Based on our simulations and synthesis, the logic part is able to operate at 200 MHz in 0. 18 µm CMOS technology. Adding a NoC interface into our architecture did not considerably increase area and power costs...|$|R
40|$|Remote {{attestation}} is {{the process}} of securely veri-fying internal state of a remote hardware platform. It can be achieved either statically (at boot time) or dy-namically, at run-time in order to establish a dynamic root of trust. The latter allows full isolation of a code region from preexisting software (including the oper-ating system) and guarantees untampered execution of this code. Despite the untrusted state of the overall platform, a dynamic root of trust facilitates execution of critical code. Prior software-based techniques lack concrete security guarantees, while hardware-based approaches involve security co-processors that are too costly for low-end embedded devices. In this paper, we develop a new primitive (called SMART) based on hardware-software co-design. SMART is a simple, efficient and secure approach for establishing a dynamic root of trust in a re-mote embedded device. We focus on low-end micro-controller units (MCU) that lack specialized memory management or protection features. SMART requires minimal changes to existing MCUs (while providing concrete security guarantees) and assumes few restric-tions on adversarial capabilities. We demonstrate both practicality and feasibility of SMART by implementing it – via hardware modifications – on two common MCU platforms: AVR and MSP 430. Results show that SMART implementations require only a few changes to <b>memory</b> bus <b>access</b> <b>logic.</b> We also synthesize both implementations to an 180 nm ASIC process to confirm its small impact on MCU size and overall cost. 1...|$|R
40|$|A {{concept for}} {{molecular}} electronics exploiting carbon nanotubes as both molecular device elements and molecular wires for {{reading and writing}} information was developed. Each device element {{is based on a}} suspended, crossed nanotube geometry that leads to bistable, electrostatically switchable ON/OFF states. The device elements are naturally addressable in large arrays by the carbon nanotube molecular wires making up the devices. These reversible, bistable device elements could be used to construct nonvolatile random <b>access</b> <b>memory</b> and <b>logic</b> function tables at an integration level approaching 1012 elements per square centimeter and an element operation frequency in excess of 100 gigahertz. The viability of this concept is demonstrated by detailed calculations and by the experimental realization of a reversible, bistable nanotube-based bit...|$|R
50|$|There {{are many}} {{examples}} of shared memory (multiprocessors): UMA (Uniform <b>Memory</b> <b>Access),</b> COMA (Cache Only <b>Memory</b> <b>Access)</b> and NUMA (Non-Uniform <b>Memory</b> <b>Access).</b>|$|R
40|$|Multicore {{architectures}} {{have established}} {{themselves as the}} new generation of computer architectures. As part of the one core to many cores evolution, <b>memory</b> <b>access</b> mechanisms have advanced rapidly. Several new <b>memory</b> <b>access</b> mechanisms have been implemented in many modern commodity multicore architectures. By specifying how processing cores <b>access</b> shared <b>memory,</b> <b>memory</b> <b>access</b> mechanisms directly influence the synchronization capabilities of multicore architectures. Therefore, it is crucial to investigate the synchronization power of these new <b>memory</b> <b>access</b> mechanisms. This paper investigates the synchronization power of coalesced <b>memory</b> <b>accesses,</b> a family of <b>memory</b> <b>access</b> mechanisms introduced in recent large multicore architectures such as the Compute Unified Device Architecture (CUDA). We first define three <b>memory</b> <b>access</b> models to capture the fundamental features of the new <b>memory</b> <b>access</b> mechanisms. Subsequently, we prove the exact synchronization power of these models in terms of their consensus numbers. These tight results show that the coalesced <b>memory</b> <b>access</b> mechanisms can facilitate strong synchronization between the threads of multicore architectures, without the need of synchronization primitives other than reads and writes. In the case of the contemporary CUDA processors, our results imply that the coalesced <b>memory</b> <b>access</b> mechanisms have consensus numbers up to 64...|$|R
50|$|Reducing Memory Access: Changing the {{structure}} of the program by replacing the operations which require frequent <b>memory</b> <b>access</b> with those need less <b>memory</b> <b>access</b> is also profitable as <b>memory</b> <b>access</b> is a costly operation.|$|R
40|$|Abstract—Multicore {{architectures}} {{have established}} {{themselves as the}} new generation of computer architectures. As part of the one core to many cores evolution, <b>memory</b> <b>access</b> mechanisms have advanced rapidly. Several new <b>memory</b> <b>access</b> mechanisms have been implemented in many modern commodity multicore architectures. By specifying how processing cores <b>access</b> shared <b>memory,</b> <b>memory</b> <b>access</b> mechanisms directly influence the synchronization capabilities of multicore architectures. Therefore, it is crucial to investigate the synchronization power of these new <b>memory</b> <b>access</b> mechanisms. This paper investigates the synchronization power of coalesced <b>memory</b> <b>accesses,</b> a family of <b>memory</b> <b>access</b> mechanisms introduced in recent large multicore architectures such as the Compute Unified Device Architecture (CUDA). We first define three <b>memory</b> <b>access</b> models to capture the fundamental features of the new <b>memory</b> <b>access</b> mechanisms. Subsequently, we prove the exact synchronization power of these models in terms of their consensus numbers. These tight results show that the coalesced <b>memory</b> <b>access</b> mechanisms can facilitate strong synchronization between the threads of multicore architectures, without the need of synchronization primitives other than reads and writes. In the case of the contemporary CUDA processors, our results imply that the coalesced <b>memory</b> <b>access</b> mechanisms have consensus numbers up to sixty four. Index Terms—Memory access models, consensus, multicore architectures, inter-process synchronization. I...|$|R
5000|$|... {{non-uniform}} <b>memory</b> <b>access</b> (NUMA): <b>memory</b> <b>access</b> time {{depends on}} the memory location relative to a processor; ...|$|R
40|$|Algorithms that exhibit {{irregular}} <b>memory</b> <b>access</b> {{patterns are}} known to show poor performance on multiprocessor architectures, particularly when <b>memory</b> <b>access</b> latency is variable. Many common data structures, including graphs, trees, and linked-lists, exhibit these irregular <b>memory</b> <b>access</b> patterns. While FPGA-based code accelerators have been successful on applications with regular <b>memory</b> <b>access</b> patterns, {{they have not been}} further explored for irregular <b>memory</b> <b>access</b> patterns. Multithreading {{has been shown to be}} an e↵ective technique in masking long latencies. We describe the compiler generation of concurrent hardware threads for FPGAs with the objective of masking the memory latency caused by irregular <b>memory</b> <b>access</b> patterns. We extend the ROCCC compiler to generate customized state information for each dynamically generated thread...|$|R
30|$|The {{total energy}} {{consumption}} of NAND Flash <b>memory</b> <b>access</b> {{can be obtained}} by adding that for <b>memory</b> <b>access</b> and that for error correction. We observe that high output precision increases the energy for <b>memory</b> <b>access,</b> while it can reduce the LDPC decoding energy.|$|R
40|$|Hardware data {{prefetching}} {{is widely}} adopted to hide long memory latency. A hardware data prefetcher predicts the memory address {{that will be}} accessed {{in the near future}} and fetches the data at the predicted address into the cache memory in advance. To detect <b>memory</b> <b>access</b> patterns such as a constant stride, most existing prefetchers use differences between addresses in a sequence of <b>memory</b> <b>accesses.</b> However, prefetching based on the differences often fail to detect <b>memory</b> <b>access</b> patterns when aggressive optimizations are applied. For example, out-of-order execution changes the <b>memory</b> <b>access</b> order. It causes inaccurate prediction because the sequence of memory addresses used to calculate the difference are changed by the optimization. To overcome the problems of existing prefetchers, we propose Access Map Pattern Matching (AMPM). The AMPM prefetcher has two key components: a <b>memory</b> <b>access</b> map and hardware pattern matching <b>logic.</b> The <b>memory</b> <b>access</b> map is a bitmap-like data structure for holding past <b>memory</b> <b>accesses.</b> The AMPM prefetcher divides the memory address space into memory regions of a fixed size. The <b>memory</b> <b>access</b> map is mapped to th...|$|R
40|$|Abstract: This paper studies {{self-similarity}} of <b>memory</b> <b>accesses</b> in {{high-performance computer}} systems. We analyze the autocorrelation functions of <b>memory</b> <b>access</b> intervals in SPEC CPU workloads with different time scales and present the statistical evidence that <b>memory</b> <b>accesses</b> have self-similar behavior. For memory traces studied in our experiments, all estimated Hurst parameters {{are larger than}} 0. 5, which indicate that self-similarity {{seems to be a}} general property of <b>memory</b> <b>access</b> behaviors. In addition, a selfsimilar model is proposed to generate <b>memory</b> <b>access</b> series and experimental results show that this model can faithfully emulate the complex access behaviors of real memory systems...|$|R
50|$|A <b>memory</b> <b>access</b> {{is said to}} be aligned {{when the}} datum being {{accessed}} is n bytes long and the datum address is n-byte aligned. When a <b>memory</b> <b>access</b> is not aligned, it {{is said to be}} misaligned. Note that by definition byte <b>memory</b> <b>accesses</b> are always aligned.|$|R
40|$|Abstract:- While {{deployment}} of embedded and distributed network services imposes new demands for flexibility and programmability, IP address lookup has become significant performance bottleneck {{for the highest}} performance routers. Amid their vast array of academic and commercial solutions to the problem, few achieve a favorable balance of performance, efficiency, and cost. New commercial products utilize Content Addressable Memory (CAM) devices that achieve high lookup speeds and an exorbitantly high hardware cost with limited flexibility. In this paper a new IP forwarding hardware algorithm, based on gray-code encoding, along with its dedicated scalable lookup engine is proposed. The corresponding programmable router is able to achieve high performance {{with the use of}} a small portion of Dynamic Reconfigurable Logic (DRL) device and a commodity Random <b>Access</b> <b>Memory</b> (RAM) <b>logic.</b> Experimental evaluation using Ns 2 simulator and a small routing table from Mae West [1] has been conducted. The results show that the programmable router can be scaled to achieve guaranteed worst-case performance of over 66 million lookups per second with a single SRAM operation at the fairly clock of 100 MHz...|$|R
30|$|This {{equation}} {{can be used}} {{to compute}} individually yLimfor identical arithmetic, identical read <b>memory</b> <b>access</b> or identical write <b>memory</b> <b>access</b> operations.|$|R
5000|$|... and programmers understand,analyse {{and improve}} the <b>memory</b> <b>access</b> pattern, e.g., VTune, Vectorization Advisor, and others, {{including}} tools to address GPU <b>memory</b> <b>access</b> patterns ...|$|R
40|$|This paper {{presents}} an effective <b>memory</b> <b>access</b> method for a high-speed data transfer on mobile systems using a direct <b>memory</b> <b>access</b> controller that considers {{the characteristics of}} a multi-port memory controller. The direct <b>memory</b> <b>access</b> controller has an integrated channel management function to control multiple direct <b>memory</b> <b>access</b> channels. The channels are physically separated and operate independently from each other. Experimental results show that the proposed direct <b>memory</b> <b>access</b> method improves the transfer performance by up to 72 % and 69 % on read and write transfer cycles, respectively. The total number of transfer cycles of the proposed method is 63 % less than in a commercial method under 4 -channel access...|$|R
40|$|<b>Memory</b> <b>access</b> tracing is {{a program}} {{analysis}} technique with many different applications, ranging from architectural simulation to (on-line) data placement optimization and security enforcement. In this article we propose a <b>memory</b> <b>access</b> tracing approach based on static x 86 binary instrumentation. Unlike non-selective schemes, which instrument all the <b>memory</b> <b>access</b> instructions, our proposal selectively instruments a subset of those instructions that are the most (or fully) representative of the actual <b>memory</b> <b>access</b> pattern. The selection of the <b>memory</b> <b>access</b> instructions to be instrumented {{is based on a}} new method, which clusters instructions {{on the basis of their}} compile/link-time observable address expressions and selects representatives of these clusters. This allows for reducing the runtime cost for running instrumented code, while still enabling high accuracy in the determination of <b>memory</b> <b>accesses.</b> The trade-off between overhead and precision of the tracing process is user-tunable, so that it can be set depending on the final objective of <b>memory</b> <b>access</b> tracing (say on-line vs off-line exploitation). Additionally, our approach can track <b>memory</b> <b>access</b> at different granularity (e. g., virtual-pages or cache line-sized buffers), thus having applications in a variety of different contexts. The effectiveness of our proposal is demonstrated via experiments with applications taken from the PARSEC benchmark suite...|$|R
50|$|Locality of {{reference}} {{refers to a}} property exhibited by <b>memory</b> <b>access</b> patterns.A programmer will change the <b>memory</b> <b>access</b> pattern (by reworking algorithms) to improve the locality {{of reference}}, and/or to increase potential for parallelism. A programmer or system designer may create frameworks or abstractions (e.g. C++ templates or higher-order functions) that encapsulate a specific <b>memory</b> <b>access</b> pattern.|$|R
40|$|Cache-only <b>memory</b> <b>access</b> (COMA) multiprocessors support {{scalable}} coherent {{shared memory}} with a uniform <b>memory</b> <b>access</b> programming model. The local portion of shared memory {{associated with a}} processor is organized as a cache. This cache-based organization of memory results in long remote <b>memory</b> <b>access</b> latencies. Latency-hiding mechanisms can reduce effective remote <b>memory</b> <b>access</b> latency by making data present in a processor's local memory {{by the time the}} data are needed. In this paper we study the effectiveness of latency-hiding mechanisms on the KSR 2 multiprocessor in improving the performance of three programs. The communication patterns of each program are analyzed and the mechanisms for latency hiding are applied. Results from a 52 -processor system indicate that these mechanisms hide {{a significant portion of the}} latency of remote <b>memory</b> <b>accesses.</b> The results also quantify benefits in overall application performance. Keywords: Cache-only <b>memory</b> <b>access</b> (COMA) multiprocessors, latency [...] ...|$|R
40|$|Scalable Flat Cache Only Memory Architectures (Flat COMA) are {{designed}} for reduced <b>memory</b> <b>access</b> latencies while minimizing programmer and operating system involvement. Indeed, to keep <b>memory</b> <b>access</b> latencies low, neither the programmer needs to perform clever data placement nor the operating system needs to perform page migration. The hardware automatically replicates the data and migrates it to the attraction memories of the nodes that use it. Unfortunately, part of the latency of <b>memory</b> <b>accesses</b> is superfluous. In particular, reads often perform unnecessary attraction <b>memory</b> <b>accesses,</b> require too many network hops, or perform necessary attraction <b>memory</b> <b>accesses</b> inefficiently. In this paper, we propose relatively inexpensive schemes that address these three problems. To eliminate unnecessary attraction <b>memory</b> <b>accesses,</b> we propose a small direct-mapped cache called Invalidation Cache (IVC). To {{reduce the number of}} network hops, the IVC is augmented with hint pointers to processors. T [...] ...|$|R
5|$|Computer {{architectures}} {{in which}} each element of main <b>memory</b> can be <b>accessed</b> with equal latency and bandwidth are known as uniform <b>memory</b> <b>access</b> (UMA) systems. Typically, {{that can be achieved}} only by a shared memory system, in which the memory is not physically distributed. A system that does not have this property is known as a non-uniform <b>memory</b> <b>access</b> (NUMA) architecture. Distributed memory systems have non-uniform <b>memory</b> <b>access.</b>|$|R
50|$|Most modern CPUs reorder <b>memory</b> <b>accesses</b> {{to improve}} {{execution}} efficiency (see memory ordering for types of reordering allowed). Such processors invariably give {{some way to}} force ordering in a stream of <b>memory</b> <b>accesses,</b> typically through a memory barrier instruction. Implementation of Peterson's and related algorithms on processors which reorder <b>memory</b> <b>accesses</b> generally requires use of such operations to work correctly to keep sequential operations from happening in an incorrect order. Note that reordering of <b>memory</b> <b>accesses</b> can happen even on processors that don't reorder instructions (such as the PowerPC processor in the Xbox 360).|$|R
30|$|The {{ratio of}} {{computation}} to <b>memory</b> <b>access</b> is N: 4 for basic dGEMM(N, N, N) and N: 2 for zGEMM(N, N, N). Compared with the computational amount, {{the amount of}} <b>memory</b> <b>access</b> is very small. Because of the rapid computational power and slow <b>memory</b> <b>access</b> performance, the <b>memory</b> wall is still the bottleneck of GEMM performance. Many {{attempts have been made}} to optimize the BLAS 3 with the normal optimization technologies such as loop unrolling, software pipelining or data prefetching of processor. Loop unrolling is used to enhance the re-use of the data in caches to reduce the accounts of <b>memory</b> <b>access.</b> Software pipelining is used to eliminate the correlation between the execution and <b>memory</b> <b>access,</b> and the execution and <b>memory</b> <b>access</b> units can progress in parallel. However, the theoretical peak performance is too high, and the time of <b>memory</b> <b>access</b> of processors cannot be concealed by the execution. Only approximately 35 % of the theoretical peak performance can be obtained. Moreover, the parameters of loop unrolling have been adjusted, and the performance is still very low. Therefore, the bottleneck cannot be solved by using normal optimization technologies.|$|R
50|$|These prefetches are {{non-blocking}} memory operations, i.e. these <b>memory</b> <b>accesses</b> do {{not interfere}} with actual <b>memory</b> <b>accesses.</b> They do not change {{the state of the}} processor or cause page faults.|$|R
40|$|Abstract—In this paper, {{a memory}} {{efficient}} Fine Grain Scalability (FGS) coefficient encoding method is proposed {{to reduce the}} external <b>memory</b> <b>access</b> requirement. In the H. 264 /AVC Scalable Video Extension, the FGS coefficients encoding is frame based. However, the frame based mechanism results in the difficulty of hardware implementation due to large internal memory requirements and external <b>memory</b> <b>accesses.</b> Therefore, a non-uniform memory size design which can achieve low external <b>memory</b> <b>access</b> is proposed to realize the macroblock based FGS coefficients encoding. Compared to previous work, our proposed method can save at least 38 KB external <b>memory</b> <b>accesses</b> per frame in average. I...|$|R
50|$|The {{processor}} can be paused {{with the}} address bus tri-stated for external direct <b>memory</b> <b>access</b> (DMA). <b>Memory</b> <b>accesses</b> are always 16 bits wide, with the CPU automatically performing read-before-write operations for byte-wide accesses.|$|R
30|$|In a H. 264 decoder, {{there are}} several modules that require {{intensive}} use of the off-chip memory. Wang [2] and Yoon [3] concluded that MC requires 75 % of all <b>memory</b> <b>access</b> in a H. 264 decoder, in contrast with only 10 % required for storing the frames. This high <b>memory</b> <b>access</b> ratio of the MC module demands for highly optimized <b>memory</b> <b>accesses</b> to improve the total performance of the decoder.|$|R
40|$|Over {{the last}} several decades, two {{important}} shifts {{have taken place in}} the computing world: first, the speed of processors has vastly outstripped the speed of <b>memory,</b> making <b>memory</b> <b>accesses</b> by far the most expensive operations that a typical symbolic program performs. Second, dynamically compiled languages such as Java and C# have become popular, placing new pressures on compiler writers to create effective systems for run-time code generation. This paper addresses the need created by the lagging speeds of <b>memory</b> <b>accesses</b> in the context of dynamically compiled systems. In such systems <b>memory</b> <b>access</b> optimization is important for resultant program performance, but the compilation time required by most traditional <b>memory</b> <b>access</b> optimizations is prohibitively high for use in such contexts. In this paper, we present a new analysis, memory dependence analysis, which amortizes the cost of performing <b>memory</b> <b>access</b> analysis to a level that is acceptable for dynamic compilation. In addition, we present two <b>memory</b> <b>access</b> optimizations based on this new analysis, and present empirical evidence that using this approach results in significantly improved compilation times without significant loss in resultant code quality...|$|R
30|$|As {{shown in}} (8), {{in order to}} enhance the {{performance}} P, the variables T_shuffle, T_sync, T_extra and λ _ 1 should be reduced, while ϱ and v should be increased. In the DAE architecture, APs and EPs can work in parallel. To reduce the <b>memory</b> <b>access</b> overhead, APs accomplish most missions of <b>memory</b> <b>access,</b> and the normal <b>memory</b> <b>access</b> unit is responsible for the remaining missions of <b>memory</b> <b>access.</b> Most GEMM tasks are computations, and extra overhead makes little difference to the overall runtime. ϱ is influenced by the computation to <b>memory</b> <b>access</b> overhead ratio, and it is mainly determined by the features of the algorithm and hardware. Variables ϱ and T_extra will not be discussed in this paper. In the following subsections, the optimizations of v, T_shuffle, λ _ 1, and T_sync are mainly discussed.|$|R
40|$|Some typical <b>memory</b> <b>access</b> {{patterns}} are provided and programmed in C, {{which can be}} used as benchmark to characterize the various techniques and algorithms aim to improve the performance of NUMA <b>memory</b> <b>access.</b> These access patterns, called MAP-numa (<b>Memory</b> <b>Access</b> Patterns for NUMA), currently include three classes, whose working data sets are corresponding to 1 -dimension array, 2 -dimension matrix and 3 -dimension cube. It is dedicated for NUMA <b>memory</b> <b>access</b> optimization other than measuring the memory bandwidth and latency. MAP-numa is an alternative to those exist benchmarks such as STREAM, pChase, etc. It is used to verify the optimizations' (made automatically/manually to source code/executive binary) capacities by investigating what locality leakage can be remedied. Some experiment results are shown, which give an example of using MAP-numa to evaluate some optimizations based on Oprofile sampling. © IFIP International Federation for Information Processing 2012. Some typical <b>memory</b> <b>access</b> {{patterns are}} provided and programmed in C, {{which can be used}} as benchmark to characterize the various techniques and algorithms aim to improve the performance of NUMA <b>memory</b> <b>access.</b> These access patterns, called MAP-numa (<b>Memory</b> <b>Access</b> Patterns for NUMA), currently include three classes, whose working data sets are corresponding to 1 -dimension array, 2 -dimension matrix and 3 -dimension cube. It is dedicated for NUMA <b>memory</b> <b>access</b> optimization other than measuring the memory bandwidth and latency. MAP-numa is an alternative to those exist benchmarks such as STREAM, pChase, etc. It is used to verify the optimizations' (made automatically/manually to source code/executive binary) capacities by investigating what locality leakage can be remedied. Some experiment results are shown, which give an example of using MAP-numa to evaluate some optimizations based on Oprofile sampling. © IFIP International Federation for Information Processing 2012...|$|R
40|$|This paper {{makes the}} {{following}} contributions: It proposes a new methodology for quantifying remote <b>memory</b> <b>access</b> contention on hardware DSM multiprocessors. The most valuable {{aspect of this}} methodology is that it assesses the impact of contention on real parallel programs running on real hardware. The methodology uses as input the number of accesses from each DSM node to each page in memory. A trace of the <b>memory</b> <b>accesses</b> of the program obtained at runtime from hardware counters is used to compute an accurate estimate of the fraction of execution time wasted due to contention. The paper presents also a new algorithm which detects potential hot spots in pages and resolves contention on them using dynamic page migration. The algorithm balances the remote <b>memory</b> <b>accesses</b> across the nodes of the system, while trying to improve <b>memory</b> <b>access</b> locality. Experiments with five parallel codes with irregular <b>memory</b> <b>access</b> patterns on a 128 processor Origin 2000 show that our algorithm yields respectable reductions of execution time, averaging 27. 7 %...|$|R
40|$|The {{available}} memory bandwidth {{of existing}} high performance computing platforms turns out {{as being more}} and more the limitation to various applications. Therefore, modern microarchitectures integrate the memory controller on the processor chip, {{which leads to a}} non-uniform <b>memory</b> <b>access</b> behavior of such systems. This access behavior in turn entails major challenges in the development of shared memory parallel applications. An improperly implemented <b>memory</b> <b>access</b> functionality results in a bad ratio between local and remote <b>memory</b> <b>access,</b> and causes low performance on such architectures. To address this problem, the developers of such applications rely on tools to make these kinds of performance problems visible. This work presents a new tool for the visualization of performance data of the non-uniform <b>memory</b> <b>access</b> behavior. Because of the visual design of the tool, the developer is able to judge the severity of remote <b>memory</b> <b>access</b> in a time-dependent simulation, which is currently not possible using existing tools...|$|R
50|$|In computing, remote direct <b>memory</b> <b>access</b> (RDMA) is {{a direct}} <b>memory</b> <b>access</b> from the <b>memory</b> of one {{computer}} into that of another without involving either one's operating system. This permits high-throughput, low-latency networking, which is especially useful in massively parallel computer clusters.|$|R
