66|1340|Public
5000|$|.....But can {{optionally}} skip to a <b>multiple</b> <b>thread</b> / {{process model}} (as in: you'll {{be free to}} block or use slow filesystems).|$|E
50|$|Single instruction, <b>multiple</b> <b>thread</b> (SIMT) is an {{execution}} model used in parallel computing where single instruction, multiple data (SIMD) {{is combined with}} multithreading.|$|E
50|$|The {{hardware}} schedules thread {{blocks to}} an SM. In general an SM can handle <b>multiple</b> <b>thread</b> blocks {{at the same}} time. An SM may contains up to 8 thread blocks in total. A thread ID is assigned to a thread by its respective SM.|$|E
5000|$|... #Subtitle level 3: Single instruction, <b>multiple</b> <b>threads</b> (SIMT) ...|$|R
5000|$|<b>Multiple</b> <b>threads</b> {{can exist}} {{within a single}} {{application}} domain.|$|R
5000|$|... #Caption: Fig 2: Locks and {{critical}} sections in <b>multiple</b> <b>threads</b> ...|$|R
5000|$|The browser uses cloud {{acceleration}} and data compression technology. UC Browser's servers {{act as a}} proxy which compresses and renders the data of web pages before sending it to users. This process helps to load web content faster. The browser can adapt to some network environments and support multi-file format downloading. In addition, UC Browser has HTML5 web app and cloud syncing features.It has the feature of [...] "fast download" [...] which increases the downloading speed using <b>multiple</b> <b>thread</b> connection download technique.|$|E
5000|$|An {{overlock}} {{is a kind}} of stitch that sews {{over the}} edge of one or two pieces of cloth for edging, hemming, or seaming. Usually an overlock sewing machine will cut the edges of the cloth as they are fed through (such machines being called [...] "sergers" [...] in North America), though some are made without cutters. The inclusion of automated cutters allows overlock machines to create finished seams easily and quickly. An overlock sewing machine differs from a lockstitch sewing machine in that it uses loopers fed by <b>multiple</b> <b>thread</b> cones rather than a bobbin. Loopers serve to create thread loops that pass from the needle thread to the edges of the fabric so that the edges of the fabric are contained within the seam.|$|E
50|$|A {{worm gear}} is {{a species of}} helical gear, but its helix angle is usually {{somewhat}} large (close to 90 degrees) and its body is usually fairly long in the axial direction. These attributes give it screw like qualities. The distinction between a worm and a helical gear is {{that at least one}} tooth persists for a full rotation around the helix. If this occurs, it is a 'worm'; if not, it is a 'helical gear'. A worm may have as few as one tooth. If that tooth persists for several turns around the helix, the worm appears, superficially, to have more than one tooth, but what one in fact sees is the same tooth reappearing at intervals {{along the length of the}} worm. The usual screw nomenclature applies: a one-toothed worm is called single thread or single start; a worm with more than one tooth is called <b>multiple</b> <b>thread</b> or multiple start. The helix angle of a worm is not usually specified. Instead, the lead angle, which is equal to 90 degrees minus the helix angle, is given.|$|E
5000|$|Example (C program): Display [...] "Hello, world." [...] using <b>multiple</b> <b>threads.</b>|$|R
5000|$|Special {{consideration}} {{must be made}} in scenarios where Flyweight {{objects are}} created on multiple threads.If the list of values is finite and known in advance the Flyweights can be instantiated {{ahead of time and}} retrieved from a container on <b>multiple</b> <b>threads</b> with no contention. If Flyweights are instantiated on <b>multiple</b> <b>threads</b> there are two options: ...|$|R
40|$|Abstract: Executing <b>multiple</b> <b>threads</b> on {{a single}} {{processor}} will {{play a key role}} the future scaling of computer performance, and while many new architectures propose novel uses for threads, few address the complexity required to support <b>multiple</b> <b>threads</b> in a single processor core. This paper describes extensions to WaveScalar, a recently proposed dataflow instruction set, and the WaveCache, a WaveScalar processor, that allow <b>multiple</b> <b>threads</b> to execute simultaneously. The original WaveCache is significantly less complex than a modern out-of-order von Neumann processor, and the modifications it requires for multithreading are very small. We demonstrate that resulting multithreaded architecture can efficiently execute applications from the Splash 2 benchmark suite. ...|$|R
40|$|Continuous-media {{applications}} {{require more}} efficient and flexible support from real-time threads than traditional real-time systems. It includes functionalities such as the dynamic management of thread attributes {{and the support of}} <b>multiple</b> <b>thread</b> models. In this paper, we will describe the design and implementation of user-level real-time threads on the RT-Mach micro kernel. Since they are implemented at user-level, both of the fast management of thread attributes and the support of <b>multiple</b> <b>thread</b> models are possible. ...|$|E
40|$|Abstract. A Single Instruction <b>Multiple</b> <b>Thread</b> CUDA {{interpreter}} provides SIMD like parallel {{evaluation of}} the whole GP population of 1 4 million reverse polish notation (RPN) expressions on graphics cards and nVidia Tesla. Using sub-machine code tree GP a sustain peak performance of 665 billion GP operations per second (10, 000 speed up) and an average of 22 peta GP ops per day is reported for a single GPU card on a Boolean induction benchmark never attempted before, let alone solved. ...|$|E
40|$|We {{propose the}} Fuce {{architecture}} based on dataflow computing model. The {{goal of this}} architecture is fusion of communication and execution. The <b>multiple</b> <b>thread</b> execution model of the architecture takes the continuation-based multithreading model that manages dependency among fine-grain 2 ̆ 2 uninterruptible 2 ̆ 2 threads. In this paper, the continuation-based multithreading model constructs more resourceful parallel I/O processing cooperated with by processor and operating system. Our model {{is different from the}} conventional I/O processing model that handles 2 ̆ 2 interrupt. 2 ̆ 2 We illustrate the continuation-based parallel I/O processing model...|$|E
50|$|If <b>multiple</b> <b>threads</b> {{have names}} which alias the same memory location, two {{problems}} arise.|$|R
5000|$|Copies files in-parallel (multi-threaded): Distributes <b>multiple</b> <b>threads</b> through logical cores during {{file copy}} operations.|$|R
5000|$|Sorting {{software}} can use <b>multiple</b> <b>threads,</b> {{to speed up}} the process on modern multicore computers.|$|R
40|$|This paper {{outlines}} {{the design of}} a compilation framework for applications in embedded systems programmed in Java and targeted at multi-threaded architectures. These architectures have <b>multiple</b> <b>Thread</b> Processing Units (TPUs) to support loop-level parallelism, where each TPU employs instruction-level parallelism, and data and control speculation techniques to improve performance. The paper describes the compilation framework, with particular emphasis on the design of the Java Intermediate Representation (JIR) based on Static SingleAssignment (SSA). The framework cleanly separates target-independent compilation issues from target-specific ones, which enables the comparative study of multi-threaded models of computation...|$|E
40|$|Design of high {{performance}} Web servers {{has become a}} recent research thrust to meet the increasing demand of networkbased services. In this paper, we propose a new Web server architecture, called multi-threaded PIPELINED Web server, suitable for Symmetric Multi-Processor (SMP) or Systemon -Chip (SoC) architectures. The proposed PIPELINED model consists of <b>multiple</b> <b>thread</b> pools, where each thread pool consists of five basic threads and two helper threads. The main advantages of the proposed model are global information sharing by the threads, minimal synchronization overhead due to less number of threads, and non-blocking I/O operations, possible with the helper threads...|$|E
40|$|This paper {{investigates the}} use of idle {{graphics}} processors to accelerate audio DSP for real-time algorithms. Several common algorithms have been identified for acceleration and were executed in <b>multiple</b> <b>thread</b> and block configurations to ascertain the desired configuration for the different algorithms. The GPU and CPU performing on the same data sizes and algorithm are compared against each other. From these results the paper discusses the importance of optimising the code for GPU operation including the allocating shared resources, optimising memory transfers and forced serialisation of feedback loops. It also introduces a new method for audio processing using GPU's as the default processor instead of an accelerator...|$|E
5000|$|<b>Multiple</b> <b>threads</b> {{can safely}} {{simultaneously}} access different [...] and [...] objects {{that point to}} the same object.|$|R
5000|$|Simultaneous {{multithreading}} - where {{functional elements}} of a CPU core are allocated across <b>multiple</b> <b>threads</b> of execution ...|$|R
5000|$|Thread safe: Implementation is {{guaranteed}} {{to be free of}} race conditions when accessed by <b>multiple</b> <b>threads</b> simultaneously.|$|R
3000|$|Thread {{execution}} {{is implemented}} using a single instruction <b>multiple</b> <b>thread</b> (SIMT) architecture, where each SP executes threads independently. Threads are executed in parallel, {{in groups of}} 32 consecutive threads, called warps. Every instruction time, a warp that is ready to execute is selected and the same instruction is then issued to all threads of that warp, so maximum speed is obtained {{if there are no}} divergent branches in the code. For example, in an [...] "IF" [...] statement, if 17 threads follow one branch, and 15 the other, the 17 are suspended and 15 execute, and then 17 execute with the 15 suspended, so effectively both branches of the [...] "IF" [...] statement are processed.|$|E
40|$|We {{present a}} unique and elegant {{graphics}} hardware realization of multi agent simulation. Specifically, we adapted Velocity Obstacles that suits well parallel computation on single instruction, <b>multiple</b> <b>thread,</b> SIMT, type architecture. We explore hash based nearest neighbors search to considerably optimize the algorithm when mapped on to the GPU. Moreover, to alleviate inefficiencies of agent level concurrency, primarily exposed in small agent count (≤ 32) scenarios, we exploit nested data parallel in unrolling the inner velocity iteration, demonstrating an appreciable performance increase. Simulation of ten thousand agents created with our system runs on current hardware at a real time rate of eighteen frames per second. Our software implementation builds on NVIDIA’s CUDA...|$|E
40|$|We {{propose the}} Fuce (FUsion of Comunication and Execution) architecture. The Fuce {{processor}} {{of the architecture}} executes all processing composed of fine-grain 2 ̆ 2 uninterruptible 2 ̆ 2 threads. The <b>multiple</b> <b>thread</b> execution model of the architecture takes the continuation-based multithreading model. The execution model takes dependency among threads as the continuation instruction. The parallel I/O processing with the execution model divides I/O processing into fine-grain threads, and associates in Chip Multiprocessor as processor cores and fine-grain threads. In this paper, the parallel I/O processing in the continuation-based multithreading compares the parallel I/O processing with interrupt. This paper shows the parallel I/O processing performance is almost equal, and the response time in the continuation-based multithreading can be shortened...|$|E
50|$|Multithreading - Managing <b>multiple</b> <b>threads,</b> {{to execute}} ActionScript 3 code in the {{background}} without freezing the user interface.|$|R
40|$|Observing and {{interacting}} with multi-thread {{programs can be}} difficult for the programmer. Simple input/output (I/O) can become a nightmare when <b>multiple</b> <b>threads</b> read and write simultaneously. A solution would separate the I/O streams of the <b>multiple</b> <b>threads,</b> windowing techniques can achieve this. This honours project report presents the design and implementation of Ceramic, a development tool which assists in observing {{and interacting}} with multi-thread programs. Multiple viewers (windows) can be opened to control I/ 0 streams of <b>multiple</b> <b>threads.</b> Ceramic has an object-oriented design based on design patterns captured from Mossenbock's OberonO viewer system. Another feature are the hierarchical tiling viewers which are a hybrid of Elastic Windows developed by Kandogan & Shneiderman. Tiling viewers have some significant advantages over overlapping windows which Ceramic has exploited...|$|R
25|$|Note that is process {{cannot be}} used on a small worm wheel that mate with a <b>multiple</b> <b>threaded</b> worm.|$|R
40|$|Formally {{ensuring}} the correctness of component-based, concurrent systems is an arduous task, mainly because exhaustive {{methods such as}} model-checking quickly run into state-explosion problems; this is typically caused by the <b>multiple</b> <b>thread</b> interleavings of the system being analysed, {{and the range of}} data the system can input and react to. Runtime Verification (RV) is an appealing compromise towards ensuring correctness, as it circumvents such scalability issues by only verifying the current system execution. Runtime Enforcement (RE) builds on RV by automating recovery procedures once a correctness violation is detected so as to mitigate or rectify the effects of the violation. We can therefore see Runtime enforcement as made of two parts: (i) Verification (Monitoring), and (ii) Recovery Actions. peer-reviewe...|$|E
40|$|General Purpose Graphical Processing Units (GPGPUs) rose to {{prominence}} {{with the release}} of the Fermi architecture by Nvidia in 2009. It introduced the idea of Single Instruction <b>Multiple</b> <b>Thread</b> (SIMT) execution, as well as dramatically improving the CUDA programming language used to program on GPGPUs. Since then, GPGPUs have grown as an alternative to traditional CPU based computing for a variety of parallel applications such as neural networking, Big-Data analytics, and machine learning. However, the SIMT execution model breaks down for programs with control divergence (namely branches) because the system can only support a single instruction stream. Thus, threads that do not take the current executing branch must wait their turn often leading to dramatic performance loss. In order to combat this, this paper proposes a Multiple Fragment <b>Multiple</b> <b>Thread</b> (MFMT) architecture allowing multiple instruction streams to execute in parallel on GPGPUs. ^ MFMT 2 ̆ 7 s key insight is that a small number of control flow paths can be supported by current GPGPUs without major modifications, instead harnessing resources that are under-utilized in control-divergent applications. This means that with minimal area and energy costs, GPGPUs can be transformed to support a small but meaningful number of instructions for different control flow paths (dubbed fragments) dramatically benefiting control-divergent programs. For non control-divergent applications, MFMT will naturally mimic to the original SIMT execution model and maintain the original high level of performance. ^ MFMT is evaluated via a GPGPU architectural simulator, comparing it with the baseline SIMT scheme as well as 2 current state of the art solutions for control-divergence: Multi-Path and Variable Warp Sizing (VWS). MFMT achieves 11...|$|E
40|$|Automatic {{verification}} of concurrent programs written in low-level languages like ANSI-C {{is an important}} task as multi-core architectures are gaining widespread adoption. Formal verification, although very valuable for this domain, rapidly runs into the state-explosion problem due to <b>multiple</b> <b>thread</b> interleavings. Recently, Bounded Model Checking (BMC) {{has been used for}} this purpose, which does not scale in practice. In this work, we develop a method to further constrain the search space for BMC techniques using underapproximations of data flow of shared memory and lazy demand-driven refinement of the approximation. A novel contribution of our method is that our underapproximation is guided by likely data-flow invariants mined from dynamic analysis and our refinement is based on proof-based learning. We have implemented our method in a prototype tool. Initial experiments on benchmark examples show potential performance benefit...|$|E
5000|$|TFSt: The actual, full-featured TFS functions, called {{directly}} by the runtime library. Supports <b>multiple</b> <b>threads</b> in a single application.|$|R
50|$|Using {{memory for}} {{communication}} inside a single program, e.g. among its <b>multiple</b> <b>threads,</b> is {{also referred to}} as shared memory.|$|R
5000|$|Simultaneous {{multithreading}} (SMT): Issue multiple {{instructions from}} <b>multiple</b> <b>threads</b> in one cycle. The processor must be superscalar to do so.|$|R
