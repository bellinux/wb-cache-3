18|35|Public
5000|$|The Librarian, {{supporting}} a core-image library, and optionally a <b>macro</b> <b>library</b> and a relocatable library.|$|E
5000|$|MTS Volume I: Introduction; Concepts and facilities; Calling conventions; Batch, Terminal, Tape, and Data Concentrator user's guides; Description of UMMPS and MTS; Files and devices; Command language; User Programs; Subroutine and <b>macro</b> <b>library</b> descriptions; Public or library file descriptions; and Internal specifications: Dynamic loader (UMLOAD), File and Device Management (DSRI prefix and postfix), Device Support Routines (DSRs), and File {{routines}} ...|$|E
5000|$|Input {{is entirely}} command driven, {{allowing}} unattended batch operation; a built-in Macro programming language allows {{the construction of}} a simple new [...] "command" [...] that can execute other commands in a complex manner. Users store their own previously written macros in a macro libraries (a single disk file), and there is an extensive [...] "permanent" [...] <b>macro</b> <b>library</b> available for all users.|$|E
40|$|A set of <b>macro</b> <b>libraries</b> {{has been}} {{developed}} that allows programmers to write portable FORTRAN code for multiprocessors. This document presents, in tutorial form, the macros used to implement three common synchronization patterns: self-scheduling DO-loops, barrier synchronization, and the askfor monitor...|$|R
50|$|In the 1980s {{and early}} 1990s, desktop PCs were only running {{at a few}} MHz and {{assembly}} language routines were commonly used to speed up programs written in C, Fortran, Pascal and others. These languages, at the time, used different calling conventions. Macros {{could be used to}} interface routines written in assembly language to the front end of applications written in almost any language. Again, the basic assembly language code remained the same, only the <b>macro</b> <b>libraries</b> needed to be written for each target language.|$|R
50|$|A new {{and even}} more {{powerful}} interpreted token-based macro recording and scripting language came with both DOS and Windows 6.0 versions, and that became {{the basis of the}} language named PerfectScript in later versions. PerfectScript has remained the mainstay scripting language for WordPerfect users ever since. It dealt with functions rather than with keystrokes. There was no way to import DOS macros, and users who had created extensive <b>macro</b> <b>libraries</b> were forced to continue using WordPerfect 5.1, or to rewrite all the macros from scratch using the new programming language.|$|R
5000|$|...TITLE HELLO WORLD [...]MCALL [...]TTYOUT,.EXITHELLO:: MOV #MSG,R1STARTING ADDRESS OF STRING1$: MOVB (R1)+,R0FETCH NEXT CHARACTER BEQ DONEIF ZERO, EXIT LOOP [...]TTYOUTOTHERWISE PRINT IT BR 1$REPEAT LOOPDONE: [...]EXITMSG: [...]ASCIZ /Hello, world!/ [...]END HELLOThe [...]MCALL pseudo-op warns the {{assembler}} {{that the}} code {{will be using}} the [...]TTYOUT and [...]EXIT macros. The [...]TTYOUT and [...]EXIT macros are defined in the standard system <b>macro</b> <b>library</b> to expand to the EMT instructions to call the RT-11 monitor to perform the requested functions.|$|E
50|$|By {{the late}} 1950s the macro {{language}} {{was followed by}} the Macro Assemblers. This was a combination of both where one program served both functions, that of a macro pre-processor and an assembler in the same package. This allowed assembly language programmers to implement their own macro-language and allowed limited portability of code between two machines running the same CPU but different operating systems, for example, early versions of MSDOS and CPM-86. The <b>macro</b> <b>library</b> would need to be written for each target machine but not the overall assembly language program. Note that more powerful macro assemblers allowed use of conditional assembly constructs in macro instructions that could generate different code on different machines or different operating systems, reducing the need for multiple libraries.|$|E
40|$|A SAS ® <b>macro</b> <b>library</b> {{developed}} in-house is {{a valuable}} resource for providing standard tools to support statistical planning, analysis, and presentation of results for work in the pharmaceutical industry. The library ensures quality, minimizes rework and validation efforts, and facilitates preparation of regulatory submissions and responses to agency questions. The maintenance and continued growth of the library requires dedicated efforts from many developers and users. This paper will describe the organization and maintenance of a SAS ® <b>macro</b> <b>library</b> that has evolved {{over the last seven}} years for use by statisticians and statistical programmers in support of development of drug and vaccine products. The distributed structure of subcommittees designed to handle the communication and processing for continued growth of the library will be described. The roles of each subcommittee and the challenges faced in the management of the <b>macro</b> <b>library</b> will be discussed...|$|E
40|$|Code {{generation}} for embedded special-purpose processors {{is usually}} a difficult task for compiler writers {{as well as for}} assembly language programmers. This report describes an experimental demonstration prototype of a code generation tool. The tool is a retargetable assembly-code-level macro expander capable of program flow analysis. The main advantage offered by this macro expander is its strong support for macro hierarchy. The enhanced modularity provided by hierarchical <b>macro</b> <b>libraries</b> can make the code (produced either by the compiler writer or by the assembly language programmer) more easily readable, maintainable, and reusable. Still, a procedure written in the macro language retains its machine-specificity and, consequently, its efficiency...|$|R
40|$|Standard {{cell and}} gate array <b>macro</b> <b>libraries</b> are in common use with {{workstation}} {{computer aided design}} (CAD) tools for {{application specific integrated circuit}} (ASIC) semi-custom application and have resulted in significant improvements in the overall design efficiencies as contrasted with custom design methodologies. Similar design methodology enhancements in providing for the efficient development of the library cells {{is an important factor in}} responding to the need for continuous technology improvement. The characteristics of a library development system that provides design flexibility and productivity enhancements for the library development engineer as he provides libraries in the state-of-the-art process technologies are presented. An overview of Gould's library development system ('Accolade') is also presented...|$|R
50|$|SMP/E manages {{two types}} of {{libraries}}. Target libraries (TLIBS) contain the executable code and other information used to run the system. Originally there were {{a limited number of}} target libraries: SYS1.LINKLIB for executable programs, SYS1.MACLIB for standard macros, etc., but as of 2012 each software product usually has its own set of target libraries. Distribution Libraries (DLIBS) contain the master copy of each element for a system. Each product (FMID) has its own set of distribution libraries which are normally used only by SMP/E. Libraries in OS/360 and successors, unlike directories in unix, usually contain only one type and format of data. A software package may have object libraries (MOD), ISPF panels (PNL), <b>macro</b> <b>libraries</b> (MAC) and many more.|$|R
40|$|TPS is a card-based word {{processor}} {{for use with}} the IBM OS/MVT operating system. TPS commands are imbedded in the free-format text input stream. User defined macros (with arguments) and a report generator <b>macro</b> <b>library,</b> (TPSLIB) provide automated formatting facilities similiar to those available from a competent technical typist. This report is a primer for TPS and TPSLIB; appendices provide detailed reference information and examples. (Author) [URL]...|$|E
40|$|MINITAB macros are an {{invaluable}} teaching aid for multivariate analysis. Specific examples are provided {{to illustrate the}} use of the MINITAB Multivariate <b>Macro</b> <b>Library</b> produced by the authors, and available via anonymous file transfer protocol and the World Wide Web. The components of this library are described, together with a collection of useful data sets. We discuss how the library may be used to teach a multivariate analysis course...|$|E
40|$|Centralized macro {{libraries}} {{can provide}} efficient solutions to project programming and analysis needs. They also may provide consistency of statistical methodologies, programming techniques {{and implementation of}} industry standards across projects. In the pharmaceutical industry, programming is required to comply with 21 Code of Federal Regulation (CFR) Part 11. This requires traceability of changes to programs or macros. Additionally, validation of programs or macros is needed to ensure they perform the functionality intended. SAS Drug Development (SDD) provides a regulatory compliant repository. This paper addresses the challenges of architecting a centralized <b>macro</b> <b>library</b> stored in SDD for use in analysis and reporting...|$|E
40|$|This paper {{describes}} an experimental prototype of a code generation tool for embedded special-purpose processors. The tool is a retargetable assembly-code-level macro expander capable of program flow analysis. The main {{advantage of the}} tool is its strong support for macro hierarchy: hierarchical <b>macro</b> <b>libraries</b> make the code (produced either by the compiler writer or by the assembly language programmer) more modular. 1. Introduction Code generation for embedded special-purpose processors is a demanding task: typically, one has to face both high performance requirements and a rather irregular processor architecture. This paper introduces an experimental prototype of a code generation tool for special-purpose processors. The tool is a retargetable assembly-code-level macro expander. It {{is intended to be}} used both {{as a part of a}} compiler back-end and as a stand-alone macro assembler. The novel feature of our macro expander is its program flow analysis capability. By being aware of the da [...] ...|$|R
50|$|While macro {{instructions}} can {{be defined}} by a programmer for any set of native assembler program instructions, typically macros are associated with <b>macro</b> <b>libraries</b> delivered with the operating system allowing access to operating system functions such as peripheral access by access methods (including macros such as OPEN, CLOSE, READ and WRITE) and other operating system functions such as ATTACH, WAIT and POST for subtask creation and synchronization. Typically such macros expand into executable code, e.g., for the EXIT macroinstruction, a list of define constant instructions, e.g., for the DCB macro, {{or a combination of}} code and constants, with the details of the expansion depending on the parameters of the macro instruction (such as a reference to a file and a data area for a READ instruction); he executable code often terminated in either a branch and link register instruction to call a routine, or a supervisor call instruction to call an operating system function directly.|$|R
50|$|Flatt {{served as}} one of four editors of the Revised^6 Report on the Scheme {{programming}} language. The report is influenced by his design of Racket, especially the module system, the exception system, the record system, the <b>macro</b> system, and <b>library</b> links.|$|R
40|$|Grice's maxims of {{conversation}} [Grice 1975] are framed as directives {{to be followed}} by a speaker of the language. This paper argues that, when considered {{from the point of view}} of natural language generation, such a characterisation is rather misleading, and that the desired behaviour falls out quite naturally if we view language generation as a goal-oriented process. We argue this position with particular regard to the generation of referring expressions. Comment: LaTeX file, needs aaai. sty (available from the cmp-lg <b>macro</b> <b>library).</b> This paper was presented at the 1996 AAAI Spring Symposium on Computational Models of Conversational Implicatur...|$|E
40|$|This paper {{presents}} a new method {{used in our}} RTL-synthesis tool to perform technology mapping with Sequen-tial Functional Modules (SFMs) such as counters, accumu-lators, shift-registers, or rotators from any target or <b>macro</b> <b>library.</b> If the library contains SFMs, the method automat-ically recognizes them. If an RTL design contains patterns that can be implemented on SFMs, the method maps them to the SFMs found in the target library. This mapping re-duces the design time by leveraging the library developer’s effort, leads to more regular and often smaller and faster designs, and helps to reduce timing and routing problems at later stages of the design process. ...|$|E
40|$|Macros {{provide one}} {{potential}} avenue for research into learning for planning. Most existing macro-learning methods acquire only macros that are observable from given example plans. This means they cannot learn other macros {{that are not}} observable from their training examples. A randomly selected collection of examples might not encompass many useful macros. However, a <b>macro</b> <b>library</b> should contain a comprehensive collection of macros. In this work, we learn macros that are not observable from given example plans. Further, we show the importance of viewing macro-learning as a search problem. With a view to building a macrolibrary, we particularly investigate the potential of exploring the whole macro space and thus give interesting insights...|$|E
40|$|The {{article is}} {{based on the idea that}} every library can design {{instruments}} for creating events and managing the important resources of today's world, especially to manage the changes. This process can only be successful if libraries use adequate marketing methods. Strategic marketing planning starts with the analysis of library's mission, its objectives, goals and corporate culture. By analysing the public environment, the competitive environment and the <b>macro</b> environment, <b>libraries</b> recognise their opportunities and threats. These analyses are the foundations for library definitions: What does the library represent?, What does it aspire to? Which goals does it want to reach? What kind of marketing strategy will it use for its target market...|$|R
40|$|Learn a {{quick and}} easy way to run {{multiple}} SAS programs from one “Main ” program. This method provides flexibility and helps with the organization of macro variables that are used across multiple programs. By using the %INCLUDE statement you can retrieve and run external SAS programs. This paper will help you prepare to write a Main program, organize your <b>macro</b> variables and <b>library</b> references, and utilize the %INCLUDE statement...|$|R
50|$|Many seminal {{programs}} {{from the early}} days of AI research have suffered from irreparable software rot. For example, the original SHRDLU program (an early natural language understanding program) cannot be run on any modern day computer or computer simulator, as it was developed during the days when LISP and PLANNER were still in development stage, and thus uses non-standard <b>macros</b> and software <b>libraries</b> which do not exist anymore.|$|R
40|$|Abstract- Technology {{advances}} allow integrating on {{a single}} chip entire system, including memories and peripherals. The test of these devices is becoming a major issue for manufacturing industries. This paper presents a methodology for inducing testprograms similar to genetic programming. However, it includes the ability to explicitly specify registers and resorts to directed acyclic graphs instead of trees. Moreover, it exploits a database containing the assembly-level semantic associated to each graph node. This approach is extremely efficient and versatile: candidate solutions are translated into source-code programs allowing millions of evaluations per second. The proposed approach is extremely versatile: the <b>macro</b> <b>library</b> allows easily changing target processor and environment. The approach was verified on three processors with different instruction sets, different formalisms and different conventions. A complete set of experiments on a test function are also reported for the SPARC processor. ...|$|E
40|$|This paper investigates {{specification}} and {{verification of}} synchronous circuits using DILL (Digital Logic in LOTOS). After {{an overview of}} the DILL approach, the paper focuses on the characteristics of synchronous circuits. A more constrained model is presented for specifying digital components and verifying them. Two standard benchmark circuits are specified using this new model, and analysed by the CADP toolset (Csar/Aldbaran Development Package). 1. INTRODUCTION 1. 1 Background DILL (Digital Logic in LOTOS [14, 16, 17, 25]) is an approach for specifying digital circuits using LOTOS (Language Of Temporal Ordering Specification [12]). DILL offers higher-level abstractions for describing hardware using a <b>macro</b> <b>library</b> for typical components and designs. DILL is used to formally specify digital hardware, using LOTOS at various abstraction levels. DILL addresses functional and timing aspects, supported by a library of common components and circuit designs, and using standard LOTOS tools. Th [...] ...|$|E
40|$|The more 'Enterprise ' {{you want}} in your Enterprise Business Intelligence initiative, the more {{confidence}} you should have in your integrated data. Given a heterogeneous collection of data sources, {{it should be a}} given that the owners of a BI system 1) discover data problems as soon as they occur upstream, and 2) notify the right resources about those issues when they occur, then 3) restart the data integration process after corrective action has been taken. This paper expects to show how SAS® ' Data Integration Studio (DI Studio) can be used both natively, and with custom transforms around a <b>macro</b> <b>library,</b> to help a development team manage a data integration process rigorously- one that will give accurate results at best, and avoid propagation of spurious errors at worst. We will also explore the utility of saving the information of each event in a database warehouse of tracked issues. Benefits of such a system could be to produce trending reports on the stability of the system, identify which sources might be less reliable with their data feeds, etc. This system would give you the ability to manage your own business processes intelligently...|$|E
50|$|Wilkes is also {{credited}} {{with the idea of}} symbolic labels, <b>macros</b> and subroutine <b>libraries.</b> These are fundamental developments that made programming much easier and paved the way for high-level programming languages. Later, Wilkes worked on an early timesharing systems (now termed a multi-user operating system) and distributed computing. Toward the end of the 1960s, Wilkes also became interested in capability-based computing, and the laboratory assembled a unique computer, the Cambridge CAP.|$|R
40|$|Defining {{specific}} {{guidelines and}} standards for designing and developing <b>library</b> <b>macros</b> can significantly increase the quality of those macros. This paper details some of the macro design and development guidelines we’ve defined in our organization for developing quality, easy to use, flexible macros that have minimal impact on the SAS session {{in which they are}} executed. Sample code to assist in adhering to these guidelines will also be discussed. This paper is intended for programmers with a sound foundation in SAS macro programming...|$|R
40|$|Graduation date: 2004 This thesis work {{evaluates the}} need for a re-configurable cross {{compiler}} for the X 32 V processor architecture and discusses the process of developing a cross compiler for X 32 V. X 32 V is a new processor intended at the embedded applications domain whose instruction set is designed based on the widely used MIPS processor. A Cross compiler for X 32 V was required for performing statistical analysis of the Instruction set and facilitates further profiling and recommendations for the architecture. GCC was chosen as the base compiler as it is a stable and reliable compilers with front-end support for multiple languages and back-end support for most of the architectures, contemporary and older as well. Porting GCC to X 32 V involved identifying architecture similar to X 32 V and obtaining the port for the same. Further, relevant modifications are made to the GCC back-end including the machine descriptions, the calling conventions and macros. Porting process requires detailed understanding of the target architecture mainly its instruction set, register file and define the ABI (Application Binary Interface) for the target. ABI is the method used to pass arguments, register usage conventions and layout of stack frame allocation. The coding is done in RTL (Register-transfer-language), which is a lisp like language and has its own in-built expressions and constructs. Also the floating point operations are supported using off-the-shelf specific <b>macro</b> <b>libraries.</b> The concept of re-configurability was kept in mind while developing a working X 32 V cross compiler. New instructions when required can be added with ease by editing the machine descriptions. This compiler produces assembly code for X 32 V which is further assembled to produce binaries using a third party assembler. The binaries are run on a simulator designed for this purpose. Thus we have set up basic infrastructure (cross compiler, simulator, and assembler) which would help in continuing this computer architectural research to new heights...|$|R
40|$|International audienceAutomated {{planning}} {{has achieved}} significant breakthroughs in recent years. Nonetheless, attempts to improve search algorithm efficiency remain {{the primary focus}} of most research. However, it is also possible to build on previous searches and learn from previously found solutions. Our approach consists in learning macro-actions and adding them into the planner's domain. A macro-action is an action sequence selected for application at search time and applied as a single indivisible action. Carefully chosen macros can drastically improve the planning performances by reducing the search space depth. However, macros also increase the branching factor. Therefore, the use of macros entails a utility problem: a trade-off has to be addressed between the benefit of adding macros to speed up the goal search and the overhead caused by increasing the branching factor in the search space. In this paper, we propose an online domain and planner-independent approach to learn 'useful' macros, i. e. macros that address the utility problem. These useful macros are obtained by statistical and heuristic filtering of a domain specific <b>macro</b> <b>library.</b> The library is created from the most frequent action sequences derived from an n-gram analysis on successful plans previously computed by the planner. The relevance of this approach is proven by experiments on International Planning Competition domains...|$|E
40|$|This {{paper will}} {{demonstrate}} the stored compiled macro {{facility in the}} multi-user clinical data management environment of Cancer and Leukemia Group B (CALGB). CALGB is a large cancer research cooperative group. Clinical data is gathered from hundreds of institutions {{and sent to the}} CALGB statistical center, where it is stored in a single database and processed by dozens of statisticians, programmers and data managers. At any given time CALGB coordinates more than 80 actively accruing clinical trials. This setting necessitates the need for streamlining data cleaning, data queries, and reports. The compiled stored macro facility permits all production level macros to be stored in one common location. In the multi-user setting it is crucial that all changes to the stored compiled macros in the macro catalog are authorized. The stored compiled macro facility permits a small group of users to control the content of the <b>macro</b> <b>library,</b> while a larger group of users has only read access. Step-by-step instructions on how to create and use compiled stored macros will be presented. Several examples from the CALGB statistical center will be used to demonstrate the flexibility and efficiency of the facility including multilevel macro programs which allow for easy customization of stored programs...|$|E
40|$|Abstract—Automated {{planning}} {{has achieved}} significant breakthroughs in recent years. Nonetheless, attempts to im-prove search algorithm efficiency remain {{the primary focus}} of most research. However, it is also possible to build on previous searches and learn from previously found solutions. Our approach consists in learning macro-actions and adding them into the planner’s domain. A macro-action is an action sequence selected for application at search time and applied as a single indivisible action. Carefully chosen macros can drastically improve the planning performances by reducing the search space depth. However, macros also increase the branching factor. Therefore, the use of macros entails a utility problem: a trade-off has to be addressed between the benefit of adding macros to speed up the goal search and the overhead caused by increasing the branching factor in the search space. In this paper, we propose an online domain and planner-independent approach to learn ’useful ’ macros, i. e. macros that address the utility problem. These useful macros are obtained by statistical and heuristic filtering of a domain specific <b>macro</b> <b>library.</b> The library is created from the most frequent action sequences derived from an n-gram analysis on successful plans previously computed by the planner. The relevance of this approach is proven by experiments on International Planning Competition domains. Keywords- automated planning; macro-actions; n-gram anal-ysis; supervised learning. I...|$|E
40|$|Abstract—To achieve high {{performance}} on modern computers, {{it is vital}} to map algorithmic parallelism to that inherent in the hardware. From an application developer’s perspective, it is also important that code can be maintained in a portable manner across a range of hardware. Here we present targetDP (target Data Parallel), a lightweight programming layer that allows the abstraction of data parallelism for applications that employ structured grids. A single source code may be used to target both thread level parallelism (TLP) and instruction level parallelism (ILP) on either SIMD multi-core CPUs or GPU-accelerated platforms. targetDP is implemented via standard C preprocessor <b>macros</b> and <b>library</b> functions, can be added to existing applications incrementally, and can be combined with higher-level paradigms such as MPI. We present CPU and GPU performance results for a benchmark taken from the lattice Boltzmann application that motivated this work. These demon-strate not only performance portability, but also the optimisation resulting from the intelligent exposure of ILP. I...|$|R
40|$|We {{describe}} the programming language FOBS-X (Extensible FOBS). FOBS-X is interpreted, and is {{intended as a}} universal scripting language. One {{of the more interesting}} features of FOBS-X is its ability to be extended, allowing it to be adopted to new scripting environments. FOBS-x is structured as a core language that is parsed by the interpreter, and an extended language that is translated to the core by macro expansion. The syntax of the language can easily be modified by writing new <b>macros.</b> The <b>library</b> for FOBS-X is reconfigurable, allowing the semantics of the language to be modified, and adapted to facilitate the interaction with interfaces to new scripting environments. This paper focuses on the tools used for the semantic extension of the language. A tool called FEDELE has been developed, allowing the user to add library modules to the FOBS-X library. In this way the semantics of the language can be enhanced, and the language can be adapted...|$|R
5000|$|Nim is statically typed, with {{a simple}} syntax. It {{supports}} compile-time metaprogramming features such as syntactic macros and term rewriting macros. Term rewriting <b>macros</b> enable <b>library</b> implementations of common data structures such as bignums and matrices to be implemented with an efficiency as {{if they would have}} been builtin language facilities. Iterators are supported and can be used as first class entities in the language as can functions, these features allow for functional programming to be used. Object-oriented programming is supported by inheritance and multiple dispatch. Functions can be generic and can also be overloaded, generics are further enhanced by the support for type classes. Operator overloading is also supported. Nim includes automatic garbage collection based on deferred reference counting with cycle detection. Andrew Binstock (editor-in-chief of Dr. Dobb's) says Nim (formerly known as Nimrod) [...] "presents a most original design that straddles Pascal and Python and compiles to C code or JavaScript." [...] Today, Nim compiles to C++ too.|$|R
