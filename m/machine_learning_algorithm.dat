1584|10000|Public
50|$|In <b>machine</b> <b>learning,</b> <b>algorithm</b> {{selection}} {{is better known}} as meta-learning. The portfolio of algorithms consists of machine learning algorithms (e.g., Random Forest, SVM, DNN), the instances are data sets and the cost metric is for example the error rate. So, {{the goal is to}} predict which <b>machine</b> <b>learning</b> <b>algorithm</b> will have a small error on each data set.|$|E
5000|$|Jakulin and Bratko (2003b) {{provide a}} <b>machine</b> <b>learning</b> <b>algorithm</b> which uses {{interaction}} information.|$|E
5000|$|ALOPEX (an acronym from [...] "ALgorithms Of Pattern EXtraction") is a {{correlation}} based <b>machine</b> <b>learning</b> <b>algorithm</b> first proposed by Tzanakou and Harth in 1974.|$|E
40|$|In {{this paper}} we present {{applications}} of different <b>machine</b> <b>learning</b> <b>algorithms</b> in aquaculture. <b>Machine</b> <b>learning</b> <b>algorithms</b> <b>learn</b> models from historical data. In aquaculture historical data are obtained from farm practices, yields, and environmental data sources. Associations between these different variables {{can be obtained}} by applying <b>machine</b> <b>learning</b> <b>algorithms</b> to historical data. In this paper we present applications of different <b>machine</b> <b>learning</b> <b>algorithms</b> in aquaculture applications. Comment: 2 page...|$|R
40|$|This paper {{presents}} {{work which}} extends previous corpus-based work on training <b>Machine</b> <b>Learning</b> <b>Algorithms</b> to perform Prepositional Phrase attachment. Besides recreating others' experiments {{to see how}} algorithms' performance changes {{with the number of}} training examples and using n-fold cross-validation to produce more accurate error rates, we implemented our own vanilla <b>Machine</b> <b>Learning</b> <b>Algorithms</b> as a comparison. We also had people perform exactly the same task as the <b>Machine</b> <b>Learning</b> <b>Algorithms</b> to indicate whether the way forward lies in improving <b>Machine</b> <b>Learning</b> <b>Algorithms</b> or in improving the data sets used to train <b>Machine</b> <b>Learning</b> <b>Algorithms.</b> The results from all these experiments feed into our other work transforming the Penn TreeBank into a more useful resource for training <b>Machine</b> <b>Learning</b> <b>Algorithms</b> to do Prepositional Phrase attachment...|$|R
40|$|<b>Machine</b> <b>learning</b> <b>algorithms</b> are everywhere, {{ranging from}} simple data {{analysis}} and pattern recognition tools used across the sciences to complex systems that achieve super-human performance on various tasks. Ensuring {{that they are}} well-behaved [...] -that they do not, for example, cause harm to humans or act in a racist or sexist way [...] -is therefore not a hypothetical problem {{to be dealt with}} in the future, but a pressing one that we address here. We propose a new framework for designing <b>machine</b> <b>learning</b> <b>algorithms</b> that simplifies the problem of specifying and regulating undesirable behaviors. To show the viability of this new framework, we use it to create new <b>machine</b> <b>learning</b> <b>algorithms</b> that preclude the sexist and harmful behaviors exhibited by standard <b>machine</b> <b>learning</b> <b>algorithms</b> in our experiments. Our framework for designing <b>machine</b> <b>learning</b> <b>algorithms</b> simplifies the safe and responsible application of <b>machine</b> <b>learning...</b>|$|R
50|$|The sample {{complexity}} of a <b>machine</b> <b>learning</b> <b>algorithm</b> represents {{the number of}} training-samples that it needs in order to successfully learn a target function.|$|E
5000|$|NPatternRecognizer , a fast <b>machine</b> <b>learning</b> <b>algorithm</b> library {{written in}} C#. It {{contains}} support vector machine, neural networks, bayes, boost, k-nearest neighbor, decision tree, ..., etc.|$|E
50|$|MatrixNet is a {{proprietary}} <b>machine</b> <b>learning</b> <b>algorithm</b> developed by Yandex and used widely throughout the company products. The algorithm {{is based on}} gradient boosting and was introduced since 2009.|$|E
40|$|When {{considering}} new datasets {{for analysis}} with <b>machine</b> <b>learning</b> <b>algorithms,</b> we encounter {{the problem of}} choosing the algorithm which is best suited for the task at hand. The aim of meta-level learning is to relate the performance of different <b>machine</b> <b>learning</b> <b>algorithms</b> to {{the characteristics of the}} dataset. The relation is induced on the basis of empirical data about the performance of <b>machine</b> <b>learning</b> <b>algorithms</b> on the different datasets...|$|R
30|$|The {{trends of}} <b>machine</b> <b>learning</b> studies for big data {{analytics}} {{can be divided}} into twofold: one attempts to make <b>machine</b> <b>learning</b> <b>algorithms</b> run on parallel platforms, such as Radoop [129], Mahout [87], and PIMRU [124]; the other is to redesign the <b>machine</b> <b>learning</b> <b>algorithms</b> to make them suitable for parallel computing or to parallel computing environment, such as neural network algorithms for GPU [126] and ant-based algorithm for grid [127]. In summary, both of them make it possible to apply the <b>machine</b> <b>learning</b> <b>algorithms</b> to big data analytics although still many research issues need to be solved, such as the communication cost for different computer nodes [86] and the large computation cost most <b>machine</b> <b>learning</b> <b>algorithms</b> require [126].|$|R
40|$|This thesis {{addresses}} {{improving the}} performance of <b>machine</b> <b>learning</b> <b>algorithms</b> with a particular focus on classification tasks with large, imbalanced and noisy datasets. The field of <b>machine</b> <b>learning</b> addresses {{the question of how}} best to use experimental or historical data to discover general patterns and regularities and improve the process of decision making. However, applying <b>machine</b> <b>learning</b> <b>algorithms</b> to very large scale problems still faces challenges. Additionally, class imbalance and noise in the data degrade the prediction accuracy of standard <b>machine</b> <b>learning</b> <b>algorithms.</b> The main focus of this thesis is designing <b>machine</b> <b>learning</b> <b>algorithms</b> and approaches that are faster, data efficient and less demanding in computational resources to achieve scalable algorithms for large scale problems. This thesis addresses these problems in active and online learning frameworks. The particular focus of the thesis is on Support Vector Machine (SVM) algorithm with classification problems, but the proposed approaches on active and online learning are also well extensible to other widely used <b>machine</b> <b>learning</b> <b>algorithms...</b>|$|R
50|$|Coupled Pattern Learner (CPL) is a <b>machine</b> <b>{{learning}}</b> <b>algorithm</b> which couples the semi-supervised {{learning of}} categories and relations to forestall {{the problem of}} semantic drift associated with boot-strap learning methods.|$|E
50|$|In 2006, Garfinkel {{introduced}} cross-drive analysis, an unsupervised <b>machine</b> <b>learning</b> <b>algorithm</b> for automatically reconstructing {{social networks}} from hard drives {{and other kinds}} of data-carrying devices that are likely to contain pseudo-unique information.|$|E
50|$|In 2014, it {{has been}} {{reported}} that a <b>machine</b> <b>learning</b> <b>algorithm</b> has been applied in Art History to study fine art paintings, and that it may have revealed previously unrecognized influences between artists.|$|E
40|$|Abstractâ€”While {{research}} has been conducted in <b>machine</b> <b>learning</b> <b>algorithms</b> and in privacy preserving in data mining (PPDM), a gap in the literature exists which combines the aforementioned areas to determine how PPDM affects common <b>machine</b> <b>learning</b> <b>algorithms.</b> The aim of this research is to narrow this literature gap by investigating how a common PPDM algorithm, K-Anonymity, affects common <b>machine</b> <b>learning</b> and data mining algorithms, namely neural networks, logistic regression, decision trees, and Bayesian classifiers. This applied research reveals practical implications for applying PPDM to data mining and <b>machine</b> <b>learning</b> and serves as a critical first step learning how to apply PPDM to <b>machine</b> <b>learning</b> <b>algorithms</b> and the effects of PPDM on <b>machine</b> <b>learning.</b> Results indicate that certain <b>machine</b> <b>learning</b> <b>algorithms</b> are more suited for use with PPDM techniques...|$|R
30|$|GPUs {{have been}} used in the {{development}} of faster <b>machine</b> <b>learning</b> <b>algorithms.</b> Some libraries such as GPUMiner [33] implement few <b>machine</b> <b>learning</b> <b>algorithms</b> on GPU using the CUDA framework. Experiments have shown many folds speedup using the GPU compared to a multicore CPU.|$|R
5000|$|Automated {{semantic}} knowledge extraction using <b>machine</b> <b>learning</b> <b>algorithms</b> {{is used to}} [...] "extract machine-processable {{information at}} a relatively low complexity cost". DBpedia uses structured content extracted from infoboxes by <b>machine</b> <b>learning</b> <b>algorithms</b> to create a resource of linked data in a Semantic Web.|$|R
50|$|Large margin nearest {{neighbor}} (LMNN) classification is a statistical <b>machine</b> <b>learning</b> <b>algorithm</b> for metric learning. It learns a pseudometric designed for k-{{nearest neighbor}} classification. The algorithm {{is based on}} semidefinite programming, a sub-class of convex optimization.|$|E
5000|$|... is a <b>machine</b> <b>learning</b> <b>algorithm</b> that generalizes the k-Nearest Neighbors (kNN) {{classifier}}.Whereas the kNN classifier supports binary classification, multiclass classification and regression, the Structured kNN (SkNN) allows {{training of}} a classifier for general structured output labels.|$|E
50|$|As {{part of his}} research, {{he created}} the <b>machine</b> <b>learning</b> <b>algorithm</b> behind Twitter's Who-To-Follow project and {{subsequently}} released it to Open Source. During that time he also led research tracking earthquake damage via Machine Learning, gaining wide media attention.|$|E
40|$|As {{statistical}} <b>machine</b> <b>learning</b> <b>algorithms</b> {{and techniques}} continue to mature, many researchers and developers see statistical <b>machine</b> <b>learning</b> {{not only as}} a topic of expert study, but also as a tool for software development. Extensive prior work has studied software development, but little prior work has studied software developers applying statistical <b>machine</b> <b>learning.</b> This paper presents interviews of eleven researchers experienced in applying statistical <b>machine</b> <b>learning</b> <b>algorithms</b> and techniques to human-computer interaction problems, as well as a study of ten participants working during a five-hour study to apply statistical <b>machine</b> <b>learning</b> <b>algorithms</b> and techniques to a realistic problem. We distill three related categories of difficulties that arise in applying statistical <b>machine</b> <b>learning</b> as a tool for software development: (1) difficulty pursuing statistical <b>machine</b> <b>learning</b> as an iterative and exploratory process, (2) difficulty understanding relationships between data and the behavior of statistical <b>machine</b> <b>learning</b> <b>algorithms,</b> and (3) difficulty evaluating the performance of statistical <b>machine</b> <b>learning</b> <b>algorithms</b> and techniques in the context of applications. This paper provides important new insight into these difficulties and the need for development tools that better support the application of statistical <b>machine</b> <b>learning...</b>|$|R
30|$|The {{purpose of}} using <b>machine</b> <b>learning</b> to analyze textual data is {{to lay the}} groundwork for {{constructing}} our own detection methods. The unique advantages of <b>machine</b> <b>learning</b> <b>algorithms</b> in pattern recognition make it a popular choice for analyzing textual data. There are two reasons for choosing <b>machine</b> <b>learning</b> <b>algorithms</b> to analyze data. First, the use of <b>machine</b> <b>learning</b> <b>algorithms</b> to analyze and verify the data first fully proves the feasibility of subsequent ideas. And the idea of telecommunication fraud detection is inspired by <b>machine</b> <b>learning</b> <b>algorithms.</b> We determine whether a call is a fraudulent or not through features. Second, after data collection, it is necessary to verify whether the data we collected fully describe the characteristics of telecommunication fraud and distinguish it from ordinary text. <b>Machine</b> <b>learning</b> could verify the quality of the dataset and help select appropriate dataset.|$|R
30|$|This {{section is}} mainly {{described}} {{the process of}} how we collect textual data and how we selected data by <b>machine</b> <b>learning</b> <b>algorithms.</b> This section divided into two parts. The first part is data collection and preprocessing {{and the second part}} is how we select data by <b>machine</b> <b>learning</b> <b>algorithms.</b> In the first part, we describe the data source, data collection, and data preprocessing. In the second part, we analyze textual data that collect from two data sources by <b>machine</b> <b>learning</b> <b>algorithms</b> and choose one data set of two to continue next research.|$|R
5000|$|Underfitting {{occurs when}} a {{statistical}} model or <b>machine</b> <b>learning</b> <b>algorithm</b> cannot capture the underlying trend of the data. Underfitting would occur, for example, when fitting a linear model to non-linear data. Such a model would have poor predictive performance.|$|E
5000|$|A {{statistical}} <b>machine</b> <b>learning</b> <b>algorithm</b> {{to detect}} whether a sentence contained a [...] "That's what she said" [...] double entendre {{was developed by}} Kiddon and Brun (2011). [...] There is an open-source Python implementation of Kiddon & Brun's TWSS system.|$|E
50|$|The {{structured}} support vector {{machine is}} a <b>machine</b> <b>learning</b> <b>algorithm</b> that generalizes the Support Vector Machine (SVM) classifier. Whereas the SVM classifier supports binary classification, multiclass classification and regression, the structured SVM allows training of a classifier for general structured output labels.|$|E
40|$|This paper {{considers}} the main aspects of Bazhenov Formation {{interpretation and application}} of <b>machine</b> <b>learning</b> <b>algorithms</b> for the Kolpashev type section of the Bazhenov Formation, application of automatic classification algorithms that would change the scale of research from small to large. <b>Machine</b> <b>learning</b> <b>algorithms</b> help interpret the Bazhenov Formation in a reference well and in other wells. During this study, unsupervised and supervised <b>machine</b> <b>learning</b> <b>algorithms</b> were applied to interpret lithology and reservoir properties. This greatly simplifies the routine problem of manual interpretation and has an economic effect {{on the cost of}} laboratory analysis...|$|R
5000|$|Apache Mahout - <b>Machine</b> <b>Learning</b> <b>algorithms</b> {{implemented}} on Hadoop ...|$|R
5000|$|... #Subtitle level 2: Attacks against <b>machine</b> <b>learning</b> <b>algorithms</b> (supervised) ...|$|R
50|$|Co-training is a <b>machine</b> <b>learning</b> <b>algorithm</b> {{used when}} there are only small amounts of labeled data and large amounts of unlabeled data. One of its uses is in text mining for search engines. It was {{introduced}} by Avrim Blum and Tom Mitchell in 1998.|$|E
50|$|Robust machine {{learning}} typically {{refers to the}} robustness of {{machine learning}} algorithms. For a <b>machine</b> <b>learning</b> <b>algorithm</b> to be considered robust, either the testing error has {{to be consistent with}} the training error, or the performance is stable after adding some noise to the dataset.|$|E
5000|$|Stability, {{also known}} as {{algorithmic}} stability, is a notion in computational learning theory of how a [...] <b>machine</b> <b>learning</b> <b>algorithm</b> is perturbed by small changes to its inputs. A stable learning algorithm is one for which the prediction does not change much when the training data is modified slightly. For instance, consider a <b>machine</b> <b>learning</b> <b>algorithm</b> that is being trained to recognize handwritten letters of the alphabet, using 1000 examples of handwritten letters and their labels ("A" [...] to [...] "Z") as a training set. One way to modify this training set is to leave out an example, so that only 999 examples of handwritten letters and their labels are available. A stable learning algorithm would produce a similar classifier with both the 1000-element and 999-element training sets.|$|E
5000|$|Deep {{learning}} is a class of <b>machine</b> <b>learning</b> <b>algorithms</b> that: ...|$|R
5000|$|Classify: {{a set of}} {{supervised}} <b>machine</b> <b>learning</b> <b>algorithms</b> for classification ...|$|R
5000|$|Regression: {{a set of}} {{supervised}} <b>machine</b> <b>learning</b> <b>algorithms</b> for regression ...|$|R
