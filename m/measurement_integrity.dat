15|173|Public
50|$|Isolation is {{the degree}} of {{attenuation}} from an unwanted signal detected {{at the port of}} interest. Isolation becomes more important at higher frequencies. High isolation reduces the influence of signals from other channels, sustains the integrity of the measured signal, and reduces system measurement uncertainties. For instance, a switch matrix may need to route a signal to a spectrum analyzer for measurement at -70 dBm and to simultaneously route another signal at +20 dBm. In this case, switches with high isolation, 90 dB or more, will keep the <b>measurement</b> <b>integrity</b> of the low-power signal.|$|E
50|$|The Data Cap Integrity Act, {{also called}} the Data <b>Measurement</b> <b>Integrity</b> Act, is a bill {{introduced}} in the United States Senate by Senator Ron Wyden. The bill would require Internet service providers that have bandwidth caps to only apply caps on service to reduce network congestion rather than discourage Internet use, count all data usage equally toward caps, regardless of its source or content, and use a standard method of metering data use, {{which is to be}} defined by the Federal Communications Commission (FCC). The FCC would also be required to provide software to allow users to monitor their bandwidth usage.|$|E
40|$|The <b>measurement</b> <b>integrity</b> of M. A. Rahim's Organizational Conflict Inventory (ROCI-II) and D. M. Roussel's Managerial Frames of Mind Survey (MFRS) was investigated, {{as was the}} {{relationship}} between scores on the two measures. Subjects were 369 managers from three types of organizations: (1) higher education; (2) not-for-profit public sector organizations; and (3) a for-profit corporation. Analyses conducted included classical reliability analyses, both exploratory and confirmatory factor analyses, and a multivariate analysis of relationships between the two measures. Results focusing on the <b>measurement</b> <b>integrity</b> of the ROCI-II are generally supportive of the measure, suggesting that it evaluates five dimensions, as postulated. Confirmatory factor analysis suggested that the scales were not orthogonal ccnstructs. Results focusing on the <b>measurement</b> <b>integrity</b> of the MFMS are less favorable. Exploratory factor analyses supported the measure, but confirmatory analyses {{did not result in}} a reasonable fit of the model to the data. Results suggest the need for additional item refinement and construct elaboration. The canonical analysis identified some manager archetypes for further exploration. Six tables and two graphs present study data. (Autbor/SLD) * Reproductions supplied by EDRS are the best that can be made * from the original document...|$|E
40|$|Guangdong University of Business Studies; Guangdong Key Lab. Electron. Commer. Mark. Appl. Technol.; Peoples Friendship University of Russia; South China University of Technology; Fudan UniversityTCGs Trusted Platform Modules {{provide the}} {{functionality}} of remote attestation, which {{based on the}} integrity of software components in a specific platform configuration. <b>Integrity</b> <b>Measurement</b> Architecture(IMA) is the accredited remote attestation methods which formulates the <b>integrity</b> <b>measurement</b> process and <b>integrity</b> reporting protocol. However, as a binary attestation, all <b>integrity</b> <b>measurements</b> must be exposed to remote party-verifier. This can disclose the privacy of attesting platform. In this paper, We slightly adapt the <b>Integrity</b> <b>Measurement</b> Architecture(IMA) to provide privacy preserving. System configuration is partitioned into privacy-relevant tasks based on the measurement relationships and dependency relationships between components. During integrity reporting in remote attestation, only the measurements of task-relevant software components are released to verifier. Shield factors are introduced to hide <b>integrity</b> <b>measurements</b> during measurement process and hide the task-irrelevant <b>integrity</b> <b>measurement</b> during <b>integrity</b> report. © 2010 IEEE...|$|R
30|$|The core {{mechanism}} of trusted computing is remote attestation; each node {{in the cloud}} computing environment is by remote attestation mechanism to build mutual trust and to guarantee the security of application. The validity of remote attestation is based on <b>integrity</b> <b>measurement.</b> In the TCG specifications, it measures the integrity only from BIOS to operating system, not including the application layer. In fact, the trustworthiness of the application layer is critical to cloud computing security. Therefore, the researchers proposed many enhanced <b>integrity</b> <b>measurement</b> mechanisms, for instance, IMA (<b>integrity</b> <b>measurement</b> architecture) [2] and PRIMA [3]. But in IaaS environment, the user virtual machine is a private property, the privacy protection {{is very important in}} the process of <b>integrity</b> <b>measurement.</b>|$|R
40|$|In {{order to}} solve the existed {{problems}} of dynamic <b>integrity</b> <b>measurement</b> method, a dynamic <b>integrity</b> <b>measurement</b> model based on Memory Paging Mechanism is proposed in this paper. The model takes memory pages of executable subjects as measurement objects. When the pages are scheduled into memory, the measurement points are inserted, the pages are measured, and their integrities are verified. The model is able to insure the integrity and trust of each executable page and assure that {{the integrity of the}} whole executable subjects is not destroyed. To verify this model, XEN hypercall mechanism is used to acquire executable subjects’ pages scheduled into memory, and the <b>integrity</b> <b>measurement</b> and verification codes are put into hypercall handler. Accordingly, dynamic <b>integrity</b> <b>measurement</b> to executable subjects is implemented...|$|R
40|$|Recent {{years have}} seen Corporate Responsibility (CR) {{developing}} rapidly as a key business issue. Addressing CR {{in the field of}} supply is an especially prominent and challenging area of activity. This paper outlines the findings of a pilot study of CR within the procurement processes of nine large UK organisations, with a predominant focus on utilities and service providers. Cross-case analysis shows that the rate of CR developments and the focus of CR elements that are given priority vary significantly. The paper then discusses a number of key issues including CR data <b>measurement,</b> <b>integrity</b> and sharing before using force field analysis to explore some of the key issues that need to be tackled if CR is to be more effectively integrated into supply and procurement strategies...|$|E
40|$|Founded {{in social}} {{cognitive}} theory, teachers' self-efficacy beliefs have been repeatedly associated with positive teaching behaviors and student outcomes. However, teacher efficacy {{has developed a}} storied history regarding construct validity and <b>measurement</b> <b>integrity.</b> Study of teacher efficacy now stands {{on the verge of}} maturity, but such developmental growth will likely be contingent on development of strong theoretical models and effective instrumentation to assess theoretical constructs. The purpose of the present article is to: (a) briefly review the theoretical foundation of teacher efficacy and critically evaluate historical attempts to measure teacher efficacy, (b) discuss important substantive implications stemming from efficacy research that may advance the field, (c) present recent measurement advances, and (d) highlight several methodologies that have been underutilized in development of teacher efficacy instruments. Teacher Efficacy Research 3 Teacher Self-Efficacy: [...] ...|$|E
40|$|Abstract- Cloud {{computing}} provides on-demand {{services with}} high {{performance in a}} flexible manner. Fast and easy deployment, scalability and service-oriented architecture are its main features. It promises substantial cost reduction together with flexibility than the traditional IT operations. Service provider’s offers substantial amount of services with different performance characteristics. A broker cloud service provider (BCSP) is implemented, which provide its own services and also act as a broker redirecting the request to other cloud service providers. The BCSP gets payment from clients and provides an efficient service to them. In turn BCSP would pay other cloud service providers for using their services. An algorithm is implemented in BCSP based on client requests analysis that provides faster and cost-efficient of allocation resources for clients request. This technique provides an effective requestresource allocation based on various criteria. Index Terms- pricing and resource allocation, performance attributes, performance and usage <b>measurement,</b> <b>integrity,</b> availability C I...|$|E
30|$|In {{this paper}} we propose an {{architecture}} for a cloud-based <b>Integrity</b> <b>Measurement</b> Service (cIMS). The cIMS performs {{the evaluation of the}} <b>integrity</b> <b>measurements</b> received about the client, and issues a trust score reflecting its evaluation against one or more predetermined profiles for client measurements. We use the classic SAML 2.0 ecosystem[5] as a means to illustrate usage of the cIMS. In this model, a client seeking access to a Service Provider (SP) must first be authenticated by an Identity Provider (IdP), who issues SAML assertions pertaining to the client. Here we extend the SAML 2.0 model by having the IdP request also from the client an <b>integrity</b> <b>measurement</b> report. In order to satisfy the need for a high LOA level, we propose the use of a trustworthy portable computing platform, the Trust Extension Device (TED) to provide the client-side trusted computing environment capable of performing <b>integrity</b> <b>measurements</b> of the client-side components.|$|R
30|$|<b>Integrity</b> <b>measurement</b> is the {{foundation}} of remote attestation. In TCG specifications, <b>integrity</b> <b>measurement</b> is defined as the process of obtaining metrics of platform characteristics that affects the integrity (trustworthiness) of a platform and putting digests of those metrics in PCRs. Based on the <b>integrity</b> <b>measurement,</b> the TCG solution of remote attestation for platform authentication is sometimes called binary attestation. The advantages of binary attestation process are simple and reliable, with no other trusted third party involved. IMA is the solution of IBM to the binary attestation [2]. As the same as IMA, [6 – 8] are typical of static measurement method. To solve the problem of TOCTOU [9, 10], a dynamic measurement method is proposed.|$|R
3000|$|Generate nonce by TPM of MVM {{and send}} <b>integrity</b> <b>measurement</b> command with nonce to {{front-end}} measurement module [...]...|$|R
40|$|The 5300. 4 {{series of}} NASA Handbooks for Reliability and Quality Assurance Programs have {{provisions}} {{for the establishment}} and utilization of a documented metrology system to control measurement processes and to provide objective evidence of quality conformance. The intent of these provisions is to assure consistency and conformance to specifications and tolerances of equipment, systems, materials, and processes procured and/or used by NASA, its international partners, contractors, subcontractors, and suppliers. This Measurement Assurance Program (MAP) guideline has the specific objectives to: (1) ensure the quality of measurements made within NASA programs; (2) establish realistic measurement process uncertainties; (3) maintain continuous control over the measurement processes; and (4) ensure measurement compatibility among NASA facilities. The publication addresses MAP methods as applied within and among NASA installations {{and serves as a}} guide to: control measurement processes at the local level (one facility); conduct measurement assurance programs in which a number of field installations are joint participants; and conduct <b>measurement</b> <b>integrity</b> (round robin) experiments in which a number of field installations participate to assess the overall quality of particular measurement processes at a point in time...|$|E
40|$|Summary Recent {{years have}} seen Corporate Responsibility (CR) {{developing}} rapidly as a key business issue. CR has increasingly come to embrace social, ethical as well as environmental and sustainability challenges. Addressing CR {{in the field of}} procurement is an especially prominent and demanding area of activity. This paper outlines the findings of a pilot study of CR within the procurement processes of nine large organisations, with a predominant focus on utilities and service providers. Cross-case analysis shows that the rate of CR developments and the focus of CR elements that are given priority vary significantly. The paper then discusses a number of key issues including terminological complexity and CR data <b>measurement,</b> <b>integrity</b> and sharing before using force-field analysis to explore some of the key issues that need to be tackled if CR is to be more effectively integrated into supply and procurement strategies. Proposals are made to reduce cynicism and inertia as well as increasing CR data coverage, staff awareness of CR and changing existing reward mechanisms with respect to risk. Corporate responsibility Procurement Supply Sustainability Risk efficiency Force-field analysis...|$|E
40|$|Researchers {{in early}} {{intervention}} recognize that instruments {{used to measure}} child developmental status should yield reliable and valid data. Many test manuals, however, present little empirical evidence regarding psychometric integrity of test scores, especially as regards young children with exception-alities. We use data on the Battelle Developmental Inventory (BDI) from a sample of young children with disabilities as a heuristic for demonstrating the importance of reliability and validity analyses and to illustrate {{some of the many}} plausible analyses that can be employed by researchers. Establishing <b>measurement</b> <b>integrity</b> is a critical component of all quantitative research. Without establishing the degree of psychometric integrity of scores that are subjected to substantive analyses, little con-fidence can be placed in the noteworthiness of substantive conclusions (Kerlinger, 1986). The importance of the preceding observations cannot be over-emphasized. As Kerlinger (1986) noted, "Poor measurement can inval-idate any scientific investigation. There is growing understanding that all measuring instruments must be critically and empirically examined for their reliability and validity " (p. 431). Specific to early interven...|$|E
40|$|Abstract. <b>Integrity</b> <b>measurement</b> and {{attestation}} mechanisms {{have already}} been developed for PC and server platforms, however, porting these technologies directly on mobile and resource-limited devices does not truly satisfy their performance constraints. Therefore, there are ongoing research efforts on mobileefficient <b>integrity</b> <b>measurement</b> and attestation mechanisms. In this {{paper we propose a}} simple and efficient solution for this problem by considering the unique features of mobile phone devices. Our customized secure boot mechanism ensures that a platform can boot to a secure state. During runtime an information flow–based integrity model is leveraged to maintain high integrity status of the system. Our solution satisfies identified security goals of <b>integrity</b> <b>measurement</b> and attestation. We have implemented our solution on a LiMo compatible mobile phone platform. ...|$|R
30|$|We use {{the term}} “Client” to denote the {{application}} software running on the TED platform that performs the SSO to the IdP. The Client software is the piece of software that integrates the authentication client and is entity that interacts with the SP and IdP {{on behalf of the}} user. It is also the software that triggers the <b>integrity</b> <b>measurements</b> performed by TED, and as such must always be included in any platform <b>measurements</b> and in <b>integrity</b> self-checks.|$|R
40|$|Mobile phones {{have evolved}} into {{indispensable}} devices that run many exciting applications that users can download from phone vendor’s application stores. However, {{as it is}} not practical to fully vet all application code, users may download malware-infected applications, which may steal or modify security-critical data. In this paper, we propose a security architecture for phone systems that protects trusted applications from such downloaded code. Our architecture uses reference monitors in the operating system and user-space services to enforce mandatory access control policies that express an approximation of Clark-Wilson integrity. In addition, we show how we can justify the integrity of mobile phone applications by using the Policy Reduced <b>Integrity</b> <b>Measurement</b> Architecture (PRIMA), which enables a remote party to verify the integrity of applications running on a phone. We have implemented a prototype on the Openmoko Linux Platform, using an SELinux kernel with a PRIMA module and user-space services that leverage the SELinux user-level policy server. We find that the performance of enforcement and <b>integrity</b> <b>measurement</b> is satisfactory, and the SELinux policy can be reduced in size by 90 % (although even more reduction should ultimately be possible), enabling practical system integrity with a desirable usability model. Copyright c ○ 2009 John Wiley & Sons, Ltd. KEY WORDS: Mobile phones, <b>integrity</b> <b>measurement,</b> Clark-Wilson <b>integrity,</b> mandatory access control, reference monitor, SELinux 1...|$|R
40|$|The <b>measurement</b> <b>integrity</b> of the NOVA 201 digital {{ultrasonic}} {{thickness gage}} (NOVA gage) was demonstrated {{by comparing the}} NOVA gage measurements to the thickness gage measurements, and determining the bias and uncertainty of the NOVA gage when measuring redesigned solid rocket motor (RSRM) hardware per engineering test plans (ETP). The NOVA gage was tested by three different operators on steel and aluminum RSRM hardware for wall thickness. The {{results show that the}} measurement bias is not consistent. The uncertainty of the bias is caused by the heterogeneous material properties of the RSRM components that influence the time of flight of ultrasonic waves. The measurement uncertainty inherent to the design and operation of the NOVA gage is less in comparison to the uncertainty of the bias. The total measurement uncertainty cannot be substantially reduced by taking more than one measurement. There is no correlation between bias and the surface finish range of this test unless 3 -in-One oil is used as a couplant, in which case {{there appears to be a}} slight trend. There is no correlation between uncertainty and the surface finish range. The measurement uncertainty of the NOVA gage can be reduced using 3 -in-One oil as a couplant...|$|E
40|$|This {{paper will}} {{describe}} CiDRA's patented clamp-on, passive sonar array-based flow meter technology which performs two fundamental and independent measurements – flow rate and entrained air. Firstly, the meter provides the {{volumetric flow rate}} of the mixture by measuring {{the speed at which}} naturally occurring turbulent structures convect with the flow past an axial array of sensors wrapped around existing process pipe. Secondly, the meter utilizes similar sonar-based processing techniques and the naturally occurring acoustical propagation in the process pipe to measure sound speed and hence the entrained air levels in slurries and fluids. This unique ability enables robust, reliable flow measurements {{in a wide variety of}} flows – high solids content slurries, heavy oils, bitumen flows and liquids/slurries with entrained air content. Also to be presented in this paper is the adoption and application of CiDRA’s SONARtrac™ product technology in a variety of oil sands, oil & gas, and minerals processing applications, helping to address the needs of <b>measurement</b> <b>integrity,</b> reliability and value delivery to the customer. In addition, case studies will be presented describing how the clamp-on sonar-based technology can be leveraged and applied to help characterize and deliver new insight into fluid mechanics of slurries and fluids in these industries, leveraging the two fundamental measurements of the clamp-on, passive sonar-based technology into new product extensions such as velocity profiling, gas hold-up for column flotation and secondary-phase measurement for density-based meters in the presence of air/gas...|$|E
40|$|The National Aeronautics and Space Administration (NASA) Glenn Research Center (GRC) Plum Brook Station (PBS) Spacecraft Propulsion Research Facility, {{commonly}} referred to as B- 2, is NASA s third largest thermal-vacuum facility with propellant systems capability. B- 2 has completed a modernization effort of its facility legacy data, video and control systems infrastructure to accommodate modern integrated testing and Information Technology (IT) Security requirements. Integrated systems tests have been conducted to demonstrate the new data, video and control systems functionality and capability. Discrete analog signal conditioners have been replaced by new programmable, signal processing hardware that is integrated with the data system. This integration supports automated calibration and verification of the analog subsystem. Modern measurement systems analysis (MSA) tools are being developed to help verify system health and <b>measurement</b> <b>integrity.</b> Legacy hard wired digital data systems have been replaced by distributed Fibre Channel (FC) network connected digitizers where high speed sampling rates have increased to 256, 000 samples per second. Several analog video cameras have been replaced by digital image and storage systems. Hard-wired analog control systems have been replaced by Programmable Logic Controllers (PLC), fiber optic networks (FON) infrastructure and human machine interface (HMI) operator screens. New modern IT Security procedures and schemes have been employed to control data access and process control flows. Due to the nature of testing possible at B- 2, flexibility and configurability of systems has been central to the architecture during modernization...|$|E
40|$|Software <b>integrity</b> <b>measurement</b> and {{attestation}} (M&A) {{are critical}} technologies {{for evaluating the}} trustworthiness of software platforms. To best support these technologies, next generation systems must provide a centralized service for securely selecting, collecting, and evaluating <b>integrity</b> <b>measurements.</b> Centralization of M&A avoids duplication, minimizes security risks to the system, and ensures correct ad- ministration of integrity policies and systems. This paper details the desirable features and properties of such a system, and introduces Maat, a prototype implementation of an M&A service that meets these properties. Maat is a platform service that provides a centralized policy-driven framework for determining which measurement tools and protocols to use {{to meet the needs}} of a given integrity evaluation. Maat simplifies the task of integrating <b>integrity</b> <b>measurements</b> into a range of larger trust decisions such as authentication, network access control, or delegated computations...|$|R
40|$|Abstract—Emerging {{distributed}} computing architectures, such as grid and cloud computing, {{depend on the}} high integrity execution of each system in the computation. While <b>integrity</b> <b>measurement</b> enables systems to generate proofs of their integrity to remote parties, we find that current <b>integrity</b> <b>measurement</b> approaches are insufficient to prove runtime integrity for systems in these architectures. <b>Integrity</b> <b>measurement</b> approaches that are flexible enough have an incomplete view of runtime integrity, possibly leading to false integrity claims, and approaches that provide comprehensive integrity do so only for computing environments that are too restrictive. In this paper, we propose an architecture for building comprehensive runtime integrity proofs for general purpose systems in {{distributed computing}} architectures. In this architecture, we strive for classical integrity, using an approximation of the Clark-Wilson integrity model as our target. Key to building such integrity proofs is a carefully crafted host system whose long-term integrity can be justified easily using current techniques and a new component, called a VM verifier, which comprehensively enforces our integrity target on VMs. We have built a prototype based on the Xen virtual machine system for SELinux VMs, and find that distributed compilation can be implemented, providing accurate proofs of our integrity target with less than 4 % overhead. Keywords-cloud computing, <b>integrity</b> <b>measurement,</b> virtual machines I...|$|R
40|$|This paper {{presents}} HyperSentry, a novel {{framework to}} enable <b>integrity</b> <b>measurement</b> of a running hypervisor (or any other highest privileged software {{layer on a}} system). Unlike existing solutions for protecting privileged software, Hyper-Sentry does not introduce a higher privileged software layer below the <b>integrity</b> <b>measurement</b> target, which could start another race with malicious attackers in obtaining the highest privilege in the system. Instead, HyperSentry introduces a software component that is properly isolated from the hypervisor to enable stealthy and in-context measurement of the runtime integrity of the hypervisor. While stealthiness is necessary {{to ensure that a}} compromised hypervisor does not have a chance to hide the attack traces upon detecting an up-coming measurement, in-context measurement is necessary to retrieve all the needed inputs for a successful <b>integrity</b> <b>measurement.</b> HyperSentry uses an out-of-band channel (e. g., Intelligent Platform Management Interface (IPMI), which is commonly available on server platforms) to trigger the stealthy measurement, and adopts the System Management Mode (SMM) to protect its base code and critical data. A key contribution of HyperSentry is the set of novel techniques that overcome SMM’s limitation, providing an <b>integrity</b> <b>measurement</b> agent with (1) the same contextual information available to the hypervisor, (2) completely protected execution, and (3) attestation to its output. To evaluate HyperSentry...|$|R
40|$|Real-time {{monitoring}} of physiological data {{can reduce the}} likelihood of injury in noncombat military personnel and first-responders. MIT Lincoln Laboratory is developing a tactical Real-Time Physiological Status Monitoring (RT-PSM) system architecture and reference implementation named OBAN (Open Body Area Network), the purpose of which is to provide an open, government-owned framework for integrating multiple wearable sensors and applications. The OBAN implementation accepts data from various sensors enabling calculation of physiological strain information which may be used by squad leaders or medics to assess the team's health and enhance safety and effectiveness of mission execution. Security in terms of <b>measurement</b> <b>integrity,</b> confidentiality, and authenticity is an area of interest because OBAN system components exchange sensitive data in contested environments. In this thesis, I analyze potential cyber-security threats and their associated risks to a generalized version of the OBAN architecture. Using the threat analysis, I identify security requirements for RT-PSM systems and describe the development of a secure RT-PSM system, called the Authenticated and Trustworthy Open Body Area Network (AUTOBAN) proof-of-concept implementation, that meets those requirements using cryptographic primitives that operate efficiently on low-power embedded devices. The threat analysis and proof-of-concept application, are intended to inform the development of secure RT-PSM architectures and implementations. by John H. Holliman, III. Thesis: M. Eng., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 2016. This electronic version was submitted by the student author. The certified thesis is available in the Institute Archives and Special Collections. Cataloged from student-submitted PDF version of thesis. Includes bibliographical references (pages 89 - 94) ...|$|E
40|$|The thesis {{deals with}} the subject of process {{performance}} measurement in a financial accounting department {{in the context of}} outsourcing these processes or their parts. The aim of the thesis is to propose a set of indicators for measuring the performance of financial accounting department in the context of processes in an international telecommunications company with respect to effective <b>measurement,</b> <b>integrity</b> with corporate business processes and meaningful and correct values of these metrics. Firstly, in the theoretical part, the reader is informed about the core processes in a finance accounting department. The reader is informed about the basic terms related to performance measurement, in particular KPI's (definition, implementation, evaluation, meaning [...] .). Finally, basic concepts associated with outsourcing are explained and the importance of performance indicators in the relationship between the supplier of the accounting services and the customer is outlined. The practical part {{deals with the}} design of performance indicators for a finance accounting department of an international telecommunications company. The indicators are designed to reflect the processes in the company. An emphasis is on those processes (or parts thereof) that are provided externally - by a third party provider of accounting services. Each indicator is defined by its description, meaning and method of measurement. If any adjustments to the processes are necessary, these process changes are described. The weaknesses and risks of implementation are indicated in the following section. The results of the individual measurements are presented for a minimum of 6 consecutive months (where it was possible to obtain data retrospectively, the values are calculated for the minimum of 12 months) ...|$|E
30|$|During the <b>measurement,</b> file <b>{{integrity}}</b> {{has only}} two results: integrity and non-integrity, we use Beta distribution to describe it. Let m represent the number of <b>measurement</b> result of <b>integrity</b> and n represent the number of measurement of non-integrity. As in Eq.(3), P represents the probability of measurement result of non-integrity, let α[*]=[*]m[*]+[*] 1 and β[*]=[*]n[*]+[*] 1. When 10 files are integrity measured, if eight results are with integrity, two results are with non-integrity, then the probability density distribution of probability p that measured the result as with integrity is f(p|(8 [*]+[*] 1),(2 [*]+[*] 1)[*]=[*]f(p| 9, 3)), according to the mathematical expectation of Beta distribution equation Exp(p| 9, 3)[*]=[*] 0.75. Here, 0.75 represents that the probability of integrity of the file during measurement is 0.75.|$|R
40|$|Trusted {{computing}} allows attesting remote system’s trustworthiness {{based on}} the software stack whose integrity has been measured. However, attacker can corrupt system as well as measurement operation. As a result, nearly all <b>integrity</b> <b>measurement</b> mechanism suffers {{from the fact that}} what is measured may not be same as what is executed. To solve this problem, a novel <b>integrity</b> <b>measurement</b> called dynamic instruction trace measurement (DiT) is proposed. For DiT, processor’s instruction cache is modified to stores back instructions to memory. Consequently, it is designed as a assistance to existing <b>integrity</b> <b>measurement</b> by including dynamic instructions trace. We have simulated DiT in a full-fledged system emulator with level- 1 cache modified. It can successfully update records at the moment the attestation is required. Overhead in terms of circuit area, power consumption, and access time, is less than 3 % for most criterions. And system only introduces less than 2 % performance overhead in average...|$|R
30|$|Client Authenticates to IdP: Here the Client {{has been}} {{redirected}} by the Cloud Provider (ie. SP) to the Identity Provider (IdP) for user authentication. The method for user authentication is out-of scope in this paper, {{and has been}} well treated elsewhere (for example, see SAML 2.0 SSO profile).After successful authentication by the IdP, the Client is further re-directed to the cloud-based <b>Integrity</b> <b>Measurement</b> Service (cIMS) selected by the IdP. The cIMS {{is assumed to be}} a trusted third party operating under the same trust framework as the IdP and Cloud provider.In requesting that an <b>integrity</b> <b>measurement</b> be performed, the IdP needs to indicate which components in the client’s environment must be measured. One possible approach is for the IdP to include (in its re-direction of the Client) an integrity schema [12] which indicates to the cIMS which platform components are of interest to the IdP.In this way the Client can initiate the TED platform to perform <b>measurements</b> following the <b>integrity</b> schema. Later, in compiling the integrity report, TED will format the report also following the integrity schema, and return it The integrity schema should be a core part of the client-profile that the IdP maintains for that Client.|$|R
40|$|We propose an <b>integrity</b> <b>measurement</b> {{approach}} {{based on}} information flow integrity, which we call the Policy-Reduced <b>Integrity</b> <b>Measurement</b> Architecture (PRIMA). The recent availability of secure hardware has made it practical for a system to measure its own integrity, such that it can generate an integrity proof for remote parties. Various approaches have been proposed, but most simply measure the loaded code and static data to approximate runtime system integrity. We find that these approaches suffer from two problems: (1) the load-time measurements of code alone do not accurately reflect runtime behaviors, {{such as the use}} of untrusted network data, and (2) they are inefficient, requiring all measured entities to be known and fully trusted even if they have no impact on the target application. Classical integrity models are {{based on information}} flow, so we design the PRIMA approach to enable measurement of information flow integrity and prove that it achieves these goals. We prove how a remote party can verify useful information flow integrity properties using PRIMA. A PRIMA prototype has been built based on the open-source Linux <b>Integrity</b> <b>Measurement</b> Architecture (IMA) using SELinux policies to provide the information flow...|$|R
40|$|Runtime <b>integrity</b> <b>measurement</b> systems {{provide the}} {{capability}} to observe the runtime state of a process and {{to determine whether or}} not it is acceptable. Existing software systems tend to forgo integrity checks altogether or to enlist static mechanisms (e. g., assertions) to detect unacceptable process states at runtime. A large and growing base of malicious software necessitates more sophisticated handling of threats to process integrity. In this paper, we describe an approach to runtime <b>integrity</b> <b>measurement</b> we call the Java <b>Measurement</b> Framework(JMF) thatpresentsanewwaytodefineandcheckruntime <b>integrity</b> policies. We define a policy language based on Java that provides an accessible way to write integrity policies and we describe a periodic, dynamic measurer that obtains snapshots of process state, which are evaluated with respect to a policy by an appraiser. With full process state available to the appraiser, policies can express rich relationships between multiple objects, thereby detecting abnormalitiesinanapplication’sdatastructures. Ourframeworkmay be used to detect a powerful adversary who has {{the capability to}} modify both the runtime bytecode and data structures ofJavaapplications. Weshowthatourprototypeimplementation in Java has acceptable overhead and that it can be used to detect runtime integrity violations in several real Java programs...|$|R
40|$|While most microkernel-based systems {{implement}} non-essential {{software components}} as user space tasks and strictly separate those tasks during runtime, they often {{rely on a}} static configuration and composition of their software components to ensure safety and security. In this paper, we extend a microkernel-based system architecture with a Trusted Platform Module (TPM) and propose a verification mechanism for a microkernel runtime environment, which calculates <b>integrity</b> <b>measurements</b> before allowing to load (remote) binaries. As a result, our approach {{is the first to}} adopt the main ideas of the <b>Integrity</b> <b>Measurement</b> Architecture (IMA), which has been proposed for Linux-based systems, to a microkernel. In comparison, however, it significantly reduces the Trusted Computing Base (TCB) and allows for a strict separation of the integrity verification component from any rich operating system, such as GNU/Linux or Android, running in parallel. In our implementation, which is based on L 4 /Fiasco. OC with L 4 Re as runtime environment, we present our extension of the existing L 4 Re loader service that calculates <b>integrity</b> <b>measurements</b> for each binary. We also evaluate our implementation on two ARM-based developer boards and discuss code size, security, and performance of our proposed integrity verification mechanism...|$|R
30|$|The cIMS {{forwards}} a Trust Score to IdP: Upon {{completing the}} evaluation of the integrity report obtained from TED in the previous step, the cIMS generates a trust score for the Client. The trust score reflects the judgement of the cIMS (regarding the Client) as compared against some <b>Integrity</b> <b>Measurement</b> Policies stored at the cIMS. The cIMS logs all the measurement and evaluation events, and archives all received integrity reports and resulting trust scores in order to maintain audit and accountability information.Note that in the multi-host situation, the IdP may have an account with the cIMS within which it defines the set of <b>integrity</b> <b>measurement</b> policies for all clients that the IdP re-directs to the cIMS. As such, the cIMS becomes a provider to multiple IdPs.|$|R
30|$|Cloud-based <b>Integrity</b> <b>Measurement</b> Service (cIMS): The cIMS is {{the service}} that {{performs}} the <b>integrity</b> <b>measurement</b> {{and evaluation of}} the Client platform. In general, {{we assume that the}} IdP has a trust relationship and a contractual business agreement with the cIMS based on a mature trust framework. There are a number of possible outputs from an evaluation by the cIMS. Here we assume that at the very least the cIMS returns an integrity score based on some measurement scale agreed upon with the IdP.Although the cIMS is shown in Figure 4 as separate from the IdP, the cIMS could in fact be a service operating within the IdP. This approach would allow the IdP to offer a wider set of services while operating under a single trust framework.|$|R
40|$|This poster {{discusses}} {{a strategy}} for automatic whitelist generation and enforcement using techniques from in-formation flow control and trusted computing. Dur-ing a measurement phase, a cloud provider uses dy-namic taint tracking to generate a whitelist of executed code and associated file hashes generated by an <b>integrity</b> <b>measurement</b> system. Then, at runtime, it can again use dynamic taint tracking to enforce execution only of code from files whose names and <b>integrity</b> <b>measurement</b> hashes exactly match the whitelist, preventing adver-saries from exploiting buffer overflows or running their own code on the system. This provides the capability for runtime integrity enforcement or attestation. Our prototype system, built on top of Intel’s PIN emula-tion environment and the libdft taint tracking system, demonstrates high accuracy in tracking the sources of instructions. ...|$|R
40|$|Emerging distributing {{computing}} architectures, such as {{grid and}} cloud computing, {{depend on the}} high integrity execution of each system in the computation. While <b>integrity</b> <b>measurement</b> enables systems to generate proofs of their integrity to remote parties, we find that current <b>integrity</b> <b>measurement</b> approaches are insufficient to prove runtime integrity for systems in these architectures. <b>Integrity</b> <b>measurement</b> approaches that are flexible enough have an incomplete view of runtime integrity, possibly leading to false integrity claims, and approaches that provide comprehensive integrity do so only for computing environments that are too restrictive. In this paper, we propose an architecture for building comprehensive runtime integrity proofs for general purpose systems in distributed computing architectures. In this architecture, we strive for classical integrity, using an approximation of the Clark-Wilson integrity model as our target. Key to building such integrity proofs is a carefully crafted host system whose long-term integrity can be justified easily using current techniques and a new component, called a VM verifier, that can enforce our integrity target on VMs comprehensively. We have built a prototype based on the Xen virtual machine system for SELinux VMs, and find distributed compilation can be implemented, providing accurate proofs of our integrity target with less than 4 % overhead. ...|$|R
