1|2|Public
40|$|This thesis {{presents}} {{a series of}} studies to explore and understand the design of eyes-free interfaces for mobile devices. The motivation is to devise a holistic design concept that is based on the WIMP paradigm and is adapted to the requirements of mobile user interaction. It is proposed that audio is a very efficient and effective modality for use in an eyes-free mobile interface. Methods to transfer the WIMP paradigm to eyes-free interfaces are proposed and evaluated. Guidelines for the implementation of the paradigm are given and – by means of an example – a holistic design concept is proposed. This thesis begins with an introduction to and critical reflection of re- currently important themes and research methods from the disciplines of psychoacoustics, psychology, and presence research. An overview of related work is given, paying particular attention to the use of interface metaphors in mobile eyes-free interfaces. The notion of distance is discussed as a method to prioritise, structure, and manage attention in eyes-free interfaces. Practical issues arising from sources becoming inaudible with increasing distance can be addressed by proposing a method modeled on echo location. This method was compared to verbally coded distance information and proved useful for identifying the closest of several objects, while verbally coded distance infor- mation was found to be more efficient for identifying the precise distance of an object. The knowledge gained from the study can contribute to improv- ing other applications, such as GPS based navigation. Furthermore, the issue of gaining an overview of accessible objects by means of sound was exam- ined. The results showed that a minimum of 200 ms between adjacent sound samples should be adhered to. Based on these findings, both earcons and synthesized speech are recommendable, although speech has the advantage of being more flexible and easier to learn. <b>Monophonic</b> <b>reproduction</b> yields comparable results to spatial reproduction. However, spatial reproduction has the additional benefit of indicating an item’s position. These results are transferable and generally relevant for the use of audio in HCI. Tactile interaction techniques were explored as a means to interact with an auditory interface and were found to be both effective and enjoyable. One of the more general observations was that 2 D and 3 D gestures were intuitively used by participants, who transferred their knowledge of established gestures to auditory interfaces. It was also found that participants often used 2 D ges- tures to select an item and proceeded to manipulate it with a 3 D gesture. The results suggest the use of a small gesture set with reversible gestures for do/undo-type actions, which was further explored in a follow up study. It could be shown that simple 3 D gestures are a viable way of manipulating spatialized sound sources in a complex 3 D auditory display. While the main contribution of this thesis lies in the area of HCI, pre- viously unresearched issues from adjacent disciplines that impact the user experience of auditory interfaces have been addressed. It was found that regular, predictable movement patterns in 3 D audio spaces cause symptoms of simulator sickness. However, these were found to be minor and only oc- curred under extreme conditions. Additionally, the influence of the audio reproduction method on the perception of presence, social presence, and realism was examined. It was found that both stereophonic and binaural reproduction have advantages over monophonic sound reproduction: stereo- phonic sound increases the perception of social presence while binaural sound increases the feeling of being present in a virtual environment. The results are important contributions insofar as one of the main applications of mobile devices is voice based communication; {{it is reasonable to assume}} that there will be an increase in real-time voice based social and cooperative networking applications. This thesis concludes with a conceptual design of a system called “Foogue”, which uses the results of the previous experiments as the basis of an eyes-free interface that utilizes spatial audio and gesture input...|$|E
5000|$|Monaural or <b>monophonic</b> sound <b>reproduction</b> (often {{shortened}} to mono) {{is intended}} to be heard {{as if it were a}} single channel of sound perceived as coming from one position (unlike stereo, which uses two channels to convey the impression of sound coming from different places from left, middle, and right). In mono, only one loudspeaker is necessary, but, when played through multiple loudspeakers or headphones, identical signals are fed through each of the wires into each speaker, resulting in the perception of a one-channel sound, which [...] "images" [...] in one sonic space between the speakers (provided that the speakers are set up in a proper symmetrical critical-listening placement). Monaural recordings, like stereo, customarily use multiple microphones, fed into multiple channels on a recording console, but each channel is [...] "panned" [...] to be in the center. In the final stage, the various center-panned signal paths are usually mixed down to two identical tracks, which because they are identical, are perceived upon playback as representing a single unified signal in a single place in the soundstage. In some cases the multitrack source is mixed down to a one track tape becoming one signal. In the mastering stage, particularly in the days of mono records, the one-track or two-track mono master tape was then transferred to a one-track lathe intended to be used in the pressing of a monophonic record. However, today monaural recordings are usually mastered to be played on stereo and multi-track formats, yet retain their center-panned mono soundstage characteristics when played back.|$|R
40|$|In recent years, {{significant}} {{development of}} {{digital audio signal}} processing has achieved practical solutions to 2 D soundfield reproduction using headphones and loudspeakers. The perceptual audio quality is extensively enhanced by providing rich spatialisation effects for the surround sound compared to traditional <b>monophonic</b> and stereophonic <b>reproduction</b> systems. While surround sound has been widely commercialised, the audience can only perceive the soundfield in a pre-rendered manner. However, with the recent development on 3 D TV and free viewpoint TV, the visual content can be flexibly perceived where the users can view the recorded scene from their desired angle and viewpoint. These new technologies can also be employed for the surveillance of large public spaces or infrastructure where the ability to zoom-in and view from multiple angles is desired. While the visual contents can be selectively reproduced, the selective audio playback is required to accompany the changing video scenes. The soundfield navigation technology presented in this thesis provides a sound object based solution to achieve soundfield navigation, which allows the listeners to selectively choose the desired listening position of the recorded soundfield by receiving the same audio stream. The proposed framework aims to provide a complete solution starting from the recording configuration to post-processing of the recordings, followed by efficient compression and packet-loss protection techniques. The recording and post-processing techniques are firstly presented. The proposed framework employs a pair of co-located microphone arrays to capture suffcient information that ensures soundfield navigation. Compared to a traditional recording set-up, using a pair of co-located microphone arrays ensures enhanced source separation quality {{as well as the}} ability to estimate the geographical location of the sources. In this thesis, a new low-delay stochastic-based direction of arrival estimation approach is presented that is used in the individual co-located microphone array to achieve low-delay (approximately 150 ms) direction of arrival estimation. The proposed direction of arrival estimation method employs a time-frequency energy weighted direction of arrival histogram that achieves improved estimation performance compared to existing histogram-based methods. These direction of arrival estimates are then used within the proposed collaborative blind source separation scheme to achieve enhanced blind source separation. Comparing existing blind source separation approaches that are based on separating the sparse time-frequency component of the recorded signals, the collaborative approach can also separate the non-sparse time-frequency components and thus ensures enhanced separation performance. The graphical location of the sources can be also obtained from the microphone collaboration. A psychoacoustic-based analysis-by-synthesis framework that compresses the separated sources and their spatial locations is also proposed. The proposed compression framework is able to compress up to three simultaneously occurring speech sources into one mono mixture signal that can be compressed using a traditional speech codec at 32 kbps. The spatial information of the sources can be preserved as side information indicating the origin of the time-frequency sources. In the reproduction site, by receiving the same mono mixture plus side information, the audiences can selectively navigate to their desired listening point as well as selective play back of interested sources. This compression scheme is extended to jointly compress up to 8 audio objects using a stereo mixture signal by the extended multilevel decomposition scheme. Finally, a hybrid MDC-FEC based joint source-channel coding scheme is presented. The proposed scheme is firstly analysed from a theoretical point of view and then applied to speech and audio mixture signals created by the perceptual-based analysis-by-synthesis framework. The theoretical evaluation results indicate that the proposed approach leads to an optimal model for joint source-channel coding based on a combination of MDC and FEC. These models are then applied to protect the speech and audio mixture signals for practical packet-loss transmission. By dynamically adjusting optimised transmission models, the quality of the speech and audio objects are maintained as confirmed by evaluations. The robust low-delay soundfield navigation system is ensured for practical transmission channels. The independent and correlated soundfield navigation frameworks are presented by combining the techniques proposed in this thesis. The independent soundfield navigation system aims to achieve soundfield navigation between independent soundfields. Practical applications include multi-party teleconferencing where each site can be regarded as one independent soundfield, or surveillance for a large area where each site is considered as one independent soundfield. A correlated soundfield navigation system is also proposed to achieve free listening point navigation within a large soundfield through limited number of correlated observations of the soundfield. The audience is able to select any listening point within the soundfield...|$|R

