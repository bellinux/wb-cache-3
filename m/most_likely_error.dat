21|10000|Public
50|$|Coset {{leaders are}} used in the {{construction}} of a standard array for a linear code, which can then be used to decode received vectors. For a received vector y, the decoded message is y - e, where e is the coset leader of y. Coset leaders {{can also be used to}} construct a fast decoding strategy. For each coset leader u we calculate the syndrome uH&prime;. When we receive v we evaluate vH&prime; and find the matching syndrome. The corresponding coset leader is the <b>most</b> <b>likely</b> <b>error</b> pattern and we assume that v+u was the codeword sent.|$|E
50|$|As {{mentioned}} earlier, this architecture makes error predictions {{based on}} the amount of overlapping features. Such as, the <b>most</b> <b>likely</b> <b>error</b> for R should be P. Thus, in order to show this architecture represents the human pattern recognition system we must put these predictions into test. Researchers have constructed scenarios where various letters are presented in situations that make them difficult to identify; then types of errors were observed, which was used to generate confusion matrices: where all of the errors for each letter are recorded. Generally, the results from these experiments matched the error predictions from the pandemonium architecture. Also {{as a result of these}} experiments, some researchers have proposed models that attempted to list all of the basic features in the Roman alphabet.|$|E
3000|$|... 2)= 4), {{whereas in}} the other cases, they only contain 1 and 2 errors, respectively. Consequently, even if the {{residual}} interference is perfectly removed, the bit error rate (BER) performance would be poor because the <b>most</b> <b>likely</b> <b>error</b> events are those that contain 3 or 4 errors.|$|E
40|$|Talk {{will include}} an {{examination}} of systematic effects of emission from mirrors, calibrator, grids and other areas within the PIXIE instrument. The systematic effects of detector non-ideality and asymmetry will also be presented. These show that the residual systematic effects do not jeopardize either the polarization or spectrum measurements {{and that there are}} sufficient safeguards to find and calibrate out the <b>most</b> <b>likely</b> <b>errors...</b>|$|R
50|$|A further {{refinement}} makes Maven's endgame solutions asymptotically optimal even in {{the presence}} of errors. When the B* search terminates with a proof that one move is best, and there is still time remaining, then Maven widens its estimates by 1 point and searches again. These re-searches are usually very quick, because the tree from the previous search is still largely valid. Repeated use of this policy will progressively identify errors, starting with the smallest (and presumably <b>most</b> <b>likely)</b> <b>errors.</b>|$|R
50|$|For unknown reasons, this crater was {{omitted from}} early {{maps of the}} Moon. This crater is not of recent origin, however, so the {{omission}} was <b>most</b> <b>likely</b> an <b>error</b> {{on the part of}} the map-makers.|$|R
3000|$|..., of the ML decoding, {{the list}} size L must be large. Hence, we can obtain a {{practical}} list construction by assuming the L sufficiently probable error patterns rather than assuming the L <b>most</b> <b>likely</b> <b>error</b> patterns. We restate Theorems 1 and 2 in [4] {{to obtain the}} likely error patterns and to define the practical list decoding algorithms.|$|E
40|$|We {{describe}} a quantum error correction scheme aimed at protecting {{a flow of}} quantum information over long distance communication. It is largely inspired by the theory of classical convolutional codes which are used in similar circumstances in classical communication. The particular example shown here uses the stabilizer formalism, which provides an explicit encoding circuit. An associated error estimation algorithm is given explicitly and shown to provide the <b>most</b> <b>likely</b> <b>error</b> over any memoryless quantum channel, while its complexity grows only linearly {{with the number of}} encoded qubits. Comment: 4 pages, uses revtex 4. Minor correction in the encoding and decoding circuit...|$|E
40|$|We {{propose a}} new error {{modeling}} and optimization-based localization approach for sensor networks in presence of range measurement errors. The approach is solely {{based on the}} concept of consistency. The error models are constructed using non-parametric statistical techniques; they do not only indicate the <b>most</b> <b>likely</b> <b>error,</b> but also provide the likelihood distribution of particular errors occurring. The models are evaluated using the learn-and-test method and served as the objective functions for the task of localization. In addition, we also developed a localized localization algorithm where a specified communication cost or the location accuracy is guaranteed while optimizing the other...|$|E
40|$|The {{annual mean}} of the global-average net {{radiation}} budget should be quite small. Because of errors in the measurement and data analysis, the annual mean global-average net radiation as computed by the CERES program does not meet this requirement. A method is presented for computing the <b>most</b> <b>likely</b> <b>errors</b> in the measurement and data production which will bring the radiation into balance {{within the range of}} interannual variations. The need for a data set which is globally balanced is demonstrated by the computation of the annual-mean zonal distribution of heat flux by the atmosphere and oceans...|$|R
40|$|Helper {{data systems}} {{mitigate}} {{the risk that}} biometric templates are stolen from 3 ̆cbr/ 3 ̆ea biometric data base. Yet, current systems face the drawback that strong Error 3 ̆cbr/ 3 ̆eCorrection is {{needed in order to}} mitigate variations in the measured biometric 3 ̆cbr/ 3 ̆eduring verification. Error correction codes are not always attractive, as these may 3 ̆cbr/ 3 ̆eseverely reduce the effective entropy extracted from the biometric. We study an 3 ̆cbr/ 3 ̆ealternative, namely to check the (set of) <b>most</b> <b>likely</b> <b>errors,</b> based on a posterior 3 ̆cbr/ 3 ̆eside information, using soft decision during verification...|$|R
25|$|Cozumel {{is a large}} island {{near the}} {{mainland}} of the Mexican state of Quintana Roo. The rodent fauna includes several species, all with close relations to forms from the adjacent mainland. The pocket gopher Orthogeomys hispidus has also been recorded, but <b>most</b> <b>likely</b> in <b>error.</b>|$|R
30|$|We {{have studied}} the {{proposed}} receivers in both FBMC/OQAM and FBMC/QAM. This latter can offer the best performance in some situation since the global intrinsic interference is reduced. For FBMC/OQAM, we have shown by simulations that only PaIC/Viterbi- 3 with the PHYDYAS filter can provide satisfactory performance compared to CP-OFDM/ML one. Whereas for FBMC/QAM, all the three PaIC/Viterbi configurations with the PHYDYAS filter exhibit the same performance as CP-OFDM/ML. However, the interference coefficients of the IOTA filter do not allow any PaIC/Viterbi configuration to reach the optimal BER performance. Indeed, {{in spite of the}} lowest RISI power for PaIC/Viterbi- 3 with the IOTA filter, the BER performance is limited {{because of the fact that}} the <b>most</b> <b>likely</b> <b>error</b> events in the Viterbi detector contain more than 3 errors.|$|E
40|$|We {{classify}} {{the time}} complexities of three important decoding problems for quantum stabilizer codes. First, {{regardless of the}} channel model, quantum bounded distance decoding is shown to be NP-hard, like what Berlekamp, McEliece and Tilborg did for classical binary linear codes in 1978. Then over the depolarizing channel, the decoding problems for finding a <b>most</b> <b>likely</b> <b>error</b> and for minimizing the decoding error probability are also shown to be NP-hard. Our results indicate that finding a polynomial-time decoding algorithm for general stabilizer codes may be impossible, but this, on the other hand, strengthens the foundation of quantum code-based cryptography. Comment: There are six pages in this paper. Part {{of this paper was}} presented in the 2012 International Symposium on Information Theory and its Applications (ISITA 2012), Hawaii, USA, October 28 [...] 31, 201...|$|E
40|$|An {{analysis}} was performed of the possible error mechanisms which degraded the IR net flux measurements made by the three small atmospheric probes dispatched from the Pioneer Venus spacecraft. The larger errors began below 30 km, and caused the data to be inconsistent with previous estimates of the atmospheric opacity. Evaluations were made of the possible radiation field perturbations behind each probe, cloud particle deposition on the sensor windows, and thermal disturbances within the sensors because of gas flow through the window retainers. The gas flow through the retainers was identified as the <b>most</b> <b>likely</b> <b>error</b> source, and was demonstrated in laboratory tests. A strong Reynolds number dependence was also found. Radiative transfer calculations were performed {{to account for the}} errors, using the constraints defined by the tests. Upper and lower bounds were calculated for the true net flux for both day and night conditions...|$|E
5000|$|While Manetho gives Osorkon I a {{reign of}} 15 Years in his Ægyptiaca, this is <b>most</b> <b>likely</b> an <b>error</b> for 35 Years {{based on the}} {{evidence}} of the second Heb Sed bandage, as Kenneth Kitchen notes. Osorkon I's throne name--Sekhemkheperre--means [...] "Powerful are the Manifestations of Re." ...|$|R
2500|$|<b>Most</b> <b>likely,</b> this <b>error</b> is {{admitted}} {{for the first}} time in work of Marquis de Custine [...] "La Russie en 1839", in which it confuses cholera revolt of 1831 to an episode of 1825 (Decembrist revolt). Russian researcher Nikolay Shilder has specified in the works in this error.|$|R
40|$|International audienceWe {{focus on}} textual entailments {{mediated}} by syntax and propose a new methodology to evaluate textual entailment recognition systems on such data. The main {{idea is to}} generate a syntactically annotated corpus of pairs of (non-) entailments and to use error mining to identify the <b>most</b> <b>likely</b> sources of <b>errors.</b> To illustrate the approach, we apply this methodology to the Afazio RTE system and show how it permits identifying the <b>most</b> <b>likely</b> sources of <b>errors</b> made by this system on a testsuite of 10 000 (non) entailment pairs...|$|R
40|$|We {{present and}} analyze {{protocols}} for fault-tolerant quantum computing using color codes. We present circuit-level schemes for extracting the error syndrome of these codes fault-tolerantly. We further present an integer-program-based decoding algorithm for identifying the <b>most</b> <b>likely</b> <b>error</b> given the syndrome. We simulated our syndrome extraction and decoding algorithms against three physically-motivated noise models using Monte Carlo methods, {{and used the}} simulations to estimate the corresponding accuracy thresholds for fault-tolerant quantum error correction. We also used a self-avoiding walk analysis to lower-bound the accuracy threshold for two of these noise models. We present and analyze two architectures for fault-tolerantly computing with these codes: one with 2 D arrays of qubits are stacked atop each other and one in a single 2 D substrate. Our analysis demonstrates that color codes perform slightly better than Kitaev's surface codes when circuit details are ignored. When these details are considered, we estimate that color codes achieve a threshold of 0. 082 (3) ...|$|E
40|$|As schools {{grow more}} crowded and {{required}} testing outcomes become more stringent, teachers experience increasing demands on their time. To ease this load, I designed {{a program that}} is able to give students thorough, automated feedback on their mathematics assignments. This will allow teachers to spend less time grading (and consequently more time on other activities that might better help their students) without losing any of the feedback and error correcting a human {{would be able to}} provide. Based on a number of different test cases, using a wide variety of elementary algebra problems, the program can correctly identify the lines in which errors are introduced. The program is also adept at finding the precise error as long as the student has made minimal changes per step. If multiple changes have been made, the program is forced to make its best guess at the <b>most</b> <b>likely</b> <b>error</b> without resorting to testing hundreds of possible combinations. by Stephanie Denise Carter Greene. Thesis: M. Eng., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, February 2015. Cataloged from PDF version of thesis. "September 2014. "Includes bibliographical references (page 65) ...|$|E
40|$|We have {{developed}} a new error modeling and optimization-based localization approach for sensor networks in presence of distance measurement noise. The approach is solely based {{on the concept of}} consistency. The error models are constructed using non-parametric statistical techniques; they do not only indicate the <b>most</b> <b>likely</b> <b>error,</b> but also provide the likelihood distribution of particular errors occurring. The models are evaluated using the learn-and-test techniques and serve as the objective functions for the task of localization. The localization problem is formulated as task of maximizing consistency between measurements and calculated distances. We evaluated the approach in (i) both GPS-based and GPS-less scenarios; (ii) 1 -D, 2 -D and 3 -D spaces, on sets of acoustic ranging-based distance measurements recorded by deployed sensor networks. The experimental evaluation indicates that localization of only a few centimeters is consistently achieved when the average and median distance measurement errors are more than a meter, even when the nodes have only a few distance measurements. The relative performance in terms of location accuracy compare favorably with respect to several state-of-the-art localization approaches. Finally, several insightful observations about the required conditions for accurate localization are deduced by analyzing the experimental results...|$|E
50|$|The {{reports that}} all Notaras' sons were {{executed}} are <b>most</b> <b>likely</b> an <b>error.</b> <b>Most</b> historical {{accounts of the}} executions do not allude {{to the fate of}} the youngest son, Jacob Notaras, who was sent to Mehmed’s harem. The only one male survivor of the Notaras family, Jacob escaped from the Ottomans in 1460.|$|R
50|$|A king {{with the}} name Neferka, written in the papyrus as Neferka-khered appears in the Turin King List between king Netjerkare Siptah and a king Nefer. Neferka and Nefer are <b>most</b> <b>likely</b> writing <b>errors</b> in the papyrus. Several {{scholars}} regard Neferka as mistake for Neferkare and identify him with Neferkare Pepiseneb. Other identify him with Menkare.|$|R
50|$|In the English dub of the movie, BAHRAM {{is called}} Bafram. It is also called {{this in the}} art gallery in the special features. However this is <b>most</b> <b>likely</b> a {{translation}} <b>error.</b>|$|R
40|$|Abstract—We have {{developed}} a new error modeling and optimization-based localization approach for sensor networks in presence of distance measurement noise. The approach is solely based {{on the concept of}} consistency. The error models are constructed using non-parametric statistical techniques; they do not only indicate the <b>most</b> <b>likely</b> <b>error,</b> but also provide the likelihood distribution of particular errors occurring. The models are evaluated using the learn-and-test techniques and serve as the objective functions for the task of localization. The localization problem is formulated as task of maximizing consistency between measurements and calculated distances. We evaluated the approach in (i) both GPS-based and GPS-less scenarios; (ii) 1 -D, 2 -D and 3 -D spaces, on sets of acoustic ranging-based distance measurements recorded by deployed sensor networks. The experimental evaluation indicates that localization of only a few centimeters is consistently achieved when the average and median distance measurement errors are more than a meter, even when the nodes have only a few distance measurements. The relative performance in terms of location accuracy compare favorably with respect to several state-of-the-art localization approaches. Finally, several insightful observations about the required conditions for accurate localization are deduced by analyzing the experimental results. Keywords-consistency; error modeling; location discovery I...|$|E
40|$|In {{order to}} improve the {{performance}} of non-binary low-density parity check codes (LDPC) hard decision decoding algorithm and to reduce the complexity of decoding, a sum of the magnitude for hard decision decoding algorithm based on loop update detection is proposed. This will also ensure the reliability, stability and high transmission rate of 5 G mobile communication. The algorithm {{is based on the}} hard decision decoding algorithm (HDA) and uses the soft information from the channel to calculate the reliability, while the sum of the variable nodes’ (VN) magnitude is excluded for computing the reliability of the parity checks. At the same time, the reliability information of the variable node is considered and the loop update detection algorithm is introduced. The bit corresponding to the error code word is flipped multiple times, before this is searched in the order of <b>most</b> <b>likely</b> <b>error</b> probability to finally find the correct code word. Simulation results show that the performance of one of the improved schemes is better than the weighted symbol flipping (WSF) algorithm under different hexadecimal numbers by about 2. 2 dB and 2. 35 dB at the bit error rate (BER) of 10 − 5 over an additive white Gaussian noise (AWGN) channel, respectively. Furthermore, the average number of decoding iterations is significantly reduced...|$|E
40|$|Quantum {{computers}} (QCs) {{have many}} potential hardware implementations ranging from solid-state silicon-based structures to electron-spin qubits on liquid helium. However, all QCs {{must contend with}} gate infidelity and qubit state decoherence over time. Quantum error correcting codes (QECCs) {{have been developed to}} protect program qubit states from such noise. Previously, Monte Carlo noise simulators have been developed to model the effectiveness of QECCs in combating decoherence. The downside to this random sampling approach is that it may take days or weeks to produce enough samples for an accurate measurement. We present an alternative noise modeling approach that performs combinatorial analysis rather than random sampling. This model tracks the progression of the <b>most</b> <b>likely</b> <b>error</b> states of the quantum program through its course of execution. This approach has the potential for enormous speedups versus the previous Monte Carlo methodology. We have found speedups with the combinatorial model on the order of 100 X- 1, 000 X over the Monte Carlo approach when analyzing applications utilizing the [[7, 1, 3]] QECC. The combinatorial noise model has significant memory requirements, and we analyze its scaling properties relative {{to the size of the}} quantum program. Due to its speedup, this noise model is a valuable alternative to traditional Monte Carlo simulation. ...|$|E
40|$|We {{describe}} {{a system for}} modeling BrainMap [...] a neuroscience database describing activation foci reported from many neuroimaging studies. We apply machine learning techniques {{in the form of}} probability density models and use them in order to spot "novelty" among activation foci, and find several outliers that are <b>most</b> <b>likely</b> entry <b>errors.</b> The system can be incorporated into the database entry program...|$|R
50|$|Tolkien {{also argues}} that Finnsburuh is <b>most</b> <b>likely</b> an <b>error</b> by either Hickes or his printer, since that {{construction}} appears nowhere else, and the word should be Finnesburh. It {{is not clear whether}} this was the actual name of the hall or only the poets description of it. Where exactly the hall was, or even whether it was in Frisia, is not known.|$|R
30|$|As some of {{our results}} point out, errors in data can give rise to {{corrupted}} results. While simple filtering of the data removed some of the errors, others did persist. <b>Most</b> <b>likely</b> these <b>errors</b> {{could have been avoided}} at source: the errors have to do with changing base station locations, base station ID’s that have been switched between stations, or other technical issues at the operator’s end.|$|R
40|$|Quantum error {{correction}} codes were introduced {{as a means}} to protect quantum information from decoherance and operational errors. Based on their approach to error control, error correcting codes can be divided into two different classes: block codes and convolutional codes. There has been significant development towards finding quantum block codes, since they were first discovered in 1995. In contrast, quantum convolutional codes remained mainly uninvestigated. In this thesis, we develop the stabilizer formalism for quantum convolutional codes. We define distance properties of these codes and give a general method for constructing encoding circuits, given a set of generators of the stabilizer of a quantum convolutional stabilizer code, is shown. The resulting encoding circuit enables online encoding of the qubits, i. e., the encoder does not {{have to wait for the}} input transmission to end before starting the encoding process. We develop the quantum analogue of the Viterbi algorithm. The quantum Viterbi algorithm (QVA) is a maximum likehood error estimation algorithm, the complexity of which grows linearly with the number of encoded qubits. A variation of the quantum Viterbi algorithm, the Windowed QVA, is also discussed. Using Windowed QVA, we can estimate the <b>most</b> <b>likely</b> <b>error</b> without waiting for the entire received sequence...|$|E
40|$|Paper {{presented}} at International Conference on Computing in Civil Engineering 2005, Cancun, Mexico July 12 - 15, 2005 Read More: [URL] behavior {{is one of}} the most important design considerations for high-rise structures; natural frequencies and corresponding mode shapes are basic data for seismic and wind response analyses. The final results of the analysis and design depend very much on the quality of the three-dimensional (3 -D) finite element model developed. Comparison of natural modes of the mathematical analysis with those of field measurements is the most widely used approach to assess the credibility of the mathematical model. Finite element model updating of high-rise structures, a very time consuming procedure to achieve the credible model, has rarely been studied before. Different from the most popular model updating approaches which are based on sensitivity analysis techniques, this study presents an application of finite element model updating of high-rise structures with a computer aided model updating system (CAMUS). After investigating knowledge and experiences of model updating techniques and finite element modeling of high-rise structures, a knowledge-based system was developed to implement finite element model updating of high-rise structures. Based on the correlation analysis results of natural frequency that are stored in the knowledge base, the system searches the <b>most</b> <b>likely</b> <b>error</b> in the model over the knowledge base of possible problems and guides the user to update the model. The performance of the algorithm was demonstrated with an application of finite element model updating of a 66 -storey frame-core wall office tower. Read More: [URL]...|$|E
40|$|A two-source model {{based on}} the energy balance {{equation}} and the Penman-Monteith combination equation was used to describe energy exchanges of a soybean canopy and the soil surface. Combination equations which eliminated soil surface resistance to soil latent heat flux were developed, but the equations contained a new variable, the soil surface vapor pressure deficit. Objectives were to quantify soil surface vapor pressure deficit and determine if the modified model improved estimates of soil and total latent heat flux compared with the original model. ^ Research was conducted in 1994 at North Platte, Nebraska. Total latent heat flux from an irrigated soybean field was estimated with the Bowen ratio-energy balance method. Latent heat flux from the soil was measured with microlysimetry. Soil surface vapor pressure deficit was quantified with a device which sampled air near the soil surface. ^ Soil surface vapor pressure was generally underestimated. Drier air from above the soil surface was mixed with near-surface air and reduced its humidity. Soil surface vapor pressure deficit was also underestimated, with the <b>most</b> <b>likely</b> <b>error</b> the underestimation of soil surface temperature. ^ There {{were no significant differences}} between the original soil surface resistance model and the modified soil deficit model in the estimation of total or soil latent heat fluxes. Two-source models estimated total latent heat flux much better than a single-source model when the canopy was sparse, but there was no difference when the canopy was full. The models predicted soil latent heat flux best when there was no canopy. Their accuracy decreased when the canopy was sparse and was poorest when the canopy was full. Overestimation of soil latent heat flux by the surface deficit model was attributed to underestimation of soil surface temperature. However, overestimation when the canopy was full was attributed to canopy-dependent factors such as available energy at the soil surface, within-canopy aerodynamic resistance or canopy surface resistance. ...|$|E
50|$|The Finnery River's name {{is first}} {{recorded}} as Feneure in The Civil Survey A.D. 1654-56, vol. viii county of Kildare. The spelling Finouse also appears, <b>most</b> <b>likely</b> an <b>error.</b> The 1752 Noble and Keenan map of County Kildare {{calls it the}} River Fennery, while Finnery appears first in the 1807 Civil Survey. The Irish language name appears in 1837 as Fionnabhair or Abhainn Fionnabhrach, possibly from the personal name Finnabair.|$|R
5000|$|Carroll and Siler {{says that}} [...] "The Septuagint’s {{translation}} was <b>most</b> <b>likely</b> in <b>error</b> {{because it seems}} unlikely that nataph is a form of myrrh [...] [...] [...] it seems that its translation in the Septuagint as stacte was made simply because both nataph and stacte mean 'to drip' [...] [...] [...] the storax tree seems more likely. Our word storax may even come from the Hebrew tsori." ...|$|R
50|$|Since {{it is very}} unlikely, {{although}} possible, that a 32-bit integer {{would take}} this specific value, the appearance of such a number in a debugger or memory dump <b>most</b> <b>likely</b> indicates an <b>error</b> such as a buffer overflow or an uninitialized variable.|$|R
