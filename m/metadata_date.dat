1|43|Public
40|$|Camera traps {{have become}} a widely used {{technique}} for conducting biological inventories, generating {{a large number of}} database records of great interest. The main aim {{of this paper is to}} describe a new free and open source software (FOSS), developed to facilitate the management of camera-trapped data which originated from a protected Mediterranean area (SE Spain). In the last decade, some other useful alternatives have been proposed, but ours focuses especially on a collaborative undertaking and on the importance of spatial information underpinning common camera trap studies. This FOSS application, namely, “Camera Trap Manager” (CTM), has been designed to expedite the processing of pictures on the. NET platform. CTM has a very intuitive user interface, automatic extraction of some image <b>metadata</b> (<b>date,</b> time, moon phase, location, temperature, atmospheric pressure, among others), analytical (Geographical Information Systems, statistics, charts, among others), and reporting capabilities (ESRI Shapefiles, Microsoft Excel Spreadsheets, PDF reports, among others). Using this application, we have achieved a very simple management, fast analysis, and a significant reduction of costs. While we were able to classify an average of 55 pictures per hour manually, CTM has made it possible to process over 1000 photographs per hour, consequently retrieving a greater amount of data. This project has been carried out thanks to the UNCROACH project (CGL- 2011 - 30581 -CC 02 - 01) from the Spanish Ministry of Economy and Competitiveness; and has been partially funded by the public call for research projects of the Conselleria of Education (T 6217 - 2010); and the Institute of Culture Juan Gil-Albert-Alicante...|$|E
5000|$|Searching by keywords, caption text, <b>metadata,</b> <b>dates,</b> {{location}} or title ...|$|R
50|$|Search Connect is {{a feature}} {{announced}} by MyHeritage in July 2015 and released in November that same year. The feature indexes search queries {{along with their}} <b>metadata</b> <b>dates,</b> places, relatives, etc. and then displays them in search results when others perform a similar search. The feature allows users performing similar searches to connect {{with each other for}} collaboration.|$|R
5000|$|The {{ability to}} filter based on <b>metadata</b> such as <b>dates,</b> email {{addresses}} and file types ...|$|R
30|$|The columns create/modify {{date and}} GPS present are counters: {{the first one}} counts {{the number of images}} in which the <b>{{metadata}}</b> create <b>date</b> and modify date are not identical. The second refers to the metadata GPS Position and counts the number of images in which this tag is not empty.|$|R
50|$|In <b>metadata,</b> {{the term}} <b>date</b> is a {{representation}} {{term used to}} specify a calendar date in the Gregorian calendar. Many data representation standards such as XML, XML Schema, Web Ontology Language specify that ISO date format ISO 8601 should be used.|$|R
50|$|November 1998 - {{first edition}} of Eye2eye Britain (1.0) published. Supplied on CD-ROM, this mapped out Britain in 10,000 digital photos (no panoramas). Each photo is captioned and indexed using <b>metadata,</b> e.g. topic, <b>date</b> of {{heritage}} sites on a timeline, as well as map location.|$|R
50|$|Museums and {{galleries}} {{as well as}} other online art image databases, such as Google Art Project and Artstor, digitize artworks for public access, but beyond providing basic <b>metadata</b> (artist, title, <b>date,</b> medium) these databases do not extensively classify works of art or create connections between them.|$|R
30|$|All {{videos were}} {{acquired}} using the video encoder H. 264 /avc 1 and mp 4 a for encoding audio. We remark that, for the D 23 device videos were not captured at the maximum resolution available, {{as opposed to}} all other acquisitions. Similarly to Appendix Table 6, we included in Appendix Table 7 the columns create/modify date and GPS present. The former counts the number of videos in which <b>metadata</b> create <b>date</b> and modify date are not identical, while the latter counts the number of videos in which the GPS location tag is not empty.|$|R
50|$|File {{attributes}} are metadata {{associated with}} computer files that define file system behavior. Each attribute can {{have one of}} two states: set and cleared. Attributes are considered distinct from other <b>metadata,</b> such as <b>dates</b> and times, filename extensions or file system permissions. In addition to files, folders, volumes and other file system objects may have attributes.|$|R
40|$|The dataset {{contains}} basic <b>metadata</b> (ID, source, <b>date,</b> job title, institution, place, application deadline, link) about 8291 job advertisements from {{libraries and}} information facilities posted at the German portal [URL] within 17. 07. 2012 (first post at openbiblio) and 31. 12. 2016. The data {{does not contain}} the job postings themselves. The column "source" provides information about how the respective entry was obtained...|$|R
5000|$|Timecode {{is stored}} in the {{metadata}} areas of captured DV AVI files, and some software is able to [...] "burn" [...] (overlay) this into the video frames. For example, DVMP Pro is able to [...] "burn" [...] timecode or other items of DV <b>metadata</b> (such as <b>date</b> and time, iris, shutter speed, gain, white balance mode, etc.) into DV AVI files.|$|R
40|$|Metavid is a {{community}} driven archive of legislative video from both houses of the U. S. Congress, spanning from early 2006 to the present. This archive is searchable by speaker name, spoken text, <b>date,</b> <b>metadata</b> scraped from outside sources and user contributions. Metavid is video wiki where users improve its accuracy by fixing transcripts and annotating speeches. All contributed content and metadata is made available for reuse in its entirety under a free content license. Video content is in the public domain...|$|R
40|$|We {{developed}} a desktop search interface called “DashSearch” that enables users to retrieve stored data by efficiently using various <b>metadata</b> (e. g. <b>date</b> and author). DashSearch consists of several desktop widgets, i. e., simple applications related to metadata (e. g. calendar and address book). Each widget {{works as an}} input and output interface. Users can intuitively set search conditions by combining widgets and use widget to browse search results from various viewpoints. ACMClassification: H 5. 2 [Information interfaces and pre...|$|R
50|$|The most {{essential}} software feature is bibliographic information of individual content and advanced search. The {{content of the}} Digital library is categorized with added tags presented by faceted display or tree-like structure. User can mark items or their parts as favourite, make annotations and share them with specific person a user can freely define himself. User and „his group“ can comment on the shared annotations and share annotations, collections, objects or specific parts of displayed objects over Facebook, Twitter and Google+. The Digital library software operates effective tools for selection of whole page or selected textual and graphical zones from the original scanned files, which can be copied and translated instantly and be easily extracted into other software applications. All items enable search through the content and hits are shown directly as overlaid highlights on images. It is also possible to search the content via <b>metadata</b> or <b>date</b> in combination with categories. Search results can be visually organized and sorted according to various criteria.|$|R
40|$|Research {{networks}} {{provide a}} framework for review, synthesis and systematic testing of theories by multiple scientists across international borders critical for addressing global-scale issues. In 2012, a GHG research network referred to as MAGGnet (Managing Agricultural Greenhouse Gases Network) was established within the Croplands Research Group of the Global Research Alliance on Agricultural Greenhouse Gases (GRA). With involvement from 46 alliance member countries, MAGGnet seeks to provide a platform for the inventory and analysis of agricultural GHG mitigation research throughout the world. To <b>date,</b> <b>metadata</b> from 315 experimental studies in 20 countries have been compiled using a standardized spreadsheet. Most studies were completed (74...|$|R
40|$|Abstract. We {{investigate}} the automatic harvesting of research paper metadata from recent scholarly events. Our system, Kairos, combines a focused crawler and an information extraction engine, to convert {{a list of}} conference websites into a index filled with fields of metadata that correspond to individual papers. Using event <b>date</b> <b>metadata</b> extracted from the conference website, Kairos proactively harvests metadata about the individual papers soon after they are made public. We use a Maximum Entropy classifier to classify uniform resource locators (URLs) as scientific conference websites and use Conditional Random Fields (CRF) to extract individual paper metadata from such websites. Experiments show an acceptable measure of classification accuracy of over 95 % {{for each of the}} two components. ...|$|R
50|$|MediaWiki code is {{designed}} to allow for data to be written to a master database and read from slave databases, although the master {{can be used for}} some read operations if the slaves are not yet up to <b>date.</b> <b>Metadata,</b> such as article revision history, article relations (links, categories etc.), user accounts and settings can be stored in core databases and cached; the actual revision text, being more rarely used, can be stored as append-only blobs in external storage. The software is suitable for the operation of large scale wiki farms such as Wikimedia, which had about 800 wikis as of August 2011. However, MediaWiki comes with no built-in GUI to manage such installations.|$|R
40|$|Many {{multimedia}} collections {{include only}} <b>metadata</b> such as <b>date</b> created and file size and remain largely unannotated. So, browsing them is cumbersome. Automatic content-analysis techniques yield metadata {{in the form}} of high-level content-based descriptors. However, these techniques' accuracy is insufficient to automate collection categorization. A human is essential to validate and organize automated techniques' results. MediaTable helps users efficiently categorize an image or video collection. A tabular interface gives an overview of multimedia items and associated metadata, and a bucket list lets users quickly categorize materials. MediaTable uses familiar interface techniques for sorting, filtering, selection, and visualization. Evaluations with expert and nonexpert users indicate that MediaTable supports efficient categorization and provides valuable insight into the collection. Keywords: computer graphics, graphics and multimedia, image and video search systems, interactive multimedia retrieval, multimedia collection categorization, table-based browsing...|$|R
40|$|In {{the recent}} years, photo context <b>metadata</b> (e. g., <b>date,</b> GPS coordinates) have been {{proved to be}} useful in the {{management}} of personal photos. However, these metadata are still poorly considered in photo retrieving systems. In order to overcome this limitation, we propose an approach to incorporate contextual metadata, in a keyword-based photo retrieval process. We use metadata about the photo shot context (address location, nearby objects, season, light status…) to generate a bag of words for indexing each photo. We extend the Vector Space Model in order to transform these shot context words into document-vector terms. In addition, spatial reasoning and geographical ontologies are used to infer new indexing terms. This facilitates the query-document matching process and also allows performing semantic comparison between the query terms and photo annotations. 1...|$|R
5000|$|RSS (Rich Site Summary; {{originally}} RDF Site Summary; {{often called}} Really Simple Syndication) {{is a type}} of web feed which allows users to access updates to online content in a standardized, computer-readable format. These feeds can, for example, allow a user to keep track of many different websites in a single news aggregator. The news aggregator will automatically check the RSS feed for new content, allowing the content to be automatically passed from website to website or from website to user. This passing of content is called web syndication. Websites usually use RSS feeds to publish frequently updated information, such as blog entries, news headlines, audio, video. An RSS document (called [...] "feed", [...] "web feed", or [...] "channel") includes full or summarized text, and <b>metadata,</b> like publishing <b>date</b> and author's name.|$|R
40|$|High-fidelity, {{textured}} geometric {{models are}} a fundamental {{starting point for}} computer graphics, simulation, visualization, design, and analysis. Existing tools for acquiring 3 d models of large-scale (e. g., urban) geometry from imagery require significant manual input and suffer other, algorithmic scaling limitations. We are pursuing a research and engineering effort to develop a novel sensor, and associated geometric algorithms, to achieve fully automated reconstruction from close-range color images of textured geometric models representing built urban structures. The sensor is a geo-located camera, which annotates each acquired digital image with <b>metadata</b> recording the <b>date</b> and time of image acquisition, and estimating the position and orientation of the acquiring camera in a global (geodetic) coordinate system. This metadata enables the formulation of reconstruction algorithms which scale well both with the number and spatial density of input images, and {{the complexity of the}} recons [...] ...|$|R
40|$|The HiRISE camera aboard NASA's Mars Reconnaissance Orbiter has {{captured}} images of Martian surface features of unprecedented quality, resolving objects {{just a few}} meters across. This archive of HiRISE imagery contains over a thousand images and is searchable by theme (climate change, aeolian processes, fluvial processes, volcanic processes, and tectonic processes), or by keyword. For users with slower internet connections, the site also features an online map viewer that can be zoomed and panned to select images to explore in full resolution. The front page displays a featured image {{and a set of}} images from the previous week. Images are downloadable and are accompanied by <b>metadata,</b> including acquisition <b>date,</b> location, and range to target site; some are also accompanied by a brief written description detailing what is being shown. Educational levels: Middle school, High school, Undergraduate lower division, Undergraduate upper division, Graduate or professional...|$|R
40|$|Research Object (RO) {{repositories}} extend {{traditional forms}} of scholarly communication by providing scientists the means necessary to store, share and reuse datasets generated at various stages of the research process. Yet this shift to digital publication does not guarantee that outputs, results or methods are reusable. Data quality is absolutely vital for the dissemination, reuse and sharing of digital resources. Manual metadata quality control is practically impossible and as a result, many quality criteria, both semantically and structurally get overlooked and digital objects may become problematic. The {{aim of the research}} reported on this paper was to identify the data quality problems associated with the Dryad research data repository. In particular, three <b>metadata</b> elements (Creator, <b>Date</b> and Resource Type) were analysed and quality issues associated to these elements were identified. The paper concludes with some recommendations for improving the quality of metadata in research data repositories...|$|R
40|$|Abstract- With the {{explosion}} {{in the amount of}} semi-structured data users access and store, {{there is a need for}} complex search tools to retrieve often very heterogeneous data in a simple and efficient way. Existing tools usually index text content, allowing for some IR-style ranking on the textual part of the query, but only consider structure (e. g., file directory) and <b>metadata</b> (e. g., <b>date,</b> file type) as filtering conditions. We propose a novel multidimensional querying approach to semi-structured data searches in personal information systems by allowing users to provide fuzzy structure and metadata conditions in addition to traditional keyword conditions. The provided query interface is more comprehensive than content-only searches as it considers three query dimensions (content, structure, metadata) in the search. We have implemented our proposed approach in the Wayfinder file system. In this demo, we will use this implementation to both present an overview of the unified scoring framework underlying the fuzzy multi-dimensional querying approach and demonstrate its potential in improving search results. I...|$|R
40|$|In {{order to}} achieve true {{content-based}} information retrieval on video we should analyse and index video with high-level semantic concepts in addition to using user-generated tags and structured <b>metadata</b> like title, <b>date,</b> etc. However the range of such high-level semantic concepts, detected either manually or automatically, is usually limited compared to the richness of information content in video and the potential vocabulary of available concepts for indexing. Even though there is work to improve the performance of individual concept classifiers, we should strive {{to make the best}} use of whatever partial sets of semantic concept occurrences are available to us. We describe in this paper our method for using association rule mining to automatically enrich the representation of video content through a set of semantic concepts based on concept co-occurrence patterns. We describe our experiments on the TRECVid 2005 video corpus annotated with the 449 concepts of the LSCOM ontology. The evaluation of our results shows the usefulness of our approach...|$|R
40|$|The author {{presents}} GNU Eprints. EPrints software {{has been}} created so that institutions can create OAI-compliant Archives quickly, easily and for free. OAI-compliance means all Archives created in this way are "interoperable. " They use the same (OAI) convention for tagging their <b>metadata</b> (author, title, <b>date,</b> journal, etc.). That means the contents of all such Archives can be harvested integrated, navigated and searched seamlessly, {{as if they were}} all in one global "virtual" archive. The primary purpose of the EPrints software is to help create open access to the peer-reviewed research output of all scholarly and scientific research institutions (mainly universities). Maximizing the access to research findings maximizes their usage and their impact on further research, to the benefit of researchers, theit institutions, the society that supports research, and to research itself. The EPrints software was designed primarily to be used by researchers and their institutions to maximize the access to [...] and hence the impact of [...] their research outpu...|$|R
50|$|The {{metadata}} written {{within the}} file, implemented in Vista, is also utilized in Windows 7. This can sometimes lead to long wait times displaying {{the contents of}} a folder. For example, if a folder contains many large video files totaling hundreds of gigabytes, and the Window Explorer pane is in Details view mode showing a property contained within the <b>metadata</b> (for example <b>Date,</b> Length, Frame Height), Windows Explorer might have to search the contents of the whole file for the meta data. Some damaged files can cause a prolonged delay as well. This is due to metadata information being able to be placed anywhere within the file, beginning, middle, or end, necessitating a search of the whole file. Lengthy delays also occur when displaying {{the contents of a}} folder with many different types of program icons. The icon is contained in the metadata. Some programs cause the activation of a virus scan when retrieving the icon information from the metadata, hence producing a lengthy delay.|$|R
40|$|Abstract — With the {{explosion}} {{in the amount of}} semi-structured data users access and store, {{there is a need for}} complex search tools to retrieve often very heterogeneous data in a simple and efficient way. Existing tools usually index text content, allowing for some IR-style ranking on the textual part of the query, but only consider structure (e. g., file directory) and <b>metadata</b> (e. g., <b>date,</b> file type) as filtering conditions. We propose a novel multidimensional querying approach to semi-structured data searches in personal information systems by allowing users to provide fuzzy structure and metadata conditions in addition to traditional keyword conditions. The provided query interface is more comprehensive than content-only searches as it considers three query dimensions (content, structure, metadata) in the search. We have implemented our proposed approach in the Wayfinder file system. In this demo, we will use this implementation to both present an overview of the unified scoring framework underlying the fuzzy multi-dimensional querying approach and demonstrate its potential in improving search results. I...|$|R
40|$|Abstract—With the {{explosion}} {{in the amount of}} semi-structured data users access and store in personal information management systems, there is a critical need for powerful search tools to retrieve often very heterogeneous data in a simple and efficient way. Existing tools typically support some IR-style ranking on the textual part of the query, but only consider structure (e. g., file directory) and <b>metadata</b> (e. g., <b>date,</b> file type) as filtering conditions. We propose a novel multi-dimensional search approach that allows users to perform fuzzy searches for structure and metadata conditions in addition to keyword conditions. Our techniques individually score each dimension and integrate the three dimension scores into a meaningful unified score. We also design indexes and algorithms to efficiently identify the most relevant files that match multi-dimensional queries. We perform a thorough experimental evaluation of our approach and show that our relaxation and scoring framework for fuzzy query conditions in noncontent dimensions can significantly improve ranking accuracy. We also show that our query processing strategies perform and scale well, making our fuzzy search approach practical for every day usage...|$|R
40|$|The drive towards more {{transparency}} {{in research and}} open data increases {{the importance of being}} able to find information and make links to the data. The amount of electronic data produced and the move away from the familiar and convenient paper notebook to electronic lab notebooks (ELNs) provides more scope for sharing information, but also makes it more difficult to find. Machine generated metadata helps with sharing across software and systems and also assists with retrieval for machines, but not necessarily in a form friendly for the humans that need it. Metadata is an essential ingredient for use in ELNs to make experiment data and associated notes and interpretations easier to retrieve and more organized. Electronic lab notebooks and other recording software captures useful <b>metadata</b> like <b>date,</b> time, and location, but are reliant upon users to add meaningful metadata like topic and person after the recording process. The University of Southampton has developed an ELN that enables users to add their own metadata to notebook entries. A survey of 110 of these ELNs was completed to assess user behavior and patterns of metadata usage within ELNs. In addition, the user perceptions and expectations of metadata in ELNs were gathered through user interviews and user testing activities of different groups. The findings from both indicate that whilst some groups are comfortable with metadata and are able to design a metadata structure that works effectively, many users have no knowledge of where to start to define metadata or even an understanding of what it is, and why it is useful. Strategies for encouraging and improving metadata use in ELNs from the study findings including improving interface design, user education, standard schema designs, and encouraging collaboration between same discipline groups to promote consistency and best practices. <br/...|$|R
40|$|Texte intégral en ligne : [URL] audience[URL] This paper {{presents}} the new TXM software platform giving online access to Old French Text Manuscripts images and tagged transcriptions for concordancing and text mining. This platform {{is able to}} import medieval sources encoded in XML according to the TEI Guidelines for linking manuscript images to transcriptions, encode several diplomatic levels of transcription including abbreviations and word level corrections. It includes a sophisticated tokenizer {{able to deal with}} TEI tags at different levels of linguistic hierarchy. Words are tagged on the fly during the import process using IMS TreeTagger tool with a specific language model. Synoptic editions displaying side by side manuscript images and text transcriptions are automatically produced during the import process. Texts are organized in a corpus with their own <b>metadata</b> (title, author, <b>date,</b> genre, etc.) and several word properties indexes are produced for the CQP search engine to allow efficient word patterns search to build different type of frequency lists or concordances. For syntactically annotated texts, special indexes are produced for the Tiger Search engine to allow efficient syntactic concordances building. The platform has also been tested on classical Latin, ancient Greek, Old Slavonic and Old Hieroglyphic Egyptian corpora (including various types of encoding and annotations) ...|$|R
40|$|Research {{networks}} {{provide a}} framework for review, synthesis and systematic testing of theories by multiple scientists across international borders critical for addressing global-scale issues. In 2012, a GHG research network referred to as MAGGnet (Managing Agricultural Greenhouse Gases Network) was established within the Croplands Research Group of the Global Research Alliance on Agricultural Greenhouse Gases (GRA). With involvement from 46 alliance member countries, MAGGnet seeks to provide a platform for the inventory and analysis of agricultural GHG mitigation research throughout the world. To <b>date,</b> <b>metadata</b> from 315 experimental studies in 20 countries have been compiled using a standardized spreadsheet. Most studies were completed (74 %) and conducted within a 1 - 3 -year duration (68 %). Soil carbon and nitrous oxide emissions were measured in over 80 % of the studies. Among plant variables, grain yield was assessed across studies most frequently (56 %), followed by stover (35 %) and root (9 %) biomass. MAGGnet has contributed to modeling efforts and has spurred other research groups in the GRA to collect experimental site metadata using an adapted spreadsheet. With continued growth and investment, MAGGnet will leverage limited-resource investments by any one country to produce an inclusive, globally shared meta-database focused on the science of GHG mitigation. 201...|$|R
40|$|There are 24, 000 peer-reviewed {{journals}} worldwide, publishing 2. 5 million articles per year. No university can afford {{all or most}} of the journals its researchers may need. Hence all articles are losing some of their research impact (usage and citations). Recent findings show that articles whose authors supplement subscription-based access by self-archiving their own final drafts free for all on the web are downloaded and cited twice as much across all 12 disciplines analysed so far. Citation counts are robust indicators of research performance; self-archived articles have a substantial competitive. Only 15 % of the 2. 5 million articles published annually are being spontaneously self-archived worldwide today. Creating an Institutional Repository (IR) and encouraging staff to self-archive is a good first step, but the only institutions that are reliably approaching a 100 % annual self-archiving rate today are those that not only create an IR and provide library help for depositing, but also adopt a self-archiving policy requirement or mandate. There is no need for any penalties for non-compliance. Two international, cross-disciplinary JISC surveys have found that 95 % of authors will comply. The four institutions worldwide that have adopted a self-archiving mandate to date have confirmed this. 93 % of journals have already endorsed author self-archiving; only 7 % of journals have not. What needs to be mandated: (1) immediately upon acceptance for publication (2) deposit in the Institution’s OA Repository (3) the author’s final accepted draft (not the publisher’s proprietary PDF) (4) both its full-text and its bibliographic <b>metadata</b> (author, <b>date,</b> title, journal, etc.) (Note that only the depositing itself needs to be mandated. Setting the access privileges to the full-text can be left up to the author, with Open Access strongly encouraged, but not mandated.) Self-archiving is effortless, taking only a few minutes and a few keystrokes; library help is available too (but hardly necessary). The mandate need have no penalties or sanctions in order to be successful; it need only be formally adopted, with the support of Heads of Schools, the library, and computing services. The rest {{will take care of itself}} naturally of its own accord, as the experience of Southampton ECS, Minho, QUT and CERN has already demonstrated...|$|R
40|$|With the {{explosion}} {{in the amount of}} semi-structured data users access and store in personal information management systems, {{there is a need for}} complex search tools to retrieve often very heterogeneous data in a simple and efficient way. Existing tools usually index text content, allowing for some IR-style ranking on the textual part of the query, but only consider structure (e. g., file directory) and <b>metadata</b> (e. g., <b>date,</b> file type) as filtering conditions. We propose a novel multi-dimensional approach to semi-structured data searches in personal information management systems by allowing users to provide fuzzy structure and metadata conditions in addition to keyword conditions. Our techniques provide a complex query interface that is more comprehensive than content-only searches as it considers three query dimensions (content, structure, metadata) in the search. We propose techniques to individually score each dimension, as well as a framework to integrate the three dimension scores into a meaningful unified score. Our work is integrated in Wayfinder, an existing fully-functioning file system. We perform a thorough experimental evaluation of our techniques to show the effect of approximating individual dimensions on the overall scores and ranks of files, as well as on query performance. Our experiments show that our scoring strategy adequately takes into account the approximation in each dimension to efficiently evaluate fuzzy multi-dimensional queries. In addition, fuzzy query conditions in non-content dimensions can significantly improve scoring (and thus ranking) accuracy. 1...|$|R
40|$|WMO Publication 47 (Pub. 47) {{contains}} {{information on}} the Voluntary Observing Ship (VOS) observations contained within ICOADS. However, this data has been collected over a 50 year period by numerous countries and {{in a number of}} different formats, ranging from 13 fields in the 1950 ’s up to around 120 fields in the modern editions. The changing formats and contents of Pub. 47 are described in Kent et al. (2006). The addition of new fields leads to information becoming available which may be valid for records in earlier editions. As a result, it should be possible to increase the amount of information available from Pub. 47 by copying new fields into earlier additions as they become available. Due to the operational nature of Pub. 47 and its collection by different agencies there are a large number of coding differences and typographical errors in the metadata. Also, due to WMO regulations, once data has been added to Pub. 47 by a country that data persists until the next entry from that country. This leads to out of <b>date</b> <b>metadata</b> and may lead to ambiguous entries where multiple records for the same ship from different countries exist in the same edition. This report describes the process used to homogenize the metadata data set, correct typographic and coding errors and to copy information into the earlier editions when new fields are added. Section 2 describes the process of homogenizing the dataset and section 3 makes an assessment of the homogenization...|$|R
