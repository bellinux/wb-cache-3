0|10000|Public
40|$|Comparing liver {{transplant}} (LT) programmes internationally can improve outcomes by stimulating cross-national learning. Yet, comparison of crude outcomes, by using registry data, {{is limited by}} <b>missing</b> <b>data,</b> <b>not</b> allowing proper risk-adjustment for donor- and recipient-related factors. The objective {{of this study was}} to compare two European LT programmes based on high-quality national longitudinal databases prospectively collected in Italy and UK respectively...|$|R
40|$|Research in {{the social}} {{sciences}} is routinely affected by <b>missing</b> <b>data.</b> <b>Not</b> addressing <b>missing</b> <b>data</b> appropriately may yield research findings that are either 2 ̆ 7 slightly off 2 ̆ 7 or 2 ̆ 7 plain wrong 2 ̆ 7. This study demonstrates why and how frequently used simple remedies for <b>missing</b> <b>data</b> can impact on research results. The authors provide the target audience (i. e. producers and consumers of social science research) with a step-by-step guide on how to implement multiple imputation, which is the standard method for dealing with <b>missing</b> <b>data.</b> They encourage researchers to carefully consider the potential impact of incomplete information and to use modern <b>missing</b> <b>data</b> methods whenever possible in their own work...|$|R
30|$|Adjustment for covariates, age, ASA-classification, {{perioperative}} opioid use, {{and type}} of surgery was entered in the model. For the primary outcome measure, substitution of <b>missing</b> <b>data</b> was <b>not</b> performed as the multilevel linear model is sufficiently robust in handling <b>missing</b> <b>data.</b>|$|R
50|$|Nevertheless, {{in spite}} of these recent {{research}} studies, a higher response rate is preferable because the <b>missing</b> <b>data</b> is <b>not</b> random. There is no satisfactory statistical solution to deal with <b>missing</b> <b>data</b> that may <b>not</b> be random. Assuming an extreme bias in the responders is one suggested method of dealing with low survey response rates. A high response rate (>80%) from a small, random sample is preferable to a low response rate from a large sample.|$|R
30|$|For {{continuous}} variables, {{means and}} SD or median (IQR), {{in case of}} non-normality of distribution, will be reported. For categorical variables, number of patients in each category and corresponding percentages will be given. <b>Missing</b> <b>data</b> will <b>not</b> be replaced.|$|R
5000|$|In statistics, ignorability is {{a feature}} of an {{experiment}} design whereby the {{method of data collection}} (and the nature of <b>missing</b> <b>data)</b> do <b>not</b> depend on the <b>missing</b> <b>data.</b> A <b>missing</b> <b>data</b> mechanism such as a treatment assignment or survey sampling strategy is [...] "ignorable" [...] if the <b>missing</b> <b>data</b> matrix, which indicates which variables are observed or missing, is independent of the <b>missing</b> <b>data</b> conditional on the observed data.|$|R
40|$|It is {{demonstrated}} that, when a complete sufficient statistic exists, weaker conditions {{than those of}} Rubin (1976) are sufficient to allow one to make inferences about the sampling distribution {{in the case of}} <b>missing</b> <b>data,</b> where the process that causes <b>missing</b> <b>data</b> is <b>not</b> independent of the data. An example fronTanimal breeding is presented...|$|R
25|$|The {{most common}} method of {{uploading}} large binary posts to Usenet is {{to convert the}} files into RAR archives and create Parchive files for them. Parity files are used to recreate <b>missing</b> <b>data</b> when <b>not</b> {{every part of the}} files reaches a server.|$|R
40|$|Pedometers {{have become}} a {{frequently}} used tool to measure physical activity. The accuracy of pedometers has been an interest of many researchers given {{the popularity of the}} tool. When, pedometers are employed, there are many potential sources of error, including <b>missing</b> <b>data.</b> Because previous studies indicated that the amount of <b>missing</b> <b>data</b> does <b>not</b> alter the strength of the relationship between PA levels and relevant health outcome variables, some researchers argued that the <b>missing</b> <b>data</b> may <b>not</b> be a significant concern. In a public health perspective, previous researchers’ point of view may be true. This <b>missing</b> <b>data,</b> however, can be a significant problem if researchers are interested in directly comparing between groups and/or conditions. Without knowing how much information is missing, it is impossible to make direct comparisons. Therefore, {{the purpose of the study}} is to examine how much information is missed when participants are asked to follow the common instructions of pedometer daily wear...|$|R
30|$|All {{statistical}} {{analyses were performed}} with SAS 9.3 (SAS, Cary, NC). Values were considered significant at the level of α[*]=[*] 0.05. The Wilcoxon signed-rank test was used to test if a change was significantly different from 0. <b>Missing</b> <b>data</b> were <b>not</b> imputed for patients included in this Per-Protocol analysis.|$|R
30|$|The {{possible}} {{limitations of}} our study are: (1) because of the study design (retrospective cross-sectional study) we had no influence on <b>data</b> collection (<b>missing</b> <b>data),</b> and (2) <b>not</b> routinely performed DEXA measurements (selection bias).|$|R
40|$|As {{data gaps}} and gross errors inevitably exist in GPS time series, robust {{detection}} and interpolation procedures {{are needed to}} obtain a uniform time series for various geospatial studies and applications. The use of traditional methods for this purpose is usually based on some improper assumptions, which are not derived based on the real properties of the data. Moreover, different interpolation methods {{may need to be}} investigated for interpolation for various gaps with different types and amount of <b>missing</b> <b>data.</b> These make the interpolation for <b>missing</b> <b>data</b> <b>not</b> easy. To address the issue of mentioned above, in this study, a data-analysis method named singular spectrum analysis (SSA) for <b>missing</b> <b>data</b> is assessed for reconstructing a reliable model from unevenly sampled time series without the need for any a priori knowledge of the time series data. In this method, the interpolation and detection of gross errors are carried out in one go along with the reliable reconstructed model. Both simulation data and real GPS data testing results showed that this was an efficient method for interpolation and gross error detection...|$|R
3000|$|<b>Data</b> cleaning—remove <b>missing</b> <b>data,</b> {{cumulative}} frequency <b>not</b> between 99 and 101, blends where a single coal represented more than 90 % of the blend (i.e., only blends where {{used in this}} stage of the process) [...]...|$|R
40|$|The aim of {{this study}} was to design and {{validate}} an interviewer-administered pelvic floor questionnaire that integrates bladder, bowel and sexual function, pelvic organ prolapse, severity, bothersomeness and condition-specific quality of life. Validation testing of the questionnaire was performed using data from 106 urogynaecological patients and a separately sampled community cohort of 49 women. <b>Missing</b> <b>data</b> did <b>not</b> exceed 2...|$|R
40|$|Likelihood {{factors that}} can be disregarded for {{inference}} are termed ignorable. We demonstrate that close ties exist between ignorability and identification of causal effects by covariate adjustment. A graphical condition, stability, plays a role analogous to that of missingness at random, but is applicable to general longitudinal data. Our formulation of ignorability {{does not depend on}} any notion of <b>missing</b> <b>data,</b> so is appealing in situations where <b>missing</b> <b>data</b> may <b>not</b> actually exist. Several examples illustrate how stability may be assessed...|$|R
40|$|Version {{control and}} bug {{tracking}} systems contain {{large amounts of}} historical information that can give deep insight into {{the evolution of a}} software project. Unfortunately, these systems provide only insufficient support for a detailed analysis of software evolution aspects. We address this problem and introduce an approach for populating a release history database that combines version data with bug tracking <b>data</b> and adds <b>missing</b> <b>data</b> <b>not</b> covered by version control systems such as merge points. Then simple queries {{can be applied to the}} structured data to obtain meaningful views showing the evolution of a software project. Such views enable more accurate reasoning of evolutionary aspects and facilitate the anticipation of software evolution. We demonstrate our approach on the large Open Source project Mozilla that offers great opportunities to compare results and validate our approach. 1...|$|R
30|$|Another {{issue is}} that whole network data was sought for the project. Unfortunately, not all {{organisational}} members participated across the three time points, resulting in <b>missing</b> <b>data.</b> While full network data is preferable in most network analyses, there were <b>missing</b> <b>data</b> from individuals across the time points in this sample. However, Costenbader and Valente (2003) have found that <b>missing</b> <b>data</b> is <b>not</b> such a critical issue when including the data collected from SNA respondents on everyone, including those {{who may not have}} responded, as was the case here. It has also been observed that centrality measures do express a degree of robustness under conditions of <b>missing</b> <b>data</b> (Borgatti et al. 2006).|$|R
30|$|Since {{the primary}} methodological {{technique}} used {{in building the}} measurement model was to be factor analysis, {{it was necessary to}} test the data for suitability with regard to the underlying assumptions. The sample size was well above recommended minimum guidelines (at least 10 subjects/factor) since we only wished to extract a few factors. <b>Missing</b> <b>data</b> was <b>not</b> problematic for our data and was handled by listwise deletion since the sample size was large and the percentage of <b>missing</b> <b>data</b> was so small 4 (<[*] 2 %).|$|R
30|$|The data in {{this report}} consist of the Full Analysis dataset. This {{includes}} all patients enrolled who provided a baseline fibroid volume assessment and received treatment with the VizAblate System. Patients who received a surgical reintervention were considered treatment failures, and their subsequent data was imputed using the last observation carried forward (LOCF) method. <b>Missing</b> <b>data</b> was <b>not</b> imputed for patients who conceived or who neglected to complete a questionnaire.|$|R
40|$|Introduction and hypothesis: The aim of {{this study}} was to {{validate}} a self-administered version of the already validated interviewer-administered Australian pelvic floor questionnaire. Methods: The questionnaire was completed by 163 women attending an urogynecological clinic. Face and convergent validity was assessed. Reliability testing and comparison with the interviewer-administered version was performed in a subset of 105 patients. Responsiveness was evaluated in a subset of 73 women. Results: <b>Missing</b> <b>data</b> did <b>not</b> exceed 4...|$|R
30|$|We {{aimed to}} analyze the data on an intention-to-treat basis (ITT). We tried to obtain as {{frequently}} as possible <b>missing</b> <b>data</b> after contacting the primary study authors. If <b>missing</b> <b>data</b> could <b>not</b> be obtained, we undertook imputation of individual values for the primary outcomes only by assuming that live births or de novo adhesions would not have occurred in participants without a reported primary outcome. For all other main outcomes, we used an available data analysis. We subjected any imputation of <b>missing</b> <b>data</b> for the primary outcomes to sensitivity analyses; any substantial difference in the imputed ITT analyses compared to available data analyses was incorporated in the interpretation of the study findings and the discussion.|$|R
30|$|None of {{the seven}} studies {{satisfied}} the criteria {{for the assessment of}} exposure/outcome, as the interviewer was not blind to whether the participant was a FEM patient or HC. Most studies used the same method of ascertainment for both FEM and HC participants, but this was not clearly specified in one study (Fleck et al. 2008). Furthermore, clear details regarding <b>missing</b> <b>data</b> were <b>not</b> provided by most studies, besides for one study (Strakowski et al. 2008).|$|R
40|$|Part 2 : Data MiningInternational audienceA Voting Advice Application (VAA) is a web {{application}} that recommends to a voter the party or the candidate, who replied like him/her {{in an online}} questionnaire. Every question is responding to the political positions of each party. If the voter fails to answer some questions, it is likely the VAA to offer him/her the wrong candidate. Therefore, {{it is necessary to}} inspect the <b>missing</b> <b>data</b> (<b>not</b> answered questions) and try to estimate them. In this paper we formulate the VAA missing value problem and investigate several different approaches of collaborative filtering to tackle it. The evaluation of the proposed approaches was done by using the data obtained from the Cypriot presidential elections of February 2013 and the parliamentary elections in Greece in May, 2012. The corresponding datasets are made freely available to other researchers working in the areas of VAA and recommender systems through the Web...|$|R
40|$|Abstract. <b>Missing</b> <b>data</b> pose a {{potential}} threat to learning and classification in that they may compromise {{the ability of a}} system to develop robust, generalized models of the environment in which they operate. This investigation reports on the effects of three approaches to covering these data using an XCS-style learning classifier system. Using fabricated datasets representing a wide range of missing value densities, it was found that <b>missing</b> <b>data</b> do <b>not</b> appear to adversely affect LCS learning and classification performance. Furthermore, three types of missing value covering were found to exhibit similar efficiency on these data, with respect to learning rate and classification accuracy. ...|$|R
40|$|This paper {{introduces}} {{a method for}} modelling the deterministic component of eddy covariance CO 2 flux time series in order to supplement <b>missing</b> <b>data</b> in these important data sets. The method is based on combining multidimensional semi-parametric spline interpolation with an assumed but unstated dependence of net CO 2 flux on light, temperature and time. We test the model using a range of synthetic canopy data sets generated using several canopy simulation models realized for different micrometeorological and vegetation conditions. The method appears promising for filling large systematic gaps providing the associated <b>missing</b> <b>data</b> do <b>not</b> overerode critical information content in the conditioning data used for the model optimization...|$|R
40|$|A dilemma {{frequently}} {{faced by}} empirical researchers {{is whether they}} should keep observations without complete information in the analysis. Assuming missingness is not biased in any perceivable direction, most studies use a complete case analysis approach, whereby only observations with complete information are kept for empirical estimation. However, the literature on statistics (e. g., Little and Rubin 2002) suggests that potential biases may arise from such practice, especially if <b>missing</b> <b>data</b> are <b>not</b> <b>missing</b> completely at random (MCAR). When there are <b>missing</b> <b>data,</b> Littles MCAR test (1988) can be performed to reveal whether imputation methods are necessary to minimize the problems arising from incomplete data. We take two recently studied insurance data sets as examples to show that <b>missing</b> <b>data</b> issues can be better handled. ...|$|R
40|$|Select all, {{copy and}} past this form in a ClarisWorks or Word file, save as a model, [...] . {{ready for use}} IMPORTANT NOTE: The Author(s) is/are the sole {{responsible}} in case of copyright conflicts with previous sources of data; the Editor of the Atlas cannot check if figures or parts of texts are "copied-and-past " from other sources, including other databases; "copy-and-past " <b>data</b> are <b>not</b> <b>accepted</b> in the Atla...|$|R
40|$|In {{tests with}} time limits, items {{at the end}} are often not reached. Usually, the pattern of missing {{responses}} depends on the ability level of the respondents; therefore, <b>missing</b> <b>data</b> are <b>not</b> ignorable in statistical inference. This study models data {{using a combination of}} two item response theory (IRT) models: one for the observed response data and one for the <b>missing</b> <b>data</b> indicator. The <b>missing</b> <b>data</b> indicator is modeled using a sequential model with linear restrictions on the item parameters. The models are connected by the assumption that the respondents’ latent proficiency parameters have a joint multivariate normal distribution. Model parameters are estimated by maximum marginal likelihood. Simulations show that treating <b>missing</b> <b>data</b> as ignorable can lead to considerable bias in parameter estimates. Including an IRT model for the <b>missing</b> <b>data</b> indicator removes this bias. The method is illustrated with data from an intelligence test with a time limit...|$|R
30|$|The proper {{handling}} of <b>missing</b> <b>data</b> {{is a complicated}} endeavor [10]. One possible course of action would be to eliminate all individuals with any missing landmarks. That would call {{for the removal of}} over 25 % of the data set, which seems extreme. Several other cut points would be defensible, for example, removing individuals with more than 3 missing landmarks, 5, and so forth. It was decided, instead, to retain all 947 individuals. Most individuals (72 %) had no missing landmark coordinates, and less than 1 % had six or more missing landmarks out of the twenty-eight with <b>missing</b> <b>data.</b> If the occurrence of <b>missing</b> <b>data</b> is <b>not</b> random with respect to the morphology of the individuals, then removing individuals will reduce the variability that this study is seeking to quantify. <b>Missing</b> <b>data</b> were estimated by simply substituting mean coordinate values.|$|R
30|$|We also {{checked the}} effect of <b>missing</b> <b>data</b> on our full multivariable {{analysis}} with a best-/worst-case scenario. As expected, all risks were “diluted” by the assumption that all patients without follow-up had died. However, the risk associated with colloid use did not increase by this approach although colloid use was associated with greater loss to 90 -day follow-up. Thus, the best-case/worst-case analysis suggests that <b>missing</b> follow-up <b>data</b> did <b>not</b> substantially affect the overall result.|$|R
40|$|AbstractThis paper {{presents}} a decomposition for the posterior {{distribution of the}} covarianee matrix of normal models under a family of prior distributions when <b>missing</b> <b>data</b> are ignorable and monotone. This decomposition {{is an extension of}} Bartlett′s decomposition of the Wishart distribution to monotone <b>missing</b> <b>data.</b> It is <b>not</b> only theoretically interesting but also practically useful. First, with monotone <b>missing</b> <b>data,</b> it allows more efficient drawing of parameters from the posterior distribution than the factorized likelihood approach. Furthermore, with nonmonotone <b>missing</b> <b>data,</b> it allows for a very efficient monotone date augmentation algorithm and thereby multiple imputation or the <b>missing</b> <b>data</b> needed to create a monotone pattern...|$|R
30|$|The data {{collection}} forms were {{reviewed by the}} primary author for completion following the procedure. If the forms had any <b>missing</b> <b>data</b> that could <b>not</b> {{be obtained from the}} medical record, they were returned to the operator for completion. If information on the form contained inconsistencies, the operator was interviewed by the primary author for clarification.|$|R
30|$|Statistical {{analysis}} of the collected quantitative data was conducted using the SPSS (version 18). Many were descriptive statistical tests (e.g. frequencies, means and cross-tabulations). The Chi-squares and t-tests were used to detect and compare differences between groups (e.g. sex and age differences, mean differences of psychosocial variables between adolescents with problematic gambling parents and those without). Pearson product moment tests were computed to identify the correlates of adolescent problem gambling. Results are noted significant at p[*]<[*] 0.05. Fortunately, <b>missing</b> <b>data</b> is <b>not</b> an issue because the participants answered all the essential survey questions.|$|R
40|$|In {{designing}} longitudinal studies, {{researchers must}} {{determine the number}} of subjects to randomize based on the power to detect a clinically meaningful treatment difference and a proposed analysis plan. In this paper, we present formulas for sample size estimation and an assessment of statistical power for a two-treatment repeated measures design allowing for subject attrition. These formulas can be used for comparing two treatment groups across time in terms of linear contrasts. Subjects are assumed {{to drop out of the}} study at random so that the <b>missing</b> <b>data</b> do <b>not</b> alter the parameters of interest. ...|$|R
40|$|A {{probabilistic}} query {{may not be}} estimable from observed <b>data</b> {{corrupted by}} <b>missing</b> values if the <b>data</b> are <b>not</b> <b>missing</b> at random (MAR). It is therefore of theoretical interest and practical importance to determine in principle whether a probabilistic query is estimable from <b>missing</b> <b>data</b> or <b>not</b> when the <b>data</b> are <b>not</b> MAR. We present an algorithm that systematically determines whether the joint probability is estimable from observed <b>data</b> with <b>missing</b> values, assuming that the data-generation model is represented as a Bayesian network containing unobserved latent variables that not only encodes the dependencies among the variables but also explicitly portrays the mechanisms responsible for the missingness process. The result significantly advances the existing work...|$|R
30|$|Sensitivity {{analysis}} To {{evaluate the}} differences in effect size between the two hospitals, mixed-effect Cox regressions were done, which showed similar effect sizes, suggesting stable models. The Cox models with imputed <b>missing</b> <b>data</b> showed a significant effect for FFP transfusion and platelets transfusion, suggesting that <b>missing</b> <b>data</b> did <b>not</b> influence results. The influence of pre-ICU transfusions was also investigated, as these may influence risk of infection while on ICU. Additional file 2 : Tables S 5 and S 6 show that the different outcomes were not sensitive to exclusion of the pre-ICU transfusion period. To correct for disease severity occurring later during ICU admission, the model was re-run with SOFA score on day 3. The HR of infection following platelet transfusion remained significant in this model.|$|R
