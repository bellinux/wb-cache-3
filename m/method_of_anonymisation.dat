0|10000|Public
40|$|As a {{consequence}} of a recent curation project, the Dortmund Chat Corpus is available in CLARIN-D research infrastructures for download and querying. In a legal expertise it had been recommended that standard measures <b>of</b> <b>anonymisation</b> {{be applied to the}} corpus before it could be republished. This paper reports about the anonymisation campaign that was conducted for the corpus. Anonymisation has been realised as categorisation, and the taxonomy <b>of</b> <b>anonymisation</b> categories applied is introduced and the <b>method</b> <b>of</b> applying it to the TEI files is demonstrated. The results <b>of</b> the <b>anonymisation</b> campaign as well as issues of quality management are discussed. Finally, pseudonymisation as an alternative to categorisation is discussed in general as a <b>method</b> <b>of</b> the <b>anonymisation</b> <b>of</b> CMC data, as well as possibilities of a (partial) automatisation of the process...|$|R
40|$|In this annex, we {{will set}} out a few {{examples}} <b>of</b> the <b>anonymisation</b> <b>of</b> data, to indicate the range of techniques available to the information manager. The aim is not to provide a manual <b>of</b> <b>anonymisation,</b> but to give a flavour of the field, and {{of the variety of}} the options. We do not pretend that this is an exhaustive list <b>of</b> <b>methods,</b> or that the methods we have chosen are applicable to all anonymisation problems. We try to keep the language as accessible as possible; however, some of these techniques are statistically quite complex, which in some cases is inevitably reflected in the descriptions...|$|R
40|$|We {{presented}} a novel data anonymisation approach, which {{takes into account}} the reliability of data requesters and the relative attribute importance for the application purpose. We quantified the level <b>of</b> <b>anonymisation</b> through the concept of the degree <b>of</b> data <b>anonymisation,</b> and derived a decomposition algorithm for data anonymisation. Our experimental results show that our data anonymisation method achieves better data utility than general approaches with regard to the trust and application purposes...|$|R
40|$|Abstract: This paper {{highlights}} the main issues concerned with preserving fieldwork &quot;contracts&quot;, such as informed consent agreements, {{as they relate}} to the conduct of research and the archiving of qualitative data. We pay particular attention to the techniques and efficacy <b>of</b> <b>anonymisation</b> and, outline <b>methods</b> <b>of</b> gate-keeping for access to data. Our discussions are based on seven years experience of Qualidata, the ESRC Qualitative Data Archival Resource Centre in dealing {{with a wide range of}} qualitative data, including interviews with public figures, and the raw material arising from some of the most classi...|$|R
40|$|This paper {{highlights}} the main issues concerned with preserving fieldwork "contracts", such as informed consent agreements, {{as they relate}} to the conduct of research and the archiving of qualitative data. We pay particular attention to the techniques and efficacy <b>of</b> <b>anonymisation</b> and, outline <b>methods</b> <b>of</b> gate-keeping for access to data. Our discussions are based on seven years experience of Qualidata, the ESRC Qualitative Data Archival Resource Centre in dealing {{with a wide range of}} qualitative data, including interviews with public figures, and the raw material arising from some of the most classic empirical studies in the UK. URN: urn:nbn:de: 0114 -fqs 00037...|$|R
3000|$|Our {{analyses}} highlight {{that the}} empirical protection {{guaranteed by the}} algorithm <b>of</b> <b>anonymisation</b> {{is much higher than}} the theoretical protection. Only few attacks have a protection very close to 1 /k. We observe as an example that when the day-level dataset is anonymised with k= 5 our empirical risk analysis shows that 90 [...]...|$|R
3000|$|A {{well known}} <b>method</b> for <b>anonymisation</b> <b>of</b> data before release is k-anonymity [2]. The k-anonymity model was also {{studied in the}} context of {{trajectory}} data [4 – 6]. Given an input dataset D_T⊆ T of trajectories, the objective of the data release is to transform D_T into some k-anonymised form D'_T. Without this transformation, the publication of the original data can put at risk the privacy of individuals represented in the data. Indeed, an intruder who gains access to the anonymous dataset may possess some background knowledge allowing him/her to conduct attacks that may enable inferences on the dataset. We refer to any such intruders as an attacker. An attacker may know a sub-trajectory of the trajectory of some specific person and could use this information to infer the complete trajectory of the same person from the released dataset. Given the attacker’s background knowledge of partial trajectories, a k-anonymous version has to guarantee that the re-identification probability of the whole trajectory within the released dataset has to be at most 1 /k. If we denote the probability of re-identification of the trajectories as Pr(r [...]...|$|R
40|$|Current {{analytic}} metaphysics {{has recently}} suffered a ‘bad press’, with {{claims that it}} is, at best, {{out of touch with}} modern physics, at worst, actually in conflict with the latter (Callender 2011, Ladyman and Ross 2007). While agreeing with some of these claims, {{it has been suggested that}} metaphysics may still be of service by providing a kind of ‘toolbox’ of devices, moves and manoeuvres that philosophers of science can avail themselves of in order to help provide an interpretation of theories in fundamental physics (reference removed for purposes <b>of</b> <b>anonymisation).</b> In subsequent work the viability of this position has been explored in the face of concerns that it is inherently unstable (reference removed for purposes <b>of</b> <b>anonymisation)</b> and in the context of a specific set of examples concerning dispositionalism (reference removed for purposes <b>of</b> <b>anonymisation).</b> In the latter case in particular, it has been argued that ‘standard’ dispositional accounts simply cannot be sustained in the context of modern physics but that certain ‘non-standard’ views may provide the resources to help explicate the sense in which physics may be regarded as ‘modally informed’. Here that exploration will be further extended in order to consider the implications of this view both with regard to the overall relevance of metaphysics given advances in science and for the prospects of a naturalised metaphysics more generally. In conclusion, this paper will focus on three concerns that arise in this context: that the particular tools identified are not, in fact, ‘scientifically disinterested’ and thus that the distinction between ‘naturalised’ and ‘non-naturalised’ metaphysics is at best vague or poorly drawn; that the usefulness of such tools depends on their being shaped to fit the relevant physics and thus the latter ‘guts’ metaphysics; that if metaphysics does prove to be useful in this sense then we have no reason to scorn non-naturalised metaphysics to begin with...|$|R
40|$|Speaker: Dr Kieron O'Hara Organiser: Time: 04 / 02 / 2015 11 : 00 - 11 : 45 Location: B 32 / 3077 Abstract In {{order to}} reap the {{potential}} societal benefits of big and broad data, {{it is essential to}} share and link personal data. However, privacy and data protection considerations mean that, to be shared, personal data must be anonymised, so that the data subject cannot be identified from the data. Anonymisation is therefore a vital tool for data sharing, but deanonymisation, or reidentification, is always possible given sufficient auxiliary information (and as the amount of data grows, both in terms of creation, and in terms of availability in the public domain, the probability of finding such auxiliary information grows). This creates issues for the management <b>of</b> <b>anonymisation,</b> which are exacerbated not only by uncertainties about the future, but also by misunderstandings about the process(es) <b>of</b> <b>anonymisation.</b> This talk discusses these issues in relation to privacy, risk management and security, reports on recent theoretical tools created by the UKAN network of statistics professionals (on which the author is one of the leads), and asks how long anonymisation can remain a useful tool, and what might replace it...|$|R
40|$|K-anonymisation is a {{technique}} for protecting privacy contained within a dataset. Many k-anonymisation algorithms have been proposed, and one class of such algorithms are clustering-based. These algorithms can offer high quality solutions, but are rather inefficient to execute. In this paper, we propose a method that partitions a dataset into groups first and then clusters the data within each group for k-anonymisation. Our experiments show that combining partitioning with clustering can improve the performance of clustering-based k-anonymisation algorithms significantly while maintaining the quality <b>of</b> <b>anonymisations</b> they produce...|$|R
40|$|This article {{asks whether}} the {{necessity}} of many public services results in a readiness of individuals to share personal data, and thus sacrifice {{a certain level of}} privacy, in connection with their provision. It will explore the value of privacy {{in the context of the}} on-going debates around personal data sharing, with particular focus on the public sector in England, using the UK government’s care. data project as an example. The impact on trust relations between the government, the National Health Service (NHS) and the citizen will be considered. The importance <b>of</b> <b>anonymisation</b> <b>of</b> personal data as a <b>method</b> <b>of</b> minimising privacy risks and increasing trust will be discussed. Using the results of the author’s exploratory empirical study into attitudes to sharing personal data with the public sector, the article will suggest that the benefits-versus-costs privacy problem is particularly significant in relation to data sharing projects in the public sector. The lack of definitive answers in relation to the risk of re-identification contributes to the problem. Finally, the article will suggest that future work may wish to investigate how trust in, and acceptance of, data sharing initiatives could be improved by a bottom-up institution-led approach...|$|R
40|$|Abstract. Sign {{language}} and Web 2. 0 applications are currently incompatible, {{because of the}} lack <b>of</b> <b>anonymisation</b> and easy editing of online sign language contributions. This paper describes Dicta-Sign, a project aimed at developing the technologies required for making sign language-based Web contributions possible, by providing an integrated framework for sign language recognition, animation, and language modelling. It targets four different European sign languages: Greek, British, German, and French. Expected outcomes are three showcase applications for a search-by-example sign language dictionary, a sign language-to-sign language translator, and a sign language-based Wiki...|$|R
40|$|Anonymisation {{has become}} an {{important}} tool to maximise the utility of personal data while complying with data protection laws, converting datasets into a form {{where they can be}} shared by reducing their information content so that data subjects cease to be easily identifiable. Well-publicised re-identification attacks on supposedly anonymised data and new theories of jigsaw identification have eroded faith in anonymisation’s efficacy. The UK Anonymisation Network (UKAN), discuss the realities and risks <b>of</b> <b>anonymisation,</b> and offer their pragmatic views on how the nexus between re-identification and anonymisation could be manage...|$|R
40|$|Sign {{language}} and Web 2. 0 applications are currently incompatible, {{because of the}} lack <b>of</b> <b>anonymisation</b> and easy editing of online sign language contributions. This paper describes Dicta-Sign, a project aimed at developing the technologies required for making sign language-based Web contributions possible, by providing an integrated framework for sign language recognition, animation, and language modelling. It targets four different European sign languages: Greek, British, German, and French. Expected outcomes are three showcase applications for a search-by-example sign language dictionary, a sign language-to-sign language translator, and a sign language-based Wiki. © 2009 Springer Berlin Heidelberg...|$|R
40|$|The <b>anonymisation</b> <b>of</b> {{personal}} data has multiple purposes within research: {{as a marker}} of ethical practice, a means of reducing regulation and as a safeguard for protecting respondent privacy. However, the growing capabilities of technology to gather and analyse data have raised concerns over the potential reidentification of anonymised data-sets. This has sparked a wide ranging debate amongst both academic researchers and policy makers as to whether anonymisation can continue to be relied upon. This debate {{has the potential to}} create important implications for market research. This paper analyses the key arguments both for and against anonymisation as an effective tool given the changing technological environment. We consider the future position <b>of</b> <b>anonymisation</b> and question whether anonymisation can remain its key role given the potential impact on both respondent trust and the nature of self-regulation within market research...|$|R
40|$|This {{document}} outlines {{some thoughts}} and discussions {{we have been}} having about strategies <b>of</b> <b>anonymisation</b> <b>of</b> data to be collected through the ESRC / NCRM Real Life Methods Node Connected Lives project 1. It is commonplace for social science research to adopt a policy of ‘blanket anonymisation’, whereby all names, places and other identifying features are disguised across a data set, including from interview transcripts, diaries and field notes. Here, I consider the practical and theoretical implications of such a strategy and suggest that anonymisation is not a process to be conducted – and assumed completed – at just one stage of the research process. Moreover, anonymisation strategies cannot be separated out from other methodological (such as issues around archiving or mixing methods) or indeed substantive issues (such as enabling deeper appreciation of the relationality of networks, or {{the ways in which}} space might be constructed). The implications <b>of</b> whatever <b>anonymisation</b> strategy researchers adopt on the future ability to appreciate the social and spatial processes behind networks, neighbourhoods and communities, need to be made clear throughout the research process. In summation, this document argues for a more reflexive, iterative approach to anonymisation and confidential that situates these, and other ethical concerns, {{in the context of the}} social process...|$|R
30|$|With richer {{user data}} {{available}} for data mining, work in privacy preserving data mining and privacy preserving data publishing have gained {{momentum in the}} recent years. Techniques such as adding random noise and perturbing outputs while preserving certain statistical aggregates are often used [22 – 25]. Some notable data anonymisation work include k-anonymity [2], l-diversity [26], t-closeness [27], p-sensitive k-anonymity [28], (α,k)-anonymity [29] and ε-differential privacy [30]. The k-anonymity model has been also studied and adapted {{in the context of}} movements data in different works: [4] exploits the inherent uncertainty of the moving object’s whereabouts; [5] proposes a technique based on suppression of the dangerous observations from each trajectory; and [6] proposes a data-driven spatial generalization approach to achieve k-anonymity. A critique by Domingo-Ferrer and Torra [31] analyses the drawbacks of some <b>of</b> those <b>anonymisation</b> <b>methods.</b> The trade-off between the privacy guarantees <b>of</b> <b>anonymisation</b> models and the data mining utility have been considered by authors in [32, 33]. Sramka et al. [34] compared data utility versus privacy based on two well known privacy models – k-anonymity and ε-differential privacy.|$|R
40|$|Secondary use {{of health}} data has {{a vital role}} in {{improving}} and advancing medical knowledge. While digital health records offer scope for facilitating the flow of data to secondary uses, it remains essential that steps are taken to respect wishes of the patient regarding secondary usage, and to ensure the privacy of the patient during secondary use scenarios. Consent, together with depersonalisation and its related concepts <b>of</b> <b>anonymisation,</b> pseudonymisation, and data minimisation are key methods used to provide this protection. This paper gives an overview of technical, practical, legal, and ethical aspects of secondary data use and discusses their implementation in the multi-institutional @neurIST research project...|$|R
40|$|This {{book has}} been {{developed}} to address {{a need for a}} practical guide to anonymisation that gives more operational advice than the ICO’sAnonymisation Code of Practice, whilst being less technical and forbidding than the statistics and computer science literature. The book may be of interest to an anonymisation specialist who would appreciate a fresh, integrated perspective on the topic. However, it is primarily intended for those who have data that they need to anonymise with confidence, usually in order to share it. Our aim is that you should finish the book with a practical understanding <b>of</b> <b>anonymisation</b> and an idea about how to utilise it to advance your business or organisational goals...|$|R
40|$|Popularity and {{awareness}} <b>of</b> <b>anonymisation</b> systems increased tremendously {{over the past}} years, however only a very few systems made it from research to production. These systems usually add intermediate nodes in the communication path aiming to hide user identities. Several attacks against these systems exist, like timing attacks or exploitation of latency information. In this paper, we propose an alternative approach to disclose users <b>of</b> current popular <b>anonymisation</b> systems in practice by the means of virtual network coordinate systems, a widely accepted method for latency prediction and network optimisation. Mapping physical nodes to a n-dimensional space can reveal a geographical proximity {{that is used to}} disclose users, who expect to stay anonymous. We define a model that leverages network coordinates in order to measure quantitatively the anonymity services and evaluate it on the Planet-Lab research network. The basic idea is to analyse the relative distance between nodes and to calculate the probability of nodes being hosted in the same location. Evaluation proves that our proposed model {{can be used as a}} measure of anonymity...|$|R
40|$|Privacy {{protection}} in published data sets is of crucial importance, and anonymisation is one well-known technique for privacy protection {{that has been}} successfully used in practice. However, existing anonymisation frameworks have in mind specific data structures (i. e., tabular data) and, because of this, these frameworks are difficult to apply {{in the case of}} RDF data. This paper presents an RDF anonymisation framework that has been developed to address the particularities of the RDF specification. Such framework includes an anonymisation model for RDF data, a set <b>of</b> <b>anonymisation</b> operations for the implementation of such model, and a metric for measuring precision and distortion of anonymised RDF data. Furthermore, this paper presents a use case of the proposed RDF anonymisation framework...|$|R
40|$|To {{balance the}} {{protection}} of geo-privacy and the accuracy of spatial patterns, we developed a geo-spatial tool (GeoMasker) intended to mask the residential locations of patients or cases in a geographic information system (GIS). To elucidate the effects of geo-masking parameters, we applied 2010 dengue epidemic data from Taiwan testing the tool’s performance in an empirical situation. The similarity of pre- and post-spatial patterns was measured by D statistics under a 95 % confidence interval. In the empirical study, different magnitudes <b>of</b> <b>anonymisation</b> (estimated Kanonymity ≥ 10 and 100) were achieved and different degrees of agreement on the pre- and post-patterns were evaluated. The application is beneficial for public health workers and researchers when processing data with individuals’ spatial information...|$|R
40|$|Most {{existing}} works <b>of</b> data <b>anonymisation</b> target at the optimization <b>of</b> the <b>anonymisation</b> metrics {{to balance the}} data utility and privacy, whereas they ignore {{the effects of a}} requester's trust level and application purposes during the data <b>anonymisation.</b> Our aim <b>of</b> this paper is to propose a much finer level anonymisation scheme with regard to the data requester's trust value and specific application purpose. We prioritize the attributes for anonymisation based on how important and critical they are related to the specified application purposes and propose a trust evaluation strategy to quantify the data requester's reliability, and further build the projection between the trust value and the degree of data anonymiztion, which intends to determine to what extent the data should be anonymizd. The decomposition algorithm is developed to find the desired anonymous solution, which guarantees the uniqueness and correctnes...|$|R
40|$|Historians {{and social}} {{scientists}} generally understand nationalism to be the defining feature of fascism. This study challenges that assumption with the examination of Swedish fascist movements through the concept of self-identity. Based on interwar fascist periodicals, the development of Swedish fascists‘ self-identity in relation to race, nation, and the signifiers of 'fascism' and 'National Socialism', is traced from the early 1920 s when an overt attachment to Mussolini‘s model was displayed, through a National Socialist phase showing a cautious commitment to Nazi Germany, ending with a final phase of 'anonymisation'. In the face of criticism that their ideology was alien to Sweden, fascists adapted their self-representation to accommodate nationalist commitments, developing a transnational racialist ideology believed to be {{more in tune with}} Swedish political culture. When public opinion turned decisively against 'international fascism' in the mid- 1930 s, they were forced to discard the name and image of 'fascism' altogether, in a final phase <b>of</b> <b>anonymisation,</b> which however did not entail any significant ideological metamorphosis...|$|R
50|$|In the United Kingdom, barnardisation is {{sometimes}} employed by public agencies {{in order to}} enable them to provide information for statistical purposes without infringing the information privacy rights of the individuals to whom the information relates. The question whether barnardisation may fall short <b>of</b> the complete <b>anonymisation</b> <b>of</b> data and the status of barnardised data under the complex provisions of the Data Protection Act 1998 were considered by the House of Lords in the case of Common Services Agency v Scottish Information Commissioner 2008 1 WLR 1550, the above case is also reported at All ER 2008 (4) 851.|$|R
40|$|We analyse {{the effect}} <b>of</b> the <b>anonymisation</b> <b>method</b> multiplicative {{stochastic}} noise on the within estimation of a linear panel model. In particular, we {{concentrate on the}} panel model with serially correlated regressors. In addition to anonymisation as such, the serial correlation in a data set with only few points in time increases the bias of the within estimator and therefore {{must be taken into}} account in correction methods...|$|R
40|$|Following the {{publication}} of our article [1] we became aware of errors introduced in the following variables which are all dates of events variables, i. e. : DMAJNCHD: Date of above (days elapsed from randomisation) DSIDED: Date of above (days elapsed from randomisation) DRSISCD: Date of above (days elapsed from randomisation) DRSHD: Date of above (days elapsed from randomisation) DRSUNKD: Date of above (days elapsed from randomisation) DPED: Date of above (days elapsed from randomisation) DALIVED: Date of above (days elapsed from randomisation) DDEADD: Date of above (days elapsed from randomisation) FLASTD: Date of last contact (days elapsed from randomisation) FDEADD: Date of death (days elapsed from randomisation) FU 1 _RECD: Date discharge form received (days elapsed from randomisation) FU 2 _DONE: Date 6 month follow-up done (days elapsed from randomisation) FU 1 _COMP: Date discharge form completed (days elapsed from randomisation). These errors, that were introduced in the process <b>of</b> <b>anonymisation,</b> have now been corrected in the final, double checked version of the dataset, which is also available at the University of Edinburgh repository (se...|$|R
40|$|Data <b>anonymisation</b> is <b>of</b> {{increasing}} {{importance for}} allowing sharing individual data among various data requesters {{for a variety}} of social network data analysis and mining applications. Most existing works <b>of</b> data <b>anonymisation</b> target at the optimization <b>of</b> the <b>anonymisation</b> metrics to balance the data utility and privacy, whereas they ignore the effects of a requester's trust level and application purposes during the data <b>anonymisation.</b> Our aim <b>of</b> this paper is to propose a much finer level anonymisation scheme with regard to the data requester's trust and specific application purpose. We firstly prioritize the attributes for anonymisation based on their importance to application purposes. Secondly, we build the projection between the trust value and the degree of data anonymization, which intends to determine to what extent the data should be anonymized. The decomposition algorithm is developed to find the desired anonymous solution, which ensures the uniqueness and correctness. Finally, we conduct extensive experiments on two real-world data sets and the results show the benefits of our approach for both data requesters and providers. ...|$|R
40|$|Anonymity and {{confidentiality}} of participants {{are central to}} ethical research practice in social research. Where possible, researchers aim to assure participants that every effort {{will be made to}} ensure that the data they provide can not be traced back to them in reports, presentations and other forms of dissemination. The primary method researchers use to preserve anonymity and confidentiality is the use of pseudonyms for participants and also for the location of the research. In addition, other practices, such as changing the reported characteristics of participants (such as gender or occupation) are also used by some researchers to conceal identities and thereby maintain the {{confidentiality of}} the data provided by participants. There are several issues that such practices raise. One is that it is difficult for researchers to know how far to take <b>anonymisation</b> <b>of</b> individuals in order for them not to be identifiable, given that research findings may be presented to a variety of audiences, including members of participants’ communities. A second issue is that research participants hold differing views about the desirability <b>of</b> <b>anonymisation,</b> presenting researchers with difficult choices between respecting the preferences of those participants who wish to be identifiable and those who prefer to remain anonymous. A third issue is that of whether or not to attempt the <b>anonymisation</b> <b>of</b> the location of the research, which may be adjudged more or less practical or impractical (depending on its distinctiveness) and more or less desirable (depending on its importance in providing the social context of the analysis that is being developed). This paper explores these issues by looking at how they have been handled by researchers in the field of community sociology (broadly defined) who have used visual data in their reports. This analysis allows the argument to be developed that although the issues themselves are not new, {{the ways in which they}} are handled by researchers are necessarily evolving in the context of technological change, the growth of research regulation, and shifts in the expectations of research that participants hol...|$|R
50|$|The {{campaign}} {{proved to}} be a success as in June 2014, the Court of BiH revised its rulebook and eliminated the <b>anonymisation</b> <b>of</b> names. However, the issue of full audio and video recordings has not changed as of yet.|$|R
30|$|Ethical {{approval}} {{was obtained}} from the Clinical Research Ethics Committee (CREC) of the Cork Teaching Hospitals, University College Cork. A waiver of informed consent was also granted by the CREC given the retrospective nature of the study and the <b>anonymisation</b> <b>of</b> data.|$|R
40|$|As {{a result}} <b>of</b> the <b>anonymisation</b> <b>of</b> parts of public space and {{increasing}} individualisation {{of social relations}} and forms of life, mechanisms of social regulation are changing within public space. Increases of private and public video surveillance were first seen in Great Britain and the United States, later in continental Europe. On {{the basis of a}} study of spatial distribution of video surveillance cameras in the context of public space in the city of Geneva, different strategies and forms of surveillance by video cameras are indicated. The investigation shows {{the degree to which the}} locality of a video surveillance camera reflects the economic activities in the area. Consequently, the implications of mostly privately owned video surveillance on the urban territoriality of different actors within public space could be analysed. Included in the research are the results of a public opinion poll involving 500 inhabitants of Olten on public perception of video surveillance. In this context, special attention was paid to video surveillance as a symbolic and material mediator, transforming social relations between individuals as well as their relationship with public space...|$|R
40|$|In this paper, {{we propose}} a novel preference-constrained {{approach}} to k-anonymisation. In {{contrast to the}} existing works on k-anonymisation which attempt to satisfy a minimum level of protection requirement as a constraint and then optimise data utility within that constraint, we allow data owners and users to specify their detailed protection and usage requirements {{as a set of}} preferences on attributes or data values, treat such preferences as constraints and solve them as a multi-objective optimisation problem. This ensures that anonymised data will be actually useful to data users in their applications and sufficiently protected for data owners. Our preliminary experiments show that our <b>method</b> is capable <b>of</b> producing <b>anonymisations</b> that satisfy a range of preferences and have a high level of data utility and protection...|$|R
40|$|Outlines {{the scope}} of Council Directive 95 / 46. Discusses whether the {{principles}} of data protection apply to data rendered anonymous. Examines the difficulty in applying sufficient protection to data once it has been rendered anonymous and {{stresses the importance of}} data controllers informing data subjects <b>of</b> any anticipated <b>anonymisation...</b>|$|R
30|$|This {{paper is}} an {{extension}} of our earlier work presented at an international conference in 2014, the proceedings of which were published by Springer. In particular, this paper describes an extension of our risk model for k-anonymity applied to c-safety, which is a framework for <b>anonymisation</b> <b>of</b> semantic trajectories. We have also described the risk analysis on null models.|$|R
30|$|In this paper, {{we propose}} an {{empirical}} risk model for privacy based on k-anonymous data release. We also discuss {{the relation of}} risk {{to the cost of}} any attack on privacy as well as the utility of the data. We validate our model against experimental car trajectory data gathered in the Italian cities of Pisa and Florence. Our experiments highlight that the empirical evaluation of the protection guaranteed by the algorithm <b>of</b> <b>anonymisation</b> on real-world data is much higher than the theoretical protection. This happens because in real life the user movements are influenced by a lot of external constraints such as the existence of one or more streets, the direction of a specific street, the traffic intensity, and so on. As an example, if in a specific area we have only one street for going from the place A to the place B all people will cross the same street and will produce similar trajectory data. This helps the result of the empirical privacy protection evaluation. To prove this fact we also generate some synthetic movement data without using any of those constraints and as expected we found that in this kind of data the empirical privacy protection is lower than that one on real data and the data quality decreases. We also discuss how the empirical risk model can be adapted to semantic trajectories anonymized by considering the privacy model c-safety [3].|$|R
