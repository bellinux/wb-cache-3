7|0|Public
40|$|Identity {{deception}} {{is common}} online; however, most people (both {{college students and}} <b>MTurkers,</b> N = 262) believed an online confederate posing as a 13 -year-old in a chat room. A well-developed online persona that uses both content and stylistic cues to portray age and gender may be effective for category deception...|$|E
40|$|Social media {{services}} such as Twitter generate phenomenal volume of content for most real-world events on a daily basis. Digging through the noise and redundancy to understand the {{important aspects of the}} content is a very challenging task. We propose a search and summarization framework to extract relevant representative tweets from a time-ordered sample of tweets to generate a coherent and concise summary of an event. We introduce two topic models that take advantage of temporal correlation in the data to extract relevant tweets for summarization. The summarization framework has been evaluated using Twitter data on four real-world events. Evaluations are performed using Wikipedia articles on the events as well as using Amazon Mechanical Turk (MTurk) with human readers (<b>MTurkers).</b> Both experiments show that the proposed models outperform traditional LDA and lead to informative summaries...|$|E
40|$|Labeling {{large-scale}} datasets {{with very}} accurate object segmentations is an elaborate task {{that requires a}} high de-gree of quality control and a budget of tens or {{hundreds of thousands of}} dollars. Thus, developing solutions that can automatically perform the labeling given only weak super-vision is key to reduce this cost. In this paper, we show how to exploit 3 D information to automatically generate very ac-curate object segmentations given annotated 3 D bounding boxes. We formulate the problem as the one of inference in a binary Markov random field which exploits appearance models, stereo and/or noisy point clouds, a repository of 3 D CAD models as well as topological constraints. We demon-strate the effectiveness of our approach in the context of au-tonomous driving, and show that we can segment cars with the accuracy of 86 % intersection-over-union, performing as well as highly recommended <b>MTurkers!</b> 1...|$|E
40|$|Nairobi {{is one of}} {{the fastest}} growing {{metropolitan}} cities and a major business and technology powerhouse in Africa. How-ever, Nairobi currently lacks monitoring technologies to ob-tain reliable data on traffic and road infrastructure conditions. In this paper, we investigate the use of mobile crowdsourc-ing as means to gather and document Nairobi’s road quality information. We first present the key findings of a city-wide road quality survey about the perception of existing road qual-ity conditions in Nairobi. Based on the survey’s findings, we then developed a mobile crowdsourcing application, called CommuniSense, to collect road quality data. The applica-tion serves as a tool for users to locate, describe, and photo-graph road hazards. We tested our application through a two-week field study amongst 30 participants to document various forms of road hazards from different areas in Nairobi. To ver-ify the authenticity of user-contributed reports from our field study, we proposed to use online crowdsourcing using Ama-zon’s Mechanical Turk (MTurk) to verify whether submitted reports indeed depict road hazards. We found 92 % of user-submitted reports to match the <b>MTurkers</b> judgements. While our prototype was designed and tested on a specific city, our methodology is applicable to other developing cities. ACM Classification Keyword...|$|E
40|$|Several {{scholars}} {{have suggested that}} Americans’ (distorted) beliefs about the rate of upward social mobility in the United States may affect political judgment and decision-making outcomes. In this article, we consider {{the psychometric properties of}} two different questionnaire items that researchers have used to measure these subjective perceptions. Namely, we report the results of a new set of experiments (N = 2, 167 U. S. <b>MTurkers)</b> in which we compared the question wording employed by Chambers, Swan and Heesacker (2015) with the question wording employed by Davidai and Gilovich (2015). Each (independent) research team had prompted similar groups of respondents to estimate the percentage of Americans born {{into the bottom of the}} income distribution who improved their socio-economic standing by adulthood, yet the two teams reached ostensibly irreconcilable conclusions: that Americans tend to underestimate (Chambers et al.) and overestimate (Davidai and Gilovich) the true rate of upward social mobility in the U. S. First, we successfully reproduced both contradictory results. Next, we isolated and experimentally manipulated one salient difference between the two questions’ response-option formats: asking participants to divide the population into either (a) “thirds” (tertiles) or (b) “ 20 %” segments (quintiles). Inverting this tertile-quintile factor significantly altered both teams’ findings, suggesting that these measures are inappropriate (too vulnerable to question-wording and item-formulation artifacts) for use in studies of perceptual (in) accuracy. Finally, we piloted a new question for measuring subjective perceptions of social mobility. We conclude with tentative recommendations for researchers who wish to model the causes and consequences of Americans’ mobility-related beliefs...|$|E
40|$|In this {{supplementary}} document, {{we present}} additional results {{that we could}} not fit in the main paper. Please refer to the corresponding sections in the main paper for discussions and analyses. Interactive exploration and alignment: Figures 1 - 4 show the average images and top retrieved images found by our system, k-means clustering, spectral clustering [3], and the recent discriminative sub-category discovery algorithm of [1] on Kids with Santa, Wedding kiss, LFW, and Church, respectively. User preference study: We conducted a user preference study to compare our average images against those produced by several baselines: (1) ArtistAverage: created by Jason Salavon (Figure 1 a in paper); (2) ManualAverage: created by manually selecting images from a database; (3) ClusteringAverage: created by [1]. All methods use∼ 100 images to generate an average image. We asked 100 Amazon Mechanical Turkers to compare the average images of the methods and select the one that conveys the most information, i. e., clearly depicts a single concept related to the keyword used to create the database (e. g., ‘Kids with Santa’). For ‘Kids with Santa’, Jason Salavon created only one average image [2], so we select the average image that is most visually similar to his result for all methods. 96, 3, and 1 <b>MTurkers</b> selected AverageExplorer, ArtistAverage, and ManualAv-erage, respectively. We also performed the same study for ‘Faces in the Wild’, ‘Wedding kiss ’ and ‘Church’, and compared against ClusteringAverage. This time, we compared sets of 6 average images (as shown in Figures 2 - 4). 99 % of MTurker...|$|E
40|$|Security {{and privacy}} {{researchers}} often rely on {{data collected from}} Amazon Mechanical Turk (MTurk) to evaluate security tools, to understand users' privacy preferences, to measure online behavior, and for other studies. While the demographics of MTurk are broader than some other options, researchers have also recently begun to use census-representative web-panels to sample respondents with more representative demographics. Yet, we know little about whether security and privacy results from either of these data sources generalize to a broader population. In this paper, we compare {{the results of a}} survey about security and privacy knowledge, experiences, advice, and internet behavior distributed using MTurk (n= 480), a nearly census-representative web-panel (n= 428), and a probabilistic telephone sample (n= 3, 000) statistically weighted to be accurate within 2. 7 % of the true prevalence in the U. S. Surprisingly, we find that MTurk responses are slightly more representative of the U. S. population than are responses from the census-representative panel, except for users who hold no more than a high-school diploma or who are 50 years of age or older. Further, we find that statistical weighting of MTurk responses to balance demographics does not significantly improve generalizability. This leads us to hypothesize that differences between <b>MTurkers</b> and the general public are due not to demographics, but to differences in factors such as internet skill. Overall, our findings offer tempered encouragement for researchers using MTurk samples and enhance our ability to appropriately contextualize and interpret the results of crowdsourced security and privacy research...|$|E

