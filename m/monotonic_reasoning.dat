52|8|Public
40|$|Over {{the past}} few decades, non-monotonic {{reasoning}} has developed {{to be one of}} the most important topics in computational logic and artificial intelligence. The non-monotonicity here refers to the fact that, while in usual (<b>monotonic)</b> <b>reasoning</b> adding more axioms leads to potentially more possible conclusions...|$|E
40|$|The major {{approaches}} to belief revision and non <b>monotonic</b> <b>reasoning</b> {{proposed in the}} literature differ along a number of dimensions, including whether they are "syntax-based" or "semantic-based", "foundational" or "coherentist", "consistence [...] restoring" or "inconsistency [...] tolerant". Our contribution towards clarifying the connections between these various approaches is threefold: ffl We show that the two main {{approaches to}} belief revision, the foundations and coherence theories, are mathematically equivalent, thus answering a question left open in [Gar 90, Doy 92]. The distinction between syntaxbased approaches to revision and approaches based on (semantic) preferential structures falls along similar lines, and their expressive equivalence {{is a consequence of}} this result. ffl We formally clarify the connection between belief revision and non <b>monotonic</b> <b>reasoning,</b> in a particularly simple way which also throws light on the connection between consistence [...] restoring and reasoning [...] from [...] [...] ...|$|E
40|$|Minimal model {{generation}} has received great attention as people have {{deeper and deeper}} understanding in the semantics of the logic programming, deductive database, non <b>monotonic</b> <b>reasoning</b> and the relationships among them. But most proposed minimal model generation procedures in literature are inappropriate {{in the sense that}} while generating minimal model, they also generate non-minimal models. This means an explicit minimization has to be employed to obtain minimal models. This may be a great factor of inefficiency. In this paper we develop an approach to generate the minimal models, without explicit minimization process, based on E-hyper tableau which is a variant of hyper tableaux. The soundness and completeness of the procedure are proved. 1 Introduction Minimal model {{generation has}} received great attention as people have deeper and deeper understanding in the semantics of the logic programming, deductive database, non <b>monotonic</b> <b>reasoning</b> and the relationships among them [...] ...|$|E
40|$|Abstract. Because of the {{complexity}} and fuzziness of the real world, it’s hard to build a dense knowledge system and reason in it with traditional methods. But man can deal with such tasks freely. Inspired by cognition and linguistics, a solution is advanced for reasoning dense knowledge in this paper. Objects and concepts are organized {{in the form of}} concept graph. Soaking the nodes in the graph until the result is represented in the graph the final graph can be the explanation of the scenario. With the naïve algorithm, <b>monotonic</b> scenario <b>reasoning</b> problem can be solved in dense knowledge environment...|$|R
50|$|Reasoning {{systems may}} employ the closed world {{assumption}} (CWA) or open world assumption (OWA). The OWA {{is often associated with}} ontological knowledge representation and the Semantic Web. Different systems exhibit a variety of approaches to negation. As well as logical or bitwise complement, systems may support existential forms of strong and weak negation including negation-as-failure and ‘inflationary’ negation (negation of non-ground atoms). Different reasoning systems may support <b>monotonic</b> or non-monotonic <b>reasoning,</b> stratification and other logical techniques.|$|R
40|$|A more {{detailed}} formalization {{of part of}} the QSIM framework {{has been found to}} be useful. That formalization is presented here in brief. The new formalization makes it easier to make and prove some useful statements. The formalization of temporal reasoning about continuous systems [7] and the precise definition of completeness are examples of the usefulness of this formalization. 1 Introduction In many applications in which ordinary differential equations (ODEs) are used, complete information about initial conditions or the specific relationship between a pair of quantities is not completely known. In some cases constants are known to lie in a certain range or the relationship between quantities is only known to be <b>monotonic.</b> Qualitative <b>reasoning</b> allows this information to be used to generate descriptions of solutions to any ODE which abstracts to the known information. We call such an abstract ODE a qualitative differential equation (QDE). In many such applications we want to draw co [...] ...|$|R
40|$|Default Logic and Logic Programming with stable model {{semantics}} {{are recognized}} as powerful frameworks for incomplete information representation. Their expressive power are suitable for non <b>monotonic</b> <b>reasoning,</b> but the counterpart is their {{very high level of}} theoretical complexity. The {{purpose of this paper is}} to show how heuristics issued from combinatorial optimization and operation research can be used to built non monotonic reasonning systems...|$|E
40|$|Default Logic is {{recognized}} as a powerful framework for knowledge representation and incomplete information management. Its expressive power is suitable for non <b>monotonic</b> <b>reasoning,</b> but the counterpart is its {{very high level of}} computational complexity. The {{purpose of this paper is}} to show how heuristics such as Genetic Algorithms, Ant Colony Optimization and Local Search can be used to elaborate an effcient non-monotonic reasoning system...|$|E
40|$|Commonsense {{reasoning}} is the reasoning of agents {{interacting with the}} real world. Non <b>monotonic</b> <b>reasoning</b> is a well developed research area gathering the logical formalisms that treat commonsense reasoning. One {{of the best known}} of such formalisms is Default logic. In this paper we discuss Default logic at both the proof-theoretic and semantics levels and show that Default logic provides a clear and formal framework to understand the logical nature of commonsense reasoning...|$|E
40|$|We {{present an}} {{approach}} to database update {{as a form of}} non <b>monotonic</b> temporal <b>reasoning,</b> the main idea of which is the (circumscriptive) minimization of changes with respect to a set of facts declared "persistent by default. " The focus of the paper is on the relation between this approach and the update semantics recently proposed by Katsuno and Mendelzon. Our contribution in this regard is twofold: Γ We prove a representation theorem for KM semantics in terms of a restricted subfamily of the operators defined by our construction. Γ We show how the KM semantics can be generalized by relaxing our construction in a number of ways, each justified in certain intuitive circumstances and each corresponding to one specific postulate. It follows that there are reasonable update operators outside the KM family. Our approach is not dependent for its plausibility on this connection with KM semantics. Rather, it provides a relatively rich and flexible framework in which the frame and [...] ...|$|R
40|$|Abstract. There is {{an ongoing}} {{discussion}} whether reasoning in the Semantic Web should be monotonic or not. It seems however that the problem concerns not only the reasoning over knowledge but knowledge itself, where apart from nondefeasible knowledge the prototypical knowledge can be distinguished. In the current paper we rely on the cognitive Dual Theory comprising the classical theory of definition with the prototype theory. We develop a metaontology for representing coherently both defeasible and nondefeasible knowledge and used it for an annotation of OWL axioms. The translation of annotated OWL axioms into a logic program under answer set semantics is provided. Hence the answer set solver Smodels {{may be used as}} the reasoner for annotated ontologies, handling properly the distinction between <b>monotonic</b> and nonmonotonic <b>reasoning...</b>|$|R
40|$|We suggest why, {{and show}} how, to {{represent}} defeasible reasoning about prioritization-type precedence. We define Defeasible Axiomatized Policy (DAP) circumscription: {{it is the}} first formalism to express defeasible prioritization. DAP circumscription can represent one or more (generally, a finite reflective tower) of meta-levels of such reasoning, without resorting to a more powerful logical language. We argue for the usefulness, and analyze the expressive significance, of this representational generalization. We show that it can often be achieved with only a modest increase in the mathematical complexity of inference: DAP circumscription often reduces to a series of prioritized predicate circumscriptions, for which inference procedures are currently available. DAP circumscription also offers an improved approach to pointwise prioritization and circumscription, even in the basic, <b>monotonic</b> case of <b>reasoning</b> about prioritization. We observe that unsatisfiability and representational awkw [...] ...|$|R
40|$|This paper {{extends the}} logical {{inference}} of Horn clauses in Petri net models {{to cover a}} large class of non-Horn clauses. Based on four-valued logic and the conflict transition concept, we show how the Petri net model for this class of non-Horn clauses can be constructed. The clause inference is solved by the T-invariant method or the fixpoint of markings. Both forward and backward inference {{can be used in}} our:model. It is further shown that these techniques are efficient for the common classes of <b>monotonic</b> <b>reasoning.</b> (C) 1998 John Wiley & Sons, Inc...|$|E
40|$|This paper {{addresses}} {{the problem of}} scheduling multiple tinre and priority sensltlve tasks eflicientl, ~ in an envrron-ment where the number ofresources is litnited and the re-sources have vatying capabilities and restricted capacities. We use a help desk environrnenr as our work~ng model, however, the methodologv could also be adapted to a vari-ety ofjob shop scheduling problems in general. It'e intro-duce a metric called priority time usage {{as a measure of}} task urgency and of schedule efficiency. We also introdztce a method of considering user satisfaction in scheduling b-v uti!izing f u z y <b>monotonic</b> <b>reasoning.</b> CVe propose a meth-odology for imp/ernenting a heuristic genetic algorithn~ (GA) to accomplish the scheduling task. CVe discuss how such a qvstem can use ongoing data about historical schedule performance to adapt and create progressive(v more accurate schedules in the future. We consider mod$cations to the scheduling approach which could allow for task inter-dependencies. We present an inizritive user interface which we developed to aid help desk adt?~inishators in using the system. In addition to providing a front end to the SOGA system, the inferfoce allows the user ofthe system to perform "what i f ' anaLvsis with actual schedules. Lastly, we present preliininary assessnients of the utility of both the optinzization engine and the user interface. Ke?wonls: scheduling, genetic algorithm fuzzy logic. constraint satisfac-tion problems, hslp desk, optimization hsuristics, graphical user interface. hybrid expert s) stem, <b>monotonic</b> <b>reasoning.</b> ...|$|E
40|$|In Artificial Intelligence, Default Logic is {{recognized}} as a powerful framework for knowledge representation when one {{has to deal with}} incomplete information. Its expressive power is suitable for non <b>monotonic</b> <b>reasoning,</b> but the counterpart is its very high level of theoretical complexity. Today, some operational systems are able to deal with real world applications. However, finding a default logic extension in a practical way is not yet possible in whole generality. This paper which is an extended version of 18 shows how heuristics such as Genetic Algorithms and Local Search techniques can be used and combined to build an automated default reasoning system. We give a general description of the required basic components and we exhibit experimental results. ...|$|E
40|$|In {{the present}} paper strong-monotonic, <b>monotonic</b> and weak-monotonic <b>reasoning</b> is studied {{in the context of}} {{algorithmic}} language learning theory from positive as well as from positive and negative data. Strong-monotonicity describes the requirement to only produce better and better generalizations when more and more data are fed to the inference device. Monotonic learning reflects the eventual interplay between generalization and restriction during the process of inferring a language. However, it is demanded that for any two hypotheses the one output later has to be at least as good as the previously produced one with respect to the language to be learnt. Weak-monotonicity is the analogue of cumulativity in learning theory. We relate all these notions one to the other as well as to previously studied modes of identification, thereby in particular obtaining a strong hierarchy...|$|R
40|$|The {{purpose of}} this {{tutorial}} {{is to get the}} audience familiar with the Answer Set Programming (ASP) Paradigm in the perspective of its fruitful usage for Semantic Web applications. ASP is a declarative programming paradigm with its roots in Knowledge Representation and Logic Programming. Systems and languages based on ASP are ready for tackling many of the challenges the Semantic Web offers, and in particular, are good candidates for solving a variety of issues which have been delegated to the Rule/Logic Layers in the Semantic Web vision. ASP systems are scalable, allow to mix <b>monotonic</b> with nonmonotonic <b>reasoning,</b> permit to combine rules with ontologies, and can interface external reasoners. Moreover, ASP is especially tailored at solving configuration and matchmaking problems involving reasoning with preferences by featuring easy to use, fully declarative soft & hard constraint specification languages. We introduce the attendees to the ASP basics and its principal extensions tailored at Semantic Web applications. We discuss the current impact of Answer Set Programming in the Semantic Web Area and possible future directions. Applications and exercises are presented. The attendees will practice through an online interface using one of the state-of-the-art ASP solvers and some of its extensions...|$|R
40|$|The aim of {{this work}} {{is to provide a}} unified {{framework}} for ordinal representations of uncertainty lying at the crosswords between possibility and probability theories. Such confidence relations between events are commonly found in <b>monotonic</b> <b>reasoning,</b> inconsistency management, or qualitative decision theory. They start either from probability theory, making it more qualitative, or from possibility theory, making it more expressive. We show these two trends converge to a class of genuine probability theories. We provide characterization results for these useful tools that preserve the qualitative nature of possibility rankings, while enjoying the power of expressivity of additive representations. Comment: Appears in Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence (UAI 2004...|$|E
40|$|Institute for Communicating and Collaborative SystemsDefault {{inheritance}} {{reasoning is}} a propositional approach to non <b>monotonic</b> <b>reasoning</b> designed to model reasoning with natural language generics. Inheritance reasoners model sets of natural language generics as directed acyclicgraphs,and inference {{corresponds to the}} specification of paths through those networks. A proliferation of inheritance proof theories {{exist in the literature}} along with extensive debate about the most reasonable way to construct inferences, based on intuitions about interpretations of particular inheritance networks. There has not been an accepted semantics for inheritance which unifies the set of possible proof theories, which would help identify truly ill motivated proof theories. This thesis attempts to clarify the inheritance literature in the three ways indicated in the title: psychological plausibility, proof theory and semantics...|$|E
40|$|Introduction Deciding the satisfiability of a Quantified Boolean Formula (QBF) is an {{important}} research issue in Artificial Intelligence. Many reasoning tasks involving planning [1], abduction, reasoning about knowledge, non <b>monotonic</b> <b>reasoning</b> [2], can be directly mapped into the problem of deciding the satisfiability of a QBF. In this paper we present QUBE, a system for deciding QBFs satisfiability. We start our presentation in x 2 with some terminology and definitions necessary {{for the rest of}} the paper. In x 3 we present a high level description of QUBE's basic algorithm. QUBE's available options are described in x 4. We end our presentation in x 5 with some experimental results showing QUBE effectiveness in comparison with other systems. QUBE, and more information about QUBE, are available at www. mrg. dist. unige. it/star/qube. 2 For...|$|E
40|$|Logic {{programming}} {{is one of}} the most popular and widely used knowledge representation tools. In very large scale dynamic domains where there is a constant change in knowledge, sourcing the existing knowledge which is being effected and updating the knowledge base is a crucial point to keep the knowledge base in stable condition. This paper investigates the issues involved in, simple fact/rule based update, simple fact/rule based abduction. A SLDNF based procedural approach to perform simple fact/rule based update and abduction is proposed. In this context a knowledge base is realized as a normal abductive logic program. It is shown that using this algorithm, one can always find a minimal explanation for the observation if there exists such an explanation. Key words: Abduction,Update, knowledge representation, non <b>monotonic</b> <b>reasoning.</b> ...|$|E
40|$|Our {{purpose is}} to exhibit a modular {{systematic}} method of representing non [...] <b>monotonic</b> <b>reasoning</b> problems with the Well Founded Semantics WFS of extended logic programs augmented with eXplicit negation (WFSX), augmented by its Contradiction Removal Semantics (CRSX) when needed. We apply this semantics, and its contradiction removal semantics counterpart, to represent non-monotonic reasoning problems. We show how to cast {{in the language of}} logic programs extended with explicit negation such forms of non-monotonic reasoning as defeasible reasoning, abductive reasoning and hypothetical reasoning and apply them to such different domains of knowledge representation as hierarchies and reasoning about actions. We then abstract a modular systematic method of representing non-monotonic problems in a logic programming semantics comprising two forms of negation avoiding some drawbacks of other proposals, with which we relate our work...|$|E
40|$|Deciding the satisfiability of a Quantified Boolean Formula (QBF) is an {{important}} research issue in Artificial Intelligence. Many reasoning tasks involving planning [1], abduction, reasoning about knowledge, non <b>monotonic</b> <b>reasoning</b> [2], can be directly mapped into the problem of deciding the satisfiability of a QBF. In this paper we present quBE, a system for deciding QBFs satisfiability. We start our presentation is Section 2 with some terminology and definitions necessary {{for the rest of}} the paper. In Section 3 we present a high level description of QuBE's basic algorithm. QuBE's available options are described in Section 4. We end our presentation is Section 5 with some experimental results showing QuBE effectiveness in comparison with other systems. QuBE, and more information about QuBE are available at www. mrg. dist. unige. it/star/qub...|$|E
40|$|Solving Quantified Boolean Formulas (QBF) {{has become}} an {{attractive}} research area in Artificial intelligence. Many important artificial intelligence problems (planning, non <b>monotonic</b> <b>reasoning,</b> formal verification, etc.) {{can be reduced to}} QBFs. In this paper, a new DLL-based method is proposed that integrates binary decision diagram (BDD) to set free the variable ordering heuristics that are traditionally constrained by the static order of the QBF quantifiers. BDD is used to represent in a compact form the set of models of the boolean formula. Interesting reduction operators are proposed in order to dynamically reduce the BDD size and to answer the validity of the QBF. Experimental results on instances from the QBF’ 03 evaluation show that our approach can efficiently solve instances that are very hard for current QBF solvers...|$|E
40|$|Abstract. Possibilistic Stable model Semantics is an {{extension}} of Stable Model Semantics that allows to merge uncertain and non <b>monotonic</b> <b>reasoning</b> into a unique framework. To achieve this aim, knowledge is represented by a normal logic program where each rule is given with its own degree of certainty. By this way, it formally defines a distribution of possibility over atom sets that, on its turn, induces for each atom a possibility and a necessity measures. The latter underpins the definition of a possibilistic stable model in which every consequence of the program is given with a level of certainty. In this work we explain how we can compute the possibilistic stable models of a possibilistic normal logic program by using available softwares for Answer Set Programming and we describe the main lines of the system that we have developed. ...|$|E
40|$|Based on an {{abstract}} framework for nonmonotonic reasoning, Bondarenko et al. have extended the logic programming semantics of admissible and preferred arguments to other nonmonotonic formalisms such as circumscription, auto-epistemic logic and default logic. Although the new semantics have been tacitly assumed {{to mitigate the}} computational problems of nonmonotonic reasoning under the standard semantics of stable extensions, it seems questionable whether they improve the worst-case behaviour. As a matter of fact, we show that credulous reasoning under the new semantics in propositional logic programming and propositional default logic has the same computational complexity as under the standard semantics. Furthermore, sceptical reasoning under the admissibility semantics is easier [...] since it is trivialised to <b>monotonic</b> <b>reasoning.</b> Finally, sceptical reasoning under the preferability semantics is harder than under the standard semantics. 1 Introduction Bondarenko et al. [1997] show t [...] ...|$|E
40|$|In {{the present}} paper strong [...] {{monotonic}}, monotonic and weak [...] <b>monotonic</b> <b>reasoning</b> is studied {{in the context of}} algorithmic language learning theory from positive as well as from positive and negative data. Strong [...] monotonicity describes the requirement to only produce better and better generalizations when more and more data are fed to the inference device. Monotonic learning reflects the eventual interplay between generalization and restriction during the process of inferring a language. However, it is demanded that for any two hypotheses the one output later has to be at least as good as the previously produced one with respect to the language to be learnt. Weak [...] monotonicity is the analogue of cumulativity in learning theory. We relate all these notions one to the other as well as to previously studied modes of identification, thereby in particular obtaining a strong hierarchy...|$|E
40|$|Knowledge-based systems use {{forms of}} {{reasoning}} {{that do not}} satisfy the monotonicity property of classical logical reasoning. Therefore, several nonmonotonic logics {{have been developed for}} the investigation of theoretical foundations of knowledge-based systems. This work investigates efficient inference procedures for knowledge representation using the framework of nonmonotonic logics. Recent results show that nonmonotonic reasoning cannot be brought to tractable level by restricting merely the form of the formulae. In this work the impact of the structural property of stratification on the complexity of nonmonotonic reasoning is investigated. Autoepistemic logic is used as the framework for establishing the results, because several other nonmonotonic formalisms can be embedded in it. The main result of the work is that in stratified cases the complexity of nonmonotonic reasoning is approximately {{the same as that of}} corresponding classical <b>monotonic</b> <b>reasoning.</b> This is shown by giving [...] ...|$|E
40|$|Humans {{have always}} done nonmonotonic reasoning, but {{rigorous}} <b>monotonic</b> <b>reasoning</b> in reaching given conclusions has been deservedly more respected and admired. Euclid contains the first extended monoton-ically reasoned text available to a large public. I sus-pect that even Euclid did nonmonotonic reasoning in arguing for the postulates. It is unfortunate that the rigorous <b>monotonic</b> <b>reasoning</b> of Euclid has been de-emphasized in education, because Euclid generates in {{people who are not}} mathematically minded a respect for rigor. Conclusions derived by monotonic logical reasoning from precisely stated premises have always been the ideal. When people jump to conclusions, they are crit-icized for the gaps in their reasoning, because the con-clusions are not guaranteed to follow from the premises. Worse yet, the premises are often unstated. The inability to base all conclusions on logical reason-ing from precise and agreed premises has been long noted. There are two main reactions. One is to try to develop other principles of reasoning, and the other is to abandon logic- a big mistake. In my 1977 pa-per, Epistemological problems of artificial intelligence (McCarthy 1976). I referred to people saying logic was inadequate. I may have been referring to Marvin Min-sky’s 1975 paper and not getting around to looking it up. I think {{he was the first to}} use the phrase “non-monotonic reasoning”. Long before I wrote Epistemological problems [...] ., I knew deduction wouldn’t suffice for AI. However, writ-ing that summarizing paper obliged me to say some-thing systematic about nonmonotonic reasoning and I introduced circumscription, perhaps with Occam’s ra-zor in mind. Here’s how I summarized it. The intuitive idea of circumscription is as fol-lows: We know some objects in a given class and we have some ways of generating more. We jump to the conclusion that this gives all the objects in the class. Thus we circumscribe the class to the objects we know how to generate. In (McCarthy 1986) I summarized the proposed uses of circumscription as follows...|$|E
40|$|Due to {{the recent}} {{explosion}} of available data coming from the Web, sensor readings, social media, government authorities and scientific databases, both academia and industry have increased their interest in utilizing this knowledge. Processing huge amounts of data introduces several scientific and technological challenges, and creates new opportunities. Existing works on large-scale reasoning through mass parallelization (namely parallelization based on utilizing {{a large number of}} processing units) concentrated on <b>monotonic</b> <b>reasoning,</b> which can process only consistent datasets. The question arises whether and how mass parallelization can be applied to reasoning with huge amounts of imperfect (e. g. inconsistent, incomplete) information. Potential scenarios involving such imperfect data and knowledge include ontology evolution, ontology repair and smart city applications combining a variety of heterogeneous data sources. In this thesis, we overcome the limitations of <b>monotonic</b> <b>reasoning,</b> by studying several nonmonotonic logics that have the ability to handle imperfect knowledge, and it is shown that large-scale reasoning is indeed achievable for such complex knowledge structures. This work is mainly focused on adapting existing methods, thus ensuring that the proposed solutions are parallel and scalable. Initially, preliminaries and literature review are presented in order to introduce the reader to basic background and the state-of-the-art considering large-scale reasoning. Subsequently, each chapter presents an approach for large-scale reasoning over a given logic. Large-scale reasoning over defeasible logic is supported allowing conflict resolution by prioritizing the superiority among rules in the rule set. A solution for stratified semantics is presented where rules may contain both positive and negative subgoals, thus allowing reasoning over missing information in a given dataset. The approach for stratified semantics is generalized in order to fully support the well-founded semantics, where recursion through negation is allowed. Finally, conclusion includes observations from a preliminary investigation on a restricted form of answer set programming, a generic evaluation framework for large-scale reasoning, a discussion of the main findings of this work, and opportunities for future work...|$|E
40|$|This paper {{describes}} a general {{framework for the}} formalization of <b>monotonic</b> <b>reasoning</b> about belief in a multiagent environment. The agents* beliefs are modeled as logical theories. The reasoning about their beliefs is formalized in still another theory, which we call {{the theory of the}} computer. The framework is used to model non-omniscient belief and shown to have many advantages. For instance, it allows for an exhaustive classification of the &quot;basic &quot; forms of non logical omniscience and for their &quot;composition&quot; into the structure of the system modeling multiagent omniscient belief. 1 The approach This paper {{describes a}} general framework for the formalization of <b>monotonic</b> <b>reasoning</b> about belief in a multiagent environment. The most common solution is to take a first order (propositional) theory, to extend it using a set of modal operators, and to take as meaning that an agent believes A (see for instance [Halpern and Moses, 1985]). There is only one theory of the world, however this theory proves facts about the agents ' beliefs. According to a first interpretation, this theory is taken to model things how they really are. It is therefore a finite (and possibly incomplete) presentation of what is true in the world, and the fact that B i A is a theorem means that it is, in fact, the case that a i believes A. According to another interpretation, this theory is taken to be the perspective that a generic reasoner has of the world. It is therefore a finite presentation of the reasoner's beliefs, and the fact that A is a theorem means that the reasoner believes that believes A. Once one accepts the second interpretation (as we do), a mechanized theory is naturally taken as representing the beliefs of the computer where it is implemented. Moreover, in the case of multiagent belief, a further step is to have, together with the theory of the computer, one theory (at least, see later) for each agent...|$|E
40|$|We {{provide a}} way to ease the {{verification}} of programs whose state evolves monotonically. The main idea is that a property witnessed in a prior state can be soundly recalled in the current state, provided (1) state evolves according to a given preorder, and (2) the property is preserved by this preorder. In many scenarios, such <b>monotonic</b> <b>reasoning</b> yields concise modular proofs, saving the need for explicit program invariants. We distill our approach into the monotonic-state monad, a general yet compact interface for Hoare-style reasoning about monotonic state in a dependently typed language. We prove the soundness of the monotonic-state monad {{and use it as}} a unified foundation for reasoning about monotonic state in the F* verification system. Based on this foundation, we build libraries for various mutable data structures like monotonic references and apply these libraries at scale to the verification of several distributed applications. Comment: POPL' 18 camera read...|$|E
40|$|We {{show how}} to solve the {{classical}} ATP benchmark test problem Schubert 's Steamroller, and other puzzles, in the nonclassical framework of extended disjunctive logic programming (EDLP) where neither Contraposition nor Reasoning by Cases are valid principles of inference. While the `Steamroller' can be solved using only Detachment and Disjunctive Syllogism, another puzzle, `Who killed aunt Agatha ?', needs in addition a local form of Contraposition, resp. Reasoning by Cases, {{which is achieved by}} declaring certain predicates as exact in the sense of [Kor 66]. Besides being able to solve certain classical ATP problems in a <b>monotonic</b> <b>reasoning</b> mode, EDLP can as well treat commonsense reasoning problems by employing its intrinsic nonmonotonic inference capabilities. This is illustrated by means of a nonmonotonic variant of the `Steamroller' problem. EDLP thus proves itself as a powerful general-purpose AI reasoning system. 1 Introduction Logic programming can be considered as a computationa [...] ...|$|E
40|$|Nonmonotonic {{reasoning}} {{is intended to}} apply specifically in situation where the initial information is incomplete. The most important property of traditional system is monotonicity, i. e. addition of new facts to the database or to the theory does not result in any previous fact being retracted. There does not exist any inconsistency between the old statements and newly added statements and is assume that situation do not change. But in real world problem situation changes and so many new assumptions are generated. A <b>monotonic</b> <b>reasoning</b> system cannot work efficiently in real life environments because information available is always incomplete. These problem can be solved using nonmonotonic reasoning. Nonmonotonic reasoning tend to be introduced proof theoretically and little {{attention is paid to}} their semantic characteristics or their computational tractability. I present a different approach for construction of consistent belief set using least fix point semantics, declarative semantics and procedural semantics...|$|E
40|$|Commonsense {{reasoning}} is the reasoning of agents {{interacting with the}} real world. Non <b>monotonic</b> <b>reasoning</b> is a well developed research area gathering the logical formalisms that treat commonsense reasoning. One {{of the best known}} of such formalisms is Default logic. In this paper we discuss Default logic at both the proof-theoretic and semantics levels and show that Default logic provides a clear and formal framework to understand the logical nature of commonsense reasoning. Fondazione Ugo Bordoni via Baldassarre Castiglione 59, 00142 Roma, Italia. e-mail: gba@fub. it. Work carried out in the framework of the agreement between the Italian PT Administration and Fondazione Ugo Bordoni y Dipartimento di Informatica e Sistemistica Universit`a di Roma "La Sapienza" via Salaria 113, 00198 Roma, Italia e-mail:aiello,pirri@assi. dis. uniroma 1. it 1 Introduction Commonsense {{reasoning is}} the way an agent reasons about the real world: drawing conclusions from an incomplete information on the cu [...] ...|$|E
40|$|In this paper, we {{investigate}} the proof theory of default reasoning. We generalize Reiter's framework to a <b>monotonic</b> <b>reasoning</b> system, {{and in particular}} allow formulae with nested defaults. We give proof rules for this extended default logic, called default ionic logic, and give deduction theorems. We also give examples of applications of our framework to some well-known problems: weak implication, disjunctive information, default transformation, and normal versus non-normal defaults. 1 Introduction In this paper {{we investigate}} the proof theory of default reasoning. We have as a goal a general theory for combining defaults, and a logical tool for choosing among defaults for implementation purposes. The calculus on extensions developed by Reiter [6] has yielded a kind of reasoning that has been called non-monotonic. This non-monotonicity {{is the source of}} some major problems for implementers. The applicability of modus tollens, a fundamental tool in resolution, is unclear. For example [...] ...|$|E
