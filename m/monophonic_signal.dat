14|51|Public
50|$|Station {{returned}} to the air on May 18, 2012 from a new transmitter site via FCC Special Temporary Authority (STA) for lower antenna height while new 120 foot tower is being constructed. Signal strength is strong {{in most of the}} city due to unique audio processing and temporarily <b>monophonic</b> <b>signal.</b>|$|E
50|$|Surround mixes of {{more or less}} {{channels}} are acceptable, if they are compatible, as described by the ITU-R BS. 775-1, with 5.1 surround. The 3-1 channel setup (consisting of one monophonic surround channel) is such a case, where both LS and RS are fed by the <b>monophonic</b> <b>signal</b> at an attenuated level of -3 dB.|$|E
5000|$|The codecs {{used are}} AAC and aacPlus v1 and v2 and sample rates of 8 kHz (telephone quality) to 96 kHz (surround sound quality). The other codecs used are AMR-WB+ that can create more {{multiple}} audio {{programs as well}} as limited multimedia can also be broadcast, as with HD Radio and DAB. The available broadcasting bandwidth for digital audio varies from 40 kbit/s while sharing the space with existing analog signals, or 156 kbit/s if all analog signals (except the base <b>monophonic</b> <b>signal)</b> are dropped. (For comparison, iBiquity's Hybrid Digital/analog system offers 100-150 kbit/s in shared mode, and 300 kbit/s in pure digital mode.) ...|$|E
50|$|For <b>monophonic</b> tonal <b>signals,</b> the {{zero-crossing}} rate {{can be used}} as {{a primitive}} pitch detection algorithm.|$|R
40|$|An {{efficient}} representation for the amplitude {{spectrum of}} harmonic sounds is proposed. The representation {{is based on}} modeling the rough spectral shape using Mel-frequency cepstral coefficients and their temporal evolution using the attack-decaysustain -release-model. The representation significantly reduces the number of parameters while preserving most important perceptual features of sound. The proposed representation is applied {{as a part of}} an object-based audio coding system. Demonstrations of <b>monophonic</b> <b>signals</b> are available at [URL] demopage. html...|$|R
40|$|In this paper, {{we present}} an {{incremental}} improvement of a known fundamental frequency estimation algorithm for <b>monophonic</b> <b>signals.</b> This {{is viewed as}} a case study of using our signal graph based synthesis language, PWGLSynth, for audio analysis. The roles of audio and control signals are discussed in both analysis and synthesis contexts. The suitability of the PWGLSynth system for this field of applications is examined and some problems and future work is identified. 1...|$|R
50|$|WLTL {{launched}} in January 1968 as a 10 watt radio {{station on the}} third floor of the Vaughan Building at LTHS' North Campus, with a simple omnidirectional antenna. WLTL originally operated on an assigned frequency of 88.3 MHz, but by 1969 changed frequency to 88.1 to permit WHSD, Hinsdale to operate on 88.5 and avoid having the two relatively close stations operate on adjacent channels. While licensed to operate at 10 watts, with a transmitter capable only of 10 watts power output, and a single bay horizontally polarized antenna with inexpensive transmission line, it was estimated that the actual ERP of the station at that time was approximately 7 watts. The original studio furniture was donated by local La Grange station WTAQ. The station transmitted a <b>monophonic</b> <b>signal</b> until the mid-1980s.|$|E
40|$|We {{introduce}} a new scheme for simultaneous placement {{of a number of}} sources in auditory space. The scheme is based on an assumption about the relevance of localization cues in different critical bands. Given the sum signal of a number of sources, i. e. a <b>monophonic</b> <b>signal,</b> and a set of parameters (side-information) the scheme is capable of generating a binaural signal by spatially placing the sources contained in the <b>monophonic</b> <b>signal.</b> Potential applications for the scheme are multi-talker desktop conferencing and audio coding. Preliminary experimental results suggest that the listener’s ability to identify messages in a multi-talker environment significantly improves by enhancing a <b>monophonic</b> <b>signal</b> with the proposed scheme. 1...|$|E
30|$|The <b>monophonic</b> <b>signal</b> {{model in}} (2) is a {{harmonic}} signal model, while the tonal and polyphonic signal models in (1) and (3) are not. We should stress {{that of all}} LP models described below, the pitch prediction model described in Section 4.3 is the only model in which the harmonicity property is exploited. The other models do not rely on harmonicity, although the calculation of the LP model parameters may be simplified by taking harmonicity into account.|$|E
40|$|We {{introduce}} an algorithm implementation for the de-composition of quasi-steady state {{audio signals}} using har-monic matching pursuits. Specifically, we propose an initial low-resolution pitch analysis {{followed by a}} high resolution harmonic grain extraction based on local complex interpo-lation within the spectral domain. We describe the imple-mentation of the algorithm and illustrate its applications to musical analysis of <b>monophonic</b> <b>signals</b> before finally dis-cussing possible improvements {{that could lead to}} the design of an iterative multi-pitch harmonic analysis system. 1...|$|R
40|$|In this paper, {{an audio}} coder using the {{discrete}} wavelet transform (DWT) and a warped linear prediction (WP) model, is proposed. In contrast to conventional LP, WLP {{allows for the}} control of frequency resolution closely match {{the response of the}} human auditory system. The residual from the inverse WLP filtering is analyzed by a wavelet filterband designed to approximate the critical bands. For <b>monophonic</b> <b>signals</b> sampled at 44. 1 KHz, the coder achieves near transparent quality at an average bit-rate of 64 Kb/s...|$|R
40|$|Abstract:- This work is {{concerned}} with pitch * determination in vocal music <b>monophonic</b> <b>signals.</b> The proposed Pitch Detection Algorithm (PDA) {{is based on the}} autocorrelation function, one of the most explored fundamental frequency detection methods. However, a new approach to the estimation process is developed. This new strategy consists in the introduction of a new logic processing interaction unit that enhances the co-operatio between the central extractor and postprocessor blocks (two of the three blocks that characterise most PDAs), in order to avoid erroneous pitch estimates...|$|R
40|$|In virtual {{auditory}} environments, sound {{generation is}} typically {{based on a}} two-stage approach: synthesizing a <b>monophonic</b> <b>signal,</b> implicitly equivalent to a point source, and simulating the acoustic space. The directivity, spatial distribution and position of the source can be simulated thanks to signal processing applied to the monophonic sound. A one-stage synthesis/spatialization approach, taking into account both timbre and spatial attributes of the source as low-level parameters, would achieve a better computational efficiency essential for real-time audio synthesis in interactive environments. Such approach involves a careful examination of sound synthesis and spatialization techniques to reveal {{how they can be}} connected together. This paper concentrates on the sinusoidal sound model and 3 D positional audio rendering methods. We present a real-time algorithm that combines Inverse Fas...|$|E
40|$|Research in audio source {{separation}} {{has progressed}} a long way, producing {{systems that are}} able to approximate the component signals of sound mixtures. In recent years, many efforts have focused on learning time-frequency masks {{that can be used to}} filter a <b>monophonic</b> <b>signal</b> in the frequency domain. Using current web audio technologies, time-frequency masking can be implemented in a web browser in real time. This allows applying source separation techniques to arbitrary audio streams, such as internet radios, depending on cross-domain security configurations. While producing good quality separated audio from monophonic music mixtures is still challenging, current methods can be applied to remixing scenarios, where part of the signal is emphasized or deemphasized. This paper describes a system for remixing musical audio on the web by applying time-frequency masks estimated using deep neural networks. Our example prototype, implemented in client-side Javascript, provides reasonable quality results for small modifications...|$|E
40|$|Presented at the 2 nd Web Audio Conference (WAC), April 4 - 6, 2016, Atlanta, Georgia. Research in audio source {{separation}} {{has progressed}} a long way, producing {{systems that are}} able to approximate the component signals of sound mixtures. In recent years, many efforts have focused on learning time-frequency masks {{that can be used to}} filter a <b>monophonic</b> <b>signal</b> in the frequency domain. Using current web audio technologies, time-frequency masking can be implemented in a web browser in real time. This allows applying source separation techniques to arbitrary audio streams, such as internet radios, depending on cross-domain security configurations. While producing good quality separated audio from monophonic music mixtures is still challenging, current methods can be applied to remixing scenarios, where part of the signal is emphasized or deemphasized. This paper describes a system for remixing musical audio on the web by applying time-frequency masks estimated using deep neural networks. Our example prototype, implemented in client-side Javascript, provides reasonable quality results for small modifications...|$|E
40|$|In this paper, a novel audio coder {{using the}} {{discrete}} wavelet transform (DWT) and a warped linear prediction (WLP) model, is proposed. In contrast to conventional LP, WLP {{allows for the}} control of frequency resolution to closely match {{the response of the}} human auditory system. The residual from the inverse WLP filtering is analyzed by a wavelet filterbank designed to approximate the critical bands. For <b>monophonic</b> <b>signals</b> sampled at 44. 1 kHz, the coder achieves near transparent quality at an average bit-rate of 64 kb/s...|$|R
40|$|This paper {{presents}} an informed source separation technique of monophonic mixtures. Although {{the vast majority}} of the separation methods are based on the time-frequency energy of each source, we introduce a new approach using solely phase information to perform the separation. The sources are iteratively reconstructed using an adaptation of the Multiple Input Spectrogram Inversion (MISI) algorithm from Gunawan and Sen. The proposed method is then tested against conventional MISI and Wiener filtering on <b>monophonic</b> <b>signals</b> and oracle conditions. Results show that at the cost of a larger computation time, our method outperforms both MISI and Wiener filtering in oracle conditions with much higher objective quality even with phase quantization. 1...|$|R
40|$|International audienceThis paper {{presents}} an informed source separation technique of monophonic mixtures. Although {{the vast majority}} of the sepa- ration methods are based on the time-frequency energy of each source, we introduce a new approach using solely phase informa- tion to perform the separation. The sources are iteratively recon- structed using an adaptation of the Multiple Input Spectrogram In- version (MISI) algorithm from Gunawan and Sen. The proposed method is then tested against conventional MISI and Wiener filter- ing on <b>monophonic</b> <b>signals</b> and oracle conditions. Results show that at the cost of a larger computation time, our method outper- forms both MISI and Wiener filtering in oracle conditions with much higher objective quality even with phase quantization...|$|R
40|$|International audienceIn virtual {{auditory}} environments, sound {{generation is}} typically {{based on a}} two-stage approach: synthesizing a <b>monophonic</b> <b>signal,</b> implicitly equivalent to a point source, and simulating the acoustic space. The directivity, spatial distribution and position of the source can be simulated thanks to signal processing applied to the monophonic sound. A one-stage synthesis/spatialization approach, taking into account both timbre and spatial attributes of the source as low-level parameters, would achieve a better computational efficiency essential for real-time audio synthesis in interactive environments. Such approach involves a careful examination of sound synthesis and spatialization techniques to reveal {{how they can be}} connected together. This paper concentrates on the sinusoidal sound model and 3 D positional audio rendering methods. We present a real-time algorithm that combines Inverse Fast Fourier Transform (FFT- 1) synthesis and directional encoding to generate sounds whose sinusoidal components can be independently positioned in space. In addition to the traditional frequency-amplitude-phase parameter set, partials positions are used to drive the synthesis engine. Audio rendering can be achieved on a multispeaker setup, or in binaural over headphones, depending on the available reproduction system...|$|E
40|$|International audienceIn this paper, a new model-based {{algorithm}} for optimizing the MPEG-Advanced Audio Coder (AAC) in MS-stereo mode is presented. This {{algorithm is}} an extension to stereo signals of prior work on a statistical model of quantization noise. Traditionally, MS-stereo coding approaches replace the Left (L) and Right (R) channels by the Middle (M) and Sides (S) channels, each channel being independently processed, almost like a <b>monophonic</b> <b>signal.</b> In contrast, our method proposes a global approach for coding both channels in the same process. A model for the quantization error allows us to tune the quantizers on channels M and S {{with respect to a}} distortion constraint on the reconstructed channels L and R as they will appear in the decoder. This approach leads to a more efficient perceptual noise-shaping and avoids using complex psychoacoustic models built on the M and S channels. Furthermore, it provides a straightforward scheme to choose between LR and MS modes in each subband for each frame. Subjective listening tests prove that the coding efficiency at a medium bitrate (96 kbits/s for both channels) is significantly better with our algorithm than with the standard algorithm, without increase of complexity...|$|E
40|$|In this paper, a {{multichannel}} {{version of}} the sinusoids plus noise model (also known as deterministic plus stochastic decomposition) is proposed and applied to spot microphone signals of a music recording. These are the recordings captured by the various microphones placed in a venue, before the mixing process produces the final multichannel audio mix. Coding these microphone signals makes them available to the decoder, allowing for interactive audio reproduction which is a necessary component in immersive audio applications. The proposed model uses a single reference audio signal in order to derive a noise signal per spot microphone. This noise signal can significantly enhance the sinusoidal representation of the corresponding spot signal. The reference {{can be one of}} the spot signals or a downmix, depending on the application. Thus, for a collection of multiple spot signals, only the reference is fully encoded (e. g. as an MP 3 <b>monophonic</b> <b>signal).</b> For the remaining spot signals, their sinusoidal parameters and corresponding noise spectral envelopes are retained and coded, resulting in bitrates for this side information in the order of 15 kbps for perceptual performance above the 4. 0 grade on the MOS (Mean Opinion Score) scale...|$|E
30|$|Extensive {{simulation}} results were reported {{with the aim}} of assessing the performance of the conventional and alternative LP models. Summarizing, we can state that a high-order all-pole model appears to be better suited to the audio LP problem than a conventional, low-order all-pole model. However, the HOLP model, which typically has half as many model parameters as the number of samples in the analysis window, is impractically complex in many applications. It could hence be expected that the PZLP model is a good alternative, since it can approximate the HOLP PEF impulse response with fewer parameters. This seems to be true only for <b>monophonic</b> audio <b>signals,</b> and even in this case, estimating the model parameters without prior knowledge on the fundamental frequency range is not a trivial task. Another good alternative to the HOLP model in the case of <b>monophonic</b> <b>signals</b> is the PLP model, especially when cascaded with a conventional LP model, as is common use in speech analysis. Finally, for polyphonic audio LP, the WLP model performance comes very close to the optimal HOLP model performance, however, the WLP model performs poorly in terms of perceptual frequency resolution, unless its model order is chosen to be an order of magnitude larger than the number of tonal components in the observed signal [12].|$|R
40|$|Sound source {{separation}} {{refers to}} the task of estimating the signals produced by individual sound sources from a complex acoustic mixture. It has several applications, since <b>monophonic</b> <b>signals</b> can be processed more efficiently and flexibly than polyphonic mixtures. This thesis deals with the separation of monaural, or, one-channel music recordings. We concentrate on separation methods, where the sources to be separated are not known beforehand. Instead, the separation is enabled by utilizing the common properties of real-world sound sources, which are their continuity, sparseness, and repetition in time and frequency, and their harmonic spectral structures. One of the separation approaches taken here use unsupervised learning and the other uses model-based inference based on sinusoidal modeling. Most of the existing unsupervised separation algorithms are based on a linear instantaneous signal model, where each frame of the input mixture signal i...|$|R
40|$|We {{present a}} novel {{approach}} to pitch estimation and note detection in polyphonic audio signals. We pose the problem in a Bayesian probabilistic framework, which allows us to incorporate prior knowledge {{about the nature of}} musical data into the model. We exploit the high correlation between model parameters in adjacent frames of data by explicitly modelling the frequency variation over time using latent variables. Parameters are estimated jointly across a number of adjacent frames to increase the robustness of the estimation against transient events. Individual frames of data are modelled as the sum of harmonic sinusoids. Parameter estimation is performed using Markov chain Monte Carlo (MCMC) methods. 1. INTRODUCTION Pitch estimation of polyphonic musical signals has received little attention compared to that of <b>monophonic</b> <b>signals.</b> Most <b>monophonic</b> techniques are not suited to polyphonic data, for instance cepstral and pitch-synchronous methods. There have been some very diverse approac [...] ...|$|R
40|$|Abstract—In this paper, a {{multichannel}} {{version of}} the sinusoids plus noise model (also known as deterministic plus stochastic de-composition) is proposed and applied to spot microphone signals of a music recording. These are the recordings captured by the var-ious microphones placed in a venue, before the mixing process pro-duces the final multichannel audio mix. Coding these microphone signals makes them available to the decoder, allowing for interac-tive audio reproduction which is a necessary component in immer-sive audio applications. The proposed model uses a single reference audio signal in order to derive a noise signal per spot microphone. This noise signal can significantly enhance the sinusoidal represen-tation of the corresponding spot signal. The reference {{can be one of}} the spot signals or a downmix, depending on the application. Thus, for a collection of multiple spot signals, only the reference is fully encoded (e. g., as an MP 3 <b>monophonic</b> <b>signal).</b> For the remaining spot signals, their sinusoidal parameters and corresponding noise spectral envelopes are retained and coded, resulting in bitrates for this side information in the order of 15 kb/s for perceptual perfor-mance above the 4. 0 grade on the mean opinion score (MOS) scale. Index Terms—Deterministic plus stochastic decomposition, immersive audio, multichannel audio, noise transplantation, sinusoidal model. I...|$|E
40|$|I {{certify that}} this thesis, and the {{research}} to which it refers, {{are the product of}} my own work, and that any ideas or quotations from the work of other people, published or otherwise, are fully acknowledged in accordance with the standard referencing practices of the discipline. I acknowledge the helpful guidance and support of my supervisor, Dr Simon Dixon. i Automatic music transcription is the process of converting an audio recording into a symbolic representation using musical notation. It has numerous ap-plications in music information retrieval, computational musicology, and the creation of interactive systems. Even for expert musicians, transcribing poly-phonic pieces of music is not a trivial task, and while the problem of automatic pitch estimation for <b>monophonic</b> <b>signals</b> is considered to be solved, the creation of an automated system able to transcribe polyphonic music without setting restrictions on the degree of polyphony and the instrument type still remain...|$|R
40|$|A {{simple but}} {{efficient}} voice activity detector {{based on the}} Hilbert transform and a dynamic threshold is presented to be used on the pre-processing of audio signals [...] The algorithm to define the dynamic threshold is a modification of a convex combination found in literature [...] This scheme allows the detection of prosodic and silence segments on a speech in presence of non-ideal conditions like a spectral overlapped noise [...] The present work shows preliminary results over a database built with some political speech [...] The tests were performed adding artificial noise to natural noises over the audio signals, and some algorithms are compared [...] Results will be extrapolated {{to the field of}} adaptive filtering on <b>monophonic</b> <b>signals</b> and the analysis of speech pathologies on futures works 20 th Argentinean Bioengineering Society Congress, SABI 2015 (XX Congreso Argentino de Bioingeniería y IX Jornadas de Ingeniería Clínica) 28 – 30 October 2015, San Nicolás de los Arroyos, Argentin...|$|R
40|$|In this paper, {{enhancements}} to {{the classical}} Waveform Similarity Overlap-Add (WSOLA) algorithm are proposed. As a time-domain approach, it works best for small speed changes and quasi-periodic, <b>monophonic</b> <b>signals.</b> Some of our enhancements are especially effective for small, others for large speed changes. As a consequence, significant improvements for all scaling factors are achieved extending the usability of the new scheme to larger speed changes and more complex signal characteristics. The reduction in computational complexity is analyzed by comparing the number of splice points needed to time-scale the input signal. As will be shown, these are the only points where real signal processing is performed. Therefore, a reduction in their number results in an equivalent decrease in computational demand. Additionally, {{they are also the}} only points where artifacts may arise so that, in many cases, a reduction in their number can serve as an indicator for improvements in the signal quality, too...|$|R
40|$|In this paper, {{we develop}} {{a method to}} obtain pitch {{frequency}} of <b>monophonic</b> musical <b>signals.</b> The auto-correlation function is used as the main feature to discriminate the notes. Acoustical signals are recorded from an electronic piano, digitized and stored on a computer. Feature extraction, i. e., the short-time autocorrelation computation, is performed and then notes are recognized by using a peak search method. I...|$|R
40|$|This thesis {{describes}} {{the design and}} implementation of audio effect for pitch shifting of <b>monophonic</b> singing <b>signals.</b> The effect can generate two pitch shifted voices from input signal in real-time, while preserving formants. Amount of the shift can be controlled via MIDI controller. The effect is implemented as VST module {{in the form of}} dynamic-link library. This work also includes theoretical introduction to related DSP techniques...|$|R
3000|$|Like for speech signals, we {{can also}} assume {{short-term}} stationarity for audio <b>signals.</b> <b>Monophonic</b> audio <b>signals</b> can typically be divided in musical notes of different durations. Each note can then be subdivided in four parts: the attack, decay, sustain, and release parts. The sustain part is usually the longest part of the note, and exhibits the highest degree of stationarity. The attack and decay parts are the shortest, and may show transient behavior, such that stationarity can only be assumed on very short time windows (a few milliseconds). Whereas LP of speech signals is typically performed on time windows of around 20 milliseconds, longer windows appear to be beneficial for LP of audio signals. In our examples, a time window of 46.4 milliseconds is used, corresponding to [...]...|$|R
30|$|We {{will only}} {{consider}} tonal audio signals, that is, signals having a continuous spectrum containing {{a finite number}} of dominant frequency components. In this way, the majority of audio signals is covered, except for the class of percussive sounds. The performance of the different LP models described below will be evaluated for three types of audio signals: synthetic audio signals consisting of a sum of harmonic sinusoids in white noise, true <b>monophonic</b> audio <b>signals,</b> and true polyphonic audio signals.|$|R
40|$|Abstract:- In this paper, {{we develop}} {{a method to}} obtain pitch {{frequency}} of <b>monophonic</b> musical <b>signals.</b> The auto-correlation function is used as the main feature to discriminate the notes. Acoustical signals are recorded from an electronic piano, digitized and stored on a computer. Feature extraction, i. e., the short-time autocorrelation computation, is performed and then notes are recognized by using the proposed peak search method. Examples are presented to illustrate the performance of our method. Key-Words:- Monophonic music transcription, Musical note recognition, Acoustic signal processing...|$|R
50|$|The {{introduction}} of FM stereo transmission, or color television, allowed forward compatibility, since monophonic FM radio receivers and black-and-white TV sets still could receive {{a signal from}} a new transmitter. It also allowed backward compatibility since new receivers could receive <b>monophonic</b> or black-and-white <b>signals</b> generated by old transmitters.|$|R
40|$|Abstract: In {{this paper}} we present {{analysis}} of the achievable watermark channel data rate in high data rate data hiding applications. As the perceptual entropy for wideband <b>monophonic</b> audio <b>signals</b> is {{in the range of}} 4 - 5 hps, for an uncompressed audo signal, a significant amount of additional information can be inserted into signal without causing a perceptual distortion. Test results showed that transform domain watermark embeddlng outperform significantly watermark embedding in time domain and that signal decompositions with a high GTC, lke the wavelet transform, are the most suitable for hgh data rate information hiding applications. 1...|$|R
40|$|This {{processor}} facilitates a {{range of}} timbral manipulations and resynthesis approaches for <b>monophonic</b> input <b>signals.</b> The input signal’s fundamental frequency is tracked, and the signal is passed {{through a series of}} 11 resonance filters, each tuned to an integer multiple of the input signal’s fundamental frequency over time. Alternatively, white noise can be harmonically filtered using the fundamental frequency data of an input signal. This creates a synthetic tone which mimics the input signal melodically, but whose timbral qualities are defined by the user input. This report includes a brief outline of the signal flow, and some discussion of the possible applications and usefulness of this processor. Architecture & Allied Art...|$|R
