775|2046|Public
25|$|Because of this, the {{negative}} binomial distribution {{is also known as}} the gammaâ€“Poisson (<b>mixture)</b> <b>distribution.</b>|$|E
500|$|As another example, {{consider}} a distribution which 6/10 {{of the time}} returns a standard normal distribution, and 4/10 of the time returns exactly the value 3.5 (i.e. a partly continuous, partly discrete <b>mixture</b> <b>distribution).</b> [...] The density function of this distribution can be written as ...|$|E
2500|$|Note {{also that}} all of the {{univariate}} distributions below are singly peaked; that is, it is assumed that the values cluster around a single point. [...] In practice, actually observed quantities may cluster around multiple values. [...] Such quantities can be modeled using a <b>mixture</b> <b>distribution.</b>|$|E
50|$|<b>Mixture</b> <b>distributions</b> {{arise in}} many {{contexts}} {{in the literature}} and arise naturally where a statistical population contains two or more subpopulations. They are also sometimes used {{as a means of}} representing non-normal distributions. Data analysis concerning statistical models involving <b>mixture</b> <b>distributions</b> is discussed under the title of mixture models, while the present article concentrates on simple probabilistic and statistical properties of <b>mixture</b> <b>distributions</b> and how these relate to properties of the underlying distributions.|$|R
30|$|The {{objective}} {{of this study is}} to propose three new <b>mixture</b> <b>distributions,</b> viz., Weibull-lognormal (WL), GEV-lognormal (GEVL), and Weibull-GEV (WGEV) for wind speed forecasting. Comparison of the proposed <b>mixture</b> <b>distributions</b> with existing distribution functions is done to demonstrate their suitability in describing wind speed characteristics.|$|R
5000|$|... #Article: Location {{testing for}} Gaussian scale <b>mixture</b> <b>distributions</b> ...|$|R
2500|$|When {{the image}} (or range) of [...] is finite or countably infinite, the {{variable}} and its distribution {{can be described}} by a probability mass function which assigns a probability to each value {{in the image of}} [...] If the image is uncountably infinite then [...] is called a continuous random variable. In the special case that it is absolutely continuous, its distribution can be described by a probability density function, which assigns probabilities to intervals; in particular, each individual point must necessarily have probability zero for an absolutely continuous random variable. Not all continuous random variables are absolutely continuous, for example a <b>mixture</b> <b>distribution.</b> Such random variables cannot be described by a probability density or a probability mass function.|$|E
5000|$|... #Caption: Univariate <b>mixture</b> <b>{{distribution}},</b> showing {{bimodal distribution}} ...|$|E
5000|$|... #Caption: Multivariate <b>mixture</b> <b>distribution,</b> showing four modes ...|$|E
5000|$|Rayleigh <b>mixture</b> <b>distributions</b> have {{probability}} density {{functions of the}} form ...|$|R
30|$|In this section, {{instead of}} using the Kernel method (given by (2)), a Gaussian Mixture (GM) model will be used. The mixture model is used in general for its {{mathematical}} flexibilities. For example, a mixture of two Gaussian distributions with different means and different variances results in a density with two modes, {{which is not a}} standard parametric <b>distribution</b> model. <b>Mixture</b> <b>distributions</b> can model extreme events better than the basic Gaussian ones. More details about <b>Mixture</b> <b>distributions</b> can be found in [21].|$|R
40|$|<b>Mixture</b> <b>distributions</b> {{have become}} a very {{flexible}} and common class of distributions, used in many different applications, but hardly any literure {{can be found on}} tests for assessing their goodnes of fit. We propose two types of smooth tests of goodness of fit for <b>mixture</b> <b>distributions.</b> The first test is a genuine smooth test, and the second test makes explicitly use of the mixture structure. In a simulation study the tests are compared to some traditional goodness of fit tests that, however, are not customised for <b>mixture</b> <b>distributions.</b> The first smooth test has overall good power and generally outperforms the other tests. The second smooth test is particularly suitable for assessing the fit of each component distribution separately. The tests are applicable to both continuous and discrete distributions and they are illustrated on three example data sets...|$|R
50|$|Because of this, the {{negative}} binomial distribution {{is also known as}} the gamma-Poisson (<b>mixture)</b> <b>distribution.</b>|$|E
50|$|This {{is not to}} be {{confused}} with the sum of normal distributions which forms a <b>Mixture</b> <b>distribution.</b>|$|E
5000|$|Any convex {{combination}} or <b>mixture</b> <b>distribution</b> of iid sequences of random variables is exchangeable. A converse proposition is de Finetti's theorem.|$|E
40|$|International audienceWe {{introduce}} <b>mixtures</b> {{of probability}} <b>distributions</b> to model empirical distributions of financial asset returns. In this framework, {{we examine the}} problem of maximizing performance measures. For this purpose, we consider a large class of reward/risk ratios such as the Kappa measures {{and in particular the}} Omega ratio. This latter measure is associated to a downside risk measure based on a put component. All these measures can take account of the asymmetry of the probability distribution, which is important when dealing with <b>mixture</b> of <b>distributions.</b> We examine first a fundamental example: the ranking and maximization of Gaussian <b>mixture</b> <b>distributions,</b> according to the Omega performance measure. Then we provide a general result for the maximization of <b>mixture</b> <b>distributions</b> with respect to a very large family of performance measures, including Kappa measures...|$|R
40|$|This paper {{presents}} Rayleigh <b>mixtures</b> of <b>distributions</b> {{in which}} the weight functions {{are assumed to be}} chi-square, and sampling distributions. The exact probability density functions of the mixture of two correlated Rayleigh random variables have been derived. Different moments, characteristic functions, shape characteristics, and the estimates of the parameters of the proposed <b>mixture</b> <b>distributions</b> using method of moments have also been provided...|$|R
40|$|This paper {{extends the}} {{stochastic}} dominance rules for normal <b>mixture</b> <b>distributions</b> derived by Levy and Kaplanski (2015). First, the portfolios under consideration {{are allowed to}} follow different regime-switching processes. Second, the results are extended from second- to fourth-order stochastic dominance, which {{is known to be}} closely related to kurtosis aversion in financial markets and allows to compare <b>mixture</b> <b>distributions</b> with the same overall variance. In particular, when a risk-free asset is available, checking for fourth-order stochastic dominance turns out to amount to a comparison of the regime-specific and overall Sharpe ratios of the portfolios under consideration...|$|R
50|$|These {{equations}} {{show the}} joint distribution or density characterised as a <b>mixture</b> <b>distribution</b> {{based on the}} underlying limiting empirical distribution (or a parameter indexing this distribution).|$|E
50|$|In {{probability}} and statistics, a <b>mixture</b> <b>distribution</b> is {{the probability}} distribution of a random variable that {{is derived from}} a collection of other random variables as follows: first, a random variable is selected by chance from the collection according to given probabilities of selection, and then {{the value of the}} selected random variable is realized. The underlying random variables may be random real numbers, or they may be random vectors (each having the same dimension), in which case the <b>mixture</b> <b>distribution</b> is a multivariate distribution.|$|E
50|$|A {{compound}} distribution may usually also be approximated to {{a sufficient}} degree by a <b>mixture</b> <b>distribution</b> using {{a finite number}} of mixture components, allowing to derive approximate density, distribution function etc.|$|E
40|$|The use of finite <b>mixture</b> <b>distributions</b> {{to control}} for unobserved {{heterogeneity}} has become increasingly popular among those estimating dynamic discrete choice models. One of the barriers to using mixture models is that parameters that could previously be estimated in stages must now be estimated jointly: using <b>mixture</b> <b>distributions</b> destroys any additive separability of the log likelihood function. The EM algorithm reintroduces additive separability, however, thus allowing the option of estimating parameters sequentially during each maximization step. We show that, relative to full information maximum likelihood, the EM algorithm with sequential maximization (ESM) can generate large computational savings with little loss of efficiency. ...|$|R
40|$|Some {{properties}} of the mean residual lifetime (MRL) functions are studied. The main focus is on relative characteristics. It is proved that under certain assumptions the relative hazard rate ordering leads to the corresponding ultimate MRL ordering. This result is applied to stochastic comparison of random variables, described by a baseline and <b>mixture</b> <b>distributions,</b> respectively. Mean residual life function Failure rate <b>Mixture</b> of <b>distributions</b> Relative mean residual life function Random environment...|$|R
40|$|A {{computational}} {{algorithm is}} presented for the Bayesian Cramer-Rao lower bound (BCRB) in filtering applications with measurement noise from <b>mixture</b> <b>distributions</b> with jump Markov switching structure. Such <b>mixture</b> <b>distributions</b> are common for radio propagation in mixed line- and non-line-of-sight environments. The newly derived BCRB is tighter than earlier more general bounds proposed in literature, and thus gives {{a more realistic}} bound on actual estimation performance. The resulting BCRB {{can be used to}} compute a lower bound on root mean square error of position estimates in a large class of radio localization applications. We illustrate this on an archetypical tracking application using a nearly constant velocity model and time of arrival observations...|$|R
5000|$|A mixture {{defining}} a new probability distribution from some existing ones, as in a <b>mixture</b> <b>distribution</b> or a compound distribution. Here {{a major problem}} often is to derive {{the properties of the}} resulting distribution.|$|E
5000|$|GMM - is a {{probabilistic}} model used for representing {{the existence of}} subpopulations within the overall population. Each sub-population is described using the <b>mixture</b> <b>distribution,</b> which allows for classification of observations into the sub-populations.|$|E
5000|$|With this <b>mixture</b> <b>distribution,</b> {{we apply}} the formula above {{and get the}} {{information}} dimension [...] of the distribution and calculate the -dimensional entropy.The normalized right part of the zero-mean Gaussian distribution has entropy , hence ...|$|E
40|$|In this paper, several {{portfolio}} selection {{problems with}} normal <b>mixture</b> <b>distributions</b> including fuzziness are proposed. Until now, many researchers have proposed portfolio models {{based on the}} stochastic approach, {{and there are some}} models considering both random and ambiguous conditions, particularly using fuzzy random or random fuzzy variables. However, the model including normal <b>mixture</b> <b>distributions</b> with fuzzy numbers has not been proposed yet. Our proposed problems are not well-defined problems due to randomness and fuzziness. Therefore, setting some criterions and introducing chance constrains, main problems are transformed into deterministic programming problems. Finally, we construct a solution method to obtain a global optimal solution of the problem...|$|R
40|$|In this article, <b>mixture</b> <b>distributions</b> and {{weighted}} likelihoods {{are derived}} within an information-theoretic framework and {{shown to be}} closely related. This surprising relationship obtains {{in spite of the}} arithmetic form of the former and the geometric form of the latter. <b>Mixture</b> <b>distributions</b> are shown to be optima that minimize the entropy loss under certain constraints. The same framework implies the weighted likelihood when the <b>distributions</b> in the <b>mixture</b> are unknown and information from independent samples generated by them have to be used instead. Thus the likelihood weights trade bias for precision and yield inferential procedures such as estimates that can be more reliable than their classical counterparts...|$|R
40|$|We {{consider}} phase-type scale <b>mixture</b> <b>distributions</b> which {{correspond to}} distributions {{of a product}} of two independent random variables: a phase-type random variable $Y$ and a nonnegative but otherwise arbitrary random variable $S$ called the scaling random variable. We investigate conditions for such a class of distributions to be either light- or heavy-tailed, we explore subexponentiality and determine their maximum domains of attraction. Particular focus is given to phase-type scale <b>mixture</b> <b>distributions</b> where the scaling random variable $S$ has discrete support [...] - such a class of distributions has been recently used in risk applications to approximate heavy-tailed distributions. Our results are complemented with several examples. Comment: 18 pages, 0 figur...|$|R
5000|$|Similarly, a convex {{combination}} [...] of probability distributions [...] is a weighted sum (where [...] satisfy the same constraints as above) of its component probability distributions, often called a finite <b>mixture</b> <b>distribution,</b> with probability density function: ...|$|E
5000|$|... that mixes {{different}} distributions , {{is called}} a <b>mixture</b> <b>distribution,</b> mixture or -parameterization or mixture for short. All such parameterizations are related through an affine transformation [...] A parameterization with such a transformation rule is called flat.|$|E
5000|$|This {{requires}} computation of {{the conditional}} probabilities [...] The multiple atlas orbit model randomizes over the denumerable set of atlases [...] The model on {{images in the}} orbit {{take the form of}} a multi-modal <b>mixture</b> <b>distribution</b> ...|$|E
50|$|Examples {{of common}} {{distributions}} {{that are not}} exponential families are Student's t, most <b>mixture</b> <b>distributions,</b> and even the family of uniform distributions when the bounds are not fixed. See the section below on examples for more discussion.|$|R
40|$|Predictive {{intervals}} {{of a future}} observation for a <b>mixture</b> of exponentials <b>distribution</b> with timecensored sampling are studied, assuming inverted gamma priors. Effects of the prior information and sample size on the predictive interval are discussed. Distributional properties of Monte Carlo sampling distributions of the predictive intervals are examined and Pearsonian curves fitted. Bayes estimators future observation inverted gamma <b>mixture</b> <b>distributions</b> prediction interval...|$|R
40|$|We {{consider}} Bayesian inference for <b>mixture</b> <b>distributions</b> {{of known}} number of components via {{a set of}} filtering recursions. We extend a method - proposed in an earlier article - of direct simulation for discrete <b>mixture</b> <b>distributions</b> in order to analyze continuous mixture models. Furthermore, we introduce resampling steps {{similar to those in}} particle filters within the steps of the filtering recursions, which make calculations efficient and enable us to analyze larger datasets. The proposed algorithm for "resampled direct simulation" is a generalization of the particle filter which allows for merging identical/similar particles prior to resampling. We compare the proposed algorithm with this particle filter and with the Gibbs sampler using simulated data and real datasets...|$|R
