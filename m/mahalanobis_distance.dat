1459|303|Public
25|$|Chi distance. A {{precursor}} {{and special}} {{case of the}} <b>Mahalanobis</b> <b>distance.</b>|$|E
25|$|<b>Mahalanobis</b> <b>distance</b> and {{leverage}} {{are often}} used to detect outliers, especially {{in the development of}} linear regression models.|$|E
25|$|A serious {{difficulty}} arises if the variables are not {{measured in the}} same units. First consider measuring distance between a data point and the curve – what are the measurement units for this distance? If we consider measuring distance based on Pythagoras' Theorem then {{it is clear that}} we shall be adding quantities measured in different units, and so this leads to meaningless results. Secondly, if we rescale one of the variables e.g., measure in grams rather than kilograms, then we shall end up with different results (a different curve). To avoid this problem of incommensurability it is sometimes suggested that we convert to dimensionless variables—this may be called normalization or standardization. However there are various ways of doing this, and these lead to fitted models which are not equivalent to each other. One approach is to normalize by known (or estimated) measurement precision thereby minimizing the <b>Mahalanobis</b> <b>distance</b> from the points to the line, providing a maximum-likelihood solution; the unknown precisions could be found via analysis of variance.|$|E
3000|$|... (M 1) matrix {{contains}} {{the values of}} Extended <b>Mahalanobis</b> <b>distances</b> between the components of weak weights.|$|R
3000|$|... {{show the}} Extended <b>Mahalanobis</b> <b>distances</b> between {{a block of}} the current image and all blocks in a search window [...]...|$|R
40|$|Abstract:-Fuzzy C-Means (FCM) {{clustering}} {{algorithm is}} used {{in a variety of}} application domains. Fundamentally, it cannot be used for the subsequent data (adaptive data). A complete dataset has to be static prior to implementing the algorithm. This paper presents an alternative adaptive FCM which is able to cope with this limitation. The adaptive FCM using Euclidean and <b>Mahalanobis</b> <b>distances</b> were compared to alternative adaptive FCM for performance evaluation purposes. Two different datasets were taken into consideration for the compared test. In this respect, adaptive FCM using Euclidean and <b>Mahalanobis</b> <b>distances</b> results in more misclassified data. By implementing synthesis dataset with outlier, adaptive FCM using Euclidean and <b>Mahalanobis</b> <b>distances</b> give 9 % and 14 % of misclassification, respectively. While implemented in alternative adaptive FCM the proposed method exhibits the promising performance by giving 2 % of misclassification. This result shows similar manner for carrying out in iris dataset...|$|R
2500|$|<b>Mahalanobis</b> <b>distance</b> generalizing {{number of}} {{standard}} deviations to the mean ...|$|E
2500|$|The {{inequality}} can {{be written}} {{in terms of the}} <b>Mahalanobis</b> <b>distance</b> as ...|$|E
2500|$|The squared <b>Mahalanobis</b> <b>distance,</b> [...] {{which is}} {{generated}} by the convex function [...] This {{can be thought of}} as a generalization of the above squared Euclidean distance.|$|E
40|$|We use {{the forward}} search to provide robust <b>Mahalanobis</b> <b>distances</b> {{to detect the}} {{presence}} of outliers {{in a sample of}} multivariate normal data. Theoretical results on order statistics and on estimation in truncated samples provide the distribution of our test statistic. We also introduce several new robust distances with associated distributional results. Comparisons of our procedure with tests using other robust <b>Mahalanobis</b> <b>distances</b> show the good size and high power of our procedure. We also provide a unification of results on correction factors for estimation from truncated samples. Copyright (c) 2009 Royal Statistical Society. ...|$|R
40|$|Summary. We use {{the forward}} search to provide robust <b>Mahalanobis</b> <b>distances</b> {{to detect the}} {{presence}} of outliers {{in a sample of}} multivariate normal data. Theoretical results on order sta-tistics and on estimation in truncated samples provide the distribution of our test statistic. We also introduce several new robust distances with associated distributional results. Comparisons of our procedure with tests using other robust <b>Mahalanobis</b> <b>distances</b> show the good size and high power of our procedure. We also provide a unification of results on correction factors for estimation from truncated samples...|$|R
30|$|The treated {{wastewater}} {{samples taken}} at the outlet of BWWTP during a year were evaluated by multivariate statistical analysis. The <b>Mahalanobis</b> <b>distances</b> calculated from the original 11 variables were evaluated according to their logMD magnitudes.|$|R
2500|$|The other {{distributions}} {{are considered}} to represent a foreground distribution. Then, when the new frame incomes at times , a match test is made of each pixel. [...] A pixel matches a Gaussian distribution if the <b>Mahalanobis</b> <b>distance</b> ...|$|E
2500|$|... {{where we}} have used [...] to stand for the {{weighted}} norm [...] (compare with the <b>Mahalanobis</b> <b>distance).</b> In the Bayesian interpretation [...] is the inverse covariance matrix of , [...] is the expected value of , and [...] is the inverse covariance matrix of [...] The Tikhonov matrix is then given as a factorization of the matrix [...] (e.g. the Cholesky factorization), and is considered a whitening filter.|$|E
50|$|In {{order to}} use the <b>Mahalanobis</b> <b>distance</b> to {{classify}} a test point as belonging to one of N classes, one first estimates the covariance matrix of each class, usually based on samples known to belong to each class. Then, given a test sample, one computes the <b>Mahalanobis</b> <b>distance</b> to each class, and classifies the test point as belonging to that class for which the <b>Mahalanobis</b> <b>distance</b> is minimal.|$|E
40|$|Local feature {{matching}} is {{an essential}} component of many image retrieval algorithms. Euclidean and <b>Mahalanobis</b> <b>distances</b> are mostly used in order to compare two feature vectors. The first distance does not give satisfactory results in many cases and is inappropriate in the typical case where the components of the feature vector are incommensurable, whereas the second one requires training data. In this paper a stability based similarity measure (SBSM) is introduced for feature vectors that are composed of arbitrary algebraic combinations of image derivatives. Feature matching based on SBSM is shown to outperform algorithms based on Euclidean and <b>Mahalanobis</b> <b>distances,</b> and does not require any training...|$|R
40|$|Data on 8 anthropometric {{characters}} of 6663 adult males belonging to 22 caste groups, distributed in 38 districts of Central India were taken for present investigation, {{to study the}} morphometric variation. Cephalic index, nasal Index, Generalized <b>Mahalanobis</b> <b>distances</b> and its size and shape components were computed and dendrograms were drawn. Comparison of coefficient of variations shows that there exists variation in nasal breadth, nasal length, weight and hence in nasal index. But no marked variation was seen in respect of other anthropometric variables. Comparison of <b>Mahalanobis</b> <b>distances</b> leads to some close clusters among the caste groups, {{but many of the}} caste groups remained separated from other caste groups keeping their identities...|$|R
30|$|The {{distribution}} analysis allows {{to exclude}} {{the presence of an}} anomalous value. According to the <b>Mahalanobis</b> <b>distances</b> method, the results of which are shown in Fig.  1, it was highlighted that all the data was below the critical value.|$|R
50|$|<b>Mahalanobis</b> <b>distance</b> is {{preserved}} under full-rank linear transformations {{of the space}} spanned by the data. This means that if the data has a nontrivial nullspace, <b>Mahalanobis</b> <b>distance</b> can be computed after projecting the data (non-degenerately) down onto any space of the appropriate dimension for the data.|$|E
50|$|The {{coefficient}} {{can be used}} {{to determine}} the relative closeness of the two samples being considered. It is used to measure the separability of classes in classification and it is considered to be more reliable than the <b>Mahalanobis</b> <b>distance,</b> as the <b>Mahalanobis</b> <b>distance</b> is a particular case of the Bhattacharyya distance when the standard deviations of the two classes are the same. Consequently, when two classes have similar means but different standard deviations, the <b>Mahalanobis</b> <b>distance</b> would tend to zero, whereas the Bhattacharyya distance grows depending on the difference between the standard deviations.|$|E
50|$|<b>Mahalanobis</b> <b>distance</b> and {{leverage}} {{are often}} used to detect outliers, especially {{in the development of}} linear regression models. A point that has a greater <b>Mahalanobis</b> <b>distance</b> {{from the rest of the}} sample population of points is said to have higher leverage since it has a greater influence on the slope or coefficients of the regression equation. <b>Mahalanobis</b> <b>distance</b> is also used to determine multivariate outliers. Regression techniques can be used to determine if a specific case within a sample population is an outlier via the combination of two or more variable scores. Even for normal distributions, a point can be a multivariate outlier even if it is not a univariate outlier for any variable (consider a probability density concentrated along the line , for example), making <b>Mahalanobis</b> <b>distance</b> a more sensitive measure than checking dimensions individually.|$|E
40|$|Outlier {{detection}} {{is often}} a key task in a statistical analysis and helps guard against poor decision-making based on results that {{have been influenced by}} anomalous observations. For multivariate data sets, large <b>Mahalanobis</b> <b>distances</b> in raw data space or large <b>Mahalanobis</b> <b>distances</b> in principal components analysis, transformed data space, are routinely used to detect outliers. Detection in principal components analysis space can also utilise goodness of fit distances. For spatial applications, however, these global forms can only detect outliers in a non-spatial manner. This can result in false positive detections, such as when an observation’s spatial neighbours are similar, or false negative detections such as when its spatial neighbours are dissimilar. To avoid mis-classifications, we demonstrate that a local adaptation of various global methods can be used to detect multivariate spatial outliers. In particular, we account for local spatial effects via the use of geographically weighted data with either <b>Mahalanobis</b> <b>distances</b> or principal components analysis. Detection performance is assessed using simulated data as well as freshwater chemistry data collected over all of Great Britain. Results clearly show value in both geographically weighted methods to outlier detection...|$|R
40|$|The {{evaluation}} of the predictive ability of a model, is an essential moment of all the chemometrical techniques. So it must be performed very carefully. However, {{in the case of}} selection of relevant variables (an essential step in the case of data sets with many, frequently thousands, variables) the selection is generally performed using all the available objects. In some recent classification and class modeling techniques, from the original or from the selected variables the <b>Mahalanobis</b> <b>distances</b> of the leverages from the centroids of the categories in the problem are computed, and then added to the original variables. Also here the <b>Mahalanobis</b> <b>distances</b> are computed with all the objects. The consequence is an overestimate of the prediction ability, very large when the ratio between the number of the objects and that of the variables is rather low, so that the variance-covariance matrix is unstable. In this paper the correct validation procedures are described for the cases of selection of variables and of the addition of <b>Mahalanobis</b> <b>distances</b> computed on the original variables or the selected variables. The estimates of the prediction ability are compared with those obtained with insufficient validation strategies...|$|R
30|$|The {{univariate}} normality {{assumption was}} checked with the one-sample Kolmogorov-Smirnov Test (SPSS) and multivariate normality was assessed with the Shapiro-Wilk goodness-of-fit test (using JMP Pro 11) {{on the distribution}} of the <b>Mahalanobis</b> <b>distances.</b> There were significant departures from normality on both tests.|$|R
5000|$|<b>Mahalanobis</b> <b>distance</b> - Prasanta Chandra Mahalanobis (প্রশান্ত চন্দ্র মহলানবিস) ...|$|E
5000|$|<b>Mahalanobis</b> <b>distance</b> generalizing {{number of}} {{standard}} deviations to the mean ...|$|E
5000|$|The {{descriptive}} statistic [...] {{is known}} as the <b>Mahalanobis</b> <b>distance,</b> which represents the distance of the test point [...] from the mean [...] Note that in case when , the distribution reduces to a univariate normal distribution and the <b>Mahalanobis</b> <b>distance</b> reduces to the absolute value of the standard score. See also Interval below.|$|E
40|$|In {{the multivariate}} one-sample {{location}} model, we propose {{a class of}} flexible robust, affine-equivariant L-estimators of location, for distributions invoking affine-invariance of <b>Mahalanobis</b> <b>distances</b> of individual observations. An involved iteration process for their computation is numerically illustrated. Comment: 16 pages, 4 figures, 6 table...|$|R
40|$|This {{research}} presents {{methods for}} detecting and isolating faults in multiple Micro-Electro-Mechanical System (MEMS) Inertial Measurement Unit (IMU) configurations. Traditionally, in the inertial technology, the task Fault Detection and Isolation (FDI) is realized by the parity space method. However, this approach performs poorly with low-cost MEMS-IMUs, although, it provides satisfactory results {{when applied to}} tactical or navigation grade IMUs. In this article, we propose a more complex approach to detect outliers {{that takes into account}} the shape and size of multivariate data. The proposed method is based on <b>Mahalanobis</b> <b>distances.</b> Such approach has already been successfully applied in other fields of applied multivariate statistics, however, it has never been tested with inertial sensors. As <b>Mahalanobis</b> <b>distances</b> (as well as the parity space method) is very sensitive to the presence of the same outliers this method aims to detect, we propose using its robust version. The performances of the proposed algorithm are evaluated using dynamical experiments with several MEMS-IMUs and a reference signal provided by a tactical-grade IMU run in parallel. The conducted experiment shows that, for example, the percentage of false alarms is approximately ten times lower when using a method based on <b>Mahalanobis</b> <b>distances</b> as compared to that based on the parity space approach...|$|R
40|$|Two-year-old cones were {{collected}} from 388 dwarf mountain pine (Pinus mugo Turra) plants of ten populations of this species from the Tatra Mts. - five of them from calcareous and five from calcium-free undersoil. Their 14 morphological traits are described. The data served for performing {{multivariate analysis of variance}} and testing of statistical hypotheses, for discriminate analysis. for calculating <b>Mahalanobis</b> <b>distances</b> between populations and plotting a dendrite based on the shortest <b>Mahalanobis</b> <b>distances</b> and for agglomerative clustering by the method of nearest neighbourhood. Wide differences in the populations were found as regards all the studied traits and the existence of two groups in the population, which, do not, however, correlate with the substrate type. The "calcareous and calcium-free" populations show statistically significant differences in six of the 14 studied traits...|$|R
50|$|Chi distance. A {{precursor}} {{and special}} {{case of the}} <b>Mahalanobis</b> <b>distance.</b>|$|E
5000|$|Bregman {{divergence}} (the <b>Mahalanobis</b> <b>distance</b> is {{an example}} of a Bregman divergence) ...|$|E
5000|$|The {{inequality}} can {{be written}} {{in terms of the}} <b>Mahalanobis</b> <b>distance</b> as ...|$|E
40|$|Our {{objective}} {{in this paper}} is the detection of atypical growth profiles, which is illustrated in a growth study from 74 families of conifers. Our approach starts by fitting a 2 -level linear model where we assign the measurements made on time in each family to the first level of the model, and assign the families to the second level. In order to identify atypical growth profiles we analyze the (multivariate) residuals in the second level of the fitted model. <b>Mahalanobis</b> <b>distances</b> to the origin indicate potential atypical growth profiles, however, Hadi´s more sophisticated procedure concludes {{that there are no}} outlying residuals, thus avoiding the wrong conclusion that observations with high <b>Mahalanobis</b> <b>distances</b> to the origin are necessarily outliers. Key words: outliers in multivariate data, second level residuals in 2 -level models...|$|R
40|$|This work {{focuses on}} {{detecting}} outliers within large and very large datasets using a computationally efficient procedure. The algorithm uses Tukey’s biweight function applied on the dataset {{to filter out}} the effects of extreme values for obtaining appropriate location and scale estimates. Robust <b>Mahalanobis</b> <b>distances</b> for all data points are calculated using these location and scale estimates. A suitable rejection point for the outliers is determined by a separation boundary obtained using non-parametric density estimation by Parzen window where the probability density curve of the robust <b>Mahalanobis</b> <b>distances</b> descends and then again ascends for outlying distances. This procedure demonstrates good success at identifying outliers even in cases where data is highly skewed and overlapping, compared to established statistical outlier detection methods for both univariate and multivariate data where the underlying distribution needs to be known...|$|R
40|$|Figure 3 - Dendrogram of {{hierarchical}} {{cluster analysis}} based on 17 morphological characters (squared <b>Mahalanobis</b> <b>distances)</b> using unweighted pair-group average linkage among 29 samples of Hyalopterus. Sample numbers {{the same as}} in Table 1. ar samples from Prunus armeniaca, d Prunus domestica, du Prunus dulcis, p Prunus persica, ph Phragmites communis...|$|R
