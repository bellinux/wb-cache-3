104|9304|Public
25|$|A {{probability}} {{is a way}} of assigning {{every event}} a value between zero and one, with the requirement that the event made up of all possible results (in our example, the event {1,2,3,4,5,6}) is assigned a value of one. To qualify as a probability, the assignment of values must satisfy the requirement {{that if you look at}} a collection of <b>mutually</b> <b>exclusive</b> <b>events</b> (events with no common results, e.g., the events {1,6}, {3}, and {2,4} are all mutually exclusive), the probability that at least one of the events will occur is given by the sum of the probabilities of all the individual events.|$|E
2500|$|The {{probability}} measure [...] {{is a function}} returning an event's probability. A probability is a real number between zero (impossible events have probability zero, though probability-zero events are not necessarily impossible) and one (the event happens almost surely, with almost total certainty). Thus [...] is a function [...] The {{probability measure}} function must satisfy two simple requirements: First, {{the probability of a}} countable union of <b>mutually</b> <b>exclusive</b> <b>events</b> must be equal to the countable sum of the probabilities of each of these events. For example, the probability of the union of the <b>mutually</b> <b>exclusive</b> <b>events</b> [...] and [...] in the random experiment of one coin toss, , is the sum of probability for [...] and the probability for , [...] Second, the probability of the sample space [...] must be equal to 1 (which accounts for the fact that, given an execution of the model, some outcome must occur). In the previous example the probability of the set of outcomes [...] must be equal to one, because it is entirely certain that the outcome will be either [...] or [...] (the model neglects any other possibility) in a single coin toss.|$|E
2500|$|Probability {{is a way}} of {{assigning}} every [...] "event" [...] a value between {{zero and}} one, with the requirement that the event made up of all possible results (in our example, the event {1,2,3,4,5,6}) be assigned a value of one. To qualify as a probability distribution, the assignment of values must satisfy the requirement {{that if you look at}} a collection of <b>mutually</b> <b>exclusive</b> <b>events</b> (events that contain no common results, e.g., the events {1,6}, {3}, and {2,4} are all mutually exclusive), the probability that any of these events occurs is given by the sum of the probabilities of the events.|$|E
50|$|Discovery of a {{specific}} flaw or vulnerability is not a <b>mutually</b> <b>exclusive</b> <b>event,</b> multiple researchers with differing motivations can and do discover the same flaws independently.|$|R
25|$|The {{probability}} {{that any one}} of the events {1,6}, {3}, or {2,4} will occur is 5/6. This is the same as saying that the probability of event {1,2,3,4,6} is 5/6. This event encompasses the possibility of any number except five being rolled. The <b>mutually</b> <b>exclusive</b> <b>event</b> {5} has a probability of 1/6, and the event {1,2,3,4,5,6} has a probability of 1, that is, absolute certainty.|$|R
50|$|This {{argument}} presupposes that vulnerability {{discovery is}} a <b>mutually</b> <b>exclusive</b> <b>event,</b> {{that only one}} person can discover a vulnerability. There are many examples of vulnerabilities being discovered simultaneously, often being exploited in secrecy before discovery by other researchers. While there may exist users who cannot benefit from vulnerability information, full disclosure advocates believe this demonstrates a contempt for the intelligence of end users. While it's true that some users cannot benefit from vulnerability information, if they're concerned with the security of their networks {{they are in a}} position to hire an expert to assist them as you would hire a mechanic to help with a car.|$|R
5000|$|Any {{countable}} {{sequence of}} disjoint sets (synonymous with <b>mutually</b> <b>exclusive</b> <b>events)</b> [...] satisfies ...|$|E
50|$|The {{concepts}} of {{mutually independent events}} and <b>mutually</b> <b>exclusive</b> <b>events</b> are separate and distinct.|$|E
5000|$|In {{probability}} theory, events E1, E2, ..., En {{are said}} to be mutually exclusive if the occurrence of any one of them implies the non-occurrence of the remaining n − 1 events. Therefore, two <b>mutually</b> <b>exclusive</b> <b>events</b> cannot both occur. Formally said, the intersection of each two of them is empty (the null event): A ∩ B = ∅. In consequence, <b>mutually</b> <b>exclusive</b> <b>events</b> have the property: P(A ∩ B) = 0.|$|E
50|$|It {{should be}} noted that the A- and B-series are not <b>mutually</b> <b>exclusive.</b> If <b>events</b> form an A-series they {{automatically}} also form a B-series (anything in the present is earlier than anything in the future, and later than everything past). The question is not therefore whether time forms an A- or a B-series; the question is whether time forms both an A- and a B-series, or only a B-series.|$|R
40|$|The {{analysis}} of events containing multiple high Pt leptons (electrons and muons) produced in ep collisions has been {{performed with the}} H 1 and ZEUS detectors at HERA, using the full data sets collected by the experiments in the period 1994 - 2007. <b>Mutually</b> <b>exclusive</b> <b>event</b> topologies containing at least two charged leptons are analysed. The H 1 and ZEUS data, corresponding to a total integrated luminosity of about 1 fb^- 1, are combined in a common phase space. The observed event yields are compared to the predictions from the Standard Model. In general a good agreement is found, where the expectation is dominated by photon-photon collisions. Interesting events at high mass and high Pt are observed by both experiments. The total and differential cross sections for multi-lepton production at HERA are also measured. Comment: 4 pages, 5 figures, 2 PoS files. Conference proceedings of talk given at the 35 th International Conference of High Energy Physics (ICHEP 2010), Paris, July 22 - 28, 201...|$|R
40|$|Abstract—We {{show that}} {{likelihood}} judgments are biased toward an ignorance-prior probability that assigns equal credence to each <b>mutually</b> <b>exclusive</b> <b>event</b> {{considered by the}} judge. The value of the ignorance prior depends crucially on how the set of possibilities (i. e., the state space) is subjectively partitioned by the judge. For instance, asking “what is the probability that Sunday will be hotter than any other day next week? ” facilitates a two-fold case partition, {Sunday hotter, Sunday not hotter}, thus priming an ignorance prior of 1 / 2. In contrast, asking “what is {{the probability that the}} hottest day of the week will be Sunday? ” facilitates a seven-fold class partition, {Sunday hottest, Monday hottest, etc. }, priming an ignorance prior of 1 / 7. In four studies, we observed systematic partition dependence: Judgments made by participants presented with either case or class formulations of the same query were biased toward the corresponding ignorance prior. Over the past 30 years, an abundance of psychological research ha...|$|R
5000|$|X1 = X2 = 0 and X1 = X2 = 1 are <b>mutually</b> <b>exclusive</b> <b>events,</b> {{so we can}} say ...|$|E
5000|$|Photons {{travelling}} {{through a}} semi-transparent mirror. The <b>mutually</b> <b>exclusive</b> <b>events</b> (reflection/transmission) are detected and associated to ‘0’ or ‘1’ bit values respectively.|$|E
50|$|In fact, <b>mutually</b> <b>exclusive</b> <b>events</b> {{cannot be}} {{statistically}} independent, since knowing that one occurs gives {{information about the}} other (specifically, that it certainly does not occur).|$|E
40|$|Summary: <b>Mutually</b> <b>exclusive</b> {{splicing}} is {{a strictly}} regulated pattern of alternative splicing. A specific group of <b>mutually</b> <b>exclusive</b> splicing <b>events</b> {{has been shown}} to be regulated by the formation of specific RNA secondary structures. This type of regulation {{has been shown to}} exist only in arthropods. The present study involved a detailed sequence analysis of human gene structures that undergo <b>mutually</b> <b>exclusive</b> splicing, which showed that this type of regulation may also occur in dynamin 1 (dnm 1) in mammals. A phylogenetic analy-sis revealed that the dnm 1 orthologs in invertebrates did not share the same sequence features, which suggests that the regulatory mechanism has independently evolved in the mammalian lineage. Therefore, the emergence of this elaborate mechanism for <b>mutually</b> <b>exclusive</b> splicing may be attributable to mechanistic convergence. Contact...|$|R
40|$|The {{capacity}} to imagine {{and prepare for}} alternative future possibilities is central to human cognition. Recent research suggests that between age 2 and 4 children gradually begin to demonstrate a {{capacity to}} prepare for two simple, <b>mutually</b> <b>exclusive</b> alternatives of an immediate future event. When children were {{given the opportunity to}} catch a target an experimenter dropped into an inverted Y-shaped tube, 2 -year olds—as well as great apes—tended to cover only one of the exits, whereas 4 -year-olds spontaneously and consistently prepared for both possible outcomes. Here we gave children, age 2 to 4 years, and chimpanzees a different opportunity to demonstrate potential competence. Given that social behaviour is particularly full of uncertainty, we developed a version of the task where the outcome was still unpredictable yet obviously controlled by an experimenter. Participants could ensure they would catch the target by simply covering two tube exits. While 4 -year-olds demonstrated competence, chimpanzees and the younger children instead tended to cover only one exit. These results substantiate the conclusion that the capacity for simultaneous preparation for <b>mutually</b> <b>exclusive</b> <b>event</b> outcomes develops relatively late in children and they are also in line with the possibility that our close animal relatives lack this capacity...|$|R
40|$|In {{survival}} analyses, {{competing risks}} are encountered where the subjects under study {{are at risk}} for more than one <b>mutually</b> <b>exclusive</b> failure <b>event</b> [1]. Competing risks are often analysed using either cause-specific or subdistribution (cumulative incidence) proportional hazards models. Cause-specific hazards model the rate of occurrence of an event, whereas subdistribution hazards model the risk of failure of a specific event. Results of competing risks analyses are being presented more frequently in the medical literature, but the difference in the interpretation of various estimates, compared to standard Cox hazard ratios, is rarely considered...|$|R
5000|$|Probability density {{function}} : A probability distribution [...] on a sample space [...] is a mapping from events of [...] to real numbers such that [...] for any event , and [...] for any two <b>mutually</b> <b>exclusive</b> <b>events</b> [...] and ...|$|E
5000|$|In {{probability}} theory, the craps {{principle is}} a theorem about event probabilities under repeated iid trials. Let [...] and [...] denote two <b>mutually</b> <b>exclusive</b> <b>events</b> which might occur {{on a given}} trial. Then the probability that [...] occurs before [...] equals the conditional probability that [...] occurs given that [...] or [...] occur on the next trial, which is ...|$|E
50|$|In the coin-tossing example, both {{outcomes}} are, in theory, jointly exhaustive, {{which means}} {{that at least one}} of the outcomes must happen, so these two possibilities together exhaust all the possibilities. However, not all <b>mutually</b> <b>exclusive</b> <b>events</b> are collectively exhaustive. For example, the outcomes 1 and 4 of a single roll of a six-sided die are mutually exclusive (both cannot happen at the same time) but not collectively exhaustive (there are other possible outcomes; 2,3,5,6).|$|E
50|$|An {{a priori}} {{probability}} is a probability that is derived purely by deductive reasoning. One way of deriving a priori probabilities is the principle of indifference, which has the character of saying that, if there are N <b>mutually</b> <b>exclusive</b> and exhaustive <b>events</b> {{and if they are}} equally likely, then the probability of a given event occurring is 1/N. Similarly the probability of one of a given collection of K events is K/N.|$|R
40|$|PMCID: PMC 3614517 Background: Competing {{risks are}} a common {{occurrence}} in survival analysis. They arise when a patient is at risk {{of more than one}} <b>mutually</b> <b>exclusive</b> <b>event,</b> such as death from different causes, and the occurrence of one of these may prevent any other event from ever happening. Methods: There are two main approaches to modelling competing risks: the first is to model the cause-specific hazards and transform these to the cumulative incidence function; the second is to model directly on a transformation of the cumulative incidence function. We focus on the first approach in this paper. This paper advocates the use of the flexible parametric survival model in this competing risk framework. Results: An illustrative example on the survival of breast cancer patients has shown that the flexible parametric proportional hazards model has almost perfect agreement with the Cox proportional hazards model. However, the large epidemiological data set used here shows clear evidence of non-proportional hazards. The flexible parametric model is able to adequately account for these through the incorporation of time-dependent effects. Conclusion: A key advantage of using this approach is that smooth estimates of both the cause-specific hazard rates and the cumulative incidence functions can be obtained. It is also relatively easy to incorporate time-dependent effects which are commonly seen in epidemiological studies. Peer-reviewedPublisher Versio...|$|R
40|$|Equations {{are derived}} for the {{estimation}} of the parameters in a nonlinear model for the probability {{of more than two}} <b>mutually</b> <b>exclusive</b> and exhaustive <b>events.</b> The estimated probabilities are between zero and one and sum to one. The equations for least squares and maximum likelihood estimation are given, and it is pointed out that the maximum likelihood estimate has the form of weighted least squares with estimated weights giving more weight to low probability events. In a recent paper by Brelsford and Jones [I], the fitting of a logistic curve to zero-one data using least squares and maximum likelihood was presented using a technique developed by Walker and Duncan [2]. The paper stated that the dichotomous case generalizes to a model suggested by Cox [3] for estimating probabilities fort more than two events. Several requests have been received for details of this generalization, and this note provides the details together with more explicit derivations. Suppose there are k <b>mutually</b> <b>exclusive</b> and exhaustive <b>events.</b> For the observation at time t let z,(t) = 1 if event i occurs and zero otherwise. This gives k c zt(t) =l. i = 1 The model to be fitted to the data is The regression coefficients occur nonlinearly so some form of iterative solution is necessary. The equations can be linearized by expanding Pi(t) in a series about an initial guess at the regression coefficients, p;...|$|R
5000|$|The {{probability}} measure [...] {{is a function}} returning an event's probability. A probability is a real number between zero (impossible events have probability zero, though probability-zero events are not necessarily impossible) and one (the event happens almost surely, with almost total certainty). Thus [...] is a function [...] The {{probability measure}} function must satisfy two simple requirements: First, {{the probability of a}} countable union of <b>mutually</b> <b>exclusive</b> <b>events</b> must be equal to the countable sum of the probabilities of each of these events. For example, the probability of the union of the <b>mutually</b> <b>exclusive</b> <b>events</b> [...] and [...] in the random experiment of one coin toss, , is the sum of probability for [...] and the probability for , [...] Second, the probability of the sample space [...] must be equal to 1 (which accounts for the fact that, given an execution of the model, some outcome must occur). In the previous example the probability of the set of outcomes [...] must be equal to one, because it is entirely certain that the outcome will be either [...] or [...] (the model neglects any other possibility) in a single coin toss.|$|E
50|$|But if one {{sets the}} price of the third ticket too low, a prudent {{opponent}} will buy that ticket and sell the other two tickets to the price-setter. By considering the three possible outcomes (Red Sox, Yankees, some other team), one will note that regardless of which of the three outcomes eventuates, one will lose. An analogous fate awaits if one set {{the price of}} the third ticket too high relative to the other two prices. This parallels the fact that probabilities of <b>mutually</b> <b>exclusive</b> <b>events</b> are additive (see probability axioms).|$|E
50|$|A {{probability}} {{is a way}} of assigning {{every event}} a value between zero and one, with the requirement that the event made up of all possible results (in our example, the event {1,2,3,4,5,6}) is assigned a value of one. To qualify as a probability, the assignment of values must satisfy the requirement {{that if you look at}} a collection of <b>mutually</b> <b>exclusive</b> <b>events</b> (events with no common results, e.g., the events {1,6}, {3}, and {2,4} are all mutually exclusive), the probability that at least one of the events will occur is given by the sum of the probabilities of all the individual events.|$|E
40|$|If a new high-mass {{resonance}} {{is discovered}} at the Large Hadron Collider, model-independent techniques {{to identify the}} production mechanism will be crucial to understand its nature and effective couplings to Standard Model particles. We present a powerful and model-independent method to infer the initial state {{in the production of}} any high-mass color-singlet system by using a tight veto on accompanying hadronic jets to divide the data into two <b>mutually</b> <b>exclusive</b> <b>event</b> samples (jet bins). For a resonance of several hundred GeV, the jet binning cut needed to discriminate quark and gluon initial states is in the experimentally accessible range of several tens of GeV. It also yields comparable cross sections for both bins, making this method viable already with the small event samples available shortly after a discovery. Theoretically, the method is made feasible by utilizing an effective field theory setup to compute the jet cut dependence precisely and model independently and to systematically control all sources of theoretical uncertainties in the jet binning, as well as their correlations. We use a 750  GeV scalar resonance as an example to demonstrate the viability of our method. United States. Dept. of Energy. Office of Nuclear Physics (Grant DE-SC 0011090) Deutsche Forschungsgemeinschaft (Collaborative Research Center (SFB) 676 Particles, Strings and the Early Universe) Simons Foundation (Investigator Grant 327942) Netherlands Organization for Scientific Research (VEN) I Grant) MIT International Science and Technology Initiatives (Collaboration Grant) Netherlands Organization for Scientific Research (VENI Grant) Deutsche Forschungsgemeinschaft (Emmy-Noether Grant TA 867 / 1 - 1...|$|R
40|$|There is {{an urgent}} need to elicit and {{validate}} highly efficacious targets for combinatorial intervention from large scale ongoing molecular characterization efforts of tumors. We established an in silico bioinformatic platform in concert with a high throughput screening platform evaluating 37 novel targeted agents in 669 extensively characterized cancer cell lines reflecting the genomic and tissue-type diversity of human cancers, to systematically identify combinatorial biomarkers of response and co-actionable targets in cancer. Genomic biomarkers discovered in a 141 cell line training set were validated in an independent 359 cell line test set. We identified co-occurring and <b>mutually</b> <b>exclusive</b> genomic <b>events</b> that represent potential drivers and combinatorial targets in cancer. We demonstrate multiple cooperating genomic events that predict sensitivity to drug intervention independent of tumor lineage. The coupling of scalable in silico and biologic high throughput cancer cell line platforms for the identification of co-events in cancer delivers rational combinatorial targets fo...|$|R
40|$|We present non {{equilibrium}} {{molecular dynamics}} experiments of the unfolding and refolding of an alanine decapeptide in vacuo {{subject to a}} Nose-Hoover thermostat. Forward (unfolding) and reverse (refolding) work distribution are numerically calculated for various duration times of the non equilibrium experiments. Crooks theorem is accurately verified for all non equilibrium regimes and the time asymmetry of the process is measured using the recently proposed Jensen-Shannon divergence [E. H. Fend, G. Crooks, Phys. Rev. Lett, 101, 090602]. Results on the alanine decapeptide are found similar to recent experimental data on m-RNA molecule, thus evidencing the universal character of the Jensen-Shannon divergence. The patent non-Markovianity of the process is rationalized by assuming that the observed forward and reverse distributions can be each described {{by a combination of}} two normal distributions satisfying the Crooks theorem, representative of two <b>mutually</b> <b>exclusive</b> linear <b>events.</b> Such bimodal approach reproduce with surprising accuracy the observed non Markovian work distributions...|$|R
50|$|Thus, {{the world}} of {{strength}} athletics became fragmented, {{with a number of}} individuals being able to lay claim to be the strongest in the world by virtue of having won <b>mutually</b> <b>exclusive</b> <b>events.</b> Athletes affiliated to IFSA Strongman were not allowed to compete in the World's Strongest Man ("WSM"), which is produced by TWI and thus neither WSM and its associated Strongman Super Series nor the IFSA circuit could claim to have a comprehensive field of the top athletes. Some events did exist that bridged the divide between the major organizations, such as the Arnold Strongman Classic and Fortissimus.|$|E
5000|$|Probability {{is a way}} of {{assigning}} every [...] "event" [...] a value between {{zero and}} one, with the requirement that the event made up of all possible results (in our example, the event {1,2,3,4,5,6}) be assigned a value of one. To qualify as a probability distribution, the assignment of values must satisfy the requirement {{that if you look at}} a collection of <b>mutually</b> <b>exclusive</b> <b>events</b> (events that contain no common results, e.g., the events {1,6}, {3}, and {2,4} are all mutually exclusive), the probability that any of these events occurs is given by the sum of the probabilities of the events.|$|E
5000|$|Equiprobability is a {{philosophical}} concept in probability theory that {{allows one to}} assign equal probabilities to outcomes when they are judged to be equipossible or to be [...] "equally likely" [...] in some sense. The best-known formulation of the rule is Laplace's principle of indifference (or principle of insufficient reason), which states that, when [...] "we have no other information than" [...] that exactly N <b>mutually</b> <b>exclusive</b> <b>events</b> can occur, we are justified in assigning each the probability 1/N. This subjective assignment of probabilities is especially justified for situations such as rolling dice and lotteries since these experiments carry a symmetry structure, and one's state of knowledge must clearly be invariant under this symmetry.|$|E
40|$|This paper {{presents}} {{a method for}} analysing fault trees that contain independent sets of <b>mutually</b> <b>exclusive</b> (disjoint) <b>events</b> of different cardinality. Disjoint events {{can be used to}} model several issues, e. g. multi-state systems with multistate components, different attack alternatives in se-curity related studies, and components in phased mission systems. Basic events of coherent and non coherent binary trees can be considered as belonging to sets of cardinality 2. Each event is associated with a binary variable, and a labelling technique is used to distinguish the variables belonging to different sets. The proposed analysis method is based on the approach of Binary Decision Diagrams (BDD). The application of the rules for the construction of the BDD is driven by the labels associated with disjoint variables. The BDD is then transformed into a Ternary Decision Diagram (TDD). The TDD re{{presents a}} very straightforward data structure for performing the probabilistic quantification. The use of the proposed method is clarified with simple examples. JRC. G. 7 -Traceability and vulnerability assessmen...|$|R
40|$|The {{probability}} {{of an event}} occurring or the proportion of patients experiencing an event, such as death or disease, is often of interest in medical research. It is a measure that is intuitively appealing to many consumers of statistics and yet the estimation is not always clearly understood or straightforward. Many researchers will take the complement of the survival function, obtained using the Kaplan-Meier estimator. However, in situations where patients are also at risk of competing events, the interpretation of such estimates may not be meaningful. Competing risks are present in almost all areas of medical research. They occur when patients {{are at risk of}} more than one <b>mutually</b> <b>exclusive</b> <b>event,</b> such as death from different causes. Although methods for the analysis of survival data in the presence of competing risks have been around since the 1760 s there is increasing evidence that these methods are being underused. The primary aim of this thesis is to develop and apply new and accessible methods for analysing competing risks in order to enable better communication of the estimates obtained from such analyses. These developments will primarily involve the use of the recently established exible parametric survival model. Several applications of the methods will be considered in various areas of medical research to demonstrate the necessity of competing risks theory. As there is still a great amount of misunderstanding amongst clinical researchers about when these methods should be applied, considerations are made as to how to best present results. Finally, key concepts and assumptions of the methods will be assessed through sensitivity analyses and implications of data quality will be investigated {{through the use of a}} simulation study. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|The {{generalized}} Bayes’ rule (GBR) {{can be used}} {{to conduct}} ‘quasi-Bayesian’ analyses when prior beliefs are represented by imprecise probability models. We describe a procedure for deriving coherent imprecise probability models when the event space consists of a finite set of <b>mutually</b> <b>exclusive</b> and exhaustive <b>events.</b> The procedure is based on Walley’s theory of upper and lower prevision and employs simple linear programming models. We then describe how these models can be updated using Cozman’s linear programming formulation of the GBR. Examples are provided to demonstrate how the GBR can be applied in practice. These examples also illustrate the effects of prior imprecision and prior-data conflict on the precision of the posterior probability distribution. Copyright Springer 2005 imprecise probability, generalized Bayes’ rule, second-order probability, quasi-Bayesian analysis,...|$|R
