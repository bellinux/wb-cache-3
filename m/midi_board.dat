2|11|Public
50|$|Around {{the time}} of the reunion with Rob Halford, Tipton only used a {{modified}} Crybaby 535Q Wah, Digitech Tone Driver, DigiTech Main Squeeze, and a Yamaha <b>midi</b> <b>board</b> controlling other effects and sounds in a rack unit.|$|E
5000|$|On the September 30, 1989 {{broadcast}} of Saturday Night Live, Sampedro led {{an ad hoc}} ensemble (including drummer Steve Jordan and bassist Charley Drayton) that backed Young for [...] "No More" [...] and “Rockin’ in the Free World,” regarded by critics {{as one of the}} greatest live rock television performances of all time. (The band, said one writer, looked like a “like a bunch of car thieves.”) Sampedro also accompanied Young on mandolin and piano on the subsequent solo tour, including an acclaimed concert at the Hammersmith Odeon on December 12, 1989. In 1990, Crazy Horse (including Sampedro once again) returned for Young’s 1990 album Ragged Glory and two live albums recorded on the following tour, Weld and Arc. Young then used the Horse for Sleeps with Angels (1994) and Broken Arrow (1996). Sampedro's proficiency in emergent computer technology (honed during Young's experiments with the medium in the early 1980s) allowed him to cultivate another career. He worked as an engineer on The Tonight Show with Jay Leno from 1992 to 2010 under bandleader Kevin Eubanks, running the ensemble's <b>MIDI</b> <b>board</b> as well as being an assistant/project manager to Eubanks.|$|E
5000|$|Axess Electronics FX1+ {{expansion}} <b>board,</b> <b>midi</b> foot controllers w/ axess CFX4 {{control function}} switchers and custom made Axess Electronics Deville footswitch interfaces ...|$|R
50|$|The {{protocol}} {{used on the}} SPI {{link between}} the MIOS processor and the network processor {{is based on the}} same format as USB (using 32 bits words containing a complete MIDI message) and has been proposed as an open standard for communication between network processor modules and <b>MIDI</b> application <b>boards.</b>|$|R
50|$|The {{control devices}} include the Midi Octopus and Dual Remote Loop. The Midi Octopus {{is used for}} {{controlling}} other effects. It could be used with a <b>Midi</b> pedal <b>board,</b> or be controlled by a Midi sequencer. The Dual Remote Loop is used to interface effects and amplifiers that can't controlled by other means.|$|R
50|$|The Juno-60 is {{controllable}} with sequencers using proprietary DCB protocol, {{similar to}} MIDI. Roland produced several DCB-enabled sequencers, or, alternatively, MIDI-to-DCB converters {{can be used}} to drive DCB-enabled synths. There are also at least two commercial third-party retrofits available to add <b>MIDI</b> on <b>board</b> the Juno 60. In the Juno-106, DCB support was dropped in favor of MIDI.|$|R
50|$|Many add-on {{cards were}} {{released}} for the system, including networking (Neptune-X), SCSI, memory upgrades, CPU enhancements (JUPITER-X 68040/060 accelerator), and <b>MIDI</b> I/O <b>boards.</b> The system has two joystick ports, both 9-pin male and supporting Atari standard joysticks and MSX controllers. Capcom produced a converter that was originally sold packaged with the X68000 version of Street Fighter II&prime; that allowed users {{to plug in}} a Super Famicom or Mega Drive controller into the system. The adapter was made specifically so that users could plug in the Capcom Power Stick Fighter controller into the system.|$|R
40|$|MIDI Gesamtkunstwerk and a Schema for Creative Design" formulates a {{model for}} {{creative}} thought and examines its impact when applied to the making of art. Use of the Schema as a system of design has propelled my work from sculpture to multimedia performance, specifically under computer control. :MIDI (Musical Instrument Digital Interface) is proposed {{as a solution to}} multimedia performance as Gesamtkunstwerk, with examples from my own experience and research. The Schema for Creative Design derives from and cross-pollinates three systems: I) William Glasser's theories of perception; 2) semiotic theory as formulated by Umberto Eco and others; and 3) Lowry Burgess' "Tools for Thought. " The model makes conscious use of subconscious processes to exploit them in stimulating expanded and unique creative thought. The use of MIDI to create a unique interaction between media is postulated as modem "Gesamtkunstwerk. " Gesamtkunstwerk is a term associated with composer Richard Wagner's music-dramas of the 19 th century. It characterized not only a performance mode, but also a philosophical orientation. This thesis considers a contemporary version of this philosophy as influenced by the changes in available media and methods of interaction. Because MIDI can interface physical objects, electronics, lighting, sound and special effects into a single score, composition and orchestration elements can be conveniently recorded, replayed, rehearsed and refmed. The data from a <b>MIDI</b> light <b>board,</b> <b>MIDI</b> mix <b>board,</b> and <b>MIDI</b> synthesizer are interchangeable. The artist has immediate access to sound, light, and video image. Specific intermedia control is a unique development which allows exact timing to program convincing intermedia gesture. Computer-control over the various media enables the artist to perfect performance similar to the way film is edited for maximum effect, and brings the ideal Gesamtkunstwerk - all the artforms working together toward a common expression - closer to realization. A MIDI Gesamtkunstwerk was attempted in the performance the "midicube. " The MIDI-ized media performers are members of the evolving "MIDI Robot Orchestra," hybrid objects developed from preexisting items, such as toys and tools, that usually produce sounds. The sounds are underscored by hybrid digital samples. The Macintosh computer records and plays back the code to form a precise musical ensemble. MIDI Gesamtkunstwerk considers practical aspects of scored multimedia performance, proposes a computer-controlled intermedia studio and discusses the advantages and limitations of a MIDI modified system. Concepts of deconstructed forms, reconstruction via permutation and the conflicted tension of the hybrid object also figure into the realization of MIDI Gesamtkunstwerk. by David Atherton. Thesis (M. S. V. S.) [...] Massachusetts Institute of Technology, Dept. of Architecture, 1989. Includes bibliographical references (leaves 105 - 107) ...|$|R
50|$|Pd's native objects {{range from}} the basic mathematical, logical, and bitwise {{operators}} found in every programming language to general and specialized audio-rate DSP functions (designated by a tilde (~) symbol), such as wavetable oscillators, the Fast Fourier transform (fft~), {{and a range of}} standard filters. Data can be loaded from file, read in from an audio <b>board,</b> <b>MIDI,</b> via Open Sound Control (OSC) through a Firewire, USB, or network connection, or generated on the fly, and stored in tables, which can then be read back and used as audio signals or control data.|$|R
40|$|We present Euclide, a {{multimodal}} {{system for}} live animation of a virtual puppet that {{is composed of}} a data glove, <b>MIDI</b> music <b>board,</b> keyboard, and mouse. The paper reports on a field study in which Euclide was used in a science museum to animate visitors as they passed by five different stations. Quantitative and qualitative analysis of several hours of videos served investigation of how the various features of the multimodal system were used by different puppeteers in the unfolding of the sessions. We found that the puppetry was truly multimodal, utilizing several input modalities simultaneously; the structure of sessions followed performative strategies; and the engagement of spectators was co-constructed. The puppeteer uses nonverbal resources (effects) and we examined how they are instrumental to talk as nonverbal turns, verbal accompaniment, and virtual gesturing. These findings allow describing digital puppetry as an emerging promising field of application for HCI {{that acts as a}} source of insights applicable in a range of multimodal performative interactive systems...|$|R
40|$|This thesis {{engages with}} experiential {{performance}} modes through the lenses of phenomenology and affect theory. Because experiential performance relies per definition on personal, subjective ‘experience’, specific responses cannot be anticipated. However, {{by attempting to}} compose ‘affect’, a performance {{has the potential to}} ‘move’ an attendant towards response. Deleuze and Guattari define ‘affect’ as “an ability to affect and be affected…. a prepersonal intensity corresponding to the passage from one experiential state of the body to another and implying an augmentation or diminution in that body’s capacity to act” (1987 : xvi). One current strategy for manifesting affect in performance seems to be the ways in which different configurations of body, sound and technology are employed. The body is the means through which sound is received or ‘experienced’ in the phenomenological sense, but it can also act as a source for sonic material. The body is furthermore the means by which sonic technology is manipulated. It is the complex, reverberating relationships between body, sound and technology, and their potential for eliciting affective transformation, which is the focus of my enquiry. In the first chapter I unpack the roles of the natural phenomena, body and sound, and their complex relationships to affect. The chapter serves as philosophical basis {{for the rest of the}} investigation, and draws largely on works by philosophers Susan Kozel, Maurice Merleau-Ponty, Brian Massumi, Gille Deleuze and Félix Guatarri and sound theorists Don Ihde, Marshall McLuhan, Brandon LaBelle and Frances Dyson. In the remaining three chapters I discuss current South African theatre works that employ the strategy of placing emphasis on sound, sonic technology, and its relationship to the human body. These works are my own piece herTz (2014), Jaco Bouwer’s pieces Samsa-masjien (2014) and Na-aap (2013), and First Physical Theatre Company’s Everyday Falling (2010). While they range from being plays to physical theatre performances to performative experiments, they all place specific emphasis on sonic devices, drawing attention to sound by revealing microphones, speakers, <b>midi</b> <b>boards,</b> etc. to the attendants, and including the generation and manipulation of sound in the action of the performance...|$|R
40|$|This thesis {{proposes a}} {{systematic}} methodology for the guidance, control, and navigation, of a quadrotor {{to perform a}} choreographed dance in real-time {{as a function of}} the music performed by a musician. The four main components of a human choreography (namely the notions of space, shape, time and structure) are analyzed and mathematically formulated for a robotic performance. This allows for a real-time interaction with a musician without prior knowledge of the music, and based on the pitch of the acoustic signal. A novel approach for mapping music features to trajectory parameters is proposed, as well as the design of a trajectory shaping filter based on two coefficients that are set in real-time by an artist through a <b>MIDI</b> foot-pedal <b>board.</b> The two coefficients are inspired by a mathematical description of acoustic signals. The proposed approach maps motion parameters and the music to trajectory motifs that are then switched in harmony with the music chord structure. The mathematical formulation of a quadrotor choreography is simulated. The simulation relies on the linearized dynamics and the physical properties of a quadrotor, and produces a graphical representation of the quadrotor choreography. To validate the control system, the position of the quadrotor is compared with the desired position. To measure the effectiveness of the link between music and the position of the quadrotor, the trajectory generator system is inverted to generate a sequence of music pitches. The melodic phrase generated by the position of the quadrotor is played back to the musician. A real-time musical interaction occurs between the musician and the quadrotor. Simulation results show that the proposed methodology yields an effective real-time performance for a quadrotor choreography...|$|R
40|$|With {{the high}} price of large mixing consoles, {{aspiring}} artists are restricted to using a mouse to control digital facsimiles of knobs, faders, switches, and buttons. Though using the software controls is considered a simple task, dedicated hardware allows for tactile, visual, and utility. At a low cost, the heart of the MTech M- 1 can be customized and placed into any shell with any combination of controls as possible with the underlying platform. Modern MIDI controllers require significant physical space due to their preset button layout and space consuming setups. Despite their high price, modern MIDI controllers have only one setup. With one setup, music producers or artists have a hard time carrying their MIDI controllers around for concerts or other performances. The M- 1 MIDI controller addresses the high cost that physical MIDI controllers currently hold on the market by allowing users to make/create their own specific <b>MIDI</b> control <b>board</b> using any combination of knobs, dials, buttons, and sliders. This customization of the controller allows the user to save space and money, while also accommodating for their style or a specific performance. The M- 1 controller is an affordable and fully customizable mechanical system. The fully customizable MIDI controller represents a digital MIDI controller while allowing the user to fully customize the physical layout of the controller. Users can place the buttons for the delay, reverb, compression, distortion, etc. in different areas of the device according to their needs. Being customizable allows the user to truly personalize the controller for their specific needs {{depending on the type of}} song being produced. Another feature of the M- 1 controller is its plug-and-play capability through USB 3. 0 / 2. 0. Owning a cheap MIDI controller with plug-and-play capability allows the user to connect multiple setups of the MIDI controller because of its low price...|$|R

