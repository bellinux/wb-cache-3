17|87|Public
40|$|W 3 C's Video in the Web {{activity}} aims at {{a better}} integration of media within the Web. In this paper, we show how two specifications currently under development within this activity, i. e., <b>Media</b> <b>Fragment</b> URIs and Media Annotations, can be combined within media-enabled HTML 5 Web applications. In particular, we introduce a number of extensions for the Media Annotations ontology in order {{to close the gap}} with <b>Media</b> <b>Fragment</b> URIs. Additionally, we show how rich <b>media</b> <b>fragment</b> annotations can be converted into a WebVTT file. The latter can be used by HTML 5 -enabled players to show the annotations in a synchronized way. Further, a fully integrated Web application has been developed that shows how both specifications can work together within an HTML 5 environment. This application relies on Nin-Suna, our media delivery platform targeted at multichannel publication that provides server-side support for both <b>Media</b> <b>Fragment</b> URIs and Media Annotations...|$|E
40|$|HTTP {{adaptive}} streaming {{was introduced}} {{with the general}} idea that user agents interpret a manifest file (describing different representations and segments of the media); where-after they retrieve the media content using sequential HTTP progressive download operations. MPEG started with the standardization of an HTTP streaming protocol, defining the structure and semantics of a manifest file and additional restrictions and extensions for container formats. At the same time, W 3 C {{is working on a}} specification for addressing media fragments on the Web using Uniform Resource Identifiers. The latter not only defines the URI syntax for <b>media</b> <b>fragment</b> identifiers but also the protocol for retrieving media fragments over HTTP. In this paper, we elaborate on the role of <b>Media</b> <b>Fragment</b> URIs within HTTP adaptive streaming scenarios. First, we elaborate on how different media representations can be addressed by means of <b>Media</b> <b>Fragment</b> URIs, by using track fragments. Additionally, we illustrate how HTTP adaptive streaming is realized relying on the Media Fragments URI retrieval protocol. To validate the presented ideas, we implemented Apple's HTTP Live streaming technique using <b>Media</b> <b>Fragment</b> URI...|$|E
40|$|Current {{multimedia}} {{applications in}} Web 2. 0 have generated a {{massive amount of}} multimedia resources, but most search results for multimedia resources still focus on the whole resource level. Media fragments expose the inside content of multimedia resources for annotations, but they are yet fully explored and indexed by major search engines. W 3 C has published <b>Media</b> <b>Fragment</b> 1. 0 as a standard way to describe media fragments on the Web. In this proposal, we make use of Google’s Ajax Application Crawler to index media fragments represented by <b>Media</b> <b>Fragment</b> URIs. Each <b>media</b> <b>fragment</b> with related annotations will have an individual snapshot page, which could be indexed by the crawler. Initial evaluation {{has shown that the}} snapshot pages are successfully fetched by Googlebot and we are expecting more media fragments to be indexed using this method, so that the search for multimedia resources would be more efficient...|$|E
40|$|Current {{multimedia}} {{applications in}} Web 2. 0 have generated large repositories for multimedia resources and annotations, {{so there is}} an urgent requirement to interlink annotations of these resources across different repositories to achieve better indexing and searching. To solve this problem, many researchers {{have been trying to}} apply semantic Web technologies to <b>media</b> <b>fragments</b> and annotations. Linked data has brought forward a promising way to expose, index and search <b>media</b> <b>fragments</b> and annotations which used to be isolated in different applications. This paper discusses in depth three key research problems when applying linked data principles in multimedia annotations: choosing URIs for <b>media</b> <b>fragments,</b> dereferencing <b>media</b> <b>fragments</b> and ontology alignment. An architecture is designed based on the possible solutions of the research problems. The key idea of the architecture is that it should act like an extra layer built on top of old applications when publishing linked data. A demo is built as an implementation of the architecture to show that <b>media</b> <b>fragments</b> can be published and linked to various datasets in the linked data cloud. In the future, some algorithms should be designed to make full use of the interlinked <b>media</b> <b>fragments</b> and annotations for indexing and searching...|$|R
5000|$|As of September 2012 the <b>Media</b> <b>Fragments</b> URI 1.0 (basic) is a W3C Recommendation.|$|R
5000|$|In URIs for MIME audio/*, image/*, video/* documents, {{very few}} have defined {{fragments}} or <b>fragment</b> semantics. The <b>Media</b> <b>Fragments</b> URI 1.0 (basic) syntax supports addressing a media resource along two dimensions (temporal and spatial) using the keywords [...] and [...] Therefore, {{one can use}} the following <b>media</b> <b>fragments</b> URI in the [...] attribute of the [...] or [...] HTML5 element: ...|$|R
40|$|In this demonstration, {{we present}} NinSuna, a fully {{integrated}} multimedia content adaptation platform based on Semantic Web technologies. Moreover, we show how NinSuna {{can be used}} as a server-side implementation of W 3 C <b>Media</b> <b>Fragment</b> URIs, which enables addressing media fragments on the Web using URIs...|$|E
40|$|With {{the fast}} growth of {{multimedia}} sharing and annotating applications on the Web, {{there is an}} increasing research interests in semantic annotations of multimedia. However, applying linked data principles in multimedia annotations {{is a relatively new}} topic, especially when annotations are related to media fragments. This paper, therefore, discusses this problem and further breaks it down into three fundamental sub-questions: 1) choosing <b>media</b> <b>fragment</b> URIs 2) Dereferencing <b>media</b> <b>fragment</b> URIs 3) Ontology alignment related to media fragments and annotations. This paper briefly describes how the interlinking multimedia annotations could be used in the future and concludes with a call for future research to deeply investigate the three research questions. There is a need to develop some working model to address the problems of publishing multimedia resources on the Web as linked data...|$|E
40|$|In {{the last}} few years, the {{explosion}} of multimedia content on the Web has made multimedia resources the “first class citizen” of the Web. While these resources are easily stored and shared, it is becoming more difficult to find specific video/audio content, especially to identify, link, navigate, search and share the content inside multimedia resources. The concept of <b>media</b> <b>fragment</b> refers to the deep linking into multimedia resources, but making annotations to media fragments and linking them to other resources on the Web {{have yet to be}} adopted. The Linked Data principles offer guidelines for publishing Linked Data on the Web, so that data can be better connected to each other and explored by machines. Publishing media fragments and annotations as Linked Data will enable the media fragments to be transparently integrated into current Web content. This thesis takes the Linked Data approach to realise the interlinking of media fragments to other resources on the Web and demonstrate how the Linked Data can help improve the indexing of media fragments. This thesis firstly identifies the gap between media fragments and Linked Data, and the major requirements that need to be fulfilled to bridge that gap based on the current situation of presenting and sharing multimedia data on the Web. Then, by extending the Linked Data principles, this thesis proposes Interlinking <b>Media</b> <b>Fragment</b> Principles as the basic rationale and best practice of applying Linked Data principles to media fragments. To further automate the media fragments publishing process, a core RDF model and a <b>media</b> <b>fragment</b> enriching framework are designed to link media fragments into the Linked Open Data Cloud via annotations and visualise media fragments on the Web pages. A couple of examples are implemented to demonstrate the use of interlinked media fragments, including the case to enrich YouTube videos with named entities and using media fragments for video classifications. The <b>Media</b> <b>Fragment</b> Indexing Framework is proposed to solve the fundamental problem of media fragments indexing for search engines and, as an example, Twitter is adopted as the source for <b>media</b> <b>fragment</b> annotations. The thesis concludes that applying Linked Data principles to media fragments will bring semantics to media fragments, which will improve the multimedia indexing on a fine-grained level and new research areas can be explored based on the interlinked media fragments...|$|E
40|$|This paper {{introduces}} {{a framework for}} establishing links between related <b>media</b> <b>fragments</b> within a collection of videos. A set of analysis techniques is applied for extracting information from different types of data. Visual-based shot and scene segmentation is performed for defining <b>media</b> <b>fragments</b> at different granularity levels, while visual cues are detected from keyframes of the video via concept detection and optical character recognition (OCR). Keyword extraction is applied on textual data such as the output of OCR, subtitles and metadata. This set of results {{is used for the}} automatic identification and linking of related <b>media</b> <b>fragments.</b> The proposed framework exhibited competitive performance in the Video Hyperlinking sub-task of MediaEval 2013, indicating that video scene segmentation can provide more meaningful segments, compared to other decomposition methods, for hyperlinking purposes...|$|R
40|$|The {{amount of}} {{media in the}} Web poses many {{scalability}} issues and among them copyright management. This problem becomes even bigger when not just the copyright of pieces of content has to be considered, but also <b>media</b> <b>fragments.</b> Fragments and the management of their rights, beyond simple access control, are the centrepiece for media reuse. This can become an enormous market where copyright has to be managed through the whole value chain. To attain the required level of scalability, {{it is necessary to}} provide highly expressive rights representations that can be connected to <b>media</b> <b>fragments.</b> Ontologies provide enough expressive power and facilitate the implementation of copyright management solutions that can scale in such a scenario. The proposed Copyright Ontology is based on Semantic Web technologies, which facilitate implementations at the Web scale, can reuse existing recommendations for <b>media</b> <b>fragments</b> identifiers and interoperate with existing standards. To illustrate these benefits, the papers presents a use case where the ontology is used to enable copyright reasoning on top of DDEX data, the industry standard for information exchange along media value chains. ...|$|R
40|$|The {{collision}} {{of technology and}} opera is nothing new., {{though there is a}} gap in the exploration of opera conceived and delivered for web and mobile <b>media.</b> <b>Fragments,</b> an opera for smartphones, addresses such a gap. Rooted in non-linear storytelling and adaptive music, the project reveals the creative possibilities and limitations of this emerging operatic form...|$|R
40|$|There {{still exists}} no common {{standard}} {{on how to}} publish and cite visualisations of simulation data. We introduce the TIB AV-Portal as a sustainable infrastructure for audio-visual data {{using a combination of}} digital object identifiers (DOI) and <b>media</b> <b>fragment</b> identifiers (MFID) to cite these data in accordance with scientific standards. The benefits and opportunities of enhancing publications with visual data are illustrated by showing a use case from opto-electronics...|$|E
40|$|Multimedia hyperlinking is an {{emerging}} research {{topic in the}} context of digital libraries and (cultural heritage) archives. We have been studying the concept of video-to-video hyperlinking from a video search perspective {{in the context of}} the MediaEval evaluation benchmark for several years. Our task considers a use case of exploring large quantities of video content via an automatically created hyperlink structure at the <b>media</b> <b>fragment</b> level. In this paper we report on our findings, examine the features of the definition of video hyperlinking based on results, and discuss lessons learned with respect to evaluation of hyperlinking in real-life use scenarios...|$|E
40|$|In this paper, we {{describe}} {{two examples of}} implementations of the Media Fragments URI specification which is currently being developed by the W 3 C Media Fragments Working Group. The group’s mission is to create standard addressing schemes for media fragments on the Web using Uniform Resource Identifiers (URIs). We describe two scenarios to illustrate the implementations. More specifically, we show how User Agents (UA) will either be able to resolve <b>media</b> <b>fragment</b> URIs without help from the server, or will make use of a media fragments-aware server. Finally, we present some ongoing discussions and issues regarding {{the implementation of the}} Media Fragments specification...|$|E
40|$|People {{are more}} and more {{interconnected}} online and active in publishing and sharing their thoughts, feelings, activity, and recorded experiences in media items within their so-cial networks. This new massive amount of data is however usually locked into proprietary platforms with evolving pri-vacy policies. We are interested in indexing and analyzing photos and videos shared on the Web and in studying their diffusion as well as the correlations we can deduce based on subject matters depicted. We are advocating the use of semantic web and forensic imaging technologies for provid-ing new tools to protect users privacy. We use linked data technologies to represent and expose all metadata gathered during these analysis processes. We are actively working in the W 3 C <b>Media</b> <b>Fragments</b> Working Group for standardiz-ing how to express <b>media</b> <b>fragments</b> in a URI. Finally, we are developing environments that enable contextualized explo-ration of multimedia content within social networks reveal-ing unexpected connections between objects and people...|$|R
50|$|OWP covers Web {{standards}} such as HTML5, CSS 2.1, CSS3 (including the Selectors, Media Queries, Text, Backgrounds and Borders, Colors, 2D Transformations, 3D Transformations, Transitions, Animations, and Multi-Columns modules), CSS Namespaces, SVG 1.1, MathML 3, WAI-ARIA 1.0, ECMAScript 5, 2D Context, WebGL, Web Storage, Indexed Database API, Web Workers, WebSocket Protocol/API, Geolocation API, Server-Sent Events, Element Traversal, DOM Level 3 Events, <b>Media</b> <b>Fragments,</b> XMLHttpRequest, Selectors API, CSSOM View Module, Cross-Origin Resource Sharing, File API, RDFa, WOFF, HTTP 1.1 (part 1-7), TLS 1.2, and IRI.|$|R
40|$|Abstract. This demo {{enables the}} {{automatic}} creation of semantically annotated YouTube <b>media</b> <b>fragments.</b> A video is first ingested in the Synote {{system and a}} new method enables to retrieve its associated subtitles or closed captions. Next, NERD is used to extract named entities from the transcripts which are then temporally aligned with the video. The entities are disambiguated in the LOD cloud and a user interface enables to browse through the entities detected in a video or get more information. We evaluated our application with 60 videos from 3 YouTube channels...|$|R
40|$|The Web {{applications}} {{today have}} been enriched with various multimedia resources and annotations. However, {{there is still}} a lack of semantic interlinking between media fragments and annotations, which leads to the insufficient index of inside content of multimedia resources. This paper shows a demo of applying linked data principles in media fragments and annotations to improve the index of multimedia resources. Using linked data a <b>media</b> <b>fragment</b> can be universally identified by a URI and linked to annotations or other media fragments in the linked data cloud. The demo is based on the UK Parliament Debate scenario. The RDF file containing media fragments and annotations of the debate video has been published in Sindice semantic web index and linked to other resources in the linked data cloud...|$|E
40|$|With {{more and}} more video {{resources}} shared on the Web, the practice of sharing a video object from a certain time point (deep-linking) has been implemented by many video sharing platforms. With so many media fragments created, annotated and shared, however, indexing video objects on a fine-grained level on the Web scale is still not implemented by major search engines. To solve this problem, this paper proposes Twitter <b>Media</b> <b>Fragment</b> Indexer, which monitors the Tweet text and uses the embedded URLs pointing to video fragments as the media to create index for media fragments. In this paper, we show a preliminary evaluation that thousands of media fragments can be successfully indexed using this system. We are planning to expand the indexer in a larger scale and prove that millions of media fragments can be indexed by major search engines in this way...|$|E
40|$|International audienceIn this paper, we {{describe}} the organization and {{the implementation of the}} CAMOMILE collaborative annotation framework for multimodal, multimedia, multilingual (3 M) data. Given the versatile nature of the analysis which can be performed on 3 M data, the structure of the server was kept intentionally simple in order to preserve its genericity, relying on standard Web technologies. Layers of annotations, defined as data associated to a <b>media</b> <b>fragment</b> from the corpus, are stored in a database and can be managed through standard interfaces with authentication. Interfaces tailored specifically to the needed task can then be developed in an agile way, relying on simple but reliable services for the management of the centralized annotations. We then present our implementation of an active learning scenario for person annotation in video, relying on the CAMOMILE server; during a dry run experiment, the manual annotation of 716 speech segments was thus propagated to 3504 labeled tracks. The code of the CAMOMILE framework is distributed in open source...|$|E
40|$|We have {{developed}} a tile-wise histogram-based media item deduplication algorithm with additional high-level semantic matching criteria that is tailored to photos and videos gath-ered from multiple social networks. In this paper, we inves-tigate whether the <b>Media</b> <b>Fragments</b> URI addressing scheme together with a natural language generation framework real-ized through a text–to–speech system provides a feasible and practicable way to visually and audially describe the differ-ences between media items of type photo and/or video, so that human-friendly debugging of the deduplication algorithm is made possible. A short screencast illustrating the approach is available online a...|$|R
40|$|This demo {{enables the}} {{automatic}} creation of semantically annotated YouTube <b>media</b> <b>fragments.</b> A video is first ingested in the Synote {{system and a}} new method enables to retrieve its associated subtitles or closed captions. Next, NERD is used to extract named entities from the transcripts which are then temporally aligned with the video. The entities are disambiguated in the LOD cloud and a user interface enables to browse through the entities detected in a video or get more information. We evaluated our application with 60 videos from 3 YouTube channels...|$|R
40|$|Declarative {{definition}} of multimedia presentation such as provided by SMIL standard {{can be considered}} as the most significant advance in the multimedia integration domain. However, the requirements of a model that could express richer scenarios for presentations are always a challenge. The work presented here proposes an extended model based on the concept of structured media and sub-elements that allows a finer granularity and a more semantic specification of significant events and locations inside <b>media</b> <b>fragments.</b> The new <b>media</b> <b>fragments</b> can be composed in multimedia scenarios through the specification of temporal, spatial and spatio-temporal relations. Moreover, we propose an abstract animation model that can be combined with the intra-media temporal structuration to specify animation effects in a flexible, no redundant and easy to maintain way. The underlying model is the Madeus model that is a flexible model based on the structural, interval, region and relative constraints. This paper describes the sub-element and the abstract animation models and shows how they are implemented in the Madeus multimedia framework. A complete schema of the video authoring tool based on this model is also specified. Thanks to that, the fine-grained authoring of multimedia scenarios with video media can be done in an easy and effective way. Key words: multimedia document, multimedia model, media content description, animation, multimedia authoring, multimedia synchronization. ...|$|R
40|$|To make media {{resources}} a prime citizen on the Web, {{we have to}} {{go beyond}} simply replicating digital media files. The Web is based on hyperlinks between Web resources, and that includes hyperlinking out of resources (e. g., from a word or an image within a Web page) as well as hyperlinking into resources (e. g., fragment URIs into Web pages). To turn video and audio into hypervideo and hyperaudio, we need to enable hyperlinking into and out of them. The W 3 C Media Fragments Working Group is taking on the challenge to further embrace W 3 C's mission to lead the World Wide Web to its full potential by developing a <b>Media</b> <b>Fragment</b> protocol and guidelines that ensure the long-term growth of the Web. The major contribution of this paper is the introduction of Media Fragments as a media-format independent, standard means of addressing media resources using URIs. Moreover, we explain how the HTTP protocol can be used and extended to serve Media Fragments and what the impact is for current Web-enabled media formats...|$|E
40|$|In this paper, {{we propose}} a new {{framework}} to annotating subtitled YouTube EDU media fragments using textual {{features such as}} exert all the basic portions extracted from the web-based natural language processors of in relation to subtitles and temporal features such as duration of the media fragments where proper entities are spotted. We’ve created the SY-E-MFSE (Subtitled YouTube EDU <b>Media</b> <b>Fragment</b> Search Engine) as a framework to cruising on the subtitled YouTube EDU videos resident in the Linked Open Data (LOD) cloud. For realizing this purpose, we propose Unifier Module of Outcomes of Web-Based Natural Language Processors (UM-OWNLP) for extracting the essential portions of the 10 NLP tools {{that are based on}} the web, from subtitles associated to YouTube videos in order to generate media fragments annotated with resources from the LOD cloud. Then, we propone Unifier Module of Outcomes of Web-Based Named Entity (NE) Booster Processors (UM-OWNEBP) containing the six web Application Programming Interfaces (API) to boost outcomes of NEs obtained from UM-OWNLP. We’ve presented ‘UM-OWNLP ontology ’ to support all the 10 NLP web-based tools ontological features and representing them in a steadfast framework...|$|E
40|$|Some digital Libraries support {{annotations}} {{in their}} systems, but sharing these annotations with other DL systems or across the web {{is difficult because}} of the need of special applications to read and decode these annotations. Due to the frequent change on web resources, the annotation’s resources can change the annotation’s meaning. This project concentrates on minting a new URI for every annotation and creating a persistent and independent annotation system. Users should be able to select a segment of an image or a video {{to be part of the}} annotation. The fragment ID and <b>media</b> <b>fragment</b> URI described in the open annotation data model can be theoretically used, but in practice they have limits, and they face the lack of support by the browsers. So in this project, the segments of images, audios and videos can be used in the annotations without hacking the fragment ID. This paper also provides solutions for two problems, the first problem is reading the annotation when the annotated objects get updated or removed and the second problem is reading the annotation independently in web browsers without the help of any special clients...|$|E
40|$|With the {{ever-increasing}} {{offer of}} television content as internet broadcast streams, synchronisation {{of this material}} with second screen applications has received considerable interest over the last years. We introduce a novel audio fingerprinting method which can be easily implemented, and offer promising experiments on German news show material. Further, we evaluate possible additional usage for audio fingerprinting {{in the context of}} duplicate detection whenever different media shows with the same topic recycle shared <b>media</b> <b>fragments.</b> We give an outlook in how far knowledge drawn from our algorithm can be used to enhance a viewers experience, by offering automatic skipping and/or recommendation functionality...|$|R
40|$|Life {{is filled}} with stories. Modern {{technologies}} enable us to document and share life events with various kinds of media, such as photos, videos, etc. But people still find it time-consuming to select and arrange <b>media</b> <b>fragments</b> to create coherent and engaging narratives. This thesis proposes a novel storytelling system called Storied Navigation, which lets users assemble a sequence of video clips based on their roles in telling a story, rather than solely by explicit start and end times. Storied Navigation uses textual annotations expressed in unconstrained natural language, using parsing and Commonsense reasoning to deduce possible connections between the narrative intent of the storyteller, and descriptions of events and characters in the video. It helps users increase their familiarity with a documentary video corpus. It helps them develop story threads by prompting them with recommendations of alternatives as well as possible continuations for each selected video clip. We {{view it as a}} promising first step towards transforming today's <b>fragmented</b> <b>media</b> production experience into an enjoyable, integrated storytelling activity. Edward Yu-Te Chen. Thesis (S. M.) [...] Massachusetts Institute of Technology, School of Architecture and Planning, Program in Media Arts and Sciences, 2007. Includes bibliographical references (p. 113 - 118) ...|$|R
40|$|Commissioned and {{presented}} in Cardiff, in 2001, 'Polis' was realised {{within the context}} of Mike Brookes and Mike Pearson's long term collaboration, under the umbrella of their performance collective 'Pearson/Brookes'. A large-scale three hour multi-site performance event, addressing the city centre of Cardiff, and engaging the specifically structured use of low-grade analogue technology, collage dramaturgy, and the juxtaposition of live and mediated documentation generated during the performance itself. The work being structured across twenty-five located performance fragments, realised as isolated events, at twenty-five sites across the centre of the city - experienced live by spectators delivered to the encounter of each isolated fragment by guided taxi, and then immediately recombined and revealed within a specifically equipped studio theatre space, through a series of reconstructions built solely from the documentary traces captured and returned by those spectators. Documentation and original <b>media</b> <b>fragments</b> of the work are available...|$|R
40|$|Who are {{you looking}} at?' was {{developed}} in collaboration with Mike Pearson and Welsh playwright Ed Thomas, and presented {{under the umbrella of}} Brookes and Pearson's performance collective 'Pearson/Brookes'. Realised over three nights in February 2004, and built on material produced in collaboration with five young female performers - each documenting 3 mins within a particular public city centre location, on the same evening - producing footage of each location from three simultaneous and expanding points of view: [1] from a hidden camera on the performer themselves; [2] from a shadowing camera recording their movements; and [3] a wider locating shot taken from within a tracking car. Performed by Brookes, Pearson and Thomas - working around a large central table, arranged with all the necessary equipment and material - the resulting video fragments being layered with live texts and vocal recordings of the performers in conversation, mixed live within the room of the developing public event. Documentation and original <b>media</b> <b>fragments</b> of the work are available...|$|R
40|$|With {{the steady}} {{increase}} of videos published on media sharing platforms such as Dailymotion and YouTube, {{more and more}} efforts are spent to automatically annotate and organize these videos. In this paper, we propose a framework for classifying video items using both textual features such as named entities extracted from subtitles, and temporal features such as {{the duration of the}} <b>media</b> <b>fragments</b> where particular entities are spotted. We implement four automatic machine learning algorithms for multiclass classification problems, namely Logistic Regression (LG), K-Nearest Neighbour (KNN), Naive Bayes (NB) and Support Vector Machine (SVM). We study the temporal distribution patterns of named entities extracted from 805 Dailymotion videos. The results show that the best performance using the entity distribution is obtained with KNN (overall accuracy of 46. 58 %) while the best performance using the temporal distribution of named entities for each type is obtained with SVM (overall accuracy of 43. 60 %). We conclude that this approach is promising for automatically classifying online videos...|$|R
40|$|Part 2 : Key ApplicationsInternational audienceIs it {{possible}} to determine only by observing {{the behavior of a}} user what are his interests for a media? The aim of this project is to develop an application that can detect whether or not a user is viewing a content on the TV and use this information to build the user profile and to make it evolve dynamically. Our approach is based on the use of a 3 D sensor to study the movements of a user’s head to make an implicit analysis of his behavior. This behavior is synchronized with the TV content (<b>media</b> <b>fragments)</b> and other user interactions (clicks, gestural interaction) to further infer viewer’s interest. Our approach is tested during an experiment simulating the attention changes of a user in a scenario involving second screen (tablet) interaction, a behavior that has become common for spectators and a typical source of attention switches...|$|R
5000|$|However, cyberbalkinization, the {{phenomenon}} where <b>media</b> audiences <b>fragment</b> into [...] "enclaves" [...] where they only consume content they concur with—and thus theoretically promoting social polarization—may not {{have as much}} influence as believed. Utilizing Nielsen television and Internet audience data, J.G. Webster found that ideological segmentation among media users was unlikely, as “even consumers of obscure niche media devoted most of their attention to more broadly appealing fare.” ...|$|R
40|$|Better {{tools for}} {{content-based}} access of video {{are needed to}} improve access to time-continuous video data. Particularly information about linear TV broadcast programs has been available in a form limited to program guides that provide short manually described overviews of the program content. Recent development in digitalization of TV broadcasting and emergence of web-based services for catch-up and on-demand viewing bring out new possibilities to access data. In this paper we introduce our data mining system and ac-companying services for summarizing Finnish DVB broad-cast streams from seven national channels. We describe how data mining of novelty concepts can be extracted from DVB subtitles to augment web-based ”Catch-Up TV Guide ” and ”Novelty Cloud ” TV services. Furthermore, our system al-lows accessing <b>media</b> <b>fragments</b> as Picture Quotes via gen-erated word lists and provides content-based recommenda-tions to find new programs that have content similar to the user selected programs. Our index consists of over 180 000 programs {{that are used to}} recommend relevant programs. The service has been under development and available on...|$|R
40|$|This article {{presents}} the Pedagogical Itineraries, an innovative educational proposal {{to teachers and}} students of basic education {{and high school students}} (elementary and second-ary education), and to the customers of science centres (museums, zoos and other informal education places). Besides offering elements to digital didactics and educational opportuni-ties using a virtual support that allows interaction and appropriation of knowledge in topics of the natural sciences field, our goal is to offer subsidies for the use of these learning objects in a logical and flexible manner. Furthermore, new creations made by teachers, students and science centre visitors can be integrated with the pre-existing itineraries. Therefore, what is proposed is not a mass production of <b>media</b> <b>fragments,</b> but a professional structure that supports production and an interactive and educational use of these resources. So, {{it will be possible to}} contribute to increasing the scientific knowledge. In this context we have developed a didactic proposal which involves the production of learning objects with an emphasis on creativity, interactivity and interdisciplinary of content...|$|R
