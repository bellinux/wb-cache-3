27|12|Public
50|$|UNIX System V and {{its modern}} {{derivatives}} IRIX, SCO OpenServer, HP-UX and IBM AIX allow a <b>many-to-many</b> <b>mapping</b> between user threads and LWPs.|$|E
50|$|Solaris has {{implemented}} a separate LWP layer since version 2.2. Prior to version 9, Solaris allowed a <b>many-to-many</b> <b>mapping</b> between LWPs and user threads. However, this was retired {{due to the}} complexities it introduced and performance improvements to the kernel scheduler.|$|E
5000|$|Despite {{several decades}} of concerted effort, {{research}} in speech perception {{has yet to}} crack the [...] "lack of invariance" [...] problem: there is a <b>many-to-many</b> <b>mapping</b> between the acoustic patterns and percepts. The acoustic signature of a particular phoneme changes {{as a function of}} phonetic context, speaking rate, physical characteristics of talkers, dialect, acoustic environment, and so on. How listeners achieve [...] "phonetic constancy" [...] despite these sources of variability largely remains a mystery. The modal approach has been to search for invariant cues that have somehow been missed—that is, to hypothesize that there is no lack of invariance problem {{aside from the fact that}} researchers have not discovered how to detect invariant cues available to listeners.|$|E
40|$|Ontology mapping is {{a crucial}} task for {{achieving}} large scale semantic inter-operation of heterogeneous information sources. Conventional mapping solutions mainly focus on the mapping {{of a pair of}} ontologies. While such approaches are effective in creating one-to-one ontology mappings, they are less efficient when dealing with the <b>many-to-many</b> ontology <b>mapping</b> scenarios. To cope with the complexity issue faced by pair-wise mapping solutions, we propose a <b>many-to-many</b> ontology <b>mapping</b> approach. By treating ontology mapping as a concept classification problem, we aim {{to reduce the number of}} concept comparisons involved in <b>many-to-many</b> ontology <b>mapping.</b> A preliminary evaluation of performance was also carried out, and results show that the proposed mapping solution requires less concept comparison cycles for <b>many-to-many</b> ontology <b>mapping...</b>|$|R
5000|$|Associative {{tables are}} colloquially known under many names, {{including}} association table, bridge table, cross-reference table, crosswalk, intermediary table, intersection table, join table, junction table, link table, linking table, <b>many-to-many</b> resolver, <b>map</b> table, mapping table, pairing table, pivot table (as used in Laravel - {{not to be}} confused with [...] pivot table (spreadsheets)), or transition table.|$|R
40|$|In {{this paper}} we {{investigate}} the factors conditioning a morphological alternation on verbal heads in Lai. We {{show that this}} alternation eludes a simple characterization and instead exhibits a <b>many-to-many</b> form–function <b>mapping.</b> We will further show that the facts can be given a straightforward analysis in terms of default conditions based on valence and polarity, together with various constructional overrides. Our analysis thus follows recent proposals in HPSG, in particular Malouf (forthcoming), in using a constructional type hierarchy with defaults (“cooperating constructions”) {{as an alternative to}} an Optimality Theoretic system of ranked violable constraints...|$|R
50|$|A {{characteristic}} of natural language {{is that there}} are many different ways to state what you want to say: several meanings can be contained in a single text and that the same meaning can be expressed by different texts. This variability of semantic expression {{can be seen as the}} dual problem of language ambiguity. Together they result in a <b>many-to-many</b> <b>mapping</b> between language expressions and meanings. The task of paraphrasing involves recognizing when two texts have the same meaning and creating a similar or shorter text that conveys almost the same information. Textual entailment is similar but weakens the relationship to be unidirectional. Mathematical solutions to establish textual entailment can be based on the directional property of this relation, by making a comparison between some directional similarities of the texts involved.|$|E
40|$|Usually, load {{distribution}} schemes for replicated servers {{are based on}} a many-to-one mapping between client and server, meaning that while a server may serve many clients, a client has a single specific server which it queries at any point in time. In some cases, however, it is desirable that the number of accesses of a client may be distributed over multiple servers, thus yielding a <b>many-to-many</b> <b>mapping</b> between clients and servers. In this paper, we present a simple method to efficiently realize such a <b>many-to-many</b> <b>mapping</b> between clients and servers. For the sake of transparency, we add a component called "distribution module" to the communication interface of client and server. This module is responsible for distributing server accesses over multiple target machines in a well defined way. We present algorithms for the client and server component and show that they are self-stabilizing, meaning that they converge to a stable state once the access pattern becomes regular. Due to this [...] ...|$|E
40|$|International audienceModel Transformation By Example (MTBE) is {{a recent}} {{approach}} that derives model transformation rules from a source model, a target model, and matching between models. Building a match between models may be a complex task especially when models have been created or edited manually. In this paper, we propose an automated approach to generate mapping between source and target models. The novetly of our approach consists {{in the production of}} <b>many-to-many</b> <b>mapping</b> between the elements of the two models...|$|E
40|$|A generic-construct-based {{approach}} is proposed to enable <b>many-to-many</b> systematic <b>mapping</b> between distributed data warehouses, introducing a consistent and unique standard exchange format. Based on the transformation model we develop between multidimensional data model and XML data model, and {{enhanced by the}} multidimensional metadata management function proposed in this research, a general-purpose XML-based multidimensional data exchange process over web is facilitated efficiently and improved in quality. Moreover, we develop an XML-based prototype system to exchange multidimensional data, which shows that the proposed multidimensional data exchange model is feasible, and the multidimensional data exchange process is more systematic and efficient using metadata...|$|R
40|$|The article {{describes}} a flexible mapping technique realized as a <b>many-to-many</b> dynamic <b>mapping</b> matrix. Digital sound generation is typically {{controlled by a}} large number of parameters and efficient and flexible mapping is necessary to provide expressive control over the instrument. The proposed modulation matrix technique may be seen as a generic and selfmodifying mapping mechanism integrated in a dynamic interpolation scheme. It is implemented efficiently by taking advantage of its inherent sparse matrix structure. The modulation matrix is used within the Hadron Particle Synthesizer, a complex granular module with 200 synthesis parameters and a simplified performance control structure with 4 expression parameters...|$|R
40|$|Local {{search is}} a well {{established}} and highly effective method for solving complex combinatorial optimization problems. Here, local search is adapted to solve difficult geometric matching problems. Matching is posed as the problem of finding the optimal <b>many-to-many</b> correspondence <b>mapping</b> between a line segment model and image line segments. Image data {{is assumed to be}} fragmented, noisy and cluttered. The algorithms presented have been used for robot navigation, photo-interpretation and scene understanding. This paper explores how local search performs as model complexity increases, image clutter increases, and additional model instances are added to the image data. Expected run-times to find optimal matches with 95 % confidence are determined for 48 distinct problems involving 6 models. Non-linear regression is used to estimate run-time growth as a function of problem size. Both polynomial and exponential growth models are fit to the run-time data. For problems with random clutter the [...] ...|$|R
40|$|Abstract. In this paper, {{we address}} the problem of {{producing}} visible spectrum facial images as we normally see by using thermal infrared images. We apply Canonical Correlation Analysis (CCA) to extract the features, converting a <b>many-to-many</b> <b>mapping</b> between infrared and visible images into a one-to-one mapping approximately. Then we learn the relationship between two feature spaces in which the visible features are inferred from the corresponding infrared features using Locally-Linear Regression (LLR) or, what is called, Sophisticated LLE, and a Locally Linear Embedding (LLE) method is used to recover a visible image from the inferred features, recovering some information lost in the infrared image. Experiments demonstrate that our method maintains the global facial structure and infers many local facial details from the thermal infrared images. ...|$|E
40|$|This paper {{introduces}} {{a method for}} automatic redubbing of video that exploits the <b>many-to-many</b> <b>mapping</b> of phoneme sequences to lip movements modelled as dynamic visemes [1]. For a given utterance, the corresponding dynamic viseme sequence is sampled to construct a graph of possible phoneme sequences that synchronize with the video. When composed with a pronunciation dictionary and language model, this produces {{a vast number of}} word sequences that are in sync with the original video, literally putting plausible words into the mouth of the speaker. We demonstrate that traditional, one-to-many, static visemes lack flexibility for this application as they produce significantly fewer word sequences. This work explores the natural ambiguity in visual speech and offers insight for automatic speech recognition and the importance of language modeling...|$|E
40|$|Variability in {{speaking}} rate {{results in a}} <b>many-to-many</b> <b>mapping</b> between acoustic properties in speech and the linguistic interpretation of an utterance. In order to recognize the phonetic structure of an utterance, listeners must calibrate their phonetic decisions against {{the rate at which}} the speech was produced. This process of rate normalization is fast and effective allowing listeners to maintain phonetic constancy in spite of changes {{in speaking}} rate. Most of the research on rate normalization has investigated the sources of information used by listeners to determine the speaking rate. There is an assumption in much of this research that the normalization process is a passive, automatized filtering process that strips the effects of rate variation away from the signal prior to recognition...|$|E
40|$|To {{facilitate}} {{the application of}} semantics in statistical machine translation, we propose a broad-coverage predicate-argument structure mapping technique using automated resources. Our approach utilizes automatic syntactic and semantic parsers to generate Chinese-English predicate-argument structures. The system produced a <b>many-to-many</b> argument <b>mapping</b> for all PropBank argument types by computing argument similarity based on automatic word alignment, achieving 80. 5 % F-score on numbered argument mapping and 64. 6 % F-score on all arguments. By measuring predicate-argument structure similarity based on the argument mapping, and formulating the predicate-argument structure mapping problem as a linear-assignment problem, the system achieved 84. 9 % F-score using automatic SRL, only 3. 7 % F-score lower than using gold standard SRL. The mapping output covered 49. 6 % of the annotated Chinese predicates (which contains predicateadjectives that often have no parallel annotations in English) and 80. 7 % of annotated English predicates, suggesting its potential as a valuable resource for improving word alignment and reranking MT output. ...|$|R
40|$|Článek ukazuje, že použití bohatých morfologických značek v rámci třídového n-gramového jazykového modelu a kombinace tohoto modelu se standardním slovním n-gramovým modelem může zlepšit přesnost rozpoznávání oproti slovnímu modelu v úloze automatického přepisu spontánních českých rozhovorů. Automatic speech recognition, or more {{precisely}} language modeling, of the Czech language has to face challenges that are not present in the language modeling of English. Those include mainly the rapid vocabulary growth and closely connected unreliable estimates of the language model parameters. These phenomena are caused mostly by the highly inflectional nature of the Czech language. On the other hand, the rich morphology together with the well-developed automatic systems for morphological tagging can be exploited to reinforce the language model probability estimates. This paper shows that using rich morphological tags within the concept of class-based n-gram language model with <b>many-to-many</b> word-to-class <b>mapping</b> and combination of this model with the standard word-based n-gram can improve the recognition accuracy over the word-based baseline {{on the task of}} automatic transcription of unconstrained spontaneous Czech interviews...|$|R
30|$|The primary {{pedagogical}} goal of AutoTutor is {{to encourage}} the student to verbally articulate content and steps in reasoning {{during the course of}} answering challenging questions or solving challenging problems. The desired content is captured in expectations. These expectations in turn are mapped onto the knowledge components (KCs) discussed earlier (Koedinger et al. 2012). There is also a non-hierarchical <b>mapping</b> (<b>many-to-many)</b> between the KCs and the main topics. The KCs and topics unite the curriculum and all of the learning resources in ElectronixTutor. Although AutoTutor measures mastery of KCs and topics through natural language in ElectronixTutor, AutoTutor also has the capacity to accommodate student actions that involve clicking, dragging, dropping, toggling, and even interactions in virtual worlds (Cai et al. 2015; Zapata-Rivera et al. 2015). In fact, simple clicks were emphasized in a project that helps struggling adult readers who have minimal abilities to type in written responses (Graesser et al. 2016).|$|R
40|$|Abstract. We {{present an}} appearance-based 3 D hand posture {{estimation}} method that determines a ranked set of possible hand posture candidates from an unmarked hand image, {{based on an}} analysis by synthesis method and an image retrieval algorithm. We formulate the posture estimation problem as a nonlinear, <b>many-to-many</b> <b>mapping</b> problem in a high dimension space. A general algorithm called ISOSOM is proposed for nonlinear dimension reduction, applied to 3 D hand pose reconstruction to establish the mapping relationships between the hand poses and the image features. In order to interpolate the intermediate posture values given the sparse sampling of ground-truth training data, the geometric map structure of the samples ’ manifold is generated. The experimental {{results show that the}} ISOSOM algorithm performs better than traditional image retrieval algorithms for hand pose estimation. ...|$|E
40|$|Abstract. An {{overlooked}} {{problem in}} Learning From Demonstration is the ambiguity that arises, for instance, when the robot {{is equipped with}} more sensors than necessary for a certain task. Simply trying to repeat all aspects of a demonstration is seldom what the human teacher wants, and without additional information, {{it is hard for}} the robot to know which features are relevant and which should be ignored. This means that a single demonstration maps to several different behaviours the teacher might have intended. This one-to-many (or <b>many-to-many)</b> <b>mapping</b> from a demonstration (or several demonstrations) into possible intended behaviours is the ambiguity that is the topic of this paper. Ambiguity is defined as the size of the current hypothesis space. We investigate the nature of the ambiguity for different kinds of hypothesis spaces and how it is reduced by a new concept learning algorithm...|$|E
40|$|The Universal Biometric System is a {{biometric}} enabled third-party authentication system. It {{tries to}} address some of the issues relating to integration of biometrics in an enterprise level network. We present a system that hides all the complexity of biometrics and provides biometric technology, vendor, and platform independent authentication. It also introduces two novel ideas; <b>many-to-many</b> <b>mapping</b> to reduce the total cost of ownership while device-hierarchy enforces indepth security. Biometric vendor, technology, and platform independence is achieved by implementing the system on top of the BioAPI (the Defacto standard for biometrics). However our system sets its sights far beyond the BioAPI. It is designed to provide a simple development environment that does not require complex data structures, pointers, and memory management inherent to the BioAPI. This is a Proof of Concept effort. Keywords- Biometric, BioAPI, biometric servic...|$|E
40|$|Abstract: Brain-Computer Interfacing (BCI) {{offers a}} range of control options that can be {{harnessed}} for music-making applications, in particular for individuals {{with little or no}} physical motor abilities. This paper presents a new system that explores two major drawbacks when using brainwave control, in comparison to more traditional, physically controlled digital musical controllers. The first is the limited number of control options BCI systems offer, and the second is a lack of polyphonic control; controlling more than one mapping simultaneously. The hybrid BCI system presented here combines two methods of analysing brainwave data to provide greater control. The use of two techniques, being controlled simultaneously in real-time, provides the basis for more complex <b>many-to-many</b> musical <b>mapping</b> strategies than were previously available with only one analysis method and a one-to-many mapping approach. This paper presents joyBeat, a brainwave controlled drum machine that is demonstrative of this type of real-time hybrid BCMI. Control from the Steady State Visual Evoked Potential (SSVEP) eye-gaze focusing method is used to sequence and arrange drum patterns, whilst timbral characteristics and global features (such as tempo) are derived from affective states measured within EEG patterns. Here we employ arousal and valance (AV) response, which can in conjunction give a complete estimation of affective state, and also provide a 2 -dimensional space for musical feature mapping. This system provides a user with a novel method for composing and performing on-the-fly. Thus, a new real-time active-passive hybrid BCI is presented, with active user selection of sequencing, and passive, affective state-led control over timbral qualities. 1...|$|R
40|$|An {{immutable}} multi-map is a <b>many-to-many</b> thread-friendly <b>map</b> {{data structure}} with expected fast insert and lookup operations. This data structure {{is used for}} applications processing graphs or many-to-many relations as applied in static analysis of object-oriented systems. When processing such big data sets the memory overhead of the data structure encoding itself is a memory usage bottleneck. Motivated by reuse and type-safety, libraries for Java, Scala and Clojure typically implement immutable multi-maps by nesting sets as the values with the keys of a trie map. Like this, based on our measurements the expected byte overhead for a sparse multi-map per stored entry adds up to around 65 B, which renders it unfeasible to compute with effectively on the JVM. In this {{paper we propose a}} general framework for Hash-Array Mapped Tries on the JVM which can store type-heterogeneous keys and values: a Heterogeneous Hash-Array Mapped Trie (HHAMT). Among other applications, this allows for a highly efficient multi-map encoding by (a) not reserving space for empty value sets and (b) inlining the values of singleton sets while maintaining a (c) type-safe API. We detail the necessary encoding and optimizations to mitigate the overhead of storing and retrieving heterogeneous data in a hash-trie. Furthermore, we evaluate HHAMT specifically for the application to multi-maps, comparing them to state-of-the-art encodings of multi-maps in Java, Scala and Clojure. We isolate key differences using microbenchmarks and validate the resulting conclusions on a real world case in static analysis. The new encoding brings the per key-value storage overhead down to 30 B: a 2 x improvement. With additional inlining of primitive values it reaches a 4 x improvement...|$|R
40|$|The use of {{orthogonal}} {{features are}} widely illustrated {{in design and}} manufacturing publications, if not widely used. Features {{as a method of}} design and manufacturing integration is dependent upon the communication of information from one context to another. Free-form features are used to represent sculptured aesthetic surfaces, for instance, in the automotive and mould tool industries. The surface creation, manipulation and blending processes are highly labour intensive, with automation a highly desired software characteristic. An outcome of the BriteEuram IMPRESS (BREU 7049) project has been to identify that free-form features have an optional <b>many-to-many</b> <b>mapping</b> relationship between features in a design context and those in a manufacturing context. To identify with this approach, a logical process of mapping is presented. The concept has enabled the partners to investigate this mapping link and to identify the processes and strategies used within the automotive and injection moulding tool industry...|$|E
40|$|Under some views, {{a crucial}} {{function}} for conversation policies is to "constrain the messages that {{appear on the}} wire," for {{there can be a}} <b>many-to-many</b> <b>mapping</b> between an agent’s intention and the message primitive used to express that intention. In this paper, we argue that the way to constrain messages is to constrain intentions. We propose a pragmatic approach to doing this through an abstract task specification or model. Abstract task specifications are based on a simple state-space representation for problem formulation, and this representation can be used to delimit the agents’ task intentions and discourse intentions. This analysis supports a flexible and pragmatic way of handling "unexpected" messages by reference to the abstract task specification. We see an abstract task specification as one component of a publicly posted conversation policy. A simple search-assistant agent was implemented within a BDI architecture to illustrate application of these ideas...|$|E
40|$|It is {{generally}} acknowledged today that, while the intonational features speakers select when they utter a sentence are not {{determined by the}} syntax, semantics or discourse context of that sentence, knowledge of these factors can help to constrain the possible intonational features speakers are likely to choose. So, while intonational variation poses a challenge to speech recognition in one sense- in presenting yet another indicator of over-all utterance meaning to be recognized noted between intonational features and the syntax, semantics and discourse features of an utterance also present rich possibilities for help in the recognition task. The <b>many-to-many</b> <b>mapping</b> between intonational features and syntactic and discourse features can be illustrated by considering the various ways of uttering the sentences in (1). 1 (1) a. 560 CAN KIRK GET TO KODIAK BY MONDAY b. Kirk can get to Kodiak by Monday. For example, a senior officer might choose a falling pitch contour over (la) t...|$|E
40|$|This paper {{describes}} {{the design of}} a direct-style, λ-calculus-based compiler intermediate representation (IR) suitable for implementing a wide range of surface-language concurrency features while allowing flexibility in the back-end implementation. The features that this IR includes to support concurrency include a weak but inexpensive form of continuations and primitives for thread creation, scheduling, and termination. Although this work has been done in the context of implementing concurrency for the MOBY programming language, the model is general and could be used in the context of other languages. In this paper, we describe the IR’s continuations and thread manipulation primitives and illustrate their expressiveness with examples of various concurrency operations. We define an operational semantics to specify the operations precisely and to provide a tool for reasoning about the correctness of implementations. To illustrate the flexibility of our the approach, we describe two existing implementations of the threading model — one based on a one-to-one mapping of language threads to POSIX threads and one based on a <b>many-to-many</b> <b>mapping.</b> ...|$|E
40|$|Cellular {{pathways}} {{are composed}} of multiple reactions and interactions mediated by genes. Many of these reactions are common to multiple pathways, and each reaction might be potentially mediated by multiple genes in the same genome. Existing pathway reconstruction procedures assign a gene to all pathways in which it might catalyze a reaction, leading to a <b>many-to-many</b> <b>mapping</b> of genes to pathways. However, {{it is unlikely that}} all genes that are capable of mediating a certain reaction are involved in all the pathways that contain it. Rather, {{it is more likely that}} each gene is optimized to function in specific pathway(s). Hence, existing procedures for pathway construction produce assignments that are ambiguous. Here we present a probabilistic algorithm for the assignment of genes to pathways that addresses this problem and reduces this ambiguity. Our algorithm uses expression data, database annotations and similarity data to infer the most likely assignments, and estimate the affinity of each gene with the known cellular pathways. We apply the algorithm to metabolic pathways in Yeast and compare the results to assignments that were experimentally verified. 1...|$|E
40|$|Finding {{information}} about {{people on the}} Web using a search engine is difficult {{because there is a}} <b>many-to-many</b> <b>mapping</b> between person names and specific persons (i. e. referents). This paper describes a person resolution system, called WebHawk. Given a list of pages obtained by submitting a person query to a search engine, WebHawk facilitates person search in three steps: First of all, a filter removes those pages that contain no {{information about}} any person. Secondly, a cluster groups the remaining pages into different clusters, each for one specific person. To make the resulting clusters more meaningful, an extractor is used to induce query-oriented personal information from each page. Finally, a namer generates an informative description for each cluster so that users can find any specific person easily. The architecture of WebHawk is presented, and the four components are discussed in detail, with a separate evaluation of each component presented where appropriate. A user study shows that WebHawk complements most existing search engines and successfully improves users' experience of person search on the Web...|$|E
40|$|On {{the basis}} of the artifact-centric {{approach}} to business process modeling, this paper proposes a notation named ACTA that provides two equivalent forms of representation: in one, all the life cycles of the artifacts involved in the business under consideration are shown in a single model, while in the other they are defined in separate models. The latter may be easier to understand when the model is large but requires synchronization points between life cycles: two kinds of mechanisms, i. e., driven transitions and driven tasks are analyzed in this paper. The major feature of ACTA is its nature of data flow language: the activation of tasks depends on the presence of suitable input entities rather than on the precedence relationships between tasks as it takes place in the conventional activity-centric approach. The advantage is the ease with which a number of situations that are difficult to handle with the activity-centric approach can be managed. Such situations encompass the selection of homogeneous entities to be processed in batches and the <b>many-to-many</b> <b>mapping</b> between entities of different types, such as requisition orders and procurement order...|$|E
40|$|Abstract. Database applications, {{including}} SQL-based applications, {{have received}} little attention directed towards improving {{the knowledge of}} their possible faults. This paper deals with issues related to software faults and failures aiming at understanding what types of faults occur in SQL manipulation commands, {{and how they are}} propagated to the output of command execution. SQL manipulation commands are studied and their structure is organized into structural items, a step towards understanding and grouping fault types. A list of manipulation fault types is determined and presented with SQL command examples. Failure dimensions are discussed along with query and state changing operations. An experiment to abstract the types of manipulation faults for SQL was carried out and the results are presented. The experiment built databases and faulty commands to promote failure in command execution. A database was built and a set of faulty SQL commands used to map fault types and failure dimensions. The analysis of data mapping indicates: i) there is a <b>many-to-many</b> <b>mapping</b> between faults and failures; ii) failure dimensions are dependent on fault type, faulty command, and the database itself; and iii) manipulation fault knowledge is crucial for SQL programming and testing of database applications. This work represents an initial step for testing SQL programming...|$|E
40|$|What is the {{relationship}} between brain and behavior? The answer to this question necessitates characterizing the mapping between structure and function. The aim {{of this paper is to}} discuss broad issues surrounding the link between structure and function in the brain that will motivate a network perspective to understanding this question. As others in the past, I argue that a network perspective should supplant the common strategy of understanding the brain in terms of individual regions. Whereas this perspective is needed for a fuller characterization of the mind-brain, it should not be viewed as panacea. For one, the challenges posed by the <b>many-to-many</b> <b>mapping</b> between regions and functions is not dissolved by the network perspective. Although the problem is ameliorated, one should not anticipate a one-to-one mapping when the network approach is adopted. Furthermore, decomposition of the brain network in terms of meaningful clusters of regions, such as the ones generated by community-finding algorithms, does not by itself reveal 'true' subnetworks. Given the hierarchical and multi-relational relationship between regions, multiple decompositions will offer different 'slices' of a broader landscape of networks within the brain. Finally, I described how the function of brain regions can be characterized in a multidimensional manner via the idea of diversity profiles. The concept can also be used to describe the way different brain regions participate in networks. Comment: 61 pages, 24 figure...|$|E
40|$|M. Sc. Real-time terrain {{rendering}} (RTTR) is {{an exciting}} eld in computer graphics. The algorithms and techniques developed in this domain allow immersive virtual environments to be created for interactive applications. Many di culties are encountered in this eld of research, including acquiring the data to model virtual worlds, handling huge amounts of geometry, and texturing landscapes that appear to go on forever. RTTR has been widely studied, and powerful methodologies {{have been developed to}} overcome many of these obstacles. Complex natural terrain features such as detailed vertical surfaces, overhangs and caves, however, are not easily supported by the majority of existing algorithms. It becomes di cult to add such detail to a landscape. Existing techniques are incredibly e cient at rendering elevation data, where for any given position on a 2 D horizontal plane we have exactly 1 altitude value. In this case we have a many-to- 1 mapping between 2 D position and altitude, as many 2 D coordinates may map to 1 altitude value but any single 2 D coordinate maps to 1 and only 1 altitude. In order to support the features mentioned above we need to allow for a <b>many-to-many</b> <b>mapping.</b> As an example, with a cave feature for a given 2 D coordinate we would have elevation values for the oor, the roof and the outer ground. In this dissertation we build upon established techniques to allow for this manyto- many mapping, and thereby add support for complex terrain features. The <b>many-to-many</b> <b>mapping</b> is made possible by making use of geometry images in place of height-maps. Another common problem with existing RTTR algorithms is texture distortion. Texturing is an inexpensive means of adding detail to rendered terrain. Many existing technique map texture coordinates in 2 D, leading to distortion on steep surfaces. Our research attempts to reduce texture distortion in such situations by allowing a more even spread of texture coordinates. Geometry images make this possible as they allow for a more even distribution of sample positions. Additionally we devise a novel means of blending tiled texture that enhances the important features of the individual textures. Fully sampled terrain employs a single global texture that covers the entire landscape. This technique provides great detail, but requires a huge volume of data. Tiled texturing requires comparatively little data, but su ers from disturbing regular patterns. We seek to reduce the gap between tiled textures and fully sampled textures. In particular, we aim at reducing the regularity of tiled textures by changing the blending function. In summary, the goal of this research is twofold. Firstly we aim to support complex natural terrain features|speci cally detailed vertical surfaces, over-hangs and caves. Secondly we wish to improve terrain texturing by reducing texture distortion, and by blending tiled texture together in a manner that appears more natural. We have developed a level of detail algorithm which operates on geometry images, and a new texture blending technique to support these goals...|$|E
40|$|This {{thesis is}} about phonetic events, phonetic representations, and the {{grammatical}} constraints on those representations, {{with respect to}} one particular phonetic dimension: time. It focuses on a process called beat mapping, whose clearest manifestation is in singing (as opposed to "ordinary" speech). This is the mapping of a sequence of syllables/segments onto a sequence of timing units or beats. The empirical ground is provided by Ancient Greek musical scores. We analyze the way that sensitivity to syllable weight manifests itself in beat mapping. In Ancient Greek, the musical quantity of syllables (their duration, counted in beats) is tightly controlled by their type. Taking this as a robust example of a weight-sensitive process, {{we set out to}} demonstrate that syllable weight is not about syllables, but about segments; this is contrary to what current theories of syllable weight assume (see Gordon 2004). We attempt to derive both syllable weight and syllable constituency itself from constraints on the beat mapping of segments. This beat mapping grammar is developed within the general framework of Generalized Correspondence Theory (McCarthy and Prince 2005), and exploits certain properties of correspondence relations, notably non-linearity and reciprocity (bidirectionality). The mapping of segments onto beats respects their linear order but does not reflect them: it is a <b>many-to-many</b> <b>mapping.</b> Correspondence also provides the basis for a new definition of "syllable," which rests on two things: the reciprocity of correspondence relations, and a principle of "salience matching" in mappings between non-homologous domains. by J. D. Hill. Thesis (S. M.) [...] Massachusetts Institute of Technology, Dept. of Linguistics and Philosophy, 2008. Includes bibliographical references (p. 89 - 91) ...|$|E
40|$|Computer-generated {{character}} animation, where {{human or}} anthropomorphic characters are animated {{to tell a}} story, holds tremendous potential to enrich education, human communication, perception, and entertainment. However, current animation procedures rely on a time consuming and difficult process that requires both artistic talent and technical expertise. Despite the tremendous amount of artistry, skill, and time dedicated to the animation process, there are few techniques to help with reuse. Although individual aspects of animation are well explored, there is little work that extends {{beyond the boundaries of}} any one area. As a consequence, the same procedure must be followed for each new character without the opportunity to generalize or reuse technical components. This dissertation describes techniques that ease the animation process by offering opportunities for reuse and a more intuitive animation formulation. A differential specification of arbitrary deformation provides a general representation for adapting deformation to different shapes, computing semantic correspondence between two shapes, and extrapolating natural deformation from a finite set of examples. (cont.) Deformation transfer adds a general-purpose reuse mechanism to the animation pipeline by transferring any deformation of a source triangle mesh onto a different target mesh. The transfer system uses a correspondence algorithm to build a discrete <b>many-to-many</b> <b>mapping</b> between the source and target triangles that permits transfer between meshes of different topology. Results demonstrate retargeting both kinematic poses and non-rigid deformations, as well as transfer between characters of different topological and anatomical structure. Mesh-based inverse kinematics extends the idea of traditional skeleton-based inverse kinematics to meshes by allowing the user to pose a mesh via direct manipulation. The user indicates the dass of meaningful deformations by supplying examples that can be created automatically with deformation transfer, sculpted, scanned, or produced by any other means. This technique is distinguished from traditional animation methods since it avoids the expensive character setup stage. It is distinguished from existing mesh editing algorithms since the user retains the freedom to specify the class of meaningful deformations. Results demonstrate an intuitive interface for posing meshes that requires only a small amount of user effort. by Robert Walker Sumner. Thesis (Ph. D.) [...] Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2005. This electronic version was submitted by the student author. The certified thesis is available in the Institute Archives and Special Collections. Includes bibliographical references (p. 117 - 131) ...|$|E
40|$|Like {{the stock}} market, cities, or ant colonies, {{language}} is a complex adaptive system (Holland 1992; Beckner et al. 2009; Bybee 2010). One of the properties of such systems is that they rely on what in biology is called ‘degeneracy’. In this context, degeneracy {{has nothing to do}} with its common sense meaning of deterioration, but is a technical term for the phenomenon that structurally different elements or strategies can fulfill the same function (see Edelman & Gally 2001). A simple example is thermoregulation in the human body, which is degenerately controlled by (a) goose bumps, (b) countercurrent flow, (c) transpiration, (d) arteriolar vasodilation, (e) walking upright to catch less solar radiation etc. Degeneracy differs from mere redundancy, as the different strategies have other functions as well, constituting a <b>many-to-many</b> <b>mapping</b> between form and function. Walking upright, for instance, does not only help in thermoregulation, but has other functions, like increasing the visual perimeter. An example of degeneracy in language is the expression of aorist past tense in Ancient Greek or Sanskrit by an augment e- and a sigmatic marker -s-, as in Greek é-lu-s-a ‘I unbound’. Note that the sigmatic marker also occurs elsewhere in the paradigm, for instance in the future lú-s-ō ‘I shall unbind’. In light of the growing idea that language change can be modeled by appealing to general evolutionary processes (Croft 2000; Ritt 2004; Mufwene 2008; Rosenbach 2008; Steels 2011), we may arrive at a better understanding of grammatical change by applying the concept of degeneracy to form-function change, where degeneracy may serve the function of making languages flexible to manage instability in times of syntactic change. Indeed, degeneracy has been shown to enhance robustness and evolvability of complex adaptive systems (see Whitacre and Bender 2010), including language (Van de Velde 2014; Winter 2014; Van de Velde & Fonteyn 2017). I will show how degeneracy plays out in (i) argument realisation, (ii) the inner syntax of the NP, (iii) interrogative mood marking. I will focus on West Germanic. Beckner, C., R. Blythe, J. Bybee, M. H. Christiansen, W. Croft, N. C. Ellis, J. Holland, J. Ke, D. Larsen-Freeman, T. SChoenemann. 2009. ‘Language is a complex adaptive system: position paper’. Language Learning 59 (S 1) : 1 - 26. Bybee, J. 2010. Language, usage, and cognition. Cambridge: CUP. Croft, W. 2000. Explaining language change: an evolutionary approach. Harlow: Longman. Edelman, G. M. & J. A. Gally. 2001. ‘Degeneracy and complexity in biological systems’. Proceedings of the National Academy of Sciences 98 (24) : 13763 - 13768. Holland J. 1992. ‘Complex adaptive systems’. Daedalus 121 (1) : 17 - 30. Mufwene, S. S. 2008. Language Evolution. Contact, Competition and Change. London: Continuum. Ritt, N. 2004. Selfish sounds. A Darwinian approach to language change. Cambridge: CUP. Rosenbach, A. 2008. ‘Language change as cultural evolution: evolutionary approaches to language change’. In: R. Eckhardt, G. Jäger, T. Veenstra (eds.), Variation, selection, development. Probing the evolutionary model of language change. Berlin: Mouton de Gruyter. 23 - 72. Steels, L. 2011. ‘Modeling the cultural evolution of language’. Physics of Life Review 8 : 339 - 356. Van de Velde, F. 2014. ‘Degeneracy: the maintenance of constructional networks’. In: R. Boogaart, T. Colleman & G. Rutten (eds.), The extending scope of construction grammar. Berlin: De Gruyter. 141 - 179. Van de Velde, Freek & Lauren Fonteyn. 2017. ‘Degeneracy. The evolutionary advantage of the violation of isomorphism’. SLE 50 Workshop Advances in diachronic Construction Grammar – Debating theoretical tenets and open questions. Zürich, 10 - 13 September 2017. Whitacre, J. & A. Bender. 2010. ‘Degeneracy: a design principle for achieving robustness and evolvability’. Journal of Theoretical Biology 263 : 143 - 153. Winter, B. 2014. ‘Spoken language achieves robustness and evolvability by exploiting degeneracy and neutrality’ Bioessays 36 : 960 - 967. status: publishe...|$|E
