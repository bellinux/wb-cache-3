4202|6947|Public
5|$|The {{length of}} the time step chosen within the model {{is related to the}} {{distance}} between the points on the computational grid, and is chosen to maintain numerical stability. Time steps for global models are on the order of tens of minutes, while time steps for regional models are between one and four minutes. The global models are run at varying times into the future. The Met Office's Unified Model is run six days into the future, the European Centre for Medium-Range Weather Forecasts model is run out to 10days into the future, while the Global Forecast System model run by the Environmental Modeling Center is run 16days into the future. The visual output produced by a model solution is known as a prognostic chart, or prog. The raw output is often modified before being presented as the forecast. This can be in the form of statistical techniques to remove known biases in the model, or of adjustment to take into account consensus among other numerical weather forecasts. MOS or <b>model</b> <b>output</b> statistics is a technique used to interpret numerical <b>model</b> <b>output</b> and produce site-specific guidance. This guidance is presented in coded numerical form, and can be obtained for nearly all National Weather Service reporting stations in the United States. As proposed by Edward Lorenz in 1963, long range forecasts, those made at a range of two weeks or more, are impossible to definitively predict the state of the atmosphere, owing to the chaotic nature of the fluid dynamics equations involved. In numerical models, extremely small errors in initial values double roughly every five days for variables such as temperature and wind velocity.|$|E
25|$|N. Gregory Mankiw, David Romer, and David Weil {{created a}} human capital {{augmented}} {{version of the}} Solow–Swan model that can explain the failure of international investment to flow to poor countries. In this <b>model</b> <b>output</b> and the marginal product of capital (K) are lower in poor countries because they have less human capital than rich countries.|$|E
25|$|The {{model is}} run by {{selecting}} a food consumption database, a chemical residue database and a population. Based on these selections the <b>model</b> <b>output</b> is the average daily intake of a chemical for an individual with the average diet for the selected population. A diet can be specified based on selecting core foods then that diet can be matched with residue data and used to estimate exposure.|$|E
40|$|The Bayesian {{computer}} model calibration method {{has proven to}} be effective {{in a wide range of}} applications. In this framework, input parameters are tuned by comparing <b>model</b> <b>outputs</b> to observations. However, this methodology becomes computationally expensive for large spatial <b>model</b> <b>outputs.</b> To overcome this challenge, we employ a truncated basis representations of the <b>model</b> <b>outputs.</b> We then aim to match the <b>model</b> <b>outputs</b> coefficients with the coefficients from observations in the basis representations; we also optimize the truncation level. In a second step, we enhance the calibration with the addition of the INLA-SPDE technique. We incorporate the nonstationary behavior and the derivative information of the spatial field into the calibration by inserting two INLA-SPDE parameters into the calibration. Several synthetic examples and a climate model illustration highlight the benefits of our approach for <b>model</b> <b>outputs</b> distributed over the plane or the sphere...|$|R
5000|$|Specify {{uncertain}} model {{parameters and}} <b>model</b> <b>outputs</b> of interest ...|$|R
5000|$|Using the {{resulting}} <b>model</b> <b>outputs,</b> calculate the sensitivity measures of interest.|$|R
500|$|Because {{forecast}} models {{based upon}} the equations for atmospheric dynamics do not perfectly determine weather conditions, statistical methods {{have been developed to}} attempt to correct the forecasts. [...] Statistical models were created {{based upon the}} three-dimensional fields produced by numerical weather models, surface observations and the climatological conditions for specific locations. [...] These statistical models are collectively referred to as <b>model</b> <b>output</b> statistics (MOS), and were developed by the National Weather Service for their suite of weather forecasting models in the late 1960s.|$|E
500|$|In {{the same}} way that many {{forecasts}} from a single model can be used to form an ensemble, multiple models may also be combined to produce an ensemble forecast. This approach is called multi-model ensemble forecasting, and it has been shown to improve forecasts when compared to a single model-based approach. [...] Models within a multi-model ensemble can be adjusted for their various biases, which is a process known as superensemble forecasting. [...] This type of forecast significantly reduces errors in <b>model</b> <b>output.</b>|$|E
500|$|The {{output of}} {{forecast}} models based on atmospheric dynamics {{is unable to}} resolve some details of the weather near the Earth's surface. As such, a statistical relationship between the output of a numerical weather model and the ensuing conditions at the ground {{was developed in the}} 1970s and 1980s, known as <b>model</b> <b>output</b> statistics (MOS). [...] Starting in the 1990s, model ensemble forecasts have been used to help define the forecast uncertainty and to extend the window in which numerical weather forecasting is viable farther into the future than otherwise possible.|$|E
500|$|UK {{storm surge}} <b>model</b> <b>outputs</b> and {{real-time}} tide gauge {{information from the}} ...|$|R
5000|$|What {{data sources}} and <b>model</b> <b>outputs</b> are {{available}} for quantification of these parameters? ...|$|R
3000|$|For this {{proof of}} concept study, the model {{identification}} process was tested with 46 sets of porcine data from five pigs with induced pulmonary embolism [23]. Relative percentage errors and accuracy and precision indices derived from absolute errors were calculated. The accuracy indices were derived by calculating {{the mean of the}} absolute error of the <b>model</b> <b>outputs</b> for each pig. The precision indices describe the 90 th percentile range of the absolute error of the <b>model</b> <b>outputs</b> for each pig. The relative and absolute errors were calculated by comparing the measured LVEDV and REVDV, and maximum left and right ventricular pressures to the <b>model</b> <b>outputs</b> (P [...]...|$|R
500|$|Manipulating {{the vast}} {{datasets}} and performing the complex calculations necessary to modern {{numerical weather prediction}} requires {{some of the most}} powerful supercomputers in the world. [...] Even with the increasing power of supercomputers, the forecast skill of numerical weather models extends to only about six days. Factors affecting the accuracy of numerical predictions include the density and quality of observations used as input to the forecasts, along with deficiencies in the numerical models themselves. [...] Post-processing techniques such as <b>model</b> <b>output</b> statistics (MOS) have been developed to improve the handling of errors in numerical predictions.|$|E
500|$|Because {{forecast}} models {{based upon}} the equations for atmospheric dynamics do not perfectly determine weather conditions near the ground, statistical corrections were developed to attempt to resolve this problem. Statistical models were created {{based upon the}} three-dimensional fields produced by numerical weather models, surface observations, and the climatological conditions for specific locations. [...] These statistical models are collectively referred to as <b>model</b> <b>output</b> statistics (MOS), and were developed by the National Weather Service for their suite of weather forecasting models by 1976. [...] The United States Air Force developed {{its own set of}} MOS based upon their dynamical weather model by 1983.|$|E
500|$|<b>Model</b> <b>output</b> {{statistics}} {{differ from}} the perfect prog technique, which assumes that the output of numerical weather prediction guidance is perfect. [...] MOS can correct for local effects that cannot be resolved by the model due to insufficient grid resolution, as well as model biases. [...] Because MOS is run after its respective global or regional model, its production is known as post-processing. [...] Forecast parameters within MOS include maximum and minimum temperatures, percentage chance of rain within a several hour period, precipitation amount expected, chance that the precipitation will be frozen in nature, chance for thunderstorms, cloudiness, and surface winds.|$|E
40|$|This paper {{describes}} {{the structure of}} the Cost Benefit Model of Diabetes Prevention and Care and lists the data sources used and assumptions embedded in the model. Agnes Walker, Stephen Colagiuri and Michele McLennan validate the model through checks of <b>model</b> <b>outputs</b> against data published by other organisations. They also discuss the sensitivity of <b>model</b> <b>outputs</b> to changes in certain key assumptions...|$|R
40|$|In this study, global (50 °S- 50 °N) {{distribution}} of water vapor is investigated using COSMIC GPS RO measurements. Detailed comparisons {{have been made}} between COSMIC and high resolution GPS radiosonde measurements across 13 tropical stations and <b>model</b> <b>outputs</b> (ERA-Interim, NCEP, and JRA- 25 reanalyses data sets). In comparison with independent techniques like radiosonde (Väisälä), {{it is found that}} COSMIC GPS RO wet profiles are accurate up to 7 - 8. km (assuming radiosonde as standard technique). In general, comparisons with corresponding seasonal means of <b>model</b> <b>outputs</b> are qualitatively in good agreement, although they differ quantitatively especially over convective regions of South America, Africa, and Indonesia. In tropical latitudes, the COSMIC specific humidity values are higher than the <b>model</b> <b>outputs.</b> Among various <b>model</b> <b>outputs,</b> ERA-Interim data set show near realistic features to that observed by COSMIC GPS RO measurements. Large asymmetry in the specific humidity distribution is observed between northern and southern hemispheres. © 2011 Elsevier Ltd...|$|R
40|$|Climate change {{adaptation}} {{is largely}} a local matter, and adaptation planning can benefit from local climate change projections. Such projections are typically generated by accepting climate <b>model</b> <b>outputs</b> in a relatively uncritical way. We argue, based on the IPCC’s treatment of <b>model</b> <b>outputs</b> from the CMIP 5 ensemble, that this approach is unwarranted and that subjective expert judgment should {{play a central role}} in the provision of local climate change projections intended to support decision-making...|$|R
500|$|Because {{the output}} of {{forecast}} models based on atmospheric dynamics requires corrections near ground level, <b>model</b> <b>output</b> statistics (MOS) were developed in the 1970s and 1980s for individual forecast points (locations). The MOS apply statistical techniques to post-process {{the output of}} dynamical models with the most recent surface observations and the forecast point's climatology. [...] This technique can correct for model resolution as well as model biases. [...] Even with the increasing power of supercomputers, the forecast skill of numerical weather models only extends to about two weeks into the future, since the density and quality of observationstogether with the chaotic nature of the partial differential equations {{used to calculate the}} forecastintroduce errors which double every five days. The use of model ensemble forecasts since the 1990s helps to define the forecast uncertainty and extend weather forecasting farther into the future than otherwise possible.|$|E
500|$|In the United States, {{the first}} {{operational}} regional model, the limited-area fine-mesh (LFM) model, {{was introduced in}} 1971. [...] Its development was halted, or frozen, in 1986. [...] The NGM debuted in 1987 and was also used to create <b>model</b> <b>output</b> statistics for the United States. [...] Its development was frozen in 1991. [...] The ETA model was implemented for the United States in 1993 and in turn was upgraded to the NAM in 2006. The U.S. also offers the Rapid Refresh (which replaced the RUC in 2012) for short-range and high-resolution applications; both the Rapid Refresh and NAM are built on the same framework, the WRF. Metéo France has been running their Action de Recherche Petite Échelle Grande Échelle (ALADIN) mesoscale model for France, based upon the ECMWF global model, since 1995. [...] In July 1996, the Bureau of Meteorology implemented the Limited Area Prediction System (LAPS). The Canadian Regional Finite-Elements model (RFE) went into operational use on April 22, 1986. [...] It {{was followed by the}} Canadian Global Environmental Multiscale Model (GEM) mesoscale model on February 24, 1997.|$|E
2500|$|... and a 3.6-liter flat-six [...] EZ36D {{are carried}} {{over from the}} {{fifth-generation}} <b>model.</b> <b>Output</b> is slightly increased on the four-cylinder model to [...] The six-cylinder model remains unchanged from the [...] 3.6-litre engine.|$|E
3000|$|<b>Model</b> <b>outputs</b> were {{compared}} with eddy covariance data during the growing season. The best model performance corresponded to runs with f [...]...|$|R
5000|$|Beginning with PAQ7, each <b>model</b> <b>outputs</b> a {{prediction}} (instead {{of a pair}} of counts). These predictions are averaged in the logistic domain: ...|$|R
30|$|Development of a metrics {{transformation}} that queries a model conforming to the architecture metamodel and creates measures and measurements in an <b>output</b> <b>model.</b> This <b>output</b> <b>model</b> is conforming to the metrics metamodel.|$|R
2500|$|<b>Model</b> <b>output</b> {{statistics}} {{differ from}} the perfect prog technique, which assumes that the output of numerical weather prediction guidance is perfect. [...] MOS can correct for local effects that cannot be resolved by the model due to insufficient grid resolution, as well as model biases. [...] Forecast parameters within MOS include maximum and minimum temperatures, percentage chance of rain within a several hour period, precipitation amount expected, chance that the precipitation will be frozen in nature, chance for thunderstorms, cloudiness, and surface winds.|$|E
2500|$|Because {{forecast}} models {{based upon}} the equations for atmospheric dynamics do not perfectly determine weather conditions near the ground, statistical corrections were developed to attempt to resolve this problem. Statistical models were created {{based upon the}} three-dimensional fields produced by numerical weather models, surface observations, and the climatological conditions for specific locations. [...] These statistical models are collectively referred to as <b>model</b> <b>output</b> statistics (MOS), and were developed by the National Weather Service for their suite of weather forecasting models. [...] The United States Air Force developed {{its own set of}} MOS based upon their dynamical weather model by 1983.|$|E
2500|$|Forecasts are {{computed}} using mathematical equations for {{the physics}} and {{dynamics of the}} atmosphere. [...] These equations are nonlinear and are impossible to solve exactly. Therefore, numerical methods obtain approximate solutions. [...] Different models use different solution methods. [...] Global models often use spectral methods for the horizontal dimensions and finite-difference methods for the vertical dimension, while regional models usually use finite-difference methods in all three dimensions. [...] For specific locations, <b>model</b> <b>output</b> statistics use climate information, output from numerical weather prediction, and current surface weather observations to develop statistical relationships which account for model bias and resolution issues.|$|E
50|$|In addition, while {{validity}} may {{be assessed}} with comparison between human {{data and the}} <b>model's</b> <b>output,</b> free parameters are flexible to incorrectly fit data.|$|R
50|$|This <b>model's</b> <b>output</b> {{is only as}} good as {{the data}} on which it is based and the LOS model it is used to correct.|$|R
5000|$|In PAQ7 and later, each <b>model</b> <b>outputs</b> a {{probability}} {{rather than a}} pair of counts. The probabilities are combined using an artificial neural network.|$|R
2500|$|Because {{the output}} of {{forecast}} models based on atmospheric dynamics requires corrections near ground level, <b>model</b> <b>output</b> statistics (MOS) were developed in the 1970s and 1980s for individual forecast points (locations). [...] Even with the increasing power of supercomputers, the forecast skill of numerical weather models only extends to about two weeks into the future, since the density and quality of observationsmdash&together with the chaotic nature of the partial differential equations {{used to calculate the}} forecastmdash&introduce errors which double every five days. [...] The use of model ensemble forecasts since the 1990s helps to define the forecast uncertainty and extend weather forecasting farther into the future than otherwise possible.|$|E
2500|$|The Environmental Data Server (EDS) {{collects}} {{and stores}} environmental {{information for use}} within SAROPS. [...] Local SAROPS servers around the United States request environmental information from the EDS based upon the area of interest. Different environmental products are cataloged on the server ranging from observational systems to modeling products. [...] Observations include sea surface temperature, air temperature, visibility, wave height, global/region tides and currents to name a few. [...] High-resolution <b>model</b> <b>output</b> from operational forecast models like the hybrid coordinate ocean model (HYCOM) and Global NRL Coastal Ocean (NCOM) provide temporally and spatially varying wind and current information. [...] Lastly, the EDS is capable of providing objective analysis tools and aggregation. [...] The list of available products is always changing as researchers in the Navy, local universities and research centers continually improve the accuracy and reliability of products and make them available on a consistent basis.|$|E
2500|$|Prior to SAROPS, SAR {{controllers}} in the U.S. Coast Guard {{used the}} Computer Assisted Search Planning (CASP) and Joint Automated Work Sheets (JAWS), which used dated search planning techniques and algorithms. [...] More specifically, CASP {{was based on}} old computing technology and JAWS was taken directly from pen and pencil techniques for shorter durations of drift in coastal environments. Environmental data consisted of low-resolution (1-degree latitude/longitude grid) wind and current information that was applied every 12 hours. For most areas, CASP used monthly-averaged current values while JAWS used one wind and current value during the SAR case. Neither system was capable of accessing timely high-resolution wind nor current <b>model</b> <b>output,</b> which was a significant disadvantage since {{one of the main}} components that determine the accuracy of the drift solution is the presence of precise and accurate wind and current information for the given area of interest.|$|E
30|$|The HIGH-TOOL {{model was}} {{subjected}} to an extensive validation and testing approach, consisting of: robustness tests {{to ensure that the}} model is capable of discovering invalid inputs; check of coherence of <b>model</b> <b>outputs</b> with the EU Reference Scenario 2013 (European Commission 2013); comparison of <b>model</b> <b>outputs</b> with the ASTRA-EC model; sensitivity tests to test the plausibility of the modules’ reactions on changes in input variables; and the conduction of case studies (see van Meijeren et al., 2016; Kiel et al. (2016 a)).|$|R
40|$|The {{increasing}} {{application of}} models in water quality management {{has pointed out}} {{the need for a}} correct estimation of input parameters involved; therefore, uncertainty analysis has become a relevant topic in water quality models application. The paper deals with sensitivity analysis for a river dissolved oxygen model; this analysis has been performed through two different approaches; the former is based on the estimation of <b>model</b> <b>outputs</b> standard deviation, while the latter searches for maximum and minimum values of <b>model</b> <b>outputs...</b>|$|R
30|$|In {{all these}} {{proposed}} scenarios, the optimal pumping rate and optimal pumping cost {{during the time}} steps of simulation are estimated from the predicted <b>model</b> <b>outputs.</b>|$|R
