260|10000|Public
5000|$|... #Subtitle level 3: Hebbian {{development}} of a <b>multilayer</b> <b>neural</b> <b>network</b> ...|$|E
5000|$|... "Extracting legal {{knowledge}} {{by means of}} <b>multilayer</b> <b>neural</b> <b>network.</b> Application to municipal jurisprudence." [...] avec L. Bochereau et P. Bourgine, in Proceedings of the 3rd ICAIL, June 25-28, 1991, Oxford, ACM, p. 288 ...|$|E
5000|$|A <b>multilayer</b> <b>neural</b> <b>network</b> {{model by}} Linsker, having local {{connections}} from each cell layer to the next, whose connection strengths develop {{according to a}} Hebbian rule, generates orientation-selective cells and orientation columns. The resulting columnar arrangement contains fractures and [...] "pinwheel" [...] singularities of the same types as those found experimentally.|$|E
25|$|Neocognitron, a {{hierarchical}} <b>multilayered</b> <b>neural</b> <b>network</b> proposed by Professor Kunihiko Fukushima in 1987, {{is one of}} the first Deep Learning <b>Neural</b> <b>Networks</b> models.|$|R
40|$|Abstract-This letter {{proposes a}} new type of neurons called mul-tithreshold {{quadratic}} sigmoidal neurons to improve the classification capability of <b>multilayer</b> <b>neural</b> <b>networks.</b> In cooperation with single-threshold quadratic sigmoidal neurons, the multithreshold quadratic sigmoidal neurons can be used to improve the classification capability of <b>multilayer</b> <b>neural</b> <b>networks</b> by a factor of four compared to committee machines and by a factor of two compared to the conventional sigmoidal multilayer perceptrons. I...|$|R
5000|$|Sabbatini, R.M.E.: A <b>multilayered</b> <b>neural</b> <b>network</b> for {{processing}} 2D tomographic images in neurosurgery. Proceed. Nuclear Science Symposium and Medical Imaging Conference, IEEE, 1992 ...|$|R
5000|$|The wake-sleep {{algorithm}} is an unsupervised learning algorithm for a stochastic <b>multilayer</b> <b>neural</b> <b>network.</b> The algorithm adjusts the parameters {{so as to}} produce a good density estimator. There are two learning phases, the “wake” phase and the “sleep” phase, which are performed alternately. It was first designed {{as a model for}} brain functioning using variational Bayesian learning. After that, the algorithm was adapted to machine learning. It {{can be viewed as a}} way to train a Helmholtz Machine ...|$|E
3000|$|..., the {{conditions}} (6)-(9) {{ensure that the}} switched <b>multilayer</b> <b>neural</b> <b>network</b> (3)-(4) is exponentially stable.|$|E
40|$|Hidden Markov model based various phoneme {{recognition}} {{methods for}} Bengali language is reviewed. Automatic phoneme recognition for Bengali language using <b>multilayer</b> <b>neural</b> <b>network</b> is reviewed. Usefulness of <b>multilayer</b> <b>neural</b> <b>network</b> over single layer neural network is discussed. Bangla phonetic feature table construction and enhancement for Bengali speech recognition is also discussed. Comparison among these methods is discussed. Comment: 6 page...|$|E
3000|$|... {{exponential}} stability {{criteria for}} switched <b>multilayer</b> dynamic <b>neural</b> <b>networks.</b> These sets of sufficient stability criteria in forms of linear matrix inequality (LMI) and matrix norm are presented, under which switched <b>multilayer</b> dynamic <b>neural</b> <b>networks</b> reduce {{the effect of}} external input to a predefined level. The proposed sets of criteria also guarantee exponential stability for switched <b>multilayer</b> dynamic <b>neural</b> <b>networks</b> without external input.|$|R
3000|$|... {{exponential}} stability {{criteria for}} switched <b>multilayer</b> dynamic <b>neural</b> <b>networks.</b> These sets of sufficient stability criteria {{are represented by}} matrix norm and LMI. The proposed sets of criteria ensured that switched <b>multilayer</b> dynamic <b>neural</b> <b>networks</b> attenuate the effect of external input on the state vector. These sets of criteria also guaranteed exponential stability for switched <b>multilayer</b> dynamic <b>neural</b> <b>networks</b> {{when there is no}} external input.|$|R
40|$|Abstract—The Al-Alaoui {{algorithm}} is a weighted mean-square error (MSE) approach to pattern recognition. It employs cloning of the erro-neously classified samples {{to increase the}} population of their corresponding classes. The algorithm was originally developed for linear classifiers. In this paper, the {{algorithm is}} extended to <b>multilayer</b> <b>neural</b> <b>networks</b> which {{may be used as}} nonlinear classifiers. It is also shown that the application of the Al-Alaoui algorithm to <b>multilayer</b> <b>neural</b> <b>networks</b> speeds up the conver-gence of the back-propagation algorithm. Index Terms—Al-Alaoui algorithm, back-propagation algorithm, Bayes classifier, character recognition, Levenberg–Marquardt algorithm, neura...|$|R
3000|$|... {{exponential}} stability {{criterion of}} the switched <b>multilayer</b> <b>neural</b> <b>network</b> (3)-(4) is derived {{in the following}} theorem.|$|E
40|$|In {{this paper}} {{we use a}} <b>multilayer</b> <b>neural</b> <b>network</b> (MNN) {{constructed}} by a Monte Carlo based algorithm to forecast time series events. Experiments are carried out on two benchmark problems in time series forecasting literature. Our result shows a comparative performance with other prediction methods. In our approach, a <b>multilayer</b> <b>neural</b> <b>network</b> with high level of generalization ability is obtained without sensible choice of external parameters...|$|E
3000|$|..., {{a set of}} LMI {{conditions}} (19)-(20) {{ensure that}} the switched <b>multilayer</b> <b>neural</b> <b>network</b> (3)-(4) is exponentially stable.|$|E
40|$|The {{relationship}} between the geometrical structure of weight space and replica symmetry breaking (RSB) in <b>multilayer</b> <b>neural</b> <b>networks</b> is studied using a toy model. The distribution of sizes of the disconnected domains of solution space is computed analytically and compared to the RSB calculation of the Gardner volume. We are able to show explicitly that ergodicity breaking and RSB are not equivalent. Repeating these calculations using the cavity approach allows us to interpret the geometrical meaning of a RSB ansatz. It is well-known [1] that a full study of the storage properties of <b>multilayer</b> <b>neural</b> <b>networks</b> within the Gardner framework [2] requires the breaking of replica symmetry [3]. This differs from {{the case of the}} simple perceptron, where a replica symmetric solution {{has been shown to be}} exact, and where the spin glass order parameter which emerges has a simple geometrical meaning. In <b>multilayer</b> <b>neural</b> <b>networks,</b> replica symmetry breaking is believed to reflect the existence of a [...] ...|$|R
50|$|<b>Multilayer</b> <b>neural</b> <b>networks</b> {{can be used}} {{to perform}} feature learning, since they learn a {{representation}} of their input at the hidden layer(s) which is subsequently used for classification or regression at the output layer.|$|R
40|$|It {{is shown}} how a <b>neural</b> <b>network</b> can learn {{of its own}} accord to control a {{nonlinear}} dynamic system. An emulator, a <b>multilayered</b> <b>neural</b> <b>network,</b> learns to identify the system's dynamic characteristics. The controller, another <b>multilayered</b> <b>neural</b> <b>network,</b> next learns to control the emulator. The self-trained controller is then used to control the actual dynamic system. The learning process continues as the emulator and controller improve and track the physical process. An example is given to illustrate these ideas. The 'truck backer-upper,' a <b>neural</b> <b>network</b> controller that steers a trailer truck while the truck is backing up to a loading dock, is demonstrated. The controller is able to guide the truck to the dock from almost any initial position. The technique explored should be applicable {{to a wide variety of}} nonlinear control problems...|$|R
3000|$|... {{exponential}} {{stability of}} the switched <b>multilayer</b> <b>neural</b> <b>network</b> (3)-(4). This set of LMI criteria can be facilitated readily via standard numerical algorithms [21, 22].|$|E
40|$|Text {{categorization}} - {{the assignment}} of natural language documents {{to one or more}} predefined categories based on their semantic content- is an important component in many information organization and management tasks. Performance of neural networks learning is known {{to be sensitive to the}} initial weights and architecture. This paper discusses the use <b>multilayer</b> <b>neural</b> <b>network</b> initialization with decision tree classifier for improving text categorization accuracy. An adaptation of the algorithm is proposed in which a decision tree from root node until a final leave is used for initialization of <b>multilayer</b> <b>neural</b> <b>network.</b> The experimental evaluation demonstrates this approach provides better classification accuracy with Reuters- 21578 corpus, one of the standard benchmarks for text categorization tasks. We present results comparing the accuracy of this approach with <b>multilayer</b> <b>neural</b> <b>network</b> initialized with traditional random method and decision tree classifiers...|$|E
40|$|Abstract: A Burg {{technique}} is employed {{to model the}} long wavelength localization and imaging problem. A Burg method {{is used as a}} high resolution and stable technique. The idea of in-line holography is used to increase the ratio of the signal to noise due to the effect of concealing media that decreases the value of the received signal. The performance is enhanced by using <b>multilayer</b> <b>neural</b> <b>network</b> for noise reduction. The aim of using <b>multilayer</b> <b>neural</b> <b>network</b> is to extract the essential knowledge from a noisy training data. Theoretical and experimental results have showed that preprocessing the noisy data with <b>multilayer</b> <b>neural</b> <b>network</b> will decrease the effect of noise as much as possible. Applying the enhanced data to spectral estimation methods has improved the performance of the model. A comparison is made for the two cases: with and without application of neural network for different values of signal to noise ratio. Also, the performance is investigated for different numbers of samples...|$|E
40|$|An {{algorithm}} {{for constructing}} and training <b>multilayer</b> <b>neural</b> <b>networks,</b> dependence identification, {{is presented in}} this paper. Its distinctive features are that (i) it transforms the training problem into a set of quadratic optimization problems that are solved {{by a number of}} linear equations and (ii) it constructs an appropriate network to meet the training specifications. The architecture and network weights produced by the algorithm can also be used as initial conditions for further on-line training by backpropagation or a similar iterative gradient descent training algorithm if necessary. In addition to constructing an appropriate network based on training data, the dependence identification algorithm significantly speeds up learning in feedforward <b>multilayer</b> <b>neural</b> <b>networks</b> compared to standard backpropagation. ...|$|R
40|$|Guidelines for {{the design}} of <b>multilayer</b> <b>neural</b> <b>networks</b> for the {{identification}} of nonlinear mappings are considered. Since nonlinear mappings can be approximated by a one-hidden-layer <b>neural</b> <b>network,</b> an approach to determine the sufficient number of hidden layer nodes to achieve a global minima of the identification error function is considered...|$|R
40|$|Abstract — In this paper, {{we present}} an {{efficient}} technique for mapping a backpropagation (BP) learning algorithm for <b>multilayered</b> <b>neural</b> <b>networks</b> onto {{a network of}} workstations (NOW’s). We adopt a vertical partitioning scheme, where each layer in the <b>neural</b> <b>network</b> is divided into � disjoint partitions, and map each partition onto an independent workstation in a network of � workstations. We present a fully distributed version of the BP algorithm and also its speedup analysis. We compare the performance of our algorithm with a recent work involving the vertical partitioning approach for mapping the BP algorithm onto a distributed memory multiprocessor. Our results on SUN 3 / 50 NOW’s show {{that we are able}} to achieve better speedups by using only two communication sets and also by avoiding some redundancy in the weights computation for one training cycle of the algorithm. Index Terms — Backpropagation algorithm, distributed memory multiprocessors, <b>multilayered</b> <b>neural</b> <b>networks,</b> network of workstations, network partitioning, pattern partitioning, performance analysis. I...|$|R
30|$|In this study, a {{feed-forward}} back-propagation <b>multilayer</b> <b>neural</b> <b>network</b> was used. The back-propagation training algorithm technique {{adjusts the}} weights to obtain network that is {{closed to the}} desired output [32].|$|E
40|$|A new {{derivation}} {{is presented}} for the bounds {{on the size}} of a <b>multilayer</b> <b>neural</b> <b>network</b> to exactly implement an arbitrary training set; namely, the training set can be implemented with zero error with two layers and with the number of the hidden-layer neurons equal to no. 1 is greater than p - 1. The derivation does not require the separation of the input space by particular hyperplanes, as in previous derivations. The weights for the hidden layer can be chosen almost arbitrarily, and the weights for the output layer can be found by solving no. 1 + 1 linear equations. The method presented exactly solves (M), the <b>multilayer</b> <b>neural</b> <b>network</b> training problem, for any arbitrary training set...|$|E
40|$|Abstract:- This paper proposes an {{evolutionary}} design methodology of multilayer feedforward neural networks based on constructive approach. We elaborate an adjustable processing element as primitive neuron model. The neural layer {{can be constructed}} by assembling several neurons. The <b>multilayer</b> <b>neural</b> <b>network</b> can be finally constructed through cascading several neural layers. The constructive approach facilitates substantially to extract design specifications from a <b>multilayer</b> <b>neural</b> <b>network.</b> Based on the constructive representation of multilayer feedforward neural networks, we use a genetic encoding method, after which the evolution process is elaborated for designing the optimal neural network. The results of our experiments reveal that our methodology is superior to the error back-propagation algorithm both for its executing efficiency and performance...|$|E
40|$|Abstract: Artificial <b>neural</b> <b>networks</b> one of {{the basic}} {{directions}} of the modern theory of an artificial intelligence. In the first part of work biological preconditions of creation of the theory of artificial <b>neural</b> <b>networks</b> are stated. It is defined formal neuron. Single-layered and <b>multilayered</b> <b>neural</b> <b>networks</b> are considered. The basic theorem for approximation of functions by means of <b>multilayered</b> <b>neural</b> <b>networks</b> is resulted and the base algorithm of training of <b>neural</b> <b>networks</b> is described. General characteristics of the relaxing and self-organizing <b>neural</b> <b>networks</b> are given. The second part of work is devoted to application of artificial <b>neural</b> <b>networks</b> in various areas. The example of movement of the mobile robot {{on the basis of the}} rangefinder’s data is resulted. Recognition during movement of the mobile robot of a local relative attributes is considered, and application self-organizing maps at distribution of financings of institute is also described. Note: Publication language:russia...|$|R
40|$|There {{are many}} {{successful}} applications of Backpropagation (BP) for training <b>multilayer</b> <b>neural</b> <b>networks.</b> However, {{they have many}} shortcomings. Learning often takes insupportable time to converge, and it may fall into local minima at all. One of the possible remedies to escape from local minima is using a very small learning rate, but this will slow the learning process. The pro-posed algorithm is presented for the training of <b>multilayer</b> <b>neural</b> <b>networks</b> with very small learn-ing rate, especially when using large training set size. It can apply in a generic manner for any network size that uses a backpropgation algorithm through optical time. This paper studies {{the performance of the}} Optical Backpropagation algorithm OBP (Otair & Salameh, 2004 a, 2004 b. 2005) on training a <b>neural</b> <b>network</b> for online handwritten character recognition in comparison with backpropagation BP...|$|R
40|$|A {{new method}} for {{training}} <b>multilayer</b> <b>neural</b> <b>networks</b> has been developed. This method combines {{the speed of}} a least squares approach with the iterative nature of backpropagation. This method converges quickly, typically within 10 iterations, where back propagation can take tens of thousands of iterations...|$|R
40|$|The paper compares {{global and}} local {{approximation}} methods used for walking robot stability model. Global approximators {{are represented by}} feedforward <b>multilayer</b> <b>neural</b> <b>network</b> (FFNN) trained by gradient method; local approximators are represented by Locally Weighted Regression (LWR) and Receptive Field Weighted Regression (RFWR) methods...|$|E
40|$|Abstract — {{this paper}} proposes a novel {{adaptive}} method to improve relevance feedback procedure in content based image retrieval. First, we transform low-level features to high-level ones {{by means of}} a <b>multilayer</b> <b>neural</b> <b>network</b> and these features are employed as the input of a radial basis function network for relevance feedback. This approach reduces the semantic gap and feature dimensionality considerably. In low-level into high-level feature transformation, we employ one thousand images of Corel database in training phase and, ten thousands images from the same database to test the relevance feedback. The experimental result shows the improvement of precision rate due to fast convergence after third iteration. Index Terms — Relevance feedback, <b>Multilayer</b> <b>neural</b> <b>network,</b> content-based image retrieval, radial basis function networ...|$|E
40|$|Abstract: In {{this paper}} {{we present a}} new method for voice {{disorders}} classification based on <b>multilayer</b> <b>neural</b> <b>network.</b> The processing algorithm {{is based on a}} hybrid technique which uses the wavelets energy coefficients as input of the <b>multilayer</b> <b>neural</b> <b>network.</b> The training step uses a speech database of several pathological and normal voices collected from the national hospital “Rabta- Tunis ” and was conducted in a supervised mode for discrimination of normal and pathology voices and in a second step classification between neural and vocal pathologies (Parkinson, Alzheimer, laryngeal, dyslexia…). Several simulation results will be presented in function of the disease and will be compared with the clinical diagnosis in order to have an objective evaluation of the developed tool...|$|E
40|$|In this paper, two {{artificial}} <b>neural</b> <b>networks</b> models, {{namely the}} <b>multilayer</b> feedforward <b>neural</b> <b>network</b> and the recurrent <b>neural</b> <b>network</b> are applied for Malaysia's load forecasting. A half hourly load data is divided equally into three distinct sets for training, validation and testing. Backpropagation is {{selected as the}} learning algorithm whereas the transfer function for both hidden layer and output layer is sigmoid the function. The forecasting performances were compared between these two models. The results show that, the sum squared error (SSE) of <b>multilayer</b> feedforward <b>neural</b> <b>network</b> were the lowest hence the <b>multilayer</b> feedforward <b>neural</b> <b>network</b> is a better model for a half hourly Malaysia's load...|$|R
40|$|This paper {{investigates the}} {{dynamics}} of batch learning in <b>multilayer</b> <b>neural</b> <b>networks.</b> First, we present experimental results on the behavior in the steepest descent learning of multilayer perceptrons and linear <b>neural</b> <b>networks.</b> From the results of both models, we see that strong overtraining, the increase of generalization error, occurs in overrealizable cases where the target function is realized by {{a smaller number of}} hidden units than the model. Next, under the assumption of asymptotical limit, we mathematically prove the existence of overtraining in overrealizable cases of linear <b>neural</b> <b>networks.</b> From this theoretical analysis, we know that the overtraining is not a feature observed in the final stage of learning, but it occurs in the intermediate interval of time and forms the global shape of a learning curve. 1 Introduction This paper discusses {{the dynamics of}} batch learning in <b>multilayer</b> <b>neural</b> <b>networks.</b> <b>Multilayer</b> networks like multilayer perceptrons have been extensively used [...] ...|$|R
40|$|The {{proposed}} {{model is}} based upon the Wavelet Transform to extract the features of disturbance signals and is found to detect disturbance correctly even in the presence of noise. Classification of voltage disturbances such as sag, swell, interruption and harmonics is performed with <b>multilayer</b> and modular <b>neural</b> <b>network.</b> Modular <b>neural</b> <b>network</b> has given better classification accuracy and reduced training time by using less number of hidden layer nodes compared wavelet based traditional <b>multilayer</b> <b>neural</b> <b>networks.</b> Simulation and experimental results verify that W-transform based modular <b>neural</b> <b>network</b> has correctly classified and characterized the disturbances [...] keywords – wavelet transform, <b>neural</b> <b>network,</b> faults. I...|$|R
