3448|284|Public
5|$|A 2002 {{research}} paper on <b>machine</b> <b>vision</b> by computer scientists Roman Goldenberg, Ron Kimmel, Ehud Rivlin, and Michael Rudzsky used Futurism's techniques of motion, as embodied by Dynamism of a Dog on a Leash, {{to illustrate the}} mathematical representation of periodic motion using {{a small number of}} eigenshapes.|$|E
5|$|Specific {{instances}} include removing {{of parts}} from the mould {{immediately after the}} parts are created, as well as applying <b>machine</b> <b>vision</b> systems. A robot grips the part after the ejector pins have been extended to free the part from the mould. It then moves them into either a holding location or directly onto an inspection system. The choice depends upon the type of product, {{as well as the}} general layout of the manufacturing equipment. Vision systems mounted on robots have greatly enhanced quality control for insert moulded parts. A mobile robot can more precisely determine the placement accuracy of the metal component, and inspect faster than a human can.|$|E
5|$|Lasers {{emitting}} in {{the green}} part of the spectrum are widely available {{to the general public}} {{in a wide range of}} output powers. Green laser pointers outputting at 532nm (563.5 THz) are relatively inexpensive compared to other wavelengths of the same power, and are very popular due to their good beam quality and very high apparent brightness. The most common green lasers use diode pumped solid state (DPSS) technology to create the green light. An infrared laser diode at 808nm is used to pump a crystal of neodymium-doped yttrium vanadium oxide (Nd:YVO4) or neodymium-doped yttrium aluminium garnet (Nd:YAG) and induces it to emit 281.76 THz (1064nm). This deeper infrared light is then passed through another crystal containing potassium, titanium and phosphorus (KTP), whose non-linear properties generate light at a frequency that is twice that of the incident beam (563.5 THz); in this case corresponding to the wavelength of 532nm ("green"). Other green wavelengths are also available using DPSS technology ranging from 501nm to 543nm. Green wavelengths are also available from gas lasers, including the helium–neon laser (543nm), the Argon-ion laser (514nm) and the Krypton-ion laser (521nm and 531nm), as well as liquid dye lasers. Green lasers have a wide variety of applications, including pointing, illumination, surgery, laser light shows, spectroscopy, interferometry, fluorescence, holography, <b>machine</b> <b>vision,</b> non-lethal weapons and bird control.|$|E
60|$|Nam-Bok {{thought of}} a {{combined}} harvester, and of the <b>machines</b> wherein <b>visions</b> of living men were to be seen, and of the machines from which came the voices of men, and he knew his people could never understand.|$|R
5000|$|Kulpa, Zenon. [...] "Diagrammatic {{representation}} and reasoning." [...] <b>Machine</b> GRAPHICS & <b>VISION</b> 3 (1/2. 1994.|$|R
5000|$|Schwan, Andrea “Construction Begins On Architect Jean Nouvel’s <b>Vision</b> <b>Machine</b> Along Manhattan’s West Side” nouvelchelsea.com ...|$|R
25|$|Applications {{range from}} {{tasks such as}} {{industrial}} <b>machine</b> <b>vision</b> systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and <b>machine</b> <b>vision</b> fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. <b>Machine</b> <b>vision</b> usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications.|$|E
25|$|<b>Machine</b> <b>vision</b> is {{the process}} of {{applying}} a range of technologies & methods to provide imaging-based automatic inspection, process control and robot guidance in industrial applications. <b>Machine</b> <b>vision</b> tends to focus on applications, mainly in manufacturing, e.g., vision based robots and systems for vision based inspection or measurement. This implies that image sensor technologies and control theory often are integrated with the processing of image data to control a robot and that real-time processing is emphasised by means of efficient implementations in hardware and software. It also implies that the external conditions such as lighting can be and are often more controlled in <b>machine</b> <b>vision</b> than they are in general computer vision, which can enable the use of different algorithms.|$|E
25|$|Yakhnenko, O., and Honavar, V. (2011). Multi-Instance Multi-Label Learning for Image Classification with Large Vocabularies. In: Proceedings of the British <b>Machine</b> <b>Vision</b> Conference.|$|E
50|$|Noisy ReLUs {{have been}} used with some success in {{restricted}} Boltzmann <b>machines</b> for computer <b>vision</b> tasks.|$|R
5000|$|Kulpa, Zenon. [...] "Diagrammatic {{representation}} for a {{space of}} intervals." [...] <b>Machine</b> Graphics & <b>Vision.</b> 1997.|$|R
50|$|The {{first and}} only company that Clemens sent prototypes to was Galoob. They loved it and {{wanted to go to}} {{contract}} immediately. Saul Jodell and David Galoob were the masterminds behind marketing Micro <b>Machines.</b> Their <b>vision</b> at Galoob drove Micro Machines to become best-selling toys.|$|R
25|$|A second {{application}} area {{in computer}} vision is in industry, sometimes called <b>machine</b> <b>vision,</b> where information is extracted {{for the purpose}} of supporting a manufacturing process. One example is quality control where details or final products are being automatically inspected in order to find defects. Another example is measurement of position and orientation of details to be picked up by a robot arm. <b>Machine</b> <b>vision</b> is also heavily used in agricultural process to remove undesirable food stuff from bulk material, a process called optical sorting.|$|E
25|$|Active in {{the field}} of Artificial Neural Networks since 1989. Current {{research}} programmes within the group are focused on the improvement of man-machine-interfaces, robot-force-control, eye-tracking experiments, <b>machine</b> <b>vision,</b> virtual reality and distributed systems.|$|E
25|$|Theories and {{observations}} of visual perception {{have been the}} main source of inspiration for computer vision (also called <b>machine</b> <b>vision,</b> or computational vision). Special hardware structures and software algorithms provide machines with the capability to interpret the images coming from a camera or a sensor. Artificial Visual Perception has long been used in the industry and is now entering the domains of automotive and robotics.|$|E
40|$|This paper {{reports the}} error model, the {{identifiability}} study of its parameters, its software simulation and the experimental validation used {{to compensate the}} error of a triangulation <b>vision</b> <b>machine</b> (AVS-Active <b>Vision</b> System) intended for monitoring and diagnosing monuments and other Cultural Heritage. A careful alignment is required but not sufficient to achieve the sought performance. In addition, operation in open air – where temperature fluctuations up to ± 15 K are expected - and transportation would require realignment on the spot. As the errors resulting from misalignment are highly repeatable, they are best compensated in software...|$|R
50|$|A {{technology}} {{paper and}} source code {{for the computer}} <b>vision</b> <b>machine</b> learning component of the 2005 Stanford entry has been published.|$|R
40|$|Vision is simple. We {{open our}} eyes and, instantly, the world {{surrounding}} us is perceived {{in all its}} splendor. Yet Artificial Intelligence has been trying with very limited success for over 20 years to endow machines with similar abilities. A large van, filled with computers and driving unguided at a mile per hour across gently sloping hills in Colorado and using a laser-range system to “see” is the most we have accomplished so far. On the other hand, computers can play a decent game of chess or prove simple mathematical theorems. It is ironic that {{we are unable to}} reproduce perceptual abilities which we share with most animals while some of the features distinguishing us from even our closest cousins, chimpanzees, can be carried out by <b>machines.</b> <b>Vision</b> is difficult. ...|$|R
25|$|Industrial {{automation}} deals {{primarily with}} the automation of manufacturing, quality control and material handling processes. General purpose controllers for industrial processes include Programmable logic controllers, stand-alone I/O modules, and computers. Industrial automation is {{to replace the}} decision making of humans and manual command-response activities {{with the use of}} mechanized equipment and logical programming commands. One trend is increased use of <b>Machine</b> <b>vision</b> to provide automatic inspection and robot guidance functions, another is a continuing increase in the use of robots. Industrial automation is simply done at the industrial level.|$|E
25|$|Others In addition, machine {{operators}} {{often use}} user interface devices, typically touchscreen units, which {{serve as the}} operator control panel. The operator can switch from program to program, make adjustments within a program and also operate a host of peripheral devices that may be integrated within the same robotic system. These include end effectors, feeders that supply components to the robot, conveyor belts, emergency stop controls, <b>machine</b> <b>vision</b> systems, safety interlock systems, bar code printers and an almost infinite array of other industrial devices which are accessed and controlled via the operator control panel.|$|E
25|$|The fields {{most closely}} related to {{computer}} vision are image processing, image analysis and <b>machine</b> <b>vision.</b> There is a significant overlap {{in the range of}} techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented.|$|E
30|$|Xueping Liu {{received}} his M.S. {{degree from the}} Shenyang Aerospace University, in 2010. He is currently a graduate student studying for Ph.D. degree in the College of Automation Engineering, Nanjing University of Aeronautics and Astronautics. He has published over 10 technical research papers. His research interests include <b>machine</b> learning, <b>vision</b> analysis, and pattern recognition.|$|R
40|$|International audienceThe {{principles}} of modern architecture expounded by Le Corbusier are still reinterpreted. The "architectural promenade" {{and the visual}} link to the landscape inspired projects by A. Siza and R. Koolhaas as <b>machines</b> of <b>vision.</b> Their exceptional locations in open landscapes lead to a design based on visual appearances, framings and points of view...|$|R
40|$|Autonomous robots, {{knowledge}} representation and reasoning, human-robot collaboration, <b>machine</b> learning, computer <b>vision,</b> and applied cognitive science. My primary {{research interests include}} {{knowledge representation}} and reasoning, <b>machine</b> learning, computer <b>vision,</b> and cognitive science as applied to autonomous mobile robots. The objective is to enable mobile robots to collaborate with non-expert human participants, acquiring and using sensor inputs and high-level human feedback based on need and availability. Furthermore, I am interested in designing learning and inference algorithms for domains characterized by {{a significant amount of}} uncertainty...|$|R
25|$|M-theory {{is based}} on a {{quantitative}} theory of the ventral stream of visual cortex. Understanding how visual cortex works in object recognition is still a challenging task for neuroscience. Humans and primates are able to memorize and recognize objects after seeing just couple of examples unlike any state-of-the art <b>machine</b> <b>vision</b> systems that usually require a lot of data in order to recognize objects. Prior to the use of visual neuroscience in computer vision has been limited to early vision for deriving stereo algorithms (e.g.,) and to justify the use of DoG (derivative-of-Gaussian) filters and more recently of Gabor filters. No real attention has been given to biologically plausible features of higher complexity. While mainstream computer vision has always been inspired and challenged by human vision, it seems to have never advanced past the very first stages of processing in the simple cells in V1 and V2. Although some of the systems inspired - to various degrees - by neuroscience, have been tested on at least some natural images, neurobiological models of object recognition in cortex have not yet been extended to deal with real-world image databases.|$|E
500|$|This {{paradigm}} led to innovative work in <b>machine</b> <b>vision</b> by Gerald Sussman (who led the team), Adolfo Guzman, David Waltz (who invented [...] "constraint propagation"), {{and especially}} Patrick Winston. At the same time, Minsky and Papert built a robot arm that could stack blocks, bringing the blocks world to life. The crowning {{achievement of the}} micro-world program was Terry Winograd's SHRDLU. It could communicate in ordinary English sentences, plan operations and execute them.|$|E
2500|$|T. Bouwmans, L. Davis, J. Gonzalez, M. Piccardi, C. Shan, Special Issue on [...] "Background Modeling for Foreground Detection in Real-World Dynamic Scenes", Special Issue in <b>Machine</b> <b>Vision</b> and Applications, July 2014.|$|E
5000|$|Linking {{computational}} action, perception, representation, {{transformation and}} generation processes to real or virtual worlds: statistical <b>machine</b> learning, computer <b>vision,</b> mobile and humanoid robotics, motor control, graphics and visualization.|$|R
50|$|The <b>machine</b> tool {{industry}} <b>vision</b> is {{to develop}} technologies for future and raise production level to reduce import dependence, counter technology denials, provide sustained manufacturing competitiveness and strengthen national security.|$|R
5000|$|History and Histrionics: <b>Vision</b> <b>Machine's</b> Digital Poetics. (With Michael Uwemedimo, co-author). In: Marchessault, Janine and Lord, Susan, (eds.) Fluid screens, {{expanded}} cinema. University of Toronto Press, 2007, Toronto, Canada, pp. 167-183[...]|$|R
2500|$|The {{classical}} {{problem in}} computer vision, image processing, and <b>machine</b> <b>vision</b> {{is that of}} determining {{whether or not the}} image data contains some specific object, feature, or activity. Different varieties of the recognition problem are described in the literature: ...|$|E
2500|$|Anderson is {{a member}} of the British Computer Society, the British <b>Machine</b> <b>Vision</b> Association, Eurographics, and the British Society for the Philosophy of Science. He is also a teacher in the Computer Science {{department}} (School of Systems Engineering) at the University of Reading. [...] He was ...|$|E
2500|$|An AI {{accelerator}} is (as of 2016) {{an emerging}} class of microprocessor [...] or computer {{system designed to}} accelerate artificial neural networks, <b>machine</b> <b>vision</b> and other machine learning algorithms for robotics, internet of things and other data-intensive or sensor-driven tasks. They are sometimes manycore designs (mirroring the massively-parallel nature of biological neural networks). Many vendor-specific terms exist for devices in this space.|$|E
5000|$|... 1983 - Moved the Tokyo {{office in}} Chuo-ku, Tokyo Nihonbashihoridome town. Founded the Nichibutsu Sapporo Co., Ltd. {{to the island}} in Sapporo, Hokkaido Toyohiraku District. Founded the Nichibutsu Sendai Co., Ltd. in Sendai City, Miyagi Prefecture Uesugi. Founded the Nichibutsu Hiroshima Co., Ltd. in Hiroshima, Hiroshima Prefecture, Minami-ku, Higashikasumi town. Release of home-use game <b>machine</b> My <b>Vision,</b> and {{embarked}} on software supply to the home-use game machine.|$|R
40|$|For both {{biological}} systems and <b>machines,</b> <b>vision</b> {{begins with a}} large and unwieldy array of measurements {{of the amount of}} light reflected from surfaces in the environment. The goal of vision is to recover physical properties of objects in the scene, such as the location of object boundaries and the structure, color and texture of object surfaces, from the two-dimensional image that is projected onto the eye or camera. This goal is not achieved in a single step; vision proceeds in stages, with each stage producing increasingly more useful descriptions of the image and then the scene. The first clue about the physical properties of the scene are provided by the changes of intensity in the image. The importance of intensity changes and edges in early visual processg has led to extensive research on their detection, description and. use, both in computer and biological vision systems. This article reviews some of the theory that underlies the detection of edges, and the methods used to carry out this analysis...|$|R
40|$|Visions {{from the}} economy of waste is a {{collection}} of interactive media pieces that explores what happens when a human by-product becomes a point of convergence between humans and <b>machines.</b> <b>Visions</b> starts with a simple premise: In the near future, technology finds a way to store data in human feces. Cheaper to produce and infinitely renewable, shit replaces computer hard drives, CD-ROMs, and floppy disks as the data storage option of choice. Technology transforms shit from human waste to digital necessity, and makes it a focal point in the information economy. Each of the nine media art pieces explores a particular facet of the future use value of shit. Each piece is based on a fictional character working and living in the new economy of waste. Each piece is created with a specific media in mind that appropriately expresses, in form, content and interactivity, the narrative arc of the fictional characters. When finished, this project is staged as an installation that resemble an exhibit at a natural history museum, documenting the social, political, and technological genealogy of the shit to come...|$|R
