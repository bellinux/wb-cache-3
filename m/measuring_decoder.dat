2|11|Public
40|$|This essay {{shows up}} the {{monitoring}} and error measuring in transportation stream MPEG- 2 TS during the digital TV broadcat recieving accordingly to the DVB standard. There are DVB {{definitions and standards}} for transportation stream transmition, decoding conditions and service informations chart described in the introduction of this essay. The <b>measuring</b> <b>decoder</b> R&S DVMD {{has been used for}} experimental measuring in the laboratory. This decoder enables long-time monitoring and automatized errors evaluation of particular priorities. The processed results of DVB-T multiplex 1 and 2 and DVB-S, paket CS link, TV broadcat recieving are the part of this essay as well...|$|E
40|$|This bachelor’s thesis {{describes}} {{structure of}} the transport stream MPEG- 2 TS of the DVB-T standard and subjective and objective measuring methods of the video quality. The work also includes {{the description of the}} measurement devices and software Quality Analyzer R&S DVQ, <b>measuring</b> <b>decoder</b> ¸ DVMD, MPEG- 2 Realtime Monitor, MPEG- 2 Quality Monitor, which were used for the measuring the video quality and errors of the MPEG- 2 TS in DVB-T. In the next part the work deals with the measurement and analysis of errors and quality of digital video stream in MPEG- 2 TS. The obtained results from the measurement are presented in tables and graphically evaluated. In this work a twentyfour hour measurements of errors and quality of digital video stream in MPEG- 2 TS were made. The obtained results from these measurements are evaluated and discussed. Finally, a new laboratoy work for the simultaneously measurement of the errors in video stream and video quality of the MPEG- 2 TS stream was proposed...|$|E
40|$|We {{extend our}} earlier work on {{guessing}} subject to distortion {{to the joint}} sourcechannel coding context. We consider {{a system in which}} there is a source connected to a destination via a channel and the goal is to reconstruct the source output at the destination within a prescribed distortion level w. r. t. some distortion <b>measure.</b> The <b>decoder</b> is a guessing decoder in the sense that it is allowed to generate successive estimates of the source output until the distortion criterion is met. The problem is to design the encoder and the decoder so as to minimize the average number of estimates until successful reconstruction. We derive estimates on nonnegative moments of the number of guesses, which are asymptotically tight as the length of the source block goes to infinity. Using the close relationship between guessing and sequential decoding, we give a tight lower bound to the complexity of sequential decoding in joint source-channel coding systems, complementing earlier works by Koshelev and [...] ...|$|R
40|$|Transcription of {{broadcast}} news {{is an interesting}} and challenging application for large-vocabulary continuous speech recognition (LVCSR). We present in detail {{the structure of a}} manually segmented and annotated corpus including over 160 hours of German broadcast news, and propose it as an evaluation framework of LVCSR systems. We show our own experimental results on the corpus, achieved with a state-of-the-art LVCSR <b>decoder,</b> <b>measuring</b> the effect of different feature sets and decoding parameters, and thereby demonstrate that real-time decoding of our test set is feasible on a desktop PC at 9. 2 % word error rate. Comment: submitted to INTERSPEECH 2010 on May 3, 201...|$|R
40|$|This work {{establishes}} a benchmark {{by which to}} measure models of the ventral stream using crowd-sourced human behavioral measurements. We collected human error patterns on an object recognition task {{across a variety of}} images. By comparing the error pattern of these models to the error pattern of humans, we can measure how similar to the human behavior the model's behavior is. Each model we tested was composed of two parts: an encoding phase which translates images to features, and a decoding phase which translates features to a classifier decision. We measured the behavioral consistency of three encoder models: a convolutional neural network, and a particular view of neural activity of either are V 4 or IT. We <b>measured</b> three <b>decoder</b> models: logistic regression and 2 different types of support vector machines. We found the most consistent error pattern to come from a combination of IT neurons and a logistic regression but found that this model performed far worse than humans. After accounting for performance, the only model that was not invalidated was a combination of IT neurons and an SVM. by Diego Ardila. Thesis: S. M. in Neuroscience, Massachusetts Institute of Technology, Department of Brain and Cognitive Sciences, 2015. Cataloged from PDF version of thesis. Includes bibliographical references (page 17) ...|$|R
40|$|Cataloged from PDF {{version of}} article. We extend our earlier work on {{guessing}} subject to distortion {{to the joint}} source-channel coding context. We consider {{a system in which}} there is a source connected to a destination via a channel and the goal is to reconstruct the source output at the destination within a prescribed distortion level with respect to (w. r. t.) some distortion <b>measure.</b> The <b>decoder</b> is a guessing decoder in the sense that it is allowed to generate successive estimates of the source output until the distortion criterion is met. The problem is to design the encoder and the decoder so as to minimize the average number of estimates until successful reconstruction. We derive estimates on nonnegative moments of the number of guesses, which are asymptotically tight as the length of the source block goes to infinity. Using the close relationship between guessing and sequential decoding, we give a tight lower bound to the complexity of sequential decoding in joint source-channel coding systems, complementing earlier works by Koshelev and Hellman. Another topic explored here is the probability of error for list decoders with exponential list sizes for joint source-channel coding systems, for which we obtain tight bounds as well. It is noteworthy that optimal performance w. r. t. the performance measures considered here can be achieved in a manner that separates source coding and channel coding...|$|R
40|$|Abstract This paper {{presents}} a BitpLAne SelecTive (BLAST) distributed video coding (DVC) system. In the proposed system, {{the significance of}} each bitplane is <b>measured</b> at the <b>decoder</b> based on an estimated distortion-rate ratio that makes use of a correlation model for the original source information and the side information. Only the syndrome bits of the bitplanes that have estimated distortion-rate ratios higher than a target distortion-rate ratio, are transmitted and are used to decode the associated bitplanes. The remaining bitplanes are estimated using a minimum-distance symbol reconstruction scheme which makes use of the side information and the LDPCA-decoded bitplanes. Coding results and comparisons with existing DVC schemes and with H. 264 intra- and inter-frame coding are presented to illustrate {{the performance of the}} proposed system. Keywords Distributed video coding. Bitplane selective. Low-density parity-check accumulate codes. Wyner-Ziv coding...|$|R
40|$|Abstract—Homophonic coding, or {{homophonic}} substitution, {{is referred}} to as a technique that contributes to reliability of the secret-key cipher systems. Its main goal is to convert the plaintext into a sequence of completely random (equiprobable and independent) code letters. In solving this problem three characteristics are to be considered: i) redundancy, defined as the difference between the mean codeword length and the source entropy, ii) an average number of random bits used in encoding, and iii) complexity of the encoder and <b>decoder,</b> <b>measured</b> by memory size (in bits) and computation time (in bit operations). A class of homophonic codes is suggested for which both the redundancy and the average number of random bits can be made as small as required with nonexponential growth of memory size and roughly logarithmic growth of computation time. Index Terms — Computational complexity, homophonic coding, randomization, secret key cryptosystems, source coding...|$|R
30|$|The BE-CDF bc {{allows us}} to {{guarantee}} failed decoding with high probability over certain SNR regions for t-error correcting codes. However, a failed decoder does not necessarily imply that the eavesdropper cannot obtain most of the message bits at the output. Hence, in this section, we introduce a metric that can guarantee decoder failure with BER close to 0.5 in the estimated message bits to strengthen the security guarantee. For this section, let P̂_b be the measured proportion of bit errors at the output of an error-correcting <b>decoder</b> <b>measured</b> over S_b decoded message bits. For the case where the code being used is a block (n,k) code, {{it makes sense to}} let S_b be an integer multiple of k. The metric we propose in this section allows a user to specify a required error rate at the output of the eavesdropper’s error-control decoder over S_b bits using the probability that P̂_b > 0.5 -δ for any δ desired.|$|R
30|$|The {{proposed}} architecture {{also features}} specific <b>measures</b> at the <b>decoder</b> {{to reduce the}} suffered distortion due to any SW coded information that fails to decode properly. For this purpose, the principles of successive SI refinement[33] are adopted. The WZ frames are decoded in distinct stages called refinement levels[34], where at each stage, a higher quality version of the SI is generated. Given the improved SI at every refinement level, SW decoding is reattempted for all SW coded information that failed to decode at previous levels, when only a poorer version of the SI was available. The proposed decoding process thereby merges SI refinement, SW decoding and reattempts at SW decoding of any SW coded information that failed to decode at previous refinement levels. In contrast, the approach in[23] first decodes an entire WZ frame, after which additional SI updates create new opportunities to attempt decoding bit-planes that failed to SW decode successfully.|$|R
40|$|Abstract — Among {{the various}} network {{protocols}} {{that can be}} used to stream the video data, RTP over UDP is the best to do with real time streaming in H. 264 based video streams. Videos transmitted over a communication channel are highly prone to errors; it can become critical when UDP is used. In such cases real time error concealment becomes an important aspect. A subclass of the error concealment is the motion vector recovery which is used to conceal errors at the decoder side. Lagrange Interpolation is the fastest and a popular technique for the motion vector recovery. This paper proposes a new system architecture which enables the RTP-UDP based real time video streaming as well as the Lagrange interpolation based real time motion vector recovery in H. 264 coded video streams. A completely open source H. 264 video codec called FFmpeg is chosen to implement the proposed system. Proposed implementation was tested against the different standard benchmark video sequences and the quality of the recovered videos was <b>measured</b> at the <b>decoder</b> side using various quality measurement metrics. Experimental results show that the real time motion vector recovery does not introduce any noticeable difference or latency during display of the recovered video...|$|R
40|$|Typescript. Thesis (Ph. D.) [...] University of Hawaii at Manoa, 1990. Includes bibliographical {{references}} (leaves 214 - 215) Microfiche. xv, 215 leaves, bound ill. 29 cmThe {{development of}} simple coding and decoding schemes for the Phase Ambiguous Additive White Gaussian Channel is considered in this dissertation. By phase ambiguous we {{mean that a}} narrowband signal transmitted over the Additive White Gaussian (AWG) Channel is shifted in phase by a random variable that remains fixed {{for the duration of}} the signal. When the phase ambiguity is modeled as an uniformly distributed random variable, the channel is referred to as the Noncoherent Additive White Gaussian (NAWG) Channel. When the first stage of the decoder is a phase locked loop, the phase ambiguity can be modeled by a discrete random variable that takes values corresponding to the phase angles of the signal set underlying the constellation used by the encoder. This channel is termed the Acoherent Additive White Gaussian (AAWG) Channel. Even when the encoder is a finite state machine, the optimum decoder for the NAWG channel cannot be implemented. We examine a suboptimum decoding scheme for the NAWG Channel. The primary component of the suboptimum decoder is a set of p identical decoders for the AWG Channel. When a finite state machine is used as the encoder, the suboptimum decoders we propose are implementable, We study the performance of our suboptimum decoder in relation to that of the optimum decoder for the NAWG channel and that of the optimum decoder for the AWG channel. We show that the suboptimum decoder, for values of p equal to 4 and 8 performs close to optimum in a variety of situations. Reliable communication for the AAWG channel is achieved by using rotationally invariant (RI) codes. We demonstrate that the decoders used in RI schemes are suboptimum. We develop a performance <b>measure</b> for our <b>decoder</b> when used on the AAWG channel. We construct alternative codes, which, when used with our suboptimum decoders, perform as well and in some cases even better, than RI codes. Finally we simulate the performance of the scheme we propose, and demonstrate its reliability for communication over the AAWG and NAWG channels...|$|R
40|$|In {{this thesis}} a {{hydraulic}} low pressure winch {{system has been}} modeled using bond graph theory. The hydraulic winch system {{is assumed to be}} installed on an offshore vessel affected by environmental forces and disturbances such as waves and currents. The hydraulic system powering the winch consists mainly of two pilot operated 3 / 3 -directional valves controlled by two 4 / 3 -directional valves and a hydraulic motor. The system also includes a pressure relief valve, check valves, pump systems, piping and filters. The 3 / 3 -directional valves are the main focus in the model and are therefore modeled with less simplifications compared to the other subsystems. A thorough model study has been initiated to figure out the model limitations, sensitivity of model parameters and the ability to simplify the derived model without loosing essential dynamics and characteristics. The effects of variable bulk modulus and fluid inertia in the 3 / 3 -directional valves have been studied by comparing different step responses and motor load characteristics. The observations and results from this model analysis laid the groundwork for control of the hydraulic motor. A clear relation between the main valve displacements and the motor velocity and torque in 4 / 3 valve configuration gave reasons to believe that manual motor control done by the winch operator through control of the valve displacements was possible. Adaptive PID controllers were used as inner controllers to control the control slides in the main valves. These controllers were later on replaced with PD-controllers when outer control was derived because the adaptive controllers tended to be a bit slow. Simplified state equations describing the motor dynamics were derived for control design purposes. The state equations extracted from the bond graph model showed high complexity, containing logic and discrete quantities, and were not suitable for control design. Model based speed- and torque controllers, based on sliding mode and backstepping control theory, were derived based on the simplified equations and implemented in the model. Different load cases were initiated to test the two controllers. A lumped wire-load model containing hydrodynamics, wire- and reel dynamics and environmental disturbances such as current and heave motions of the vessel were added in the total winch model to test the controllers in different operations with varying conditions and environments. The results from these controller tests gave reason to believe that a combination of these two controllers would be favourable in certain operations and would give increased safety in extreme cases such as stuck load and loss of load. The derived speed and torque controller were put into a hybrid controller framework and a switching algorithm was designed with focus on switching stability and wanted functionalities for the winch system. It was observed that switching stability and winch functionality were closely connected and different winch operations were essential {{in the design of the}} switching algorithm. Dwell time and tracking error switching were used as the main controller switching restrictions together with functionality based switching conditions. Different simulations were initiated to test the hybrid controller such as stuck load, loss of load and landing of a load at the sea floor. A Luenberger observer was derived to estimate the motor load and the motor velocity by using the simplified state equations and the differential pressure across the hydraulic motor as measurement in order to ensure redundancy in the control system and be able to control the hydraulic winch even though the <b>decoder</b> <b>measuring</b> the hydraulic motor velocity fails. </p...|$|R

