59|25|Public
2500|$|The song first {{appeared}} on the Beach Boys' 1966 album Pet Sounds in <b>monophonic</b> <b>sound</b> format. Initially, Brian considered releasing it as a single under Carl Wilson's name, but the group were in demand for a new single. Because their impending [...] "Good Vibrations" [...] was not yet ready, [...] "God Only Knows" [...] was issued instead.|$|E
50|$|Recorded in <b>monophonic</b> <b>sound.</b>|$|E
5000|$|Format: {{black and}} white and color by Eastmancolor - <b>monophonic</b> <b>sound</b> - 35 mm ...|$|E
40|$|We {{are looking}} for a {{mathematical}} model of <b>monophonic</b> <b>sounds</b> with independent time and phase dimensions. With such a model we can resynthesise a sound with arbitrarily modulated frequency and progress of the timbre. We propose such a model and show that it exactly fulfils some natural properties, like a kind of time-invariance, robustness against non-harmonic frequencies, envelope preservation, and inclusion of plain resampling as a special case. The resulting algorithm is efficient and allows to process data in a streaming manner with phase and shape modulation at sample rate, what we demonstrate with an implementation in the func-tional language Haskell. It allows a wide range of applications, namely pitch shifting and time scaling, creative FM synthesis ef-fects, compression of <b>monophonic</b> <b>sounds,</b> generating loops for sampled sounds, synthesise sounds similar to wavetable synthesis, or making ultrasound audible. 1...|$|R
40|$|This paper {{deals with}} the {{fundamental}} frequency estimation for <b>monophonic</b> <b>sounds</b> in the SMS analysis environment. The importance of the fundamental frequency {{as well as some}} uses in SMS is commented. The particular method of F 0 estimation based on a two-way mismatched measure is described as well as some modifications. Finally we explain how pitch-unpitched decision is performed...|$|R
40|$|Mixture Models is {{introduced}} and evaluated for audio morphing. To this aim, the GMM {{is used to}} build the acoustic model of the source sound, {{and a set of}} conversion functions, which rely on the acoustic model, is used to transform the source sound. The method is experimented on a set of <b>monophonic</b> <b>sounds</b> and results show that it provides promising features...|$|R
5000|$|The Simitar, 7278 release {{runs for}} 87 minutes {{and is in}} <b>monophonic</b> <b>sound</b> format.|$|E
5000|$|... #Caption: Modelling a <b>monophonic</b> <b>sound</b> as {{observation}} along a helix of {{a function}} with a cylinder domain ...|$|E
50|$|The EP was reissued in 1992 {{as part of}} a 15-disc Compact Disc EP Collection, {{with the}} songs in their {{original}} <b>Monophonic</b> <b>sound.</b>|$|E
40|$|In {{this work}} a sound {{transformation}} model based on Gaussian Mixture Models is introduced and evaluated for audio morphing. To this aim, the GMM {{is used to}} build the acous- tic model of the source sound, {{and a set of}} conversion func- tions, which rely on the acoustic model, is used to trans- form the source sound. The method is experimented on a set of <b>monophonic</b> <b>sounds</b> and results show that it provides promising features...|$|R
50|$|The <b>monophonic</b> bass <b>sound</b> {{available}} on {{the lower half of}} the keyboard has no variable-gain amplifier but has a low-pass filter.|$|R
5000|$|... 1974 - ARP Explorer (small, portable, <b>monophonic</b> preset, {{programmable}} <b>sounds)</b> ...|$|R
5000|$|In 2006 he {{released}} Merseybeat Mono, which contained {{all the songs}} from his first two albums in <b>monophonic</b> <b>sound,</b> as well as Concerto for Violin with Orchestra, the acoustic Fulton Avenue, and the single [...] "She's Good".|$|E
50|$|The SDII (Sound Designer II, {{sometimes}} seen abbreviated as SD2) is a monophonic/stereophonic {{audio file}} format, originally developed by Digidesign for their Macintosh-based recording/editing products. It is {{the successor to}} the original <b>monophonic</b> <b>Sound</b> Designer I audio file format.|$|E
50|$|Monsieur Fabre (Mr Fabre) is a 90-minute {{black and}} white French film comedy from 1951, {{directed}} by Henri Diamant-Berger. It was on 35 mm film, in 1,37:1 format, with <b>monophonic</b> <b>sound.</b> It was released in France on 5 July 1951.|$|E
40|$|Abstract. At a {{time when}} the {{quantity}} of music media surrounding us is rapidly increasing and the access to recordings as well as the amount of music files available on the Internet is constantly growing, the problem of building music recommendation systems is of great importance. In this work, we perform a study on automatic classification of musical instruments. We use <b>monophonic</b> <b>sounds.</b> The latter have successfully been classified in the past, with main focus on pitch. We propose new temporal features and incorporate timbre descriptors. The advantages of this approach are: preservation of temporal information and high classification accuracy. ...|$|R
40|$|This {{combined}} fMRI and MEG {{study investigated}} brain activations during listening and attending to natural auditory scenes. We first recorded, using in-ear microphones, vocal non-speech sounds and environmental sounds that were mixed to construct auditory scenes containing two concurrent sound streams. During the brain measurements, subjects attended {{to one of}} the streams while spatial acoustic information of the scene was either preserved (stereophonic <b>sounds)</b> or removed (<b>monophonic</b> <b>sounds).</b> Compared to <b>monophonic</b> <b>sounds,</b> stereophonic sounds evoked larger blood-oxygenation-level-dependent (BOLD) fMRI responses in the bilateral posterior superior temporal areas, independent of which stimulus attribute the subject was attending to. This finding is consistent with the functional role of these regions in the (automatic) processing of auditory spatial cues. Additionally, significant differences in the cortical activation patterns depending on the target of attention were observed. Bilateral planum temporale and inferior frontal gyrus were preferentially activated when attending to stereophonic environmental sounds, whereas when subjects attended to stereophonic voice sounds, the BOLD responses were larger at the bilateral middle superior temporal gyrus and sulcus, previously reported to show voice sensitivity. In contrast, the time-resolved MEG responses were stronger for mono- than stereophonic sounds in the bilateral auditory cortices at ~ 360 ms after the stimulus onset when attending to the voice excerpts within the combined sounds. The observed effects suggest that during the segregation of auditory objects from the auditory background, spatial sound cues together with other relevant temporal and spectral cues are processed in an attention-dependent manner at the cortical locations generally involved in sound recognition. More synchronous neuronal activation during <b>monophonic</b> than stereophonic <b>sound</b> processing, as well as (local) neuronal inhibitory mechanisms in the auditory cortex, may explain the simultaneous increase of BOLD responses and decrease of MEG responses. These findings highlight the complimentary role of electrophysiological and hemodynamic measures in addressing brain processing of complex stimuli...|$|R
30|$|Note {{that some}} of the {{stimulus}} sets contain only <b>monophonic</b> <b>sounds,</b> whereas others contain only stereophonic sounds, and, although the acoustic features are calculated on both channels in the latter case, the salience of an indicator in one channel compared to the other depends on the recording context. For example, if a car interior sound has been recorded from the driver's seat, the most relevant channel for a given sound feature will probably not be the same {{as if it had been}} recorded from the passenger's seat. Accordingly, the features in the correlation tables can be either from the left or the right channel, or from the mean of both channels.|$|R
50|$|Analog TV {{started off}} with <b>monophonic</b> <b>sound,</b> and later evolved to {{stereophonic}} sound with two independent audio signal channels. DTV will allow up to 5 audio signal channels plus a sub-woofer bass channel, with broadcasts similar in quality to movie theaters and DVDs.|$|E
50|$|Channel 9 {{broadcasts}} in color, but -as well as {{many other}} TV channels outside Buenos Aires- it has <b>monophonic</b> <b>sound</b> only. The station produces two newscasts: one with local news and other with news from Catamarca. A weekend cultural show called La Rioja que Usted no Conoce ("La Rioja You Don't Know") is also presented.|$|E
50|$|Mono is {{the eighth}} studio album by American country music band The Mavericks. It was {{released}} on February 17, 2015 via Valory Music Group. The album sold 8,000 copies in its first week of release, debuting at number 5 on the Billboard Top Country Albums chart. The album was recorded and mixed in <b>monophonic</b> <b>sound.</b>|$|E
50|$|The {{complicated}} polyphonies of what {{is called}} the Ars Nova began to be heard in the 14th century and 15th century; popular items such as madrigals employed increasing dense overlays of different melodies sung at the same time, the point being to create an interwoven and euphonious texture of sound; this is NOT the same as harmony, the sounding of many notes together in order to form a chord. That is a later invention. Nevertheless, the move from the <b>monophonic</b> <b>sounds</b> of chants to the many simultaneous melodies of polyphony does represent a revolution in our musical perceptions: to wit, you can have more one thing sounding {{at the same time and}} still find it pleasant to listen to.|$|R
30|$|The F-measure results (shown in Figure 5) for {{the methods}} {{described}} in Section 2.3 {{are lower than}} those given elsewhere in the literature, but this was expected as real-time performance is significantly more challenging at the peak-picking and thresholding stages. The nature of the sample set {{must also be taken}} into account, as evidently, the heavy bias towards <b>monophonic</b> <b>sounds</b> is reflected by the surprisingly strong performance of the energy-based methods. As noted in [8], the various parameter settings can have a large impact on overall performance. We tried to select a parameter set that gave a fair reflection on each algorithm, but {{it must be noted that}} every method can probably be improved by some parameter adjustments, especially if prior knowledge of the sound source is available.|$|R
3000|$|We {{provide a}} new {{solution}} to the problem of feature variations caused by the overlapping of sounds in instrument identification in polyphonic music. When multiple instruments simultaneously play, partials (harmonic components) of their sounds overlap and interfere, which makes the acoustic features different from those of <b>monophonic</b> <b>sounds.</b> To cope with this, we weight features based on how much they are affected by overlapping. First, we quantitatively evaluate the influence of overlapping on each feature as the ratio of the within-class variance to the between-class variance in the distribution of training data obtained from polyphonic sounds. Then, we generate feature axes using a weighted mixture that minimizes the influence via linear discriminant analysis. In addition, we improve instrument identification using musical context. Experimental results showed that the recognition rates using both feature weighting and musical context were 84.1 [...]...|$|R
50|$|Horn Massive is a 2.25-ton (4,500 lbs) 3.5 x 3.1 x 4 m mobile 2,000-watt {{steel and}} {{aluminum}} horn sound system {{powered by a}} commercial 12-inch speaker driver. It functions as a mobile audio input station to project audio content a distance of one kilometre. Horn Massive is a <b>monophonic</b> <b>sound</b> projector designed and built by artist Matt Hope.|$|E
5000|$|The song first {{appeared}} on the Beach Boys' classic 1966 album Pet Sounds in <b>monophonic</b> <b>sound</b> format. Initially, Brian considered releasing it as a single under Carl Wilson's name, but the group were in demand for a new single. Because their impending [...] "Good Vibrations" [...] was not yet ready, [...] "God Only Knows" [...] was issued instead.|$|E
50|$|Having been {{a record}} {{collector}} since the 1920s, Nunn {{began to make}} records to improve their audio quality. He was a recording engineer who believed <b>monophonic</b> <b>sound</b> (mono) was better than stereophonic sound (stereo). His records impressed High Fidelity magazine and G. A. Briggs, the designer of Wharfedale speakers. In 1947, he started Audiophile Records in Saukville, Wisconsin before moving it to Mequon, Wisconsin in 1965.|$|E
40|$|A set of {{features}} extracted from audio sources is investigated for content-based classification of musical instrument timbres. The adopted features describe spectral characteristics of <b>monophonic</b> <b>sounds</b> {{and rely on}} the previous segmentation of the signal and the estimation of pitch. The dataset is composed by 1007 tones from 27 musical instruments ranging from orchestral sounds (strings, woodwinds, brass) to pop/electronic instruments (bass, electric and distorted guitar). The extracted features are then classified by widely used pattern recognition techniques. A thorough evaluation of the resulting performances and comparative analysis with previous works is presented. Quadratic Discriminant Analysis shows an error rate of 7. 19 % for the individual instruments and 3. 23 % for instrument families. These results are by far superior to the performances of other classification methods (Canonical Discriminant Analysis, Support Vector Machines, Nearest Neighbours). The use of a machine-built decision hierarchy did not improve the results. ...|$|R
40|$|This paper {{deals with}} the {{fundamental}} frequency estimation for <b>monophonic</b> <b>sounds</b> in the SMS analysis environment. The importance of the fundamental frequency {{as well as some}} uses in SMS is commented. The particular method of F 0 estimation based on a two-way mismatched measure is described as well as some modifications. Finally we explain how pitch-unpitched decision is performed. 1 Introduction The particular approach of SMS is based on modeling sounds as stable sinusoids (partials) plus noise (residual component), therefore analyzing sounds with this model and generating new sounds from the analyzed data. The analysis procedure detects partials by studying the time-varying spectral characteristics of a sound and represents them with time-varying sinusoids. These partials are then subtracted from the original sound and the remaining "residual" is represented as a time-varying filtered white noise component [1]. This article is part of the current work at the Audiovisual Institu [...] ...|$|R
50|$|Oram's {{composition}} machine {{consisted of}} a large rectangular metal frame, providing a table-like surface traversed by ten synchronised strips of clear, sprocketed 35mm film. The musician drew shapes on the film to create a mask, which modulated the light received by photocells. Although the output from the machine was <b>monophonic,</b> the <b>sounds</b> could be added to multitrack tapes to provide more texture.|$|R
50|$|The RCA tape {{cartridge}} format offers four discrete audio tracks {{that provide a}} typical playtime of 30 minutes of stereo sound per side, or double that for <b>monophonic</b> <b>sound.</b> Some models can also play and record at 1.875 IPS, doubling playing time with {{a significant reduction in}} sound quality. This speed was of too low quality for music on these machines, but was fully acceptable for voice recording.|$|E
5000|$|Descriptions of {{stereophonic}} sound tend {{to stress the}} ability to localize the position of each instrument in space, but this would only be true in a carefully engineered and installed system, where speaker placement and room acoustics are taken into account. In reality, many playback systems, such as all-in-one boombox units and the like, are incapable of recreating a realistic stereo image. Originally, in the late 1950s and 1960s, {{stereophonic sound}} was marketed as seeming [...] "richer" [...] or [...] "fuller-sounding" [...] than <b>monophonic</b> <b>sound,</b> but these sorts of claims were and are highly subjective, and again, dependent on the equipment used to reproduce the sound. In fact, poorly recorded or reproduced stereophonic sound can sound far worse than well done <b>monophonic</b> <b>sound.</b> When playing back stereo recordings, the best results are obtained by using two identical speakers, in front of and equidistant from the listener, with the listener located on a center line between the two speakers. In effect, an equilateral triangle is formed, with the angle between the two speakers around 60 degrees {{as seen from the}} listener's point of view.|$|E
50|$|Similarly, {{the film}} version of James Goldman's The Lion in Winter (1968), {{although}} a filmed-on-location roadshow release, was shown in 35mm Panavision and Technicolor, but with mono sound. It was only in Australia and in its 1973 London re-release that it was shown in 70mm and stereophonic sound. 1971's Nicholas and Alexandra, another roadshow release, was also shown in 70mm 6-track only in Europe, while its U.S. release was in regular Panavision with <b>monophonic</b> <b>sound.</b>|$|E
40|$|We propose an {{iterative}} algorithm {{to detect}} transient segments in audio signals. Short time Fourier transform(STFT) {{is used to}} detect rapid local changes in the audio signal. The algorithm has two steps that iteratively - (a) calculate {{a function of the}} STFT and (b) build a transient signal. A dynamic thresholding scheme is used to locate the potential positions of transients in the signal. The iterative procedure ensures that genuine transients are built up while the localised spectral noise are suppressed by using an energy criterion. The extracted transient signal is later compared to a ground truth dataset. The algorithm performed well on two databases. On the EBU-SQAM database of <b>monophonic</b> <b>sounds,</b> the algorithm achieved an F-measure of 90 % while on our database of polyphonic audio an F-measure of 91 % was achieved. This technique is being used as a preprocessing step for a tempo analysis algorithm and a TSR (Transients + Sines + Residue) decomposition scheme...|$|R
40|$|Abstract: Recently, {{numerous}} successful {{approaches have}} been developed for instrument recognition in <b>monophonic</b> <b>sounds.</b> Unfortunately, none of them can be successfully applied to polyphonic sounds. Identification of music instruments in polyphonic sounds is still difficult and challenging. This has stimulated a number of research projects on music sound separation and new features development for content-based automatic music information retrieval. The paper introduces several temporal features based on pitch to improve automatic music instrument recognition. The results from experiments show that these new features, with the pitch information removed from them, tend to provide less distraction for timber estimation. Sometime, the addition of new features to the database of music instruments does not help and related classifiers still do not perform well. One possibility to handle this problem is to build classifiers which learn not only the descriptions of music instruments but also their generalizations on different granularity levels. We show that by introducing several optional hierarchical classifications of musical instruments and constructing related classifiers, we increase a chance to build a system of good performance in terms of successful indexing of music by instruments and their types...|$|R
40|$|We {{provide a}} new {{solution}} to the problem of feature variations caused by the overlapping of sounds in instrument identification in polyphonic music. When multiple instruments simultaneously play, partials (harmonic components) of their sounds overlap and interfere, which makes the acoustic features different from those of <b>monophonic</b> <b>sounds.</b> To cope with this, we weight features based on how much they are affected by overlapping. First, we quantitatively evaluate the influence of overlapping on each feature as the ratio of the within-class variance to the between-class variance in the distribution of training data obtained from polyphonic sounds. Then, we generate feature axes using a weighted mixture that minimizes the influence via linear discriminant analysis. In addition, we improve instrument identification using musical context. Experimental results showed that the recognition rates using both feature weighting and musical context were 84. 1 % for duo, 77. 6 % for trio, and 72. 3 % for quartet; those without using either were 53. 4, 49. 6, and 46. 5 %, respectively. Copyright © 2007 Tetsuro Kitahara et al. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. 1...|$|R
