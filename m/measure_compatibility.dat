8|153|Public
40|$|The {{recently}} developed "Data Set Diagonalization" method (DSD) {{is applied to}} <b>measure</b> <b>compatibility</b> of the data sets {{that are used to}} determine parton distribution functions (PDFs). Discrepancies among the experiments are found to be somewhat larger than is predicted by propagating the published experimental errors according to Gaussian statistics. The results support a tolerance criterion of Δχ^ 2 ≈ 10 to estimate the 90...|$|E
40|$|Results of a {{study to}} <b>measure</b> <b>compatibility</b> and {{the success or failure}} of the {{relationship}} between a person and his or her first guide dog were published in the proceedings of the 11 th International Mobility Conference (Lloyd, La Grow, Budge & Stafford, 2003). This presentation builds on these findings by discussing how feelings at the end of the partnership affect the handlers’ relationships with subsequent dogs, and shows trends in the dataset concerning multiple dog use. The effects of being without a dog, after experiencing guide dog mobility, on quality of life will also be presented...|$|E
40|$|Methods that <b>measure</b> <b>compatibility</b> between mention pairs are {{currently}} the dominant approach to coreference. However, they {{suffer from a}} number of drawbacks including difficulties scaling to large numbers of mentions and limited representational power. As these drawbacks become increasingly restrictive, the need to replace the pairwise approaches with a more expressive, highly scalable alternative is becoming urgent. In this paper we propose a novel discriminative hierarchical model that recursively partitions entities into trees of latent sub-entities. These trees succinctly summarize the mentions providing a highly compact, information-rich structure for reasoning about entities and coreference uncertainty at massive scales. We demonstrate that the hierarchical model is several orders of magnitude faster than pairwise, allowing us to perform coreference on six million author mentions in under four hours on a single CPU. ...|$|E
5000|$|Lack of {{generally}} accepted criteria for <b>measuring</b> <b>compatibility</b> ("degrees of compatibility") ...|$|R
5000|$|Among {{existing}} psychological {{tools for}} studying and/or <b>measuring</b> interpersonal <b>compatibility,</b> {{the following are}} noteworthy: ...|$|R
3000|$|... [...]. The {{advantage}} of the adopted energy function {{is that there is}} no need for estimating normalized probability distributions over the input space. The scalar energy function E <b>measures</b> the <b>compatibility</b> between X [...]...|$|R
40|$|Methods that <b>measure</b> <b>compatibility</b> between mention pairs are {{currently}} the dominant approach to coreference. However, they {{suffer from a}} number of drawbacks including difficulties scaling to large numbers of mentions and limited representational power. As the severity of these drawbacks continue to progress with the growing demand for more data, the need to replace the pairwise approaches with a more expressive, highly scalable alternative is becoming increasingly urgent. In this paper we propose a novel discriminative hierarchical model that recursively structures entities into trees. These trees succinctly summarize the mentions providing a highly-compact information-rich structure for reasoning about entities and coreference uncertainty at small, large, and massive scales. The unique recursive structure of our entities allows our model to adapt to entities of various sizes, express features over entity hierarchies, and scale to massive data, making our approach a desirable new standard to replace the antiquated pairwise model. ...|$|E
40|$|The {{objective}} of vehicle crash compatibility is the optimisation of vehicle design to minimise {{the total number}} of injuries and fatalities that occur in all collisions in the accident environment. It is hence distinguished from traditional perceptions of occupant protection in that it requires vehicle designs to be optimised to protect other road users in addition to the vehicle’s own occupants. The aim of this thesis is to define an objective method to assess the compatibility of a front-to-front or front-to-side collision between two passenger vehicles. Accident statistics from the German In-Depth Accident Survey (GIDAS) relational database are analysed to determine the significance of passenger vehicle occupant casualties with regards to other road users and also to set priorities for the assessment of compatibility between passenger vehicles. Collision obstacles, configurations, velocity, and vehicle mass are analysed with respect to the Abbreviated Injury Scale (AIS). A method is defined to objectively <b>measure</b> <b>compatibility</b> by comparing the post-collision deformations from a vehicle-to-vehicle collision with the post-collision deformations from a series of vehicle-to-barrier collisions. The result is quantified with respect to injury data derived from the accident statistics. Ideal models of horizontal and vertical structural homogeneity are developed and applied in front-to-front and front-to-side collision simulations between a mid-sized passenger car and a large Sports Utility Vehicle (SUV). Hence, for the first time, independent conclusions are able to be drawn based on the effects of ideal horizontal homogeneity and ideal vertical homogeneity. The effects of front-end structural homogeneity are also investigated in combination with changes to the stiffness of various components in the vehicle side structure. Finally, the findings of the simulations and the accident analysis are used to describe the necessary characteristics for a test procedure to assess a vehicle’s compatibility. The merits of existing test procedures are discussed, and alternative concepts are proposed...|$|E
40|$|Abstract. —Discriminate {{compatibility}} {{measures are}} introduced. The character discriminate compatibility {{and the average}} character discriminate compatibility measures are used in weighted parsimony analysis in an iterative procedure, the reduction routine, to build trees. The data set discriminate compatibility measure is used to order and polarize characters. The average data set discriminate compatibility measure is {{used to measure the}} fit of different data sets and different user trees. Three inadequacies in the Le Quesne compatibility tests (Le Quesne, 1969, Syst. Zool. 18 : 201 - 205; 1979, Syst. Zool. 28 : 92 - 94) as applied by Penny and Hendy (1985, Cladistics 1 : 266 - 272; 1986, Mol. Biol. Evol. 3 : 403 - 417) and Sharkey (1989, Cladistics 5 : 63 - 86) are discussed. [Char-acter weighting; cladistics; compatibility; congruence; consistency index; phylogeny. ] The importance, for classification, of trifling char-acters, mainly depends on their being correlated with many other characters. —Charles Darwin, 1859 : 417 The more correlated a character state is with respect to other character states, the more one may be convinced that it is a synapomorphy. As indicated by the quo-tation from Darwin, the criterion of cor-relation has been an intuitive method of character evaluation for almost 150 years. In the present paper, I attempt to quantify this thinking to obtain an objective, ra-tional, and repeatable method of phylo-genetic reconstruction. There are two basic types of character correlation. When characters are correlated with respect to a phylogenetic hypothesis, they are termed congruent with the hy-pothesis. When characters are correlated with each other in the data set, they are termed compatible. Farris (1971) made these distinctions clear. Compatibility (a form of which is used here) is an a priori concept of correlation, whereas congruence is an a posteriori notion. The two most fundamental questions to be addressed in this paper are how does one <b>measure</b> <b>compatibility,</b> and how is this...|$|E
40|$|A (nonlocal) linear {{integral}} equation is studied, {{which allows for}} Bäcklund transformations in the <b>measure.</b> The <b>compatibility</b> of three of these transformations leads to an integrable nonlinear three-dimensional lattice equation. In appropriate continuum limits the two-dimensional Toda-lattice equation and the Kadomtsev-Petviashvili equation are derived as special examples...|$|R
40|$|To appear. International audienceChecking the {{compatibility}} {{of service}} interfaces {{allows one to}} avoid erroneous executions when composing services together. In this paper, we propose a flooding-based approach for <b>measuring</b> the <b>compatibility</b> degree of service interfaces specified using interaction protocols. This proposal is fully automated by a prototype tool we have implemented...|$|R
40|$|An {{objective}} {{method for}} <b>measuring</b> <b>compatibility</b> {{is a prerequisite}} for improving crash performance and to further advance occupant safety in car-to-car collisions, but no suitable procedure has been developed to date. To date,based on car-to-car crashes, it is possible to conclude which pairs of cars are compatible, but {{it is not possible to}} quantify and compare the compatibility of one pair of cars to another. This study presents a further development of the Relative Equivalent Energy Displacement (RED) method to assess vehicle crash compatibility. The compatibility rating quantifies how the crash response of a car in barrier test differs from its crash response in a corresponding car-to-car crash configuration with another car assessed in a same test. It was shown that the method can be used even in the development phase of a vehicle crash structure...|$|R
40|$|The human {{resource}} is constantly cited as an organisation's greatest asset. In {{a rapidly changing}} technological environment this is most applicable to the Information Technology (IT) function. Organisations are experiencing IT {{human resource}} problems such as low satisfaction, early plateauing, high turnover, burnout, limited advancement potential, nominal corporate commitment, supervisory aversion, poor organisational culture, and exceptional compensation. These problems {{are directly related to}} the IT professional's career. There is a lack of information and awareness surrounding IT careers to deal effectively with these problems. The research aims to create increased awareness of IT careers and the inherent problems through the development of a career management model. The research aims to identify the factors that influence IT careers, provide career management with a means to <b>measure</b> <b>compatibility</b> of the factors, and suggest solutions to incompatibility. The solving of this problem will be of mutual benefit to both organisations and individuals as they seek to better manage IT careers. After reviewing research literature relating to career anchors, IT job types, IT skills portfolios, and career dynamics a model for Effective IT Career Management (EITCM) has been constructed. The model represents the dynamic interactions between individual, organisational, and dependent factors. The model examines the compatibility of these interacting factors by measuring the levels of relevant career variables. The model suggests appropriate career management techniques to increase the compatibility of the interacting factors. An empirical study was designed and launched online to provide data that would confirm the seven Critical Success Factors (CSF) relating to the proposed model. The responses from the members of the Computer Society of South Africa (CSSA) allowed the seven hypotheses derived from the CSFs to be tested. The results of the empirical study were positive but required modification to five of the CSFs before they could be confirmed. The EITCM model was modified to reflect the improved CSFs. An awareness of career influencing factors combined with active career management is advantageous to both IT professionals and their organisations...|$|E
40|$|Graduation date: 1989 A Land Evaluation and Site Assessment (LESA) Model was {{developed}} for the forested soils of Lane County, in western Oregon, based on soil potential ratings and indexes of parcel size and adjacent and surrounding land use conflict. Lane County's economy is heavily dependent on resource production uses of land for forestry. At the same time, population growth around metropolitan areas creates pressure to convert rural land from large resource use parcels to smaller rural residential parcels. Planning for future allocation of land among competing uses promoted the county to develop an objective method for determining the relative quality of any parcel of land for forestry. Parcels of lower quality could then be considered for conversion to rural residential uses. LESA {{was developed}} by the SCS for use by state and local governments as an objective method of evaluating the resource production quality of land for planning purposes. Land evaluation (LE) measures the relative suitability of the soils of a given parcel for forestry. Site Assessment (SA) measures the relative suitability of the setting in which the parcel occurs. The soil potential ratings (SPR's) were developed from soil map unit characteristics defined in the Soil Survey of the Lane County Area. SPR's are indexes of the net return to soil management for forestry. Each soil is assigned an expected output, or yield, to soil management for forestry using a computer model called DFSIM. Management practices required to achieve that yield also are specified. Monetary values are determined for both yields and management practices, and the difference between price received and total costs is a measure of soil potential. The soil having the highest net return to soil management is assigned an arbitrary value of 100 points. All other soils are rated by expressing their net return as a percent of the maximum. Management practices in each of four categories - site preparation and stand establishment, thinning, harvest, and road construction and maintenance - were prescribed, and their costs determined, based on their interactions with soil slope, erodibility, depth, bedrock hardness, and coarse fragment content. Land evaluation was completed by overlaying a soil map of the land parcel of interest, determining the fractional amount of each soil present, and multiplying that amount by the corresponding soil potential rating. The sum of all the products is a weighted average soil potential rating for a parcel. Development of the Site Assessment (SA) portion of the model was guided by a technical committee of forest management professionals and land use specialists. The committee chose the factors that were considered important in site assessment and how much weight to give to each factor. For this LESA model, two factors were identified: compatibility with other land uses, and parcel size. The concept of compatibility implies that large scale forestry uses are compatible with each other but are not compatible with small scale residential uses. Generally, the more non-resource related dwellings in forestry areas, the greater the potential conflict due to noise, chemical spraying, dust, smoke, and vandalism. Two empirical formulas were developed to <b>measure</b> <b>compatibility</b> effects. One accounts for the number and density of non-compatible parcels adjacent to the parcel of interest. The other measures the density of non-compatible parcels within a specified distance of the target parcel, which was 1 / 2 mile. Parcel size implies that large parcels are more suitable for resource uses than small ones, and that parcels surrounded by a few large parcels are more favorable than parcels surrounded by many small parcels. An empirical formula was derived to measure these effects. Optimum parcel sizes depended on slope, parcel shape, and the number of streams running through the parcel. The final step in the LESA model development was to specify a total point value, and to decide on the proportion of that total that would go to each of the factors, soils, compatibility, and parcel size. In previous LESA models the point total has been 300. This total was allocated to each of the factors as follows: soils 105, adjacent use 75, surrounding use 45, and parcel size 75. Validation is a critical part of the development of a LESA model, and it is done by applying the LESA criteria to several parcels that represent a range of soil resource quality, sizes, and land use settings. Each parcel must then be examined in the field by the LESA development committee. Field examination is essential in order to make needed adjustments in empirical formulas. Through the repeating of this validation process, the model is fine tuned and its accuracy for planning purposes is validated. LESA scores can be used to distinguish between primary and secondary land resources. Primary resource lands are sufficiently valuable for forest uses that land use controls are justified to prevent the introduction of non-resource development. Secondary resource land is of lesser quality and is a more appropriate site for smaller scale resource uses and certain non-resource uses. Information from the test parcels was used to set primary/secondary thresholds for each factor and to develop empirical criteria for classifying each parcel...|$|E
40|$|The year 2014 was {{the year}} for restricting EU nationals' access to {{benefits}} in the UK, {{with a series of}} measures introduced since 1 January. This paper assesses each <b>measure's</b> <b>compatibility</b> with EU law, examining the legal texts and the accompanying guidance, which may lead to infringements by administrative decision makers. The paper then analyses the cumulative programme of reforms, and identifies three societal concerns. First, the programme represents a departure from EU Treaty principles. Second, the effects of the new measures are felt by all EU migrants, not just the ‘economically inactive’, since they are subject to extra tests and delays, face amplified xenoscepticism, and are placed in a more precarious position, with greater risks attendant upon loss of work. Third, the measures represent a pure form of an individualist ideology, potentially lowering our resistance to child poverty and destitution...|$|R
50|$|Given a Lie algebroid (A,ρ,.,.) {{together}} with a connection ∇ on its vector bundle we can define two associated A-connections as followsMoreover we can introduce the mixed curvature asThis curvature <b>measures</b> the <b>compatibility</b> of the Lie bracket with the connection {{and is one of}} the two conditions of A {{together with}} TM forming a matched pair of Lie algebroids.|$|R
50|$|New {{production}} facilities in South Africa (seat systems) and Bremen (door systems) expand Brose’s production capacity in 2014. Berlin-based Brose Antriebstechnik GmbH & Co begins series {{production of the}} first pedelec electric motor in fall {{as part of a}} joint project with bicycle manufacturer Rotwild. Due to the increasing electrification and digitization of vehicles, Brose invests in a test center in Würzburg to <b>measure</b> electromagnetic <b>compatibility.</b>|$|R
30|$|A loss function, also {{referred}} to as a cost function, <b>measures</b> the <b>compatibility</b> between output predictions of the network through forward propagation and given ground truth labels. Commonly used loss function for multiclass classification is cross entropy, whereas mean squared error is typically applied to regression to continuous values. A type of loss function is one of the hyperparameters and needs to be determined according to the given tasks.|$|R
40|$|We first {{present some}} basic {{properties}} of a quantum <b>measure</b> space. <b>Compatibility</b> of sets {{with respect to}} a quantum measure is studied and the center of a quantum measure space is characterized. We characterize quantum measures in terms of signed product measures. A generalization called a super-quantum measure space is introduced. Of a more speculative nature, we show that quantum measures may be useful for computing and predicting elementary particle masses. ...|$|R
40|$|Links {{the concept}} of market-driven {{business}} strategies with the design of production systems. It draws upon {{the case of a}} firm which, during the last decade, changed its strategy from being “technology led” to “market driven”. The research, based on interdisciplinary fieldwork involving long-term participant observation, investigated the factors which contribute to the successful design and implementation of flexible production systems in electronics assembly. These investigations were conducted in collaboration with a major computer manufacturer, with other electronics firms being studied for comparison. The research identified a number of strategies and actions seen as crucial to the development of efficient flexible production systems, namely: effective integration of subsystems, development of appropriate controls and performance <b>measures,</b> <b>compatibility</b> between production system design and organization structure, {{and the development of a}} climate conducive to organizational change. Overall, the analysis suggests that in the electronics industry there exists an extremely high degree of environmental complexity and turbulence. This serves to shape the strategic, technical and social structures that are developed to match this complexity, examples of which are niche marketing, flexible manufacturing and employee harmonization...|$|R
40|$|Edge {{bundling}} methods {{became popular}} for visualising large dense networks; however, most of previous work mainly relies on geometry to define compatibility between the edges. In this paper, {{we present a}} new framework for edge bundling, which tightly integrates topology, geometry and importance. In particular, we introduce new edge <b>compatibility</b> <b>measures,</b> namely importance <b>compatibility</b> and topology compatibility. More specifically, we present four variations of force directed edge bundling method based on the framework: Centrality-based bundling, Radial bundling, Topology-based bundling, and Orthogonal bundling. Our experimental results with social networks, biological networks, geographic networks and clustered graphs indicate that our new framework can be very useful to highlight the most important topological skeletal structures of the input networks. ...|$|R
40|$|In {{order to}} study the {{precision}} of qualitative land suitability classification method for main irrigated crops (i. e. potato, sugar beet, wheat and alfalfa) in the Shahrekord plain, qualitative land suitability maps were obtained for all the studied crops according to representative pedon analysis using simple limitation method. In the next step, a regular grid sampling consisting of 100 sample points with a distance of 375 m was designed. Then all required analyses were done to recognize the suitability class of these sites for each land use. Finally, land suitability results for all the observation points in each map unit were compared {{with the results of}} its representative pedon. The results showed the average of <b>measured</b> <b>compatibility</b> between representative pedon and other observation points in each map unit in class and subclass levels was about 60 % and 38 %, respectively. Due to the generalization of representative pedon analyses to all unit area, the use of soil map units as land suitability units may lead to unsatisfactory results. Therefore, the use of representative pedon is not recommended in sustainable land management and precision agriculture. However, new techniques like geostatistics can be used to improve the conventional soil mapping methods...|$|R
50|$|Joint {{compatibility}} {{branch and}} bound (JCBB) is an algorithm in computer vision and robotics commonly used for data association in simultaneous localization and mapping. JCBB <b>measures</b> the joint <b>compatibility</b> {{of a set of}} pairings that successfully rejects spurious matchings and is hence known to robust in complex environments.|$|R
30|$|All the {{calculations}} related to hierarchical analysis process based on judgment of decision makers who was stated in a paired matrix template. Any type of error and incompatibility in comparisons and determining {{the importance of}} criteria effects the final results. The compatibility value, is a tool which determines the compatibility and shows how much the resulted weights are reliable. Mikhailov stated {{that it is possible}} to use λ as a proper criteria for <b>measuring</b> the <b>compatibility</b> of comparisons and a λ approximately more than 1, shows the compatibility value of comparisons [15].|$|R
40|$|Many {{multiple}} attribute {{decision analysis}} problems include both {{quantitative and qualitative}} attributes with various kinds of uncertainties such as ignorance, fuzziness, interval data, and interval belief degrees. An evidential reasoning (ER) approach developed in the 1990 s {{and in recent years}} can be used to model these problems. In this paper, the ER approach is extended to group consensus (GC) situations for multiple attributive group decision analysis problems. In order to construct and check the GC, a <b>compatibility</b> <b>measure</b> between two belief structures is developed first. Considering two experts' utilities, the compatibility between their assessments is naturally constructed using the <b>compatibility</b> <b>measure.</b> Based on the compatibility between two experts' assessments, the GC at a specific level that may be the attribute level, the alternative level, or the global level, can be constructed and reached after the group analysis and discussion within specified times. Under the condition of GC, we conduct a study on the forming of group assessments for alternatives, the achievement of the aggregated utilities of assessment grades, and the properties and procedure of the extended ER approach. An engineering project management software selection problem is solved by the extended ER approach to demonstrate its detailed implementation process, and its validity and applicability. Decision analysis Multiple attributive group decision analysis Evidential reasoning approach Group consensus <b>Compatibility</b> <b>measure...</b>|$|R
40|$|Nowadays, large {{software}} {{systems are}} mostly built using existing services. These {{are not always}} designed to interact, i. e., their public interfaces often present some mismatches. Checking compatibility of service interfaces allows one to avoid erroneous executions when composing the services and ensures correct reuse and interaction. Service compatibility has been intensively studied, in particular for discovery purposes, but most of existing approaches return a Boolean result. In this paper, we present a quantitative approach for <b>measuring</b> the <b>compatibility</b> degree of service interfaces. Our method is generic and flooding-based, and fully automated by a prototype tool...|$|R
5000|$|Interchange <b>compatibility</b> <b>measures</b> {{how much}} {{individuals}} {{share the same}} need strengths.The example is two people with both high eA and wA ("Optimist" [...] or [...] "Overpersonal Personal-compliant"). They [...] "will be compatible because both will see Affection behaviors asthe basis of the relationship, and they will engageeach other around Affection needs."(i.e. freely give and receive).|$|R
40|$|Seven <b>compatibility</b> <b>measures</b> are {{paired with}} two fuzzy set {{aggregation}} operators to generate support measures for fuzzy evidential reasoning applied to an identification problem. The {{effectiveness of the}} combinations was empirically tested by varying the imprecision in the domain information and the evidence. The results are analyzed using the level-n ranking and the U-uncertainty of the support assignment...|$|R
40|$|We propose an {{approach}} to achieve appropriate exchange of services and data in distributed systems subject to semantic heterogeneity. We assume differentiated ontologies: that terms have formal definitions as concepts related to other concepts, that local concepts inherit from concepts that are shared, and that most or all primitives are shared. We then develop <b>measures</b> of description <b>compatibility</b> using {{the structure of the}} source and target definitions. We evaluate these measures by generating description-logic ontologies in artificial worlds. In our simulations, the "meaning" of a concept is its denotation in a finite universe of instances. The accuracy of the description <b>compatibility</b> <b>measures</b> can thus be judged by their success in predicting the overlap of concept denotations. Description compatibility can be used to guide agent search for services across communities that subscribe to differentiated ontologies. 1 1 Introduction We are interested in the semantics of agent commu [...] ...|$|R
40|$|Factors {{which will}} {{determine}} the future supply and cost of aviation turbine fuels are discussed. The most significant fuel properties of volatility, fluidity, composition, and thermal stability are discussed along with the boiling ranges of gasoline, naphtha jet fuels, kerosene, and diesel oil. Tests were made to simulate the low temperature of an aircraft fuel tank to determine fuel tank temperatures for a 9100 -km flight with and without fuel heating; the effect of N content in oil-shale derived fuels on the Jet Fuel Thermal Oxidation Tester breakpoint temperature was <b>measured.</b> Finally, <b>compatibility</b> of non-metallic gaskets, sealants, and coatings with increased aromatic content jet fuels was examined...|$|R
40|$|International audienceAttributes are an {{intermediate}} representation whose {{purpose is to}} enable parameter sharing between classes, a must when training data is scarce. We propose to view attribute-based image classification as a label-embedding problem: each class {{is embedded in the}} space of attribute vectors. We introduce a function which <b>measures</b> the <b>compatibility</b> between an image and a label embedding. The parameters of this function are learned on a training set of labeled samples to ensure that, given an image, the correct class has a higher compatibility than the incorrect ones. Experimental results on two standard image classification datasets are presented, resp. on the Animals With Attributes and on Caltech-UCSD-Birds datasets...|$|R
40|$|The {{self-organizing}} map (SOM) algorithm for finite data is derived as an approximate MAP estimation algorithm for a Gaussian mixture {{model with a}} Gaussian smoothing prior, which is equivalent to a generalized deformable model (GDM). For this model, objective criteria for selecting hyperparameters are obtained {{on the basis of}} empirical Bayesian estimation and crossvalidation, which are representative model selection methods. The properties of these criteria are compared by simulation experiments. These experiments show that the cross-validation methods favor more complex structures than the expected log likelihood supports, which is a <b>measure</b> of <b>compatibility</b> between a model and data distribution. On the other hand, the empirical Bayesian methods have the opposite bias. ...|$|R
40|$|We {{introduce}} a <b>measure</b> of the <b>compatibility</b> between quantum states [...] {{the likelihood that}} two density matrices describe the same object. Our measure is motivated by two elementary requirements, which lead to a natural definition. We list some properties of this measure, and discuss its relation {{to the problem of}} combining two observers' states of knowledge. Comment: 4 pages, no figure...|$|R
40|$|The {{paper is}} {{concerned}} with the problem of the explicit granulation of data in presence of some labeled patterns. The granulation process is realized as an organic growth of multi-dimensional hyperboxes guided by a <b>compatibility</b> <b>measure.</b> The organic growth signifies that there are no prior assumptions about the number and shape of the information granules. Instead, only the relative position and size of the patterns in the pattern space determine the progression of the granulation process. The rationale for a specific form of the <b>compatibility</b> <b>measure</b> is explained using some illustrative examples. The inclusion of a small number of labeled patterns in the input data is shown to provide a very effective way of coping with complex decision hyperplanes in multi-dimensional pattern spaces. The method is illustrated using several synthetic data sets as well as the Iris data set that is widely regarded as a reference for the comparison of classification and clustering algorithm...|$|R
40|$|Handling {{of large}} {{industrial}} mechanical assemblies implies structure interactions commonly modeled with contact formulations. In cases where component interfaces are discretized using non conforming meshes, classical contact solutions have difﬁculties producing correct contact pressure ﬁelds. The method {{presented in this}} paper gives a relevant <b>measure</b> of interface <b>compatibility</b> and shows how it can be exploited to obtain regular contact pressures or limit over-integration in the contact formulation...|$|R
40|$|Compatibility {{modification}} inference {{alters the}} consequent of a rule {{based on the}} compatibility of the antecedent with the input. Classically, set theoretic measures are {{used to assess the}} compatibility. Geometric plausible inference employs the dissemblance index to produce a metric <b>compatibility</b> <b>measure.</b> This type of inference utilizes graduality of nearness. Two approaches to geometric plausible inference are described and compared with other inference techniques using graduality...|$|R
