44|40|Public
5000|$|... {{which is}} maximum at [...] when , thus {{providing}} a <b>minimum</b> <b>precision</b> of [...] binary digits.|$|E
50|$|The {{standard}} {{also recommends}} extended format(s) {{to be used}} to perform internal computations at a higher precision than that required for the final result, to minimise round-off errors: the standard only specifies <b>minimum</b> <b>precision</b> and exponent requirements for such formats. The x87 80-bit extended format is the most commonly implemented extended format that meets these requirements.|$|E
5000|$|Programming {{languages}} {{should allow}} a user to specify a <b>minimum</b> <b>precision</b> for intermediate calculations of expressions for each radix. This {{is referred to}} as [...] "preferredWidth" [...] in the standard, and {{it should be possible to}} set this on a per block basis. Intermediate calculations within expressions should be calculated, and any temporaries saved, using the maximum of the width of the operands and the preferred width, if set. Thus for instance a compiler targeting x87 floating point hardware should have a means of specifying that intermediate calculations must use doubled extended format. The stored value of a variable must always be used when evaluating subsequent expressions, rather than any precursor from before rounding and assigning to the variable.|$|E
40|$|This paper {{presents}} a new method for computing {{whether or not}} two triangles in three dimensions intersect. The code is very efficient and requires <b>minimum</b> arithmetic <b>precision.</b> Indeed, all branching decisions are carried out by evaluating the signs of degree three polynomials. In addition, an efficient test is proposed for the two-dimensional case...|$|R
30|$|Furthermore, in our approach, we have {{obtained}} a <b>minimum</b> of <b>precision</b> rate equal to 25  % {{regardless of the}} similarity threshold (s) {{or the number of}} candidate queries (k) (i.e., at least 25  % of the recommendations are chosen by users). The precision rate is above 40  % in most cases, and may reach 76  %. These values demonstrate the efficiency of our approach.|$|R
30|$|The <b>minimum</b> {{pairwise}} clock <b>precision</b> (maximum error) of the SPS-SE protocol is 8.46 μ s. Therefore, {{right after}} two sensors run the SPS-SE protocol, {{the accuracy of}} their synchronized clocks ranges from 0 to 8.46 μ s.|$|R
5000|$|The new API {{features}} shader tracing and HLSL compiler enhancements, {{support for}} <b>minimum</b> <b>precision</b> HLSL scalar data types, [...] UAVs (Unordered Access Views) at every pipeline stage, target-independent rasterization (TIR), option to map SRVs of dynamic buffers with NO_OVERWRITE, shader processing of video resources, option to use logical operations in a render target, option to bind a subrange of a constant buffer to a shader and retrieve it, option to create larger constant buffers than a shader can access, option to discard resources and resource views, option to change subresources with new copy options, option {{to force the}} sample count to create a rasterizer state, option to clear {{all or part of}} a resource view, option to use Direct3D in Session 0 processes, option to specify user clip planes in HLSL on feature level 9 and higher, support for shadow buffer on feature level 9, support for video playback, extended support for shared Texture2D resources, and on-the-fly swapping between Direct3D 10 and 11 contexts and feature levels. Direct3D 11.1 includes new feature level 11_1, which brings minor updates to the shader language, such as larger constant buffers and optional double-precision instructions, as well as improved blending modes and mandatory support for 16-bit colour formats to improve the performance of entry-level GPUs such as Intel HD Graphics. WARP has been updated to support feature level 11_1.|$|E
30|$|The <b>minimum</b> <b>precision</b> of BAN time {{combines}} {{both the}} precision of SPS-SE and RATS.|$|E
30|$|The <b>minimum</b> <b>precision</b> of WSN time is 22.79 μ s, 68.46 μ s, and 31.34 μ s, {{for each}} {{aforementioned}} scenario, respectively. For these calculations, we considered {{a number of}} 10 consecutive BAN controller nodes and a re-synchronization period of 16 minutes.|$|E
40|$|Abstract. A common {{geometric}} {{problem in}} computer graphics and geographic information systems is {{to compute the}} arrangement {{of a set of}} n segments that can be colored red and blue so that there are no red/red or blue/blue crossings. We give a sweep algorithm that uses the <b>minimum</b> arithmetic <b>precision</b> and runs in optimal O(n log n + k) time and O(n) space to output an arrangement with k vertices, or O(n log n) time to determine k. Our initial implementation in Java can be found a...|$|R
40|$|A Finite Impulse Response (FIR) filter {{architecture}} {{based on}} a Distributed Arithmetic (DA) approach with two supply voltages and variable bit precision operation is presented. The filter is able to adapt itself to the <b>minimum</b> bit <b>precision</b> required by the incoming data and also operate at a lower voltage so that it still meets a fixed throughput constraint. As opposed to the worst case fixed precision design, our precision-on-demand implementation has an energy requirement that varies linearly with the average bit precision required by the input signal. We also demonstrate that 50 % to 60 % energy savings can easily be obtained {{in the case of}} speech data...|$|R
40|$|Abstract. We {{study the}} dark matter from an inert doublet and a complex scalar singlet {{stabilized}} by ZN symmetries. This field content is the minimal one that allows dimensionless semi-annihilation couplings for N> 2. We consider explicitly the Z 3 and Z 4 cases and take into account constraints from perturbativity, unitarity, vacuum stability, necessity for the electroweak ZN preserving vacuum to be the global <b>minimum,</b> electroweak <b>precision</b> tests, upper limits from direct detection and properties of the Higgs boson. Co-annihilation and semi-annihilation of dark sector particles as well as dark matter conversion significantly modify the cosmic abundance and direct detection phenomenology. ArXiv ePrint: 14 xx. xxxxar...|$|R
40|$|This paper {{presents}} the results of a large scale empirical study of coherent dependence clusters. All statements in a coherent dependence cluster depend upon the same set of statements and a↵ect the same set of statements; a coherent cluster’s statements have ‘coherent’ shared backward and forward dependence. We introduce an approximation to efficiently locate coherent clusters and show that it has a <b>minimum</b> <b>precision</b> of 97. 76...|$|E
30|$|In this paper, {{we focus}} {{on two of the}} above {{computation}} bottlenecks: one is the FK step in forward continued-based migration, which includes a square root function and a complex exponential operation; the other one is the 3 D convolution in reverse time migration. We perform automated precision exploration of these two computation cores, so as to figure out the <b>minimum</b> <b>precision</b> that can still generate accurate enough seismic images.|$|E
30|$|All {{activation}} {{values and}} link weights described below are Common Lisp long-floats in POPCO. In Steel Bank Common Lisp (SBCL), the implementation in which I usually run POPCO, long-floats are mapped to double-floats, as {{allowed by the}} ANSI Common Lisp standard (American National Standards Institute 1996). Double-floats have a <b>minimum</b> <b>precision</b> of 50 bits, with an 8 -bit minimum exponent precision. In practice this means that activation values and weights are specified to 16 or 17 decimal places.|$|E
50|$|The Service’s Wetlands Geodatabase {{contains}} five units (map areas) {{that are}} populated with digital vector data and raster images. These units include the conterminous U.S., Alaska, Hawaii, Puerto Rico and the U.S. Virgin Islands, and the Pacific Trust Territories. Each {{unit of the}} geodatabase contains seamless digital map data in ArcSDE geodatabase format. Data are in a single standard projection (Albers Equal-Area Conic Projection), horizontal planar units in meters, horizontal planar datum is the North American Datum of 1983 (also called NAD83), and <b>minimum</b> coordinate <b>precision</b> of one centimeter. Links are available to supplemental wetland information and metadata records that are compliant with the Federal Geographic Data Committee (FGDC) Content Standards for Digital Geospatial Metadata, Version 2.0. The Wetlands Geodatabase also contains other propriety Service datasets and developmental data, feature classes or information.|$|R
30|$|Table  5 {{shows the}} tools average recall (R) and {{precision}} (P) for all nine versions of MobileMedia. An overview of data {{shows that the}} minimum average recall is 0 % and the maximum is 58 %, while the <b>minimum</b> average <b>precision</b> is 0 % and the maximum 100 %. From the results of Table 5, we made the following observations. For God Class, JSpIRIT and PMD have similar accuracy, i.e., lower average recalls of 17 %, but higher precisions of 67 and 78 % when compared to JDeodorant, with a 58 % average recall and 28 % average precision. Despite the highest average recall, JDeodorant reports many false positives, increasing the effort to validate the results by programmers. inFusion has the lowest average recall of 9 %, however, it has an average precision of 33 %, 5 % higher than JDeodorant (28 %).|$|R
5000|$|The display {{has eight}} digits, {{with both a}} minus sign and an {{overflow}} indicator dot on the right hand side. The decimal point is [...] "floating"—it is positioned automatically by the calculator logic. This was an advanced feature for the time; many desk calculators of this era had fixed decimal points and required very wide displays to maintain a <b>minimum</b> level of <b>precision</b> across {{the entire range of}} numbers available. The QT-8D's floating decimal allowed its display to be much narrower while still keeping eight digits of precision.|$|R
30|$|Measurements {{were mainly}} {{carried out on}} the {{movement}} bed. The incoming flow velocity upstream of the pipe and the bottom velocity in the scour hole were measured using an acoustic doppler velocimeter (ADV). Such flow measurements were used to investigate the variation of bottom velocity for different height of spoiler and gap. Furthermore, after the scour attained equilibrium, the section profiles of the scour hole were measured using a measuring pin with the <b>minimum</b> <b>precision</b> of 0.1  mm so as to obtain the angle of repose and verify the formulas.|$|E
40|$|Multipartite quantum systems show {{properties}} {{which do}} not admit a classical explanation. In particular, even nonentangled states can enjoy a kind of quantum correlations called quantum discord. I discuss some recent results {{on the role of}} quantum discord in metrology. Given an interferometric phase estimation protocol where the Hamiltonian is initially unknown to the experimentalist, the quantum discord of the probe state quantifies the <b>minimum</b> <b>precision</b> of the estimation. This provides a physical interpretation to a widely investigated information-theoretic quantity. Comment: Contribution to the conference "DICE 2014 : Spacetime - Matter - Quantum Mechanics...|$|E
30|$|To {{solve the}} above {{problem in the}} seismic {{application}} domain, we develop a tool that performs an automated precision exploration of different number formats, and figures out the <b>minimum</b> <b>precision</b> that can still generate good enough seismic results. By using the minimized number format, we implement core algorithms in seismic applications (complex exponential step in forward continued based migration and 3 D convolution in reverse time migration) on FPGA and show speedups ranging from 5 to 7 by including the transfer time {{to and from the}} processors. Provided sufficient bandwidth between CPU and FPGA, we show that a further increase to 48 X speedup is possible.|$|E
30|$|Table  6 {{shows the}} tools average recall (R) and {{precision}} (P) {{for all the}} ten versions of Health Watcher. An overview of the tables shows that the minimum average recall is 0 % and the maximum is 100 %, while the <b>minimum</b> average <b>precision</b> is 0 % and the maximum 85 %. From the results of Table 6, we made the following observations. For God Class, PMD has the best accuracy, with an average recall of 100 % and the highest average precision of 36 %. JDeodorant has the second highest average recall of 70 % and the lowest average precision of 8 %, {{with the exception of}} inFusion. inFusion has the worst accuracy, with an average of 0 % for recall and undefined precision, since it did not detect any instance of God Class in any version of the system. Therefore, JSpIRIT presents a better accuracy when compared to inFusion, with an average recall and precision of 10 %.|$|R
30|$|The {{oil and gas}} {{industry}} has an increasingly large demand for high-performance computation over huge volume of data. Compared to common processors, field-programable gate arrays (FPGAs) can boost the computation performance with a streaming computation architecture and the support for application-specific number representation. With hardware support for reconfigurable number format and bit width, reduced precision can greatly decrease the area cost and I/O bandwidth of the design, thus multiplying the performance with concurrent processing cores on an FPGA. In this paper, we present a tool to determine the <b>minimum</b> number <b>precision</b> that still provides acceptable accuracy for seismic applications. By using the minimized number format, we implement core algorithms in seismic applications (the FK step in forward continued-based migration and 3 D convolution in reverse time migration) on FPGA and show speedups ranging from 5 to 7 by including the transfer time to and from the processors. Provided sufficient bandwidth between CPU and FPGA, we show that a further increase to 48 X speedup is possible.|$|R
40|$|The International Society for Clinical Densitometry (ISCD) Committee on Standards of Bone Measurement (CSBM) {{consists}} of experts in {{technical aspects of}} bone densitometry. The CSBM recently reviewed the scientific literature on cross-calibration and precision assessment. A report with recommendations {{was presented at the}} 2005 ISCD Position Development Conference (PDC). Based on a thorough review of the data by the ISCD Expert Panel during the conference, the ISCD adopted Official Positions with respect to (1) cross-calibration when changing or replacing hardware; (2) the approach to cross-calibration when an entire system is changed to one made by either the same or a different manufacturer; (3) when no cross-calibration study or bone mineral density (BMD) comparison is done between facilities; and (4) the <b>minimum</b> acceptable <b>precision</b> for an individual technologist. We present here the ISCD Official Positions on these topics that were established {{as a result of the}} 2005 PDC, together with the associated rationales and supportive evidence...|$|R
40|$|In {{order to}} achieve frame {{synchronization}} in packetbased communication systems, a known sequence (preamble) is sent {{at the beginning of}} each packet. This paper presents a low power scheme for preamble detection using dynamic precision configuration. The proposed scheme is based on the observation that signal processing algorithms at the front end (including mixer, Square root raised cosine (SRRC) matched filter and code matched filter(CMF)) of the receiver, can be operated with <b>minimum</b> <b>precision</b> during the acquisition phase without incurring significant acquisition performance degradation. The result is an acquisition system design that consumes less than 25 % of power dissipation that is consumed by other typical implementations...|$|E
30|$|This paper {{describes}} {{our work}} on accelerating seismic applications by using customized number representations on FPGAs. The focus {{is to improve}} the performance of the FK step in downward continued-based migration and the acoustic 3 D convolution kernel in reverse time migration. To investigate the tradeoff between precision and speed, we develop a tool that performs an automated precision exploration of different number formats, and figures out the <b>minimum</b> <b>precision</b> that can still generate good enough seismic results. By using the minimized number format, we implement the FK step in forward continued-based migration and 3 D convolution in reverse time migration on FPGA and show speedups ranging from 5 to 7 by including the transfer time to and from the processors. We also show that there are further potentials to accelerate these applications by above 10 or even 48 times.|$|E
40|$|Ground based laser {{scanning}} is now well {{settled in the}} fields of cultural heritage, as-built modeling and monitoring applications. We intend to use {{laser scanning}}to detect changes on building sites or inside facilities, mainly for monitoringpurposesbut also to be able to provide syntheticinformation in case ofan emergency. In both cases,the main constraint istime. A <b>minimum</b> <b>precision</b> on the measure ofmovementsisalso required,dependingon the type ofapplication. Laser scanner seemsto be the perfect toolfor such applicationsasit quicklyacquiresa large amount ofaccurate 3 D data. But the comparison ofso huge datasetsimpliesthe use of appropriate structuresand ad-hoc algorithms. A specific octree structure isdescribed,and then severalsimple cloud-to-cloud comparison techniquesare presented. The best one,based on the Hausdorffdistance computation isimproved on variouspoints. Also,asa fu lautomaticprocessseemssti lunachievable,a software frameworkhasbeen developed. It intendsto minimize human intervention and therefore prevents from wasting the LiDAR speed in time-consuming post-processing operations...|$|E
40|$|Shirayanagi and Sweedler {{proved that}} a large class of {{algorithms}} over the reals can be modified slightly so that they also work correctly on fixed-precision floating-point numbers. Their main theorem states that, for each input, there exists a <b>precision,</b> called the <b>minimum</b> converging <b>precision</b> (MCP), at and beyond which the modified “stabilized ” algorithm follows the same sequence of instructions {{as that of the}} original “exact ” algorithm. Bounding the MCP of any non-trivial and useful algorithm has remained an open problem. This paper studies the MCP of an algorithm for finding the GCD of two univariate polynomials based on the QRfactorization. We show that the MCP is generally incomputable. Additionally, we derive a bound on the minimal precision at and beyond which the stabilized algorithm gives a polynomial with the same degree as that of the exact GCD, and another bound on the minimal precision at and beyond which the algorithm gives a polynomial with the same support as that of the exact GCD...|$|R
5000|$|Two {{methods are}} {{employed}} {{to find the}} nodes. [...] One is to use some type of voltage indicator, such as an RF voltmeter or light bulb, attached {{to a pair of}} contacts that slide up and down the wires. [...] When the bulb reaches a node, the voltage between the wires goes to zero, so the bulb goes out. If the indicator has too low an impedance it will disturb the standing wave on the line, so a high impedance indicator must be used; a regular incandescent bulb has too low a resistance. Lecher and early researchers used long thin Geissler tubes, laying the glass tube directly across the line. The high voltage of early transmitters excited a glow discharge in the gas. In modern times small neon bulbs are often used. One problem with using glow discharge bulbs is their high striking voltage makes it difficult to localize the exact voltage <b>minimum.</b> In <b>precision</b> wavemeters an RF voltmeter is used.|$|R
40|$|This paper {{describes}} a robust architecture for high speed serial links for embedded SoC applications, implemented {{to satisfy the}} 1. 5 Gb/s and 3 Gb/s Serial-ATA PHY standards. To meet the primary design requirements of a sub-system that is very tolerant of device variability and is easy to port to smaller nanometre CMOS technologies, a <b>minimum</b> of <b>precision</b> analog functions are used. All digital functions are implemented in rail-to-rail CMOS with maximum use of synthesized library cells. A single fixed frequency low-jitter PLL serves the transmit and receive paths in both modes so that tracking and lock time issues are eliminated. A new oversampling CDR with a simple feed-forward error correction scheme is proposed which relaxes the requirements for the analog front-end {{as well as for}} the received signal quality. Measurements show that the error corrector can almost double the tolerance to incoming jitter and to DC offsets in the analog front-end. The design occupies less than 0. 4 mm 2 in 90 nm CMOS and consumes 75 m...|$|R
40|$|Abstract—The {{precision}} {{used in an}} algorithm {{affects the}} error and performance of individual computations, the memory usage and the potential parallelism for a fixed hardware budget. This paper describes a new method to determine the <b>minimum</b> <b>precision</b> required to meet a given error specification for an algorithm that consists of the basic algebraic operations. Using this approach, {{it is possible to}} significantly reduce the computational word-length in comparison to existing methods, and this can lead to superior hardware designs. We demonstrate the proposed procedure on an iteration of the conjugate gradient algorithm, achieving proofs of bounds that can translate to global wordlength savings ranging from a few bits to proving the existence of ranges that must otherwise be assumed to be unbounded when using competing approaches. We also achieve comparable bounds to recent literature in {{a small fraction of the}} execution time, with greater scalability...|$|E
30|$|This paper {{analyzes}} {{the determinants of}} governance transparency. In our model, entrepreneurs optimally decide the precision of their earning reporting by trading off the possibility of expropriating profits against the capacity to attract external funding. We find that information is only valuable if enough quality of it is disclosed. Otherwise, the entrepreneur will always pretend to be unsuccessful and the capital market will break down. If, by contrast, a <b>minimum</b> <b>precision</b> level is ensured, fund diversion will be zero but full disclosure is still not achieved. We show that an important driving force behind governance transparency is product market competition. Tougher competition leads to more firms competing for funding, which in turn changes how resources are allocated since each individual firm becomes less important in the portfolio choice. Firms react to this loss of market power by increasing transparency. Furthermore, firms characterized by low corporate profits or firms {{in a country with}} a strong legal system {{will be more likely to}} avoid voluntary disclosure regimes.|$|E
40|$|Abstract – The study {{objective}} is to extract HydroGeoMorphic Units in wetlands from LiDAR data estimating the required accuracy for assessing their functions in various hydrological and vegetation environments. Several Digital Terrain Models (DTMs) have been derived from LiDAR data with different point densities, ranging from 4 to 1 point/m with four interpolation methods. Then, the HGMU have been extracted from the more accurate DTM with an Object-based Image Analysis. This approach, validated with field data, has been applied in a study site located near the Mont-Saint-Michel, France. Results showed that the HGMU map quality highly depends primarily on the LIDAR data precision (point-density) and, in a lesser extent, on the interpolation method used. A <b>minimum</b> <b>precision</b> of 2 points per m was required to properly restitute the ditches network in wetlands and thus to evaluate their hydrological functions. The Nearest Neighbors interpolation provides best results and in the fastest computation time...|$|E
30|$|Finally, {{the last}} {{interesting}} work we propose {{as a future}} extension of this work is building domain-specific catalogs of software concerns. For example, {{with the help of}} experts in some domain, such as embedded systems and business documents related to this domain, it is possible to build catalogs of concerns to be imported and updated by other researchers/professionals in the ObasCId-Tool. From an economic perspective, these catalogs could be sold by companies specialized in some domain, ensuring <b>minimum</b> recall and <b>precision</b> values provided by these catalogs in the context of concern identification and classification. This idea would be an interesting way of reducing the cost/effort in using ObasCId approach.|$|R
40|$|Abstract. A power {{analysis}} allows {{estimation of the}} probability of detecting upward or downward trends in abundance using linear regression, given number of samples and estimates of sample variability and rate of change. Alternatively, the <b>minimum</b> number or <b>precision</b> of samples required to detect trends with a given degree of confidence can be computed. The results are applicable to an experimental situation in which samples are taken at regular intervals in time or space. The effects of linear and exponential change and of having sample variability {{be a function of}} abundance are investigated. Results are summarized graphically and, as an example, applied to the monitoring of the California sea otter population with aerial surveys...|$|R
40|$|In {{this paper}} we propose a low-error {{approximation}} of the sigmoid function and hyperbolic tangent, which are mainly used to activate the artificial neuron, based on the piecewise linear method. Here, the hyperbolic tangent is alternatively approximated by exploiting its mathematical relationship with the sigmoid function, showing better results. Special {{attention has been paid to}} study the <b>minimum</b> number of <b>precision</b> bits to achieve the convergence of a multi-layer perceptron network in finite arithmetic machine. All the approximation results show lower mean relative and absolute error than those reported in the state-of-the-art. Finally, the sigmoid digital implementation is discussed and assessed in terms of work frequency, complexity and error in comparison with the state-of-the-art. (C) 2011 Elsevier B. V. All rights reserved...|$|R
