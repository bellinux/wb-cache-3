18|120|Public
5000|$|Conflicts {{are handled}} {{with the older}} file being renamed with a [...] "sync-conflict" [...] suffix (along with time and date stamp), {{enabling}} the user {{to decide how to}} manage two or more files of the same name that have been changed between syncing. GUI Wrappers can use these files to present the user with a method of resolving conflicts without having to resort to <b>manual</b> <b>file</b> handling.|$|E
50|$|Despite this, Microsoft {{in their}} {{application}} releases muddied the issue, releasing 32-bit versions of Microsoft Office {{right up to}} Office 97 SR2b, but relying upon 16-bit versions of Internet Explorer technology. This is probably because 32-bit versions of Internet Explorer 4.0 and later integrated with the Windows 95 desktop, and NT 3.51 still used the Windows 3.1 desktop. Thereafter, up to IE 5.0, but no later 5.x versions, were offered. However, the open-source SeaMonkey internet suite supported NT 3.51 through version 1.1.19, released on 16 March 2010; it requires a few <b>manual</b> <b>file</b> updates to work without compromising browsing security.|$|E
50|$|An {{efficient}} CMS, {{like a good}} relational database, {{should not}} have duplicate records and should not require that the same information be recorded {{in more than one}} place in the system. At the same time, the system should be flexible enough to accommodate more data as the collections expand. The user must also understand that not all information must be entered into a Collections Management System; for example, complex information such as complicated dimensions and measurements. Some institutions may not want to record confidential information such as private donor information in a CMS and instead keep it in a <b>manual</b> <b>file</b> or a separate, secure digital file, with pointers to the file’s location recorded in the CMS. However, others argue that such confidential information should be recorded in the CMS to protect the information {{in the event of a}} disaster where manual files may be destroyed.|$|E
5000|$|The {{relevant}} {{filing system}} issue - What {{is meant by}} a [...] "relevant filing system" [...] {{in the definition of}} [...] "data" [...] in section 1(1) of the 1998 Act, so as to render personal information recorded in a <b>manual</b> <b>filing</b> system [...] "personal data" [...] disclosable to its subject under section 7(1) - more particularly here, was the FSA's <b>manual</b> <b>filing</b> such a system so as to require it to disclose to Mr. Durant from those files information that would, if it were in computerised form, constitute [...] "personal data" [...] within section 1(1)? ...|$|R
50|$|The main {{decision}} {{was given by}} Auld LJ, who commenced by noting that the primary objective of the 1995 Data Protection Directive, upon which the Act was based, was to protect individuals' fundamental rights, notably {{the right to privacy}} and accuracy of their personal data held by others ("data controllers") in computerised form or similarly organised <b>manual</b> <b>filing</b> systems.|$|R
40|$|Georgetown University {{has created}} an online card catalog based on machine {{readable}} cataloging records (MARC) loaded from archival tapes or online via the OCLC network. The system is programmed in MUMPS and uses the medical subject headings (MeSH) authority file created by the National Library of Medicine. The online catalog may be searched directly by library users and has eliminated the need for <b>manual</b> <b>filing</b> of catalog cards...|$|R
40|$|We {{present an}} initial {{exploration}} into the usability of 3 D file browsing. To explore the 3 D file browsing technique design space, we analyzed {{the existing literature}} and developed three representative 3 D file browsing techniques that cover many of their characteristics. Block 3 D uses a priority weighting scheme to elevate and display files in a grid-based structure. Cluster 3 D uses sets of animated racks to display files. LTreeCube 3 D visualizes files and directories using groups of semi-transparent cubes within a larger cube-like structure. We conducted an experiment exploring the affect these 3 D file browsing technique have on users in a <b>manual</b> <b>file</b> searching task. Our evaluation is based on task completion time and a postquestionnaire used to gather subjective feedback on each technique in terms of user preference. The results indicate that users completed the <b>manual</b> <b>file</b> search task significantly faster using Block 3 D than both LTreeCube 3 D and Cluster 3 D. Although subjective ranking showed users preferred the Block 3 D technique, user feedback also showed merits of the other techniques...|$|E
40|$|Based on an {{analysis}} of the existing literature, we extracted important features regarding 3 D file organization and layout. In this way, three separate 3 D file browsing techniques were evaluated in a comparison study. Block 3 D uses a priority weighting scheme to elevate and display files in a grid-based structure. Cluster 3 D uses sets of animated racks to display files. TreeCubePlus 3 D visualizes files and directories using groups of semi-transparent cubes within a larger cube-like structure. Across all techniques, each file was represented as a thumbnail image of the file’s image, document or document with pictures data. The thumbnails were also augmented with meta-information such as filename and relevancy information to simulate a realistic search. Our experiment explores the effectiveness of each 3 D file browsing technique in a <b>manual</b> <b>file</b> searching task. Our evaluation is based on task completion time and a post-questionnaire used to gather subjective feedback on each technique in terms of user preference. The results indicate that users completed the <b>manual</b> <b>file</b> searc...|$|E
40|$|These Matlab {{functions}} extend Janelia Farms 2 ̆ 7 ScanImage microscope control software. The functions {{allow the}} user to view, mark and track positions of the motor stage/objective in x, y, and z, which get displayed in a separate window. This helps {{to keep track of}} the stage movements, regions of interest and makes it easy to document areas imaged and map out the specimen being investigated. For detailed information about the functionality as well as installation, view the included <b>manual</b> <b>file...</b>|$|E
50|$|The Central Board of Direct Taxes (CBDT) {{has made}} it {{compulsory}} for Individual and Hindu Undivided Families earning an income in excess of Rupees Five Lakh to file their Tax Returns only through the E-Filing Process. The <b>manual</b> <b>filing</b> of returns is no more an option for Assessees who come under this category. Electronic Filing of their Tax Returns {{is the only way}} this category can file their Income Tax Returns.|$|R
5000|$|... whereis, a Unix command, {{can locate}} some special files of a Unix command like the binary, source and <b>manual</b> page <b>files.</b>|$|R
5000|$|In September and October 2001, Mr Durant {{made two}} {{requests}} to the FSA under section 7 of the Data Protection Act, seeking disclosure of personal data held by it, both electronically and in <b>manual</b> <b>files.</b> In October 2001 the FSA provided Mr Durant with copies of documents relating {{to him that}} it held in computerised form, disclosure that went beyond his entitlement under the Act, which is to have communicated to him in an intelligible form [...] "information constituting any personal data" [...] {{of which he was}} the subject. However some of the documents were redacted so as not to disclose the names of others. The FSA later made further disclosure of computerised material. But the FSA refused the whole of his request for information held on <b>manual</b> <b>files</b> on the ground that that the information sought was not [...] "personal" [...] within the definition of [...] "personal data" [...] in section 1(1) of the 1998 Act, and that, even if it was, it did not constitute [...] "data" [...] within the separate definition of that word in section 1(1)(c). The FSA has since maintained that refusal, which encompasses four categories of file.|$|R
40|$|Genome-wide linkage {{analysis}} using multipletraits and statistical software packages is a tedious process {{which requires a}} significant amount of <b>manual</b> <b>file</b> manipulation. Different linkage analy-sis programs require different input file formats, making the task of analyzing data with multiple methods even more time-consuming. We have developed a software tool, AUTOGSCAN, that auto-mates file formatting, the running of statistical analyses, and the summarizing of resulting statistics for whole genome scans with a push of a button, using several independent, and often idiosyncratic, statistical software packages such as MERLIN, SOLAR and GENEHUNTER. We also describe a program, ANALYZE, designed to run qualitative {{linkage analysis}} with several different statistica...|$|E
40|$|The {{sheer volume}} of modern data makes <b>manual</b> <b>file</b> {{management}} impractical. Search-oriented file systems, where data and metadata are indexed for fast search, are increasingly viewed as a necessity, everywhere from desktops to HPC. However, current techniques have been designed and tested for file system metadata, such as POSIX metadata, and fail {{to account for the}} wide variety of metadata users would like to search. In particular, the scientific world has been vocal about a desire to search extended and content metadata. While file system metadata is well characterized by a variety of workload studies, scientific metadata is much less well understood. We characterize scientific metadata, in order to better understand the implications for index design. We demonstrate that previously suggested index structures, such as k-d trees, R-trees, and row major databases, are not well suited to scientific metadata. Finally, we provide suggestions for a system design based on our findings. ...|$|E
40|$|Background: Data {{extraction}} tools (DETs) {{are increasingly}} being used for research and audit of general practice, despite their limitations. Objective: This study explores the accuracy of Pap smear rates obtained with a DET {{compared to that of}} the Pap smear rate obtained with a <b>manual</b> <b>file</b> audit. Method: A widely available DET was used to establish the rate of Pap smears in a large multi-general practice (multi-GP) in regional New South Wales followed by a manual audit of patient files. The main outcome measure was identification of possible discrepancies between the rates established. Results: The DET used significantly underestimated the level of cervical screening compared to the manual audit. In some instances, the patient file contained phone/specialist record of Pap smear conducted elsewhere, which accounted for the failure of the DET to detect some smears. Those patients who had Pap smears whose pathology codes differed between time intervals, i. e. from different pathology providers or from within the same provider but using a different code, were less likely to have had their most recent Pap smear detected by the DET (p 3 ̆c 0. 001). Conclusion: Data obtained from DETs should be used with caution as they may not accurately reflect the rate of Pap smears from electronic medical records...|$|E
5000|$|The {{personal}} data issue - What makes [...] "data", whether held in computerised or <b>manual</b> <b>files,</b> [...] "personal" [...] within {{the meaning of}} the term [...] "{{personal data}}" [...] in section 1(1) of the 1998 Act so as to entitle a person identified by it to its disclosure under section 7(1) of the Act - more particularly in this context, to what, if any, extent, is information relating to the FSA's investigation of Mr. Durant's complaint about Barclay's Bank within that definition? ...|$|R
5000|$|On {{retirement}} from the Royal Navy in 1973 Moore became {{editor of the}} authoritative Jane's Fighting Ships, which he did for the fifteen years until 1988, expanding the number of countries whose navies were described in detail from 108 to 152. During this period the first personal computers became available, but Moore did not use one, preferring a <b>manual</b> <b>filing</b> system in a shed in the garden {{at his home in}} Rickney, Sussex and communicating with contacts worldwide by letters written in longhand, ...|$|R
40|$|The authors {{report the}} {{incidence}} of cancer in a large cohort of employees identified from all 99 Danish utility companies. Personal data and information on employment and exposure to magnetic fields and asbestos were obtained from <b>manual</b> <b>files</b> at the companies, the Danish Supplementary Pension Fund, and the public payroll administration. A total of 32, 006 individuals with more than 3 months of employment were linked with the files of the Danish Cancer Registry. The period of follow-up for cancer occurrence among the employees was fro...|$|R
40|$|Abstract {{copyright}} UK Data Service {{and data}} collection copyright owner. The objectives of the project were: to measure and investigate, {{with the use of}} stated preference (SP) techniques and qualitative interviews, citizens' demands for a range of urban living solutions; to model citizens' demands for particular components of the compact city environment; to use results from SP analysis and qualitative research to gain understanding of the market for new forms of sustainable urban living; to use the results to gain an understanding of the problems and possibilities when attempting to re-engineer the residential preferences of UK citizens in favour of more sustainable forms of living environment; to interview a small number of key players in the development industry to assist understanding of the supply-side of the market. Main Topics : The dataset contains preference rankings and purchase prices for six sets of six residential scenarios (a total of 36 options). These scenarios comprise different combinations of the eight sets of variables: location; dwelling type; access to open space; parking provisions; land use mix; dwelling character; other residents' socio-economic characteristics; and frequency of passers-by. The file also contains responses to the administered questionnaire detailed in the coding <b>manual</b> (<b>file</b> name: SPcoding), including, household and respondent characteristics...|$|E
40|$|Abstract — The Grid Collector is {{a system}} that {{facilitates}} the effective analysis and spontaneous exploration of scientific data. It combines an efficient indexing technology with a Grid file management technology to speed up common analysis jobs on high-energy physics data and to enable some previously impractical analysis jobs. To analyze a set of high-energy collision events, one typically specifies the files containing the events of interest, reads all the events in the files, and filters out unwanted ones. Since most analysis jobs filter out significant number of events, a considerable amount of time is wasted by reading the unwanted events. The Grid Collector removes this inefficiency by allowing users to specify more precisely what events are of interest and to read only the selected events. This speeds up most analysis jobs. In existing analysis frameworks, the responsibility of bringing files from tertiary storages or remote sites to local disks falls on the users. This forces most of analysis jobs to be performed at centralized computer facilities where commonly used files are kept on large shared file systems. The Grid Collector automates file management tasks and eliminates the labor-intensive <b>manual</b> <b>file</b> transfers. This makes it much easier to perform analyses that require data files on tertiary storages and remote sites. It also makes more computer resources available for analysis jobs since they are no longer bound to the centralized facilities...|$|E
40|$|Although Root ZX is {{considered}} the gold standard electronic foramen locator (EFL), two variations of this device were launched, however without different operating mechanisms. This investigation aims to evaluate the precision of Root ZX (RZX), Root ZX II (RII), and Root ZX Mini (RM) EFLs. After access cavity preparation, 32 mandibular single rooted human premolars had their real length measured {{with the aid of}} a # 15 K-type <b>manual</b> <b>file</b> under magnification (25 x). Electronic measurements were performed by the devices in an alternate order until the apical foramen was reached (0. 0). Each measurement was performed with adjusted file to the real length of the teeth and verified with a digital caliper. The accuracy of the EFLs was 68. 8 % (RZX), 65. 8 % (RII), and 68. 8 % (RM), considering ± 0. 5 [*]mm as a margin of tolerance. The mean errors of the devices were 0. 37 ± 0. 25 [*]mm (RZX), 0. 41 ± 0. 34 [*]mm (RII), and 0. 32 ± 0. 28 [*]mm (RM). ANOVA and Tukey test were applied to analyze the obtained data, which showed that there were no statistically significant differences among the locators (P>. 05). It can be concluded that the three tested devices demonstrated precise measurements of the real length of the canal without performance differences among them...|$|E
40|$|The Health Sciences Library, University of Maryland, has {{implemented}} the Integrated Library System (ILS), a minicomputer-based library automation system {{developed by the}} Lister Hill National Center for Biomedical Communications, National Library of Medicine. The process of moving a library from a manual to a computerized system required comprehensive planning and strong commitment by the staff. Implementation activities included hardware and software modification, conversion of <b>manual</b> <b>files,</b> staff training, and system publicity. ILS implementation resulted in major changes in procedures in the circulation, reference, and cataloging departments...|$|R
40|$|AbstractThe main {{objective}} {{of this study is}} to estimate compliance costs for personal income tax (PIT) system in Malaysia and to investigate the effect of e-filing on compliance costs of PIT system in Malaysia. It is found that, on average, e-filing consumed about 10 hours while <b>manual</b> <b>filing</b> took about 13 hours. However, statistically, the difference is not significant. The findings suggest that e-filing is only a tool to facilitate the filing process. It is not meant to reduce the whole burden of annual filing for personal income taxpayers...|$|R
40|$|A {{system is}} {{described}} {{for the maintenance}} of cumulative haematology records for selected patients using a laboratory minicomputer with limited storage capacity. Records are indexed by the patient's name and location since, as in the majority of hospital laboratories, there is no unique numbering scheme which covers all patients. The computer system imitates the procedures of <b>manual</b> <b>filing,</b> listing uncertain record matches for human decision. Applications include the production of a regularly updated printing of of cumulative files, the preparation of graphs from cumulative blood count records, and the provision of a summary and follow-up service for general practitioners...|$|R
40|$|The Grid Collector is {{a system}} that {{facilitates}} the effective analysis and spontaneous exploration of scientific data. It combines an efficient indexing technology with a Grid file management technology to speed up common analysis jobs on high-energy physics data and to enable some previously impractical analysis jobs. To analyze a set of high-energy collision events, one typically specifies the files containing the events of interest, reads all the events in the files, and filters out unwanted ones. Since most analysis jobs filter out significant number of events, a considerable amount of time is wasted by reading the unwanted events. The Grid Collector removes this inefficiency by allowing users to specify more precisely what events are of interest and to read only the selected events. This speeds up most analysis jobs. In existing analysis frameworks, the responsibility of bringing files from tertiary storages or remote sites to local disks falls on the users. This forces most of analysis jobs to be performed at centralized computer facilities where commonly used files are kept on large shared file systems. The Grid Collector automates file management tasks and eliminates the labor-intensive <b>manual</b> <b>file</b> transfers. This makes it much easier to perform analyses that require data files on tertiary storages and remote sites. It also makes more computer resources available for analysis jobs since they are no longer bound to the centralized facilities...|$|E
40|$|During {{the last}} few years {{developing}} computer industry has realized one of the potential, which is the Armed Forces Applications. The nature of the Functional Applications was appropriate for the database systems. This is because the <b>manual</b> <b>file</b> organization and the flow charts were available for being computerized. Nevertheless classical database queries became insufficient in satisfying the needs of the users. The reason for this was the requirement of professional users who knew Structured Query Language (SQL) and database technology very well. Only in the standalone Geographic Information System (GIS) software it was possible to have visual analysis. By the integration of the database applications and GIS engines, data reliability check and visual analysis opportunities were provided to the users. However there were some disadvantages of the hybrid environment. These are SQL proficiency requirements, limitations and low performance of the GIS environment in terms of digital data presentations. Currently, command and Control Information Systems (C 2 IS) are serving as Decision Support Systems (DSS). Many different functional information systems such as transportation and movement, health, logistics, intelligence etc. come together under the umbrella of C 2 IS. These kinds of complex information systems had needed some standards for applications and dedicated database architectures. International Security Organizations (ISO) ruled these standards (such as NATO ACE ACCIS GIS trial results, ATCCIS or MIP architectures) for the member countries and users to secure the interoperability among the C 2 IS which were developed independently. At the end of this period, a new kind of problem arose. Existed database architectures and GIS would b...|$|E
40|$|Statement of Problem: Root canal {{cleaning}} and {{elimination of the}} source of infection are the most important basis and the main requirements for successful root treatment since the main cause of failure in root treatment is the permeation of bacteria or their products into the periapical tissues. Nowadays, the newly designed and prcsented instruments for canal instrumentation can improve root treatment. Purpose: The aim {{of this study was to}} compare the decrease in the number of intracanal Enterococcus-faecalis (Ef) among three mechanical instrumentation methods: manual (K-type) and rotary (Race, Profile). Materials and Methods: In this experimental study, 30 single rooted teeth were selected. Three cases were considered as negative and three cases as posetive controls and 24 remainder cases were divided into three experimental groups. All root canals were inoculated by Ef and samples were taken from all canals to prepare microbial cultures. The three instrumentation procedures were: - Crown- down technique with K-type manual system file - Crown- down technique with Profile rotary system - Crown- down technique wiht Race rotary system After instrumentation, microbial cultures were taken from root canals and the reduction rate of bacteria were evaluated and compared using one way ANOVA test with P< 0. 05 as the limit of significance. Results: There was no significant difference among the three instrumentation procedures regarding bacterial elimination. Conclusion: According to the finding of this study, K-type <b>manual</b> <b>file,</b> Profile and Race rotary systems, all can be used in canal instrumentaion. However, since manual files are more accessible and require less equipment compared with rotary systems, and since the ability of all of these methods is identical regarding bacterial elimination, manual files can be used in straight canal instead of rotary systems...|$|E
5000|$|PaperKiller and RoboAuthor: Help {{authoring}} tools for creating <b>manuals,</b> guides, help <b>files</b> ...|$|R
40|$|The <b>manual</b> <b>files</b> {{are still}} widely used for initial canal {{negotiation}} prior {{the use of}} nickel-titani-um shaping instruments, to determine working length and to verify patency. A mechanical glide path can be performed using <b>manual</b> <b>files</b> with handpieces, such as M 4 Handpiece (SybronEndo, USA) that allows a 30 °/ 30 ° reciprocating motion. The Pathfinders (SybronEndo, USA) are hand files designed to negotiate complex canals, made from stainless steel (SS) or carbon steel (CS) alloys. The aim of this in vitro study was to compare cyclic fatigue resistance of these {{two different types of}} manual Pathfinder instruments used in a M 4 reciprocating handpiece in double curved artificial canals. Manual instruments designed for glide path (size # 9 ISO. 02 taper) made from different alloys were selected: Group SS - stainless steel Pathfinders (Sybron Endo) and Group CS - carbon steel Pathfinders size K 2 (Sybron Endo). Ten instruments of each group were tested for resistance to cyclic fatigue with a reciprocating M 4 handpiece inside an artificial S-shaped canal; the time to fracture was recorded for each file and data were statistically analyzed (ANOVA). Mean values (and SD) were 527 (± 89) seconds for the CS instruments and 548 (± 104) seconds for the SS files. No significant differences were observed between groups (p= 0, 062). According to the results, both carbon and stainless steel instruments presented similar fatigue resistance when used with M 4 reciprocating handpiece in double curved canals...|$|R
50|$|Xing.com plugins are {{available}} for free download that allow contact synchronization with Lotus Notes, Microsoft Outlook, Windows Address Book and Outlook Express. It also allows <b>manual</b> CSV <b>File</b> import-export and has a Firefox search plug-in.|$|R
40|$|PopulationProfiler – is {{light-weight}} cross-platform open-source {{tool for}} data analysis in image-based screening experiments. The main {{idea is to}} reduce per-cell measurements to per-well distributions, each represented by a histogram. These can be optionally further reduced to sub-type counts based on gating (setting bin ranges) of known control distributions and local adjustments to histogram shape. Such analysis is necessary {{in a wide variety}} of applications, e. g. DNA damage assessment using foci intensity distributions, assessment of cell type specific markers, and cell cycle analysis. The software imports measurements from a simple text file, visualizes population distributions in a compact and comprehensive way, and can create gates for subpopulation classes based on control samples. The simple graphical user interface (GUI) allows selection of multiple csv files with image-based screening data. Each file is treated as a separate plate (i. e. independent experiment) with rows representing cell measurements. One measurement is processed at a time and cells are grouped based on well labels. The measurement is selected by the user from a drop-down list created from the csv file header (first row). The GUI also allows selection of control wells based on the treatment labels. If such labels are not available, the user can select control wells manually. The corresponding data is pooled and stored as a separate record in the output csv file. PopulationProfiler thereafter calculates and displays the distribution of the selected measurement as a histogram for each well. A vector representation of each well’s histogram is saved in the output file, and can be used as input for e. g., cluster analysis, elsewhere. The cell count for each well is also saved as a measure of statistical relevance of population effects. For more information on PopulationProfiler use go to the <b>manual</b> <b>file.</b> This software can be cited as: Matuszewski, D. J., Wählby, C., Puigvert, J. C., Sintorn, I. -M. PopulationProfiler: a tool for population analysis and visualization of image-based cell screening data. PLoS ONE. (2016...|$|E
40|$|The {{computer}} simulation of scientific processes is playing {{an increasingly important}} role in scientific research. For example, the development of adequate flight simulation environments, numeric wind tunnels, and numeric propulsion systems is reducing the danger and expense involved in prototyping new aircraft and engine designs. One serious problem that hinders the full realization of the potential of scientific simulation {{is the lack of}} tools and techniques for dealing with the heterogeneity inherent in today's computational resources and applications. Typically, either ad hoc connection techniques, such as <b>manual</b> <b>file</b> transfer between machines, or approximation techniques, such as boundary value equations, are employed. This dissertation develops a programming model in which scientific applications are designed as heterogeneous distributed programs, or meta-computations. The central feature of the model is an interconnection system that handles the transfer of control and data among the heterogeneous components of the meta-computation, and provides configuration tools to assist the user in starting and controlling the distributed computation. Key benefits of this programming model include the ability to simulate the interactions among the physical processes being modeled through the free exchange of data between computational components. Another benefit is the possibility of improved user interaction with the meta-computation, allowing the monitoring of intermediate results during long simulations and the ability to steer the simulation, either directly by the user or through the incorporation of an expert system into the meta-computation. This dissertation describes a specific realization of this model in the Schooner interconnection system, and its use in the construction of a number of scientific meta-computations. Schooner uses a type specification language and an application-level remote procedure call mechanism to ease the task of the scientific programmer in building meta-computations. It also provides static and dynamic configuration management features that support the creation of meta-computations from components at runtime, and their modification during execution. Meta-computations constructed using Schooner include examples involving molecular dynamics and neural nets. Schooner is also in use in several major projects as part of a NASA effort to develop improved jet engine simulations...|$|E
40|$|O objetivo do presente estudo foi avaliar o trasnporte apical e a capacidade de limpeza promovida por diferentes t?cnicas de cateterismo associadas ao uso de um sistema de instrumento ?nico. Foram selecionados 52 molares inferiores com grau de curvatura da raiz mesial entre 20 ? e 40 ? e raio de curvatura menor ou igual a 10 mm. Com o aux?lio de instrumentos tipo K # 10, avaliou-se a presen?a de forames independentes. Com o objetido de padronizar o tamanho dos dentes, as coroas foram parcialmente seccionadas em 16 mm e mediu-se 3 mil?metros na raiz mesial, que foram transpostos para a raiz distal, e esta foi seccionada naquele ponto. Os dentes selecionados foram separados aleatoriamente (n= 13) dentes cada, Grupo 1, cateterismo inicial com lima manual tamanhos 10 e 15 e instrumenta??o com R 25; Grupo 2, Reciproc R 25 sem cateterismo; Grupo 3, cateterismo com PathFile e preparo com Reciproc R 25 e Grupo 4, controle, que n?o foi instrumentado. Em seguida, a raiz distal foi inserida em blocos de resina, para padroniza??o das tomografias na mesma posi??o antes e ap?s a instrumenta??o e para ser afixado em uma morsa de bancada, a fim de se padronizar a posi??o de instrumenta??o. O comprimento de trabalho dos canais mesiais foi definido a 0, 5 mm aqu?m ao ?pice. Ap?s o cateterismo, os canais foram instrumentados seguindo as recomenda??es do fabricante. Hipoclorito de s?dio a 2, 5 % foi a solu??o irrigadora utilizada. Ap?s a instrumenta??o, os dentes foram submetidos novamente a tomografia e em seguida o ter?o apical de cada esp?cime foi submetido ao processamento histot?cnico de rotina para obten??o de 12 cortes com 5 ?m de espessura. A an?lise do desvio apical se deu pela aplica??o de f?rmulas aos valores obtidos antes e ap?s a instrumenta??o e a an?lise histol?gica se deu pela avalia??o dos cortes histol?gicos e determina??o do percentual de debris presente nos canais por meio da sobreposi??o de uma grade de integra??o. Os dados obtidos foram submetidos a testes de Kruskall-Wallis e teste de M?ltiplas Compara??es de Dunn. Os resultados do desvio apical n?o mostraram diferen?as significantes entre os grupos avaliados (p > 0, 05). Em rela??o a capacidade de limpeza, houve diferen?a significativa entre o grupo 1 e o grupo 2 (p ? 0, 01). Com base na metodologia empregada e nos resultados obtidos, concluiu-se que n?o houve diferen?a significante em rela??o ao desvio apical entre as diferentes t?cnicas de cateterismo, o sistema Reciproc, sem cateterismo pr?vio, apresentou maior capacidade de limpeza do sistema de canais radiculares. The {{purpose of}} this study was to {{evaluate}} apical transportation using CT Scans, and, using histological analysis, to evaluate the cleaning effectiveness of different glide path techniques associated with the use of Reciproc system`s R 25 instrument. To perform the experiment, 52 mandibular molars were selected, with mesial root canal curvature between 20 ? and 40 ? and canal curvature radius less than or equal to 10 mm. Size 10 K-type instruments were used to determine the presence of independent foramen. In order to standardize tooth size, the crowns were partially sectioned at 16 mm and then measured 3 mm in the mesial root, which were transferred to the distal root, to be sectioned at that point. The selected teeth were divided into four groups of 13 : Group 1 : glide path with <b>manual</b> <b>file,</b> sizes 10 and 15 and instrumentation with the R 25; Group 2 : Reciproc R 25 without a glide path; Group 3 : the glide path performed with PathFile and instrumentation with the Reciproc R 25 and Group 4 : control group, no instrumentation. The distal root was then embedded in resin blocks for the scans to be standardized in the same position, before and after instrumentation, and to be fixed in a vise, in order to standardize instrumentation position. The working length of the mesial canals was set at 0. 5 mm from the apex. After performing the glide path, the root canals were prepared following manufacturer recommendations. Sodium hypochlorite 2. 5 % was the irrigating solution used. After instrumentation, teeth again underwent CT scanning, after which the apical third of each specimen was subjected to routine histotechnical processing in order to obtain 12 sections 5 ?m thick. Apical deviation was assessed by applying formulas to values obtained before and after instrumentation and histological analysis was performed by evaluating the histological sections with a 230 x magnification and the quantification of debris through the superposition of a grid. Data were subjected to the Kruskal-Wallis Test and the Dunn Multiple Comparisons Test. The results of the apical deviation presented no significant difference between the experimental groups (p > 0, 05). As for cleaning ability, the difference between groups 1 and 2 was found to be statistically significant (p ? 0, 01). The methodology and the results lead to the conclusion that the glide path technique used did not produce a statistically significant difference in apical deviation. However, the Reciproc system instrumentation technique, without a glide path, showed greater capacity for cleaning the root canal system...|$|E
50|$|On 12 June 1897, the Council of the Governor General of India {{approved}} a committee report that fingerprints {{should be used}} for classification of criminal records. After that year, the Kolkata Anthropometric Bureau became the world's first Fingerprint Bureau. He was working in the Calcutta Anthropometric Bureau (before it became the Fingerprint Bureau) with Azizul Haque. He and Haque were the two Indian fingerprint experts credited with primary development of the Henry Classification System (named for their supervisor, Edward Richard Henry). The Henry Classification System is still used in all English-speaking countries (primarily as the <b>manual</b> <b>filing</b> system for accessing paper archive files that have not been scanned and computerized).|$|R
50|$|In the mid-1990s, several firms began {{creating}} and using single-source content for technical documentation (Boeing Helicopter, Sikorsky Aviation and Pratt & Whitney Canada) and user manuals (Ford owners manuals) based on tagged SGML and XML content generated using the Arbortext Epic editor with add-on functions {{developed by a}} contractor. The concept behind this usage was that complex, hierarchical content that did not lend itself to discrete componentization could be used {{across a variety of}} requirements by tagging the differences within a single document using the capabilities built into SGML and XML.Ford, for example, was able to tag its single owner's <b>manual</b> <b>files</b> so that 12 model years could be generated via a resolution script running on the single completed file. Pratt & Whitney, likewise, was able to tag up to 20 subsets of its jet engine <b>manuals</b> in single-source <b>files,</b> calling out the desired version at publication time. World Book Encyclopedia also used the concept to tag its articles for American and British versions of English.|$|R
40|$|The VFA- 125 Safety Office {{located at}} NAS Lemoore is {{burdened}} with the enormous administrative responsibility {{of managing the}} NATOPS qualifications for over 200 pilots and passengers. During this period of military downsizing and operational funding cuts, this responsibility will require the increased attention of a smaller staff with a limited budget. The burden of paper file management could be eased through the introduction of automated record keeping while simultaneously increasing accuracy and efficiency. The potential for non-qualified personnel to fly squadron aircraft could be eliminated. Based on VFA- 125 Safety Office requirements, this thesis designs and implements a database management system. The primary objective is to automate the currently utilized <b>manual</b> NATOPS <b>filing</b> system to allow the squadron Safety Officer immediate access to all NATOPS-related pilot qualification data. This system will store, sort and compare data relevant to all squadron pilots while minimizing the time spent verifying the previously-used <b>manual</b> <b>filing</b> system. Additionally, the staff {{will be able to}} query reports and generate memoranda with minimal effort. The system is also analyzed to determine possible enhancements in the future. The Aviation Safety Database System is designed using dBASE III Plus and dBASE for Windows 5. 0. NAU. S. Navy (U. S. N.) autho...|$|R
