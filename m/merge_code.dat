6|128|Public
50|$|Streams in AccuRev {{automatically}} {{share and}} <b>merge</b> <b>code</b> with each other. This is a main distinction between streams and branches.|$|E
40|$|Abstract. In {{software}} product line engineering, feature composition generates software tailored to specific requirements {{from a common}} set of artifacts. Superimposition is a technique to <b>merge</b> <b>code</b> pieces belonging to different features. The advent of model-driven development raises {{the question of how to}} support the variability of {{software product}} lines in modeling techniques. We propose to use superimposition as a model composition technique in order to support variability. We analyze the feasibility of superimposition for model composition, offer corresponding tool support, and discuss our experiences with three case studies (including an industrial case study). ...|$|E
40|$|Refactoring is a {{well-known}} technique that is widely adopted by software engineers to improve the design and enable {{the evolution of a}} system. Knowing which refactoring operations were applied in a code change is a valuable information to understand software evolution, adapt software components, <b>merge</b> <b>code</b> changes, and other applications. In this paper, we present RefDiff, an automated approach that identifies refactorings performed between two code revisions in a git repository. RefDiff employs a combination of heuristics based on static analysis and code similarity to detect 13 well-known refactoring types. In an evaluation using an oracle of 448 known refactoring operations, distributed across seven Java projects, our approach achieved precision of 100 % and recall of 88 %. Moreover, our evaluation suggests that RefDiff has superior precision and recall than existing state-of-the-art approaches. Comment: Paper accepted at 14 th International Conference on Mining Software Repositories (MSR), pages 1 - 11, 201...|$|E
40|$|Problem statement: Image {{processing}} applications were drastically increasing {{over the}} years. In such a scenario, the fact that, the digital images need {{huge amounts of}} disk space {{seems to be a}} crippling disadvantage during transmission and storage. So, there arises a need for data compression of images. Approach: This study proposed a novel technique called binary <b>merge</b> <b>coding</b> for lossless compression of images. This method was based on spatial domain of the image and it worked under principle of Inter-pixel redundancy reduction. This technique was taken advantage of repeated values in consecutive pixels positions. For a set of repeated consecutive values only one value was retained. Results: The proposed binary <b>merge</b> <b>coding</b> achieved the compression rate of the brain image was 1. 6572479. Comparatively, it is 100 % more than the compression rate achieved by standard JPEG. Conclusion/Recommendations: This technique was simple in implementation and required no additional memory area. The experimental results of binary <b>merge</b> <b>coding</b> were compared with standard JPEG and it showed that, the binary <b>merge</b> <b>coding</b> improved compression rate compared to JPEG. The same algorithm can be extending to color images. This algorithm can also used for lossy compression with few modifications...|$|R
40|$|Abstract: Problem statement: Image {{processing}} applications were drastically increasing {{over the}} years. In such a scenario, the fact that, the digital images need {{huge amounts of}} disk space {{seems to be a}} crippling disadvantage during transmission and storage. So, there arises a need for data compression of images. Approach: This study proposed a novel technique called binary <b>merge</b> <b>coding</b> for lossless compression of images. This method was based on spatial domain of the image and it worked under principle of Inter-pixel redundancy reduction. This technique was taken advantage of repeated values in consecutive pixels positions. For a set of repeated consecutive values only one value was retained. Results: The proposed binary <b>merge</b> <b>coding</b> achieved the compression rate of the brain image was 1. 6572479. Comparatively, it is 100 % more than the compression rate achieved by standard JPEG. Conclusion/Recommendations: This technique was simple in implementation and required no additional memory area. The experimental results of binary <b>merge</b> <b>coding</b> were compared with standard JPEG and it showed that, the binary <b>merge</b> <b>coding</b> improved compression rate compared to JPEG. The same algorithm can be extending to color images. This algorithm can also used for lossy compression with few modifications. Key words: Huffman coding technique, JPEG, bit plane, data tabl...|$|R
5000|$|On May 31, 2010, Compare++ got an Editor's Pick by Brothersoft. The editor {{described}} that [...] "Compare++ structured compares and <b>merges</b> <b>code,</b> {{files and}} folders, and can detect function changes" [...] and [...] "highlights differences in a side-by-side interface".|$|R
40|$|In this paper, {{we propose}} {{a method to}} enable aggressive, interprocedural {{optimization}} in a setting where code can be replaced at runtime. Code replacement involves both introducing a new module into the system and deallocating old code. Code purging deallocates replaced code, which is required in long running systems. Our approach, module merging, is simple and practical: we <b>merge</b> <b>code</b> modules and insert code to check for code replacement at the appropriate points. We show how to preserve the behavior of code purging. The net result is that merged modules preserve the original code replacement behavior, while enabling optimization across code replacement boundaries. We finally show how to perform inlining and dataflow analysis in merged modules. 1 Introduction As the computer industry moves into the age of ubiquitous networking, servers with extremely high availability become crucial. Such systems are expected to run indefinitely, with downtimes of a few minutes per year [9, Ch. 12], [...] ...|$|E
40|$|Collaboration becomes {{increasingly}} important in programming as projects become more complex. With traditional text-based programming languages, programmers typically use a source code management system {{to manage the}} code, <b>merge</b> <b>code</b> from multiple authors, and optionally lock files for conflict-free editing. There is a limited corpus of work around collaborative editing of code in visual programming languages such as block-based programming. I present a collaborative programming environment to MIT App Inventor, a web-based visual platform for building Android applications with blocks, which enables many programmers to collaborate in real time on the same MIT App Inventor project. I design and implement three collaboration models to evaluate the efficacy of these different collaborative models for users of App Inventor so as to understand which approach best enables collaboration. Our results demonstrate that real-time programming decreases the completion time of a task, improves the interaction between users, and increases users' likeability towards collaborative programming. I anticipate that this new collaborative programming environment will change the way users use MIT App Inventor, and more curriculum based on the collaborative tools will be designed. by Xinyue Deng. Thesis: M. Eng., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 2017. This electronic version was submitted by the student author. The certified thesis {{is available in the}} Institute Archives and Special Collections. Cataloged from student-submitted PDF version of thesis. Includes bibliographical references (pages 79 - 81) ...|$|E
30|$|The project {{under study}} is an {{umbrella}} project, consisting of various teams developing mobile applications for different platforms, e.g., Android and iOS. Those applications should enable users to create small creative projects without prior domain knowledge and are primarily targeted at teenagers. These projects can be shared, even between different platforms, e.g. a project created on Android should also work on iOS. Some applications {{have already been}} released to the public, others are still in development. The project is a hybrid student FOSS project, meaning {{that the majority of}} developers is doing one part of their studies within the project, e.g., their Bachelor thesis or Master project. The project fulfils all terms of the open source definition [29] and uses the GNU Affero General Public License. The project does not exactly adhere to the onion model as described in FOSS research [8, 26, 28, 37]. The layers are similar but outer layers are not larger by an order of magnitude. Most developers (around 130) are students, non-student contributors focus their efforts mostly on translating the applications (around 90 people), creating tutorials, example projects, Youtube videos and bug reporting (around 80 people created an account on the project’s issue tracking system; numbers of reporters using other means of reporting are not tracked). Students usually stay between six month and approximately two years with breaks between their Bachelor thesis and Master project. Coding is sometimes done at the university, {{but most of the time}} developers contribute from all over the world. Developers change all the time and there are no core teams or core developers, who stay with the project for multiple years and have all the tacit knowledge and experience, which often builds the backbone of a project. Contributions to the code are more evenly spread than in usual FOSS projects, where a small percentage of contributors develops a large percentage of code [19]. Developers know only a little or a fair amount about agile methods. Practical software development skills range from beginner to intermediate. Different teams work on the applications for the different platforms and are only loosely connected. But as all user-created project should work on every platform, teams have to discuss and agree how the user-created content is stored and exchanged. Teams have to cooperate on the design of the user interface as well, so that the applications feel familiar on every device while adhering to the platform-specific style guidelines at the same time. Some teams are sub-teams of a bigger team and are more closely connected to each other. Super- and sub-teams have to adhere to the same coding standards, they have to jointly decide how the code of sub-teams is integrated in the code structure and they have to ensure that their code is not interfering with each other. These teams also show the pattern of “enforced ownership” [28], whereas teams without sub-teams do not show such a clear ownership pattern. As software development method, an agile approach is used. It contains elements of XP and Kanban as already described in Section 1. The project shows the same main differences [28] to industrial settings as other FOSS projects: the software is developed by a large number of people, they are not assigned to teams, they self-select the team and topic they are willing to work on, there exists no system-level design, or detailed design [39], and there is no list of deliverables or a project plan. The only distinct roles within the teams are: coordinators, who are the main contact persons for other teams or other interested parties, senior members, who are able and allowed to <b>merge</b> <b>code</b> into the main repository, and developers. The team coordinators have a good overview who is performing which task in the team and make sure the whole team moves into the same direction. Coordinators and seniors volunteer for the position and the team jointly decides who is ready and suitable for the position. Only two people can be considered central management, one is responsible for organizational activities, e.g., managing infrastructure and accounts, and the other, the project head, is mainly responsible for the overall orientation of the umbrella project. The project head is also the project initiator, who had an idea to develop software for teenagers. So the project is not “scratching a developer’s personal itch” [31] but instead implementing an educator’s vision. Because developers are not the target group of the software a Usability and User Experience (UX) team helps other teams understand user needs and adjust the applications accordingly.|$|E
40|$|MCNP{trademark} and LAHET{trademark} {{are two of}} {{the codes}} {{included}} in the LARAMIE (Los Alamos Radiation Modeling Interactive Environment) code system. Both MCNP and LAHET are three-dimensional continuous-energy Monte Carlo radiation transport codes. The capabilities of MCNP and LAHET are currently being <b>merged</b> into one <b>code</b> for the Accelerator Production of Tritium (APT) program at Los Alamos National Laboratory. Concurrently, a significant effort is underway to improve the accuracy of the physics in the <b>merged</b> <b>code.</b> In particular, full nuclear-data evaluations (in ENDF 6 format) for many materials of importance to APT are being produced for incident neutrons and protons up to an energy of 150 -MeV. After processing, cross-section tables based on these new evaluations will be available for use fin the <b>merged</b> <b>code.</b> In order to utilize these new cross-section tables, significant enhancements are required for the <b>merged</b> <b>code.</b> Neutron cross-section tables for MCNP currently specify emission data for neutrons and photons only; the new evaluations also include complete neutron-induced data for protons, deuterons, tritons, and alphas. In addition, no provision in either MCNP or LAHET currently exists for the use of incident charged-particle tables other than for electrons. To accommodate the new neutron-induced data, it was first necessary to expand the format definition of an MCNP neutron cross-section table. The authors have prepared a 150 -MeV neutron cross-section library in this expanded format for 15 nuclides. Modifications to MCNP have been implemented so that this expanded neutron library can be utilized...|$|R
50|$|Bitbucket Server (formerly {{known as}} Stash) is a {{combination}} Git server and web interface product. It allows users to do basic Git operations (such as reviewing or <b>merging</b> <b>code,</b> similar to GitHub) while controlling read and write access to the code. It also provides integration with other Atlassian tools.|$|R
5000|$|Math.NET Numerics started 2009 by <b>merging</b> <b>code</b> {{and teams}} of dnAnalytics with Math.NET Iridium. It is {{influenced}} by ALGLIB, JAMA and Boost, among others, and has accepted numerous code contributions. It {{is part of the}} Math.NET initiative to build and maintain open mathematical toolkits for the [...]NET platform since 2002.|$|R
5000|$|August 29, 2005: Linux kernel version 2.6.13 released, {{containing}} <b>merged</b> inotify <b>code</b> ...|$|R
40|$|Arithmetic coding {{achieves}} a superior coding rate when encoding a bi-nary source, but {{its lack of}} speed makes it an inferior choice when true high– performance encoding is needed. We present our work on a practical implemen-tation of fast entropy coders for binary messages utilizing only bit shifts and table lookups. To limit code table size we limit our code lengths with a type of variable–to–variable (VV) length code created from source string merging. We refer to these codes as “merged codes”. With <b>merged</b> <b>codes</b> {{it is possible to}} achieve a desired level of speed by adjusting the number of bits read from the source at each step. The most efficient <b>merged</b> <b>codes</b> yield a coder with a worst–case inefficiency of 0. 4 %, relative to the Shannon entropy. Using a hybrid Golomb–VV Bin Coder we are able to achieve a compression ratio that is competitive with other state–of–the–art coders, at a superior throughput. ...|$|R
25|$|As of 2006, {{approximately}} {{two percent of}} the Linux kernel was written by Torvalds himself. Because thousands have contributed to the Linux kernel, this percentage {{is one of the largest}} contributions to it. However, he stated in 2012 that his own personal contribution is now mostly <b>merging</b> <b>code</b> written by others, with little programming. Torvalds retains the highest authority to decide which new code is incorporated into the standard Linux kernel.|$|R
5000|$|In 2003, {{geocoding}} platforms {{were capable}} of <b>merging</b> postal <b>codes</b> with street data, updated monthly. This process became known as [...] "conflation".|$|R
50|$|Up until Cinelerra 2.1 the {{versioning}} of Cinelerra-CV {{followed that}} of Heroine Virtual. After Heroine Virtual produced a release, Cinelerra-CV examined the changes {{introduced by the}} new version and merged them into their version. CV was appended {{to the end of}} the version number to indicate the community version. (For example, after the 2.1 merger the CV version was labeled 2.1CV.)Starting with release 2.2, Cinelerra-CV uses its own versioning scheme, but still <b>merges</b> <b>code</b> from Cinelerra-HV.|$|R
5000|$|Enables {{multiple}} {{developers to}} work on a single class concurrently without the need to <b>merge</b> individual <b>code</b> into one file at a later time.|$|R
50|$|The model codes {{may either}} be adopted {{outright}} as the building codes for a jurisdiction, {{or they may}} be adopted with amendments or additional rules. In some cases, the amendments or additional requirements and exemptions are issued as a separate document or, in other cases, the jurisdiction may print, under its own title, a <b>merged</b> <b>code,</b> incorporating all of the local revisions. For example, the City of Los Angeles 2011 Building Code is based on the 2009 International Building Code, which is a model code developed by the International Code Council (ICC).|$|R
5000|$|Interactive margin {{area with}} drag & drop linking, <b>coding,</b> <b>merging</b> ...|$|R
2500|$|Code — code {{development}} and review, source code management tools, <b>code</b> <b>merging</b> ...|$|R
5000|$|Change files, {{which can}} be {{automatically}} <b>merged</b> into the <b>code</b> when compiling/printing.|$|R
40|$|This paper {{describes}} {{a comparison between}} internal and external run-time coupling of CFD and building energy simulation software. Internal coupling {{can be seen as}} the 2 ̆ 2 traditional 2 ̆ 2 way of developing software, i. e. the capabilities of existing software are expanded by <b>merging</b> <b>codes.</b> With external coupling, two or more software packages run simultaneously while exchanging calculation results at appropriate time intervals. One of the most notable advantages of external coupling is that it is no longer necessary to make major changes to one of the coupled simulation packages should the other one be expanded with new or improved features. Furthermore, it is much easier to externally couple some software than to <b>merge</b> its <b>code</b> into existing software. Obviously in commercial software the code would not even be available. This paper presents the implementation of external coupling between BES and CFD. The external coupling method will be compared with internal coupling method by simulating the IAE Annex 26 Atrium (Yokohama, Japan). The results show the applicability and advantages of the external coupling method...|$|R
50|$|As part of {{the reform}} in the early 1970s, the {{categorization}} of municipalities was abolished, but the code was in most cases preserved. When several municipalities were <b>merged,</b> the <b>code</b> for the biggest municipality was kept.|$|R
50|$|Therefore:Institute {{a process}} of <b>merging</b> all <b>code</b> and other {{implementation}} artifacts frequently, with automated tests to flag fragmentation quickly. Relentlessly exercise the ubiquitous language to hammer out a shared view of the model as the concepts evolve in different people’s heads.|$|R
40|$|Improved coding {{of atoms}} in image {{compression}} by Matching Pursuits (MP) after a wavelet decomposition is achieved. The positions ofatoms in the wavelet sub-bands are communicated by run length <b>coding.</b> The <b>MERGE</b> <b>coding</b> scheme replaces individual coding of any alphabet of symbols {{by dividing the}} symbols into groups and run length coding their positions. It is shown theoretically {{that the use of}} MERGE with an efficient run length coder approaches the theoretical entropy of an alphabet of symbols. The embedded nature of MERGE makes it a near-ideal coding scheme for the amplitudes of atoms. An additional layer ofMERGE coding for the basis indices of atoms approaches the theoretical optimum and is superior to either separable or combined Huffman coding...|$|R
40|$|Clone {{refactoring}} (<b>merging</b> duplicate <b>code)</b> is {{a promising}} solution {{to improve the}} maintainability of source code. In this position paper, we discuss directions towards the ad-vancement of clone refactoring, and show a perspective of active support based on online analysis of code modification on an editor of IDE...|$|R
40|$|This survey paper {{investigates the}} current state of the art with respect to {{collaborative}} computing. Specifically, the paper addresses the field of collaborative software engineering and focuses on the background and issues related to distributed software development. The paper begins by exploring collaborative computing in general, discusses synchronous and asynchronous collaboration and communication mechanisms to ensure updates are handled properly, and then focuses on elements that have significant impact on distributed software engineering: mutual exclusion, achieving “undo ” and “redo, ” organizational theory, <b>merging</b> <b>code,</b> and distributed version control. The paper then examines some of the human-computer interface (HCI) issues of such collaborative systems and presents various classification schemes that are helpful in comparing various collaborative domains and applications. The paper concludes by discussing recent and future work in the field...|$|R
5000|$|G {{language}} being non-textual, {{software tools}} such as versioning, side-by-side (or diff) comparison, and version code change tracking cannot be applied {{in the same manner}} as for textual programming languages. There is some additional tool to make comparison and <b>merge</b> of <b>code</b> with source code control (versionning) like subversion, CVS, Perforce.|$|R
50|$|Copyleft is {{a general}} concept. Copylefting an actual program {{requires}} {{a specific set of}} distribution terms. Different copyleft licenses are usually “incompatible” due to varying terms, which makes it illegal to <b>merge</b> the <b>code</b> using one license with the code using the other license. If two pieces of software use the same license, they are generally mergeable.|$|R
40|$|This {{correspondence}} {{addresses the}} problem of human visual weighted quantization for transform/subband coding of interlaced pictures. <b>Merged</b> field <b>coding</b> is assumed. The criterion proposed for the optimization of the quantizer is the distortion measured on the deinterlaced and decoded image. This criterion and the associated weighting factors are shown {{to be dependent on}} the motion between two successive fields...|$|R
40|$|Superinstruction is {{well-known}} techniques of improving performance of interpreters. Superinstructions eliminate jumps between VM operations (interpreter dispatch) and enable more optimizations in <b>merged</b> <b>code.</b> In past, processors with simple BTB-based branch predictors had high misprediction rate when executing interpreted code, resulting in high overhead of interpreter dispatch, so superinstructions {{were used to}} reduce it. However, this assumption is incorrect for Ruby on current hardware. Accordingly, using superinstructions for eliminating jump instructions only marginally improves performance. In this paper, we consider applying superinstructions differently to improve performance of floating point computation. We note that high percentage of objects allocated during numeric computation are boxed floating point values, meanwhile garbage collection takes {{significant part of the}} execution time. Using superinstructions composed from pairs of arithmetic operations we were able to reduce allocation of boxed floats by up to 36 %, and obtain improvement in performance of up to 22 %. 1...|$|R
50|$|In {{addition}} to source code hosting, Gitorious provides projects with wikis, a web interface for <b>merge</b> requests and <b>code</b> reviews, and activity timelines for projects and developers.|$|R
40|$|Part 1 : PapersInternational audienceArtifact-based {{research}} {{provides a}} mechanism whereby researchers may study {{the creation of}} software yet avoid many of the difficulties of direct observation and experimentation. However, {{there are still many}} challenges that can affect the quality of artifact-based studies, especially those studies examining software evolution. Large commits, which we refer to as “Cliff Walls,” are one significant threat to studies of software evolution because they do not appear to represent incremental development. We used Latent Dirichlet Allocation to extract topics from over 2 million commit log messages, taken from 10, 000 SourceForge projects. The topics generated through this method were then analyzed to determine the causes of over 9, 000 of the largest commits. We found that branch <b>merges,</b> <b>code</b> imports, and auto-generated documentation were significant causes of large commits. We also found that corrective maintenance tasks, such as bug fixes, did not {{play a significant role in}} the creation of large commits...|$|R
50|$|In the 1990s, Python was {{extended}} to include an array type for numerical computing called Numeric (This package was eventually replaced by Travis Oliphant who wrote NumPy in 2006 as a blending of Numeric and Numarray which had been started in 2001). As of 2000, there was {{a growing number of}} extension modules and increasing interest in creating a complete environment for scientific and technical computing. In 2001, Travis Oliphant, Eric Jones, and Pearu Peterson <b>merged</b> <b>code</b> they had written and called the resulting package SciPy. The newly created package provided a standard collection of common numerical operations on top of the Numeric array data structure. Shortly thereafter, Fernando Pérez released IPython, an enhanced interactive shell widely used in the technical computing community, and John Hunter released the first version of Matplotlib, the 2D plotting library for technical computing. Since then the SciPy environment has continued to grow with more packages and tools for technical computing.|$|R
5000|$|That same month, McGregor {{released}} an innovative {{new way to}} compare and merge branches of software developed in parallel, through a product known initially as Merge Ahead, and in subsequent releases as Merge Right. This product received a cover notice in [...] "Sun World" [...] that month, along with a review from new product reviews editor David A. Taylor praising the products remarkable ease of learning and ease of use. An unusual characteristic of this merge tool was its use of colored backgrounds and video game-like color-matching strategies for hinting which branch to accept when <b>merging</b> <b>code</b> conflicts. What was innovative about this approach was that other tools had used symbolic coding, or color-coding of text which put additional cognitive load on the symbolic and linear processing centers of the brain, whereas the color matching utilized graphical and parallel processing centers. The result, verified by product reviewers was a reduction in undetected user errors during the merge task and an improvement in task performance times.|$|R
