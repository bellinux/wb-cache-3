2346|251|Public
25|$|In its {{simplest}} form, the bound {{states that}} the variance of any unbiased estimator {{is at least as}} high as the inverse of the Fisher information. An unbiased estimator which achieves this lower bound is said to be (fully) efficient. Such a solution achieves the lowest possible mean squared error among all unbiased methods, and is therefore the <b>minimum</b> <b>variance</b> unbiased (MVU) estimator. However, in some cases, no unbiased technique exists which achieves the bound. This may occur even when an MVU estimator exists.|$|E
500|$|These {{are known}} to be the uniformly <b>minimum</b> <b>variance</b> {{unbiased}} (UMVU) estimators for the continuous uniform distribution. In comparison, the maximum likelihood estimates for this problem [...] and [...] are biased and have higher mean-squared error.|$|E
2500|$|So δ1 {{is clearly}} a very much {{improved}} estimator of that last quantity. [...] In fact, since S'n is complete and δ0 is unbiased, δ1 is the unique <b>minimum</b> <b>variance</b> unbiased estimator by the Lehmann–Scheffé theorem.|$|E
40|$|AbstractThis paper {{investigates the}} {{estimation}} of covariance matrices in multivariate mixed models. Some sufficient conditions are derived for a multivariate quadratic form and a linear combination of multivariate quadratic forms to be the BQUE (quadratic unbiased and severally <b>minimum</b> <b>varianced)</b> estimators of its expectations...|$|R
40|$|This paper {{investigates the}} {{estimation}} of covariance matrices in multivariate mixed models. Some sufficient conditions are derived for a multivariate quadratic form and a linear combination of multivariate quadratic forms to be the BQUE (quadratic unbiased and severally <b>minimum</b> <b>varianced)</b> estimators of its expectations. BQUE estimator cumulant covariance matrix multivariate mixed models quadratic estimation...|$|R
2500|$|... or the <b>minimum</b> {{possible}} <b>variance</b> for an {{unbiased estimator}} divided by its actual variance.|$|R
2500|$|Estimator [...] {{is called}} the sample mean, {{since it is the}} {{arithmetic}} mean of all observations. The statistic [...] is complete and sufficient for μ, and therefore by the Lehmann–Scheffé theorem, [...] is the uniformly <b>minimum</b> <b>variance</b> unbiased (UMVU) estimator. In finite samples it is distributed normally: ...|$|E
2500|$|The {{accompanying}} plot of skewness as {{a function}} of variance and mean shows that maximum variance (1/4) is coupled with zero skewness and the symmetry condition (μ = 1/2), and that maximum skewness (positive or negative infinity) occurs when the mean is located at one end or the other, so that the [...] "mass" [...] of the probability distribution is concentrated at the ends (<b>minimum</b> <b>variance).</b>|$|E
2500|$|The Gauss–Markov theorem. In {{a linear}} {{model in which}} the errors have {{expectation}} zero conditional on the independent variables, are uncorrelated and have equal variances, the best linear unbiased estimator of any linear combination of the observations, is its least-squares estimator. [...] "Best" [...] means that the least squares estimators of the parameters have <b>minimum</b> <b>variance.</b> The assumption of equal variance is valid when the errors all {{belong to the same}} distribution.|$|E
30|$|I. <b>Minimum</b> <b>variances</b> for main axes {{were set}} to 402 m 2 for weak CAs and to 52 m 2 for strong CAs. Most of the weak CAs had smaller variances than this limit, and they were {{extended}} using Equation 34. According to [19], the two-level normal CA models have around 10 % larger errors than traditional location fingerprinting, but require only storage of ten parameters for each AP.|$|R
40|$|Numerical {{experiments}} {{are carried out}} on the triangular 	mesh generation associated with the harmonic function on a 	polygonally bounded region. The inverse Laplace equation is 	solved under the Dirichlet boundary condition which is obtained 	as a result of coding to generate a uniform mesh. The 	automatic boundary value assignment produces the mesh having 	approximately the <b>minimum</b> <b>variances</b> in the distributions of 	areas and angles of the whole triangular elements...|$|R
40|$|One of {{the most}} {{important}} parameters in population genetics is θ = 4 N(e) μ where N(e) is the effective population size and μ is the rate of mutation per gene per generation. We study two related problems, using the maximum likelihood method and the theory of coalescence. One problem is the potential improvement of accuracy in estimating the parameter θ over existing methods and the other is the estimation of parameter λ which is the ratio of two θ's. The <b>minimum</b> <b>variances</b> of estimates of the parameter θ are derived under two idealized situations. These <b>minimum</b> <b>variances</b> serve as the lower bounds of the variances of all possible estimates of θ in practice. We then show that Watterson's estimate of θ {{based on the number of}} segregating sites is asymptotically an optimal estimate of θ. However, for a finite sample of sequences, substantial improvement over Watterson's estimate is possible when θ is large. The maximum likelihood estimate of λ = θ(1) /θ(2) is obtained and the properties of the estimate are discussed...|$|R
2500|$|Another common {{technique}} is beamforming, wherein a theoretical {{model of the}} magnetic field produced by a given current dipole {{is used as a}} prior, along with second-order statistics of the data {{in the form of a}} covariance matrix, to calculate a linear weighting of the sensor array (the beamformer) via the Backus-Gilbert inverse. This is also known as a linearly constrained <b>minimum</b> <b>variance</b> (LCMV) beamformer. When the beamformer is applied to the data, it produces an estimate of the power in a [...] "virtual channel" [...] at the source location.|$|E
2500|$|The {{difference}} between s2 and [...] becomes negligibly small for large ns. In finite samples however, the motivation behind {{the use of}} s2 {{is that it is}} an unbiased estimator of the underlying parameter σ2, whereas [...] is biased. Also, by the Lehmann–Scheffé theorem the estimator s2 is uniformly <b>minimum</b> <b>variance</b> unbiased (UMVU), which makes it the [...] "best" [...] estimator among all unbiased ones. However it can be shown that the biased estimator [...] is [...] "better" [...] than the s2 in terms of the mean squared error (MSE) criterion. In finite samples both s2 and [...] have scaled chi-squared distribution with [...] degrees of freedom: ...|$|E
5000|$|... #Subtitle level 2: Completeness and Lehmann - Scheffé <b>minimum</b> <b>variance</b> ...|$|E
5000|$|... or the <b>minimum</b> {{possible}} <b>variance</b> for an {{unbiased estimator}} divided by its actual variance.The Cramér-Rao lower bound thus gives ...|$|R
40|$|Ulysses {{magnetic}} and plasma {{data obtained}} in the fast polar solar wind are used to study the nature of fluctuations in the frequency range between the turbulent inertial range and the lower-frequency range dominated by quasi-static structures originating at the Sun. For daily variations of hourly averages of the magnetic field components the anisotropy of the fluctuations is less than in the higher-frequency range, and the <b>minimum</b> <b>varianced</b> direction loses its alignment with the magnetic field {{in favor of a}} more radial direction as the solar distance increases...|$|R
40|$|A {{statistical}} theory developed previously {{is applied}} to predictions made with three simple atmospheric models under similar boundary and initial conditions. The theory gives <b>minimum</b> <b>variances</b> in height fields of various isobaric levels. The governing equations of each model are utilized to transform these initial variances to ha 1 variances of forecast fields. These variances are {{a measure of the}} theoretical minimum errors expected at any future states due to presence of initial uncertainties. Using the normal frequency function, these theoretical variances are further transformed to probabilities of obtaining forecast heights within specified magnitudes of true heights. These theoretical probabilities are compared with observed probabilities of errors in forecast fields obtained by various models for three synoptic situations. The theoretical probabilities are found to be larger everywhere than the observed ones, in support of the statistical theory that provides limiting probabilities not to be exceeded. A comparison of theoretical <b>minimum</b> <b>variances</b> indicates that the growth of these variances is more pronounced in more complex models that incorporate additional terms in the governing equations. The effect of hypothetically increasing the number of reporting stations indicates that a substantial reduction in initial and final variances is realized when the number of reporting stations is increased by two to three times the present number. The results of this study offer a possibility of choosing an optimum model to obtain the most reliable short-range weather prediction for a given synoptic situation. 1...|$|R
5000|$|The unique <b>minimum</b> <b>variance</b> {{unbiased}} estimator radj {{is given by}} ...|$|E
5000|$|An {{efficient}} estimator {{is also the}} <b>minimum</b> <b>variance</b> unbiased estimator (MVUE).This is because an {{efficient estimator}} maintains equality on the Cramér-Rao inequality for all parameter values, which means it attains the <b>minimum</b> <b>variance</b> for all parameters (the definition of the MVUE). The MVUE estimator, even if it exists, is not necessarily efficient, because [...] "minimum" [...] does not mean equality holds on the Cramér-Rao inequality.|$|E
5000|$|The <b>Minimum</b> <b>Variance</b> Distortionless Response beamformer, {{also known}} as the Capon {{beamforming}} algorithm, has a power given by ...|$|E
5000|$|... where [...] is the Fisher {{information}} of the sample.Thus e(T) is the <b>minimum</b> possible <b>variance</b> for an unbiased estimator divided by its actual variance. The Cramér-Rao bound {{can be used}} to prove that e(T) ≤ 1.|$|R
40|$|This paper {{presents}} a framework to track people using wavelet transform and Kalman filter in unconstrained environments. We adopt simultaneously the maximum and <b>minimum</b> <b>variances</b> {{of the color}} information in the trunk part of the tracked people to be the tracked features. However, the shadow {{is one of the}} environmental factors influencing on processing of monitored images. To make the system more robust, a shadow-removal scheme is devised. A multi-resolution method and the color information are used to eliminate shadows. The Kalman filter and DSA is adopted to be the estimator in this system. Experiments show that the proposed system achieves optimal performance in the complicated backgrounds. ...|$|R
3000|$|To {{obtain an}} {{accurate}} threshold value, these three parameters must be optimized. In other words, the threshold {{that provides the}} <b>minimum</b> within-class <b>variance</b> (σ w 2 [...]) and the maximum between-class variance (σ B 2 [...]) {{will be the best}} threshold value.|$|R
5000|$|The initial cluster {{distances}} in Ward's <b>minimum</b> <b>variance</b> method {{are therefore}} defined {{to be the}} squared Euclidean distance between points: ...|$|E
50|$|The {{difference}} between the treatment and the control can thus be given <b>minimum</b> <b>variance</b> (i.e. maximum precision) by maximising the covariance (or the correlation) between X and Y.|$|E
5000|$|Two estimators {{can have}} , but the {{dispersion}} around their mean determines {{the difference between}} the quality of estimators. To find an estimator with <b>minimum</b> <b>variance,</b> we need to minimize [...]|$|E
40|$|<b>Minimum</b> vector <b>variance</b> (MVV) {{is one of}} {{the latest}} {{contributions}} in the study of multivariate robust estimators. MVV estimators possess three important properties of a good robust estimator, namely, high breakdown point, affine equivariance and computational efficiency. However, highly robust affine equivariant estimators with the best breakdown point commonly have to compensate with low statistical efficiency. In order to cater this drawback, a reweighted <b>minimum</b> vector <b>variance</b> (RMVV) which is capable of increasing the efficiency while retaining the highest breakdown point is proposed in this paper. A simulation study was conducted to investigate the asymptotic relative efficiency and finite-sample behavior of the estimators for several types of distributions. The numerical results revealed that the reweighed scheme is able to attain higher efficiency compared to MVV estimators...|$|R
40|$|Recent work on <b>minimum</b> <b>variances</b> estimators {{based on}} Cauchy {{distributions}} appear relevant to orbital drag estimation. Samples form Cauchy distributions which {{are part of}} a class of heavy-tailed distributions, are characterized by long stretches of fairly small variation, punctuated by large variations that are many times larger than could be expected from a Gaussian. Such behavior can occur when solar storms perturb the atmosphere. In this context, the present work describes an embedding of the scalar Idan-Speyer Cauchy Estimator to estimate density corrections, within an Extended Kalman Filter that estimates the state of a low Earth orbiter. In contrast to the baseline Kalman approach, the larger formal errors of the present approach fully and conservatively bound the predictive error distribution, {{even in the face of}} unanticipated density disturbances of hundreds of percent...|$|R
40|$|For {{choosing}} speci"c cross-ratios as 2 D projective coordinates {{in various}} computer vision applications, a reasonable error analysis model is usually required. This investigation adopts {{the assumption of}} normal distribution for positioning errors of point features in an image to formulate the error variances of cross-ratios. Based on a geometry-based error analysis, a straightforward way of identifying the cross-ratios with <b>minimum</b> error <b>variances</b> is proposed. Simulation {{results show that the}} proposed approach, as well as a further simpli"ed alternative, yield much better estimations of <b>minimum</b> error <b>variances</b> in terms of accuracy, cost, and stability compared with some other methods, e. g., the one based on the rule given by Georis et al. (IEEE Trans. Pattern Anal. Mach. Intell. 20 (4) (1998) 366). Some causes of the performance di!erences in the estimations are explained using a special con"guration of point features. 2001 Patter...|$|R
50|$|ORTHOGONALITY:The {{property}} that allows individual {{effects of the}} k-factors to be estimated independently without (or with minimal) confounding. Also orthogonality provides <b>minimum</b> <b>variance</b> estimates of the model coefficient {{so that they are}} uncorrelated.|$|E
5000|$|Ward's <b>minimum</b> <b>variance</b> method can be {{implemented}} by the Lance-Williams formula. For disjoint clusters [...] and [...] with sizes [...] and [...] respectively:Hence Ward's method can {{be implemented}} as a Lance-Williams algorithm with ...|$|E
50|$|So δ1 {{is clearly}} a very much {{improved}} estimator of that last quantity. In fact, since Sn is complete and δ0 is unbiased, δ1 is the unique <b>minimum</b> <b>variance</b> unbiased estimator by the Lehmann-Scheffé theorem.|$|E
40|$|In {{order to}} reduce the {{dimension}} of input vectors before construction of ap-proximation MAVE-type methods (<b>minimum</b> average <b>variance</b> estimation) can be used, however they are very computationally intensive. In the present work the modification of method MAVE is described which allows substantial decrease of algorithm run time at the expense of small error increase...|$|R
40|$|We {{introduce}} {{a class of}} dimension reduction estimators based on an ensemble of the <b>minimum</b> average <b>variance</b> estimates of functions that characterize the central subspace, such as the characteristic functions, the Box [...] Cox transformations and wavelet basis. The ensemble estimators exhaustively estimate the central subspace without imposing restrictive conditions on the predictors, and have the same convergence rate as the <b>minimum</b> average <b>variance</b> estimates. They are flexible and easy to implement, and allow repeated use of the available sample, which enhances accuracy. They are applicable to both univariate and multivariate responses in a unified form. We establish the consistency and convergence rate of these estimators, and the consistency of a cross validation criterion for order determination. We compare the ensemble estimators with other estimators {{in a wide variety}} of models, and establish their competent performance. Comment: Published in at [URL] the Annals of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
40|$|Normalization of gene {{expression}} data refers {{the process of}} minimizing non biological variation in measured probe intensity levels so that biological differences in {{gene expression}} can be appropriately detected. Several linear normalization within arrays approaches has already been proposed. Recently, use of non-linear methods has been gained quite attention. In this study, our objective is to formulate non-linear normalization methods using support vector regression (SVR) and support vector machine quantile regression (SVMQR) approaches, more easier way and, assess the consistency of these methods with respect to other standard normalization methods for further application in gene expression data. SVR and SVMQR normalization methods have been implemented and their performance have been evaluated with respect to other standard normalization methods namely, locally weighted scatter plot smoothing and Kernel regression. It {{has been found that}} the normalized data based on proposed methods are capable of producing <b>minimum</b> <b>variances</b> within replicate groups and also able to detect truly expressible significant genes with respect to above mentioned other normalized data...|$|R
