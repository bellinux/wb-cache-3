484|10000|Public
2500|$|The Theil {{index is}} derived from Shannon's <b>measure</b> <b>of</b> <b>information</b> entropy , where entropy {{is a measure of}} {{randomness}} in a given set of information. In information theory, physics, and the Theil index, the general form of entropy is ...|$|E
2500|$|Information {{theory is}} based on {{probability}} theory and statistics. [...] Information theory often concerns itself with measures of information of the distributions associated with random variables. Important quantities of information are entropy, a <b>measure</b> <b>of</b> <b>information</b> in a single random variable, and mutual information, a <b>measure</b> <b>of</b> <b>information</b> in common between two random variables. [...] The former quantity is a property of the probability distribution of a random variable and gives a limit on {{the rate at which}} data generated by independent samples with the given distribution can be reliably compressed. The latter is a property of the joint distribution of two random variables, and is the maximum rate of reliable communication across a noisy channel in the limit of long block lengths, when the channel statistics are determined by the joint distribution.|$|E
2500|$|Bekenstein's topical {{overview}} [...] "A Tale of Two Entropies" [...] describes potentially profound {{implications of}} Wheeler's trend, {{in part by}} noting a previously unexpected connection between the world of information theory and classical physics. This connection was first described shortly after the seminal 1948 papers of American applied mathematician Claude E. Shannon introduced today's most widely used <b>measure</b> <b>of</b> <b>information</b> content, now known as Shannon entropy. As an objective measure of the quantity of information, Shannon entropy has been enormously useful, as the design of all modern communications and data storage devices, from cellular phones to modems to hard disk drives and DVDs, rely on Shannon entropy.|$|E
40|$|We {{introduce}} a three-parameter generalized normal distribution, which {{belongs to the}} Kotz type distribution family, to study the generalized entropy type <b>measures</b> <b>of</b> <b>information.</b> For this generalized normal, the Kullback-Leibler information is evaluated, which extends the well known result for the normal distribution, and {{plays an important role}} for the introduced generalized information measure. These generalized entropy type <b>measures</b> <b>of</b> <b>information</b> are also evaluated and presented...|$|R
40|$|In {{this paper}} {{we present a}} theorem {{that allows us to}} {{construct}} interval-valued intuitionistic fuzzy sets from intuitionistic fuzzy sets. We also study the way of recovering intuitionistic fuzzy sets used {{in the construction of the}} interval-valued intuitionistic fuzzy set from different operators. We analise the numerical <b>measures</b> <b>of</b> <b>information</b> <b>of</b> the internal-valued intuitionistic fuzzy set constructed in function with the numerical <b>measures</b> <b>of</b> <b>information</b> <b>of</b> the intuitionistic fuzzy sets used in its construction. We review the most important properties of intuitionistic fuzzy sets and of internal-valued intuitionistic fuzzy sets and we analise three operators among these sets along with their properties...|$|R
40|$|AbstractThis paper culminates a {{main line}} of {{research}} on additive sum form <b>measures</b> <b>of</b> <b>information,</b> to which many authors have contributed. We determine all <b>measures</b> <b>of</b> inset <b>information,</b> depending on any number of discrete probability distributions, which are additive and have the measurable sum property...|$|R
2500|$|The <b>measure</b> <b>of</b> <b>information</b> entropy {{associated}} with each possible data value is the negative logarithm of the probability mass function for the value. Thus, when the data source has a lower-probability value (i.e., when a low-probability event occurs), the event carries more [...] "information" [...] ("surprisal") than when the source data has a higher-probability value. The amount of information conveyed by each event defined in this way becomes a random variable whose expected value is the information entropy. Generally, entropy refers to disorder or uncertainty, and the definition of entropy used in information theory is directly analogous to the definition used in statistical thermodynamics. The concept of information entropy was introduced by Claude Shannon in his 1948 paper [...] "A Mathematical Theory of Communication".|$|E
2500|$|Prior to this paper, limited information-theoretic ideas {{had been}} {{developed}} at Bell Labs, all implicitly assuming events of equal probability. [...] Harry Nyquist's 1924 paper, Certain Factors Affecting Telegraph Speed, contains a theoretical section quantifying [...] "intelligence" [...] and the [...] "line speed" [...] at which it can be transmitted by a communication system, giving the relation [...] (recalling Boltzmann's constant), where W is the speed of transmission of intelligence, m {{is the number of}} different voltage levels to choose from at each time step, and K is a constant. [...] Ralph Hartley's 1928 paper, Transmission of Information, uses the word information as a measurable quantity, reflecting the receiver's ability to distinguish one sequence of symbols from any other, thus quantifying information as , where S was the number of possible symbols, and n the number of symbols in a transmission. The unit of information was therefore the decimal digit, which has since sometimes been called the hartley in his honor as a unit or scale or <b>measure</b> <b>of</b> <b>information.</b> Alan Turing in 1940 used similar ideas as part of the statistical analysis of the breaking of the German second world war Enigma ciphers.|$|E
60|$|It {{was useless}} {{to waste time}} in {{wondering}} at an event which neither Allan nor his mother had ever thought of as even remotely possible. The only thing to be done was {{to go back to}} England at once. The next day found the travelers installed once more in their London hotel, and the day after the affair was placed in the proper professional hands. The inevitable corresponding and consulting ensued, and one by one the all-important particulars flowed in, until the <b>measure</b> <b>of</b> <b>information</b> was pronounced to be full.|$|E
40|$|ABSTRACTCountermeasures in {{information}} security and cybernetic vulnerability: empirical evidence of Brazilian companiesRecently, {{a series of}} cyber attacks to firms and governments, in Brazil and abroad highlighted the potential economic impact {{of this kind of}} activity, from private and public perspectives. There is a vast economic literature — theoretical and empirical — that evaluates the incentives for the adoption <b>of</b> <b>measures</b> <b>of</b> <b>information</b> security. The current study developed an empirical evaluation of this phenomenon in Brazil. Logit and ordered probit models were to evaluate the effects on the probability <b>of</b> occurrence <b>of</b> <b>information</b> security problems, considering firms’ characteristics, including <b>measures</b> <b>of</b> <b>information</b> security. Results indicate that there is a positive correlation between <b>measures</b> <b>of</b> <b>information</b> security and the probability of identifying the occurrence of cyber attacks, what suggests that the sophistication <b>of</b> these <b>measures</b> <b>of</b> protection increase the probability of identification of the problems...|$|R
40|$|The {{encoding}} <b>of</b> <b>information</b> in time {{intervals of}} an echelon of laser pulses {{of an object}} pulse in the optical echo processor is considered. The <b>measures</b> <b>of</b> <b>information</b> are introduced to describe the transformation <b>of</b> classical <b>information</b> in quantum information. It is shown that in the description <b>of</b> <b>information</b> transformation into quantum information, the most appropriate measure is a <b>measure</b> <b>of</b> quantum <b>information</b> based on the algorithmic information theory...|$|R
5000|$|It {{turns out}} {{that one of the}} most useful and {{important}} <b>measures</b> <b>of</b> <b>information</b> is the mutual information, or transinformation. This is a <b>measure</b> <b>of</b> how much <b>information</b> can be obtained about one random variable by observing another. The mutual <b>information</b> <b>of</b> [...] relative to [...] (which represents conceptually the average amount <b>of</b> <b>information</b> about [...] that can be gained by observing [...] ) is given by: ...|$|R
60|$|But if {{scientific}} training is to yield its most eminent results, it must, I repeat, be made practical. That is to say, in explaining {{to a child}} the general phaenomena of Nature, you must, as far as possible, give reality to your teaching by object-lessons; in teaching him botany, he must handle the plants and dissect the flowers for himself; in teaching him physics and chemistry, you must not be solicitous to fill him with information, but you must be careful that what he learns he knows of his own knowledge. Don't be satisfied with telling him that a magnet attracts iron. Let him see that it does; let him feel {{the pull of the}} one upon the other for himself. And, especially, tell him that it is his duty to doubt until he is compelled, by the absolute authority of Nature, to believe that which is written in books. Pursue this discipline carefully and conscientiously, and you may make sure that, however scanty may be the <b>measure</b> <b>of</b> <b>information</b> which you have poured into the boy's mind, you have created an intellectual habit of priceless value in practical life.|$|E
5000|$|Shannon derived a <b>measure</b> <b>of</b> <b>information</b> content {{called the}} self-information or [...] "surprisal" [...] {{of a message}} m: ...|$|E
50|$|The {{concept of}} an {{error-free}} capacity awaited Claude Shannon, who built on Hartley's observations about a logarithmic <b>measure</b> <b>of</b> <b>information</b> and Nyquist's observations {{about the effect of}} bandwidth limitations.|$|E
40|$|There is {{interest}} in artificial intelligence for principled techniques to analyze inconsistent information. This {{stems from the}} recognition that the dichotomy between consistent and inconsistent sets of formulae that comes from classical logics is not sufficient for describing inconsistent information. We review some existing proposals and make new proposals for <b>measures</b> <b>of</b> inconsistency and <b>measures</b> <b>of</b> <b>information,</b> and then prove {{that they are all}} pairwise incompatible. This shows that the notion of inconsistency is a multi-dimensional concept where different measures provide different insights. We then explore relationships between <b>measures</b> <b>of</b> inconsistency and <b>measures</b> <b>of</b> <b>information</b> in terms <b>of</b> the trade-offs they identify when using them to guide resolution of inconsistency...|$|R
40|$|AbstractIn the {{probabilistic}} theory <b>of</b> <b>information,</b> <b>measures</b> <b>of</b> <b>information</b> depend only {{upon the}} probabilities of the events, {{whereas in the}} nonprobabilistic theory these measures depend only upon the events. In a new, mixed theory <b>of</b> <b>information</b> the <b>measures</b> <b>of</b> <b>information</b> are assumed to depend on both the probabilities and the events. In this paper we consider measures depending upon a n-tuple of events and upon {{a finite number of}} n-ary complete, discrete probability distributions and characterize these <b>measures</b> <b>of</b> <b>information</b> only by two properties: by a recursivity condition, which states how the information changes by splitting one event of a system (one outcome of an experiment, market situation etc.) into two events, and by a weak symmetry condition (no regularity condition is assumed). Our result generalizes all recent results on this topic and especially we get from this one theorem a lot of characterization theorems for some well-known (purely probabilistic) information measures like the Shannon entropy, the entropy of degree α, the inaccuracy, the directed divergence, and the information improvement...|$|R
40|$|The paper {{deals with}} the {{stability}} of the fundamental equation <b>of</b> <b>information</b> <b>of</b> multiplicative type. It will be proved that the equation in question is stable in the sense of Hyers and Ulam under some assumptions. This result will be applied to prove the stability of a system of functional equations that characterizes the recursive <b>measures</b> <b>of</b> <b>information</b> <b>of</b> multiplicative type. Comment: 7 pages, published in Colloq. Math. in 200...|$|R
50|$|Entropy of {{a source}} is the <b>measure</b> <b>of</b> <b>information.</b> Basically, source codes {{try to reduce}} the {{redundancy}} present in the source, and represent the source with fewer bits that carry more information.|$|E
5000|$|... In these articles, three truly revolutionary {{ideas were}} {{introduced}} {{into a field}} which is considered being classical. These ideas followed from the new definition of entropy based on Shannon's <b>measure</b> <b>of</b> <b>information</b> (SMI).|$|E
5000|$|The key {{concept in}} the {{exploration}} {{problem is the}} notion of information gain, that is, the amount of knowledge acquired while pushing the frontiers. A probabilistic <b>measure</b> <b>of</b> <b>information</b> gain is defined by the entropy ...|$|E
40|$|VPN ??? ??????? ??????????, ?? ??????????? ? ?????????? ??????. Telemedicine systems (providing {{health care}} {{services}} via telecommunication) are considered from the standpoint <b>of</b> <b>information</b> security. The necessity <b>of</b> additional <b>measures</b> <b>of</b> <b>information</b> protection is grounded, possible threats are studied, adequate methods of protection are proposed. Special attention is payed to VPN technology...|$|R
50|$|Likewise, one of {{the most}} common <b>measures</b> <b>of</b> <b>information</b> seeking behavior, library {{circulation}} statistics, also follows the 80-20 rule. This suggests that information seeking behavior is a manifestation not of a normal distribution curve, but a power law curve.|$|R
40|$|I n {{the present}} communication, we review the {{existing}} <b>measures</b> <b>of</b> fuzzy <b>information.</b> Wedefine and characterize two fuzzy information measures which are sub additive and differentfrom known <b>measures</b> <b>of</b> fuzzy <b>information.</b> We also study monotonic behavior and particularcases <b>of</b> these fuzzy <b>information</b> <b>measures.</b> 2000 Mathematics Subject Classification: 94 A 17 and 94 D 0...|$|R
5000|$|The Theil {{index is}} derived from Shannon's <b>measure</b> <b>of</b> <b>information</b> entropy , where entropy {{is a measure of}} {{randomness}} in a given set of information. In information theory, physics, and the Theil index, the general form of entropy is ...|$|E
50|$|This {{has great}} use in {{compression}} theory as {{it provides a}} theoretical means for compressing data, allowing us to represent any sequence Xn using nH(X) bits on average, and, hence, justifying the use of entropy as a <b>measure</b> <b>of</b> <b>information</b> from a source.|$|E
5000|$|Directed information, , is a <b>measure</b> <b>of</b> <b>information</b> {{theory and}} it {{measures}} {{the amount of}} information that flows from the process [...] to , where [...] denotes the vector [...] and [...] denotes [...] The term [...] "directed information" [...] was coined by James Massey and is defined as ...|$|E
40|$|The aims of {{this course}} are to {{introduce}} the principles and applications <b>of</b> <b>information</b> theory. The course will study how information is <b>measured</b> in terms <b>of</b> probability and entropy, and the relationships among conditional and joint entropies; how these are {{used to calculate the}} capacity of a communication channel, with and without noise; coding schemes, including error correcting codes; how discrete channels and <b>measures</b> <b>of</b> <b>information</b> generalise to their continuous forms; the Fourier perspective; and extensions to wavelets, complexity, compression, and efficient coding <b>of</b> audio-visual <b>information.</b> Lectures • Foundations: probability, uncertainty, <b>information.</b> How concepts <b>of</b> randomness, redundancy, compressibility, noise, bandwidth, and uncertainty are related to information. Ensembles, random variables, marginal and conditional probabilities. How the metrics <b>of</b> <b>information</b> are grounded in the rules of probability. • Entropies defined, and why they are <b>measures</b> <b>of</b> <b>information.</b> Marginal entropy, joint entropy, conditional entropy, and the Chain Rule for entropy. Mutual <b>information</b> between ensembles <b>of</b> random variables. Why entropy is the fundamental <b>measure</b> <b>of</b> informatio...|$|R
40|$|<b>Measures</b> <b>of</b> <b>information</b> theory {{are applied}} to the {{analysis}} of several thousand brief interactions between male grasshoppers. The system analyzed is neither ergodic nor stationary. Measurements of time-varying entropies and transinformations, and of the relative importance of various signals, illuminate the communication taking place...|$|R
40|$|Cryptography {{relies on}} the secrecy <b>of</b> keys. <b>Measures</b> <b>of</b> <b>information,</b> and thus secrecy, are called entropy. Previous work does not {{formally}} assess the cryptographically appropriate entropy of secret keys. This report defines several new forms of entropy appropriate for cryptographic situations. This report define...|$|R
50|$|Information {{theory is}} based on {{probability}} theory and statistics. Information theory often concerns itself with measures of information of the distributions associated with random variables. Important quantities of information are entropy, a <b>measure</b> <b>of</b> <b>information</b> in a single random variable, and mutual information, a <b>measure</b> <b>of</b> <b>information</b> in common between two random variables. The former quantity is a property of the probability distribution of a random variable and gives a limit on {{the rate at which}} data generated by independent samples with the given distribution can be reliably compressed. The latter is a property of the joint distribution of two random variables, and is the maximum rate of reliable communication across a noisy channel in the limit of long block lengths, when the channel statistics are determined by the joint distribution.|$|E
5000|$|... where [...] is the {{probability}} that message m is chosen from all possible choices in the message space [...] The base of the logarithm only affects a scaling factor and, consequently, the units in which the measured information content is expressed. If the logarithm is base 2, the <b>measure</b> <b>of</b> <b>information</b> is expressed in units of bits.|$|E
5000|$|Ralph Hartley's 1928 paper, Transmission of Information, {{uses the}} word [...] "information" [...] as a {{measurable}} quantity, reflecting the receiver's ability to distinguish one sequence of symbols from any other. The natural unit of information was therefore the decimal digit, much later renamed the hartley in his honour as a unit or scale or <b>measure</b> <b>of</b> <b>information.</b>|$|E
40|$|In {{order to}} provide {{adequate}} multivariate <b>measures</b> <b>of</b> <b>information</b> flow between neural structures, modified expressions of partial directed coherence (PDC) and directed transfer function (DTF), two popular multivariate connectivity measures employed in neuroscience, are introduced and their formal relationship to mutual information rates are proved...|$|R
40|$|Abstract Using the Genetic Analysis Workshop 14 (GAW 14) {{simulated}} dataset, {{we compare}} microsatellite and single-nucleotide polymorphism (SNP) markers in terms <b>of</b> two <b>measures</b> <b>of</b> <b>information</b> content, the traditional entropy-based information content measure, {{and a new}} "relative information" measure. Both attempt to <b>measure</b> the amount <b>of</b> <b>information</b> contained in the markers about the identity-by-descent (IBD) sharing among relatives. The performance <b>of</b> the two <b>information</b> <b>measures</b> are compared based on their variability and ability to predict change in the LOD score (ΔLOD) as map density increases for SNP markers. Although in a linked region, LOD scores are correlated with <b>measures</b> <b>of</b> <b>information,</b> we observe that none <b>of</b> the <b>measures</b> predict the LOD score itself very well. In an unlinked region, the LOD score {{is not related to}} either <b>measures</b> <b>of</b> <b>information.</b> The <b>information</b> content <b>of</b> microsatellite markers with 7. 5 -cM spacing is slightly higher than that of SNP markers with 3 -cM spacing. At these map densities, microsatellites are found to be uniformly more informative than SNPs irrespective of their level of heterozygosity. For SNPs, we found that as the level of heterozygosity increases, the information content increases. As reported in all other previous studies, we also found that high-density SNPs have higher information content compared to low-density microsatellites. Performance of both the two information measures considered here are similar, but the relative information measure predicts ΔLOD as marker density increases better than the traditional entropy-based information measure. </p...|$|R
40|$|The {{prominent}} educational {{theories of}} Vygotsky have just entered the discipline <b>of</b> <b>information</b> literacy. I {{will concentrate on}} three of his themes: the dialectical interdependence {{of the environment and}} the self, the need to relate to a student's potential rather than his or her achievement, and the inadequacy <b>of</b> most current <b>measures</b> <b>of</b> <b>information</b> literacy...|$|R
