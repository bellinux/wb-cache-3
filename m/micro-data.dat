1153|0|Public
50|$|The {{resulting}} micro-datasets can {{be accessed}} by researchers via the SAFE Center at the premises of Eurostat in Luxembourg or the anonymised <b>micro-data</b> via CD Rom's; some countries provides also access to their <b>micro-data</b> on similar safe centers. Eurostat also provides access to the EU-wide dataset for selected countries. Some non-EU countries perform very similar surveys according to the same methodology. These include Canada, Australia, New Zealand and South Africa.|$|E
5000|$|The Luxembourg Income Study is a {{non-profit}} <b>micro-data</b> archive and research institute. The center {{is located in}} Luxembourg with a satellite in New York City at the CUNY Graduate center. It serves the global community of researchers, educators and policy makers. It houses the Luxembourg Income Study Database, {{which is the largest}} available database of <b>micro-data</b> collected from multiple countries and the Luxembourg Wealth Study Database, which is the only cross-national wealth database in existence.|$|E
50|$|Integrated Research Infrastructure in Social Sciences at CEPS/INSTEAD (IRISS) was a visitor's {{programme}} at CEPS/INSTEAD. Its {{mission was}} to organise short visits of researchers willing to undertake empirical research in economics and other social sciences using the archive of <b>micro-data</b> available at the Centre.|$|E
50|$|The Social Policy Center Regular CPS {{activities}} include generating statistics and analyses {{based on the}} processing of <b>micro-data</b> - one of the Center’s main characteristics - and contributing to the design, implementation and evaluation of public policies - industrial and general, at the domestic and international levels - and private sector strategies.|$|E
50|$|A {{baseline}} cohort {{for the six}} {{participating countries}} was created {{as part of the}} larger World Health Survey effort and contains data on the situation of 65,964 adults aged 18 years and older, including over 20,000 persons aged 50 years and older. Samples of these respondents were followed-up as a part of SAGE Wave 1 (2007-10) data collection in four of the six SAGE countries (Ghana, India, Mexico and the Russian Federation). Meta- and <b>micro-data</b> are in the public domain through WHO at http://apps.who.int/healthinfo/systems/surveydata/index.php/catalog/whs.|$|E
5000|$|In 2007, Mikhail Golosov and Robert Lucas {{found that}} the size of the menu cost needed to match the <b>micro-data</b> of price {{adjustment}} inside an otherwise standard business cycle model is implausibly large to justify the menu-cost argument. The reason is that such models lack [...] "real rigidity". This is a property that markups do not get squeezed by large adjustment in factor prices (such as wages) that could occur in response to the monetary shock. Modern New Keynesian models address this issue by assuming that the labor market is segmented, so that the expansion in employment by a given firm does not lead to lower profits for the other firms.|$|E
50|$|The land change {{modeling}} {{community can}} {{also benefit from}} Global Positioning System and Internet-enabled mobile device data distribution. Combining various structural-based data-collecting methods can improve the availability of microdata and the diversity of people that see the findings and outcomes of land change modeling projects. For example, citizen-contributed data supported the implementation of Ushahidi in Haiti after the 2010 earthquake, helping at least 4,000 disaster events. Universities, non-profit agencies, and volunteers are needed to collect information on events like this to make positive outcomes and improvements in land change modeling and land change modeling applications. Tools such as mobile devices are available {{to make it easier}} for participants to participate in collecting <b>micro-data</b> on agents. Google Maps uses cloud-based mapping technologies with datasets that are co-produced by the public and scientists. Examples in agriculture such as coffee farmers in Avaaj Otalo showed use of mobile phones for collecting information and as an interactive voice.|$|E
50|$|The Dongsan Library, {{originally}} {{known as}} the Library of Keimyung Christian College, was established in July 1958. It was moved to its current site at the Seongseo Campus in March 1993 to facilitate expansion into a much larger facility that {{meets the needs of}} the information age. The Dongsan Library comprises three separate libraries: the main Dongsan Library at Seongseo Campus (seven stories above and two below ground level, with a total floor space of 6,538 pyeong), the second Dongsan Library at the Daemyung Dong Campus (seven stories above and two below ground level, with a total floor space of 5,392 pyeong) and the Medical Library at Dongsan Medical Center. Equipped with sophisticated multi-media functions and an advanced information retrieval network, the Dongsan Library is now the focal point for research activities of faculty members as well as students. At present, the Dongsan Library houses around one and a half- million books, including specialized reference books, scientific journals, theses, ancient documents, <b>micro-data,</b> CD-ROMs and multi-media materials. The information or data owned by the Dongsan Library is made available for users around the world through the Keimyung University Library Integrated Information Management System (KIMS).|$|E
5000|$|All {{participants}} in the technological employment debates agree that temporary job losses can result from technological innovation. Similarly, there is no dispute that innovation sometimes has positive effects on workers. Disagreement focuses on whether {{it is possible for}} innovation to have a lasting negative impact on overall employment. Levels of persistent unemployment can be quantified empirically, but the causes are subject to debate. Optimists accept short term unemployment may be caused by innovation, yet claim that after a while, compensation effects will always create at least as many jobs as were originally destroyed. While this optimistic view has been continually challenged, it was dominant among mainstream economists for most of the 19th and 20th centuries. For example, labor economists Jacob Mincer and Stephan Danninger develop an empirical study using <b>micro-data</b> from the Panel Study of Income Dynamics, and find that although in the short run, technological progress seems to have unclear effects on aggregate unemployment, it reduces unemployment in the long run. When they include a 5-year lag, however, the evidence supporting a short-run employment effect of technology seems to disappear as well, suggesting that technological unemployment [...] "appears to be a myth".|$|E
50|$|Demetriades’s {{academic}} work {{focuses on the}} relationship between finance and growth as well as the interactions between public and private capital and productivity. His early work on finance and growth challenged the view that growth in the financial system is, by itself, sufficient to deliver more economic growth and emphasised the importance of good institutions. Moreover, he has shown that, where present, the contribution of the banking sector to growth is more significant than that of equity finance through the stock market. More recent work has challenged widely held views about government owned banks by showing that, far from acting as an obstacle to growth, such banks can actually enhance both financial and economic development. His work on public capital contradicts the ‘crowding out’ argument often used by mainstream economists to justify cuts in public spending and reduce the size of government. Specifically, he shows that public investment can actually promote the productivity of the private sector and stimulate trade, employment and long run growth. In this sense, his work has a distinctly Keynesian flavour, influenced by new Keynesians such as Joseph Stiglitz. Although his work can be classified as macroeconomics, he also utilises both <b>micro-data</b> and micro-econometrics. His {{academic work}} has been influential, as evidenced by nearly 6,900 citations in Google Scholar. According to the RePEc rankings, he is ranked among the top 2% of the world’s most widely read economists as well as the top 5% of economic authors worldwide based on criteria like average rank score, number of works number of distinct works, number of citations etc.|$|E
40|$|Detecting {{outliers}} in longitudinal <b>micro-data</b> is a {{very involved}} task. First, the <b>micro-data</b> must be generated and stored {{in a manner that}} enables them to be linked through time. Additionally, since it is impractical to review each <b>micro-data</b> record, especially if the data series contains many millions of <b>micro-data</b> records, the data must be aggregated properly. Too high a level of aggregation may have the affect of removing variation in the time-series, thus eliminating observable outliers in the data. This paper presents techniques for identifying outliers in a time-series comprised of cross-sectional <b>micro-data...</b>|$|E
30|$|This ranking still {{emerges from}} <b>micro-data,</b> as well. See Giordano et al. (2011). The {{substantial}} public sector wage premium in Greece {{is also a}} feature identified in <b>micro-data</b> (see, e.g., Christopoulou and Monastiriotis (2014, 2016)).|$|E
40|$|The <b>micro-data</b> {{on prices}} {{indicate}} that prices, on average, remain unchanged for several months. However, central banks (e. g. European Central Bank) use models for monetary policy analysis that ignore this fact {{on the grounds}} that they can explain the persistence of inflation well. In this paper, I evaluate the consequences of implementing policies that are optimal from the perspective of a model that ignores the <b>micro-data.</b> The findings reported in the paper suggest that policy conclusions are significantly affected by whether persistence arises in a manner consistent with the <b>micro-data</b> and that employing models that do not respect <b>micro-data</b> can lead to costly policy mistakes. ...|$|E
40|$|Micro-econometric demand {{modelling}} {{has been}} receiving an increasing attention in empirical research, mainly {{due to the}} increasing availability of <b>micro-data.</b> In this paper we provide a review of some relevant market and policy issues that can be analysed {{with the use of}} <b>micro-data</b> on demand. Problems arising from the treatment of <b>micro-data</b> are revised, mainly with reference to the standard neo-classical framework, although other approaches are also sketched. Finally, building on previous research, a dynamic model accounting for health issues, mainly obesity, is proposed for future research. Demand and Price Analysis, Health Economics and Policy, Research Methods/ Statistical Methods,...|$|E
40|$|Published {{version of}} {{an article in the}} journal: Pattern Analysis and Applications. Also {{available}} from the publisher at: [URL] consider the micro-aggregation problem which involves partitioning a set of individual records in a <b>micro-data</b> file into a number of mutually exclusive and exhaustive groups. This problem, which seeks for the best partition of the <b>micro-data</b> file, is known to be NP-hard, and has been tackled using many heuristic solutions. In this paper, we would like to demonstrate that {{in the process of developing}} micro-aggregation techniques (MATs), it is expedient to incorporate information about the dependence between the random variables in the <b>micro-data</b> file. This can be achieved by pre-processing the <b>micro-data</b> before invoking any MAT, in order to extract the useful dependence information from the joint probability distribution of the variables in the <b>micro-data</b> file, and then accomplishing the micro-aggregation on the "maximally independent" variables-thus confirming the conjecture [A conjecture, which was recently proposed by Domingo-Ferrer et al. (IEEE Trans Knowl Data Eng 14 (1) : 189 - 201, 2002), was that the phenomenon of micro-aggregation can be enhanced by incorporating dependence-based information between the random variables of the <b>micro-data</b> file by working with (i. e., selecting) the maximally independent variables. Domingo-Ferrer et al. have proposed to select one variable from among the set of highly correlated variables inferred via the correlation matrix of the <b>micro-data</b> file. In this paper, we demonstrate that this process can be automated, and that it is advantageous to select the "most independent variables" by using methods distinct from those involving the correlation matrix. ] of Domingo-Ferrer et al. Our results, on real life and artificial data sets, show that including such information will enhance the process of determining how many variables are to be used, and which of them should be used in the micro-aggregation process...|$|E
40|$|We {{consider}} the micro-aggregation problem which involves partitioning {{a set of}} individual records in a <b>micro-data</b> file {{into a number of}} mutually exclusive and exhaustive groups. This problem, which seeks for the best partition of the <b>micro-data</b> file, is known to be NP-hard, and has been tackled using many heuristic solutions. In this paper, we would like to demonstrate that {{in the process of developing}} micro-aggregation techniques (MATs), it is expedient to incorporate information about the dependence between the random variables in the <b>micro-data</b> file. This can be achieved by pre-processing the <b>micro-data</b> before invoking any MAT, in order to extract the useful dependence information from the joint probability distribution of the variables in the <b>micro-data</b> file, and then accomplishing the micro-aggregation on the "maximally independent" variables-thus confirming the conjecture [A conjecture, which was recently proposed by Domingo-Ferrer et al. (IEEE Trans Knowl Data Eng 14 (1) : 189 - 201, 2002), was that the phenomenon of micro-aggregation can be enhanced by incorporating dependence-based information between the random variables of the <b>micro-data</b> file by working with (i. e., selecting) the maximally independent variables. Domingo-Ferrer et al. have proposed to select one variable from among the set of highly correlated variables inferred via the correlation matrix of the <b>micro-data</b> file. In this paper, we demonstrate that this process can be automated, and that it is advantageous to select the "most independent variables" by using methods distinct from those involving the correlation matrix. ] of Domingo-Ferrer et al. Our results, on real life and artificial data sets, show that including such information will enhance the process of determining how many variables are to be used, and which of them should be used in the micro-aggregation process...|$|E
40|$|The popular Calvo {{model with}} {{indexation}} (Christiano, Eichenbaum and Evans (2005)) and sticky information (Mankiw and Reis (2002)) models have guided {{much of the}} monetary policy discussion. The strength of these approaches {{is that they can}} explain the persistence of inflation. However, both of these theories are inconsistent with the micro data on prices. In this paper, I evaluate the consequences of implementing policies that are optimal from the perspective of models that overlook the <b>micro-data.</b> To do so, I employ two models: the model proposed by Gali and Gertler (1999) and the Generalized Taylor Economy (GT E). These models can explain the persistence of inflation and are consistent with the <b>micro-data.</b> The findings reported in the paper suggest that policy conclusions are significantly affected by whether persistence arises {{in a manner consistent with}} the <b>micro-data</b> and illustrate the potential for conclusions from the models that ignore the <b>micro-data</b> to be misleading...|$|E
30|$|Access to the {{unrestricted}} <b>micro-data</b> file {{was provided}} through the Toronto Research Data Center (RDC).|$|E
3000|$|All {{raw data}} was {{downloaded}} from the Integrated Public Use <b>Micro-data</b> Series (IPUMS) available at [URL] [...]...|$|E
40|$|Important {{questions}} can be answered using frequency tables. However, when publishing or releasing frequency tables, many data custodians are legally required {{to ensure that the}} risk of disclosing information about a person or organisation is acceptably low. A relatively recent way of releasing frequency tables is via a remote server. A remote server automatically responds to queries on the <b>micro-data</b> that are submitted by analysts, but it does not allow analysts to view the underlying <b>micro-data.</b> The challenge is to develop an automatic response to queries that has an acceptable disclosure risk releases tables that are useful to analysts. This paper describes the methodology underlying the Australian Bureau of Statistic 2 ̆ 7 remote server, called TableBuilder, for releasing frequency tables. We introduce a framework for measuring the disclosure risk of allowing analysts access to <b>micro-data</b> via TableBuilder. We apply this framework to meaure the disclosure risk of <b>micro-data</b> created by linking the Australian Population Census to administrative migration data, supplied by the Australian Department of Immigration and Customs. We conclude that a remote server provides a very effective balance between disclosure risk and utility...|$|E
40|$|Microsimulation is {{a rapidly}} {{expanding}} area of spatial modelling, {{which seems to}} offer great potential for applied policy analysis. However, currently there is considerable debate on the most appropriate methodology for estimating <b>micro-data.</b> Household or individual attribute data can be represented both as lists and/or as tabulations. It has long been argued (Birkin and Clarke, 1995; Clarke, 1996; Williamson et al, 1998) that the representation of information on households and individuals {{in the form of}} lists offers greater efficiency of storage and spatial flexibility as well as an ability to update and forecast. This paper reviews the possibilities and methodologies of building list-based population <b>micro-data</b> for small areas. First, it evaluates the methods, which have been developed and employed so far for the estimation of population <b>micro-data,</b> outlining the advantages and drawbacks of each one of them. Then the paper investigates the comparison of methods for generating conditi [...] ...|$|E
3000|$|The {{data used}} in this article have been {{downloaded}} from the OECD website in October 2014 {{in the form of}} anonymised public use <b>micro-data</b> files for research purposes ([URL] [...]...|$|E
40|$|This paper {{uses the}} <b>micro-data</b> from the Russia Longitudinal Monitoring Survey (RLMS) to {{identify}} factors that explain fertility between 1995 and 2004. The analysis based on <b>micro-data</b> supports {{the experience of}} other countries that fertility is not solely determined by short-term factors such as rising incomes or by the economic climate. Evidence also suggests that childbirth incentive measures may only have a short-term impact. There are questions meanwhile over the sustainability of providing cash payments in return for childbirth {{on a scale that}} exceeds average incomes ‒ {{as is the case with}} the Mothers ʼ Fund...|$|E
40|$|We {{evaluate}} {{consumption and}} income {{measures of the}} material well-being of the poor. We begin with conceptual and pragmatic reasons that favor income or consumption. Then, we empirically examine the quality of standard data by studying measurement error and under-reporting, and by comparing <b>micro-data</b> from standard surveys to administrative <b>micro-data</b> and aggregates. We also compare low reports of income and consumption to other measures of hardship and well-being. The closer link between consumption and well-being and its better measurement favors the use of consumption when setting benefits and evaluating transfer programs. However, income retains its convenience for determining program eligibility. ...|$|E
40|$|Competition has {{pervasive}} and long-lasting effects on economic performance by affecting economic actors’ incentive structures, by encouraging their innovative activities, by stimulating technology spillovers, and by selecting more efficient firms from less efficient ones over time. A {{growing number of}} empirical studies using longitudinal <b>micro-data</b> confirm that firm dynamics (i. e., entry and exit, growth and decline of individual firms) {{is an important component}} of innovation and of aggregate productivity growth. The dynamism of Asian NIEs (Newly Industrializing Economies) revealed in their export-oriented growth paths has drawn substantial attention from researchers. But, empirical studies based on longitudinal <b>micro-data</b> in Asia are still rare, mainly {{due to the lack of}} readily available data. Based on the unpublished plant-level data underlying the Annual Report on Mining and Manufacturing Survey of Korea (1980 - 2000), this study proposes to assess the links between competition, technology spillovers, and firm dynamics in Korea. This research aims to contribute to understanding the micro-level innovation/selection processes of export-oriented economies in Asia over economic fluctuations under intensifying global competitioncompetition, export, productivity, spillovers, <b>micro-data...</b>|$|E
30|$|The Fog {{paradigm}} {{envisions a}} set of <b>micro-data</b> centers, placed {{at the edge of}} the network, with the following characteristics [9]: location awareness, mobility support, real-time interactions, low latency, geographical distribution, heterogeneity, interoperability, security, and privacy.|$|E
40|$|The aim of {{this paper}} is to {{estimate}} the role of territory and the individual firm in innovation in the Brazilian industrial economy after the trade-opening period from 1998 onwards. This study is based on a database whose <b>micro-data</b> are a merger between the Technological Innovation Survey (PINTEC) and the Yearly Industrial Survey (PIA) of the Brazilian Institute of Geography and Statistics (IBGE). These <b>micro-data</b> are analyzed by the logit regression method as well as using hierarchical regression models. The main results reveal that firm-level variables and region-level variables are complementary but with the former having more impact on the propensity to innovate than the latter...|$|E
30|$|The above {{policy and}} {{economic}} settings also influence our general approach in this paper. We take a three-stepped approach based on aggregate indicators of labor market outcomes {{over time and}} across countries using <b>micro-data</b> from the EU Labour Force Survey (EU-LFS).|$|E
3000|$|The {{datasets}} generated and/or analysed {{during the}} current study {{are available in the}} OECD patent-related database repository (OECD 2017) upon request via an online form to the OECD/STI <b>Micro-data</b> Lab. Further information can be found in this page: [URL] [...]...|$|E
40|$|Part 3 : Policy and StakeholdersInternational audienceThis paper {{explores the}} <b>micro-data</b> from the ICT Households Survey {{in order to}} {{categorize}} the Brazilian Internet users according to the diversity of activities undertaken by these users on-line and assess the propensity of these Internet user groups to use e-gov services. The Amartya Sen’s Capability Approach was adopted as theoretical framework for its consideration of people’s freedom to decide on {{their use of the}} available resources and their competencies for these decisions, leading to the use of e-government services. This paper uses a positivistic approach a descriptive and exploratory analysis of secondary data (<b>micro-data)</b> from the 2007, 2009 and 2011 editions of the ICT Household survey...|$|E
40|$|In {{this paper}} the {{structure}} and the semantic properties of the entities stored in databases, whose data are only aggregate-type data, are defined and discussed. This choice is justified by the wide spread use of aggregate data without the corresponding raw data (i. e. <b>micro-data,</b> such as census data). Aggregate data are often derived by applying statistical aggregation (e. g. sum, count) and statistical analysis functions over <b>micro-data,</b> so that the relative databases are called "statistical databases". For this reason in this paper the above entities are called statistical object and a new representation model based on a graph representation is proposed. Finally representational problems in current models are identified and some solutions are proposed...|$|E
40|$|In {{disclosing}} <b>micro-data</b> with sensitive attributes, {{the goal}} is usually two fold. First, the data utility of disclosed data should be maximized for analysis purposes. Second, the private information contained in such data must be limited to an acceptable level. Recent studies show that adversarial inferences using knowledge about a disclosure algorithm can usually render the algorithm unsafe. In this paper, we show that an existing unsafe algorithm can {{be transformed into a}} large family of distinct safe algorithms, namely, k-jump algorithms. We prove that the data utility of different k-jump algorithms is generally incomparable. Therefore, a secret choice can be made among all k-jump algorithms to eliminate adversarial inferences while improving the data utility of disclosed <b>micro-data.</b> 1...|$|E
40|$|We {{describe}} statistical disclosure control methods (SDC) {{developed for}} a public release Canadian Hospitals Injury Reporting and Prevention Program (CHIRPP) <b>micro-data</b> file. CHIRPP is a national injury surveillance database managed by the Public Health Agency of Canada (PHAC). After describing CHIRPP, the paper includes {{a brief overview of}} basic SDC concepts, as an introduction to the process for selecting and developing the appropriate SDC methods for CHIRPP given its specific challenges and requirements. We then summarize some key results. The paper concludes with a discussion of the implication of this work for the health information field and closing remarks with respect to the some methodological issues for consideration. KEY WORDS: public use <b>micro-data</b> file; injury surveillance; cell suppression...|$|E
40|$|This paper employs firm-level data {{to analyze}} the {{relative}} importance of firm characteristics and agglomeration externalities in explaining variation in innovation rates across firms. More specifically, we combine <b>micro-data</b> and census data to estimate the probability that a firm will introduce a goods, service or process innovation. We consider internal firm-level characteristics as well as externalities, using information on the regional production structure to test for Marshall-Arrow-Romer, Porter and Jacobs effects. Our results show that most firm-specific variables are highly statistically significant, whereas agglomeration variables are only significant for a few specific sectors, and even then only for some types of innovation. innovation, absorptive capacity, agglomeration externalities, Community Innovation Survey, <b>micro-data,</b> firm behavior...|$|E
40|$|The {{study of}} <b>micro-data</b> {{disclosure}} issue has largely {{focused on the}} privacy preservation aspect, whereas the integrity of a published <b>micro-data</b> table has received limited attention. Unauthorized updates to such a table may lead users to believe in misleading data. Traditional cryptographic stamp-based approaches allow users to detect unauthorized updates using credentials issued by the data owner. However, to localize the exact corrupted tuples would require {{a large number of}} cryptographic stamps to be stored, leading to prohibitive storage requirements. In this thesis, we explore the fact that tuples in a <b>micro-data</b> table must be stored in a particular order, which has no inherent meaning under the relational model. We propose a series of algorithms for embedding watermarks through reordering the tuples. The embedded watermarks allow users to detect, localize, and restore corrupted tuples with a single secret key issued by the data owner, and no additional storage is required. At the same time, our algorithms also allow for efficient updates by the data owner or legitimate users who know the secret key. The proposed algorithms are implemented and evaluated through experiments with real data...|$|E
40|$|There is {{increasing}} realisation that edge devices, which {{are closer to}} a user, {{can play an important}} part in supporting latency and privacy sensitive applications. Such devices have also continued to increase in capability over recent years, ranging in complexity from embedded resources (e. g. Raspberry Pi, Arduino boards) placed alongside data capture devices to more complex “micro data centres”. Using such resources, a user is able to carry out task execution and data storage in proximity to their location, often making use of computing resources that can have varying ownership and access rights. Increasing performance requirements for stream processing applications (for instance), which incur delays between the client and the cloud have led to newer models of computation, which requires an application workflow to be split across data centre and edge resource capabilities. With recent emergence of edge/fog computing it has become possible to migrate services to <b>micro-data</b> centres and to address the performance limitations of traditional (centralised data centre) cloud based applications. Such migration can be represented as a cost function that involves incentives for <b>micro-data</b> centres to host services with associated quality of services and experience. Business models need to be developed for creating an open edge cloud environment where <b>micro-data</b> centres have the right incentives to support service hosting, and for large scale data centre operators to outsource service execution to such micro data centres. We describe potential revenue models for <b>micro-data</b> centers to support service migration and serve incoming requests for edge based applications. We present several cost models which involve combined use of edge devices and centralised data centres...|$|E
