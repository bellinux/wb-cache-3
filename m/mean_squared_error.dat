5792|10000|Public
25|$|<b>Mean</b> <b>squared</b> <b>error</b> is {{used for}} obtaining {{efficient}} estimators, a widely used class of estimators. Root mean square error is simply the square root of <b>mean</b> <b>squared</b> <b>error.</b>|$|E
25|$|The <b>mean</b> <b>squared</b> <b>error</b> of the Rao–Blackwell {{estimator}} {{does not}} exceed {{that of the}} original estimator.|$|E
25|$|The <b>mean</b> <b>squared</b> <b>error</b> of an {{estimator}} is {{the expected}} {{value of the}} square of its deviation from the unobservable quantity being estimated.|$|E
30|$|The minimum <b>mean</b> <b>square</b> <b>error</b> {{algorithm}} can {{minimize the}} <b>mean</b> <b>square</b> <b>error</b> between the expected and actual output values of adaptive filter. It {{is widely used}} for characteristics such as less calculation and easy to implement. The minimum <b>mean</b> <b>square</b> <b>error</b> algorithm {{is based on the}} steepest descent principle, that is, search along the negative gradient direction of weight and achieve the optimal weight to minimize the filtered <b>mean</b> <b>square</b> <b>error.</b>|$|R
3000|$|... of the <b>mean</b> <b>square</b> <b>error</b> is determined, that is, a {{distance}} estimation with a <b>mean</b> <b>square</b> <b>error</b> smaller than [...]...|$|R
40|$|This article {{discusses}} three chain estimators {{for population}} mean using double sampling, {{which is a}} review article from Choudhury and Singh [Statistics In Transition-new series 2012 : 519 - 536]. The three estimators are biased estimators, then their <b>mean</b> <b>square</b> <b>errors</b> are determined. Furthermore, the <b>mean</b> <b>square</b> <b>errors</b> are compared to the <b>mean</b> <b>square</b> <b>error</b> of each estimator. This comparison shows that the estimator having thesmallest <b>mean</b> <b>square</b> <b>error</b> is the most efficient estimator...|$|R
25|$|<b>Mean</b> <b>squared</b> <b>error,</b> {{a measure}} of how 'good' an {{estimator}} of a distributional parameter is (be it the maximum likelihood estimator or some other estimator).|$|E
25|$|As a result, {{there was}} {{considerable}} shock and disbelief when Stein demonstrated that, in terms of <b>mean</b> <b>squared</b> <b>error</b> , this approach is suboptimal. The result became known as Stein's phenomenon.|$|E
25|$|When {{the mean}} is not known, the minimum <b>mean</b> <b>squared</b> <b>error</b> {{estimate}} of the variance of a sample from Gaussian distribution is achieved by dividing by nnbsp&+nbsp&1, rather than nnbsp&−nbsp&1 or nnbsp&+nbsp&2.|$|E
3000|$|<b>Mean</b> <b>square</b> <b>error</b> (MSE): <b>mean</b> of <b>square</b> <b>errors</b> between {{estimated}} {{prediction error}} degrees and their true degrees [...]...|$|R
40|$|Abstract — The {{problem of}} the {{sensitivity}} analysis of color scanning filters is addressed in this paper. The second differential of the <b>mean</b> <b>square</b> <b>error</b> provides a <b>means</b> of calculating {{the sensitivity of the}} <b>mean</b> <b>square</b> <b>error</b> to filter fabrication errors. Tolerances on the allowable change in the <b>mean</b> <b>square</b> <b>error</b> are used to define bounds on the filter fabrication errors at all wavelengths and at single wavelengths. I...|$|R
30|$|The {{influence}} of the semivariogram model selection is highlighted and illustrated by thematic maps drawn using four different models (Gaussian, magnetic, spherical and exponential). Then, a guideline to select the most suitable model, using <b>mean</b> <b>error</b> (ME), <b>mean</b> <b>square</b> <b>error</b> (MSE), root <b>mean</b> <b>square</b> <b>error</b> (RMSE), average standard error (ASE), and root <b>mean</b> <b>square</b> standardized <b>error</b> (RMSSE), is proposed.|$|R
25|$|The Rao–Blackwell theorem, {{a result}} which yields {{a process for}} finding the best {{possible}} unbiased estimator (in the sense of having minimal <b>mean</b> <b>squared</b> <b>error).</b> The MLE is often a good starting place for the process.|$|E
25|$|Efficiency, i.e., it {{achieves}} the Cramér–Rao {{lower bound}} when {{the sample size}} tends to infinity. This means that no consistent estimator has lower asymptotic <b>mean</b> <b>squared</b> <b>error</b> than the MLE (or other estimators attaining this bound).|$|E
25|$|The Cramér–Rao bound {{can also}} be used to bound the {{variance}} of biased estimators of given bias. In some cases, a biased approach can result in both a variance and a <b>mean</b> <b>squared</b> <b>error</b> that are below the unbiased Cramér–Rao lower bound; see estimator bias.|$|E
3000|$|This {{method is}} {{evaluated}} {{on the basis}} of visual perception and the <b>mean</b> <b>square</b> <b>error</b> calculation. <b>Mean</b> <b>square</b> <b>error</b> is calculated by line fitting applied on input and output images with the help of following equations: [...]...|$|R
30|$|Weighted <b>mean</b> <b>square</b> <b>error.</b>|$|R
30|$|Normalised <b>mean</b> <b>square</b> <b>error</b> (NMSE).|$|R
25|$|Between two {{estimator}}s {{of a given}} parameter, the {{one with}} lower <b>mean</b> <b>squared</b> <b>error</b> {{is said to be}} more efficient. Furthermore, an estimator is said to be unbiased if its expected value is equal to the true value of the unknown parameter being estimated, and asymptotically unbiased if its expected value converges at the limit to the true value of such parameter.|$|E
25|$|This bias-corrected {{estimator}} is second-order efficient (at least {{within the}} curved exponential family), {{meaning that it}} has minimal <b>mean</b> <b>squared</b> <b>error</b> among all second-order bias-corrected estimators, up {{to the terms of}} the order n−2. It is possible to continue this process, that is to derive the third-order bias-correction term, and so on. However, as was shown by , the maximum likelihood estimator is not third-order efficient.|$|E
25|$|In its {{simplest}} form, the bound {{states that}} the variance of any unbiased estimator {{is at least as}} high as the inverse of the Fisher information. An unbiased estimator which achieves this lower bound is said to be (fully) efficient. Such a solution achieves the lowest possible <b>mean</b> <b>squared</b> <b>error</b> among all unbiased methods, and is therefore the minimum variance unbiased (MVU) estimator. However, in some cases, no unbiased technique exists which achieves the bound. This may occur even when an MVU estimator exists.|$|E
40|$|Abstract: The {{shrinkage}} estimators {{are widely}} used in estimation processes. These estimators have smaller <b>Mean</b> <b>Square</b> <b>Error’s</b> or Variance {{as compared with the}} conventional estimators. In this article we have proposed a general class of shrinkage estimators for estimation of any population characteristic. The <b>Mean</b> <b>Square</b> <b>Error</b> of the proposed estimator has also been developed. Some examples are given for practical applicability of the proposed estimator. Key words: <b>Mean</b> <b>square</b> <b>error</b> • shrinkage estimator • population characteristics • regression estimato...|$|R
30|$|The above {{mathematical}} modeling is {{the training}} representation of our proposed predictor. We train the MLP with the slot(s) state and idle time slot(s) history. The {{training of the}} proposed predictor is done by changing the weights according to (7) {{with the aim of}} minimizing the <b>mean</b> <b>square</b> <b>error,</b> i.e., E in (6). We have repeated the above weight updating procedure until the threshold in terms of the required <b>mean</b> <b>square</b> <b>error</b> is achieved, where the threshold <b>mean</b> <b>square</b> <b>error</b> is the tolerable prediction error.|$|R
40|$|Ordinary and jack-knifed ridge type estimators are {{compared}} for different measures of goodness. Although jack-knifing reduces bias considerably the jack-knifed ridge estimators have larger variance {{and may have}} a larger <b>Mean</b> <b>Square</b> <b>Error</b> than the usual ridge estimators. Variance bias <b>mean</b> <b>square</b> <b>error</b> signal to noise ratio...|$|R
25|$|Supervised neural {{networks}} that use a <b>mean</b> <b>squared</b> <b>error</b> (MSE) cost function can use formal statistical methods {{to determine the}} confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of the output of the network, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.|$|E
25|$|The numerator, n−p, is the {{statistical}} degrees of freedom. The first quantity, s2, is the OLS estimate for σ2, whereas the second, , is the MLE estimate for σ2. The two estimators are quite similar in large samples; {{the first one}} is always unbiased, while the second is biased but minimizes the <b>mean</b> <b>squared</b> <b>error</b> of the estimator. In practice s2 is used more often, since it is more convenient for the hypothesis testing. The square root of s2 is called the standard error of the regression (SER), or standard error of the equation (SEE).|$|E
25|$|Ridge regression, {{and other}} forms of penalized {{estimation}} such as Lasso regression, deliberately introduce bias into the estimation of β {{in order to reduce the}} variability of the estimate. The resulting estimators generally have lower <b>mean</b> <b>squared</b> <b>error</b> than the OLS estimates, particularly when multicollinearity is present or when overfitting is a problem. They are generally used when the goal is to predict the value of the response variable y for values of the predictors x that have not yet been observed. These methods are not as commonly used when the goal is inference, since it is difficult to account for the bias.|$|E
40|$|This paper {{discusses}} three ratio regression linear estimators {{for population}} mean Y using two auxiliary variables X and, Z using {{the coefficient of}} variation and coefficient of kurtosis on simple random sampling. This paper is {{a review of the}} article Singh et. al [Statistics in Transition 10 (1) : 85 - 100]. The three estimators are biased estimators and their <b>mean</b> <b>square</b> <b>errors</b> are determined. Each estimator compared to the value of the <b>mean</b> <b>square</b> <b>error.</b> Comparison shows that the estimator with the minimum <b>mean</b> <b>square</b> <b>error</b> is an efficient estimator...|$|R
5000|$|For any {{particular}} value of θ the new estimator will improve {{at least one}} of the individual <b>mean</b> <b>square</b> <b>errors</b> [...] This is not hard − for instance, if [...] is between −1 and 1, and σ = 1, then an estimator that moves [...] towards 0 by 0.5 (or sets it to zero if its absolute value was less than 0.5) will have a lower <b>mean</b> <b>square</b> <b>error</b> than [...] itself. But there are other values of [...] for which this estimator is worse than [...] itself. The trick of the Stein estimator, and others that yield the Stein paradox, is that they adjust the shift in such a way that there is always (for any θ vector) at least one [...] whose <b>mean</b> <b>square</b> <b>error</b> is improved, and its improvement more than compensates for any degradation in <b>mean</b> <b>square</b> <b>error</b> that might occur for another [...] The trouble is that, without knowing θ, you don't know which of the n <b>mean</b> <b>square</b> <b>errors</b> are improved, so you can't use the Stein estimator only for those parameters.|$|R
50|$|This {{choice of}} filter {{parameters}} minimizes the <b>mean</b> <b>square</b> <b>error.</b>|$|R
25|$|To {{assess how}} {{severely}} the dimensionality of a data set affects the analysis {{within the context}} of ABC, analytical formulas have been derived for the error of the ABC estimators as functions of the dimension of the summary statistics. In addition, Blum and François have investigated how the dimension of the summary statistics is related to the <b>mean</b> <b>squared</b> <b>error</b> for different correction adjustments to the error of ABC estimators. It was also argued that dimension reduction techniques are useful to avoid the curse-of-dimensionality, due to a potentially lower-dimensional underlying structure of summary statistics. Motivated by minimizing the quadratic loss of ABC estimators, Fearnhead and Prangle have proposed a scheme to project (possibly high-dimensional) data into estimates of the parameter posterior means; these means, now having the same dimension as the parameters, are then used as summary statistics for ABC.|$|E
2500|$|However, we {{can achieve}} a lower <b>mean</b> <b>squared</b> <b>error</b> using a biased {{estimator}}. The estimator ...|$|E
2500|$|This {{approach}} of minimizing integrated <b>mean</b> <b>squared</b> <b>error</b> from Scott's rule can be generalized beyond normal distributions, by using leave-one out cross validation: ...|$|E
40|$|Optimal {{powers of}} thc Gaussian and Jeffreys priors are {{obtained}} {{so that they}} minimize the asymptotic <b>mean</b> <b>square</b> <b>error</b> of the linear predictor and {{the sum of the}} asymptotic <b>mean</b> <b>square</b> <b>errors</b> of associated parameter estimators. Conditions that the summarized <b>mean</b> <b>square</b> <b>errors</b> using powers of the priors are smaller than those by maximum likelihood are given. In the case of a scalar canonical parameter in the exponential family，a matching prior for the Jeffreys power prior is found，where the Wald confidence interval has second-order accurate coverage. The results are numerically illustrated using the categorical distribution and logistic regression...|$|R
30|$|R[*]=[*] 0.98783 and <b>mean</b> <b>square</b> <b>error</b> MSE[*]=[*] 0.00171 after 30 epochs.|$|R
3000|$|... [...]. The {{distance}} consistency {{property of}} valid locators {{states that the}} <b>mean</b> <b>square</b> <b>error</b> of the location estimation based on the correct distance measurements is lower than a small threshold while the <b>mean</b> <b>square</b> <b>error</b> of the location estimation based on the distance measurements which contains some incorrect ones is not lower than the threshold.|$|R
