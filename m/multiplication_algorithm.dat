567|460|Public
5|$|The fast Fourier {{transform}} is an algorithm for rapidly computing the discrete Fourier transform. It is used {{not only}} for calculating the Fourier coefficients but, using the convolution theorem, also for computing the convolution of two finite sequences. They in turn are applied in digital filters and as a rapid <b>multiplication</b> <b>algorithm</b> for polynomials and large integers (Schönhage-Strassen algorithm).|$|E
25|$|See also Booth's <b>multiplication</b> <b>algorithm.</b>|$|E
25|$|Note: Due to {{the variety}} of {{multiplication}} algorithms, M(n) below stands in for {{the complexity of the}} chosen <b>multiplication</b> <b>algorithm.</b>|$|E
5000|$|... #Subtitle level 2: Fast <b>multiplication</b> <b>algorithms</b> {{for large}} inputs ...|$|R
50|$|It {{plays a role}} in {{research}} of fast matrix <b>multiplication</b> <b>algorithms.</b>|$|R
5000|$|When {{performing}} any {{of these}} <b>multiplication</b> <b>algorithms</b> the following [...] "steps" [...] should be applied.|$|R
25|$|If both {{first and}} second number each have only one digit then their product is given in the {{multiplication}} table, and the <b>multiplication</b> <b>algorithm</b> is unnecessary.|$|E
25|$|The common {{methods for}} {{multiplying}} numbers using {{pencil and paper}} require a multiplication table of memorized or consulted products of small numbers (typically any two numbers from 0 to 9), however one method, the peasant <b>multiplication</b> <b>algorithm,</b> does not.|$|E
25|$|There are no {{cancellation}} or absorption {{problems with}} multiplication or division, though small errors may accumulate as operations are performed in succession. In practice, {{the way these}} operations are carried out in digital logic can be quite complex (see Booth's <b>multiplication</b> <b>algorithm</b> and Division algorithm).|$|E
40|$|International audienceElectrical {{activity}} {{variations in}} a circuit {{are one of}} the information leakage used in side channel attacks. In this work, we present GF(2 ^m) multipliers with reduced activity variations for asymmetric cryptography. Useful activity of typical <b>multiplication</b> <b>algorithms</b> is evaluated. The results show strong shapes, which {{can be used as a}} small source of information leakage. We propose modified <b>multiplication</b> <b>algorithms</b> and multiplier architectures to reduce useful activity variations during an operation...|$|R
40|$|Abstract — In today’s digital world, where {{portable}} computers have become {{as small as}} the size of palm limitation on processing speed has increased. Thus there’s a need for modification in the traditional approach to overcome this limitation. An implementation using parallel and pipelined approach could work at higher speed while occupying limited number of slices. Paper deals with analyzing and reviewing different <b>multiplication</b> <b>algorithms</b> viz. Vedic, Chinese, Wallace, Booth, Karatsuba and Toom-Cook by performing 11 * 8 bit multiplication using parallel and pipelined approach. Keywords- <b>Multiplication</b> <b>Algorithms,</b> Pipelining. I...|$|R
40|$|Abstract. The Truncated Multiplication computes a {{truncated}} product, a contiguous subsequence of the digits of {{the product}} of 2 integers. A few truncated polynomial <b>multiplication</b> <b>algorithms</b> are presented and adapted to integers. They {{are based on the}} most often used n-digit full <b>multiplication</b> <b>algorithms</b> of time complexity O(n α), with 1 < α ≤ 2, but a constant times faster. For example, the least significant half products with Karatsuba multiplication need only 80 % of the full multiplication time. The faster the multiplication, the less relative time saving can be achieved...|$|R
25|$|While {{taking the}} product of two (or more) {{expressions}} {{can be done by}} following a <b>multiplication</b> <b>algorithm,</b> the reverse process of factoring relies frequently on the recognition of a pattern in the expression to be factored and recalling how such a pattern arises. The following are some well known patterns.|$|E
25|$|The {{asymptotically}} best {{efficiency is}} obtained by computing n! from its prime factorization. As documented by Peter Borwein, prime factorization allows n! to be computed in time O(n(lognbsp&nnbsp&lognbsp&lognbsp&n)2), provided that a fast <b>multiplication</b> <b>algorithm</b> is used (for example, the Schönhage–Strassen algorithm). Peter Luschny presents source code and benchmarks for several efficient factorial algorithms, {{with or without}} the use of a prime sieve.|$|E
25|$|If the {{multiplicand}} {{does not}} have a hundreds-digit then if there is no carry digit then the <b>multiplication</b> <b>algorithm</b> has finished. If there is a carry digit (carried over from the tens-column) then write it in the hundreds-column under the line, and the algorithm is finished. When the algorithm finishes, the number under the line is the product of the two numbers.|$|E
40|$|The {{communication}} cost of algorithms (also {{known as}} I/O-complexity) {{is shown to}} be {{closely related to the}} expansion properties of the corresponding computation graphs. We demonstrate this on Strassen’s and other fast matrix <b>multiplication</b> <b>algorithms,</b> and obtain the first lower bounds on their communication costs. In the sequential case, where the processor has a fast memory of size M, too small to store three n-by-n matrices, the lower bound on the number of words moved between fast and slow memory is, for a large class of matrix <b>multiplication</b> <b>algorithms,</b> Ω...|$|R
5000|$|Obviously, at most {{half of the}} digits are non-zero, {{which was}} the reason it was {{introduced}} by G.W. Reitweisner [...] for speeding up early <b>multiplication</b> <b>algorithms,</b> much like Booth encoding.|$|R
25|$|Both {{shifting}} and doubling the precision {{are important for}} some <b>multiplication</b> <b>algorithms.</b> Note that unlike addition and subtraction, width extension and right shifting are done differently for signed and unsigned numbers.|$|R
25|$|The {{multiplication}} {{will consist}} of two parts. The first part {{will consist of}} several multiplications involving one-digit multipliers. The operation of each one of such multiplications was already described in the previous <b>multiplication</b> <b>algorithm,</b> so this algorithm will not describe each one individually, but will only describe how the several multiplications with one-digit multipliers shall be coordinated. The second part will add up all the subproducts of the first part, and the resulting sum will be the product.|$|E
25|$|The most {{commonly}} used reduction is a polynomial-time reduction. This means that the reduction process takes polynomial time. For example, the problem of squaring an integer {{can be reduced to}} the problem of multiplying two integers. This means an algorithm for multiplying two integers can be used to square an integer. Indeed, this can be done by giving the same input to both inputs of the <b>multiplication</b> <b>algorithm.</b> Thus we see that squaring is not more difficult than multiplication, since squaring can be reduced to multiplication.|$|E
25|$|Between {{the first}} and second lines, the ten-thousands-column will contain either one or two digits: the hundreds-digit of the hundreds-column and (possibly) the thousands-digit of the tens-column. Find the sum of these digits (if the one in the tens-row is missing {{think of it as a}} 0), and if there is a carry-digit from the thousands-column (written in {{superscript}} under the second line in the ten-thousands-column) then add this carry-digit as well. If the resulting sum has one digit then write it down under the second line in the ten-thousands-column; if it has two digits then write the last digit down under the line in the ten-thousands-column, and carry the first digit over to the hundred-thousands-column, writing it as a superscript to the yet-unwritten hundred-thousands digit under the line. However, if the hundreds-row has no thousands-digit then do not write this carry-digit as a superscript, but in normal size, in the position of the hundred-thousands-digit under the second line, and the <b>multiplication</b> <b>algorithm</b> is over.|$|E
40|$|A {{generalization}} of recent group-theoretic matrix <b>multiplication</b> <b>algorithms</b> to an analogue {{of the theory}} of partial matrix multiplication is presented. We demonstrate that the added flexibility of this approach can in some cases improve upper bounds on the exponent of matrix multiplication yielded by group-theoretic full matrix multiplication. The group theory behind our partial matrix <b>multiplication</b> <b>algorithms</b> leads to the problem of maximizing a quantity representing the "fullness" of a given partial matrix pattern. This problem is shown to be NP-hard, and two algorithms, one optimal and another non-optimal but polynomial-time, are given for solving it. Comment: 14 pages, 3 figure...|$|R
50|$|Quote notation’s {{arithmetic}} algorithms {{work with}} a typical right-to-left direction, in which the addition, subtraction, and <b>multiplication</b> <b>algorithms</b> have the same complexity for natural numbers, and division is easier than a typical division algorithm.|$|R
40|$|This article address {{efficient}} hardware implementations for multiplication over GF(2 163). Hardware implementations of <b>multiplication</b> <b>algorithms</b> {{are suitable}} for elliptic curve cryptoprocessor designs, which allow that elliptic curve based cryptosystems implemented in hardware provide more physical security and higher speed than software implementations. In this case, the multipliers were designed using conventional, modified and fast <b>multiplication</b> <b>algorithms,</b> the synthesis and simulation were carried out using Quartus II v. 5. 0 of Altera, and the designs were synthesized on the Stratix II EP 2 S 60 F 1020 C 3. The simulation {{results show that the}} multipliers designed present a very good performance using small area...|$|R
2500|$|Schönhage–Strassen {{algorithm}} - asymptotically fast <b>multiplication</b> <b>algorithm</b> {{for large}} integers ...|$|E
2500|$|Another (randomized) {{algorithm}} by Mucha and Sankowski, {{based on}} the fast matrix <b>multiplication</b> <b>algorithm,</b> gives [...] complexity.|$|E
2500|$|With the modulus {{out of the}} way, the {{asymptotic}} {{complexity of}} the algorithm only depends on the <b>multiplication</b> <b>algorithm</b> used to square s at each step. The simple [...] "grade-school" [...] algorithm for multiplication requires O(p2) bit-level or word-level operations to square a p-bit number. Since this happens O(p) times, the total time complexity is O(p3). A more efficient <b>multiplication</b> <b>algorithm</b> is the Schönhage–Strassen algorithm, {{which is based on}} the Fast Fourier transform. It only requires O(p log p log log p) time to square a p-bit number. This reduces the complexity to O(p2 log p log log p) or Õ(p2). from O(p3) to O(p2 log p log log p) bit operations. |doi=10.2307/2008415}} Currently the most efficient known <b>multiplication</b> <b>algorithm,</b> Fürer's algorithm, only needs [...] time to multiply two p-bit numbers.|$|E
40|$|In {{this paper}} we present an {{adaptable}} fast matrix <b>multiplication</b> (AFMM) <b>algorithm,</b> for two nxn dense matrices which computes the product matrix with average complexity Tavg(n) = d 1 d 2 n 3 with the acknowledgement {{that the average}} count is obtained for addition as the basic operation rather than multiplication which is probably the unquestionable choice for basic operation in existing matrix <b>multiplication</b> <b>algorithms.</b> Comment: No of tables is 1. No of diagrams is 0...|$|R
40|$|In this paper, {{we propose}} {{efficient}} new algorithms for multi-dimensional multi-point evaluation and interpolation on certain subsets of so called tensor product grids. These point-sets naturally {{occur in the}} design of efficient <b>multiplication</b> <b>algorithms</b> for finite-dimensional C-algebras of the form A=C[x 1, [...] .,xn]/I, where I is finitely generated by monomials of the form x 1 ^i 1 [...] . xn^in; one particularly important example is the algebra of truncated power series C[x 1, [...] . xn]/(x 1, [...] .,xn) ^d. Similarly to what is known for multi-point evaluation and interpolation in the univariate case, our algorithms have quasi-linear time complexity. As a known consequence, we obtain fast <b>multiplication</b> <b>algorithms</b> for algebras A of the above form...|$|R
50|$|All {{the above}} <b>multiplication</b> <b>algorithms</b> {{can also be}} {{expanded}} to multiply polynomials. For instance the Strassen algorithm {{may be used for}} polynomial multiplicationAlternatively the Kronecker substitution technique may be used to convert the problem of multiplying polynomials into a single binary multiplication.|$|R
2500|$|An {{improvement}} over {{this is the}} Hopcroft–Karp algorithm, which runs in [...] time. An alternative randomized approach {{is based on the}} fast matrix <b>multiplication</b> <b>algorithm</b> and gives [...] complexity, which is better in theory for sufficiently dense graphs, but in practice the algorithm is slower. Finally, for sparse graphs, [...] is possible with Madry's algorithm based on electric flows.|$|E
2500|$|To {{multiply}} {{a pair of}} digits {{using the}} table, find {{the intersection of the}} row of the first digit with the column of the second digit: the row and the column intersect at a square containing the product of the two digits. Most pairs of digits produce two-digit numbers. In the <b>multiplication</b> <b>algorithm</b> the tens-digit of the product of a pair of digits is called the [...] "carry digit".|$|E
2500|$|Because of the {{possibility}} of blockwise inverting a matrix, where an inversion of an [...] matrix requires inversion of two half-sized matrices and six multiplications between two half-sized matrices, and since matrix multiplication has a lower bound of [...] operations, it can be shown that a divide and conquer algorithm that uses blockwise inversion to invert a matrix runs with the same time complexity as the matrix <b>multiplication</b> <b>algorithm</b> that is used internally.|$|E
40|$|The work is {{concerned}} with the power increase methods of the microprocessors. The aim of the work is to investigate the new <b>multiplication</b> <b>algorithms</b> for creation of the high-power small-dimensional devices for digital processing of the signals and control systems and also to synthesize the multifunctional devices. The following methods have been used: methods of algebra, mathematical analysis, Bulean algebra, mathematical modelling, theory of automates, theory of computer design, probability theory and mathematical statistics. A number of the theorems substantiating the <b>multiplication</b> <b>algorithms</b> has been approved, the methods of creating structures and processors on base of the new algorithms have been developed; the algorithmic and hardware methods of accelerating <b>multiplication</b> <b>algorithms</b> have been developed; the one-crystal function processor and device for transfer of the data into the logarithmic system have developed. The criteria of the temporary complexity for multiplication algori thms have been obtained; the acting multifunctional devices for calculation of 14 functions on base of the 1533 and BMK- 15 ChM 1 microcircuits have been made, the one-crystal function processor surpassing the foreign and domestic analogs in their parameters has been developed, the synthesis methods of the units for base operation in the <b>multiplication</b> <b>algorithms</b> have been developed. The basic algorithmic facilities, software and circuit engineering designs have been introduced in the enterprises of the Scientific-Production Association "ORION" and in the State Research Institute of Computer-Aided Systems. The power of the functional transformation processors is increased 2 - 16 times. Application field: computer engineering, digital processing of signals, digital processing of images, navigation, ballistics and other science and engineering fields using functional transformation processorsAvailable from VNTIC / VNTIC - Scientific & Technical Information Centre of RussiaSIGLERURussian Federatio...|$|R
40|$|Matrix {{multiplication}} {{is a key}} primitive {{in block}} matrix algorithms such as those found in LAPACK. We present results from our study of matrix <b>multiplication</b> <b>algorithms</b> on the Intel Touchstone Delta, a distributed memory message-passing architecture with a two-dimensional mesh topology. We obtain an implementation that uses communications primitives highly suited to the Delta and exploits the single node assembly-coded matrix <b>multiplication.</b> Our <b>algorithm</b> is completely general, {{able to deal with}} arbitrary mesh aspect ratios and matrix dimensions, and has achieved parallel efficiency of 86...|$|R
40|$|The RN-codings are {{particular}} cases of signed-digit representations, for which rounding {{to the nearest}} is always identical to truncation. In radix $ 2 $, Booth recoding is an RN-coding. In this paper, we suggest several <b>multiplication</b> <b>algorithms</b> able to handle RN-codings, and we analyze their properties...|$|R
