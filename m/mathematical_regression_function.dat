0|3413|Public
40|$|We {{propose the}} {{generalized}} profiling method {{to estimate the}} multiple <b>regression</b> <b>functions</b> {{in the framework of}} penalized spline smoothing, where the <b>regression</b> <b>functions</b> and the smoothing parameter are estimated in two nested levels of optimization. The corresponding gradients and Hessian matrices are worked out analytically, using the Implicit Function Theorem if necessary, which leads to fast and stable computation. Our main contribution is developing the modified delta method to estimate the variances of the <b>regression</b> <b>functions,</b> which include the uncertainty of the smoothing parameter estimates. We further develop adaptive penalized spline smoothing to estimate spatially heterogeneous <b>regression</b> <b>functions,</b> where the smoothing parameter is a function that changes along with the curvature of <b>regression</b> <b>functions.</b> The simulations and application show that the generalized profiling method leads to good estimates for the <b>regression</b> <b>functions</b> and their variances. ...|$|R
30|$|As stated above, through {{putting the}} feature vector into the <b>regression</b> <b>function,</b> we can {{estimate}} its target value once the <b>regression</b> <b>function</b> is established by SVR. Therefore, the radar returns from diverse walls with different parameters can be collected as the training data, {{and one can}} train a <b>regression</b> <b>function</b> {{for each of the}} wall parameters. Then, when we confront a new wall, its parameters can be estimated by taking its radar return into those corresponding <b>regression</b> <b>functions.</b>|$|R
40|$|For a {{well-known}} class of nonparametric <b>regression</b> <b>function</b> estimators of nearest neighbor type the uniform measure of {{deviation from the}} estimators to the true <b>regression</b> <b>function</b> is studied. Under weak regularity conditions it is shown that the estimators are uniformly consistent with probability one and the corresponding rate of convergence is near-optimal. Strong consistency <b>regression</b> <b>function</b> near neighbour rules...|$|R
40|$|Suppose the nonparametric <b>regression</b> <b>function</b> of a {{response}} variable Y on covariates X and Z is an affine function of X such that the slope β and the intercept α are real valued measurable functions on {{the range of the}} completely arbitrary random element Z. Assume that X has a finite moment of order {{greater than or equal to}} 2, Y has a finite moment of conjugate order, and α(Z) and α(Z) X have finite first moments. Then, the nonparametric <b>regression</b> <b>function</b> equals the least squares linear <b>regression</b> <b>function</b> of Y on X with all the moments that appear in the expression of the linear <b>regression</b> <b>function</b> calculated conditional on Z. Consequently, conditional mean independence implies zero conditional covariance and a degenerate version of the aforesaid affine form for the nonparametric <b>regression</b> <b>function,</b> whereas the aforesaid affine form and zero conditional covariance imply conditional mean independence. Further, it turns out that the nonparametric <b>regression</b> <b>function</b> has the aforesaid affine form if X is Bernoulli, and since 1 is the conjugate exponent of ∞, the least squares linear regression formula for the nonparametric <b>regression</b> <b>function</b> holds when Y has only a finite first moment and Z is completely arbitrary. Comment: Revised and fixed typo...|$|R
40|$|AbstractWe {{show that}} there exist {{individual}} lower bounds corresponding to the upper bounds for the rate of convergence of nonparametric pattern recognition which are arbitrarily close to Yang's minimax lower bounds, for certain “cubic” classes of <b>regression</b> <b>functions</b> used by Stone and others. The rates are equal to the ones of the corresponding <b>regression</b> <b>function</b> estimation problem. Thus for these classes classification is not easier than <b>regression</b> <b>function</b> estimation...|$|R
40|$|In this paper, a {{lower bound}} is {{determined}} in the minimax sense for change point estimators {{of the first}} derivative of a <b>regression</b> <b>function</b> in the fractional white noise model. Similar minimax results presented previously in the area focus on change points in the derivatives of a <b>regression</b> <b>function</b> in the white noise model or consider estimation of the <b>regression</b> <b>function</b> {{in the presence of}} correlated errors. 5 page(s...|$|R
40|$|In {{this work}} {{we present a}} simple {{estimator}} for the cumulative <b>regression</b> <b>function</b> that involves no smoothing and we give its asymptotic distribution. By comparing cumulatives a test for comparison of nonparametric <b>regression</b> <b>functions</b> is proposed. The proposed test does not involve a smoothing parameter and has good power against alternatives that are uniformly ordered. The test-statistic works for both fixed and random designs. Cumulative <b>regression</b> <b>function</b> Nonlinear <b>regression</b> Nonparametric test...|$|R
40|$|Various {{parametric}} and nonparametric regression {{procedures have}} been constructed according to different possible characteristics of the underlying <b>regression</b> <b>function.</b> To reduce the dependence on subjective assumptions, the theme of adaptive estimation is to construct a procedure that provides an accurate estimate of the <b>regression</b> <b>function</b> for various scenarios without knowing which one describes the data well. A closely related question is: Given a regression procedure, how many <b>regression</b> <b>functions</b> are estimated accurately? In this work, for a given sequence of prescribed estimation accuracy (in sample size), we give an upper bound (in terms of metric entropy) {{on the number of}} <b>regression</b> <b>functions</b> for which the accuracy is achieved. A consequence is that if one demands near optimal performance for a target class of <b>regression</b> <b>functions,</b> then the same accuracy can not be achieved for many additional <b>regression</b> <b>functions.</b> This has a negative implication on adaptive estimation. The main result is also applied to show that as far as polynomial rates of convergence are concerned, any regression procedure is essentially no better than a method based on sparse approximation...|$|R
40|$|Consider the nonparametric {{estimation}} of a multivariate <b>regression</b> <b>function</b> and its derivatives for a regression model with long-range dependent errors. We adopt local linear #tting approach and establish the joint asymptotic distributions for the estimators of the <b>regression</b> <b>function</b> and its derivatives. The nature of asymptotic distributions {{depends on the}} amount of smoothing resulting in possibly non-Gaussian distributions for large bandwidth and Gaussian distributions for small bandwidth. It turns out that the condition determining this dichotomy is di#erent for the estimates of the <b>regression</b> <b>function</b> than for its derivatives; this leads to a double bandwidth dichotomy whereas the asymptotic distribution for the <b>regression</b> <b>function</b> estimate can be non-Gaussian whereas those of the derivatives estimates are Gaussian. Asymptotic distributions of estimates of derivatives in the case of large bandwidth are the scaled version of that for estimates of the <b>regression</b> <b>function,</b> resembling [...] ...|$|R
40|$|Discrete {{versions}} of the mean integrated squared error (MISE) provide stochastic measures of accuracy to compare different estimators of regression fuctions. These measures of accuracy {{have been used in}} Monte Carlo trials and have been employed for the optimal bandwidth selection for kernel <b>regression</b> <b>function</b> estimators, as shown in [5], Optimal Bandwidth Selection in Nonparametric <b>Regression</b> <b>Function</b> Estimation. Inst. of Statistics Mimeo Series No. 1530, Univ. of North Carolina, Chapel Hill). In the present paper it is shown that these stochastic measures of accuracy converge to a weighted version of the MISE of kernel <b>regression</b> <b>function</b> estimators, extending a result of [6], Biometrika 69, 383 - 390) and Marron (1983, J. Multivariate Anal. 18, No. 2) to <b>regression</b> <b>function</b> estimation. stochastic measure of accuracy nonparametric <b>regression</b> <b>function</b> estimation optimal bandwidth selection limit theorems mean square error...|$|R
40|$|Most common {{parametric}} {{families of}} copulas are totally ordered, {{and in many}} cases they are also positively or negatively regression dependent and therefore they lead to monotone <b>regression</b> <b>functions,</b> which makes them not suitable for dependence relationships that imply or suggest a non-monotone <b>regression</b> <b>function.</b> A gluing copula approach is proposed to decompose the underlying copula into totally ordered copulas that combined may lead to a non-monotone <b>regression</b> <b>function.</b> Comment: 12 pages, 4 figure...|$|R
40|$|Background {{subtraction}} {{is important}} for many vision applications. Existing techniques can adapt to gradual changes in illumination but fail to cope with sudden changes often seen in indoor environment. In this paper, we propose a novel background subtraction technique that models the change of illumination as a <b>regression</b> <b>function</b> of spatial image coordinates. Such spatial dependency is significant when light sources are close to or within the scene. The <b>regression</b> <b>function</b> is learned from highly probable back-ground regions and applied {{to the rest of}} the background models to compensate for the illumination change. While a single <b>regression</b> <b>function</b> is adequate for a smooth Lam-bertian surface, multiple <b>regression</b> <b>functions</b> are needed to handle depth discontinuities, shadows, and non-Lambertian surfaces. The change of illumination is first segmented and different <b>regression</b> <b>functions</b> are applied to different seg-ments. Experimental results comparing our techniques with other schemes show better foreground segmentation during illumination change. 1...|$|R
40|$|We propose {{several new}} tests for {{monotonicity}} of <b>regression</b> <b>functions</b> based on different empirical processes of residuals. The residuals are obtained from recently developed simple kernel based estimators for increasing <b>regression</b> <b>functions</b> based on increasing rearrangements of unconstrained nonparametric estimators. The test statistics are estimated distance measures between the <b>regression</b> <b>function</b> and its increasing rearrangement. We discuss the asymptotic distributions, consistency, and small sample {{performances of the}} tests. AMS Classification: 62 G 10, 62 G 08, 62 G 3...|$|R
40|$|Abstract. In this paper, {{we explain}} how works a nonparametric {{algorithm}} for estimating a <b>regression</b> <b>function</b> when {{the response is}} censored. The strategy {{is based on an}} adequate transformation of the data in order to take the censoring into account and on a standard mean-square contrast for the estimation of the <b>regression</b> <b>function.</b> We illustrate the method through several empirical experiments, in particular in the bivariate setting of an additive <b>regression</b> <b>function</b> and also on real data sets. 1...|$|R
40|$|In {{problems}} {{dealing with}} <b>regression</b> <b>functions</b> {{the choice of}} model and estimation method {{is due to a}} priori information about the <b>regression</b> <b>function.</b> In some situations it is motivated to consider <b>regression</b> <b>functions</b> with specific non-parametric characteristics, for instance monotonicity and/or concaVity/convexity. In situations when we only have one y-observation for each Xi we propose two new variance approximation methods, one for curves that fulfil monotonicity restrictions and one for curves that fulfil concavity/ convexity restrictions...|$|R
40|$|The nonparametric {{estimation}} of a <b>regression</b> <b>function</b> from conditional moment restrictions involving instrumental variables is considered. The {{rate of convergence}} of penalized estimators is studied in the case where the <b>regression</b> <b>function</b> is not identified from the conditional moment restriction. We also study the gain of modifying the penalty in the estimation, considering derivatives in penalty. We analyze {{the effect of this}} modification on the identification of the <b>regression</b> <b>function</b> and the rate of convergence of its estimator...|$|R
5000|$|... {{is called}} the {{smoothing}} parameter because it controls {{the flexibility of the}} LOESS <b>regression</b> <b>function.</b> Large values of [...] produce the smoothest functions that wiggle the least in response to fluctuations in the data. The smaller [...] is, the closer the <b>regression</b> <b>function</b> will conform to the data. Using too small a value of the smoothing parameter is not desirable, however, since the <b>regression</b> <b>function</b> will eventually start to capture the random error in the data.|$|R
40|$|AbstractConsider the nonparametric {{estimation}} of a multivariate <b>regression</b> <b>function</b> and its derivatives for a regression model with long-range dependent errors. We adopt local linear fitting approach and establish the joint asymptotic distributions for the estimators of the <b>regression</b> <b>function</b> and its derivatives. The nature of asymptotic distributions {{depends on the}} amount of smoothing resulting in possibly non-Gaussian distributions for large bandwidth and Gaussian distributions for small bandwidth. It turns out that the condition determining this dichotomy is different for the estimates of the <b>regression</b> <b>function</b> than for its derivatives; this leads to a double bandwidth dichotomy whereas the asymptotic distribution for the <b>regression</b> <b>function</b> estimate can be non-Gaussian whereas those of the derivatives estimates are Gaussian. Asymptotic distributions of estimates of derivatives in the case of large bandwidth are the scaled version of that for estimates of the <b>regression</b> <b>function,</b> resembling the situation of {{estimation of}} cumulative distribution function and densities under long-range dependence. The borderline case between small and large bandwidths is also examined...|$|R
40|$|The authors {{consider}} the M-type estimators of <b>regression</b> <b>function</b> and show their almost-sure consistency under some easier to check {{conditions on the}} nonparametric family of <b>regression</b> <b>functions.</b> The results {{can be used to}} treat some parametric, nonparametric and semiparametric models. © 1995. link_to_subscribed_fulltex...|$|R
40|$|The {{non-reversibility}} of the <b>regression</b> <b>functions,</b> {{the simultaneous}} {{relations between the}} economic phenomena and the so-called paired <b>regression</b> <b>functions</b> are treated {{on the basis of}} cybernetic considerations. It is shown, that the treatment of this type of <b>regression</b> <b>functions</b> requires the existence of simultaneous relations and multicollinearity, which leads, in turn, to based results, when estimating them seperately {{on the basis of the}} classical Least-Squares-Method. Digitalizacja i deponowanie archiwalnych zeszytów RPEiS sfinansowane przez MNiSW w ramach realizacji umowy nr 541 /P-DUN/ 201...|$|R
40|$|For {{estimating}} <b>regression</b> <b>function</b> {{we can use}} many proceedings. In this paper, we {{have chosen}} to apply scaling functions to the estimation of <b>regression</b> <b>functions.</b> When one knows many bivariate date with the values of two variables, in the goal to express a correlation between the two variables we use the <b>regression</b> <b>function.</b> The raw estimator of this function must be "smoothed out" {{in some way to}} get a final estimator. For this, we use the scaling functions, examples of such function being the Battle-Lemarié family and Daubechies family. After introducing several notions (multiresolution analysis, filter and projection of function onto approximation spaces), these are applied to obtain the estimators. In the last part, we present the algorithm for estimating nonparametric <b>regression</b> <b>function</b> through the scaling <b>functions.</b> nonparametric <b>regression,</b> scaling <b>functions,</b> filter, multiresolution analysis, ap-proximation space, estimator...|$|R
40|$|We use the sinc kernel to {{construct}} an estimator for the integrated squared <b>regression</b> <b>function.</b> Asymptotic normality of the estimator at different rates is established, {{depending on whether the}} <b>regression</b> <b>function</b> vanishes or not. Key words: asymptotic normality; nonparametric regression; quadratic regression functional; sinc kerne...|$|R
40|$|For a nonparametric {{regression}} {{problem with}} errors in variables, we consider a shape-restricted <b>regression</b> <b>function</b> estimate, {{which does not}} require the choice of bandwidth parameters. We demonstrate that this estimate is consistent for classes of <b>regression</b> <b>function</b> candidates, which are closed under the graph topology...|$|R
40|$|In this paper, {{we study}} nonparametric models {{allowing}} for locally stationary regressors and a <b>regression</b> <b>function</b> that changes smoothly over time. These models are a natural {{extension of time}} series models with time-varying coefficients. We introduce a kernel-based method to estimate the time-varying <b>regression</b> <b>function</b> and provide asymptotic theory for our estimates. Moreover, we show that the main conditions of the theory are satis ed for a large class of nonlinear autoregressive processes with a time-varying <b>regression</b> <b>function.</b> Finally, we examine structured models where the <b>regression</b> <b>function</b> splits up into time-varying additive components. As will be seen, estimation in these models does not su er from the curse of dimensionality. We complement the technical analysis of the paper by an application to financial data...|$|R
3000|$|The <b>mathematical</b> <b>regression</b> {{equations}} of the {{quadratic polynomial}} model, which represent {{the relations between}} the removal of Congo red dye (%) and the operating parameters, were obtained from Design Expert Software (Version 6.0. 5) and expressed for coded units as follows: [...]...|$|R
40|$|Upper bounds {{are derived}} for {{the rates of}} {{convergence}} for trigonometric series regression estimators of an unknown, smooth <b>regression</b> <b>function.</b> The resulting rates depend on the <b>regression</b> <b>function</b> satisfying certain periodic boundary conditions that may not hold in practice. To overcome such difficulties alternative estimators are proposed which are obtained by <b>regression</b> on trigonometric <b>functions</b> and low-order polynomials. These estimators are shown to always be capable of obtaining the optimal rates of convergence over a particular smoothness class of functions, irregardless {{of whether or not}} the <b>regression</b> <b>function</b> is periodic. Guaranteed rates mean squared error nonparametric regression orthogonal series...|$|R
40|$|A {{new test}} for strict {{monotonicity}} of the <b>regression</b> <b>function</b> is proposed {{which is based}} on a composition of an estimate of the inverse of the <b>regression</b> <b>function</b> with a common regression estimate. This composition is equal to the identity if and only if the ?true? <b>regression</b> <b>function</b> is strictly monotone, and a test based on an L 2 -distance is investigated. The asymptotic normality of the corresponding test statistic is established under the null hypothesis of strict monotonicity. [...] nonparametric regression,strictly monotone regression,goodness-of-fit test...|$|R
30|$|The {{proposed}} {{approach is}} comprised of three stages. In the first stage, the feature vectors are redesigned and extracted in a different way, and then three <b>regression</b> <b>functions</b> are trained. The estimation process is implemented {{in the last two}} stages. In the former, the permittivity of the wall is estimated by the <b>regression</b> <b>function,</b> and in the latter, the estimate of the thickness is obtained by minimizing a predefined cost function in which the estimated permittivity and the other two <b>regression</b> <b>functions</b> are involved.|$|R
40|$|A precise {{determination}} of tolerance bands of S-N curves is quite difficult problem; therefore simple approximate constructions of these bands are often used. Castillo et al. suggested a sophisticated {{and at the}} same time easy procedure for their estimation which, however, needs a special type of <b>regression</b> <b>function</b> for describing S-N curve. This type of <b>regression</b> <b>function</b> called the Castillo function is suitable only for high-cycle fatigue region close to fatigue limit. The paper focuses on other <b>regression</b> <b>functions</b> used in the Castillo procedure, namely on the Kohout and Vechet function and the Stromeyer function...|$|R
40|$|A new nonparametric {{estimate}} of a convex <b>regression</b> <b>function</b> is proposed and its stochastic properties are studied. The method starts with an unconstrained {{estimate of}} the derivative of the <b>regression</b> <b>function,</b> which is firstly isotonized and then integrated. We prove asymptotic normality of the new estimate and show that it is first order asymptotically equivalent to the initial unconstrained estimate if the <b>regression</b> <b>function</b> is in fact convex. If convexity is not present the method estimates a convex function whose derivative has the same L p-norm as the derivative of the (non-convex) underlying <b>regression</b> <b>function.</b> The finite sample properties of the new estimate are investigated {{by means of a}} simulation study and the application of the new method is demonstrated in two data examples. AMS Subject classification: 62 G 05, 62 G 0...|$|R
25|$|SPSS: Included as {{an option}} in the <b>Regression</b> <b>function.</b>|$|R
40|$|We {{consider}} {{the problem of}} estimating a <b>regression</b> <b>function</b> with nonrandom design points and dependent errors. We construct a spline estimate of the <b>regression</b> <b>function</b> and obtain its rate of convergence. It {{turns out that the}} dependence of the observations is reflected in this rate. B-splines mixing rate of convergence...|$|R
40|$|AbstractFor a {{well-known}} class of nonparametric <b>regression</b> <b>function</b> estimators of nearest neighbor type the uniform measure of {{deviation from the}} estimators to the true <b>regression</b> <b>function</b> is studied. Under weak regularity conditions it is shown that the estimators are uniformly consistent with probability one and the corresponding rate of convergence is near-optimal...|$|R
40|$|We {{give the}} uniform almost sure {{convergence}} of the kernel estimate of the <b>regression</b> <b>function</b> over a sequence of compact sets which increases to dRI when n approaches the infinity and the observed process is ~-mixing. The used estimator for the <b>regression</b> <b>function</b> is the kernel estimator proposed by Nadaraya, Watson (1964) ...|$|R
40|$|Recent {{development}} of high-throughput analytical techniques {{has made it}} possible to qualitatively identify a number of metabolites simultaneously. Correlation and multivariate analyses such as principal component analysis have been widely used to analyse those data and evaluate correlations among the metabolic profiles. However, these analyses cannot simultaneously carry out identification of metabolic reaction networks and prediction of dynamic behaviour of metabolites in the networks. The present study, therefore, proposes a new approach consisting of a combination of statistical technique and mathematical modelling approach to identify and predict a probable metabolic reaction network from time-series data of metabolite concentrations and simultaneously construct its <b>mathematical</b> model. Firstly, <b>regression</b> <b>functions</b> are fitted to experimental data by the locally estimated scatter plot smoothing method. Secondly, the fitted result is analysed by the bivariate Granger causality test to determine which metabolites cause the change in other metabolite concentrations and remove less related metabolites. Thirdly, S-system equations are formed by using the remaining metabolites within the framework of biochemical systems theory. Finally, parameters including rate constants and kinetic orders are estimated by the Levenberg-Marquardt algorithm. The estimation is iterated by setting insignificant kinetic orders at zero, i. e., removing insignificant metabolites. Consequently, a reaction network structure is identified and its mathematical model is obtained. Our approach is validated using a generic inhibition and activation model and its practical application is tested using a simplified model of the glycolysis of Lactococcus lactis MG 1363, for which actual time-series data of metabolite concentrations are available. The results indicate the usefulness of our approach and suggest a probable pathway for the production of lactate and acetate. The results also indicate that the approach pinpoints a probable strong inhibition of lactate on the glycolysis pathway...|$|R
40|$|This {{article is}} {{concerned}} with the estimation of the integral of a squared <b>regression</b> <b>function</b> using Latin hypercube sampling. A class of generalized nearest-neighbour estimators is proposed and their properties are investigated with respect to various smoothness classes of <b>regression</b> <b>functions.</b> In particular, mild conditions are established which ensure that achieves a root-n convergence rate. It is further shown that has an asymptotic mean squared error smaller than that of any regular estimator based on an i. i. d. sample of the same size. Integrated squared <b>regression</b> <b>function</b> Latin hypercube sampling Nearest neighbour estimator Nonparametric information bound Rate of convergence...|$|R
