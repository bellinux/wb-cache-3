33|6|Public
25|$|Historically, the Gravity Pipe (GRAPE) {{system for}} {{astrophysics}} at the University of Tokyo was distinguished not by its {{top speed of}} 64 Tflops, but by its cost and energy efficiency, having won the Gordon Bell Prize in 1999, at about $7 per <b>megaflops,</b> using special purpose processing elements.|$|E
50|$|IBM Roadrunner {{was shut}} down on March 31, 2013. While the {{supercomputer}} {{was one of the}} fastest in the world, its energy efficiency was relatively low. Roadrunner delivered 444 <b>megaflops</b> per watt vs the 886 <b>megaflops</b> per watt of a comparable supercomputer. Before the supercomputer is dismantled, researchers will spend one month performing memory and data routing experiments that will aid in designing future supercomputers.|$|E
5000|$|The LNS-based GRAPE-5 {{architecture}} won the Price Performance {{category of}} the Gordon Bell Prize in 1999, at about $7 per <b>MegaFLOPS.</b> This category measures the price efficiency of a particular machine {{in terms of the}} price in dollars per <b>megaFLOPS.</b> The particular implementation [...] "Grape-6" [...] also won prizes in 2000 and 2001 (see external links).Grape-DR was ranked first in the June 2010 Little Green500 List, a ranking of supercomputer's performance per unit power consumption published by the Green500.org.|$|E
50|$|Alliant was founded, as Dataflow Systems, in May 1982 by Ron Gruner, Craig Mundie and Rich McAndrew {{to produce}} {{machines}} for scientific and engineering users who needed smaller, less costly machines than offerings from Cray Computer and similar high-end vendors. Machines that addressed this market segment later {{became known as}} minisupercomputers. At the {{time there was a}} huge gap on the price/performance curve as a highly configured VAX 11/780 had a performance of about a MIP and <b>MegaFLOP</b> for around $1M USD and a Cray-1S or Cray 1M over $10M USD.|$|R
500|$|While chairman, Jobs visited {{university}} {{departments and}} faculty members to sell Macintosh. Jobs met Paul Berg, a Nobel Laureate in chemistry, at a luncheon held in Silicon Valley to honor François Mitterrand, then President of France. Berg was frustrated by the expense of teaching students about recombinant DNA from textbooks instead of in wet laboratories, used for the testing and analysis of chemicals, drugs, and other materials or biological matter. Wet labs were prohibitively expensive for lower-level courses and were too complex to be simulated on personal computers of the time. Berg suggested to Jobs to use his influence at Apple to create a [...] "3M computer" [...] workstation for higher education, featuring more than one megabyte of random-access memory (RAM), a megapixel display and <b>megaFLOP</b> performance, hence the name [...] "3M".|$|R
40|$|Matrix {{multiplication}} may {{be considered}} as a model problem for analyzing the performance of more complex algorithms. On Cray and IBM computer systems, there are library routines which for this task operate at high <b>megaflop</b> rates. Other programs from numerical linear algebra do not always achieve this level of sophistication; e. g., they suffer from performance degradation caused by memory access conflicts. This effect has been studied considering the performance of subroutines for matrix multiplication on Cray X-MP, Cray Y-MP, and IBM 3090. Results are analyzed by means of simulation. It is shown that, on a Cray, a degradation of performance by bank conflicts may be reduced if the stride of references to memory is odd. It is demonstrated that a more elaborate approach is required for the IBM 3090 computer system with a more complex storage hierarchy...|$|R
50|$|The bay's many basins {{that are}} broken up by banks serve as {{plentiful}} fishing grounds for snook (Centropomus undecimalis), redfish (Sciaenops ocellatus), spotted seatrout (Cynoscion nebulosus), tarpon (<b>Megaflops</b> atlanticus), bonefish (Albula vulpes), and permit (Trichinous falcatus), among others.|$|E
50|$|Processor {{cycle time}} was 167 nanoseconds, giving {{a speed of}} 6 MHz. Since it could present two {{floating}} point results per cycle, one from the adder and the other from the multiplier, a capacity of 12 <b>Megaflops</b> was claimed for the processor.|$|E
5000|$|The Y-MP {{could be}} {{equipped}} with two, four or eight vector processors, with two functional units each and a clock cycle time of 6 ns (167 MHz). Peak performance was thus 333 <b>megaflops</b> per processor. Main memory comprised 128, 256 or 512 MB of SRAM.|$|E
40|$|This article {{describes}} the design rationale, a C implementation, and conformance testing of {{a subset of the}} new Standard for the BLAS (Basic Linear Algebra Subroutines) : Extended and Mixed Precision BLAS. Permitting higher internal precision and mixed input/output types and precisions allows us to implement some algorithms that are simpler, more accurate, and sometimes faster than possible without these features. The new BLAS are challenging to implement and test because there are many more subroutines than in the existing Standard, and because we must be able to assess whether a higher precision is used for internal computations than is used for either input or output variables. We have therefore developed an automated process of generating and systematically testing these routines. Our methodology is applicable to languages besides C. In particular, our algorithms used in the testing code will be valuable to all other BLAS implementors. Our extra precision routines achieve excellent performance [...] close to half of the machine peak <b>Megaflop</b> rate even for the Level 2 BLAS, when the data access is stride one...|$|R
40|$|This paper desc ibes {{the design}} rationale, a C implementation, onformanc testing of {{a subset of}} the new Standard for the BLAS (Basic Linear Algebra Subroutines) : Extended and Mixed Prec 0 fifi n BLAS. Permitting higher {{internal}} precNNfi n and mixed input/output types and prec 85 fiM 1 permits us to implement some algorithms that are simpler, moreaceM 08 N 3 and sometimes faster than possible without these features. The new BLAS hallenging to implement and test bec use there are many more subroutines than in the existing Standard, and bec use we must be able to assess whether a higher prec 8 =N n is used for omputations than is used either for input or output variables. So we have developed an automated proc 0 N of generating lly testing these routines. Our methodology is applic ble to languages besides C. In ular, our algorithms used in the ode would be very valuable to all the other BLAS implementors. Our extra precG=fiM routinesac excGfiq 3 performancM 18 = 08 to half of the hine peak <b>Megaflop</b> rate even for the Level 2 BLAS, when the dataacaM 0 is stride one. # This research was suj orted in part by the National Science Foufi) tion Cooperative Agreement No. ACI 9619020, NSF Grant No. ACI- 9813362, the Department of Energy Grant Nos. DE-FG 03 - 94 ER 25219 and DEFC 03 - 98 ER 25351, and gifts from the IBM Shared University Research Program, Su n Microsystems, and Intel. This project alsou tilizedresou rces of the National Energy Research Scientific ting Center (NERSC) which is su pported by the Director, O#ce of Advanced Scientific ting Research, Division of Mathematical, Information, Compu 1 BjPzPfl Sciences of the U. S. Department of Energyuerg contract nu ber DE-AC 03 - 76 SF 00098. The information presented here does not necessarily reflect the position or the policy of the Government and no o#cial e [...] ...|$|R
50|$|Historically, the Gravity Pipe (GRAPE) {{system for}} {{astrophysics}} at the University of Tokyo was distinguished not by its {{top speed of}} 64 Tflops, but by its cost and energy efficiency, having won the Gordon Bell Prize in 1999, at about $7 per <b>megaflops,</b> using special purpose processing elements.|$|E
5000|$|The {{original}} NeXT Computer {{was introduced}} in 1988 as a 3M machine by Steve Jobs, who first heard this term at Brown University. Its so-called [...] "MegaPixel" [...] display had just over 930,000 pixels with four shades of gray. However, floating point performance, powered with the Motorola 68882 FPU was only about [...]25 <b>megaflops.</b>|$|E
50|$|The CDC 6600 was the {{flagship}} mainframe supercomputer of the 6000 series of computer systems manufactured by Control Data Corporation.Generally {{considered to be}} the first successful supercomputer, it outperformed its fastest predecessor, the IBM 7030 Stretch, by a factor of three. With performance of up to three <b>megaFLOPS,</b> the CDC 6600 was the world's fastest computer from 1964 to 1969, when it relinquished that status to its successor, the CDC 7600.|$|E
50|$|Each iWarp CPU {{included}} a 32-bit ALU with a 64-bit FPU running at 20 MHz. It was purely scalar and completed one instruction per cycle, so the performance was 20 MIPS or 20 <b>megaflops</b> for single precision and 10 MFLOPS for double. The communications were handled by a separate unit on the CPU that drove four serial channels at 40 MB/s, and included networking support in hardware that allowed {{for up to}} 20 virtual channels (similar to the system added to the INMOS T9000).|$|E
50|$|In the US, {{a series}} of {{computers}} at Control Data Corporation (CDC) were designed by Seymour Cray to use innovative designs and parallelism to achieve superior computational peak performance. The CDC 6600, released in 1964, is generally considered the first supercomputer. The CDC 6600 outperformed its predecessor, the IBM 7030 Stretch, by about a factor of three. With performance of about 1 <b>megaFLOPS,</b> the CDC 6600 was the world's fastest computer from 1964 to 1969, when it relinquished that status to its successor, the CDC 7600.|$|E
5000|$|In 1992, Cray {{launched}} the cheaper Y-MP EL (Entry Level) model. This was a reimplementation of the Y-MP architecture in CMOS technology, {{based on the}} S-2 design acquired by Cray from Supertek Computers in 1990. The EL was an air-cooled system with a completely different VMEbus-based IOS. EL configurations with up to four processors (each with a peak performance of 133 <b>megaflops)</b> and 32 MB to 1 GB of DRAM were available. The Y-MP EL was later developed into the Cray EL90 series (EL92, EL94 and EL98).|$|E
5000|$|The Cray Y-MP, also {{designed}} by Steve Chen, {{was released in}} 1988 as an improvement of the X-MP and could have eight vector processors at 167 MHz with a peak performance of 333 <b>megaflops</b> per processor. [...] In the late 1980s, Cray's experiment {{on the use of}} gallium arsenide semiconductors in the Cray-3 did not succeed. Cray began to work on a massively parallel computer in the early 1990s, but died in a car accident in 1996 before it could be completed. Cray Research did, however, produce such computers.|$|E
50|$|The CS-2 was an all-new modular {{architecture}} based around SuperSPARC or hyperSPARC processors and, optionally, Fujitsu μVP vector processors. These implemented an instruction set {{similar to the}} Fujitsu VP2000 vector supercomputer and had a nominal performance of 200 <b>megaflops</b> on double precision arithmetic and double that on single precision. The SuperSPARC processors ran at 40 MHz initially, later increased to 50 MHz. Subsequently, hyperSPARC processors were introduced at 66, 90 or 100 MHz. The CS-2 was intended to scale up to 1024 processors. The largest CS-2 system built was a 224-processor system installed at Lawrence Livermore National Laboratory.|$|E
50|$|In November 2008, {{it reached}} a top {{performance}} of 1.456 petaflops, retaining its top {{spot in the}} TOP500 list. It was also the fourth-most energy-efficient supercomputer in {{the world on the}} Supermicro Green500 list, with an operational rate of 444.94 <b>megaflops</b> per watt of power used. The hybrid Roadrunner design was then reused for several other energy efficient supercomputers. Roadrunner was decommissioned by Los Alamos on March 31, 2013. In its place, Los Alamos uses a supercomputer called Cielo, which was installed in 2010. Cielo is smaller and more energy efficient than Roadrunner, and cost $54 million.|$|E
5000|$|For {{the second}} series the naming was changed, and they created the single-chip nCUBE 2 processor. This was {{otherwise}} {{similar to the}} nCUBE 10's CPU, but ran faster at 25 MHz to provide about 7 MIPS and 3.5 <b>megaFLOPS.</b> This was later improved to 30 MHz in the 2S model. RAM was increased as well, with 4 to 16 MB of RAM on a [...] "single wide" [...] 1 inch x 3.5 inch module, with additional form factors of [...] "double wide" [...] (double modules), and quadruple that in a double wide, double side module. The I/O cards generally had less RAM, with different backend interfaces to support SCSI, HIPPI and other protocols.|$|E
50|$|Companies like Hauppauge and Microway {{that were}} {{impacted}} by their new competitor that made their living accelerating floating point applications being run on PCs followed suit by venturing into the Intel i-860 vector coprocessor business: Hauppauge {{came out with}} an Intel 80486 motherboard that included an Intel i-860 vector processor while Microway came out with add-in cards that had between one or more i-860s. These products along with Transputer-based add-in cards would eventually lead into {{what became known as}} HPC (high performance computing). HPC was actually initiated in 1986 by an English company, Inmos, that designed a CPU competitive with an Intel 80386/387 that also included four twisted pair high speed interconnects that could communicate with other Transputers and be linked to a PC motherboard making it possible to create distributed memory processing computers that could employ 32 processors with the same throughput as 32 Intel 386/387s operating in a single PC. The add in card parallel processing business morphed from the Transputer to the Intel i-860 around 1989 when Inmos was purchased by STmicroelectronics that cut R&D funding eventually forcing companies that had entered the parallel processing business to shift to the Intel i860. The i-860 was a vector processor with graphics extensions that could initially provide 50 <b>Megaflops</b> of throughput in an era when an 80486 with an Intel 80487 peaked at half a Megaflop and would eventually top out at 100 <b>Megaflops</b> making it as fast as 100 Inmos T414 Transputers. i-860 Add in cards made it possible for as many as 20 Intel i-860s to run in parallel and could be programmed using a software library similar to today's MPI libraries which today support distributed memory parallel processing in which servers sitting in 1U rack mount chassis that are essentially PCs provide the horsepower behind the majority of the world's Supercomputers. This same approach could be employed using Hauppauge's motherboards connected by Gigabit Ethernet, something that was however first demonstrated using a wall of IBM RS/6000 PCs at the 1991 Supercomputing Conference. IBM's lead was quickly followed by academic users who realized they could {{do the same thing with}} much less expensive hardware by adapting their x86 PCs to run in parallel at first using a software library adapted from similar Transputer libraries called PVM (parallel virtual machines) that would eventually morph into today's MPI. Products like the Intel i860 vector processor that could be employed both as a vector and graphics processor were end of life'd around 1993 at the same time that Intel introduced the Intel Pentium P5: a CISC processor that used CISC instructions that were pipelined into hard coded lower level RISC like primitives that provided the Pentium with a Superscalar architecture that also could execute the x87 FPU instruction set using a built in FPU that was essentially implemented using the scalar instructions of the i-860 as well as a memory bus that provided a 400 MB/sec interface to memory that was borrowed from the i-860 as well. This high speed bus played a crucial role in speeding up the most common floating point intensive applications that at this point in time used Gauss Elimination to solve simultaneous linear equations buy which today are solved using blocking and LU decomposition. The Intel Pentium while good, did not provide enough floating point performance to compete with a 300 MHz 21164 DEC Alpha that provided 600 <b>Megaflops</b> in 1995. At the same point in time Intel Supercomputing had moved from the 50 MHz Intel i-860XP that was six times slower than the DEC 21164 to the special version of their Pentium that at 200 <b>Megaflops</b> was only three times slower than the 21164. However, the impending speed upgrade of the Alpha to 600 MHz ultimately doomed the future of Intel Supercomputing.|$|E
50|$|The {{powerful}} supercomputers {{of the era}} were at {{the other}} end of the computing spectrum from the microcomputers, and they also used integrated circuit technology. In 1976, the Cray-1 was developed by Seymour Cray, who had left Control Data in 1972 to form his own company. This machine was the first supercomputer to make vector processing practical. It had a characteristic horseshoe shape to speed processing by shortening circuit paths. Vector processing uses one instruction to perform the same operation on many arguments; it has been a fundamental supercomputer processing method ever since. The Cray-1 could calculate 150 million floating point operations per second (150 <b>megaflops).</b> 85 were shipped at a price of $5 million each. The Cray-1 had a CPU that was mostly constructed of SSI and MSI ECL ICs.|$|E
50|$|FLOPS and MIPS are {{units of}} measure for the {{numerical}} computing {{performance of a}} computer. Floating-point operations are typically used in fields such as scientific computational research. The unit MIPS measures integer performance of a computer. Examples of integer operation include data movement (A to B) or value testing (If A = B, then C). MIPS as a performance benchmark is adequate when a computer is used in database queries, word processing, spreadsheets, or to run multiple virtual operating systems. Frank H. McMahon, of the Lawrence Livermore National Laboratory, invented the terms FLOPS and MFLOPS (<b>megaFLOPS)</b> {{so that he could}} compare the so-called supercomputers of the day by the number of floating-point calculations they performed per second. This was much better than using the prevalent MIPS to compare computers as this statistic usually had little bearing on the arithmetic capability of the machine.|$|E
50|$|In 2012, the Yellowstone {{supercomputer}} {{was installed}} in the NWSC as its inaugural HPC resource. Yellowstone is an IBM iDataPlex cluster consisted of 72,288 Intel Sandy Bridge EP processor cores in 4,518 16-core nodes, each with 32 gigabytes of memory. All nodes are interconnected with a full fat tree Mellanox FDR InfiniBand network. Yellowstone has a peak performance of 1.504 petaflops and has demonstrated a computational capability of 1.2576 petaflops {{as measured by the}} High-Performance LINPACK (HPL) benchmark. It debuted as the world’s 13th fastest computer in the November 2012 ranking by the TOP500 organization. Also in November 2012, Yellowstone debuted as the 58th most energy efficient supercomputer in the world by operating at 875.34 <b>megaflops</b> per watt as ranked by the Green500 organization. Yellowstone is expected to remain in production operation through the end of 2017.|$|E
5000|$|In 1968, Northwestern University {{students}} Larry Atkin, David Slate and Keith Gorlen {{began work}} on Chess (Northwestern University).On July 25, 1976, Chess 4.5 scored 5-0 in the Class B (1600-1799) section of the 4th Paul Masson chess tournament in Saratoga, California. This {{was the first time}} a computer won a human tournament. Chess 4.5 was rated 1722. Chess 4.5 running on a Control Data Corporation CDC Cyber 175 supercomputer (2.1 <b>megaflops)</b> looked at less than 1500 positions per second.On February 20, 1977, Chess 4.5 won the 84th Minnesota Open Championship with 5 wins and 1 loss. It defeated expert Charles Fenner rated 2016.On April 30, 1978, Chess 4.6 scored 5-0 at the Twin Cities Open in Minneapolis. Chess 4.6 was rated 2040. International Master Edward Lasker stated that year, [...] "My contention that computers cannot play like a master, I retract. They play absolutely alarmingly. I know, because I have lost games to 4.7." ...|$|E
5000|$|The 6600 gained speed by [...] "farming out" [...] work to {{peripheral}} computing elements, {{freeing the}} CPU (Central Processing Unit) to process actual data. The Minnesota FORTRAN compiler for {{the machine was}} developed by Liddiard and Mundstock at the University of Minnesota {{and with it the}} 6600 could sustain 500 kiloflops on standard mathematical operations. In 1968 Cray completed the CDC 7600, again the fastest computer in the world. At 36 MHz, the 7600 had about three and a half times the clock speed of the 6600, but ran significantly faster due to other technical innovations. They sold only about 50 of the 7600s, not quite a failure. Cray left CDC in 1972 to form his own company. Two years after his departure CDC delivered the STAR-100 which at 100 <b>megaflops</b> was three times the speed of the 7600. Along with the Texas Instruments ASC, the STAR-100 {{was one of the first}} machines to use vector processing - the idea having been inspired around 1964 by the APL programming language.|$|E
40|$|Concomitantly {{with recent}} {{advances}} in speech coding, recognition and production, parallel computer systems are now commonplace delivenng raw computing power measured in hundreds of MIPS and <b>Megaflops.</b> It seems inevitable that within the next decade or so, gigaflop parallel processors will be achievable at modest cost. Indeed, gigaflops per cubic foot is now becoming a standard of measure for parallel computers...|$|E
40|$|Abstract. Large {{industrial}} aerodynamic calculations are nowadays performed indifferently on parallel vector computers or on {{clusters of}} SMPs. From a software design {{point of view}} {{it is crucial to}} ensure the best possible sustained <b>megaflops</b> rate on both platforms while for sake of minimal labour effort it is important to maintain only one source code. In this paper we describe software techniques that have been implemented to comply with these constraints. We first show how we succeeded to obtain a sustained <b>megaflops</b> rate that is independent of the mesh size. We then investigate the possibilities of using OpenMP in some of the main time consuming routines to replace the vectorization by fine grain parallelism to better exploit the shared memory available within each node of the SMP computers. Numerical experiments are reported on HP-Compaq, IBM SP based on Power 3 and Power 4 where tremendous savings can be obtained at a cost of a very little code change. ...|$|E
40|$|Each {{of the six}} GONG observing {{stations}} {{will produce}} three, 16 -bit, 256 X 256 images of the Sun every 60 sec of sunlight. These data {{will be transferred from}} the observing sites to the GONG Data Management and Analysis Center (DMAC), in Tucson, on high-density tapes at a combined rate of over 1 gibabyte per day. The contemporaneous processing of these data will produce several standard data products and will require a sustained throughput in excess of 7 <b>megaflops.</b> Peak rates may exceed 50 <b>megaflops.</b> Archives will accumulate at the rate of approximately 1 terabyte per year, reaching nearly 3 terabytes in 3 yr of observing. Researchers will access the data products with a machine-independent GONG Reduction and Analysis Software Package (GRASP). Based on the Image Reduction and Analysis Facility, this package will include database facilities and helioseismic analysis tools. Users may access the data as visitors in Tucson, or may access DMAC remotely through networks, or may process subsets of the data at their local institutions using GRASP or other systems of their choice. Elements of the system will reach the prototype stage by the end of 1988. Full operation is expected in 1992 when data acquisition begins...|$|E
40|$|High {{performance}} {{scientific data}} analysis is plagued by chronically inadequate I/O performance. The situation is aggravated by ever improving processor performance. For high performance multicomputers, {{such as the}} Touchstone Delta that possess in excess of 500, 60 <b>megaflops,</b> processor I/O will be the bottleneck for many scientific applications. This report describes ELFS (an ExtensibLe File System). ELFS attacks the problems of 1) providing high bandwidth and low latency I/O to applications programs on high performance architectures, 2) reducing the cognitive burden faced by applications programmers when they attempt to optimize their I/O operations to fit existing file system models, and 3) seamlessly managing the proliferation of data formats and architectural differences. The ELFS solution consists of language and run-time system support that permits the specification of a hierarchy of file classes. 1 ELFS: Object-Oriented Extensible File Systems 1. Introduction Contemporary hi [...] ...|$|E
40|$|The {{emergence}} of high speed networks and {{the proliferation of}} high performance workstations have attracted {{a lot of interest in}} workstation-based distributed computing. Current trend in local area networks is toward higher communication bandwidth as we progress from Ethernet networks that operate at 10 Mbit/sec to higher speed networks that can operate in Gbit/sec range. Also, current workstations are capable of delivering tens and hundreds of <b>Megaflops</b> of computing power. By using a cluster of such high-performance workstations and the high-speed networks, a high-performance distributed computing environment could be built in cost-effective manner as an alternative of supercomputing platform. However, in current local area networks, the bandwidths achievable at the application level are often an order of magnitude lower than that provided at the network medium [3, 7]. It is therefore not sufficient to have even a Gigabit data link if user applications could only use a small portion of [...] ...|$|E
40|$|In this paper, {{we present}} an {{approach}} to build a hypercube-based distributed system using ATM Local Area Network (LAN). We also present {{the design of a}} host-to-ATM network interface and the ATM switch needed to build such a system. The ATM interface accesses cache memory directly to minimize the communication latency. Data copying rate between host and network is further improved by implementing pipelined read/write operations during data movement. The interface also dispatches arriving messages automatically to support parallel activities during message passing. The ATM switch, by input/output trunk grouping[12], can achieve high throughput in both uniform and non-uniform traffic conditions. The system uses switch-based architecture instead of traditional ring or bus architectures to support a large number of users and gigabit applications. 1 Introduction With modern computers that are capable of delivering tens and hundreds of <b>Megaflops</b> of computing power and the emerging of high spe [...] ...|$|E
40|$|Numerical {{simulations}} {{of the strong}} nuclear force, known as quantum chromodynamics or QCD, {{have proven to be}} a demanding, forefront problem in high-performance computing. In this report, we describe a new computer, QCDOC (QCD On a Chip), designed for optimal price/performance in the study of QCD. QCDOC uses a six-dimensional, low-latency mesh network to connect processing nodes, each of which includes a single custom ASIC, designed by our collaboration and built by IBM, plus DDR SDRAM. Each node has a peak speed of 1 Gigaflops and two 12, 288 node, 10 + Teraflops machines are to be completed in the fall of 2004. Currently, a 512 node machine is running, delivering efficiencies as high as 45 % of peak on the conjugate gradient solvers that dominate our calculations and a 4096 -node machine with a cost of $ 1. 6 M is under construction. This should give us a price/performance less than $ 1 per sustained <b>Megaflops.</b> ...|$|E
40|$|International Telemetering Conference Proceedings / October 30 -November 02, 1989 / Town & Country Hotel & Convention Center, San Diego, CaliforniaDuring {{the past}} several years Lockheed's CATS group has built four {{large-scale}} data acquisition machines that are based on the APTEC I/O computer. Features of these systems include: * Speed: Up to 5, 000, 000 samples per second acquired to mass storage. * Duration: Several minutes per test (billions of samples). * Accuracy:. 05 % of full scale. * Real Time Display: Multi-channel, multi display. * Real time and post-processing calculation: 40 <b>megaFlops.</b> * Data access: Immediate, random access at test completion. The machines are appropriate for acoustic and structural-dynamic testing, windtunnel research, and scram jet engine performance analysis. Traditionally, these applications have been done with "Multiplexed FM" or "PCM" magnetic tape systems. Where they are applicable, I/O computer based systems are more accurate, versatile, and convenient than their predecessors. This paper describes the I/O computer based systems that are in use and explores the near-term extensions to the technology. Topics discussed include: * Real time displays of structural deflection. * Closed loop control systems (for structural-dynamic and acoustic testing and control-structure-interaction research. * Extensions to an aggregate rate of 20, 000, 000 samples/second at high accuracy...|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimitedResults {{of a highly}} vectorized and multitasked model of the world ocean circulation were analyzed. This model which uses realistic physics, geometry, and forcing on a high-resolution grid, was run on the NCAR Cray X-MP/ 48 using a robust-diagnostic strategy. Twenty years of model integration using one-half degree horizontal resolution and 20 levels of vertical resolution were accomplished after 200 wall-clock hours at a maximum FORTRAN performance speed of 450 <b>megaflops.</b> Seven key {{regions of the world}} ocean were analyzed using an ocean model processor. A representation of the global ocean circulation emerged that compared well with observations and that included strong advective features, fronts, and subtropical meanders. A diagnostic analysis program was developed to analyze meridional heat and volume transports. The results in all basins appear to be reasonable when compared to the results of other studies. For example, an anomalous northward heat transport of 3. 8 x 10 to the 14 th power W at 30 deg in the South Atlantic compares favorably with the estimate of 4. 2 x 10 to the 14 th power W at 32 deg S by Bennett (1978) using hydrographic data. The results of simulations conducted in this study can be compared and contrasted against the results of future eddy-resolving simulations. Keywords: Digital simulation, Advection, Heat transport, Meridional volume transport, Oceanographic fronts, Meanders, Thermoclines, Finite difference analysis. Theses. (EDC) [URL] Commander, United States Naval Reserv...|$|E
