12|8875|Public
50|$|The PUT {{strategy}} {{is designed to}} sell a sequence of one-month, at-the-money, S&P 500 Index puts and invest cash at one- and three-month Treasury Bill rates. The number of puts sold varies from month to month, but is limited so that the amount held in Treasury Bills can finance the <b>maximum</b> <b>possible</b> <b>loss</b> from final settlement of the SPX puts. The PUT Index was introduced in 2007 by the Chicago Board Options Exchange (CBOE). The Index is calculated and disseminated daily by the CBOE and the daily price history of the PUT Index dates back to June 30, 1986.|$|E
5000|$|The {{algorithm}} evaluates each {{leaf node}} using a heuristic evaluation function, obtaining the values shown. The moves where the maximizing player wins are assigned with positive infinity, while the moves {{that lead to}} a win of the minimizing player are assigned with negative infinity. At level 3, the algorithm will choose, for each node, {{the smallest of the}} child node values, and assign it to that same node (e.g. the node on the left will choose the minimum between [...] "10" [...] and [...] "+∞", therefore assigning the value [...] "10" [...] to itself). The next step, in level 2, consists of choosing for each node the largest of the child node values. Once again, the values are assigned to each parent node. The algorithm continues evaluating the maximum and minimum values of the child nodes alternately until it reaches the root node, where it chooses the move with the largest value (represented in the figure with a blue arrow). This is the move that the player should make in order to minimize the <b>maximum</b> <b>possible</b> <b>loss.</b>|$|E
30|$|A {{negative}} index number, say − 20, {{reveals that}} overall a trading strategy generates a loss. However, the actual loss is only 20  % of the <b>maximum</b> <b>possible</b> <b>loss</b> (HOD) during a simulation. The index with {{a value of}} “− 100 ” means that a trading strategy incurs the <b>maximum</b> <b>possible</b> <b>loss</b> (HOD).|$|E
30|$|One {{can view}} the {{capability}} design methods as attempts to avoid uncertainty (Cyert and March 1963). The methods help achieve maximum knowledge (minimum ignorance) in decision-making under high uncertainty. In parallel, the approach implements the minimax principle, in which agents seek to bound uncertainty by minimizing <b>maximum</b> <b>possible</b> <b>losses</b> while seeking to gain large profits (Epstein 2001).|$|R
40|$|Initially, the {{development}} of a dual-purpose clutch was based on racing experiences and application requirements, as well as the results from testing the new power unit in the existing prototype vehicle. In order to achieve the highest possible driving range of the prototype vehicle, it has been necessary to eliminate the <b>maximum</b> <b>possible</b> <b>losses</b> and drive in unnecessary components. The design aimed to achieve simple access, reliability and low weight...|$|R
5000|$|The <b>maximum</b> <b>possible</b> heat <b>loss</b> from an annular fin {{occurs when}} the fin is isothermal. This ensures that the {{temperature}} difference between the fin and the surrounding fluid is maximized at every point along the fin, increasing heat transfer by convection, and ultimately heat loss Q: ...|$|R
40|$|We {{focus on}} {{automatic}} strategies to optimize life cycle savings and investment. Classical optimal savings theory establishes that, given {{the level of}} risk aversion, a saver would keep the same relative amount invested in risky assets at any given time. We show that, when optimizing lifecycle investment, performance and risk assessment {{have to take into}} account the investor’s risk aversion and the maximum amount the investor could lose, simultaneously. When risk aversion and <b>maximum</b> <b>possible</b> <b>loss</b> are considered jointly, an optimal savings strategy is obtained, which follows from constant rather than relative absolute risk aversion. This result is fundamental to prove that if risk aversion and the <b>maximum</b> <b>possible</b> <b>loss</b> are both high, then holding a constant amount invested in the risky asset is optimal for a standard lifetime saving/pension process and outperforms some other simple strategies. Performance comparisons are based on downside risk-adjusted equivalence that is used in our illustration...|$|E
40|$|The optimal-exercise {{policy of}} an American option {{dictates}} when the option should be exercised. In this paper, {{we consider the}} implications of missing the optimal exercise time of an American option. For the put option, this means holding the option until it is deeper in-the-money when the optimal decision {{would have been to}} exercise instead. We derive an upper bound on the <b>maximum</b> <b>possible</b> <b>loss</b> incurred by such an option holder. This upper bound requires no knowledge of the optimal-exercise policy or true price function. This upper bound is a function of only the option-holder’s exercise strategy and the intrinsic value of the option. We show that this result holds true for both put and call options under a variety of market models ranging from the simple Black–Scholes model to complex stochastic-volatility jump-diffusion models. Numerical illustrations of this result are provided. We then use this result to study numerically how the cost of delaying exercise varies across market models and call and put options. We also use this result as a tool to numerically investigate the relation between an option-holder’s risk-preference levels and the <b>maximum</b> <b>possible</b> <b>loss</b> he may incur when adopting a target-payoff policy that is a function of his risk-preference level...|$|E
40|$|In finance, {{the price}} of a {{volatile}} asset can be modeled using fractional Brownian motion (fBm) with Hurst parameter H> 1 / 2. The Black-Scholes model for the values of returns of an asset using fBm is given as, [Y_t=Y_ 0 ((r+μ) t+σ B_t^H), t≥ 0], where Y_ 0 is the initial value, r is constant interest rate, μ is constant drift and σ is constant diffusion coefficient of fBm, which is denoted by (B_t^H) where t ≥ 0. Black-Scholes model can be constructed with some Markov processes such as Brownian motion. The advantage of modeling with fBm to Markov proccesses is its capability of exposing the dependence between returns. The real life data for a volatile asset display long-range dependence property. For this reason, using fBm is a more realistic model compared to Markov processes. Investors would be interested in any kind of information on the risk in order to manage it or hedge it. The <b>maximum</b> <b>possible</b> <b>loss</b> is one way to measure highest possible risk. Therefore, it is an important variable for investors. In our study, we give some theoretical bounds on the distribution of <b>maximum</b> <b>possible</b> <b>loss</b> of fBm. We provide both asymptotical and strong estimates for the tail probability of maximum loss of standard fBm and fBm with drift and diffusion coefficients. In the investment point of view, these results explain, how large values of possible loss behave and its bounds...|$|E
40|$|Justification {{to remove}} the 333 Building fire {{suppression}} system is provided. The <b>Maximum</b> <b>Possible</b> Fire <b>Loss</b> (MPFL) is provided (approximately $ 800 K), potential radiological and toxicological impacts from a postulated fire are discussed, Life Safety Code issues are addressed, and coordination with the Hanford Fire Department is assured...|$|R
30|$|Potential maximum loss (PMLt). PMLt=ln(Ht)-ln(Ct). The {{potential}} <b>maximum</b> <b>loss</b> {{measures the}} <b>possible</b> <b>maximum</b> <b>loss</b> {{from the high}} price extreme to the closing price.|$|R
40|$|Various {{methods of}} option pricing in {{discrete}} time models are discussed. The classical risk minimization method {{often results in}} negative prices and a natural modification is proposed. Another method of risk minimization using an inductive procedure as in the Cox-Ross-Rubinstein model is also proposed. The definition of the risk interpreted as the <b>maximum</b> of <b>possible</b> <b>loss</b> is discussed. Incomplete Markets, Derivative Securities,...|$|R
40|$|Abstract: This paper {{introduces}} a calculation method of engineering insurance rate based on engineering cost. The research has great significance for promoting {{the development of}} the engineering insurance and improving the exits project risk management model. The paper uses the modification method and the <b>maximum</b> <b>possible</b> <b>loss</b> method to calculate the engineering insurance rate. Firstly, the paper calculates the insurance rate of part project according to engineering costs and risks of each separated project, then gets the insurance rate of unit project through the weighted average of the proportion of part project costs. Similarly, it is available to get the insurance rate of other single project. Ultimately, the overall engineering insurance rate for the engineering project can be obtained...|$|E
40|$|Value at Risk is {{a measure}} of risk {{exposure}} of a portfolio and is defined as the <b>maximum</b> <b>possible</b> <b>loss</b> in a certain time frame, typically 1 - 20 days, and within a certain confidence, typically 95 %. Full valuation of a portfolio under a large number of scenarios is a lengthy process. To speed it up, one can make use of the total delta vector and the total gamma matrix of a portfolio and compute a Gaussian integral over a region bounded by a quadric. We use methods from harmonic analysis to find approximate analytic formulas for the Value at Risk as a function of time and of the confidence level. In this framework, the calculation is reduced to the problem of evaluating linear algebra invariants such as traces of products of matrices, which arise from a Feynmann expansion. The use of Fourier transforms is crucial to resum the expansions and to obtain formulas that smoothly interpolate between low and large confidence levels, as well as between short and long time horizons. 1 e-mail: al [...] ...|$|E
40|$|The {{main focus}} of this work is on hedging of {{currency}} risks with special emphasis {{on the case of}} Czech export. In the first chapter, I create a motivation for further studying of the problem. I describe the state of export industries and {{the economy as a whole}} and how these aspects are connected to the exchange rates. In the second chapter, I explain how firms create their assumptions about future exchange rates. I also run a Monte Carlo analysis on historical data and come with predictions of my own. In the third chapter, I am discussing the relevance of using VaR models for estimating the <b>maximum</b> <b>possible</b> <b>loss</b> of funds due to unwanted moves in the exchange rate. Furthermore, I describe various instruments usable for hedging of currency exposure including forwards, options, swaps and other derivatives. In the final chapter of this work, I am asking financial and sales directors of 51 Czech firms about how currency risks influence their businesses and how they protect themselves against these threats...|$|E
40|$|As a {{new type}} of cyber attacks, {{advanced}} persistent threats (APTs) pose a severe threat to modern society. This paper focuses on the assessment of the risk of APTs. Based on a dynamic model characterizing the time evolution of the state of an organization, the organization's risk is defined as its <b>maximum</b> <b>possible</b> expected <b>loss,</b> and the risk assessment problem is modeled as a constrained optimization problem. The influence of different factors on an organization's risk is uncovered through theoretical analysis. Based on extensive experiments, we speculate that the attack strategy obtained by applying the hill-climbing method to the proposed optimization problem, which we call the HC strategy, always leads to the <b>maximum</b> <b>possible</b> expected <b>loss.</b> We then present a set of five heuristic attack strategies and, through comparative experiments, show that the HC strategy causes a higher risk than all these heuristic strategies do, which supports our conjecture. Finally, the impact of two factors on the attacker's HC cost profit is determined through computer simulations. These findings help understand the risk of APTs in a quantitative manner. Comment: advanced persistent threat, risk assessment, expected loss, attack strategy, constrained optimizatio...|$|R
40|$|This paper {{suggests}} that {{the difference in the}} Theil indices of inequality between two economies approximately measures the relative loss of aggregate productivity caused by distortions in labor allocation. Moreover, the Theil index itself can be interpreted approximately as the <b>possible</b> <b>maximum</b> <b>loss</b> of aggregate productivity caused by these distortions. ...|$|R
40|$|We present three-dimensional, non-axisymmetric {{distorted}} {{black hole}} initial data which generalizes the axisymmetric, distorted, non-rotating [1] and rotating [2] single black hole data developed by Bernstein, Brandt, and Seidel. These initial data should {{be useful for}} studying the dynamics of fully 3 D, distorted black holes, such as those created by the spiraling coalescence of two black holes. We describe the mathematical construction of several families of such data sets, and show how to construct numerical solutions. We survey quantities associated with the numerically constructed solutions, such as ADM masses, apparent horizons, measurements of the horizon distortion, and the <b>maximum</b> <b>possible</b> radiation <b>loss</b> (MRL) ...|$|R
40|$|Copyright © 2014 Russell Gerrard et al. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. We focus on automatic strategies to optimize life cycle savings and investment. Classical optimal savings theory establishes that, given the level of risk aversion, a saver would keep the same relative amount invested in risky assets at any given time. We show that, when optimizing lifecycle investment, performance and risk assessment {{have to take into}} account the investor’s risk aversion and the maximum amount the investor could lose, simultaneously. When risk aversion and <b>maximum</b> <b>possible</b> <b>loss</b> are considered jointly, an optimal savings strategy is obtained, which follows from constant rather than relative absolute risk aversion. This result is fundamental to prove that if risk aversion and themaximumpossible loss are both high, then holding a constant amount invested in the risky asset is optimal for a standard lifetime saving/pension process and outperforms some other simple strategies. Performance comparisons are based on downside risk-adjusted equivalence that is used in our illustration. 1...|$|E
40|$|Abstract – This paper {{presents}} a practical procedure for using real options valuation {{in the design}} optimization of complex engineering systems. Recognition of future uncertainty in both design requirements and the operating environment {{is the point of}} departure – a significant shift away from traditional design practice that posits known values for key technical and economic factors. The process leads to the identification of an initial design and a strategy for implementing future expansions according to the way the future unfolds – in contrast to the usual real options analyses that define a price for the option. Optimization of the design with the recognition of uncertainty leads to significant (greater than 10 %) improvements in system performance. The optimization results in multiple desirable attributes, not only the maximization of expected value but also other considerations that may be useful to project managers, such as <b>maximum</b> <b>possible</b> <b>loss</b> or gain and the robustness of the design. A Value-at-Risk diagram conveniently displays these criteria. The optimization itself is extraordinarily complex, compared to standard options analyses, because the reality of design means that the performance of future states is not path independent (because the system evolves in response to its environment) so that the number of possible combinations is astronomical (over 10 to the power of 60 for the simple example problem). A Genetic Algorithm provides a practical solution to such problems, as demonstrated by a generic problem concerning the development of an offshore oil pipeline network...|$|E
40|$|Recently some papers, such as Aban, Meerschaert and Panorska (2006), Nuyts (2010) and Clark (2013), {{have drawn}} {{attention}} to possible truncation in Pareto tail modelling. Sometimes natural upper bounds exist that truncate the probability tail, {{such as the}} <b>Maximum</b> <b>Possible</b> <b>Loss</b> in insurance treaties. At other instances ultimately at the largest data, deviations from a Pareto tail behaviour become apparent. This matter is especially important when extrapolation outside the sample is required. Given that in practice one does not always know whether the distribution is truncated or not, we consider estimators for extreme quantiles both under truncated and non-truncated Pareto-type distributions. Hereby we {{make use of the}} estimator of the tail index for the truncated Pareto distribution first proposed in Aban et al. (2006). We also propose a truncated Pareto QQ-plot and a formal test for truncation in order to help deciding between a truncated and a non-truncated case. In this way we enlarge the possibilities of extreme value modelling using Pareto tails, offering an alternative scenario by adding a truncation point T that is large with respect to the available data. In the mathematical modelling we hence let T →∞ at different speeds compared to the limiting fraction (k/n → 0) of data used in the extreme value estimation. This work is motivated using practical examples from different fields of applications, simulation results, and some asymptotic results. Comment: arXiv admin note: text overlap with arXiv: 1410. 409...|$|E
30|$|At each {{particular}} recovery stage, {{the exposed}} {{area of the}} micromodel to the radiation heat loss is {{a fraction of the}} total surface area due to partial depletion of the model. Because the steady state operating temperature of each pore-scale SAGD test was kept relatively constant during each experiment, one can calculate the instantaneous heat loss by multiplying the <b>maximum</b> <b>possible</b> heat <b>loss</b> by the fraction of the model surface area which is exposed to the radiation heat loss. Details of this procedure is described later in the paper where the total cumulative steam consumed during each pore-scale SAGD trial was corrected for the amount of steam which condensed in the model to cover up for the heat loss to the surrounding.|$|R
40|$|The {{reduction}} {{characteristics of}} Lampung (Sumatra) lump ore uses carbon monoxide {{have been studied}} by thermogravimetry. The course of reaction has been expressed in term of degree of reduction defined as weight loss measured {{at a given time}} with respect to the <b>maximum</b> <b>possible</b> weight <b>loss.</b> The results of the reduction characteristics of the ore have shown that the presence of goethite. identified by X-ray diffraction, in an essential magneto-hematite ore complicates the reduction. Some fixed time reduction experiments have been candied out on ore samples where the reduced sample was examined under optical and SEM and also some physical properties were measured. The resulting reduction and porosity curves shows considerable variation behaviour and reflect the physical variation in the individual ore...|$|R
40|$|An {{investigation}} {{was made of}} the performance of nine conical cooling-air ejectors at primary jet pressure ratios from 1 to 10, secondary pressure ratios to 4. 0, and a temperature ratio of unity. This phase of the {{investigation was}} limited to conical ejectors having shroud exit to primary nozzle exit diameter ratios of 1. 06 and 1. 40, with several spacing ratios for each. The experimental results indicated that the pumping range and amount of cooling-air flow obtained with a 1. 06 diameter ratio ejector were relatively small for cooling purposes but that the <b>maximum</b> <b>possible</b> thrust <b>loss,</b> which occurred with no secondary flow, was only 7 percent of convergent nozzle thrust. The 1. 40 diameter ratio ejector produced a large cooling air flow and showed a <b>possible</b> thrust <b>loss</b> of 29. 5 percent with no cooling air flow. Thrust gains were attained with ejectors of both diameter ratios at secondary pressure ratios greater than 1. 0. The limiting primary pressure ratio below which an ejector can operate at a specific secondary pressure ratio (cut-off point) may be estimated for various flight conditions from data contained herein...|$|R
40|$|This Fire Hazards Analysis (FHA) is {{intended}} to assess comprehensively the risk from fire associated with the disposal of low level radioactive mixed waste in trenches within the lined landfills, provided by Project W- 025, designated Trench 31 and 34 of the Burial Ground 218 -W- 5. Elements within the FHA make recommendations for minimizing risk to workers, the public, and the environment from fire {{during the course of}} the operation`s activity. Transient flammables and combustibles present that support the operation`s activity are considered and included in the analysis. The graded FHA contains the following elements: description of construction, protection of essential safety class equipment, fire protection features, description of fire hazards, life safety considerations, critical process equipment, high value property, damage potential [...] maximum credible fire <b>loss</b> (MCFL) and <b>maximum</b> <b>possible</b> fire <b>loss</b> (MPFL), fire department/brigade response, recovery potential, potential for a toxic, biological and/or radiation incident due to a fire, emergency planning, security considerations related to fire protection, natural hazards (earthquake, flood, wind) impact on fire safety, and exposure fire potential, including the potential for fire spread between fire areas. Recommendations for limiting risk are made in the text of this report and printed in bold type. All recommendations are repeated in a list in Section 18. 0...|$|R
40|$|Normalized {{change is}} a {{familiar}} expression used to measure student 2 ̆ 7 s improvement in physics education research, including critical thinking skill improvement. A widely used standardized critical thinking test is the Cornell Critical Thinking Test. The CCTT scoring method, rights minus one-half the number wrong, results from possible interval scores ranging from the negative minimum score to positive maximum score. The problem then arises {{in the use of}} the normalized change in CCTT scores, particularly in the situation when the post-test score is worse than the pre-test score. We reveal the used equation deficiencies and demonstrate the mistakes made by undergraduate researchers, as well as suggesting a modified equation that can be used under the normalized change rationale, i. e. the ratio of the gain or the <b>loss</b> of the <b>maximum</b> <b>possible</b> gain or <b>loss.</b> Some frequently asked questions about normalized change are also discussed...|$|R
30|$|A novel {{method for}} heat loss {{prevention}} {{during the course}} of in situ bitumen recovery using the SAGD process was utilized {{with the aid of a}} high-precision inverted-bell vacuum chamber. This method was effective in diminishing the convective heat transfer mechanism of thermal energy losses from the porous media at elevated temperatures of superheated steam. Heat loss experiments were conducted to determine the <b>maximum</b> <b>possible</b> heat <b>losses,</b> facilitated with the radiation heat transfer mechanism, at corresponding operating conditions when there was no oil in the micromodels. Fractional instantaneous heat losses from the porous media under radiation heat transfer mechanism were calculated during the actual pore-scale SAGD experiments considering the fraction of the swept area of glass micromodels, real-time bitumen-steam interface tracking, and energy dissipation calculations using information derived from the heat loss experiments. The analysis of heat loss was an integrated part of calculating the net CSOR values to correct the total amount of consumed thermal energy for the volume of steam which was consumed to cover the radiation heat loss. The net CSOR values signify the actual thermal energy needed to mobilize the in situ bitumen under pertaining operating conditions. The net CSOR data were validated using the theory of gravity drainage and its application in the SAGD process. A scaling parameter was defined based on this theory to correlate the net CSOR values with the experimental variables. An acceptable linear correlation was obtained by plotting all the net CSOR data versus this scaling parameter over the entire range of the experimental variables.|$|R
40|$|This {{study was}} {{initiated}} to develop {{information regarding the}} use of fly ash in portland cement concrete for state construction projects. Concrete mixes containing 10 %, 20 %, 30 %, 40 % and 60 % fly ash were evaluated in the laboratory in combination with various cement contents. Type C fly ash was selected from three local sources which had been approved by the department for concrete mixes. Also, specifications were developed for using fly ash in a paving project. In general, fly ash when used at replacement below 40 % by weight of cement was found to be satisfactory in concrete. In areas that the <b>maximum</b> <b>possible</b> strength <b>loss</b> cannot exceed 10 % of control, replacement rate of less than 25 % is recommended. Increasing amounts of fly ash caused a reduction in compressive strength, especially when air-entraining agents were used or when the concrete was less than 28 days old. Retardation in the set times was also noticed with increasing amounts of fly ash. However, strength gains of up to 10 % were noticed in some mixes after extended curing periods. There were no adverse effects observed on the plastic properties, freeze and thaw durability, modulus of elasticity, length change, abrasion resistance, or absorption characteristics of fly ash concrete at the replacement rates evaluated. Based on the overall results of this study, no changes are recommended to the current fly ash concrete specifications developed earlier in this project. 17. Key Words fly ash concrete, strength, concrete mixing, admixture...|$|R
40|$|The {{demand for}} spices and {{extracts}} is growing {{all over the}} world, although the spices are exported {{in a variety of}} forms in the international spice trade, about 80 - 85 % are sold in the whole ungrounded state. The rest are marketed as ground spices or in mixes and as spice essential oils and oleoresins. Dehydrated garlic as spice is of considerable importance in world trade. The quality of dehydrated product is affected by number of factors and is dependent on the quality of raw material used, method of preparation, processing treatments and drying conditions. The freeze drying applied to the manufacturing of certain foods can result in economically unprofitable products. The difficulties encountered in the heat transfer can be avoided by combining other dehydration methods. The garlic cloves for <b>maximum</b> <b>possible</b> Moisture <b>Loss</b> (ML) and retention of volatile oils (VOR) with minimum changes in sensory quality scores (SQS maximum 20) were subjected to hot air drying (13. 3 % mL, 17. 1 / 20 SQS, 60 % VOR, at 60 C for 3 h), Fluidized Bed Drying (FBD) (17. 1 % mL, 14. 2 / 20 SQS, 65 % VOR at 55 C for 2 h), microwave heating (9. 75 mL, 16. 9 / 20 SQS, 75 % VOR for 14 sec) and freeze drying treatment (58. 8 % mL, 19. 3 / 20 SOS, 98. 7 % VOR). By analyzing their effect on the loss of moisture, volatile oil retention and sensory quality, it could be concluded that Freeze drying operation could be combined with fluidized bed drying to get a product with desirable quality attributes and maximum moisture removal at minimum processing cost...|$|R
40|$|ABSTRACT Using {{observations}} of calibration {{fields in the}} star clusters NGC 6791 and NGC 104 / 47 Tuc, we have investigated {{the evolution of the}} charge transfer efficiency (CTE) of the WFC 3 /UVIS detectors over a 2 -year period starting 5 months after the WFC 3 installation, ranging from October 2009 to October 2011. We find a strong evolution of the CTE, amounting to more than 0. 1 mag CTE loss per year for stars with a total flux of 1000 electrons, at large distances from the readout amplifiers, in short-exposure zero sky background images. These <b>maximum</b> <b>possible</b> CTE <b>losses</b> for such stars have reached 0. 3 mag (24 %) as of October 2011 in 3 -pixel radius apertures. For even fainter stars around 300 electrons, these losses can reach 50 %. The CTE degradation and associated losses decrease for increasing source fluxes, and for higher sky background levels which partially fill the detector charge traps during readout. Already a low background of 2 - 3 electrons per pixel significantly lowers CTE losses, while a 20 - 30 electron per pixel background in longer-exposure broadband images brings CTE losses to few percent for sources ranging from few 100 to 10 s of 1000 s of electrons. For faint sources on near-zero sky backgrounds, we also see significantly lower CTE losses in longer (approx. 350 seconds) exposures than in shorter ones (30 - 60 seconds). This most likely reflects that the larger total charge collected in longer images has a similar, yet non-uniformly distributed trap-filling effect as a higher sky background. We present an empirical polynomial model that corrects CTE losses for point source aperture photom-etry as a function of observation date, source flux, and source distance on the detector from the readout amplifiers. The model coefficients are given for low and high backgrounds and long and short exposures separately, while we collect further calibration data to include a smooth parameterization of the latter parameters...|$|R
40|$|Using row {{functions}} of egen, scores calculates scores according to fcn using variables listed in varlist and assigns {{them to the}} new variable newvar. If the number of valid values of varlist is less than minvalid(#) the resulting score will be set to missing. fcn {{can be one of}} several egen functions. -scores- now allows to calculate median and percentile scores, proportions of <b>maximum</b> <b>possible</b> scores, and shrunken proportions of <b>maximum</b> <b>possible</b> scores. Additionally, it allows by: and fweights, aweights, or iweights (only useful if requesting z-scores, mean centered scores, or shrunken proportions of <b>maximum</b> <b>possible</b> scores). scores, percent of <b>maximum</b> <b>possible,</b> POMP, egen...|$|R
50|$|The <b>maximum</b> <b>possible</b> {{score is}} 244.|$|R
50|$|The <b>maximum</b> <b>possible</b> {{individual}} {{score is}} 50.|$|R
2500|$|The emf of {{the cell}} at zero current is the <b>maximum</b> <b>possible</b> emf. It is used to {{calculate}} the <b>maximum</b> <b>possible</b> electrical energy that could be obtained from a chemical reaction. This energy is referred to as electrical work and is expressed by the following equation: ...|$|R
40|$| the <b>maximum</b> <b>possible</b> yields {{to obtain}} in each|$|R
5000|$|Operational Range: 150-200 km (<b>Maximum</b> <b>Possible</b> Range: approx. 300km) ...|$|R
