318|859|Public
5000|$|User Datagram Protocol (UDP) echo, for VoIP jitter and <b>Mean</b> <b>Opinion</b> <b>Score</b> (<b>MOS)</b> ...|$|E
5000|$|The {{main idea}} of {{measuring}} subjective video quality {{is similar to}} the <b>Mean</b> <b>Opinion</b> <b>Score</b> (<b>MOS)</b> evaluation for audio. To evaluate the subjective video quality of a video processing system, the following steps are typically taken: ...|$|E
50|$|Genista Corporation uses {{computational}} {{models of}} human visual and auditory systems to measure what viewers see and hear. Resulting perceptual metrics complement existing technologies by predicting experienced quality {{measured by a}} <b>Mean</b> <b>Opinion</b> <b>Score</b> (<b>MOS)</b> formerly assessed using subjective tests from actual viewers.|$|E
50|$|POLQA results principally model <b>mean</b> <b>opinion</b> <b>scores</b> (<b>MOS)</b> {{that cover}} {{a scale from}} 1 (bad) to 5 (excellent).|$|R
50|$|PESQ results principally model <b>mean</b> <b>opinion</b> <b>scores</b> (<b>MOS)</b> {{that cover}} {{a scale from}} 1 (bad) to 5 (excellent). A mapping {{function}} to MOS-LQO is outlined under P.862.1.|$|R
50|$|Since {{objective}} video quality {{models are}} expected to predict results given by human observers, they are developed {{with the aid of}} subjective test results. During development of an objective model, its parameters should be trained so as to achieve the best correlation between the objectively predicted values and the subjective scores, often available as <b>mean</b> <b>opinion</b> <b>scores</b> (<b>MOS).</b>|$|R
50|$|POLQA {{covers a}} model to predict speech quality, by means of digital speech signal analysis. The {{predictions}} of those objective measures should come {{as close as possible}} to subjective quality scores as obtained in subjective listening tests. Usually, a <b>Mean</b> <b>Opinion</b> <b>Score</b> (<b>MOS)</b> is predicted. POLQA uses real speech as a test stimulus for assessing telephony networks.|$|E
50|$|<b>Mean</b> <b>opinion</b> <b>score</b> (<b>MOS)</b> is {{a measure}} used {{in the domain of}} Quality of Experience and {{telecommunications}} engineering, representing overall quality of a stimulus or system. It is the arithmetic mean over all individual “values on a predefined scale that a subject assigns to his opinion of the performance of a system quality”. Such ratings are usually gathered in a subjective quality evaluation test, but they can also be algorithmically estimated.|$|E
5000|$|Opinions {{of viewers}} are {{typically}} averaged into the <b>Mean</b> <b>Opinion</b> <b>Score</b> (<b>MOS).</b> To this aim, the labels of categorical scales may {{be translated into}} numbers. For example, the responses [...] "bad" [...] to [...] "excellent" [...] can be mapped to the values 1 to 5, and then averaged. MOS values should always be reported with their statistical confidence intervals so that the general agreement between observers can be evaluated.|$|E
30|$|For each of {{the five}} {{subjective}} tests (one audio, two video, two audiovisual), the scores were averaged over subjects, yielding <b>mean</b> <b>opinion</b> <b>scores</b> (<b>MOS),</b> were linearly transformed to the 5 -point ACR MOS scale by aligning the numbers of the scales, and further transformed to the 100 -point model scale using the conversion defined in ITU-T Recommendation G. 107 [14].|$|R
30|$|In this dataset, {{there are}} 88 models, between 40 K and 50 K vertices, which were {{generated}} from four reference objects: Armadillo, Venus, Dinosaur, and RockerArm. Two types of distortion, noise addition and smoothing, were applied with different strengths at four locations: {{on the whole}} model, on smooth areas, on rough areas, and on intermediate areas. The dataset also includes <b>mean</b> <b>opinion</b> <b>scores</b> (<b>MOS)</b> from 12 observers and 7 static metric results for these models.|$|R
40|$|This paper {{analyzes}} the actual usage of <b>Mean</b> <b>Opinion</b> <b>Scores</b> (<b>MOS)</b> in the telecommunication industry {{and looks back}} to the tremendous evolutions that has encountered this sector since subjective test methodologies were standardized by ITU-T. It is pointed out that MOS have severe drawbacks and that test methods should evolve by taking into account valuable inputs from connected research fields such as the psychology of emotions, cognitive science, usability engineering and marketing research. 1...|$|R
5000|$|PEVQ (Perceptual Evaluation of Video Quality) is an {{end-to-end}} (E2E) measurement algorithm {{to score}} the picture {{quality of a}} video presentation {{by means of a}} 5-point <b>mean</b> <b>opinion</b> <b>score</b> (<b>MOS).</b> It is therefore a video quality model. PEVQ was benchmarked by the Video Quality Experts Group (VQEG) {{in the course of the}} Multimedia Test Phase 2007-2008. Based on the performance results, in which the accuracy of PEVQ was tested against ratings obtained by human viewers, PEVQ became part of the new International Standard ITU-T Rec. J. 247 (2008).|$|E
50|$|PSQM uses a psychoacoustical {{mathematical}} modeling (both perceptual and cognitive) algorithm {{to analyze}} the pre and post transmitted voice signals, yielding a PSQM value which {{is a measure of}} signal quality degradation and ranges from 0 (no degradation) to 6.5 (highest degradation). In turn, this result may be translated into a <b>Mean</b> <b>Opinion</b> <b>Score</b> (<b>MOS),</b> which is an accepted measure of the perceived quality of received media on a numeric scale ranging from 1 to 5. A value of 1 indicates unacceptable, poor quality voice while a value of 5 indicates high voice quality with no perceptible issues.|$|E
5000|$|In {{contrast}} to network related measures like throughput, QoE is typically not measured on ratio scales. Hence, fairness measures like Jain's fairness index cannot be applied, as the measurement scale requires to be a ratio scale with a clearly defined zero point (see examples of misuse for coefficients of variation). QoE may be measure on interval scales. A typical {{example is a}} 5-point <b>mean</b> <b>opinion</b> <b>score</b> (<b>MOS)</b> scale, with 1 indicating lowest quality and 5 indicating highest quality. While the coefficient of variation is meaningless, the standard deviation [...] provides {{a measure of the}} dispersion of QoE among users.|$|E
3000|$|... [...]) {{are needed}} to {{represent}} a TF function and thereby the signal itself. These five parameters were to be quantized {{in such a way}} that the quantization error introduced was imperceptible while, at the same time, obtaining good compression. Each of the five parameters has different characteristics and dynamic range. After careful analysis of them the following bit allocations were made. In arriving at the final bit allocations informal <b>Mean</b> <b>Opinions</b> <b>Score</b> (<b>MOS)</b> tests were conducted to compare the quality of the audio samples before and after quantization stage.|$|R
30|$|Further {{attempts}} of designing VQA systems {{based on a}} single ML predictor were made, e.g., by Liu et al. [17], who used a feedforward neural network to predict the <b>mean</b> <b>opinion</b> <b>scores</b> (<b>MOS)</b> of images distorted with JPEG compression and blur. In [14], second-order histograms were used to characterize distortion perception, and then a PCA was adopted to select the most significant features. An extreme learning machine [52], i.e., {{an extension of the}} classic multilayer perceptron paradigm, was used to accomplish the final mapping due to its high non-linear modeling capabilities.|$|R
40|$|Abstract — This paper assesses VoIP quality over access {{networks}} in Pakistan using a delay jitter measurement methodology {{for evaluating the}} perceptual quality of voice calls using the ITU-T G. 107 speech quality E-model. Passive measurements for voice calls {{in the presence of}} background Internet data traffic for G. 723. 1 and G. 729 a codecs are carried out using a non-intrusive parametric model. The R-factor and resultant <b>Mean</b> <b>Opinion</b> <b>Scores</b> (<b>MOS)</b> were calculated at different link loads and congestion hot spots were identified. The study highlights the inadequacy of {{access networks}} for handling VoIP traffic at current in Pakistan and suggests alleviating congestion by increasing capacity in access networks...|$|R
50|$|A {{performance}} comparison {{for still}} image compression {{was done in}} January 2013 using the HEVC HM 8.0rc2 encoder, Kakadu version 6.0 for JPEG 2000, and IJG version 6b for JPEG. The performance comparison used PSNR for the objective assessment and <b>mean</b> <b>opinion</b> <b>score</b> (<b>MOS)</b> values for the subjective assessment. The subjective assessment used the same test methodology and images as those used by the JPEG committee when it evaluated JPEG XR. For 4:2:0 chroma sampled images the average bit rate reduction for HEVC compared to JPEG 2000 was 20.26% for PSNR and 30.96% for MOS while compared to JPEG it was 61.63% for PSNR and 43.10% for MOS.|$|E
50|$|Latency in {{telephone}} calls is {{sometimes referred to}} as mouth-to-ear delay; the telecommunications industry also uses the term quality of experience (QoE). Voice quality is measured according to the ITU model; measurable quality of a call degrades rapidly where the mouth-to-ear delay latency exceeds 200 milliseconds. The <b>mean</b> <b>opinion</b> <b>score</b> (<b>MOS)</b> is also comparable in a near-linear fashion with the ITU's quality scale - defined in standards G.107 (page 800), G.108 and G.109 - with a quality factor R ranging from 0 to 100. An MOS of 4 ('Good') would have an R score of 80 or above; to achieve 100R requires an MOS exceeding 4.5.|$|E
50|$|Service quality {{monitoring}} typically involves making test calls across the network to a fixed test unit {{to assess the}} relative quality of various services using <b>Mean</b> <b>opinion</b> <b>score</b> (<b>MOS).</b> Quality monitoring focuses on the end user experience of the service, and allows mobile network operators to react to what effectively subjective quality degradations by investigating the technical {{cause of the problem}} in time-correlated data collected during the drive test. Service {{quality monitoring}} is typically carried out in an automated fashion, using devices that run largely without human intervention carried in vehicles that regularly ply typical drive testing routes such as garbage collection vehicles, taxis or buses.|$|E
50|$|A {{number of}} {{protocols}} have been defined {{to support the}} reporting of quality of service (QoS) and quality of experience (QoE) for VoIP calls. These include RTCP Extended Report (RFC 3611), SIP RTCP Summary Reports, H.460.9 Annex B (for H.323), H.248.30 and MGCP extensions. The RFC 3611 VoIP Metrics block is generated by an IP phone or gateway during a live call and contains information on packet loss rate, packet discard rate (because of jitter), packet loss/discard burst metrics (burst length/density, gap length/density), network delay, end system delay, signal / noise / echo level, <b>Mean</b> <b>Opinion</b> <b>Scores</b> (<b>MOS)</b> and R factors and configuration information related to the jitter buffer.|$|R
40|$|AbstractIn digital transmission, images may undergo quality {{degradation}} due to {{lossy compression}} and error-prone channels. Efficient measurement tools {{are needed to}} quantify induced distortions and to predict their impact on perceived quality. In this paper, an artifcial neural network (ANN) is proposed for perceptual image quality assessment. The quality prediction is based on image features such as EPSNR, blocking, and blur. Training and testing of the ANN are performed with the <b>mean</b> <b>opinion</b> <b>scores</b> (<b>MOS)</b> provided by the Laboratory for Image and Video Engineering (LIVE). It is shown that the proposed image quality assessment model is capable of predicting MOS of the five types’ image distortions...|$|R
40|$|Accurate user-perceived video quality {{estimation}} {{models are}} increasingly needed with {{the proliferation of}} multimedia services. Previous research studies have focused on proposing and evaluating objective Video Quality Assessment (VQA) metrics, without mapping their values to <b>Mean</b> <b>Opinion</b> <b>Scores</b> (<b>MOS).</b> This paper presents a model to compute the estimated user-perceived video quality (EMOS), by combining multiple objective VQA metrics whose continuous values are mapped to discrete scores on the 0 - 5 MOS scale. The results analysis of a subjective video quality assessment study with 60 participants have shown that combining multiple VQA metric mappings can improve the user-perceived quality estimation accuracy up to 98. 5 %...|$|R
50|$|To measure {{this level}} of QoE, human ratings can be used. The <b>Mean</b> <b>Opinion</b> <b>Score</b> (<b>MOS)</b> is a widely used measure for {{assessing}} the quality of media signals; it is a limited form of QoE measurement, relating to a specific media type, in a controlled environment and without explicitly taking into account user expectations. The MOS {{as an indicator of}} experienced quality has been used for audio and speech communication, {{as well as for the}} assessment of quality of Internet video, television and other multimedia signals, and web browsing. Due to inherent limitations in measuring QoE in a single scalar value, the usefulness of the MOS is often debated.|$|E
50|$|An {{alternative}} and disputable definition of QoS, used especially in application layer {{services such as}} telephony and streaming video, is requirements on a metric that reflects or predicts the subjectively experienced quality. In this context, QoS is the acceptable cumulative effect on subscriber satisfaction of all imperfections affecting the service. Other terms with similar meaning are the quality of experience (QoE) subjective business concept, the required “user perceived performance”, the required “degree of satisfaction of the user” or the targeted “number of happy customers”. Examples of measures and measurement methods are <b>mean</b> <b>opinion</b> <b>score</b> (<b>MOS),</b> perceptual speech quality measure (PSQM) and perceptual evaluation of video quality (PEVQ). See also Subjective video quality.|$|E
5000|$|The main {{advantage}} over the <b>Mean</b> <b>Opinion</b> <b>Score</b> (<b>MOS)</b> methodology (which serves a similar purpose) is that it requires fewer participants to obtain statistically significant results [...] This is because all codecs are presented at the same time, on the same samples, so that a paired t-test or a repeated measures anova {{can be used for}} statistical analysis. Also, the 0-100 scale makes it possible to rate very small differences. In MUSHRA, the listener is presented with the reference (labeled as such), a certain number of test samples, a hidden version of the reference and one or more anchors. The recommendation specifies that a low-range and a mid-range anchor {{should be included in the}} test signals. These are typically a 7 kHz and a 3.5 kHz low-pass version of the reference. The purpose of the anchor(s) is to make the scale be closer to an [...] "absolute scale", making sure that minor artifacts are not rated as having very bad quality. This is particularly important when comparing or pooling results from different labs.|$|E
40|$|The phenomenal {{growth of}} Skype {{in recent years}} has surpassed all expectations. Much of the application’s success is {{attributed}} to its FEC mechanism, which adds redundancy to the voice streams to sustain audio quality under network impairments. Adopting the quality-of-experience (QoE) approach, i. e., measuring the <b>Mean</b> <b>Opinion</b> <b>Scores</b> (<b>MOS),</b> we examine how much redundancy Skype adds to its voice streams and systematically explore the optimal level of redundancy for different network and codec settings. This study reveals (1) Skype’s FEC mechanism, not so surprisingly, falls in the ballpark, but (2) there is surprisingly a significant margin for improvement to ensure consistent user satisfaction...|$|R
40|$|With {{the rapid}} growth in video-based services, and as users are {{becoming}} increasingly quality-aware, the reliable estimation of video quality has become extremely important. While a multitude of objective Video Quality Assessment (VQA) metrics with various performance and complexity have been proposed, the nonlinearity of video quality {{and the lack of}} clear interpretations of the metrics make difficult to understand how the objective metric values reflect the video quality as perceived subjectively in terms of <b>Mean</b> <b>Opinion</b> <b>Scores</b> (<b>MOS).</b> This paper proposes and evaluates a methodology for mapping objective VQA metric values to subjective <b>MOS</b> <b>scores</b> based on publicly available VQA databases. Three different databases were used for comparing the performance of various objective metrics and evaluating the proposed methodology...|$|R
40|$|International audienceWhile {{objective}} and subjective quality assessment {{of images and}} video have been an active research topic in the recent years, multimedia technologies require new quality metrics and methodologies {{taking into account the}} fundamental differences in the human visual perception and the typical distortions of both video and audio modalities. Because of the importance of faces and especially the talking faces in the video sequences, this paper presents an audiovisual database that contains a different talking scenario. In addition to the video, the database also provides subjective quality scores obtained using a tailored single-stimulus test method (ACR). The resulting <b>mean</b> <b>opinion</b> <b>scores</b> (<b>MOS)</b> can be used to evaluate the performance of audiovisual quality metrics {{as well as for the}} comparison and for the design of new models...|$|R
30|$|Target {{parameters}} (QoS/QoE): {{these are}} the parameters that the decision engine aims to optimize by taking corrective actions (e.g., throughput, latency, jitter, packet loss, <b>mean</b> <b>opinion</b> <b>score</b> (<b>MOS)).</b>|$|E
3000|$|... [...])[1], formant deviation[7, 20], and {{spectral}} distortion[20]. The {{commonly used}} subjective {{measures such as}} <b>Mean</b> <b>Opinion</b> <b>Score</b> (<b>MOS)</b> and ABX are used to verify the quality and similarity of the converted speech signal[21].|$|E
30|$|<b>Mean</b> <b>opinion</b> <b>score</b> (<b>MOS)</b> {{is used to}} {{test the}} quality of the SSS system. Eight male and eight female {{listeners}} took the listening tests. All of the listeners were native speakers of Turkish.|$|E
40|$|Developers of text-to-speech synthesizers (TTS) {{often make}} use of human raters to assess the quality of {{synthesized}} speech. We demonstrate that we can model human raters' <b>mean</b> <b>opinion</b> <b>scores</b> (<b>MOS)</b> of synthesized speech using a deep recurrent neural network whose inputs consist solely of a raw waveform. Our best models provide utterance-level estimates of MOS only moderately inferior to sampled human ratings, as shown by Pearson and Spearman correlations. When multiple utterances are scored and averaged, a scenario common in synthesizer quality assessment, AutoMOS achieves correlations approaching those of human raters. The AutoMOS model {{has a number of}} applications, such as the ability to explore the parameter space of a speech synthesizer without requiring a human-in-the-loop. Comment: 4 pages, 2 figures, 2 tables, NIPS 2016 End-to-end Learning for Speech and Audio Processing Worksho...|$|R
40|$|In our {{previous}} papers, we studied many input-to-output objective speech quality measures, {{some of which}} achieved high correlation when used to predict subjective <b>mean</b> <b>opinion</b> <b>scores</b> (<b>MOS)</b> of real cellular phone speech samples. Two problems of input-to-output measures are that the input must be available, which is almost never {{the case in the}} cellular phone situation, and the input must be accurately synchronized with the output. Output-based measures which do not need the input are thus highly desirable. In this paper, we propose two output-based objective speech measures which are based on visual features of the spectrogram. In our experiment, one measure OBM 1 achieves a correlation of 0. 65 which is higher than most input-to-output measures and is close to the 0. 73 achieved by the best input-to-output measure...|$|R
40|$|Abstract. We aim {{to predict}} the {{perceived}} quality of estimated source signals {{in the context of}} audio source separation. Recently, we proposed a set of metrics called PEASS that consist of three computation steps: de-composition of the estimation error into three components, measurement of the salience of each component via the PEMO-Q auditory-motivated measure, and combination of these saliences via a nonlinear mapping trained on subjective <b>opinion</b> <b>scores.</b> The parameters of the decomposi-tion were shown to have little influence on the prediction performance. In this paper, we evaluate the impact of the parameters of PEMO-Q and the nonlinear mapping on the prediction performance. By selecting the optimal parameters, we improve the average correlation with <b>mean</b> <b>opinion</b> <b>scores</b> (<b>MOS)</b> from 0. 738 to 0. 909 in a cross-validation setting. The resulting improved metrics are used {{in the context of the}} 2011 Signa...|$|R
