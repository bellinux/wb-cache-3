17|39|Public
50|$|ADALINE (Adaptive Linear Neuron {{or later}} Adaptive Linear Element) {{is an early}} {{single-layer}} artificial neural network {{and the name of}} the physical device that implemented this network. The network uses memistors. It was developed by Professor Bernard Widrow and his graduate student Ted Hoff at Stanford University in 1960. It is based on the <b>McCulloch-Pitts</b> <b>neuron.</b> It consists of a weight, a bias and a summation function.|$|E
5000|$|He {{proposed}} landmark theoretical {{formulations of}} neural activity and generative processes that influenced diverse {{fields such as}} cognitive sciences and psychology, philosophy, neurosciences, computer science, artificial neural networks, cybernetics and artificial intelligence, together with {{what has come to}} be known as the generative sciences. He is best remembered for having written along with Warren McCulloch, a seminal paper entitled [...] "A Logical Calculus of Ideas Immanent in Nervous Activity" [...] (1943). This paper proposed the first mathematical model of a neural network. The unit of this model, a simple formalized neuron, is still the standard of reference in the field of neural networks. It is often called a <b>McCulloch-Pitts</b> <b>neuron.</b>|$|E
40|$|The <b>McCulloch-Pitts</b> <b>Neuron</b> {{was one of}} the rst {{attempts}} to capture the essential prop-erties of a real neuron&quot;. This simple ersatz incorporated the idea of a threshold and the notion of multiple inputs (postsynaptic potentials) and a single output (action potential). These features in principle allowed networks of interconnected neurons of any size. Formally the <b>McCulloch-Pitts</b> <b>neuron</b> can be described mathematically by where hi(t) = NX Jij j(t) j=...|$|E
40|$|This paper {{investigates the}} {{evolution}} of evolved autonomous agents that solve a memory-dependent delayed response task. Two types of neurocontrollers are evolved: networks of <b>McCulloch-Pitts</b> <b>neurons,</b> and spiky networks, evolving also the parameterization of the spiking dynamics. We show how {{the ability of a}} spiky neuron to accumulate voltage is utilized for the delayed response processing. We further confront new questions about the nature of "spikiness", showing that the presence of spiking dynamics does not necessarily transcribe to actual spikiness in the network, and identify two distinct properties of spiking dynamics in embedded agents. Our main result is that in tasks possessing memory-dependent dynamics, neurocontrollers with spiking neurons can be less complex and easier to evolve than neurocontrollers employing <b>McCulloch-Pitts</b> <b>neurons.</b> Additionally the combined utilization of spiking dynamics with incremental evolution can lead to the successful evolution of response behavior over very long delay periods...|$|R
5000|$|Every pair {{of units}} i and j in a Hopfield network have a {{connection}} that {{is described by}} the connectivity weight [...] In this sense, the Hopfield network can be formally described as a complete undirected graph , where [...] {{is a set of}} <b>McCulloch-Pitts</b> <b>neurons</b> and [...] is a function that links pairs of nodes to a real value, the connectivity weight.|$|R
50|$|Quantum {{neural network}} {{research}} {{is still in}} its infancy, and a conglomeration of proposals and ideas of varying scope and mathematical rigor have been put forward. Most of them are based on the idea of replacing classical binary or <b>McCulloch-Pitts</b> <b>neurons</b> with a qubit (which can be called a “quron”), resulting in neural units that can be in a superposition of the state ‘firing’ and ‘resting’.|$|R
40|$|AbstractLearning real weights for a <b>McCulloch-Pitts</b> <b>neuron</b> is {{equivalent}} to linear programming and can hence be done in polynomial time. Efficient local learning algorithms such as Perceptron Learning further guarantee convergence in finite time. The problem becomes considerably harder, however, when it is sought to learn binary weights; this {{is equivalent}} to integer programming which {{is known to be}} NP-complete. A family of probabilistic algorithms which learn binary weights for a <b>McCulloch-Pitts</b> <b>neuron</b> with inputs constrained to be binary is proposed here, the target functions being majority functions of a set literals. These algorithms have low computational demands and are essentially local in character. Rapid (average-case) quadratic rates of convergence for the algorithm are predicted analytically and confirmed through computer simulations when the number of examples is within capacity. It is also shown that, for the functions under consideration, Preceptron Learning converges rapidly (but to an, in general, non-binary solution weight vector) ...|$|E
40|$|With methods {{developed}} in a prior article on the chemical kinetic implementation of a <b>McCulloch-Pitts</b> <b>neuron,</b> connections among neurons, logic gates, and a clocking mechanism, we construct examples of clocked finite-state machines. These machines include a binary decoder, a binary adder, and a stack memory. An example of {{the operation of the}} binary adder is given, and the chemical concentrations corresponding to the state of each chemical neuron are followed in time. Using these methods, we can, in principle, construct a universal Turing machine, and these chemical networks inherit the halting proble...|$|E
40|$|We study a modular neuron {{alternative}} to the <b>McCulloch-Pitts</b> <b>neuron</b> that arises naturally in analog devices in which the neuron inputs are represented as coherent oscillatory wave signals. Although the modular neuron can compute $XOR$ at the one neuron level, it is still characterized by the same Vapnik-Chervonenkis dimension as the standard neuron. We give the formulas needed for constructing networks using the new neuron and training them using back-propagation. A numerical study of the modular neuron on two data sets is presented, which demonstrates that the new neuron performs at least {{as well as the}} standard neuron. Comment: 19 pages, 7 figures(not included) available upon request, postscript fil...|$|E
50|$|The {{feedforward}} {{neural network}} {{was the first and}} simplest type. In this network the information moves only from the input layer directly through any hidden layers to the output layer without cycles/loops. Feedforward networks can be constructed with various types of units, such as binary <b>McCulloch-Pitts</b> <b>neurons,</b> the simplest of which is the perceptron. Continuous neurons, frequently with sigmoidal activation, are used in the context of backpropagation.|$|R
50|$|Depending on the {{specific}} model used they may be called a semi-linear unit, Nv neuron, binary neuron, linear threshold function, or <b>McCulloch-Pitts</b> (MCP) <b>neuron.</b>|$|R
40|$|Abstract. A {{parallel}} algorithm {{for finding}} Ramsey numbers is presented where analog/digital CMOS circuits for the hysteresis <b>McCulloch-Pitts</b> binary <b>neuron</b> are described. The hysteresis <b>McCulloch-Pitts</b> binary <b>neuron</b> model {{is used in}} order to suppress the oscillatory behaviors of neural dynamics so that the convergence time is shortened. The proposed algorithm using the hysteresis McCultoch-Pitts binary neuron found five Ramsey numbers. The analog CMOS sigmoid circuit with variable gain controls has been fabricated and tested using the SAC data acquisition board interfaced with a TMS 32010 processor. Hysteresis can be implemented by the positive feedback in the fabricated CMOS analog circuit. 1...|$|R
40|$|Abstract. A {{hysteresis}} binary <b>McCulloch-Pitts</b> <b>neuron</b> {{model is}} proposed {{in order to}} suppress the complicated oscillatory behaviors of neural dynamics. The artificial hysteresis binary neural network is used for scheduling time-multiplex crossbar switches in order to demon-strate the effects of hysteresis. Time-multiplex crossbar switching systems must control traffic on demand such that packet blocking probability and packet waiting time are minimized. The system using n x n processing elements solves an n x n crossbar-control problem with O(1) time, while the best existing parallel algorithm requires O(n) time. The hysteresis binary neural net-work maximizes the throughput of packets through a crossbar switch. The solution quality of our system does not degrade with the problem size...|$|E
40|$|Recent {{research}} has indicated that Neural Networks may offer powerful and effective solutions to certain classes of problems which von Neumann machines do not handle effectively. These problems include the areas of visual and speech recognition. This thesis describes the basic principles behind neural networks and neural network implementation. The main aim of the thesis is to investigate the implementation of neural networks in silicon integrated technology. To this end the design of a CMOS VLSI test neural network is described and the problems of VLSI implementation discussed. The neural network design utilizes switched techniques to implement the basic <b>McCulloch-Pitts</b> <b>neuron.</b> A test chip has been fabricated using this circuit technique and tested successfully. Comparisons between the switched capacitor technique and other neural network implementations are also discussed...|$|E
40|$|It has {{remained}} unknown whether one can in principle carry out reliable digital computations with networks of biologically realistic models for neurons. This article presents rigorous constructions for simulating in real-time arbitrary given boolean circuits and fi-nite automata with arbitrarily high reliability by networks of noisy spiking neurons. In addition {{we show that}} {{with the help of}} "shunting inhibition" even networks of very unreliable spiking neurons can simulate in real-time any <b>McCulloch-Pitts</b> <b>neuron</b> (or "threshold gate"), and therefore any multilayer perceptron (or "threshold circuit") in a reliable manner. These constructions provide a possible explana-tion for the fact that biological neural systems can carry out quite complex computations within 100 msec. It turns out that the assumption that these constructions require about the shape of the EPSP's and the behaviour of the noise are surprisingly weak. ...|$|E
40|$|Abstract This article {{investigates the}} {{evolution}} of autonomous agents that perform a memory-dependent counting task. Two types of neurocontrollers are evolved: networks of <b>McCulloch-Pitts</b> <b>neurons,</b> and spiking integrate-and-fire networks. The results demonstrate {{the superiority of the}} spiky model in evolutionary success and network simplicity. The combination of spiking dynamics with incremental evolution leads to the successful evolution of agents counting over very long periods. Analysis of the evolved networks unravels the counting mechanism and demonstrates how the spiking dynamics are utilized. Using new measures of spikiness we find that even in agents with spiking dynamics, these are usually truly utilized only when they are really needed, that is, in the evolved subnetwork responsible for counting...|$|R
40|$|This article initiates a {{rigorous}} theoretical {{analysis of the}} computational power of circuits that employ modules for computing winner-take-all. Computational models that involve competitive stages have so far been neglected in computational complexity theory, although they are widely used in computational brain models, artificial neural networks, and analog VLSI. Our theoretical analysis shows that winner-take-all is a surprisingly powerful computational module in comparison with threshold gates (= <b>McCulloch-Pitts</b> <b>neurons)</b> and sigmoidal gates. We prove an optimal quadratic lower bound for computing winner-take-all in any feedforward circuit consisting of threshold gates. In addition we show that arbitrary continuous functions can be approximated by circuits employing a single soft winner-take-all gate as their only nonlinear operation. Ou...|$|R
40|$|We {{investigate}} the computational {{power of a}} model for a spiking neuron in the Boolean domain by comparing it with traditional neuron models such as threshold gates (or <b>McCulloch-Pitts</b> <b>neurons)</b> and sigma-pi units (or polynomial threshold gates). In particular, we estimate the number of gates required to simulate a spiking neuron by a disjunction of threshold gates and we establish tight bounds for this threshold number. Furthermore, we analyze the degree of the polynomials that a sigma-pi unit must use for the simulation of a spiking neuron. We show that this degree cannot be bounded by any fixed value. Our results give evidence that the use of continuous time as a computational resource endows single-cell models with substantially larger computational capabilities...|$|R
40|$|The {{discrete}} {{dynamics of}} a dissipative nonlinear model neuron with self-interaction is discussed. For units with self-excitatory connection hysteresis effects, i. e. bistability over certain parameter domains, are observed. Numerical simulations demonstrate that self-inhibitory units with non-zero decay rates exhibit complex dynamics including period doubling routes to chaos. These units {{may be used}} as basic elements for networks with higherorder information processing capabilities. appeared in: Physica D, 104, 205 - 211, 1997. 1 1 Introduction Biological neurons exhibit a large variety of dynamical behaviors even when they are not embedded in a network. This type of dynamics is captured by biologically inspired neuron models like the Hodgkin-Huxley [1] or the FitzHugh-Nagumo equations [2], [3]. On the other hand, formal neurons used in artificial neural networks like the <b>McCulloch-Pitts</b> <b>neuron</b> or the graded response neurons [4] have only trivial, i. e. convergent dynamics as s [...] ...|$|E
40|$|We {{propose a}} {{reversible}} reaction mechanism {{with a single}} stationary state in which certain concentrations assume either high or low values dependent on the concentration of a catalyst. The properties of this mechanism are those of a <b>McCulloch-Pitts</b> <b>neuron.</b> We suggest a mechanism of interneuronal connections in which the stationary state of a chemical neuron {{is determined by the}} state of other neurons in a homogeneous chemical system and is thus a "hardware" chemical implementation of neural networks. Specific connections are determined for the construction of logic gates: AND, NOR, etc. Neural networks may be constructed in which the flow of time is continuous and computations are achieved by the attainment of a stationary state of the entire chemical reaction system, or in which the flow of time is discretized by an oscillatory reaction. In another article, we will give a chemical implementation of finite state machines and stack memories, with which in principle the construction of a universal Turing machine is possible...|$|E
40|$|This paper {{introduces}} a higher-order neural network called the Binary Pi-sigma Network (BPSN), {{which is a}} feedforward network with a single "hidden" layer and product units in the output layer. As training proceeds, the BPSN forms an internal representation of the conjunctive normal form expression corresponding to the Boolean function to be learned. This enables the network to have a regular structure and to exhibit fast learning. We formally prove that the BPSN can realize any Boolean function. Simulation {{results show that the}} network converges very fast and in a stable manner. Introduction Since the introduction of the <b>McCulloch-Pitts</b> <b>neuron,</b> there have been many efforts to model logical expressions using neural networks [1]. The McCullochPitts neuron {{can be used as a}} threshold logic unit and hence can implement an AND or OR function of its inputs. Negation of the inputs also allows the NOT function. Thus, any Boolean expression can be realized using either the disjunctive normal [...] ...|$|E
40|$|We {{consider}} a randomly diluted higher-order network with noise, consisting of <b>McCulloch-Pitts</b> <b>neurons</b> that interact by Hebbian-type connections. For this model, exact dynamical equations are derived and solved for both parallel and random sequential updating algorithms. For parallel dynamics, {{we find a}} rich spectrum of different behaviors including static retrieving and oscillatory and chaotic phenomena {{in different parts of}} the parameter space. The bifurcation parameters include first- and second-order neuronal interaction coefficients and a rescaled noise level, which represents the combined effects of the random synaptic dilution, interference between stored patterns, and additional background noise. We show that a marked difference in terms of the occurrence of oscillations or chaos exists between neural networks with parallel and random sequential dynamics...|$|R
40|$|In this master's thesis {{we compare}} and combine {{learning}} algorithms for two different architectures. The {{first is the}} standard Multi-layered Feed-forward Network, {{the second is the}} BP-SOM architecture. The BP-SOM architecture combines a Multi-layered Feed-forward Network with one or more Self-Organizing Maps. The second part of this Master's Thesis looks at rule-extraction in the BP-SOM architecture. We show how the Self-Organizing Map can be used to extract rules, and how these rules show us the information contained in the Self-Organizing map. This Master's Thesis was written at the Computer Science Department of Leiden University. My supervisors were Dr. I. G. Sprinkhuizen-Kuyper and Dr. W. A. Kosters. Contents 1 Introduction 4 I Multi-layered Feed-forward Networks 6 2 Multi-Layered Feed-Forward Networks 7 2. 1 <b>McCulloch-Pitts</b> <b>Neurons......................</b> 8 3 Back-Propagation 9 3. 1 Errors................................. 11 3. 2 [...] ...|$|R
40|$|A {{supervised}} learning feedforward neural net, which combines {{the advantages of}} Neocognitron and Perceptron, is introduced. The net topology is constrained to local connections between layers. A weight-sharing technique is used similiar to Fukushima's Neocognitron network model. Thus objects deformed in shape, shifted in position or variing in size can be recognized without any prepocessing or normalisation. In contrast ot the Neocognitron layers containing simple cells (S-cells), complex cells (C-cells) and inhibitory cells (V-cells) are subsituted by layers built up from <b>McCulloch-Pitts</b> <b>neurons</b> with sigmoidal nonlinearity. Additionally, {{in order to train}} each layer independently the {{supervised learning}} algorithm of the Neocognitron is replaced by the Least-Mean-Square learning rule commonly used for Perceptrons. In a first application the network has been successfully trained to recognize handwritten numerals of different size and position within a 24 by 24 pixel image...|$|R
40|$|A single <b>McCulloch-Pitts</b> <b>neuron,</b> that is, {{the simple}} {{perceptron}} is studied, with {{focus on the}} region beyond storage capacity. It is shown that Parisi's hierarchical ansatz for the overlap matrix of the synaptic couplings with so called continuous replica symmetry breaking is a solution, and as we propose it is the exact one, to the equilibrium problem. We describe {{some of the most}} salient features of the theory and give results about the low temperature region. In particular, the basics of the Parisi technique and the way to calculate thermodynamical expectation values is explained. We have numerically extremized the replica free energy functional for some parameter settings, and thus obtained the order parameter function, i. e., the probability distribution of overlaps. That enabled us to evaluate the probability density of the local stability parameter. We also performed a simulation and found a local stability density closer to the theoretical curve than previous numerical results were. Comment: 15 Latex pages (with REVTeX 3. 0), 4 figures (1 latex using eepic, 3 ps using psfig), accepted for Journal of Statistical Physics, Proceedings of the NATO-ARW, Brussels, 1999, in honor of G. Nicolis, "Nonlinear Science: Dynamics & Stochasticity...|$|E
40|$|The maximum {{absolute}} value of integral weights sufficient to represent any linearly separable Boolean function is investigated. It is shown that upper bounds exhibited by Muroga (1971) for rational weights satisfying the so-called normalized system of inequalities also hold for integral weights. This improves previously known upper bounds. 1 Introduction The logical {{behavior of a}} <b>McCulloch-Pitts</b> <b>neuron</b> with n binary inputs is described by a linearly separable Boolean function f : f 0; 1 g n ! f 0; 1 g (see e. g. [1]). It is represented by a real vector (w 1; : : :; w n; t) of weights such that for all x 2 f 0; 1 g n w 1 x 1 + ΔΔΔ + w n x n t iff f(x 1 : : : x n) = 1 : (1) Function f is also called threshold function and t {{is known as the}} threshold. It is a wellknown fact that the possibly infinite information contained in the real components of the weight vector can be made finite without restricting the class of representable functions by requiring all weights to be [...] ...|$|E
40|$|The {{perceptron}} (also {{referred to}} as <b>McCulloch-Pitts</b> <b>neuron,</b> or linear threshold gate) is commonly used as a simplified model for the discrimi-nation and learning capability of a biological neuron. Criteria that tell us when a perceptron can implement (or learn to implement) all possible di-chotomies over a given set of input patterns are well known, but only for the idealized case, where one assumes that {{the sign of a}} synaptic weight can be switched during learning. We present in this letter an analysis of the classification capability of the biologically more realistic model of a sign-constrained perceptron, where the signs of synaptic weights re-main fixed during learning (which is the case for most types of biological synapses). In particular, the VC-dimension of sign-constrained percep-trons is determined, and a necessary and sufficient criterion is provided that tells us when all 2 m dichotomies over a given set ofm patterns can be learned by a sign-constrained perceptron. We also show that uniformity of L 1 norms of input patterns is a sufficient condition for full represen-tation power in the case where all weights are required to be nonnega-tive. Finally, we exhibit cases where the sign constraint of a perceptron drastically reduces its classification capability. Our theoretical analysis is complemented by computer simulations, which demonstrate in par-ticular that sparse input patterns improve the classification capability of sign-constrained perceptrons. ...|$|E
40|$|It is {{well known}} that (<b>McCulloch-Pitts)</b> <b>neurons</b> are {{efficiently}} trainable to learn an unknown halfspace from examples, using linear-programming methods. We want to analyze how the learning performance degrades when the representational power of the neuron is overstrained, i. e., if more complex concepts than just halfspaces are allowed. We show that the problem of learning a probably almost optimal weight vector for a neuron is so difficult that the minimum error cannot even be approximated to within a constant factor in polynomial time (unless RP = NP); we obtain the same hardness result for several variants of this problem. We considerably strengthen these negative results for neurons with binary weights 0 or 1. We also show that neither heuristical learning nor learning by sigmoidal neurons with a constant reject rate is efficiently possible (unless RP = NP) ...|$|R
40|$|Spiky neural {{networks}} {{are widely used}} in neural modeling, due to their biological relevance and high computational power. In this paper we investigate the usage of spiking dynamics in embedded artificial {{neural networks}}, that serve as a control mechanism for evolved autonomous agents performing a delayed-response task. The synaptic weights and spiking dynamics are evolved using a genetic algorithm. We compare evolved spiky networks with evolved McCulloch-Pitts networks, while confronting new questions {{about the nature of}} "spikiness" and its contribution to the neurocontroller's processing. On the behavioral level, we show that in a memory-dependent task, network solutions that incorporate spiking dynamics can be less complex and easier to evolve than neurocontrollers involving <b>McCulloch-Pitts</b> <b>neurons.</b> On the functional level, we identify and rigorously characterize two distinct properties of spiking dynamics in embedded agents: spikiness evident influence and spikiness functional contribution...|$|R
40|$|Abstract—We {{propose a}} simple retinomorphic neural network that {{consists}} of photoreceptors generating nonuni-form outputs for common optical inputs with random off-sets, an ensemble of noisyMcCulloch-Pitts neurons {{each of which has}} random threshold values, local synaptic connec-tions between the photoreceptors and the neurons with vari-able receptive fields (RFs), output cells, and local synaptic connections between the neurons and output cells. Through numerical simulations, we observed stochastic resonance among the proposed pixels. We calculated correlation val-ues between the optical inputs and the outputs {{as a function of the}} RF size and intensities of the random components in photoreceptors and the <b>McCulloch-Pitts</b> <b>neurons,</b> and then found nonzero optimal RF sizes as well as optimal noise in-tensities of the neurons under the nonidentical photorecep-tors. This implies that SR-based night-scope image sensors with an array of nonidentical photosensors would be devel-oped with less efforts to implement uniform pixel devices. 1...|$|R
40|$|ABSTRACT. We {{propose to}} study the {{dynamics}} ofMcCulloch-Pitts’neural network and general Boolean networks at the most fundamental level. We propose to study Hopfield’s theorem for chaotic iteration and its application to pattern recognition. We propose to furnish amathematical model of Hebb’s postulate of learning. We propose {{to study the}} Jacobian problem for Boolean networks. 1. Introductory remarks In 1943, the neurophysiologist W. McCulloch and aMathematician W. Pitts[6] claimed that the brain could be modeled as anetwork of logical operations such as and, or, not, and so forth. It had been a revolutionary idea at the time, and had proved to be immensely influential. McCulloch-Pitts model was the first example of what now call aneural network. It was the first attempt to understand mental activity {{as a form of}} information processing-an insight that provided the inspiration for artificial intelligence and cognitive psychology. McCulloch-Pitts model was the first indication that anetwork of very simple logic gates could perform exceedingly complex computation-an insight that was soon incorporated into the general theory of computing machines. McCulloch-Pitts’paper influenced von Neumann to use idealized switch-delay elements derived from the <b>McCulloch-Pitts</b> <b>neuron</b> {{in the construction of the}} EDVAC(Electronic Discrete Variable Automatic Computer) (see Aspray and Burks[l]). In this note, we propose to study the dynamics of McCulloch-Pitts’neural network and general Boolean networks at the most fundamental level. 2. McCulloch-Pitts neural network and its dynamics In anervous system, each neuron exhibits an impulse of one electric state, called action potential. The state of each neuron can be distinguished by the existence and nonexistence of an action potential. Suppos...|$|E
40|$|In {{this article}} the {{framework}} for Parisi's spontaneous replica symmetry breaking is reviewed, and subsequently applied to {{the example of the}} statistical mechanical description of the storage properties of a <b>McCulloch-Pitts</b> <b>neuron.</b> The technical details are reviewed extensively, with regard to the wide range of systems where the method may be applied. Parisi's partial differential equation and related differential equations are discussed, and a Green function technique introduced for the calculation of replica averages, the key to determining the averages of physical quantities. The ensuing graph rules involve only tree graphs, as appropriate for a mean-field-like model. The lowest order Ward-Takahashi identity is recovered analytically and is shown to lead to the Goldstone modes in continuous replica symmetry breaking phases. The need for a replica symmetry breaking theory in the storage problem of the neuron has arisen due to the thermodynamical instability of formerly given solutions. Variational forms for the neuron's free energy are derived in terms of the order parameter function x(q), for different prior distribution of synapses. Analytically in the high temperature limit and numerically in generic cases various phases are identified, among them one similar to the Parisi phase in the Sherrington-Kirkpatrick model. Extensive quantities like the error per pattern change slightly with respect to the known unstable solutions, but there is {{a significant difference in the}} distribution of non-extensive quantities like the synaptic overlaps and the pattern storage stability parameter. A simulation result is also reviewed and compared to the prediction of the theory. Comment: 103 Latex pages (with REVTeX 3. 0), including 15 figures (ps, epsi, eepic), accepted for Physics Report...|$|E
40|$|Circuits {{composed}} of threshold gates (<b>McCulloch-Pitts</b> <b>neurons,</b> or per-ceptrons) are simplifiedmodels of neural circuits with the advantage {{that they are}} theoretically more tractable than their biological counterparts. However, when such threshold circuits are designed to perform a specific computational task, they usually differ in one important respect from computations in the brain: they require very high activity. On average ev-ery second threshold gate fires (sets a 1 as output) during a computation. By contrast, the activity of neurons in the brain is much sparser, with only about 1 % of neurons firing. This mismatch between threshold and neuronal circuits {{is due to the}} particular complexitymeasures (circuit size and circuit depth) that have beenminimized in previous threshold circuit constructions. In this letter, we investigate a new complexity measure for threshold circuits, energy complexity,whoseminimization yields compu-tations with sparse activity. We prove that all computations by threshold circuits of polynomial size with entropy O(log n) can be restructured s...|$|R
40|$|Artificial neural {{networks}} of sigmoidal and <b>McCulloch-Pitts</b> <b>neurons</b> have found increasing favour in industry research {{because of their}} most attractive features, abstraction of hardly accessible knowledge and generalisation from distorted sensor signals. In recent years experimental evidence has been accumulating to suggest that biological {{neural networks}}, which communicate through spikes, use the timing of these spikes to encode and compute information in a more efficient way. In this paper it is presented a simplified version of a Self Organizing neural architecture based on Spiking Neurons and it is shown that this computational architectures have a greater potential to unveil embedded information in tool wear monitoring data sets and that smaller structures, compared to sigmoidal neural networks, are needed to capture and model the inherent complexity embedded in tool wear monitoring data. Additional, it is proposed a robust methodology based on tool wear estimation historical evolution that should improve estimation and predictive capabilities of Tool Condition Monitoring systems...|$|R
40|$|Computations by spiking neurons are {{performed}} using {{the timing of}} action potentials. We investigate the computational power of a simple model for such a spiking neuron in the Boolean domain by comparing it with traditional neuron models such as threshold gates (or <b>McCulloch-Pitts</b> <b>neurons)</b> and sigma-pi units (or polynomial threshold gates). In particular, we estimate the number of gates required to simulate a spiking neuron by a disjunction of threshold gates and we establish tight bounds for this threshold number. Furthermore, we analyze the degree of the polynomials that a sigma-pi unit must use for the simulation of a spiking neuron. We show that this degree cannot be bounded by any fixed value. Our results give evidence {{that the use of}} continuous time as a computational resource endows single-cell models with substantially larger computational capabilities. 1 Introduction Biological neurons communicate by sending spikes among themselves. A spike is a discrete event in continuous ti [...] ...|$|R
40|$|Quantum walks {{have been}} shown to be {{fruitful}} tools in analysing the dynamic properties of quantum systems. This article proposes to use quantum walks as an approach to Quantum Neural Networks (QNNs). QNNs replace binary <b>McCulloch-Pitts</b> <b>neurons</b> with a qubit in order to use the advantages of quantum computing in neural networks. A quantum walk on the firing states of such a QNN is supposed to simulate central properties of the dynamics of classical neural networks, such as associative memory. It is shown that a biased discrete Hadamard walk derived from the updating process of a biological neuron does not lead to a unitary walk. However, a Stochastic Quantum Walk between the global firing states of a QNN can be constructed and it is shown that it contains the feature of associative memory. The quantum contribution to the walk accounts for a modest speed-up in some regimes. Comment: 9 pages, 7 figure...|$|R
