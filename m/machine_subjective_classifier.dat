0|23|Public
40|$|This paper {{presents}} {{our work}} in the simplified Chinese opinion analysis task in NTCIR 7. For identifying the subjective sentences, the domain adaptation technique was applied in our method, so that the data in NTCIR 6 {{can be used for}} training <b>subjective</b> <b>classifier.</b> The evaluation results proves that the method proposed in this paper is effective. In extracting the opinion holder, we used the CRF model, which was combined with manual designed heuristics rules. For CRF model we not only extracted part-of-speech features, semantic class features, contextual features, but also some dependency features through parsing analysis. The evaluation results prove that the proposed method is effective for extracting opinion holders...|$|R
30|$|This {{qualitative}} inquiry {{has investigated}} poker <b>machine</b> gamblers’ <b>subjective</b> views on their gambling behaviour and cognition. Although recurrent themes form {{a basis for}} analysis, idiosyncratic opinions, which do not reflect commonly encountered views, may {{be present in the}} analysis. Nonetheless, the aim of this research was to understand subjective views in the context of rich background information such as gambling histories and trajectories.|$|R
40|$|Information {{extraction}} (IE) {{systems are}} prone to false hits {{for a variety of}} reasons and we observed that many of these false hits occur in sentences that contain subjective language (e. g., opinions, emotions, and sentiments). Motivated by these observations, we explore the idea of using subjectivity analysis to improve the precision of information extraction systems. In this paper, we describe an IE system that uses a <b>subjective</b> sentence <b>classifier</b> to filter its extractions. We experimented with several different strategies for using the subjectivity classifications, including an aggressive strategy that discards all extractions found in subjective sentences and more complex strategies that selectively discard extractions. We evaluated the performance of these different approaches on the MUC- 4 terrorism data set. We found that indiscriminately filtering extractions from subjective sentences was overly aggressive, but more selective filtering strategies improved IE precision with minimal recall loss...|$|R
40|$|A point machine’s gap is an {{important}} indication of its healthy status. An edge detection algorithm is proposed to measure and calculate a point machine’s gap from the gap image captured by CCD plane arrays. This algorithm integrates adaptive wavelet-based image denoising, locally adaptive image binarization, and mathematical morphology technologies. The adaptive wavelet-based image denoising obtains not only an optimal denoising threshold, but also unblurred edges. Locally adaptive image binarization {{has the advantage of}} overcoming the local intensity variation in gap images. Mathematical morphology may suppress speckle spots caused by reflective metal surfaces in point <b>machines.</b> The <b>subjective</b> and objective evaluations of the proposed method are presented by using point machine gap images from a railway corporation in China. The performance between the proposed method and conventional edge detection methods has also been compared, and the result shows that the former outperforms the latter...|$|R
40|$|The thesis set out {{to solve}} a {{practical}} problem of sentiment analysis of Twitter posts about news. The thesis has made contributions is data collection of tweets about news, empirical study {{of the role of}} context in sentiment analysis of tweets about news, and best feature selection. Test data was collected by a bootstrapping approach where some tweets that contain some number of words from the news headline were used to extract links and to obtain more tweets about news from Twitter. The test data was manually inspected and annotated for its sentiment to see if context plays role in determining sentiment of a tweet. Uni-gram+bi-gram was selected as a feature that captures two important features of the data: uni-grams provides better coverage of the data, and bi-grams capture sentiment expression patterns. The thesis has shown that tweets about news can be automatically collected and successfully analyzed for their sentiment. Multinomial Naive Bayes classifier using uni-gram+bi-gram presence was found to give the highest accuracy, an accuracy of 87. 78 % for a three-classed classification and an accuracy of 90. 79 % for the two-classed (<b>subjective,</b> objective) <b>classifier</b> derived from the three-classed classifier. The accuracies of the classifier on both three-classed and two-classed classification is impressive and can be applied for practical applications dealing with sentiment analysis of tweets. ...|$|R
40|$|Affective {{computation}} generally {{focuses on}} the informatics of affect: structuring, formalizing, and representing emotion as informational units. We propose instead an enigmatics of affect, a critical technical practice that respects the rich and undefinable complexities of human affective experience. Our interactive installation, the Influencing Machine, allows users to explore a dynamic landscape of emotionally expressive sound and child-like drawings, using a tangible, intuitive input device that supports open-ended engagement. The Influencing <b>Machine</b> bridges the <b>subjective</b> experience of the user and the necessary objective rationality of the underlying code. It functions as a cultural probe, reflecting and challenging users {{to reflect on the}} cultural meaning of affective computation. Keywords affective computation, affective interaction, believable agents, agen...|$|R
40|$|One of the {{features}} of the digital ecosystem is the integration of human cognition and socio-economic themes into the process of new product development (NPD). In a socio-economic theme-based NPD, ranking a set of product prototypes that have been designed always requires the participation of multiple evaluators and consideration of multiple evaluation criteria. Using the well-being theme-based garment NPD as a background, this paper first presents a fuzzy hierarchical criteria group decision-making (FHCGDM) method which can effectively calculate final ranking results through fusing all assessment data from human beings and machines. It then presents a garment NPD comprehensive evaluation model with hierarchical criteria under the well-being theme through identifying a set of marketing tactics from a consumer acceptance survey. It further provides an establishment process for an NPD evaluation model under the digital ecosystem framework. Finally, a garment NPD case study further demonstrates the proposed well-being NPD comprehensive evaluation model and the FHCGDM method. The advantages of the proposed evaluation method include successfully handling criteria in a hierarchical structure, automatically processing both objective measurements from <b>machines</b> and <b>subjective</b> assessments from human evaluators, and using the most suitable type of fuzzy numbers to describe linguistic terms...|$|R
40|$|A {{series of}} {{imitation}} games involving 3 -participant (simultaneous comparison of two hidden entities) and 2 -participant (direct interrogation of a hidden entity) were conducted at Bletchley Park on the 100 th anniversary of Alan Turing’s birth: 23 June 2012. From the ongoing analysis of over 150 games involving (expert and non-expert, males and females, adults and child) judges, machines and hidden humans (foils for the machines), we present six particular conversations {{that took place}} between human judges and a hidden entity that produced unexpected results. From this sample we focus on features of Turing’s machine intelligence test that the mathematician/code breaker did not consider in his examination for <b>machine</b> thinking: the <b>subjective</b> nature of attributing intelligence to another mind...|$|R
40|$|Abstract — In {{this paper}} we explore the use of {{electrical}} biosignals measured on scalp and corresponding to mental relaxation and concentration tasks {{in order to control}} an object in a video game. To evaluate the requirements of such a system in terms of sensors and signal processing we compare two designs. The first one uses only one scalp electroencephalographic (EEG) electrode and the power in the alpha frequency band. The second one uses sixteen scalp EEG electrodes and machinelearning methods. The role of muscular activity is also evaluated using five electrodes positioned on the face and the neck. Results show that the first design enabled 70 % of the participants to successfully control the game, whereas 100 % of the participants managed to do it with the second design based on <b>machine</b> learning. <b>Subjective</b> questionnaires confirm these results: users globally felt to have control in both designs, with an increased control with the second one. Offline analysis of face and neck muscle activity shows that this activity could also be used to distinguish between relaxation and concentration tasks. Results suggest that the combination of muscular and brain activity could improve performance of this kind of system. They also suggest that muscular activity has probably been recorded by EEG electrodes. I...|$|R
40|$|In {{software}} engineering, associating each reported defect with {{a category}} allows, {{among many other}} things, for the appropriate allocation of resources. Although this classification task can be automated using standard machine learning techniques, the categorization of defects for model training requires expert knowledge, which is not always available. To circumvent this dependency, we propose to apply the learning from crowds paradigm, where training categories are obtained from multiple non-expert annotators (and so may be incomplete, noisy or erroneous) and, dealing with this <b>subjective</b> class information, <b>classifiers</b> are efficiently learnt. To illustrate our proposal, we present two real applications of the IBM’s orthogonal defect classification working on the issue tracking systems from two different real domains. Bayesian network classifiers learnt using two state-of-the-art methodologies from data labeled by a crowd of annotators are used to predict the category (impact) of reported software defects. The considered methodologies show enhanced performance regarding the straightforward solution (majority voting) according to different metrics. This shows the possibilities of using non-expert knowledge aggregation techniques when expert knowledge is unavailable...|$|R
40|$|In {{this paper}} we explore the use of {{electrical}} biosignals measured on scalp and corresponding to mental relaxation and concentration tasks {{in order to control}} an object in a video game. To evaluate the requirements of such a system in terms of sensors and signal processing we compare two designs. The first one uses only one scalp electroencephalographic (EEG) electrode and the power in the alpha frequency band. The second one uses sixteen scalp EEG electrodes and machine learning methods. The role of muscular activity is also evaluated using five electrodes positioned on the face and the neck. Results show that the first design enabled 70 % of the participants to successfully control the game, whereas 100 % of the participants managed to do it with the second design based on <b>machine</b> learning. <b>Subjective</b> questionnaires confirm these results: users globally felt to have control in both designs, with an increased feeling of control in the second one. Offline analysis of face and neck muscle activity shows that this activity could also be used to distinguish between relaxation and concentration tasks. Results suggest that the combination of muscular and brain activity could improve performance of this kind of system. They also suggest that muscular activity has probably been recorded by EEG electrodes. Comment: International Conference of the IEEE EMBS (2011...|$|R
40|$|Este artigo analisa três documentários subjetivos. São eles: Rua de mão dupla (Cao Guimarães, 2002), Jogo de cena (Eduardo Coutinho, 2006) e Serious games (Harum Farocki, 2010). A análise busca compreender a singularidade dessas obras por meio das formulações de jogos de linguagem, de Ludwig Wittgenstein, e máquinas subjetivas, de Felix Guatarri. Pretende-se, com esses conceitos, contribuir para parte do debate sobre as relações existentes entre documentário contemporâneo, subjetividade, linguagem e ética no documentário. This paper {{analyzes}} three subjective documentaries: Rua de mão dupla (Cao Guimarães, 2002), Jogo de cena (Eduardo Coutinho, 2006) and Serious games (Harum Farocki, 2010). The analysis aims at {{understanding the}} singularity of such works {{by means of}} the language games formulations, by Ludwig Wittgenstein, and of the <b>subjective</b> <b>machines,</b> by Felix Guatarri. By using such notions, one intends to contribute to the debate on the existing relations between contemporary documentaries, subjectivity, language and ethics in documentaries...|$|R
40|$|Dental {{laboratory}} is {{a laboratory}} for produced dentures. In production process, dental laboratory workers are exposed to various dangers: both chemical and physical dangers. One of the physical dangers is mechanical vibration. grinder vibration is used during labor for more than 4 hours or even 8 hours a day, so {{it is possible to}} cause health disorder in physic, physiology and performance. Therefore, prevention is needed to reduce complaints that may arise on the workers. This research aims at studying the incidence of subjective health complaints (hand arm vibration syndrome) due to dental grinder vibration on dental laboratory workers in Abadi Dental Laboratory Surabaya. This research was a descriptive observational research in which the data were take using cross sectional approach. The subject of the research was 18 laboratory workers, drawn from the total population. Variables in the research were individual characteristics (age, sex, work hour and work duration), the intensity of grinder vibration exposure, and subjective complaints. The result of the subjective complaints (hand arm vibration syndrome) of the dental laboratory workers in Abadi Dental Laboratory showed that from 18 person, there were 13 person (72 %) who had a subjective complaints. The factors which related to subjective complaints were age (p= 0, 002) and worked duration (p= 0, 002). Unrelated factors of subjective complaints were sex (p= 0, 278) and work hour (p= 0, 114). It is recommended to do stretching for 5 minutes in every 30 minutes during the use of grinder machine, reducing an overtime work, and treating the grinder <b>machine.</b> Keywords: <b>subjective</b> complaints (hand arm vibration syndrome), Dental laboratory, and grinder vibration exposur...|$|R
30|$|Building {{models to}} {{classify}} data {{according to a}} predefined coding scheme is an essential task in data science, especially in research involving <b>machine</b> classification of <b>subjective</b> matter. Building a model to predict house prices can use historical and factual data. Building a model to predict emotions, beliefs or sentiments (such as hateful remarks) in electronic text requires an additional step to establish a ‘gold standard’ that is suitable for training and testing supervised machine classifiers, {{and is based on}} human agreement on which class a piece of text belongs to. Commonly, this is obtained by sampling from a larger data set and employing human annotators to label each data point (tweet) according to a coding frame ([16, 17]). The coding frame serves as a set of categories or classes into which each data point can be classified. Computationally crowdsourcing human annotations is now becoming popular, and Web services such as CrowdFlower or the Amazon Mechanical Turk provide programmatic application programming interfaces (APIs) through which researchers can automatically upload a data set, coding frame, and set of instructions for annotation. The results of the annotation tasks can then be split into training and testing data sets for machine learning.|$|R
40|$|This paper {{presents}} a {{design of a}} non-player character (AI) for promoting balancedness in use of body segments when engaging in full-body motion gaming. In our experiment, we settle a battle between the proposed AI and a player by using FightingICE, a fighting game platform for AI development. A middleware called UKI is used to allow the player to control the game by using body motion instead of the keyboard and mouse. During gameplay, the proposed AI analyze health states of the player; it determines its next action by predicting how each candidate action, recommended by a Monte-Carlo tree search algorithm, will induce the player to move, and how the player's health tends to be affected. Our result demonstrates successful improvement in balancedness in use of body segments on 4 out of 5 subjects. Comment: A revised version of our paper for 2017 AAAI Spring Symposium Series (Well-Being AI: From <b>Machine</b> Learning to <b>Subjective</b> Oriented Computing), San Francisco,USA, Mar. 27 - 29, 2017. Revised contents, due to our correction of (8), are highlighted in red. Many apologies, but {{the effectiveness of the}} proposed method/approach in the paper still hold...|$|R
40|$|This {{article will}} discuss the {{technology}} of SWSS and then present and compare these three systems. It {{is divided into three}} parts; the first introduces SWSS in terms of progressive examples. Part two compares the three systems using the same two instrument/score examples written in each of them. The final section presents informal benchmark tests of the systems run on two different hardware platforms [...] -a Sun Microsystems SPARCstation- 2 IPX and a Next Computer Inc. TurboCube <b>machine</b> [...] -and <b>subjective</b> comments on various features of the languages and programming environments of state-of-the-art SWSS software. This author's connection with this topic is that of extensive experience with several different SWSS systems over the last 15 years, starting with MUS 10 and including all three compared here: Csound (in the form of Music- 11 initially) at the CMRS studio in Salzburg (Pope 1982); cmusic in the CARL environment at PCS/Cadmus computers in Munich (Pope 1986); and more recently a combination of cmix, Csound, and various vocoder software packages with user interfaces written in Smalltalk- 80 at the CCRMA Center for Computer Research in Music and Acoustics at Stanford University (Pope 1992) ...|$|R
40|$|Recently {{there has}} been an {{increase}} of interest in implementing a new set of home appliances, known as Smart Appliances that integrate Information Technologies, the Internet of Things and the ability of communicating with other devices. While Smart Appliances are characterized as an important milestone on the path to the Smart Grid, by being able to automatically schedule their loads according to a tariff or reflecting the power that is generated using renewable sources, there is not a clear understanding on the impact that the behavior of such devices will have in the comfort levels of users, when they shift their working periods to earlier, or later than, a preset time. Given these considerations, in this work we analyse the results of an assessment survey carried out to a group of Home Appliance users regarding their habits when dealing with these <b>machines</b> and the <b>subjective</b> impact in quality caused by either finishing its programs before or after the time limit set by the user. The results of this work are expected to be used as input for the evaluation of load scheduling algorithms running in energy management systems. © 2014 Springer International Publishing...|$|R
40|$|The {{purpose of}} this paper is to report efforts towards the {{construction}} of a  model for urban spatial dynamics simulation, based on multi-agents and space.   The underlying idea is to have urban space producers and consumers operating  in a two-layer, two-circuit model. The first layer holds urban space and its  successive transformations, a second layer contains agents related to space, the  first circuit simulates space production, and a second one simulates space  consumption. Relationship between layers is represented as objective spatial  features that agents are submitted to (the <b>machine)</b> and <b>subjective</b> meanings  agents attach to each spatial feature (the ghost). While space works always in  the same way, meanings vary according to each agentis background and  context. Relationships between circuits are represented by means of a market  game in which producers try to maximize their profits by gambling with their  risks, whereas consumers try to foresee the spatial distribution of local  externalities that maximizes their utilities and investments. Urban Spatial  Features are captured through centrality and land use patterns, every single  agentis action leads to changes in both patterns. Producersi profit is a function  of built form location. Consumersi local externalities are concerned basically  with present and future services. The model iteration is twofold: first it  generates and allocates a number of built forms within a previously determined  spatial system (a cellular matrix, for example), and second it allocates users to  built forms. Population of users have its social profile and growth rate  externally determined. Built form allocation is decided on the basis of a  combination of profit Xrisk perspectives. Usersi locational choice is supported  by accessibility to services and present/future neighbourhood profile. Built  form allocation works as parameter for usersi locational assessment, whereas  users choices are used as parameters for developers. The model tends to  adjust itself, in terms of quantities and types of built forms to be erected,  although through a market lag of some iterations. Allocations are always made  through weighted draws, so that mutations (non deterministic allocations) do  occur. ...|$|R
30|$|The {{automatic}} DJ system {{presented in}} this paper generates a seamless music mix from a library of Drum and Bass songs. It uses beat tracking, downbeat tracking and structural segmentation algorithms to analyze the structural properties of the music. Evaluated on a hold-out test set of 220 songs, a beat tracking accuracy of 98.2 %, a downbeat tracking accuracy of 98.1 % and a segmentation accuracy of 94.3 % is achieved, meaning that in total 90.9 % of the songs get structurally correct annotations. This efficacy is made possible in part by adopting a genre-specific approach. The extracted structural knowledge is used in a rule-based framework, inspired by DJing best practices, to create transitions between songs. To the best of the authors’ knowledge, this system is the first that combines all basic DJing best practices in one integrated solution. Musical consistency between songs is ensured by employing a harmonic mixing technique and a custom style descriptor feature. The track selection algorithm also optimizes the rhythmic onset pattern similarity in the overlapped music segments and avoids vocal clashes by means of a support vector <b>machine</b> <b>classifier.</b> A <b>subjective</b> evaluation in which 18 Drum and Bass fans participated indicates that the style descriptor feature for song selection drastically improves the mix quality. The influence of the ODF similarity matching is less clear. Overall, the automatic DJ system is able to seamlessly join together Drum and Bass songs and create an enjoyable mix.|$|R
40|$|Maxillary canine {{impaction}} {{and root}} resorption of adjacent lateral incisors {{is a well-known}} and relatively common phenomenon in orthodontic practice. The risk of canine impaction and associated root resorption is relatively high in patients needing orthodontic treatment. Early diagnosisremains a critical problem, {{and there are no}} straightforward clinical clues concerning the treatment planning, prediction, or prevention as regards canine impaction and the associated root resorption of the adjacentlateral incisor. The introduction of CBCT in dentomaxillofacial radiology has created new diagnostic challenges, including some potential opportunities for evaluating the impacted canines. With this new technology, it was obligatory to investigate and determine if this new information provides another and better way of diagnostic approach, treatment planning, improved treatment outcome and early prediction. Consequently, the present thesis attempted to link the radiological observations to diagnostic, therapeutic and further preventive measures. The main objectives were to develop an improved diagnostic methodology that would enable optimal diagnosis, treatment, and early prediction. The diagnostic accuracy for the detection of simulated canine-induced external root resorption lesions in maxillary lateral incisors was compared between conventional panoramic radiographic imaging and CBCT systems in vitro. The results show that the performance of CBCT imaging was significantly better than that of panoramic radiography. After the CBCT had been proven to perform better than conventional 2 D panoramic images, the question to be answered remained whether {{there is a difference between}} CBCT <b>machines.</b> Therefore, the <b>subjective</b> image quality of the different CBCT systems in vitro was determined. The results suggest that the CBCT radiographic method is moresensitive and that high image quality is important when trying to detect root resorption. There were no significant differences between the CBCT systems in the detection of root resorption. The findings of a previous in vitro study were confirmed in vivo: the results show that CBCT was a reliable diagnostic method for the localization of impacted canines and the detection of root resorption of adjacent lateral incisors. The treatment of impacted canines usually requires a multidisciplinary approachand is associated with prolonged treatment times, increased costs, complexity, and a risk of failure and complications. The diagnostic consequences of using 2 D or 3 D radiography may have a significant impact on therapeutic interventions. Therefore, this aspect was investigated by comparing the orthodontic treatment planning between conventional and CBCT-based planning. Similarly, the influence on pre-surgical treatment planningwas also studied. The findings of those studies show no statistically significant difference in treatment planning or in pre-surgical treatmentplanning between the use of conventional and CBCT sets. The only significant difference was related to the precise localization of impacted canine but had no effect in the treatment plans. However, a high confidencelevel was observed in CBCT treatment based planning. The influence of CBCT on the treatment methods used and treatment outcomes achieved for orthodontically treated patients was then investigated. No difference was found either in the number of treatment methods or treatment outcome. Inthe last part of this thesis, a method for early prediction and prevention of canine impaction and root resorption was explored. Early prediction based on radiographic factors might clinically stimulate the application of preventive measures. Therefore, a prediction model for root resorption on panoramic radiographs was constructed. The early prediction of root resorption might reduce complications before, during and/or after treatment because additional clinical measure can be taken. The prediction of root resorption was carried out on the basis of available panoramicradiographs because they are routinely present in orthodontic records. Furthermore, the diagnosis of root resorption based on panoramic radiographs is difficult, and the final prediction model for root resorption could be a helpful tool in justifying the need for additional CBCT examination. The purpose was to reduce the need for additional radiation exposure, certainly in cases where the probability of the presence of root resorption is low. Finally, the prediction model for canine impaction was established on the basis of CBCT with a high level of accuracy, which mayhelp orthodontists in identifying the probability of impaction, which, in turn, is helpful in defining the optimal intervention method. status: publishe...|$|R
40|$|The work "Modelling {{the human}} {{approach}} to world" {{is part of}} the mutlidisciplinary field of informatics which deals with production and use of information systems in enterprises and other communities. It contributes to its development in the area of human-machine relationship and communicating information. At present information systems (IS) are used for collecting, distributing, storing, processing and conveying information. Their components are hardware (machines) and software. Without information systems today's work with information wouldn't be possible. People remain in spite of intensive implementation of information technologies the main element for companies' operation. Information technologies help man, cannot replace him however. To understand the role of information technologies {{it is important to be}} aware of the difference between man and machine (computer) information processing from which we can deduce the implications for business operation. The computer is only able to work with a reality model prepared by a human being. Reality is in information systems represented by means of signs. Even though we can understand the information system as a tool similar to classical tools (typing machine, filing cabinet etc.), there is a difference, because these tools don't exist as physical objects, but only as signs. The symbolic nature of information systems didn't interest researchers in the past very much, but now it increases. The first reason can be found in the massive implementation of information and communication technologies (ICT), which calls for intelligibility of their representation; the second reason can be found in the fact that computers are used as media with functions similar to textbooks, letters, newspapers, telephones, films, where the importance of semiotics has already been recognized. The advancement of hardware requires from software developers abilities similar to professional artists. The third reason lies in the fact that production processes are more and more controlled by computers. Cooperating people must rely on symbols mediated by computers (Andersen, 1997). IS are more complex and important is their influence on people on many levels and through many means. Structuralism may be of some help as it studied similar phenomena in the first half of the 20 th century. In the first part of my work I described the differences between human and computer information processing, in the second part I analysed the modelling theories which take into account the human approach and at the end I deduced general conclusions for the relationship between man and computer in information system and suggested some recommendations on how to improve their relationship and make human understanding and use of information in information system easier. One of the researchers who studied the difference between man and machine was Hubert Dreyfus. He came to the conclusion that there is a principle difference between man and machine. He identified some typical situations where man and machine differ: Rule following, bodily existence, situations, pattern recognition etc. Dreyfus' analysis will be supplemented with the analysis of public space, intentionality and language. I don't want to present a complete description of human approach to world, I rather sketch some key points to clarify the difference between man and machine. From the analysis of man and machine two conclusions emerge: on the one hand proposal how to analyse and propose information system which would better connect subjective and objective aspects; on the other hand importance of IS integration into the business culture and the whole context of human work. As a suitable approach appears in compliance with P. Ricoeur structuralism and its analysis of narrativity. In the area of IS development there are four so called socio-technical theories which try to incorporate some principles of human approach to world into their modelling procedures and balance the superiority of technical view on the human computer interaction. The main common feature of these theories lies in the emphasis on the importance of human factor for organisations (they consider organisations social systems). The role of technical equipment is seen in the support of human activities. They are called Language Action Perspective, specifically methodology DEMO, Organisational Semiotics, Theory of Organised Activity and Human Interaction Management. DEMO methodology focuses on communication which is analysed by means of four axioms based on the language act theory. The theory differentiates between locution, illocution and perlocution. Locution deals with the proposition's content, illocution is related to the intention which we communicate the proposition with and perlocution contains effects in the addressee. From these three perspectives production and communication acts are analysed. Organizational semiotics deals with the semiotic aspect of human communication. It uses a six-stage semiotic ladder with physical, empirical, syntactic, semantic, pragmatic and social level. On all these levels communication and its signs are analysed. On the first three levels machines can be used, on the next three levels the human work with information is carried out and machines don't help there much. Organisational semiotics emphasizes the social level where organisational, social and cultural norms play an important role. Theory of organized activity is based on the theory of units which the group has in common. The theory divides the world on actors and things. For the activity to start both these elements must be present. Typical features of actors are interest and responsibility. The human interaction management tries to modify the proposal and functioning of information systems in order to respect the principles of human work. The basic argument of human interaction management is that most processes where people are involved is in instant change which cannot be governed by rules independent of the process participants. Flexibility is necessary for success. K. H. Broninski, the author of Management of Human Interaction, calls for support for mental work a transformation of information into knowledge. He proposes a procedure structuring work. It consist of five parts: research, evaluation, analysis, constrain, task. Another piece of knowledge of the Human interaction management is that a big proportion of human work has small concrete results, and cannot be quantified. However time spent on researching, comparing, evaluating, generally information processing and their transformation into knowledge, is a substantial part of a worker cooperating with colleagues and other people. It is better for employees to have supportive rather than directive leadership, as people are individuals. People do things in different times, in different ways, on the basis of communication with others, according to the state of resources and their mood. They do what they consider most appropriate in given situation. Continual process change must be possible and it is necessary to support it. Human activities are creative, exploratory and loosely structured. The socio-technical theories neither explicitly address the difference between man and machine nor try to unify their perspectives organically. However they show some content elements which are important for human beings. When considering the human approach to world we cannot be limited to a set of perceptions where man and machine differ. We should consider their arrangement which allows for new ways of understanding reality. The synthesis of the elements can be carried out only when the elements are linked together into a structure, e. g. a plot. Another problem lies in the necessity to consider time. Progressive development and its modalities constitute integral part of a homogenous stylistic form. Two rationalities are at work here; the one understands the world in its unity mediated by a form, best of all and most generally by a narrative; the other systemizes the first understanding and transforms it onto a syntactic level. The modelling techniques use the second syntactic rationality; we can reach it only in time, however. Stories have many advantages in comparison to other ways of conveying information: aims, causes, chances, agents are unified in time and space into a plot. Originally different elements are unified in the plot. They seem accidental; however they gain necessity in the plot. Other important aspects are source of information, narrator, rhythm, repetition and focalization. These elements can be separated on three levels: text, plot, fabula. They concentrate on different elements and their relations. We will use the results of work of the structuralist J. Greimas and his theory of actants suggesting which role can be considered in narrative information conveying. His semiotic square is a method of understanding context elements. Analysis of the elements of narrative which are the results of structuralist research cannot substitute the narrative understanding; we can only approximate it and explain it in further details. We encounter a creative act which cannot be reduced on rules and formalized. That is why a creative approach of the author is necessary in the development of IS. The conclusion achieved in the analysis of differences between human and machine that human understanding and behaviour is not completely formalizable was found in our attempt to use the structuralist approach on design and functioning of information systems. The interconnection of objective world of <b>machines</b> with the <b>subjective</b> human understanding is possible in a story created by a creative human being able to ensure organic and natural unity...|$|R

