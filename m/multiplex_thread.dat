0|19|Public
40|$|FreeBSD has {{historically}} {{had less than}} ideal support for multi-threaded application programming. At present, there are two threading libraries available. libc r is entirely invisible to the kernel, and <b>multiplexes</b> <b>threads</b> within a single process. The linuxthreads port, which creates a separate process for each thread via rfork(), plus one thread to handle thread synchronization, relies on the kernel scheduler to <b>multiplex</b> <b>threads</b> onto the available processors. Both approaches have scaling problems. libc r does {{not take advantage of}} multiple processors and cannot avoid blocking when doing I/O on fast devices. The linuxthreads port requires one process per thread, and thus puts strain on the kernel scheduler, as well as requiring signicant kernel resources. In addition, thread switching can only be as fast as the kernel's process switching. This paper summarizes various methods of implementing threads for application programming, then goes on to describe a new threading architecture for FreeBSD, based on what are called kernel-scheduled entities, or scheduler activations. ...|$|R
40|$|We {{describe}} an {{implementation of a}} threads library that provides extremely lightweight threads within a single UNIX process while allowing fully concurrent access to system resources. The threads are lightweight enough {{so that they can}} be created quickly, there can be thousands present, and synchronization can be accomplished rapidly. These goals are achieved by providing user <b>threads</b> which <b>multiplex</b> on a pool of kernel-supported threads of control. This pool is managed by the library and will automatically grow or shrink as required to ensure that the process will make progress while not using an excessive amount of kernel resources. The programmer can also tune the relationship between threads and kernel supported threads of control. This paper focuses on scheduling and synchronizing user threads, and their interaction with UNIX signals in a <b>multiplexing</b> <b>threads</b> library. Introduction This paper describes a threads library implementation that provides extremely lightweight threads w [...] ...|$|R
40|$|AbstractIn recent years, {{studies have}} shown that {{expression}} profiling of carefully chosen intermediary gene sets, comprising approximately 10 to 100 genes, can convey the most relevant information compared to much more complex whole-genome studies. In this paper, we present a novel method suitable for expression profiling of moderate gene sets in a large number of samples. The assay implements the parallel amplification features of the trinucleotide threading technique (TnT), which encompasses linear transcript-based DNA thread formation in conjunction with exponential <b>multiplexed</b> <b>thread</b> amplification. The amplifications bestow the method with high sensitivity. The TnT procedure together with thread detection, relying on thread-specific primer extension followed by hybridization to universal tag arrays, allows for three distinction levels, thus offering high specificity. Additionally, the assay is easily automated and flexible. A gene set, comprising 18 protein epitope signature tags from the Swedish Human Protein Atlas program, was analyzed with the TnT-based approach and the data were compared with those generated by both real-time PCR and genome-wide cDNA arrays, with the highest correlation observed between TnT and real-time PCR. Taken together, expression profiling with trinucleotide threading represents a reliable approach for studies of intermediary gene sets...|$|R
40|$|This paper {{describes}} Strings, a {{high performance}} distributed shared memory system designed for such SMP clusters. The distinguishing {{feature of this}} system {{is the use of}} a fully multi-threaded runtime system, using kernel level threads. Strings allows multiple application threads to be run on each node in a cluster. Since most modern UNIX systems can <b>multiplex</b> these <b>threads</b> on kernel level light weight processes, applications written using Strings can exploit multiple processors on a SMP machine. This paper describes some of the architectural details of the system and illustrates the performance improvements with benchmark programs from the SPLASH- 2 suite, some computational kernels as well as a full fledged applicatio...|$|R
40|$|ABSTRACT: Lateral-flow immunochromatographic assays are low-cost, simple-to-use, {{rapid tests}} for point-of-care {{screening}} of infectious diseases, drugs of abuse, and pregnancy. However, lateral flow assays {{are generally not}} quantitative, give a yes/no answer, and lack <b>multiplexing.</b> <b>Threads</b> have recently been proposed as a support for transporting and mixing liquids in lateral-flow immunochromatographic assays, but their use for quantitative high-sensitivity immunoassays {{has yet to be}} demonstrated. Here, we introduce the immunochromatographic assay on thread (ICAT) in a cartridge format that is suitable for multiplexing. The ICAT is a sandwich assay performed on a cotton thread knotted to a nylon fiber bundle, both of which are precoated with recognition antibodies against one target analyte. Upon sample application, the assay results become visible to the eye within a few minutes and are quantified using a flatbed scanner. Assay conditions were optimized, the binding curves for C-reactive protein (CRP) in buffer and diluted serum were established and a limit of detection of 377 pM was obtained. The possibility of multiplexing was demonstrated using three knotted threads coated with antibodies against CRP, osteopontin, and leptin proteins. The performance of the ICAT was compared with that of the paper-based and conventional assays. The results suggest that thread is a suitable support for making low-cost, sensitive, simple-to-use, and multiplexed diagnostic tests. The lateral flow immunoassay, a type of sandwich assay,relies on a pair of antibodies to recognize two independent epitopes of a protein, and therefore it can achieve high specificity. 1 A typical lateral flow assay strip is composed o...|$|R
50|$|The {{programming}} language used in LabVIEW, named G, is a dataflow {{programming language}}. Execution {{is determined by}} the structure of a graphical block diagram (the LabVIEW-source code) on which the programmer connects different function-nodes by drawing wires. These wires propagate variables and any node can execute as soon as all its input data become available. Since this might be the case for multiple nodes simultaneously, G can execute inherently in parallel. Multi-processing and multi-threading hardware is exploited automatically by the built-in scheduler, which <b>multiplexes</b> multiple OS <b>threads</b> over the nodes ready for execution.|$|R
40|$|Multiple {{lightweight}} processes or threads {{have multiple}} stacks, and a thread context switch moves execution from one stack to another. On the SPARC 1 architecture, {{parts of a}} thread's stack can be cached in register windows while the thread is running. The cached data must be flushed to memory when the thread is suspended. Doing the flushing both efficiently and correctly can be tricky. This document discusses {{the implementation of a}} non-preemptive user-space threads package under SunOS 2. 1 Introduction Lightweight processes executing in a single address space are called threads [Birrell 89]. Conceptually, each of the threads of control can run independently and concurrently, and, since they share one address space, they can share data. An implementation can run a large number of threads on a small number of processors by <b>multiplexing</b> the <b>threads</b> onto the processors. There are two alternatives for threads. Systems such as Mach [Young et al. 87] and Topaz [Thacker et al. 88] have [...] ...|$|R
40|$|This paper {{introduces}} Strings, a {{high performance}} distributed shared memory system designed for clusters of symmetrical multiprocessors (SMPs). The distinguishing {{feature of this}} system {{is the use of}} a fully multi-threaded runtime system, written using POSIX threads. Strings also allows multiple application threads to be run on each node in a cluster. Since most modern UNIX systems can <b>multiplex</b> these <b>threads</b> on kernel level light weight processes, applications written using Strings can use all the processors in a SMP machine. This paper describes some of the architectural details of the system and analyzes the performance improvements with two example programs and a few benchmark programs from the SPLASH- 2 suite. 1. Introduction Though current microprocessors are getting faster at a very rapid rate, there are still some very large and complex problems that can only be solved by using multiple cooperating processors. These problems include the so-called Grand Challenge Problems, such [...] ...|$|R
40|$|In {{a shared}} memory multiprocessor, a thread {{may have an}} {{affinity}} to a processor because of the data remaining in the processor's cache from a previous dispatch. We show that two basic problems should be addressed in a Unix-like system to exploit cache affinity for improved performance: First, the limitation of the Unix dispatcher model ("processor seeking a thread"); Second, pseudo-affinity caused by low-cost waiting techniques used in a threads package such as C Threads. We demonstrate that the affinity scheduling is most effective when used in a threads package that supports <b>multiplexing</b> of user <b>threads</b> on kernel threads...|$|R
40|$|Helper {{threading}} is {{a technology}} to accelerate a program by exploiting a processor’s multithreading capability to run “assist ” threads. Previous experiments on hyper-threaded processors have demonstrated significant speedups by using helper threads to prefetch hard-to-predict delinquent data accesses. In order to apply this technique to processors {{that do not}} have built-in hardware support for multithreading, we introduce virtual multithreading (VMT), a novel form of switch-on-event user-level multithreading, capable of fly-weight <b>multiplexing</b> of event-driven <b>thread</b> executions on a single processor without additional operating system support. The compiler {{plays a key role in}} minimizing synchronization cost by judiciously partitioning register usage among the user-level threads. The VMT approach make...|$|R
40|$|The {{increasing}} {{prevalence of}} multicore, multiprocessor commodity hardware calls for server software architectures that are cycle-efficient on individual cores and can maximise concurrency across an entire machine. In {{order to achieve}} both ends this dissertation advocates stage architectures that put software concurrency foremost and aggressive CPU scheduling that exploits the common structure and runtime behaviour of CPU-intensive servers. For these servers user-level scheduling policies that <b>multiplex</b> one kernel <b>thread</b> per physical core can outperform those that utilise pools of worker threads per stage on CPU-intensive workloads. Boosting the hardware efficiency of severs in userspace means a single machine can handle more users without tuning, operating system modifications, or better hardware. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
30|$|The native {{system is}} a single address space shared among all processes. The hosted system, which is the one of {{interest}} {{for the purpose of this}} paper, has an address space per UNIX command, shared among multiple Go processes. Go processes (called goroutines by the creators of the language) are lightweight user level <b>threads</b> <b>multiplexed</b> among multiple UNIX processes. In Clive, each Go process has a Clive process context. A context includes a name space that maps path prefixes to file trees, a path for the current working directory, a set of named I/O channels, and a set of environment variables. When a process is created, it uses the parent’s context. But it is also feasible to spawn new processes that have their own contexts.|$|R
40|$|The popular M-to-N <b>thread</b> {{scheduling}} model <b>multiplexes</b> many user-level <b>threads</b> on top {{of fewer}} kernel-level threads. While this model {{is designed to be}} scalable and efficient without excessive resource consumption, we isolate several elementary examples under which the M-to-N model exhibits highly nonintuitive performance. We use a runtime performance monitor for multithreaded programs which we have developed, ThreadMon, to explain the causes of the unexpected results. We conclude that the complexity and nondeterminism exported to the programmer make performance tuning to the intricate M-to-N model extremely difficult. Moreover, we show that the insulation of user-level scheduling from kernel-level scheduling can have undesirable side-effects. 1 Introduction Since the approval of the POSIX threads standard [6], multithreaded programming has experienced an explosion of interest. Many applications have appreciated performance gains through multithreading; the popular Netscape web browse [...] ...|$|R
40|$|Tango Lite is a {{software-based}} multiprocessor simulation environment {{useful in}} the study and design of both parallel architectures and the software for these machines. Tango Lite runs on MIPS-based uniprocessors and provides a multiprocessor environment through the <b>multiplexing</b> of lightweight <b>threads</b> onto this single context machine. These threads differentiate Tango Lite from Tango, its predecessor, and have led to greatly improved performance. Tango Lite also achieves efficiency by allowing the user to specify the level of detail appropriate for the simulation requirements. 3 of 21 1 Introduction With the proliferation of multiprocessors, the need for accurate simulations has become a necessity for programmers and architects alike. New multiprocessor machines are significantly more complex than uniprocessors, with a multitude of possible configurations. An ideal development environment would allow real-world applications to run on a number of differently configured machines to d [...] ...|$|R
40|$|This paper {{describes}} Strings, a multi-threaded DSM {{developed by}} us. The distinguishing feature of Strings {{is that it}} incorporates Posix 1. c <b>threads</b> <b>multiplexed</b> on kernel light-weight processes for better performance. The kernel can schedule multiple threads across multiple processors using these lightweight processes. Thus, Strings is designed to exploit data parallelism at the application level and task parallelism at the DSM system level. We show how using multiple kernel threads can improve the performance even {{in the presence of}} false sharing, using matrix multiplication as a case-study. We also show the performance results with benchmark programs from the SPLASH- 2 suite [17]. Though similar work has been demonstrated with SoftFLASH [18], our implementation is completely in user space and thus more portable. Some other researach has studied the effect of clustering in SMPs suing simulations [19]. We have shown results from runs on an actual network of SMP...|$|R
40|$|Hardware {{multithreading}} {{is becoming}} a generally applied technique {{in the next generation}} of microprocessors. Several multithreaded processors are announced by industry or already into production in the areas of high-performance microprocessors, media, and network processors. A multithreaded processor is able to pursue two or more threads of control in parallel within the processor pipeline. The contexts of two or more threads of control are often stored in separate on-chip register sets. Unused instruction slots, which arise from latencies during the pipelined execution of single-threaded programs by a contemporary microprocessor, are filled by instructions of other threads within a multithreaded processor. The execution units are <b>multiplexed</b> between the <b>thread</b> contexts that are loaded in the register sets. Underutilization of a superscalar processor due to missing instruction-level parallelism can be overcome by simultaneous multithreading, where a processor can issue multiple instructions from multiple threads each cycle. Simultaneous multithreaded processors combine the multithreading technique with a wide-issu...|$|R
40|$|Due to clock {{frequency}} limitations, performing more computer operations in parallel {{is now being}} more widely explored for increasing total execution performance. Several varieties of parallelism are available, with the easiest and most attractive being thread-level parallelism. Recently introduced microprocessors have provided capabilities such as: multiple <b>threads</b> <b>multiplexed</b> onto a single CPU (simultaneous multithreading or hyperthreading), multiple CPU cores on a single integrated circuit, multiple microprocessor integrated circuits placed on the same logic module or logic printed circuit board, or sometimes various combinations of all of these. Although these approaches increase the total computing throughput available, they do nothing for more serially dependent program workloads. Rather these approaches require workloads that can be highly or totally parallelizable. There are machine design approaches that attempt to address serial workloads through the extraction of instruction level parallelism. One such approach is that of Explicit Parallel Instruction Computing (or EPIC for short). This approach attempts to execute several instructions in parallel {{through the use of}} clever scheduling by its compiler. However, substantial performance increases using this architectural technique have not been achieved to date. Further, these approaches requir...|$|R
40|$|Abstract: In-memory {{database}} management systems {{have the potential}} to reduce the execution time of complex operational analytical queries to the order of seconds while executing business transactions in parallel. The main reasons for this increase of per-formance are massive intra-query parallelism on many-core CPUs and primary data storage in main memory instead of disks or SSDs. However, {{database management}} systems in enterprise scenarios typically run a mix of different applications and users, of varying importance, concurrently. As an example, interactive applications have a much higher response-time objective compared to periodic jobs producing daily re-ports and should be run with priority. In addition to strict prioritization, enforcing a fair share of database resources is desirable, if several users work on applications that share a database. Solutions for resource management based on priorities have been proposed for disk-based database management systems. They typically rely on multi-plexing threads on a number of processing units, which is unfavorable for in-memory databases on multi-cores, as single queries are executed in parallel and numerous context switches disrupt cache-conscious algorithms. Consequently, we propose an approach towards resource management based on a task-based query execution that avoids <b>thread</b> <b>multiplexing.</b> The basic idea is to calculate the allowed share of execu-tion time for each user based on the priorities of all users and adjust priorities of tasks of incoming queries to converge to this share. ...|$|R
40|$|Relentless CMOS scaling {{coupled with}} lower design {{tolerances}} is making ICs increasingly susceptible to transient faults, wear-out related permanent faults and process variations. Decreasing CMOS reliability implies that high-availability systems which were previously {{restricted to the}} domain of mainframe computers or specially designed fault-tolerant systems may be come important for the commodity market as well. In this thesis we tackle the problem of enabling efficient, low cost and configurable fault-tolerance using Chip Multiprocessors (CMPs). Our work studies architectural fault detection methods based on redundant execution, specifically focusing on “leader-follower” architectures. In such architectures redundant execution is performed on two cores/threads of a CMP. One thread acts as the leading thread while the other acts as the trailing thread. The leading thread assists {{the execution of the}} trailing thread by forwarding the results of its execution. These forwarded results are used as predictions in the trailing thread and help improve its performance. In this thesis, we introduce a new form of execution assistance called critical value forwarding. Critical value forwarding uses heuristics to identify instructions on the critical path of execution and forwards the results of these instructions to the trailing core. The advantage of critical value forwarding is that it provides much of the speed up obtained by forwarding all values {{at a fraction of the}} bandwidth cost. We propose two architectures to exploit the idea of critical value forwarding. The first of these operates the trailing core at lower voltage/frequency levels in order to provide energy-efficient redundant execution. In this context, we also introduce algorithms to dynamically adapt the voltage/frequency level of the trailing core based on program behavior. Our experimental evaluation shows that this proposal consumes only 1. 26 times the energy of a non-fault-tolerant baseline and has a mean performance overhead of about 1 %. We compare our proposal to two previous energy-efficient fault-tolerant CMP proposals and find that our proposal delivers higher energy-efficiency and lower performance degradation than both while providing a similar level of fault coverage. Our second proposal uses the idea of critical value forwarding to improve fault-tolerant CMP throughput. This is done by using coarse-grained multithreading to mul-tiplex trailing threads on a single core. Our evaluation shows that this architecture delivers 9 – 13 % higher throughput than previous proposals, including one configuration that uses simultaneous multithreading(SMT) to <b>multiplex</b> trailing <b>threads.</b> Since this proposal increases fault-tolerant CMP throughput by executing multiple threads on a single core, it comes at a modest cost in single-threaded performance, a mean slowdown between 11 – 14 %...|$|R

