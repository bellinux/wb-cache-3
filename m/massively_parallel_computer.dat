324|10000|Public
5|$|In {{the early}} 1970s, at the MIT Computer Science and Artificial Intelligence Laboratory, Marvin Minsky and Seymour Papert started {{developing}} {{what came to}} be known as the Society of Mind theory which views biological brain as <b>massively</b> <b>parallel</b> <b>computer.</b> In 1986, Minsky published The Society of Mind, which claims that “mind is formed from many little agents, each mindless by itself”. The theory attempts to explain how what we call intelligence could be a product of the interaction of non-intelligent parts. Minsky says that the biggest source of ideas about the theory came from his work in trying to create a machine that uses a robotic arm, a video camera, and a computer to build with children's blocks.|$|E
500|$|Similar models (which also view {{biological}} {{brain as}} <b>massively</b> <b>parallel</b> <b>computer,</b> i.e. {{the brain is}} made up of a constellation of independent or semi-independent agents) were also described by: ...|$|E
500|$|SIMD {{parallel}} computers {{can be traced}} back to the 1970s. The motivation behind early SIMD computers was to amortize the gate delay of the processor's control unit over multiple instructions. In 1964, Slotnick had proposed building a <b>massively</b> <b>parallel</b> <b>computer</b> for the Lawrence Livermore National Laboratory. His design was funded by the US Air Force, which was the earliest SIMD parallel-computing effort, ILLIAC IV. The key to its design was a fairly high parallelism, with up to 256processors, which allowed the machine to work on large datasets in what would later be known as vector processing. However, ILLIAC IV was called [...] "the most infamous of supercomputers", because the project was only one fourth completed, but took 11years and cost almost four times the original estimate. When it was finally ready to run its first real application in 1976, it was outperformed by existing commercial supercomputers such as the Cray-1.|$|E
50|$|Neural & <b>Massively</b> <b>Parallel</b> <b>Computers,</b> 1988.|$|R
5000|$|... 2003 David Kent IV, New Quantum Monte Carlo Algorithms to Efficiently Utilize <b>Massively</b> <b>Parallel</b> <b>Computers</b> ...|$|R
50|$|Furthermore, the {{equations}} of motion describe {{a process that is}} inherently serial, so there is little to be gained from using <b>massively</b> <b>parallel</b> <b>computers.</b>|$|R
5000|$|... #Subtitle level 2: Biological {{brain as}} <b>massively</b> <b>parallel</b> <b>computer</b> ...|$|E
5000|$|... the world's first commercially {{available}} <b>massively</b> <b>parallel</b> <b>computer,</b> the Distributed Array Processor (DAP), that first ran as an attached processor to the ICL 2980.|$|E
5000|$|Similar models (which also view {{biological}} {{brain as}} <b>massively</b> <b>parallel</b> <b>computer,</b> i.e. {{the brain is}} made up of a constellation of independent or semi-independent agents) were also described by: ...|$|E
50|$|In 1988, Corliss {{joined the}} NASA Goddard Space Flight Center's {{high-performance}} computing division. There he began using <b>massively</b> <b>parallel</b> <b>computers</b> (the Goodyear MPP and later MasPar MP-1) for cellular automata simulations of evolutionary systems.|$|R
40|$|General {{strategies}} are developed to optimize particle-cell-codes written in Fortran for RISC processors which {{are commonly used}} on <b>massively</b> <b>parallel</b> <b>computers.</b> These strategies include data reorganization to improve cache utilization and code reorganization to improve efficiency of arithmetic pipelines...|$|R
3000|$|For more {{efficient}} computation of seismic cycles, we execute simulations with H-matrices on <b>massively</b> <b>parallel</b> <b>computers.</b> In the parallelized simulation code, the total sub-faults {{are divided into}} groups depending {{on the number of}} usable CPUs. Then, the velocities V [...]...|$|R
50|$|In computing, remote direct {{memory access}} (RDMA) is a {{direct memory access}} from the memory of one {{computer}} into that of another without involving either one's operating system. This permits high-throughput, low-latency networking, which is especially useful in <b>massively</b> <b>parallel</b> <b>computer</b> clusters.|$|E
50|$|Goodyear MPP was {{an early}} {{implementation}} of a <b>massively</b> <b>parallel</b> <b>computer</b> architecture. MPP architectures are {{the second most common}} supercomputer implementations after clusters, as of November 2013. Examples of services and products which has a MPP implementation include Microsoft's Azure SQL Data Warehouse and Microsoft’s on premise data warehousing product, Parallel Data Warehouse (PDW), which runs on the Analytics Platform System (APS).|$|E
50|$|As {{well as its}} use {{in general}} network modeling, {{asymptotic}} throughput is used in modeling performance on <b>massively</b> <b>parallel</b> <b>computer</b> systems, where system operation is highly dependent on communication overhead, as well as processor performance. In these applications, asymptotic throughput is used in Xu and Hwang model (more general than Hockney's approach) which includes the number of processors, so that both the latency and the asymptotic throughput are functions {{of the number of}} processors.|$|E
40|$|Over {{the last}} three years, very {{significant}} advances {{have been made in}} refining the grid resolution of ocean models and in improving the physical and numerical treatments of ocean hydrodynamics. Some of these advances have occurred {{as a result of the}} successful transition of ocean models onto <b>massively</b> <b>parallel</b> <b>computers,</b> which has been led by Los Alamos investigators. Major progress has been made in simulating global ocean circulation and in understanding various ocean climatic aspects such as the effect of wind driving on heat and freshwater transports. These steps have demonstrated the capability to conduct realistic decadal to century ocean integrations at high resolution on <b>massively</b> <b>parallel</b> <b>computers.</b> U. S. Department of Energ...|$|R
40|$|This article {{deals with}} {{rationale}} and concepts of a programming model for massive parallelism. We mention the basic properties of <b>massively</b> <b>parallel</b> applications {{and develop a}} programming model for data parallelism on distributed-memory computers. Its key features are a suitable combination of homogeneity and heterogeneity aspects, an unified representation of data point configuration and interconnection schemes by explicit virtual data topologies, and various synchronization schemes and nondeterminisms. The outline of the linguistic representation and the abstract executional model are given. Keywords: architecture independence, data parallelism, data topologies, massive parallelism, programming model, SPMD. 1 Introduction A programming model for present and future <b>massively</b> <b>parallel</b> <b>computers</b> is to be developed. <b>Massively</b> <b>parallel</b> <b>computers</b> will certainly need to have distributed memory; in all other respects the programming model is to be architecture independent. Instead of att [...] ...|$|R
50|$|After the MPP was {{retired in}} 1991, it was {{donated to the}} Smithsonian Institution,and {{is now in the}} {{collection}} of the National Air and Space Museum's Steven F. Udvar-Hazy Center. It was succeeded at Goddard by MasPar MP-1 andCray T3D <b>massively</b> <b>parallel</b> <b>computers.</b>|$|R
5000|$|The BBN Butterfly was a <b>massively</b> <b>parallel</b> <b>computer</b> {{built by}} Bolt, Beranek and Newman in the 1980s. It {{was named for}} the [...] "butterfly" [...] {{multi-stage}} switching network around which it was built. Each machine had up to 512 CPUs, each with local memory, which could be connected to allow every CPU access to every other CPU's memory, although with a substantially greater latency (roughly 15:1) than for its own. The CPUs were commodity microprocessors. The memory address space was shared.|$|E
50|$|The Teramac was an {{experimental}} <b>massively</b> <b>parallel</b> <b>computer</b> designed by HP in the 1990s. The name reflected the project's vision {{to provide a}} programmable gate array system with capacity for a million gates running at a megahertz. Contrary to traditional systems, which are useless {{if there is one}} defect, Teramac used defective processors -- intentionally -- to demonstrate its defect-tolerant architecture. Even though the computer had 220,000 hardware defects, it was able to perform some tasks 100 times faster than a single-processor high-end workstation.|$|E
5000|$|The Cray Y-MP, also {{designed}} by Steve Chen, {{was released in}} 1988 as an improvement of the X-MP and could have eight vector processors at 167 MHz with a peak performance of 333 megaflops per processor. [...] In the late 1980s, Cray's experiment {{on the use of}} gallium arsenide semiconductors in the Cray-3 did not succeed. Cray began to work on a <b>massively</b> <b>parallel</b> <b>computer</b> in the early 1990s, but died in a car accident in 1996 before it could be completed. Cray Research did, however, produce such computers.|$|E
40|$|The {{performance}} of an interconnection network in a <b>massively</b> <b>parallel</b> architecture {{is subject to}} physical constraints whose impact needs to be re-evaluated from time to time. Fat-trees and low dimensional cubes have raised {{a great interest in}} the scientific community {{in the last few years}} and are emerging standards in the design of interconnection networks for <b>massively</b> <b>parallel</b> <b>computers...</b>|$|R
40|$|Artificial Intelligence {{has been}} the field of study for {{exploring}} the principles underlying thought, and utilizing their discovery to develop useful computers. Traditional AI models have been, consciously or subconsciously, optimized for available computing resources which has led AI in certain directions. The emergence of <b>massively</b> <b>parallel</b> <b>computers</b> liberates the way intelligence may be modeled. Although the AI community {{has yet to make}} a quantum leap, there are attempts {{to make use of the}} opportunities offered by <b>massively</b> <b>parallel</b> <b>computers,</b> such as memory-based reasoning, genetic algorithms, and other novel models. Even within the traditional AI approach, researchers have begun to realize that the needs for high performance computing and very large knowledge bases to develop intelligent systems requires <b>massively</b> <b>parallel</b> AI techniques. In this Computers and Thought Award lecture, I will argue that <b>massively</b> <b>parallel</b> artificial intelligence will add new dimensions to the ways that the AI goals are pursued, and demonstrate that <b>massively</b> <b>parallel</b> artificial intelligence is where AI meets the real world. 1...|$|R
40|$|Programming <b>massively</b> <b>parallel</b> <b>computers</b> {{seems to}} be at a stage {{analogous}} to the programming of early serial computers. Much of the programming is numeric and using relatively unstructured approaches. Furthermore, most of the programs are self-contained and, by today's standards, fairly modest in size. In the decades since the advent of serial programming, much has been learned about how to construct large programs. In particular, object-oriented programming techniques have dramatically increased programmer productivity and reduced program maintenance costs (including debugging time and enhancement complexity). How long will it take before <b>massively</b> <b>parallel</b> programming requires the same approaches that serial computer programmers have already discovered? This short paper describes the problems for which OO {{seems to be}} well suited. It looks at current O-O methodologies aimed at <b>massively</b> <b>parallel</b> <b>computers</b> and the shortcomings of these approaches. The technology to overcome some of th [...] ...|$|R
50|$|In {{the early}} 1970s, at the MIT Computer Science and Artificial Intelligence Laboratory, Marvin Minsky and Seymour Papert started {{developing}} {{what came to}} be known as the Society of Mind theory which views biological brain as <b>massively</b> <b>parallel</b> <b>computer.</b> In 1986, Minsky published The Society of Mind, which claims that “mind is formed from many little agents, each mindless by itself”. The theory attempts to explain how what we call intelligence could be a product of the interaction of non-intelligent parts. Minsky says that the biggest source of ideas about the theory came from his work in trying to create a machine that uses a robotic arm, a video camera, and a computer to build with children's blocks.|$|E
50|$|The ILLIAC IV {{was one of}} {{the first}} {{attempts}} to build a <b>massively</b> <b>parallel</b> <b>computer.</b> One of a series of research machines (the ILLIACs from the University of Illinois), the ILLIAC IV design featured fairly high parallelism with up to 256 processors, used to allow the machine to work on large data sets in what would later be known as vector processing. After several delays and redesigns, the computer was delivered to NASA's Ames Research Center at Moffett Airfield in Mountain View, California in 1971. After thorough testing and four years of NASA use, ILLIAC IV was connected to the ARPANet for distributed use in November 1975, becoming the first network-available supercomputer, beating Cray's Cray-1 by nearly 12 months.|$|E
50|$|Common {{modeling}} infrastructure {{refers to}} software libraries {{that can be}} shared across multiple institutions {{in order to increase}} software reuse and interoperability in complex modeling systems. Early initiatives were in the climate and weather domain, where software components representing distinct physical domains (for example, ocean or atmosphere) tended to be developed by domain specialists, often at different organizations. In order to create complete applications, these needed to be combined together, using for instance a general circulation model, that transfers data between different components. An additional challenge is that these models generally require supercomputers to run, to account for the collected data and for data analyses. Thus, it was important to provide an efficient <b>massively</b> <b>parallel</b> <b>computer</b> system, and the processing hardware and software, to account for all the different workloads and communication channels.|$|E
40|$|In {{large-scale}} scientific computing, linear sparse solver {{is one of}} {{the most}} time-consuming process. In GeoFEM, various types of preconditioned iterative method is implemented on <b>massively</b> <b>parallel</b> <b>computers.</b> It has been well-known that ILU(0) factorization is very effective preconditioning method for iterative solver. But it's also well-known that this method requires global data dependency and this is not the optimal way on <b>parallel</b> <b>computers</b> where locality is of utmost importance. In this paper, "Localized" ILU(0) preconditioning method has been implemented to various type of iterative solvers. This method provides data locality on each processor and good parallelization effect. Developed system performance has been also evaluated on workstation cluster with MPI. LINEAR SOLVERS IN GeoFEM In GeoFEM, preconditioned iterative method is implemented on <b>massively</b> <b>parallel</b> <b>computers</b> in order to solve large scale problems with more than 108 DOFs. GeoFEM solves both of symmetric [...] ...|$|R
40|$|This {{report is}} the first {{application}} study for the PROMOTER programming model. The model has been designed for data parallel applications that are to run on <b>massively</b> <b>parallel</b> <b>computers</b> with distributed memory. It is developed at GMD-FIRST in Berlin {{a member of the}} Japanese Real World Computing (RWC) Partnership...|$|R
40|$|The {{ultimate}} {{computers in}} our long-term future will deliver exaflops-scale performance (or greater) and will look {{very different from}} today’s micro-processors and <b>massively</b> <b>parallel</b> <b>computers.</b> Ironically, however, their alien structures and operational behavior can be inferred from the same technology trends driving development of today’s conventional computing systems...|$|R
50|$|In December 1999, IBM {{announced}} a US$100 million research initiative for a five-year {{effort to build}} a <b>massively</b> <b>parallel</b> <b>computer,</b> {{to be applied to}} the study of biomolecular phenomena such as protein folding. The project had two main goals: to advance our understanding of the mechanisms behind protein folding via large-scale simulation, and to explore novel ideas in massively parallel machine architecture and software. Major areas of investigation included: how to use this novel platform to effectively meet its scientific goals, how to make such massively parallel machines more usable, and how to achieve performance targets at a reasonable cost, through novel machine architectures. The initial design for Blue Gene was based on an early version of the Cyclops64 architecture, designed by Monty Denneau. The initial research and development work was pursued at IBM T.J. Watson Research Center and led by William R. Pulleyblank.|$|E
5000|$|SIMD {{parallel}} computers {{can be traced}} back to the 1970s. The motivation behind early SIMD computers was to amortize the gate delay of the processor's control unit over multiple instructions. In 1964, Slotnick had proposed building a <b>massively</b> <b>parallel</b> <b>computer</b> for the Lawrence Livermore National Laboratory. His design was funded by the US Air Force, which was the earliest SIMD parallel-computing effort, ILLIAC IV. The key to its design was a fairly high parallelism, with up to 256 processors, which allowed the machine to work on large datasets in what would later be known as vector processing. However, ILLIAC IV was called [...] "the most infamous of supercomputers", because the project was only one fourth completed, but took 11 years and cost almost four times the original estimate. When it was finally ready to run its first real application in 1976, it was outperformed by existing commercial supercomputers such as the Cray-1.|$|E
50|$|The only {{computer}} to seriously challenge the Cray-1's {{performance in the}} 1970s the single example of the ILLIAC IV. This machine was the first realized example of a true <b>massively</b> <b>parallel</b> <b>computer,</b> in which many processors worked together to solve different parts of a single larger problem. In contrast with the vector systems, which were designed to run a single stream of data as quickly as possible, in this concept, the computer instead feeds separate parts of the data to entirely different processors and then recombines the results. The ILLIAC's design was finalized in 1966 with 256 processors and offer speed up to 1 GFLOPS, compared to the 1970s Cray-1's peak of 250 MFLOPS. However, development problems led to only 64 processors being built, and the system could never operate faster than about 200 MFLOPS, while being much larger and {{more complex than the}} Cray. Another problem was that writing software for the system was difficult, and getting peak performance from {{it was a matter of}} serious effort.|$|E
40|$|Both <b>massively</b> <b>parallel</b> <b>computers</b> and {{clusters}} of workstations are considered promising platforms for numerical scientific computing. This paper describes the first distributed-memory {{implementation of the}} split-merge algorithm, an eigenvalue solver for symmetric tridiagonal matrices that uses Laguerre's iteration and exploits the separation property {{in order to create}} independent subtasks. Implementations of the splitmerge algorithm on both an nCUBE- 2 hypercube and a cluster of Sun Sparc- 10 workstations are described, with emphasis on load balancing, communication overhead, and interaction with other user processes. A performance study demonstrates the advantage of the new algorithm over a parallelization of the well-known bisection algorithm. A comparison of the performance of the nCUBE- 2 and cluster implementations supports the claim that workstation {{clusters of}}fer a cost-effective alternative to <b>massively</b> <b>parallel</b> <b>computers</b> for certain scientific applications...|$|R
40|$|Free-space optical {{interconnection}} {{is used to}} fashion a reconfigurable network. Since network reconfiguration is expensive compared to message transmission, we utilize latency hiding techniques to increase performance. For this network, we present and analyze broadcasting/multibroadcasting algorithms that utilizes latency hiding and reconfiguration in order to speed these operations. A combined total exchange algorithm has been proposed based {{on a combination of}} the direct, and standard exchange algorithms. Also, known algorithms for other collective communication primitives have been adapted to our network. Keywords: <b>Massively</b> <b>parallel</b> <b>computers,</b> collective communications, reconfigurable free-space optical interconnects. 1. This research was supported through grants from the Natural Sciences and Engineering Research Council of Canada. Technical Report ECE- 97 - 2 2 1. Introduction Message-based multicomputers, also called <b>massively</b> <b>parallel</b> <b>computers,</b> are composed of a large numb [...] ...|$|R
40|$|Two {{methods of}} multiple/large/foreign {{databases}} processing using <b>massively</b> <b>parallel</b> <b>computers</b> are described. The first method employs a Distributed Cache Subsystem (DCS) offering the total memory of parallel processing nodes {{as a very}} large read-only disk cache for other parallel processing nodes. The second method {{is based on a}} Database Slicing (DS) algorithm that provides logical partitioning of the entire database between the processing nodes. These methods were implemented using n-parallel PROLOG from Paralogic. Results of experiments on an nCUBE- 2 <b>parallel</b> <b>computer</b> are presented. INTRODUCTION It is well-known that Prolog language can be used efficiently as a 4 GL query language to access and process relational databases [1]. Furthermore the Prolog language is well suited to run on a <b>massively</b> <b>parallel</b> <b>computers.</b> In recent years a large amount of work has been done investigating parallel Prolog execution models [2]. Our interest is determining whether parallel Prolog running on [...] ...|$|R
