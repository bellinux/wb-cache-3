0|549|Public
5000|$|K 1840 had a <b>maximum</b> <b>computing</b> {{speed of}} 1.1 MIPS; it could access up to 16 MB of main memory {{and up to}} 4 GB of virtual memory.|$|R
50|$|Supercomputers {{generally}} {{aim for the}} <b>maximum</b> in capability <b>computing</b> {{rather than}} capacity computing. Capability computing is typically thought of as using the <b>maximum</b> <b>computing</b> power to solve a single large problem in the shortest amount of time. Often a capability system is able {{to solve a problem}} of a size or complexity that no other computer can, e.g., a very complex weather simulation application.|$|R
40|$|ABSTRACT. This paper {{examines}} the computation of polynomial greatest common divisors by various generalizations of Euclid's algorithm. The phenomenon of coefficient growth is de-scribed, {{and the history}} of successful efforts first to control it and then to eliminate it is re-lated. The recently developed modular algorithm is presented in careful detail, with special atten-tion to the case of multivariate polynomials. The computing times for the classical algorithm and for the modular algorithm are analyzed, and it is shown that the modular algorithm is markedly superior. In fact, in the multivariate ease, the <b>maximum</b> <b>computing</b> time for the modular algorithm is strictly dominated by the <b>maximum</b> <b>computing</b> time for the first pseudo-division in the classical algorithm...|$|R
40|$|A simple GIS {{procedure}} {{was used for}} restoring narrow linear flood diverting landscape elements in rectangular grid digital elevation models. Subsequently, four different DEM’s (fine and coarse grid, with and without linear elements) were incorporated in 2 D flood models. Finally, the various flood models were used to <b>compute</b> <b>flood</b> risk {{in the eastern part}} of the Belgian coastal plain. The results obtained with fine and coarse grids were quite different, but when linear elements were restored in both grids, they agreed remarkably well. The application of this GIS procedure allows model run times to be reduced by an order of magnitude, while still preserving flood risk accuracy...|$|R
40|$|A {{method to}} define the impact {{position}} of photons on the electromagnetic calorimeter barrel has been developed. As the input, the shower centre of gravity at {{the depth of the}} shower <b>maximum</b> <b>computed</b> from the 3 x 3 crystal array around the maximum crystal in the cluster is used. The method gives a resolution of approx. 0. 74 mm in phi and approx. 1. 0 mm in eta direction and has no dependence on the z-vertex of the photon...|$|R
30|$|For each {{segmented}} arc, {{detect the}} point where the residual value takes a <b>maximum</b> and <b>compute</b> its error curvature ϕ at this point.|$|R
50|$|SL server {{models are}} rack-based. These models are mostly used in data centers and environments where a <b>maximum</b> of <b>computing</b> power is desired.|$|R
40|$|A {{computer}} application and national geospatial database {{have been developed}} to support the calculation of flooding flow (Qf) and threshold runoff across the conterminous United States and Alaska. Flooding flow is the flow required to cause a stream to slightly overflow its bank and cause damage. Threshold runoff [L], defined as the depth of runoff required to cause <b>flooding,</b> is <b>computed</b> as <b>flooding</b> flow divided by the unit hydrograph peak flow. A key assumption in this work is that the two-year return flood (Q 2) is a useful surrogate for flooding flow. The application described here <b>computes</b> <b>flood</b> magnitude estimates for selected return periods (Q 2, Q 5, Q 10, etc.) using regression equations published by the U. S. Geological Survey for each of 210 hydrologic regions. The application delineates basin boundaries and computes all basin parameters required for the flood frequency calculations. The geographic information system database that supports these calculations contains terrain data [digital elevation models (DEMs) and DEM derivatives], reference data, and 89 additional data layers related to climate, soils, geology, and land use. Initial results indicate that there are some practical limitations associated with using Q 2 regression equations to estimate flooding flow...|$|R
40|$|The {{problems}} {{involved in}} the design of a transistor operational amplifier suitable for a repetitive computer are discussed. An analysis of parallel feedback applied to an active quadripole is advanced to show that the <b>maximum</b> <b>computing</b> accuracy for a given number of transistors is obtained by employing cascaded, common-emitter stages without local feedback. The factors which determine the bandwidth of a repetitive computer amplifier are discussed and the associated stability problems are solved by appropriate shaping of the loop transmission of the amplifier. [ [...] . ...|$|R
30|$|The {{results of}} the procedure, using a Gaussian {{function}} as expected value (Fig.  5 a), are very satisfactory. The recovered release history {{is very similar to}} the true one. Moreover, the concentrations estimated at the monitoring point (due to the recovered release history) are very close to the one observed (Fig.  4). The <b>maximum</b> nRMSE <b>computed</b> between the recovered and true release history results 2.20  % (see Table  1), while the one <b>maximum</b> <b>computed</b> between the estimated and observed concentration at the monitoring points is 1.52  %. Considering a Boxcar function as expected value (Fig.  5 b), the estimated release history is even better than the previous one; that is confirmed by the nRMSE computed, in the worst case, between the recovered and true release history in 1.34  % (see Table  1), while the one computed between the estimated and observed concentration at the monitoring points is 1.16  %.|$|R
40|$|The {{flooding}} {{records of}} continents have been computed in dynamically self-consistent models of plates and convection. Platforms flood following rapid translation of continental plates as may occur following supercontinent breakup. In these models, continental flooding is primarily controlled by deep mantle sources and continental hypsometry is {{strongly influenced by}} dynamic topography. Some models, especially bottom heated ones, predict more extensive flooding than has been observed during the Phanerozoic. However, the <b>computed</b> <b>flooding</b> can be made consistent with the observations if: the viscosity of the upper regions of the convecting system are significantly reduced compared to the deep regions, the fluid is driven predominantly by internal heating, or the convection is confined to a depth appreciably less than {{the width of the}} non-subducting plate...|$|R
5000|$|The {{computer}} was financed by Forschungszentrum Jülich, the State of North Rhine-Westphalia, the Federal Ministry for Research and Education {{as well as}} the Helmholtz Association of German Research Centres. The head of the JSC, Thomas Lippert, said that [...] "The unique thing about our JUGENE is its extremely low power consumption compared to other systems even at <b>maximum</b> <b>computing</b> power". [...] A Blue Gene/P-System should reach about 0.35 GFLOPS/Watt and is therefore an order of magnitude more effective than a common x86 based supercomputer for a similar task.|$|R
40|$|When {{performing}} {{an urban}} flood risk analysis, {{it is often}} difficult to take individual buildings into account: doing so requires the availability of a high resolution 2 D hydrodynamic model for the preparation of flood maps and detailed land use maps for the preparation of flood damage maps. As a consequence, a simplified approach is often required, involving the use of low resolution models and simplified land use maps. This study aims at evaluating the impact of such simplifications on the flood risk by means of a case study: the flooding of the city of Antwerp (Belgium) caused by wave overtopping of the flood defenses along the river Scheldt. Two methods for <b>computing</b> <b>flood</b> maps were combined with two methods for computing damage maps, yielding four different methods for <b>computing</b> urban <b>flood</b> risk. The results obtained with the four methods differ significantly. The flood risk predicted by a combination of the detailed approaches was found to be less than 30 % of the flood risk predicted by a combination of the simplified approaches. From this study, we can conclude that the procedures used for dealing with the presence of buildings can be a significant source of uncertainty in urban flood risk analysis...|$|R
3000|$|... =  0.003, {{while the}} strain {{at the extreme}} fiber on the {{opposite}} face was incremented from a strain that equaled the <b>maximum</b> <b>computed</b> tensile strain at pure bending up to a strain that was equal to the uniform compressive strain required across the entire cross section for pure compression. The summation of forces acting on concrete and reinforcing steel at each increment of strain generated one point on the cross section axial force-bending moment interaction diagram. The entire interaction diagram for a column cross section (ℓ/h =  0) similar to one shown in Fig.  7 was defined by 102 points, as stated earlier.|$|R
50|$|<b>Compute</b> <b>maximum</b> {{theoretical}} power output by Pot = Pa / 2.|$|R
40|$|Abstract The new Swedish {{guidelines}} for the estimation of design floods for dams and spillways are presented, with emphasis on highhazard dams. The method {{is based on a}} set of regional design precipitation sequences, rescaled for basin area, season and elevation above sea level, and a full hydrological model. A reservoir operation strategy is also a fundamental component of the guidelines. The most critical combination of flood generating factors is searched by systematically inserting the design precipitation sequence into a ten year climatological record, where the initial snowpack {{has been replaced by a}} statistical 30 -year snowpack. The new guidelines are applicable to single reservoir systems as well as more complex hydroelectric schemes, and cover snowmelt floods, rain floods and combinations of the two. In order to study the probabilities of the <b>computed</b> <b>floods</b> and to avoid regional inconsistencies, extensive comparisons with observed floods and frequency analyses have been carried out...|$|R
40|$|Abstract: An {{application}} {{integrating the}} Hydrologic Engineering Center’s �HEC�-Hydrologic Modeling System hydrologic simulation {{model and the}} HEC-River Analysis System hydraulic simulation model into a seamless floodplain mapping application is presented. The application is implemented with an ArcGIS 9 workflow model called Map to Map, which converts a map of rainfall data to a flood inundation map. The simulation models are integrated into the application by establishing information exchange points at which time series of information are passed to a model or returned from a model. Communication between simulation models and the Geographic Information System �GIS � is made possible by interface data models, which provide a one-to-one mapping between data structures within the simulation model and the GIS. A case study is presented for Rosillo Creek in Texas, in which the Map-to-Map model <b>computes</b> <b>flood</b> inundation polygons from rainfall data. Map to Map gives the user a powerful floodplain mapping and real-time floo...|$|R
40|$|Many {{embedded}} systems have substantially different design constraints than desktop computing applications. No single characterization {{applies to the}} diverse spectrum of {{embedded systems}}. However, some combination of cost pressure, long life-cycle, real-time requirements, reliability requirements, and design culture dysfunction can {{make it difficult to}} be successful applying traditional computer design methodologies and tools to embedded applications. Embedded systems in many cases must be optimized for life-cycle and business-driven factors rather than for <b>maximum</b> <b>computing</b> throughput. There is currently little tool support for expanding embedded computer design to the scope of holistic embedded system design. However, knowing {{the strengths and weaknesses of}} current approaches can set expectations appropriately, identify risk areas to tool adopters, and suggest ways in which tool builders can meet industrial needs. 1...|$|R
3000|$|... is the <b>maximum</b> amount <b>computed</b> exactly by {{the same}} values that apply for RMI and API. The new system {{provides}} a fixed withdrawal rate of 0.62. This means that if a person exits unemployment, for each earned euro from work, she can save € 0.62 of RSA.|$|R
40|$|The {{description}} of femoral head sphericity and related risk for femoroacetabularimpingement is currently limited to an angular estimate-the alpha angle-whose relevance and accuracy have been challenged. We developed a three-dimensional approach for both automated digital {{measurement of the}} alpha angle and the detection of camdeformities. Accuracy and diagnostic relevance of the alpha angle estimated {{by means of the}} oblique axial and multiple radial plane protocol were compared with the computed results. Using subject-specific statistical information of the femur head and mid-neck region, a method was developed to accurately <b>compute</b> the <b>maximum</b> alpha angle and to define aspherical eccentric areas at the femoral head-neck junction. The method was evaluated on 102 dry cadaver femur specimens. Average detection limit for bony prominences at the head-neck transition was 0. 98 mm. Pixel size of the investigated CT data was 0. 79 mm. Mean <b>maximum</b> <b>computed</b> alpha angle of the femurs with cam-type morphology as identified by the morphological method was 67. 72 A degrees (range 53. 04 - 88. 02 A degrees). Mean <b>maximum</b> <b>computed</b> alpha angle of the femurs without cam deformity was 47. 65 A degrees (range 38. 67 - 59. 81 A degrees). Alpha angle estimates obtained by means of the multiple radial plane protocol correlated better (R = 0. 88) and showed higher diagnostic agreement (phi = 0. 77) with the 3 D computational analysis compared to the oblique axial protocol (R = 0. 60; phi = 0. 67). The alpha angle seems to be a relevant screening tool when obtained by 3 D computed analysis or when estimated according to the multiple radial plane protocol. Estimates obtained by means of the oblique axial protocol have insufficient diagnostic and measurement accuracy...|$|R
30|$|Among the {{outcomes}} of Field 3 of SPIRE summarized in this review, the series of studies using the sub-kilometer global atmospheric simulation (Miyamoto et al. 2013, 2015; Kajikawa et al. 2016) are typical examples of “capability computing” using the K computer; that is, a single complex problem solved in the shortest time possible using the <b>maximum</b> <b>computing</b> power available. Although this kind of simulation does not always lead to the maximum output or outcome, the knowledge gained has great potential to bolster next-generation research. In much larger computer system such as the Post-K supercomputer, problems of this size will be easily handled with various configurations. We can expect to get much more information from these supercomputers in terms of “capacity computing,” or the solution of smaller or less complex problems using more efficient computing power.|$|R
30|$|Fault-tolerant target {{detection}} and localization is a challenging task in collaborative sensor networks. This paper introduces our exploratory work toward identifying the targets in sensor networks with faulty sensors. We explore both {{spatial and temporal}} dimensions for data aggregation to decrease the false alarm rate and improve the target position accuracy. To filter out extreme measurements, the median of all readings in a close neighborhood of a sensor is used to approximate its local observation to the targets. The sensor whose observation is a local <b>maxima</b> <b>computes</b> a position estimate at each epoch. Results from multiple epoches are combined together to further decrease the false alarm rate and improve the target localization accuracy. Our algorithms have low computation and communication overheads. Simulation study demonstrates the validity and efficiency of our design.|$|R
40|$|Abstract. Collective {{communication}} {{is one of}} the most powerful message passing concepts, enabling parallel applications to express complex communication patterns while allowing the underlying MPI to provide efficient implementations to minimize the cost of the data movements. However, with the increase in the heterogeneity inside the nodes, more specifically the memory hierarchies, harnessing the <b>maximum</b> <b>compute</b> capabilities becomes increasingly difficult. This paper investigates the impact of kernel-assisted MPI communication, over two scientific applications: 1) Car-Parrinello molecular dynamics(CPMD), a chemical molecular dynamics application, and 2) FFTW, a Discrete Fourier Transform (DFT). By focusing on the usage of Message Passing Interface (MPI), we found the communication characteristics and patterns of each application. Our experiments indicate that the quality of the collective communication implementation on a specific machine plays a critical role on the overall application performance. ...|$|R
40|$|Abstract—Prevailing VLSI trends {{point to}} a growing gap be-tween the scaling of on-chip {{processing}} throughput and off-chip memory bandwidth. An efficient use of memory bandwidth must become a first-class design consideration in order to fully utilize the processing capability of highly concurrent processing platforms like FPGAs. In this paper, we present key aspects of this challenge in developing FPGA-based implementations of two-dimensional fast Fourier transform (2 D-FFT) where the large datasets must reside off-chip in DRAM. Our scalable implementations address the memory bandwidth bottleneck through both (1) algorithm design to enable efficient DRAM access patterns and (2) datapath design to extract the <b>maximum</b> <b>compute</b> throughput for a given level of memory bandwidth. We present results for double-precision 2 D-FFT up to size 2, 048 -by- 2, 048. On an Altera DE 4 platform our implementatio...|$|R
40|$|Fault-tolerant target {{detection}} and localization is a challenging task in collaborative sensor networks. This paper introduces our exploratory work toward identifying the targets in sensor networks with faulty sensors. We explore both {{spatial and temporal}} dimensions for data aggregation to decrease the false alarm rate and improve the target position accuracy. To filter out extreme measurements, the median of all readings in a close neighborhood of a sensor is used to approximate its local observation to the targets. The sensor whose observation is a local <b>maxima</b> <b>computes</b> a position estimate at each epoch. Results from multiple epoches are combined together to further decrease the false alarm rate and improve the target localization accuracy. Our algorithms have low computation and communication overheads. Simulation study demonstrates the validity and efficiency of our design. </p...|$|R
40|$|The {{estimation}} of transient streamflow from stage measurements is indeed important {{and the study}} of Dottori, Martina and Todini (2009) (henceforth DMT) is useful, however, DMT seem to miss certain of its practical aspects. The goal is to infer the discharge from measurements of the stage conveniently and with accuracy adequate for practical work. This comment addresses issues of the applicability of the DMT method in the field. DMT also advocate their method as a replacement of the widely used Jones Formula. The Jones Formula was modified by Thomas (Henderson, 1966) to include the temporal derivative of the depth, instead of the spatial one, to specifically allow discharge estimation from at-a-section stage observations. The outcome of the comparison is not surprising in view of this approximation. However, this discussion intends to show that, properly evaluated, the praxis-oriented Jones Formula, which did well in the tests, can perform better than DMT imply. It will be also documented that the DMT methodology relates to a known method for <b>computing</b> <b>flood</b> depth profiles...|$|R
40|$|The {{first part}} of this thesis {{presents}} the most used synthetic hyetograph methods worldwide. A short summary of statistical analysis of precipitation events is made and the statistical tools needed for designing synthetic hyetographs are presented. The term project storm as well as the IDF and Huff curves are introduced. In the practical part of this thesis synthetic hyetograph were made using different methods for rainfall station Šmartno pri Slovenj Gradcu. They were then used in HEC-HMS hydrological model for <b>computing</b> <b>flood</b> waves of the Velunja River. The influence of the selected time interval and the position of the peak of the precipitation is also presented. Results show the differences amoung considered methods. Differences in peak discharge, time to peak and runoff volume were demonstrated. Methods having the option of choosing the time interval gave similar results for different time intervals. On the other hand, results show that changing the time of the peak of the hyetograph can significantly influence on the time and the peak of the runoff hydrograph...|$|R
40|$|Shallow-water {{models with}} {{porosity}} {{are used to}} <b>compute</b> <b>floods</b> at a relatively coarse resolution while accounting indirectly for detailed topographic data through porosity parameters. In many practical applications, these models enable a significant reduction of the computational time while maintaining an acceptable level of accuracy. In this paper, we improve the use of porosity models on Cartesian grids by three original contributions. First, a merging technique is used to handle cells with low porosity values which tend otherwise to seriously hamper computational efficiency. Next, we show that the optimal method for {{the determination of the}} porosity parameters depends on the modelling scale, i. e. the grid resolution compared to the characteristic size of obstacles and flow ways. Finally, we investigate the potential benefit of using a different porosity parameter in each term of the shallow-water equations. Five test cases, two of them being original, are used to validate the model and assess each contribution. In particular, we obtained speedup values between 10 and 100 while the errors on water depths remain around few percent. Peer reviewe...|$|R
40|$|Let $M$ be {{a closed}} Fano {{symplectic}} manifold with a semifree Hamiltonian circle action with isolated <b>maximum.</b> We <b>compute</b> the Gromov width and the Hofer-Zehnder capacity of $M$ using a moment map. Comment: 14 pages, weakened the assumption to Fano case, added a result on the Hofer-Zehnder capacity, changed titl...|$|R
50|$|The EM {{method was}} {{modified}} to <b>compute</b> <b>maximum</b> a posteriori (MAP) estimates for Bayesian inference {{in the original}} paper by Dempster, Laird, and Rubin.|$|R
40|$|Flood {{emergency}} evacuation Plans support the Civil Protection Agency during flood events defining people evacuation path, behaviour and movement during {{emergency evacuation}}. -Two-dimensional hydraulic models describing water depths and velocities development {{provide a better}} comprehension of flood event advancement underling critical points location in the territory. -The results of flood evacuation simulation supplies relevant data to <b>compute</b> potential <b>flood</b> damage, tangible and intangible, and assists the Authorities to define flood mitigation measures...|$|R
40|$|Based on the non-dimensional approach, {{this study}} focuses on {{developing}} {{a model to}} <b>compute</b> design <b>flood</b> for specific return periods whose parameter estimations are done using the Marquardt algorithm considering peak flood data of 100 Indian catchments. The selected flood data varies for majority of the sites {{for a period of}} 10 years, and for a few sites up to 36 years; and as a preliminary processing these data are checked for outliers, discordancy, and other errors. The model is calibrated for a variety of situations, and validated on selected gauged catchments. Both the descriptive and predictive goodness-of-fit measures are <b>computed</b> considering the <b>floods</b> of specific return periods estimated from the observed data. The model is found to perform well for the whole study area. Investigations reveal the model to be useful to any catchment within the hydrologically homogeneous region with limited or no flood data conditions...|$|R
40|$|International audienceIn France, two {{new kinds}} of flood {{management}} policies are promoted: floodplain restoration and vulnerability mitigation. Few experience feedback exist on these policies but they may have strong impacts on farms. Flood management on Rhône River is highly illustrative of these policies and local authorities would like to appraise the efficiency of these policies with an economic tool (Cost-Benefit Analysis) to help decision making. But the current methods of flood damage modelling do not make the appraisal of these policies possible; mainly {{because they do not}} take into account the organizational and temporal dimensions of damage formation and propagation at farm scale. After a presentation of the Rhône River context and policies, we review existing methods of flood damage modelling for agriculture and show the interest to focus on the farm scale instead of land plot scale. Based upon the theoretical frameworks for systemic approach, we detail the construction of our conceptual model of farm vulnerability before presenting a case study that shows how the model can be implemented to <b>compute</b> <b>flood</b> damage at farm scale. Finally, the outlooks concerning the use of the model to appraise vulnerability mitigation policies and its application at regional scale are developed...|$|R
40|$|A 3 -D {{finite element}} model was {{developed}} for stress analysis of ultra-thin-whitetopping (UTW) pavements under critical loading conditions. The developed 3 -D model was {{used to analyze the}} UTW test pavement sections at Ellaville weigh station in Florida, which had less than satisfactory performance. The poor performing UTW sections at Ellaville weigh station were found to have relatively higher <b>maximum</b> <b>computed</b> stresses under critical loading conditions, which appeared to explain their poor performance with high percentages of cracked slabs. The developed 3 -D model was also used to perform a parametric analysis {{to determine the effects of}} asphalt thickness, asphalt modulus, concrete thickness, concrete modulus, base stiffness, subgrade stiffness, slab dimension, temperature differential in the concrete and applied load on the maximum stresses in UTW pavements under typical Florida conditions. TRB 2003 Annual Meeting CD-ROM Paper revised from original submittal. Kumara, Tia, Wu, Choubane...|$|R
40|$|Why did only {{we humans}} evolve Turing {{completeness}}? Turing completeness is the <b>maximum</b> <b>computing</b> power, {{and we are}} Turing complete because we can calculate whatever any Turing machine can compute. Thus we can learn any natural or artificial language, {{and it seems that}} no other species can, so we are the only Turing complete species. The evolutionary advantage of Turing completeness is full problem solving, and not syntactic proficiency, but the expression of problems requires a syntax because separate words are not enough, and only our ancestors evolved a protolanguage, and then a syntax, and finally Turing completeness. Besides these results, the introduction of Turing completeness and problem solving to explain the evolution of syntax should help us to fit the evolution of language within the evolution of cognition, giving us some new clues to understand the elusive relation between language and thinking. Comment: 36 page...|$|R
40|$|Abstract: In recent years, {{more and}} more {{researchers}} have focused on various significant issues of resource configuration, such as <b>maximum</b> <b>computing</b> performance and minimum response time, especially taking existing resource utilization into account. The goal {{of this research is}} to design a resource configuration optimization system under networked manufacturing environment. A prediction mechanism is realized by using support vector regression (SVR) to estimate resource utilization according to network protocol of each manufacturing process, while redistributing resources based on the current status of existing resources pool. A resource configuration mechanism applying genetic algorithm working along with the tabu search process (GA-TS) is proposed in this study to determine the redistribution of resources. The experimental results show that the proposed scheme achieves an effective configuration via reaching the balance between the utilization of resources within existing resources and network protocol of each manufacturing process between potential resource requirements and the network resource providers...|$|R
