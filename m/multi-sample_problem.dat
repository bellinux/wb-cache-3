2|11|Public
40|$|AbstractIn {{this paper}} tests are derived for testing {{neighborhood}} hypotheses for the one- and <b>multi-sample</b> <b>problem</b> for functional data. Our methodology {{is used to}} generalize testing in projective shape analysis, which has traditionally involving data consisting of finite number of points, to the functional case. The one-sample test {{is applied to the}} problem of scene identification, {{in the context of the}} projective shape of a planar curve...|$|E
40|$|In {{this paper}} tests are derived for testing {{neighborhood}} hypotheses for the one- and <b>multi-sample</b> <b>problem</b> for functional data. Our methodology {{is used to}} generalize testing in projective shape analysis, which has traditionally involving data consisting of finite number of points, to the functional case. The one-sample test {{is applied to the}} problem of scene identification, {{in the context of the}} projective shape of a planar curve. Functional data Precise hypotheses Validation of hypotheses Image analysis Projective shape data analysis...|$|E
40|$|Overview of NonParametric Combination-based {{permutation}} {{tests for}} Multivariate <b>multi-sample</b> <b>problems</b> In this work {{we present a}} review on nonparametric combination-based permutation tests along with SAS macros allowing to deal with two-sample and one-way MANOVA design problems, within NonParametric Combination methodology framework. Applications to real case studies are also presented...|$|R
40|$|In multi-player {{problems}} several {{samples are}} taken from a finite multi-categoried population without any replacement. Of interest are joint probabilities for each sample and each category. A duality theorem proved in this paper allows the exchange of players and categories and simplifies many such problems. Multivariate hypergeometric probabilities Duality theorem Multi-player <b>problems</b> <b>Multi-sample</b> <b>problems</b> Random partitioning for search problems Counting square matrices to solve probability problems...|$|R
40|$|Precedence {{tests are}} simple yet robust nonparametric {{procedures}} useful for comparing {{two or more}} distributions. In this paper precedence type tests are considered when the data contain some right-censored observations. Generalizing the precedence statistic for uncensored data, the precedence tests for censored data {{are based on the}} Kaplan-Meier estimators of the respective distribution functions and the corresponding quantile functions. An overview of the literature is given for the two-sample as well as some <b>multi-sample</b> <b>problems.</b> Some further problems are indicated...|$|R
40|$|In {{sequential}} {{time studies}} where time {{to an event}} is of interest, such as in clinical trials and life testing problems, progressive censoring schemes (PCS) when adopted, usually lead to a shorter duration of experimentation with a combined reduction in cost and in sparing of lives of experimental units. In this dissertation time-sequential testing procedures based on empirical distribution functions, for testing simple and multiple regressions in problems where units enter into the experiment {{at the same point}} of times are developed. Asymptotic distribution theory of the proposed test statistics under the null hypothesis is studied and power properties of the tests against contiguous alternatives are examined. The two-sample and <b>multi-sample</b> <b>problems</b> are als...|$|R
40|$|We propose {{and study}} a general method for {{construction}} of consistent statistical tests {{on the basis of}} possibly indirect, corrupted, or partially available observations. The class of tests devised in the paper contains Neyman's smooth tests, data-driven score tests, and some types of multi-sample tests as basic examples. Our tests are data-driven and are additionally incorporated with model selection rules. The method allows to use a wide class of model selection rules that are based on the penalization idea. In particular, many of the optimal penalties, derived in statistical literature, can be used in our tests. We establish the behavior of model selection rules and data-driven tests under both the null hypothesis and the alternative hypothesis, derive an explicit detectability rule for alternative hypotheses, and prove a master consistency theorem for the tests from the class. The paper shows that the tests are applicable {{to a wide range of}} problems, including hypothesis testing in statistical inverse <b>problems,</b> <b>multi-sample</b> <b>problems,</b> and nonparametric hypothesis testing. Comment: Fully remastered version of the pape...|$|R
40|$|We give a nonparametric {{methodology}} for hypothesis testing for equality of extrinsic mean objects on a manifold {{embedded in a}} numerical spaces. The results obtained in the general setting are detailed further {{in the case of}} 3 D projective shapes represented in a space of symmetric matrices via the quadratic Veronese-Whitney (VW) embedding. Large sample and nonparametric bootstrap confidence regions are derived for the common VW-mean of random projective shapes for finite 3 D configurations. As an example, the VW MANOVA testing methodology is applied to the <b>multi-sample</b> mean <b>problem</b> for independent projective shapes of $ 3 D$ facial configurations retrieved from digital images, via Agisoft PhotoScan technology...|$|R
40|$|Our sensor {{selection}} algorithm targets {{the problem}} of global self-localization of multi-sensor mobile robots. The algorithm builds on the probabilistic reasoning using Bayes filters to estimate sensor measurement uncertainty and sensor validity in robot localization. For quantifying measurement uncertainty we score the Bayesian belief probability density using a model selection criterion, and for sensor validity, we evaluate belief on pose estimates from different sensors as a <b>multi-sample</b> clustering <b>problem.</b> The minimization of the combined uncertainty (measurement uncertainty score + sensor validity score) allows us to intelligently choose a subset of sensors that contribute to accurate localization of the mobile robot. We demonstrate the capability of our sensor selection algorithm in automatically switching pose recovery methods and ignoring non-functional sensors for localization on real-world mobile platforms equipped with laser scanners, vision cameras, and other hardware instrumentation for pose estimation...|$|R
40|$|Starting {{from the}} theory of the Nonparametric Combination of Dependent Permutation Tests (Pesarin, 1992 & 2001), Marozzi (2002 a & 2002 b) {{proposed}} two bi-aspect nonparametric tests for the two-sample and the <b>multi-sample</b> location <b>problems.</b> These tests are shown by simulation to be remarkably more powerful than the traditional parametric and permutation competitors (which can be seen as uni-aspect tests) under heavy-tailed and skewed distributions. After a brief presentation of the bi-aspect idea to location testing problems, three actual applications are discussed. The first one is a problem of business statistics and deals with the analysis of time for service calls. The second one is in medical statistics and deals with the analysis of the effect of cigarette smoking on maternal airway function during pregnancy. The third one is in industrial statistics and deals with the analysis of the setting of machines that produce steel ball bearings. The bi-aspect testing allows us to draw deeper and more informative inference than that allowed by traditional competitors...|$|R
40|$|A bi-aspect {{nonparametric test}} for testing {{hypotheses}} of location shifts of two populations was proposed in literature. The test {{is based on}} the nonparametric combination of dependent tests theory and is obtained by combining the traditional permutation test for the two-sample location problem with a test that takes into account whether a sample observation is or is not greater than the pooled sample median. A natural multi-sample extension of the test is proposed. The extension is shown by simulation to behave very similarly to the bi-aspect test for the two-sample problem. In fact, it is shown that the proposed test is remarkably more powerful than the traditional permutation test for the <b>multi-sample</b> location <b>problem</b> under heavy-tailed distributions like the Cauchy, the half-Cauchy, the 10 % and the 30 % outlier distributions. When sampling from the double-exponential and the exponential distributions, the proposed test appears to be better on the whole than the traditional permutation test. Under the considered t 2 distributions, the bi-aspect test is practically as powerful as the traditional permutation test. Whereas under normal, uniform and bimodal distributions it is slightly less powerful. Moreover, the proposed test maintained the type-one error rate close to the nominal significance level and was generally slightly conservative...|$|R
40|$|As {{missing data}} {{problems}} become more commonplace in biological research and other areas, a method with relaxed assumptions while {{flexible enough to}} accommodate {{a wide range of}} situations is highly desired. We propose a nonparametric imputation method for data with missing values. The inference on the parameter defined by general estimating equations is performed using an empirical likelihood method. It is shown that the nonparametric imputation method together with empirical likelihood can reduce bias and improve efficiency of the estimate relative to inference using only complete cases of the dataset. The confidence regions obtained by empirical likelihood demonstrate good coverage properties. Since our method is valid under very weak assumptions while also possessing the flexibility inherent to estimating equations and empirical likelihood, it can be applied {{to a wide range of}} problems. An example is given using mouse eye weight and gene expression data;Missing data methods are also highly valuable from an experimental design point of view. We proposed a selective transcriptional profiling approach in improving the efficiency and affordability of genetical genomics research. The high cost of microarrays tends to limit the adoption of the standard genetical genomics approach. Our method is derived in a missing data framework, in which only a subset of objects are subjected to microarray experiments. It is shown that this approach can significantly reduce experimental cost while still achieving satisfactory power. To address the need for a nonparametric method, we developed empirical likelihood based inference for <b>multi-sample</b> comparison <b>problems</b> using data with surrogate variables. By applying this result to selective transcriptional profiling, we show that the idea of using relatively inexpensive trait data on extra individuals to improve the power of test for association between a QTL and gene transcriptional abundance also applies to the empirical likelihood based method...|$|R
40|$|W pracy zostały przedstawione dwa kryteria dotyczące selekcji modeli, mianowicie kryterium Akaike: AIC (Akaike’s Information Criterion) i kryterium bayesowskie: BIC (Bayesian Information Criterion). This paper {{studies the}} AlC and B 1 C (Akaike’s and Bayesian Information Criterion) {{replacement}} for: - Box’s (1949) M {{test of the}} homogeneity of covariances, - Wilks’ (1932) Л criterion for testing the equality of mean vectors and - likelihood ratio test of the complete homogeneity as two of model - selection criterions. AIC and BIC are new procedures for comparing means and samples, and selecting the homogeneous groups from heterogenous ones in <b>multi-sample</b> data analysis <b>problems.</b> f rom the Bayesian view-point, the approach to the model-selection problem is to specily the prior probability ol each model, prior distributions for all parameters in each model and compute the posterior probability of each model given the data. That model lor which the estimated posterior probability is the largest is chosen {{to be the best}} one. A clustering technique is presented to generate all possible choices of clustering alternatives of groups and indentify the best clustering among the alternative clusterings. Zadanie pt. „Digitalizacja i udostępnienie w Cyfrowym Repozytorium Uniwersytetu Łódzkiego kolekcji czasopism naukowych wydawanych przez Uniwersytet Łódzki” nr 885 /P-DUN/ 2014 zostało dofinansowane ze środków MNiSW w ramach działalności upowszechniającej nauk...|$|R

