3|74|Public
50|$|There was the {{perception}} around Griffin Park during the 1969 off-season {{that after the}} financial austerity of the previous two-and-a-half years, the extreme cost-cutting measures enacted by former chairman Ron Blindell had reduced Brentford's target to merely staying in business, rather than challenging for promotion to the Third Division. Former director Walter Wheatley's loans to the club had taken its debts down to a <b>manageable</b> <b>proportion,</b> but after being installed as chairman (Blindell had died in January 1969), Wheatley carried on the austerity into the 1969-70 season. Manager Jimmy Sirrel once again had his hands tied in the transfer market, releasing experienced campaigners Dennis Hunt, Pat Terry and Ron Foster and bringing in three attackers, two on free transfers (Bill Brown and Micky Cook) and one on trial (Roger Frude).|$|E
40|$|Background Cardiovascular {{diseases}} (CVD) {{are currently}} {{the leading cause}} of death worldwide, and a major cause of disability. CVD, including supervision of risk factors with respect to prevention, have in recent decades become an increasingly important topic for general practice. These issues have also become prominent in public debate and health care policy. Specific strategies of individual prevention are to a large extent, at least in the Western world, in the hands of the general practitioners (GPs). In recent years, there has been much emphasis on clinical practice guidelines to aid GPs in their preventive work and guide them to the most cost-effective management. This refers both to recommendations on therapeutic options as well as methods to identify those who would benefit the most from preventive treatment. These guidelines can provide important and updated information for clinicians and function as an instrument for quality improvement and potentially also performance assessment in clinical practice. However, various studies have shown that GPs only follow the guidelines to a certain extent, and that recommended treatment goals are often not reached. Some authors have explained this in terms of physicians' inadequacy, whilst others have pointed out that at least part of the explanation is likely to lie in the nature of the guidelines as such. The quality and usefulness of clinical guidelines for prevention of CVD are of great importance to many, both on the level of individual health care and from the perspective of resource allocation. Aims The objective of this project was to study and discuss the validity and relevance of international CVD prevention guidelines for general practice. More specifically: - To document the CVD risk profile of a general population as defined by selected, authoritative preventive clinical guidelines, by means of modelling studies. - To estimate the workload associated with following the recommendations of the selected guidelines for a well-defined general population in whole. - To identify potential causes of guidelines' overestimation of risk, focusing on individual risk factors. Material and methods This dissertation is based on analyses of data from the Norwegian HUNT 2 population survey, including roughly 65 000 participants. Two studies were conducted to document the CVD risk profile of this general population and to model the implications of implementing current clinical guidelines, regarding the proportion of the population identified at “increased risk”, and the clinical workload associated with following the guideline recommendations. Subsequently, two studies were conducted to analyse whether potential causes of guidelines' overestimation of CVD risk might stem from the way two individual risk factors, cholesterol and obesity, are handled in the guidelines. The dissertation further includes analysis and identification of additional factors potentially limiting the validity and relevance of preventive CVD guidelines. Results If authoritative guideline recommendations for CVD prevention are literally applied, a vast majority of adults in Norway would exhibit “unfavourable” CVD risk profiles and thus be considered in need of individual, clinical attention and follow-up. The potential workload associated with implementing current European clinical guidelines could destabilise the healthcare system in Norway, one of the world's most long- and healthyliving nations, by international comparison. Total cholesterol was not found to be as predictive of mortality as generally assumed. Thus, possible errors regarding the role of total cholesterol in the CVD risk algorithms of many clinical guidelines were identified. If our findings are generalisable, clinical and public health recommendations regarding the “dangers” of cholesterol should be revised. Body mass index, the most widely recommended measure of obesity in preventive CVD guidelines, was found to be inferior to waist-to-hip ratio (WHR), waist-to-height ratio (WHtR), and waist circumference in relation to predicting mortality. WHR and WHtR exhibited the best predictive properties. It appears reasonable to recommend WHR as the primary clinical measure of body composition and obesity for preventive purposes. Conclusion There currently appears to be a range of factors limiting the validity and relevance of clinical practice guidelines on prevention of CVD, at least in Norway. Such limitations may have important effects on clinical practice and resource allocation, as well as population health. The guidelines appear to overestimate CVD risk and fail to correctly identify a <b>manageable</b> <b>proportion</b> of the population as “high-risk individuals”, for whom individual preventive strategies would be effective and beneficial. The strategy of targeting individuals at risk ends up being recommended at the level of mass strategy, which can hardly be regarded as sustainable or responsible. A number of factors potentially limiting the validity and relevance of current guidelines were identified. The dissertation includes a proposal of ways to improve the guidelines. PhD i samfunnsmedisinPhD in Community Medicin...|$|E
40|$|The {{materials}} {{for studying the}} nature, incidence, and distribution of crime in later medieval England are copious, but they present very serious difficulties of interpretation. Much {{work has been done}} on legal and administrative records as such, and a few tentative studies of crime have been published, but no one has hitherto attempted the detailed account of the machinery of criminal justice, and of the possibilities and limitations of the available records, which should precede any wider investigation of the criminal in society. This thesis is intended to contribute such an account. Its sources, and its relevance, are not by any means confined to the given period and county, but it was necessary to focus on a <b>manageable</b> <b>proportion</b> of the records in order to study them in adequate depth, and Hampshire {{at the end of the}} fourteenth century offered a substantial and apparently representative selection of surviving materials. A preliminary note describes, briefly and broadly, the way in which a modern notion of serious crime has been related to the less conceptual, more procedural distinctions of medieval law. This is followed by a chapter describing in detail the procedures whereby criminals were accused, detained, tried, and punished. The analysis of local first instance courts - coroners' inquests, law hundreds, tourns, peace sessions is especially revealing; a disproportionate amount of work on the peace rolls has previously led to the assumption that, by the period of the Black Death and its aftermath, peace sessions were the most important source of criminal prosecutions, whereas the case of Hampshire shows that private courts were still of prime importance until 14 OO and beyond, with coroners also making a large contribution. In general, indictment and trial procedures were far more openly flexible and less punctilious at the end of the fourteenth century than they had been a hundred years earlier, and this was reflected particularly in procedures whereby many suspects were acquitted without trial, by disavowal of mainour or by proclamation. In these as in other cases, the nature and status of evidence heard in court remains obscure. The second main chapter examines the social status of the various officials and jurors whose decisions affected the criminal. As with procedure, the simplest means of analysis is court by court, taking the officials and specimen juries and identifying them in poll tax returns and other available records. The results bear out what is already known about coroners, justices of the peace, and professional justices, suggesting also that variations in the personnel of peace commissions had less effect in practice than has been supposed. Officials holding private law hundreds seem to have been a very heterogeneous group, ranging from county magnates to obscure and minor clerks. The juries at these sessions seem to have been genuinely local and humble; coroners' juries were also demonstrably local in most cases, although there are signs of special recruitment when an adverse verdict was required. Presenting juries at peace sessions were mainly local men of substance, and openly self-interested. Trial juries, although evidently empanelled with difficulty and some improvisation, were usually as local as possible to the offence. After this discussion of procedures and the personnel who implemented them, there is a detailed consideration of the resultant documents and their evidential value. The relatively careless compilation of coroners' and peace rolls provides one problem, while the failure of many records to survive is another. While some basic information from lost documents can be reconstructed from related materials, there is no means of restoring all the lost data. Even were this possible, it would not obviate the great circumspection which the records require, since it is possible to find many examples of major and minor inaccuracies - sometimes the product of rather summary procedures, sometimes of careless transcription or redaction, sometimes of an intention to produce a particular judicial outcome. In general the local first instance records seem most nearly reliable; the trial courts, especially king's bench, provide a definitive view only of the law as it was applied. It has often been argued that the common law was peculiarly sterile on the criminal side. Chapter V attempts a defence of the medieval criminal law, on grounds of its flexibility and practicability in what was normally a limited range of contingencies, and shows, in a review of the law on particular offences, the degree to which practical considerations could be and were accommodated. The law of treason, for example, rested very largely on political expediency, with a fairly generous attitude towards petty counterfeiters. The law of homicide showed unexpected simplicity; on the other hand, the historian cannot wholly disentangle the contortions into which the law of rape threw the practice of prosecuting it. When all the foregoing points have been examined, it remains to consider the extent to which we can know about medieval crime. A chapter on criminal justice outlines those areas in which we can and cannot rely upon the available information. It is clear that some indictments were accepted despite doubtful legality or veracity, and it is also clear that many verdicts are unreliable - particularly in king's bench, where verdicts seem always to have favoured the party who brought the case to court. There is, moreover, seldom any indication of motive by which the historian can assess an accusation. As for the professional criminal, the ease of reference to persons of prominent family or substance has meant that the gentry has received more attention in this matter than has the true professional underworld; a careful scrutiny of approver's appeals, however, shows that much can be learned about the routine activities of professional petty thieves, rustlers, and highwaymen. Further discussion of varying criminal jurisdictions argues the difficulty of knowing how effectively and systematically crime was attacked and punished; but a postscript allows that crude statistical analysis, and minute scrutiny of particular topics, may allow something to be known of the nature and incidence of crime in later medieval England. </p...|$|E
5000|$|A {{process by}} which large, complex, and {{potentially}} unmanageable strategic problems are factored into progressively smaller, less complex, and hence more <b>manageable</b> <b>proportions.</b>|$|R
40|$|In {{this paper}} I shall {{summarize}} {{the most recent}} experiments on photoproduction reactions leading to two-body final states. Low-energy photoproduction and diffraction processes have been excluded by {{the organizers of the}} conference, and {{in order to reduce the}} data to <b>manageable</b> <b>proportions</b> and the talk to a finite length...|$|R
40|$|The role of {{trigger and}} on-line {{processors}} in reducing data rates to <b>manageable</b> <b>proportions</b> in e+e- physics experiments is defined not by high physics or background rates, {{but by the}} large event sizes of the general-purpose detectors employed. The rate of e+e- annihilation is low, and backgrounds are not high; yet the number of physic...|$|R
40|$|The author {{summarized}} {{certain aspects}} of the conference. He shares this task with another colleague thereby breaking the task into more <b>manageable</b> <b>proportions.</b> The author covers the low luminosity sources. He begins his review with a summary of some major themes of the conference and ends with a few speculations on possible theoretical mechanisms...|$|R
40|$|AbstractIn this note, {{we discuss}} a Markov chain {{formulation}} of the k-SAT problem and {{the properties of the}} resulting transition matrix. The motivation behind this work is to relate the phase transition in the k-SAT problem to the phenomenon of “cut-off” in Markov chains. We use the idea of weak-lumpability to reduce the dimension of our transition matrix to <b>manageable</b> <b>proportions...</b>|$|R
40|$|A theorem is {{developed}} which reduces to <b>manageable</b> <b>proportions</b> a linear programming problem of potentially infinite size. A converging iterative procedure is presented for estimating the parameters {{to be used}} in combining the activities of the original large problem. The theorem and iterative procedure are applicable to the general problem of sorting material in industrial processes, whether the material to be sorted consists of peas, pine logs, or poultry products. ...|$|R
50|$|However, Maidstone United went {{bankrupt}} {{and had to}} resign from the league in August 1992, most of their cash being taken up to gain the eagerly sought Football League place. Ground improvements, which Maidstone United had paid for, were sold to Dartford at a cost (around £500,000), which pushed Darts' debts beyond <b>manageable</b> <b>proportions.</b> Watling Street was sold to pay off creditors and Dartford withdrew from the Southern League four games into the 1992-93 season.|$|R
40|$|In {{this paper}} {{we present a}} {{probabilistic}} tabu search algorithm for the generalized minimum spanning tree problem. The basic idea behind the algorithm is to use preprocessing operations {{to arrive at a}} probability value for each vertex which roughly corresponds to its probability of being included in an optimal solution, and to use such probability values to shrink the size of the neighborhood of solutions to <b>manageable</b> <b>proportions.</b> We report results from computational experiments that demonstrate the superiority of this method over the generic tabu search method. ...|$|R
40|$|A {{scheme is}} {{introduced}} {{which makes it}} feasible to make completely self-consistent Brueckner-Hartree-Fock (BHF) and renormalized BHF calculations for spherical, closed-shell and axially-symmetric deformed nuclei. The usual requirement or orbital self-consistency has been imposed, as well as self-consistency in the starting energies and occupation probabilities. Previously, only approximate forms {{were used for the}} Pauli operator. This approximation is removed and a method for making the necessary Pauli corrections to the reaction matrix during the approach to self-consistency is presented. A discussion of the symmetries which reduce the problem to one of <b>manageable</b> <b>proportions</b> is included...|$|R
50|$|But the {{accelerating}} {{tempo of}} the war ruled out long repose in {{the shelter of the}} lagoon. Before the year ended, the carriers were back in action against airfields in the Philippines on Sakishima Gunto, and on Okinawa. These raids were intended to smooth the way for General MacArthur's invasion of Luzon through the Lingayen Gulf. While the carrier planes were unable to knock out all Japanese air resistance to the Luzon landings, they did succeed in destroying many enemy planes and thus reduced the air threat to <b>manageable</b> <b>proportions.</b>|$|R
40|$|Outlines {{problems}} {{dealing with}} large numbers of variables, explaining techniques available toreduce them to more <b>manageable</b> <b>proportions.</b> Researchers and analysts now have access to increasingly large data sets. This article outlines some of the problems of dealing with alarge number of variables and explains some of the techniques {{that can be used to}} reduce the number of available indicators to a moremanageable size. This can be helpful in analysing the data or in modelling and forecasting work. Economic & Labour Market Review (2007) 1, 62 – 67; doi: 10. 1057 /palgrave. elmr. 1410141...|$|R
40|$|This report {{reviews the}} {{literature}} on the applications of NMR to food science from 1995 until March 2001. In {{order to be able to}} keep the number of references to <b>manageable</b> <b>proportions,</b> the number of papers referred to has been limited to those applications where NMR plays a major role in the experimental programme. Applications where NMR is simply used as a routine structural tool have been left out. Following an introductory section, the report covers water in foods, biopolymers, analysis and authentication, complex systems, and new methods for food analysis...|$|R
40|$|This article {{presents}} an implementation of periodic boundary conditions (PBC) for Dislocation Dynamics (DD) simulations {{in three dimensions}} (3 D). We discuss fundamental aspects of PBC development, including preservation of translational invariance and line connectivity, the choice of initial configurations compatible with PBC and a consistent treatment of image stress. On the practical side, our approach reduces to <b>manageable</b> <b>proportions</b> the computational burden of updating the long-range elastic interactions among dislocation segments. The timing data confirms feasibility and practicality of PBC for large-scale DD simulations in 3 D...|$|R
25|$|The chosen {{source was}} a play {{performed}} in Paris {{only six months}} before, Têtes Rondes et Cavalieres (Roundheads and Cavaliers), written by Jacques-François Ancelot and Joseph Xavier Saintine, which some sources state was based on Walter Scott's novel Old Mortality, while others state {{that there is no}} connection. The composer had prepared the way for his librettist by providing him with a scenario of thirty-nine scenes (thus compressing the original drama into <b>manageable</b> <b>proportions),</b> reducing the number of characters from nine to seven and at the same time, giving them names of a more Italianate, singable quality.|$|R
40|$|If the {{concentration}} of carbon fibers (CF) is high after their dispersion due to an aircraft fire, {{there is a significant}} possibility that a number of residential, commercial, and industrial establishments might be affected by electronic equipment failure. Estimating economic losses from CF release involves characterizing an entire spectrum of buildings and electronic equipment within a given community. A number of simplified assumptions were made to reduce the data collection requirements to <b>manageable</b> <b>proportions.</b> A limited number of facility categories were identified, and assumed to be relatively homogeneous. The detailed examination of potential losses on an industry-by-industry basis is an important area for future investigation...|$|R
40|$|The Very Large Hadron Collider (or Eloisatron) {{represents}} {{what may}} well be the final step on the energy frontier of accelerator-based high energy physics. While an extremely high luminosity proton collider at 100 - 200 TeV center of mass energy can probably be built in one step with LHC technology, that machine would cost more than what is presently politically acceptable. This talk summarizes the strategies of collider design including staged deployment, comparison with electron-positron colliders, opportunities for major innovation, and the technical challenges of reducing costs to <b>manageable</b> <b>proportions.</b> It also presents the priorities for relevant R and D for the next few years...|$|R
40|$|The {{baby boom}} {{generation}} is now well into middle age, {{and over the next}} few decades will reach old age. As the boom generation grows old the costs of maintaining existing social support systems will rise, and the ability or willingness to sustain those systems has been called into question. In this paper we discuss a number of issues related broadly to population aging in Canada and the associated social "costs," including the costs of public services. We conclude that while population-related cost increases should be expected, and reallocations of resources required, the overall increases should be of <b>manageable</b> <b>proportions.</b> population aging; social support systems; baby boom...|$|R
50|$|The Detroit Diesel Series 71 is a {{two-stroke}} {{diesel engine}} series, available in both inline and V configurations, with the inline models including one, two, three, four and six cylinders, and the V-types including six, eight, 12, 16 and 24 cylinders. The two largest V units used multiple cylinder heads per bank {{to keep the}} head size and weight to <b>manageable</b> <b>proportions,</b> the V-16 using four heads from the four-cylinder inline model and the V-24 using four heads from the inline six-cylinder model. This feature also assisted in keeping down the overall cost of these large engines by maintaining parts commonality with the smaller models.|$|R
5000|$|The Hobo Day parade also {{included}} floats built by students that {{drove down the}} main street. During the 1940s and 50s, the floats were described as [...] "enormous and spectacular." [...] Due to safety concerns and collapsing floats due to their enormous size, the floats were built smaller and to more <b>manageable</b> <b>proportions.</b> One particular float that caused {{a sharp decline in}} float building was a large elaborate float that had an outhouse on the back containing a student with his pants around his ankles on the toilet. After several newspapers in the region declared SDSU students as [...] "vulgar," [...] the college shied away from obscene and large floats.|$|R
40|$|Weintroduce an {{integrated}} animation and storyboarding system that simplifies {{the creation and}} refinement of computer generated animations. The framework models both the process and product of an animated sequence, making animation more accessible for communication and as an art form. The system adopts a novel approach to animation byintegrating storyboards and the traditional film hierarchyinacomputer animation system. Traditional animation begins with storyboards representing important moments in a film. These storyboards are structured into shots and scenes which form a standard hierarchy. This hierarchy is important to long animations because it reduces the complexity to <b>manageable</b> <b>proportions.</b> We also introduce the animation proof reader, a tool for identifying awkward camera placement and motion sequences using traditional film production rules...|$|R
40|$|Abstract. Starting {{about thirty}} years ago, new ideas in {{nonlinear}} dynamics, particularly fractals and scaling, provoked an explosive growth of research both in modeling and in experimentally characterizing geosystems over wide ranges of scale. In this review {{we focus on}} scaling advances in solid earth geophysics including the topography. To reduce the review to <b>manageable</b> <b>proportions,</b> we restrict our attention to scaling fields, i. e. to the discussion of intensive quantities such as ore concentrations, rock densities, susceptibilities, and magnetic and gravitational fields. We discuss the {{growing body of evidence}} showing that geofields are scaling (have power law dependencies on spatial scale, resolution), over wide ranges of both horizontal and vertical scale. Focusing on the cases where both horizontal and vertical statistics have both been estimated fro...|$|R
50|$|In {{his letter}} to Ferlito of 11 April, Bellini {{provides}} a synopsis of the opera, indicating that his favourite singers, Giulia Grisi, Luigi Lablache, Giovanni Battista Rubini, and Antonio Tamburini, would all be available for the principal roles, {{and that he would}} begin to write the music by 15 April if he had received the verses. Before the collaboration had got underway and initially impressed by the quality of Pepoli's verses in general, Bellini had prepared the way for his librettist by providing him with a scenario of thirty-nine scenes (thus compressing the original drama into <b>manageable</b> <b>proportions),</b> reducing the number of characters from nine to seven and at the same time, giving them names of a more Italianate, singable quality.|$|R
5000|$|... 7. Although {{everyone}} in the group is normally required to buy at least one round before leaving, the advent of either drunkenness or closing time sometimes renders this ideal unattainable. In such circumstances, any non-paying participant will (a) have [...] "got away with it" [...] and (b) appoint himself [...] "opener" [...] at the next forgathering. However, any player who notices on arrival that the round has [...] "got out of hand" [...] and has no chance of reaching his turn before [...] "the last bell", may start a [...] "breakaway round" [...] by buying a drink for himself and all subsequent arrivals. This stratagem breaks the round in two, keeps the cost within <b>manageable</b> <b>proportions</b> {{and is the only}} acceptable alternative to Rule 5.|$|R
40|$|One {{effect of}} the interdependencies fuelled by {{globalization}} and new communication technologies is the disappearance of a stable sense {{of the size of}} the world and our location in it. This article looks at a number of attempts to cognitively remap the world by scaling it down to more <b>manageable</b> <b>proportions,</b> drawing on examples from anthropology, cosmopolitan discourse, Hollywood film and ‘small world’ theory. My focus is on how these new world maps transform the role of the stranger. Focusing on six degrees of separation chains in which individuals form connections with randomly encountered strangers across the world, I argue that this type of global networking reveals the many ways there are of not knowing other people and provides a useful counter-narrative to the paranoid cosmopolitanism fostered by the contemporary war on terror...|$|R
40|$|We {{introduce}} {{an integrated}} animation and storyboarding system that simplifies {{the creation and}} refinement of computer generated animations. The framework models both the process and product of an animated sequence, making animation more accessible as both a communication medium and as an art form. The system adopts a novel approach to animation by integrating storyboards and the traditional film hierarchy of scenes and shots into a computer modeling and animation system. Traditional animators initially storyboard key moments in a film to help guide the animation process. In a computer generated animation, a much more dynamic and integrated relationship between storyboards and the final animated sequence is possible. This hierarchical decomposition allows animators to reduce the complexity of long animations to <b>manageable</b> <b>proportions.</b> We also introduce the animation proof reader, a tool for identifying awkward camera placement and motion sequences using traditional film production rules [...] ...|$|R
40|$|Achieving {{sustainability}} on {{an urban}} scale is an overwhelming problem. We can address this {{by dividing the}} problem into <b>manageable</b> <b>proportions.</b> Environmental impacts of urban design fall into measurable categories, for example, air quality, biodiversity, solid wastes, water and wastewater, hazardous materials, and impacts of nonrenewable energy use. Such measures are incorporated into building rating systems {{as a way of}} codifying sustainability. In this chapter, to illustrate such codification, we examine water use as well as generated wastewater according to the requirements of a specific sustainable building rating system. Conventional calculations are coupled with building information modeling to illustrate the overall effects of parametrically selecting fixtures, systems and materials to control the use of potable water. We further demonstrate how this approach of combining parametric building information modeling with measures of their environmental impacts can be employed on an urban scale, thereby, guiding the design of sustainable urban spaces...|$|R
40|$|Despite {{several years}} of {{intensive}} study, intrusion detection systems still suffer from a key deficiency: A high rate of false alarms. To counteract this, this paper proposes to visualise {{the state of the}} computer system such that the operator can determine whether a violation has taken place. To this end a very simple anomaly detection inspired log reduction scheme is combined with graph visualisation, and applied to the log of a webserver with the intent of detecting patterns of benign and malicious (or suspicious) accesses. The combination proved to be effective. The visualisation of the output of the anomaly detection system counteracted its high rate of false alarms, while the anomaly based log reduction helped reduce the log data to <b>manageable</b> <b>proportions.</b> The visualisation was more successful in helping identifying benign accesses than malicious accesses. All the types of malicious accesses present in the log data were found. ...|$|R
40|$|In {{consumer}} theory, {{the principles}} of Lancaster's characteristics approach and hedonic pricing appear to offer the most promising insight into choice when qualitative aspects are important. The paper reconciles these principles with the family of non-parametric frontier estimation methods known as data envelopment analysis. It is shown that, with some straightforward adjustments, DEA is entirely consistent with the characteristics view of consumer choice found in the economics literature. In making Lancaster's ideas operational, the paper also addresses the theoretical concern voiced by Lancaster about combining indivisible products. The principles are illustrated with a case study involving the comparison of diesel cars. The paper concludes that the user will ultimately have to apply some judgement in choosing between competing efficient products. However, the analysis should help to restrict the number of products to be assessed to <b>manageable</b> <b>proportions.</b> Copyright © 2002 John Wiley & Sons, Ltd. ...|$|R
40|$|The {{question}} of how to control a robotic welding torch to trace the joint between two cylindrical pipes can be reduced to a problem in algebra. Maple can be utilised to derive the parametric equations describing the curve of intersection between the two cylinders, and then to explore the solution graphically for various physical parameters. This problem can serve as an excellent introduction to the use of Maple for simplifying and solving systems of equations, and offers several straightforward extensions that increase its applicability to more advanced mathematics courses. Keywords: pipe welding, cylinders, intersections, geometrical modeling Introduction: A welding problem Many problems arising in engineering and physics have a mathematical description that is too complicated for consideration in lower [...] level mathematics courses. It is often possible to make simplifyingassumptions that reduce the problem to <b>manageable</b> <b>proportions,</b> but it is often the case that these simplificati [...] ...|$|R
40|$|OBJECTIVE: This paper {{describes}} the Child and Adolescent Component of the National Survey of Mental Health and Wellbeing. METHOD: The {{aims of the}} study, critical decisions in planning for the study, progress to date and key issues which influenced {{the course of the}} study are described. RESULTS: The Child and Adolescent Component of the National Survey of Mental Health and Wellbeing is the largest study of child and adolescent mental health conducted in Australia {{and one of the few}} national studies to be conducted in the world. Results from the study will provide the first national picture of child and adolescent mental health in Australia. CONCLUSIONS: Large-scale epidemiological studies have the potential to provide considerable information about the mental health of children and adolescents. However, having a clear set of aims, ensuring that the scope of the study remains within <b>manageable</b> <b>proportions</b> and paying careful attention to the details of fieldwork are essential to ensure that high-quality data is obtained in such studies...|$|R
40|$|The nodal {{condensation}} is {{an efficient}} way {{of reducing the}} size of eigenvalue problems to <b>manageable</b> <b>proportions.</b> In this note we use a minimax characterization of the eigenvalues of the exactly condensed (nonlinear) eigenvalue problem to estimate the errors of the eigenvalues of the reduced (linear) problem. The eigenvalue approximations can be improved considerably {{by the use of}} the Rayleigh functional of the nonlinear problem. 1 Introduction The dynamic analysis of structures by finite element or finite difference methods often leads to eigenvalue problems of such a magnitude that its eigensolutions are very expensive to obtain. For large eigenvalue problems Guyan [2] and Irons [4] proposed a method {{to reduce the size of}} the matrices which is referred to as eigenvalue economization or nodal condensation or simply reduction. In this reduction process some of the variables (often known as slaves) are eliminated and one obtains an eigenvalue problem for the remaining uneliminated vari [...] ...|$|R
40|$|Much {{has been}} written about the {{advantages}} and disadvantages of high efficiency electric motors. For a given motor application it is possible to find literature that enables a plant engineer to make an informed choice between a standard efficiency and a high efficiency motor; however, few plant engineers have the time to perform a detailed analysis for each motor in their facility. A technique is needed to reduce the analysis to <b>manageable</b> <b>proportions.</b> This paper looks at efforts to identify high efficiency electric motor applications at two manufacturing facilities. It describes a technique that was used to assemble available data in a form that helped prioritize motors in terms of suitability for retrofit with high efficiency models. The technique addresses the problems of limited time and missing data, and suggests ways for quickly filling in data gaps. The motors in the studies spanned a range of 7. 5 to 250 hp. The prioritization was performed primarily on the basis of simple payback. The study results are of potential interest to persons interested in the overall applicability of high efficiency motors in manufacturing...|$|R
40|$|Combined {{exposures}} may be categorised as specified combinations or {{mixtures of}} substances, depending on composition and exposure scenario. The major {{characteristic of a}} specified combination is known composition and that of a mixture simultaneous exposure. A framework was developed {{as a guide for}} safety evaluation of combined exposures. This framework offers the possibility to evaluate mixtures as a single entity, or as a number of fractions or individual constituents. The evaluation of specified combinations will often focus on the individual components. To reduce the safety evaluation of complex exposures to <b>manageable</b> <b>proportions,</b> the "top n" and "pseudo top n" approaches were introduced, n representing the n most "risky" chemicals or groups of chemicals, respectively. To select the best method, the framework should always be walked through in its entirety, considering all options. The Mumtaz-Durkin weight-of-evidence approach is included as a prioritization instrument for combined exposures. It is based on hazard indices supplemented with qualitative and quantitative weighting and interaction factors. © 2004 Elsevier B. V. All rights reserved...|$|R
