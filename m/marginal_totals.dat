85|179|Public
5000|$|This table can be {{completed}} with the <b>marginal</b> <b>totals</b> of the two variables ...|$|E
50|$|One example {{occurs in}} 2×2 tables, where {{conditioning}} {{on all four}} <b>marginal</b> <b>totals</b> leads to a conditional likelihood based on the non-central hypergeometric distribution. This form of conditioning is also the basis for Fisher's exact test.|$|E
50|$|The {{numbers of}} the males, females, and right- and left-handed {{individuals}} are called <b>marginal</b> <b>totals.</b> The grand total (i.e. {{the total number of}} individuals represented in the contingency table) is the number in the bottom right corner.|$|E
5000|$|... where ti is a row <b>marginal</b> <b>total</b> and ui {{a column}} <b>marginal</b> <b>total</b> in the {{contingency}} table. The z-score equivalent is then given by ...|$|R
5000|$|... where [...] For {{data with}} small sample size, such as no <b>marginal</b> <b>total</b> {{is greater than}} 15 (and {{consequently}} [...] ), one should utilize Yates's correction for continuity or Fisher's exact test.|$|R
30|$|We {{estimated}} <b>total</b> <b>marginal</b> effect, ∂ E(WTP)/∂X_i, using Eq. (14) and {{the fraction}} of the <b>total</b> <b>marginal</b> effect above the zero bid, ∂ E(WTP^∗)/∂X_i, using Eq. (15).|$|R
5000|$|Kappa is {{an index}} that {{considers}} observed agreement {{with respect to}} a baseline agreement. However, investigators must consider carefully whether Kappa’s baseline agreement is relevant for the particular research question. Kappa’s baseline is frequently described as the agreement due to chance, which is only partially correct. Kappa’s baseline agreement is the agreement that would be expected due to random allocation, given the quantities specified by the <b>marginal</b> <b>totals</b> of square contingency table. Thus, Kappa = 0 when the observed allocation is apparently random, regardless of the quantity disagreement as constrained by the <b>marginal</b> <b>totals.</b> However, for many applications, investigators should be more interested in the quantity disagreement in the <b>marginal</b> <b>totals</b> than in the allocation disagreement as described by the additional information on the diagonal of the square contingency table. Thus for many applications, Kappa’s baseline is more distracting than enlightening. Consider the following example: The disagreement proportion is 14/16 or [...]875. The disagreement is due to quantity because allocation is optimal. Kappa is [...]01.The disagreement proportion is 2/16 or [...]125. The disagreement is due to allocation because quantities are identical. Kappa is -0.07.|$|E
5000|$|Before {{we proceed}} with the Fisher test, we first {{introduce}} some notations. We represent the cells by the letters a, b, c and d, call the totals across rows and columns <b>marginal</b> <b>totals,</b> and represent the grand total by n. So the table now looks like this: ...|$|E
50|$|The {{iterative}} proportional fitting procedure (IPFP, {{also known}} as biproportional fitting in statistics, RAS algorithm in economics and matrix raking or matrix scaling in computer science) is an iterative algorithm for estimating cell values of a contingency table such that the <b>marginal</b> <b>totals</b> remain fixed and the estimated table decomposes into an outer product.|$|E
40|$|Entropy {{has been}} widely {{employed}} {{as a measure of}} variability for problems, such as machine learning and signal processing. In this paper, we provide some new insights into the behaviors of entropy as a measure of multivariate variability. The relationships between multivariate entropy (joint or <b>total</b> <b>marginal)</b> and traditional measures of multivariate variability, such as total dispersion and generalized variance, are investigated. It is shown that for the jointly Gaussian case, the joint entropy (or entropy power) is equivalent to the generalized variance, while <b>total</b> <b>marginal</b> entropy is equivalent to the geometric mean of the <b>marginal</b> variances and <b>total</b> <b>marginal</b> entropy power is equivalent to the total dispersion. The smoothed multivariate entropy (joint or <b>total</b> <b>marginal)</b> and the kernel density estimation (KDE) -based entropy estimator (with finite samples) are also studied, which, under certain conditions, will be approximately equivalent to the total dispersion (or a total dispersion estimator), regardless of the data distribution...|$|R
40|$|Whilst {{estimation}} of the <b>marginal</b> (<b>total)</b> causal effect of a point exposure on an outcome {{is arguably the most}} common objective of experimental and observational studies in the health and social sciences, in recent years, investigators have also become increasingly interested in mediation analysis. Specifically, upon establishing a non-null total effect of the exposure, investigators routinely wish to make inferences about the direct (indirect) pathway of the effect of the exposure not through (through) a mediator variable that occurs subsequently to the exposure and prior to the outcome. Although powerful semiparametric methodologies have been developed to analyze observational studies, that produce double robust and highly efficient estimates of the <b>marginal</b> <b>total</b> causal effect, similar methods for mediation analysis are currently lacking. Thus, this paper develops a general semiparametric framework for obtaining inferences about so-called marginal natural direct and indirect causal effects, while appropriately accounting for a large number of pre-exposure confounding factors for the exposure and the mediator variables. Our analytic framework is particularly appealing, because it gives new insights on issues of efficiency and robustness in the context of mediation analysis. In particular, we propose new multiply robust locally efficient estimators of the marginal natural indirect and direct causal effects, and develop a novel double robust sensitivity analysis framework for the assumption of ignorability of the mediator variable...|$|R
40|$|Suppose {{that having}} {{established}} a <b>marginal</b> <b>total</b> {{effect of a}} point exposure on a time-to-event outcome, an investigator wishes to decompose this effect into its direct and indirect pathways, also know as natural direct and indirect effects, mediated by a variable known to occur after the exposure and prior to the outcome. This paper proposes a theory of estimation of natural direct and indirect effects in two important semiparametric models for a failure time outcome. The underlying survival model for the <b>marginal</b> <b>total</b> effect and thus for the direct and indirect effects, can either be a marginal structural Cox proportional hazards model, or a marginal structural additive hazards model. The proposed theory delivers new estimators for mediation analysis {{in each of these}} models, with appealing robustness properties. Specifically, in order to guarantee ignorability with respect to the exposure and mediator variables, the approach, which is multiply robust, allows the investigator to use several flexible working models to adjust for confounding by a large number of pre-exposure variables. Multiple robustness is appealing because it only requires a subset of working models to be correct for consistency; furthermore, the analyst need not know which subset of working models is in fact correct to report valid inferences. Finally, a novel semiparametric sensitivity analysis technique is developed for each of these models, to assess the impact on inference, of a violation of the assumption of ignorability of the mediator...|$|R
5000|$|For example, {{suppose we}} knew probabilities [...] with [...] such that (male studier, male non-studier, female studier, female non-studier) had {{respective}} probabilities [...] {{for each individual}} encountered under our sampling procedure. Then still, were we to calculate the distribution of cell entries conditional given marginals, we would obtain the above formula in which neither [...] nor [...] occurs. Thus, we can calculate the exact probability of any arrangement of the 24 teenagers into the four cells of the table, but Fisher showed that to generate a significance level, we need consider only the cases where the <b>marginal</b> <b>totals</b> {{are the same as}} in the observed table, and among those, only the cases where the arrangement is as extreme as the observed arrangement, or more so. (Barnard's test relaxes this constraint on one set of the <b>marginal</b> <b>totals.)</b> In the example, there are 11 such cases. Of these only one is more extreme {{in the same direction as}} our data; it looks like this: ...|$|E
50|$|These numbers {{can then}} be totaled, {{yielding}} both a grand total and <b>marginal</b> <b>totals.</b> Totaling the entire table, the number of true positives, false negatives, true negatives, and false positives add up to 100% of the set. Totaling the rows (adding horizontally) the number of true positives and false positives add up to 100% of the test positives, and likewise for negatives. Totaling the columns (adding vertically), the number of true positives and false negatives add up to 100% of the condition positives (conversely for negatives). The basic marginal ratio statistics are obtained by dividing the 2×2=4 values in the table by the <b>marginal</b> <b>totals</b> (either rows or columns), yielding 2 auxiliary 2×2 tables, {{for a total of}} 8 ratios. These ratios come in 4 complementary pairs, each pair summing to 1, and so each of these derived 2×2 tables can be summarized as a pair of 2 numbers, together with their complements. Further statistics can be obtained by taking ratios of these ratios, ratios of ratios, or more complicated functions.|$|E
5000|$|The act of {{conditioning}} on {{the marginal}} success rate from a 2x2 table {{can be shown}} to ignore some information in the data about the unknown odds ratio. The argument that the <b>marginal</b> <b>totals</b> are (almost) ancillary implies that the appropriate likelihood function for making inferences about this odds ratio should be conditioned on the marginal success rate. [...] Whether this lost information is important for inferential purposes {{is the essence of}} the controversy.|$|E
40|$|While {{estimation}} of the <b>marginal</b> (<b>total)</b> causal effect of a point exposure on an outcome {{is arguably the most}} common objective of experimental and observational studies in the health and social sciences, in recent years, investigators have also become increasingly interested in mediation analysis. Specifically, upon evaluating the total effect of the exposure, investigators routinely wish to make inferences about the direct or indirect pathways of the effect of the exposure, through a mediator variable or not, that occurs subsequently to the exposure and prior to the outcome. Although powerful semiparametric methodologies have been developed to analyze observational studies that produce double robust and highly efficient estimates of the <b>marginal</b> <b>total</b> causal effect, similar methods for mediation analysis are currently lacking. Thus, this paper develops a general semiparametric framework for obtaining inferences about so-called marginal natural direct and indirect causal effects, while appropriately accounting for a large number of pre-exposure confounding factors for the exposure and the mediator variables. Our analytic framework is particularly appealing, because it gives new insights on issues of efficiency and robustness in the context of mediation analysis. In particular, we propose new multiply robust locally efficient estimators of the marginal natural indirect and direct causal effects, and develop a novel double robust sensitivity analysis framework for the assumption of ignorability of the mediator variable. Comment: Published in at [URL] the Annals of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
40|$|Social {{navigation}} provides {{drivers with}} an advice {{that aims to}} shift traffic flows in a network from a user equilibrium to a system optimum. The advice is based on minimizing {{a combination of the}} individual travel time and the <b>marginal</b> <b>total</b> travel time in the network. The level of altruism defines {{the extent to which a}} driver is willing to sacrifice his own travel time in favor of other drivers. Fuel consumption and CO 2 emission are taking into account by converting them into time equivalent. Experimental results for a small network loosely representing the Bay area show that social navigation can reduce time delays by 10 %. The reduction is positively correlated with network load, market penetration and level of altruism...|$|R
50|$|The <b>total</b> <b>marginal</b> working {{population}} is 2187 in which marginal worker male population is 1463 and marginal worker female population is 724.|$|R
50|$|In {{cases where}} the {{expected}} value, E, {{is found to be}} small (indicating a small underlying population probability, and/or a small number of observations), the normal approximation of the multinomial distribution can fail, and in such cases it is found to be more appropriate to use the G-test, a likelihood ratio-based test statistic. When the total sample size is small, it is necessary to use an appropriate exact test, typically either the binomial test or (for contingency tables) Fisher's exact test. This test uses the conditional distribution of the test statistic given the marginal totals; however, it does not assume that the data were generated from an experiment in which the <b>marginal</b> <b>totals</b> are fixed and is valid whether or not that is the case.|$|E
50|$|The formula above {{gives the}} exact hypergeometric {{probability}} of observing this particular {{arrangement of the}} data, assuming the given <b>marginal</b> <b>totals,</b> on the null hypothesis {{that men and women}} are equally likely to be studiers. To put it another way, if we assume that the probability that a man is a studier is , the probability that a woman is a studier is also , and we assume that both men and women enter our sample independently of {{whether or not they are}} studiers, then this hypergeometric formula gives the conditional probability of observing the values a, b, c, d in the four cells, conditionally on the observed marginals (i.e., assuming the row and column totals shown in the margins of the table are given). This remains true even if men enter our sample with different probabilities than women. The requirement is merely that the two classification characteristics—gender, and studier (or not)—are not associated.|$|E
5000|$|John Rice wrote:85 Hodgkin's {{patients}} ... had {{a sibling}} {{of the same}} sexwho was free {{of the disease and}} whose age was within 5 years ofthe patient's. These investigators presented the following table:::They calculated a chi-squared statistic ... they had made an error in their analysis by ignoring the pairings.... their samples were not independent, because the siblings were paired ... we set up a table that exhibits the pairings:: It is to the second table that McNemar's test can be applied. Notice that the sum of the numbers in the second table is 85—the number of pairs of siblings—whereas the sum of the numbers in the first table is twice as big, 170—the number of individuals. The second table gives more information than the first. The numbers in the first table can be found by using the numbers in the second table, but not vice versa. The numbers in the first table give only the <b>marginal</b> <b>totals</b> of the numbers in the second table.|$|E
40|$|The {{relative}} cost {{of carbon}} emissions reductions across regions {{depends on whether}} we measure cost by <b>marginal</b> or <b>total</b> cost, private or economy-wide cost, and using market or purchasing power parity exchange rates. If all countries {{are on the same}} marginal carbon abatement cost curve then lower marginal costs of abatement are associated with higher energy intensities and higher total costs of abatement in achieving proportional cuts in emissions, equal emissions per capita, or common global carbon price targets. We test this conjecture using the results of the GTEM computable general equilibrium model as presented in the climate change economics review conducted by the Australian Treasury Department. Rankings of countries by costs do differ depending on whether <b>marginal</b> or <b>total</b> cost is used. But some regions, including OPEC and the former USSR, have high marginal costs and high emissions intensities and, therefore, high total costs and others like the EU relatively low <b>marginal</b> and <b>total</b> costs. Under a global emissions trading regime real economy-wide costs of abatement are higher in developing economies with currencies valued below purchasing power parity and large differences between private and economy-wide costs such as India contributing to the high GDP losses experienced in those countries. Climate change, costs, developing countries, computable general equilibrium, Environmental Economics and Policy, Q 52, Q 54,...|$|R
50|$|The <b>total</b> <b>{{marginal}}</b> Agricultural Labourers Population is 830 {{in which}} marginal Agricultural Labourers male population is 443 and marginal Agricultural Labourers female population is 387.|$|R
50|$|The <b>total</b> <b>{{marginal}}</b> Household Industries Population is 107 {{in which}} marginal Household Industries male population is 86 and marginal Household Industries female population is 21.|$|R
40|$|Description Raking {{a survey}} dataset entails re-weighting a sample {{by making the}} sample <b>marginal</b> <b>totals</b> agree with the {{population}} <b>marginal</b> <b>totals</b> for two survey response variables. Raking is a robust technique that is often useful for dealing with nonresponse. The 'rake ' package streamlines the process of Raking by creating the special 'rake ' class, which is essentially {{a summary of the}} sample weights...|$|E
40|$|Relaxed gravity models, that is, gravity {{models with}} {{upper and lower}} bounds on the <b>marginal</b> <b>totals,</b> often have {{solutions}} with the property that the upper or lower bounds are satisfied with equality for most <b>marginal</b> <b>totals.</b> In this paper modifications are presented for the relaxed gravity models, which overcome this unwanted model property. The paper contains illustrative examples with data from the county of O�stergo�tland, Sweden. ...|$|E
40|$|Four {{variants}} of the chi-square statistic are evaluated {{in terms of}} their adequacy as tests of association in 2 × 2 contingency tables. When both sets of <b>marginal</b> <b>totals</b> are fixed in advance, none of these variants is wholly satisfactory, even with moderately large samples. When one set of <b>marginal</b> <b>totals</b> is fixed in advance while the other is free to vary, or when neither set of <b>marginal</b> <b>totals</b> is fixed in advance, a variant of chi-square proposed by Upton (1982) is to be preferred both theoretically and in practice. The variant proposed by Yates (1934) which incorporates a correction for continuity and which is often recommended in statistics textbooks is theoretically unsound, shows an extremely conservative bias, and lacks sufficient power...|$|E
50|$|The <b>total</b> <b>{{marginal}}</b> Other Workers Population is 494 {{in which}} marginal Other Workers male population is 434 and marginal Other Workers female population is 60.|$|R
50|$|Örebro had {{a record}} cold December, and the coldest {{temperature}} for decades set in January with -28.3 C. Considering the airport's rural setting farther from Lake Hjälmaren at a slightly higher elevation, temperature extremes in the city proper may have been slightly less severe, although these differences would be <b>marginal.</b> In <b>total,</b> Örebro recorded 171 air frosts in 2010.|$|R
40|$|Monte Carlo {{resampling}} {{methods to}} obtain probability values for chi-squared and likelihood-ratio test statistics for multiway contingency tables are presented. A resampling algorithm provides random arrangements of cell frequencies in a multiway contingency table, given fixed <b>marginal</b> frequency <b>totals.</b> Probability values are {{obtained from the}} proportion of resampled test statistic values {{equal to or greater}} than the observed test statistic value...|$|R
40|$|We {{consider}} the asymptotic distribution {{of a cell}} in a 2 x [...] . x 2 contingency table as the fixed <b>marginal</b> <b>totals</b> tend to infinity. The asymptotic order of the cell variance is derived and a useful diagnostic is given for determining whether the cell has a Poisson limit or a Gaussian limit. There are three forms of Poisson convergence. The exact form is shown {{to be determined by}} the growth rates of the two smallest <b>marginal</b> <b>totals.</b> The results are generalized to contingency tables with arbitrary sizes and are further complemented with concrete examples...|$|E
3000|$|... [...]), called also Product-Binomial Sampling, is {{performed}} when the row totals (respectively columns) is fixed and we sample until this <b>marginal</b> <b>totals</b> are reached. Sampling Procedure III controls both {{row and column}} totals.|$|E
40|$|In multiplicative pricing of {{non-life}} insurance, {{we report}} a simulation study of mean square errors (MSEs) of point estimates by 1) the <b>marginal</b> <b>totals</b> method, and 2) the Standard GLM method of Poisson claim numbers and gamma distributed claim severities with constant coefficient of variation. MSEs per tariff cell are summed with insurance exposures as weights {{to give a}} total MSE. This is smallest for Standard GLM under the multiplicative assumption. But with moderate deviations from parameter multiplicativity, the study indicates that the <b>marginal</b> <b>totals</b> method is typically better in the MSE sense when there are many arguments and many claims, i. e. for mass consumer insurance. A method called MVW for confidence in-tervals, using only the Compound Poisson model, is given for the <b>marginal</b> <b>totals</b> method. These confidence intervals are compared with the ones of Standard GLM and the Tweedie method for risk premiums in a simulation study and {{are found to be}} mostly the best. The study reports both cover probabilities, which should be close to 0. 95 for 95 % confidence intervals, and interval lengths, which should be small. The Tweedie method is found to be never better than Standard GLM...|$|E
40|$|ABSTRACT This paper {{presents}} a probability {{assessment of the}} adequacy of protein intakes of toddlers (aged 18 - 30 mo) in study communities in Egypt, Kenya, and Mexico judged in relation to FAO/WHO/UNU estimates ofrequirements. Effects ofsupplemcnting amino acid intakes, or ofassuming lower bio-availability for lysine are also considered. In Egypt and Mexico existing protein intakes of toddlers were adequate. In Kenya existing intakes were <b>marginal.</b> <b>Total</b> protein intake was low and often lysine or tryptophan concentration was low. If Kenyan intakes met estimated energy requirements, protein intakes would be adequate. We conclude that protein intake {{is unlikely to be}} a primary limiting factor for toddler growth and devel-opment, and the benefit to be expected from increasing the intake of limiting amino acids is marginal. Reported associations of animal-source protein and energy with growth, size, and psy-chologic function of these toddlers arc unlikely to be causally attributable to inadequacy ofprotcin intakes. Am J Clin Nut...|$|R
50|$|As {{long as the}} {{individual}} makes withdrawals {{when he or she}} is in a lower tax bracket (that is, has a lower <b>marginal</b> tax rate), <b>total</b> taxes payable will be lower.|$|R
5000|$|Consumer {{demand for}} {{transactions}} structured in this bulk purchase model of consumption {{has led to}} the success of big-box stores. Although effected by <b>Marginal</b> cost, the <b>total</b> cost will never increase ...|$|R
