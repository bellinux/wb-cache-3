10000|266|Public
5|$|A <b>Markov</b> {{chain is}} a type of <b>Markov</b> process that has either {{discrete}} state space or discrete index set (often representing time), but the precise definition of a <b>Markov</b> chain varies. For example, it is common to define a <b>Markov</b> chain as a <b>Markov</b> process in either discrete or continuous time with a countable state space (thus regardless of the nature of time), but it is also common to define a <b>Markov</b> chain as having discrete time in either countable or continuous state space (thus regardless of the state space).|$|E
5|$|<b>Markov</b> {{processes}} and <b>Markov</b> chains are named after Andrey <b>Markov</b> who studied <b>Markov</b> chains {{in the early}} 20th century. <b>Markov</b> was interested in studying an extension of independent random sequences. In his first paper on <b>Markov</b> chains, published in 1906, <b>Markov</b> showed that under certain conditions the average outcomes of the <b>Markov</b> chain would converge to a fixed vector of values, so proving a weak law of large numbers without the independence assumption, which had been commonly regarded as a requirement for such mathematical laws to hold. <b>Markov</b> later used <b>Markov</b> chains to study the distribution of vowels in Eugene Onegin, written by Alexander Pushkin, and proved a central limit theorem for such chains.|$|E
5|$|<b>Markov</b> {{processes}} are stochastic processes, traditionally in discrete or continuous time, {{that have the}} <b>Markov</b> property, which means the next value of the <b>Markov</b> process depends on the current value, but it is conditionally independent of the previous values of the stochastic process. In other words, {{the behavior of the}} process in the future is stochastically independent of its behavior in the past, given {{the current state of the}} process.|$|E
5000|$|<b>Markov's</b> rule is the {{formulation}} of <b>Markov's</b> principle as a rule. It states that [...] is derivable as soon as [...] is, for [...] decidable. It was proved by Anne S. Troelstra that <b>Markov's</b> rule is an admissible rule in Heyting arithmetic. Later on, the logician Harvey Friedman showed that <b>Markov's</b> rule is an admissible rule in all of intuitionistic logic, Heyting arithmetic, and various other intuitionistic theories, using the Friedman translation.|$|R
40|$|In {{classical}} knot theory, <b>Markov's</b> theorem gives a way {{of describing}} all braids with isotopic closures as links in R^ 3. We present a version of <b>Markov's</b> theorem for extended loop braids with closure in B^ 3 × S^ 1, {{as a first step}} towards a <b>Markov's</b> theorem for extended loop braids and ribbon torus-links in R^ 4...|$|R
2500|$|<b>Markov's</b> {{inequality}} {{states that}} for any real-valued random variable Y and any positive number a, we have Pr(|Y|>a) ≤ E(|Y|)/a. One {{way to prove}} Chebyshev's inequality is to apply <b>Markov's</b> inequality to the random variable [...] with a = (kσ)2.|$|R
5|$|The Brownian motion {{process and}} the Poisson process (in one dimension) are both {{examples}} of <b>Markov</b> processes in continuous time, while random walks on the integers and the gambler's ruin problem are examples of <b>Markov</b> processes in discrete time.|$|E
5|$|Andrei Kolmogorov {{developed}} in a 1931 paper {{a large part of}} the early theory of continuous-time <b>Markov</b> processes. Kolmogorov was partly inspired by Louis Bachelier's 1900 work on fluctuations in the stock market as well as Norbert Wiener's work on Einstein's model of Brownian movement. He introduced and studied a particular set of <b>Markov</b> processes known as diffusion processes, where he derived a set of differential equations describing the processes. Independent of Kolmgorov's work, Sydney Chapman derived in a 1928 paper an equation, now called the Chapman–Kolmogorov equation, in a less mathematically rigorous way than Kolmogorov, while studying Brownian movement. The differential equations are now called the Kolmogorov equations or the Kolmogorov–Chapman equations. Other mathematicians who contributed significantly to the foundations of <b>Markov</b> processes include William Feller, starting in the 1930s, and then later Eugene Dynkin, starting in the 1950s.|$|E
5|$|Stochastic {{matrices}} are square matrices whose rows are probability vectors, that is, whose {{entries are}} non-negative and sum up to one. Stochastic matrices {{are used to}} define <b>Markov</b> chains with finitely many states. A row of the stochastic matrix gives the probability distribution for the next position of some particle currently in the state that corresponds to the row. Properties of the <b>Markov</b> chain like absorbing states, that is, states that any particle attains eventually, can be read off the eigenvectors of the transition matrices.|$|E
40|$|We {{explore the}} {{relationship}} between Brouwer's intuitionistic mathematics and Euclidean geometry. Brouwer wrote a paper in 1949 called "The contradictority of elementary geometry". In that paper, he showed that a certain classical consequence of the parallel postulate implies <b>Markov's</b> principle, which he found intuitionistically unacceptable. But Euclid's geometry, having served as a beacon of clear and correct reasoning for two millenia, is not so easily discarded. Brouwer started from a "theorem" {{that is not in}} Euclid, and requires <b>Markov's</b> principle for its proof. That means that Brouwer's paper did not address the question whether Euclid's "Elements" really requires <b>Markov's</b> principle. In this paper we show that there is a coherent theory of "non-Markovian Euclidean geometry. " We show in some detail that our theory is an adequate formal rendering of (at least) Euclid's Book~I, and suffices to define geometric arithmetic, thus refining the author's previous investigations (which include <b>Markov's</b> principle as an axiom). Philosophically, Brouwer's proof that his version of the parallel postulate implies <b>Markov's</b> principle could be read just as well as geometric evidence for the truth of <b>Markov's</b> principle, if one thinks the geometrical "intersection theorem" with which Brouwer started is geometrically evident. Comment: 61 pages, 18 figure...|$|R
40|$|In {{this paper}} we give a new {{proof of the}} {{characterization}} of the closed fragment of the provability logic of Heyting’s Arithmetic. We also provide a characterization of the closed fragment of the provability logic of Heyting’s Arithmetic plus <b>Markov’s</b> Principle and Heyting’s Arithmetic plus Primitive Recursive <b>Markov’s</b> Principle...|$|R
2500|$|... {{using the}} <b>Markov's</b> {{inequality}} and the expectation derived previously.|$|R
25|$|<b>Markov</b> and {{his younger}} brother Vladimir Andreevich <b>Markov</b> (1871–1897) proved <b>Markov</b> brothers' inequality.|$|E
25|$|Andrey <b>Markov</b> studied <b>Markov</b> chains in {{the early}} 20th century. <b>Markov</b> was {{interested}} in studying an extension of independent random sequences, motivated by a disagreement with Pavel Nekrasov who claimed independence was necessary for the weak law of large numbers to hold. In his first paper on <b>Markov</b> chains, published in 1906, <b>Markov</b> showed that under certain conditions the average outcomes of the <b>Markov</b> chain would converge to a fixed vector of values, so proving a weak law of large numbers without the independence assumption, which had been commonly regarded as a requirement for such mathematical laws to hold. <b>Markov</b> later used <b>Markov</b> chains to study the distribution of vowels in Eugene Onegin, written by Alexander Pushkin, and proved a central limit theorem for such chains.|$|E
25|$|Hidden <b>Markov</b> Model (HMM) is a {{statistical}} <b>Markov</b> {{model in which}} the system being modeled {{is assumed to be}} a <b>Markov</b> process with unobserved (i.e. hidden) states.|$|E
5000|$|Then for , <b>Markov's</b> {{inequality}} and {{the independence of}} [...] implies: ...|$|R
30|$|In {{the next}} lemma we prove <b>Markov’s</b> {{inequality}} from the axioms.|$|R
30|$|Proof.[*]By using <b>Markov’s</b> {{inequality}} and Theorem 2, {{the proof is}} straightforward.|$|R
25|$|A <b>Markov</b> {{chain is}} a type of <b>Markov</b> process that has either {{discrete}} state space or discrete index set (often representing time), but the precise definition of a <b>Markov</b> chain varies. For example, it is common to define a <b>Markov</b> chain as a <b>Markov</b> process in either discrete or continuous time with a countable state space (thus regardless of the nature of time), but it is also common to define a <b>Markov</b> chain as having discrete time in either countable or continuous state space (thus regardless of the state space).|$|E
25|$|A <b>Markov</b> {{process is}} a {{stochastic}} process which satisfies the <b>Markov</b> property with respect to its natural filtration.|$|E
25|$|If {{every state}} can reach an {{absorbing}} state, then the <b>Markov</b> chain is an absorbing <b>Markov</b> chain.|$|E
50|$|This {{identity}} {{is used in}} a simple proof of <b>Markov's</b> inequality.|$|R
5000|$|Chebyshev's {{inequality}} {{follows from}} <b>Markov's</b> inequality {{by considering the}} random variable ...|$|R
25|$|In {{constructive}} mathematics, the unbounded search operator {{is related}} to <b>Markov's</b> principle.|$|R
25|$|The use of <b>Markov</b> chains in <b>Markov</b> chain Monte Carlo methods covers {{cases where}} the process follows a {{continuous}} state space.|$|E
25|$|For an {{overview}} of <b>Markov</b> chains on a general state space, see the article <b>Markov</b> chains on a measurable state space.|$|E
25|$|In {{the context}} of <b>Markov</b> processes, memorylessness refers to the <b>Markov</b> property, an even {{stronger}} assumption which implies that the properties of random variables related to the future depend only on relevant information about the current time, not on information from further in the past. The present article describes the use outside the <b>Markov</b> property.|$|E
2500|$|For a nonnegative random {{variable}} [...] and , the <b>Markov's</b> inequality states that ...|$|R
50|$|In {{constructive}} mathematics, the unbounded search operator {{is related}} to <b>Markov's</b> principle.|$|R
5000|$|For a nonnegative random {{variable}} [...] and , the <b>Markov's</b> inequality states that ...|$|R
25|$|The <b>Markov</b> {{blanket of}} a node is {{the set of}} nodes {{consisting}} of its parents, its children, and any other parents of its children. The <b>Markov</b> blanket renders the node independent {{of the rest of}} the network; the joint distribution of the variables in the <b>Markov</b> blanket of a node is sufficient knowledge for calculating the distribution of the node. X is a Bayesian network with respect to G if every node is conditionally independent of all other nodes in the network, given its <b>Markov</b> blanket.|$|E
25|$|<b>Markov</b> {{chains are}} used in finance and {{economics}} to model {{a variety of different}} phenomena, including asset prices and market crashes. The first financial model to use a <b>Markov</b> chain was from Prasad et al. in 1974. Another was the regime-switching model of James D. Hamilton (1989), in which a <b>Markov</b> chain is used to model switches between periods high and low GDP growth (or alternatively, economic expansions and recessions). A more recent example is the <b>Markov</b> Switching Multifractal model of Laurent E. Calvet and Adlai J. Fisher, which builds upon the convenience of earlier regime-switching models. It uses an arbitrarily large <b>Markov</b> chain to drive the level of volatility of asset returns.|$|E
25|$|In {{recursive}} Bayesian estimation, {{the true}} state {{is assumed to}} be an unobserved <b>Markov</b> process, and the measurements are the observed states of a hidden <b>Markov</b> model (HMM).|$|E
5000|$|<b>Markov's</b> {{inequality}} {{requires only}} {{the following information}} on a random variable X: ...|$|R
5000|$|... #Caption: The {{region of}} {{interest}} for which <b>Markov's</b> inequality gives a lower bound.|$|R
5000|$|For a non-negative {{continuous}} {{random variable}} having an expectation, <b>Markov's</b> inequality states that ...|$|R
