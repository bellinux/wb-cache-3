530|538|Public
5000|$|User Datagram Protocol (UDP) echo, for VoIP jitter and <b>Mean</b> <b>Opinion</b> <b>Score</b> (MOS) ...|$|E
50|$|The ACR {{scale is}} also used for {{telephony}} voice quality to give a <b>Mean</b> <b>Opinion</b> <b>Score.</b>|$|E
5000|$|... where MOS is the <b>mean</b> <b>opinion</b> <b>score</b> and &sigma; is the {{standard}} deviation of the MOS.|$|E
50|$|POLQA results principally model <b>mean</b> <b>opinion</b> <b>scores</b> (MOS) {{that cover}} {{a scale from}} 1 (bad) to 5 (excellent).|$|R
5000|$|PSQM testing under ideal {{conditions}} yields <b>Mean</b> <b>Opinion</b> <b>Scores</b> of 4.45 for G.711 μ-law, 4.45 for G.711 A-law ...|$|R
5000|$|PSQM testing under network stress yields <b>Mean</b> <b>Opinion</b> <b>Scores</b> of 4.13 for G.711 μ-law, 4.11 for G.711 A-law ...|$|R
50|$|Circuit Merit - a {{measurement}} process {{designed to assess}} the quality of voice, as input for calculating the <b>Mean</b> <b>Opinion</b> <b>Score</b> in wired and wireless telephone circuits.|$|E
5000|$|The {{main idea}} of {{measuring}} subjective video quality {{is similar to}} the <b>Mean</b> <b>Opinion</b> <b>Score</b> (MOS) evaluation for audio. To evaluate the subjective video quality of a video processing system, the following steps are typically taken: ...|$|E
50|$|Genista Corporation uses {{computational}} {{models of}} human visual and auditory systems to measure what viewers see and hear. Resulting perceptual metrics complement existing technologies by predicting experienced quality {{measured by a}} <b>Mean</b> <b>Opinion</b> <b>Score</b> (MOS) formerly assessed using subjective tests from actual viewers.|$|E
5000|$|PSQM testing under ideal {{conditions}} yields <b>Mean</b> <b>Opinion</b> <b>Scores</b> of 4.04 for G.729a, compared to 4.45 for G.711 (μ-law) ...|$|R
5000|$|PSQM testing under network stress yields <b>Mean</b> <b>Opinion</b> <b>Scores</b> of 3.51 for G.729a, {{compared}} to 4.13 for G.711 (μ-law) ...|$|R
5000|$|PSQM testing under ideal {{conditions}} yields <b>Mean</b> <b>Opinion</b> <b>Scores</b> of 4.08 for G.723.1 (6.3 kbit/s), compared to 4.45 for G.711 (μ-law) ...|$|R
50|$|The ACR {{scale is}} {{evaluated}} based on {{numbers that are}} assigned to the individual items, where Excellent equals to 5 and Bad equals to 1. The average numeric score over all experiment participants, for each test condition that was shown, is called the <b>Mean</b> <b>Opinion</b> <b>Score.</b>|$|E
50|$|The Circuit Merit {{system is}} a {{measurement}} process designed to assess the voice-to-noise ratio in wired and wireless telephone circuits, and although its reporting scale is sometimes used as input for calculating <b>Mean</b> <b>Opinion</b> <b>Score,</b> the rating system is officially defined relative to given ranges of voice-to-noise ratios.|$|E
50|$|POLQA {{covers a}} model to predict speech quality, by means of digital speech signal analysis. The {{predictions}} of those objective measures should come {{as close as possible}} to subjective quality scores as obtained in subjective listening tests. Usually, a <b>Mean</b> <b>Opinion</b> <b>Score</b> (MOS) is predicted. POLQA uses real speech as a test stimulus for assessing telephony networks.|$|E
5000|$|PSQM testing under ideal {{conditions}} yields <b>Mean</b> <b>Opinion</b> <b>Scores</b> of 4.14 for AMR (12.2 kbit/s), compared to 4.45 for G.711 (µ-law) ...|$|R
5000|$|PSQM testing under ideal {{conditions}} yields <b>mean</b> <b>opinion</b> <b>scores</b> of 4.14 for iLBC (15.2 kbit/s), compared to 4.3 for G.711 (µ-law) ...|$|R
5000|$|PSQM testing under ideal {{conditions}} yields <b>Mean</b> <b>Opinion</b> <b>Scores</b> of 4.30 for G.726 (32 kbit/s), compared to 4.45 for G.711 (µ-law) ...|$|R
50|$|The {{reason for}} {{measuring}} subjective video quality {{is the same}} as for measuring the <b>Mean</b> <b>Opinion</b> <b>Score</b> for audio. Opinions of experts can be averaged, and the average mark is usually given with confidence interval. Additional procedures can be used for averaging. For example, experts who give unstable results may be rejected (for instance, if their correlation with average opinion is low).|$|E
50|$|<b>Mean</b> <b>opinion</b> <b>score</b> (MOS) is {{a measure}} used {{in the domain of}} Quality of Experience and {{telecommunications}} engineering, representing overall quality of a stimulus or system. It is the arithmetic mean over all individual “values on a predefined scale that a subject assigns to his opinion of the performance of a system quality”. Such ratings are usually gathered in a subjective quality evaluation test, but they can also be algorithmically estimated.|$|E
5000|$|Opinions {{of viewers}} are {{typically}} averaged into the <b>Mean</b> <b>Opinion</b> <b>Score</b> (MOS). To this aim, the labels of categorical scales may {{be translated into}} numbers. For example, the responses [...] "bad" [...] to [...] "excellent" [...] can be mapped to the values 1 to 5, and then averaged. MOS values should always be reported with their statistical confidence intervals so that the general agreement between observers can be evaluated.|$|E
5000|$|PSQM testing under network stress yields <b>Mean</b> <b>Opinion</b> <b>Scores</b> of 3.57 for G.723.1 (6.3 kbit/s), {{compared}} to 4.13 for G.711 (μ-law) ...|$|R
5000|$|PSQM testing under network stress yields <b>Mean</b> <b>Opinion</b> <b>Scores</b> of 3.79 for AMR (12.2 kbit/s), {{compared}} to 4.13 for G.711 (µ-law) ...|$|R
5000|$|In Cisco Voice over IP deployments, the K-Factor is a Cisco {{proprietary}} {{method of}} reporting endpoint <b>mean</b> <b>opinion</b> <b>scores.</b> [...] K-factor records contain the following metrics: ...|$|R
5000|$|PEVQ (Perceptual Evaluation of Video Quality) is an {{end-to-end}} (E2E) measurement algorithm {{to score}} the picture {{quality of a}} video presentation {{by means of a}} 5-point <b>mean</b> <b>opinion</b> <b>score</b> (MOS). It is therefore a video quality model. PEVQ was benchmarked by the Video Quality Experts Group (VQEG) {{in the course of the}} Multimedia Test Phase 2007-2008. Based on the performance results, in which the accuracy of PEVQ was tested against ratings obtained by human viewers, PEVQ became part of the new International Standard ITU-T Rec. J. 247 (2008).|$|E
50|$|HEVC MP {{has also}} been {{compared}} with H.264/MPEG-4 AVC HP for subjective video quality. The video encoding was done for entertainment applications and four different bitrates were made for nine video test sequences with a HM-5.0 HEVC encoder being used. The subjective assessment was done at an earlier date than the PSNR comparison and so it used {{an earlier version of}} the HEVC encoder that had slightly lower performance. The bit rate reductions were determined based on subjective assessment using <b>mean</b> <b>opinion</b> <b>score</b> values. The overall subjective bitrate reduction for HEVC MP compared with H.264/MPEG-4 AVC HP was 49.3%.|$|E
50|$|In a {{subjective}} video performance comparison released in May 2014, the JCT-VC compared the HEVC Main profile to the H.264/MPEG-4 AVC High profile. The comparison used <b>mean</b> <b>opinion</b> <b>score</b> values and {{was conducted by}} the BBC and the University of the West of Scotland. The video sequences were encoded using the HM-12.1 HEVC encoder and the JM-18.5 H.264/MPEG-4 AVC encoder. The comparison used a range of resolutions and the average bit rate reduction for HEVC was 59%. The average bit rate reduction for HEVC was 52% for 480p, 56% for 720p, 62% for 1080p, and 64% for 4K UHD.|$|E
50|$|PESQ results principally model <b>mean</b> <b>opinion</b> <b>scores</b> (MOS) {{that cover}} {{a scale from}} 1 (bad) to 5 (excellent). A mapping {{function}} to MOS-LQO is outlined under P.862.1.|$|R
50|$|Since {{objective}} video quality {{models are}} expected to predict results given by human observers, they are developed {{with the aid of}} subjective test results. During development of an objective model, its parameters should be trained so as to achieve the best correlation between the objectively predicted values and the subjective scores, often available as <b>mean</b> <b>opinion</b> <b>scores</b> (MOS).|$|R
30|$|For each of {{the five}} {{subjective}} tests (one audio, two video, two audiovisual), the scores were averaged over subjects, yielding <b>mean</b> <b>opinion</b> <b>scores</b> (MOS), were linearly transformed to the 5 -point ACR MOS scale by aligning the numbers of the scales, and further transformed to the 100 -point model scale using the conversion defined in ITU-T Recommendation G. 107 [14].|$|R
50|$|PSQM uses a psychoacoustical {{mathematical}} modeling (both perceptual and cognitive) algorithm {{to analyze}} the pre and post transmitted voice signals, yielding a PSQM value which {{is a measure of}} signal quality degradation and ranges from 0 (no degradation) to 6.5 (highest degradation). In turn, this result may be translated into a <b>Mean</b> <b>Opinion</b> <b>Score</b> (MOS), which is an accepted measure of the perceived quality of received media on a numeric scale ranging from 1 to 5. A value of 1 indicates unacceptable, poor quality voice while a value of 5 indicates high voice quality with no perceptible issues.|$|E
50|$|The {{main goal}} of many {{objective}} video quality metrics is to automatically estimate the average user's (viewer's) {{opinion on the}} quality of a video processed by a system. Procedures for subjective video quality measurements are described in ITU-R recommendation BT.500 and ITU-T recommendation P.910. Their main idea is the same as in <b>Mean</b> <b>Opinion</b> <b>Score</b> for audio: video sequences are shown to a group of viewers and then their opinion is recorded and averaged to evaluate the quality of each video sequence. However, the testing procedure may vary depending on what kind of system is tested.|$|E
5000|$|In {{contrast}} to network related measures like throughput, QoE is typically not measured on ratio scales. Hence, fairness measures like Jain's fairness index cannot be applied, as the measurement scale requires to be a ratio scale with a clearly defined zero point (see examples of misuse for coefficients of variation). QoE may be measure on interval scales. A typical {{example is a}} 5-point <b>mean</b> <b>opinion</b> <b>score</b> (MOS) scale, with 1 indicating lowest quality and 5 indicating highest quality. While the coefficient of variation is meaningless, the standard deviation [...] provides {{a measure of the}} dispersion of QoE among users.|$|E
40|$|This paper {{describes}} the SVOX system architecture and the steps we took {{to integrate the}} Blizzard database into our system. The results of the Blizzard evaluation show that SVOX {{is a leader in}} small and large footprint unit selection. Analysis of the <b>mean</b> <b>opinion</b> <b>scores</b> for specific sentences shows where our system can improve most. Some recommendations for future Blizzard Challenges are also made...|$|R
30|$|In this dataset, {{there are}} 88 models, between 40 K and 50 K vertices, which were {{generated}} from four reference objects: Armadillo, Venus, Dinosaur, and RockerArm. Two types of distortion, noise addition and smoothing, were applied with different strengths at four locations: {{on the whole}} model, on smooth areas, on rough areas, and on intermediate areas. The dataset also includes <b>mean</b> <b>opinion</b> <b>scores</b> (MOS) from 12 observers and 7 static metric results for these models.|$|R
40|$|In this study, we {{investigate}} {{the extent to}} which an image-difference metric based on structural similarity can correlate with human judgment. We introduce a modied version of the recently published iCID metric and present new results over two large image quality databases. It is particularly noteworthy that the proposed metric yields a correlation of 0. 861 with <b>mean</b> <b>opinion</b> <b>scores</b> on the 2013 version of the renowned Tampere Image Database, without dedicated parameter optimization. 1...|$|R
