1437|1628|Public
2500|$|The Kalman filter is {{a minimum}} <b>mean-square</b> <b>error</b> estimator. The {{error in the}} a posteriori state {{estimation}} is ...|$|E
2500|$|The {{design of}} [...] {{remains an open}} question. One way of {{proceeding}} is to identify a system which generates the estimation error and setting [...] equal to the inverse of that system. This procedure may be iterated to obtain <b>mean-square</b> <b>error</b> improvement {{at the cost of}} increased filter order. The same technique can be applied to smoothers.|$|E
50|$|The <b>mean-square</b> <b>error,</b> as a {{function}} of filter weights is a quadratic function which means it has only one extremum, that minimises the <b>mean-square</b> <b>error,</b> which is the optimal weight. The LMS thus, approaches towards this optimal weights by ascending/descending down the mean-square-error vs filter weight curve.|$|E
3000|$|... for a fixed {{quantizer}} bit rate. And the optimization is {{with respect}} to perceived image quality rather than <b>mean-squared</b> <b>error</b> or some visually weighted variant of <b>mean-squared</b> <b>error.</b>|$|R
40|$|Predicting {{the value}} of a random {{variable}} Y, based on the observed value of another random variable X is a common objective of data analysis. It is well-known that the minimum <b>mean-squared</b> <b>error</b> predictor of Y is the mean of the conditional distribution of Y, given X. In cases where Y is necessarily integer-valued, the conditional mean is not always a feasible value for Y and, therefore, is an unsatisfactory predicted value. In this paper, it is shown how minimum <b>mean-squared</b> <b>error</b> integer-valued predictors can be obtained. <b>Mean-squared</b> <b>error</b> integer-valued distributions unbiased prediction...|$|R
40|$|The {{paper is}} {{concerned}} with the problem of estimating (predicting) unobserved counts based on partially observed multinomial panel data from several independent realizations of finite Markov chains. The minimum <b>mean-squared</b> <b>error</b> predictors depend on unknown model parameters which need to be estimated from the same data. The maximum likelihood estimates of the parameters and their limit distributions are derived. The decomposition and approximations of the prediction <b>mean-squared</b> <b>error</b> of the estimated predictors are discussed. Inference for multinomal distributions Best linear unbiased predictors Prediction <b>mean-squared</b> <b>error</b> approximation Maximum likelihood estimation Panel data...|$|R
5000|$|Using the Minimum <b>mean-square</b> <b>error</b> criterion, {{take the}} gradient: ...|$|E
5000|$|... where [...] {{represents}} the <b>mean-square</b> <b>error</b> and [...] is a convergence coefficient.|$|E
5000|$|The <b>mean-square</b> <b>error</b> (MSE) {{from the}} FOE applied to an {{accelerating}} target is ...|$|E
40|$|A novel {{approach}} for comparing sequences of observations using an explicit-expansion kernel is demonstrated. The kernel is derived using {{the assumption of}} {{the independence of the}} sequence of observations and a <b>mean-squared</b> <b>error</b> training criterion. The use of an explicit expansion kernel reduces classifier model size and computation dramatically, resulting in model sizes and computation one-hundred times smaller in our application. The explicit expansion also preserves the computational advantages of an earlier architecture based on <b>mean-squared</b> <b>error</b> training. Training using standard support vector machine methodology gives accuracy that significantly exceeds the performance of state-of-the-art <b>mean-squared</b> <b>error</b> training for a speaker recognition task. ...|$|R
40|$|<b>Mean-squared</b> <b>errors</b> {{of surface}} geostrophic {{velocity}} estimates from the crossover and parallel-track methods are calculated for altimeters in the Ocean Topography Experiment (TOPEX) /Poseidon and Jason orbits. As {{part of the}} crossover method analysis, the filtering properties and errors of cross-track speed estimates are examined. Velocity estimates from both the crossover and parallel-track methods have substantial <b>mean-squared</b> <b>errors</b> that exceed 20...|$|R
40|$|The {{problem of}} {{accurately}} estimating the <b>mean-squared</b> <b>error</b> of small area estimators within a Fay-Herriot normal error model is studied theoretically {{in the common}} setting where the model is fitted to a logarithmically transformed response variable. For bias-corrected empirical best linear unbiased predictor small area point estimators, <b>mean-squared</b> <b>error</b> formulae and estimators are provided, with biases of smaller order than the reciprocal {{of the number of}} small areas. The performance of these <b>mean-squared</b> <b>error</b> estimators is illustrated by a simulation study and a real data example relating to the county level estimation of child poverty rates in the US Census Bureau's on-going 'Small area income and poverty estimation' project. Copyright 2006 Royal Statistical Society. ...|$|R
5000|$|Use the Minimum <b>mean-square</b> <b>error</b> {{criterion}} {{over all}} of [...] by setting its gradient to zero: ...|$|E
5000|$|D. Fried, [...] "Time-delay-induced <b>mean-square</b> <b>error</b> in {{adaptive}} optics," [...] J. Opt. Soc. Am. A 7, 1224-1225 (1990).|$|E
5000|$|The Kalman filter is {{a minimum}} <b>mean-square</b> <b>error</b> estimator. The {{error in the}} a posteriori state {{estimation}} is ...|$|E
40|$|Noncoherent minimum <b>mean-squared</b> <b>error</b> (MMSE) -based {{multiuser}} receivers {{are proposed}} for nonorthogonal multipulse modulation. The three proposed receivers {{have a common}} MMSE pre-filter and are followed by one of three distinct noncoherent decision rules. The simplest decision rule selects the maximum magnitude of the MMSE filter outputs {{and the other two}} decision rules account for the second-order statistics of the residual multiple-access interference that remains after MMSE filtering. An adaptive algorithm based on the stochastic approximation method is then proposed for the distributed implementation of these receivers. The adaptation is shown to converge in the <b>mean-squared</b> <b>error</b> sense to the deterministic MMSE pre-filter. The convergence analysis yields insight on the trade-off between the rate of convergence and the residual <b>mean-squared</b> <b>error...</b>|$|R
3000|$|... in the {{residual}} signals, the AFC filters are optimized by minimizing the <b>mean-squared</b> <b>error</b> {{of the overall}} residual signals (38).|$|R
5000|$|In statistics, Stein's {{unbiased}} {{risk estimate}} (SURE) is an unbiased estimator of the <b>mean-squared</b> <b>error</b> of [...] "a nearly arbitrary, nonlinear biased estimator." [...] In other words, {{it provides an}} indication of the accuracy of a given estimator. This is important since the true <b>mean-squared</b> <b>error</b> of an estimator {{is a function of the}} unknown parameter to be estimated, and thus cannot be determined exactly. The technique is named after its discoverer, Charles Stein.|$|R
5000|$|... where [...] are {{spectral}} densities. Provided that [...] is optimal, {{then the}} minimum <b>mean-square</b> <b>error</b> equation reduces to ...|$|E
50|$|Minimizing MSE {{is a key}} {{criterion}} {{in selecting}} estimators: see minimum <b>mean-square</b> <b>error.</b> Among unbiased estimators, minimizing the MSE is equivalent to minimizing the variance, and the estimator that does this is the minimum variance unbiased estimator. However, a biased estimator may have lower MSE; see estimator bias.|$|E
50|$|In the introduction, we {{mentioned}} that the truncated Karhunen-Loeve expansion was the best approximation of the original process {{in the sense that}} it reduces the total <b>mean-square</b> <b>error</b> resulting of its truncation. Because of this property, it is often said that the KL transform optimally compacts the energy.|$|E
5000|$|The {{importance}} of SURE {{is that it}} is an unbiased estimate of the <b>mean-squared</b> <b>error</b> (or squared error risk) of , i.e.with ...|$|R
40|$|Abstract—In this paper, {{we study}} the zero-forcing (ZF) and minimum <b>mean-squared</b> <b>error</b> (MMSE) {{algorithms}} for a multiple-input multiple-output (MIMO) relay network and compare their performance {{in terms of}} bit-error-rate (BER). In particular, we investigate their performance with and without using the successive interference cancellation (SIC) at the receiver. Our results demonstrate that the system performance can be significantly improved by using the SIC technique. Index Terms—multiple-input multiple-output (MIMO), relay networks, zero-forcing (ZF), minimum <b>mean-squared</b> <b>error</b> (MMSE), successive-interference-cancellation (SIC). I...|$|R
3000|$|... [...]. The {{complexity}} of the equivalent detector based on the minimum <b>mean-squared</b> <b>error</b> (MMSE) or the subspace-based eigenstructure analysis {{is a function of}} [...]...|$|R
5000|$|For the <b>mean-square</b> <b>error</b> {{distortion}} criterion, it can {{be easily}} shown that the optimal set of reconstruction values [...] is given by setting the reconstruction value [...] within each interval [...] to the conditional expected value (also {{referred to as the}} centroid) within the interval, as given by: ...|$|E
50|$|This {{is often}} used for {{deciding}} how many predictor variables to use in regression. Without cross-validation, adding predictors always reduces the residual sum of squares (or possibly leaves it unchanged). In contrast, the cross-validated <b>mean-square</b> <b>error</b> will tend to decrease if valuable predictors are added, but increase if worthless predictors are added.|$|E
5000|$|The {{design of}} [...] {{remains an open}} question. One way of {{proceeding}} is to identify a system which generates the estimation error and setting [...] equal to the inverse of that system. This procedure may be iterated to obtain <b>mean-square</b> <b>error</b> improvement {{at the cost of}} increased filter order. The same technique can be applied to smoothers.|$|E
40|$|Because {{of a lack}} of {{a priori}} information, the minimum <b>mean-squared</b> <b>error</b> predictor, the {{conditional}} expectation, is often not known for a non-Gaussian time series. We show that the nonparametric kernel regression estimator of the conditional expectation is mean-squared consistent for a time series: When used as a predictor, the estimator asymptotically matches the <b>mean-squared</b> <b>error</b> produced by the true conditional expectation. We also describe a more computationally efficient predictor based on the recursive kernel regression estimator, and show it can asymptotically achieve <b>mean-squared</b> <b>errors</b> arbitrarily close to the conditional expectation. Numerical examples are provided to demonstrate the effectiveness of nonparametric prediction. 1 Introduction Time-series prediction is a problem frequently encountered in many branches of science and engineering. In this problem, we would like to predict future values of a time series based on its previous observations. In practice, because li [...] ...|$|R
40|$|For {{life testing}} procedures, a Bayesian {{analysis}} is developed {{with respect to}} a random intensity parameter in the Poisson distribution. Bayes estimators are derived for the Poisson parameter and the reliability function based on uniform and gamma prior distributions of that parameter. A Monte Carlo procedure is implemented to make possible an empirical <b>mean-squared</b> <b>error</b> comparison between Bayes and existing minimum variance unbiased, as well as maximum likelihood, estimators. As expected, the Bayes estimators have <b>mean-squared</b> <b>errors</b> that are appreciably smaller than those of the other two...|$|R
40|$|Abstract — The paper studies {{distributed}} average {{consensus in}} sensor networks, when the sensors exchange quantized data at each time step. We show that randomizing the exchanged sensor data {{by adding a}} controlled amount of dither results in almost sure (a. s.) convergence of the protocol, if the network is connected. We explicitly characterize the <b>mean-squared</b> <b>error</b> (with respect to the desired consensus average) and show that, by tuning certain parameters associated with the protocol, the <b>mean-squared</b> <b>error</b> can be made arbitrarily small. We study the trade-offs between the rate of convergence and the resulting <b>mean-squared</b> <b>error.</b> The sensor network topology {{plays an important role}} in determining the convergence rate of the algorithm. Our approach, based on the convergence of controlled Markov processes, is very generic and can be applied to many other situations of imperfect communication. Finally, we present numerical studies, which verify our theoretical results...|$|R
50|$|The James-Stein {{estimator}} is a nonlinear estimator {{which can}} be shown to dominate, or outperform, the ordinary least squares technique {{with respect to a}} <b>mean-square</b> <b>error</b> loss function. Thus least squares estimation is not necessarily an admissible estimation procedure. Some others of the standard estimates associated with the normal distribution are also inadmissible: for example, the sample estimate of the variance when the population mean and variance are unknown.|$|E
50|$|The zero-forcing {{equalizer}} removes all ISI, and {{is ideal}} when the channel is noiseless. However, when the channel is noisy, the zero-forcing equalizer will amplify the noise greatly at frequencies f where the channel response H(j2&pi;f) {{has a small}} magnitude (i.e. near zeroes of the channel) {{in the attempt to}} invert the channel completely. A more balanced linear equalizer in this case is the minimum <b>mean-square</b> <b>error</b> equalizer, which does not usually eliminate ISI completely but instead minimizes the total power of the noise and ISI components in the output.|$|E
5000|$|Michael Korenberg of Queen's University in Kingston, Ontario, {{developed}} {{a method for}} choosing a sparse set of components from an over-complete set, such as sinusoidal components for spectral analysis, called fast orthogonal search (FOS). Mathematically, FOS uses a slightly modified Cholesky decomposition in a <b>mean-square</b> <b>error</b> reduction (MSER) process, implemented as a sparse matrix inversion. [...] As with the other LSSA methods, FOS avoids the major shortcoming of discrete Fourier analysis, and can achieve highly accurate identifications of embedded periodicities and excels with unequally spaced data; the fast orthogonal search method has also been applied to other problems such as nonlinear system identification.|$|E
5000|$|In this section, {{we examine}} the {{technique}} for designing 2-D IIR filters based on minimizing error functionals in the frequency domain. The <b>mean-squared</b> <b>error</b> is given as ...|$|R
40|$|This paper {{considers}} fundamental {{limits for}} solving sparse inverse {{problems in the}} presence of Pois-son noise with physical constraints. Such problems arise in a variety of applications, including photon-limited imaging systems based on compressed sensing. Most prior theoretical results in compressed sensing and related inverse problems apply to idealized settings where the noise is i. i. d., and do not ac-count for signal-dependent noise and physical sensing constraints. Prior results on Poisson compressed sensing with signal-dependent noise and physical constraints in [1] provided upper bounds on mean squared error performance for a specific class of estimators. However, it was unknown whether those bounds were tight or if other estimators could achieve significantly better performance. This work pro-vides minimax lower bounds on <b>mean-squared</b> <b>error</b> for sparse Poisson inverse problems under physical constraints. Our lower bounds are complemented by minimax upper bounds. Our upper and lower bounds reveal that due to the interplay between the Poisson noise model, the sparsity constraint and the physical constraints: (i) the <b>mean-squared</b> <b>error</b> does not depend on the sample size n other than to ensure the sensing matrix satisfies RIP-like conditions and the intensity T of the input signal plays a critical role; and (ii) the <b>mean-squared</b> <b>error</b> has two distinct regimes, a low-intensity and a high-intensity regime and the transition point from the low-intensity to high-intensity regime depends on the input signal f∗. In the low-intensity regime the <b>mean-squared</b> <b>error</b> is independent of T while in the high-intensity regime, the <b>mean-squared</b> <b>error</b> scales as s log pT, where s is the sparsity level, p is the number of pixels or parameters and T is the signal intensity. ...|$|R
40|$|A new {{approach}} to low-complexity channel estimation in orthogonal-frequency division multiplexing (OFDM) systems is described. A low-rank approximation is applied to a linear minimum <b>mean-squared</b> <b>error</b> (LMMSE) estimator that uses the frequency correlation of the channel. By using the singular-value decomposition (SVD) an optimal low-rank estimator is derived, where performance is essentially preserved - even for low computational complexities. A fixed estimator, with nominal values for channel correlation and signal-to-noise ratio (SNR), is analysed. Analytical <b>mean-squared</b> <b>error</b> (MSE) and symbol-error rates (SER) are presented for a 16 -QAM OFDM system. Godkänd; 1996; 20080220 (ysko...|$|R
