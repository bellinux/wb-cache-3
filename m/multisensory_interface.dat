5|23|Public
40|$|Tactile {{displays}} {{have been}} proposed as a <b>multisensory</b> <b>interface</b> technology that can relieve the typically overburdened visual channel of operators. This study compared the ability of operators, while simultaneously completing a tracking task, to detect and identify system faults in a monitoring task with three types of alert cues: tactile, visual, and redundant tactile and visual. For the tactile display, the location and vibration pulse speed of two tactors were mapped to four system faults. Response time was significantly faster with the tactile cue. Also, the tactile cue resulted in less interference with the concurrent tracking task, while not degrading vigilance to an additional concurrent visual monitoring task. These results suggest that further tactile cue research is warranted to examine potential applications in complex systems, such as control stations for unmanned aerial vehicles...|$|E
40|$|ABSTRACT: This study {{demonstrates}} {{that the use of}} augmented reality interfaces, designed based on a theoretically driven human-centered framework, can significantly improve task performance. The domain of unmanned ground vehicle landmine/ unexploded ordnance detection and disposal was selected because it is a real-world example of a complex human-robot interaction task. Participants in this study were assigned to one of seven experimental conditions. A new method (the augmented <b>multisensory</b> <b>interface</b> design, or AMID, method) was used to identify two unisensory and one multisensory performance-enhancing augmented reality (AR) interface solutions, along with three less appropriate interface solutions. All participants were trained in the operation of the AR devices they were assigned and also in telerobotic landmine detection operations. After completing the training, participants were given the task of clearing a minefield. Results show that participants in the AMID-compliant solution groups significantly outperformed the control group and their non-AMID-compliant group counterparts in terms of the numbers of landmines correctly identi-fied. The AMID-compliant multisensory condition was found to result in significantly increased landmine detection, increased sensitivity, and decreased task time...|$|E
40|$|The {{last few}} years have seen many {{exciting}} developments {{in the area of}} tactile and <b>multisensory</b> <b>interface</b> design. One of the most rapidly moving practical application areas for these findings is in the development of warning signals and information displays for drivers. For instance, tactile displays can be used to awaken sleepy drivers, to capture the attention of distracted drivers, and even to present more complex information to drivers who may be visually overloaded. This review highlights the most important potential costs and benefits {{associated with the use of}} tactile and multisensory information displays in a vehicular setting. Multisensory displays that are based on the latest cognitive neuroscience research findings can capture driver attention significantly more effectively than their unimodal (i. e., tactile) counterparts. Multisensory displays can also be used to transmit information more efficiently, as well as to reduce driver workload. Finally, we highlight the key questions currently awaiting further research, including: Are tactile warning signals really intuitive? Are there certain regions of the body (or the space surrounding the body) where tactile/multisensory warning signals are particularly effective? To what extent is the spatial coincidence and temporal synchrony of the individual sensory signals critical to determining the effectiveness of multisensory displays? And, finally, how does the issue of compliance versus reliance (or the "cry wolf" phenomenon associated with the presentation of signals that are perceived as false alarms) influence the effectiveness of tactile and/ or multisensory warning signals? © 2008 IEEE. link_to_subscribed_fulltex...|$|E
50|$|Rebecca Allen (born 1954) is an {{international}} artist inspired {{by a variety of}} media to create work from 3-D computer graphics, animation, music videos, video games, performance works, artificial life systems, <b>multisensory</b> <b>interfaces,</b> interactive installations, virtual and mixed reality. A pioneer in the field of computer art, her work addresses humanizing technology.|$|R
40|$|This book is {{dedicated}} to furthering the design of ergonomic <b>multisensory</b> <b>interfaces</b> by highlighting recent evidence in this area emerging from the fast-growing field of cognitive neuroscience. It focuses primarily on two aspects of driver information-processing: multisensory interactions and the spatial distribution of attention in driving. © Cristy Ho and Charles Spence, 2008. All rights reserved...|$|R
40|$|The {{emergence}} of immersive documents, which allow the “reader” to perceive unreality as real, is foreseen. This {{new type of}} document will evolve from the combination of contemporary participatory, transmedia storytelling with pervasive computing technologies and <b>multisensory</b> <b>interfaces.</b> It is argued that a research program within library and information science is needed, to investigate new information behaviors associated with such documents, the new digital literacies needed to make effective use of them, and {{their place in the}} information communication chain...|$|R
40|$|Simulation Theory (ST) and ‘Theory of Mind’ theory (ToM) are the {{traditional}} approaches to social cognition. A recent alternative to ST and ToM is the InteractionTheory (IT). IT grounds social cognition in embodied interactive processes. In this perspective, Krueger introduced {{the notion of}} ‘we space’ to mean a ‘body-centered action- space’, encompassing spatial representations in neuroscience. However, Krueguer intend the “we space” as the “reciprocal and joint actions between individuals” (i. e. coordination); Thus, the spatial representation occurring between individuals is overlooked. In neuroscience, the concept of Peripersonal Space (PPS), i. e. the space around the body, captures {{the idea of a}} <b>multisensory</b> <b>interface</b> for interactions. However, most research in this field focuses on the representation of one’s own body, whereas little is known about spatial representations between two bodies, between the Self and the Other. The research discussed in the article aims to demonstrate the following hypotheses: 1) PPS representation varies {{as a function of the}} presence of other people (co- presence). 2) PPS representation varies as a function of (even high level) social interactions. In order to test these hypotheses, we proposed two experiments where PPS representation was measured. In Experiment 1, we show that the boundaries of PPS changes when subjects observe another individual placed in far space, rather than a dummy. In Experiment 2, we demonstrate that cooperative interactions extend the boundaries of PPS between the self and the other. Such new findings suggest a link between high-level cognition, such as social cognition, and low-level sensorimotor representations, such as PPS...|$|E
40|$|Research in {{the last}} four decades has brought a {{considerable}} advance in our understanding of how the brain synthesizes information arising from different sensory modalities. Indeed, many cortical and subcortical areas, beyond those traditionally considered to be ‘associative,’ {{have been shown to be}} involved in multisensory interaction and integration (Ghazanfar and Schroeder 2006). Visuo-tactile interaction is of particular interest, because of the prominent role played by vision in guiding our actions and anticipating their tactile consequences in everyday life. In this chapter, we focus on the functional role that visuo-tactile processing may play in driving two types of body-object interactions: avoidance and approach. We will first review some basic features of visuo-tactile interactions, as revealed by electrophysiological studies in monkeys. These will prove to be relevant for interpreting the subsequent evidence arising from human studies. A crucial point that will be stressed is that these visuo-tactile mechanisms have not only sensory, but also motor-related activity that qualifies them as multisensory-motor interfaces. Evidence will then be presented for the existence of functionally homologous processing in the human brain, both from neuropsychological research in brain-damaged patients and in healthy participants. The final part of the chapter will focus on some recent studies in humans showing that the human motor system is provided with a <b>multisensory</b> <b>interface</b> that allows for continuous monitoring of the space near the body (i. e., peripersonal space). We further demonstrate that multisensory processing can be modulated on-line as a consequence of interacting with objects. This indicates that, far from being passive, the monitoring of peripersonal space is an active process subserving actions between our body and objects located in the space around us...|$|E
40|$|Current {{multimedia}} {{design guidelines}} support {{the development of}} effective <b>multisensory</b> <b>interfaces</b> but pay scant attention to the broader context in which educational multimedia is deployed. Here a multimodal approach was used to encourage senior managers to learn a complex management topic. Through co-design of multimedia with a learning context, otherwise inaccessible content was shown to have wide appeal. The aim of this short paper {{is to show that}} to confidently design effective multimedia learning experiences, it is necessary to co-design multimedia with a broader learning context...|$|R
40|$|The paper proposes an {{alternative}} approach to well-known feedback solutions, such as visual displays or warning sound MESsages, to make users perceptually aware of the energy consumption occurring when using a product. The approach is grounded {{on the use of}} <b>multisensory</b> feedback <b>interfaces</b> that are designed to make the user experience the consumption process directly during the interaction with the product. Such multisensory feedback should be intended as indications, rather then alarms, so as to naturally guide users towards a more sustainable behaviour. The daily task of opening the fridge door has been used as case study. All the steps followed to ideate and test the effectiveness of the designed <b>multisensory</b> <b>interfaces</b> are discussed. The results demonstrate how even simple stimuli, such as a gradual colour change of the fridge cavity from a cold to a warm one, may be able to reduce the time users keep the fridge door open...|$|R
40|$|The {{sense of}} touch can provide an effective, but at present underutilized, {{alternative}} to vision for information presentation to drivers. A series of empirical studies was designed to investigate the possibility of improving driver responses to potential critical emergency situations by the presentation of directional haptic warning signals. The results demonstrated the potential utility of haptic (and combined auditory and haptic) warning signals in capturing the visual attention of drivers to the direction requiring their immediate attention. These results {{have important implications for}} the increasingly popular <b>multisensory</b> <b>interfaces.</b> © 2008 SICE. link_to_subscribed_fulltex...|$|R
40|$|Abstract: The {{nanoworld}} is {{invaded by}} human technologies but remains foreign to our perception and action. Discussions about “nanos ” {{in the society}} ask for knowledge dissemination in population. Simultaneously, scientists in biology, chemistry [...] . need tools to directly manipulate single nano-object. Both aspects can be now addressed by coupling real time <b>multisensory</b> <b>interfaces</b> to advanced sensors and actuators, either real or virtual. Man exploration and activity at nanoscale using this instrument need a real time transformation of nanosensor data collected and of manipulator actions on a virtual scene. That this system enables one to adapt its senses to nanoworld for discovery and action is the challenge {{at the heart of}} this research...|$|R
40|$|Session " Enaction, Instruments & Objects" : 4 pagesInternational audienceThe {{nanoworld}} is {{invaded by}} human technologies but remains foreign to our perception and action. Discussions about "nanos" {{in the society}} ask for knowledge dissemination in population. Simultaneously, scientists in biology, chemistry [...] . need tools to directly manipulate single nano-object. Both aspects can be now addressed by coupling real time <b>multisensory</b> <b>interfaces</b> to advanced sensors and actuators, either real or virtual. Man exploration and activity at nanoscale using this instrument need a real time transformation of nanosensor data collected and of manipulator actions on a virtual scene. That this system enables one to adapt its senses to nanoworld for discovery and action is the challenge {{at the heart of}} this research...|$|R
40|$|Haptic environments are user {{interfaces}} incorporating a haptic display device, most commonly a point force device {{such as the}} PHANToM. Lederman and Klatzky’s Exploratory Procedures work casts doubt on the usability of such devices, as the human haptic system can only perform rapidly and accurately when full hand contact is used rather than a single finger. However, this work has not been extended to virtual objects displayed by point force devices. Usability of <b>multisensory</b> <b>interfaces</b> is even more complex. How does {{the addition of a}} force display change performance in a graphical system? This dissertation presents two benchmark tasks for human performance with point force displays. Stimuli were generated using Koenderink’s shape and curvedness scales for smooth quadric surfaces. The first task, psychophysical magnitude estimation of curvature of paraboloid stimuli, was used to analyze the contribution of haptics to a predominantly visual task. Estimates using vision alone made slightly better discriminations than estimates using both senses, although the effect only approached signif-v icance. The second task extended Lederman and Klatzky’s shape recognition work t...|$|R
40|$|Utilizing user-centred {{system design}} and {{evaluation}} method {{has become an}} increasingly important tool to foster better usability {{in the field of}} virtual environments (VEs). In recent years, although it is still the norm that designers and developers are concerning the technological advancement and striving for designing impressive multimodal <b>multisensory</b> <b>interfaces,</b> more and more awareness are aroused among the development team that in order to produce usable and useful interfaces, it is essential to have users in mind during design and validate a new design from users 2 ̆ 7 perspective. In this paper, we describe a user study carried out to validate a newly developed haptically enabled virtual training system. By taking consideration of the complexity of individual differences on human performance, adoption and acceptance of haptic and audio-visual I/O devices, we address how well users learn, perform, adapt to and perceive object assembly training. We also explore user experience and interaction with the system, and discuss how multisensory feedback affects user performance, perception and acceptance. At last, we discuss how to better design VEs that enhance users perception, their interaction and motor activity. <br /...|$|R
40|$|The {{present study}} investigates human multisensory {{perception}} {{of sound and}} vibration, highlighting its potential impact {{in the design of}} novel user interfaces, including those used in the automobile industry. Specifically, the present study investigates whether frontback sound localization could be altered by concurrent whole-body vibration. Previous research has shown that, when auditory and tactile stimuli are presented synchronously but from different positions, the perceived location of the auditory event is mislocalized of the tactile stimulus. Here, sounds were presented at the front or the back of participants, in isolation, or together with vibrations. Participants made a three-alternative forced choice regarding their perceived location of the sounds. Results indicate that front-back sound localization was affected by the presence of concurrent vibrations, which biased the localization of front sounds towards the partipants' rear space. Since the perceived location of events modulates the perceivers' understanding and involvement in these events, the possibility of manipulating the location of sound events using vibrations has a potential for the design of <b>multisensory</b> <b>interfaces</b> such as those included in automotive applications, where it is strongly needed to capture the attention of drivers, to provide navigational information, and to reduce sensory load...|$|R
40|$|A {{multi-user}} Virtual World {{has been}} implemented combining a flexible-object simulator with a <b>multisensory</b> user <b>interface,</b> including hand motion and gestures, speech input and output, sound output, and 3 -D stereoscopic graphics with head-motion parallax. The implementation {{is based on}} a distributed client/server architecture with a centralized Dialogue Manager. The simulator is inserted into the Virtual World as a server. A discipline for writing interaction dialogues provides a clear conceptual hierarchy and the encapsulation of state. This hierarchy facilitates the creation of alternative interaction scenarios and shared multiuser environments...|$|R
40|$|Rotator’ is {{a web-based}} <b>multisensory</b> {{analysis}} <b>interface</b> that enables users to shift streams of multichannel scientific data between their auditory and visual sensory channels {{in order to}} better discern structure and anomaly in the data. This paper provides a technical overview of the Rotator tool as well as a discussion of the motivations for integrating flexible data display into future analysis and monitoring frameworks. An audio-visual presentation mode in which only a single stream is visualized at any given moment is identified as a particularly promising alternative to a purely visual information display mode. Auditory and visual excerpts from the interface are available at [URL]...|$|R
40|$|Objective: We {{report a}} series of three {{experiments}} designed to assess the relative speed with which people can initiate speeded head-orienting responses following the presentation of spatial warning signals. Background: Recent cognitive neuroscience findings have shown that the human brain tends to treat stimuli occurring in peripersonal space as being somehow more behaviorally relevant and attention demanding than stimuli occurring in extrapersonal space. These brain mechanisms may be exploited in the design of warning signals. Method: Experiment 1 assessed the effectiveness of various different unisensory warning signals in eliciting a head-turning response to look at the potential source of danger requiring participants' immediate attention; Experiment 2 assessed the latency of a driver's responses to events occurring in the cued direction; Experiment 3 assessed the relative effectiveness of various warning signals in reorienting a person's gaze back to a central driving task while he or she was distracted by a secondary task. Results: The results show that participants initiated head-turning movements and made speeded discrimination or braking responses significantly more rapidly following the presentation of a close rear auditory warning signal than following the presentation of either a far frontal auditory warning signal, a vibrotactile warning signal presented to their waist, or a peripheral visual warning signal. Conclusion: These results support the claim that the introduction of peripersonal warning signals results in a significant performance advantage relative to traditionally designed warnings. Application: Warning systems that have been designed around the constraints of the human brain offer great potential in the future design of <b>multisensory</b> <b>interfaces.</b> Copyright © 2009, Human Factors and Ergonomics Society. link_to_subscribed_fulltex...|$|R
40|$|This article {{provides}} an overview of an ongoing program of research designed to investigate the effectiveness of haptic cuing to redirect a user’s visual spatial attention under various conditions using a visual change detection paradigm. Participants visually inspected displays consisting of rectangular horizontal and vertical elements in order to try and detect an orientation change in one of the elements. Prior to performing the visual task on each trial, the participants were tapped on the back from one of four locations by a vibrotactile stimulator. The validity of the haptic cues (i. e., the probability that the tactor location coincided with the quadrant where the visual target occurred) was varied. Response time was recorded and eye-position monitored with an eyetracker. Under conditions where the validity of the haptic cue was high (i. e., when the cue predicted the likely target quadrant), initial saccades predominantly went to the cued quadrant and response times were significantly faster as compared to the baseline condition where no haptic cuing was provided. When the cue validity was low (i. e., when the cue provided no information with regard to the quadrant in which the visual target might occur), however, the participants were able to ignore haptic cuing as instructed. Furthermore, a spotlight effect was observed in that the response time increased as the visual target moved away {{from the center of the}} cued quadrant. These results have implications for the designers of multimodal (or <b>multisensory)</b> <b>interfaces</b> where a user can benefit from haptic attentional cues in order to detect and/or process the information from...|$|R
40|$|Emotional {{events may}} {{interrupt}} ongoing cognitive processes and automatically grab attention, modulating the subsequent perceptual processes. Hence, emotional eliciting stimuli might effectively {{be used in}} warning applications, where a fast and accurate response from users is required. In addition, conveying information through an optimum multisensory combination {{can lead to a}} further enhancement of user responses. In the present study we investigated the emotional response to sounds differing in their acoustic spectra, and their influence on speeded detection of auditory-somatosensory stimuli. Higher sound frequencies resulted in an increase in emotional arousal. We suggest that emotional processes might be responsible for the different auditory-somatosensory integration patterns observed for low and high frequency sounds. The presented results might have important implications for the design of auditory and <b>multisensory</b> warning <b>interfaces...</b>|$|R
40|$|Presented at the 14 th International Conference on Auditory Display (ICAD 2008) on June 24 - 27, 2008 in Paris, France. Emotional {{events may}} {{interrupt}} ongoing cognitive processes and automatically grab attention, modulating the subsequent perceptual processes. Hence, emotional eliciting stimuli might effectively {{be used in}} warning applications, where a fast and accurate response from users is required. In addition, conveying information through an optimum multisensory combination {{can lead to a}} further enhancement of user responses. In the present study we investigated the emotional response to sounds differing in their acoustic spectra, and their influence on speeded detection of auditory-somatosensory stimuli. Higher sound frequencies resulted in an increase in emotional arousal. We suggest that emotional processes might be responsible for the different auditory-somatosensory integration patterns observed for low and high frequency sounds. The presented results might have important implications for the design of auditory and <b>multisensory</b> warning <b>interfaces...</b>|$|R
40|$|The {{last decade}} {{has seen a}} surge of {{interest}} in the development of affective driver interfaces designed to enhance driver safety and convey useful information to the driver. These technological advances have brought about changes in the ambient driving environment and clearly have the potential to enhance the driving experience in the years to come. This review provides an overview of existing research approaches to the study of these innovations. Other possible psychophysiological approaches to affective driver interface design, such as via mood induction procedures, are also discussed. Finally, we highlight the likely impact of the latest findings on the topic of multisensory integration research for the future design of affective driver interfaces. In particular, we look at how <b>multisensory</b> driver <b>interfaces</b> may evolve in the future, and assess the likely impact of multisensory warning signals that have been designed specifically to trigger the brain's defensive circuits. Copyright © 2013 Inderscience Enterprises Ltd...|$|R
40|$|Players of {{computer}} games {{tend to be}} discerning about game quality. So, to be successful, game designers need to ensure that players receive the best possible experience. A growing trend {{in the design of}} game interfaces is the use of multi-sensory (visual, auditory and haptic) interfaces to broaden the experience for players. The assumption is that, by displaying different information to different senses, it is possible {{to increase the amount of}} information available to players and so assist their performance. To test this assumption, the first-person shooter game, "Quake 3 : Arena", was evaluated in four modes: with only visual cues; with both visual and auditory cues; with both visual and haptic cues; and with visual, auditory and haptic cues. Players reported improved ‘immersion’, ‘confidence’ and ‘satisfaction’ when additional sensory cues were included, the <b>multisensory</b> game <b>interface</b> seemed to improve the player's experience, but there was no statistically significant improvement in their performance. We suspect that a better design of the information being displayed for each sense may be required if multi-sensory displays are to significantly improve the player’s performance on specific game tasks...|$|R
40|$|Virtual environment, virtual prototyping, {{geometric}} modeling, multi-modal multi-sensory interface, collaborative {{virtual environment}}, distributed virtual environment Shape modeling plays {{a vital role}} in the design process but often it is the most tedious task in the whole design cycle. In recent years the Computer Aided Design (CAD) industry has evolved through a number of advances and developments in design methodology. However, modeling in these CAD systems requires expertise and in-depth understanding of the modeling process, user interface and the CAD system itself, resulting in increased design cycle time. To overcome these problems a new methodology and a system called “Detailed Virtual Design System ” (DVDS) has been developed for detailed shape modeling in a multi-modal, multi-sensory Virtual Environment (VE). This system provides an intuitive and natural way of designing using hand motions, gestures and voice commands. Due to the lack of effective collaborative design, visualization and analysis tools, designers spend a considerable amount of time and effort in the group discussion during design process. To enable multiple designers to effectively and efficiently collaborate in a design environment, framework of a collaborative virtual environment, called “Virtual Environment to Virtual Environment ” (V 2 V), has been discussed. This framework allows same site and remote site multi-modal, <b>multisensory</b> immersive <b>interface</b> between designers...|$|R
40|$|Industrial design covers {{design of}} {{products}} taking all senses into account. However, most literature on product design focuses on vision. When designing <b>multisensory</b> human-machine <b>interfaces</b> tools and processes for design of sounds are needed. Sketching {{is essential in}} design. Schon and Wiggins discussed designers’ use of sketching and suggested that designers interact with the medium (typically pen and paper) in a seeing – moving – seeing way of working. First, the designer sketch to see a problem, then tries a solution by changing the sketch or suggesting a new sketch, and finally evaluates the solution by visual inspection. We suggest that design of sounds evolves through a similar process requiring the designer to listen, move and listen again. This process is facilitated by considering sounds created throughout the process as sketches. A case was studied where six designers were given the task to design a sound logotype for a car. Their processes were analyzed and compared with actions in visual sketching using pen and paper. The results {{support the idea of}} considering sound design as a listening – moving – listening process. Designing of sounds is a conversation with sounding material and crucially dependent on listening. By encouraging sound designers to use sounds as sketches during the design process creativity and efficiency were promoted. Godkänd; 2014; 20141210 (arny...|$|R
40|$|Today, {{the tests}} {{of a new}} product in its {{conceptual}} and design stage can be performed by using digital models owning various levels of complexity. The level of complexity depends on the nature and on {{the accuracy of the}} tests that have to be performed. Besides, the tests can involve or not the interaction with humans. Particularly, this second aspect {{must be taken into account}} when developing the simulation model. In fact, this introduces a different kind of complexity with respect to simulations where humans are not involved. Simulation models used for numerical analyses of the behavior of the product (such as Finite Element Analysis, multi-body analysis, etc.) are typically named Digital Mock-Ups. Instead, simulations that are interactive in their nature, requiring humans-in-the-loop, are named interactive Virtual Prototypes. They cannot be intended as a simple upgrade of a CAD model of a product, but they are instead a combination of functional models, mapped into sensorial terms and then accessed through <b>multisensory</b> and multimodal <b>interfaces.</b> In this paper, the validity of this concept is demonstrated through some case studies where interactive Virtual Prototypes are used to substitute the corresponding physical ones during activities concerning the product conceptualization and design...|$|R
40|$|International audienceAudio-visual speech {{perception}} {{is a special}} case of <b>multisensory</b> processing that <b>interfaces</b> with the linguistic system. One important issue is whether cross-modal interactions only depend on well-known auditory and visuo-facial modalities or, rather, might also be triggered by other sensory sources less common in speech communication. The present EEG study aimed at investigating cross-modal interactions not only between auditory, visuo-facial and audio-visuo-facial syllables but also between auditory, visuo-lingual and audio-visuo-lingual syllables. Eighteen adults participated in the study, none of them being experienced with visuo-lingual stimuli. The stimuli were acquired {{by means of a}} camera and an ultrasound system, synchronized with the acoustic signal. At the behavioral level, visuo-lingual syllables were recognized far above chance, although to a lower degree than visuo-labial syllables. At the brain level, audiovisual interactions were estimated by comparing the EEG responses to the multisensory stimuli (AV) to the combination of responses to the stimuli presented in isolation (A+V). For both visuo-labial and visuo-lingual syllables, a reduced latency and a lower amplitude of P 2 auditory evoked potentials were observed for AV compared to A+V. Apart from this sub-additive effect, a reduced amplitude of N 1 and a higher amplitude of P 2 were also observed for lingual compared to labial movements. Although participants were not experienced with visuo-lingual stimuli, our results demonstrate {{that they were able to}} recognize them and provide the first evidence for audio-visuo-lingual speech interactions. These results further emphasize the multimodal nature of speech perception and likely reflect the impact of listener's knowledge of speech production...|$|R

