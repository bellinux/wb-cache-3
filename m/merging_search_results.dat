3|10000|Public
40|$|<b>Merging</b> <b>search</b> <b>results</b> from {{different}} servers {{is a major}} problem in Distributed Information Retrieval. We used Regression-SVM and Ranking-SVM which would learn a function that merges results based on information that is readily available: i. e. the ranks, titles, summaries and URLs contained in the results pages. By not downloading additional information, such as the full document, we decrease bandwidth usage. CORI and Round Robin merging were used as our baselines; surprisingly, our results show that the SVMmethods do not improve over those baselines...|$|E
40|$|Two new {{techniques}} for <b>merging</b> <b>search</b> <b>results</b> are introduced: Feature Distance ranking algorithms and Reference Statistics. These techniques are {{compared with other}} published methods, using TREC effectiveness evaluations based on human relevance judgements and input rankings from 5 different search engines over 5 disjoint document collections. The {{new techniques}} {{are found to be}} more effective than existing methods in an isolated-server environment such as the World Wide Web. In addition, Feature Distance algorithms are found to be as effective in an isolated-server environment using Reference Statistics as they are in an integrated-server environment...|$|E
40|$|For {{efficient}} {{organization of}} speech recordings – meetings, interviews, voice mails, and lectures – {{being able to}} search for spoken keywords is essential. Today, most spoken document retrieval systems use large-vocabulary recognition. For the above scenarios, such systems suffer from the unpredictable domain, out-ofvocabulary queries, and generally high word-error rate (WER). In [1], we presented a system for phonetic indexing and searching of spontaneous speech. It is vocabulary-independent and based on phoneme lattices. In the present paper, we propose to combine it with word-based search into a hybrid approach. We explore two methods of combination: posterior combination (<b>merging</b> <b>search</b> <b>results</b> of a word-based and a phoneme-based system) and prior combination (combining word and phoneme language models and vocabularies to form a hybrid recognizer). The search accuracy of our best purely phonetic baseline is 64 % (Figure of Merit), and our purely word-based baselines are below 50 %. The new hybrid approach achieves 73 %, if the recognizer uses a language model that matches the test-set domain. With a mismatched language model, 71 % is achieved. Our {{results show that the}} proposed hybrid model benefits from the best of two worlds: Word-level language context and robustness of phonetic search to unknown words and domain mismatch. 1...|$|E
40|$|We {{have several}} {{different}} commercial Web search engines that are available. It is ex-pected {{that the combination}} of the results from different Web search engines has some impact on the accuracy of question-answering (QA) for Web documents, be-cause the <b>search</b> <b>results</b> are not identical and the combination increases the variety of information source. However, as far as we know, there are no studies on the ef-fect of combining different Web search en-gines on QA. In this paper, we examined the effect of the combination on QA. We investi-gated three different methods to combine <b>search</b> <b>results</b> from different <b>search</b> en-gines in the process of QA. The first one is a conventional method that straightfor-wardly <b>merges</b> <b>search</b> <b>results</b> from differ-ent <b>search</b> engines, then, feeds the unified <b>search</b> <b>result</b> into one QA engine. On the other hand, the second one and third one are our proposal methods that feed each <b>search</b> <b>result</b> from individual <b>search</b> en-gine to a QA engine separately, then merge the answer candidates. The experimental result showed that the methods that merge the answer candidates after QA are more effective than the method that <b>merges</b> the <b>search</b> <b>results</b> before QA. ...|$|R
40|$|This paper {{documents}} {{a comprehensive}} empirical {{study of the}} effects of heterogeneous information combination on large scale social image search. Our goal is to investigate how various kinds of information source can contribute the improvement of the retrieval effectiveness. In particular, a linear combination has been applied to <b>merge</b> <b>search</b> <b>results</b> from <b>search</b> module based on textual and visual features. Also, we propose different weighting schemes to integrate different kinds of query evidences in a nonlinear way. A series of experiments have been conducted using two large scale social image collections. Empirical results suggest that the system based on textual features yields much more effective and reliable results comparing to one using visual information. Further, the combination of two information sources can consistently enhance the final accuracy...|$|R
40|$|We {{propose a}} {{multiple}} representations approach {{to tackle the}} problem of content-based image retrieval effectiveness. Multiple representations {{is based on the}} use of multiple models or representations and make them cooperate to improve search effectiveness. We consider the case of homogeneous textures. Texture is represented using two different models: the well-known autoregressive model and a perceptual model based on perceptual features such as coarseness and directionality. In the case of the perceptual model, two viewpoints are considered: perceptual features are computed on original images and on the autocovariance function corresponding to original images. Thus, we use a total of three representations (models and viewpoints) to represent texture content. Simple results fusion models are used to <b>merge</b> <b>search</b> <b>results</b> returned by each of the three representations. Benchmarking carried out on the well-known Brodatz database using the recall graph is presented. Retrieval relevance (effectiveness) is improved in a very appreciable way with the fused model...|$|R
40|$|With the {{continued}} expansion of Electronic Patient Record systems ahead of comprehensive evidence, metrics, or future-proofing, European health informatics is {{embarking on a}} faith-driven adventure that also risks data swamping of end-users. An alternative approach is an information broker system, drawing from departmental data sources. A three-year study in health and social care has produced a first demonstrator which can search for specified information in heterogeneous distributed data stores, with source-specific permission can copy it, and then <b>merge</b> the <b>search</b> <b>results</b> in a real-time process...|$|R
40|$|Information {{retrieval}} (IR) in peer-to-peer (P 2 P) networks, {{where the}} corpus is spread across many loosely coupled peers, has recently gained importance. In contrast to IR systems on a centralized server or server farm, P 2 P IR faces the additional challenge of either being oblivious to global corpus statistics or having {{to compute the}} global measures from local statistics at the individual peers in an efficient, distributed manner. One specific measure of interest is the global document frequency for different terms, which would be very beneficial as term-specific weights in the scoring and ranking of <b>merged</b> <b>search</b> <b>results</b> that have been obtained from different peers. This paper presents an efficient solution for the problem of estimating global document frequencies in a large-scale P 2 P network with very high dynamics where peers can join and leave the network on short notice. In particular, the developed method {{takes into account the}} fact that the local document collections of autonomous peers may arbitrarily overlap, so that global counting needs to be duplicateinsensitive. The method is based on hash sketches as a technique for compact data synopses. Experimental studies demonstrate the estimator’s accuracy, scalability, and ability to cope with high dynamics. Moreover, the benefit for ranking P 2 P <b>search</b> <b>results</b> is shown by experiments with real-world Web data and queries. 1...|$|R
40|$|Abstract. Advertising {{mechanisms}} for search engines (i. e., sponsored search auctions) have recently {{received a lot}} of attention in the scientific community. Advertisers bid on keywords and, when a user enters key-words for her search, the search engines uses an auction mechanism to select the list of sponsored links to display alongside the <b>search</b> <b>results.</b> In this paper, we make a first attempt to extend the currently available {{mechanisms for}} sponsored search auctions to the new paradigms of search computing. According to them, multiple federated domain-specific search engines are integrated by a special search engine (called integrator). The user can enter a multi-domain query that is decomposed by the integra-tor in single-domain queries and these are singularly addressed to the most appropriate domain-specific search engine. The integrator <b>merges</b> the <b>search</b> <b>results.</b> We propose a business model for this scenario and we develop an economic mechanism for it resorting to the automated mechanism design approach. ...|$|R
40|$|The recent {{increase}} of domain{specific search engines, able to discover information unknown by general-purpose search engines, leads to their federation {{into a single}} entity, called federated search engine. In this paper, we focus on how it can effectively <b>merge</b> sponsored <b>search</b> <b>results,</b> provided by the domain-specific search engines, into a unique list. In particular, we discuss the {{case in which the}} same ad can be provided by multiple sources, which requires information about the ad to be merged. We approach the problem of merging and sharing the revenue using mechanism design techniques. The main impossibility result we obtain points out there exists no mechanism that satisfies the customarily required properties. Thus, we present several mechanisms that violate at most one of these properties, and we experimentally analyze them using a real-world Yahoo! dataset...|$|R
40|$|Meta Search Engines {{are finding}} tools {{developed}} for enhancing the search performance by submitting user queries to multiple search engines and combining the <b>search</b> <b>results</b> in a unified ranked list. They utilized data fusion technique, which requires three major steps: databases selection, the results combination, {{and the results}} merging. This study tries to build a framework {{that can be used}} for <b>merging</b> the <b>search</b> <b>results</b> retrieved from any set of search engines. This framework based on answering three major questions: 1. How meta-search developers could define the optimal rank order for the selected engines. 2. How meta-search developers could choose the best search engines combination. 3. What is the optimal heuristic merging function that could be used for aggregating the rank order of the retrieved documents form incomparable search engines. The main data collection process depends on running 40 general queries on three major search engines (Google, AltaVista, and Alltheweb). Real users have involved in the relevance judgment process for a five point relevancy scale. The performance of the thre...|$|R
40|$|Distributed search {{systems are}} an {{emerging}} phenomenon in Web search, in which independent topic-specific search engines provide search services, and metasearchers distribute user's queries to {{only the most}} suitable search engines. Previous research has investigated methods for engine selection and <b>merging</b> of <b>search</b> <b>results</b> (i. e. performance improvements from the user's perspective). We focus instead on performance from the service provider's point of view (e. g, income from queries processed vs. resources used to answer them). We analyse a scenario in which individual search engines compete for user queries by choosing which documents (topics) to index. The challenge is that the utilities of an engine's actions should depend on the uncertain actions of competitors. Thus, naive strategies (e. g, blindly indexing lots of popular documents) are ineffective. We model the competition between search engines as a stochastic game, and propose a reinforcement learning approach to managing search index contents. We evaluate our approach using a large log of user queries to 47 real search engines...|$|R
40|$|Distributed {{heterogeneous}} search {{systems are}} an emerging phenomenon in Web search, in which independent topic-specific search engines provide search services, and metasearchers distribute user's queries to {{only the most}} suitable search engines. Previous research has investigated methods for engine selection and <b>merging</b> of <b>search</b> <b>results</b> (i. e. performance improvements from the user's perspective). We focus instead on performance from the service provider's point of view (e. g, income from queries processed vs. resources used to answer them). We consider a scenario in which individual search engines compete for user queries by choosing which documents (topics) to index. The difficulty here {{stems from the fact}} that the utilities of local engine actions should depend on the uncertain actions of competitors. Thus, naive strategies (e. g, blindly indexing lots of popular documents) are ineffective. We model the competition between search engines as a stochastic game, and propose a reinforcement learning approach to managing search index contents. We evaluate our approach using a large log of user queries to 47 real search engines...|$|R
40|$|We {{examine the}} {{feasibility}} of fusing the outputs of multiple text retrieval engines to improve accuracy. We tested three Web-based search engines (Excite for Web Servers, Infoseek's Ultraseek Server, and Sony Search Engine) over a 74, 520 document collection (TREC Wall Street Journal articles from 1990 - 92) {{with a set of}} 125 natural-language queries with relevance judgements (TREC topics 51 - 175). We show that a weighted combination of scores produces higher precision over the top 5, 105 20, and 30 documents than any single engine over the same data set. We also compare favorably against the state-of-the-art heuristics in <b>merging</b> <b>search</b> engines. Our <b>results</b> suggest that fusing the results from the most dissimilar engines (those with the least overlap in the retrieved sets) is a more effective strategy than simply weighting the best engines more heavily. Keywords: Information Fusion, Distributed Applications, Web Search Engines, Information Retrieval, TREC...|$|R
5000|$|Method for extracting, <b>merging</b> and ranking <b>search</b> engine <b>results</b> (US 8,180,768, May 2013, with D. Braga, M. Brambilla, A. Campi, E. Della Valle, P. Fraternali, D. Martinenghi, M. Tagliasacchi) ...|$|R
40|$|As {{the speed}} of mass spectrometers, sophistication of sample fractionation, and {{complexity}} of experimental designs increase, the volume of tandem mass spectra requiring reliable automated analysis continues to grow. Software tools that quickly, effectively, and robustly determine the peptide associated with each spectrum with high confidence are sorely needed. Currently available tools that post-process the output of sequence-database search engines use three techniques to distinguish the correct peptide identifications from the incorrect: statistical significance re-estimation, supervised machine learning scoring and prediction, and combining or <b>merging</b> of <b>search</b> engine <b>results.</b> We present a unifying framework that encompasses each of these techniques in a single model-free machine-learning framework that can be trained in an unsuper-vised manner. The predictor is trained on the fly for each new set of <b>search</b> <b>results</b> without user intervention, making it robust for different instruments, search engines, and search engine parameters. We demonstrate {{the performance of the}} technique using mixtures of known proteins and by using shuffled databases to estimate false discovery rates, from data acquired on three different instruments with two different ionization technologies. We show that this approach outperforms machine-learning techniques applied to a single search engine’s output, and demonstrating that combin-ing <b>search</b> engine <b>results</b> provides additional benefit. We show that the performance of the commercial Mascot tool can be bested by the machine-learning combination of two open-source tools X!Tandem and OMSSA, but that the use of all three search engines boosts performance further still. The PepArML (Peptide identification Arbiter by Machine Learning) unsupervised, model-free, combining framework can be easily extended to support an arbitrary number of additional searches, search engines, or specialized peptide-spectrum match metrics for each spectrum dataset. PepArML is open-source and is available fro...|$|R
40|$|Most current {{search engines}} present the user a ranked list given the {{submitted}} user query. Top ranked <b>search</b> <b>results</b> generally cover few aspects of all <b>search</b> <b>results.</b> However, in many cases, the users {{are interested in}} the main themes of <b>search</b> <b>results</b> besides the ranked list so that the user will have a global view of <b>search</b> <b>results.</b> This goal is often achieved through clustering approaches. Personalized search studies ranking or reranking the <b>search</b> <b>results</b> based on implicit feedback. The personalized search system will infer user information need based on user search engine interaction and rerank the <b>search</b> <b>results.</b> Same as the ranked <b>search</b> <b>result</b> lists, clusterings of <b>search</b> <b>results</b> intuitively should also be dynamically tuned according to user search system interaction. Thus it brings interesting clustering challenges in the personalized <b>search</b> framework. Clustering <b>results</b> should change dynamically to reflect the personalized ranking of <b>search</b> <b>results.</b> However, traditional static clustering algorithms based on document similarity cannot achieve this goal. In this paper, We study how to incrementally cluster the <b>search</b> <b>results</b> and dynamically update the cluster representation based on user’s implicit feedback. 1...|$|R
3000|$|The {{feedback}} <b>search</b> <b>result</b> L(C, Dj) {{is combined}} with the initial <b>search</b> <b>result</b> to form the final <b>search</b> <b>result</b> and output it; then, step 3 is followed [...]...|$|R
40|$|Main changes: {{customizable}} indexing (allows {{to index}} strings, numbers, dates [...] . many other {{different types of}} data); display <b>search</b> <b>results</b> in categories (node types) <b>search</b> <b>results</b> can be accessed through URLs and therefore shareable through URLs. <b>search</b> <b>results</b> for table <b>search</b> are displayed in datatable ([URL] Generate a download link for table <b>search</b> <b>results...</b>|$|R
50|$|Carrot² is an {{open source}} <b>search</b> <b>results</b> {{clustering}} engine. It can automatically cluster small collections of documents, e.g. <b>search</b> <b>results</b> or document abstracts, into thematic categories. Apart from two specialized <b>search</b> <b>results</b> clustering algorithms, Carrot² offers ready-to-use components for fetching <b>search</b> <b>results</b> from various sources. Carrot² is written in Java and distributed under the BSD license.|$|R
40|$|Todays web {{services}} generate {{large amounts of}} data. Users expect these services to have a search which returns relevant <b>search</b> <b>results</b> instantaneously. Term frequency-inverse document frequency (TF-IDF) is a common technique used in search engines to deliver relevant <b>search</b> <b>results</b> fast. However, with the increasing amounts of data users expect the <b>search</b> <b>results</b> to deliver even more relevant <b>search</b> <b>results.</b> Improving the <b>search</b> <b>results</b> {{can be done by}} expanding the user's query. Providing relevant information for query expansion may be a challenge, but using a technique called pseudo-relevance feedback has shown promissing <b>results.</b> Relevant <b>search</b> <b>results</b> are important, but equally important is speed. Research by Google found that 0. 5 seconds increased load times resulted signifiant less traffic. This thesis investigates how query expansion can be implemented together with pseudo-relevance, to deliver more relevant <b>search</b> <b>results.</b> Research on how to improve <b>search</b> <b>results</b> is not new, but the focus is rarely on speed. Thus our study focuses on speed, with a requirement to deliver <b>search</b> <b>results</b> within 100 ms, which is the maximum acceptable amount of time before users will notice the delay. The final implementation showed promising results, and were able to deliver <b>search</b> <b>results</b> within within the requirement of 100 ms...|$|R
5000|$|... #Caption: The Search Center UI {{showing the}} local <b>search</b> <b>results</b> with federated <b>search</b> <b>results</b> {{on the right}} ...|$|R
50|$|The <b>searching</b> <b>results</b> {{are heavily}} US biased with many initial <b>search</b> <b>results</b> finding {{information}} from American web sites.|$|R
50|$|<b>Search</b> <b>results</b> are modified, or suspect, due to {{the large}} hosted video being given {{preferential}} treatment in <b>search</b> <b>results.</b>|$|R
40|$|Are web <b>search</b> <b>results</b> usually {{dominated}} by major websites and therefore lacking diversity? In this paper, {{we aim to}} answer this question by quantitatively modelling the diversity of <b>search</b> <b>results</b> for popular queries using two diversity measures well-studied in ecology, namely Simpson's diversity index and Shannon's diversity index. Our theoretical analysis shows how the diversity of <b>search</b> <b>results</b> is determined by the Zipfian distribution of websites. Our empirical analysis reveals that comparing Google and Bing, the former is more diverse in the top- 50 <b>search</b> <b>results,</b> while the latter is more diverse in the top- 10 <b>search</b> <b>results...</b>|$|R
40|$|Extracting <b>search</b> <b>result</b> records (SRRs) from webpages {{is useful}} for {{building}} an aggregated search engine which combines <b>search</b> <b>results</b> {{from a variety of}} search engines. Most automatic approaches to <b>search</b> <b>result</b> extraction are not portable: the complete process has to be rerun on a new <b>search</b> <b>result</b> page. In this paper we describe an algorithm to automatically determine XPath expressions to extract SRRs from webpages. Based on a single <b>search</b> <b>result</b> page, an XPath expression is determined which can be reused to extract SRRs from pages based on the same template. The algorithm is evaluated on a six datasets, including two new datasets containing a variety of web, image, video, shopping and news <b>search</b> <b>results.</b> The evaluation shows that for 85 % of the tested <b>search</b> <b>result</b> pages, a useful XPath is determined. The algorithm is implemented as a browser plugin and as a standalone application which are available as open source software...|$|R
50|$|Incorrect field {{detection}} — Google Scholar {{has problems}} identifying publications on the arXiv preprint server correctly. Interpunctuation characters in titles produce wrong <b>search</b> <b>results,</b> and authors {{are assigned to}} wrong papers, which leads to erroneous additional <b>search</b> <b>results.</b> Some <b>search</b> <b>results</b> are even given without any comprehensible reason.|$|R
40|$|Ten blue links ” {{have defined}} web <b>search</b> <b>results</b> {{for the last}} fifteen years – snippets of text {{combined}} with document titles and URLs. In this paper, we establish the notion of enhanced <b>search</b> <b>results</b> that extend web <b>search</b> <b>results</b> to include multimedia objects such as images and video, intentspecific key value pairs, and elements that allow the user to interact with the contents of a web page directly from the <b>search</b> <b>results</b> page. We show that users express a preference for enhanced results both explicitly, and when observed in their search behavior. We also demonstrate the effectiveness of enhanced results in helping users to assess the relevance of <b>search</b> <b>results.</b> Lastly, we show that we can efficiently generate enhanced results to cover a significant fraction of <b>search</b> <b>result</b> pages...|$|R
50|$|The <b>search</b> <b>results</b> {{displayed}} to {{the user}} require an annual subscription fee is required. <b>Search</b> <b>results</b> are bracketed with Google AdSense advertisements.|$|R
40|$|<b>Search</b> <b>results</b> {{clustering}} helps {{users to}} browse the <b>search</b> <b>results</b> and locate {{what they are}} looking for. In the <b>search</b> <b>result</b> clustering, the label selection which annotates a meaningful phrase for each cluster becomes the most fundamental issue. In this paper, we present a new method of using the language modeling approach over Dmoz for label selection, namely label language model. Experimental results show that our method is helpful to obtain meaningful clustering labels of <b>search</b> <b>results.</b> ...|$|R
40|$|Current {{search engines}} present <b>search</b> <b>results</b> in an ordered list even if {{semantic}} technologies {{are used for}} analyzing user queries and the document contents. The semantic information that is used during the <b>search</b> <b>result</b> generation mostly remains hidden from the user although it significantly supports users in understanding why <b>search</b> <b>results</b> are considered as relevant for their individual query. The approach {{presented in this paper}} utilizes visualization techniques for offering visual feedback about the reasons the results were retrieved. It represents the semantic neighborhood of <b>search</b> <b>results,</b> the relations between results and query terms as well as the relevance of <b>search</b> <b>results</b> and the semantic interpretation of query terms for fostering <b>search</b> <b>result</b> comprehension. It also provides visual feedback for query enhancement. Therefore, not only the <b>search</b> <b>results</b> are visualized but also further information that occurs during the search processing is used to improve t he visual presentation and to offer more transparency in <b>search</b> <b>result</b> generation. The results of an evaluation in a real application scenario show that the presented approach considerably supports users in assessment and decision-making tasks and alleviates information seeking in digital semantic knowledge bases...|$|R
40|$|We {{present a}} new image search and ranking {{algorithm}} for retrieving unannotated images by collaboratively mining online <b>search</b> <b>results</b> which consist of online image and text <b>search</b> <b>results.</b> The online image <b>search</b> <b>results</b> are leveraged as reference examples to perform content-based image search over unannotated images. The online text <b>search</b> <b>results</b> are utilized to estimate the reference images' relevance to the search query. The key feature of our method is its capability to deal with unreliable online image <b>search</b> <b>results</b> through jointly mining visual and textual aspects of online <b>search</b> <b>results.</b> Through such collaborative mining, our algorithm infers the relevance of an online <b>search</b> <b>result</b> image to a text query. Once we obtain the estimate of query relevance score for each online image <b>search</b> <b>result,</b> we can selectively use query specific online <b>search</b> <b>result</b> images as reference examples for retrieving and ranking unannotated images. We tested our algorithm both on the standard public image datasets and several modestly sized personal photo collections. We also compared our method with two well-known peer methods. The results indicate that our algorithm is superior to existing content-based image search algorithms for retrieving and ranking unannotated images. © 2011 ACM. link_to_OA_fulltextThe 20 th ACM international conference on Information and knowledge management (CIKM 2011), Glasgow, Scotland, UK., 24 - 28 October 2011. In Proceedings of the 20 th ACM CIKM, 2011, p. 485 - 49...|$|R
50|$|Connect Ontario is {{currently}} partnered with BiblioCommons and BC Libraries {{to create and}} promote its next generation online public access catalogue. The service is unique in that it <b>merges</b> <b>search</b> catalogue capabilities with easy-to-use social media tools. Oakville Public Library, the first institution to pilot the catalogue, went live with BiblioCommons in July 2008. Since then, {{more than a dozen}} Ontario public libraries have rolled out the BiblioCommons catalogue.|$|R
5000|$|Simple word-sense {{induction}} algorithms boost Web <b>search</b> <b>result</b> clustering considerably {{and improve}} the diversification of <b>search</b> <b>results</b> returned by <b>search</b> engines such as Yahoo! ...|$|R
50|$|Keyword {{clustering}} {{is based}} on the first ten <b>search</b> <b>results</b> (TOP-10) regardless of the search engine or custom settings. The TOP 10 <b>search</b> <b>results</b> are the first ten listings that a search engine shows for a certain search query. In most cases, the TOP-10 matches the first page of the <b>search</b> <b>results.</b>|$|R
40|$|Search {{engines are}} in a {{constant}} need for improvements as {{the rapid growth of}} information is affecting the search engines ability to return documents with high relevance. <b>Search</b> <b>results</b> are being lost in between pages and the search algorithms are being exploited to gain a higher ranking on the documents. This study attempts to minimize those two issues, as well as increasing the relevancy of <b>search</b> <b>results</b> by usage of clickthrough data to add another layer of weighting the <b>search</b> <b>results.</b> Results from the evaluation indicate that clickthrough data in fact can be used to gain more relevant <b>search</b> <b>results...</b>|$|R
