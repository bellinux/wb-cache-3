205|413|Public
2500|$|Any {{process that}} generates {{successive}} messages {{can be considered}} a source of information. [...] A <b>memoryless</b> <b>source</b> is one in which each message is an independent identically distributed random variable, whereas the properties of ergodicity and stationarity impose less restrictive constraints. [...] All such sources are stochastic. [...] These terms are well studied in their own right outside information theory.|$|E
5000|$|... the limit, as n goes to infinity, of {{the joint}} entropy of the first n symbols divided by n. It is common in {{information}} theory {{to speak of the}} [...] "rate" [...] or [...] "entropy" [...] of a language. This is appropriate, for example, when the source of information is English prose. The rate of a <b>memoryless</b> <b>source</b> is simply , since by definition there is no interdependence of the successive messages of a <b>memoryless</b> <b>source.</b>|$|E
50|$|Any {{process that}} generates {{successive}} messages {{can be considered}} a source of information. A <b>memoryless</b> <b>source</b> is one in which each message is an independent identically distributed random variable, whereas the properties of ergodicity and stationarity impose less restrictive constraints. All such sources are stochastic. These terms are well studied in their own right outside information theory.|$|E
5000|$|... #Subtitle level 3: For time {{invariant}} discrete <b>memoryless</b> <b>sources</b> ...|$|R
40|$|The {{structural}} {{properties of}} unifilar renewal sources are described and coding theorems for the unifilar renewal source are derived. In general the {{upper bound on}} code rate for the encoding of unifilar renewal sources is tighter than that for <b>memoryless</b> <b>sources.</b> It is suggested that encoding schemes for renewal sources may be constructed using methods similar to the encoding of <b>memoryless</b> <b>sources...</b>|$|R
40|$|Avariable-to-#xed length {{encoding}} procedure is a mapping from a dictionary of variable length strings of source outputs {{to the set}} of codewords of a given length. For <b>memoryless</b> <b>sources,</b> the Tunstall procedure {{can be applied to}} construct optimal uniquely parsable dictionaries and the resulting codes are known to work especially well for sources with small entropies. Weintroduce the idea of plurally parsable dictionaries and showhowto design plurally parsable dictionaries that can outperform the Tunstall dictionary of the same size on very predictable binary, <b>memoryless</b> <b>sources...</b>|$|R
5000|$|Redundancy of {{compressed}} data {{refers to the}} difference between the expected {{compressed data}} length of [...] messages [...] (or expected data rate [...] ) and the entropy [...] (or entropy rate [...] ). (Here we assume the data is ergodic and stationary, e.g., a <b>memoryless</b> <b>source.)</b> Although the rate difference [...] can be arbitrarily small as [...] increased, the actual difference , cannot, although it can be theoretically upper-bounded by 1 in the case of finite-entropy memoryless sources.|$|E
5000|$|The {{quantity}} [...] {{is called}} the relative redundancy and gives the maximum possible data compression ratio, when expressed as the percentage by which a file size can be decreased. (When expressed as a ratio of original file size to compressed file size, the quantity [...] gives the maximum compression ratio that can be achieved.) Complementary {{to the concept of}} relative redundancy is efficiency, defined as [...] so that [...] A <b>memoryless</b> <b>source</b> with a uniform distribution has zero redundancy (and thus 100% efficiency), and cannot be compressed.|$|E
40|$|The first- and second-order optimum {{achievable}} exponents in {{the simple}} hypothesis testing problem are investigated. The optimum achievable exponent for type II error probability, under the constraint that the type I error probability is allowed asymptotically up to epsilon, is called the epsilon-optimum exponent. In this paper, we first give the second-order epsilon-exponent in the case where the null hypothesis and the alternative hypothesis are a mixed <b>memoryless</b> <b>source</b> and a stationary <b>memoryless</b> <b>source,</b> respectively. We next generalize this setting to the case where the alternative hypothesis is also a mixed <b>memoryless</b> <b>source.</b> We address the first-order epsilon-optimum exponent in this setting. In addition, an extension of our results to more general setting such as the hypothesis testing with mixed general source and {{the relationship with the}} general compound hypothesis testing problem are also discussed. Comment: 23 page...|$|E
5000|$|Information rate is {{the average}} entropy per symbol. For <b>memoryless</b> <b>sources,</b> this is merely the entropy of each symbol, while, {{in the case of}} a {{stationary}} stochastic process, it is ...|$|R
40|$|Information Theory, IEEE Transactions on (submitted). We study a novel multi-terminal {{source coding}} setup {{motivated}} by the biclustering problem. Two separate encoders observe two stationary, <b>memoryless</b> <b>sources</b> Xn and Zn, respectively. The goal is to find rate-limited encodings f(xn) and g(zn) that maximize the mutual information I(f(Xn);g(Zn)) /n. We present non-trivial outer and inner bounds on the achievable region for this problem. These bounds are also generalized to an arbitrary collection of stationary, <b>memoryless</b> <b>sources.</b> The considered problem is intimately connected to distributed hypothesis testing against independence under communication constraints, and hence our results are expected to apply to that setting as well...|$|R
40|$|In this paper, {{lossless}} polar {{compression of}} g-ary <b>memoryless</b> <b>sources</b> in the noiseless setting is investigated. Polar compression scheme for binary <b>memoryless</b> <b>sources,</b> introduced by Cronie and Korada, is generalized to sources over prime-size alphabets. In {{order to reduce}} the average codeword length, a compression scheme based on successive cancellation list decoding is proposed. Also, a specific configuration for the compression of correlated sources is considered, and it is shown that the introduced polar compression schemes achieve the corner point of the admissible rate region. Based on this result, proposed compression schemes are extended to arbitrary finite source alphabets by using a layered approach. © 2013 IEEE...|$|R
40|$|Abstract—The minimum {{expected}} {{length for}} fixed-to-variable length encoding of an n-block <b>memoryless</b> <b>source</b> with entropy H grows as nH + O(1), where the term O(1) lies between 0 and 1. However, this well-known performance is obtained under the implicit constraint that the code {{assigned to the}} whole n-block is a prefix code. Dropping the prefix constraint, which is rarely necessary at the block level, we show that the minimum expected length for a finite-alphabet <b>memoryless</b> <b>source</b> with known distribution grows as nH 0 1 log n + O(1) 2 unless the source is equiprobable. We also refine this result up to o(1) for those memoryless sources whose log probabilities do not reside on a lattice. Index Terms—Analytic information theory, fixed-to-variable lossless compression, memoryless sources, one-to-one codes, Shannon theory, source coding. I...|$|E
40|$|Previous work on {{guessing}} {{is combined}} with the Shannon (1949) theory of cipher systems. A complexity measure is defined for breaking a cipher system by guessing at the key or the message. A single-letter characterization of the guessing effort for a discrete <b>memoryless</b> <b>source</b> (DMS) is given. © 1998 IEEE...|$|E
40|$|The {{following}} critical phenomenon {{was recently}} discovered. When a <b>memoryless</b> <b>source</b> is compressed using a variable-length fixed-distortion code, the fastest convergence {{rate of the}} (pointwise) compression ratio to the optimal R(D) bits/symbol is either O(√(n)) or O(n). We show it is always O(√(n)), except for discrete, uniformly distributed sources. Comment: 2 figure...|$|E
40|$|We {{study the}} problem of {{estimating}} the pattern maximum likelihood (PML) distribution for time-homogeneous discrete-time Markov chains (DTMCs). The PML problem for <b>memoryless</b> <b>sources</b> has been well studied in the literature and we propose {{an extension of the}} same for DTMCs. For <b>memoryless</b> <b>sources,</b> Acharya et al. have shown that plug-in estimators obtained from the PML estimate yield good estimates for symmetric functionals of the distribution. We show that this holds for the PML estimate of DTMCs as well. Finally, we express the PML estimate for DTMCs as the double minimization of a certain free energy function and discuss some mean-field approximations to approximate the PML estimate efficiently...|$|R
3000|$|Developing an EXIT chart {{analysis}} that {{is able to}} deal with the check node disparity encountered in parallel RCM-LDGM codes when driven by binary <b>memoryless</b> <b>sources</b> (both uniform and non-uniform) transmitted over AWGN and fast fading Rayleigh channels [...]...|$|R
40|$|This is {{the author}} {{accepted}} manuscript. The final version is available from IEEE via [URL] simplest example of a quantum information source with memory is a mixed source, which emits signals entirely from one of two <b>memoryless</b> quantum <b>sources</b> with given a priori probabilities. Considering a mixed source consisting of a general one-parameter family of <b>memoryless</b> <b>sources,</b> we derive the second-order asymptotic rate for fixed-length visible source coding. Furthermore, we specialize our main result to a mixed source consisting of two <b>memoryless</b> <b>sources.</b> Our results provide the first example of the second-order asymptotics for a quantum information-processing task employing a resource with memory. For {{the case of a}} classical mixed source (using a finite alphabet), our results reduce to those obtained by Nomura and Han. To prove the achievability part of our main result, we introduce universal quantum source codes achieving the second-order asymptotic rates. These are obtained by an extension of Hayashi’s construction of their classical counterparts...|$|R
40|$|We {{study the}} problem of {{communicating}} a distributed correlated <b>memoryless</b> <b>source</b> over a memoryless network, from source nodes to destination nodes, under quadratic distortion constraints. We establish the following two complementary results: (a) for an arbitrary memoryless network, among all distributed memoryless sources of a given correlation, Gaussian sources are least compressible, that is, they admit the smallest set of achievable distortion tuples, and (b) for any <b>memoryless</b> <b>source</b> to be communicated over a memoryless additive-noise network, among all noise processes of a given correlation, Gaussian noise admits the smallest achievable set of distortion tuples. We establish these results constructively by showing how schemes for the corresponding Gaussian problems {{can be applied to}} achieve similar performance for (source or noise) distributions that are not necessarily Gaussian but have the same covariance. Comment: Submitted to IEEE Transactions on Information Theor...|$|E
40|$|The {{following}} critical phenomenon {{was recently}} discovered. When a <b>memoryless</b> <b>source</b> is compressed using a variable-length xed-distortion code, the fastest convergence {{rate of the}} (pointwise) compression ratio to R(D) is either O(p n) or O(log n). We show it is always O(p n), except for discrete, uniformly distributed sources. Keywords|Redundancy, rate-distortion theory, lossy data compression I...|$|E
40|$|The {{following}} critical phenomenon {{was recently}} discovered. When a <b>memoryless</b> <b>source</b> is compressed using a variable-length xed-distortion code, the fastest convergence {{rate of the}} (pointwise) compression ratio to R(D) is either O(p n) or O(log n). We show it is always O(p n), except for discrete, uniformly distributed sources. Keywords Redundancy, rate-distortion theory, lossy data compression Department of Mathematics and Department of Statistics, Stanford University, Stanford, CA 94305. Email: amir@stat. stanford. edu Web: www-stat. stanford. edu/amir y Department of Statistics, Purdue University, 1399 Mathematical Sciences Building, W. Lafayette, IN 479071399. Email: yiannis@stat. purdue. edu Web: www. stat. purdue. edu/yiannis 1 A. D. 's research {{was supported in part}} by NFS grant # NSF-DMS 9704552. I. K. 's research {{was supported in part by}} a grant from the Purdue Research Foundation. 1 Introduction Suppose that data is produced by a stationary <b>memoryless</b> <b>source</b> fX n; n [...] ...|$|E
40|$|This paper {{describes}} a new Fixed-rate Entropy-constrained Vector Quantization (FEVQ) scheme for stationary <b>memoryless</b> <b>sources</b> {{based on a}} sequential search procedure. It is shown that the proposed algorithm results in a substantial reduction in the complexity while the degradation in performance is negligible...|$|R
5000|$|Although {{analytical}} {{solutions to}} this problem are scarce, there are {{upper and lower bounds}} to these functions including the famous Shannon lower bound (SLB), which in the case of squared error and <b>memoryless</b> <b>sources,</b> states that for arbitrary sources with finite differential entropy, ...|$|R
5000|$|In {{describing}} the redundancy of raw data, {{the rate of}} {{a source of information}} is the average entropy per symbol. For <b>memoryless</b> <b>sources,</b> this is merely the entropy of each symbol, while, in the most general case of a stochastic process, it is ...|$|R
3000|$|... } is the {{stationary}} state probability.The memory structure of Markov source {{can be characterized}} by the state transition probabilities p 1 and p 2,[*] 0 [*] [*]<[*]p 1 p 2 [*]<[*] 1, with which p 1 [*]=[*]p 2 [*]=[*] 0.5 indicates the <b>memoryless</b> <b>source,</b> while p 1 [*]≠[*] 0.5 or p 2 [*]≠[*] 0.5, and hence H(S)[*]<[*] 1 indicate source with memory.|$|E
40|$|Abstract—Conventional wisdom {{states that}} the minimum ex-pected length for fixed-to-variable length {{encoding}} of an n-block <b>memoryless</b> <b>source</b> with entropy H grows as nH+O(1). However, this performance is obtained under the constraint that the code assigned to the whole n-block is a prefix code. Dropping this unnecessary constraint we show that the minimum expected length grows as nH − 1 2 logn+O(1) unless the source is equiprobable. I...|$|E
40|$|If p is the {{probability}} of a letter of a <b>memoryless</b> <b>source,</b> the length l of the corresponding binary Huffman codeword can be very different from the value -log p. We show that, nevertheless, for a typical letter, l is approximately equal to -log p. More precisely, {{the probability}} that l differs from -log p by more than m decreases exponentially with m. Comment: 4 pages, LATE...|$|E
40|$|We use the "conservation of entropy" [1] {{to simplify}} the formula for the {{redundancy}} of a large class of variable-to-variable length codes on discrete, <b>memoryless</b> <b>sources.</b> This result leads to new asymptotic upper bounds on the redundancy of the "Tunstall-Huffman" code and the "Tunstall-Shannon-Fano" code...|$|R
40|$|International audienceMotivated {{from the}} fact that {{universal}} source coding on countably infinite alphabets is not feasible, the notion of almost lossless source coding is introduced. This idea -analog to the weak variable-length source coding problem proposed by Han [1]- aims at relaxing the lossless block-wise assumption to allow a distortion that vanishes asymptotically as the block-length goes to infinity 1. In this setup, both feasibility and optimality results are derived for the case of <b>memoryless</b> <b>sources</b> defined on countably infinite alphabets. Our results show on one hand that Shannon entropy characterizes the minimum achievable rate (known statistics) while on the other that almost lossless universal source coding becomes feasible for the family of finite entropy stationary and <b>memoryless</b> <b>sources</b> with countably infinite alphabets...|$|R
40|$|We {{consider}} {{the problem of}} compressing a binary symmetric i. i. d. source stream for two decoders, one of which has access to sideinformation. Kaspi found the rate-distortion tradeoff for this problem for general discrete <b>memoryless</b> <b>sources</b> for the two cases with and without encoder side-information. We focus on the {{case in which the}} encoder has access to side-information. We assume that the source has a symmetric Bernoulli distribution and that the side-information sequence is the output of an erasure channel whose input is the source stream. We explicitly solve the optimization problem for this setup. We also show that the Wyner-Ziv rate distortion function and the conditional rate-distortion function coincide for general discrete <b>memoryless</b> <b>sources</b> when the side-information is of the erasure type. ...|$|R
40|$|Rooted {{trees with}} probabilities {{are used to}} analyze {{properties}} of a variable length code. A bound is derived on {{the difference between the}} entropy rates of the code and a <b>memoryless</b> <b>source.</b> The bound is in terms of normalized informational divergence. The bound is used to derive converses for exact random number generation, resolution coding, and distribution matching. Comment: 5 pages. With proofs and illustrating exampl...|$|E
40|$|An {{inequality}} concerning Kullback's I-divergence {{is applied}} to obtain {{a necessary condition for}} the possibility of encoding symbols of the alphabet of a discrete <b>memoryless</b> <b>source</b> of entropy H by sequences of symbols of another alphabet of size D {{in such a way that}} the average code length be close to the optimum H/log D. The same idea {{is applied to}} the problem of maximizing entropy per second for unequal symbol lenghts, too...|$|E
40|$|This paper {{proposes a}} new linear scalar {{quantization}} algorithm. A modified Lloyd’s optimum algorithm, {{based on a}} linear quantization, is proposed to decompose a <b>memoryless</b> <b>source</b> into the bits. This {{is the case of}} the considered source which is the output of the wavelet transformation. The developed algorithm gives acceptable results for generalized Gaussian sources. Our approach is compared with the Lloyd max one’s and a second one where the quantization intervals are equiprobable. 1...|$|E
40|$|In 1999 Juels and Wattenberg {{introduced}} the fuzzy commitment scheme. Fuzzy commitment {{is a particular}} realization of a binary biometric secrecy system with a chosen secret key. Three cases of biometric sources are considered, i. e. memory- less and totally-symmetric biometric <b>sources,</b> <b>memoryless</b> and input-symmetric biometric <b>sources,</b> and <b>memoryless</b> biometric <b>sources.</b> It is shown that fuzzy commitment is only optimal for <b>memoryless</b> totally-symmetric biometric <b>sources</b> and only at the maximum secret-key rate. Moreover, it is demonstrated that for <b>memoryless</b> biometric <b>sources,</b> which are not input-symmetric, the fuzzy com- mitment scheme leaks information on both the secret key and the biometric data...|$|R
40|$|In {{this first}} part, a computable outer bound is proved for the multiterminal source coding problem, for a setup with two encoders, {{discrete}} <b>memoryless</b> <b>sources,</b> and bounded distortion measures. Comment: Submitted to the IEEE Transactions on Information Theory; Revised, November 2006. Substantial revision {{after the first}} round of review...|$|R
40|$|International audienceThis paper {{investigates the}} problem of {{distributed}} biclustering of <b>memoryless</b> <b>sources</b> and extends previous work [1] to the general case with more than two sources. Given a set of distributed stationary <b>memoryless</b> <b>sources,</b> the encoders' goal is to find rate-limited representations of these sources such that the mutual information between two selected subsets of descriptions (each of them generated by distinct encoders) is maximized. This formulation is fundamentally different from conventional distributed source coding problems since here redundancy among descriptions should actually be maximally preserved. We derive non-trivial outer and inner bounds to the achievable region for this problem and further connect them to the CEO problem under logarithmic loss distortion. Since information-theoretic biclustering {{is closely related to}} distributed hypothesis testing against independence, our results are also expected to apply to that problem...|$|R
