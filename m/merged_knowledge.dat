7|211|Public
40|$|Abstract. Many merging {{operators}} {{have been}} proposed to merge either flat or stratified knowledge bases. The result of merging by such an operator is a flat base (or a set of models of the merged base) irrespective of whether the original ones are flat or stratified. The drawback of obtaining a flat merged base is that information about more preferred knowledge (formulae) versus less preferred knowledge is not explicitly represented, and this information can be very useful when deciding which formulae should be retained {{when there is a}} conflict. Therefore, it can be more desirable to return a stratified knowledge base as a merged result. A straightforward approach is to deploy the preference relation over possible worlds obtained after merging to reconstruct such a base. However, our study shows that such an approach can produce a poor result, that is, preference relations over possible worlds obtained after merging are not suitable for reconstructing a merged stratified base. Inspired by the Condorcet method in voting systems, we propose an alternative method to stratify a set of possible worlds given a set of stratified bases and take the stratification of possible worlds as the result of merging. Based on this, we provide a family of syntax-based methods and a family of model-based methods to construct a stratified <b>merged</b> <b>knowledge</b> base. In the syntax based methods, the formulae contained in the <b>merged</b> <b>knowledge</b> base are from the original individual knowledge bases. In contrast, in the model based methods, some additional formulae may be introduced into the <b>merged</b> <b>knowledge</b> base and no information in the original knowledge bases is lost. Since the merged result is a stratified knowledge base, the commonly agreed knowledge together with a preference relation over this knowledge can be extracted from the original knowledge bases. ...|$|E
40|$|A common {{practice}} for resolving conflicts {{among a group}} of agents is to let the majority decide. We formalize this principle and apply it to the problem of merging the knowledge of multiple agents. We then postulate logical properties that all knowledge merging operators should satisfy and give a model-theoretic characterization of all merging operators that satisfy the postulates. It turns out that the operators that satisfy the postulates are precisely those that induce a certain kind of partial pre-order over the set of possible worlds such that the models of the <b>merged</b> <b>knowledge</b> base are the possible worlds that are minimal with respect to the pre-order. Then, we review several previous approaches and study them in light of the proposed postulates. We then present a particular method for knowledge merging, CMerge, that satisfies all the postulates. We show by example that CMerge appears to resolve conflicts among knowledge bases in a plausible way. Finally, we show that CMerge can [...] ...|$|E
40|$|It {{has been}} widely {{recognized}} that the relative priority of requirements can help developers to resolve inconsistencies and make some necessary trade-off decisions. However, for most distributed development such as Viewpoints-based approaches, different stakeholders may assign different levels of priority to the same shared requirements statement from their own perspectives. The disagreement in the local priorities as-signed to the same shared requirements statement often puts developers into a dilemma during inconsistency handling process. As {{a solution to this}} problem, we present a merging-based approach to handling inconsistency in the Viewpoints framework in this paper. In the Viewpoints framework, each viewpoint is a requirements collection with local prior-itization. Informally, we transform such a requirements collection with local prioritization into a stratified knowledge base. Moreover, the re-lationship between viewpoints is considered as integrity constraints. By merging these stratified knowledge bases, we then construct a <b>merged</b> <b>knowledge</b> base with a global prioritization, which may be viewed as an overall belief in these viewpoints. Finally, proposals for inconsistency handling are derived from the merged result. The global prioritization as well as the local prioritization may be used to argue these proposals and to help developers make a reasonable trade-off decision on handling inconsistency...|$|E
40|$|Abstract. In recent years, {{researchers}} {{have focused on}} <b>merging</b> <b>knowledge</b> bases in both pragmatic and theoretical points of view. In this paper, we enumerate a few attempts to deal with inconsistencies while <b>merging</b> <b>knowledge</b> bases. We focus on ontology merging and show that pragmatic and theoretical approaches are not integrated and that both could benefit from a closer relationship. We extended an existing theoretical algorithm for Description Logics and applied it for the ontology merging problem. We describe here an implementation of this algorithm as an open source Protégé plugin. 1...|$|R
40|$|Weighted linear {{proximal}} {{support vector machine}} (wLPSVM) {{is known}} as an efficient binary classification algorithm with good accuracy and class-imbalance robustness. In this work, original batch wLPSVM is facilitated with distributed incremental learning capability, which allows simultaneously learning from multiple streaming data sources that are geographically distributed. In our approach, incremental and distributed learning are solved as a merging problem at the same time. A new wLPSVM expression is derived. In the new expression, knowledge from samples are presented {{as a set of}} class-wised core matrices, and <b>merging</b> <b>knowledge</b> from two subsets of data can be simply accomplished by matrix addition. With the new expression, we are able to conduct incremental and distributed learning at the same time via <b>merging</b> <b>knowledge</b> from multiple incremental stages and multiple data sources...|$|R
40|$|This paper {{presents}} several {{industrial applications}} of ML {{in the context}} of their effort to solve the "KAML problem", i. e., the problem of <b>merging</b> <b>knowledge</b> acquisition and machine learning techniques. Case-based reasoning is a possible alternative to the problem of acquiring highly compiled expert knowledge, but it raises also many new problems that must be solved before really efficient implementations are available...|$|R
40|$|This paper {{describes}} {{the operation of}} the European Union Reference Laboratory for Feed Additives (EURL) and its role in the authorisation procedure of feed additives in the European Union. Feed additives are authorised according to Regulation (EC) No. 1831 / 2003, which introduced a completely revised authorisation procedure and also established the EURL. The regulations authorising feed additives contain conditions of use such as legal limits of the feed additives, which require the availability of a suitable method of analysis for official control purposes under real world conditions. It is the task of the EURL to evaluate the suitability of analytical methods as proposed by the industry for this purpose. Moreover, the paper shows that one of the major challenges is the huge variety of the methodology applied in feed additive analysis, thus requiring expertise in quite different analytical areas. In order to cope with this challenge, the EURL is supported by a network of national reference laboratories (NRLs) and only the <b>merged</b> <b>knowledge</b> of all NRLs allows for a scientifically sound assessment of the analytical methods. JRC. D. 5 -Standards for Food Bioscienc...|$|E
40|$|Escherichia coli strains are {{classified}} based on O-antigens that are {{components of the}} lipopolysaccharide (LPS) in the cell envelope. O-antigens are important virulence factors, targets of both the innate and adaptive immune system, and {{play a role in}} host-pathogen interactions. Because they are highly immunogenic and display antigenic specificity unique for each strain, O-antigens are the biomarkers for designating O-types. Immunologically, 185 O-serogroups and 11 OX-groups exist for classification. Conventional serotyping for O-typing entails agglutination reactions between the O-antigen and antisera generated against each O-group. The procedure is labor intensive, not always accurate, and exhibits equivocal results. In this report, we present the sequences of 71 O-antigen gene clusters (O-AGC) and a comparison of all 196 O- and OX-groups. Many of the designated O-types, applied for classification over several decades, exhibited similar nucleotide sequences of the O-AGCs and cross-reacted serologically. Some O-AGCs carried insertion sequences and others had only a few nucleotide differences between them. Thus, based on these findings, it is proposed that several of the E. coli O-groups may be <b>merged.</b> <b>Knowledge</b> of the O-AGC sequences facilitates the development of molecular diagnostic platforms that are rapid, accurate, and reliable that can replace conventional serotyping. Additionally, with the scientific knowledge presented, new frontiers in the discovery of biomarkers, understanding the roles of O-antigens in the innate and adaptive immune system and pathogenesis, the development of glycoconjugate vaccines, and other investigations, can be explored...|$|E
40|$|In {{this thesis}} we {{investigate}} some global desiderata for probabilistic knowledge merging given several possibly jointly inconsistent, but individually consistent knowledge bases. We {{show that the}} most naive methods of merging, which combine applications of a single expert inference process with {{the application of a}} pooling operator, fail to satisfy certain basic consistency principles. We therefore adopt a different approach. Following recent developments in machine learning where Bregman divergences appear to be powerful, we define several probabilistic merging operators which minimise the joint divergence between <b>merged</b> <b>knowledge</b> and given knowledge bases. In particular we prove that in many cases the result of applying such operators coincides with the sets of fixed points of averaging projective procedures - procedures which combine knowledge updating with pooling operators of decision theory. We develop relevant results concerning the geometry of Bregman divergences and prove new theorems in this field. We show that this geometry connects nicely with some desirable principles which have arisen in the epistemology of merging. In particular, we prove that the merging operators which we define by means of convex Bregman divergences satisfy analogues of the principles of merging due to Konieczny and Pino-Perez. Additionally, we investigate how such merging operators behave with respect to principles concerning irrelevant information, independence and relativisation which have previously been intensively studied in case of single-expert probabilistic inference. Finally, we argue that two particular probabilistic merging operators which are based on Kullback-Leibler divergence, a special type of Bregman divergence, have overall the most appealing properties amongst merging operators hitherto considered. By investigating some iterative procedures we propose algorithms to practically compute them...|$|E
40|$|In this paper, {{we propose}} {{a family of}} {{operators}} for <b>merging</b> stratified <b>knowledge</b> bases under integrity constraints. The operators are defined in a model-theoretic way. Our merging operators {{can be used to}} <b>merge</b> stratified <b>knowledge</b> bases where no numerical information is available. Furthermore, the original knowledge bases to be merged can be individually inconsistent. Both logical properties and computational complexity issues of the operators are studied...|$|R
40|$|AbstractWe {{present a}} {{framework}} for expressing various merging operators for belief sets. This framework generalises our earlier work on consistency-based belief revision and contraction. Two primary merging operators are identified: in the first approach, belief sources are consistently combined so that the result of <b>merging</b> <b>knowledge</b> bases K 1,…,Kn is a maximal consistent (if possible) set of formulas comprising the joint knowledge of the knowledge bases. This approach then accords with one's intuitions as to what a “merge” operator should do. The second approach is more akin to a generalised belief revision operator. Knowledge bases K 1,…,Kn are “projected” onto another (in the simplest case the knowledge base where only tautologies are known). Properties of these operators are investigated, primarily by comparing their properties with postulates that have been identified previously in the literature. Notably, the approach is independent of syntax, in that <b>merging</b> <b>knowledge</b> bases K 1,…,Kn is independent of how each Ki is expressed. As well, we investigate the role of entailment-based and consistency-based integrity constraints, the interrelationships between these approaches and belief revision, and the expression of further merging operators...|$|R
40|$|Abstract The area of <b>knowledge</b> <b>merging</b> is {{concerned}} with merging conflicting information while preserving as much as possible. Most proposals in the literature work with knowledge bases expressed in propositional logic. We propose a new framework for <b>merging</b> <b>knowledge</b> bases expressed in (subsets of) first-order logic. Dilation operators (a concept originally introduced by Bloch and Lang) are employed and developed, and by combining them {{with the concept of}} comparison orderings we obtain a framework that is driven by model-based intuitions but that can be implemented in a syntax-based manner. We demonstrate specific dilation operators and comparison orderings for use in applications. We also show how postulates from the literature on <b>knowledge</b> <b>merging</b> translate into our framework and provide the conditions that dilation operators and comparison orderings must satisfy in order for the respective merging operators to satisfy the new postulates. ...|$|R
40|$|This study {{examined}} the sense making practices of three beginning teachers. This study sought to understand how beginning teachers constructed teaching knowledge and how their collective knowledge of teaching was applied to their teaching practices. The fundamental nature of teacher knowledge construction was recognized through the careful study of the day-to-day practices of theses teachers. The naturalistic and phenomenological features of this inquiry permitted {{the investigation of the}} beginning teachers as they engaged in their teaching practices in the setting of their first classrooms. Through observation and direct interaction with the new teachers, understandings of their perceptions of teacher knowledge unfold naturally without manipulation or control measures. This discovery-oriented approach minimized the treatment of the study and placed no prior restraints or predictions on this investigation. Multiple data points were carefully examined and triangulated. Both within and cross-case analyses revealed the fashion in which knowledge is constructed contextually and synergistically. The teachers of this study seemed to develop teaching knowledge in four categories identified as: Knowledge from Theoretical Contexts, Knowledge from Personal and Autobiographical, Knowledge from Classroom Practice and Knowledge from Institutional Contexts. It appeared that the teachers of this study <b>merged</b> <b>knowledge</b> from these four contextual categories into a superset of knowing identified as Reconstructed Self Transforming Knowledge. This study has strong implications for educational theory and teacher education and mentorship in that it suggests that new teachers need more than knowledge of their theory and practice. Instead, through this study, I advocate the need for a well-structured new teacher training program that guides and supports beginning teacher knowledge construction across all four teacher knowledge contexts. Particular attention needs to be given to understanding the personal and autobiographical experiences that seemingly shape beginning teacher perceptions. Authentic and sustained mentorship experiences must be provided for the beginning teacher so that the formation and application of Reconstructed Self Transforming Knowledge is modeled for the beginning teacher. New teachers must have significant opportunities to develop their abilities to form Reconstructed Self Transforming Knowledge in their teaching preparation programs. ...|$|E
30|$|The first area {{focuses on}} the {{management}} part of the network slicing problem in the E 2 E aspect. The second area considers network slicing from a need of isolation point of view. The isolation-oriented approach <b>merges</b> <b>knowledge</b> from different branches, e.g., classical security methods approach, new soft-security methods, like trustworthiness and reputation systems, or QoS/QoE-related issues. In the authors’ opinion, both areas are very important and should be subject of further research.|$|R
40|$|A novel {{approach}} is {{proposed for the}} characterization of critical dimensions and geometric errors, suitable for application to micro-fabricated parts and devices characterized as step-like structured surfaces. The {{approach is}} based on acquiring areal maps with a high-precision optical three-dimensional profilometer and on processing topography data with novel techniques obtained by <b>merging</b> <b>knowledge</b> and algorithms from surface metrology, dimensional metrology and computer vision/image processing. Thin-foil laser targets for ion acceleration experiments are selected as the test subject. The main issues related to general applicability and metrological performance of the methodology are identified and discussed...|$|R
40|$|This paper {{discusses}} a capstone senior {{design project}} involving {{development of a}} data acquisition system for use in an undergraduate digital signal processing (DSP) course. The project allows undergraduate senior engineering students to <b>merge</b> <b>knowledge</b> of hardware architecture with concepts of Nyquist sampling theorem to explore the relationship between analog and digital signals and develop invaluable insight into the hardware aspects of DSP. The Pentium Il PC-based design involves a complete data acquisition system, including a microphone with corresponding conditioning circuitry, an analog-to-digital converter, an 8255 peripheral interface adapter, a user-friendly math intensive software interface, a digital-to-analog converter, and speakers...|$|R
50|$|In June 2008, i2 was {{acquired}} from ChoicePoint by Silver Lake Partners Sumeru fund for $185 million. In July 2009, i2 <b>merged</b> with <b>Knowledge</b> Computing Corporation (KCC), makers of COPLINK software. KCC {{was founded in}} 1998 in Tucson, Arizona.|$|R
40|$|Abstract: Middleware is a growing, {{multidisciplinary}} {{area that}} <b>merges</b> <b>knowledge</b> from diverse {{areas such as}} distributed systems, networks and, more recently, embedded systems. This paper {{presents the results of}} an extensive review of middleware related literature, and presents an overview of critical features that must be considered during middleware development. These features include: network independence, plug and play operation, quality of service provisioning, service locating and data routing, providing appropriate transactions, scheduling transactions, providing mechanisms for system recovery, and interoperability among multiple languages and middleware systems. We also present a brief overview of our own middleware system, and describe how the above features have influenced its development. 1. Introduction: In recent year...|$|R
40|$|This book {{focuses on}} the {{individual}} experiences of Western expatriates in China by <b>merging</b> academic <b>knowledge</b> and real-life testimonials given by interviewees. The author also draws on her own experience of {{living and working in}} China, to explore a range of challenges and opportunities met by Western expatriates...|$|R
40|$|Changes in aged {{care and}} health policy have {{introduced}} an increasingly complex assessment, resource option, and economic and regulatory context for decision-making regarding relocation to residential care. This paper {{reports on a}} study exploring residential placement {{from the perspective of}} spouses who place a long-term partner in an aged care facility. It highlights the importance of understanding the meaning of such decision-making for the spouse who remains at home and explores {{the ways in which the}} placement is constructed as either a continuation of, or a refusal to, care for a long-term partner. The paper draws out the implications for social work practice and identifies the challenge to <b>merge</b> <b>knowledge</b> of resource packages, care options and financial arrangements with a concern with the processes of decision-making and the emotional and symbolic aspects of such decisions...|$|R
40|$|This paper {{proposes a}} novel {{approach}} to the characterization of critical dimensions and geometric form error at micro and sub-micrometric scales, suitable for application to micro-fabricated parts and devices. Thin foil laser targets for ion acceleration experiments are selected as the test subject in this instance. The approach is based on acquiring areal maps with a high-precision 3 D optical interferometric profilometer and on processing the surface topography data with novel techniques obtained by <b>merging</b> <b>knowledge</b> and algorithms from surface metrology, dimensional metrology and computer vision/image processing for quality inspection. This new approach allows quantitative measurement results for the thin foil laser target to be obtained, and shows promise for being applied to {{a wide array of}} similar problems involving quality inspection of micro-parts and devices, and to structured surfaces in general...|$|R
40|$|ISBN : 978 - 1 - 58603 - 923 - 3 International audienceBuilding {{a domain}} {{ontology}} usually requires several resources of different types, e. g. thesaurus, object taxonomies, terminologies, databases, sets of documents, etc. where objects {{are described in}} terms of attributes and relations with other objects. One important and hard problem {{is to be able to}} combine and <b>merge</b> <b>knowledge</b> units extracted from these different resources within the representation formalism supporting the ontology. The purpose of this paper is to show which kinds of resources can be taken as starting points for building an ontology, using FCA and its extension RCA. A real-world example in microbiology is proposed, detailing the interaction with domain experts during the ontology design process. Finally, an evaluation based on recall and precision gives an idea of the efficiency of the approach and points out several research perspectives...|$|R
40|$|The study puts {{emphasis}} on the Czech-Bulgarian cross cultural works of a mathematician, journalist and translator Vladislav Šak (1860 - 1941). The study aspires to <b>merge</b> <b>knowledge</b> regarding Šak’s bulgarian studies contribution (he has translated Ch. Botev, I. Vazov, K. Christov, E. Bagrjana, K. Veličkov and others; among others he has written set of sonnets "Pod Vitoší", 1905; "Bulharsko-český slovník", 1910 - 1913; "Kniha bulharsko-české konversace", 1913; "Česko-bulharský slovník", 1926) {{on the basis of}} available literature as well as so far unexplored sources (e. g. Šak’s autobiography that has been published within this study, or the only broader correspondence collection, Šak’s letters addressed to Adolf Černy, the main editor of "Slovanský přehled", into which Šak used to regularly contribute partly under the pen-name Martin Prentov). Both Šak’s personal and professional relationships with K. Veličkov have been described for the first time...|$|R
40|$|Business Engineering (BE) " places an {{emphasis}} on the unique combination of Engineering and Business. BE <b>merge</b> the <b>knowledge</b> of 'Business decision makings' and 'Engineering problem solving' allowing business to gain the competitive edge in technically oriented economy and dynamic business environment. This talk will discuss the issues of the BE in our modern world...|$|R
30|$|The ‘Polygnosis’ {{platform}} provides reliable {{access to}} highly specialized knowledge. It {{has the ability}} to constantly update and <b>merge</b> new <b>knowledge,</b> since its contents are dynamically enriched and semantically organized. It creates a comprehensive environment with respect to a scientific issue. The system enables the comprehension and assessment of knowledge in a centralized, yet personal, way.|$|R
50|$|Following {{his college}} education, Sugiono {{returned}} to Jakarta {{to focus on}} his acting and modeling career. He successfully <b>merged</b> his <b>knowledge</b> in computer technology with his professional interests in the entertainment industry, by developing websites and writing articles and reviews of concerts and music festivals held throughout Europe during the summer. He has contributed extensively to Trax Magazine, for MTV Indonesia.|$|R
5000|$|... {{majority}} : {{the result}} of <b>merging</b> a <b>knowledge</b> base [...] with other knowledge bases can be forced to entail [...] by adding {{a sufficient number of}} other knowledge bases equivalent to this condition corresponds to a kind of vote-by-majority: a sufficiently large number of knowledge bases can always overcome the [...] "opinion" [...] of any other fixed set of knowledge bases.|$|R
40|$|We {{present a}} {{framework}} for expressing different merging operators for belief sets. This framework is a generalisation of our earlier work concerning consistency-based belief revision and contraction. Two distinct merging operators are identified: in the first approach, belief sources are consistently combined so that the result of <b>merging</b> <b>knowledge</b> bases K 1,..., Kn is a maximal consistent (if possible) set of formulas comprising the joint knowledge of the knowledge bases. This approach then accords to one's intuitions as to what a "merge" operator should do. The second approach is more akin to a generalised belief revision operator: Knowledge bases K 1,..., Kn are "projected" onto another (in the simplest case the trivially true knowledge base). In both cases, we consider the incorporation of entailment-based and consistency-based integrity constraints. Properties of these operators are investigated, primarily by comparing their properties with postulates that have been identified previously in the literature. As well, the interrelationships between these approaches and belief revision is given...|$|R
40|$|We {{investigate}} the logical properties {{of knowledge base}} combination operators proposed in the literature. These operators {{are based on the}} selection of some maximal subsets of the union of the knowledge bases. We argue that they are not fully satisfactory to <b>merge</b> <b>knowledge</b> bases, since the source of information is lost in the combination process. We show that it is the reason why those operators do not satisfy a lot of logical properties. Then we propose to use more rened selection mechanisms in order to take the distribution of information into account in the combination process. That allows to dene merging operators with a more subtle behaviour. 1 INTRODUCTION In the elds of articial intelligence and databases, one is often faced with conicting information coming from several sources. Thus, an important problem in such cases is how to reach a coherent piece of information from these contradictory ones. For example, if one wants to build an expert system from a [...] ...|$|R
40|$|This paper {{presents}} several {{industrial applications}} of ML {{in the context}} of their effort to solve the "KAML problem", i. e., the problem of <b>merging</b> <b>knowledge</b> acquisition and machine learning techniques. Case-based reasoning is a possible alternative to the problem of acquiring highly compiled expert knowledge, but it raises also many new problems that must be solved before really efficient implementations are available. 1 Introduction There are many sides to the description of what an industrial application is. In a recent paper (Kodratoff, Graner, and Moustakis, 1994) we summarized some of the experience gained during the CEC project MLT in counseling a user on which of the many types of machine learning (ML) to use for his special application. In this presentation, we shall consider two of the main subfields of the ones that need merging for an industrial application, seemingly the richest in generating future research problems: validation of KBS, and merging of ML into a kn [...] ...|$|R
40|$|Objective: to {{understand}} the work environment according to the concepts, knowledge and values expressed and practiced by nursing professionals in occupational risk management. Methods: this was an ergology-based participant study. Data collection was performed through interviews with key informants and 25 workers, as well as observations and measurements at a Basic Health Unit located in Rio Grande do Sul. Data analysis {{was based on the}} Three-Pole Dynamic Device. Results: work conditions were found to be precarious, and workers are exposed to verbal violence and other psychosocial, biological and ergonomic risks. Chemical and physical risks are neglected, and activity is constantly restandardized toward service effectiveness. Conclusion: the studied subjects worked in risky conditions on a daily basis, and this information was expressed through synergistic dialogues and participant observations. Based on the contributions of these individuals, it is possible to <b>merge</b> <b>knowledge</b> obtained from work environments with science in order to address this issue...|$|R
40|$|Abstract. Building {{a domain}} {{ontology}} usually requires several resources of di erent types, e. g. thesaurus, object taxonomies, terminologies, databases, sets of documents, etc, where objects {{are described in}} terms of attributes and relations with other objects. One important and hard problem {{is to be able to}} combine and <b>merge</b> <b>knowledge</b> units extracted from these di erent resources within an homogeneous formal representation (such as a description logic or OWL). The purpose of this article is to show which kinds of resources should be available for designing a real-world ontology in a given application domain, and then how Formal Concept Analysis and its extension- Relational Concept Analysis- can be used for materializing an associated ontology. This resulting target ontology can then be encoded within OWL or a description logic formalism, allowing classi cation-based reasoning. A real-world example in microbiology is detailed. Finally, an evaluation including tests on recall and precision shows how source resources can be completed with other existing domain resources using a semi-automatic analysis process. ...|$|R
40|$|In {{this paper}} we are {{concerned}} with the problem of acquiring knowledge by integration. Our aim is to construct an integrated knowledge base from several separate sources. The need to <b>merge</b> <b>knowledge</b> bases can arise, for example, when knowledge bases are acquired independently from interactions with several domain experts. As opinions of different domain experts may differ, the knowledge bases constructed in this way will normally differ too. A similar problem can also arise whenever separate knowledge bases are generated by learning algorithms. The objective of integration is to construct one system that exploits all the knowledge that is available and has a good performance. The aim {{of this paper is to}} discuss the methodology of knowledge integration, describe the implemented system (INTEG. 3), and present some concrete results which demonstrate the advantages of this method. 1. Introduction The areas of knowledge acquisition (KA) and machine learning (ML) have until recently exist [...] ...|$|R
40|$|International audienceBuilding {{a domain}} {{ontology}} usually requires several resources of different types, e. g. thesaurus, object taxonomies, terminologies, databases, sets of documents, etc. where objects {{are described in}} terms of attributes and relations with other objects. One important and hard problem {{is to be able to}} combine and <b>merge</b> <b>knowledge</b> units extracted from these different resources within an homogeneous formal representation (such as a description logic or OWL). This purpose of this article is to show which kinds of source resources should be available for designing a real-world ontology in a given application domain, and then how Formal Concept Analysis and its extension Relational Concept Analysis can be used for materializing an associated ontology. This resulting target ontology can then be encoded within OWL or a description logic formalism, allowing classification-based reasoning. A real-world example in microbiology is detailed. Finally, an evaluation including tests on recall and precision shows how source resources can be completed with other existing domain resources using a semi-automatic analysis process...|$|R
40|$|The aim {{of belief}} merging is to merge {{conflicting}} information while preserving {{as much of}} it as possible. Most proposals in the literature work with knowledge bases expressed in propositional logic. We propose a new framework for <b>merging</b> <b>knowledge</b> bases expressed in (subsets of) first-order logic. Dilation operators (a concept originally introduced by Bloch and Lang) are employed and developed, and by combining them with the concept of comparison orderings we obtain a framework that is driven by model-based intuitions but that can be implemented in a syntax-based manner. We demonstrate specific dilation operators and comparison orderings for use in applications. The notion of generalised consistency is introduced in order to support integrity constraints as well as other, more general, notions of consistency. We also show how postulates from the literature on belief merging translate into our framework and provide the conditions that dilation operators and comparison orderings must satisfy in order for the respective merging operators to satisfy the new postulates...|$|R
40|$|Scaling is {{becoming}} an increasingly important topic in the earth and environmental sciences as researchers attempt to understand complex natural systems {{through the lens of}} an ever-increasing set of methods and scales. The guest editors introduce the papers in this issue’s special section and present an overview of some of the work being done. Scaling {{remains one of the most}} challenging topics in earth and environmental sciences, forming a basis for our understanding of process development across the multiple scales that make up the subsurface environment. Tremendous progress has been made in discovery, explanation, and applications of scaling. And yet much more needs to be done and is being done as part of the modern quest to quantify, analyze, and manage the complexity of natural systems. Understanding and succinct representation of scaling properties can unveil underlying relationships between system structure and response functions, improve parameterization of natural variability and heterogeneity, and help us address societal needs by effectively <b>merging</b> <b>knowledge</b> acquired at different scales...|$|R
