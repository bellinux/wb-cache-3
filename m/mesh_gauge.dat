5|9|Public
40|$|AbstractIn {{this note}} we discuss {{three types of}} {{polynomial}} spline approximation: (i) Schoenberg's variation-diminishing approximation, (ii) simple variation-diminishing approximation, and (iii) a natural extrapolated approximation which {{can be obtained by}} taking the appropriate linear combination of the first two. The latter method, although not variation diminishing itself, has a number of practical advantages over either (i) or (ii), including higher-order convergence properties with respect to <b>mesh</b> <b>gauge</b> and in order of derivative, specifically, for C 2 functions defined on finite intervals, it exhibits uniform convergence behavior in value, slope, and curvature, whereas the other methods break down on the last and sometimes critical “shape-getting property”, namely, convergence in curvature...|$|E
40|$|The OMEGA gauge {{is a new}} {{objective}} <b>mesh</b> <b>gauge</b> {{that has}} been accepted by the International Council for the Exploration of the Sea (ICES) as the new standard for scientific mesh measurements to replace the ICES gauge. Trials carried out during {{the development of the}} OMEGA gauge are described here. The trials were carried Out by fisheries inspection services, fisheries research institutes and netting manufacturers in EU and ICES member countries, in Turkey and by the NAFO. The results informed changes to the mesh measuring force recommended by the ICES Study Group on Mesh Measurement Methodology that have Subsequently been adopted. (C) 2007 Elsevier B. V. All rights reserve...|$|E
40|$|Many {{technical}} measures {{aimed at the}} conservation and recovery of fish stocks {{are based on the}} implementation of minimum mesh sizes to guarantee a certain level of fishing gear selectivity. At present, different methods are used to measure mesh opening, according to whether the measurement serves fisheries inspection, fisheries research or the manufacturing of netting for fishing nets. At the very least, this causes confusion but can also have more serious consequences such as the criminalization of well intentioned fishermen or the setting of inappropriate mesh size regulations. For example, the measurements made by fishery inspectors using the wedge gauge are generally larger than those made by scientists using the ICES gauge. As a result the cod-ends used by fishermen to satisfy the fishery inspectors will have a selectivity lower than that anticipated by the scientists who provided the advice that informed the relevant legislation. In 1999, owing to the wider range of twines and netting materials used by the fishing industry, ICES established the Study Group on Mesh Measurement Methodology (SGMESH) to refine mesh measurement procedures. At the same time the different stakeholders agreed that there was a need to consider the adoption of a standard mesh measurement method. A major step in the standardization of mesh measurement methodologies was {{the development of a new}} standard <b>mesh</b> <b>gauge</b> (2002 - 2005). In October 2005, it was adopted by ICES as the new standard for mesh measurements and in June 2006 the Northwest Atlantic Fisheries Organization (NAFO) recommended its use for scientific purposes. This review paper critically examines the different mesh measurement methodologies and their related problems, and introduces and describes a new objective <b>mesh</b> <b>gauge</b> (OMEGA) to replace the former less satisfactory gauges. Its adoption will result in researchers, fisheries inspectors and the netting industry using one standard gauge and one set of standard protocols to achieve consistent mesh-size measurements...|$|E
40|$|Abstract. We study {{adaptive}} {{finite element}} methods for elliptic problems with domain corner singularities. Our model {{problem is the}} two-dimensional Poisson equation. Results of this paper are twofold. First, we prove that there exists an adaptive <b>mesh</b> (<b>gauged</b> by a discrete mesh density function) under which the recovered gradient by the polynomial preserving recovery (PPR) is superconvergent. Second, we demonstrate by numerical examples that an adaptive procedure with an a posteriori error estimator based on PPR does produce adaptive meshes that asatisfy our mesh density assumption, and the recovered gradient by PPR is indeed superconvergent in the adaptive process...|$|R
5000|$|In the United States, fencing {{usually comes}} in 20 rod and 50 ft rolls, {{which can be}} joined by [...] "unscrewing" [...] one of the end wires and then [...] "screwing" [...] it back in so that it hooks both pieces. Common heights include 3 ft, 3 ft 6 in, 4 ft, 5 ft, 6 ft, 7 ft, 8 ft, 10 ft, and 12 ft, though almost any height is possible. Common <b>mesh</b> <b>gauges</b> are 9, 11, and 11.5. Mesh length can also vary based on need, with the {{standard}} mesh length being 2". For tennis courts and ball parks, the most popular height is 10 ft, and tennis courts use a mesh length of 1.75".|$|R
40|$|This paper extends some {{first steps}} given in merging radar and rain gauge data {{for a better}} {{understanding}} of rainfall rate spatial structure. The study has been performed analyzing spatial correlation between sites within a densely <b>meshed</b> rain <b>gauge</b> network and also with their corresponding pixels in weather radar images...|$|R
40|$|At present {{both the}} wedge gauge and the ICES <b>mesh</b> <b>gauge</b> {{are used to}} examine the mesh opening of trawl cod-ends. A {{thorough}} investigation of both instruments by an EU project (MESH) has revealed a number of deficiencies. This has lead to the conclusions to not further recommend the use of both devices but to go {{for the development of a}} complete new measurement tool. This is presently progressing in a further EU project named OMEGA. Prototypes of the new device have been produced and a number of introductory tests have already been made. This contribution reports on further laboratory tests carried out at the Institute for Fishing technology and Fish Quality with special emphasis on netting yarn diameters below 4 mm. The results demonstrate further yarn size depending inaccuracies of the wedge gauge and support the rapid introduction of the newly developed measurement instrument...|$|E
40|$|An {{investigation}} {{was made into}} the effective application of porous heat exchangers of cylindrical shape through which fluid pa~es axially. On {{the basis of a}} theoretical analysis the conclusion derived was that the best thermal efficiency can be reached by the use of porous material with a large heat-exchanger surface, a high radial and low axial thermal conductivity (ie with a marked anisotropy of thermal conductivity), and a small radius of the heat exchanger operating at lower flows of cooling agent. The results of experiments carried out at helium and nitrogen temperatures are presented. These results have confirmed the high effectiveness of porous heat exchangers, even in comparison with chamber-type heat exchangers. For the temperature range from 1. 5 to 300 K the heat exchangers composed of highly conductive metal nets (<b>mesh</b> <b>gauge</b> of the order of magni-tude of 10 - 1 mm) stacked perpendicularly to the direction of flow of the cooling fluid, appear to be the most promising ones. Porous heat exchangers for continuous flow helium cryostats Z. Mzilek, L. P~st, and A. Rysk...|$|E
40|$|I {{present a}} new method for {{numerical}} simulations of general relativistic systems that eliminates constraint violating modes {{without the need}} for constraint damping or the introduction of extra dynamical fields. The method is a type of variational integrator. It is based on a discretization of an action for gravity (the Plebański action) on an unstructured mesh that preserves the local Lorentz transformation and diffeomorphism symmetries of the continuous action. Applying Hamilton's principle of stationary action gives discrete field equations on the <b>mesh.</b> For each <b>gauge</b> degree of freedom there is a corresponding discrete constraint; the remaining discrete evolution equations exactly preserve these constraints under time-evolution. I validate the method using simulations of several analytically solvable spacetimes: a weak gravitational wave spacetime, the Schwarzschild spacetime, and the Kerr spacetime. by Will M. Farr. Thesis (Ph. D.) [...] Massachusetts Institute of Technology, Dept. of Physics, 2010. This electronic version was submitted by the student author. The certified thesis is available in the Institute Archives and Special Collections. Cataloged from student submitted PDF version of thesis. Includes bibliographical references (p. 121 - 123) ...|$|R
40|$|AbstractIn {{this paper}} an adjoint- (or sensitivity-) based error measure is {{formulated}} which measures the error contribution of each solution variable to an overall goal The goal is typically embodied in an integral functional, e. g., the solution {{in a small}} region of the domain of interest. The resulting a posteriori error measures involve the solution of both primal and adjoint problems. A comparison {{of a number of}} important a posteriori error measures is made in this work. There is a focus on developing relatively simple methods that refer to information from the discretised equation sets (often readily accessible in simulation codes) and do not explicitly use equation residuals. This method is subsequently used to guide anisotropic mesh adaptivity of tetrahedral finite elements. Mesh adaptivity is achieved here with a series of optimisation heuristics of the landscape defined by mesh quality. <b>Mesh</b> quality is <b>gauged</b> with respect to a Riemann metric tensor embodying an a posteriori error measure, such that an ideal element has sides of unit length when measured with respect to this metric tensor. This results in meshes in which each finite-element node has approximately equal (subject to certain boundary-conforming constraints and the performance of the mesh optimisation heuristics) error contribution to the functional (goal) ...|$|R
40|$|This thesis {{presents}} goal-based error {{measures and}} applies them, via appropriate metric tensors, to {{the adaptation of}} three dimensional anisotropic tetrahedral finite element meshes {{for a range of}} oceanographic modelling problems. The overall aim of this work is to produce error measures which are able to resolve the flow features of an ocean over a wide range of spatial and temporal scales simultaneously. For example, western boundary currents, the Antarctic Circumpolar Current (ACC), equatorial jets, meddies (mid-latitude eddies) and Open Ocean Deep Convection. Conventional numerical ocean models generally use static meshes. The use of dynamically-adaptive meshes has many potential advantages but needs to be guided by an error measure. <b>Mesh</b> quality is <b>gauged</b> with respect to the metric tensor which embodies the error measure, such that an ideal element has sides of unit length when measured with respect to this metric tensor. The result is meshes in which each finite element node has approximately equal (subject to certain boundary conforming constraints and the performance of the mesh optimization procedure) 'error contribution. Error measures are formulated which measure the error contribution of each solution variable to an overall goal, which encompasses important features of the flow structure and is embodied in an integral form, e. g. the integral of the solution in a small region of the domain of interest. The sensitivity of the functional, taken with respect to the solution variables, is used as the basis from which error measures are derived. The error measures then act to predict those areas of the domain where resolution should be changed and lead to the solution of so-called forward and adjoint (backward) problems. Focus is given to developing relatively simple methods that refer to information readily accessible from the discretized equation sets and do not explicitly use equation residuals. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|In {{the first}} part of this work, we apply finite {{difference}} methods, specially mesh refinement techniques, in order to numerically evolve a single black hole, which is represented by the puncture initial data. We use standard second order finite differences, and the second order Iterated Crank-Nicholson integrator. We observe that, in order to obtain a second order accurate evolution we must impose second order accurate interface conditions at the refinement boundaries. We test our evolution with both the geodesic and the 1 +log slicing conditions, and observe the expected results. We conclude that our mesh refinement technique generates convergent evolutions, and the puncture method behaves very well with it. The second part of this work deals with a modification of the hybrid ``Lazarus'' method for wave extraction. This method is divided in three parts: an early evolution, a set of transformations to produce perturbations over a Kerr background from the numerical data, and Teukolsky evolution. By using our evolution code (with <b>mesh</b> refinement) and <b>gauges</b> (1 +log, gamma-driver, shifting-shift), we deviate from the original Lazarus approach. We used an independent implementation of the Lazarus transformations, validating the original results, and of the Teukolsky equation. We obtained results similar to the original Lazarus, both on the waveforms as well as on the negative results at later times. For instance, strong pulses that contaminate some gauge transformations, which may be explained in part by the propagating gauge modes of the 1 +log slicing. Increasing the accuracy of the initial black hole evolution we seem to obtain better final results for the Kerr test case. Because of the gauge problems, we develop an approximated embedding method which approximates location of the numerical slice into the Kerr spacetime. This method is much less sensitive to the gauge perturbations. Given the difficulties of the Lazarus procedure, we decide to use the Lazarus method as a wave extraction tool. Using this embedding technique we developed the ``spacelike'' wave extraction method. Our preliminary result is consistent with the numerical waveforms for at least three cycles. Although we see some differences, it is too early to claim physical reality on them...|$|R
40|$|The {{solution}} of complex three-dimensional {{computational fluid dynamics}} (CFD) problems in general necessitates {{the use of a}} large number of mesh points to approximate directional flow features such as shocks, boundary layers, vortices and wakes. Such large grid sizes have motivated researchers to investigate methods of introducing very high aspect ratio elements to capture these features. In this Thesis, an anisotropic adaptive grid method has been developed for the {{solution of}} three-dimensional inviscid and viscous flows by the finite element method. An edge-based error estimate drives a mesh movement strategy that allows directional stretching and re-orientation of the grid with more mesh points introduced along those directions with rapidly changing gradients. The error estimate is built from a modified positive-definite form of the Hessian tensor of a selected solution variable or combination of variables. The resulting metric tensor controls the magnitude as well as, the direction of the grid stretching. The desired directionally adapted anisotropic mesh is constructed in physical space by a coordinate transformation based on this tensor. This research thus seeks a near-isotropic mesh in the transformed metric space and an equidistribution of the error over the mesh edges. The adaptive strategy can be considered to be the first 3 -D implementation of an improved spring analogy-based algorithm originally applied on quadrilateral meshes. The adaptive methodology has been validated on various benchmark cases on both hexahedral and tetrahedral meshes. The numerical results obtained span inviscid and viscous flows, as well as internal and external aerodynamics. The effectiveness of the adaptive scheme to equidistribute the interpolation error over the edges of tetrahedral and hexahedral <b>meshes</b> has been <b>gauged</b> on analytical test cases where near-Gaussian distributions of the error were obtained. It was further demonstrated that the error estimate closely follows the true solution error. In analyzing the solution error of different sized non-adapted and adapted grids, one could not only achieve the same level of solution error by adapting and solving on a much coarser grid, but a significant reduction in solution time as well. All test cases revealed that the flow solver required lower amounts of artificial dissipation for solution on the final adapted grids. The current work should convincingly pave the way for its logical extension to unstructured grids, taking further advantage of refinement, coarsening and edge-swapping operations. It is strongly anticipated that this approach will shortly result in "optimal" grids...|$|R

