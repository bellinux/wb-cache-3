30|167|Public
5000|$|... #Caption: Diagram {{illustrating}} compaction of {{data in a}} log-structured <b>merge</b> <b>tree</b> ...|$|E
5000|$|This is {{particularly}} important in some log structured storage systems that use the log-structured merge-tree or LSM-tree. The LSM-tree is actually a collection of trees but which is treated as a single key-value store. One variation of the LSM-Tree is the Sorted Array <b>Merge</b> <b>Tree</b> or SAMT. [...] In this variation, a SAMT's component trees are called Wanna-B-trees. Each Wanna-B-tree has an associated quotient filter. A query on the SAMT is directed at only select Wanna-B-trees as evidenced by their quotient filters.|$|E
40|$|We {{consider}} {{the problem of}} structure prediction for sparse LU factorization with partial pivoting. In this context, {{it is well known}} that the column elimination tree plays an important role for matrices satisfying an irreducibility condition, called the strong Hall property. Our primary goal in this paper is to address the structure prediction problem for matrices satisfying a weaker assumption, which is the Hall property. For this we {{consider the}} row merge matrix, an upper bound that contains the nonzeros in L and U for all possible row permutations that can later appear in the numerical factorization due to partial pivoting. We discuss the row <b>merge</b> <b>tree,</b> a structure that represents information obtained from the row merge matrix; that is, information on the dependencies among the columns in Gaussian elimination with partial pivoting and on structural upper bounds of the factors L and U. We present new theoretical results that show that the nonzero structure of the row merge matrix can be described in terms of branches and subtrees of the row <b>merge</b> <b>tree.</b> These results lead to an efficient algorithm for the computation of the row <b>merge</b> <b>tree,</b> that uses as input the structure of A alone, and has a time complexity almost linear in the number of nonzeros in A. We also investigate experimentally the usage of the row <b>merge</b> <b>tree</b> for structure prediction purposes on a set of matrices that satisfy only the Hall property. We analyze in particular the size of upper bounds of the structure of L and U, the reordering of the matrix based on a postorder traversal and its impact on the factorization runtime. We show experimentally that for some matrices, the row <b>merge</b> <b>tree</b> is a preferred alternative to the column elimination tree...|$|E
40|$|In {{this paper}} we study {{functions}} on the interval {{that have the}} same persistent homology. By introducing an equivalence relation modeled after topological conjugacy, which we call graph-equivalence, a precise enumeration of functions with the same persistent homology is given, inviting comparisons with Arnold's Calculus of Snakes. The equivalence classes used here are indexed by chiral <b>merge</b> <b>trees,</b> which are binary <b>merge</b> <b>trees</b> where a left-right ordering {{of the children of}} each vertex is given. Enumeration of <b>merge</b> <b>trees</b> and chiral <b>merge</b> <b>trees</b> with the same persistence makes essential use of the Elder Rule (a criterion for pairing critical points), which is given a new proof here as well. Comment: 15 pages, 5 figure...|$|R
5000|$|... #Caption: This {{shows the}} merger of two {{binomial}} heaps. This is accomplished by <b>merging</b> two binomial <b>trees</b> of the same order one by one. If the resulting <b>merged</b> <b>tree</b> has the same order as one binomial tree {{in one of the}} two heaps, then those two are merged again.|$|R
50|$|Deletions {{are very}} similar to insertions. One first finds the key k in one of the {{balanced}} binary search trees and delete it from this tree T. To ensure that all balanced binary search trees contain O(log M) elements, one merges T with the balanced binary search tree of its successor or predecessor if it contains less than (log M)/4 elements. The representatives of the <b>merged</b> <b>trees</b> are removed from the x-fast trie. It is possible for the <b>merged</b> <b>tree</b> to contain more than 2 log M elements. If this is the case, the newly formed tree is split into two trees of about equal size. Next, one picks a new representative for each of the new trees and one inserts these into the x-fast trie.|$|R
30|$|Reeber is a topological {{analysis}} code, which constructs merge {{trees of}} scalar fields. A <b>merge</b> <b>tree</b> describes {{the relationship among}} the components of super-level sets, i.e., regions of the data set with values above a given threshold. The leaves of a <b>merge</b> <b>tree</b> represent maxima; its internal nodes correspond to saddles where different components merge; its root corresponds to the global minimum (Morozov and Weber 2013). In the case of Reeber operating on Nyx data sets, the points x and y correspond to distinct grid points r_ 1 and r_ 2, and the function f corresponds to the density ρ(r).|$|E
40|$|International audienceThis paper {{presents}} a new algorithm for the fast, shared memory multi-core computation of augmented merge trees on triangulations. In contrast to most existing parallel algorithms, our technique computes augmented trees. This augmentation {{is required to}} enable {{the full extent of}} <b>merge</b> <b>tree</b> based applications, including data segmentation. Our approach completely revisits the traditional, sequential <b>merge</b> <b>tree</b> algorithm to re-formulate the computation as a set of independent local tasks based on Fibonacci heaps. This results in superior time performance in practice, in sequential as well as in parallel thanks to the OpenMP task runtime. In the context of augmented contour tree computation, we show that a direct usage of our <b>merge</b> <b>tree</b> procedure also results in superior time performance overall, both in sequential and parallel. We report performance numbers that compare our approach to reference sequential and multi-threaded implementations for the computation of augmented merge and contour trees. These experiments demonstrate the run-time efficiency of our approach as well as its scalability on common workstations. We demonstrate the utility of our approach in data segmentation applications. We also provide a lightweight VTK-based C++ implementation of our approach for reproduction purposes...|$|E
30|$|Implementing a {{scalable}} {{representation of}} merge trees on distributed memory systems is a challenging but critical endeavor. Because the <b>merge</b> <b>tree</b> {{is a global}} representation of the entire field, traditional approaches for distributing the tree across independent processes inevitably require communication-intensive reduction to construct the final tree, which in turn lead to poor scalability. Furthermore, modern simulations operate on data sets that are too large for all topological information to fit on a single compute node. Reeber’s ‘local-global’ representation addresses this problem by distributing the <b>merge</b> <b>tree,</b> so that each node stores detailed information about its local data, together with information about how the local data fits into the global <b>merge</b> <b>tree.</b> The overhead from the extra information is minimal, yet it allows individual processors to globally identify components of super-level sets without any communication (Morozov and Weber 2013). As a result, the merge trees can be queried in a distributed way, where each processor is responsible for answering the query with respect to its local data, and a simple reduction is sufficient to add up contribution from different processes. A detailed description of merge trees, contour trees, and their ‘local-global’ representation, which allows Reeber to scale efficiently on distributed memory systems, is given in Morozov and Weber (2013), Morozov and Weber (2014) and its application to halo finding in Morozov et al. (in preparation).|$|E
40|$|Abstract. We {{present a}} new {{algorithm}} for automatic and interactive segmentation of neuron structures from electron microscopy (EM) im-ages. Our method selects {{a collection of}} nodes from the watershed merg-ing tree as the proposed segmentation. This is achieved by building a conditional random field (CRF) whose underlying graph is the <b>merging</b> <b>tree.</b> The maximum a posteriori (MAP) prediction of the CRF is the output segmentation. Our algorithm outperforms state-of-the-art meth-ods. Both the inference and the training are very efficient as the graph is tree-structured. Furthermore, we develop an interactive segmentation framework which selects uncertain regions for a user to proofread. The uncertainty {{is measured by the}} marginals of the graphical model. Based on user corrections, our framework modifies the <b>merging</b> <b>tree</b> and thus improves the segmentation globally...|$|R
40|$|Abstract — A novel {{method to}} {{efficiently}} merge an ensemble or of forced-split decision trees into an “enlightened” decision tree {{is presented in}} this paper. The <b>merged</b> <b>tree</b> combines the best features of trees from the ensemble, expressing the same function as the voted ensemble. This allows much faster evaluation of an ensemble of decision trees produced by Bagging or Boosting, and the resulting enlightened tree is far more interpretable than the ensemble, literally allowing one to more clearly “see the forest through the trees. ” Though no complexity regularization is explicitly performed, the <b>merged</b> <b>tree</b> actually {{turns out to be}} a pruned version of a tree constructed from the entire set of training examples. We also introduce and test a “super-uniform sampling ” technique which outperforms conventional uniform sampling in training individual <b>trees.</b> <b>Merging</b> an ensemble of trees can play a big role in privacy preservation in data, as well as enhance both distributed and online learning of decision trees...|$|R
5000|$|Let [...] and [...] be topological {{spaces and}} let [...] be a {{continuous}} map. Let [...] be a finite open covering of [...] The output of MAPPER is the nerve of the pullback cover , where each preimage is split into its connected components. This {{is a very}} general concept, of which the Reeb graph and <b>merge</b> <b>trees</b> are special cases.|$|R
40|$|Abstract—The ever {{increasing}} {{amount of data}} generated by scientific simulations coupled with system I/O constraints are fu-eling a need for in-situ analysis techniques. Of particular interest are approaches that produce reduced data representations while maintaining the ability to redefine, extract, and study features in a post-process to obtain scientific insights. This paper presents two variants of in-situ feature extraction techniques using segmented merge trees, which encode {{a wide range of}} threshold based features. The first approach is a fast, low communication cost technique that generates an exact solution but has limited scalability. The second is a scalable, local approximation that nevertheless is guaranteed to correctly extract all features up to a predefined size. We demonstrate both variants using some of the largest combustion simulations available on leadership class supercomputers. Our approach allows state-of-the-art, feature-based analysis to be performed in-situ at significantly higher frequency than currently possible and with negligible impact on the overal simulation runtime. Keywords—topological data analysis, feature extraction, in situ analysis, <b>merge</b> <b>tree</b> computation, segmented <b>merge</b> <b>tree</b> I...|$|E
40|$|Abstract Merge trees {{represent}} the topology of scalar functions. To assess the topo-logical similarity of functions, one can compare their merge trees. To do so, one needs {{a notion of}} a distance between merge trees, which we define. We provide examples of using our <b>merge</b> <b>tree</b> distance and compare this new measure to other ways used to characterize topological similarity (bottleneck distance for persistence diagrams) and numerical difference (L∞-norm {{of the difference between}} functions) ...|$|E
40|$|AbstractIn {{the era of}} Bigdata, {{millions}} of searches, queries etc. happens in a second. There {{is a need for}} next generation databases which can store and process bigdata more effectively. NoSQL databases are created to solve the problems of scalability issues related with traditional databases. Year by year the bigdata is getting new dimensions. HBase is a NoSQL database suitable for random, real-time read/write access to Big Data. LSM tree used in HBase helps to achieve this high performance. Commodity hardware have moderate RAM size. SLSM, an optimized Log Structured <b>Merge</b> <b>Tree</b> which dramatically reduces the read amplification and write amplification is proposed in this paper for commodity hardware. Incorporation of variant of bloom filters increases the read performance...|$|E
30|$|We {{also take}} {{advantage}} of the fact that a tree piece can access other tree pieces within the same address space. All the tree pieces within the same address space are merged. After the <b>merge,</b> each <b>tree</b> piece has read-only access to the tree data structure that is constructed by <b>merging</b> multiple <b>tree</b> pieces. For additional details, we refer the reader to Jetley et al. (2008).|$|R
5000|$|Merge: Use a supertree {{method to}} <b>merge</b> the <b>trees</b> on the subsets {{into a tree}} on the full dataset.|$|R
50|$|Adding a {{value to}} a skew heap is like <b>merging</b> a <b>tree</b> with one node {{together}} with the original tree.|$|R
40|$|Scalar fields occur quite {{commonly}} {{in several}} application areas in both static and time-dependent forms. Hence a proper visualization of scalar fieldsneeds to {{be equipped with}} tools to extract and focus on important features of the data. Similarity detection and pattern search techniques in scalar fields present a useful way of visualizing important features in the data. This is done by isolating these features and visualizing them independently or show all similar patterns that arise from a given search pattern. Topological features are ideal for this purpose of isolating meaningful patterns in the data set and creating intuitive feature descriptors. The <b>Merge</b> <b>Tree</b> is one such topological feature which has characteristics ideally suited for this purpose. Subtrees of merge trees segment the data into hierarchical regions which are topologically defined. This kind of feature-based segmentation is more intelligent than pure data based segmentations involving windows or bounding volumes. In this thesis, we explore several different techniques using subtrees of merge trees as features in scalar field data. Firstly, {{we begin with a}} discussion on static scalar fields and devise techniques to compare features - topologically segmented regions given by the subtrees of the <b>merge</b> <b>tree</b> - against each other. Second, we delve into time-dependent scalar fields and extend the idea of feature comparison to spatio-temporal features. In this process, we also come up with a novel approach to track features in time-dependent data considering the entire global network of likely feature associations between consecutive time steps. The highlight of this thesis is the interactivity that is enabled using these feature-based techniques by the real-time computation speed of our algorithms. Our techniques are implemented in an open-source visualization framework Inviwo and are published in several peer-reviewed conferences and journals. QC 20171020 </p...|$|E
40|$|We {{present a}} method to find repeating topological {{structures}} in scalar data sets. More precisely, we compare all subtrees of two merge trees against each other – in an efficient manner exploiting redundancy. This provides pair-wise distances between the topological structures defined by sub/superlevel sets, which can be exploited in several applications such as finding similar structures in the same data set, assessing periodic behavior in time-dependent data, and comparing the topology of two different data sets. To do so, we introduce a novel data structure called the extended branch decomposition graph, which is composed of the branch decompositions of all subtrees of the <b>merge</b> <b>tree.</b> Based on dynamic programming, we provide two highly efficient algorithms for computing and comparing extended branch decomposition graphs. Several applications attest to the utility of our method and its robustness against noise. 1...|$|E
40|$|There are {{numerous}} applications that require on-line {{access to a}} history of business events. Ideally, both historical and current data should be logically integrated into some form of temporal database, {{also known as a}} multi-version database, historical database, or rollback database. The underlying access method should support the migration of old record versions onto inexpensive write-once media, different forms of “time-travel ” queries, and potentially high rates of update events. This paper introduces a new access method for history data, called the Log-structured History data Access Method (LHAM). The basic principle of LHAM is to partition the data into successive components based on the timestamps of the record versions, and to employ a rolling merge process for efficient data migration between components. The concept of a rolling merge process was introduced in [OCGO 92] with the Log-Structured <b>Merge</b> <b>Tree</b> (LSM-tree). ...|$|E
40|$|Abstract- A {{huge amount}} of {{information}} on the World Wide Web has a structured HTML form as they are generated dynamically from databases and have the same template. This paper proposes a page-level web data extraction system that extracts schema and templates from these template-based web pages automatically. The proposed system uses visual clues for comparing web pages for fixed/variant template detection. From fixed template pages, we construct pattern tree which is used to detect schema & extract data. It detects schema by applying <b>tree</b> <b>merging,</b> <b>tree</b> alignment and mining techniques. The experiments show a good result for the web pages used in many web data extraction. Index Terms- Information Retrieval, Multiple <b>trees</b> <b>merging,</b> Web data extraction, Wrapper Inductio...|$|R
40|$|International audienceMesh {{segmentation}} is {{a fundamental}} problem in computer graphics and has become an important component in many applications. This paper proposes a new hierarchical mesh segmentation method based on waterfall and dynamics. Watershed transformation allows segmenting the mesh in small patches and dynamics provide information about boundaries and merging possibilities. From region and boundary information, a hierarchical process based on waterfall is computed {{in order to build}} the <b>merging</b> <b>tree.</b> This tree contains all the schemes of segmentation and the user can easily browse the different levels of segmentation...|$|R
40|$|Models of Computation 4 - 2 - 3 <b>Merge</b> <b>Trees</b> 4 - 2 - 4 Precedence Trees for Multiprocessor Scheduling 4 - 3 Minimum Spanning Trees 4 - 4 Geometric Minimum Spanning Trees 4 - 5 Acyclic Digraphs 4 - 5 - 1 Bill of Materials (Topological Sorting) 4 - 5 - 2 Deadlock Avoidance (Cycle Testing) 4 - 5 - 3 PERT (Longest Paths) 4 - 5 - 4 Optimal Register Allocation (Tree Labelling) 4 - 6 Fibonacci Heaps and Minimum Spanning Trees References and Further Reading Exercises 5 Depth First Search 5 -...|$|R
40|$|Resource-sharing {{techniques}} {{are widely used}} by VOD servers. Stream merging {{is one of the}} most efficient resource-sharing techniques. ERMT is able to achieve merge trees with the closest cost of optimal <b>merge</b> <b>tree.</b> Full VCR support has become a “must have” feature for VOD services. This researcher proposed an algorithm to enable VCR support on ERMT. Furthermore, client local buffer and fixed-interval periodical multicasting were also deployed by the algorithm to improve the stream-client ratio. After thorough runs of simulations and numerous comparisons to BEP, the highly efficient resource- sharing technique, the proposed algorithm with client local buffer utilization and fixed- interval multicasting showed better performance in all simulations. The biggest discovery is that the best-performer is modified ERMT with client local buffer support for VCR without fixed-interval multicasting. Another discovery is that bigger client buffer size hurts the performance of ERMT...|$|E
40|$|Modern {{parallel}} and cluster file systems provide {{highly scalable}} I/O bandwidth by enabling highly parallel {{access to file}} data. Unfortunately metadata access does not benefit from parallel data transfer, so metadata performance scaling is less common. To support metadata-intensive workloads, we offer a middleware design that layers on top of existing cluster file systems, adds support for load balanced and high-performance metadata operations without sacrificing data bandwidth. The core idea is to integrate a distributed indexing mechanism with a metadata optimized on-disk Log-Structured <b>Merge</b> <b>tree</b> layout. The integration requires several optimizations including cross-server split operations with minimum data migration, and decoupling of data and metadata paths. To demonstrate the feasibility of our approach, we implemented a prototype middleware layer GIGA+TableFS and evaluated it with a Panasas parallel file system. GIGA+TableFS improves metadata performance of PanFS by as much an order of magnitude, while still performing comparably on data-intensive workloads...|$|E
40|$|Abstract—In {{this paper}} we {{describe}} the design and imple-mentation of ACaZoo 1, a key-value store that combines strong consistency with high performance and high availability. ACaZoo supports the popular column-oriented data model of Apache Cassandra and HBase. It implements strongly-consistent data replication using primary-backup atomic broadcast of a write-ahead log, which records data mutations to a Log-structured <b>Merge</b> <b>Tree</b> (LSM-Tree). ACaZoo scales by horizontally partition-ing the key space via consistent primary-key hashing on available replica groups (RGs). LSM-Tree compactions can hamper perfor-mance, especially when they take place at RG primaries. ACaZoo addresses this problem by changing RG leadership prior to heavy compactions, a method that can improve throughput by up to 40 % in write-intensive workloads. We evaluate ACaZoo using the Yahoo Cloud Serving Benchmark (YCSB) and compare it to Oracle’s NoSQL Database and to Cassandra providing serial consistency via {{an extension of the}} Paxos algorithm. I...|$|E
5000|$|... recursive: This is {{the default}} when pulling or merging one branch, {{and is a}} variant of the {{three-way}} merge algorithm. [...] When there are more than one common ancestors {{that can be used for}} three-way merge, it creates a <b>merged</b> <b>tree</b> of the common ancestors and uses that as the reference tree for the three-way merge. This has been reported to result in fewer merge conflicts without causing mis-merges by tests done on prior merge commits taken from Linux 2.6 kernel development history. Also, this can detect and handle merges involving renames. Linus Torvalds ...|$|R
40|$|We {{employ a}} {{stochastic}} approach to probing {{the origin of}} the log-normal distributions of halo spin in N-body simulations. After analyzing spin evolution in halo <b>merging</b> <b>trees,</b> it was found that a spin change can be characterized by a stochastic random walk of angular momentum. Also, spin distributions generated by random walks are fairly consistent with those directly obtained from N-body simulations. We derived a stochastic differential equation from a widely used spin definition and measured the probability distributions of the derived angular momentum change from a massive set of halo <b>merging</b> <b>trees.</b> The roles of major merging and accretion are also statistically analyzed in evolving spin distributions. Several factors (local environment, halo mass, merging mass ratio, and redshift) are found to influence the angular momentum change. The spin distributions generated in the mean-field or void regions tend to shift slightly to a higher spin value compared with simulated spin distributions, which seems to be caused by the correlated random walks. We verified the assumption of randomness in the angular momentum change observed in the N-body simulation and detected several degrees of correlation between walks, which may provide a clue for the discrepancies between the simulated and generated spin distributions in the voids. However, the generated spin distributions in the group and cluster regions successfully match the simulated spin distribution. We also demonstrated that the log-normality of the spin distribution is a natural consequence of the stochastic differential equation of the halo spin, which is well described by the Geometric Brownian Motion model. Comment: 21 pages, 31 figures, Accepted for publication in ApJ...|$|R
50|$|If {{only one}} of the heaps {{contains}} a tree of order j, this tree is moved to the merged heap. If both heaps contain a tree of order j, the two <b>trees</b> are <b>merged</b> to one <b>tree</b> of order j+1 so that the minimum-heap property is satisfied. Note that it may later be necessary to <b>merge</b> this <b>tree</b> with some other tree of order j+1 present in one of the heaps. In the course of the algorithm, we need to examine at most three trees of any order (two from the two heaps we merge and one composed of two smaller trees).|$|R
40|$|Several recent SIGGRAPH {{papers on}} surface {{simplification}} are described. Keywords: Mesh simplification, decimation. The stringent demands of real-time graphics have engendered {{a need for}} simplification of object models. Here several papers on aspects of the problem for 3 D polygonal models are described at a high level. 1. Levels of Detail A consensus may be emerging {{in favor of the}} representation of complex models as a single, hierarchical data structure that represents many levels of detail simultaneously, from the simplified root to the fully detailed leaves. The hierarchy is known under various names: vertex tree, 7 <b>merge</b> <b>tree,</b> 9 vertex hierarchy, 6 progressive mesh, 5 progressive simplicial complex. 8 We will use vertex tree here to refer to the generic concept. Typically the tree is binary, with each node represting a vertex of some simplification of the original model. The two children vertices v 1 and v 2 of their parent v are merged (or identified, or unified, or [...] ...|$|E
40|$|Abstract. View-dependent mesh {{refinement}} techniques typically pre-compute {{a hierarchical}} data structure that is queried at run-time {{to produce an}} approximation of a given mesh. This approximation, or level-of-detail (LOD), {{can be used in}} place of the original object so long as the viewpoint does not change. This should result in no loss of visible detail, yet should be less computationally expensive. The approach of existing techniques is to collapse or expand nodes in the hierarchical data structure level by level. We propose a new method for fast generation of view-dependent level-of-details when the frame-to-frame coherence is low, such as when an object moves or rotates at a rapid rate with respect to the viewpoint. Our method is based on two new techniques for aggressive detection of visible parts of the <b>merge</b> <b>tree</b> data structure. The jump split and jump collapse move the active nodes front up or down the tree many generations at a time, reducing the number and expense of iterations required to refine the mesh. ...|$|E
40|$|We {{develop and}} {{implement}} a concurrent dictionary data structure based on the Log Structured <b>Merge</b> <b>tree</b> (LSM), suitable for current massively parallel GPU architectures. Our GPU LSM is dynamic (mutable) in that it provides fast updates (insertions and deletions). For example, on an NVIDIA K 40 c GPU we can get an average update rate of 225 M elements/s (13. 5 x faster than merging with a sorted array). GPU LSM also supports lookup, count, and range query operations with an average rate of 75 M, 32 M and 23 M queries/s respectively. For lookups, we are 7. 5 x (and 1. 75 x) slower than a hash table (and a sorted array). However, none of these other data structures are considered mutable, and hash tables cannot even support count and range queries. We believe that our GPU LSM is the first dynamic general-purpose GPU data structure. Comment: 11 pages, in preparation for publicatio...|$|E
40|$|We {{present an}} {{application}} of Integer Programming {{to the design of}} arrival routes for aircraft in a Terminal Maneuvering Area (TMA). We generate operationally feasible <b>merge</b> <b>trees</b> of curvature-constrained routes, using two optimization criteria: (1) total length of the tree, and (2) distance flown along the tree paths. The output routes guarantee that the overall traffic pattern in the TMA can be monitored by air traffic controllers; in particular, we keep merge points for arriving aircraft well separated, and we exclude conflicts between arriving and departing aircraft. We demonstrate the feasibility of our method by experimenting with arrival routes for a runway at Arlanda airport in the Stockholm TMA. Our approach can easily be extended in several ways, e. g., to ensure that the routes avoid no-fly zones...|$|R
40|$|In this paper, {{we present}} our new {{experimental}} system of merging dependency representations of two parallel sentences into one dependency tree. All the inner nodes in dependency tree represent source-target pairs of words, the extra words are {{in form of}} leaf nodes. We use Universal Dependencies annotation style, in which the function words, whose usage often differs between languages, are annotated as leaves. The parallel treebank is parsed in minimally supervised way. Unaligned words are there automatically pushed to leaves. We present a simple translation system trained on such <b>merged</b> <b>trees</b> and evaluate it in WMT 2016 English-to-Czech and Czech-to-English translation task. Even though the model is so far very simple and no language model and word-reordering model were used, the Czech-to-English variant reached similar BLEU score as another established tree-based system...|$|R
40|$|In {{this paper}} we {{proposed}} a new approach called Extended Fivatech from the problem of multiple input pages. Extended FivaTech can reduce the schema and templates for each individual web page generated from a CGI program. It also merge the multiple input pages into the single page to generate the dynamic web pages Extended FiaTech uses tree templates. Extended FivaTech reduce the schema templates for each individual Deep web site which contains either singleton or multiple data records in one Web page and merging of web pages into a single page. Extended FivaTech applies <b>tree</b> <b>merging,</b> <b>tree</b> alignment and mining techniques, merging of the input pages to get the outstanding task. These experiments show a good result for the web pages used in many web data extractions. 1...|$|R
