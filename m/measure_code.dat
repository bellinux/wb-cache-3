20|564|Public
50|$|The QA {{department}} can’t {{measure the}} code’s integrity {{even after all}} their tests are run. The only way to <b>measure</b> <b>code</b> integrity, and be sure of your code, is by unit testing your code, and reaching high code coverage.|$|E
50|$|Like any {{security}} <b>measure,</b> <b>code</b> signing can be defeated. Users can be tricked into running unsigned code, or {{even into}} running code {{that refuses to}} validate, and the system only remains secure {{as long as the}} private key remains private.|$|E
5000|$|In {{software}} engineering, many reuse metrics {{and models}} {{have been developed to}} <b>measure</b> <b>code</b> reuse and reusability. A metric is a quantitative indicator of an attribute of a thing. A model specifies relationships among metrics. Reuse models and metrics can be categorized into six types: ...|$|E
5000|$|To <b>measuring</b> <b>code</b> integrity, use the {{following}} formula:1 − (Non-covered bugs) / (Total bugs) ...|$|R
50|$|Cobertura is an {{open source}} tool for <b>measuring</b> <b>code</b> coverage. It does so by instrumenting the byte code.|$|R
5000|$|Robert Cockburn, {{electronics}} engineer. He {{directed the}} development of radar jamming systems (counter <b>measures)</b> <b>code</b> named Window and widely known as Chaff. An obituary describes this work as [...] "a main contributor to the reduction of civilian casualties ... and [...] losses". He is in a group photograph. Later, he was knighted.|$|R
50|$|VB Watch Profiler {{measures}} {{the speed of}} a running Visual Basic program. It displays the time spent in each procedure and/or a line of code. This information can be used in code optimization to detect bottleneck procedures and lines. The Profiler {{can also be used to}} <b>measure</b> <b>code</b> coverage during software testing.|$|E
40|$|In article present <b>measure</b> <b>code</b> {{distance}} algorithm of binary and ternary linear {{block code}} using block Korkin-Zolotarev (BKZ). Proved the {{upper bound on}} scaling constant for <b>measure</b> <b>code</b> distance of non-systematic linear block code using BKZ-method for different value of the block size. Introduced method show linear decrease of runtime from number of threads and work especially good under not dense lattices of LDPC-code. These properties allow use this algorithm to measure the minimal distance of code with length several thousand. The algorithm can further improve by transform into probabilistic algorithm using lattice enumerating pruning techniquesComment: 14 pages, 2 tables, in Russia...|$|E
40|$|The paper {{describes}} which free instruments can be {{used during}} development process and for what this instruments is used. There is {{a large number of}} such software to <b>measure</b> <b>code</b> quality, collect project metrics, build code coverage statistics, manage source code etc. And all this instruments distributed under free-like licences...|$|E
5000|$|... #Caption: A {{two-dimensional}} visualisation of the Hamming distance, {{a critical}} <b>measure</b> in <b>coding</b> theory.|$|R
5000|$|Parasoft C/C++test is a {{combined}} {{set of tools}} that helps developers test their software. It's delivered as a standalone application that runs from the command line, or as a plug-in to Eclipse or Microsoft Visual studio. Various modules in the set assist software developers in performing static and dynamic analysis, creating, executing and maintaining unit tests, <b>measuring</b> <b>code</b> coverage and other software metrics, and executing regression tests.|$|R
50|$|The {{national}} Weights & <b>Measures</b> <b>codes</b> {{and regulations}} control the {{wholesale and retail trade}} requirements to facilitate fair trade. The regulations and accuracy requirements vary widely between countries and commodities, {{but they all}} have one common characteristic - “traceability”. There is always a procedure that defines the validation process where the duty meter is compared to a standard that is traceable to the legal metrology agency of the respective region.|$|R
40|$|The Gini {{coefficient}} is {{a prominent}} measure {{to quantify the}} inequality of a distribution. It is often used {{in the field of}} economy to describe how goods, e. g., wealth or farmland, are distributed among people. We use the Gini coefficient to <b>measure</b> <b>code</b> ownership by investigating how changes made to source code are distributed among the developer population. The results of our study with data from the Eclipse platform show that less bugs can be expected if a large share of all changes are accumulated, i. e., carried out, by relatively few developers. Accepted for publication in the proceedings of the InternationalWorkshop on Principles on Software Evolution, ERCIM Workshop on Software Evolution, 2011, ACM Press. Software TechnologyElectrical Engineering, Mathematics and Computer Scienc...|$|E
40|$|In any stored-program {{computer}} system, {{information is}} constantly transferred between the {{memory and the}} instruction processor. Machine instructions are {{a major portion of}} this traffic. Since transfer bandwidth is a limited resource, inefficiency in the encoding of instruction information (low code density) can have definite hardware and performance costs. Starting with a parameterized baseline RISC design, we compare performance for two instruction encodings for the architecture. One is a variant of DLX, the other is a 16 -bit format which sacrifices some expressive power while retaining essential RISC features. Using optimizing compilers and software simulation, we <b>measure</b> <b>code</b> density and path length for a suite of benchmark programs, relating performance differences to specific instruction set features. We measure time to completion performance while varying memory latency and instruction cache size parameters. The 16 -bit format is shown to have significant cost-performance advantage [...] ...|$|E
40|$|Abstract — Every {{software}} Industry {{requires the}} quality of code. Formal specifications are mathematically based techniques whose purposes are {{to help with the}} implementation of systems and software. They are used to describe a system, to analyze its behavior, and to aid in its design by verifying key properties of interest through rigorous and effective reasoning tools. These specifications are formal {{in the sense that they}} have syntax, their semantics fall within one domain, and they are able to be used to infer useful information. Formal specifications can help with program testing, optimization, refactoring. However, they are difficult to write manually, and automatic mining techniques suffer from 90 – 99 % false positive rates. To address this problem, we propose to augment a temporal-property miner by incorporating code quality metrics. We <b>measure</b> <b>code</b> quality by extracting additional information from the software engineering process, and using information from code that is more likely to be correct as well as code that is less likely to be correct...|$|E
30|$|Three {{performance}} parameters {{were used to}} <b>measure</b> <b>coding</b> efficiency, computational complexity and memory complexity. The average bitrate reduction and the average video quality improvement were used to <b>measure</b> <b>coding</b> efficiency. Both were exploited from rate-distortion curves using the average differences for bitrate and PSNR (for the luminance component) when applying two different prediction architectures. The total coding time was used to reflect the computational complexity of a particular prediction architecture, {{since most of the}} coding time is consumed during the prediction stage. Average coding time reduction was calculated by measuring the running time when applying a prediction architecture (A) compared to corresponding time from another prediction architecture (B). Therefore, coding time reduction is the result of dividing the difference between coding times for these architectures by the coding time consumed when deploying prediction architecture (B). Similarly, memory complexity was calculated; it is defined by the minimum number of reference frames stored in the decoded picture buffer (taking into account full spatial-resolution frames which would be decimated prior to predicting frames with lower spatial-resolution and vice versa).|$|R
3000|$|... in Eq. (1) {{includes}} the base model variables and each couple member’s own health status (a binary <b>measure</b> <b>coded</b> as 1 {{if the individual}} records their health as fair or poor, {{as opposed to being}} good, very good or excellent). Sample attrition is found to be relevant and should be addressed when considering coupled migration within Australia (as indicated by the significant Inverse Mills Ratio in the penultimate row of Table  4).|$|R
30|$|From Figure 7, we {{can observe}} {{that when the}} SER is 10 - 3, the <b>measured</b> {{spreading}} <b>code's</b> SNR of 4 dB {{is better than the}} OOK's SNR of 10 dB. Given a SER of 10 - 2, the <b>measured</b> spreading <b>code's</b> SNR is - 4 dB which is even much better than the OOK's SNR of 7 dB as our algorithm provides error-resilient capability to suppress the out-of-band and the in-band interference when the receiving signal strength is under the noise floor.|$|R
40|$|Abstract. We {{address the}} problem of {{creating}} entire and complete maps of software code clones (copy features in data) in a corpus of binary artifacts of unknown provenance. We report on a practical methodology, which employs enhanced suffix data structures and partial orderings of clones to compute a compact representation of most interesting clones features in data. The enumeration of clone features is useful for malware triage and prioritization when human exploration, testing and verifica-tion is the most costly factor. We further show that the enhanced arrays may be used for discovery of provenance relations in data and we intro-duce two distinct Jaccard similarity coefficients to <b>measure</b> <b>code</b> similar-ity in binary artifacts. We illustrate the use of these tools on real malware data including a retro-diction experiment for measuring and enumerat-ing evidence supporting common provenance in Stuxnet and Duqu. The results indicate the practicality and efficacy of mapping completely the clone features in data...|$|E
40|$|We {{address the}} problem of {{creating}} entire and complete maps of software code clones (copy features in data) in a corpus of binary artifacts of unknown provenance. We report on a practical methodology, which employs enhanced suffix data structures and partial orderings of clones to compute a compact representation of most interesting clones features in data. The enumeration of clone features is useful for malware triage and prioritization when human exploration, testing and verification is the most costly factor. We further show that the enhanced arrays may be used for discovery of provenance relations in data and we introduce two distinct Jaccard similarity coefficients to <b>measure</b> <b>code</b> similarity in binary artifacts. We illustrate the use of these tools on real malware data including a retro-diction experiment for measuring and enumerating evidence supporting common provenance in Stuxnet and Duqu. The results indicate the practicality and efficacy of mapping completely the clone features in data. Comment: 17 pages, 4 figures, 3 table...|$|E
40|$|Abstract—Formal {{specifications}} {{can help}} with program testing, optimization, refactoring, documentation, and, most importantly, debugging and repair. However, they are difficult to write manually, and automatic mining techniques suffer from 90 – 99 % false positive rates. To address this problem, we propose to augment a temporal-property miner by incorporating code quality metrics. We <b>measure</b> <b>code</b> quality by extracting additional information from the software engineering process, and using information from code that {{is more likely to}} be correct as well as code that is less likely to be correct. When used as a preprocessing step for an existing specification miner, our technique identifies which input is most indicative of correct program behavior, which allows off-the-shelf techniques to learn the same number of specifications using only 45 % of their original input. As a novel inference technique, our approach has few false positives in practice (63 % when balancing precision and recall, 3 % when focused on precision), while still finding useful specifications (e. g., those that find many bugs) on over 1. 5 million lines of code...|$|E
40|$|A hybrid {{approach}} that utilizes both statistical techniques and empirical methods seeks {{to provide more}} information about the performance of an application. In this paper, we present a general approach to creating hybrid models of this type. We show that for the scientific applications of interest, the scaled performance is somewhat predictable due to the regular characteristics of the <b>measured</b> <b>codes.</b> Furthermore, the resulting method encourages streamlined performance evaluation by determining which analysis steps may provide further insight to code performance...|$|R
40|$|The {{features}} of a TDM (time-division multiplexed) link model are described. A PCM telemetry sequence was coded for error correction and multiplexed with a digitized voice channel. An all-digital implementation of a variable-slope delta modulation algorithm was used to digitize the voice channel. The results of extensive testing are reported. The <b>measured</b> <b>coding</b> gain and the system performance over a Gaussian channel are compared with theoretical predictions and computer simulations. Word intelligibility scores are reported {{as a measure of}} voice channel performance...|$|R
5000|$|Criminal and {{criminal}} procedure - the Penal Code (1982), the Penal Procedure Code (1987), the Military Justice Code (2003) and the Sentence and Custodial <b>Measures</b> Execution <b>Code</b> (2009); ...|$|R
40|$|Abstract—In {{software}} development, early {{identification of}} fault-prone classes {{can save a}} considerable amount of resources. In the literature, source code structural metrics have been widely investigated as one of the factors {{that can be used to}} identify faulty classes. Structural metrics <b>measure</b> <b>code</b> complexity, one aspect of the source code quality. Complexity might affect program understanding and hence increase the likelihood of inserting errors in a class. Besides the structural metrics, we believe that the quality of the identifiers used in the code may also affect program understanding and thus increase the likelihood of error insertion. In this study, we measure the quality of identifiers using the number of Lexicon Bad Smells (LBS) they contain. We investigate whether using LBS in addition to structural metrics improves fault prediction. To conduct the investigation, we assess the prediction capability of a model while using i) only structural metrics, and ii) structural metrics and LBS. The results on three open source systems, ArgoUML, Rhino, and Eclipse, indicate that there is an improvement in the majority of the cases. I...|$|E
40|$|Users like sharing {{personal}} {{photos with}} others through social media. At the same time, {{they might want}} to make automatic identification in such photos difficult or even impossible. Classic obfuscation methods such as blurring are not only unpleasant but also not as effective as one would expect. Recent studies on adversarial image perturbations (AIP) suggest {{that it is possible to}} confuse recognition systems effectively without unpleasant artifacts. However, in the presence of counter measures against AIPs, it is unclear how effective AIP would be in particular when the choice of counter measure is unknown. Game theory provides tools for studying the interaction between agents with uncertainties in the strategies. We introduce a general game theoretical framework for the user-recogniser dynamics, and present a case study that involves current state of the art AIP and person recognition techniques. We derive the optimal strategy for the user that assures an upper bound on the recognition rate independent of the recogniser's counter <b>measure.</b> <b>Code</b> is available at [URL] To appear at ICCV' 1...|$|E
40|$|It is {{naturally}} easier to comprehend simple code relative to com-plicated code. Regrettably, {{there is little}} agreement on how to effectively <b>measure</b> <b>code</b> complexity. As a result simple general-purpose metrics are often used, such as lines of code (LOC), Mc-Cabe’s cyclomatic complexity (MCC), and Halstead’s metrics. But such metrics just count syntactic features, and ignore details of the code’s global structure, which may also {{have an effect on}} under-standability. In particular, we suggest that code regularity—where the same structures are repeated time after time—may significantly reduce complexity, because once one figures out the basic repeated element it is easier to understand additional instances. We demon-strate this by controlled experiments where subjects perform cogni-tive tasks on different versions of the same basic function. The re-sults indicate that versions with significant regularity lead to better comprehension, while taking similar time, despite being longer and having higher MCC. These results indicate that regularity is another attribute of code that should be taken into account in the context of studying the code’s complexity and comprehension. Moreover, the fact that regularity may compensate for LOC and MCC demon-strates that complexity cannot be decomposed into independently addable contributions by individual attributes...|$|E
40|$|Intuitive {{interaction}} {{is based on}} past experience and is fast and often non conscious. We have conducted ten studies into this issue {{over the past ten}} years, involving more than 400 participants. Data collection methods have included questionnaires, interviews, observations, concurrent and retrospective protocols, and cognitive <b>measures.</b> <b>Coding</b> schemes have been developed to suit each study and involve robust, literature based heuristics. Some other researchers have investigated this issue and their methods are also examined. The paper traces the development of the methods and compares the various approaches used over the years...|$|R
40|$|Our {{questions}} Is there {{a correlation}} between a student 2 ̆ 7 s spatial abilities and her ability in programming? Spatial abilities are measured through the R-PSVT, and CS programming ability is measured the 2009 AP CS MC questions If yes, can we increase programming success through the teaching of spatial skills? Results Spatial training seemed to be correlated with better CS gains, and in particular helped Hispanic women and students from low SES backgrounds Caveats We <b>measured</b> <b>code</b> reading, but taught code writing Differing student demographics for the 2 sessions Small...|$|R
50|$|In 1930 and 1945 {{general public}} welfare funds were {{introduced}} that included retirement plans. The {{employees of the}} companies who had previously been granted pension schemes decided to keep them instead of participating in the new plans. The ordonnance (law created by the executive organ of State) of October 4, 1945, which is now incorporated into the set of laws regulating social securities <b>measures</b> (<b>code</b> de la Sécurité Sociale), officially permitted these older pension plans to subsist and they {{became known as the}} régimes spéciaux (special retirement plans).|$|R
30|$|The {{techniques}} we used {{to measure}} the results of our experiments were statement and branch coverage and mutation analysis. The risk associated to that choice is that they may not correspond to actual defect detection capabilities of the test sets. However, they are commonly used with this goal by the software testing community and are considered reliable baseline references for the evaluation of test sets. Smaller issues, often considered as construct validity threats, are related to the auxiliary tools and processes used to <b>measure</b> <b>code</b> coverage and mutation scores such as slightly diverging results observed with different versions of the code coverage tool. The conclusions of the study should not be affected however by those small measurement variations. Another construct validity threat is specifically related to the Lua API case study. The reduction of the Lua API model, forced by a state explosion problem in the original model, had as a secondary consequence that the resulting model, although much more complex than previously performed case studies was not as demanding of BETA as first expected. As one of the main objectives of this case study was to evaluate the behavior of BETA with a complete real world specification, this reduction slightly hindered our goal.|$|E
40|$|The Solar-A {{satellite}} {{being prepared}} by the Institute for Sapce and Astronautical Sciences (ISAS) in Japan is dedicated to high energy observations of solar flares. The Soft X Ray Telescope (SXT) is being prepared to provide filtered images in the 2 to 60 A interval. The flight model is now undergoing tests in the 1000 foot tunnel at MSFC. Launch will be in September 1991. Earlier resolution and efficiency tests on the grazing incidence mirror have established its performance in soft x rays. The one-piece, two mirror grazing incidence telescope is supported in a strain free mount separated from the focal plane assembly by a carbon-epoxy metering tube whose windings and filler are chosen to minimize thermal and hygroscopic effects. The CCD detector images both the x ray and the concentric visible light aspect telescope. Optical filters provide images at 4308 and 4700 A. The SXT will be capable of producing over 8000 of the smallest partial frame images per day, or fewer but larger images, up to 1024 x 1024 pixel images. Image sequence with {{two or more of}} the five x ray analysis filters, with automatic exposure compensation to optimize the charge collection by the CCD detector, will be used to provide plasma diagnostics. Calculations using a differential emission <b>measure</b> <b>code</b> were used to optimize filter selection over the range of emission measure variations and to avoid redundancy, but the filters were chosen primarily to give ratios that are monotonic in plasma temperature...|$|E
40|$|Introduction: Because lack of {{inpatient}} {{capacity is}} associated with emergency department (ED) crowding, more efficient bed management could potentially alleviate this problem. Our goal {{was to assess the}} impact of involving a patient placement manager (PPM) early in the decision to hospitalize ED patients. The PPMs are clinically experienced registered nurses trained in the institution-specific criteria for correct unit and bed placement. Methods: We conducted two pilot studies that included all patients who were admitted to the adult hospital medicine service: 1) 10 / 24 to 11 / 22 / 2010 (30 days); and 2) 5 / 24 to 7 / 4 / 2011 (42 days). Each pilot study consisted of a baseline control period and a subsequent study period of equal duration. In each pilot we measured: 1) the number of “lateral transfers” or assignment errors in patient placement, 2) median length of stay (LOS) for “all” and “admitted” patients and 3) inpatient occupancy. In pilot 2, we added as a <b>measure</b> <b>code</b> 44 s, i. e. status change from inpatient to observation after patients are admitted, and also equipped all emergency physicians with portable phones in order to improve the efficiency of the process. Results: In pilot 1, the number of “lateral transfers” (incorrect patient placement assignments) during the control period was 79 of the 854 admissions (9. 3 %) versus 27 of 807 admissions (3. 3 %) during the study period (P< 0. 001). We found no statistically significant differences in inpatient occupancy or ED LOS for “all” or for “admitted” patients. In pilot 2, the number of “lateral transfers” was 120 of 1, 253 (9. 6 %) admissions in the control period and 42 of 1, 229 (3. 4 %) admissions in the study period (P< 0. 001). We found a 49 -minute (352 vs. 401 minutes) decrease in median LOS for “admitted” ED patients during the study period compared with the control period (P= 0. 04). The code 44 rates, median LOS for “all” patients and inpatient occupancy did not change. Conclusion: Inclusion of the PPM in a three-way handoff conversation between emergency physicians and hospitalist providers significantly decreased the number of “lateral transfers. ” Moreover, adding status determination and portable phones for emergency physicians improved the efficiency of the process and was associated with a 49 (12 %) minute decrease in LOS for admitted patients. [West J Emerg Med. 2014; 15 (6) :- 0...|$|E
40|$|Abstract. Existing Java {{profilers}} mostly {{use one of}} {{two distinct}} profiling methods, sampling and instrumentation. Sampling is not as informative as instrumentation but the overall overhead can be small. Instrumentation is more informative than sampling, since it intercepts every entrance and exit in the <b>measured</b> <b>code,</b> but the overhead is large. In this paper, we propose a method that collects profiling information associated with a specific object instance, rather than with a specific code location. Our method, called object instance profiling, can collect contextual information similarly to other instrumentation methods, but can be used more selectively and therefore with lesser overhead. ...|$|R
30|$|A good exemplar {{for this}} design is the {{investigation}} of technology collaborations (Davis and Eisenhardt 2011). The {{purpose of this paper}} is to understand processes by which technology collaborations support innovations. Eight technology collaborations among ten firms were sampled for theoretical reasons. Qualitative and quantitative data were used from semi-structured interviews, public and private data, materials provided by informants, corporate intranets, and business publications. The data was <b>measured,</b> <b>coded,</b> and triangulated. Writing case histories was a basis for within-case and cross-case analysis. Iteration between cases and emerging theory and considering the relevant literature provided the basis for the development of a theoretical framework.|$|R
40|$|A {{test was}} {{constructed}} to <b>measure</b> <b>coding</b> ability- skill in recognizing classes of words grouped by their phonic {{structure and the}} grapheme-phoneme combinations they contained. When the test was administered to low and middle group readers and to children in a learning disability program, marked differences in mas-tery of these skills appeared among children who had similar word recognition scores on standardized tests. The ability to read any or most words incorporating particular grapheme-phoneme combinations within a particular phonic structure requires mastery of the coding process and represents a more complex skill than the ability to recognize by sight a few words incorporating these same phonic ele...|$|R
