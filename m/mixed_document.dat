7|153|Public
40|$|Currently, {{there have}} been several high {{performance}} OCR products for Chinese or for English. However, no one OCR technique can be simultaneously fit for both the English and the Chinese due to the large differences between Chinese and English. On the other hand, Chinese/English <b>mixed</b> <b>document</b> increases drastically with the globalization, so it is rather important to study the Chinese/English <b>mixed</b> <b>document</b> processing. Obviously, the key problem to resolve is how to split the <b>mixed</b> <b>document</b> into two parts: Chinese part and English part, so that the different OCR techniques can be applied to different parts. To further improve the previous system performance, a novel Chinese/English split algorithm based on global information is proposed and a rule for language identification is achieved by Bayesian formula. Experiment shows, the system error rate drops from 1. 52 % to 0. 87 % on magazine samples and from 1. 32 % to 0. 75 % on book samples, more than 2 / 5 of errors are excluded, which provides an experimental support for our research work. 1...|$|E
40|$|This paper {{presents}} a new extracting method for {{several types of}} texts from a text/graphic <b>mixed</b> <b>document</b> image. We also propose a new word grouping method when intersected words are placed on a circular arc or any line segment with an arbitrary orientation. The basic strategy of our algorithm {{is based on the}} analysis of the run-length of the document image. The average and variance of the number of runs in a run-length encoding provide a nice structural property for symbols and texts. We propose 3 -dimensional neighborhood graph for grouping word from a set of isolated characters, which are obtained from the first character-isolating phase. This graph maps each letter to a vertex in 3 -dimensional space according to the "volume" of the character. Experimental results show that more than 97 % of words were successfully extracted from a text/graphic <b>mixed</b> <b>document.</b> Keywords: document analysis, pattern recognition, text extraction, image processing 1 INTRODUCTION There is an eve [...] ...|$|E
40|$|Existing {{document}} decomposition models fail in well {{separating the}} zone areas of printed text and handwritten text {{when they are}} close or even touching each other. This paper presents a simple and robust algorithm to filter out the printed content in a <b>mixed</b> <b>document.</b> Following the traditional bottom-up approach, the printed text candi-dates are extracted and detected in the connected compo-nent level. Then the relative spatial relation and window-based filter help providing useful information to decide the components to be removed finally. Results with manually cropped signature blocks that contain extraneous printed text show that over 85 % of printed text components are removed while preserving handwritten content. ...|$|E
40|$|In general, {{digital images}} can be {{classified}} into photographs, textual and <b>mixed</b> <b>documents.</b> This taxonomy is very useful in many applications, such as archiving task. However, there are no effective methods to perform this classification automatically. In this paper, we present a method for classifying and archiving document into the following semantic classes: photographs, textual and <b>mixed</b> <b>documents.</b> Our method is based on combining low-level image features, such as mean, Standard deviation, Skewness. Both the Decision Tree and Neuronal Network Classifiers are used for classification task...|$|R
40|$|This paper {{presents}} an effective automated analysis system for <b>mixed</b> <b>documents</b> consisting of handwritten texts and graphic images. In the preprocessing step, an input image is binarized, then graphic regions {{are separated from}} text parts using chain codes of connected components. In the character recognition step, we recognize two different sets of handwritten characters: Korean and alphanumeric characters. Considering the structural complexity and variations of Korean characters, we separate them based on partial recognition results of vowels and extract primitive phonemes using a branch and bound algorithm based on dynamic programming (DP) matching. Finally, to validate recognition results, a dictionary and knowledge are employed. Computer simulation with 50 test documents shows that the proposed algorithm analyzes effectively <b>mixed</b> <b>documents.</b> c ○ 1998 Academic Press 1...|$|R
5000|$|Coding of multimedia, hypermedia, and <b>mixed</b> reality <b>document</b> {{interchange}} formats ...|$|R
40|$|Conventional halftoning {{methods such as}} error {{diffusion}} {{and ordered}} dithering are poorly suited to the compression of halftone images using the baseline fax compression schemes CCITT G 3 and G 4. This paper proposes an efficient and flexible solution for binary representation of mixed content documents using CCITT G 3 /G 4 compression. The solution includes two variations which we refer to as FastFax and ReadableFax. FastFax performs edge detection and text detection by applying locally adaptive binary thresholding and combines the two detection results together. The FastFax algorithm produces an accurate representation of binary <b>mixed</b> <b>document</b> content with high compressibility using CCITT G 3 /G 4 compression. ReadableFax is based on FastFax and applies clustered dot screening to background and halftone regions to enhance graphic content. Both methods provide accurate representation of image content while allowing for substantial compressibility, and provides a tradeoff between representation quality and bitrate...|$|E
40|$|Automatic Text location, {{character}} recognition and image {{understanding of a}} given paper document are main objectives of computer vision area. The rst stage for these problems is extracting text information and separating graphic symbol from texts. Pre-vious text location algorithm could not extract the negative text(e. g., newspaper head-line) which is a white colored text on some solid background color plane. Also they could extract only the horizontal or vertical text in a document, so the inclined text or text on a circular arc can not be located by the previous works. In this paper, we propose a new extracting method for these negative texts and real texts from a text/graphics <b>mixed</b> <b>document</b> image. Also we propose a new word grouping method when texts are intersected each other or placed on a circular arc or an inclined line segment with an arbitrary orientation. The basic strategy of our algorithm {{is based on the}} frequency analysis of the run-length encoded le of the image segment. Generally the number of runs in the run-length encoding for a text(character) is smaller than that of graphic symbol. And the average and variance of the number of runs in a run-lengt...|$|E
40|$|This paper {{considers}} how {{to separate}} text and/or graphics from smooth background in screen content and <b>mixed</b> <b>document</b> images and proposes two approaches to perform this segmentation task. The proposed methods {{make use of}} the fact that the background in each block is usually smoothly varying and can be modeled well by a linear combination of a few smoothly varying basis functions, while the foreground text and graphics create sharp discontinuity. The algorithms separate the background and foreground pixels by trying to fit background pixel values in the block into a smooth function using two different schemes. One is based on robust regression, where the inlier pixels will be considered as background, while remaining outlier pixels will be considered foreground. The second approach uses a sparse decomposition framework where the background and foreground layers are modeled with a smooth and sparse components respectively. These algorithms have been tested on images extracted from HEVC standard test sequences for screen content coding, and are shown to have superior performance over previous approaches. The proposed methods can be used in different applications such as text extraction, separate coding of background and foreground for compression of screen content, and medical image segmentation...|$|E
5000|$|Presentation {{and support}} for {{creation}} of multimedia, hypermedia, and <b>mixed</b> reality <b>documents</b> ...|$|R
50|$|These are the {{fundamental}} {{principles of the}} 4 Ps of marketing (the marketing <b>mix)</b> first <b>documented</b> by E. Jerome McCarthy in 1960.|$|R
50|$|Element {{names are}} defined by the developer. This often results in a {{conflict}} when trying to <b>mix</b> XML <b>documents</b> from different XML applications.|$|R
5000|$|PDFsam Basic or PDF Split and Merge is a {{free and}} open source {{cross-platform}} desktop application to split, merge, extract pages, rotate and <b>mix</b> PDF <b>documents.</b>|$|R
40|$|Abstract—This paper {{presents}} a modified JPEG coder that {{is applied to}} the compression of <b>mixed</b> <b>documents</b> (containing text, natural images, and graphics) for printing purposes. The modified JPEG coder proposed in this paper takes advantage of the distinct perceptually significant regions in these documents to achieve higher perceptual quality than the standard JPEG coder. The region-adaptivity is performed via classified thresholding being totally compliant with the baseline standard. A computationally efficient classification algorithm is presented, and the improved performance of the classified JPEG coder is verified. I...|$|R
40|$|Includes bibliographical {{references}} (pages [75]- 80) A generalized computer-based automated {{documentation system}} which processes engineering documents is extremely desirable. Since document archives is memory intensive, data compression algorithms {{are becoming increasingly}} important A revolutionary technique which separates text from <b>mixed</b> text/graphic <b>documents</b> and succinctly describes graphics has been introduced. This thesis introduces two new main algorithms. The first one focuses on the separation of text from <b>mixed</b> text/graphic <b>documents</b> (Chapter 3). It includes an Edge Expanding Search (EES) algorithm for the searching of character-shaped objects, Neighborhood Checking (NC) algorithm for the checking of the neighborhood of the object, and Touching Character Recognition (TCR) algorithm for {{the identification of the}} character touching on a graphic. The second algorithm relates to the description of graphics (Chapter 4). The performance of these algorithms, both in terms of their effectiveness and efficiency, is evaluated with fifteen <b>mixed</b> text/graphic engineering <b>documents.</b> The superior performance of these algorithms as compared to other techniques as described in Chapter 2 is clear from the evaluation results. M. S. (Master of Science...|$|R
3000|$|... a where, for example, {{correctly}} classified text documents do not {{go through}} the last two classification nodes. The overhead, however, is small. The average running time per test image is approximately 0.268 s 1 on an Intel(R) Core(TM) i 7 - 4770 3.40 GHz desktop computer for our proposed soft classification algorithm. The average running time for text documents in our test set for the hard classifier is approximately 0.212 s. The average running time for <b>mix</b> <b>documents</b> for the hard classifier is approximately 0.259 s. For correctly classified photo and picture documents, all classification nodes of the hard classifier must be visited, and therefore the average running time for such documents {{is the same as}} for the soft classifier.|$|R
5000|$|Like The Print Shop, Print Magic lets users {{create a}} variety of {{customized}} <b>documents,</b> <b>mixing</b> graphics and text, such as: ...|$|R
50|$|MO:DCA-P is a {{specification}} of final-form presentation data of an Image. It {{is not a}} programming language, does not contain any file operators, and therefore cannot corrupt a receiver's file system or programming environment. MO:DCA and <b>Mixed</b> Object <b>Document</b> Content Architecture are trademarks of the IBM Corporation.|$|R
25|$|From {{the ratio}} of {{sedentary}} to nomadic graves, Bichir concludes that the sedentary folk constituted {{the majority of the}} population of Moldavia. In the <b>mixed</b> cemeteries <b>documented</b> by Bichir, nomadic graves constitute about 28% of the total. However, in Moldavia as a whole, nomadic graves represent no more than 1% of all graves.|$|R
40|$|We {{investigate}} the following problem: Given {{a set of}} documents of a particular topic or class #, and a large set # of <b>mixed</b> <b>documents</b> that contains documents from class # {{and other types of}} documents, identify the documents from class # in #. The key feature of this problem {{is that there is no}} labeled non- # document, which makes traditional machine learning techniques inapplicable, as they all need labeled documents of both classes. We call this problem partially supervised classification. In this paper, we show that this problem can be posed as a constrained optimization problem and that under appropriate conditions, solutions to the constrained optimization problem will give good solutions to the partially supervised classification problem. We present a novel technique to solve the problem and demonstrate the effectiveness of the technique through extensive experimentation...|$|R
50|$|The Document Content Architecture, or DCA for short, is a {{standard}} developed by IBM for text documents in the early 1980s. DCA was used on mainframe and iSeries systems, and {{formed the basis of}} DisplayWrite's file format. DCA was later extended as MO:DCA (<b>Mixed</b> Object <b>Document</b> Content Architecture), which added embedded data files, like graphics.|$|R
40|$|The Sweave {{system of}} Leisch (2002) is a literate {{programming}} tool based on ideas of Knuth (1984) {{and is currently}} part of the core R installation. Specifically, Sweave is a system for processing <b>documents</b> that <b>mix</b> LATEX <b>document</b> formatting with R code. R code can be interspersed within the LATEX markup by indicating “code chunks”. These code chunks are evaluated by the Sweav...|$|R
40|$|Our opinion {{retrieval}} system has four steps. In the first step, documents which are deemed relevant {{by the system}} {{with respect to the}} query are retrieved, without taking into consideration whether the documents are opinionative or not. In the second step, the abbreviations of query concepts in documents are recognized. This helps in identifying whether an opinion is in the vicinity of a query concept (which can be an abbreviation) in a document. The third step of opinion identification is designed for recognizing query-relevant opinions within the documents. In the forth step, for each query, all retrieved opinionated documents are ranked by various methods which take into account IR scores, opinion scores and the number of concepts in query. For the polarity subtask, the opinionative documents are classified into positive, negative and mixed types by two classifiers. Since TREC 2008 does not require <b>mixed</b> <b>documents,</b> all documents which are deemed mixed by our system are discarded. 1...|$|R
40|$|In TREC 2007 Blog Track, we {{developed}} a three-step algorithm for the opinion retrieval task. An information retrieval step retrieves the query-relevant documents. A following opinion identification step identifies the opinionative texts in these documents. A ranking step identifies the query-related opinions in the documents and ranks them by calculating their opinion similarity scores. For the polarity task, our strategy {{is to find the}} positive and negative documents respectively, and then find the <b>mixed</b> opinionative <b>documents</b> in the intersection of the positive and negative document sets. We implemented our opinion retrieval algorithm in two special cases, one to retrieve the positive documents, and the other to retrieve the negative documents. A judging function labeled a subset of the documents, which were in the intersection of the positive and negative <b>documents,</b> as the <b>mixed</b> opinionative <b>documents.</b> We studied two parameters in our opinion retrieval algorithm, each of which had two values to compare. This resulted in four submitted opinion retrieval runs and their corresponding polarity runs. 1...|$|R
40|$|International audienceThis paper {{deals with}} {{signature}} based document retrieval from documents with cluttered background. Here, a signature object {{is characterized by}} spatial features computed from recognition result of background blobs. The background blobs are computed by analyzing character holes and water reservoir zones in different directions. For the indexation purpose, a codebook of the background blobs is created using a set of training data. Zernike Moment feature is extracted from each blob and a K-Mean clustering algorithm is used to create the codebook of blobs. During retrieval, Generalized Hough Transform (GHT) is used to detect the query signature and a voting is casted to find possible location of the query signature in a document. The spatial features computed from background blobs found in the target document are used for GHT. The peak of votes in GHT accumulator validates the hypothesis of the query signature. The proposed method is tested on a collection of <b>mixed</b> <b>documents</b> (handwritten/printed) of various scripts and we have obtained encouraging results from the experiments...|$|R
25|$|Pre-World War II {{official}} <b>documents</b> <b>mix</b> katakana and kanji in {{the same}} way that hiragana and kanji are mixed in modern Japanese texts, that is, katakana were used for okurigana and particles such as wa or o.|$|R
40|$|XML {{has emerged}} as the {{standard}} format for representing and exchanging data on the World Wide Web. For practical purposes, it is found to be critical to have efficient mechanisms to store and query XML data, as well as to exploit the full power of this new technology. Several researchers have proposed to use relational databases to store and query XML data. With the understanding the limitations of current approaches, this thesis aims to propose an algorithm for automatic mapping XML documents to RDBMS with XML-API as a database utility. The algorithm uses best fit auto mapping technique, and dynamic shredding, of a specified selected XML document type (datacentric, document-centric, and <b>mixed</b> <b>documents).</b> e. The propose algorithm use DOM(Data Object Model) as a warehouse and stack as a data structure to mapping the XML document into relational database and reconstructing the XML document from the relational database. The experiment study show that the algorithm mapping document and reconstructing it again well. Finally, the algorithm compare with other algorithms the result is good in time and efficiency, also the algorithm complexity is O(11 n+ 2) ...|$|R
40|$|In multi lingual {{environment}} where {{in a single}} image document {{have more than one}} script occur there is need of script identification system. Automatic identification of scripts in document facilitates (i) Automatic archiving of multilingual documents, (ii) Searching online archives of document images, (iii) Selection of script specific OCR in a multilingual environment. The main objective of this system is to identify the specific script and feed them into their specified Optical Character Recognition (OCR) system. OCR is the system which converts the image document into editable text document. Script identification of written text in the domain of Indian script based languages is a well-studied research field. In this paper a technique of script Identification is described to discriminate three major south Indian scripts: Oriya, Telugu and Kannada. These three scripts are member of Brahmi script and most of the character shapes are near similar. This method is applied over segmented line from the image document and it is completely free from size and font. The proposed technique uses the basic distinguishable features based on texture analysis. The approach is based on the analysis of horizontal projection and vertical projection profile. We obtain overall 98. 64 % accuracy from test dataset of three ancient <b>mix</b> <b>document</b> images at line level...|$|R
50|$|In the gym, {{trainers}} Nicky and Jennifer confront Taylor and Valerie {{about the}} importance of writing down what they eat in their food journals. Nutritionist Ashley Koff R.D. meets with the two and shows them some new and fun ways to <b>mix</b> calories while <b>documenting</b> them.|$|R
40|$|Recent {{literature}} <b>documents</b> <b>mixed</b> {{findings on}} the impact of information technology (IT) investment on firm innovation outputs. We explain why firms with greater IT investment are more likely to search across boundaries in recombining knowledge, which has a curvilinear relationship with innovation outputs, and find empirical evidence corroborating our theory. Link_to_subscribed_fulltex...|$|R
40|$|A <b>mixed</b> content <b>document</b> {{typically}} {{contains a}} mixture of text, graphics, halftone regions, and pictures. Since the characteristics and spatial behaviors in these regions are dramatically distinct, efficient and accurate representation of a <b>mixed</b> content <b>document</b> becomes very challenging in many document imaging applications. This thesis includes three chapters focusing on three different application areas. ^ In Chapter 1, our objective is to develop an effective and high quality color document coding method based on the mixed raster content (MRC) model. While most MRC based methods can yield very high compression ratios, the binary representation of MRC tends to distort fine document details. To address this problem, we propose a method called resolution enhanced rendering (RER), which works by adaptively dithering the encoded binary mask, and then applying a nonlinear predictor to decode a gray level mask at the same or higher resolution. This method substantially improves the decoded document quality of text and graphics at a fixed bit rate. ^ In Chapter 2, we propose an efficient and flexible solution for binary representation of <b>mixed</b> content <b>documents</b> using CCITT G 3 /G 4 compression. The solution includes two variations which we refer to as FastFax and ReadableFax. Both methods provide accurate binary representation of image content with high compressibility. Based on FastFax, we propose a layer-based compression scheme, MixedPDF, for color document representation in the PDF format. This method includes an efficient segmentation algorithm to separate the document into a text layer and a background layer, which are then separately coded by different compressions. The MixedPDF algorithm substantially outperforms conventional JPEG compression with low computational and memory requirements. ^ In Chapter 3, we propose an efficient error diffusion algorithm optimized for PackBits compression. This method, which we refer to as POED (PackBits optimized error diffusion), {{is a form of}} threshold modulation error diffusion which takes advantage of the byte-oriented run length structure of PackBits compression by encouraging repetition of bytes in the resulting binary image. The POED method with PackBits compression yields higher compression ratios than the conventional error diffusion method, while maintaining desirable visual quality with low computational and memory requirements. ...|$|R
40|$|Parallel text {{alignment}} {{is a key}} {{procedure in}} the automated translation area. A large number of aligners have been presented along the years, but these require that the target resources have been pre-prepared for alignment (either manually or automatically). It is rather normal to encounter <b>mixed</b> language <b>documents,</b> that is, documents where the same information is written in many languages (Ex: manuals of electronic devices, touristic information, PhD thesis with dual language abstracts, etc). In this article we present MLT-prealigner: a tool aimed at helping those that need to process mixed texts in order to feed alignment tools and other related language systems...|$|R
40|$|The goal of {{this work}} is to add the {{capability}} to segment documents containing text, graphics, and pictures in the open source OCR engine OCRopus. To achieve this goal, OCRopus 2 ̆ 7 RAST algorithm was improved to recognize non-text regions so that <b>mixed</b> content <b>documents</b> could be analyzed in addition to text-only documents. Also, a method for classifying text and non-text regions was developed and implemented for the Voronoi algorithm enabling users to perform OCR on documents processed by this method. Finally, both algorithms were modified to perform at a range of resolutions. Our testing showed an improvement of 15 - 40...|$|R
5000|$|In 2007 Colonial Williamsburg {{launched}} www.iCitizenForum.com. A <b>mix</b> {{of historical}} <b>documents</b> and user-generated content such as blogs, videos, and message boards, the site aims to prompt {{discussion about the}} roles, rights, and responsibilities of citizens in a democracy. Preservation of the Founding Fathers' ideals {{in light of recent}} world events is a special focus of the site.|$|R
50|$|The Library of Congress {{is trying}} to extend its brick and mortar library {{services}} to include services to the entire web. While the original Library {{was focused on the}} needs of the US Congress, dealing with the whole world through the Internet is something it is still struggling with. The collection includes an eclectic <b>mix</b> of <b>documents,</b> images, videos and sound recordings. Images include maps, sheet music, handwritten documents, drawings and architectural diagrams. The goal of a Library of Congress Internet Library should be to provide access to those materials unique to the Library of Congress, and a clear guide to any internet materials related to the United States.|$|R
50|$|A tool {{called a}} punch down tool {{is used to}} push the wire down firmly and {{properly}} into the slot. Some will automatically cut any excess wire off. The exact {{size and shape of}} the tool blade varies by manufacturer, which can cause problems for those working on existing installations, especially when there is a poorly <b>documented</b> <b>mix</b> of different brands.|$|R
