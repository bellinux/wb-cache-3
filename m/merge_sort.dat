318|193|Public
5|$|Von Neumann was a {{founding}} figure in computing. Donald Knuth cites von Neumann as the inventor, in 1945, of the <b>merge</b> <b>sort</b> algorithm, {{in which the}} first and second halves of an array are each sorted recursively and then merged.|$|E
25|$|A variant named binary <b>merge</b> <b>sort</b> uses {{a binary}} {{insertion}} sort to sort groups of 32 elements, {{followed by a}} final sort using <b>merge</b> <b>sort.</b> It combines the speed of insertion sort on small data sets {{with the speed of}} <b>merge</b> <b>sort</b> on large data sets.|$|E
25|$|<b>Merge</b> <b>sort</b> on arrays has {{considerably}} better data cache performance, often outperforming heapsort {{on modern}} desktop computers because <b>merge</b> <b>sort</b> frequently accesses contiguous memory locations (good locality of reference); heapsort references are {{spread throughout the}} heap.|$|E
50|$|Some {{computer}} languages provide built-in or library {{support for}} <b>merging</b> <b>sorted</b> collections.|$|R
5000|$|For large n and a {{randomly}} ordered input list, <b>merge</b> <b>sort's</b> expected (average) {{number of}} comparisons approaches α·n {{fewer than the}} worst case where ...|$|R
5000|$|The {{search engine}} should {{be capable of}} {{performing}} Federated searches. Federated searching enables concept queries {{to be used for}} simultaneously searching multiple datasources for information, which are then <b>merged,</b> <b>sorted,</b> and displayed in the results.|$|R
25|$|Heapsort is not {{a stable}} sort; <b>merge</b> <b>sort</b> is stable.|$|E
25|$|<b>Merge</b> <b>sort</b> {{has seen}} a {{relatively}} recent surge in popularity for practical implementations, due to its use in the sophisticated algorithm Timsort, which {{is used for the}} standard sort routine in the programming languages Python and Java (as of JDK7). <b>Merge</b> <b>sort</b> itself is the standard routine in Perl, among others, and has been used in Java at least since 2000 in JDK1.3.|$|E
25|$|Fibonacci {{numbers are}} used in a {{polyphase}} version of the <b>merge</b> <b>sort</b> algorithm in which an unsorted list {{is divided into two}} lists whose lengths correspond to sequential Fibonacci numbers– by dividing the list so that the two parts have lengths in the approximate proportion φ. A tape-drive implementation of the polyphase <b>merge</b> <b>sort</b> was described in The Art of Computer Programming.|$|E
50|$|<b>Merge</b> <b>sort's</b> {{most common}} {{implementation}} does not sort in place; therefore, the memory {{size of the}} input must be allocated for the sorted output to be stored in (see below for versions that need only n/2 extra spaces).|$|R
50|$|Also, many {{applications}} of external sorting use {{a form of}} <b>merge</b> <b>sorting</b> where the input get split up to a higher number of sublists, ideally to a number for which merging them still makes the currently processed set of pages fit into main memory.|$|R
40|$|The use of {{parallelism}} in I/O {{systems is}} becoming increasingly important as the performance gap between processors and disks continues to widen. This thesis studies the performance of multiple external <b>merge</b> <b>sorts</b> in an I/O system with multiple disks. Specifically, we investigate the impact of data placement on I/O performance. For one intuitively good placement policy, a race develops among concurrent <b>merge</b> <b>sorts,</b> resulting in the serialization of job executions and significant performance degradation. We present {{a model of a}} system with two jobs performing concurrent I/O and analyze the model. Our analysis accurately predicts the development of the race condition. We also present methods to control the race based on data placement and disk scheduling policies, which are shown to be effective through simulations...|$|R
25|$|<b>Merge</b> <b>sort</b> {{is used in}} {{external}} sorting; heapsort is not. Locality {{of reference}} is the issue.|$|E
25|$|Practical general sorting {{algorithms}} {{are almost}} always based on an algorithm with average time complexity (and generally worst-case complexity) O(n log n), of which the most common are heap sort, <b>merge</b> <b>sort,</b> and quicksort. Each has advantages and drawbacks, with the most significant being that simple implementation of <b>merge</b> <b>sort</b> uses O(n) additional space, and simple implementation of quicksort has O(n2) worst-case complexity. These problems can be solved or ameliorated {{at the cost of}} a more complex algorithm.|$|E
25|$|<b>Merge</b> <b>sort</b> parallelizes {{well and}} can achieve close to linear speedup with a trivial implementation; {{heapsort}} {{is not an}} obvious candidate for a parallel algorithm.|$|E
40|$|This {{qualitative}} research collected themes and factors {{of importance to}} Generation Y nurses related to nursing leadership roles. The findings from three focus groups were <b>merged,</b> <b>sorted</b> and grouped into thematic categories. Factors were weighted based on rankings (by group). Results were compared to original study (2003) with Generation X nurses...|$|R
40|$|Motivation: Analysis of genomic {{sequencing}} data requires efficient, easy-to-use {{access to}} alignment results and flexible data management tools (e. g. filtering, <b>merging,</b> <b>sorting,</b> etc.). However, the {{enormous amount of}} data produced by current sequencing technologies is typically stored in compressed, binary formats that are not easily handled by the text-based parsers commonly used in bioinformatics research...|$|R
40|$|We {{present a}} {{framework}} for translating procedural production rules, such as parametric L-Systems based on scene graphs instead of turtle graphics, into a form that can iteratively grow geometry completely on graphics hardware. The formulation is extended to handle context-sensitive, conditional and non-deterministic rules. We also examine and enhance existing GPU-based techniques for bitonic <b>merge</b> <b>sorting.</b> The result is procedural geometry synthesized directly on the graphics card...|$|R
25|$|An early two-subproblem D {{algorithm}} {{that was}} specifically developed for computers and properly analyzed is the <b>merge</b> <b>sort</b> algorithm, invented by John von Neumann in 1945.|$|E
25|$|The sort {{operation}} transforms an {{unordered set}} of elements into an ordered set of elements. The most common implementation on GPUs is using radix sort for integer and floating point data and coarse-grained <b>merge</b> <b>sort</b> and fine-grained sorting networks for general comparable data.|$|E
25|$|The {{number of}} swaps {{can be reduced}} by calculating the {{position}} of multiple elements before moving them. For example, if the target position of two elements is calculated before they are moved into the right position, the number of swaps can be reduced by about 25% for random data. In the extreme case, this variant works similar to <b>merge</b> <b>sort.</b>|$|E
5000|$|Clyde P. Kruskal, [...] "Searching, <b>Merging,</b> and <b>Sorting</b> in Parallel Computation", IEEE Trans. Comput. 32 942-946 (1983) ...|$|R
5000|$|If , then extract {{and sort}} the [...] {{smallest}} elements from , and <b>merge</b> this <b>sorted</b> list with [...]|$|R
40|$|Abstract To explore chip-level parallelism, the PSC (Parallel Shared Cache) {{model is}} {{provided}} in this paper to describe high performance shared cache of Chip Multi-Processors (CMP). Then for a specific application, parallel sorting, a cache-conscious parallel algorithm, PMCC (Partition-Merge based Cache-Conscious) is designed based on the PSC model. The PMCC algorithm consists of two steps: the partition-based in-cache sorting and merge-based k-way <b>merge</b> <b>sorting.</b> In the first stage, PMCC first divides the input dataset into multiple blocks so that each block can fit into the shared L 2 cache, and then employs multiple cores to perform parallel cache sorting to generate sorted blocks. In the second stage, PMCC first selects an optimized parameter k which can not only improve the parallelism but also reduce the cache missing rate, then performs a k-way <b>merge</b> <b>sorting</b> to <b>merge</b> all the <b>sorted</b> blocks. The I/O complexity of the in-cache sorting step and k-way merge step are analyzed in detail. The simulation {{results show that the}} PSC based PMCC algorithm can out-performance the latest PEM based cache-conscious algorithm and the scalability of PMCC is also discussed. The low I/O complexity, high parallelism and the high scalability of PMCC can take advantage of CMP to improve its performance significantly and deal with large scale problem efficiently...|$|R
25|$|Quicksort (sometimes called partition-exchange sort) is an {{efficient}} sorting algorithm, {{serving as a}} systematic method for placing the elements of an array in order. Developed by Tony Hoare in 1959 and published in 1961, {{it is still a}} commonly used algorithm for sorting. When implemented well, it can be about two or three times faster than its main competitors, <b>merge</b> <b>sort</b> and heapsort.|$|E
25|$|Bubble sort has {{worst-case}} {{and average}} complexity both О(n2), where n {{is the number}} of items being sorted. There exist many sorting algorithms, such as <b>merge</b> <b>sort</b> with substantially better worst-case or average complexity of O(nlogn). Even other О(n2) sorting algorithms, such as insertion sort, tend to have better performance than bubble sort. Therefore, bubble sort is not a practical sorting algorithm when n is large.|$|E
25|$|Quicksort {{has some}} {{disadvantages}} {{when compared to}} alternative sorting algorithms, like <b>merge</b> <b>sort,</b> which complicate its efficient parallelization. The depth of quicksort's divide-and-conquer tree directly impacts the algorithm's scalability, and this depth is highly dependent on the algorithm's choice of pivot. Additionally, {{it is difficult to}} parallelize the partitioning step efficiently in-place. The use of scratch space simplifies the partitioning step, but increases the algorithm's memory footprint and constant overheads.|$|E
40|$|AbstractWe {{show that}} in the {{deterministic}} comparison model for parallel computation, p = n processors can select the kth smallest item from a set of n numbers in O(log log n) parallel time. With this result all comparison tasks (selection, <b>merging,</b> <b>sorting)</b> now have {{upper and lower bounds}} of the same order in both random and deterministic models. This optimal time bound holds even if p = o(n) ...|$|R
40|$|<b>Merging</b> two <b>sorted</b> arrays is a {{prominent}} building block for sorting and other functions. Its efficient parallelization requires balancing the load among compute cores, minimizing the extra work {{brought about by}} parallelization, and minimizing inter-thread synchronization requirements. Efficient use of memory is also important. We present a novel, visually intuitive approach to partitioning two input sorted arrays into pairs of contiguous sequences of elements, one from each array, such that 1) each pair comprises any desired total number of elements, and 2) the elements of each pair form a contiguous sequence in the output <b>merged</b> <b>sorted</b> array. While the resulting partition and the computational complexity {{are similar to those}} of certain previous algorithms, our approach is different, extremely intuitive, and offers interesting insights. Based on this, we present a synchronization-free, cache-efficient <b>merging</b> (and <b>sorting)</b> algorithm. While we use a shared memory architecture as the basis, our algorithm is easily adaptable to additional architectures. In fact, our approach is even relevant to cache-efficient sequential sorting. The algorithms are presented, along with important cache-related insights. Keywords: Parallel algorithms, Parallel processing, Merging, Sortin...|$|R
40|$|Abstract—We generalize the {{well-known}} odd-even <b>merge</b> <b>sorting</b> algorithm, originally due to Batcher [2], and show how this generalized algorithm {{can be applied}} to sorting on product networks. If G is an arbitrary factor graph with N nodes, its r-dimensional product contains N r nodes. Our algorithm sorts N r keys stored in 2 the r-dimensional product of G in O (rFN ()) time, where F(N) depends on G. We show that, for any factor graph G, F(N) is, a...|$|R
25|$|While these {{algorithms}} are asymptotically efficient on random data, {{for practical}} efficiency on real-world data various modifications are used. First, the overhead of these algorithms becomes significant on smaller data, so often a hybrid algorithm is used, commonly switching to insertion sort once {{the data is}} small enough. Second, the algorithms often perform poorly on already sorted data or almost sorted data – these are common in real-world data, and can be sorted in O(n) time by appropriate algorithms. Finally, {{they may also be}} unstable, and stability is often a desirable property in a sort. Thus more sophisticated algorithms are often employed, such as Timsort (based on <b>merge</b> <b>sort)</b> or introsort (based on quicksort, falling back to heap sort).|$|E
25|$|<b>Merge</b> <b>sort</b> takes {{advantage}} of the ease of merging already sorted lists into a new sorted list. It starts by comparing every two elements (i.e., 1 with 2, then 3 with 4...) and swapping them if the first should come after the second. It then merges each of the resulting lists of two into lists of four, then merges those lists of four, and so on; until at last two lists are merged into the final sorted list. Of the algorithms described here, this is the first that scales well to very large lists, because its worst-case running time is O(n log n). It is also easily applied to lists, not only arrays, as it only requires sequential access, not random access. However, it has additional O(n) space complexity, and involves a large number of copies in simple implementations.|$|E
500|$|For example, to find {{a number}} in a sorted list, the binary search {{algorithm}} checks the middle entry and proceeds with the half {{before or after the}} middle entry if the number is still not found. This algorithm requires, on average, [...] comparisons, where N is the list's length. Similarly, the <b>merge</b> <b>sort</b> algorithm sorts an unsorted list by dividing the list into halves and sorting these first before merging the results. <b>Merge</b> <b>sort</b> algorithms typically require a time approximately proportional to [...] The base of the logarithm is not specified here, because the result only changes by a constant factor when another base is used. A constant factor is usually disregarded in the analysis of algorithms under the standard uniform cost model.|$|E
5000|$|The C++'s Standard Template Library has the {{function}} , which <b>merges</b> two <b>sorted</b> ranges of iterators, and , which <b>merges</b> two consecutive <b>sorted</b> ranges in-place. In addition, the [...] (linked list) class {{has its own}} [...] method which merges another list into itself. The type of the elements merged must support the less-than (...) operator, or it must be provided with a custom comparator.|$|R
40|$|We {{implement}} a promising algorithm for sparse-matrix sparse-vector multiplication (SpMSpV) on the GPU. An efficient k-way merge {{lies at the}} heart of finding a fast parallel SpMSpV algorithm. We examine the scalability of three approaches—no <b>sorting,</b> <b>merge</b> <b>sorting,</b> and radix sorting—in solving this problem. For breadth-first search(BFS), we achieve a 1. 26 x speedup over state-of-the-art sparse-matrix dense-vector (SpMV) implementations. The algorithm seems generalizeable for single-source shortest path (SSSP) and sparse-matrix sparse-matrix multiplication, and other core graph primitives such as maximal independent set and bipartite matching...|$|R
40|$|We {{create a}} {{scalable}} parallel algorithm based on parallelizing the <b>merge</b> <b>sorting</b> network algorithm. We implemented our scalable parallel algorithm using two modern shared-memory parallel programming languages, Go [1] and Cilk- 5 [2]. We also compare Go and Cilk speedup and performance. Our experimental {{results show that}} our parallel algorithm is scalable and provides near linear speedup for all large problem sizes. We compare Go and Cilk scheduling and synchronization efficiency {{as well as the}} overall performance of our parallel code. Categories and Subject Descriptor...|$|R
