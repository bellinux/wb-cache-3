172|69|Public
50|$|The {{research}} focus {{lies on the}} semantics and pragmatics of discourse. The group develops software {{facilitating the}} <b>multimodal</b> <b>dialogue</b> between users and machines. The aim {{is to use the}} computer for understanding and generating language and texts and to make use of computers more naturally in the long term.|$|E
50|$|The Snack Sound Toolkit {{is used in}} {{a number}} of {{linguistic}} research tools, including WaveSurfer, Transcriber, and SndBite, as well as in other applications, such as the snackAmp MP3 music player. It was used in academic research at the Delft University of Technology. The Springer collection Perception in <b>Multimodal</b> <b>Dialogue</b> Systems includes a research essay utilizing Snack for human-computer speech interaction. The toolkit is included as a component of ActiveState's Tcl and Python distributions.|$|E
40|$|The goal of {{this paper}} is to define a {{methodology}} for the end-to-end evaluation of the <b>multimodal</b> <b>dialogue</b> system SmartKom along the lines of the DARPA guidelines for spoken dialogue systems. The methodology consists of an extended framework for the evaluation of a <b>multimodal</b> <b>dialogue</b> system, evaluation metrics for its various components, and an approach to compare the user satisfaction with the system's technical performance...|$|E
40|$|This paper {{identifies}} {{several issues}} in <b>multimodal</b> <b>dialogues</b> between a companion robot and a human user. Specifically, these issues {{pertain to the}} synchronization of multimodal input and output, and the handling of expected and unexpected input, including input contradicting over different modalities. Furthermore, a novel way of visually representing <b>multimodal</b> <b>dialogues</b> is presented. Ultimately, this work represents some steps towards {{the development of a}} principled and generic method for programming <b>multimodal</b> <b>dialogues.</b> 1...|$|R
40|$|This paper {{discusses}} {{the problem of}} endophoric/deictic references in <b>multimodal</b> <b>dialogues</b> involving speech and pointing gestures. The dialogue system in focus is a building information system implemented using a general workbench architecture "Chameleon". Chameleon and the building information system have been developed within the IntelliMedia 2000 + project initiated 1996 by the Institute of Electronic Systems, Aalborg University [3]. Further development and experiments were {{carried out in the}} EU Esprit-project 24 493 "Language and Image Data Fusion using Stochastic Models and Spatial Context Modeling" together with LIMSI and Bertin (coordinator), France [2]. We focus on two problems (1) cross-media references and (2) crossuser /system references. 1. INTRODUCTION Reference problems in natural language understanding have become a subject for an increasing number of studies by researchers working in the field of <b>multimodal</b> <b>dialogues.</b> An example is the workshop "Referring Phenomena in a [...] ...|$|R
40|$|The paper {{discusses}} current design {{issues for}} {{graphical user interfaces}} and presents trends for improving the interaction between user and system. Object-oriented principles for designing dialogues, structuring of dialogues and user group specific user interface design are important issues {{for the design of}} GUls. Finally, the paper gives an overview of future developments concerning <b>multimodal</b> <b>dialogues</b> and dynamic visualisations...|$|R
40|$|Industrial robot, ontology, web services, <b>multimodal</b> <b>dialogue.</b> Achieving rapid, intuitive, and {{error-free}} robot {{setup and}} instruction is a challenge. We present our work towards an assistive infrastructure for robot setup and instruction {{that attempts to}} address it. In this paper, we describe the ongoing development {{of a system that}} automatically generates <b>multimodal</b> <b>dialogue</b> interaction from product and process ontologies. The prototype currently generates two modalities, digital paper and spoken dialogue. ...|$|E
40|$|This paper {{presents}} the NICE fairy-tale game system, in which {{adults and children}} can interact with various animated characters in a 3 D world. Computer games is an interesting application for spoken and <b>multimodal</b> <b>dialogue</b> systems. Moreover, {{for the development of}} future computer games, <b>multimodal</b> <b>dialogue</b> has the potential to greatly enrichen the user’s experience. In this paper, we also present some requirements that have to be fulfilled to successfully integrate spoken dialogue technology with a computer game application. ...|$|E
40|$|This study {{examines}} coordination of referring expressions in multimodal human-computer dialogue, i. e. {{to what extent}} users' choices of referring expressions {{are affected by the}} referring expressions that the system is designed to use. An experiment was conducted, using a semi-automatic <b>multimodal</b> <b>dialogue</b> system for apartment seeking. The user and the system could refer to areas and apartments on an interactive map by means of speech and pointing gestures. Results indicate that the referring expressions of the system have great influence on the user's choice of referring expressions, both in terms of modality and linguistic content. From this follows a number of implications for the design of <b>multimodal</b> <b>dialogue</b> systems...|$|E
40|$|We {{demonstrate}} work {{in progress}} using the Nite XML Toolkit on a corpus of <b>multimodal</b> <b>dialogues</b> with an MP 3 player collected in a Wizard-of-Oz (WOZ) experiments and annotated with a rich feature set at several layers. We designed an NXT data model, converted experiment log file data and manual transcriptions into NXT, and are building annotation tools using NXT libraries...|$|R
40|$|Two {{important}} {{themes in}} current work on interfaces are multimodal interaction {{and the use}} of <b>dialogue.</b> Human <b>multimodal</b> <b>dialogues</b> are symmetric, i. e., both participants communicate multimodally. We describe a proof of concept system that supports symmetric multimodal communication for speech and sketching in the domain of simple mechanical device design. We discuss three major aspects of the communication: multimodal input processing, multimodal output generation, and creating a dynamic dialogue. While previous systems have had some of these capabilities individually, their combination appears to be unique. We provide examples from our system that illustrate a variety of user inputs and system outputs. Author Keywords <b>multimodal,</b> dynamic <b>dialogue,</b> sketch recognition, sketch generation, speech ACM Classification Keyword...|$|R
40|$|This paper {{discusses}} {{the problem of}} endophoric/deictic references in <b>multimodal</b> <b>dialogues</b> involving speech and pointing gestures. The dialogue system in focus is a building information system implemented using a general workbench architecture “Chameleon”. Chameleon and the building information system have been developed within the IntelliMedia 2000 + project initiated 1996 by the Institute of Electronic Systems, Aalborg University [3]. Further development and experiments were {{carried out in the}} EU Esprit-project 2...|$|R
40|$|<b>Multimodal</b> <b>dialogue</b> systems allow {{users to}} input {{information}} in multiple modalities. These systems can handle simultaneous or sequential composite multimodal input. Different coordination schemes require such systems to capture, collect and integrate user input in different modalities, and then {{respond to a}} joint interpretation. We performed a study to understand the variability of input in <b>multimodal</b> <b>dialogue</b> systems and to evaluate methods to perform the collection of input information. An enhancement {{in the form of}} incorporation of a dynamic time window to a multimodal input fusion module was proposed in the study. We found that the enhanced module provides superior temporal characteristics and robustness when compared to previous methods. ...|$|E
40|$|<b>Multimodal</b> <b>dialogue</b> {{systems are}} {{interactive}} systems which support several modes {{of communication with}} the user, for example speech, mouse clicks and drawings. We present a method for writing multimodal grammars in the Grammatical Framework (GF) and for using such grammars in dialogue systems. Parallel multimodality, where the same information is represented in multiple modalities, is achieved by using multiple concrete syntaxes for the same abstract syntax. Integrated multimodality, where the different modalities are used together to represent some information, is done by using record types. We will also present an example <b>multimodal</b> <b>dialogue</b> system implemented using these techniques. This work {{is part of the}} TALK projec...|$|E
40|$|We {{describe}} work towards {{developing a}} scalable and portable framework for enabling map-based <b>multimodal</b> <b>dialogue</b> interaction over the web. Working {{in the context}} of a restaurant-guide system, we show how large information databases harvested from the web can be accommodated in our speech recognizer, parser, and web-based GUI. We compare two dynamic language modeling techniques, which calculate context-dependent weights for the large sets of proper nouns associated with geographical entities such as restaurants and streets. We show that the more fine-grained approach results in a 7. 8 % reduction in concept error rate. Index Terms: <b>multimodal</b> <b>dialogue</b> system, language modeling, restaurants, maps, world wide we...|$|E
50|$|At La Colección Jumex, Martín {{promoted}} multilevel and <b>multimodal</b> <b>dialogues</b> around {{contemporary art}} through sponsorship agreements with artists, curators, investigators and public institutions. In {{the seven years}} that Martín {{was in charge of}} the evolution and growth of the art collection, more than 1,200 works of art became part of it. Since the end of the 1990s, she acquired for the collection important works from contemporary artists such as Thomas Demand, Olafur Eliasson, Fischli & Weiss, Ceal Floyer, Dan Graham, Bas Jan Ader, Donald Judd, On Kawara, Rivane Neuenschwander, Robert Smithson, among others.|$|R
40|$|In {{this paper}} a brief {{outline of the}} {{elements}} of a theory of multimodal representation and interpretation is presented. The theory is illustrated {{with the help of an}} interactive system called GRAFLOG which is able to support a multimodal graphical and linguistic dialogue. In this dialogue graphical symbols with their interpretation are introduced incrementally, and the system is able to assist the user in the solution of simple design problems. The theory has been used as a framework for the development of a number of master thesis and experimental systems addressing different aspects of representation and problem-solving in the context of <b>multimodal</b> <b>dialogues...</b>|$|R
40|$|A normal {{conversation}} {{between two people}} is typically multimodal, using both speech and gestures to effect communication. It is also symmetric because there is two-way multimodal interaction between the two parties. In contrast, when a human interacts with a computer, it is done through a strict and limited interface, usually a keyboard or mouse. Unlike the human-human conversation, this interaction is neither multimodal nor symmetric. The goal of this thesis is to empower computers to carry out symmetric, <b>multimodal</b> <b>dialogues</b> with humans, thereby providing a more natural human-computer interaction. To do so, we modified and extended Adler's <b>Multimodal</b> Interactive <b>DialOgue</b> System (MIDOS) {{to be a more}} flexible and domain-independent platform for supporting symmetric, multimodal interaction. We built an application that utilizes MIDOS in order to design and implement a normalized relational database, and then demonstrate the application's capabilities by using it to design the database for an after-action report wiki. by James Roy Oleinik. Thesis (M. Eng.) [...] Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2010. Cataloged from PDF version of thesis. Includes bibliographical references (p. 104) ...|$|R
40|$|We explore several {{language}} modeling {{strategies for}} increasing the recognition accuracy among large sets of proper nouns in a mapbased <b>multimodal</b> <b>dialogue</b> system which provides restaurant information. In particular, we evaluate several mechanisms for exploiting dialogue context, the two most promising of which involve a semistatic metropolitan-region based large set of proper nouns competing with a smaller, in-focus subset. We show that these techniques decrease word, concept, and proper noun error rates under several training conditions. We also present a technique to generalize sparse training data through derived templates to improve language model robustness. Index Terms — <b>multimodal</b> <b>dialogue</b> system, language modeling, context-sensitive, restaurants, proper nouns 1...|$|E
40|$|This {{deliverable}} {{provides a}} collection of chapters describing various challenges and ongoing work on usability and evaluation within the areas of spoken <b>multimodal</b> <b>dialogue</b> systems, visionbased systems, haptics-based systems, mixed-reality systems in surgery, and tools for remote usability evaluation...|$|E
40|$|We try {{to develop}} a {{communication}} basis between interface designers, natural language processing experts and scenario experts for <b>multimodal</b> <b>dialogue</b> systems. The question we address is, how to represent different kinds of results (infor-mation design) and interaction metaphors (interaction and presentation design) which are created and presented to the user during the dialogue. We present a pattern language for interaction design and show its strength to achieve a common data model. The common data model helps to handle the great complexity in the user-system interaction of our <b>multimodal</b> <b>dialogue</b> system SmartWeb, which pro-vides Question Answering (QA) functionality {{and access to the}} Semantic Web. We motivate the construction of Inter-action Ontologies as a natural extension for Semantic Web-based applications...|$|E
40|$|This paper {{identifies}} key issues, {{solutions and}} actions for multimodal interaction, communication and navigation at the user interface with ICT systems and terminals. It specifically addresses the usage context of transactional interactions for independent living, {{and focus on}} the dynamics of <b>multimodal</b> transactions/user <b>dialogues</b> for the consumers of ICT systems and terminals. It identifies how simplifications, translations, sensory transpositions, or other presentation or content manipulations of a multimodal transaction can be used to improve ease of access to telecommunications products and services for people with sensory, motor or cognitive impairments. Consultation with users and user groups were carried out to identify the areas of transactional interactions that are currently providing a barrier to ICT access. To overcome these barriers a structured set of design and implementation principles is presented. The principles given here have been produced from a &quot;design for all &quot; perspective. While they have been produced out of observation and consultation with disabled people they have been constructed to be of value in making <b>multimodal</b> <b>dialogues</b> easier and more effective for all users not just those with special needs...|$|R
40|$|We {{describe}} a corpus of <b>multimodal</b> <b>dialogues</b> with an MP 3 player collected in Wizard-of-Oz experiments and annotated {{with a rich}} feature set at several layers. We are using the Nite XML Toolkit (NXT) (Carletta et al., 2003) to represent and further process the data. We designed an NXT data model, converted experiment log file data and manual transcriptions into NXT, and are building tools for additional annotation using NXT libraries. The annotated corpus {{will be used to}} (i) investigate various aspects of multimodal presentation and interaction strategies both within and across annotation layers; (ii) design an initial policy for reinforcement learning of multimodal clarification requests. 1...|$|R
40|$|This paper {{presents}} a simple, yet effective model for managing attention and interaction control in <b>multimodal</b> spoken <b>dialogue</b> systems, allowing {{the user to}} switch attention between the system and other humans. An evaluation in a tutoring setting shows that the user’s attention can be effectively monitored using head tracking. ...|$|R
40|$|Interfaces for mobile {{information}} access need {{to allow}} users flexibility in {{their choice of}} modes and interaction style {{in accordance with their}} preferences, the task at hand, and their physical and social environment. This paper describes the approach to multimodal language processing in MATCH (Multimodal Access To City Help), a mobile multimodal speech-pen interface to restaurant and subway information for New York City. Finite-state methods for multimodal integration and understanding enable users to interact using pen, speech, or dynamic combinations of the two, and a speech-act based <b>multimodal</b> <b>dialogue</b> manager enables mixedinitiative <b>multimodal</b> <b>dialogue.</b> 1. LANGUAGE PROCESSING FOR MOBILE SYSTEMS Mobile information access devices (PDAs, tablet PCs, next generation phones) offer limited screen real estate and no keyboard o...|$|E
40|$|We present {{on-going}} work on how {{to combine}} a <b>multimodal</b> <b>dialogue</b> system with techniques from information extraction and question-answering systems. A first combined system including both dialogue features and information extraction (BirdQuest) is presented. We conclude by listing {{a number of issues}} for further research...|$|E
40|$|Abstract — <b>Multimodal</b> <b>dialogue</b> systems provide {{multiple}} modalities in {{the form}} of speech, mouse clicking, drawing or touch that can enhance human-computer interaction. However, one of the drawbacks of the existing multimodal systems is that they are highly domain-specific and they do not allow information to be shared across different providers. In this paper, we propose a semantic multimodal system, called Semantic Restaurant Finder, for the Semantic Web in which the restaurant information in different city/country/language are constructed as ontologies to allow the information to be sharable. From the Semantic Restaurant Finder, users can make use of the semantic restaurant knowledge distributed from different locations on the Internet to find the desired restaurants. Index Terms — <b>Multimodal</b> <b>dialogue</b> systems, Semantic Web, Ontolog...|$|E
40|$|Groups {{of people}} {{involved}} in collaboration on a task often incorporate the objects in their mutual environment into their discussion. With this comes physical reference to these 3 -D objects, including: gesture, gaze, haptics, and possibly other modalities, {{over and above the}} speech we commonly associate with human-human communication. From a technological perspective, this human style of communication not only poses the challenge for researchers to create multimodal systems capable of integrating input from various modalities, but also to do it well enough that it supports [...] but does not interfere with [...] the primary goal of the collaborators, which is their own human-human interaction. This paper offers a first step towards building such multimodal systems for supporting face-to-face collaborative work by providing both qualitative and quantitative analyses of multiparty <b>multimodal</b> <b>dialogues</b> in a field setting...|$|R
40|$|The {{adoption}} of SharedPlans {{as a basis}} for <b>multimodal</b> <b>dialogues</b> is discussed. An extension to the model of plan augmentation for discourse is proposed so that it applies for multimodal interaction. The proposed process exploits SharedPlans and Adjacency Pairs in conjunction to account for global and local collaboration. Finally, multimedia coordination is taken into account. An example is followed throughout the paper to make the consequences of the proposal more concrete. Collaborating on the Interface In this paper, we consider the multimodal interface as a place where actions occur that may be considered both as domain actions and communicative (linguistic and nonlinguistic) actions. This is true for user actions and for system actions: when the system holds the initiative it performs some domain actions, some communicative actions or actions of both kinds. The interface {{is at the same time}} a sensorial organ, the medium (or the collection of media) through which the message is real [...] ...|$|R
40|$|A meaning {{representation}} {{language is}} described {{which includes a}} typed firstorder logic with relativised quantification, using an ontology with reified events and actions. This has been developed to support the requirements of <b>multimodal</b> co-operative <b>dialogue,</b> including ambiguity, temporal and aspectual reference in natural language, reference to graphical and dialogue objects of varying complexity, explanation, and the differing approaches needed to interpret assertions, questions and commands...|$|R
40|$|When {{designing}} <b>multimodal</b> <b>dialogue</b> systems allowing {{speech as}} well as graphical operations, {{it is important to}} understand not only how people make use of the different modalities in their utterances, but also how the system might influence a user's choice of modality by its own behavior. This paper describes an experiment in which subjects interacted with two versions of a simulated <b>multimodal</b> <b>dialogue</b> system. One version used predominantly graphical means when referring to specific objects; the other used predominantly verbal referential expressions. The purpose of the study was to find out what effect, if any, the system's referential strategy had on the user's behavior. The results provided limited support for the hypothesis that the system can influence users to adopt another modality for the purpose of referring...|$|E
40|$|In this paper, we {{describe}} a distributed <b>multimodal</b> <b>dialogue</b> system architecture {{based on the}} concept of hybrid-VoiceXML. It utilizes a special hybrid-construct to integrate multiple multimedia, multimodal processes into one dialogue that includes VoiceXML as its voice modality. The hybridconstruct in our approach has several important functions. It provides an additional abstraction layer for dynamic dialogue generation, which can greatly improve the efficiency and flexibility of the dialogue system. Under the proposed approach, the dialogue control between each interaction channel can be exchanged through the interface of a dynamic XML page. Several case studies are performed. It indicates that the proposed hybrid-VoiceXML approach is highly extensible. It can be used to form platform independent and distributed extensions for <b>multimodal</b> <b>dialogue</b> interaction beyond voice...|$|E
40|$|This paper {{describes}} a multimodal application architecture that facilitates rapid prototyping of multimodal interfaces with flexible input and adaptive output. Our testbed application MATCH (Multimodal Access To City Help) provides a mobile multimodal speech-pen interface to restaurant and subway information for New York City. Finite-state methods for multimodal language understanding are employed to enable users to interact using pen, speech, or dynamic combinations of the two. A speech-act based <b>multimodal</b> <b>dialogue</b> manager provides support for mixed-initiative <b>multimodal</b> <b>dialogue.</b> Multimodal generation and text planning components enable {{the system to}} respond using synchronized multimodal presentations that combine dynamic graphics with text-to-speech and are tailored to the user's individual preferences. The development of this architecture is driven by ongoing scenario-based evaluation and data collection with user...|$|E
40|$|Abstract: In {{this paper}} we {{introduce}} a formal {{framework for the}} modelling of rational <b>multimodal</b> <b>dialoguing</b> agents. Based on a rational theory of interaction, the framework enables agents to be conscious of several aspects of the communication: {{the characteristics of the}} user, the environment (noisy states, failing modes, etc.), the (un-) available media and modalities, the communicative goals and the types of information to produce. These agents are based on “traditional ” rationality principles extended with multimodal rules. These principles and rules are defined as a set of generic axioms, which enables agents to plan communicative acts, that actually are multimodal acts. As the agents have a description of the entire communication process, they are not limited in number or kinds of media and modalities. Moreover, the generic and task specific rules of multimodal behavior enable these agents to adapt to the applicative context and environment. In this paper, we will focus on the following aspects: the ability of the agents {{to take care of the}} user’s preferences and explicit choices, the personal data, the physical handicaps, the dynamic management of own media (failures, additions to the current dialogue, etc.), and the allocation of modalities by the system...|$|R
40|$|A general {{overview}} of the AdApt project and the research that is performed within the project is presented. In this project various aspects of human-computer interaction in a <b>multimodal</b> conversational <b>dialogue</b> systems are investigated. The project will also include studies on the integration of user/system/dialogue dependent speech recognition and multimodal speech synthesis. A domain in which multimodal interaction is highly useful has been chosen, namely, finding available apartments in Stockholm. A Wizard-of-Oz data collection within this domain is also described. 1...|$|R
40|$|International audienceWe present {{automatic}} {{systems that}} implement <b>multimodal</b> social <b>dialogues</b> involving humour with the humanoid robot Nao for the 16 th Interspeech conference. Humorous {{capabilities of the}} systems are based on three main techniques: riddles, challenging the human participant, and punctual interventions. The presented prototypes will automatically record and analyse audio and video streams to provide a real-time feedback. Using these systems, we expect to observe rich and varied reactions from international English-speaking volunteers to humorous stimuli triggered by Nao...|$|R
