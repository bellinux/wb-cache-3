50|5|Public
50|$|Future plans include porting {{the user}} {{interface}} from Tk to GTK+, a multiple document interface, and <b>multi-font,</b> fully integrated multiline text.|$|E
50|$|Bravo was {{the first}} WYSIWYG {{document}} preparation program. It provided <b>multi-font</b> capability using the bitmap displays on the Xerox Alto personal computer. It was produced at Xerox PARC by Butler Lampson, Charles Simonyi and colleagues in 1974.|$|E
50|$|In the mid-1970s, Raymond Kurzweil {{invented the}} first <b>multi-font</b> reading machine for the blind, {{consisted}} of the earliest CCD flat-bed scanner and text-to-speech synthesizer. In 1976, Stevie Wonder heard about the demonstration of this new machine on The Today Show, and later he became the user of first production unit, Kurzweil Reading Machine. It {{was the beginning of}} a long-term relationship between them.|$|E
40|$|International audienceIn this paper, {{we propose}} a new scheme for scriptand nature identification. The {{objective}} is to discriminate betweenmachine-printed/handwritten and Latin/Arabic scripts at wordlevel. It is relatively a complex task due to possible use of <b>multi-fonts</b> and sizes, complexity and variation in handwriting. In theproposed script identification system, we extract features fromword images using Co-occurrence Matrix of Oriented Gradients(Co-MOG). The classification is done using k Nearest Neighbors(k-NN) classifier. Extensive experimentation has been carried on 24000 words extracted from standard databases. An averageidentification accuracy of 99. 85 % is achieved which clearlyoutperforms results of some existing system...|$|R
40|$|Abstract In the paper, {{we present}} a {{hierarchical}} stroke extraction algorithm based on Stroke Segments and rough clustering, which is the pre-processing step for CJK outline font structure-based compression, and consists of two levels. In the higher level, Stroke Segments are extracted from CJK outline font topologically, in order to remove the intersections and some attachments. Then the strokes are composed by merging the Stroke Segments, and roughly clustered by the constitution and font knowledge. In the lower level, the strokes which are unsatisfying ones determined by the clustering results are sub-extracted according to the constituted Feature Segments. The experiments reveal that the approach is not only effective to resolve two difficulties in traditional methods, which influence CJK outline font compression greatly, but also suitable for <b>multi-fonts...</b>|$|R
40|$|In {{this paper}} {{we present a}} novel method to {{classify}} machine printed Chinese characters by matching the code strings generated from pseudo skeleton features. In our approach, the pseudo skeletons of Chinese characters are extracted rather than using skeletons extracted by traditional thinning algorithms. The features of the pseudo skeletons of both input and template characters are then encoded into two code strings. Finally, the edit-distance algorithm is employed to compute the similarity between the two characters based on their corresponding encoded strings. The main contribution {{of this paper is}} to effectively classify <b>multi-fonts</b> Chinese characters using a single-font reference database. Experiments were conducted on 5401 daily-used Chinese characters of various fonts and sizes. Experimental results demonstrate the validity and efficiency of our proposed method for classifying Chinese characters...|$|R
5000|$|In 1988, Tritek® {{introduced}} the first <b>multi-font</b> programmable optical reader for mail processing machines. Tritek® developed first high-speed flat sorter in 1991 and named it the 91-5 Ultrasorter which was {{tested by the}} United States Postal Service under their development project called FMBCS ( [...] flat mail barcode sorter) as a high speed flats sorter replacement. Technologies of that machine are still in use today as licensed technology from Tritek®.|$|E
50|$|These {{computers}} were {{famous for their}} simplicity allowing people with little technical ability to produce various hardware add-ons such as FDD controllers, AD/DA converters or software (such as Desktop — unique WYSIWYG word processor with functions like proportional text, pictures in text support, block functions, <b>multi-font</b> support etc.). Both version of these computers had been produced in Skalica, Slovakia. Didaktik's glory went out with price fall of 16-bit computers, such as the Atari and Amiga, around the middle 90s until it was finally steam-rolled by the PC soon after.|$|E
5000|$|Time {{and again}} the Bangalore Government has {{attempted}} to take positive action to save the lake, but in the end, the maximum efforts come from only from the citizens who plan plantation drives and cleanup programs around the lake. RTI activists like CH Ram have repeatedly brought the Lake's plight to the BBMP's attention, and have been promised affirmative action to save the lake. In 2010, the BBMP adopted a Lake Rejuvenation Program. Under this program, the lake received new fencing around its perimeter, only a few saplings were planted and the lake was cleaned up. In 2015, led by IT employee Sanchita Jha, a <b>multi-font</b> online campaign was raised by people of the city, with around fifty thousand petitioners, gaining wide scale media attention and forcing the CM to direct the authorities to draw up an action plan.However, the sewage treatment plants and small industries that dispel their wastes into the lake, have still not been stopped and so the lake's main problems still continue.|$|E
40|$|An {{experiment}} {{for testing}} a new initial alphabet is advocated. This Multi-Fontal Alphabet (MFA) maximizes the pattern differences be-tween characters {{without the use}} of new symbols. Other techniques for training dyslexic children are also discussed. The hypothesis is presented that a diverse set of characters (<b>multi-fonts)</b> would make the initial reading task easier, especially where reversals and other symbol similarities are a problem. The authors have not performed any experiments to validate this hypothesis; they hope this report will act as a stimulant for experiments by others. Dyslexia, a neurological symptom known to medicine for over half a century, manifests itself as an inability to acquire reading and spelling skills commensurate with a person's intelligence. It is particularly insidious since a large number of dyslexics are very intelligent and many are well endowed with the ability for abstract thinking (Crosby, 1968; DeHirsch et al., 1966; Hoffman, 1964). These children can learn by all the usual methods except reading. Many of them partially surmount their reading deficiencies by developing highly refined oral memories, just as many blind individuals con-centrate on their ability to listen (Crosby). * Articles appearing in this column are not necessarily supported or authenticated by research. However, the Journal feels that they should be presented for consideration...|$|R
40|$|Arabic text {{recognition}} was not researched {{as thoroughly}} as other natural languages. The need for automatic Arabic text recognition is clear. In {{addition to the}} traditional applications like postal address reading, check verification in banks, and office automation, {{there is a large}} interest in searching scanned documents that are available on the internet and for searching handwritten manuscripts. Other possible applications are building digital libraries, recognizing text on digitized maps, recognizing vehicle license plates, using it as first phase in text readers for visually impaired people and understanding filled forms. This research work aims to contribute to the current research in the field of optical character recognition (OCR) of printed Arabic text by developing novel techniques and schemes to advance the performance {{of the state of the}} art Arabic OCR systems. Statistical and analytical analysis for Arabic Text was carried out to estimate the probabilities of occurrences of Arabic character for use with Hidden Markov models (HMM) and other techniques. Since there is no publicly available dataset for printed Arabic text for recognition purposes it was decided to create one. In addition, a minimal Arabic script is proposed. The proposed script contains all basic shapes of Arabic letters. The script provides efficient representation for Arabic text in terms of effort and time. Based on the success of using HMM for speech and text recognition, the use of HMM for the automatic recognition of Arabic text was investigated. The HMM technique adapts to noise and font variations and does not require word or character segmentation of Arabic line images. In the feature extraction phase, experiments were conducted with a number of different features to investigate their suitability for HMM. Finally, a novel set of features, which resulted in high recognition rates for different fonts, was selected. The developed techniques do not need word or character segmentation before the classification phase as segmentation is a byproduct of recognition. This seems to be the most advantageous feature of using HMM for Arabic text as segmentation tends to produce errors which are usually propagated to the classification phase. Eight different Arabic fonts were used in the classification phase. The recognition rates were in the range from 98 % to 99. 9 % depending on the used fonts. As far as we know, these are new results in their context. Moreover, the proposed technique could be used for other languages. A proof-of-concept experiment was conducted on English characters with a recognition rate of 98. 9 % using the same HMM setup. The same techniques where conducted on Bangla characters with a recognition rate above 95 %. Moreover, the recognition of printed Arabic text with <b>multi-fonts</b> was also conducted using the same technique. Fonts were categorized into different groups. New high recognition results were achieved. To enhance the recognition rate further, a post-processing module was developed to correct the OCR output through character level post-processing and word level post-processing. The use of this module increased the accuracy of the recognition rate by more than 1 %. EThOS - Electronic Theses Online ServiceKing Fahd University of Petroleum and Minerals (KFUPM) GBUnited Kingdo...|$|R
50|$|As a researcher, Dr. Lehal’s main {{contribution}} {{has been}} {{development of technologies}} related to computerization of Punjabi language. Prominent among these are first Gurmukhi OCR, first bilingual Gurmukhi/Roman OCR, first Punjabi font identification and conversion system, first <b>multi-font</b> Punjabi spell checker, first high accuracy Gurmukhi-Shahmukhi and Shahmukhi-Gurmukhi transliteration systems and first Intelligent Predictive Roman-Gurmukhi transliteration techniques for simplifying Punjabi typing. Dr. Lehal has published more than 100 research papers in various national and international journals and conference proceedings. Dr. Lehal has handled research projects worth more than 43 million Rupees including three international projects, which were awarded in an open competition among contestants from more than 30 countries. As a software engineer, Dr. Lehal has developed more than 25 software systems including first commercial Punjabi word processor, Akhar. As an academician, Dr. Lehal has taught and supervised research activity of postgraduate and doctorate students. He has guided more than 100 post graduate Research scholars and 11 PhD students on various topics related to computerization of Punjabi, Hindi, Urdu and Sindhi languages.|$|E
40|$|In this paper, {{we present}} a new scheme for {{off-line}} recognition of <b>multi-font</b> numerals using the Takagi-Sugeno (TS) model. In this scheme, the binary image of a character is partitioned into a fixed number of sub-images called boxes. The features consist of normalized vector distances (gamma) from each box. Each feature extracted from different fonts {{gives rise to a}} fuzzy set. However, when we have a small number of fonts {{as in the case of}} <b>multi-font</b> numerals, the choice of a proper fuzzification function is crucial. Hence, we have devised a new fuzzification function involving parameters, which take account of the variations in the fuzzy sets. The new fuzzification function is employed in the TS model for the recognition of <b>multi-font</b> numerals...|$|E
40|$|In this paper, {{we propose}} a new scheme for {{off-line}} recognition of <b>multi-font</b> numerals. In the proposed scheme, the binary {{image of a}} character is partitioned into a fixed number of sub-images called boxes. The features consist of normalized vector distances and angles from each box. These features give rise to fuzzy sets. However, when we have small number of samples {{as in the case}} of <b>multi-font</b> numerals, the choice of a proper fuzzification function is crucial. Hence, we have devised a new fuzzification function involving parameters, which take account of the variations in the fuzzy sets. These parameters are obtained by optimization...|$|E
40|$|The {{problem of}} {{determining}} the script and language of a document image {{has a number of}} important applications in the field of document analysis, for example as a precursor to OCR. Previous work has shown that visual texture is an effective method of performing script recognition, however such an approach is highly susceptible to changes in font. In this paper, a method of <b>multi-font</b> script recognition using a clustered discriminate function is proposed, allowing the training of a single model for each script class incorporating all fonts. Experimental evidence shows that such an approach can lead to significantly reduced error rates when classifying <b>multi-font</b> scripts. Griffith Sciences, Griffith School of EngineeringNo Full Tex...|$|E
40|$|In {{this note}} we present and discuss results of {{experiments}} comparing {{the performance of}} six neural network architectures (back propagation, recurrent network with dampened feedback, network with multiple hidden layers each with a different activation function, jump connection networks, probabilistic neural networks and general regression neural networks) applied to a simplified <b>multi-font</b> recognition problem...|$|E
40|$|This paper {{presents}} {{on accuracy}} improvement of <b>multi-font</b> rotated character recognition. Until now, a recognition method for rotated characters {{was based on}} distance criterion on the eigen sub-space. That is, an unknown pattern is projected onto the eigensubspace of each category. The category which shows the closest distance between the projected point and the category locus is chosen. However, this simple method could not be cope with <b>multi-font</b> characters. Therefore, some unknown patterns were created by rotating the input pattern and projected onto the eigensubspace of each category. By that method, a good performance was achieved for small size of categories like alphabetic 26 capital letters. However, the performance fell down by {{increasing the number of}} categories like 62 alpha-numeric letters. By considering the cause of the misclassification, we found that the distance criterion accidentally caused misclassification. This paper proposes a new feature based on periodic property of projected points on the eigen space. The experimental results showed a considerably high recognition rate...|$|E
40|$|In {{this paper}} {{we present a}} <b>Multi-font</b> OCR system to be {{employed}} for document processing, which performs, at the same time, both the character recognition and the font-style detection of the digits belonging to {{a subset of the}} existing fonts. The detection of the font-style of the document words can guide a rough automatic classification of documents, and {{can also be used to}} improve the character recognition...|$|E
30|$|All the {{approaches}} discussed earlier {{based on the}} recognition of multi-oriented characters and are limited to proper script recognition. There {{is not a single}} approach in the literature which is based on the straightening of curved text-lines or words and script independence. In contrast, an approach for curved text-line straightening is proposed in this work which can handle <b>multi-font</b> size and type and multi-script text-lines in a single document.|$|E
40|$|The goal of {{this work}} is to detect the {{alteration}} in distribution of the vegetable covering in floodplain Paraná River between 1975 and 2007. This area might {{have been affected by}} human occupation and alteration on stream flow. Those mappings were accomplished using the <b>multi-font</b> data: multispectral images of sensor MSS and TM/Landsat, integrated with SRTM topography data, texture and vegetation index (NDVI) extracted from the multispectral images. The adopted approach was oriented by supervised classification based on region segmentation for 1975 and multi-layer neural networks for other images in subsequent years, due to the use of some non-parametric attributes. For this temporal analysis, several radiometric transformations on TM data were necessary, such as atmospheric correction of the reference scene with the model based on radiactive transfer theory (5 S) through the software SCORADIS and radiometric normalization of the other scenes by the Multivariate Alteration Detection (MAD) method. As a preliminry result it was noticed that there was alteration in the vegetable covering in that period of time and the <b>multi-font</b> approach allows to obtain more suitable results. Pages: 6157 - 616...|$|E
40|$|International audienceExisting {{works on}} the font {{recognition}} and based on texture analysis often used Gray Level Cooccurence Matrix (GLCM), Gabor Filters (GF) and wavelet. In this paper, we use Steerable Pyramid (SP) for texture analysis of arabic homogeneous and normalized text block in order to font recognition. In this frameworks, we use K Nearest Neighbors (KNN) and Back-propagation Artificial Neural Network (BpANN) for classification. The Obtained experimental results on the APTID/MF database (Arabic Printed Text Image/ <b>Multi-Font)</b> are encouragents...|$|E
30|$|In contrast, an {{approach}} for curved text-line straightening is proposed {{in this work}} which can handle <b>multi-font</b> size and type and multi-script text-lines within a single document. The proposed approach is limited to straightening of text-lines and words only and also cannot work well on text-lines not having gap between text-lines. Selection of size of structuring element is set manually in our proposed approach. Automation of structuring element's size selection {{is required for the}} enhancement of the accuracy of the proposed approach.|$|E
40|$|In this paper, {{we propose}} a {{comparative}} study between the affixal approach and the analytical approach for off-line Arabic decomposable word recognition. The analytical approach {{is based on}} the modeling of alphabetical letters. The affixal approach {{is based on the}} modeling of the linguistic entity namely prefix, infix, suffix and root. The experimental results obtained by these two last approaches are presented {{on the basis of the}} printed decomposable word data set in mono-font nature by varying the character sizes. We achieve then our paper by the current improvements of our works concerning the Arabic <b>multi-font,</b> multi-style and multi-size word recognition. 1...|$|E
40|$|This paper {{presents}} {{two methods}} for enhancing recognition rate for Arabic typewritten digits. The first is node method that computes number of terminal nodes, {{and the second}} is right side method that studies the shape from the right side. These methods can recognize <b>multi-font</b> digits using two stages; each method produces specific results and then compares these results to obtain the final output. The recognition of <b>multi-font</b> typewritten is essential. It is a bit complicated to process, due to the difference in shape and size of the same digit. Therefore, the researcher used two methods for recognizing multi-fonts. The recognition system contains several steps, image preprocessing, which includes converting into binary, cropping the digit in single image with resize to 32 x 42 pixel, and thinning the shape of the digit to get the skeleton of the digit. Feature extraction includes number of terminal nodes from nodes method and two characters to specify the curve of right side of the shape. The recognition includes logical comparison between two vectors, one from each method. The proposed technique was implemented and tested The experimental results showed that the proposed technique is efficient for recognizing typewritten digits. The results proved that the techniques work properly and are able to give recognition rate for 11 fonts up to 100 %, or less for other fonts due to irregularity for some fonts or failing for one of two methods. The dataset contains multi-size and multi-fonts for the digits from 0 to 9...|$|E
40|$|In {{this paper}} a novel {{approach}} is proposed based on single Euler number feature which is free from thinning and size normalization for <b>multi-font</b> and multi-size Kannada numeral recognition system. A nearest neighbor classification is used for classification of Kannada numerals by considering the Euclidian distance. A total 1500 numeral images with different font sizes between (10 [...] 84) are tested for algorithm efficiency and the overall the classification accuracy {{is found to be}} 99. 00 %. The said method is thinning free, fast, and showed encouraging results on varying font styles and sizes of Kannada numerals. Comment: 4 pages, 1 figure, 5 tables, "Recent Trends in Information Technology(RTIT- 2009) ...|$|E
40|$|In {{this paper}} a new scheme is {{proposed}} for off-line recognition of <b>multi-font</b> numeral, using neural networks. Recognition of numerals {{has been a}} research area for many years because of its various applications. But there wasn’t much research done for recognition of <b>multi-font</b> numerals. The approaches proposed so far, suffer from larger computation time and training because they must {{have a set of}} training samples per each font. They can be extended to recognize many more fonts but the accuracy decreases rapidly. So as to eliminate these drawbacks, in this paper, a method is presented which recognizes 30 different fonts of different sizes varying from size 10 to 28, with an accuracy of 99. 55 % on a database of 2000 numeral images. The {{purpose of this study is}} to provide a new method to recognize digits based on neural network that can identify the same symbols after training without limitation on the type of the font. In the proposed method, a high accuracy rate is achieved in recognizing digits by extracting the appropriate features without the need for complex neural network architecture. This method uses a self-organizing map (SOM) neural network to measure similarity between the features of digits and the features of the indicators associated with the digits from 0 to 9 obtained in the training stage. In this method, one sample is used for each digit to train the network. So, the proposed method can be used to recognize typed letters without limitation on fonts...|$|E
40|$|A {{computational}} {{model for the}} recognition of <b>multi-font</b> machine-printed word images of highly variable quality is given. The model integrates three word recognition algorithms, each of which utilizes a different form of shape and context information. The approaches are character recognition based, segmentation based, and word-shape analysis based. The model overcomes limitations of previous solutions that focus on isolated characters. In an experiment using a lexicon of 33, 850 words and a test set of 1, 671 highly variable word images, the algorithm achieved a correct rate of 89 % at the top choice and 95 % in the top ten choices. Keywords: character recognition, word recognition, pattern recognition, multiple classifiers, decision combination. ...|$|E
40|$|This paper {{addresses}} {{the problem of}} Bangla basic character recognition. <b>Multi-font</b> Bangla character recognition has not been attempted previously. Twenty popular Bangla fonts {{have been used for}} the purpose of character recognition. A novel feature extraction scheme based on the digital curvelet transform is proposed. The curvelet transform, although heavily utilized in various areas of image processing, has not been used as the feature extraction scheme for character recognition. The curvelet coefficients of an original image as well as its morphologically altered versions are used to train separate k– nearest neighbor classifiers. The output values of these classifiers are fused using a simple majority voting scheme to arrive at a final decision...|$|E
40|$|Recognition of {{arbitrary}} noisy English text {{has been}} difficult because of problems in character segmentation and <b>multi-font</b> symbol classification. Both segmentation and recognition can be easier with more knowledge of the dominant font used in a given text page. This has led to some recent studies that show promising methods for extracting character prototypes from a text image provided that truth is given {{for part of the}} image. In this paper we investigate the feasibility of such a strategy without dependence on ground truth. We replace the needed truth by results of direct recognition of some frequently occurring words. The method makes use of the observation that over half of the words in a typical English text passage are contained in a very small lexicon...|$|E
40|$|We {{report on}} the {{creation}} of a database composed of images of Arabic Printed words. The purpose of this database is the large-scale benchmarking of openvocabulary, <b>multi-font,</b> multi-size and multi-style text recognition systems in Arabic. The challenges that are addressed by the database are in the variability of the sizes, fonts and style used to generate the images. A focus is also given on low-resolution images where anti-aliasing is generating noise on the characters to recognize. The database is synthetically generated using a lexicon of 113 ’ 284 words, 10 Arabic fonts, 10 font sizes and 4 font styles. The database contains 45 ’ 313 ’ 600 single word images totaling to more than 250 million characters. Ground truth annotation is provided for each image. The database is called APTI for Arabic Printed Text Images. 1. Introduction an...|$|E
40|$|In this paper, a new {{algorithm}} for {{vehicle license}} plate identification is proposed, {{on the basis}} of a novel adaptive image segmentation technique (Sliding Windows) in conjunction with a character recognition Neural Network. The algorithm was tested with 2820 natural scene gray level vehicle images of different backgrounds and ambient illumination. The camera focused on the plate, while the angle of view and the distance from the vehicle varied according to the experimental setup. The license plates properly segmented were 2719 over 2820 input images (96. 4 %). The Optical Character Recognition (OCR) system is a two layer Probabilistic Neural Network with topology 108 - 180 - 36, whose performance reached 97. 4 %. The PNN was trained to identify <b>multi-font</b> alphanumeric characters from car license plates based on data obtained from algorithmic image processing. © 2005 IEEE...|$|E
40|$|Amulet {{is a new}} user {{interface}} software environment for C++ to support future {{user interface}} software research. This environment, which will be portable across X/ 11, Microsoft Windows, and the Macintosh, {{is designed to be}} very flexible: parts can be replaced and new technologies and widgets can be easily created and evaluated. Built-in support will be provided for direct manipulation, <b>multi-font</b> text editing, gesture recognition, speech recognition, 2 -D and 3 -D animations, visualizations including maps and large data sets, world-wide-web browsing and editing, and multiple people interacting with the system at the same time (CSCW). Another goal is to be useful for students, which means that Amulet must be easy to learn. Finally, the system will provide sufficient performance, robustness and documentation so it will be useful for general user interface developers. Keywords : User Interface Software, User Interface Management Systems, Toolkits. This research was sponsored by NCCOSC under [...] ...|$|E
40|$|A {{parallel}} {{approach to}} OCR {{may be useful}} in high-volume document processing applications. This paper evaluates several parallelisation strategies and describes the implementation of OCHRE-P, a prototype OCR system modelled on a task farming paradigm, in which each task consists of a line of text. A feed-forward neural network with a single hidden layer is used for pattern classification. Using a novel boundary scanning technique along with a scaled bitmap to provide the input feature vector, a recognition accuracy of around 97 % was recorded for correctly segmented, <b>multi-font,</b> upper-case letters. Parallel performance on a cluster of seven Ethernet workstations and on a Transputer Computing Surface is compared: the shared bus architecture of the cluster delivers nearoptimal scalability and would seem to be more suited to this problem. Possible extensions are considered and the viability of a parallel approach to OCR discussed in light of the results. EPCC-SS 95 - 06 2 Contents 1 Introdu [...] ...|$|E
40|$|We {{present in}} this paper two {{applications}} of stochastic models to text recognition. The ørst application concerns <b>multi-font</b> printed text recognition (ptr) while the second deals with handwritten word recognition (hwr). The former is built around ørst and second order hidden Markov models (hmm) and uses an extended Viterbi algorithm for recognition. The method operates in a bottomup manner by proposing a list of candidates for each character and then the system uses combinations of stochastic and dictionary veriøcation methods for word recognition and error-correction. The later deals with unconstrained ooeline hwr with a limited vocabulary. It {{is based on the}} maximization of a given word in terms of probabilities of its component characters. This approach operates in a top-down manner by giving for each word of the lexicon against which the pattern is matched the character plausibility. The word having the highest average plausibility per character is selected. This approach tends to [...] ...|$|E
40|$|Abstract: In this paper, {{we propose}} a {{combined}} method for recognition of <b>multi-font</b> Persian numeral characters. At first, the binary {{image of a}} character is divided into a fixed number of sub-images called boxes. The average vector distance and angle of each box are computed as features. These features have some variations in different fonts of any character. So, we can employ the fuzzy sets to face with recognition problem. To have a best effect of fuzzy measure for any box features we employ an exponential fuzzification involving two extra parameters, which {{take account of the}} variations in the fuzzy sets. These parameters are obtained by minimizing the entropy of fuzzy membership function. After defuzzification, the three most probable candidates of numbers are selected. These candidates are post-processed with another fuzzy recognition system which uses the other features of numerals, i. e. the type of primitives. This combined method increases the robustness of recognition...|$|E
40|$|Features {{derived from}} the trispectra of DFT {{magnitude}} slices are used for <b>multi-font</b> digit recognition. These features are insensitive to translation, rotation, or scaling of the input. They are also robust to noise. Classification accuracy tests were conducted on a common data base of 256 × 256 pixel bilevel images of digits in 9 fonts. Randomly rotated and translated noisy versions were used for training and testing. The {{results indicate that the}} trispectral features are better than moment invariants and affine moment invariants. They achieve a classification accuracy of 95 % compared to about 81 % for Hu's (1962) moment invariants and 39 % for the Flusser and Suk (1994) affine moment invariants on the same data in the presence of 1 % impulse noise using a 1 -NN classifier. For comparison, a multilayer perceptron with no normalization for rotations and translations yields 34 % accuracy on 16 × 16 pixel low-pass filtered and decimated versions of the same data...|$|E
