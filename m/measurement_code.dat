14|333|Public
40|$|With the industry’s {{efforts in}} {{promoting}} the use of Web services, {{a huge number of}} Web services are being developed for the web. In this paper we describe a framework for providing QoS measurement for Web services through the use of DAML-QoS Ontology and <b>measurement</b> <b>code</b> generator. The framework comprises the DAML-QoS Ontology that specifies QoS constraints for Web services and works as a complement to DAML-S; the measurement framework that checks the system’s compliance with the service level agreement; code generator that helps to generate the <b>measurement</b> <b>code</b> according to the DAML-QoS specification. Based on the Ontology level, semantics in the specification helps to achieve better interoperability, automation and extensibility...|$|E
40|$|Release Notes: New Features: Initial {{implementation}} of grid removal. Log scale support for polar diagrams. Bug fixes and code clean-up: Eliminate dependence on numeric. js for a lighter and more predictable code base. Improve rendering of data points. Refactor <b>measurement</b> <b>code</b> {{to allow for}} path length and area measurement tools in the future...|$|E
40|$|We use a {{suite of}} {{simulated}} images based on Year 1 of the Dark Energy Survey to explore the impact of galaxy neighbours on shape measurement and shear cosmology. The hoopoe image simulations include realistic blending, galaxy positions, and spatial variations in depth and PSF properties. Using the im 3 shape maximum-likelihood shape <b>measurement</b> <b>code,</b> we identify four mechanisms by which neighbours can have a non-negligible influence on shear estimation. These effects, if ignored, would contribute a net multiplicative bias of m ∼ 0. 03 - 0. 09 in the DES Y 1 im 3 shape catalogue, though the precise impact will be dependent on both the <b>measurement</b> <b>code</b> and the selection cuts applied. This {{can be reduced to}} percentage level or less by removing objects with close neighbours, at a cost to the effective number density of galaxies n_eff of 30...|$|E
40|$|The work {{described}} in this paper details {{the development of a}} low-cost, shortdevelopment time data acquisition and processing system for a coherent Doppler lidar. This was done using common laboratory equipment and a small software investment. This system provides near real-time wind profile <b>measurements.</b> <b>Coding</b> flexibility created a very useful test bed for new techniques. 1...|$|R
5000|$|Metrics and <b>measurement.</b> Analyze <b>code</b> for complexity, {{modularity}} (e.g., no [...] "go to's"), performance, etc.|$|R
50|$|For {{analysis}} and <b>measurement</b> of <b>code</b> coverage, TPT can interact with coverage tools like Testwell CTC++ for C-code.|$|R
40|$|In {{this paper}} we present fsdisco- a {{portable}} perl script {{to discover the}} characteristics of a file system. We describe its design and report its capabilities and limitations, as currently implemented. We also present the results of an empirical study of the Linux ext 2 file system. These results demonstrate that a portable perl script can perform the necessary fine-grained measures, allowing the programmer to conveniently maintain and reuse <b>measurement</b> <b>code.</b> ...|$|E
40|$|This thesis studies {{executable}} instrumentation, {{a technique}} for augmenting a compiled and linked program with <b>measurement</b> <b>code</b> to analyze program performance. Instrumenting a complete executable {{instead of the}} source code or individual object les has the benet of programming language independence {{and access to the}} whole program code at a level that allows very precise measurements. Instrumentation is conceptually simple, but in practice complicated by limitations in the operating system and the format of the executable file of the used environment, conventions of the compilers and the operating system, and peculiarities of the instruction set of the target architecture. An instrumentation [...] ...|$|E
40|$|Worst case {{scheduling}} {{techniques for}} real-time applications often result in severe underutilization of the processor resources since most tasks finish in much {{less time than}} their anticipated worst-case execution times. In this paper we describe compiler-based techniques that classify the application code {{on the basis of}} predictability and monotonicity, introduce <b>measurement</b> <b>code</b> fragments at selected points in the application code, and use the results of run-time measurements to dynamically adapt worst-case schedules. This results in better utilization of the system and early failure detection and recovery. 1 Introduction 1. 1 Problem Definition The success of real-time application software depends on its ability to produce a functionally correct result within definite timing constraints. Hard real-time applications, especially embedded applications, interact with and influence the environment in which they execute. Consequently, safety and timeliness of execution are critical issues [...] ...|$|E
40|$|Satellite {{navigation}} {{is based}} on <b>measurements</b> of <b>code</b> pseudo-ranges, so accuracy of <b>code</b>  coordinate <b>measurements</b> depends on accuracy of <b>measurements</b> of <b>code</b> pseudo-ranges. The  differential mode with phase pseudo-ranges is used for high-precision coordinate measurements, but he doesn't solve a problem of integer measurements of phase ambiguity. Filter Kalman gives the chance to improve accuracy of phase pseudo-ranges and essentially eliminates errors at use of phase and code pseudo-ranges in a kinematic mode which are measured throughout one epoc...|$|R
40|$|We {{describe}} an algorithm and data structures {{to carry out}} auxiliary computations on arbitrary surfaces cutting through the computational domain of FDTD, and their implementation within the Guile process grafted {{on top of a}} legacy application. The tools so developed are then used to verify the code by comparing numerical and analytical solutions for harmonic wave scattering on a dielectric ball. Keywords: computational electrodynamics, finite-difference time-domain method, fluxes, fourier analysis, near-to-far field transformation, surfaces, virtual <b>measurements,</b> <b>code</b> grafting, object-oriented programming, component-based software engineerin...|$|R
40|$|The Monte Carlo code MCNP-DSP was {{developed}} from the Los Alamos MCNP 4 a code to calculate the time and frequency response statistics obtained from subcritical <b>measurements.</b> The <b>code</b> {{can be used to}} simulate a variety of subcritical measurements including source-driven noise analysis, Rossi-{alpha}, pulsed source, passive frequency analysis, multiplicity, and Feynman variance <b>measurements.</b> This <b>code</b> can be used to validate Monte Carlo methods and cross section data sets with subcritical measurements and replaces the use of point kinetics models for interpreting subcritical measurements...|$|R
40|$|Abstract: Real-time {{programs}} must be logically correct and must complete without timing errors. Errors due to overloaded resources are exposed very late in a development process, and often at run-time. Specifically, when {{a set of}} tasks {{are found to be}} overloaded and not schedulable, the developers usually remedied these problems by manual-intensive operations such as instrumentation, <b>measurement,</b> <b>code</b> tuning and eventually redesign. Such operations are slow and uncontrollable. We propose a semi-automatic alternative to this manual process, based on the task restructuring. Our approach is an extension of the one proposed by R. Gerber that only treats some monolithic and simple programs. It consists for the scheduling perspective, in restructuring real-time programs, which can be complex and contain a collection of procedures. This restructuring, by using the interprocedural program slicing concept and a finer dependence model, allows to make flexible the deadline task and therefore, to improve its schedulability...|$|E
40|$|In {{this paper}} {{we present a}} compiler-based {{technique}} to help develop correct real-time systems. The domain we consider is that of multi-programmed real-time applications, in which periodic tasks control physical systems via interacting with external sensors and actuators. While a system is up and running, these operations must be performed as specified [...] otherwise the system may fail. Correctness depends not only on each program individually, {{but also on the}} time-multiplexed behavior of all of the programs running together. Errors due to overloaded resources are exposed very late in a development process, and often at runtime. They are usually remedied by humanintensive activities such as instrumentation, <b>measurement,</b> <b>code</b> tuning and redesign. We describe a static alternative to this process, which relies on well-accepted technologies from optimizing compilers and fixed-priority scheduling. Specifically, when a set of tasks are found to be overloaded, a scheduling analyzer determines c [...] ...|$|E
40|$|This work {{focuses on}} {{relative}} comparisons of individual method's performance. It {{is based on}} Stochastic Performance Logic, which allows to express, for example, that one method runs at most two times longer than another method. These results are more portable than absolute values. It extends standard unit tests with performance assumptions, which are evaluated during actual run-time of a released application. Dynamically added and removed instrumentation is used for automatic modification of the production code. Instrumentation part uses DiSL framework {{to be able to}} seamlessly measure even Java system classes. Methods are measured sequentially, the number of concurrently measured methods is dynamically changed and the <b>measurement</b> <b>code</b> is removed as soon as required data are obtained. The results show that this approach may bring appreciably lower overhead peaks than measuring all methods at once. The prototype was compared with JMH tool and the results show that it is able to accurately measure methods running longer than 1 ms. Powered by TCPDF (www. tcpdf. org...|$|E
40|$|On 14 th December 2002, the Australian small {{satellite}} FedSat {{was launched}} with a Japanese HII-A rocket into an 800 km sun-synchronous orbit with an inclination of 98. 6 degrees. One of the payloads is a dual frequency GPS BlackJack receiver from NASA Jet Propulsion Laboratory (JPL). This receiver provides a Position, Velocity and Time (PVT) solution and also raw <b>measurements</b> (<b>code</b> and carrier), {{which will be}} used in different ways for Precise Orbit Determination (POD), Orbit Determination (OD) based on GPS position solutions, 2 -axis Attitude Determination (AD) and atmospheric experiments...|$|R
40|$|High data-rate {{atmospheric}} optical free-space (FSO) lasercom systems typically {{suffer from}} relatively long link degradations. They {{are caused by}} pointing- and tracking-errors or deep signal-fades produced by index of refraction turbulence effects. Based on measurement results we will present in this paper a channel characterization model for freespace optical links. Further a forward-error-correction (FEC) coding scheme is introduced which is able to overcome these link outages. The performance of these codes has been proven by <b>measurements.</b> <b>Code</b> design recommendations and validation test results are discussed in this paper...|$|R
40|$|The {{subject of}} this {{bachelor}} thesis is code coverage determination, methods for <b>measurement</b> of <b>code</b> coverage and code coverage tools {{with the intention of}} Java programming language. The aim of this thesis is explanation of basic methods for <b>measurement</b> of <b>code</b> coverage and testing and assessment of the three code coverage tools. The thesis is divided into five chapters. Content of the first chapter is introduction to subject of the thesis. The second chapter brings out the importance of testing during software development and contains account of kinds of testing. Explanation of code coverage and particular methods for <b>measurement</b> of <b>code</b> coverage is in the third chapter. The fourth chapter contains view of available code coverage tools and description of three tools, which were tested. Further the fourth chapter contains assessment of tested tools by force of multicriteria evaluation of alternates. In the last chapter results of assessment of tested tools are mentioned...|$|R
40|$|Importing and {{executing}} untrusted foreign code {{has become}} an everyday occurrence: Web servers download plug-ins and applets; databases load type-spe-cific extensions; and operating systems load customized policies and performance <b>measurement</b> <b>code.</b> Certification of {{the safety of the}} untrusted code is crucial in these domains. I have developed new methods to determine statically whether it is safe for untrusted machine code to be loaded into a trusted host system. My safety-check-ing technique operates directly on the untrusted machine-code program, requir-ing only that the initial inputs to the untrusted program be annotated with typestate information and linear constraints. This approach opens up the possi-bility of being able to certify code produced by any compiler from any source lan-guage. It eliminates the dependence of safety on the correctness of the compiler because the final product of the compiler is checked. It leads to the decoupling of the safety policy from the language in which the untrusted code is written, and consequently, makes it possible for safety checking to be performed with respec...|$|E
40|$|We {{present and}} {{describe}} IM 3 SHAPE, a new publicly available galaxy shape <b>measurement</b> <b>code</b> for weak gravitational lensing shear. IM 3 SHAPE performs a maximum likelihood fit of a bulgeplus-disc galaxy model to noisy images, incorporating an applied point spread function. We detail challenges faced and choices made in its design and implementation, and then discuss various limitations that affect {{this and other}} maximum likelihood methods. We assess the bias arising from fitting an incorrect galaxy model using simple noise-free images and find {{that it should not}} be a concern for current cosmic shear surveys. We test IM 3 SHAPE on the GREAT 08 Challenge image simulations, and meet the requirements for upcoming cosmic shear surveys in the case that the simulations are encompassed by the fitted model, using a simple correction for image noise bias. For the fiducial branch of GREAT 08 we obtain a negligible additive shear bias and sub-two percent level multiplicative bias, which is suitable for analysis of current surveys. We fall short of the sub-percent level requirement for upcoming surveys, which we attribute to a combination of noise bias and the mis-match between our galaxy model and the model used in the GREAT 08 simulations. We meet the requirements for current survey...|$|E
40|$|Use is {{free for}} {{scientific}} purposes, provided the aforementioned reference is appropriately cited. The dataset represents the seismic noise measurements performed in Subequana Valley (central Italy) {{and used in}} the publication “Gori, S., Falcucci, E., Ladina, C., Marzorati, S., and Galadini, F. : Active faulting, 3 -D geological architecture and Plio-Quaternary structural evolution of extensional basins in the central Apennine chain, Italy, Solid Earth, 8, 319 - 337, doi: 10. 5194 /se- 8 - 319 - 2017, 2017 ”. The measurements are archived in SAC format ([URL] Each SAC file contains information on the measurement parameters. Main header fields: delta: sampling (s) stla: measurement latitude (°N) stlo: measurement longitude (°E) stel: measurement elevation (m a. s. l.) user 0 : sensor sensitivity (V/m/s) user 1 : datalogger sensitivity (µV/count) kstnm: <b>measurement</b> <b>code</b> kevnm: experiment name kuser 0 : gain kuser 1 : unit of measure kuser 2 : type of the sensor kcmpnm: seismic channel knetwk: network code of the experiment kinst: type of the datalogger Description of files: - CSVnoise_fromCV 35 _toCV 87. zip: the archive of SAC files relative to noise measurements from CV 35 to CV 87 - CSVnoise_fromCV 90 _toC 119. zip: the archive of SAC files relative to noise measurements from CV 90 to C 119 - CSVnoise_fromC 120 _toC 218. zip: the archive of SAC files relative to noise measurements from C 120 to C 21...|$|E
40|$|The diploma thesis {{presents}} {{the collection and}} analysis of data from the public infrastructure, which {{is presented as a}} case study of reconstruction of a local road with all the accompanying public infrastructure lines, Otiški vrh - Šentjanž in the Dravograd municipality. The reconstruction includes a description of the geodetic works, geodetic equipment used for the collection of data in the field, as well as preliminary preparation for the <b>measurements.</b> <b>Coding</b> of the recorded points on the public infrastructure is also presented, which enables faster case analysis in the office {{with the help of the}} INFRA program, which is an independent program for the analysis of graphical and descriptive data for the public infrastructure...|$|R
40|$|There are {{systematic}} multipath errors on BeiDou <b>code</b> <b>measurements,</b> {{which are}} range from several decimeters to larger than 1 meter. They {{can be divided}} into two categories, which are systematic variances in IGSO/MEO <b>code</b> <b>measurement</b> and in GEO <b>code</b> <b>measurement.</b> In this contribution, a methodology of correcting BeiDou GEO code multipath is proposed base on Kalman filter algorithm. The standard deviation of GEO MP Series decreases about 10 %~ 16 % after correction. The weight of code in single-frequency PPP is great, therefore, code systematic multipath errors have impact on single-frequency PPP. Our analysis indicate that about 1 m bias will be caused by these systematic errors. Then, we evaluated the improvement of single-frequency PPP accuracy after code multipath correction. The systematic errors of GEO <b>code</b> <b>measurements</b> are corrected by applying our proposed Kalman filter method. The systematic errors of IGSO and MEO <b>code</b> <b>measurements</b> are corrected by applying elevation-dependent model proposed by Wanninger and Beer. Ten days observations of four MGEX (Multi-GNSS Experiment) stations are processed. The results indicate that the single-frequency PPP accuracy can be improved remarkably by applying code multipath correction. The accuracy in up direction can be improved by 65 % after IGSO and MEO code multipath correction. By applying GEO code multipath correction, the accuracy in up direction can be further improved by 15 %...|$|R
40|$|<b>Code</b> <b>measurement</b> {{hand-held}} GPS {{serves a}} fast growing user community involved in navigation {{and data collection}} for field data collection, tourism, various public services and commercial applications. Most handheld GPS code receivers, however, do not provide instantaneous and intuitive information on the positional error during measurement, which may be important to compromise data accuracy against time efficiency in daily field practice. Herein we describe an experiment on how to assess the positional error of handheld code receivers and discuss a method that improves on the reporting of the positional error to the user in the field when reference points are available. In the experiment, the x and y coordinates of 100 reference points {{were used to assess}} the positional error of a handheld GPS code receiver. The coordinates of the points were computed from DGPS carrier phase measurements, and assumed to be error free. The error distribution in x and y for 1, 3, 5 and 10 minute averaging periods were computed to generate a figure of merit that is intuitive and informative to a non-expert GPS user. Results obtained are well within the range of the theoretical accuracy that can be expected for single <b>measurements</b> <b>code</b> receivers...|$|R
40|$|The {{discovery}} of a light scalar at the Large Hadron Collider is in basic agreement with the predictions of an elementary Higgs in the Standard Model (SM). Nonetheless, a light, fundamental scalar is difficult to accommodate in the SM because quantum corrections suggest its mass should be {{much higher than the}} scale of electroweak symmetry breaking (EWSB). A natural possibility is to replace the Higgs by a strongly coupled composite. Composite dynamics also gives a natural explanation to the origin of EWSB. Phenomenologically viable composite models of EWSB are constrained by experiment to feature approximate scale invariance. This behavior may follow from near conformal dynamics. At present, lattice gauge theory (LGT) provides the only quantitative method to study near conformal composite Higgs dynamics in a fully consistent strongly coupled relativistic quantum field theory. As a novel approach to the question of finding and studying near conformal theories, I will apply LGT to the study of a generalization of Quantum ChromoDynamics (QCD) with four chiral fermion flavors plus eight flavors of finite, tunable mass. By continuously varying the mass of the eight heavy flavors, I can tune between the four flavor chirally broken theory, which exhibits features similar to QCD, and the twelve flavor theory, which is known to have a conformal fixed point. This is the " 4 + 8 Model" for directly studying near-conformal behavior. In this dissertation, I will review modern composite phenomenology, followed by outlining a study of the 4 + 8 Model over a range of heavy flavor masses. As a check of near-conformal behavior, I will measure the scale dependent coupling with the method of the Wilson Flow. After verifying the existence of controllable, approximate scale invariance, I will measure the low energy particle spectrum of the 4 + 8 Model. This includes a Higgs-like light composite scalar. Throughout this dissertation I will make reference to LGT <b>measurement</b> <b>code</b> I wrote and contributed to the software package FUEL...|$|E
40|$|This report {{presents}} {{the process of}} design and implementation of a battery charger for a Li-ion battery. The development of this battery charger includes the component from Linear Technology LTC 4015. This component integrates the functions of a battery charger configured as a buck (step-down) converter. This device must be integrated in a Printed Circuit Board with a specific design. Also, it must be configured using a microcontroller named Raspberry Pi, which also performs the measurements. The method of design is divided in two parts. One is focused on developing the printed circuit board, which includes the simulation of the device {{and the development of}} the PCB, and the second one is focused on developing the program used in the microcontroller to manage the operation of the LTC 4015. The result is a charge controller device that can be used with different configurations with a buck converter topology. The different parts of the design process are the simulation, the design and the implementation. Each of these parts have a section of results in this report. The simulation section includes results obtained with LTSpice and the device LTC 4020, which is a similar device to the LTC 4015 but without the Maximum Power Point Tracking mode, which is not modelled in LTSpice. PV is the main power source considered to charge the battery, and is carefully studied in this project. The PV input is studied with LTSpice, first simulating the I-V curve of the schematic of the solar cell. Second, integrating a solar cell in the simulation of the LTC 4015. Third, operating the device LTC 4015 with a solar panel that is also characterized. The design section includes the electronic components used {{for the development of the}} board that integrates the charge controller, the LTC 4015 in this case, based on the calculations performed for the requirements of the LTC 4015. Finally, the implementation section includes the description of the board implemented but also the description of the configuration and <b>measurement</b> <b>code.</b> The conclusions presented in this report show that the LTC 4015 is a battery buck charger with different functions that make it suitable to be used in different solar applications. Also, this report opens new future work lines, such as the full characterization of the board, the implementation of a test bench and the integration of the board in different applications for solar energy systems...|$|E
50|$|This is {{the tool}} {{that was used}} to make the C source <b>code</b> <b>measurements</b> given in the book Safer C by Les Hatton.|$|R
40|$|A central {{feature of}} the {{evolution}} of large software systems is that change [...] which is necessary to add new functionality, accommodate new hardware and repair faults [...] becomes increasingly difficult over time. In this paper we approach this phenomenon, which we term code decay, scientifically and statistically. We define code decay, and propose a number of <b>measurements</b> (<b>code</b> decay indices) on software, and on the organizations that produce it, that serve as symptoms, risk factors and predictors of decay. Using an unusually rich data set (the fifteen-plus year change history of the millions of lines of software for a telephone switching system), we find mixed but on the whole persuasive statistical evidence of code decay, which is corroborated by developers of the code. Suggestive indications that perfective maintenance can retard code decay are also discussed...|$|R
40|$|Experiments {{and devices}} based on {{continuous}} waves (CW) optical beams can provide solutions to optical quantum information processing which are analogous to those {{carried out with}} single photon entangled states. We report on our experimental achievements and goals for using squeezed light and entangled beams for quantum nondemolition <b>measurements,</b> dense <b>coding,</b> teleportation and quantum cryptography...|$|R
5000|$|... {{end-to-end}} <b>measurements</b> of <b>code,</b> {{for example}} performed by setting an I/O pin on {{the device to}} high {{at the start of}} the task, and to low at the end of the task and using a logic analyzer to measure the longest pulse width, or by measuring within the software itself using the processor clock or instruction count.|$|R
40|$|Abstract. ETOMIC is {{a network}} traffic {{measurement}} platform with high precision GPS-synchronized monitoring nodes. The infrastructure is publicly {{available to the}} network research community, supporting ad-vanced experimental techniques by providing high precision hardware equipments and a Central Management System. Researchers can de-ploy their own active <b>measurement</b> <b>codes</b> to perform experiments on the public Internet. Recently, the functionalities of the original system were significantly extended and new generation measurement nodes were deployed. The system now also includes well structured data reposito-ries to archive and share raw and evaluated data. These features make ETOMIC {{as one of the}} experimental facilities that support the design, development and validation of novel experimental techniques for the fu-ture Internet. In this paper we focus on the improved capabilities of the management system, the recent extensions of the node architecture and the accompanying database solutions. Key words: ETOMIC, network measurement infrastructure, active measurements...|$|R
40|$|The {{aim of this}} master {{thesis is}} to explore the area of {{software}} metrics and to identify software metrics related to the code complexity. In this thesis, thorough study is made {{to determine whether or}} not the automatic <b>measurement</b> of source <b>code</b> complexity is possible. A tool for automatic <b>measurement</b> of source <b>code</b> complexity is implemented during this thesis to prove the idea that the automatic measurement is achievable. This report summaries the theory about software metrics, purpose and classification of the metrics, and the areas where metrics can be helpful to use. Detail description about some selected metrics (like Cyclomatic Complexity and Halstead metrics) is also a part of this report. Three core requirements of this thesis are: 1) <b>Measurement</b> of <b>code</b> complexity for the code written in C. 2) Measurement should perform automatically on the code base and on a regular basis for new code releases. 3) Run on Solaris. Some of the existing complexity measurement tools (open-source and commercial) are evaluated in this thesis. QA-C is an existing commercially available tool for the code complexity of C code. The tool implemented in this thesis uses QA-C as a foundation for analyzing C code on Solaris. Web interfaces are designed to present the results of <b>code</b> complexity <b>measurement.</b> Validerat; 20110525 (anonymous...|$|R
40|$|Information {{covering}} sensor cables, sensor installation, and {{sensor calibration}} for the XV- 15 aircraft number 1 is included. For each junction box (J-box) designation {{there is a}} schematic of the J-box disconnect harness instrumentation worksheets which show sensor location, and calibration data sheets for each sensor associated with that J-box. An index of <b>measurement</b> item <b>codes</b> to J-box locations is given in a table. Cross references are given...|$|R
40|$|The Accuracy and Coverage Evaluation (A. C. E.) Revision II {{estimates}} are {{measures of the}} net coverage of the Census 2000 household population. The A. C. E. Revision II estimation method relies on Dual System Estimates (DSEs) that incorporate corrections for measurement errors detected by the A. C. E. Revision II <b>measurement</b> <b>coding</b> operation and the Further Study of Person Duplication. The estimation method is designed to handle overlap of measurement error detected by both studies {{in order to avoid}} overcorrecting for measurement error. An additional and new feature of the estimation method is that the DSEs for adult males are adjusted for estimated correlation bias. This report provides net coverage estimates for Census 2000 based on the A. C. E. Revision II estimation methodology. Standard errors of {{estimates are}} in parentheses. Table 1 {{at the end of the}} executive summary gives percent net undercount estimates for major population groups. Here are some key findings. Statements of comparison are statistically significant at the " = 0. 10 level using a 2 -sided test...|$|R
40|$|The Center for Orbit Determination in Europe (CODE) is {{contributing}} {{as a global}} Analysis center to the International GNSS Service (IGS) since many years. The processing of GPS and GLONASS data is well established in CODE’s ultra-rapid, rapid, and final product lines. With {{the introduction of new}} signals for the established and new GNSS, new challenges and opportunities are arising for the GNSS data management and processing. The IGS started the Multi-GNSS-EXperiment (MGEX) in 2012 in order to gain first experience with the new data formats and to develop new strategies for making optimal use of these additional <b>measurements.</b> <b>CODE</b> has started to contribute to IGS MGEX with a consistent, rigorously combined triple-system orbit solution (GPS, GLONASS, and Galileo). SLR residuals for the computed Galileo satellite orbits are of the order of 10 cm. Furthermore CODE established a GPS and Galileo clock solution. A quality assessment shows that these experimental orbit and clock products allow even a Galileo-only precise point positioning (PPP) with accuracies on the decimeter- (static PPP) to meter-level (kinematic PPP) for selected stations...|$|R
40|$|For both {{static and}} {{kinematic}} relative positioning with GPS, {{the success of}} baseline resolution depends closely on the reliable fixing of integer ambiguities of carrier phase <b>measurements.</b> Low noise <b>code</b> correlated dual-frequency GPS receivers offer accurate pseudo-range measurements on both carrier frequencies. With {{the help of a}} combination of code and carrier phase measurements, wide-lane ambiguities can be easily determined and used for the improvement of ambiguity resolutions on both L 1 and L 2 frequencies. A carrier smoothing technique is utilized to eliminate the noise of <b>code</b> <b>measurements.</b> Baselines ranging from 1. 5 km to 48 km are used to test the proposed method. Department of Land Surveying and Geo-Informatic...|$|R
