5|90|Public
50|$|Most reader servers support posting, {{either through}} NNTP or a special inews program. When an article is posted, {{the process is}} much the same as when a transit server {{receives}} news, but with additional checks. For posting, the server will normally fill in <b>missing</b> <b>Path</b> and Message-ID lines and check the syntax of headers intended for human readers, such as From and Subject. If the article is posted to a moderated group, the server will attempt to mail it to the newsgroup moderator if the Approved header is absent. Additional identity checks and filters are also typically applied at this point.|$|E
40|$|Abstract. Prediction {{of extreme}} events {{is a highly}} {{important}} and challenging problem in science, engineering, finance, and many other areas. The observed extreme events in these areas are often associated with complex nonlinear dynamics with intermittent instability. However, {{due to lack of}} resolution or incomplete knowledge of the dynamics of nature these instabilities are typically hidden. To describe nature with hidden instability, a stochastic parameterized model is used as the low-order reduced model. Bayesian inference incorporating data augmentation, regarding the <b>missing</b> <b>path</b> of the hidden processes as the augmented variables, is adopted in a Markov chain Monte Carlo (MCMC) algorithm to estimate the parameters in this reduced model from the partially observed signal. Howerver, direct application of this algorithm leads to an extremely low acceptance rate of the <b>missing</b> <b>path.</b> To overcome this shortcoming, an efficient MCMC algorithm which includes a pre-estimation of hidden processes is developed. This algorithm greatly increases the acceptance rate and provides the low-order reduced model with a high skill in capturing the extreme events due to intermittency...|$|E
40|$|International audienceIn this paper, we {{implement}} {{and evaluate}} {{a system that}} predicts and tracks Internet path changes to maintain an up-to-date network topology. Based on empirical observations, we claim that monitors can enhance probing according to the likelihood of path changes. We design a simple predictor of path changes and show {{that it can be}} used to enhance probe targeting. Our path tracking system, called DTRACK, focuses probes on unstable paths and spreads probes over time to minimize the chances of <b>missing</b> <b>path</b> changes. Our evaluations of DTRACK with trace-driven simulations and with a prototype show that DTRACK can detect up to three times more path changes than traditional traceroute-based topology mapping techniques...|$|E
50|$|It {{requires}} no vectors, {{so it does}} not <b>miss</b> <b>paths.</b>|$|R
40|$|Abstract: In {{this study}} we uses an {{approach}} for increasing web performance by analyzing and predicting the user behavior from user access log, identifying the <b>missing</b> <b>paths</b> in the logs and completing the same using a web structure so as to enhance the accuracy of web usage mining. This paper uses a simulated example to compare {{the result of the}} Pre-fetched pages after completing the <b>missing</b> <b>paths</b> in the logs using web structure and if this paths are not completed. The result shows the accuracy of the predicted and pre-fetched pages has improved after completing the <b>missing</b> <b>paths</b> in the logs. This paper uses Markov Model 1 st order and 2 nd order to predict most frequently accessed pages...|$|R
40|$|Cache {{analysis}} plays a {{very important}} role in obtaining precise Worst Case Execution Time (WCET) estimates of programs for real-time systems. While Abstract Interpretation based approaches are almost universally used for cache analysis, they fail to take advantage of its unique requirement: {{it is not necessary to}} find the guaranteed cache behavior that holds across all executions of a program. We only need the cache behavior along one particular program path, which is the path with the maximum execution time. In this work, we introduce the concept of cache <b>miss</b> <b>paths,</b> which allows us to use the worst-case path information to improve the precision of AI-based cache analysis. We use Abstract Interpretation to determine the cache <b>miss</b> <b>paths,</b> and then integrate them in the IPET formulation. An added advantage is that this further allows us to use infeasible path information for cache analysis. Experimentally, our approach gives more precise WCETs as compared to AI-based cache analysis, and we also provide techniques to trade-off analysis time with precision to provide scalability...|$|R
40|$|Mutation {{analysis}} on model checking specifications {{is a recent}} development. This approach mutates a specification, then applies a model checker to compare the mutants with the original specification to automatically generate tests or evaluate coverage. The properties of specification mutation operators have not been explored in depth. We report our work on theoretical and empirical comparison of these operators. Our future plans include studying how {{the form of a}} specification influences the results, finding relations between different operators, and validating the method against independent metrics. Keywords: specification mutation, mutation operators, test generation, model checking. 1 Introduction Mutation analysis is typically performed on program code. However, a specification provides additional valuable information. For instance, specification-based testing may detect a <b>missing</b> <b>path</b> error [15], that is, a situation when an implementation neglects an aspect of a problem and a [...] ...|$|E
40|$|The {{design space}} {{exploration}} (DSE) phase {{is used to}} tune configurable system parameters and it generally consists of a multiobjective optimization (MOO) problem. It is usually done at pre-design phase and consists of the evaluation of large design spaces where each configuration requires long simulation. Several heuristic techniques have been proposed {{in the past and}} the recent trend is reducing the exploration time by using analytic prediction models to approximate the system metrics, effectively pruning sub-optimal configurations from the exploration scope. However, there is still a <b>missing</b> <b>path</b> towards the effective usage of the underlying computing resources used by the DSE process. In this work, we will show that an alternative and almost orthogonal approach - focused on exploiting the available parallelism in terms of computing resources - can be used to better schedule the simulations and to obtain a high speedup with respect to state of the art approaches, without compromising the accuracy of exploration results. Experimental results will be presented by dealing with the DSE problem of a shared memory multi-core system considering a variable number of available parallel resources to support the DSE phase...|$|E
30|$|Our {{investigation}} {{shows that}} these are the structural paths whose presence in a file is most indicative that the file is benign or alternatively, whose absence indicates that a file is malicious. For example, malicious files are not likely to contain metadata in order to minimize file size, they do not jump to a page in the document when it is opened and are not well-formed so they are <b>missing</b> <b>paths</b> such as /Type and /Pages/Count.|$|R
40|$|In {{the process}} of model modification, {{parameters}} of residual covariances are often treated as free parameters to improve model fit. However, the effect of such measurement model modifications on the important structural parameter estimates under various measurement model misspecifications has not been systematically studied. Monte Carlo simulation was conducted to compare structural estimates before and after measurement model modifications of adding residual covariances under varying sample sizes and model misspecifications. Results showed that researchers should pay attention when such measurement model modifications are made to initially misspecified model with <b>missing</b> <b>path(s)</b> ...|$|R
6000|$|... "I {{think we}} can keep on, sir. The nearer we get there the better; and if we should <b>miss</b> the <b>path</b> we can halt then and wait till daybreak." ...|$|R
5000|$|<b>Missing</b> return <b>path</b> {{amplifier}}. During {{the early}} days of the trial, few repair trucks carried return path capable amplifiers. One amplifier was replaced with a one way amplifier and improperly noted.|$|R
50|$|Hamad International Airport in Qatar will {{narrowly}} <b>miss</b> the annular <b>path.</b>|$|R
50|$|Thomas And The Sunday School OutingThomas has to {{take the}} Vicar's Sunday School on their outing into the {{mountains}} when Bertie breaks down. He nearly <b>misses</b> his <b>path</b> home when the Sunday school is late returning to Ffarquhar Station.|$|R
40|$|We {{present an}} {{approach}} to Bayesian model selection for finitely observed di#usion processes. We tackle this problem using data augmentation by treating the paths between observed points as missing data. For a fixed model formulation, the strong dependence between the <b>missing</b> <b>paths</b> and the volatility of the di#usion {{can be broken down}} by adopting the recently presented method of Roberts and Stramer (Biometrika, 2001). We describe how this method may be extended to the case of model selection via reversible jump MCMC. In addition we extend the formulation of a di#usion model to capture a potential non-Markov state dependence in the drift. Issues of appropriate choices of priors and e#cient trans-dimensional proposal distributions for the reversible jump algorithm are also addressed. The approach is illustrated using simulated data and an example from finance...|$|R
40|$|Quantifying path {{changes from}} {{multiple}} granularity levels is often required. However, existing methods all characterize path changes on each level independently, thus duplicating and/or <b>missing</b> certain <b>path</b> differences. In this paper, we propose DiffP, an integrative multi-level analysis {{to overcome this}} deficiency, and demonstrate its applications in analyzing path changes during an ISP transition...|$|R
40|$|Many {{contemporary}} multiple issue processors employ out-of-order scheduling hardware in {{the processor}} pipeline. Such scheduling hardware can yield good performance without relying on compile-time scheduling. The hardware can also schedule around unexpected run-time occurrences such as cache misses. As issue widths increase, {{the complexity of}} such scheduling hardware increases considerably and {{can have an impact}} on the cycle time of the processor. This paper presents the design of a multiple issue processor that uses an alternative approach called <b>miss</b> <b>path</b> scheduling. Scheduling hardware is removed from the processor pipeline altogether and placed on the path between the instruction cache and the next level of memory. Scheduling is performed at cache miss time, as instructions are received from memory. Scheduled blocks of instructions are issued to an aggressively clocked in-order execution core. Details of a hardware scheduler that can perform speculation are outlined and [...] ...|$|R
5000|$|Rewind the Crisis, Bad Case of Big Mouth, Cavalcade of the Odd, Stray {{from the}} <b>Path,</b> <b>Miss</b> May I, Cavalcade of the Odd, Whitechapel, The Wonder Years ...|$|R
5000|$|An unknown shooter hides under a wagon, {{and when}} Smith {{approaches}} his home, the shooter fires across the <b>path,</b> <b>missing</b> Smith but lodging {{a bullet in}} a cow.|$|R
6000|$|Sig. [...] Confusion! [...] Stand to your arms, {{and guard}} the door--all's lost [...] 230 [...] Unless that fearful bell be silenced soon. [...] The officer hath <b>missed</b> his <b>path</b> or purpose, [...] Or met some {{unforeseen}} and hideous obstacle,[ey] [...] Anselmo, with thy company proceed [...] Straight to the tower; the rest remain with me. [...] [Exit {{part of the}} Guard.|$|R
30|$|Since {{cost and}} time are tightly related, current {{techniques}} necessitate the preprocessing of collected huge data sets before analyzing the relevant remaining entries. Such preprocessing {{takes time and}} sometimes requires manual intervention in order to remove some irrelevant entries, and to complete the <b>missing</b> <b>paths.</b> Our system does not require such preprocessing, simply {{because it is not}} starting from huge data. The Base Server only receives relevant records and can work on them directly or as a background process. Moreover, several mining techniques that use server log files were limited to the collection of few-days of data in order to minimize the probability of privacy disclosure, and in order to handle the massive amount of data. Furthermore, previous server log file mining techniques first collect data from many major agents before applying the pre-processing on the whole data. Such a multi-step process costs time and money since some web servers do not give free access to their log files.|$|R
40|$|In {{this paper}} we {{develop a new}} {{approach}} for the inference of a broad class of non-linear continuous time models, when the data are observed at discrete time points. We employ a Bayesian approach for model estimation based on MCMC methods. We use the Hastings-Metropolis algorithm with collection of Brownian bridges and linear diffusion bridges as candidates for the independent sampler to obtain data augmentation algorithms for the <b>missing</b> <b>paths</b> between any two observations. We thus obtain algorithms which are independent of the sample intervals. KEYWORDS: diffusions, diffusion bridges, Bayesian estimation, data augmentation, MCMC 1 Introduction As is noted by Berliner (1991), an extremely important class of examples of discrete dynamical systems arises in the numerical solution of nonlinear stochastic differential equations. In many cases, the data y t 0; : : :; y t N are obtained from sampling an underlying continuoustime process at discrete times t 0; t 1; : : :; t N. These mod [...] ...|$|R
40|$|Worst Case Execution Time (WCET) is an {{important}} metric for programs running on real-time systems, and finding precise estimates of a program’s WCET is crucial to avoid over-allocation and wastage of hardware resources and to improve the schedulability of task sets. Hardware Caches {{have a major impact}} on a program’s execution time, and accurate estimation of a program’s cache behavior generally leads to significant reduction of its estimated WCET. However, the cache behavior of an access cannot be determined in isolation, since it depends on the access history, and in multi-path programs, the sequence of accesses made to the cache is not fixed. Hence, the same access can exhibit different cache behavior in different execution instances. This issue is further exacerbated in shared caches in a multi-core architecture, where interfering accesses from co-running programs on other cores can arrive at any time and modify the cache state. Further, cache analysis aimed towards WCET estimation should be provably safe, in that the estimated WCET should always exceed the actual execution time across all execution instances. Faced with such contradicting requirements, previous approaches to cache analysis try to find memory accesses in a program which are guaranteed to hit the cache, irrespective of the program input, or the interferences from other co-running programs in case of a shared cache. To do so, they find the worst-case cache behavior for every individual memory access, analyzing the program (and interferences to a shared cache) to find whether there are execution instances where an access can super a cache miss. However, this approach loses out in making more precise predictions of private cache behavior which can be safely used for WCET estimation, and is significantly imprecise for shared cache analysis, where it is often impossible to guarantee that an access always hits the cache. In this work, we take a fundamentally different approach to cache analysis, by (1) trying to find worst-case behavior of groups of cache accesses, and (2) trying to find the exact cache behavior in the worst-case program execution instance, which is the execution instance with the maximum execution time. For shared caches, we propose the Worst Case Interference Placement (WCIP) technique, which finds the worst-case timing of interfering accesses that would cause the maximum number of cache misses on the worst case execution path of the program. We first use Integer Linear Programming (ILP) to find an exact solution to the WCIP problem. However, this approach does not scale well for large programs, and so we investigate the WCIP problem in detail and prove that it is NP-Hard. In the process, we discover that the source of hardness of the WCIP problem lies in finding the worst case execution path which would exhibit the maximum execution time in the presence of interferences. We use this observation to propose an approximate algorithm for performing WCIP, which bypasses the hard problem of finding the worst case execution path by simply assuming that all cache accesses made by the program occur on a single path. This allows us to use a simple greedy algorithm to distribute the interfering accesses by choosing those cache accesses which could be most affected by interferences. The greedy algorithm also guarantees that the increase in WCET due to interferences is linear in the number of interferences. Experimentally, we show that WCIP provides substantial precision improvement in the final WCET over previous approaches to shared cache analysis, and the approximate algorithm almost matches the precision of the ILP-based approach, while being considerably faster. For private caches, we discover multiple scenarios where hit-miss predictions made by traditional Abstract Interpretation-based approaches are not sufficient to fully capture cache behavior for WCET estimation. We introduce the concept of cache <b>miss</b> <b>paths,</b> which are abstractions of program path along which an access can super a cache miss. We propose an ILP-based approach which uses cache <b>miss</b> <b>paths</b> to find the exact cache behavior in the worst-case execution instance of the program. However, the ILP-based approach needs information about the worst-case execution path to predict the cache behavior, and hence it is difficult to integrate it with other micro-architectural analysis. We then show that most of the precision improvement of the ILP-based approach can be recovered without any knowledge of the worst-case execution path, by a careful analysis of the cache <b>miss</b> <b>paths</b> themselves. In particular, we can use cache <b>miss</b> <b>paths</b> to find the worst-case behavior of groups of cache accesses. Further, we can find upper bounds on the maximum number of times that cache accesses inside loops can exhibit worst-case behavior. This results in a scalable, precise method for performing private cache analysis which can be easily integrated with other micro-architectural analysis...|$|R
5000|$|After {{crossing}} Waverley Rd, {{turn down}} Ivanhoe St {{and follow the}} path East to Winmalee Dr turn down Koonalda Ave and cross over Springvale Rd at the pedestrian lights (see [...] ). A road section at Whites Lane leads to a tennis club. Leave the road for the path at this point. The turn off {{is very easy to}} <b>miss.</b> The <b>path</b> continues to Belvedere Ave, Mackintosh Rd and Lum Rd.|$|R
40|$|Neglected conditions, also {{referred}} as <b>missing</b> <b>paths,</b> {{are known to}} be an important class of software defects. Revealing neglected conditions around individual API calls in an application requires the knowledge of programming rules that must be obeyed while reusing those APIs. To mine those implicit programming rules and hence to detect neglected conditions, we develop a novel framework, called NEGWeb, that substantially expands mining scope to billions of lines of open source code available on the web by leveraging a code search engine. We evaluated NEGWeb to detect violations of mined rules in local code bases or open source code bases. In our evaluation, we show that NEGWeb finds three real defects in Java code reported in the literature and also finds three previously unknown defects in a large-scale open source project called Columba (91, 508 lines of Java code) that reuses 541 classes and 2225 methods. We also report a high percentage of real rules among the top 25 reported patterns mined for APIs provided by five popular open source applications...|$|R
40|$|In {{this article}} we examine two {{relatively}} new MCMC methods which allow for Bayesian infer-ence in diffusion models. First, the Monte Carlo within Metropolis (MCWM) algorithm (O’Neil, Balding, Becker, Serola and Mollison, 2000) uses an importance sampling approximation for the likelihood and yields a limiting stationary distribution {{that can be made}} arbitrarily “close ” to the posterior distribution (MCWM is not a standard Metropolis-Hastings algorithm, however). The second method, described in Beaumont (2003) and generalized in Andrieu and Roberts (2009), introduces auxiliary variables and utilizes a standard Metropolis-Hastings algorithm on the enlarged space; this method preserves the original posterior distribution. When applied to diffusion models, this approach {{can be viewed as a}} generalization of the popular data augmen-tation schemes that sample jointly from the <b>missing</b> <b>paths</b> and the parameters of the diffusion volatility. We show that increasing the number of auxiliary variables dramatically increases the acceptance rates in the MCMC algorithm (compared to basic data augmentation schemes), allowing for rapid convergence and mixing. The efficacy of our approach is demonstrated in a simulation study of the Cox-Ingersoll-Ross (CIR) and Heston models, and is applied to two well known datasets...|$|R
40|$|Policymaking is {{a complex}} process {{influenced}} by a multitude of factors and effects which are not always transparent, and is executed by policy makers who do not always act rationally. Following the concept of evidence-based policymaking, science and research (S&R) should inform policy makers {{in such a way}} that the process of policymaking is rational, rather than opinion-based (Sutcliffe, Court 2006; Davies 2004). However, the existence of evidence alone is not a guarantee that it will inform policy makers, as shown by today’s realities in developing (as well as in developed) countries. Besides fulfilling quality standards such as credibility, problem specificity, solution orientation, and communicability, the given evidence has to be placed into the policy process which consequently, demands more action on the side of S&R institutions. Often, it lacks of formal or informal information channels and linkages between S&R institutions at national and international levels, and with other actors in the policy arena. Additionally, existing linkages are not fully used. This paper presents network analysis as a tool to identify relevant actors, and the existing or <b>missing</b> <b>paths</b> and channels among them. It uses a case study in Burkina Faso as an example and examines the reasons for success and failure in efforts for evidence-based policymaking. The analysis is based o...|$|R
40|$|The {{relevance}} of energy production on a biomethane-basis has continuously increased {{during the last}} decades. Therefore, the necessity for a reliable, efficient and flexible production of methane from organic substrate requires {{a better understanding of}} the complete anaerobe digestion process and its control. The currently available models are not covering all sub-processes. In addition, the parameters of these models are difficult to identify. Here, a new approach for an efficient and flexible control for a predictable and demand meeting production of biogas in a full scale plant is presented. In a first step a new, detailed metabolic pathway is drawn, showing all branches, intercellular intermediates, and products of metabolism. This is going to be aligned with the existing Anaerobic Digestion Model No. 1 (ADM 1). So, <b>missing</b> <b>paths</b> and dependencies can be added. In a second step, the lack of parameter knowledge is met by measurements from additional state of the art sensors in a full scale plant. Pattern recognition methods are then used on the large data stream in order to find correlations between the dataset entries and predict the expected biogas production. The obtained models can then be used to evaluate different control strategies making the biogas production more reliable and flexible...|$|R
40|$|Traditional Profile Guided Optimization (PGO) uses program {{instrumentation}} {{with one or}} more small training input data sets to generate edge or value profiles to guide compiler optimizations. This approach has been effective in predicting branch directions for many applications. However, for optimizations that are more dependent on the performance characteristics and the accuracy of the profiles, {{it is not clear whether}} profiles generated with small input data sets can reliably predict the program behavior under different input sets. We studied the frequent execution <b>paths,</b> data cache <b>miss</b> <b>paths,</b> IPC and the stall cycles breakdown of the test, train and different reference input sets of the SPEC 2000 Int benchmarks. Our studies indicate that small input sets are less effective in predicting the program behavior for larger input data sets. We propose to use Hardware Performance Monitor (HPM) sampling based profiles to guide optimizations, because it can work with larger input sets and gather information on important performance events. As a proof of concept, we have implemented one type of HPM sampling based PGO. We use the dynamic call path sampled by HPM to automatically guide procedure inlining in the ORC Compiler. Our results show that this approach has much lower profiling overhead, and offers significant performance gains. ...|$|R
60|$|After consultation, {{and with}} the aid of a road map, they were fairly well agreed as to direction, so were able to hire a taxi without more ado and drive out on the road leading to Treaddur Bay. They {{instructed}} the man to go slowly, and watched narrowly so as not to <b>miss</b> the <b>path.</b> They came to it not long after leaving the town, and Tommy stopped the car promptly, asked in a casual tone whether the path led down to the sea, and hearing it did paid off the man in handsome style.|$|R
40|$|The 1 : {{track model}} for fault {{tolerant}} 20 processor arrays is extended to 30 mesh architec-tures. Non-intersecting, continuous, straight and non-near <b>miss</b> compensation <b>paths</b> are considered. It is shown that when six directions in the 30 mesh are allowed for compensa-tion paths, then switches with 13 states {{are needed to}} preserve the 30 mesh topology after faults. It is also shown that switch reconfiguration after faults is local {{in the sense that}} the state of each switch is uniquely determined by the state of the 2 processors connected to it. ...|$|R
6000|$|... "Well," [...] said Friedmund, as if half ashamed, [...] "they were twin eaglets, {{and their}} mother had left them, and I felt as though I could not harm them; so I only bore off their provisions, and stuck some {{feathers}} in my cap. But {{by that time the}} sun was down, and soon I could not see my footing; and, when I found that I had <b>missed</b> the <b>path,</b> I thought I had best nestle in the nook where I was, and wait for day. I grieved for my mother's fear; but oh, to see her here!" ...|$|R
40|$|The {{bachelor}} thesis {{solves the}} plan of the bicycle path between Haviřov and Žermanice. The task was to plan the <b>missing</b> bicycle <b>path</b> that leads out of burdened communication. Part of the path runs along the road III/ 4735 at the moment, where is high intensity of bicycle and car traffic. The road traffic is currently in dangerous condition. The plan runs along the river Lučina and it is devided into two sections. Each section has several variants. One variant is chosen and elaborated in detail. The path was solved {{in accordance with the}} applicable standards...|$|R
30|$|Second, IC {{triangle}} rule: {{the case}} (b) from Figure  7 {{shows that it}} is possible for the routing protocol to <b>miss</b> the only <b>path</b> to the sink. The path could be not accessible from the boundary of the hole because of the crossing edges (Figure  6 b) and thus, a transmitted packet may fall into a routing loop.|$|R
60|$|She {{was of the}} world, worldly. It in no way disgusted {{her that}} Sir Lionel was an old rip, and that she knew him to be so. There were a great many old male rips at Littlebath and elsewhere. <b>Miss</b> Todd's <b>path</b> in life had brought her across {{more than one or}} two such. She {{encountered}} them without horror, welcomed them without shame, and spoke of them with a laugh rather than a shudder. Her idea was, that such a rip as Sir Lionel would best mend his manners by marriage; by marriage, but not with her. She knew better than trust herself to any Sir Lionel.|$|R
6000|$|... "I don't deny, sir, {{you were}} right as it has turned out; only I wouldn't have {{believed}} that I could have <b>missed</b> the <b>path,</b> and I did want {{to get close to}} the place before we were observed. I knew that we couldn't actually surprise them till morning; for the hut lies some distance in a bog, {{and there would be no}} crossing it unless we could see. Still if we could have got to the edge without the alarm being given, they would not have time to hide the things before we reached them. I have ridden across this place many a time after dark, and never missed my way." ...|$|R
