9|15|Public
50|$|While {{conventional}} {{systems of the}} era, including large mainframes, had <b>mean-time-between-failures</b> (MTBF) {{on the order of}} a few days, the NonStop system was designed to failure intervals 100 times longer, with uptimes measured in years. Nevertheless, the NonStop was designed to be price-competitive with {{conventional systems}}, with a simple 2-CPU system priced at just over twice that of a competing single-processor mainframe, as opposed to four or more times of other fault-tolerant solutions.|$|E
40|$|Presented in {{this thesis}} is the Airborne Countermeasures Ejection and/or Release (ALE) -XX Cockpit Control Unit (CCU) Countermeasures Dispensing System Network Controller (CMDSNC) design. ALE-XX CCU CMDSNC was {{designed}} {{as part of the}} total ALE-XX system to replace the problematic ALE- 40. Fiber optic technology is incorporated into ALE-XX as the communication medium to eliminate Electromagnetic Interference (EMI). ALE-XX CMDSNC uses a star network to solve system operation failures from the existing daisy-chain topology. A comprehensive Built-In-Test (BIT) allows fault diagnose and isolation of hardware problems reported on the CCU Visual Display. Digital electronics replace the electro-mechanical devices, lowers the number of Line Replaceable Units (LRUs), and raises the <b>Mean-Time-Between-Failures</b> (MTBF). The information contained herein {{could be used as a}} design aid for future CMDSNCs or other related instrumentation...|$|E
40|$|A second {{version of}} the Availability, Cost and Resource Allocation (ACARA) {{computer}} program has become available. The first version was reported in an earlier tech brief. To recapitulate: ACARA analyzes the availability, <b>mean-time-between-failures</b> of components, life-cycle costs, and scheduling of resources of a complex system of equipment. ACARA uses a statistical Monte Carlo method to simulate the failure and repair of components while complying with user-specified constraints on spare parts and resources. ACARA evaluates {{the performance of the}} system {{on the basis of a}} mathematical model developed from a block-diagram representation. The previous version utilized the MS-DOS operating system and could not be run by use of the most recent versions of the Windows operating system. The current version incorporates the algorithms of the previous version but is compatible with Windows and utilizes menus and a file-management approach typical of Windows-based software...|$|E
40|$|The {{intent of}} this paper is to discuss a {{reliability}} calculation technique, using Markov modeling of a parallel redundant system which can be repairable or non-repairable. In this paper, we will use Markov Modeling technique to provide the derivation for the mean-time-to-failure (non-repairable system) or <b>mean-time-between-failure</b> (repairable system) of a parallel redundant system, with different unit failure or repair rate, to evaluate the reliability and dependability of a parallel operative redundant system...|$|R
40|$|An {{analysis}} of the potential life of refurbished and restored bearings was performed. The sensitivity of 10 -percent life and <b>mean-time-between-failure</b> {{to the effects of}} cumulative fatigue damage and the amount of stressed volume removed in the restoration process were examined. A modified Lundberg-Palmgren theory was used to predict that the expected 10 -percent life of a restored bearing, which is dependent on the previous service time and the volume of material removed from the race surfaces, can be between 74 and 100 percent of the new bearing life. Using renewal theory, it is found that the mean time between failure ranged from 90 to 100 percent of that for a new bearing...|$|R
50|$|Testing is {{even more}} {{important}} for software than hardware. Even the best software development process results in some software faults that are nearly undetectable until tested. As with hardware, software is tested at several levels, starting with individual units, through integration and full-up system testing. Unlike hardware, it is inadvisable to skip levels of software testing. During all phases of testing, software faults are discovered, corrected, and re-tested. Reliability estimates are updated based on the fault density and other metrics. At a system level, <b>mean-time-between-failure</b> data can be collected and used to estimate reliability. Unlike hardware, performing exactly the same test on exactly the same software configuration does not provide increased statistical confidence. Instead, software reliability uses different metrics, such as code coverage.|$|R
40|$|Abstract—Random {{physical}} variations {{and noise}} are growing challenges for advanced electronic systems. Field programmable systems can, in principle, adapt to these phenomena, but two main problems must be addressed: how to efficiently characterize random variations {{and how to}} perform subsequent optimization. This paper addresses both of these questions. First, an approach to self-test is presented that uses on-chip noise emulation to quickly characterize some of the hidden variations in latches. Our noise-injection experiments demonstrate {{that there can be}} significant spreads in latch reliability even with current 65 nm field-programmable gate arrays (FPGAs). We detected coefficients of variation as high as 77 %. Second, we propose an approach to self-optimization using local resource swapping. Experiments on two FPGAs show improvements in <b>mean-time-between-failures</b> (MTBF) of up to 60 %. Keywords—self-adaptation; self-test; self-optimization; FPGAs; variations; transient faults; reconfigurable computing I...|$|E
40|$|The {{management}} {{of a population}} of centrifugal pumps in the petrochemical industry can be daunting. A popular method for strategically optimizing the maintenance and operation of these centrifugal pumps is to measure {{the reliability of the}} whole pump population in terms of <b>Mean-Time-Between-Failures</b> (MTBF). Too often MTBF improvement becomes the focus rather than a means of supporting the actual goal, namely Total Cost of Ownership (TCO) reduction. An increase in MTBF is intuitively associated with a reduction of TCO, but this is not necessarily true. This study proposes that the ‘MTBF vs time ’ and ‘TCO vs MTBF ’ curves are characteristics of a specific industrial plant. A tool for approximating the optimum TCO vs MTBF for a plant is introduced, and a qualitative framework for managing pump reliability in the TCO context is proposed. OPSOMMING Die bestuur van ’n sentrifugale pomppopulasie in die petrochemiese bedryf kan oorweldigend wees. ’n Populêre metode om die instandhouding en bedryf va...|$|E
40|$|The running {{times of}} many {{computational}} science applications are {{much longer than}} the <b>mean-time-between-failures</b> (MTBF) of current high-performance computing platforms. To run to completion, such applications must tolerate hardware failures. Checkpoint-and-restart (CPR) is {{the most commonly used}} scheme for accomplishing this- the state of the computation is saved periodically on stable storage, and when a hardware failure is detected, the computation is restarted from the most recently saved state. Most automatic CPR schemes in the literature can be classified as system-level checkpointing schemes because they take core-dump style snapshots of the computational state when all the processes are blocked at global barriers in the program. Unfortunately, a system that implements this style of checkpointing is tied to a particular platform amd cannot optimize the checkpointing process using application-specific knowledge. We are exploring an alternative called automatic applicationlevel checkpointing. In our approach, programs are transformed by a pre-processor so that they become self-checkpointing and self-restartable on any platform. In this paper, we evaluate a mechanism that utilizes application knowledge to minimize the amount of information saved in a checkpoint. 1...|$|E
50|$|A common {{reliability}} metric is {{the number}} of software faults, usually expressed as faults per thousand lines of code. This metric, along with software execution time, is key to most software reliability models and estimates. The theory is that the software reliability increases as the number of faults (or fault density) decreases or goes down. Establishing a direct connection between fault density and <b>mean-time-between-failure</b> is difficult, however, because of the way software faults are distributed in the code, their severity, and the probability of the combination of inputs necessary to encounter the fault. Nevertheless, fault density serves as a useful indicator for the reliability engineer. Other software metrics, such as complexity, are also used. This metric remains controversial, since changes in software development and verification practices can have dramatic impact on overall defect rates.|$|R
40|$|Extreme scale {{parallel}} computing systems will have {{tens of thousands}} of optionally accelerator-equiped nodes with hundreds of cores each, as well as deep memory hierarchies and complex interconnect topologies. Such Exascale systems will provide hardware parallelism at multiple levels and will be energy constrained. Their extreme scale and the rapidly deteriorating reliablity of their hardware components means that Exascale systems will exhibit low <b>mean-time-between-failure</b> values. Furthermore, existing programming models already require heroic programming and optimisation efforts to achieve high efficiency on current supercomputers. Invariably, these efforts are platform-specific and non-portable. In this paper we will explore the shortcomings of existing programming models and runtime systems for large scale computing systems. We then propose and discuss important features of programming paradigms and runtime system to deal with large scale computing systems with a special focus on data-intensive applications and resilience. Finally, we also discuss code sustainability issues and propose several software metrics that are of paramount importance for code development for large scale computing systems...|$|R
2500|$|Beginning in 1970, Texas Instruments and Lockheed Air Service {{worked to}} adapt the {{existing}} AN/APQ-122 Adverse Weather Aerial Delivery System (AWADS) with terrain following/terrain avoidance modes to replace the original APQ-115, which suffered throughout its life with an unacceptably adverse <b>mean-time-between-failure</b> (MTBF) rate. In 1970 they succeeded, and coupled the APQ-122 with the Litton LN-15J Inertial Navigation System (INS). Known as MOD-70, the modified radar was installed in all 12 operational Combat Talons and the four Heavy Chain test beds between 1971-1973. The system proved so successful that it continued in service until the late 1980s. Following the completion of MOD-70, the Combat Talons were divided into three designations: C-130E(CT) for the [...] "Clamp" [...] aircraft, C-130E(Y) for the [...] "Yank" [...] (formerly [...] "Yard") Talons, and C-130E(S) for the [...] "Swap". The Combat Talon I designations were consolidated in 1977 as the MC-130 and have remained under that designation since. The Combat Talon became the Combat Talon I in 1984 with the authorization for the modification of 24 C-130Hs to Combat Talon II specifications.|$|R
40|$|Abstract The {{continuous}} {{progress in}} the performance of supercomputers has made possible the understanding of many fundamental problems in science. Simulation, the third scientific pillar, constantly demands more powerful machines to use algorithms that would otherwise be unviable. That will inevitably lead to the deployment of an exascale machine during the next decade. However, fault tolerance is a major challenge that has to be overcome to make such a machine usable. With an unprecedented number of parts, machines at extreme scale will have a small <b>mean-time-between-failures.</b> The popular checkpoint/restart mechanism used in today’s machines may not be effective at that scale. One promising way to revamp checkpoint/restart is to use message-logging techniques. By storing messages during execu-tion and replaying them in case of a failure, message logging is able to shorten recovery time and save a substantial amount of energy. The downside of message logging is that memory footprint may grow to unsustainable levels. This paper presents a technique that decreases the memory pressure in message-logging protocols by only storing the necessary messages in collective-communication operations. We introduce CAMEL, a protocol that has a low memory overhead for multicast and reduction operations. Our results show that CAMEL can reduce memory footprint in a molecular dynamics benchmark for more than 95 % on 16, 384 cores. Keywords fault tolerance · resilience · message logging · collective-communication operation...|$|E
40|$|Purpose - The {{reliability}} and maintainability of tunnel infrastructure and systems {{is an important}} factor in assuring normal operation of a tunnel. Evaluating availability of a large-scale tunnel that includes civil, electrical, mechanical and electronic systems is a difficult task. The purpose of this paper is to present a methodology for performing such assessments, featuring the use of the Markov model. Design/methodology/approach - The methodology involves application of failure mode, effect and criticality analysis (FMECA), state space diagram construction, formulation of state space equations, and development of transitional matrices. It also involves transformation of multi-state models into two-state models (each comprises of an "up" state and a "down" state) through the use of the frequency and duration method for determining the failure and repair rates, as well as the <b>mean-time-between-failures</b> (MTBF) of the entire tunnel. By using the proposed bottom-up approach, a MTBF tree linking the availability measures of individual equipment with those of sub-systems, and ultimately the whole tunnel can be developed. Findings - The tunnel availability measures obtained by this analysis can be used in making comparisons between different tunnel designs so as to determine the value for money of various options. Furthermore, weaknesses in a tunnel design can be identified in the analysis. The information obtained from this method can also be used to evaluate adequacy, security and maintainability of a tunnel. Practical implications - The {{reliability and}} maintainability of tunnel infrastructure and systems are crucial factors for ensuring safety of tunnel operation. Unsafe conditions will cause closure of a tunnel. Efforts to improve availability of a tunnel often increase the tunnel's construction cost. Due to the complexity of tunnel systems, it is difficult to compare different tunnel designs, and trade-off analyses to strike a balance between target availability and construction cost of a tunnel design are seldom performed. This paper presents a systematic methodology to address these issues. This methodology allows tunnel management to evaluate the adequacy, security and maintainability of a tunnel so that design weaknesses can be identified and the value of design improvements can be determined. The methodology can also be used to evaluate designs of other complex systems such as power generation or petrochemical processing plants. Originality/value - A worked example demonstrating the application of the proposed methodology is presented in this paper. Department of Electrical EngineeringDepartment of Industrial and Systems Engineerin...|$|E
40|$|Abstract—Clusters {{featuring}} the InfiniBand interconnect {{are continuing to}} scale. As an example, the “Ranger” system at the Texas Advanced Computing Center (TACC) includes over 60, 000 cores with nearly 4, 000 InfiniBand ports. The latest Top 500 list shows 30 % of systems and over 50 % of the top 100 are now using InfiniBand as the compute node interconnect. As these systems continue to scale, the <b>Mean-Time-Between-Failure</b> (MTBF) is reducing and additional resiliency must be provided to the important components of HPC systems, including the MPI library. In this paper we present a design that leverages the reliability semantics of InfiniBand, but provides a higherlevel of resiliency. We are able to avoid aborting jobs {{in the case of}} network failures as well as failures on the endpoints in the InfiniBand Host Channel Adapters (HCA). We propose reliability designs for rendezvous designs using both Remote DMA (RDMA) read and write operations. We implement a prototype of our design and show that performance is near-identical to that of a non-resilient design. This shows that we can have both the performance and the network reliability needed for large-scale systems. I...|$|R
40|$|Work {{performed}} by the McDonnell Douglas Helicopter Company and Lucas Western, Inc. within the U. S. Army/NASA Advanced Rotorcraft Transmission (ART) Program is summarized. The design of a 5000 horsepower transmission for a next generation advanced attack helicopter is described. Government goals for the program were to define technology and detail design the ART to meet, as a minimum, a weight reduction of 25 percent, an internal noise reduction of 10 dB plus a mean-time-between-removal (MTBR) of 5000 hours compared to a state-of-the-art baseline transmission. The split-torque transmission developed using face gears achieved a 40 percent weight reduction, a 9. 6 dB noise reduction and a 5270 hour MTBR in meeting or exceeding the above goals. Aircraft mission performance and cost improvements resulting from installation of the ART would include a 17 to 22 percent improvement in loss-exchange ratio during combat, a 22 percent improvement in <b>mean-time-between-failure,</b> a transmission acquisition cost savings of 23 percent of $ 165 K, per unit, and an average transmission direct operating cost savings of 33 percent, or $ 24 K per flight hour. Face gear tests performed successfully at NASA Lewis are summarized. Also, program results of advanced material tooth scoring tests, single tooth bending tests, Charpy impact energy tests, compact tension fracture toughness tests and tensile strength tests are summarized...|$|R
40|$|High {{availability}} is an ever-changing area in networking. It is a {{very broad}} area with different definition depending on who you ask. Cisco high availability model {{is designed to help}} companies determine the correct path to reduce <b>Mean-Time-Between-Failure</b> and Mean-Time-To-Repair. There are several paths one can take depending on current limitations and business goals. Since best practices can change depending on new technologies or prerequisites, designing the network for high availability is a continuous process. The purpose of this research is to find the key factors in high availability and build a model around these to be used for evaluation around high availability. There are several other models for implementing availability these are created and directed towards specific vendors leaving an independent model of this lacking. The method used in this project is very similar to the top-down-approach. The model is designed around the method and was used to evaluate the network at Stora Enso pulp mill AB in Skutskär. The model has the ability to define the enterprise availability requirements and availability goals. Using these the model will provide the enterprise with the tools to evaluate if the enterprise reach their availability goals. After using the model limitations on maintaining a high availability network has been detected. It does however succeed in evaluating the enterprise availability. The model was designed to work independently of enterprise however it has only been tested at Stora Enso...|$|R
40|$|This {{handbook}} is {{for guidance}} only. not cite this document as a requirement. J: ?V h<;’S 1 ti DISTR 1 [3 (’” 1 ’JONST, 4 TIih’l ?W A,iiiL,’i iiiLi Approved for public release. Distribution is unlimifcdI I’htshandbook]Sapproved fbr use by aHDepartments ml Agencies of the Department of Defense. 2, This handbook is for guidance only This handbook cannot he {{cited as a}} requirement If it is, the contractor {{does not have to}} comply 3. MIL-HDBK- 781 contains test methods, test plans, and environmental profile data presented in a manner which facilitates their use with tailorable tasks when appropriate, 4. The testing of equipment procured for new military systems is an increasingly complex process Test methods, test plans, and test environments must be seiected wh]ch will ensure that contractually required reliability levels are attained in the field and early defect failures are removed prior to field deployment NIIL-HDBK- 781 provides a menu of test plans. test methods, and environmental profiles The most appropriate material may be selected for each program and incorporated into the tailored reliability test program 5, The handbook sections on reliability test methods and test plans present methods tor grownh momtonng. environmental stress screening, <b>mean-time-between-failure</b> assurance testing. sequential tests, fixed-duration tests, and aH-equipment tests. including a durabilitvjeconomic Life ‘rest The sections on test environmental profiles provide typical test environments for fixed-dt. wwnd equipment, mchi!c ground, shipboard, jet aircrafl, turbop up ad INJJGopieI,and missiles and assembied external stores equipment The references provided will expand the user’s knowledge and aid in the design and implementation of reliability test programs through more detailed data 6. Beneficial comments (recommendations, additions, deletions) and any pertinent data which may be of use in improving this document should be addressed to Commander. Space an...|$|R
40|$|With rapid {{technology}} scaling, flip-flops {{are becoming}} more susceptible to metastability due to tighter timing budgets and the more prominent effects of process, temperature, and voltage variation that can result in frequent setup and hold time violations. This thesis presents a detailed methodology and analysis {{on the design of}} metastable-hardened, high-performance, and low-power flip-flops. The design of metastable-hardened flip-flops is focused on optimizing the value of τ mainly due to its exponential relationship with the metastability window δ and the <b>mean-time-between-failure</b> (MTBF). Through small-signal modeling, τ is determined to be a function of the load capacitance and the transconductance in the cross-coupled inverter pair for a given flip-flop architecture. In most cases, the reduction of τ comes at the expense of increased delay and power. Hence, two new design metrics, the metastability-delay-product (MDP) and the metastability-power-delay-product (MPDP), are proposed to analyze the tradeoffs between delay, power and τ. Post-layout simulation results have shown that the proposed optimum MPDP design can reduce the metastability window δ by at least an order of magnitude depending on the value of the settling time and the flip-flop architecture. In this work, we have proposed two new flip-flop designs: the pre-discharge flip-flop (PDFF) and the sense-amplifier-transmission-gate (SATG) based flip-flop. Both flip-flop architectures facilitate the usage in both single and dual-supply systems as reduced clock-swing flip-flop and level-converting flip-flop. With a cross-coupled inverter in the master-stage that increases the overall transconductance and a small load transistor associated with the critical node, the architecture of both the PDFF and the SATG is very attractive for the design of metastable-hardened, high-performance, and low-power flip-flops. The amount of overhead in delay, power, and area is all less than 10 % under the optimum MPDP design scheme when compared to the traditional optimum PDP design. In designing for metastable-hardened and soft-error tolerant flip-flops, the main methodology is to improve the metastability performance in the master-stage while applying the soft-error tolerant cell in the slave-stage for protection against soft-error. The proposed flip-flops, PDFF-SE and SATG-SE, both utilize a cross-coupled inverter on the critical path in the master-stage and generate the required differential signals to facilitate the usage of the Quatro soft-error tolerant cell in the slave-stage...|$|R
40|$|Globalization and {{the advent}} of major {{industrial}} disasters have highlighted the need for capital intensive industries to remain competitive and profitable. In order to achieve this, companies have had to focus on mitigating operational risk. Two aspects of operational risk management involve assets and process. Both of these have particular importance to capital intensive industries. The mining industry still {{plays an important role in}} the global and national economy. It is considered a capital intensive industry and is faced with a number of challenges. It is of particular importance to this industry that operational risk be mitigated. The Time-in-State Metric (TISM) has been designed as a tool to mitigate operational risk, by means of improving process and asset performance. The TISM presents an overarching metric that delivers a method to manage the process and equipment pro-actively at systems level because the metric is generated in real time, is focused in key operating and equipment parameters, and generates a consistent response recommendation that aligns the operating philosophy of the operational personnel. The TISM has been successfully developed and implemented at Nkomati Mine, a nickel producing operation in the Mpumalanga Province of South Africa. This will be utilised as a case study to understand the development and implementation process and to evaluate the impact the TISM has had on the process and asset performance of the operation. Nkomati Mine has seen a significant improvement in its asset performance, with improvements in availability, utilization, reliability, Overall Equipment Effectiveness (OEE) <b>Mean-Time-Between-Failure</b> (MTBF), Mean-Time-to-Restore (MTTR) and the number of failure incidents. Nkomati Mine has also seen a significant improvement in the process performance. Directly after implementation of the TISM there has been an improvement in feedrate and recovery of nickel. Over the long term, Nkomati Mine has seen improvement in all major performance parameters, including throughput, feedrate, recovery and final concentrate grade. The TISM has been proven to have a positive impact on the bottom-line of the organization (reduced maintenance cost, improved production, reduced off-mine cost, etc.) and to mitigate Operational Risk. Mini-dissertation (MBA) [...] University of Pretoria, 2016. sn 2016 Gordon Institute of Business Science (GIBS) MBAUnrestricte...|$|R
40|$|This {{research}} {{describes the}} development of an experimental radiation testing environment to investigate the single event effect (SEE) susceptibility of the 486 -DX 4 microprocessor. SEE effects are caused by radiation particles that disrupt the logic state of an operating semiconductor, and include single event upsets (SEU) and single event latchup (SEL). The relevance of this work can be applied directly to digital devices that are used in spaceflight computer systems. The 486 -DX 4 is a powerful commercial microprocessor that is currently under consideration for use in several spaceflight systems. As part of its selection process, it must be rigorously tested to determine its overall reliability in the space environment, including its radiation susceptibility. The goal of this research is to experimentally test and characterize the single event effects of the 486 -DX 4 microprocessor using a cyclotron facility as the fault-injection source. The test philosophy is to focus on the "operational susceptibility," by executing real software and monitoring for errors while the device is under irradiation. This research encompasses both experimental and analytical techniques, and yields a characterization of the 486 -DX 4 's behavior for different operating modes. Additionally, the test methodology can accommodate a wide range of digital devices, such as microprocessors, microcontrollers, ASICS, and memory modules, for future testing. The goals were achieved by testing with three heavy-ion species to provide different linear energy transfer rates, and a total of six microprocessor parts were tested from two different vendors. A consistent set of error modes were identified that indicate {{the manner in which the}} errors were detected in the processor. The upset cross-section curves were calculated for each error mode, and the SEU threshold and saturation levels were identified for each processor. Results show a distinct difference in the upset rate for different configurations of the on-chip cache, as well as proving that one vendor is superior to the other in terms of latchup susceptibility. Results from this testing were also used to provide a <b>mean-time-between-failure</b> estimate of the 486 -DX 4 operating in the radiation environment for the International Space Station...|$|R
40|$|Addressing {{the urgent}} need to develop LCOE {{competitive}} renewable energy solutions for US energy security and to replace fossil-fuel generation with the associated benefits to environment impacts including a reduction in CO 2 emissions, this Project focused on the advantages of using hydraulic energy transfer (HET) in large-scale Marine Hydrokinetic (MHK) systems for harvesting off-shore tidal energy in US waters. A recent DOE resource assessment, identifies water power resources have a potential to meet 15 % of the US electric supply by 2030, with MHK technologies being a major component. The work covered a TRL- 4 laboratory proof-in-concept demonstration plus modeling of a 15 MW full scale system based on an approach patented by NASA-JPL, in which submerged high-ratio gearboxes and electrical generators in conventional MHK turbine systems are replaced by a submerged hydraulic radial pump coupled to on-shore hydraulic motors driving a generator. The advantages are; first, the <b>mean-time-between-failure</b> (MTBF), or maintenance, can be extended from approximately 1 to 5 years and second, the range of tidal flow speeds which can be efficiently harvested can be extended beyond that of a conventional submerged generator. The approach uses scalable, commercial-off-the-shelf (COTS) components, facilitating scale-up and commercialization. All {{the objectives of the}} Project have been successfully met (1) A TRL 4 system was designed, constructed and tested. It simulates a tidal energy turbine, with a 2 -m diameter blade in up to a 2. 9 m/sec flow. The system consists of a drive motor assembly providing appropriate torque and RPM, attached to a radial piston pump. The pump circulates pressurized, environmentally-friendly, HEES hydraulic fluid in a closed loop to an axial piston motor which drives an electrical generator, with a resistive load. The performance of the components, subsystems and system were evaluated during simulated tidal cycles. The pump is contained in a tank for immersion testing. The COTS pump and motor were selected to scale to MW size and were oversized for the TRL- 4 demonstration, operating at only 1 - 6 % of rated values. Nevertheless, in for 2 - 18 kW drive power, in agreement with manufacturer performance data, we measured efficiencies of 85 - 90 % and 75 - 80 % for the pump and motor, respectively. These efficiencies being 95 - 96 % at higher operating powers. (2) Two follow-on paths were identified. In both cases conventional turbine systems can be modified, replacing existing gear box and generator with a hydraulic pump and on-shore components. On a conventional path, a TRL 5 / 6 15 kW turbine system can be engineered and tested on a barge at an existing site in Maine. Alternatively, on an accelerated path, a TRL- 8 100 kW system can be engineered and tested by modifying a team member's existing MHK turbines, with barge and grid-connected test sites in-place. On both paths the work can be expedited and cost effective by reusing TRL- 4 components, modifying existing turbines and using established test sites. (3) Sizing, performance modeling and costing of a scaled 15 MW system, suitable for operation in Maine's Western Passage, was performed. COTS components are identified and the performance projections are favorable. The estimated LCOE is comparable to wind generation with peak production at high demand times. (4) We determined that a similar HET approach can be extended to on-shore and off-shore wind turbine systems. These are very large energy resources which can be addressed in parallel for even great National benefit. (5) Preliminary results on this project were presented at two International Conferences on renewable energy in 2012, providing a timely dissemination of information. We have thus demonstrated a proof-in-concept of a novel, tidal HET system that eliminates all submerged gears and electronics to improve reliability. Hydraulic pump efficiencies of 90 % have been confirmed in simulated tidal flows between 1 and 3 m/s, and at only 1 - 6 % of rated power. Total system efficiencies have also been modeled, up to MW-scale, for tidal, and wind, systems. Projected efficiencies are between 81 % (full rated flow) and 86 % (1 / 3 rated flow). This high efficiency in a wide operating range compares favorably with conventional systems having a performance range of 87 % (full rated flow) to 0 % (1 / 3 rated flow) efficiency. An accelerated path to commercialization is identified, leveraging conventional MHK system technology and COTS components to meet {{the urgent need}} for renewable energy generation...|$|R

