44|0|Public
40|$|The Java {{language}} {{and its associated}} libraries and environment provide a powerful and flexible platform for programming computer clusters. Java tools and technologies enable experimentation in both management aspects as well as performance aspects of cluster systems. We discuss the current interesting problems in cluster computing including those derived from distributed computing {{as well as the}} more performance-related ones derived from the discipline of parallel computing. We describe our experiments in building <b>meta-computing</b> management software in Java and message-passing high performance support software also written in pure Java. Our DISCWorld <b>meta-computing</b> system allows cluster applications to be embedded as services in a problem-solving environment. Our JUMP message-passing system allows users to develop conventional stand-alone Java applications that use the message-passing model of parallelism on cluster systems. JUMP applications are also compatible with DISCWorld and can b [...] ...|$|E
40|$|Task {{migration}} {{is an effective}} strategy to achieve load balancing and high resource utilization in a <b>meta-computing</b> environment. In this paper, we argue that traditional task migration methods take into account only the computation, and neglect any necessary data migration. Accordingly, we introduce a new definition of a task, propose a novel dynamic task migration scheme, and evaluate this scheme {{within the context of}} the JIAJIA software DSM system. The results of our experiments show that the execution time of three benchmarks improved between 36 % to 50 %, compared with other static task allocation schemes. Also, these benchmarks performed with our new migration scheme an average of 30 % better than with traditional task migration method. Furthermore, higher utilization is achieved with the new task migration scheme. 1 Introduction The four substantial characteristics of <b>meta-computing</b> environments, namely heterogeneity, variable resource capacity, distribution, and non-dedicated nat [...] ...|$|E
40|$|In this paper, {{we present}} a {{performance}} model and the experimental results of parallel execution of Java Remote Method Invocation (RMI). The Java RMI is used for remote execution of parallel jobs on the Internet. The main use of the model {{is to determine the}} job size of the distributed objects that are suitable for the metacomputing on the Internet. 1 INTRODUCTION The advent of mobile programming languages [1] such as Java [2] opens the entire Internet for parallel computing. While the parallel computing on clusters of workstations (COWs) also uses general network protocols such as TCP/IP for interprocessor communication, there is a fundamental difference: the processors of a COW share the same (networked) file system, but the computers on the Internet don't. The parallel computing on the Internet is also known as <b>meta-computing.</b> The idea of <b>meta-computing</b> is to explore the huge aggregate computing power of millions of computers on the Internet for large computational problems. With [...] ...|$|E
40|$|Abstract. We {{discuss the}} use of PVM as a system that {{supports}} General Process Management for DCEs. This system allows PVM to initialise MPI and other <b>meta-computing</b> systems. The impetus for such a system {{has come from the}} PVMPI project which required complex taskers and resource managers to be constructed. Such development is normally too time consuming for PVM users, due to the in-depth knowledge of PVM's internals, which are required to facilitate such systems correctly and reliably. This project examines contemporary systems such asvarious MPIRUN systems and other general schedulers, and compares their requirements to the capabilities of the current PVM system. Current PVM internal operations are explained, and an experimental system based on the experience gained from the PVMPI project is demonstrated. Performance of some standardised plug-in allocation schemes that will be distributed with the new PVM 3. 4 release are then demonstrated. It is hoped that this project will provide users of dynamic <b>meta-computing</b> environments the controlled &quot; exibility that has previously been di cult to achieve without the user having to rely upon external third party scheduler and job control systems. ...|$|E
40|$|This paper defines meta-applications as large, related {{collections}} of computational tasks, designed {{to achieve a}} specific overall result, running on a (possibly geographically) distributed, non-dedicated <b>meta-computing</b> platform. To carry out such applications in an industrial context, one requires resource management and job scheduling facilities (including capacity planning), {{to ensure that the}} application is feasible using the available resources, that each component job will be sent to an appropriate resource, and that everything will finish before the computing resources are needed for other purposes...|$|E
40|$|Evolving <b>meta-computing</b> {{middleware}} like Globus or Legion {{provides for}} the creation of computational grids that can incorporate widely distributed resources. The services these new technologies provide are mainly accessible to the user by command-line utilities or through specialized applications which differ in usage and appearance depending on the problem domain and the underlying grid architecture. For the computational grids to become versatile and easy to use a unified user interface is needed to compose and manipulate complex “meta-programs ” spanning multiple resources on possibly differing grid architectures and grid software. Symphony was developed as a Java-based composition and manipulation framework for distributed legacy resources and is currently being extended to interface with major <b>meta-computing</b> systems. It can be pictured as an abstraction layer above the different grid architectures and their middleware to provide a unified interface to the user. 1 The Symphony Framework The description of a sequence of operations to be performed on a computational grid {{can be viewed as a}} metaprogram. The term “meta ” is used because the individual operations (e. g. execute a program, move a file) are of a higher order when compared to the operations in normal programming. A meta-program can be represented b...|$|E
40|$|An {{extension}} of the corresponding states approach of Pitzer and of Lee and Kesler (Pitzer-Lee-Kesler (PLK) strategy) {{based on the work}} of Teja et al. (PLKT strategy) provides a clear pedagogical setting for describing the underlying basis of the PLK strategy itself and also its extension to families of non-normal fluids. Application of the strategy to two families of nonnormal fluids is illustrated. Furthermore, the PLKT implementation using <b>meta-computing</b> software provides a convenient tool to perform quantitatively accurate calculations while simultaneously emphasizing the thermodynamic problem structure. Procedures and an example are given for illustration...|$|E
40|$|Modern {{middleware}} has {{roots in}} object-based distributed programming models[4] and actors [1] {{developed in the}} mid- 1980 s. Later, the need for component integration and interoperability across different operat-ing systems motivated development of standardized interface definitions, protocols, and architectures, including the Object Management Group’s CORBA. More recently, Java and its related technologies, includ-ing JavaBeans, have provided {{a new set of}} powerful facilities for distributed programming using remote method invocation, code mobility, dynamic class loading, object serializa-tion, and reflection. Yet another path to middleware development is motivated by <b>meta-computing</b> executing parallel pro-grams on networked computers; results include Globus and other WAN-base...|$|E
40|$|Recent {{trends in}} hardware, in {{particular}} in interconnection technologies, have paved {{the way to the}} exploitation of heterogeneous, distributed computing platforms for advanced scientific applications. This infrastructure enables the building of meta-applications that are composed of several modules which may be implemented in different languages, exploit heterogeneous platforms, and employ several forms of parallelism. However, software development has not kept pace with the advances in hardware. Despite the existence of high level parallel languages and <b>meta-computing</b> frameworks there is, for the time being, a lack of a unified framework that efficiently deals with heterogeneity, interoperability, and hybrid parallelism at a high level...|$|E
40|$|Performance Engineering is {{concerned}} with the reliable prediction and estimation of the performance of scientific and engineering applications on a variety of parallel and distributed hardware. This paper reviews the present state of the art in ‘Performance Engineering ’ for both parallel computing and <b>meta-computing</b> environments and attempts to look forward to the application of these techniques in the wider context of Problem Solving Environments and the Grid. The paper compares various techniques such as benchmarking, performance measurements, analytical modelling and simulation, and highlights the lessons learned in the related projects. The paper concludes with a discussion of the challenges of extending such methodologies to computational Grid environments. 1 1...|$|E
40|$|Abstract- We {{discuss a}} set of novel {{techniques}} we are de-veloping, which build on standard tools, to make distributed computing for large-scale simulations across multiple ma-chines (even scattered across different continents) a real-ity. With these techniques we demonstrate here {{that we are able}} to scale a tightly coupled scientific application in <b>meta-computing</b> environments. Such research and development in metacomputing will lead the way to'routine, straightfor-ward and efflcient use of distributed computing resources anywhere around the world. This work applies not only to the large-scale simulations in astrophysics which provide the motivation for this work, but also opens the way for new, innovative application scenarios...|$|E
40|$|The {{goal of the}} OURAGAN {{project is}} to provide access of <b>meta-computing</b> {{resources}} to Scilab users. We present here an approach that consists, given a Scilab script, in scheduling and executing this script on an heterogeneous cluster of machines. One {{of the most effective}} scheduling technique is called clustering which consists in grouping tasks on virtual processors (clusters) and then mapping clusters onto real processors. In this paper, we study and apply the clustering technique for heterogeneous systems. We present a clustering algorithm called triplet, study its performance and compare it to the HEFT algorithm. We show that triplet has good characteristics and outperforms HEFT in most of the cases. ...|$|E
40|$|Colloque avec actes et comité de lecture. internationale. International audienceThe {{goal of the}} OURAGAN {{project is}} to provide access of <b>meta-computing</b> {{resources}} to Scilab users. We present here an approach that consists, given a Scilab script, in scheduling and executing this script on an heterogeneous cluster of machines. One {{of the most effective}} scheduling technique is called clustering which consists in grouping tasks on virtual processors (clusters) and then mapping clusters onto real processors. In this paper, we study and apply the clustering technique for heterogeneous systems. We present a clustering algorithm called triplet, study its performance and compare it to the HEFT algorithm. We show that triplet has good characteristics and outperforms HEFT in most of the cases...|$|E
40|$|Due to {{heterogeneity}} {{in modern}} computing systems, methodological and technological problems arise {{for the development}} of parallel applications. Heterogeneity occurs for instance at architecture level, when processing units use different data representation or exhibit various performances. At interconnection level, heterogeneity {{can be found in the}} programming interfaces and in the communication performances. In this paper, we focus on problems related to heterogeneity in the frame of interconnected clusters of workstations. In order to help the programmer to overcome these problems, we propose an extension to the multi-threaded programming environment PM². It facilitates the development of efficient parallel applications on such systems. The implementation of this extension is then described, the mechanisms involved are fundamental for an environment for <b>meta-computing...</b>|$|E
40|$|Emerging {{telemedicine}} applications {{require the}} ability to exploit diverse, geographi-cally distributed resources. These applications use high-speed networks to integrate supercomputers, large databases, archival storage devices, advanced visualization de-vices, and/or sophisticated instruments. This form of networked virtual supercomput-ers {{is also known as}} metacomputers and is beeing used by many other scientic applica-tions areas. In this article, we analyze requirements necessary for a telemedicine com-puting infrastructure and compare them with requirements found in a typical <b>meta-computing</b> environment. We will show that metacomputing environments can be used to enable a more powerful and unied computational infrastructure for telemedicine. The Globus metacomputing environment can provide the necessary basis to enable a large scale telemdical infrastructure. Globus is designed in a modular fashion and ca...|$|E
40|$|Abstract: In this article, {{the design}} of a global control {{architecture}} for teams of soccer playing robots is presented. Therefore, the metaphor of a ”virtual robot ” is introduced. This <b>meta-computing</b> based concept of a virtual robot- a collection of a dynamically varying number of robots- is used in order to offer a simple method to share sensor information and processing power efficiently in a team of autonomous walking robots, additionally opening the possibility of ”call-for-action ” requests. Sensor fusion allows for the calculation of a coherent global world model which in turn will be processed locally to get optimal action sequences of the robots in the team. Coordination of the robots is achieved by local schedulers which use the global world model as a basis for action selection...|$|E
40|$|Applications have {{to chose}} between the slow TCP and the {{unreliable}} UDP. There {{is now an}} alternative: the Variable Reliability Protocol. The applications can specify what are the allowable losses in the data, and the protocols guarantees that the given loss tolerance parameters will be respected. It should result in a faster throughput over WAN which have typically a higher loss rate. VRP was proposed by Robin Kravets from the GaTech. In this paper we describe how it has been improved, implemented, and integrated to Nexus, the communication library of the <b>meta-computing</b> environment Globus. Contents 1 Introduction 5 1. 1 Globus and Nexus.......................... 5 1. 2 Variable reliability.......................... 6 1. 2. 1 Aim of variable reliability.................. 6 1. 2. 2 Loss tolerance......................... 6 2 The Variable Reliability Protocol 7 2. 1 General principle..... [...] ...|$|E
40|$|Computer network {{technologies}} and strategic management methodologies have undoubtedly {{had a major}} impact {{in the way people}} communicate, exchange information and participate in attractive e-commerce and e-business opportunities. The small and medium sized businesses, by pursuing their ambitious plans to grow into major e-commerce players and in their continuous effort to take advantage of these new rapidly expanded {{technologies and}} methodologies, have to consider carefully certain related critical factors such as presentation time of new products, physical location of the enterprise, effective channels of distribution, niche marketing et al. Certain key issues concerning strategic management methodologies of e-businesses in the case of small and medium sized publishing firms are examined. The concepts of meta-problems, <b>meta-computing</b> and meta-strategies are considered and their application and solution to e-business is briefly discussed. 1...|$|E
40|$|The Java {{programming}} language and environment is stimulating new research activities {{in many areas}} of computing, {{not the least of which}} is parallel computing. Parallel techniques are themselves finding new uses in cluster computing systems. Although there are excellent software tools for scheduling, monitoring and message-based programming on parallel clusters, these systems are not yet well integrated and do not provide very high-level parallel programming paradigm support. We have prototyped a multi-paradigm parallel programming toolkit in Java, specifically targeting an integrated approach on Beowulf-style cluster computers. Our JUMP system builds on ideas from the message-passing community as well as from distributed systems technologies. We believe our system promises high performance for parallel programming as well as integrating with <b>meta-computing</b> frameworks for sharing resources across administrative boundaries. The ever-improving Java development environment all [...] ...|$|E
40|$|Most Java-based {{systems that}} support {{portable}} parallel and distributed computing either require the programmer {{to deal with}} intricate low-level details of Java {{which can be a}} tedious, timeconsuming and error-prone task, or prevent the programmer from controlling locality of data. In this paper we describe JavaSymphony, a programming paradigm for distributed and parallel computing that provides a software infrastructure for wide classes of heterogeneoussystems ranging from small-scale cluster computing to large scale wide-area <b>meta-computing.</b> The software infrastructure is written entirely in Java and runs on any standard compliant Java virtual machine. In contrast to most existing systems, JavaSymphony provides the programmerwith the flexibility to control data locality and load balancing by explicit mapping of objects to computing nodes. Virtual architectures are specified to impose a virtual hierarchy on a distributed system of physical computing nodes. Objects can be mapped and dyn [...] ...|$|E
40|$|NASA's Information Power Grid (IPG) is an {{infrastructure}} designed {{to harness the}} power of graphically distributed computers, databases, and human expertise, in order to solve large-scale realistic computational problems. This type of a <b>meta-computing</b> environment is necessary to present a unified virtual machine to application developers that hides the intricacies of a highly heterogeneous environment and yet maintains adequate security. In this paper, we present a novel partitioning scheme. called MinEX, that dynamically balances processor workloads while minimizing data movement and runtime communication, for applications that are executed in a parallel distributed fashion on the IPG. We also analyze the conditions that are required for the IPG {{to be an effective}} tool for such distributed computations. Our results show that MinEX is a viable load balancer provided the nodes of the IPG are connected by a high-speed asynchronous interconnection network...|$|E
40|$|International audienceFuture {{computing}} platforms will {{be distributed}} and heterogeneous. Such platforms range from heterogeneous networks of workstations (NOWs) to collections of NOWs and parallel servers scattered throughout the world and linked through high-speed networks. Implementing tightlycoupled algorithms on such platforms raises several challenging issues. New data distribution and load balancing strategies are required to squeeze {{the most out of}} heterogeneous platforms. In this paper, we rst summarize previous results obtained for heterogeneous NOWs, dealing with the implementation of standard numerical kernels such as nite-dierence stencils or dense linear solvers. Next we target distributed collections of heterogeneous NOWs, and we discuss data allocation strategies for dense linear solvers on top of such platforms. These results indicate that a major algorithmic and software eort is needed to come up with eÆcient numerical libraries on the computational grid. Keywords: <b>meta-computing,</b> heter [...] ...|$|E
40|$|Rapid advancements in {{processor}} and networking technologies {{have led to}} the evolution of cluster and grid computing frameworks. These high-performance computing environments exploit geographically distributed, diverse resources with the goal of providing efficient computing solutions to all kinds of parallel and distributed applications. OCEAN (Open Computation Exchange and Arbitration Network) provides a scalable market-based infrastructure to such <b>meta-computing</b> frameworks. OCEAN aims to build a marketplace where resources like CPU time, associated memory usage and network bandwidth are the traded commodities. This paper explains the technical challenges faced in the design of OCEAN and discusses our proposed solution. To facilitate finding suitable resources for buyers, we developed efficient matching and evolution protocols for the peer-to-peer matching network. The architecture and various components of OCEAN are described in detail. We implemented OCEAN on Java and. NET platforms and describe results from our preliminary experiments. 1...|$|E
40|$|In this paper, {{we present}} a {{distributed}} computing framework for problems characterized by a highly irregular search tree, whereby no reliable workload prediction is available. The framework {{is based on a}} peer-to-peer computing environment and dynamic load balancing. The system allows for dynamic resource aggregation, does not depend on any specific <b>meta-computing</b> middleware and is suitable for large-scale, multi-domain, heterogeneous environments, such as computational Grids. Dynamic load balancing policies based on global statistics are known to provide optimal load balancing performance, while randomized techniques provide high scalability. The proposed method combines both advantages and adopts distributed job-pools and a randomized polling technique. The framework has been successfully adopted in a parallel search algorithm for subgraph mining and evaluated on a molecular compounds dataset. The parallel application has shown good calability and close-to linear speedup in a distributed network of workstations...|$|E
40|$|We explore an {{approach}} based on remote evaluation, {{rather than the}} traditional TCP/IP based remote procedure calls for parallelism on a network of workstations. The remote evaluation approach is embodied in mobile code [...] {{referred to as a}} mobile agent. We investigate issues in employing mobile agents in problem solving environments (PSE), with the Aglets workbench from IBM. A PSE is generally composed of various computational resources that need to interoperate efficiently. We propose a mechanism where mobile code can be used to migrate the computation, enabling complex communication and distribution patterns, and support interoperability between various distributed resources. An application based on a neural network is used to show the concepts involved. Keywords: Mobile Agents, Aglets, Neural Networks, Problem Solving Environments 1 Introduction Advances in networking technology and computational infrastructure has made it possible to build large <b>meta-computing</b> environ [...] ...|$|E
40|$|While {{languages}} directly {{address the}} needs of distributed object-oriented databases, operating systems do not. Operating systems offer abstractions that are closely related to hardware and inadequate for close cooperation at the application level. This mismatch can be addressed by viewing an operating system as a mapping from the application domain to system resources. A new model of OODB applications as abstract directed graphs lends structure to this mapping. Experience with the ARCADE data abstractions, which essentially realize the passive portion of this model, clearly indicates the value of encapsulation, metadata and even <b>meta-computing.</b> It has lead to the proposed p object architecture as an overall solution to the mapping problem. p is based on the generalized object model and supports meta-computation through manipulation of event-based interfaces. Its goal is to present an application-based interface which effectively utilizes hardware resources...|$|E
40|$|Nimrod {{is a tool}} {{which makes}} it easy to parallelise and {{distribute}} large computational experiments based on the exploration of a range of parameterised scenarios. Using Nimrod, it is possible to specify and generate a parametric experiment, and then control the execution of the code across distributed computers. Nimrod has been applied to a range of application areas, including Bioinformatics, Operations Research, Electronic CAD, Ecological Modelling and Computer Movies. Nimrod was extremely successful at generating work, but it contained no mechanisms for scheduling the computation on the underlying resources. Consequently, users would not have any idea when an experiment might complete. We are currently building a new version of Nimrod, called Nimrod/G. Nimrod/G will integrate Nimrod job generation techniques with Globus, an international project which is building the underlying infrastructure for large <b>meta-computing</b> applications. Using Globus, it will be possible for Nimrod users t [...] ...|$|E
40|$|This paper {{proposes a}} Java-based mobile agent infrastructure, TRAVELER, to support wide area {{parallel}} applications. Unlike other <b>meta-computing</b> systems, TRAVELER {{allows users to}} dispatch their compute-intensive jobs as mobile agents via a resource broker. The broker forms a parallel virtual machine atop servers to execute the agents. Since the agents can be programmed to satisfy their goals, even if they move and lose contact with their creators, they can survive intermittent or unreliable network connection. During their lifetime, the agents can also move themselves autonomously from one machine to another for load balancing, enhancing data locality, and tolerating faults. TRAVELER relies on an integrated distributed shared array runtime system in support of agent communications on clusters of servers. We demonstrated the feasibility of the TRAVELER in LU factorization problems. 1 Introduction The 1990 s are seeing the explosive growth of Internet and Web-based information sharing a [...] ...|$|E
40|$|The {{cost for}} {{stochastic}} sampling of QCD vacuum configurations outweighs {{by far the}} costs of the remaining computational tasks in Lattice QCD, due to the nonlocal forces arising from the dynamics of fermion loops in the vacuum fluctuations. The evaluation of quality and hence efficiency of sampling algorithms is largely determined by the assessment of their decorrelation capacity along the Monte Carlo time series. In order to gain control over statistical errors, state-of-the-art research and development on QCD sampling algorithms needs substantial amount of Teraflopshours. Over the past years two German-Italian collaborations, SESAM and TØL, carried out exploratory simulations, joining their resources in a <b>meta-computing</b> effort on various computer platforms in Italy and Germany. In this article, we shall discuss the practical aspects of this work, present highlights of autocorrelation measurements, illustrate the impact of unquenching on some fundamental parameters of QCD and describe [...] ...|$|E
40|$|Studies {{have shown}} that for a {{significant}} fraction of the time desktop PCs and workstations are underutilized. To exploit these idle resources, various Desktop/Workstation Grid systems have been developed. The ultimate goal of such systems is to maximize efficiency of resource usage while maintaining low obtrusiveness to machine owners. To this end, we created a new finegrain cycle stealing approach and conducted a performance comparison study against the traditional coarsegrain cycle stealing. We developed a prototype of finegrain cycle stealing, the Linger-Longer system, on a Linux cluster. The experiments on a cluster of desktop Linux PCs with benchmark applications show that, overall, fine-grain cycle stealing can improve efficiency of idle cycle usage by increasing the guest job throughput by 50 % to 70 %, while limiting obtrusiveness {{with no more than}} 3 % of host job slowdown. Index Terms – Desktop grid, <b>meta-computing,</b> cluster computing, process migration, networks of workstations...|$|E
40|$|Abstract — Inter-clouds as a {{recently}} emerged {{approach has been}} introduced as to expand the cloud capabilities and to challenge {{the level of the}} cloud elasticity in services. By facilitating an opportunistic environment, it aims to consistently realise a scalable resource provisioning setting that handles sudden variation in user demands. Herein we propose an architectural strategy for scheduling jobs in inter-enterprises with a particular focus to inter-clouds. The scheduling concept is based on the <b>meta-computing</b> paradigm, with the purpose of establishing a wide and decentralized policy control among various resources and resource owners. To this extend, we start with presenting an analysis of the cloud characteristics and inter-cloud requirements. Following this, we present an identification of the most critical issues when developing the strategy based on the functional requirements analysis. This has {{led to the development of}} a strategic plan which provides the architecture for an interoperable, efficient and flexible environment to support the inter-cooperation between heterogeneous inter-enterprises...|$|E
40|$|We {{present the}} design of an {{interface}} to allow applications to export tuning alternatives to a higher-level system. By exposing different parameters that can be changed at runtime, applications {{can be made to}} adapt to changes in their execution environment due to other programs, or the addition or deletion of nodes, communication links etc. An integral part of this interface is that an application not only expose its options, but also the resource utilization of each option and the effect that the option will have on the application's performance. We discuss how these options can be evaluated to tune the overall performance of a collection of applications in the system. Finally, we show preliminary results from a database application that is automatically reconfigured by the system from query shipping to data shipping {{based on the number of}} active clients. 1. Introduction <b>Meta-computing,</b> the simultaneous and coordinated use of semi-autonomous computing resources in physically separat [...] ...|$|E
40|$|Optimal {{scheduling}} in <b>meta-computing</b> environments {{still is}} an open research question. Various research management (RM) architectures have been proposed in the literature. In the present paper we explore, through simulation, various muli-level scheduling strategies for compound computing environments comprising several clusters of workstations. We study global and local RM and their interaction. The local RM comprises both the cluster management and operating system schedulers. Each level refines the scheduling decisions of the layer above it, {{taking into account the}} latest resource information. Our experiments explore conventional strategies like First Come, First Served (FCFS) and Shortest Job First (SJF) at the global RM level. At all levels, the schedulers strive to maintain a good load balance. The unit of load balancing at the global level is the job consisting of one or more parallel tasks; at the local level it is the task. The results of our similutions indicate that, especially at high system loads, the use of a global RM can result in a significant performance gain...|$|E
40|$|Mobile agent {{paradigm}} {{and technology}} are profitably being {{applied for the}} construction of a wide range of applications and systems in several areas encompassing e-business, telecommunications, <b>meta-computing</b> and military simulations. Such systems are usually complex so requiring methodologies and tools for an in-depth understanding of their dynamics and for their validation. This paper presents and exemplifies a methodology for modeling and validation through simulation of mobile agents based systems and applications. Mobile agents are modeled as asynchronous, event-driven single threaded entities whose behavior is specified using Distilled Statecharts. A distilled statechart, which embeds both the activity and the interactions a mobile agent performs during its lifecycle, can be easily translated into a Java composite object using the mobile active object framework. In order to analyze an agent-based system, a discrete-event, reflective simulation framework which allows for concurrency, timing and meta-level customization, is used. A case study is also provided which highlights the main features of the proposed methodology...|$|E
40|$|As {{the high}} {{performance}} computational {{requirements of the}} scientific community grow more complex in an increasingly distributed environment, there is a rising need to manage the allocation of these resources across larger boundaries. Local site management needs a means to fairly distribute the underlying computing resources (processors, memory, disk) to the various users or projects that have access to them. Some sites have attempted to resolve this deficiency by writing rather simplistic homegrown scripts that do little more than track project/account CPU usage in a periodic postprocessing fashion. However, these mechanisms are becoming increasingly inadequate. Without scalable and flexible resource allocation and control mechanisms, wide-scale distributed computing will not be realized. In this paper, QBank is introduced as an effective tool in managing resource allocations within high performance computational environments. QBank’s design and architectural features are considered. Strategies for deployment are explored. Scenarios for how QBank might operate within a <b>meta-computing</b> context are described. Issues of scalability, security, and future work are discussed. 1...|$|E
40|$|AbstractThe CCS (Calculus of Communicating System) process algebra is a {{well-known}} formal model of synchronization and communication, useful {{for the analysis of}} safety and liveness in protocols or distributed programs, and in more recent works their security properties. BSP (Bulk-synchronous parallelism) is an algorithm- and programming model of data-parallel computation. It is useful for the design, analysis and programming of scalable parallel algorithms. Many current evolutions require the integration of distributed- and parallel programming: grid systems for sharing resources across the Internet, secure and reliable global access to parallel computer systems, geographic distribution of confidential data on randomly accessible systems, etc. Such software services must provide guarantees of safety, liveness, and security together with scalable and reliable performance. Formal models are therefore needed to combine parallel performance and concurrent behavior. With this goal in mind, we propose here an integration of BSP with CCS semantics, generalize its cost (performance) model and sketch its application to scheduling problems in <b>meta-computing...</b>|$|E
