154|287|Public
40|$|This paper {{utilizes}} recent simulation {{techniques in}} a two-stage estimation method which is applicable {{for a wide}} range of statistical models in the presence of missing data. The first stage of the method provides a way to estimate (and simulate from) the joint distribution of <b>missing</b> <b>variables</b> when the <b>missing</b> <b>variables</b> are continuous, binary, or ordered discrete. The second stage uses the first-stage estimates to “integrate” out the effects of the <b>missing</b> <b>variables</b> and obtain model estimates. The implementation of the method in this paper allows theoretically important, partially missing wage and school characteristic variables-which are not necessarily independently determined-to be included in a proportional hazard model of teacher attrition. © 1999 by the President and Fellows of Harvard College and the Massachusetts Institute of Technology...|$|E
30|$|The {{sample size}} of the {{different}} variables is the same, because observation {{with one or more}} <b>missing</b> <b>variables</b> were deleted from the sample.|$|E
40|$|In {{this paper}} we propose {{recurrent}} neural networks with feedback into the input units for handling two types of data analysis problems. On the one hand, this scheme {{can be used for}} static data when some of the input variables are missing. On the other hand, it can also be used for sequential data, when some of the input variables are missing or are available at different frequencies. Unlike in the case of probabilistic models (e. g. Gaussian) of the <b>missing</b> <b>variables,</b> the network does not attempt to model the distribution of the <b>missing</b> <b>variables</b> given the observed variables. Instead it is a more "discriminant" approach that fills in the <b>missing</b> <b>variables</b> {{for the sole purpose of}} minimizing a learning criterion (e. g., to minimize an output error). 1 Introduction Learning from examples implies discovering certain relations between variables of interest. The most general form of learning requires to essentially capture the joint distribution between these variables. However, for many spe [...] ...|$|E
50|$|It is {{possible}} that any <b>missing</b> <b>variable</b> as well as errors in values of included variables can lead to erroneous results.|$|R
3000|$|... is {{considered}} to be a latent construct, θ is an inherently unobserved (i.e., <b>missing)</b> <b>variable.</b> <b>Missing</b> data in the individual response pattern (the 0 / 1 answers of persons to test items) constitute the second type, and missing data in background information constitute the third type of missing data.|$|R
40|$|<b>Missing</b> <b>variable</b> {{models are}} typical {{benchmarks}} for new computational techniques {{in that the}} ill-posed nature of <b>missing</b> <b>variable</b> models o#er a challenging testing ground for these techniques. This {{was the case for}} the EM algorithm and the Gibbs sampler, and this is also true for importance sampling schemes. A population Monte Carlo scheme taking advantage of the latent structure of the problem is proposed. The potential of this approach and its specifics in missing data problems are illustrated in settings of increasing di#culty, in comparison with existing approaches. The improvement brought by a general Rao [...] Blackwellisation technique is also discussed...|$|R
30|$|Observations {{with either}} <b>missing</b> <b>variables</b> or {{negative}} values are {{dropped from the}} data set. In sum, there is 21, 000 enterprises per year {{and a total of}} 177, 086 observations over the eight years from 1998 to 2005.|$|E
3000|$|In the econometric {{estimation}} {{of the impact of}} trade costs on the export share of each industry, the possible impact of “endogeneity bias” on estimated value needs to be paid attention. The trade cost {{may be related to the}} error term, which makes the ordinary least square method product biased and inconsistent estimates. “Simultaneity bias” and the <b>missing</b> <b>variables</b> are the two sources of the endogenous nature, and the most likely one is the omission of other relevant variables (Milner and McGowan 2013). 7 These <b>missing</b> <b>variables</b> are related to the trade costs, and potentially determine the export share that is not explained by the existing explanatory variables. In order to overcome the endogeneity bias, time-fixed effects γ [...]...|$|E
30|$|Using the employer-employee matched data, we {{overcome}} {{the problem of}} <b>missing</b> <b>variables</b> and make the estimates of unreasonable parts of the high income of monopoly industries more reliable. By the decomposition method, the wage gaps between monopoly and competitive industries are decomposed into the reasonable part and the unreasonable part.|$|E
40|$|Little and An (2004,  Statistica Sinica   14, 949 – 968) {{proposed}} a penalized spline of propensity prediction (PSPP) method of imputation of missing values that yields robust model-based inference under the missing at random assumption. The propensity score for a <b>missing</b> <b>variable</b> is estimated and a regression model is fitted {{that includes the}} spline of the estimated logit propensity score as a covariate. The predicted unconditional mean of the <b>missing</b> <b>variable</b> has a double robustness (DR) property under misspecification of the imputation model. We show that a simplified version of PSPP, which does not center other regressors prior to including them in the prediction model, also has the DR property. We also propose two extensions of PSPP, namely, stratified PSPP and bivariate PSPP, that extend the DR property to inferences about conditional means. These extended PSPP methods are compared with the PSPP method and simple alternatives in a simulation study and applied to an online weight loss study conducted by Kaiser Permanente...|$|R
40|$|The ill-posed {{nature of}} <b>missing</b> <b>variable</b> models offers a {{challenging}} testing ground for new computational techniques. This {{is the case}} for the mean-field variational Bayesian inference. The behavior of this approach in the setting of the Bayesian probit model is illustrated. It is shown that the mean-field variational method always underestimates the posterior variance and, that, for small sample sizes, the meanfield variational approximation to the posterior location could be poor...|$|R
30|$|For {{each of the}} {{variables}} occupation, industry, and employment type, {{if one of them}} is missing but the two others are the same as in the previous or next year, then we assume that the <b>missing</b> <b>variable</b> also stays the same. This is not problematic for occupation and industry because in non-missing cases, the concordance is more than 98 %. For employment type, the concordance is still very high (90 %) but not quite as high.|$|R
40|$|We {{introduce}} {{the problem of}} reconstructing a sequence of multidimensional real vectors {{where some of the}} data are missing. This problem contains regression and mapping inversion as particular cases where the pattern of missing data is independent of the sequence index. The problem is hard because it involves possibly multivalued mappings at each vector in the sequence, where the <b>missing</b> <b>variables</b> can take more than one value given the present variables; and the set of <b>missing</b> <b>variables</b> can vary from one vector to the next. To solve this problem, we propose an algorithm based on two redundancy assumptions: vector redundancy (the data live in a low-dimensional manifold), so that the present variables constrain the missing ones; and sequence redundancy (e. g. continuity), so that consecutive vectors constrain each other. We capture the low-dimensional nature of the data in a probabilistic way with a joint density model, here the generative topographic mapping, which results in a Gaussian mixture. Candidate reconstructions at each vector are obtained as all the modes of the conditional distribution of <b>missing</b> <b>variables</b> given present variables. The reconstructed sequence is obtained by minimising a global constraint, here the sequence length, by dynamic programming. We present experimental results for a toy problem and for inverse kinematics of a robot arm. Comment: 30 pages, 9 figures. Original manuscript dated January 27, 2004 and not updated since. Current author's email address: mcarreira-perpinan@ucmerced. ed...|$|E
40|$|In {{the present}} work, we {{illustrate}} {{the process of}} constructing a simplified model for complex multi-scale combustion systems. To this end, reduced models of homogeneous ideal gas mixtures of methane and air are first obtained by the novel Relaxation Redistribution Method (RRM) and thereafter used for the extraction of all the <b>missing</b> <b>variables</b> in a reactive flow simulation with a global reaction mode...|$|E
40|$|A {{complete}} {{solution to}} the forward-bias puzzle should provide an econometric solution and an economic explanation for that solution. A complete solution should also explain the closely related failure of uncovered interest parity. In addition it should explain some related anomalies. One such anomaly is that variances for changes in exchange rates are over 100 times larger than variances for interest rate differentials and forward premiums. My econometric solution is that the relevant test equations omit two variables that covered interest parity implies should be included. For my data, the <b>missing</b> <b>variables</b> explain the failure of uncovered interest parity and the forward-bias puzzle. The <b>missing</b> <b>variables</b> also explain why the variance for changes in exchange rates is over 100 {{times larger than the}} variance for both interest rate differentials and forward premiums. My economic explanation is that, in general, forward rates do not equal expected future spot rates. exchange rates; forward bias; covered interest parity; uncovered interest parity; arbitrage, Finance, International Economics, Macroeconomics...|$|E
5000|$|The {{previous}} {{number two}} and three American promotions, the aforementioned World Championship Wrestling and Extreme Championship Wrestling, had folded earlier in 2001, leaving only one national wrestling company. As such, the letter [...] "X" [...] in XWF stood for the <b>missing</b> <b>variable</b> in the sport. The official definition of the [...] "X" [...] became Xcitement as cited by [...] "Mean" [...] Gene Okerlund in the Extras on the XWF DVD and not [...] "Xtreme" [...] as sometimes written.|$|R
30|$|In the Lazear framework, the {{mechanism}} driving the {{positive correlation between}} academic and occupational choices and entrepreneurial entry is an unobservable entrepreneurial skill. Hence, the positive correlation between academic or occupational diversity and entrepreneurship is not causal but {{a reflection of a}} common <b>missing</b> <b>variable</b> that affects all choices in the same direction. This paper explores that possibility by treating choice of academic program, occupational path and entrepreneurship as joint decisions planned at the time of college entry. The results are broadly consistent with the theory.|$|R
40|$|This paper {{presents}} {{a procedure that}} imputes missing values by using random forests on semi-supervised data. We found {{that the rate of}} correct classification of our method is higher than that of other methods: a simple expansion of Liaw’s “rfImpute ” for (un) supervised data and the k-nearest neigh-bor method (kNN). Our method can handle <b>missing</b> predic-tor <b>variables</b> as well as <b>missing</b> response <b>variable.</b> An im-putation that uses random forests for semi-supervised cases in the training data set has never been implemented until now...|$|R
40|$|We present {{methods for}} dealing with <b>missing</b> <b>variables</b> {{in the context of}} Gaussian Processes and Support Vector Machines. This solves an {{important}} problem which has largely been ignored by kernel methods: How to systematically deal with incomplete data? Our method can also be applied to problems with partially observed labels {{as well as to the}} transductive setting where we view the labels as missing data. Our approach relie...|$|E
40|$|Asymmetry in the price-volume {{relation}} is investigated using {{cross-sectional data}} on the stock prices and trading volume of more than 100 companies listed on the Kuwait Stock Exchange over 11 consecutive weeks. The results show that asymmetry can be concealed by <b>missing</b> <b>variables</b> such as market capitalisation. There is some evidence for asymmetry that {{takes the form of}} higher trading volume in a declining market. Some explanations are put forward for the finding of asymmetry...|$|E
40|$|This paper revisits {{the main}} {{problems}} faced by linear regression models. The exogenous condition for linear regression is verified by using the triangular law of vectors and dot product of vectors. Errors in variables and <b>missing</b> <b>variables</b> are analyzed analytically. This paper shows that these two problems will cause large sample bias of LS estimator for the parameters. Practical suggestions are made on how these problems can be minimized on a case to case basis...|$|E
40|$|Prediction models {{should be}} {{externally}} validated before {{being used in}} clinical practice. Many published prediction models have never been validated. Uncollected predictor variables in otherwise suitable validation cohorts are the main factor precluding external validation. We used individual patient data from 9 different cohort studies conducted in the United States, Europe, and Latin America that included 7, 892 patients with {{chronic obstructive pulmonary disease}} who enrolled between 1981 and 2006. Data on 3 -year mortality and the predictors of age, dyspnea, and airflow obstruction were available. We simulated missing data by omitting the predictor dyspnea cohort-wide, and we present 6 methods for handling the <b>missing</b> <b>variable.</b> We assessed model performance with regard to discriminative ability and calibration and by using 2 vignette scenarios. We showed that the use of any imputation method outperforms the omission of the cohort from the validation, which is a commonly used approach. Compared with using the full data set without the <b>missing</b> <b>variable</b> (benchmark), multiple imputation with fixed or random intercepts for cohorts was the best approach to impute the systematically missing predictor. Findings of this study may facilitate the use of cohort studies that do not include all predictors and pave the way for more widespread external validation of prediction models even if 1 or more predictors of the model are systematically missin...|$|R
40|$|International audienceMissing {{variable}} {{models are}} typical benchmarks for new computational techniques {{in that the}} ill-posed nature of <b>missing</b> <b>variable</b> models offer a challenging testing ground for these techniques. This {{was the case for}} the EM algorithm and the Gibbs sampler, and this is also true for importance sampling schemes. A population Monte Carlo scheme taking avantage of the latent structure of the problem is proposed. The potential of this approach and its specifics in missing data problems are illustrated in settings of increasing difficulty, in comparison with existing approaches. The improvement brought by a general Rao-Blackwellisation technique is also discussed...|$|R
40|$|In this paper, {{we propose}} a {{grossing-up}} algorithm {{that allows for}} gross income calculation based on tax rules and observed variables in the sample. The algorithm is applicable in tax-benefit microsimulation models, which are mostly used by taxation policy makers to support government legislative processes. Typically, tax-benefit microsimulation models are based on datasets, where only the net income is known, though the data about gross income is needed to successfully simulate the impact of taxation policies on the economy. The algorithm that we propose allows for an exact reproduction of a <b>missing</b> <b>variable</b> by applying a set of taxation rules that are known {{to refer to the}} variable in question and to other variables in the dataset during the data generation process. Researchers and policy makers can adapt the proposed algorithm with respect to the rules and variables in their legislative environment, which allows for complete and exact restoration of the <b>missing</b> <b>variable.</b> The algorithm incorporates an estimation of partial analytical solutions and a trial-and-error approach to find the initial true value. Its validity was proven by a set of tax rule combinations at different levels of income that are used in contemporary tax systems. The algorithm is generally applicable, with some modifications, for data imputation on datasets derived from various tax systems around the world. Povzetek: Članek predstavlja algoritem obrutenja, ki omogoča izračunavanje bruto dohodkov iz neto dohodkov ob širokem naboru davčnih pravil različnih davčnih sistemov. Algoritem omogoča reproduciranje manjkajočih spremenljivk in je široko uporaben pri mikrosimulacijskem modeliranju. ...|$|R
40|$|When using {{parametric}} cost estimation, it {{is important}} to note the possibility of the regression coefficients having the wrong sign. A wrong sign is defined as a sign on the regression coefficient opposite to the researcher's intuition and experience. Some possible causes for the wrong sign discussed in this paper are a small range of x's, leverage points, <b>missing</b> <b>variables,</b> multicollinearity, and computational error. Additionally, techniques for determining the cause of the wrong sign are given...|$|E
40|$|Introduction: Diabetes {{mellitus}} (DM) has {{the potential}} to impact the pathogenesis, treatment, and outcome of pancreatic cancer. This study evaluates the impact of DM on pancreatic cancer survival. Methods: We conducted a retrospective cohort study from the Veterans Affairs (VA) Central Cancer Registry (VACCR) for pancreatic cancer cases between 1995 and 2008. DM and no-DM cases were identified from comorbidity data. Univariate and multivariable analysis was performed. Multiple imputation method was employed to account for <b>missing</b> <b>variables...</b>|$|E
40|$|This article {{deals with}} the {{identification}} of gene regulatory networks from experimental data using a statistical machine learning approach. A stochastic model of gene interactions capable of handling <b>missing</b> <b>variables</b> is proposed. It {{can be described as}} a dynamic Bayesian network particularly well suited to tackle the stochastic nature of gene regulation and gene expression measurement. Parameters of the model are learned through a penalized likelihood maximization implemented through an extended version of EM algorithm. Our approac...|$|E
40|$|We {{propose a}} nonparametric {{imputation}} procedure for data with missing values and establish an empirical likelihood inference for parameters defined by general estimating equations. The imputation {{is carried out}} multiple times via a nonparametric estimator of the conditional distribution of the <b>missing</b> <b>variable</b> given the always observable vari-able. The empirical likelihood is used to construct a profile likelihood for the parameter of interest. We demonstrate that the proposed nonparametric imputation can remove the selection bias in the missingness and the empirical likelihood leads to more efficient param-eter estimation. The proposed method is evaluated by simulation and an empirical study {{on the relationship between}} eye weight and gene transcriptional abundance of recombinant inbred mice. ...|$|R
40|$|We {{argue that}} {{research}} on interdependencies fit is an underexplored variable {{in strategy and}} organization research and is the <b>missing</b> <b>variable</b> that differentiates the performance of “built to last” organizations from the rest. Interdependencies fit relates to how well activities and processes within the organization or between the organization and its environment mutually reinforce one another. We suggest that the major reason underlying variation in firm performance may be rooted in differences of whether and how firms manage interdependencies within and across an organization’s strategic activities. Progress on researching interdependencies fit could be realized by focusing on strategically important activities, and the research challenge is to identify the unobservable processes and routines that underlie interdependencies fit. <br /...|$|R
40|$|In their UIP regressions, Huisman et al. (1998. Extreme {{support for}} {{uncovered}} interest parity, Journal for International Money and Finance 17, 211 - 228.) focus on extreme forward premia and find much higher coefficients. We show that, for such results, the expectation signal {{needs to be}} thicker-tailed than the <b>missing</b> <b>variable.</b> Transaction costs may produce the right sort of bias. It is (i) bounded (i. e. it has no tails at all), (ii) wide (i. e. it may generate betas below 1 / 2) and (iii) U-distributed, which makes an "extreme" sample quite effective. We derive theoretical and numerical results {{in the direction of}} what Huisman et al. observe. We also tighten Fama's moment conditions. (c) 2005 Published by Elsevier Ltd. status: publishe...|$|R
40|$|Some key econometric {{concepts}} {{and problems of}} great importance to Trygve Haavelmo and Ragnar Frisch are discussed within the gen-eral framework of a cointegrated VAR. The focus is on problems typ-ical of time-series data such as multicollinearity, spurious correlation and regression, time dependent residuals, model selection, <b>missing</b> <b>variables,</b> simultaneity, autonomy and identi 8 ̆ 5 cation. The paper ar-gues that the more recent development of unit root econometrics has been instrumental for {{a solution to the}} above problems...|$|E
40|$|Neural autoregressive {{models are}} {{explicit}} density estimators that achieve state-of-the-art likelihoods for generative modeling. The D-dimensional data distribution is factorized into an autoregressive product of one-dimensional conditional distributions {{according to the}} chain rule. Data completion is a more involved task than data generation: the model must infer <b>missing</b> <b>variables</b> for any partially observed input vector. Previous work introduced an order-agnostic training procedure for data completion with autoregressive models. <b>Missing</b> <b>variables</b> in any partially observed input vector can be imputed efficiently by choosing an ordering where observed dimensions precede unobserved ones and by computing the autoregressive product in this order. In this paper, we provide evidence that the order-agnostic (OA) training procedure is suboptimal for data completion. We propose an alternative procedure (OA++) that reaches better performance in fewer computations. It can handle all data completion queries while training fewer one-dimensional conditional distributions than the OA procedure. In addition, these one-dimensional conditional distributions are trained proportionally to their expected usage at inference time, reducing overfitting. Finally, our OA++ procedure can exploit prior knowledge about the distribution of inference completion queries, as opposed to OA. We support these claims with quantitative experiments on standard datasets used to evaluate autoregressive generative models...|$|E
40|$|Diabetes {{mellitus}} (DM) has {{the potential}} to impact the pathogenesis, treatment, and outcome of pancreatic cancer. This study evaluates the impact of DM on pancreatic cancer survival. We conducted a retrospective cohort study from the Veterans Affairs (VA) Central Cancer Registry (VACCR) for pancreatic cancer cases between 1995 and 2008. DM and no-DM cases were identified from comorbidity data. Univariate and multivariable analysis was performed. Multiple imputation method was employed to account for <b>missing</b> <b>variables.</b> Of 8, 466 cases of pancreatic cancer DM status was known in 4728 cases that comprised this analysis. Males accounted for 97. 7 % cases, and 78 % were white. Overall survival was 4. 2 months in DM group and 3. 6 months in the no-DM group. In multivariable analysis, DM had a HR = 0. 91 (0. 849 - 0. 974). This finding persisted after accounting for <b>missing</b> <b>variables</b> using multiple imputations method with the HR in DM group of 0. 93 (0. 867 - 0. 997). Our data suggest DM is associated with a reduction in risk of death in pancreatic cancer. Future studies should be directed towards examining this association, specifically impact of DM medications on cancer outcome...|$|E
40|$|We {{consider}} an empirical likelihood inference for parameters defined by general estimating equations when some {{components of the}} random observations are subject to missingness. As {{the nature of the}} estimating equations is wide-ranging, we propose a nonparametric imputation of the missing values from a kernel estimator of the conditional distribution of the <b>missing</b> <b>variable</b> given the always observable variable. The empirical likelihood is used to construct a profile likelihood for the parameter of interest. We demonstrate that the proposed nonparametric imputation can remove the selection bias in the missingness and the empirical likelihood leads to more efficient parameter estimation. The proposed method is further evaluated by simulation and an empirical study on a genetic dataset on recombinant inbred mice. 1. Introduction. Missin...|$|R
40|$|Traditional {{economics}} {{assumes that}} interest rate effects inflation {{by changing the}} aggregate demand (Barth and Ramay, 2002). On the other hand, many economists {{in recent years have}} explored the cost side effects of monetary transmission and found very strong evidences in favour of cost channel. One of such studies is that by Rehman (2015) which explores the relationship between interest rate and inflation for a large data set comprising various measures of interest rate and inflation from countries around the globe. Rehman (2015) computes the correlation between two variables and he finds that the correlation between two variables is either positive or insignificant. Rehman argues that the finding is quite robust and does not change with a change in measure of interest rate and/or inflation. If the correlation between interest rate and inflation is positive then using interest rate to control inflation would be counterproductive. Thus it will endorse the warning of Wright Patman, a US congressman and Chairman of Joint Economic Committee who argues that “senseless of trying to fight inflation by raising interest rate, throwing the gasoline on fire to put out the flames would be as logical”. Findings of Rehman (2015) are based on correlation coefficients. The correlation without having control variables could only provide a clue and could be subject to serious <b>missing</b> <b>variable</b> bias. However, Rehman (2015) argues that thousands of similar clues from the entire globe collectively become very strong evidence. However, given the importance of the topic, it is necessary to do a more careful analysis and summarize the relationship between two variables which is not subject to <b>missing</b> <b>variable</b> bias. Therefore, this paper applies more sophisticated econometric techniques including Granger Causality and Static Long Run Solution to find the impact of interest rate and inflation...|$|R
40|$|COMPETING EXPLANATIONS OF MALE INTERRACIAL WAGE DIFFERENTIALS: MISSING VARIABLE MODELS VERSUS JOB COMPETITION Persistent interracial wage {{differentials}} present {{a challenge for}} neoclassical models of discrimination, which claim that long run competition is not consistent with persistent discrimination. Accordingly, several <b>missing</b> <b>variable</b> explanations are proposed in the literature. These modifications have two implications. One, interracial wage inequality is due to interracial inequality in pre-labor market factors. Two, there is no correlation between intergroup segregation and interracial {{wage differentials}}. However, the job competition model of discrimination argues that persistent wage discrimination and racial and gender employment segregation are causally related. Also, this model shows that racial discrimination {{is linked to the}} profit maximizing behavior of firms and, thereby, is consistent with the existence of competitive labor markets. This study provides an empirical exa [...] ...|$|R
