8|3|Public
50|$|Fue y cayó. Y queda solamente la inútil cifra con pocos destinos poderosos, tristes devenires sin el <b>más</b> <b>sencillo</b> bien. Idiota, re idiota, sabe que sus encantos son ya latosos decimales. Pobre...|$|E
5000|$|Paola {{filmed in}} the Mexican movie, ¡Como va! Lo <b>más</b> <b>sencillo</b> es complicarlo todo, in early 2016 on {{location}} in Mexico City, Puerto Vallarta, and Querétaro. A release date {{has not yet been}} set, but the film is expected to be released in Mexico and United States.In mid-April 2016, she was confirmed as a cast member in the upcoming telenovela drama, [...] "La Doña", produced by Telemundo. The telenovela will film in Mexico with Aracely Arámbula and David Chocarro.|$|E
40|$|A la memoria de mi padre, de quien aprendí que sólo cuando hacemos caso al corazón por encima de la razón, podemos ser felices. A mi madre, que me enseñó que sólo el control de la razón puede llevar a buen puerto los impulsos del corazón. A ambos, de cuya unión he aprendido el necesario equilibrio que hace <b>más</b> <b>sencillo</b> {{el camino}} de la vida. A Elena, mi amor, es decir, mi vida. A Juan, Lucía y Míkel, mi vida, es decir, mi amor. To write here {{the list of}} all persons to whom I am {{grateful}} for their contribution in a direct or indirect manner to the elaboration of this thesis I would need more time, space and memory than I have available. I will only emphasize the names of Luis Javier Hernández Paricio and Werner M. Seiler, my two supervisors, which have given me their advice and support, and have encouraged me and teached me many things during these years, {{not only in the}} mathematical aspects of the thesis. I also want to emphasise Julio Rubio, who acted as a “third supervisor ” and has lead me in these first steps int...|$|E
40|$|El monitoreo de Espectro Radioeléctrico es una tarea {{fundamental}} que deben realizar todas las administraciones para controlar y vigilar su utilización. Sus principales componentes son los equipos de monitoreo (i. e., analizadores de Espectro, antenas y GPS) que están dispuestos en lugares estratégicos a {{lo largo de}} un territorio para obtener información sobre las características de los servicios de radiocomunicaciones. Es de especial interés complementar dicho despliegue con equipos <b>más</b> <b>sencillos</b> y de fácil transporte, para obtener la mayor cantidad de información del espectro, principalmente en zonas a las que no se puede {{llegar a}} través de la red principal de monitoreo. Una alternativa para implementarlos es Software Defined Radio (SDR), una técnica que permite construir dispositivos que permiten remplazar algunos componentes de hardware por rutinas de software, reduciendo así sus costos y tamaño. En la industria existen diferentes equipos y herramientas de programación para construirlos, que permiten el desarrollo de sistemas flexibles, fácilmente actualizables y diseñados para tareas específicas. Este artículo describe los componentes principales de un sistema de monitoreo desarrollado con SDR, y las herramientas más utilizadas para implementarlo. Spectrum monitoring is an essential task to be performed for a country administration to control and monitor its use. Its main components are the monitoring equipment (i. e. spectrum analyzers, antennas and GPS) which are arranged in strategic locations throughout a territory to get information about the characteristics of radio services. There is a special interest to complement this system with simpler and more portable equipment, to get much spectrum information, mainly in areas that cannot be reached through the main monitoring network. An alternative is to use Software Defined Radio (SDR), a technique to build devices that can replace some hardware by software routines, thus reducing costs and size. In industry there are different equipment and programming tools to build flexible devices, easily upgradeable and reconfigurable for specific tasks. This paper describes {{the main components of}} a monitoring system developed with SDR, and the tools used to implement it. Spectrum monitoring is an essential task to be performed for a country administration to control and monitor its use. Its main components are the monitoring equipment (i. e. spectrum analyzers, antennas and GPS) which are arranged in strategic locations throughout a territory to get information about the characteristics of radio services. There is a special interest to complement this system with simpler and more portable equipment, to get much spectrum information, mainly in areas that cannot be reached through the main monitoring network. An alternative is to use Software Defined Radio (SDR), a technique to build devices that can replace some hardware by software routines, thus reducing costs and size. In industry there are different equipment and programming tools to build flexible devices, easily upgradeable and reconfigurable for specific tasks. This paper describes the main components of a monitoring system developed with SDR, and the tools used to implement it...|$|R
40|$|In {{architectural}} works, structural calculations {{are usually}} few and simple, they mostly corroborate {{what is already}} known. Calculations are made {{for the most part}} with the purpose of exploring solutions. When working in a specific case operations ought to be framed in the practical understanding, with predictable values and tabulated solutions, with no surprises. To write numbers and trust them blindly, is like a reckless appointment, something that should not be done in structures. This article shows how to use the structural knowledge in order to explore a specific solution: a collar for metal supports. En las obras de arquitectura los cálculos estructurales suelen ser pocos y <b>sencillos,</b> <b>más</b> bien para corroborar algo que se sabía de antemano. Los números se hacen, en su mayor parte, para explorar soluciones; cuando se trabaja en un caso particular debe operarse en un marco conocido, con valores predictables y soluciones tabuladas, sin sorpresas. Hacer números para fiarse a pies juntillas de ellos, es como una cita a ciegas, algo no recomendable en estructuras. Este artículo muestra cómo usar los conocimientos estructurales para explorar una solución: la de un collarín para soportes metálicos...|$|R
40|$|El balistocardiograma es el registro del movimiento del cuerpo que se origina por la energía transmitida desde el corazón hacia las grandes arterias como consecuencia de la salida de la sangre desde los ventrículos y del retroceso del cuerpo como reacción. Esta energía causa un desplazamiento del cuerpo en sentido cefálico, para luego cambiar en dirección opuesta cuando la sangre fluye por la aorta descendente. El sistema, que consta de tres partes, una tabla móvil en el sentido horizontal, un sistema de registro del movimiento {{corporal}} y un sistema de amplificación, permite generar un registro gráfico. La primera descripción corresponde a Gordon en 1877, cuando registró los movimientos horizontales y verticales de pacientes en una cama suspendida por cuatro cuerdas. En 1905 Yandel Henderson captó los movimientos en sentido horizontal, utilizando una tabla que se balanceaba unida a una serie de poleas; para el registro utilizó un quimógrafo. Pero fue Starr quien le dio el nombre al instrumento y elaboró un balistocardiógrafo indirecto en 1939, que inscribía las ondas en papel fotográfico. Pocos años después, Dock y Taubman diseñaron el primer balistocardiógrafo directo, usando unas barras metálicas ajustadas a las tibias del paciente. Henderson y Bixby mejoraron la precisión de la calibración a través de un microscopio. Autores como Hamilton, Dow, Thompson, Rappaport y Sprague describieron las ondas y su relación con los eventos del ciclo cardiaco. Su correlación con la fonocardiografía y el registro de los pulsos carotídeo, venoso y ápex-cardiograma fue de gran importancia para relacionar las ondas con los eventos fisiológicos. Starr demostró la relación de las ondas con el gasto cardiaco y su utilidad en el seguimiento de pacientes con insuficiencia cardiaca. Luis Carlos Barón Plata, cardiólogo bogotano, diseñó y elaboró un balistocardiógrafo directo, artesanal. Difirió del aparato de Dock y Taubman en la forma de obtener un registro de los movimientos corporales: diseñó un sensor que se colocaba contra la cabeza para registrar así el desplazamiento del cráneo. Con esto registró en forma adecuada las ondas del balistocardiograma. Su construcción fue muy austera y se conectaba mediante dos cables al electrocardiógrafo. Con el desarrollo de métodos <b>más</b> <b>sencillos</b> y precisos para observar y cuantificar la fisiología del corazón, la balistocardiografía perdió popularidad y casi ha desaparecido en todos los servicios de cardiología. The ballistocardiogram is {{the registry}} of the body&# 8217;s movement {{generated by the}} energy transmitted from the heart to the great arteries, {{as a consequence of}} the displacement of blood out from the ventricles and the recoil of the body. The energy transmitted cause a displacement in cephalic direction, and change in the opposite direction when the blood flows in the descending aorta. The system is formed by three parts: a mobile board in the horizontal plane, a system to register the corporal movement and an amplification system, to make possible generate a graphical register. The first description was made by Dr. JD. Gordon in 1877, when he registered the horizontal and vertical movements of a patient lying in a bed hanged by four supports. In the year 1905 Dr. Yandel Henderson was able to register the horizontal movements in a hanging panel and a series of pulleys; for the register he used a quimograph. Dr. Isaac Starr assigns the name to the instrument and made an indirect ballistocardiograph in 1939, printing the waves in photographic paper. Few years after, doctors Dock and Taubman designed the first direct ballistocardiograph, using metallic rods adjusted to the patient&# 8217;s tibia. Henderson and Bixby improved precision of the calibration by the use of a microscope. Authors like doctors Hamilton, Dow, Thompson, Rappaport and Sprague described the waves and their relationship with events of the cardiac cycle. The correlation with phonocardiography and the registry of carotid and venous pulse, and the apexcardiogram, was of great importance to relate the waves with physiological events. Dr. Starr recognized the relationship between the waves and cardiac output and it&# 8217;s usefulness in the follow-up of patients with heart failure. Luis Carlos Barón Plata, a cardiologist born in Bogotá, designed and handcrafted a direct ballistocardiograph. It differed from Dock and Taubman&# 8217;s machine in the way of obtaining the corporal movements: he designed a sensor placed in contact with the head in order to record the displacement of the cranium. With this he was able to register adequately the ballistocardiographic waves. The construction was austere and was connected by two cables to the electrocardiograph. With the development of simpler and more accurate methods to observe and quantify the heart&# 8217;s physiology, the ballistocardiography lost popularity, and that technique has disappeared from almost all the cardiology units...|$|R
40|$|This project {{offers a}} {{possible}} analysis of American gay drama by taking its most representative characters as the focal point. A suggestion on a methodological approach for these characters is presented before the character analysis properly starts, so that readers {{are free to}} judge {{whether it should be}} applied {{to the rest of the}} plays belonging to this genre or not. It is also worth mentioning that some plays have not been subject to much previous critical analysis, which enriches the conclusion of this project. A special emphasis is put on how these characters represent the struggle of the gay individual with the homophobic society they belong to. Therefore, by observing how they cope with the heteronormative world, it is easier to frame and appreciate the gay identity. Finally, an understanding of this sexual identity is offered, paying significant attention to its long-denied, and still not granted, social place. Este proyecto ofrece un posible análisis del teatro gay americano, tomando sus personajes más representativos como punto central. Previo al análisis de dichos personajes, se sugiere una propuesta metodológica crítica, de manera que el lector pueda juzgar la validez de ésta para el resto de obras que pertenecen a este género. Cabe destacar que algunas de las obras seleccionadas apenas han estado sujetas al ojo crítico, lo que enriquece la conclusión de este proyecto. La lucha del individuo homosexual respecto a la sociedad homófoba a la que pertenece recibe un énfasis especial. Por ello, al observar cómo estos personajes lidian con un mundo heteronormativo, es <b>más</b> <b>sencillo</b> encuadrar y entender la identidad gay. Finalmente, se ofrece una visión acerca de esta identidad sexual, prestando especial atención al lugar social que le corresponde, aún por conceder...|$|E
40|$|Este trabajo se ha realizado con el objetivo {{principal}} de mejorar sustancialmente uno de los procesos más importantes en el campo {{de la industria}} como lo son las plantas de tratamiento de agua y en nuestro caso particular la etapa de floculación que es donde se aglutinan las partículas contaminantes del agua mediante un proceso de agitación para posteriormente ser retiradas tras una fase de sedimentación. Gracias a la implementación de la automatización de procesos industriales se diseñará un sistema centralizado en donde se controlarán las condiciones de operación de cada uno de los motores que componen los agitadores y se supervisarán las variables de estado más representativas del proceso a través de una red de comunicación industrial y así lograr una operación óptima del sistema. Se evidenciará posterior a todo el desarrollo que la operación del sistema de modo automático y controlado por los operarios o por el personal a cargo del sistema es mucho <b>más</b> <b>sencillo</b> y eficiente que el sistema actual el cual opera de manera manual y a criterio de algunas condiciones no necesariamente reales. This {{work was}} carried out with the primary aim of substantially improving {{one of the most}} important processes in the field of industry as are the water treatment plants and in our particular case the flocculation stage is where contaminant particles coalesce of water through agitation and then are removed after a period of sedimentation. Thanks to the implementation of industrial process automation design a centralized system which controls the operating conditions of each of the motors that make the stirrers, and supervise the most representative state variables of the process through a network of industrial communication and thus achieve optimal operation of the system. Was evident after all the development that the system operation and controlled automatically by the operators or staff in charge of the system is much simpler and efficient than the current system which operates manual and in the view of some conditions are not necessarily real...|$|E
40|$|Esta investigación busca establecer cómo el periodismo actual recibe influencia de los temas discutidos en los medios sociales, especialmente las redes sociales. A través de la observación, se ha tratado de delimitar la función que cumplen las redes sociales y demostrar cómo los ciudadanos, a través de sus manifestaciones en las redes, también pueden determinar los temas a tratar en los diferentes medios. En nuestro análisis, queremos {{considerar}} tanto las ventajas que supone el uso de redes sociales como por ejemplo twitter o facebook a la hora de tratar los diferentes temas de actualidad (inmediatez, cercanía, mayor difusión…) como los inconvenientes (mal contraste de la información, poca fiabilidad de la fuente…). Todo esto se debe a que internet ha provocado que el acceso a la información sea cada vez <b>más</b> <b>sencillo</b> pero la red y en especialmente las redes sociales han conseguido alterar el tradicional esquema de “Fuente – Periodista – Lector”, provocando en numerosas ocasiones una información de mala calidad y errónea. Por eso debemos delimitar en esta investigación si debemos considerar a las redes sociales como un nuevo tipo de fuentes de información o simplemente como un canal que, a través del cual, podamos acceder a la verdadera fuente de informaciónThis investigation {{seeks to}} establish how the current journalism receives {{influence of the}} topics discussed in the social means, specially the social networks. Across the observation, {{it has been a}} question of delimiting the function that the social networks fulfill and demonstrating how the citizens, across his manifestations in the networks, also can of determining the topics to treat in the different means. In our analysis, we want to consider so much the advantages that there supposes the use of social networks as for example twitter or facebook at the moment of treating the different today's news (immediacy, nearness, major diffusion …) as the disadvantages (badly contrast of the information, few reliability of the source …). All that owes to itself that Internet it has provoked that the access to the information is increasingly simple but the network and in specially the social networks they have managed to alter the traditional scheme of " Source - Journalist - Reader ", provoking in numerous occasions an information of bad quality and erroneous. Because of it we must delimit in this investigation if we must consider to the social networks as a new type of sources of information or simply as a channel that, across which, we could accede to the real source of information...|$|E
40|$|Segmentation {{is a tool}} {{presented}} for representation and approximation of data, according {{to a set of}} appropriate models. These procedures have applications to many different domains, such as time series analysis, polygonal approximation, Air Traffic Control, [...] . Different heuristic and metaheuristic proposals have been introduced to deal with this issue. This thesis provides a novel multiobjective evolutionary method, analyzing the required general tools for the application evolutionary algorithms to real problems and the specific modifications required over the different steps of general proposals to adapt them to the segmentation domain. An introduction to the domain is presented by means of the design of a specific heuristic for segmentation of Air Traffic Control (ATC) data. This domain has a series of characteristics which make it difficult to be faced with traditional techniques: noisy data and a large number of measurements. The proposal works on two phases, using a pre-segmentation which introduces available domain information and applying a standard technique over this initial technique's results. Its results according to the presented domain, tested with a set of eight different representative trajectories, show competitive advantages compared to general approaches, which oversegmentate noisy data and, in some cases, exhibit poor scalability. This heuristic proposal shows the costly process of adapting available approaches and designing specific ones, along with the multi-objective nature of the problem, which requires the use of quality indicators for a proper comparison process. Applying evolutionary algorithms to segmentation provides several advantages, highlighting the fact that the problem dependance of heuristics make it costly to adapt these heuristics to new domains, as introduced by the designed heuristic to ATC. However, the practical application of these algorithms requires the study of a topic which has received little research effort from the community: stopping criteria. An evolutionary approach should contain a dynamic procedure which can determine when stagnation has taken place and stop the algorithm accordingly (as opposed to a-priori cost budgets, either in function evaluations or generations, which are usually applied for test datasets). Stopping criteria have been faced for single and multi-objective cases in this thesis. Single-objective stopping criteria have been approached proposing an active role of the stopping criteria, actively increasing the diversity in the variable space while tracking the updates in the fitness function. Thus, the algorithm reuses the information obtained for the stopping decision and feeds it to a stopping prevention mechanism in order to prevent problematic situations such as early convergence. The presented algorithm has been tested according to a set of 27 different functions, with different characteristics regarding their dimensionality, search space, local minima [...] . The results show that the introduced mechanisms enhance the robustness of the results, due to the improved exploration and the early convergence prevention. Multi-objective stopping criteria are faced with the use of progress indicators (comparison measures {{of the quality of the}} evolution results at different generations) and an associated data gathering tool. The final proposal uses three different progress indicators, (hypervolume, epsilon and Mutual Dominance Rate) and considers them jointly according to a decision fusion architecture. The stagnation analysis is based on the least squares regression parameters of the indicators values, including a normality analysis as well. The online nature of these algorithms is highlighted, preventing the recomputation of the indicators values which were present in other available alternatives, and also focusing on the simplicity of the final proposal, in order to reduce the cost of introducing it into available algorithms. The proposal has been tested with instances of the DTLZ algorithm family, obtaining satisfactory stops with a standard set of configuration values for the technique. However, there is a lack of quantitative measures to determine the objective quality of a stop and to properly compare its value to other alternatives. The multi-objective nature of the segmentation problem is analyzed to propose a multiobjective evolutionary algorithm (MOEA) to deal with it. This nature is analyzed according to a selection of available approaches, highlighting the difficulties which had to be faced in the parameter configuration in order to guide the processes to the desired solution values. A multi-objective a-posteriori approach such as the one presented allows the decision maker to choose from the front of possible final solutions the one which suits him best, simplifying this process. The presented approach chooses SPEA 2 as its underlying MOEA, analyzing different representation and initialization proposals. The results have been validated against a representative set of heuristic and metaheuristic techniques, using three widely extended curves from the polygonal approximation domain (chromosome, leaf and semicircle), obtaining statistically better results for almost all the different test cases. This initial MOEA approach had unresolved issues, such as the archiving technique complexity order, and also lacked the proper specific design considerations to adapt it to the application domain. These issues have been faced according to different improvements. First of all, an alternative representation is proposed, including partial fitness information and associated fitness-aware transformation operators (transformation operators which compute children fitness values according to their changes and the parents partial values). A novel archiving procedure is introduced according to the bi-objective nature of the domain, being one of them discrete. This leads to a relaxed Pareto dominance check, named epsilon glitches. Multi-objective local search versions of the traditional algorithms are proposed and tested for the initialization of the algorithm, along with the stopping criterion proposal which has also been adapted to the problem characteristics. The archive size in this case is big enough to contain all the different individuals in the optimal front, such that quality assessment is simplified and a simpler mechanism can be introduced to detect stagnation, according to the improvements in each of the possible individuals. The final evolutionary proposal is scalable, requires few configuration parameters and introduces an efficient dynamic stopping criterion. Its results have been tested against the original technique and the set of heuristic and metaheuristic techniques previously used, including the three original curves and also more complex versions of them (obtained with an introduced generation mechanism according to these original shapes). Even though the stopping results are very satisfactory, the obtained results are slightly worse than the original MOEA for the three simpler problem instances with the established configuration parameters (as was expected, due to the computational effort of the a-priori established number of generations and population size, based on the analysis of the algorithm's results). However, the comparison versus the alternative techniques stills shows the same statistically better results, and its reduced computational cost allows its application to a wider set of problems. La segmentación es una técnica creada para la representación y la aproximación de conjuntos de datos a través de un conjunto de modelos apropiados. Estos procedimientos tienen aplicaciones para múltiples dominios distintos, como el análisis de series temporales, la aproximación poligonal o el Control de Tráfico Aéreo. Se han hecho múltiples propuestas tanto de carácter heurístico como metaheurístico para lidiar con este problema. Esta tesis proporciona un nuevo método evolutivo multiobjetivo, analizando las herramientas generales necesarias para la aplicación de algoritmos evolutivos a problemas reales y las modificaciones específicas necesarias sobre los distintos pasos de las propuestas genéricas para adaptarlos al dominio de la segmentación. Se presenta una introducción al dominio mediante el diseño de una heurística específica para la segmentación de datos procedentes del Control de Tráfico Aéreo (CTA). Este dominio tiene una serie de características que dificultan la aplicación de técnicas tradicionales: datos con ruido y un gran número de muestras. La propuesta realizada funciona de acuerdo a dos fases, utilizando una presegmentación que introduce información del dominio disponible para posteriormente aplicar una técnica estándar sobre los resultados de esta técnica inicial. Sus resultados para el dominio presentado, probado con un conjunto de ocho trayectorias representativas distintas, presentan ventajas competitivas frente a los enfoques generales, que sobresegmentan los datos con ruido y, en algunos casos, presentan una mala escalabilidad. Esta propuesta heurística muestra el costoso proceso que implica adaptar los enfoques existentes o el diseño de otros nuevos, junto a la naturaleza multiobjectivo del problema, que precisa del uso de indicadores de calidad para realizar un proceso de comparación apropiado. La aplicación de algoritmos evolutivos a la segmentación tiene múltiples ventajas, destacando el hecho de la dependencia existente entre las heurísticas y el problema específico para el que han sido diseñadas, lo que hace que su adaptación a nuevos dominios sea costosa, como se ha introducido a través de la propuesta heurística para CTA. A pesar de ello, la aplicación práctica de estos algoritmos requiere el estudio de una faceta que ha recibido poca atención por parte de la comunidad desde el punto de vista de la investigación: los criterios de parada. Un enfoque evolutivo debería tener una técnica dinámica que pueda detectar cuando se ha producido el estancamiento del proceso, y parar el algoritmo de acuerdo a ello (de manera opuesta a los criterios a-priori que establecen un coste predeterminado, expresado como número de evaluaciones o de generaciones, y que son habitualmente aplicados para los conjuntos de datos de prueba). Los criterios de parada se han afrontado tanto desde el caso de un único objetivo como desde el caso multiobjectivo en esta tesis. Los criterios de parada para un único objetivo se han abordado proponiendo un rol activo para el criterio, aumentando la diversidad en el espacio de variables de una manera activa, mientras se monitorizan los cambios en la función objetivo. De esta manera, el algoritmo reutiliza la información obtenida para la decisión de parada y la inserta en un mecanismo de prevención de la parada con la finalidad de prevenir situaciones problemáticas como la convergencia temprana. El algoritmo presentado se ha probado sobre un conjunto de 27 funciones distintas, con diferentes características respecto a su dimensionalidad, espacio de búsqueda, mínimos locales [...] . Los resultados muestran que los mecanismos introducidos mejoran la robustez de los resultados, haciendo uso de la exploración mejorada y la prevención de la convergencia temprana. Los criterios de parada multiobjetivo se han planteado con el uso de indicadores de avance (medidas comparativas de la calidad de los resultados de la evolución en diferentes generaciones) y una herramienta de recolección de datos asociada. La propuesta final utiliza tres indicadores de avance distintos (hypervolumen, epsilon y ratio de dominancia mutua) y los considera de una manera conjunta de acuerdo a una arquitectura de fusión de decisiones. El análisis del estancamiento se basa en los parámetros de una regresión de mínimos cuadrados sobre los valores de los indicadores, incluyendo asimismo un análisis de normalidad. Se recalca la naturaleza online de estos algoritmos, evitando el recálculo de los valores de los indicadores que estaba presente en otras alternativas disponibles, y también focalizándose en la simplicidad de la propuesta final, de manera que se facilite el proceso de introducir el criterio en los algoritmos existentes. La propuesta ha sido probada con instancias de la familia de algoritmos DTLZ, obteniendo resultados de parada satisfactorios con un conjunto de valores de configuración estándar para la técnica. Sin embargo, existe una falta de medidas cuantitativas para determinar la calidad objetiva de una parada, así como para comparar de manera apropiada su valor frente al de otras alternativas. La naturaleza multiobjetivo del problema de segmentación se ha analizado para proponer un algoritmo evolutivo multiobjetivo (AEMO) para resolverlo. Esta naturaleza ha sido analizada de acuerdo a una selección de los enfoques disponibles, destacando las dificultades que se tienen que afrontar en la configuración de los parámetros de cara a guiar el proceso hacia los valores de solución deseados. Un enfoque multiobjetivo a-posteriori como el que se ha presentado permite al responsable elegir del frente de posibles soluciones finales aquella que encaja mejor, simplificando este proceso. El enfoque presentado ha elegido SPEA 2 como algoritmo de base, analizando diferentes propuestas de inicialización y representación. Los resultados se han validado frente a un conjunto significativo de técnicas heurísticas y metaheurísticas, utilizando tres curvas ampliamente extendidas en el dominio de la segmentación poligonal (cromosoma, hoja y semicírculo), obteniendo resultados estadísticamente mejores para la casi totatilidad de los casos de prueba. Esta propuesta inicial de AEMO presentaba una serie de problemas sin resolver, como el orden de complejidad de la técnica de almacenaje, y además carecía de las consideraciones específicas de diseño para su adaptación al dominio de aplicación. Estos problemas se han afrontado de acuerdo a diferentes mejoras. Por un lado, se ha propuesto una representación alternativa, incluyendo información parcial de la función objetivo y operadores de transformación informados (operadores de transformación que calculan los valores de la función objetivo de los hijos de acuerdo a los cambios realizados y los valores parciales de los padres). Una nueva técnica de almacenaje se ha introducido de acuerdo a la naturaleza biobjetivo del dominio, siendo uno de ellos además discreto. Esta naturaleza ha llevado a la aplicación de una forma relajada de dominancia de Pareto, que hemos denominado pulsos épsilon. Versiones multiobjetivo de los algoritmos tradicionales de búsqueda local han sido propuestas y probadas para la inicialización del algoritmo, junto con la propuesta de criterio de parada, que también ha sido adaptada a las características del problema. En este caso, el tamaño del almacén es suficientemente grande como para almacenar todos los individuos del frente óptimo, de manera que las técnicas de análisis de calidad de los frentes se simplifican, y un mecanismo <b>más</b> <b>sencillo</b> puede ser introducido para detectar el estancamiento, de acuerdo a las mejoras en cada uno de los individuos posibles. La propuesta evolutiva final es escalable, requiere pocos parámetros de configuración e introduce un criterio de parada dinámico y eficiente. Sus resultados se han probado frente a la técnica original y el conjunto de técnicas heurísticas y metaheurísticas previamente utilizadas, incluyendo las tres curvas originales y versiones más complejas de las mismas (obtenidas con un mecanismo de generación incluido de acuerdo a estas tres formas originales). A pesar de que los resultados de parada son muy satisfactorios, los resultados obtenidos son ligeramente peores que el AEMO original para las tres instancias del problema más simples, utilizando el conjunto de parámetros de configuración establecidos (como cabía esperar, dado el coste computacional del número de generaciones y tamaño de la población establecidos a priori, basados en el análisis de los resultados del algoritmo). En cualquier caso, la comparación frente a las técnicas alternativas todavía presenta los mismos resultados estadísticamente mejores, y las mejoras en el coste computacional permiten su aplicación a un mayor conjunto de problemas. Programa Oficial de Doctorado en Ciencia y Tecnología InformáticaPresidente: Pedro Isasi Viñuela. - Secretario: Rafael Martínez Tomás. - Vocal: Javier Segovia Pére...|$|E
40|$|During {{the last}} {{years there has been}} an {{enormous}} advance in FPGAs. Traditionally, FPGAs have been used mainly for prototyping as they offer significant advantages at a suitable low cost: flexibility and verification easiness. Their flexibility allows the implementation of different generations of a given application and provides space to designers to modify implementations until the very last moment, or even correct mistakes once the product has been released. Second, the verification of a design mapped into an FPGA is easier and simpler than in ASICs which require a huge verification effort. Additionally to these advantages, the technological advances have added great capabilities and per- formance to FPGAs, and even though FPGAs are not as efficient as ASICs in terms of performance, area or power, it is true that nowadays they can provide better performance than standard or digital signal processor (DSP) based systems. This fact, in conjunction with the enormous logic capacity allowed by today’s technologies, makes FPGAs an attractive choice for implementation of complex digital systems. Furthermore, with their newly acquired digital signal processing capabilities, FPGAs are now expanding their traditional prototyping roles to help offload computationally intensive functions from standard processors. This Thesis is focused on the last point, the use of FPGAs to accelerate computationally intensive applications. The use of FPGAs for hardware acceleration is an active research field. However, there are still several challenges concerning the use of FPGAs as accelerators: • Availability of Cores. • Capability and performance of FPGAs. • Methods, algorithms and techniques suited for FPGAs. • Design tools. • Hardware-Software co-design and integration. Studying in depth each one of these five challenges related to hardware acceleration is not feasible in just one Thesis. The great variety of applications that can be accelerated and the different features among them imply that the complexity of each task is high. Therefore, in this Thesis we have chosen one subset of applications to be studied, dealing with the implementation of a real application of this subset. Selecting a complex subset of applications, in our case Monte Carlo simulations, allows us to make a general analysis of the main topic, hardware acceleration, from the study, analysis and design of a particular application. This subset of applications has several features shared with other applications and allows us to make a general analysis of the main topic, hardware acceleration, from the study, analysis and design of a given application. Specifically, we have selected a financial application, the Monte Carlo based LIBOR Market Model. Developing an FPGA application from scratch is almost impossible and availability of cores is a must for shorten development time. Following this idea, one of the main objectives is to study the common elements that {{play a key role in}} Monte Carlo simulations and in our target application (and shared with many other applications). Two common elements have been outstood: • The random number generators that are required for the underlying random variables, • Floating-point operators, which are the base elements for implementing the mathematical models that are evaluated. In this way, the first objective of this Ph. D. Thesis is the study, design and implementation of random number generators. In particular, we have focused on Gaussian random number generation and the implementation of a complete generator compatible with variance reduction techniques that can be used for our target application and for other applications. In this field we have developed a high-quality high-performance Gaussian random number generator which is parameterizable and compatible with the also developed parameterizable Latin Hypercube core and a high performance Mersenne Twister generator. Research results in this field demonstrate that random number generation is ideal for hardware acceleration, as an isolated core or within bigger accelerators. Meanwhile, the second objective has dealt with the implementation of efficient and FPGA-oriented mathematical operators (both basic and complex and using floating-point arithmetic). We focused on the design, development and characterization of libraries of components. Instead of focusing on the algorithms of the operators, our approach has been to study how the format can be simplified to obtain operators that are better suited for FPGAs and present better performance. One important goal searched here was to achieve libraries of general purpose components that can be reused in several applications and not just in a particular target application. Different design decisions have been studied and analyzed, and from this analysis, the impact of the overhead due to some of the floating-point standard features has been determined. The format overhead implies a major use of resources and reducing it is a must to obtain operators, independently of what underlying calculation algorithm, that are better suited for FPGAs while present better performances. In particular, the handling of denormalized numbers has a major impact on the FPGA operators. Following the results obtained in that studied, we have discussed and selected a set of features that implies improved performance and reduced resources. This set, has been chosen to design two additional hardware FPGAs-oriented libraries that ensure (or even improve) the accuracy and resolution given by the standard. The operators of these libraries are the base components for the implementation of target application. Additionally, a second analysis has been carried out to study the capabilities of FPGAs to implement complex datapaths. This analysis shows the huge capabilities of current FPGAs which allow up to hundreds of single floating-point operators. Although this capacity, this second analysis has also demonstrate how the working frequency of the operators is severely affected by the routing of their elements when the operators are not isolated and a high percentage of the resources of an FPGA are used. Related to the target application, a third objective of this work was to deepen on the implementation of a particular operator, the exponentiation function. This operator is required in many scientific and financial simulations. Its complexity and the lack of previous general purpose implementations have deserved special attention. We have developed and presented an accurate exponentiation operator for FPGAs based on the straightforward translation of xy into a chain of sub-operators and on the FPGA flexibility which allows tailored precisions. Taking advantage of this flexibility, the provided error analysis focused on determining which precisions are needed in the partial results and in the internal architectures of the sub-operators to obtain an accurate operator with a maximum error of one u. Finally, the integration of this error analysis and the development of the operator within the FloPoCo project have allowed to automatize the generation of exponentiation operators with variable precisions. The next objective we tackle was related to the global purpose of the Thesis of validating all the previously developed elements for the implementation of a complex Monte Carlo simulation which involves all the features that can be found in Monte Carlo simulations. In this way, we have deal with the implementation of the target application, the LIBOR Market Model (LMM). Special attention was devoted to all the features, requirements and circumstances that affect to the performance of the accelerator. A complete LMM hardware core has been developed and its results validated against the original software implementation. Three main features were analyzed: • Correctness of the results obtained. • Accuracy. • Speedup factors obtained by the global application and by each of the main components. Finally, the last objective was the integration of the hardware accelerator within the original software application. All issues related to the communication mechanism are studied putting special focus on how performance is affected by data transfers and by the hardware-software partitioning policy implemented. Following the partitioning policy selected, we have developed the infrastructure (both hardware and software) required to make possible the integration of our accelerator within a software application. A mechanism, based on the use of two RAM memory zones and a PCI-E core with Bus Master capabilities in the FPGA, has been proposed and implemented. And it has allowed us to extend the intrinsic parallelism of Monte Carlo simulations to how the CPU and the FPGA work together. In this way, we exploit the CPU to work in parallel with the FPGA, overlapping their execution times. Hence, the software execution time affecting the performance is reduced to the initial and final processing and to the product valuation in case it is slower that LMM plus the random generator in the FPGA. With this scheme we have achieved high speedups, around 18 times, and close to the theoretical limit for our cases: when there is no software not ported to Hardware or which execution is overlapped with the FPGA execution (the LMM plus RNG achievable speedup). In this case, the speedup achieved could be considerably improved using new FPGAs and several LMM cores in parallel. Durante los últimos años ha habido un enorme avance en la tecnología y capacidades de las FPGAs. Tradicionalmente, las FPGAs se han utilizado principalmente para el desarrollo de prototipos, ya que ofrecen importantes ventajas a un bajo coste: flexibilidad y facilidad de verificación. Su flexibilidad permite la implementación de las diferentes versiones de una aplicación determinada y permite a los diseñadores modificar las implementaciones hasta el último momento, o incluso corregir errores una vez que el producto esta siendo utilizado. En segundo lugar, la verificación de un diseño en una FPGA es más fácil y <b>más</b> <b>sencillo</b> que en ASIC, donde requieren un esfuerzo de verificación enorme. Además de estas ventajas, los avances tecnológicos han permitido FPGAs con grandes capacidades a la vez que se ha aumentado su rendimiento. Y aunque las FPGAs no sean tan eficientes como los ASIC en términos de rendimiento, recursos o el consumo de potencia, hoy en día pueden ofrecer un mejor rendimiento que un sistema estándar o que uno basado en procesadores digitales de señal (DSP). Esto, junto con la enorme capacidad de recursos lógicos alcanzada por las tecnologías de hoy, hace de las FPGAs una opción atractiva para la implementación de sistemas digitales complejos. Además, con su recientemente adquirida capacidad de procesamiento de señal digital, las FPGAs están ampliando su rol tradicional de prototipos al rol de coprocesador para descargar de cálculos intensivos a los procesadores estándar. Esta tesis se centra en el último punto, el uso de FPGAs para acelerar las aplicaciones com- putacionalmente intensivas. El uso de FPGAs para la aceleración de hardware es un área activa de investigación. Sin embargo, todavía hay varios desafíos relativos al uso de FPGAs como aceleradores: • Disponibilidad de cores de implementación. • Capacidad y rendimiento de las FPGAs. • Necesidad de métodos, algoritmos y técnicas adecuadas para FPGAs. • Herramientas de diseño. • Co-diseño de Hardware-Software y su integración El estudio detallado de cada uno de estos cinco desafíos relacionados con la aceleración de hardware no es factible en tan sólo una tesis. La gran variedad de aplicaciones que pueden ser aceleradas y las diferentes características entre ellas, implica que la complejidad de cada tarea es alta. Por lo tanto, en esta tesis se ha elegido un conjunto de aplicaciones a estudiar, y se ha llevado a cabo la implementación de una aplicación real de este subgrupo. La selección de un subconjunto de aplicaciones complejas, en nuestro caso las simulaciones Monte Carlo, nos permite hacer un análisis general de la aceleración de hardware, nuestro campo principal, desde el estudio, análisis y diseño de una aplicación en particular. Este conjunto de aplicaciones tiene varias características compartidas con otras aplicaciones y nos permite hacer un análisis general de la aceleración de hardware desde el estudio, análisis y diseño de una aplicación dada. En concreto, hemos seleccionado una aplicación financiera, la simulación del LIBOR Market Model basado en Monte Carlo. El desarrollo de las aplicaciones en FPGAs a partir de cero es casi imposible y la disponibilidad de los cores es una necesidad para acortar el tiempo de desarrollo. Siguiendo esta idea, uno de nuestros principales objetivos es el estudio de los elementos comunes que juegan un papel clave en las simulaciones de Monte Carlo y en la aplicación seleccionada (y compartidos con muchas otras aplicaciones). Dos elementos comunes han sido destacados: • Los generadores de números aleatorios que se requieren para las variables aleatorias subyacentes. • Los operadores de punto flotante, que son los elementos base para implementar los modelos matemáticos que se evalúan. De esta manera, el primer objetivo de esta Tesis es el estudio, diseño e implementación de generadores de números aleatorios. En particular, nos hemos centrado en la generación de números aleatorio con distribución Gaussiana y en la implementación de un generador completo y compatible con técnicas de reducción de varianza que se utilizan en la aplicación seleccionada y en otras aplicaciones. En este campo de investigación hemos desarrollado un generador de números aleatorios gaussianos de alta calidad y alto rendimiento. A su vez, este generador es parametrizable y compatible con el módulo parametrizable de hipercubo latino también desarrollado y con un generador Mersenne Twister de alto rendimiento. Los resultados de investigación en este campo demuestran que la generación de números aleatorios es idónea para la aceleración de hardware, tanto como un núcleo aislado o integrado en aceleradores mayores. El segundo objetivo se ha ocupado del desarrollo de operadores matemáticos eficientes y orientados a FPGAs (tanto básicos como complejos y con aritmética de punto flotante). Nos hemos centrado en el diseño, desarrollo y caracterización de las librerías de componentes. En lugar de centrarnos en los algoritmos de los operadores, nuestro enfoque ha sido la de estudiar cómo el formato se puede simplificar para obtener operadores más adecuados para FPGAs y que a su vez presenten un mejor rendimiento. Un objetivo importante aquí buscado ha sido lograr librerías de componentes de propósito general que pueden ser reutilizados en varias aplicaciones y no sólo en una aplicación seleccionada en esta tesis. Diferentes decisiones de diseño se han estudiado y analizado. De este análisis, hemos determinado el impacto de la sobrecarga debido a algunas de las características del estándar de punto flotante. La sobrecargas que presenta este formato implican un mayor uso de los recursos y su reducción es una necesidad para obtener operadores más adecuados para FPGAs y con mejor rendimiento, independientemente de lo que el algoritmo de cálculo subyacente. En particular, el manejo de los números denormalizados tiene un gran impacto en los operadores de FPGA. Con los resultados obtenidos en ese estudio, hemos analizado y seleccionado un conjunto de características que implican un mejor rendimiento y una reducción de los recursos. Este conjunto, ha sido elegido para diseñar dos librerías adicionales para FPGA orientadas a garantizar (o incluso mejorar) la precisión y la resolución dada por el estándar. Los operadores de estas librerías son los componentes básicos para la implementación de la aplicación seleccionada. Además, un segundo análisis se ha llevado a cabo para estudiar las capacidades de los FPGAs para implementar complejos arquitecturas de datos. Este análisis muestra las enormes capacidades de FPGAs actuales que permiten a la implementación de cientos de operadores punto flotante en la misma FPGA. A pesar de esta capacidad, este segundo análisis también demuestra cómo la frecuencia de trabajo de los operadores se ve gravemente afectada por el interconexionado de sus elementos cuando los operadores no están aislados y se están utilizando un alto porcentaje de los recursos de la FPGA. Relacionado con la aplicación de destino, un tercer objetivo de este trabajo ha sido profundizar sobre la implementación de un operador en particular, la función exponenciación. Este operador es utilizado en muchas simulaciones científicas y financieras. Su complejidad, y la falta de las anteriores implementaciones de propósito general han merecido una atención especial. Hemos desarrollado y presentado un operador exponenciación exacto para FPGAs basado en la traducción directa de xy en una cadena de sub-operadores y en la flexibilidad de las FPGA que permite precisones a medida. Tomando ventaja de esta flexibilidad, el análisis de error se centró en determinar que lprecisiones son necesarias en los resultados parciales y en la arquitectura interna de los operadores de sub-para obtener un operador exacto con un error máximo de un ulp. Por último, la integración de este análisis de error y el desarrollo del operador en el proyecto FloPoCo han permitido automatizar la generación de los operadores de exponenciación con precisiones variables. El segundo objetivo se ha ocupado del desarrollo de operadores matemáticos eficientes y orientados a FPGAs (tanto básicos como complejos y con aritmética de punto flotante). Nos hemos centrado en el diseño, desarrollo y caracterización de las librerías de componentes. En lugar de centrarnos en los algoritmos de los operadores, nuestro enfoque ha sido la de estudiar cómo el formato se puede simplificar para obtener operadores más adecuados para FPGAs y que a su vez presenten un mejor rendimiento. Un objetivo importante aquí buscado ha sido lograr librerías de componentes de propósito general que pueden ser reutilizados en varias aplicaciones y no sólo en una aplicación seleccionada en esta tesis. Diferentes decisiones de diseño se han estudiado y analizado. De este análisis, hemos determinado el impacto de la sobrecarga debido a algunas de las características del estándar de punto flotante. La sobrecargas que presenta este formato implican un mayor uso de los recursos y su reducción es una necesidad para obtener operadores más adecuados para FPGAs y con mejor rendimiento, independientemente de lo que el algoritmo de cálculo subyacente. En particular, el manejo de los números denormalizados tiene un gran impacto en los operadores de FPGA. Con los resultados obtenidos en ese estudio, hemos analizado y seleccionado un conjunto de características que implican un mejor rendimiento y una reducción de los recursos. Este conjunto, ha sido elegido para diseñar dos librerías adicionales para FPGA orientadas a garantizar (o incluso mejorar) la precisión y la resolución dada por el estándar. Los operadores de estas librerías son los componentes básicos para la implementación de la aplicación seleccionada. Además, un segundo análisis se ha llevado a cabo para estudiar las capacidades de los FPGAs para implementar complejos arquitecturas de datos. Este análisis muestra las enormes capacidades de FPGAs actuales que permiten a la implementación de cientos de operadores punto flotante en la misma FPGA. A pesar de esta capacidad, este segundo análisis también demuestra cómo la frecuencia de trabajo de los operadores se ve gravemente afectada por el interconexionado de sus elementos cuando los operadores no están aislados y se están utilizando un alto porcentaje de los recursos de la FPGA. Relacionado con la aplicación de destino, un tercer objetivo de este trabajo ha sido profundizar sobre la implementación de un operador en particular, la función exponenciación. Este operador es utilizado en muchas simulaciones científicas y financieras. Su complejidad, y la falta de las anteriores implementaciones de propósito general han merecido una atención especial. Hemos desarrollado y presentado un operador exponenciación exacto para FPGAs basado en la traducción directa de xy en una cadena de sub-operadores y en la flexibilidad de las FPGA que permite precisones a medida. Tomando ventaja de esta flexibilidad, el análisis de error se centró en determinar que lprecisiones son necesarias en los resultados parciales y en la arquitectura interna de los operadores de sub-para obtener un operador exacto con un error máximo de un ulp. Por último, la integración de este análisis de error y el desarrollo del operador en el proyecto FloPoCo han permitido automatizar la generación de los operadores de exponenciación con precisiones variables...|$|E

