1|10000|Public
40|$|Projecte realitzat en col. laboració amb Helsinki University of TechnologyFour MERIS Case-II Water Processors are studied, {{compared}} and evaluated: Coastal Case 2 Regional Processor, Boreal Lakes Processor, Eutrophic Lakes Processor and FUB/Wew Water Processor. In situ {{data from}} the Baltic Sea {{have been used to}} evaluate the water constituent estimations. In addition, the effect of adjacency effect ICOL on the estimation has been analyzed. For this purpose, a set of tools has been developed to automatise the evaluation process and <b>make</b> <b>the</b> <b>comparisons</b> <b>with</b> large amounts of MERIS products and in situ data. Results show that the processors are far from being accurate. Both absolute errors and estimation concentrations are in the same order of magnitude. Coastal Case 2 Regional provides the best performance in the studied area of the Baltic Sea...|$|E
40|$|As {{part of a}} large {{project to}} {{calibrate}} all the Schmidt plates of the ESO Quick blue and the red survey, CCD-photometry in B and R has been obtained for galaxies on 67 different survey fields. On these frames synthetic-aperture photometry is applied in order to present the data in a way which <b>makes</b> <b>the</b> <b>comparison</b> <b>with</b> photographic photometry easy...|$|R
30|$|Hence, {{the first}} step of the {{methodology}} is a requirement for not only any new stego algorithm but also new feature sets/steganalyzers, willing to present its performances: a sufficient number of images for the stego algorithm and the steganalyzer used to test it have to be assessed in order to have stable results (i.e., with a small enough standard deviation of its results to <b>make</b> <b>the</b> <b>comparison</b> <b>with</b> current state of the art techniques meaningful).|$|R
40|$|National audienceIn this work, {{we present}} a new {{arithmetic}} unit for the multiplication in prime finite fields. This modular multiplier {{is intended to be}} part of an hardware accelerator dedicated to Hyper-Elliptic Curve Cryptography. Various configurations of the multiplier have been implemented in different FPGAs. We will present the results we obtained in terms of circuit area and computation time and we will <b>make</b> <b>the</b> <b>comparison</b> <b>with</b> <b>the</b> actual state-of-the-art in mplementation of modular multipliers...|$|R
30|$|Here, {{we present}} closed-form {{expressions}} for the MSEE resulting {{from some of}} the considered algorithms. We <b>make</b> <b>the</b> <b>comparison</b> <b>with</b> <b>the</b> Cramer-Rao bound (CRB) and the modified CRB (MCRB), which are fundamental lower bounds on the MSEE of unbiased estimates. Note that these bounds do {{not take into account the}} a priori distribution of the parameter to be estimated. For the derivation of the (M)CRBs in the context of an amplify-and-forward relaying system, we refer to [15].|$|R
5000|$|The Centauri Republic {{reflects}} many imperial {{cultures of}} Earth, though J. Michael Straczynski specifically <b>makes</b> <b>the</b> <b>comparison</b> <b>with</b> [...] "the British Empire {{once upon a}} time... It was a great military power. But slowly, as can happen, they grew content, and lazy, and gradually their own empire began to slip between their fingers". In the film The Gathering, Babylon 5 ambassador Londo Mollari laments that Centauri Prime {{has been reduced to}} [...] "A tourist attraction... See the Centauri Republic, nine to five... Earth time"'.|$|R
40|$|The {{modelling}} {{of massive}} star evolution {{is a complex}} task, and is very sensitive to the way physical processes (such as convection, rotation, mass loss, etc.) are included in stellar evolution code. Moreover, the very high observed fraction of binary systems among massive stars <b>makes</b> <b>the</b> <b>comparison</b> <b>with</b> observations difficult. In this paper, we focus on discussing the uncertainties linked to the modelling of convection and rotation in single massive stars. Comment: 8 pages, {{to appear in the}} proceedings of the Second BRITE-Constellation Science Conference: small satellites - big science, Innsbruck, 201...|$|R
40|$|The {{observation}} of several low energy events during the SN 1987 A burst made by Kamiokande-II is somewhat embarrassing {{when compared with}} the theoretical expectations and with the observations of IMB, and has an important weight in the attempts to use these data to learn on the properties of the supernova neutrinos. We show however that the distributions in space and in energy suggest the presence of a few events due to background, and this <b>makes</b> <b>the</b> <b>comparison</b> <b>with</b> theory and with IMB less problematic. PACS: 97. 60. Bw Supernovae; 95. 85. Ry Neutrino astronomical observation; 95. 55. Vj Neutrino detectors. 1 Motivation an...|$|R
40|$|We {{study the}} {{ultraviolet}} asymptotics in non-simply laced affine Toda theories considering them as perturbed non-affine Toda theories, which possess the extended conformal symmetry. We calculate the reflection amplitudes, in non-affine Toda theories {{and use them}} to derive the quantization condition for the vacuum wave function, describing zero-mode dynamics. The solution of this quantization conditions for the ground state energy determines the UV asymptotics of the effective central charge. These asymptotics are in a good agreement with Thermodynamic Bethe Ansatz(TBA) results. To <b>make</b> <b>the</b> <b>comparison</b> <b>with</b> TBA possible, we give the exact relations between parameters of the action and masses of particles as well as the bulk free energies for non-simply laced affine Toda theories. 2...|$|R
40|$|In the {{framework}} of Anisotropic Chromo Dynamics, a non-perturbative model of QCD based on magnetic condensation in the vacuum, we develope {{a picture of the}} Pomeron. Within this model we are able to calculate the diffractive contribution to deep inelastic scattering in the small Bjorken x region, actually probed at Hera. We calculate the diffractive structure function of <b>the</b> proton and <b>make</b> <b>the</b> <b>comparison</b> <b>with</b> <b>the</b> experimental data of Zeus and H 1 collaborations. Good agreement with the experimental data, both for the diffractive structure function $F_ 2 ^D$ and for the ''structure function'' of the Pomeron, is achieved. Comment: 13 pag., Latex 2. 09 (or 2 e), 5 figures in separate. eps file...|$|R
40|$|The one-loop quantum {{corrections}} for BTZ {{black hole}} are considered using the dimensionally reduced 2 D model. Cases of 3 D minimal and conformal coupling are analyzed. Two cases are considered: minimally coupled and conformally coupled 3 D scalar matter. In the minimal case, the Hartle-Hawking and Unruh vacuum states are defined {{and the corresponding}} semiclassical corrections of the geometry are found. The calculations are done for the conformal case too, in order to <b>make</b> <b>the</b> <b>comparison</b> <b>with</b> <b>the</b> exact results obtained previously in the special case of spinless black hole. Beside that we find exact corrections for AdS 2 black hole for 2 D minimally coupled scalar field in the Hatrle-Hawking and Boulware state...|$|R
40|$|AbstractAmong the {{innovations}} aimed at tackling the transportation issues in urban areas, {{one of the}} most promising solutions is the possibility of making virtual trains of vehicles so as to provide a new kind of transportation system. Even if this kind of solution is now widespread in literature, some difficulties still need to be resolved. For instance, one must find solutions to <b>make</b> <b>the</b> crossing of the train possible while maintaining train composition (trains must not be split) and safety conditions. This paper proposes a multi-level decision process aimed at dealing with this issue. This proposal is based on dynamic adaptation of train parameters which lead to trains crossing without stopping any of them. Results, obtained in simulations, <b>make</b> <b>the</b> <b>comparison</b> <b>with</b> a classical crossing strategy...|$|R
40|$|The {{asymmetry}} in the conductance peaks {{shown in}} recent PCT and STM measurements {{may be a}} signature of the d-wave gap. This is based on our model for the tunneling density of states (DOS) which incorporates the group velocity, tunneling directionality, and a realistic band structure. We also <b>make</b> <b>the</b> <b>comparison</b> <b>with</b> <b>the</b> isotropic s-wave and extended s-wave gap symmetries. Whereas the asymmetry of the conductance peaks for the isotropic s-wave tends to reflect the normal state DOS, the asymmetry for the extended s-wave is opposite {{to that of the}} d-wave gap. The model shows that a change in the gap symmetry from d-wave to extended s-wave may be accompanied by a switching in the conductance peaks asymmetry. Comment: 1 page, 1 figure, RevTe...|$|R
40|$|The final {{publication}} {{is available}} at link. springer. com. International audienceTermination analyzers generally synthesize ranking functions or relations, which represent checkable proofs of their results. In [], we proposed an approach for conditional termination analysis based on abstract fixpoint computation by policy iteration. This method {{is not based on}} ranking functions and does not directly provide a ranking relation, which <b>makes</b> <b>the</b> <b>comparison</b> <b>with</b> existing approaches difficult. In this paper we study the relationships between our approach and ranking functions and relations, focusing on extensions of linear ranking functions. We show that it can work on programs admitting a specific kind of segmented ranking functions, and that the results can be checked by the construction of a disjunctive ranking relation. Experimental results show the interest of this approach...|$|R
40|$|University of Pisa {{participated in}} the {{international}} activity aimed at understanding Boiling Water Reactor (BWR) stability performance. The activity is based upon measured data taken from the Ringhals BWR Unit in Sweden. The original BWR data were elaborated to <b>make</b> easier <b>the</b> <b>comparison</b> <b>with</b> cod calculation results. The University of Pisa prepared comments {{in relation to the}} data processing...|$|R
30|$|In this contribution, {{we present}} two pilot-based and two space-alternating {{generalized}} expectation-maximization (SAGE) [10] algorithms for estimating at the destination both the overall channel gain and (unlike the cascaded channel estimation from [5]) the overall noise variance. In contrast with [6 â€“ 9], the proposed algorithms {{also take a}} priori channel knowledge into account. We restrict our attention to a low-complexity amplify-and-forward relay, which does not add pilot symbols of its own nor performs channel estimation. We derive in closed form the joint a priori distribution of the parameters to be estimated and use this distribution to derive an approximately maximum a posteriori (MAP) estimate and an approximation of the LMMSE estimate. We investigate the mean-square estimation error (MSEE) and frame error rate (FER) performance resulting from these estimates and <b>make</b> <b>the</b> <b>comparison</b> <b>with</b> <b>the</b> performance that results from the joint ML channel gain and the noise variance estimates from [8].|$|R
40|$|Abstract. We {{report the}} first {{result of a}} {{supernova}} search program designed to measure {{the evolution of the}} supernova rate <b>with</b> redshift. To <b>make</b> <b>the</b> <b>comparison</b> <b>with</b> local rates more significant we copied, as much as possible, the same computation recepies as for the measurements of local rates. Moreover, we exploited the multicolor images and the photometric redshift technique to characterize the galaxy sample and accurately estimate the detection efficiency. Combining our data with the recently published meaurements of the SN Ia rate at different redshifts, we derived the first, direct measurement of the core collapse supernova rate at z = 0. 26 as rcc = 1. 45 + 0. 55 − 0. 45 h 2 SNu [h=H 0 / 75]. This is a factor three (± 50 %) larger than the local estimate. The increase for a look back time of ”only ” 2. ...|$|R
40|$|In this article, {{the basic}} {{principles}} of the scaling procedure are first reviewed by a presentation of scale factors. Then, taking an idealized example of a brittle volcanic cone intruded by a viscous magma, the way to choose appropriate analogue materials for both the brittle and ductile parts of the cone is explained by the use of model ratios. Lines of similarity are described to show that an experiment simulates a range of physical processes instead of a unique natural case. The pi theorem is presented as an alternative scaling procedure and discussed through the same idealized example to <b>make</b> <b>the</b> <b>comparison</b> <b>with</b> <b>the</b> model ratio procedure. The appropriateness of the use of gelatin as analogue material for simulating dyke formation is investigated. Finally, the scaling of some particular experiments such as pyroclastic flows or volcanic explosions is briefly presented to show the diversity of scaling procedures in volcanology...|$|R
40|$|Considering the {{barriers}} to discussions of racism, we start by acknowl-edging our appreciation for the feedback and insights of the editors of The Counseling Psychologist {{as well as those}} of the responders. Spanierman and Poteat (2005 [this issue]) note that racist incidents are “most easily compara-ble with the established notion of trauma when they are overt and distinct events experienced directly by an individual. Divergence from one or more of these characteristics <b>makes</b> <b>the</b> <b>comparison</b> <b>with</b> <b>the</b> traditional understand-ing of trauma less direct ” (p. 517). In short, racist incidents perpetrated at the individual level by an overtly racist perpetrator that involve verbal, physical, or some other type of abuse or assault fit the standard definition of trauma. In fact, these incidents would be traumatic regardless of the motivation (racist or otherwise). However, viewing racist incidents through the narrow lens of overt, individual racism removes the responsibility of action against covert and/or institutionalized racism. To assist counselors and researchers in identifying potentially traumati...|$|R
40|$|This {{bachelor}} thesis {{deals with}} the issue of indebtedness of households (individuals) in a time of economic crisis. The theoretical part devotes attention to emergence of the economic crisis. Furthermore, I focus on how this crisis spread in the Czech Republic and what measures our country has taken to overcome the crisis. I will compare the situation in Slovakia and also I will <b>make</b> <b>the</b> <b>comparison</b> <b>with</b> <b>the</b> states of the European Union. I pointed out the main motivations which are the most common reasons leads to debts and the structure of the debts. I realized which financial institutions operate on the Czech market, and I introduced here overview of debt. In the thesis is also mentioned, what the risks are in the event of default and the ways off the situation. The practical part analyzes the development of household indebtedness and to try to discover the causes of indebtedness, which caused the increasing of interest in family grants, loans and mortgages...|$|R
40|$|A {{regional}} ocean {{general circulation}} {{model of the}} Mediterranean is used to study the climate of the Last Glacial Maximum. The atmospheric forcing for these simulations has been derived from simulations with an atmospheric general circulation model, which in turn was forced with surface conditions from a coarse resolution earth system model. The model is successful in reproducing the general patterns of reconstructed sea surface temperature anomalies with the strongest cooling in summer in the northwestern Mediterranean and weak cooling in the Levantine, although the model underestimates {{the extent of the}} summer cooling in the western Mediterranean. However, there is a strong vertical gradient associated with this pattern of summer cooling, which <b>makes</b> <b>the</b> <b>comparison</b> <b>with</b> reconstructions complicated. <b>The</b> exchange with the Atlantic is decreased to roughly one half of its present value, which {{can be explained by the}} shallower Strait of Gibraltar as a consequence of lower global sea level. This reduced exchange causes a strong increase of salinity in the Mediterranean in spite of reduced net evaporation...|$|R
40|$|Abstract: Several {{satellite}} rainfall {{products are}} commonly used for data-scarce catchments because of their extensive coverage together with applicable spatial and temporal resolutions. Although satellite rainfall products have limited accuracy, in data-scarce situations they may be preferable to interpolating between raingauges. Various previous studies focus on assessing the performance of different satellite-based rainfall products in sparsely gauged regions but few <b>make</b> <b>the</b> <b>comparison</b> <b>with</b> interpolated rainfall. This paper aims to evaluate {{the performance of the}} Tropical Rainfall Measurement Mission (TRMM) product and a customised rainfall interpolation technique using the case study of the upper Ping River basin, Thailand, both in terms of rainfall rates at different time resolutions and also in terms of rainfall-flow indices. The two methods of rainfall estimation data being assessed are TRMM_ 3 B 42 version 6 with a spatial resolution of 0. 25 x 0. 25 degrees; and interpolated rainfall based on the combination of lapse rate and inverse distance weighting (IDW) averaged over the same 0. 25 x 0. 25 degree grid squares...|$|R
40|$|Parkinson Disease (PD) occurs due to {{the loss}} of {{dopamine}} in the brains thalamic region that results in involuntary or oscillatory movement in the body. Normally Doctors diagnosis the PD disease clinically with their expertise and experience. But most of the time wrong diagnosis and treatment are reported. For this, patients need to take number of tests for diagnosis, but most of the time, these all tests still not sufficient to diagnosis Parkinson Disease effectively. Firstly, this paper is proposed to apply some data mining technique to select the best attributes to increase the classification performance (according to the voice measurement datasets). In second step, Ada-Boost algorithm is applied to classify the Parkinson disease on the basis of Voice measurements data of PD patients. Then, Support vector Machine with Sequential Minimal Optimization classifier, is used to <b>make</b> <b>the</b> <b>comparison</b> <b>with</b> <b>the</b> result of Ada-Boost classifier to find out the best classifier. In addition, six other best classifiers ex: Naïve bayes, J 48 Tree, LogitBoost, ADTree, BFTree, and Decision Stump Tree are used to make <b>comparison</b> <b>with</b> Parkinson dataset and to select the best classifier...|$|R
40|$|In this paper, we {{investigate}} {{the relation between}} a recently proposed subspace method based on predictor identification (PBSID), known also as ldquowhitening filter algorithm,rdquo and the classical CCA algorithm. <b>The</b> <b>comparison</b> is motivated by i) the fact that CCA {{is known to be}} asymptotically efficient for time series identification and optimal for white measured inputs and ii) some recent results showing that a number of recently developed algorithms are very closely related to PBSID. We show that PBSID is asymptotically equivalent to CCA precisely in the situations in which CCA is optimal while an ldquooptimizedrdquo version of PBSID behaves no worse than CCA also for nonwhite inputs. Even though PBSID (and its optimized version) are consistent regardless of the presence of feedback, in this paper we work under the assumption that there is no feedback to <b>make</b> <b>the</b> <b>comparison</b> <b>with</b> CCA meaningful. <b>The</b> results of this paper imply that the ldquooptimizedrdquo PBSID, besides being able to handle feedback, is to be preferred to CCA also when there is no feedback; only in very specific cases (white or no inputs) are the two algorithms (asymptotically) equivalent...|$|R
40|$|Weakly {{interacting}} massive {{particles are}} a widely well-probed dark matter candidate by {{the dark matter}} direct detection experiments. Theoretically, there are {{a large number of}} ultraviolet completed models that consist of a weakly interacting massive particle dark matter. The variety of models <b>makes</b> <b>the</b> <b>comparison</b> <b>with</b> <b>the</b> direct detection data complicated and often non-trivial. To overcome this, in the non-relativistic limit, the effective theory was developed in the literature which works very well to significantly reduce the complexity of dark matter-nucleon interactions and to better study the nuclear response functions. In the effective theory framework for a spin- 1 / 2 dark matter, we combine three independent likelihood functions from the latest PandaX, LUX, and XENON 1 T data, and give a joint limit on each effective coupling. The astrophysical uncertainties of the dark matter distribution are also included in the likelihood. We further discuss the isospin violating cases of the interactions. Finally, for both dimension-five and dimension-six effective theories above the electroweak scale, we give updated limits of the new physics mass scales. Comment: 33 pages, 11 figures, PandaX run 10 data included and version accepted in JHEP, "code is available at the LikeDM website, [URL]...|$|R
40|$|A huge {{volume of}} {{research}} has resulted from the recent discovery of superconductivity in the RFeAsO series of compounds (where R represents a rare-earth element) characterized by its atomic-like 4 f electrons. The replacement of this species {{by a member of}} the actinide series presents an exciting opportunity to study the effect of varying electron correlation. The successful substitution with actinides to form the AnFeAsO parent compounds (where An=Np,Pu) has already been reported. For both NpFeAsO and PuFeAsO materials, the anti-ferromagnetic (AF) order occurs at critical temperatures TN, 57 K (Np) and 50 K (Pu). In sharp contrast to the rare-earth analogs, no tetragonal-to-orthorhombic distortion is found. Instead, the negative thermal expansion (- 0. 2 %) – an Invar behavior - is found in NpFeAsO below TN. Here, we present the results of the first-principles electronic structure calculations for the actinide-based oxypnictides, NpFeAsO and PuFeAsO, and <b>make</b> <b>the</b> <b>comparison</b> <b>with</b> <b>the</b> bulk experimental data. We used the full-potential LAPW method (FP-LAPW) which includes all relativistic effects (scalar-relativistic and spin-orbit coupling), and relativistic version of the rotationally invariant LSDA+U method. JRC. E. 6 -Actinide researc...|$|R
40|$|As the LHC {{continues}} {{to search for}} new weakly interacting particles, {{it is important to}} remember that the search is strongly motivated by the existence of dark matter. In view of a possible positive signal, it is essential to ask whether the newly discovered weakly interacting particle can be be assigned the label "dark matter". Within a given set of simplified models and modest working assumptions, we reinterpret the relic abundance bound as a relic abundance range, and compare the parameter space yielding the correct relic abundance with projections of the Run II exclusion regions. Assuming that dark matter is within the reach of the LHC, we also <b>make</b> <b>the</b> <b>comparison</b> <b>with</b> <b>the</b> potential 5 σ discovery regions. Reversing the logic, relic density calculations can be used to optimize dark matter searches by motivating choices of parameters where the LHC can probe most deeply into the dark matter parameter space. In the event that DM is seen outside of the region giving the correct relic abundance, we will learn that either thermal relic DM is ruled out in that model, or the DM-quark coupling is suppressed relative to the DM coupling strength to other SM particles. Comment: 17 pages; Updated to match published version, fixed typos and added reference...|$|R
40|$|Abstract. As the LHC {{continues}} {{to search for}} new weakly interacting particles, {{it is important to}} remember that the search is strongly motivated by the existence of dark matter. In view of a possible positive signal, it is essential to ask whether the newly discovered weakly interacting particle can be be assigned the label “dark matter”. Within a given set of simplified models and modest working assumptions, we reinterpret the relic abundance bound as a relic abundance range, and compare the parameter space yielding the correct relic abundance with projections of the Run II exclusion regions. Assuming that dark matter is within the reach of the LHC, we also <b>make</b> <b>the</b> <b>comparison</b> <b>with</b> <b>the</b> potential 5 σ discovery regions. Reversing the logic, relic density calculations can be used to optimize dark matter searches by motivating choices of parameters where the LHC can probe most deeply into the dark matter parameter space. In the event that DM is seen outside of the region giving the correct relic abundance, we will learn that either thermal relic DM is ruled out in that model, or the DM-quark coupling is suppressed relative to the DM coupling strength to other SM particles. ar X i...|$|R
40|$|We compare various {{alternative}} {{explanations of}} why embryo development is sometimes slow relative to juvenile and adult {{development on the}} basis of the standard Dynamic Energy Budget (DEB) model and <b>make</b> <b>the</b> <b>comparison</b> <b>with</b> avian altricial versus precocial development. We discuss the role of the energy investment ratio, which combines four different aspects of DEBs: allocation, assimilation, mobilisation and costs for structure. We show how this ratio affects the morphology of growth curves: the ratio of the slopes at start and birth during embryonic growth, as well as the von Bertalanffy time as function of ultimate length during post-embryonic growth. We propose an extension of the standard DEB model that combines a Gompertz (i. e. exponential) start with a von Bertalanffy 'tail' with a smooth transition; a combination that has been applied frequently in fisheries research and here given a mechanistic significance. Implications are that a slow embryonic development is combined with a fast post-metamorphic one and that parameters at metamorphosis depend on feeding history prior to metamorphosis. Identical individuals, in terms of parameter values and amounts of reserve and structure, will become permanently different when they experience different (local) environments, even if they experience identical environments after metamorphosis. This might explain part of the parameter variation amongst individuals. © 2011 Elsevier B. V...|$|R
40|$|The "Divide and Concur'' (DC) algorithm, {{recently}} introduced by Gravel and Elser, {{can be considered}} a competitor to the belief propagation (BP) algorithm, in that both algorithms can be applied {{to a wide variety of}} constraint satisfaction, optimization, and probabilistic inference problems. We show that DC can be interpreted as a message-passing algorithm on a constraint graph, which helps <b>make</b> <b>the</b> <b>comparison</b> <b>with</b> BP more clear. The "difference-map'' dynamics of the DC algorithm enables it to avoid "traps'' which may be related to the "trapping sets'' or "pseudo-codewords'' that plague BP decoders of low-density parity check (LDPC) codes in the error-floor regime. We investigate two decoders for low-density parity-check (LDPC) codes based on these ideas. The first decoder is based directly on DC, while the second decoder borrows the important "difference-map'' concept from the DC algorithm and translates it into a BP-like decoder. We show that this "difference-map belief propagation'' (DMBP) decoder has dramatically improved error-floor performance compared to standard BP decoders, while maintaining a similar computational complexity. We present simulation results for LDPC codes on the additive white Gaussian noise and binary symmetric channels, comparing DC and DMBP decoders with other decoders based on BP, linear programming, and mixed-integer linear programming...|$|R
40|$|The ”Divide and Concur ” (DC) algorithm, & {{recently}} introduced by Gravel and Elser, {{can be considered}} a competitor to the belief propagation (BP) algorithm, in that both algorithms can be applied {{to a wide variety of}} constraint satisfaction, optimization, and probabilistic inference problems. We show that DC can be interpreted as a message-passing algorithm on a constraint graph, which helps <b>make</b> <b>the</b> <b>comparison</b> <b>with</b> BP more clear. The ”difference-map ” dynam-ics of the DC algorithm enables it to avoid ”traps ” which may be related to the ”trapping sets” or ”pseudo-codeworks ” that plague BP decoders of low-density parity check (LDPC) codes in the error-floor regime. We investigate two decoders for low density parity-check (LDPC) codes based on these ideas. The first decoder is based directly on DC, while the second decoder borrows the important ”difference-map ” concept from the DC algorithm and translates it into a BP-like decoder. We show that this ”difference-map belief propagation ” (DMBP) decoder has dramati-cally improved error-floor performance compared to standard BP decoders, while maintaining a similar computational complexity. We present simulation results for LDPC codes on the additive white Gaussian noise and binary symmetric channels, comparing DC and DMBP decoders with other decoders based on BP, linear programming, and mixed-integer linear programming...|$|R
30|$|The {{selection}} of the regularization parameters highly affects the image restoration results, and related to <b>make</b> <b>the</b> fair <b>comparison</b> <b>with</b> different denoising models. The penalty parameters μ which relies on unknown noise level highly influences {{the speed of the}} algorithms. In experiments, we set μ =[0.01, 0.001] in the PIDAL algorithm. In the PID-Split algorithms, we choose μ =[0.0004; 0.1, 0.0001]. In the TGV model, we set μ =[0.1; 10, 5, 3]. The penalty parameter in the proposed method is empirically set μ =[0.1; 0.6; 0.1; 0.02]. Thus, we may have a good restoration results.|$|R
30|$|Here, in our {{identity}} attack detection problem, {{we use the}} distance in the signal space and <b>make</b> <b>the</b> decision in <b>comparison</b> <b>with</b> <b>the</b> calculated threshold. Then, the acceptance region Ω and the detection rate {{are based on the}} specified T. If the attack is present, then our proposed null hypothesis will be rejected.|$|R
40|$|International audienceThe {{impact of}} the {{presence}} of risk of destructive event on the silvicultural practice of a forest stand is investigated. For that, we consider a model of population dynamics. This model has allowed us to <b>make</b> <b>the</b> <b>comparison</b> without and <b>with</b> risk, and highlight the influence {{of the presence of}} risk of destructive event on optimal thinning and optimal rotation period...|$|R
40|$|The {{impact of}} the {{presence}} of risk of destructive event on the silvicultural practice of a forest stand is investigated. For that, we consider a model of population dynamics. This model has allowed us to <b>make</b> <b>the</b> <b>comparison</b> without and <b>with</b> risk, and highlight the influence {{of the presence of}} risk of destructive event on optimal thinning and optimal rotation period. Faustmann rotation Optimal cutting age Model Thinning Natural risk...|$|R
40|$|The {{purpose of}} this {{bachelor}} thesis was {{to find out the}} definition and role of the producer in the Estonian National Television and compare the outcome with the role of the independent TV-producer in Estonia. The aim of the paper was to study the concept and responsibilities of the in-house producer in the Estonian National Television and <b>make</b> <b>the</b> <b>comparison</b> <b>with</b> <b>the</b> producers in the independent production companies. Firstly, a general overview of the definition of a producer’s role was given. Also, the brief overview and background information was given regarding the implementation of the System of Producers at the Estonian National Television and the reasons for the first independent producers emerged. The study tasks and methodology of the study was introduced {{in the second part of}} the thesis. The next part consisted from analysis, based on in-depth empirical interviews conducted with producers and people with key importance to the producers system in Estonian National Television. Also, additional in-depth interviews were used from the bachelor thesis written by Olavi Paide in the year 2006. As a conclusion of the survey it has to be said that the role and the definition of the producer varied a lot. The independent producers saw themselves more like the creative entrepreneurs while the producers working for the Estonian National Television can be seen as creative organizers. Organizational tasks also <b>make</b> <b>the</b> in-house producer more similar to the executive or associate producers. Finally, the conclusions were made and suggestions were presented in regarding issues that were not directly connected to the theme of this thesis...|$|R
