1|10000|Public
5000|$|... {{conditional}} entropy, <b>mean</b> <b>conditional</b> <b>information</b> <b>content,</b> average conditional {{information content}} H(X|Y) ...|$|E
40|$|We {{describe}} a compression-based distance for genomic sequences. Instead {{of using the}} usual conjoint <b>information</b> <b>content,</b> as in the classical Normalized Compression Distance (NCD), it uses the <b>conditional</b> <b>information</b> <b>content.</b> To compute this Normalized Conditional Compression Distance (NCCD), we need a normal conditional compressor, that we built using a mixture of static and dynamic finite-context models. Using this approach, we measured chromosomal distances between Hominidae primates and also between Muroidea (rat and mouse), observing several insights of evolution that so far have not {{been reported in the}} literature. Comment: Full version of DCC 2014 paper "A conditional compression distance that unveils insights of the genomic evolution...|$|R
40|$|Multivariate GARCH {{specifications}} {{are typically}} determined {{by means of}} practical considerations such as the ease of estimation, which often results in a serious loss of generality. A new type of multivariate GARCH model is proposed, in which potentially large covariance matrices can be parameterized with a fairly large degree of freedom while estimation of the parameters remains feasible. The model {{can be seen as}} a natural generalization of the O-GARCH model, while it is nested in the more general BEKK model. In order to avoid convergence difficulties of estimation algorithms, we propose to exploit unconditional information first, so that the number of parameters that need to be estimated by <b>means</b> of <b>conditional</b> <b>information</b> is more than halved. Both artificial and empirical examples are included to illustrate the model. Copyright 2002 John Wiley & Sons, Ltd. 1...|$|R
40|$|Two {{approaches}} to <b>conditional</b> <b>information</b> used in feature-based linguistic theories, especially headdriven phrase structure grammar, are compared and their interrelation is explicated on a formal and a conceptual level. For this purpose concepts of locale theory are introduced that allow to define feature descriptions and structures in a unified manner and, in particular, {{to make the}} difference between both approaches transparent. In addition, a foundation for attribute-value logic as a predicate-functor logic based on regimented and formalized descriptions is proposed. It turns out that the relative pseudo-complement version of conditional constraints as put forward by Pollard and Sag (1987) is the wrong choice. From a formal perspective, the mistake is due to the confusion of the category of Heyting algebras with that of frames. Only the former are equipped with an operation corresponding to the conditional, that is, with <b>means</b> to represent <b>conditional</b> <b>information</b> as an element with [...] ...|$|R
40|$|If the <b>conditional</b> <b>information</b> of a {{classical}} probability distribution of three random variables is zero, then it obeys a Markov chain condition. If the <b>conditional</b> <b>information</b> {{is close to}} zero, then {{it is known that}} the distance (minimum relative entropy) of the distribution to the nearest Markov chain distribution is precisely the <b>conditional</b> <b>information.</b> We prove here that this simple situation does not obtain for quantum <b>conditional</b> <b>information.</b> We show that for tri-partite quantum states the quantum <b>conditional</b> <b>information</b> is always a lower bound for the minimum relative entropy distance to a quantum Markov chain state, but the distance can be much greater; indeed the two quantities can be of different asymptotic order and may even differ by a dimensional factor. Comment: 14 pages, no figures; not for the feeble-minde...|$|R
40|$|Abstract: The {{purpose of}} this paper is to justify the use of the Gini {{coefficient}} and two close relatives for summarizing the basic information of inequality in distributions of income. To this end we employ a specific transformation of the Lorenz curve, the scaled <b>conditional</b> <b>mean</b> curve, rather than the Lorenz curve as the basic formal representation of inequality in distributions of income. The scaled <b>conditional</b> <b>mean</b> curve is shown to possess several attractive properties as an alternative interpretation of the <b>information</b> <b>content</b> of the Lorenz curve and furthermore proves to yield essential information on polarization in the population. The paper also provides asymptotic distribution results for the empirical scaled <b>conditional</b> <b>mean</b> curve and the related family of empirical measures of inequality. Keywords: The scaled <b>conditional</b> <b>mean</b> curve, measures of inequality, the Gini coefficient, the Bonferroni coefficient, measures of social welfare, principles of transfer sensitivity, estimation, asymptotic distributions...|$|R
40|$|This study {{examines}} {{the effects of}} systemic risk on global hedge fund returns. We consider systemic risk as a <b>conditional</b> <b>information</b> variable to predict the underlying exposures to various asset market returns and risk factors. This {{study examines}} a proxy for global systemic risk employed by investment professionals known as the Treasury/Eurodollar (TED) spread. The findings reveal that increases in systemic risk causes some hedge fund investment styles to dynamically reduce their equity and stock momentum exposures while others increase their exposures to investment grade bonds and commodities. The <b>information</b> <b>content</b> of systemic risk via the TED spread assists us in better understanding the behaviour of global hedge fund returns. Griffith Business School, Department of Accounting, Finance and EconomicsFull Tex...|$|R
40|$|We give {{a partial}} {{answer to a}} {{question}} raised by Bratteli and Robinson, by showing that the mean entropy equals the <b>mean</b> <b>conditional</b> entropy for a large class of translation invariant states. In the general case we show that equality holds if and only if the <b>mean</b> <b>conditional</b> entropy is upper semicontinuous in the w*-topology. ...|$|R
40|$|The {{original}} publication {{is available}} at www. springer. comThe {{purpose of this paper}} is to justify the use of the Gini coefficient and two close relatives for summarizing the basic information of inequality in distributions of income. To this end we employ a specific transformation of the Lorenz curve, the scaled <b>conditional</b> <b>mean</b> curve, rather than the Lorenz curve as the basic formal representation of inequality in distributions of income. The scaled <b>conditional</b> <b>mean</b> curve is shown to possess several attractive properties as an alternative interpretation of the <b>information</b> <b>content</b> of the Lorenz curve and furthermore proves to yield essential information on polarization in the population. The paper also provides asymptotic distribution results for the empirical scaled <b>conditional</b> <b>mean</b> curve and the related family of empirical measures of inequality. Keywords: The scaled <b>conditional</b> <b>mean</b> curve, measures of inequality, the Gini coefficient, the Bonferroni coefficient, measures of social welfare, principles of transfer sensitivity, estimation, asymptotic distributions...|$|R
40|$|We study <b>conditional</b> linear <b>information</b> inequalities, i. e., linear inequalities for Shannon entropy {{that hold}} for {{distributions}} whose entropies meet some linear constraints. We prove that some <b>conditional</b> <b>information</b> inequalities cannot {{be extended to}} any unconditional linear inequalities. Some of these conditional inequalities hold for almost entropic points, while others do not. We also discuss some counterparts of <b>conditional</b> <b>information</b> inequalities for Kolmogorov complexity. Comment: Submitted to the IEEE Transactions on Information Theor...|$|R
40|$|The aim of {{this paper}} is to present, by axiomatic way, an idea about the general <b>conditional</b> <b>information</b> of a single, fixed fuzzy set when the {{conditioning}} fuzzy event is variable. The properties of this <b>conditional</b> <b>information</b> are translated in a system of functional equations. Some classes of solutions of this functional system are founded...|$|R
40|$|The {{purpose of}} this paper is to justify the use of the Gini {{coefficient}} and two close relatives for summarizing the basic information of inequality in distributions of income. To this end we employ a specific transformation of the Lorenz curve, the scaled <b>conditional</b> <b>mean</b> curve, rather than the Lorenz curve as the basic formal representation of inequality in distributions of income. The scaled <b>conditional</b> <b>mean</b> curve is shown to possess several attractive properties as an alternative interpretation of the <b>information</b> <b>content</b> of the Lorenz curve and furthermore proves to yield essential information of polarization in the population...|$|R
5000|$|... is the <b>conditional</b> <b>information</b> entropy of the {{sequence}} of random variables. Equivalently, one has ...|$|R
40|$|We {{discuss the}} notions of mutual <b>information</b> and <b>conditional</b> <b>information</b> for noncomposite systems, {{classical}} and quantum; both the mutual <b>information</b> and the <b>conditional</b> <b>information</b> {{are associated with the}} presence of hidden correlations in the state of a single qudit. We consider analogs of the entanglement phenomena in the systems without subsystems related to strong hidden quantum correlations. Comment: 12 page...|$|R
40|$|We {{present an}} {{extension}} of the well-known information bottleneck framework, called <b>conditional</b> <b>information</b> bottleneck, which takes negative relevance information into account by maximizing a <b>conditional</b> mutual <b>information</b> score. This general approach can be utilized in a data mining context to extract relevant information that {{is at the same time}} novel relative to known properties or structures of the data. We present possible applications of the <b>conditional</b> <b>information</b> bottleneck in information retrieval and text mining for recovering non-redundant clustering solutions, including experimental results on the WebKB data set which validate the approach...|$|R
40|$|To {{appear in}} IEEE Transactions on Information Theory. An Early Access article is {{available}} in IEEE Xplore {{in advance of the}} final print version. International audienceWe study <b>conditional</b> linear <b>information</b> inequalities, i. e., linear inequalities for Shannon entropy that hold for distributions whose entropies meet some linear constraints. We prove that some <b>conditional</b> <b>information</b> inequalities cannot be extended to any unconditional linear inequalities. Some of these conditional inequalities hold for almost entropic points, while others do not. We also discuss some counterparts of <b>conditional</b> <b>information</b> inequalities for Kolmogorov complexity...|$|R
40|$|International audienceIn 1997, Z. Zhang and R. W. Yeung {{found the}} first example of a <b>conditional</b> <b>information</b> {{inequality}} in four variables that is not "Shannon-type". This linear inequality for entropies is called conditional (or constraint) since it holds only under condition that some linear equations are satisfied for the involved entropies. Later, the same authors and other researchers discovered several unconditional information inequalities that do not follow from Shannon's inequalities for entropy. In this paper we prove that some non Shannon-type conditional inequalities are "essentially" conditional, i. e., they cannot be extended to any unconditional inequality. We prove one new essentially <b>conditional</b> <b>information</b> inequality for Shannon's entropy and discuss <b>conditional</b> <b>information</b> inequalities for Kolmogorov complexity...|$|R
30|$|It <b>means</b> <b>conditional</b> {{convergence}} exists. We {{have also}} tested the absolute convergence, but {{results are not}} reported here, available on request.|$|R
40|$|In 1997, Z. Zhang and R. W. Yeung {{found the}} first example of a <b>conditional</b> <b>information</b> {{inequality}} in four variables that is not "Shannon-type". This linear inequality for entropies is called conditional (or constraint) since it holds only under condition that some linear equations are satisfied for the involved entropies. Later, the same authors and other researchers discovered several unconditional information inequalities that do not follow from Shannon's inequalities for entropy. In this paper we show that some non Shannon-type conditional inequalities are "essentially" conditional, i. e., they cannot be extended to any unconditional inequality. We prove one new essentially <b>conditional</b> <b>information</b> inequality for Shannon's entropy and discuss <b>conditional</b> <b>information</b> inequalities for Kolmogorov complexity. Comment: v 4 : substantial corrections; 13 page...|$|R
40|$|The {{ability to}} {{anticipate}} forthcoming events has clear evolutionary advantages, and predictive successes or failures often entail significant psychological and physiological consequences. In music perception, the confirmation and violation of expectations {{are critical to}} the communication of emotion and aesthetic effects of a composition. Neuroscientific research on musical expectations has focused on harmony. Although harmony is important in Western tonal styles, other musical traditions, emphasizing pitch and melody, have been rather neglected. In this study, we investigated melodic pitch expectations elicited by ecologically valid musical stimuli by drawing together computational, behavioural, and electrophysiological evidence. Unlike rule-based models, our computational model acquires knowledge through unsupervised statistical learning of sequential structure in music and uses this knowledge to estimate the <b>conditional</b> probability (and <b>information</b> <b>content)</b> of musical notes. Unlike previous behavioural paradigms that interrupt a stimulus, we devised a new paradigm for studying auditory expectation without compromising ecological validity. A strong negative correlation {{was found between the}} probability of notes predicted by our model and the subjectively perceived degree of expectedness. Our electrophysiological results showed that low-probability notes, as compared to high-probability notes, elicited a larger (i) negative ERP component at a late time period (400 - 450 ms), (ii) beta band (14 - 30 Hz) oscillation over the parietal lobe, and (iii) long-range phase synchronization between multiple brain regions. Altogether, the study demonstrated that statistical learning produces information-theoretic descriptions of musical notes that are proportional to their perceived expectedness and are associated with characteristic patterns of neural activity...|$|R
5000|$|Like mutual <b>information,</b> <b>conditional</b> mutual <b>information</b> can be {{expressed}} as a Kullback-Leibler divergence: ...|$|R
40|$|We {{show that}} the {{separability}} of states in quantum mechanics has a close counterpart in classical physics, and that <b>conditional</b> mutual <b>information</b> (a. k. a. <b>conditional</b> <b>information</b> transmission) is a very useful quantity {{in the study of}} both quantum and classical separabilities. We also show how to define entanglement of formation in terms of <b>conditional</b> mutual <b>information.</b> This paper lays the theoretical foundations for a sequel paper which will present a computer program that can calculate a decomposition of any separable quantum or classical state. 1...|$|R
40|$|The {{objective}} {{of this study is}} to investigate whether net income, net sale, and operating cash flow has incremental <b>information</b> <b>content.</b> This study also investigate whether net sales and operating cash flow have relative <b>information</b> <b>content.</b> Sample of this study is collected from Indonesian Capital Market (IDX). Collected sample is done during 2000 to 2004 in manufacturing industry. The result of this study is net income has not incremental <b>information</b> <b>content.</b> Net sale and operating cash flow have incremental <b>information</b> <b>content.</b> Net sale has not relative <b>information</b> <b>content</b> but operating cash flow has relative <b>information</b> <b>content...</b>|$|R
40|$|As digital terrain {{models are}} {{indispensable}} for visualizing and modeling geographic processes, terrain <b>information</b> <b>content</b> {{is useful for}} terrain generalization and representation. For terrain generalization, if the terrain information is considered, the generalized terrain may be of higher fidelity. In other words, the richer the terrain information at the terrain surface, the smaller the degree of terrain simplification. Terrain <b>information</b> <b>content</b> is also important for {{evaluating the quality of}} the rendered terrain, e. g., the rendered web terrain tile service in Google Maps (Google Inc., Mountain View, CA, USA). However, a unified definition and measures for terrain <b>information</b> <b>content</b> have not been established. Therefore, in this paper, a definition and measures for terrain <b>information</b> <b>content</b> from Digital Elevation Model (DEM, i. e., a digital model or 3 D representation of a terrain’s surface) data are proposed and are based on the theory of map <b>information</b> <b>content,</b> remote sensing image <b>information</b> <b>content</b> and other geospatial <b>information</b> <b>content.</b> The <b>information</b> entropy was taken as the information measuring method for the terrain <b>information</b> <b>content.</b> Two experiments were carried out to verify the measurement methods of the terrain <b>information</b> <b>content.</b> One is the analysis of terrain <b>information</b> <b>content</b> in different geomorphic types, and the results showed that the more complex the geomorphic type, the richer the terrain <b>information</b> <b>content.</b> The other is the analysis of terrain <b>information</b> <b>content</b> with different resolutions, and the results showed that the finer the resolution, the richer the terrain information. Both experiments verified the reliability of the measurements of the terrain <b>information</b> <b>content</b> proposed in this paper...|$|R
40|$|This study {{distinguishes between}} {{incremental}} and relative <b>information</b> <b>content.</b> Incremental comparisons ask whether one accounting measure provides <b>information</b> <b>content</b> beyond that provided by another, and apply when one measure {{is viewed as}} given and an assessment is desired regarding the incremental contribution of another (e. g., a supplemental disclosure). Relative comparisons ask which measure has greater <b>information</b> <b>content,</b> and apply when making mutually exclusive choices among alternatives, or when rankings by <b>information</b> <b>content</b> are desired (e. g., when comparing alternative disclosures). Questions of both incremental and relative <b>information</b> <b>content</b> arise frequently in accounting. However, few previous studies have examined questions of relative <b>information</b> <b>content.</b> Possible explanations include unfamiliarity with the relative versus incremental distinction, and the additional statistical complexity involved in testing for relative <b>information</b> <b>content.</b> First, we examine analytically the relation between incremental and relative <b>information</b> <b>content,</b> demonstrating that they address different research questions and require different tests for statistical significance. Second, we identify accounting research contexts in which questions of relative and incremental <b>information</b> <b>content</b> arise. Third, we propose a new regression-based test for relative <b>information</b> <b>content.</b> This test applies to both returns and valuation studies, generalizes to any number of predictor variables, {{and can be used}} in conjunction with White's (1980) adjustment for heteroskedasticity. Fourth, we illustrate tests for relative and incremental <b>information</b> <b>content</b> in a familiar research setting that compares the <b>information</b> <b>contents</b> of net income, cash flows, and net sales in 40 industries. link_to_subscribed_fulltex...|$|R
40|$|The {{possibility}} of identifying nonlinear time series using nonparametric {{estimates of the}} <b>conditional</b> <b>mean</b> and <b>conditional</b> variance is studied. Most nonlinear models satisfy the assumptions needed to apply nonparametric asymptotic theory. Sampling variations of the conditional quantities are studied by simulation and explained by asymptotic arguments for the first-order nonlinear autoregressive processes. The paper deals with the identification and prediction problems of the autoregressive models of nonlinear time series using nonparametric estimates of the <b>conditional</b> <b>mean</b> and <b>conditional</b> variance...|$|R
40|$|We {{propose a}} {{nonlinear}} time series model where both the <b>conditional</b> <b>mean</b> and the <b>conditional</b> variance are asymmetric functions of past information. The model is particularly useful for analysing financial time series {{where it has}} been noted that there is an asymmetric impact of good news and bad news on volatility (risk) transmission. We introduce a coherent framework for testing asymmetries in the <b>conditional</b> <b>mean</b> and the <b>conditional</b> variance, separately or jointly. To this end we derive both a Wald and a Lagrange multiplier test. Some of the new asymmetric model's moment properties are investigated. Detailed empirical results are given for the daily returns of the composite index of the New York Stock Exchange. There is strong evidence of asymmetry in both the <b>conditional</b> <b>mean</b> and the <b>conditional</b> variance functions. In a genuine out-of-sample forecasting experiment the performance of the best fitted asymmetric model, having asymmetries in both <b>conditional</b> <b>mean</b> and <b>conditional</b> variance, is compared with an asymmetric model for the <b>conditional</b> <b>mean,</b> and with no-change forecasts. This is done both in terms of <b>conditional</b> <b>mean</b> forecasting as well as in terms of risk forecasting. Finally, the paper presents some evidence of asymmetries in the index stock returns of the Group of Seven (G 7) industrialized countries. Copyright © 2004 John Wiley & Sons, Ltd. ...|$|R
40|$|In {{this paper}} we propose new nonparametric estimators {{for a family of}} <b>conditional</b> mutual <b>information</b> and divergences. Our estimators are easy to compute; they only use simple k nearest {{neighbor}} based statistics. We prove that the proposed <b>conditional</b> <b>information</b> and divergence estimators are consistent under certain conditions, and demonstrate their consistency and applicability by numerical experiments on simulated and on real data as well. ...|$|R
40|$|This paper {{presents}} our extractive summarization {{systems at}} the update summarization track of TAC 2009. This system {{is based on}} our newly developed document summarization framework under the theory of <b>conditional</b> <b>information</b> distance among many objects. The best summary is defined in this paper {{to be the one}} which has the minimum information distance to the entire document set. The best update summary has the minimum <b>conditional</b> <b>information</b> distance to a document cluster given that a prior document cluster has already been read. Experiments on the TAC dataset have proved that our method has got a good performance in many categories. ...|$|R
40|$|In {{this paper}} two notions of <b>information</b> <b>content</b> for the {{characteristic}} sequences of sets are compared. One is the minimal-program {{complexity of the}} sequences and represents a quantitative <b>information</b> <b>content,</b> {{and the other is}} the degree of unsolvability of the underlying set and represents a qualitative <b>information</b> <b>content.</b> The major conclusion from this work is that with few exceptions these measures of <b>information</b> <b>content</b> are unrelated. Various tradeoffs between these measures are also demonstrated...|$|R
40|$|A simpler {{approach}} to the characterization of vanishing <b>conditional</b> mutual <b>information</b> is presented. Some remarks are given as well. More specifically, relating the <b>conditional</b> mutual <b>information</b> to a commutator is a very promising {{approach to}}wards the approximate version of SSA. That is, it is conjectured that small <b>conditional</b> mutual <b>information</b> implies small perturbation of quantum Markov chain. Comment: LaTex, 9 pages. Minor modifications are made. Any comments are welcome...|$|R
5000|$|The {{stronger}} {{properties of}} the [...] quantities, which allow the definition of <b>conditional</b> <b>information</b> and mutual information from communication theory, may be very important in other applications, or entirely unimportant, depending on those applications' requirements.|$|R
40|$|Black hole {{is called}} optimal if <b>information</b> <b>content</b> is minimal at the University region, {{consisting}} of usual substance and one(n) black hole(s). Optimal black hole mass {{does not depend}} on the mass of the Universe region. Optimal black holes can exist when at least the two types of substance are available in the Universe: with non-linear and linear correspondence between <b>information</b> <b>content</b> and mass. <b>Information</b> <b>content</b> of optimal black hole is proportional to squared coefficient correlating <b>information</b> <b>content</b> with mass in usual substance and in inverse proportion to coefficient correlating <b>information</b> <b>content</b> with black hole mass. Concentration of mass in optimal black hole minimizes <b>information</b> <b>content</b> in the system "usual substance - black holes". Minimal <b>information</b> <b>content</b> of the Universe consisting of optimal black holes only is twice as less as <b>information</b> <b>content</b> available of the Universe of the same mass filled with usual substance only. Under the radiation temperature T ≈ 1 E + 12 K the mass of optimal black holes that emerged in the systems "radiation - black hole" is equal to the mass of optimal black holes that emerged in the systems "hydrogen (protons) - black hole". Comment: 15 page...|$|R
40|$|In {{this paper}} we {{consider}} a unified approach for fitting conditionally nonlinear time series models with heteroscedastic variances. The model considered is completely general, requiring {{only that the}} forms of the <b>mean</b> and <b>conditional</b> variance functions be specified. Based on the recent results of Mak (1993) on general estimating equations, we derive a convenient expression for the <b>conditional</b> <b>information</b> matrix. Furthermore, it is shown that estimation in such models can be performed via an iteratively weighted least squares algorithm (IWLS), so that the computational problems involved can be conveniently handled by many popular statistical packages. Its implementation is numerically illustrated using the "threshold plus ARCH" model. The algorithm is also demonstrated using both simulated and real data to be superior to the popular BHHH algorithm, which requires a much longer computing time and fails to converge if initial values are not chosen properly. link_to_subscribed_fulltex...|$|R
40|$|Abstract Based on the {{indiscernible}} {{relation of}} rough set, {{the inevitability of}} superposition and inconsistency of data makes the reduction of attributes very important in information system. Rough set has difficulty in the difference of attribute reduction between consistent and inconsistent information system. In this paper, we propose the new uncertainty measure and attribute reduction algorithm by Bayesian posterior probability for correlation analysis between condition and decision attributes. We compare the proposed method and the <b>conditional</b> <b>information</b> entropy to address the uncertainty of inconsistent information system. As the result, our method has more accuracy than <b>conditional</b> <b>information</b> entropy in dealing with uncertainty via mutual information of conditio...|$|R
40|$|A {{measure of}} the <b>information</b> <b>content</b> of an {{evidence}} inducing a belief function or a possibility function is axiomatically defined. Its major property is to be additive for distinct evidences. Measures are developed of the <b>information</b> <b>content</b> of an evidence appropriate when belief or possibility functions are used. The additivity means that the <b>information</b> <b>content</b> derived from two distinct and non-conflictual evidences {{is the sum of}} the <b>information</b> <b>contents</b> of each evidence. Properties of these measures are studied. Refs. SCOPUS: NotDefined. jinfo:eu-repo/semantics/publishe...|$|R
