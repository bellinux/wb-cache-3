7|0|Public
2500|$|An {{originally}} classified 1984 US interagency {{intelligence assessment}} states that {{in both the}} preceding 1970s and 80s, the Soviet and US military were already following the [...] "existing trends" [...] in warhead miniaturization, of higher accuracy and lower yield nuclear warheads, this is seen when assessing the most numerous physics packages in the US arsenal, which in the 1960s were the B28 and W31, however both quickly became less prominent with the 1970s mass production runs of the 50 Kt W68, the 100 Kt W76 and in the 1980s, with the B61. This trend towards miniaturization, enabled by advances in inertial guidance and accurate GPS navigation etc., was motivated by a multitude of factors, namely the desire to leverage the physics of equivalent <b>megatonnage</b> that miniaturization offered; of freeing up space to fit more MIRV warheads and decoys on each missile. Alongside the desire to still destroy hardened targets but while reducing the severity of fallout collateral damage depositing on neighboring, and potentially friendly, countries. As {{it relates to the}} likelihood of nuclear winter, the range of potential thermal radiation ignited fires was already reduced with miniaturization. For example, the most popular nuclear winter paper, the 1983 TTAPS paper, had described a 3000 Mt counterforce attack on ICBM sites with each individual warhead having approximately one Mt of energy; however not long after publication, Michael Altfeld of Michigan State University and political scientist Stephen Cimbala of Pennsylvania State University argued that the then already developed and deployed smaller, more accurate warheads (e.g. W76), together with lower detonation heights, could produce the same counterforce strike with a total of only 3 Mt of energy being expended. They continue that, [...] if the nuclear winter models prove to be representative of reality, then far less climatic-cooling would occur, even if firestorm prone areas existed in the target list, as lower fusing heights such as surface bursts, would also limit the range of the burning thermal rays due to terrain masking and shadows cast by buildings, while also temporarily lofting far more localized fallout when compared to airburst fuzing – the standard mode of employment against un-hardened targets. This logic is similarly reflected in the originally classified 1984 Interagency Intelligence assessment, which suggests that targeting planners would simply have to consider target combustibility along with yield, height of burst, timing and other factors {{to reduce the amount of}} smoke to safeguard against the potentiality of a nuclear winter. Therefore, as a consequence of attempting to limit the target fire hazard by reducing the range of thermal radiation with fuzing for surface and sub-surface bursts, this will result in a scenario where the far more concentrated, and therefore deadlier, local fallout that is generated following a surface burst forms, as opposed to the comparatively dilute global fallout created when nuclear weapons are fuzed in air burst mode.|$|E
50|$|The {{planet of}} Rathe and Home are {{locked into a}} deadly nuclear arms-race, each {{possessing}} weapons ready to launch a fiery consummation of the policy of Mutual Assured Destruction. Although satellite images show that Rathe possess only atomic (fission) weapons, less powerful than Home's thermonuclear arsenal, there is still enough <b>megatonnage</b> on each planet to totally destroy the other.|$|E
50|$|In total {{nuclear test}} <b>megatonnage,</b> from 1945-92, 520 {{atmospheric}} nuclear explosions (including 8 underwater) {{have been conducted}} with a total yield of 545 megatons, with a peak occurring in 1961-62, when 340 megatons were detonated in the atmosphere by the United States and Soviet Union. while the estimated number of underground nuclear tests conducted in the period from 1957 to 1992 is 1,352 explosions with a total yield of 90 Mt.|$|E
5000|$|The {{belief in}} [...] "overkill" [...] is also {{commonly}} encountered, with an example being {{the following statement}} made by nuclear disarmament activist Philip Noel-Baker in 1971 - [...] "Both the US and the Soviet Union now possess nuclear stockpiles large enough to exterminate mankind three or four - some say ten - times over". Brian Martin suggested that {{the origin of this}} belief was from [...] "crude linear extrapolations", and when analyzed it has no basis in reality. Similarly, it is common to see stated that the combined explosive energy released in the entirety of World War II was about 3 megatons, while a nuclear war with warhead stockpiles at Cold War highs would release 6000 WWII's of explosive energy. An estimate for the necessary amount of fallout to begin to have the potential of causing human extinction is regarded by physicist and disarmament activist Joseph Rotblat to be 10 to 100 times the <b>megatonnage</b> in nuclear arsenals as they stood in 1976; however, with the world <b>megatonnage</b> decreasing since the Cold War ended this possibility remains hypothetical.|$|E
5000|$|The 1964 book Islands in Space {{calculates that}} the nuclear <b>megatonnage</b> {{necessary}} for several deflection scenarios exists.In 1967, graduate students under Professor Paul Sandorff at the Massachusetts Institute of Technology were tasked with designing {{a method to}} prevent a hypothetical 18 month distant impact on Earth by the 1.4 kilometer wide asteroid 1566 Icarus, an object which makes regular close approaches to Earth, sometimes as close as 16 lunar distances. To achieve the task within the timeframe and with limited material knowledge of the asteroid's composition, a variable stand-off system was conceived. This would have used a number of modified Saturn V rockets sent on interception courses {{and the creation of}} a handful of nuclear explosive devices in the 100 megaton energy range—coincidentally, the maximum yield of the Soviets' 1961 Tsar Bomba if a uranium tamper had been used—as each rocket vehicle's payload. [...] The design study was later published as Project Icarus which served as the inspiration for the 1979 film Meteor.|$|E
5000|$|An {{originally}} classified 1984 US interagency {{intelligence assessment}} states that {{in both the}} preceding 1970s and 80s, the Soviet and US military were already following the [...] "existing trends" [...] in warhead miniaturization, of higher accuracy and lower yield nuclear warheads, this is seen when assessing the most numerous physics packages in the US arsenal, which in the 1960s were the B28 and W31, however both quickly became less prominent with the 1970s mass production runs of the 50 Kt W68, the 100 Kt W76 and in the 1980s, with the B61. This trend towards miniaturization, enabled by advances in inertial guidance and accurate GPS navigation etc., was motivated by a multitude of factors, namely the desire to leverage the physics of equivalent <b>megatonnage</b> that miniaturization offered; of freeing up space to fit more MIRV warheads and decoys on each missile. Alongside the desire to still destroy hardened targets but while reducing the severity of fallout collateral damage depositing on neighboring, and potentially friendly, countries. As {{it relates to the}} likelihood of nuclear winter, the range of potential thermal radiation ignited fires was already reduced with miniaturization. For example, the most popular nuclear winter paper, the 1983 TTAPS paper, had described a 3000 Mt counterforce attack on ICBM sites with each individual warhead having approximately one Mt of energy; however not long after publication, Michael Altfeld of Michigan State University and political scientist Stephen Cimbala of Pennsylvania State University argued that the then already developed and deployed smaller, more accurate warheads (e.g. W76), together with lower detonation heights, could produce the same counterforce strike with a total of only 3 Mt of energy being expended. They continue that, if the nuclear winter models prove to be representative of reality, then far less climatic-cooling would occur, even if firestorm prone areas existed in the target list, as lower fusing heights such as surface bursts, would also limit the range of the burning thermal rays due to terrain masking and shadows cast by buildings, while also temporarily lofting far more localized fallout when compared to airburst fuzing - the standard mode of employment against un-hardened targets. This logic is similarly reflected in the originally classified 1984 Interagency Intelligence assessment, which suggests that targeting planners would simply have to consider target combustibility along with yield, height of burst, timing and other factors {{to reduce the amount of}} smoke to safeguard against the potentiality of a nuclear winter. Therefore, as a consequence of attempting to limit the target fire hazard by reducing the range of thermal radiation with fuzing for surface and sub-surface bursts, this will result in a scenario where the far more concentrated, and therefore deadlier, local fallout that is generated following a surface burst forms, as opposed to the comparatively dilute global fallout created when nuclear weapons are fuzed in air burst mode.|$|E
40|$|Carbon {{nanotubes}} {{are widely}} sought {{for a variety}} of applications including gas storage, intercalation media, catalyst support and composite reinforcing material [1]. Each of these applications will require large scale quantities of CNTs. A second consideration is that some of these applications may require redispersal of the collected CNTs and attachment to a support structure. If the CNTs could be synthesized directly upon the support {{to be used in the}} end application, a tremendous savings in post-synthesis processing could be realized. Therein we have pursued both aerosol and supported catalyst synthesis of CNTs. Given space limitations, only the aerosol portion of the work is outlined here though results from both thrusts will be presented during the talk. Aerosol methods of SWNT, MWNT or nanofiber synthesis hold promise of large-scale production to supply the tonnage quantities these applications will require. Aerosol methods may potentially permit control of the catalyst particle size, offer continuous processing, provide highest product purity and most importantly, are scaleable. Only via economy of scale will the cost of CNTs be sufficient to realize the large-scale structural and power applications on both earth and in space. Present aerosol methods for SWNT synthesis include laser ablation of composite metalgraphite targets or thermal decomposition/pyrolysis of a sublimed or vaporized organometallic [2]. Both approaches, conducted within a high temperature furnace, have produced single-walled nanotubes (SWNTs). The former method requires sophisticated hardware and is inherently limited by the energy deposition that can be realized using pulsed laser light. The latter method, using expensive organometallics is difficult to control for SWNT synthesis given a range of gasparticle mixing conditions along variable temperature gradients; multi-walled nanotubes (MWNTs) are a far more likely end products. Both approaches require large energy expenditures and produce CNTs at prohibitive costs, around $ 500 per gram. Moreover these approaches do not possess demonstrated scalability. In contrast to these approaches, flame synthesis can be a very energy efficient, low-cost process [3]; a portion of the fuel serves as the heating source while the remainder serves as reactant. Moreover, flame systems are geometrically versatile as illustrated by innumerable boiler and furnace designs. Addressing scalability, flame systems are commercially used for producing <b>megatonnage</b> quantities of carbon black [4]. Although it presents a complex chemically reacting flow, a flame also offers many variables for control, e. g. temperature, chemical environment and residence times [5]. Despite these advantages, there are challenges to scaling flame synthesis as well...|$|E

