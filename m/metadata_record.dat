120|472|Public
5000|$|... (optional): [...] "used {{to obtain}} runtime {{information}} about the range of values of a <b>metadata</b> <b>record</b> element or request parameter" ...|$|E
5000|$|ISO 19139:2012 [...] {{provides}} the XML implementation schema for ISO 19115 specifying the <b>metadata</b> <b>record</b> format {{and may be}} used to describe, validate, and exchange geospatial metadata prepared in XML.|$|E
50|$|EIDR, or the Entertainment Identifier Registry, is {{a global}} unique {{identifier}} system for {{a broad array of}} audio visual objects, including motion pictures, television, and radio programs. The identification system resolves an identifier to a <b>metadata</b> <b>record</b> that is associated with top-level titles, edits, DVDs, encodings, clips, and mash-ups. EIDR also provides identifiers for Video Service providers, such as broadcast and cable networks.|$|E
40|$|This study {{describes}} an information retrieval experiment comparing the retrieval effectiveness (recall and precision) for queries run against professionally and automatically generated <b>metadata</b> <b>records.</b> The <b>metadata</b> <b>records</b> represented web pages from the National Institute of Environmental Health Sciences. The results of 10 queries were analyzed {{in terms of}} recall and precision for this small-scale study. The {{results of the study}} suggest that professionally generated <b>metadata</b> <b>records</b> are not significantly better in terms of information retrieval effectiveness than automatically generated <b>metadata</b> <b>records...</b>|$|R
50|$|The Wind Energy Metadata Clearinghouse {{included}} {{thousands of}} <b>metadata</b> <b>records</b> related to wind energy. These <b>metadata</b> <b>records</b> provide users {{with easy access}} to data and information {{from a variety of}} resources, such as peer-reviewed journal articles, government agency reports, private industry reports, GIS services, datasets, and news articles. Users can search the Clearinghouse's <b>metadata</b> <b>records</b> by a simple keyword search or narrow their results using more specific filters, including publication date and geographic area. Researchers can then search and scan these <b>metadata</b> <b>records</b> to grasp the vital information—title, author, abstract, and other bibliographic elements—about each resource, quickly assessing its potential usefulness for their work. In many <b>metadata</b> <b>records,</b> a URL to the resource itself is provided, such that those potentially useful resources may be readily viewed in their entirety.|$|R
40|$|This paper reports {{results of}} an {{exploratory}} quantitative analysis of metadata versioning in a large-scale digital library hosted by University of North Texas. The study begins {{to bridge the gap}} in the information science research literature to address metadata change over time. The authors analyzed the entire population of 691, 495 unique item-level <b>metadata</b> <b>records</b> in the digital library, with <b>metadata</b> <b>records</b> supplied from multiple institutions and by a number of metadata creators with varying levels of skills. We found that a high proportion of <b>metadata</b> <b>records</b> undergo changes, and that a substantial number of these changes result in increased completeness (the degree to which <b>metadata</b> <b>records</b> include at least one instance of each element required in the Dublin Core-based UNTL metadata scheme). Another observation {{of this study is that}} the access status of a high proportion of <b>metadata</b> <b>records</b> changes from hidden to public; at the same time the reverse process also occurs, when previously visible to the public <b>metadata</b> <b>records</b> become hidden for further editing and sometimes remain hidden. This study also reveals that while most changes [...] presumably made to improve the quality of <b>metadata</b> <b>records</b> [...] increase the record length, surprisingly, some changes decrease record length. Further investigation is needed into reasons for unexpected findings as well as into more granular dimensions of metadata change at the level of individual <b>records,</b> <b>metadata</b> elements, and data values. This paper suggests some research questions for future studies of metadata change in digital libraries that capture metadata versioning information...|$|R
50|$|The {{records are}} in XML {{according}} to the standard. Typically the records include Dublin Core, ISO 19139 or FGDC metadata, encoded in UTF-8 characters. Each record must contain certain core fields including: Title, Format, Type (e.g. Dataset, DatasetCollection or Service), BoundingBox (a rectangle of interest, expressed in latitude and longitude), Coordinate Reference System, and Association (a link to another <b>metadata</b> <b>record).</b>|$|E
50|$|For {{the past}} 15 years, GreyNet {{has sought to}} serve {{researchers}} and authors {{in the field of}} grey literature. To further this end, GreyNet has signed on to the OpenSIGLE repository and in so doing seeks to preserve and make openly available research results originating in the International Conference Series on Grey Literature. GreyNet together with INIST-CNRS have designed the format for a <b>metadata</b> <b>record,</b> which encompasses standardized PDF attachments of the full-text conference preprints, PowerPoint presentations, abstracts and biographical notes.|$|E
50|$|An Archival Resource Key (ARK) is a Uniform Resource Locator (URL) {{that is a}} {{multi-purpose}} {{persistent identifier}} for information objects of any type. An ARK contains the label ark: after the URL's hostname, which sets the expectation that, when submitted to a web browser, the URL terminated by '?' returns a brief <b>metadata</b> <b>record,</b> and the URL terminated by '??' returns metadata that includes a commitment statement from the current service provider. The ARK and its inflections ('?' and '??') gain access to three facets of a provider's ability to provide persistence.|$|E
50|$|In 2011, {{a formal}} {{memorandum}} for data exchange {{was signed by}} the DGSE and the NSA, which facilitated the transfer of millions of <b>metadata</b> <b>records</b> from the DGSE to the NSA. From December 2012 to 8 January 2013, over 70 million <b>metadata</b> <b>records</b> were {{handed over to the}} NSA by French intelligence agencies.|$|R
40|$|A {{procedure}} for querying <b>metadata</b> <b>records</b> {{of the type}}   10320 /loc associated with a DSpace digital repository to retrieve a specified  Gaussian Log file and to view the computed molecule in JSmol. A {{procedure for}} querying <b>metadata</b> <b>records</b> of the type   10320 /loc associated with a DSpace digital repository to retrieve a specified  Gaussian Log file and to view the computed molecule in JSmol. A procedure for querying <b>metadata</b> <b>records</b> of the type   10320 /loc associated with a DSpace digital repository to retrieve a specified  Gaussian Log file and to view the computed molecule in JSmol...|$|R
40|$|Abstract. In this paper, {{we study}} {{the problem of}} {{maintaining}} metadata for open Web content. In digital libraries such as DLESE, NSDL and G-Portal, <b>metadata</b> <b>records</b> are created for some good quality Web content objects {{so as to make}} them more accessible. These Web objects are dynamic making it necessary to update their <b>metadata</b> <b>records.</b> As Web <b>metadata</b> maintenance involves manual efforts, we propose to reduce the efforts by introducing the Key element-Context (KeC) modeltomonitor only those changes made on Web page content regions that concern metadata attributes while ignoring other changes. We also develop evaluation metrics to measure the number of alerts and the amount of efforts in updating Web <b>metadata</b> <b>records.</b> KeC model has been experimented on <b>metadata</b> <b>records</b> defined for Wikipedia articles, and its performance with different settings is reported. The model is implemented in G-Portal as a metadata maintenance module. ...|$|R
5000|$|... "A <b>metadata</b> <b>record</b> is a file of information, usually {{presented}} as an XML document, which captures the basic {{characteristics of a}} data or information resource. It represents the who, what, when, where, why and how of the resource. Geospatial metadata commonly document geographic digital data such as Geographic Information System (GIS) files, geospatial databases, and earth imagery but {{can also be used}} to document geospatial resources including data catalogs, mapping applications, data models and related websites. Metadata records include core library catalog elements such as Title, Abstract, and Publication Data; geographic elements such as Geographic Extent and Projection Information; and database elements such as Attribute Label Definitions and Attribute Domain Values." ...|$|E
50|$|For {{the past}} 20 years, GreyNet {{has sought to}} serve {{researchers}} and authors {{in the field of}} grey literature. To further this end, GreyNet has signed on to the OpenGrey repository and in so doing seeks to preserve and make openly available research results originating in the International Conference Series on Grey Literature. GreyNet together with INIST-CNRS have designed the format for a <b>metadata</b> <b>record,</b> which encompasses standardized PDF attachments of the full-text conference preprints, PowerPoint presentations, abstracts, biographical notes, and post-publication commentaries. GreyNet's collection of over 270 conference preprints is both current and comprehensive.Comment from Peter Suber, Open Access News (Thursday, January 29, 2009): GreyNet started making its conference proceedings OA through its repository in May 2008. I applaud its determination to complete the collection retroactively, even if it means buying permission from a publisher. Note to other conference organizers: This is a reason to self-archive your proceedings as you go, or at least to retain the right to self-archive them without a fee.|$|E
5000|$|...Electronic grey literatureThe “Nancy style” {{is mostly}} paper oriented, because {{editorial}} consistency and ethical considerations recommended for traditional documents do apply also to digital publications. Yet, progressively {{more and more}} GL is being produced, stored, published and made available electronically {{and in order to}} manage relevant GL publications, metadata are required. The importance of metadata, as the natural evolution of library catalogue records, had been already stressed in the first version of the “Nancy style” (when dealing with report structure: Section 4.2 of the Guidelines), but no metadata schema was then provided since it was difficult to find a formula that would satisfy all requirements. At present, much GL is catalogued using the Dublin Core Metadata Standard (DC). However - as Keith Jeffery of the UK Council for the Central Laboratory of the Research Councils (CCLRC) pointed out working on the “Nancy style” draft - this standard suffers from several problems: a) it is machine-readable but not machine-understandable; b) it does not have a formalised syntax or semantics and therefore is open to ambiguous interpretations. Therefore, he proposed a formalised metadata standard (an umbrella standard, mainly generated from Dublin Core metadata: “Formalised DC” based on the concepts of the CERIF Model (http://www.eurocris.org/Cerif). Yet, as the traditional cataloguing practice has different rules, similarly different communities may adopt different metadata schema. Nowadays the World Wide Web provides the possibility to search for information across heterogeneous archives/databases/catalogues, but the systems managing different information resources must be “interoperable” (capable to work together), and interoperability requires that the same metadata schema be used. As Stefania Biagioni (of the Italian Istituto di Scienza e Tecnologie dell’Informazione - ISTI, Consiglio Nazionale delle Ricerche) clearly commented, there is much work towards standardization and the Dublin Core Initiative (http://dublincore.org/) is receiving worldwide consensus as it suggests adding a very simple <b>metadata</b> <b>record</b> to any specialized one.|$|E
40|$|This paper {{explores the}} {{possible}} role of named entities in an automatic indexing process, based on text in subtitles. This {{is done by}} analyzing entity types, name density and name frequencies in subtitles and <b>metadata</b> <b>records</b> from different TV programs. The name density in <b>metadata</b> <b>records</b> {{is much higher than}} the name density in subtitles, and named entities with high frequencies in the subtitles {{are more likely to be}} mentioned in the <b>metadata</b> <b>records.</b> Personal names, geographical names and names of organizations where the most prominent entity types in both the news subtitles and news metadata, while persons, works and locations are the most prominent in culture programs...|$|R
40|$|International audienceWe propose an {{approach}} to semantically enrich <b>metadata</b> <b>records</b> of satellite imagery with external data. As a result we are able the identify relevant images using a larger set of matching criteria. Conventional methods for annotating data sets are usually based on <b>metadata</b> <b>records</b> (with attributes such as title, provider, access mode, and spatio-temporal characteristics), which other a narrow view of the world. Enriching metadata with contextual information (i. e., the region depicted in the image has been recently affected by extreme weather) requires formalizing spatio-temporal relationships between <b>metadata</b> <b>records</b> and external data sources. Semantic technologies {{play a key role}} in such scenarios by providing an infrastructure based on RDF and ontologies...|$|R
40|$|D-Net Software Toolikt This is {{a minimal}} {{instance}} of the D-Net software toolkit, a software framework for the realization of aggregative data infrastructures. Official Web Site: [URL] Need support? Contact us via email at: dnet-team@isti. cnr. it This webapp contains the minimal set of services needed to feature: 	 	Collection of <b>metadata</b> <b>records</b> in oai_dc format via OAI-PMH, FTP, local file system, HTTP. 	 	 	Transformation of the collected <b>metadata</b> <b>records</b> into an internal format named DMF (Driver Metadata Format) 	 	 	Indexing of DMF records in a Solr full-text index 	 	 	OAI-PMH export of aggregated <b>metadata</b> <b>records</b> in DMF and oai_dc formats. More formats can be added at runtime by providing a dedicated XSLT from DMF to the desired target format...|$|R
40|$|Knowledge domain {{navigation}} {{is vital}} to research activities, and this paper addresses the epistemological and ontological dimensions of the knowledge domain navigation framework. The paper addresses knowledge creation through the <b>metadata</b> <b>record,</b> and how interdisciplinarity affects the <b>metadata</b> <b>record.</b> The discussion of the ontological dimension focuses on semantic navigation, interoperability, {{and the relationship of}} taxonomies to interdisciplinarity. A number of digital initiative projects illustrate the concept of knowledge domain navigation...|$|E
40|$|Digital {{libraries}} {{populated with}} learning objects are becoming popular {{tools in the}} creation of instructional technologies. Many current efforts to create standard metadata structures that facilitate the discovery and instructional use of learning objects recommend a single, authoritative <b>metadata</b> <b>record</b> per version of the learning object. However, as we argue in this paper, a single <b>metadata</b> <b>record</b> — particularly one with fields that emphasize knowledge management and technology, while evading instructional issues — provides information insufficient to support instructional utilization decisions. To put learning objects to instructional use, users must examine the individual objects, forfeiting the supposed benefits of the metadata system. As a solution, we propose a system that includes multi-record, non-authoritative metadata focussed on the surrounding instructional context of learning objects...|$|E
40|$|A {{study of}} students’ {{information}} searching strategies This item was submitted to Loughborough University’s Institutional Repository by the/an author. Citation: BALDWIN, A. N., GADD, E. and BALATSOUKAS, P., 2010. A study of students ’ information searching strategies. CEBE Transactions, 7 (2), pp. 3 - 25. <b>Metadata</b> <b>Record...</b>|$|E
40|$|The {{rapid growth}} of diverse data types and greater volumes {{available}} to environmental sciences prompts the scientists to seek knowledge in data from multiple places, times, and scales. To facilitate such need, ONEMercury has recently been implemented {{as part of the}} DataONE project to serve as a portal for accessing environmental and observational data across the globe. ONEMercury harvests metadata from the data hosted by multiple repositories and makes it searchable. However, harvested <b>metadata</b> <b>records</b> sometimes are poorly annotated or lacking meaningful keywords, and hence would unlikely be retrieved during the search process. In this paper, we develop an algorithm for automatic metadata annotation. We transform the problem into a tag recommendation problem, and propose a score propagation algorithm for tag recommendation. Our experiments on four data sets of environmental science <b>metadata</b> <b>records</b> not only show great promises on the performance of our methods, but also shed light on the different natures of the data sets. Problem: Linking data from heterogenous sources always has a cost. One of the biggest problems that ONEMercury is facing is the different levels of annotation in the harvested <b>metadata</b> <b>records.</b> Poorly annotated <b>metadata</b> <b>records</b> tend to be missed during the search process as they lack meaningful keywords. Furthermore, such records would not be compatible with the advance mode offered by ONEMercury as it requires the <b>metadata</b> <b>records</b> be semantically annotated wit...|$|R
40|$|In {{this paper}} {{a system to}} support the {{creation}} of extended IMDI <b>metadata</b> <b>records</b> is presented. It is based on bundling definitions of the in the IMDI system user definable key-name/value pairs in a profile. The possibility of using inheritance of profiles in a corpus structure is explored. Profiles Can be created and used by the IMDI Editor, a tool specially designed to create IMDI <b>metadata</b> <b>records.</b> 1...|$|R
40|$|Automatic {{language}} identification {{has been}} applied to short texts such as queries in information retrieval, but it has not yet been applied to <b>metadata</b> <b>records.</b> Applying this technology to <b>metadata</b> <b>records,</b> particularly their title elements, would enable creators of <b>metadata</b> <b>records</b> to obtain a value for the language element, which is often left blank {{due to a lack of}} linguistic expertise. It would also enable the addition of the language value to existing <b>metadata</b> <b>records</b> that currently lack a language value. Titles lend themselves to the problem of language identification mainly due to their shortness, a factor which increases the difficulty of accurately identifying a language. This study implemented four proven approaches to language identification as well as one open-source approach on a collection of multilingual titles of books and movies. Of the five approaches considered, a reduced N-gram frequency profile and distance measure approach outperformed all others, accurately identifying over 83 % of all titles in the collection. Future plans are to offer this technology to curators of digital collections for use...|$|R
40|$|Radiation {{efficiency}} of finite plates with beam stiffeners This item was submitted to Loughborough University's Institutional Repository by the/an author. Citation: ROUSOUNELOS, A., WALSH, S. J. and KRYLOV, V. V., 2008. Ra-diation {{efficiency of}} finite plates with beam stiffeners. Proceedings of the Insti-tute of Acoustics, 30 (2), pp. 124 - 131. <b>Metadata</b> <b>Record...</b>|$|E
40|$|This is a <b>metadata</b> <b>record</b> {{relating}} {{to an article}} that cannot be shared due to publisher copyright. This research was made viable through a doctoral studentship at the School of Music. Finally, I {{would like to thank}} the Leverhulme Trust for my current fellowship, which has enabled me to write up this research...|$|E
40|$|This dataset {{contains}} data {{samples from}} metadata records {{extracted from the}} UNT Libraries' Digital Collections. It contains one sample per <b>metadata</b> <b>record</b> version in the system with aggregate counts of fields and also hash values of an element as well. Data was collected in March 2014 with dates from May 19, 2004 to February 4, 2014...|$|E
40|$|The {{description}} of preserved resources {{is one of}} the requirements in digital preservation. The description is generally created in the format of <b>metadata</b> <b>records,</b> and those records are combined to generate information packages to support the process of digital preservation. However, current strategies or models of digital preservation may not generate information packages in efficient ways. To overcome these problems, this research proposed an internal structure of information packages in digital preservation. In order to construct the internal structure, this research analyzed existing metadata standards and cataloging rules such as Dublin Core, MARC, and FRBR to extract the core elements of resource description. The extracted elements were categorized according to their semantics and functions, which resulted in three categories of core elements. These categories and core elements were manifested by using RDF syntax in order to be substantially applied to combine <b>metadata</b> <b>records</b> in digital preservation. Although the internal structure is not intended to create <b>metadata</b> <b>records,</b> it is expected to provide an alternative approach to enable combining existing <b>metadata</b> <b>records</b> in the context of digital preservation in a more flexible way...|$|R
40|$|Presented at the CUL Metadata Working Group Forum on October 15, 2010 Linked entity data in <b>metadata</b> <b>records</b> {{builds a}} {{foundation}} for the semantic web. Even though <b>metadata</b> <b>records</b> contain rich entity data, there is no linking between associated entities such as persons, datasets, projects, publications, or organizations. We conducted a small experiment using the dataset collection from the Hubbard Brook Ecosystem Study (HBES), in which we converted the entities and their relationships into RDF triples and linked the URIs contained in RDF triples to the corresponding entities in the Ecological <b>Metadata</b> Language (EML) <b>records.</b> Through the transformation program written in XML Stylesheet Language (XSL), we turned a plain EML record display into an interlinked semantic web of ecological datasets. The experiment suggests a methodological feasibility in incorporating linked entity data into <b>metadata</b> <b>records.</b> <b>Metadata</b> Working Group, Cornell University Librar...|$|R
50|$|In August 2013, it was {{revealed}} that the Bundesnachrichtendienst (BND) of Germany transfers massive amounts of <b>metadata</b> <b>records</b> to the NSA.|$|R
40|$|International audienceCurrently, {{vast amounts}} of {{geospatial}} information are o ffered through OGC's services. However this information has limited formal semantics. The most common method {{to search for a}} dataset consists in matching keywords to metadata elements. By adding semantics to available descriptions we could use modern inference and reasoning mechanisms currently available in the SemanticWeb. In this paper we present a novel architecture currently in development in which we use state of the art triplestores as the backend of a CSW service. In our approach, each <b>metadata</b> <b>record</b> is considered an instance of a given class in a domain ontology. Our architecture also adds a spatial dataset of features with toponym values. These additions allow us to provide advance searches based on 1) Instance to class matching, 2) Class to class subsuming relationships, 3) Spatial relationships resulting from comparing the bounding box of a <b>metadata</b> <b>record</b> with our toponym spatial dataset...|$|E
40|$|Many {{repositories}} are {{burdened by}} resources {{that have an}} incomplete <b>metadata</b> <b>record.</b> With some institutional repositories storing {{hundreds of millions of}} resources, it is extremely costly to manually generate resource metadata. Therefore, automatic metadata generation is a topic of interest to the digital library community. The automatic metadata generation system proposed by this paper is novel in three ways: it is computationally inexpensive, does not require the raw resource, and is independent of the resource media type (i. e. audio, video, document, etc.). Using occurrence and co-occurrence network generation algorithms, an associative network of repository resources is constructed using pre-existing repository metadata. The associative network serves as the substrate which allows metadata-rich resources to supply metadata-limited resources with potentially useful metadata information. This poster discusses the general framework for building associative networks, an algorithm for disseminating metadata through such networks, and a validation of the proposed system using a bibliographic dataset. Repository Metadata and System Architecture • For any repository resource, there exists a <b>metadata</b> <b>record...</b>|$|E
40|$|This is a <b>metadata</b> <b>record</b> for {{an article}} {{which the author}} {{does not wish to}} make openly available. THE media were unsurprisingly {{interested}} when the Supreme Court considered the case of a poverty-stricken New Age traveller turned multi-millionaire whose former wife appeared years after divorce to claim a share of his subsequently acquired wealth (Vince v Wyatt [2015] UKSC 14). The case also raised an important point of legal principle...|$|E
50|$|Camera {{functions}} enabled by {{the chip}} may include exposure metering, aperture display and control, focus confirmation and fine-tuning, and Exif <b>metadata</b> <b>recording.</b>|$|R
50|$|From December 2012 to 8 January 2013, over 70 million <b>metadata</b> <b>records</b> {{were handed}} over to the NSA by French {{intelligence}} agencies.|$|R
30|$|Borges et al. [4] {{present an}} {{unsupervised}} heuristic approach aimed at bibliographic metadata deduplication, which devotes {{special attention to}} fields that refer to the names of authors to correctly identify redundancies in these <b>metadata</b> <b>records.</b> The process begins with a mapping of the metadata fields represented in different patterns. The focus {{of this approach is}} to build similarity functions specially developed for the domain of digital libraries. <b>Metadata</b> <b>records</b> are compared using these selected functions according to the domain of each attribute.|$|R
