37|496|Public
25|$|The DEPFET is a FET {{formed in}} a fully {{depleted}} substrate and {{acts as a}} sensor, amplifier and <b>memory</b> <b>node</b> at the same time. It {{can be used as}} an image (photon) sensor.|$|E
50|$|The priming {{effect is}} {{another key aspect}} of how memory {{influences}} decisions. Priming activates a certain <b>memory</b> <b>node</b> that results in easier accessibility later on. A recent study examined the impact of playing certain types of music in a store that sells wine. German wines were sold {{at a higher rate}} when German music was played, and French wines sold better when French music was played. This effect resulted in a quarter of the variance in wine sales. Priming increases the accessibility of a memory and may be the reason we chose one alternative over the other.|$|E
50|$|Brand {{recognition}} is the initial phase of brand awareness and validates {{whether or not}} a customer remembers being pre-exposed to the brand. When customers experience brand recognition, they are triggered by either a visual or verbal cue. For example, when looking to satisfy a category need such as toilet paper, the customer would firstly be presented with multiple brands to choose from. Once the customer is visually or verbally faced with a brand, he/she may remember being introduced to the brand before. This would be classified as brand recognition, as the customer can retrieve the particular <b>memory</b> <b>node</b> that referred to the brand, once given a cue. Often, this form of brand awareness assists customers in choosing one brand over another when faced with a low-involvement purchasing decision.|$|E
40|$|Simulation {{of large}} {{sequential}} circuits {{is often a}} problem because of the unknown initial state of flipflops and other <b>memory</b> <b>nodes.</b> In real circuits this does not occur because, at power-on, electronic noise always forces each node into a O or I state {{that is consistent with}} the state of all the other nodes. In this paper, we describe an algorithm that simulates this power-on behavior in a switch-level simulator: it efficiently computes a consistent random initial state. This is important because it is often tedious or impossible to manually initialize all <b>memory</b> <b>nodes.</b> The new initializationalgorithm has been implemented in the switch-level simulator SLS and it has proven its usefulness in practice. ...|$|R
50|$|Pronghorn has 16 dual-socket IBM x360 nodes {{featuring}} Intel’s Xeon Phi 5110P coprocessors and 2.6-gigahertz Intel Sandy Bridge (Xeon E5-2670) cores. The {{system has}} 64 gigabytes of DDR3-1600 <b>memory</b> per <b>node</b> (63 GB usable <b>memory</b> per <b>node)</b> and is interconnected {{with a full}} fat tree Mellanox FDR InfiniBand network.|$|R
40|$|A {{performance}} {{prediction method}} is presented, which accurately predicts the runtime of a parallel application using the messagepassing {{model and the}} asynchronous task programming paradigm. The introduced model also considers the complete <b>memory</b> hierarchyofa multiprocessor <b>node</b> architecture. We study the optimizied parallel Linpack application as an example benchmark program. The predicted performances are compared with measurements on a PowerPC based distributed memory computer with shared <b>memory</b> <b>nodes.</b> We discuss {{the quality of the}} model for various processor numbers and problem sizes...|$|R
5000|$|Pseudo code:function SMA-star(problem): path queue: set of nodes, {{ordered by}} f-cost;begin queue.insert(problem.root-node); while True do begin if queue.empty (...) then return failure; //{{there is no}} {{solution}} that fits in the given <b>memory</b> <b>node</b> := queue.begin (...) // min-f-cost-node if problem.is-goal(node) then return success; [...] s := next-successor(node) if !problem.is-goal(s) && depth(s) == max_depth then f(s) := inf; [...] // there is no memory left to go past s, so the entire path is useless else f(s) := max(f(node), g(s) + h(s)); // f-value of the successor is the maximum of // f-value of the parent and [...] // heuristic of the successor + path length to the successor endif if no more successors then update f-cost of node and those of its ancestors if needed [...] if node.successors ⊆ queue then queue.remove(node); [...] // all children have already {{been added to the}} queue via a shorter way if memory is full then begin badNode := shallowest node with highest f-cost; for parent in badNode.parents do begin parent.successors.remove(badNode); if needed then queue.insert(parent); [...] endfor endif queue.insert(s); endwhileend ...|$|E
40|$|This thesis compares a Compute Node, {{a cluster}} compute node that can {{completely}} contain smaller processes within, but not larger ones, with a <b>Memory</b> <b>Node,</b> a cluster compute node that can fully contain large processes within main memory. The Compute Node and <b>Memory</b> <b>Node</b> are tested by executing matrix multiplication programs that {{range in size}} from small processes similar to jobs that can be run on a single workstation to large processes that would only fit entirely within a <b>Memory</b> <b>Node.</b> Memory was allocated using four types of memory allocation: static, external, dynamic, and automatic. This analysis provides a measure of the benefit obtained by using a <b>Memory</b> <b>Node</b> over a Compute Node for large processes as well as a measure of the penalties that add up when a process is forced to utilize the swap device continuously during execution. ^ The test programs executed showed the benefit of using a <b>Memory</b> <b>Node</b> over a Compute Node, with speedup values peaking at approximately 700 for the process sizes used. Additionally, the tests showed that when a Compute Node is forced to swap to the hard disk multiple times during execution, the majority of the execution time will be devoted to accessing the hard disk, not performing computations. Furthermore, the results showed that the method of allocating memory does not make a significant difference in execution times. ...|$|E
30|$|A double-donor {{system in}} Si has been {{considered}} the basic unit for quantum computing, either based on spin states [19] or charge states [20]. We study double-donor systems from a more practical approach, i.e., to demonstrate functions such as a dopant-based memory, with one donor as a sensor (conduction path) and another donor as a <b>memory</b> <b>node</b> (trap). If such a system could be identified, it would become possible to design memories in which sensing is done by a single-electron tunneling current via one donor atom, while storage is ensured by an individual donor working as a <b>memory</b> <b>node.</b>|$|E
40|$|International audienceThe {{increasing}} numbers of cores, shared caches and <b>memory</b> <b>nodes</b> within machines introduces a complex hardware topology. High-performance computing applications now have to carefully adapt their placement and behavior according to the underlying hierarchy of hardware resources and their software affinities. We introduce the Hardware Locality (hwloc) software which gathers hardware information about processors, caches, <b>memory</b> <b>nodes</b> and more, and exposes it to applications and runtime systems in a abstracted and portable hierarchical manner. hwloc may significantly help performance by having runtime systems place their tasks or adapt their communication strategies depending on hardware affinities. We show that hwloc can already be used by popular high-performance OpenMP or MPI software. Indeed, scheduling OpenMP threads according to their affinities or placing MPI processes according to their communication patterns shows interesting performance improvement thanks to hwloc. An optimized MPI communication strategy may also be dynamically chosen according {{to the location of}} the communicating processes in the machine and its hardware characteristics...|$|R
40|$|Abstract. Most HPC {{systems are}} {{clusters}} of shared <b>memory</b> <b>nodes.</b> Parallel programming must combine the distributed memory paralleliza-tion on the node inter-connect with the shared memory parallelization inside of each node. The hybrid MPI+OpenMP programming model is compared with pure MPI, compiler based parallelization, and other parallel programming models on hybrid architectures. The paper fo-cuses on bandwidth and latency aspects, but also whether programming paradigms can separate the optimization {{of communication and}} compu-tation. Benchmark results are presented for hybrid and pure MPI com-munication...|$|R
50|$|Similarly, in {{distributed}} shared <b>memory</b> each <b>node</b> of {{a cluster}} {{has access to}} a large shared memory in addition to each node's limited non-shared private memory.|$|R
40|$|We have {{analyzed}} {{an efficient}} {{integration of the}} multi-qubit echo quantum memory into the quantum computer scheme on the atomic resonant ensembles in quantum electrodynamics cavity. Here, one atomic ensemble with controllable inhomogeneous broadening {{is used for the}} quantum <b>memory</b> <b>node</b> and other atomic ensembles characterized by the homogeneous broadening of the resonant line are used as processing nodes. We have found optimal conditions for efficient integration of multi-qubit quantum memory modified for this analyzed physical scheme and we have determined a specified shape of the self temporal modes providing a perfect reversible transfer of the photon qubits between the quantum <b>memory</b> <b>node</b> and arbitrary processing nodes. The obtained results open the way for realization of full-scale solid state quantum computing based on using the efficient multi-qubit quantum memory. Comment: 13 pages, 5 figure...|$|E
40|$|Abstract: We {{experimentally}} demonstrate end-to-end memory transactions {{between a}} processor and <b>memory</b> <b>node</b> across the data vortex optical packet switch. Successful {{read and write}} transactions via the network at 4 × 2. 5 Gb/s are verified. © 2010 Optical Society of America OCIS codes: (060. 4259) Networks, packet-switched; (060. 4250) Network...|$|E
40|$|Abstract—In this work, {{we propose}} a single {{electron}} memory ‘SEM ’ design which consist of two key blocs: A memory bloc, with a voltage source VMem, a pure capacitor {{connected to a}} tunnel junction through a metallic <b>memory</b> <b>node</b> coupled to the second bloc which is a Single Electron Transistor “SET ” through a coupling capacitance. The “SET ” detects the potential variation of the <b>memory</b> <b>node</b> by the injection of electrons one by {{one in which the}} drain-source current is presented during the memory charge and discharge phases. We verify the design of the SET/SEM cell by the SIMON tool. Finally, we have developed a MAPLE code to predict the retention time and nonvolatility of various SEM structures with a wide operating temperature range. Index Terms—Single electron transistor, single electron memory, retention time, SET/SEM, SIMON I...|$|E
30|$|The LOBOC {{architecture}} is organized as a cluster, that is, {{it consists of}} 252 nodes, each with a processor composed of 24 cores using HyperThreading technology to execute up to 48 tasks in parallel. A queue system, called Altair PBS Professional, allocates the nodes to a process. The method of allocating resources (disk, <b>memory,</b> <b>nodes,</b> and cores) is simple: through a native Linux bash script file, {{it is possible to}} describe what features are needed to run a program and make use of them.|$|R
40|$|Current shared-memory {{multiprocessor}} CC-NUMA architectures {{have the}} main characteristic {{that they provide}} a global address space to applications by hardware. However, even thought the memory is virtually shared, it is physically distributed. Since <b>memory</b> <b>nodes</b> are distributed across the system, the cost to access to memory depends on {{the distance between the}} node that accesses the data and the node that physically contains the data. To reduce the impact of a bad initial memory placement, some operating systems offer a dynamic memory migration mechanism...|$|R
5000|$|... {{a shared}} <b>memory</b> compute <b>node</b> (14 {{internal}} NUMA nodes) with 112 Intel Sandy Bridge processor cores, 2 Intel Xeon Phi 5110P coprocessors and 1.7 TB of memory.|$|R
40|$|An {{elementary}} {{quantum network}} operation involves storing a qubit state in an atomic quantum <b>memory</b> <b>node,</b> and then retrieving and transporting the information through a single photon excitation {{to a remote}} quantum <b>memory</b> <b>node</b> for further storage or analysis. Implementations of quantum network operations are thus conditioned {{on the ability to}} realize matter-to-light and/or light-to-matter quantum state mappings. Here we report the generation, transmission, storage and retrieval of single quanta using two remote atomic ensembles. A single photon is generated from a cold atomic ensemble at one site 1, and is directed to another site through 100 metres of optical fibre. The photon is then converted into a single collective atomic excitation using a dark-state polariton approach 2. After a programmable storage time, the atomic excitation is converted back into a single photon. This is demonstrated experimentally, for a storage time of 0. 5 microseconds, by measurement of an anti-correlation parameter. Storage times exceeding ten microseconds are observed by intensity cross-correlation measurements. This storage period is two orders of magnitude longer than the time required to achieve conversion between photonic and atomic quanta. The controlled transfer of single quanta between remote quantum memories constitutes an important step towards distributed quantum network...|$|E
40|$|We {{demonstrate}} a photonic circuit with integrated long-lived quantum memories. Pre-selected quantum nodes - diamond micro-waveguides containing single, stable, and negatively charged nitrogen vacancy centers - are deterministically integrated into low-loss silicon nitride waveguides. Each quantum <b>memory</b> <b>node</b> efficiently couples into the single-mode waveguide (> 1 Mcps collected into the waveguide) and exhibits long spin coherence times {{of up to}} 120 μs. Our system facilitates the assembly of multiple quantum memories into a photonic integrated circuit with near unity yield, paving the way towards scalable quantum information processing. Comment: 5 pages, 4 figure...|$|E
40|$|International audienceParallelizing {{industrial}} simulation codes {{like the}} EUROPLEXUS software {{dedicated to the}} analysis of fast transient phenomena, is challenging. In this paper we focus on the efficient parallelization on a multi-core shared <b>memory</b> <b>node.</b> We propose to have each thread gather the data it needs for processing a given iteration range, before to actually advance the computation by one time step on this range. This lazy cache aware layout construction enables to keep the original data structure and leads to very localised code modifications. We show that this approach can improve the execution time by up to 40 % when the task size is set to have the data fit in the L 2 cache...|$|E
40|$|The Back Propagation Model for {{neural network}} {{simulation}} {{is a very}} simple and very popular model for solving real-world problems. It does, however, suffer from one major problem, that is, the time taken for the network to adjust its weights in order to response to an input. This paper proposes a computer architecture, specifically optimised for neural network simulation, which is capable of unlimited expansion without significant degradation in unit performance. It uses a high-speed conventional processor loosely coupled {{to a number of}} very simple processor and <b>memory</b> <b>nodes,</b> each with very limited functionality...|$|R
40|$|In modern Non-Uniform Memory Access (NUMA) systems, {{there are}} {{multiple}} <b>memory</b> <b>nodes,</b> {{each with its own}} <b>memory</b> controller. Local <b>nodes</b> can be accessed in less time than remote ones. Contention exists even if threads are scheduled on different memory domains when the threads have all or portions of their memory allocated on the same node. The contended resource in this case would be the <b>node’s</b> <b>memory</b> controller. Therefore, the scheduler must not only appropriately schedule memory-intensive applications, but it also must consider how to place the memory of these applications. In this work we design and implement a contention-aware scheduler in Linux. This scheduler strives to achieve a balanced system for workloads consisting of multiple cpu-intensive and/or memory-intensive applications. We also investigate and analyze recent Linux kernel patches that aim to provide NUMA-aware scheduling. Furthermore, we perform simulation analysis for several memory migration strategies that can be used by the scheduler whenever it decides to migrate threads. Finally, we present our collaborative work in which we design a holistic approach to NUMA scheduling...|$|R
5000|$|... an SGI ICE X cluster with 320 nodes or 7,680 Intel Ivy Bridge {{processor}} cores {{with a combined}} 20 TB of memory (24 cores and 64 GB <b>memory</b> per <b>node).</b>|$|R
40|$|The {{fabrication}} of a lateral single electron memory (LSEM) {{based on the}} integration of a metallic multiple tunnel junction (MTJ) and a <b>memory</b> <b>node</b> (MN) on a Si metal oxide semiconductor field effect transistor (MOS) has been investigated. High resolution electron beam lithography (HREBL) was used to fabricate devices presenting good morphological characteristics. The MN can be made as narrow as 50 nm, with an MTJ comprising a 5 x 5 array of sub- 5 nm Au islands deposited by thermal evaporation, and implemented on a short 0. 5 mu m channel MOS. An operating temperature close to 77 K with the two memory levels relying on the excess or shortfall of approximately 30 electrons is expected. status: publishe...|$|E
40|$|Motivation: One of {{the major}} {{challenges}} for contemporary bioinfor-matics is the analysis and accurate annotation of genomic datasets to enable extraction of useful information about the functional role of DNA sequences. This article describes a novel genome-wide statis-tical approach to the detection of specific DNA sequence motifs based on similarities between the promoters of similarly expressed genes. This new tool, cisExpress, is especially designed for use with large datasets, such as those generated by publicly accessible whole genome and transcriptome projects. cisExpress uses a task farming algorithm to exploit all available computational cores within a shared <b>memory</b> <b>node.</b> We demonstrate the robust nature {{and validity of the}} proposed method. It is applicable for use {{with a wide range of}} gen-omic databases for any species of interest. Availability: cisExpress is available at www. cisexpress. org...|$|E
30|$|D time {{measurements}} [see Figure 3 d] show two-level random telegraph signals (RTS), suggesting a two-level trap. In our device, {{it is most}} natural {{to assume that the}} trap is a donor, either ionized (D+) or neutralized (D 0). This is because the number of donors in the channel is larger than the estimated number of interface defects. Furthermore, from our previous studies [24] comparing doped and undoped channel FETs, it is evident that most of the features (irregular peaks) observed in the measured characteristics are due to the channel donors. Our further analysis [23] reveals that the trap-donor is closer to the front interface as compared with the conduction path donor. We thus identified devices in which two donors work as a sensor and as a <b>memory</b> <b>node,</b> respectively. This allows further investigation of the physics behind single-electron transfer within double-donor systems.|$|E
5000|$|... "Shared-everything" [...] {{architectures}} share both data on {{disk and}} data in <b>memory</b> between <b>nodes</b> in the cluster. This {{is in contrast}} to [...] "shared-nothing" [...] architectures that share none of them.|$|R
5000|$|... "Shared-nothing" [...] {{architectures}} share {{neither the}} data on disk nor the data in <b>memory</b> between <b>nodes</b> in the cluster. This {{is in contrast to}} [...] "shared-everything" [...] architectures, which share both.|$|R
40|$|Abstract. A major {{trend in}} HPC is the {{escalation}} toward manycore, where systems {{are composed of}} shared <b>memory</b> <b>nodes</b> featuring numerous processing units. Unfortunately, with scale comes complexity, here {{in the form of}} non-uniform memory accesses and cache hierarchies. For most HPC applications, harnessing the power of multicores is hindered by the topology oblivious tuning of the MPI library. In this paper, we propose a framework to tune every type of shared memory communications according to locality and topology. An implementation inside Open MPI is evaluated experimentally and demonstrates significant speedups compared to vanilla Open MPI and MPICH 2. ...|$|R
40|$|A {{theory is}} {{presented}} which assumes that individuals {{are represented by}} unique nodes in memory. To test the theory, simple facts were predicated of an individual person. Some facts referred to him by proper name, and other facts by his profession. In a before condition, subjects learned that the profession and name referred to the same individual before learning the facts, while in an after condition, they learned the identity after learning the facts. Subsequent to learning the facts and identities, subjects verified sentences {{based on what they}} had learned. Verification latencies indicated that in the before condition, one <b>memory</b> <b>node</b> was created to represent the individual, but two nodes were set up in the after condition. Assymmetries between proper names and professions indicate that the two types of referring expressions are treated differently in long-term memory...|$|E
40|$|This paper {{describes}} a performanceprediction model of parallel mark-sweep garbage collectors (GC) on shared memory multiprocessors. The prediction model takes the heap snapshot and memory access cost parameters (latency and occupancy) as inputs, and outputs performanceoftheparallel marking {{on any given}} number of processors. It takes several factors that affects performance into account: cache misses costs, memory access contention, and increase of misses by parallelization. We evaluate this model by comparing the predicted GC performance and measured performanceontwoarchitecturally different shared memory machines: Ultra Enterprise 10000 (crossbar connected SMP) and Origin 2000 (hypercube connected DSM). Our model accurately predicts qualitatively different speedups on the two machines that occurred in one application, which turn out to beduetocontentions on a <b>memory</b> <b>node.</b> In addition to performance analysis, applications of the proposedmodel include adaptive GC algorithm to achieve optimal performancebasedon the prediction. This paper shows the effect of automatic regulation of GC parallelism...|$|E
40|$|Abstract—Open-source routers on new PC {{hardware}} {{allows for}} forwarding speeds of 10 Gb/s and above. We present detailed performance measurements using Linux on two complex PC hardware platforms. Both platforms use PCIe gen 2, dual I/O bridges and have support for non-uniform memory access (NUMA). The AMD platform uses four processors equipped with eight cores and four nodes of local memory. The Intel platform has two quad-core CPUs each with local memory. Packets being forwarded through a PC-based router can be separated into three steps: receive-dma, lookup, and transmitdma. Each step was studied individually. In particular, we studied how varying the CPU core and <b>memory</b> <b>node</b> effects the forwarding speeds. Our results show a large performance dependency of selecting CPU cores and memory nodes. In particular, DMA works best with memory nodes {{closest to the}} I/O bridge where the interface card is connected. Correspondingly, CPU access is most efficient on local memory. Consequently, choosing CPU core and memory nodes badly leads to a significant performance decrease. I...|$|E
40|$|Today most {{systems in}} {{high-performance}} computing (HPC) feature a hierarchical hardware design: Shared <b>memory</b> <b>nodes</b> with several multi-core CPUs are connected via a network infrastructure. Parallel programming must combine distributed memory parallelization on the node interconnect with shared memory parallelization inside each node. We describe potentials {{and challenges of}} the dominant programming models on hierarchically structured hardware: Pure MPI (Message Passing Interface), pure OpenMP (with distributed shared memory extensions) and hybrid MPI+OpenMP in several flavors. We pinpoint cases where a hybrid programming model can indeed be the superior solution because of reduced communication needs and memory consumption, or improved load balance. Furthermor...|$|R
40|$|Most HPC {{systems are}} {{clusters}} of shared <b>memory</b> <b>nodes.</b> Parallel programming must combine the distributed memory parallelization on the node interconnect with the shared memory parallelization inside each node. The hybrid MPI+OpenMP programming model is compared with pure MPI, compiler based parallelization, and other parallel programming models on hybrid architectures. The paper focuses on bandwidth and latency aspects, {{and also on}} whether programming paradigms can separate the optimization of communication and computation. Benchmark results are presented for hybrid and pure MPI communication. This paper analyzes {{the strengths and weaknesses}} of several parallel programming models on clusters of SMP nodes...|$|R
30|$|A {{canonical}} node {{is assumed}} to be equipped with several PIR sensors, a micro-controller unit (MCU), and a radio as well as on-board RAM and flash <b>memory.</b> <b>Nodes</b> are assumed to be tetherless and battery-powered, and consequently, the overall constraint for each node is energy and data transmission bandwidth. The PIR sensors are responsible for collecting the time-varying HME of each sub-region, and the MCU converts the analogy signal of each PIR sensor into digital signal. If the amplitude of the PIR signal exceed the pre-defined threshold, the radio unit on the node will send the data to the sink.|$|R
