10|12|Public
50|$|Availability, {{operational}} (Ao) The {{probability that}} an item will operate satisfactorily {{at a given}} point in time when used in an actual or realistic operating and support environment. It includes logistics time, ready time, and waiting or administrative downtime, and both preventive and corrective maintenance downtime. This value {{is equal to the}} mean time between failure (MTBF) divided by the mean time between failure plus the <b>mean</b> <b>downtime</b> (MDT). This measure extends the definition of availability to elements controlled by the logisticians and mission planners such as quantity and proximity of spares, tools and manpower to the hardware item.|$|E
40|$|Visualization is the {{foundation}} for human to understand as human think and create in a graphic world. By nature, maintenance activity is a multi discipline approach, where maintenance engineer {{should be able to}} convey their idea to people with various backgrounds. In this situation, visualization is a powerful mode; it can act as a bridge to eliminate the knowledge gap within the group. Knight (2001) developed Jack-knife diagram for visualizing total downtime and factors influencing the failure (failure frequency and <b>mean</b> <b>downtime)</b> in a point estimation. As failure data is a probabilistic in nature, it is important to look not only point estimation but also interval estimation, so the precision and uncertainty of estimation can be identified. This paper proposes a way of visualizing total downtime and factors influencing the failure (failure frequency and <b>mean</b> <b>downtime)</b> in interval estimation. Case study from scaling machines is used for illustration. Godkänd; 2010; 20100823 (ysko...|$|E
40|$|It is {{essential}} that management and engineering understand {{the need for an}} availability requirement for the customer's space transportation system as it enables the meeting of his needs, goal, and objectives. There are three types of availability, e. g., operational availability, achieved availability, or inherent availability. The basic definition of availability is equal to the mean uptime divided by the sum of the mean uptime plus the <b>mean</b> <b>downtime.</b> The major difference is the inclusiveness of the functions within the <b>mean</b> <b>downtime</b> and the mean uptime. This paper will address tIe inherent availability which only addresses the <b>mean</b> <b>downtime</b> as that mean time to repair or the time to determine the failed article, remove it, install a replacement article and verify the functionality of the repaired system. The definitions of operational availability include the replacement hardware supply or maintenance delays and other non-design factors in the <b>mean</b> <b>downtime.</b> Also with inherent availability the mean uptime will only consider the mean time between failures (other availability definitions consider this as mean time between maintenance - preventive and corrective maintenance) that requires the repair of the system to be functional. It is also essential that management and engineering understand all influencing attributes relationships {{to each other and to}} the resultant inherent availability requirement. This visibility will provide the decision makers with the understanding necessary to place constraints on the design definition for the major drivers that will determine the inherent availability, safety, reliability, maintainability, and the life cycle cost of the fielded system provided the customer. This inherent availability requirement may be driven by the need to use a multiple launch approach to placing humans on the moon or the desire to control the number of spare parts required to support long stays in either orbit or on the surface of the moon or mars. It is the intent of this paper to provide the visibility of relationships of these major attribute drivers (variables) to each other and the resultant system inherent availability, but also provide the capability to bound the variables providing engineering the insight required to control the system's engineering solution. An example of this visibility will be the need to provide integration of similar discipline functions to allow control of the total parts count of the space transportation system. Also the relationship visibility of selecting a reliability requirement will place a constraint on parts count to achieve a given inherent availability requirement or accepting a larger parts count with the resulting higher reliability requirement. This paper will provide an understanding for the relationship of mean repair time (<b>mean</b> <b>downtime)</b> to maintainability, e. g., accessibility for repair, and both mean time between failure, e. g., reliability of hardware and the system inherent availability. Having an understanding of these relationships and resulting requirements before starting the architectural design concept definition will avoid considerable time and money required to iterate the design to meet the redesign and assessment process required to achieve the results required of the customer's space transportation system. In fact the impact to the schedule to being able to deliver the system that meets the customer's needs, goals, and objectives may cause the customer to compromise his desired operational goal and objectives resulting in considerable increased life cycle cost of the fielded space transportation system...|$|E
50|$|For Internet servers downtimes above 1% {{per year}} or worse can be {{regarded}} as unacceptable as this <b>means</b> a <b>downtime</b> of more than 3 days per year. For e-commerce and other industrial use any value above 0.1% is usually considered unacceptable.|$|R
50|$|In {{service level}} agreements, {{it is common}} to mention a {{percentage}} value (per month or per year) that is calculated by dividing the sum of all downtimes timespans by the total time of a reference time span (e.g. a month). 0% <b>downtime</b> <b>means</b> that the server was available all the time.|$|R
40|$|In {{the early}} 1990 ’s, {{the problem of}} sticky cot-ton became more serious than in the past. A related concern arose among {{researchers}} involving the po-tential accumulation of more dust on sticky cotton that would be released during subsequent process-ing. This might <b>mean</b> more frequent <b>downtime</b> for cleaning machinery and increased surveillance to remain {{in compliance with the}} Cotton Dust Stan-dard. This study analyzed a large number of samples with varying degrees of stickiness. The percentage of reducing sugars, and the Sticky Cotton Thermodetector and minicard ratings for stickiness were used to determine stickiness of lint samples. These samples were also assayed for cotton dust potential, which were compared with the stickines...|$|R
40|$|Condition based {{maintenance}} plus (CBM +) is {{the primary}} reliability driver in the total life-cycle systems management (TLCSM) supportability strategy of the Department of Defense. In concert with the other TLCSM enablers, such as continuous process improvement (CPI), cause and effect predictive modeling, and desired outcomes achieved through performance based logistics (PBL), CBM + strives to optimize key performance measures of materiel readiness- materiel availability, materiel reliability, <b>mean</b> <b>downtime,</b> and ownership costs. Under the authority in DoD Directive 5134. 01 (Reference (a)), this Instruction establishes policy and guidance for th...|$|E
40|$|Pareto histograms are {{commonly}} used to determine maintenance priorities by ranking equipment failure codes according to their relative cost or downtime contribution. However, such histograms do not readily enable identification of the dominant variables influencing downtime and repair costs, namely the failure frequency, <b>mean</b> <b>downtime</b> and mean repair cost associated with each failure code. Advances an alternative method for analysing equipment downtime and repair costs using logarithmic (log) scatterplots. By applying limit values, log scatterplots {{can be divided into}} four quadrants enabling failures to be classified according to acute or chronic characteristics and facilitating root cause failure analysis. Log scatterplots permit the identification of frequently occurring failures that consume relatively little repair cost or downtime yet cause frequent operational disturbances leading to production losses. In addition, by graphing the trend of failure data over successive time periods, log scatterplots provide a useful visual means of evaluating the performance of maintenance improvement initiatives. Provides examples of the practical application of log scatterplots by a number of mining companies and mining equipment suppliers in Chile...|$|E
40|$|This paper {{describes}} {{the usage of}} IDDX monitors, which provide (periodic) data to be employed for predicting the remaining lifetime of processor cores in homogeneous multi-processor SoCs during their lifetime. This forms the basis of self-repair with no mean down time for these SoCs and dramatically improving their dependability. We accomplish this goal by optimally choosing and combining our designed health monitors, such as delay and current monitors to provide non-redundant measurement data. These can be implemented nowadays as IJATG-compatible embedded instruments. Accelerated stress tests were carried out using a set of processor cores in combination {{with a number of}} health monitors providing historic data. These results form the basis of a remaining lifetime prediction model for delay (processor clock frequency), where the coefficients are determined by a genetic algorithm. After the final test of an individual SoC, these stored coefficients can be periodically updated in an embedded processor during its lifetime by the health monitors. In the case of seriously degrading cores, counteractions are automatically taken, like core isolation and (e. g. spare) replacement. In this case, use is made of advanced run-time mapping software. The result is a zero <b>mean</b> <b>downtime</b> SoC with 40 % reliability improvement in our 9 processor-core SoC...|$|E
5000|$|In {{the past}} 20 years {{telecommunication}} networks and other complex software systems have become essential parts {{of business and}} recreational activities.“At {{the same time the}} economy is in a downturn, 60% almost -- that's six out of 10 businesses -- require 99.999. That's four nines or five nines of availability and uptime for their mission-critical line-of-business applications.And 9% of the respondents, so that's almost one out of 10 companies, say that they need greater than five nines of uptime. So what that <b>means</b> is, no <b>downtime.</b> In other words, you have got to really have bulletproof, bombproof applications and hardware systems. So you know, what do you use? Well one thing you have high-availability clusters or you have the more expensive and more complex fault-tolerance servers.” ...|$|R
40|$|Software {{downtime}} {{refers to}} the time when software is unavailable or its operation is prevented for some reason. New software installations, migrations and upgrades cause downtime in the system which leads to loss in revenue and reduced general level of service for the company. Difficult part is finding the right set of tools and practices that minimize this downtime at different stages of software’s deployment. Choosing the right tools that are compatible with company’s infrastructure is also challenging and typically happens through trial and error. The focus of this thesis is to study different <b>means</b> to reduce <b>downtime</b> during software deployment. Study focuses on reviewing tools and practices {{that can be used to}} deliver and deploy software to a production environment without service interruption. The research was carried out as literature review by examining the documents and materials from different sources...|$|R
40|$|Thesis (M. Ing. (Development and Management)) [...] North-West University, Potchefstroom Campus, 2008. This {{research}} work {{focuses on the}} use of integrated control and asset management (ICAM) in optimizing process plant operations. The assets considered are mainly valves and instruments, such as pressure, flow, temperature and level transmitters. The dissertation discusses how integrated control and asset management could be used to detect potential process problems as well as incipient faults in field devices and valves. Integrated control and asset management is a technology whereby the control loop or system is made to perform functions other than its traditional function of controlling the process in order to achieve an improved production target. The conditions of the instruments, valves and the process itself are monitored with the aid of another system called asset management. The condition monitoring data is superimposed on the process control signal thus giving the name ‘integrated’. Commonly known as artificial intelligent design, the asset management software is able to unlock data buried in a distributed control system (DCS), supervisory control and digital acquisition (SCADA), programmable logic control (PLC) or other remote terminal units (RTUs), analyze it and give actionable recommendations. Two plants in Sasol which have deployed the ICAM technology were investigated namely the Auto-Thermal reforming (ATR) plant and the N-Butanol plant. The research investigation revealed that the use of ICAM gives rise to i) 	a proactive maintenance strategy ii) 	increased uptime which <b>means</b> reduced <b>downtime</b> iii) 	increased throughput iv) 	removal of unnecessary maintenance of valves and instruments and v) 	an improved way of running the process. The ICAM technology has been made possible by the HART technology, the foundation fieldbus technology as well as SMART-based instruments and valves. Some limitations of ICAM were also discovered as well as other factors that could hamper it from delivering its full benefits. Master...|$|R
40|$|To {{achieve an}} {{efficient}} risk management of a drinking water system {{the entire system}} has to be considered, from source to tap. An important part of risk management is to identify hazards and estimate risks, i. e. to conduct risk analyses. In order to provide a relevant basis for evaluating risks and efficiently prioritising risk-reduction options, a risk analysis needs to properly consider interaction between different parts and components of the system. This is especially important in complex systems. Logic tree models have the capability of properly reflect system functionality as well as facilitating quantification of risk levels. A fault tree model was therefore constructed for an integrated and probabilistic risk analysis of the drinking water system in Göteborg, Sweden. The main (top) event studied in the analysis was supply failure, which included quantity and quality failures. Quantity failure occurs when no water is delivered to the consumer and quality failure when water is delivered but {{unfit for human consumption}} according to existing water quality standards. Hard data and expert judgements were used for estimating probabilities of events, consequences and uncertainties of estimates. Monte Carlo simulations were used for the calculations in order to facilitate uncertainty analysis of risk levels. The risk analysis provided information on the probability of failure, mean failure rate and <b>mean</b> <b>downtime</b> of the system. The number of people affected was also included in the fault tree and risk levels were expressed as Costumer Minutes Lost. The primar...|$|E
40|$|Future space {{transportation}} architectures and designs must be affordable. Consequently, their Life Cycle Cost (LCC) must be controlled. For the LCC to be controlled, {{it is necessary}} to identify all the requirements and elements of the architecture {{at the beginning of the}} concept phase. Controlling LCC requires the establishment of the major operational cost drivers. Two of these major cost drivers are reliability and maintainability, in other words, the system's availability (responsiveness). Potential reasons that may drive the inherent availability requirement are the need to control the number of unique parts and the spare parts required to support the transportation system's operation. For more typical {{space transportation}} systems used to place satellites in space, the productivity of the system will drive the launch cost. This system productivity is the resultant output of the system availability. Availability is equal to the mean uptime divided by the sum of the mean uptime plus the <b>mean</b> <b>downtime.</b> Since many operational factors cannot be projected early in the definition phase, the focus will be on inherent availability which is equal to the mean time between a failure (MTBF) divided by the MTBF plus the mean time to repair (MTTR) the system. The MTBF is a function of reliability or the expected frequency of failures. When the system experiences failures the result is added operational flow time, parts consumption, and increased labor with an impact to responsiveness resulting in increased LCC. The other function of availability is the MTTR, or maintainability. In other words, how accessible is the failed hardware that requires replacement and what operational functions are required before and after change-out to make the system operable. This paper will describe how the MTTR can be equated to additional labor, additional operational flow time, and additional structural access capability, all of which drive up the LCC. A methodology will be presented that provides the decision makers with the understanding necessary to place constraints on the design definition. This methodology for the major drivers will determine the inherent availability, safety, reliability, maintainability, and the life cycle cost of the fielded system. This methodology will focus on the achievement of an affordable, responsive space transportation system. It is the intent of this paper to not only provide the visibility of the relationships of these major attribute drivers (variables) to each other and the resultant system inherent availability, but also to provide the capability to bound the variables, thus providing the insight required to control the system's engineering solution. An example of this visibility is the need to provide integration of similar discipline functions to allow control of the total parts count of the space transportation system. Also, selecting a reliability requirement will place a constraint on parts count to achieve a given inherent availability requirement, or require accepting a larger parts count with the resulting higher individual part reliability requirements. This paper will provide an understanding of the relationship of mean repair time (<b>mean</b> <b>downtime)</b> to maintainability (accessibility for repair), and both mean time between failure (reliability of hardware) and the system inherent availability...|$|E
40|$|Traditional {{fault tree}} {{analysis}} is not always sufficient when analysing complex systems. To overcome the limitations dynamic fault tree (DFT) analysis is suggested in the literature as well as different approaches for how to solve DFTs. For added value in {{fault tree analysis}}, approximate DFT calculations based on a Markovian approach are presented and evaluated here. The approximate DFT calculations are performed using standard Monte Carlo simulations and do not require simulations of the full Markov models, which simplifies model building and in particular calculations. It is shown how to extend the calculations of the traditional OR- and AND-gates, so that information {{is available on the}} failure probability, the failure rate and the <b>mean</b> <b>downtime</b> at all levels in the fault tree. Two additional logic gates are presented that make it possible to model a system’s ability to compensate for failures. This work was initiated to enable correct analyses of water supply risks. Drinking water systems are typically complex with an inherent ability to compensate for failures that is not easily modelled using traditional logic gates. The approximate DFT calculations are compared to results from simulations of theorresponding Markov models for three water supply examples. For the traditional OR- and AND-gates, and one gate modelling compensation, the errors in the results are small. For the other gate modelling compensation, the error increases with the number of compensating components. The errors are, however, in most cases acceptable with respect to uncertainties in input data. The approximate DFT calculations improve the capabilities of fault tree analysis of drinking water systems since they provide additional and important information and are simple and practically applicable...|$|E
40|$|Operations and {{maintenance}} (O&M) activities represent a significant {{share of the}} expenses during the lifetime of offshore wind farms. When compared to onshore wind farms, O&M costs are increased for the offshore case, as specialized vessels, weather windows and rough conditions <b>mean</b> more failures, <b>downtime</b> (decreasing availability), spare parts, and man-hours. This study comprises {{an analysis of the}} available O&M data from a selected offshore wind farm. The results and conclusions from this investigation could then be used to evaluate possible reliability improvements and compare options for the maintenance strategies, as well as to ponder the convenience of warranty periods and O&M agreements between wind farm operators and wind turbine manufacturers or O&M service providers. The life-cycle cost (LCC) concept is utilized in the analysis of the wind farm survey for this thesis. LCC analysis could be the starting point to make decisions regarding specific wind turbine models, as selecting the turbines with the lowest initial cost may not be necessarily the scenario which also costs the least amount of money when taking into consideration the whole life cycle. It may also be a great tool to forecast future operational incomes and expenses of offshore wind farms...|$|R
40|$|The {{quality and}} {{reliability}} of utility-generated electric power is presently receiving {{a great deal of}} attention from the chemical and refining industry. What changes have taken place to make electric power reliability a major topic of discussion at plants across the country? Has the quality {{and reliability of}} utility-generated power deteriorated over the past five or ten years? Or, has the perception of what constitutes reliable power changed with the advent, installation, and increasing usage of microprocessor-based equipment and controllers? The differing views held by both parties tend to make their relationship adversarial. Both parties have problems with their individuals views and the associated monetary costs, which can be either a loss or a gain. Improved reliability for the chemical plant means less "off spec" product, thereby resulting in more product to sell. Improved reliability for the utility <b>means</b> less customer <b>downtime,</b> thereby resulting in more KWH sales and a higher capacity factor. The biggest limiting factor to solving the actual problems is the dollar cost associated with that solution. Each solution must have a payback period that meets the economic criteria for return on investment for either the industry or the utility...|$|R
40|$|The overall {{objective}} of the current research {{is the development of}} a computationally efficient, conceptually simple, easy-to-use method providing loss estimates and other performance metrics for structural systems subjected to seismic loads. The method is based on (1) a novel probabilistic site-specific seismological model, (2) an efficient algorithm for calculating response statistics and (3) probabilistic models for life-cycle structural performance. The proposed seismic-hazard model uses earthquake records at the site of interest and the specific barrier seismological model to provide a more realistic representation of site seismic hazard. The information provided by records and the specific barrier model is aggregated in a Bayesian framework and used subsequently to simulate ground-motion samples {{as a function of the}} moment magnitude m and source-to-site distance r. Structural response statistics to simulated ground acceleration records are obtained by a novel efficient, nonintrusive method that resembles the Monte Carlo approach. Like Monte-Carlo, the method calculates structural responses to samples of the ground-motion process. Unlike Monte-Carlo, which uses a large number of samples selected at random, the proposed method uses a small number of samples selected in an optimal way. The efficiency of the proposed method allows calculation of distributions, rather than just <b>mean</b> values, for <b>downtime</b> cost, damage, and other metrics. Probability distributions can be used in insurance applications to calculate premiums to cover cost of damage. They are also essential tools for assessing tail risk, a quantity which accounts for low-probability events with high impact, used for transferring risk to reinsurance markets. These capabilities are particularly important when dealing with extreme events. Numerical results are presented for linear and non-linear systems. Life-cycle scenarios for seismic events are simulated and used to estimate life-cycle cost and damage. 2020 - 01 - 2...|$|R
40|$|Continuous {{monitoring}} of ammonia (NH 3) emissions from two mechanically ventilated commercial broiler houses {{located in the}} southeastern United States was performed during a one-year period over 2005 - 2006 as a joint effort between Iowa State University and the University of Kentucky. Ammonia concentrations were measured using Innova 1412 photoacoustic NH 3 monitors. Ventilation rates in each house were measured continuously by monitoring the building static pressure and operational status of all ventilation fans in conjunction with individual performance curves developed and verified in situ using a Fan Assessment Numeration System (FANS) unit. Expressed in various units, NH 3 emissions from the two broiler houses over the one-year production period were of the following values: a) 35. 4 g per bird marketed (77. 9 lb per 1, 000 birds marketed), including both grow-out (50 - 54 d per flock) and downtime (12 - 25 d between flocks) emissions; b) annual (365 -d) emission of 4. 63 Mg (5. 1 US tons) per house, including both grow-outs and downtime; c) maximum grow-out daily emission of 30. 6 kg/d-house (67. 4 lb/d-house) for one house and 35. 5 kg/d-house (78. 2 lb/d-house) for the other; d) mean grow-out daily emission of 14. 0 ± 9. 1 (S. D.) kg/d-house; e) <b>mean</b> <b>downtime</b> daily emission of 8. 8 ± 8. 3 kg/d-house. Flocks on new bedding had a lower emission rate of 12. 4 ± 9. 4 kg/d-house, as compared to 14. 5 ± 8. 9 kg/d-house for flocks on built-up litter. The NH 3 emission factor of 35. 4 g/bird marketed from this study is substantially lower than that cited by US EPA of 100 g/yr-bird (the US EPA yr-bird unit is equivalent to bird marketed) ...|$|E
40|$|Overview: Cloud {{technologies}} have matured quickly {{over the last}} couple of years and now provide an interesting platform on which to host grid services. The dynamic nature of these resources could ease life-cycle management for system administrators and could provide customized resources for users. However, questions remain about how these resources can meet the grid's security and operational policies. This presentation explains the challenges raised by using cloud resources for a EGEE grid site. Analysis: A typical (minimal) grid site provides computing and storage to supported Virtual Organizations (VOs) and runs a few services to make those resources visible on the grid. Amazon Web Services (AWS), the most mature of the available platforms, offers "bare metal" interfaces to virtual machines and to persistent disk images, meaning that standard EGEE tools for machine configuration and management work with little or no changes. Specifically, we take advantage of the Elastic Computing Cloud (EC 2), the Elastic Block Store (EBS), and Elastic IP services for the grid site. The machine-like interfaces mean there are few technical barriers to running grid resources on those services. The full environment where the machines are distant, in Amazon's IP space, and behind firewalls, poses challenges. We describe how various issues such as obtaining grid certificates, keeping logs, etc. were solved. We also describe the operational issues we encountered during the two month trial. Impact: For system administrators, having a pool of virtualized resources may ease the management of a grid site. Specifically the upgrade process can be more efficient because upgraded services can be deployed in tandem with existing services and tested in place. Switching to the new service can be done after the service has been verified. This <b>means</b> less <b>downtime</b> when upgrading, but also provides a more secure fallback solution when something (inevitably) goes wrong with the first installation of the upgraded service. The lowered downtime, increased reliability, and extensibility are clear benefits for users as well. The virtualization of the cloud resources also permits the execution environment to be customized. This would allow user communities to provide standard images with their software pre-installed. Heterogeneous software environments are one of the leading causes of job failures, and to date, the grid offers no comprehensive solution to this problem. Conclusions: Running grid services within the Amazon cloud is feasible as shown by the two month trial described in this presentation. In the future, site administrators may want to virtualize their own computer centers using open source cloud implementations. For them, bridging external and internal cloud resources may provide an interesting alternative to purchasing hardware. Users may want to take advantage of additional data transfer protocols ([URL] bittorrent, etc.) offered by the cloud resources...|$|R
40|$|Specially Airlines. Banks and Telecommunication firms who are {{maintaining}} 24 hrsonline {{mission critical}} applications. the hardware <b>downtime</b> <b>means</b> lost productivity. lost revenue {{and many times}} lost customers. Having effective operation and maintenance capability. these services can he operated at a very low failure rate to remain competitively in the business. On the other hand minimization of the maintenance cost is the competitive edge of the business. These firms are continuously facing a problem of maintaining their services with a high reliability {{due to the lack}} of service capability levels of the hardware. This is specially relevant to the Sri Lankan ICT Sector where it doesn't have highly reliable Electricity Infrastructure. Due to the unreliable Electricity Infrastructure all the supporting hardware such as UPSs, servers, switches and routers etc. are frequently get affected and resulting reduced lifetime and frequent interruptions of supporting services. Therefore the challenge is to select the cost effective best-fit strategy with the business objectives. According to the service capability assessment of Sri Lankan IC'T Sector (fine tuned sample), the system availability increases with the cost of service at a reducing rate and reaches a maximum at a certain value. Most of the organizations have invested too much than required to maintain their systems at their current availability level. Cost of service is negligible for some firms since they are maintaining 100 % redundant systems where it costs only the spares replacement cost. Actually there is a trade off between the cost of redundancy improvement and cost of maintenance criteria creation or improvement. There are many problems associated with developing a good service strategy for SriLankan ICT Sector as identified and explained in the thesis ICT organizations should do a thorough analysis on system maintenance at the system design stage or the purchasing stage or even at operational stage to create a best-fit service strategy. When purchasing a new hardware system: the initial stages Tender documentation stage& Tender evaluation stage. and the purchasing stage have to be addressed so carefully in order to create a best service strategy in achieving the required availability level to face the industry competition successfully. Assessing these strategies in cost perspectives gives a clear picture in selecting the best strategy for implementation. The selected strategy also should be a best fit and achievable strategy. ICT Organizations who are already having a service strategy also should do a thorough analysis to assess its service capability in terms of availability and cost perspectives. The framework identified and explained in this thesis will be helpful in assessing the existing strategy to continue with the existing strategy or to move into a better strategy...|$|R
40|$|Limited data {{exist in}} the {{literature}} regarding air emissions from U. S. turkey feeding operations. The project described in this article continuously monitored ammonia (NH 3) and particulate matter (PM) emissions from turkey production houses in Iowa (IA) and Minnesota (MN) for 10 to 16 months, with IA monitoring Hybrid tom turkeys (35 to 143 d of age, average market body weight of 17. 9 kg) for 16 months and MN monitoring Hybrid hens (35 to 84 d of age, average market body weight of 6. 7 kg) for 10 months. Mobile air emission monitoring units (MAEMUs) were used in the continuous monitoring. Based on the approximately one-year measurement, each involving three flocks of birds, daily NH 3, PM 10, and PM 2. 5 concentrations (mean ±SD) in the tom turkey barn were 8. 6 ± 10. 0 ppm, 1104 ± 719 µg m- 3, and 143 (± 124) µg m- 3, respectively. Daily NH 3 and PM 10 concentrations (mean ±SD) in the hen turkey barn were 7. 3 ± 7. 9 ppm and 301 ± 160 µg m- 3, respectively. Daily NH 3 concentrations during <b>downtime</b> (<b>mean</b> ±SD) were 38. 4 ± 20. 5 and 20. 0 ± 16. 3 ppm in the tom and hen barns, respectively. The cumulative NH 3 emissions (mean ±SE) were 141 ± 13. 1 and 1. 8 ± 0. 9 g bird- 1 for the tom turkeys during 108 d growout and 13 d downtime, respectively, and 52 ± 2. 1 and 28. 2 ± 2. 5 g bird- 1 for the hen turkeys during 49 d growout and 32 d downtime, respectively (the extended downtime for the hen house was to ensure monitoring of one flock per season). The cumulative PM 10 emission (mean ±SE) was 28. 2 ± 3. 3 g bird- 1 for the tom turkeys during 108 d growout and 4. 6 ± 2. 2 and 0. 3 ± 0. 06 g bird- 1 for the hen turkeys during 49 d growout and 32 d downtime, respectively. Downtime in the hen house was of greater duration than would be typically observed (32 d vs. 7 d to 14 d typical). The cumulative PM 2. 5 emission (mean ±SE) was 3. 6 ± 0. 7 g bird- 1 for the tom turkeys during 108 d growout (not monitored for the hen turkeys). Because farm operations will vary in flock number, growout days, and downtime; annual emissions can be calculated from the cumulative emissions and downtime emissions per bird from the data provided. Air emissions data from this study, presented in both daily emission and cumulative per-bird-marketed emission, contribute to the improved U. S. national air emissions inventory for animal feeding operations...|$|R

