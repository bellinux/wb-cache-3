126|375|Public
5000|$|<b>Memory</b> <b>Saving</b> InstancesAccurate 3D Motion BlurHair, Grass and Fur ...|$|E
5000|$|<b>Memory</b> <b>saving,</b> for example, by {{converting}} {{floating point}} {{latitude and longitude}} co-ordinates into smaller integer values.|$|E
50|$|Its general {{effect is}} that the program text consumes no writable <b>memory,</b> <b>saving</b> it for dynamic data, and that all {{instances}} of the program are run from a single copy.|$|E
40|$|We {{report on}} the {{computational}} characteristics of ab initio nuclear structure calculations in a symmetry-adapted no-core shell model (SA-NCSM) framework. We examine the computational complexity of the current implementation of the SA-NCSM approach, dubbed LSU 3 shell, by analyzing ab initio results for 6 Li and 12 C in large harmonic oscillator model spaces and SU(3) -selected subspaces. We demonstrate LSU 3 shell's strong-scaling properties achieved with highly-parallel methods for computing the many-body matrix elements. Results compare favorably with complete model space calculations and significant <b>memory</b> <b>savings</b> are achieved in physically important applications. In particular, a well-chosen symmetry-adapted basis affords <b>memory</b> <b>savings</b> in calculations of states with a fixed total angular momentum in large model spaces while exactly preserving translational invariance. Comment: 11 pages, 8 figure...|$|R
40|$|We present work on unifying the {{two main}} data {{structures}} involved during reachability analysis of timed automata. We also present result on sharing common elements between states. The experimental evaluations show speedups of up to 60 % and <b>memory</b> <b>savings</b> of up to 80 % compared to previous implementations...|$|R
40|$|The {{possibility}} of saving various computational resources {{is an argument}} often advanced in favor of permitting question-answering systems to make occasional errors. In this paper we establish absolute bounds {{on the amount of}} <b>memory</b> <b>savings</b> that is achievable with a specified error level for certain types of question-answering systems. Question-answering systems are treated as communication channels carrying information concerning the acceptable answers to an admissible set of queries. Shannon's rate-distortion theory is used to calculate bounds on the memory required for several question-answering tasks. For data retrieval, pattern-classification, and position-matching systems it was found that only small memory gains could be materialized from error-tolerance. In pair-ordering tasks on the other hand, more significant <b>memory</b> <b>savings</b> could be accomplished if small errorrates are tolerated. Similar limitations govern the tradeoffs between error and computation time. 1...|$|R
5000|$|Although not invented as a <b>memory</b> <b>saving</b> measure, the Apple II display design {{allowed a}} 6 color image to be {{displayed}} in only 8K of video memory {{despite the fact}} that colors would [...] "bleed" [...] (artifact) into each other if the pixels were incompatibly arranged.|$|E
50|$|In computing, decimal32 is a decimal floating-point {{computer}} {{numbering format}} that occupies 4 bytes (32 bits) in computer memory.It {{is intended for}} applications where {{it is necessary to}} emulate decimal rounding exactly, such as financial and tax computations. Like the binary16 format, it is intended for <b>memory</b> <b>saving</b> storage.|$|E
5000|$|The {{integrated}} MIDI sequencer allowed up {{to eight}} polyphonic tracks to play internal or MIDI sounds simultaneously. The sequencer memory could be shared with the user sound area, allowing 100 user [...] "Program" [...] sounds and 100 user [...] "Combination" [...] sounds with 4,400 sequencer notes or a reduced 50 Program and 50 Combination user sounds with 7700 notes. The sequencer's pattern structure permitted <b>memory</b> <b>saving</b> by using patterns for repetitive regions. Though paltry by current standards, the M1's sequencer offered full track editing and quantization, {{making it possible to}} produce high-quality songs entirely within the machine. The combination of the patches with the sequencer functionality led to the M1's near ubiquitous presence in late '80s and early '90s.|$|E
40|$|Abstract. We present work on unifying the {{two main}} data {{structures}} involved during reachability analysis of timed automata. We also present result on sharing common elements between states. The experimental evaluations show speedups of up to 60 % and <b>memory</b> <b>savings</b> of up to 80 % compared to previous implementations. ...|$|R
40|$|A novel {{integral}} {{formulation of}} the measured equation of invariance {{is derived from the}} reciprocity theorem. This formulation leads to a sparse matrix equation for the induced surface current, resulting in great CPU time and <b>memory</b> <b>savings</b> over the conventional approaches. The algorithm has been implemented for two-dimensional perfectly conducting scatterers. Peer ReviewedPostprint (published version...|$|R
30|$|The novel {{property}} of the DC method is that the exact evaluation of the diffusion terms is decoupled from the implicit treatment of the nonlinear terms. As a result, only a local nonlinear system needs to be solved at each spatial grid point. The numerical tests show that the method is advantageous in both CPU time and <b>memory</b> <b>savings.</b>|$|R
5000|$|With severe memory {{constraints}} and real time use, keeping {{only a single}} copy of code loaded into core was a requirement. Since the 1108 was designed for multitasking, the system was fully [...] "reentrant" [...] (thread safe). Each reentrant module accessed program data through a single memory [...] "base address", which was different for each instance of run data. Switching execution contexts {{could be done in}} a single instruction merely by setting a different base address in a single register. The system used fine-grained locking to protect shared data structures. The executive, compilers, utilities, and even sophisticated user applications that might have multiple copies running concurrently were written so that their code could be shared. This required loading only one copy into <b>memory,</b> <b>saving</b> both space and the time it took to load the code.|$|E
50|$|PE files {{normally}} {{do not contain}} position-independent code. Instead they are compiled to a preferred base address, and all addresses emitted by the compiler/linker are fixed ahead of time. If a PE file cannot be loaded at its preferred address (because it's already taken by something else), the operating system will rebase it. This involves recalculating every absolute address and modifying the code {{to use the new}} values. The loader does this by comparing the preferred and actual load addresses, and calculating a delta value. This is then added to the preferred address {{to come up with the}} new address of the memory location. Base relocations are stored in a list and added, as needed, to an existing memory location. The resulting code is now private to the process and no longer shareable, so many of the <b>memory</b> <b>saving</b> benefits of DLLs are lost in this scenario. It also slows down loading of the module significantly. For this reason rebasing is to be avoided wherever possible, and the DLLs shipped by Microsoft have base addresses pre-computed so as not to overlap. In the no rebase case PE therefore has the advantage of very efficient code, but in the presence of rebasing the memory usage hit can be expensive. This contrasts with ELF which uses fully position-independent code and a global offset table, which trades off execution time against memory usage in favor of the latter.|$|E
40|$|Abstract—We present new {{techniques}} for three-dimensional topography simulation of processes for which ballistic transport {{can be assumed}} at feature-scale. The combination of algorithms and data structures lent from the area of computer graphics allows a fast and <b>memory</b> <b>saving</b> solution of various deposition and etching processes. I...|$|E
50|$|If {{an object}} {{is known to}} be immutable, it can be copied simply by making a copy of a {{reference}} to it instead of copying the entire object. Because a reference (typically only the size of a pointer) is usually much smaller than the object itself, this results in <b>memory</b> <b>savings</b> and a potential boost in execution speed.|$|R
40|$|With {{the fast}} {{development}} of network technology, packet classification {{becomes more and}} more important in the new network applications. However, the packet classification has many technical difficulties, such as multidimensional match, many kinds of matching ways and so on. In the paper, we propose the Efficient Bit-Split Prefix Hierarchical Trie Packet Classification Algorithm (BSPHT). This structure is on the basis of hierarchical trie structure and bit split. By splitting the prefixes with the same prefixes into different sets, we can eliminate the backtracking problem in hierarchical trie structure. We also adopt some path compression strategy to make the <b>memory</b> <b>savings.</b> The experimental results show that, compared with classical and newest hierarchical trie algorithms, the evaluation results demonstrate that BSPHT algorithm provides a great improvement in searching performance with lots of <b>memory</b> <b>savings</b> {{in the case of the}} small rule sets and the big rule sets...|$|R
40|$|In a {{parallel}} processing computer system with multiple processing units and shared memory, a method is disclosed for uniformly balancing the aggregate computational load in, and utilizing minimal memory by, a network having identical computations {{to be executed}} at each connection therein. Read-only and read-write memory are subdivided into a plurality of process sets, which function like artificial processing units. Said plurality of process sets is iteratively merged and reduced {{to the number of}} processing units without exceeding the balance load. Said merger is based upon the value of a partition threshold, which {{is a measure of the}} memory utilization. The turnaround time and <b>memory</b> <b>savings</b> of the instant method are functions of the number of processing units available and the number of partitions into which the memory is subdivided. Typical results of the preferred embodiment yielded <b>memory</b> <b>savings</b> of from sixty to seventy five percent...|$|R
40|$|This paper {{presents}} a new decision diagram (DD), called MODD, for multiple output binary and multiple-valued functions. This DD is canonic {{and can be}} made minimal {{with respect to a}} given variable order. Unlike other reported DDs, our approach can represent arbitrary combination of bits at the word-level. The preliminary results show that our representation can result in considerable <b>memory</b> <b>saving</b> [1]. 1...|$|E
40|$|Compact {{algorithms}} are Estimation of Distribution Algorithms which {{mimic the}} behavior of population-based algorithms {{by means of a}} probabilistic representation of the population of candidate solutions. These algorithms have a similar behaviour with respect to population-based algorithms but require a much smaller memory. This feature is crucially important in some engineering applications, especially in robotics. A high performance compact algorithm is the compact Differential Evolution (cDE) algorithm. This paper proposes a novel implementation of cDE, namely compact Differential Evolution light (cDElight), to address not only the <b>memory</b> <b>saving</b> necessities but also real-time requirements. cDElight employs two novel algorithmic modifications for employing a smaller computational overhead without a performance loss, with respect to cDE. Numerical results, carried out on a broad set of test problems, show that cDElight, despite its minimal hardware requirements, does not deteriorate the performance of cDE and thus is competitive with other <b>memory</b> <b>saving</b> and population-based algorithms. An application in the field of mobile robotics highlights the usability and advantages of the proposed approach...|$|E
40|$|A fast, {{simple and}} <b>memory</b> <b>saving</b> {{algorithm}} for stereogram generation {{is presented in}} this paper. It is a ray-tracing like algorithm making use of the cross-talk effect in stereoscopic computer graphics for generating single-image random-dot stereogram. It actively looks for the smallest equivalent class of points with the same color so that it gives the greatest freedom of coloring for artistic design with stereogram. published_or_final_versio...|$|E
40|$|Nonlinear Model Predictive Control (NMPC) is an {{advanced}} control technique that often relies on computationally demanding optimization and integration algorithms. This paper proposes and investigates a heterogeneous hardware implementation of an NMPC controller {{based on an}} interior point algorithm. The proposed implementation provides flexibility of splitting the workload between a general-purpose CPU with a fixed architecture and a field-programmable gate array (FPGA) to trade off contradicting design objectives, namely performance and computational resource usage. A new way of exploiting {{the structure of the}} Karush-Kuhn-Tucker (KKT) matrix yields significant <b>memory</b> <b>savings,</b> which is crucial for reconfigurable hardware. For the considered case study, a 10 x <b>memory</b> <b>savings</b> compared to existing approaches and a 10 x speedup over a software implementation are reported. The proposed implementation can be tested from Matlab using a new release of the Protoip software tool, which is another contribution of the paper. Protoip abstracts many low-level details of heterogeneous hardware programming and allows quick prototyping and processor-in-the-loop verification of heterogeneous hardware implementations...|$|R
40|$|International audienceCloning a {{model is}} usually done by {{duplicating}} all its runtime objects {{into a new}} model. This approach leads to memory consumption problems for operations that create and manipulate large quantities of clones (e. g., design space exploration). We propose an original approach that exploits the fact that operations rarely modify a whole model. Given a set of immutable properties, our cloning approach determines the objects and fields that can be shared between the runtime representations of a model and its clones. Our generic cloning algorithm is parameterized with three strategies that establish a trade-off between <b>memory</b> <b>savings</b> and the ease of clone manipulation. We implemented the strategies within the Eclipse Modeling Framework (EMF) and evaluated memory footprints and computation overheads with 100 randomly generated metamodels and models. Results show {{a positive correlation between}} the proportion of shareable properties and <b>memory</b> <b>savings,</b> while the worst median overhead is 9, 5 % when manipulating the clones...|$|R
40|$|This paper {{describes}} and analyzes a probabilistic {{technique to}} reduce the memory requirement of the table of reached states maintained in verification by explicit state enumeration. The <b>memory</b> <b>savings</b> of the new scheme come {{at the price of}} a certain probability that the search becomes incomplete. However, this probability can be made negligibly small by using typically 40 bits of memory per state. From this point of view, this new scheme improves substantially on Holzmann's bitstate hashing, which has a high probability of producing an incomplete search even when using close to 1000 bits per state. The proposed scheme has been implemented in the contexts of the SPIN and Mur' verification systems. Experiments on sample protocols nicely match the predictions of the analysis. For large protocols, <b>memory</b> <b>savings</b> of two orders of magnitude are obtained. We also show how to efficiently combine the new scheme with state space caching, and we analyze bitstate hashing in order to compare it wit [...] ...|$|R
40|$|In this paper, {{we present}} a novel Disk Encompression (i. e., {{encryption}} with compression) with Tweaked Code Book mode (DETCB). DETCB is Xor-Encrypt-Xor based Tweaked Code Book mode with CipherText Stealing. The objective is to present an efficient disk encryption which is faster, <b>memory</b> <b>saving</b> and is better resistant to the attacks. The proposed design is characterized by its high throughput compared to the current solutions...|$|E
40|$|In this paper, a novel optimization-based {{stitching}} {{method is}} presented. It minimizes an energy function defined with derivatives {{up to the}} second order. We have identified some appropriate choices for its parameters, allowing it to reduce artifacts such as ghosting, color inconsistency, and misalignment. To accelerate the computation, a multi-resolution technique is introduced. The significant speedup and <b>memory</b> <b>saving</b> make it possible for use in hand-held capturing devices. © 2007 Optical Society of America. link_to_subscribed_fulltex...|$|E
40|$|This paper {{deals with}} the {{numerical}} solution of Fractional Differential Equations by means of m-step recursions. For the construction of such formulas, we consider a technique based on a rational approximation of the generating functions of Fractional Backward Differentiation Formulas (FBDFs). The so-defined methods simulate very well {{the properties of the}} underlying FBDFs with noticeable advantages in terms of <b>memory</b> <b>saving.</b> This fact becomes particularly evident when they are used for discretizing fractional partial differential equations like the ones occurring in some population dynamic models...|$|E
40|$|We {{investigate}} a streamlined method for compression, approximation and fast interpolation of gait analysis data using Catmull-Rom Splines. We are interested {{not only in}} raw compression, but also extracting the most useful data from an animation for subsequent manipulation. Our method allows compression approaching 85 percent while the resulting animation remains indistinguishable by humans from the original animation, resulting in significant <b>memory</b> <b>savings,</b> while the untransforme...|$|R
40|$|We {{propose a}} simple and {{effective}} heuristic to save memory in dynamic programming on tree decompositions when solving graph optimization problems. The introduced “anchor technique ” {{is based on a}} tree-like set covering problem. We substantiate our findings by experimental results. Our strategy has negligible computational overhead concerning running time but achieves <b>memory</b> <b>savings</b> for nice tree decompositions and path decompositions between 60 % and 98 %...|$|R
40|$|Abstract –Built-In Self-Test (BIST) {{provides}} {{an effective way}} to test configurable cores in System-on-Chip (SoC) implementations. We present a case study of the use of dynamic reconfiguration from an embedded processor core to implement BIST for the programmable logic and routing resources in configurable cores in commercially available SoCs. Experimental results from actual imple-mentations include speed-up and <b>memory</b> <b>savings</b> ob-tained and compared to traditional BIST approaches for configurable cores. ...|$|R
40|$|This article {{suggests}} {{a new approach}} to discrete Ray-Tracing in which a two step three-dimensional DDA and an octree are used. A verpy important problem regarding this kind of Ray Tracing, also known as Raster Ray Tracing, is the amount of memory required to store the 3 D raster grid which will contain the discretized scene to be visualized. Since the resolution of this grid is huge because the voxel is assumed to be approximately the size of a pixel on the screen, it is limited by the maximum amount of memory of todays´ machines. Although using case in most scenes, an important <b>memory</b> <b>saving</b> is achieved. This <b>memory</b> <b>saving</b> helps using discrete Ray Tracing in normal workstations. It is shown that the special octree presented here doesn´t slow down Ray Tracing significantly and even competes with normal discrete Ray-Tracing. It is also shown that in critical cases this octree will not occupy much more space than a normal 3 D grid. Another important problem is the bottleneck caused by the three-dimensional DDA in such huge resolution ones. The process can be divided in two steps where optimal times of three-dimensional DDA are achieved. This is shown through the comparative tests with the single step process...|$|E
40|$|Abstract—We {{present an}} {{alternative}} vector potential formu-lation of Maxwell’s equations derived upon {{introduction of a}} quantity related to the Hertz potential. Once space and time are discretized, within this formulation the electric field and vector potential components are condensed in the same point in the elementary cell. In three dimensions the formulation offers an alternative to finite-difference time-domain (FDTD) method; when reduced to a two-dimensional (2 -D) problem, only two variables, instead of three, are necessary, implying a net <b>memory</b> <b>saving</b> of 1 / 3 with respect to FDTD. I...|$|E
40|$|Due to {{enormous}} number of user and limited memory space, the <b>memory</b> <b>saving</b> is become an important issue for big data service these days. In the large scaled multiple-input multiple-output (MIMO) system, the Teoplitz channel can play the significance rule to improve the performance as well as power efficiency. In this paper, we propose a Toeplitz channel decomposition based on matrix vectorization. Here we use Toeplitz matrix to the channel for large scaled MIMO system. And we show that the Toeplitz Jacket matrices are decomposed to Cooley-Tukey sparse matrices like fast Fourier transform (FFT) ...|$|E
40|$|This paper investigates memory {{management}} for real-time multimedia applications {{running on a}} resource-constrained platform. It is shown how a shared memory pool can reduce the total memory requirements of an application comprised of a data-driven chain of tasks with a time-driven head and tail and a bounded end-to-end latency. The general technique targeted at memory-constrained streaming systems is demonstrated with a video encoding example, showing <b>memory</b> <b>savings</b> of about 19 %...|$|R
5000|$|In some {{applications}} it may {{be acceptable}} to retrieve a [...] "good guess" [...] of the nearest neighbor. In those cases, we can use an algorithm which doesn't guarantee to return the actual nearest neighbor in every case, in return for improved speed or <b>memory</b> <b>savings.</b> Often such an algorithm will find the nearest neighbor in a majority of cases, but this depends strongly on the dataset being queried.|$|R
40|$|Abstract—This paper investigates memory {{management}} for real-time multimedia applications {{running on a}} resource-constrained platform. It is shown how a shared memory pool can reduce the total memory requirements of an application comprised of a data-driven chain of tasks with a time-driven head and tail and a bounded end-to-end latency. The general technique targeted at memory-constrained streaming systems is demonstrated with a video encoding example, showing <b>memory</b> <b>savings</b> of about 19 %. I...|$|R
