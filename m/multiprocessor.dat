9399|2914|Public
5|$|A {{distributed}} computer (also {{known as}} a distributed memory <b>multiprocessor)</b> is a distributed memory computer {{system in which the}} processing elements are connected by a network. Distributed computers are highly scalable.|$|E
5|$|In {{order to}} run safely on <b>multiprocessor</b> machines, access to shared {{resources}} (like files, data structures) must be serialized so that threads or processes do {{not attempt to}} modify the same resource at the same time. In order to prevent multiple threads from accessing or modifying a shared resource simultaneously, DragonFly employs critical sections, and serializing tokens to prevent concurrent access. While both Linux and FreeBSD 5 employ fine-grained mutex models to achieve higher performance on <b>multiprocessor</b> systems, DragonFly does not. Until recently, DragonFly also employed spls, but these were replaced with critical sections.|$|E
5|$|DragonFly {{switched}} to <b>multiprocessor</b> safe slab allocator, which requires neither mutexes nor blocking operations for memory assignment tasks. It was eventually ported into standard C library in the userland, where it replaced FreeBSD's malloc implementation.|$|E
40|$|The {{choice of}} a good data {{distribution}} scheme is critical to performance of data-parallel applications on both distributed memory <b>multiprocessors</b> and NUMA shared memory <b>multiprocessors.</b> The high cost of interprocessor communication in distributed memory <b>multiprocessors</b> makes the minimization of communications the predominant issue in selecting data distribution schemes. However, on NUMA <b>multiprocessors</b> other issues such as contention, false sharing and cache affinity also affect performance. In this paper, we present empirical measurements which suggest that these issues cannot be ignored in selecting data distribution schemes. We conclude that existing methodologies used by application programmers and compilers to select data distribution schemes on distributed memory <b>multiprocessors</b> are not suitable for NUMA <b>multiprocessors...</b>|$|R
40|$|We {{report on}} {{experiments}} {{run on a}} set of shared-memory <b>multiprocessors.</b> Our goal was to demonstrate that one could conveniently utilize a set of shared-memory <b>multiprocessors</b> cooperatively working on typical state-space searches. We utilized a technology for writing portable code for <b>multiprocessors,</b> coded three depth-first state-space searches, and ran them {{on a set of}} <b>multiprocessors.</b> The final problem used substantial resources (over 65 hours on a single processor) and was successfully distributed over four distinct shared memory <b>multiprocessors</b> (2 Sequents and 2 Encores), reducing the time to perform the computation to slightly over 2 hours...|$|R
40|$|Abstract Small-scale <b>multiprocessors</b> are {{becoming}} increasinglyeconomical and common, whereas larger <b>multiprocessors</b> {{continue to have}} higher per-node costs. The NUMAchinemultiprocessor project seeks to make large-scale <b>multiprocessors</b> more economical while maintaining high perfor-mance by exploring architectural and hardware features for low-cost, modular <b>multiprocessors.</b> To demonstrate our ap-proach, we have implemented a prototype system that is scalable to 128 processors. An efficient directory-basedcache coherence protocol exploits our hierarchical ringbased interconnect and supports sequential consistency. This paper documents the design choices and the resulting performance of the system using both simulation results andmeasurements on the prototype hardware...|$|R
5|$|Parallel {{computers}} {{based on}} interconnected networks {{need to have}} some kind of routing to enable the passing of messages between nodes that are not directly connected. The medium used for communication between the processors is likely to be hierarchical in large <b>multiprocessor</b> machines.|$|E
5|$|A {{symmetric}} <b>multiprocessor</b> (SMP) is {{a computer}} system with multiple identical processors that share memory and connect via a bus. Bus contention prevents bus architectures from scaling. As a result, SMPs generally do not comprise more than 32processors. Because of {{the small size of}} the processors and the significant reduction in the requirements for bus bandwidth achieved by large caches, such symmetric multiprocessors are extremely cost-effective, provided that a sufficient amount of memory bandwidth exists.|$|E
5|$|Besides these distributions, {{there are}} some {{independent}} operating systems based on FreeBSD. DragonFly BSD is a fork from FreeBSD 4.8 aiming for a different <b>multiprocessor</b> synchronization strategy than the one chosen for FreeBSD 5 and development of some microkernel features. It does not aim to stay compatible with FreeBSD and has huge differences in the kernel and basic userland. MidnightBSD is a fork of FreeBSD 6.1 borrowing heavily from NeXTSTEP, particularly in the user interface department.|$|E
40|$|Current {{microprocessors}} exploit {{high levels}} of instruction-level parallelism (ILP) through techniques such as multiple issue, dynamic scheduling, and nonblocking reads. This paper presents the first {{detailed analysis of the}} impact of such processors on sharedmemory <b>multiprocessors</b> using a detailed executiondriven simulator. Using this analysis, we also examine the validity of common direct-execution simulation techniques that employ previous-generation processor models to approximate ILP-based <b>multiprocessors.</b> We find that ILP techniques substantially reduce CPU time in <b>multiprocessors,</b> but are less effective in reducing memory stall time. Consequently, despite the presence of inherent latency-tolerating techniques in ILP processors, memory stall time becomes a larger component of execution time and parallel efficiencies are generally poorer in ILP-based <b>multiprocessors</b> than in previous-generation <b>multiprocessors.</b> Examining the validity of direct-execution simulators with previous-gene [...] ...|$|R
40|$|Although {{much work}} has been done on parallelizing compilers for cache {{coherent}} shared memory <b>multiprocessors</b> and message-passing <b>multiprocessors,</b> there is relatively little research on parallelizing compilers for noncache coherent <b>multiprocessors</b> with global address space. In this paper, we present a preliminary study on automatic parallelization for the Cray T 3 D, a commercial scalable machine with a global memory space and noncoherent caches. ...|$|R
40|$|We {{argue that}} OS-provided data {{coherence}} on non-cache-coherent NUMA <b>multiprocessors</b> (machines with a single, global physical address space), can perform substantially better than distributed shared memory emulations on message-passing hardware, {{and almost as}} well as fully cache-coherent <b>multiprocessors.</b> 1 Introduction As a means of expressing parallel algorithms, shared memory programming models are widely believed to be easier to use than message-passing models. Small-scale, bus-based shared-memory <b>multiprocessors</b> are now ubiquitous, and several large-scale cache-coherent <b>multiprocessors</b> have been designed in recent years. Unfortunately, large machines are substantially more difficult to build than small ones, because snooping does not scale. It appears likely that large-scale cache coherence will have high per-processor costs (relative to workstations and small-scale <b>multiprocessors)</b> for the foreseeable future. Many researchers have therefore undertaken to harness the parallel proce [...] ...|$|R
5|$|In DragonFly, each CPU {{has its own}} thread scheduler. Upon creation, threads are {{assigned}} to processors and are never preemptively switched from one processor to another; they are only migrated by the passing of an inter-processor interrupt (IPI) message between the CPUs involved. Inter-processor thread scheduling is also accomplished by sending asynchronous IPI messages. One advantage to this clean compartmentalization of the threading subsystem is that the processors' on-board caches in Symmetric <b>Multiprocessor</b> Systems do not contain duplicated data, allowing for higher performance by giving each processor in the system {{the ability to use}} its own cache to store different things to work on.|$|E
25|$|In 1982, Gunther joined Xerox PARC {{to develop}} {{parametric}} and functional test software for PARC's small-scale VLSI design fabrication line. Ultimately, he was recruited onto the Dragon <b>multiprocessor</b> workstation project {{where he also}} developed the PARCbench <b>multiprocessor</b> benchmark. This was his first fore into computer performance analysis.|$|E
25|$|April 2007, ARC {{acquired}} Teja Technologies of San Jose, California, {{a specialist}} in heterogeneous <b>multiprocessor</b> software.|$|E
40|$|Restricted {{migration}} of periodic and sporadic tasks on uniform <b>multiprocessors</b> is considered. On <b>multiprocessors,</b> job migration {{can cause a}} prohibitively high overhead — particularly in real-time systems where accurate timing is essential. However, if periodic tasks do not maintain state between separate invocations, different jobs of the same task may be allowed to execute on different processors — i. e., restricted migration may be permitted. On uniform <b>multiprocessors,</b> each processor has an associated speed. A job executing on a processor of speed s for t units of time will perform s×t units of work. A utilization-based test for restricted migration on uniform <b>multiprocessors</b> is provided where each processor schedules jobs using the Earliest Deadline First (EDF) scheduling algorithm. hard real-time systems, periodic tasks, earliest deadline first, uniform <b>multiprocessors,</b> mi...|$|R
40|$|The invention, acceptance, and {{proliferation}} of <b>multiprocessors</b> are primarily {{a result of}} the quest to increase computer system performance. The most promising features of <b>multiprocessors</b> are their potential to solve problems faster than previously possible and to solve larger problems than previously possible. Large-scale <b>multiprocessors</b> offer the additional advantage of being able to execute multiple parallel applications simultaneously. The execution time of a parallel application {{is directly related to the}} number of processors it is allocated and, in shared-memory non-uniform memory access time (NUMA) <b>multiprocessors,</b> which processors it is allocated. As a result, efficient and effective scheduling becomes critical to overall system performance. In fact, it is likely to be a contributing factor in ultimately determining the success or failure of shared-memory NUMA <b>multiprocessors.</b> The subjects of this dissertation are the problems of processor allocation and application placement. [...] ...|$|R
50|$|In Flynn's taxonomy, <b>multiprocessors</b> {{as defined}} above are MIMD machines. As they are {{normally}} construed to be tightly coupled (share memory), <b>multiprocessors</b> are not the entire class of MIMD machines, which also contains message passing multicomputer systems.|$|R
25|$|From Glasgow University, {{supports}} {{clusters of}} machines or single multiprocessors. Also within Haskell is support for Symmetric <b>Multiprocessor</b> parallelism.|$|E
25|$|Native {{operating}} system support for PPM on <b>multiprocessor</b> systems, including systems using processors with multiple logical threads, multiple cores, or multiple physical sockets.|$|E
25|$|Realtime {{rendering}} of effects & transitions in DV files, including enhancements to DV rendering, <b>multiprocessor</b> support, and Altivec enhancements for PowerPC G4 systems.|$|E
40|$|In {{this paper}} we {{identify}} {{the factors that}} affect the derivation of computation and data partitions on scalable shared memory <b>multiprocessors</b> (SSMMs). We show that these factors necessitate an SSMM-conscious approach. In addition to remote memory access, which is the sole factor on distributed memory <b>multiprocessors,</b> cache affinity, memory contention and false sharing are important factors that must be considered. Experimental evidence is presented to demonstrate {{the impact of these}} factors on performance using three applications on the KSR 1 and the Hector <b>multiprocessors.</b> 1 Introduction Scalable shared memory <b>multiprocessors</b> (SSMMs) are becoming increasingly popular and a viable alternative to distributed memory <b>multiprocessors</b> (DMMs). The Stanford DASH [20], FLASH [14], the KSR 1 [24], Toronto's Hector [26], NUMAchine [1], and the Cray T 3 D [23] are some SSMMs currently in use or under development. Processors in a SSMM share a single coherent address space. However, shared memory is p [...] ...|$|R
50|$|These {{systems were}} {{organized}} quite {{differently from the}} other <b>multiprocessors</b> in this article. The operating system ran on the peripheral processors, while the user's application ran on the CPUs. Thus, the terms ASMP and SMP do not properly apply to these <b>multiprocessors.</b>|$|R
40|$|In {{this paper}} we {{identify}} {{the factors that}} affect the derivation of computation and data partitions on scalable shared memory <b>multiprocessors</b> (SSMMs). We show that these factors necessitate an SSMM-conscious approach. In addition to remote memory access, which is the sole factor on distributed memory <b>multiprocessors,</b> cache affinity, memory contention and false sharing are important factors that must be considered. Experimental evidence is presented to demonstrate {{the impact of these}} factors on performance using three applications on the KSR 1 and the Hector <b>multiprocessors...</b>|$|R
25|$|Theorem (Gunther 2002): Amdahl's law for {{parallel}} speedup {{is equivalent}} to the synchronous queueing bound on throughput in a Machine Repairman model of a <b>multiprocessor.</b>|$|E
25|$|A {{model that}} {{is closer to}} the {{behavior}} of real-world <b>multiprocessor</b> machines and takes into account the use of machine instructions, such as Compare-and-swap (CAS), is that of asynchronous shared memory. There is a wide body of work on this model, a summary of which {{can be found in the}} literature.|$|E
25|$|The {{situation}} is {{further complicated by}} the traditional uses of the terms parallel and distributed algorithm that do not quite match the above definitions of parallel and distributed systems (see below for more detailed discussion). Nevertheless, {{as a rule of}} thumb, high-performance parallel computation in a shared-memory <b>multiprocessor</b> uses parallel algorithms while the coordination of a large-scale distributed system uses distributed algorithms.|$|E
5000|$|Algorithms for {{scalable}} synchronization on shared-memory <b>multiprocessors</b> ...|$|R
30|$|Out-of-order {{execution}} and speculation result in processor models {{that are too}} complex for WCET analysis. We discuss that the transistors are better used onchip <b>multiprocessors</b> (CMPs) with simple in-order pipelines. Real-time systems are naturally multithreaded and thus map well to the explicit parallelism of chip <b>multiprocessors.</b>|$|R
40|$|The {{question}} of whether <b>multiprocessors</b> should have shared or distributed memory has attracted {{a great deal of}} attention. Some researchers argue strongly for building distributed memory machines, while others argue just as strongly for programming shared memory <b>multiprocessors.</b> A great deal of research is underway on both types of parallel systems. Special emphasis is placed on systems with {{a very large number of}} processors for computation intensive tasks and considers research and implementation trends. It appears that the two types of systems will likely converge to a common form for large scale <b>multiprocessors...</b>|$|R
25|$|Bjarne Stroustrup {{originally}} observed, in {{his book}} The Design and Evolution of C++, that pointer placement new is necessary for hardware that expects a certain object at a specific hardware address. It is also required {{for the construction of}} objects that need to reside in a certain memory area, such as an area that is shared between several processors of a <b>multiprocessor</b> computer.|$|E
25|$|In ALGOL 68S(S) from Carnegie Mellon University {{the power}} of {{parallel}} processing was improved by adding an orthogonal extension, eventing. Any variable declaration containing keyword event made assignments to this variable eligible for parallel evaluation, i.e. the right hand side {{was made into a}} procedure which was moved to one of the processors of the C.mmp <b>multiprocessor</b> system. Accesses to such variables were delayed after termination of the assignment.|$|E
25|$|The Alpha 21164 or EV5 became {{available}} in 1995 at processor frequencies {{of up to}} 333MHz. In July 1996 the line was speed bumped to 500MHz, in March 1998 to 666MHz. Also in 1998 the Alpha 21264 (EV6) was released at 450MHz, eventually reaching (in 2001 with the 21264C/EV68CB) 1.25GHz. In 2003, the Alpha 21364 or EV7 Marvel was launched, essentially an EV68 core with four 1.6 GB/s inter-processor communication links for improved <b>multiprocessor</b> system performance, running at 1 or 1.15GHz.|$|E
40|$|Abstract Scheduling {{real-time}} {{systems on}} <b>multiprocessors</b> introduces complexities {{that do not}} arise when using uniprocessors. If the processors do not all operate at the same speed, scheduling becomes even more complex. When scheduling using EDF on <b>multiprocessors,</b> breaking deadline ties in different ways can change the resulting schedule dramatically. We consider methods to resolve the ambiguities in EDF priorities due to coincident deadlines. We show that no optimal ambiguity resolver can be both on-line and priority-driven. When processor speeds differ, a job’s minimum required execution rate may also {{be considered in the}} scheduling decisions. We propose a modification of existing scheduling algorithms that considers processor speed and minimum required execution rate. We show that the resulting algorithm dominates EDF on identical <b>multiprocessors</b> and conjecture that it dominates EDF on uniform <b>multiprocessors</b> as well. ...|$|R
40|$|This paper compares eight {{commercial}} parallel processors along several di-mensions. The processors include four shared-bus <b>multiprocessors</b> (the Encore Multimax, the Sequent Balance system, the Alliant FX series, and the ELXSI Sys-tem 6400) {{and four}} network <b>multiprocessors</b> (the BBN Butterfly, the NCUBE, the Intel iPSC/ 2, and the FPS T Series). The paper contrasts the computers {{from the standpoint}} of interconnect ion structures, memory configurations, and interprocessor communication. Also, the shared-bus <b>multiprocessors</b> are com-pared in terms of cache-coherence strategies, and the network <b>multiprocessors</b> are compared in terms of node structure. Where possible, price and performance in-formation has been included. The reader is cautioned that this survey is based largely on information submitted by manufacturers; the authors have not per-formed any independent evaluation. Disc ln lmer This report is condensed from a clas...|$|R
40|$|Scheduling {{real-time}} {{systems on}} <b>multiprocessors</b> introduces complexities {{that do not}} arise when using uniproccssors. If the processors do not all operate at the same speed, scheduling becomes even more complex. When scheduling using [DF on <b>multiprocessors,</b> breaking deadline ties in different ways can change the resulting schedule dramatically. We consider methods to resolve the ambiguities in [DF pri- orities due to coincident deadlines. We show that no optimal ambiguity resolver can be both on-line and priority-driven. When processor speeds differ, a job's minimum required execution rate may also {{be considered in the}} scheduling decisions. We propose a modification of existing scheduling algorithms that considers processor speed and minimum required execution rate. We show that the resulting algorithm dominates [DF on identical <b>multiprocessors</b> and conjecture that it dominates [DF on uniform <b>multiprocessors</b> as well. ...|$|R
