9799|278|Public
5|$|Gyeongju has {{two main}} local {{newspapers}}; the Gyeongju Sinmun and the Seorabeol Sinmun. Both are weekly newspapers providing news via online {{as well and}} their headquarters {{are located in the}} neighborhood of Dongcheon-dong. The Gyeongju Sinmun was founded in 1989 and provides various news and critics on anything concerning Gyeongju. Its online newspaper, Digital Gyeongju Sinmun opened in December, 2000 to provide live local news out of the limit as a weekly newspaper and to establish <b>mutual</b> <b>information</b> exchanges from Gyeongju locals. In 2001, Gyeongju Sinmun started to present Gyeongju Citizen Awards to people who try to develop the local industry and economy, culture and education, and welfare service. Since 2003, the Wolseong Nuclear Power Plant headquarter co-hosts the awards with Gyeongju Sinmun.|$|E
25|$|<b>Mutual</b> <b>information</b> {{is closely}} related to the log-likelihood ratio test in the context of {{contingency}} tables and the multinomial distribution and to Pearson's χ2 test: <b>mutual</b> <b>information</b> can be considered a statistic for assessing independence between a pair of variables, and has a well-specified asymptotic distribution.|$|E
25|$|Other {{important}} information theoretic quantities include Rényi entropy (a generalization of entropy), differential entropy (a generalization of quantities {{of information to}} continuous distributions), and the conditional <b>mutual</b> <b>information.</b>|$|E
25|$|These {{statements}} run {{parallel to}} classical intuition, except that quantum conditional entropies can be negative, and quantum <b>mutual</b> <b>informations</b> can exceed the classical bound of the marginal entropy.|$|R
40|$|We {{conjecture}} new uncertainty relations which restrict {{correlations between}} {{results of measurements}} performed by two separated parties on a shared quantum state. The first uncertainty relation bounds the sum of two <b>mutual</b> <b>informations</b> when one party measures a single observable and the other party measures one of two observables. The uncertainty relation does not follow from Maassen-Uffink uncertainty relation and is much stronger than Hall uncertainty relation derived from the latter. The second uncertainty relation bounds the sum of two <b>mutual</b> <b>informations</b> when each party measures one of two observables. We provide numerical evidence for validity of conjectured uncertainty relations and prove them for large classes of states and observables. Comment: 7 page...|$|R
5000|$|... #Caption: Venn {{diagram of}} {{information}} theoretic measures for three variables x, y, and z, {{represented by the}} lower left, lower right, and upper circles, respectively. The conditional <b>mutual</b> <b>informations</b> , [...] and [...] are represented by the yellow, cyan, and magenta regions, respectively.|$|R
25|$|It {{states that}} the {{shortest}} program that reproduces X and Y {{is no more than}} a logarithmic term larger than a program to reproduce X and a program to reproduce Y given X. Using this statement, one can define an analogue of <b>mutual</b> <b>information</b> for Kolmogorov complexity.|$|E
25|$|An {{alternative}} method of structural learning uses optimization based search. It requires a scoring function and a search strategy. A common scoring function is posterior probability {{of the structure}} given the training data, like the BIC or the BDeu. The time requirement of an exhaustive search returning a structure that maximizes the score is superexponential {{in the number of}} variables. A local search strategy makes incremental changes aimed at improving the score of the structure. A global search algorithm like Markov chain Monte Carlo can avoid getting trapped in local minima. Friedman et al. discuss using <b>mutual</b> <b>information</b> between variables and finding a structure that maximizes this. They do this by restricting the parent candidate set to k nodes and exhaustively searching therein.|$|E
25|$|If {{ecosystems}} are governed primarily by stochastic processes, through which its subsequent {{state would be}} determined by both predictable and random actions, {{they may be more}} resilient to sudden change than each species individually. In the absence of a balance of nature, the species composition of ecosystems would undergo shifts that would depend {{on the nature of the}} change, but entire ecological collapse would probably be infrequent events. In 1997, Robert Ulanowicz used information theory tools to describe the structure of ecosystems, emphasizing <b>mutual</b> <b>information</b> (correlations) in studied systems. Drawing on this methodology and prior observations of complex ecosystems, Ulanowicz depicts approaches to determining the stress levels on ecosystems and predicting system reactions to defined types of alteration in their settings (such as increased or reduced energy flow, and eutrophication.|$|E
5000|$|... #Caption: Venn {{diagram of}} {{information}} theoretic measures for three variables x, y, and z. The dual total correlation {{is represented by}} the union of the three <b>mutual</b> <b>informations</b> and is shown in the diagram by the yellow, magenta, cyan, and gray regions.|$|R
40|$|Within border regions {{generally}} exists need of <b>mutual</b> <b>informations</b> and knowledge. This aim is fulfilled by project researching in {{the frame}} of CBC Phare subject of the territory of Czech-German borderland. Bi-lingual publication presents an output of the project that describes current socio-economic situation based on statistics figures mainly stemming from district units...|$|R
40|$|A {{study of}} linear {{dependence}} relations among Shannon's and McGill's (multiple) <b>mutual</b> <b>informations</b> {{as well as}} the lattice—theoretic description of them is presented. The concept of entropy vector is defined as a functional on the set of admissible probability distributions by which the problem can be transformed to that of investigating the algebraic structure of the (correlative) entropy space. First, we give the bases of this space as well as several basis transformations and thus determine the dimension. Next, a set of admissible values which McGill's informations can simultaneously take is obtained by establishing the elementary distributions, where, as a by-product, certain nonlinear dependence property is clarified. Finally, considerations on the duality of the lattice of random variables lead to the concept of dual (multiple) <b>mutual</b> <b>informations,</b> from which the duals of several theorems are derived...|$|R
25|$|Head motion {{correction}} {{is another}} common preprocessing step. When the head moves, the neurons under a voxel move and hence its timecourse now represents largely that {{of some other}} voxel in the past. Hence the timecourse curve is effectively cut and pasted from one voxel to another. Motion correction tries different ways of undoing this to see which undoing of the cut-and-paste produces the smoothest timecourse for all voxels. The undoing is by applying a rigid-body transform to the volume, by shifting and rotating the whole volume data to account for motion. The transformed volume is compared statistically to the volume at the first timepoint {{to see how well}} they match, using a cost function such as correlation or <b>mutual</b> <b>information.</b> The transformation that gives the minimal cost function is chosen as the model for head motion. Since the head can move in a vastly varied number of ways, {{it is not possible to}} search for all possible candidates; nor is there right now an algorithm that provides a globally optimal solution independent of the first transformations we try in a chain.|$|E
2500|$|... where [...] (Specific <b>mutual</b> <b>Information)</b> is the pointwise <b>mutual</b> <b>information.</b>|$|E
2500|$|<b>Mutual</b> <b>information</b> {{measures}} {{the amount of}} information that can be obtained about one random variable by observing another. [...] It is important in communication where {{it can be used to}} maximize {{the amount of information}} shared between sent and received signals. [...] The <b>mutual</b> <b>information</b> of [...] relative to [...] is given by: ...|$|E
40|$|The leading {{term for}} the <b>mutual</b> Rényi <b>information</b> is studied for two widely {{separated}} identical compound systems for free scalar fields in (d+ 1) Euclidean space. The compound system consists of two identical spheres in contact, with a result consistent with a universal form for the leading {{term for the}} <b>mutual</b> Rényi <b>information.</b> Comment: 10 pages, typos correcte...|$|R
40|$|We prove a coding theorem for bit-interleaved coded modulation. We {{show that}} the {{residual}} correlation among the parallel sub-channels induced by the demapper does not reduce the information rate. The total rate is given by {{the sum of the}} <b>mutual</b> <b>informations</b> per bit I(Bj; Y). This implies that an interleaver is not required to remove the correlation. ...|$|R
5000|$|There also exists an infinite-dimensional {{algebraic}} {{generalization of}} Choi's theorem, known as [...] "Belavkin's Radon-Nikodym theorem for completely positive maps", which defines a density operator as a [...] "Radon-Nikodym derivative" [...] of a quantum channel {{with respect to}} a dominating completely positive map (reference channel). It is used for defining the relative fidelities and <b>mutual</b> <b>informations</b> for quantum channels.|$|R
2500|$|This {{can also}} be restated in terms of quantum <b>mutual</b> <b>information,</b> ...|$|E
2500|$|... {{rank the}} [...] {{features}} {{according to their}} <b>mutual</b> <b>information</b> with the class labels; ...|$|E
2500|$|... Short {{introduction}} to the axioms of information theory, entropy, <b>mutual</b> <b>information,</b> Kullback–Liebler divergence, and Jensen–Shannon distance.|$|E
40|$|This paper formulates {{the power}} {{allocation}} policy that maximizes {{the region of}} <b>mutual</b> <b>informations</b> achievable in multiuser downlink OFDM channels. Arbitrary partitioning of the available tones among users and arbitrary modulation formats, possibly different for every user, are considered. The policy, derived for slowly fading channels tracked by the base station, adopts {{the form of a}} multiuser mercury/waterfilling procedure that generalizes the single-user mercury/waterfilling introduced in [1]...|$|R
30|$|The EXIT {{curve of}} the VND is given by the {{transfer}} characteristic between IE,VND=I(E(vn);U) and IA,VND=I(A(vn);U). Note that the realizations of RVs E(vn) and A(vn) are the messages exchanged in the sum-product algorithm, {ri,k} and {qk,i}, respectively. In order to evaluate these <b>mutual</b> <b>informations</b> from (5), the conditional PDF of the a priori A(vn) and the extrinsic E(vn) at a variable node decoder, given U, have to be found.|$|R
40|$|Abstract — This paper formulates {{the power}} {{allocation}} policy that maximizes {{the region of}} <b>mutual</b> <b>informations</b> achievable in multiuser downlink OFDM channels. Arbitrary partitioning of the available tones among users and arbitrary modulation formats, possibly different for every user, are considered. The policy, derived for slowly fading channels tracked by the base station, adopts {{the form of a}} multiuser mercury/waterfilling procedure that generalizes the single-user mercury/waterfilling introduced in [1]. I...|$|R
2500|$|... the <b>mutual</b> <b>information,</b> and {{the channel}} {{capacity}} of a noisy channel, including the promise of perfect loss-free communication given by the noisy-channel coding theorem; ...|$|E
2500|$|<b>Mutual</b> <b>information</b> can be {{expressed}} as the average Kullback–Leibler divergence (information gain) between the posterior probability distribution of X given the value of Y and the prior distribution on X: ...|$|E
2500|$|If the {{transmitter}} has only statistical channel state information, then the ergodic channel capacity will decrease as the signal covariance [...] {{can only be}} optimized {{in terms of the}} average <b>mutual</b> <b>information</b> as ...|$|E
40|$|McGill's {{multiple}} <b>mutual</b> <b>informations</b> {{are useful}} to systematically describe multiple interactions of frequency data with general n-way. The asymptotic behaviour {{of the maximum}} likelihood estimators of them is analysed in terms of mutually independent X 2 -distributions. On {{the basis of the}} results, the concept of semi-independence is introduced as a finer one of the concept of independence in the ordinary sense, and is used to interpret various multiple interactions...|$|R
40|$|When fitting {{a mixture}} of Gaussians to {{training}} data there are usually two choices {{for the type of}} Gaussians used. Either diagonal or full covariance. Imposing a structure, though may be restrictive and lead to degraded performance and/or increased computations. In this work I sparsify the regression matrix of each Gaussian and experiment with two di#erent structure-finding techniques; the di#erence of <b>mutual</b> <b>informations</b> and structural EM. I evaluated the approach in the 1996 NIST speaker recognition task...|$|R
3000|$|... is {{the total}} {{transmit}} power. H_ba∈CN(0, 1) and H_ea∈CN(0, 1 /m) represent the channel to the users and eavesdroppers, respectively. Here, m=σ _ea^ 2 /σ _ba^ 2 represents the gain ratio between the main and wire-tap channels. The secrecy capacity {{is defined as the}} maximization of the difference between two <b>mutual</b> <b>informations.</b> However, the channels are usually not perfectly known in reality. This situation is known as the imperfect channel state information (CSI) case in [7], which we will address in our studies.|$|R
2500|$|In Fisher's (1987) {{description}} of COBWEB, the measure {{he uses to}} evaluate {{the quality of the}} hierarchy is Gluck and Corter's (1985) category utility (CU) measure, which he re-derives in his paper. [...] The motivation for the measure is highly similar to the [...] "information gain" [...] measure introduced by Quinlan for decision tree learning. [...] It has previously been shown that the CU for feature-based classification [...] {{is the same as the}} <b>mutual</b> <b>information</b> [...] between the feature variables and the class variable (Gluck & Corter, 1985; Corter & Gluck, 1992), and since this measure is much better known, we proceed here with <b>mutual</b> <b>information</b> as the measure of category [...] "goodness".|$|E
2500|$|Free energy minimisation is {{equivalent}} to maximising the <b>mutual</b> <b>information</b> between sensory states and internal states that parameterise the variational density (for a fixed entropy variational density). This relates free energy minimization {{to the principle of}} minimum redundancy [...] and related treatments using information theory to describe optimal behaviour.|$|E
2500|$|Information theoretic {{security}} {{refers to}} {{methods such as}} the one-time pad that are not vulnerable to such brute force attacks. [...] In such cases, the positive conditional <b>mutual</b> <b>information</b> between the plaintext and ciphertext (conditioned on the key) can ensure proper transmission, while the unconditional <b>mutual</b> <b>information</b> between the plaintext and ciphertext remains zero, resulting in absolutely secure communications. [...] In other words, an eavesdropper {{would not be able}} to improve his or her guess of the plaintext by gaining knowledge of the ciphertext but not of the key. However, as in any other cryptographic system, care must be used to correctly apply even information-theoretically secure methods; the Venona project was able to crack the one-time pads of the Soviet Union due to their improper reuse of key material.|$|E
40|$|Abstract—We {{describe}} an information-theoretic {{approach to the}} analysis of music and other sequential data, which emphasises the predictive aspects of perception, and the dynamic process of forming and modifying expectations about an unfolding stream of data, characterising these using the tools of <b>information</b> theory: entropies, <b>mutual</b> <b>informations,</b> and related quantities. After reviewing the theoretical foundations, we discuss a few emerging areas of application, including musicological analysis, real-time beat-tracking analysis, and the generation of musical materials as a cognitively-informed compositional aid. I...|$|R
40|$|Generalizing the quantifiers used to {{classify}} correlations in bipartite systems, we define genuine total, quantum, and classical correlations in multipartite systems. The measure we give {{is based on}} the use of relative entropy to quantify the "distance" between two density matrices. Moreover, we show that, for pure states of three qubits, both quantum and classical bipartite correlations obey a ladder ordering law fixed by two-body <b>mutual</b> <b>informations,</b> or, equivalently, by one-qubit entropies. Comment: Accepted for publication in Phys. Rev. Let...|$|R
40|$|We {{describe}} an information-theoretic {{approach to the}} analysis of music and other sequential data, which emphasises the predictive aspects of perception, and the dynamic process of forming and modifying expectations about an unfolding stream of data, characterising these using the tools of <b>information</b> theory: entropies, <b>mutual</b> <b>informations,</b> and related quantities. After reviewing the theoretical foundations, we discuss a few emerging areas of application, including musicological analysis, real-time beat-tracking analysis, and the generation of musical materials as a cognitively-informed compositional aid. © 2012 IEEE...|$|R
