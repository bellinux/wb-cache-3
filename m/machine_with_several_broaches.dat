0|10000|Public
50|$|Genera {{supports}} one-processor <b>machines</b> <b>with</b> <b>several</b> threads (called processes).|$|R
5000|$|... 1987 Der Lauf der Dinge (The Way Things Go), 16 mm, 30 minutes, color, sound. The {{camera follows}} {{the course of}} the Rube Goldberg <b>Machine</b> <b>with</b> <b>several</b> cuts ...|$|R
50|$|Commercial establishments {{generally}} use semi-automatic <b>machines</b> <b>with</b> <b>several</b> group heads. These {{are much}} larger than consumer models and able to produce espresso shots more quickly. Many commercial machines can function in an automatic mode.|$|R
50|$|A multi-tape Turing {{machine is}} like an {{ordinary}} Turing <b>machine</b> <b>with</b> <b>several</b> tapes. Each tape has its own head for reading and writing. Initially the input appears on tape 1, and the others start out blank.|$|R
40|$|Photograph of the {{interior}} of the Northrop Corporation's Hawthorne plant, where the Snark Missile was being constructed, ca. 1950. A man in the foreground stands reading a text printout from a tall <b>machine</b> <b>with</b> <b>several</b> meters on it. A wall of machinery leads to several other men further down, engaged in different tasks...|$|R
40|$|The {{scalable}} {{implementation of}} multigrid methods for <b>machines</b> <b>with</b> <b>several</b> thousands of processors is investigated. Parallel performance models are presented for three different structured-grid multigrid algorithms, and a description is given {{of how these}} models {{can be used to}} guide implementation. Potential pitfalls are illustrated when moving from moderate-sized parallelism to large-scale parallelism, and results are given from existing multigrid codes to support the discussion. Finally, the use of mixed programming models is investigated for multigrid codes on clusters of SMPs...|$|R
50|$|YAP {{is an open}} source, {{high-performance}} {{implementation of}} the Prolog programming language developed at LIACC/Universidade do Porto and at COPPE Sistemas/UFRJ. Its Prolog engine is based in the WAM (Warren Abstract <b>Machine),</b> <b>with</b> <b>several</b> optimizations for better performance. YAP follows the Edinburgh tradition, and is largely compatible with the ISO-Prolog standard and with Quintus Prolog and SICStus Prolog. YAP has been developed since 1985. The original version was written in assembly, C and Prolog, and achieved high performance on m68k-based machines.|$|R
5000|$|The two go {{to visit}} Jim's ex-girlfriend, {{but when her}} current boyfriend shows up, a fight ensues in which the boyfriend is backed up {{by a group of}} friends. Jim is able to get the upper hand and when Mike {{produces}} a gun, they subdue the men and rob them of their possessions, including marijuana and a handgun which they later decide to sell. Jim later leaves messages on Mike's answering <b>machine</b> <b>with</b> <b>several</b> different voices, pretending to be companies responding to his resume.|$|R
50|$|Because {{it is the}} {{combination}} of two different computers it is very versatile. It can be a laboratory oriented <b>machine</b> <b>with</b> <b>several</b> facilities for I/O, auxiliary storage, and control and sensing for external equipment or a general purpose computer with a flexible I/O capability that can support multiple peripheral devices. The basic package came with dual LINCtape drives, a scope display and I/O ports for interfacing with external laboratory equipment and peripherals. In addition to a display-based OS other software packages were included for data acquisition and display, Fourier analysis and mass spectrometry.|$|R
40|$|Multilevel block {{algorithms}} {{exploit the}} data locality in linear algebra operations when executed in <b>machines</b> <b>with</b> <b>several</b> {{levels in the}} memory hierarchy. It is shown that the family we call Multilevel Orthogonal Block (MOB) algorithms is optimal and easy to design and that using the multilevel approach produces significant performance improvements. The effect of interference in the cache, of the TLB misses, and of page faults are also considered. The multilevel block algorithms are evaluated analytically for an ideal memory system with M cache levels without interferences. Moreover, experimental results of the MOB forms in some present high performance workstations are presented...|$|R
40|$|We give an {{efficient}} procedure for verifying that a finite-state concurrent system meets a specification {{expressed in a}} (propositional, branching-time) temporal logic. Our algorithm has complexity linear in both {{the size of the}} specification {{and the size of the}} global state graph for the concurrent system. We also show how this approach can be adapted to handle fairness. We argue that our technique can provide a practical alternative to manual proof construction or use of a mechanical theorem prover for verifying many finite-state concurrent systems. Experimental results show that state <b>machines</b> <b>with</b> <b>several</b> hundred states can be checked in a matter of seconds...|$|R
40|$|Abstract. Much {{work has}} been done in verifying a {{compiler}} specification, both in hand-written and mechanical proofs. However, there is still a gap between a correct compiler specification and a correct compiler implementation. To fill this gap and obtain a correct compiler implementation, we take the approach of generating a compiler from its specification. We verified the correctness of a compiler specification with the theorem prover Isabelle/HOL, and generated a Standard ML code corresponding to the specification with Isabelle’s code generation facility. The generated compiler can be executed with some hand-written codes, and it compiles a small functional programming language into the Java virtual <b>machine</b> <b>with</b> <b>several</b> program transformations. ...|$|R
5000|$|The Sierra Vista Aquatic Center, aka [...] "The Cove," [...] is a 36500 sq ft {{facility}} that contains 11347 sq ft of pool water surface, which equates to over 575000 gal of water. The Aquatic Center boasts a 0' depth, or [...] "beach", entry, eight 25-yard lap lanes for lap and competitive swimming, and a wave <b>machine</b> <b>with</b> <b>several</b> wave patterns for Open Swim. The Cove has a submersible bulkhead {{which makes the}} switch from wave pool to competitive pool possible. The Cove also has a warm water therapy pool, children's lagoon with slide, two 1 meter diving boards and one 3 meter diving board, and two enclosed water tube slides to include a 150 ft water tube slide.|$|R
40|$|AbstractARM-based CNC {{systems are}} used as a {{low-cost}} solution for controlling 2 - 3 axis machines. The increase of computing power of ARM processors facilitated their application to controlling multi-tasking and multi-axis <b>machines</b> <b>with</b> <b>several</b> independent control channels. The paper {{presents the results of}} the exploratory investigation of the transition from the PC-based to the ARM-based CNC solution. The cross-platform architecture of CNC systems, the porting of CNC kernel software to the single-board computers Raspberry Pi 2 with the Linux operating system and the control of servo drives and PLC I/O over EtherCAT fieldbus with cycle time of 2 ms are investigated as well. The dual-channel configuration of ARM-based CNC designed to control the multi-tasking turning and milling <b>machine</b> <b>with</b> inclined layout is presented. The paper illustrates the kinematic scheme and the network architecture of NAKLON 535 machining center for ARM-based CNC solution...|$|R
40|$|Investigation of hard {{machining}} {{is one of}} the major trends of cutting procedures. According to technical and technological development, hardened steels can be <b>machined</b> today <b>with</b> <b>several</b> processes referring to the accuracy and quality requirements of the precision parts. The paper compares the applicable procedures taking into account some economic aspects...|$|R
40|$|The current {{trend of}} {{aggressive}} dynamic scheduling in superscalar processors is reaching {{a point of}} diminishing returns. This calls for an architecture that utilizes the processing transistors better, thereby freeing up space {{to be used for}} more on-chip storage. Recent proposals have included multiple conventional superscalars on a chip (which we call superchip) and multithreaded superscalars, also called simultaneous multithreaded (SMT) processors. In the past, these systems have been compared in the context of sequential applications. In this paper, we focus {{on a wide range of}} parallel applications. We show that, in low-end <b>machines</b> <b>with</b> only one processor chip on which to run the multiple threads, in-order issue SMTs achieve a performance that is more stable and, on average, higher than out-of-order issue superchips. In high-end <b>machines</b> <b>with</b> <b>several</b> processor chips working on the same application, however, the higher demands on the processors often make out-of-order issue a require [...] ...|$|R
40|$|QCDOC is a massively {{parallel}} supercomputer whose processing nodes {{are based on}} an application-specific integrated circuit (ASIC). This ASIC was custom-designed so that crucial lattice QCD kernels achieve an overall sustained performance of 50 % on <b>machines</b> <b>with</b> <b>several</b> 10, 000 nodes. This strong scalability, together with low power consumption and a price/performance ratio of $ 1 per sustained MFlops, enable QCDOC to attack the most demanding lattice QCD problems. The first ASICs became available in June of 2003, and the testing performed so far has shown all systems functioning according to specification. We review the hardware and software status of QCDOC and present performance figures obtained in real hardware {{as well as in}} simulation. Comment: Lattice 2003 (machine), 6 pages, 5 figure...|$|R
40|$|The study {{presents}} the {{detailed analysis of}} reliability data of an automated croissant processing line. The line consists of several workstations, each having one or more <b>machines</b> <b>with</b> <b>several</b> failure modes. Failure and repair times are collected {{at all levels of}} the hierarchical structure of the line (failure mode, machine, workstation and entire line) for a period of 10 months and it was observed that there is a significant difference between nominal and effective processing rate. Then some alternative solutions were analyzed and propose a maintenance policy for increasing the effective processing rate of the line, without raising the cost. It can also be served as a valid data source for researchers who want to model and analyze real manufacturing systems...|$|R
40|$|International audienceRoughly speaking, {{there is}} one main model of pattern {{recognition}} support vector <b>machine,</b> <b>with</b> <b>several</b> variants of lower popularity. On the contrary, among the different multi-class support vector machines {{which can be found}} in literature, none is clearly favoured. On the one hand, they exhibit distinct statistical properties. On the other hand, multiple comparative studies between multi-class support vector machines and decomposition methods have highlighted the fact that in practice, each model has its advantages and drawbacks. In this article, we introduce a generic model of multi-class support vector machine. All the machines of this kind published so far appear as instances of this model. This definition makes it possible to devise new machines meeting specific requirements as well as to analyse globally the statistical properties of the multi-class support vector machines...|$|R
40|$|Abstract. We perform discriminative {{analysis}} of brain structures using morphometric information. Spherical harmonics technique and point distribution model {{are used for}} shape description. Classification is performed using linear discriminants and support vector <b>machines</b> <b>with</b> <b>several</b> feature selection approaches. We consider both inclusion and exclusion of volume information in the discrimination. We perform extensive experimental studies by applying different combinations of techniques to hippocampal data in schizophrenia and achieve best jackknife classification accuracies of 95 % (whole set) and 90 % (right-handed males), respectively. Our results find that the left hippocampus is a better predictor than the right in the complete dataset, but that the right hippocampus is a stronger predictor than the left in the right-handed male subset. We also propose a new method for visualization of discriminative patterns. ...|$|R
40|$|The {{scalable}} {{implementation of}} multigrid methods for <b>machines</b> <b>with</b> <b>several</b> thousands of processors is investigated. Parallel performance models are presented for two dierent structured-grid multigrid algorithms. These performance models are then {{used in the}} discussion of two implementation topics: replicating computations to reduce communications, and mixed programming models for multigrid codes on clusters of SMPs. Special attention is paid to comparing moderatesized parallelism and large-scale parallelism. Results are given from existing multigrid codes to support the discussion. 1 Introduction Computer simulations play an increasingly important role in scientic investigations. As a result, codes are being developed to solve complex multi-physics problems at very high resolutions. Such large-scale simulations require massively parallel computing, but this is not sucient. One also needs scalable algorithms such as multigrid, and scalable implementations of these algorithms. The [...] ...|$|R
40|$|The {{scalable}} {{implementation of}} multigrid methods for <b>machines</b> <b>with</b> <b>several</b> thousands of processors is investigated. Parallel performance models are presented for three dierent structured-grid multigrid algorithms, and a description is given {{of how these}} models {{can be used to}} guide implementation. Potential pitfalls are illustrated when moving from moderate-sized parallelism to large-scale parallelism, and results are given from existing multigrid codes to support the discussion. Finally, the use of mixed programming models is investigated for multigrid codes on clusters of SMPs. 1 Introduction Computer simulations play an increasingly important role in scientic investigations. As a result, codes are being developed to solve complex multi-physics problems at very high resolutions. Such large-scale simulations require massively parallel computing, but this is not sucient. One also needs scalable algorithms such as multigrid, and scalable implementations of these algorithms. The dev [...] ...|$|R
40|$|National audienceHigh Performance Computing {{machines}} {{use more}} and more Graphical Processing Units as they are very efficient for homogeneous computation such as matrix operations. However before using these accelerators, one has to transfer data from the processor to them. Such a transfer can be slow. In this report, our aim is to study the impact of communication times on the makespan of a scheduling. Indeed, with a better anticipation of these communications, we could use the GPUs even more efficiently. More precisely, we will focus on <b>machines</b> <b>with</b> one or more GPUs and on applications with a low ratio of computations over communications. During this study, we have implemented two offline scheduling algorithms within XKAAPI's runtime. Then we have led an experimental study, combining these algorithms to highlight the impact of communication times. Finally our study has shown that, by using communication aware scheduling algorithms, we can reduce substantially the makespan of an application. Our experiments have shown a reduction of this makespan up to 64 % on a <b>machine</b> <b>with</b> <b>several</b> GPUs executing homogeneous computations...|$|R
50|$|The ALF {{system was}} {{designed}} to be an efficient implementation of the combination of resolution, narrowing, rewriting, and rejection. ALF programs are compiled into instructions of an abstract machine. The abstract machine is based on the Warren Abstract <b>Machine</b> (WAM) <b>with</b> <b>several</b> extensions to implement narrowing and rewriting. In the current ALF implementation programs of this abstract machine are executed by an emulator written in C.|$|R
40|$|The Cedar multi [...] cluster {{system is}} a <b>machine</b> <b>with</b> <b>several</b> levels of {{parallelism}} and memory. We compare two different Cedar [...] implementations of a Navier [...] Stokes solver kernel, the Generalized Stokes problem which is discretized here by a Mixed Finite Element Method. The arising linear system is solved by the conjugate gradient Uzawa algorithm. The two implementation approaches are a static approach which we call "fork and join" where task distribution and synchronization points are a priori defined by the user and a dynamic approach called here "dynamic task distribution " where the tasks are placed into a queue from which a free cluster can fetch a task without violating the tree of dependencies. Both implementations are described, and numerical results for the most time consuming part, Uzawa's algorithm, are given. 1. Introduction. The Cedar system which is being developped at the Center for Supercomputing Research and Development at the University of Illinois in Urbana [...] Champaig [...] ...|$|R
40|$|Principle of parsimony (Occams razor) {{is a key}} {{principle}} {{where the}} unnecessary complexity of a classier is regulated to improve the generalization performance in pattern classication. In decision tree construction, often the complexity is regu- lated by early stopping {{the growth of a}} tree at a node whenever the impurity at that node reaches below a threshold. In this paper, we generalize this heuristic and express this principle in terms of constraining the outcome of a classier instead of explicitly regularizing the model complexity in terms of the model parameters. We construct a classier using this heuristic namely, a least square kernel <b>machine</b> <b>with</b> box constraints (LSKMBC). In our approach, we consider uniform priors and obtain the loss functional for a given margin considered to be a model selection parameter. The framework not only di?ers from the existing least square kernel machines, but also it does not require Mercer condition satisability. We also discuss the relation- ship of the proposed kernel <b>machine</b> <b>with</b> <b>several</b> other existing kernel machines. Experimentally we validate the performance of the classier over real-life datasets, and observe that LSKMBC performs competitively, and is able to produce certain results even better than SVM. 1...|$|R
40|$|In {{the area}} of testing {{communication}} systems, the interfaces between systems to be tested and their testers have great impact on test generation and fault detectability. Several types of such interfaces have been standardized by the International Standardization Organization (ISO). A general distributed test architecture, containing distributed interfaces, has been presented in the literature for testing distributed systems based on the Open Distributing Processing (ODP) Basic Reference Model (BRM), which is a generalized version of ISO distributed test architecture. We study in this paper the issue of test selection with respect to such an test architecture. In particular, we consider communication systems that can be modeled by finite state <b>machines</b> <b>with</b> <b>several</b> distributed interfaces, called ports. A test generation method is developed for generating test sequences for such finite state machines, {{which is based on}} the idea of synchronizable test sequences. Starting from the initial effort by Sarikaya, a certain amount of work has been done for generating test sequences for finite state <b>machines</b> <b>with</b> respect to the ISO distributed test architecture, all based on the idea of modifying existing test generation methods to generat...|$|R
40|$|Many {{scientific}} data sets are 3 D or 4 D scalar fields, for which typically isosurface- and volume visualization methods {{are used to}} extract information. These data sets are either massively complex (e. g., seismic data sets), or steadily increasing in size due to the permanently improving resolutions of different 3 D scanners (e. g., CT- and MRTscanners) or calculation results (e. g., CFD-simulations). Only algorithms that scale well to data set complexity are suited to visualize those increasing data sets. Isosurface ray tracing and maximum intensity projection (MIP) accelerated through implicit KD-trees have a logarithmic dependency between visualization time and scene size, making them ideal algorithms for the visualization of massively complex scalar fields. Furthermore is ray tracing efficiently parallelized on the more and more commonly used shared memory machines (e. g., desktop <b>machines</b> <b>with</b> <b>several</b> multicore processors) and {{may be used to}} realize advanced shading eff ects like shadows and reflections. We introduce new optimized implicit KD-trees which allow on today's desktop computers interactive isosurfacing and MIP of data sets that are bigger than one half of the machine's main memory...|$|R
5000|$|However, the CPU {{could only}} execute {{a limited number}} of simple {{instruction}}s. A typical CPU of the era had a complex instruction set, which included instructions to handle all the normal [...] "housekeeping" [...] tasks such as memory access and input/output. Cray instead implemented these instructions in separate, simpler processors dedicated solely to these tasks, leaving the CPU with a much smaller instruction set. (This was the first of what later came to be called reduced instruction set computer (RISC) design.) By allowing the CPU, peripheral processors (PPs) and I/O to operate in parallel, the design considerably improved the performance of the machine. Under normal conditions a <b>machine</b> <b>with</b> <b>several</b> processors would also cost a great deal more. Key to the 6600's design was to make the I/O processors, known as peripheral processors (PPs), as simple as possible. The PPs were based on the simple 12-bit CDC 160-A, which ran much slower than the CPU, gathering up data and transmitting it as bursts into main memory at high speed via dedicated hardware.|$|R
40|$|In {{this paper}} we propose a {{methodology}} based on supervised automatic learning in order to classify the behaviour of generators in terms of their performance in providing primary frequency control ancillary services. The problem is posed as a time-series classification problem, and handled by using state-of- the-art supervised learning methods such as ensembles of decision trees and support-vector <b>machines</b> combined <b>with</b> <b>several</b> preprocessing techniques. The method was designed {{in the context of the}} Belgian system and is validated on real-life data composed of more than 600 time-series recorded on this system. Peer reviewe...|$|R
40|$|Many {{different}} {{techniques have}} been developed for detecting faults in rotating machinery. This is because different fault types typically require different techniques for the effective detection of the fault. However, for many new or unknown fault types, we have found that the existing detection techniques are either incapable or ineffective, and that we therefore need to come up with brand new methods after the fault event. This can significantly constrain the usefulness and effectiveness of Prognostic Health Management (PHM) systems. In this paper we attempt to look at detecting global changes in the synchronously averaged signals as the machine’s health status progresses from healthy to faulty, and to define one unified signal processing technique and its associated condition indicators for the detection of changes caused by various types of faults in rotating machinery. The proposed method is conceptually very simple, and its effectiveness is demonstrated using vibration data from <b>machines</b> <b>with</b> <b>several</b> different types of faults. The results have shown that this single unified change detection approach can be very effective in detecting and trending changes caused by many different types of machine faults. 1...|$|R
40|$|Abstract. Separation of {{concerns}} {{is difficult to}} achieve {{in the implementation of}} a programming language interpreter. We argue that evaluator concerns (i. e., those implementing the operational semantics of the language) are, in particular, difficult to separate from the runtime concerns (e. g., memory and stack management) that support them. This precludes the former from being reused and limits variability in the latter. In this paper, we present the Game environment for composing customized interpreters from a reusable evaluator and different variants of its supporting runtime. To this end, Game offers a language for specifying the evaluator according to the generic programming methodology. Through a transformation into defunctionalized monadic style, the Game toolchain generates a generic abstract machine in which the sequencing of low-level interpretational steps is parameterized. Given a suitable instantiation of these parameters for a particular runtime, the toolchain is able to inject the runtime into the generic abstract machine such that a complete interpreter is generated. To validate our approach, we port the prototypical Scheme evaluator to Game and compose the resulting generic abstract <b>machine</b> <b>with</b> <b>several</b> runtimes that vary in their automatic memory management as well as their stack discipline. ...|$|R
40|$|This paper {{investigates the}} problem of driving a <b>machine</b> <b>with</b> <b>several</b> {{reciprocating}} heavy inertias. The drive should also realize some programmable flexibility of the motion, so that {{the operation of the}} machine, e,g., synchronization between different motions and duration of standstills, can easily be modified. The use of a single servomotor for every motion is not the best solution, since it does not allow for energy recuperation. This means that the installed servomotor power will be much higher than the required net peak power for the machine as a whole. The presented solution, however, reduces the servomotor peak torque and power by one order of magnitude, and makes energy transfer between the motions possible. The hybrid cam mechanism consists of a hybrid drive, which is a combination of a servomotor, a constant velocity motor and a cam follower mechanism. The operation principle exploits the nonlinear characteristics of the cam to add flexibility at low cost of energy. The hybrid solution is particularly successful for motions involving high peak acceleration. In addition, the concept can be used to reject disturbances. Simulations show the performance of the hybrid cam mechanism. status: publishe...|$|R
40|$|We {{describe}} the systems {{developed by the}} team of the Qatar Computing Research Institute for the WMT 12 Shared Translation Task. We used a phrase-based statistical <b>machine</b> translation model <b>with</b> <b>several</b> non-standard settings, most notably tuning data selection and phrase table combination. The evaluation results show that we rank second in BLEU and TER for Spanish-English, and in the top tier for German-English...|$|R
5000|$|The Hawker Horsley won the {{specification}} trials, so no more Handcrosses were built. [...] The {{last one}} stayed at Cricklewood until 1926, {{serving as a}} test machine and the second, moved from Farnborough to Martlesham, remained with the Armaments Trial Flight until 1928. The first <b>machine</b> was fitted <b>with</b> <b>several</b> different wooden two- and four-bladed propellers for comparison with the original metal two-bladers.|$|R
40|$|ABSTRACT: The {{ability to}} scale a web {{application}} or website is tied directly to understanding where the resource constraints lie and what impact {{the addition of}} various resources has on the application. Unfortunately, architects {{more often than not}} assume that simply adding another server into the mix can fix any performance problem and security issues. When you start adding new hardware/update existing hardware in a web cloud, the complexity starts increasing which affects performance and hence security. While priced cloud computing services save pains to maintain the computational environment, there are several drawbacks such as overhead of virtual machines, possibility to share one physical <b>machine</b> <b>with</b> <b>several</b> virtual <b>machines,</b> and indeterminacy of topological allocation of their own virtual machines. Multi-tenancy is one of key characteristics of the service oriented computing especially for Software as a Service (SaaS) to leverage economy of scale to drive down total cost of ownership for both service consumer and provider. This paper aims to study the technologies to build a cost-effective, secure and scalable multi-tenant infrastructure and how to improve the security and enhance its performance. This paper also identifies the potential performance bottlenecks, summarizes corresponding optimization approaches and best implementation practices for different multi-tenant business usage models...|$|R
