220|10000|Public
60|$|I {{have thought}} it right to submit these not inconsiderable {{proposals}} in general outline to the House of Commons at this early stage, in order that the proposals for Labour Exchanges which we are now putting forward may be properly understood, {{and may not be}} underrated or misjudged. We cannot bring the system of unemployment insurance before Parliament in a legislative form this year for five reasons: We have not now got the time; we have not yet got the money; the finance of such a system has to be adjusted and co-ordinated with the finance of the other insurance schemes upon which the Chancellor of the Exchequer is engaged; the establishment of a system of Labour Exchanges is the necessary forerunner and foundation of a system of insurance; and, lastly, no such novel departure as unemployment insurance could possibly be taken without much further consultation and negotiation with the trade unions and employers specially concerned than the conditions of secrecy under which we have been working have yet allowed. This business of conference and consultation of the fullest character will occupy the winter, when the Board of Trade will confer with all parties affected, so that the greatest <b>measure</b> <b>of</b> <b>agreement</b> may be secured for our proposals when they are next year presented in their final form.|$|E
6000|$|The Government, {{and those}} who support them, may rejoice {{that we have been}} able to take this first most {{important}} step in our South African policy with such a very general <b>measure</b> <b>of</b> <b>agreement,</b> with, indeed, a consensus of opinion which almost amounts to unanimity. Both races, every Party, every class, every section in South Africa have agreed in the course which his Majesty's Government have adopted in abandoning representative government and going at once to responsible government. That is already a very great thing, but it was not always so. Those who sat in the last Parliament will remember that it was not always so. We remember that Lord Milner was entirely opposed to granting responsible government. We know that Mr. Lyttelton wrote pages and pages in the Blue Book of last year proving how futile and dangerous responsible government would be; and the right hon. Member for West Birmingham, who took the Government decision as a matter of course on the first day of the present session, made a speech last session in which he indicated in terms of great gravity and force, that he thought it was wholly premature to grant responsible government to the Transvaal. But all that is abandoned now. I heard the right hon. Member for West Birmingham, in the name of the Party opposite, accept the policy of his Majesty's Government. I heard the hon. Member for Blackpool this afternoon say that he hoped that responsible government would be given to the Transvaal at the earliest possible moment. In regard to the Orange River Colony, it is quite true that the official Opposition, so far as I gather their view, think that it should be delayed, and should not be given at the same time as to the Transvaal; but that is not the view of the right hon. Member for West Birmingham. Speaking in the House of Commons on July 27, 1905, the right hon. gentleman said: ...|$|E
60|$|I {{have only}} one word more to say, and it is {{rendered}} necessary by the observations which fell from Lord Lansdowne last night, when, according to the Scottish papers, he informed a gathering at which he was the principal speaker that the House of Lords was not obliged to swallow the Budget whole or without mincing.[18] I ask you to mark that word. It is a characteristic expression. The House of Lords means to assert its right to mince. Now let us for our part be quite frank and plain. We want this Budget Bill to be fairly and fully discussed; we do not grudge the weeks that have been spent already; {{we are prepared to}} make every sacrifice--I speak for my honourable friends who are sitting on this platform--of personal convenience in order to secure a thorough, patient, searching examination of proposals the importance of which we do not seek to conceal. The Government has shown itself ready and willing to meet reasonable argument, not merely by reasonable answer, but when a case is shown, by concessions, and generally in a spirit of goodwill. We have dealt with this subject throughout with a desire to mitigate hardships in special cases, and to gain as large a <b>measure</b> <b>of</b> <b>agreement</b> as possible for the proposals we are placing before the country. We want the Budget not merely to be the work of the Cabinet and of the Chancellor of the Exchequer; {{we want it to be}} the shaped and moulded plan deliberately considered by the House of Commons. That will be a long and painful process to those who are forced from day to day to take part in it. We shall not shrink from it. But when that process is over, when the Finance Bill leaves the House of Commons, I think you will agree with me that it ought to leave the House of Commons in its final form. No amendments, no excision, no modifying or mutilating will be agreed to by us. We will stand no mincing, and unless Lord Lansdowne and his landlordly friends choose to eat their own mince, Parliament will be dissolved, and we shall come to you in a moment of high consequence for every cause for which Liberalism has ever fought. See that you do not fail us in that hour.|$|E
40|$|A {{comprehensive}} taxonomy categorizing passive {{behaviors in}} people with dementia was developed and revised {{through the use of}} expert raters. The taxonomy was first derived from the synthesis of 15 empirical studies that addressed this phenomenon, then was rated by an expert panel of six nurse-scientists with expertise in dementia and neuroscience research. This article describes the application <b>of</b> two <b>measures</b> <b>of</b> <b>agreement,</b> multiple rater kappa and proportion <b>of</b> <b>agreement</b> for multiple raters, calculated using Stata and SPSS, to evaluate and revise the taxonomy. The method proved useful for estimating the content validity of the taxonomy and provided evidence <b>of</b> stronger <b>agreement</b> among raters for the revised and final forms of the taxonomy. Nurse researchers will find this methodology to be an efficient, practical approach to applying <b>measures</b> <b>of</b> <b>agreement</b> for a variety of purposes, including taxonomy development. © 2001 John Wiley & Sons, Inc. Res Nurs Health 24 : 336 – 343, 200...|$|R
40|$|Using WinBUGS to {{implement}} Bayesian inferences of estimation and testing hypotheses, Bayesian Methods for <b>Measures</b> <b>of</b> <b>Agreement</b> presents useful methods {{for the design}} and analysis <b>of</b> <b>agreement</b> studies. It focuses on agreement among the various players in the diagnostic process. The author employs a Bayesian approach to provide statistical inferences based on various models of intra- and interrater agreement. He presents many examples that illustrate the Bayesian mode of reasoning and explains elements of a Bayesian application, including prior information, experimental information, the likelihood function, posterior distribution, and predictive distribution. The appendices provide the necessary theoretical foundation to understand Bayesian methods as well as introduce the fundamentals of programming and executing the WinBUGS software. Taking a Bayesian approach to inference, this hands-on book explores numerous <b>measures</b> <b>of</b> <b>agreement,</b> including the Kappa coefficient, the G coefficient, and intraclass correlation. With examples throughout and end-of-chapter exercises, it discusses how to successfully design and analyze an agreement study...|$|R
50|$|Since the intraclass {{correlation}} coefficient gives {{a composite of}} intra-observer and inter-observer variability, its results are sometimes considered difficult to interpret when the observers are not exchangeable. Alternative measures such as Cohen's kappa statistic, the Fleiss kappa, and the concordance correlation coefficient have been proposed as more suitable <b>measures</b> <b>of</b> <b>agreement</b> among non-exchangeable observers.|$|R
50|$|The most {{commonly}} used <b>measure</b> <b>of</b> <b>agreement</b> between observers is Cohen's kappa. The value of kappa {{is not always easy}} to interpret and it may perform poorly if the values are asymmetrically distributed. It also requires that the data be independent. The delta statistic may be of use when faced with the potential difficulties.|$|E
50|$|Fritz Scholz and Michael A. Stephens (1987) {{discuss a}} test, {{based on the}} Anderson-Darling <b>measure</b> <b>of</b> <b>agreement</b> between distributions, for whether a number of random samples with {{possibly}} different sample sizes may have arisen from the same distribution, where this distribution is unspecified. The R package kSamples implements this rank test for comparing k samples among several other such rank tests.|$|E
50|$|Note that Cohen's kappa {{measures}} {{agreement between}} two raters only. For a similar <b>measure</b> <b>of</b> <b>agreement</b> (Fleiss' kappa) used {{when there are}} more than two raters, see Fleiss (1971). The Fleiss kappa, however, is a multi-rater generalization of Scott's pi statistic, not Cohen's kappa. Kappa is also used to compare performance in machine learning but the directional version known as Informedness or Youden's J statistic is argued to be more appropriate for supervised learning.|$|E
40|$|Formal {{evaluation}} of the ability of clinicians and researchers to agree, for example, on the clinical assessment of patients, increasingly is becoming important. Two <b>measures</b> <b>of</b> <b>agreement,</b> ? and the intraclass correlation coefficient, are described and illustrated. The calculation of confidence intervals that correspond to these statistics {{by means of the}} 2 ̆ 7 bootstrap 2 ̆ 7 method also is discussed...|$|R
40|$|To find {{factors which}} {{influence}} {{the reliability of}} coded answers to open questions. Number of categories allowed to use / information about research from which question to be coded was obtained / information about coding by other experts / approach and criteria used for coding / revising categories and restart <b>of</b> coding / <b>measures</b> <b>of</b> <b>agreement.</b> Background variables: basic characteristics/ educatio...|$|R
40|$|Significance {{tests for}} the <b>measure</b> <b>of</b> raw <b>agreement</b> are proposed. First, it is {{shown that the}} <b>measure</b> <b>of</b> raw <b>agreement</b> can be {{expressed}} as a proportionate reduction-in-error measure, sharing this characteristic with Cohen’s κ and Brennan and Prediger’s κn. Second, it is shown that the coefficient <b>of</b> raw <b>agreement</b> is linearly related to Brennan and Prediger’s κn. Therefore, using the same base model for the estimation of expected cell frequencies as Brennan and Prediger’s κn, one can devise significance tests for the <b>measure</b> <b>of</b> raw <b>agreement.</b> Two tests are proposed. The first uses Stouffer’s Z, a probability pooler. The second test is the binomial test. A data example analyzes the agreement between two psychiatrists’ diagnoses. The covariance structure <b>of</b> the <b>agreement</b> cells in a rater by rater table is described. Simulation studies show the performance and power functions of the test statistics. ...|$|R
5000|$|Those {{who know}} how widely the churches have {{differed}} in doctrine and practice on baptism, Eucharist and ministry, will appreciate {{the importance of the}} large <b>measure</b> <b>of</b> <b>agreement</b> registered here. Virtually all the confessional traditions are included in the Commission's membership. That theologians of such widely different denominations should be able to speak so harmoniously about baptism, Eucharist and ministry is unprecedented in the modern ecumenical movement. Particularly noteworthy {{is the fact that the}} Commission also includes among its full members theologians of the Catholic and other churches which do not belong to the World Council of Churches itself.|$|E
50|$|Hoyland was {{a member}} of the Royal Society of British Artists, a member of the Society of Graphic Artists, and a member of the Sheffield Society of Artists. As well as {{portrait}} and figure paintings, Hoyland also painted flowers and landscapes, and experimented with wood cuts. In October 1931, Hoyland was quoted as saying that he believed the reason why there were so many schools of art at the time, was because it reflected the uncertainty and perplexity of the present age. He felt it would be a generation before artists could reach some <b>measure</b> <b>of</b> <b>agreement</b> as to what constitutes a good picture, but currently in a period of transition and change.|$|E
5000|$|Diop and Obenga {{attempted}} to linguistically link Egypt and Africa, {{by arguing that}} the Ancient Egyptian language was related to Diop's native Wolof (Senegal). [...] Diop's work was well received by the political establishment in the post-colonial formative phase {{of the state of}} Senegal, and by the Pan-Africanist Négritude movement, but was rejected by mainstream scholarship. In drafting that section of the report of the UNESCO Symposium, Diop claimed that Diop and Obenga's linguistic reports had a large <b>measure</b> <b>of</b> <b>agreement</b> and were regarded as [...] "very constructive." [...] However, in the discussion thereof in the work Ancient Civilizations of Africa, Volume 2, the editor has inserted a footnote stating that these are merely Diop’s opinions and that they were not accepted by all the experts participating. In particular, Prof Abdelgadir M. Abdalla stated that “The linguistic examples given by Prof Diop were neither convincing nor conclusive.” ...|$|E
40|$|In {{the context}} of micro-task crowdsourcing, each task is usually {{performed}} by several workers. This allows researchers to leverage <b>measures</b> <b>of</b> the <b>agreement</b> among workers on the same task, to estimate the reliability of collected data and to better understand answering behaviors of the participants. While many <b>measures</b> <b>of</b> <b>agreement</b> between annotators have been proposed, they are known for suffering from many problems and abnormalities. In this paper, we identify the main limits <b>of</b> the existing <b>agreement</b> <b>measures</b> in the crowdsourcing context, both by means of toy examples {{as well as with}} real-world crowdsourcing data, and propose a novel agreement measure based on probabilistic parameter estimation which overcomes such limits. We validate our new agreement measure and show its flexibility as compared to the existing agreement measures...|$|R
40|$|Abstract. Cohen’s κ (kappa) is {{typically}} {{used as a}} <b>measure</b> <b>of</b> degree <b>of</b> rater <b>agreement.</b> It is often criticized because it is marginal-de-pendent. In this article, this characteristic is explained and illustrated {{in the context of}} (1) nonuniform marginal probability distributions, (2) odds ratios that remain constant while κ changes in the presence of varying marginal distributions, and (3) percentages <b>of</b> raw <b>agreement</b> that remain constant while κ changes in the presence of varying marginal distributions. The meaning and interpretation of κ are explained with reference to the log-linear main effect model of variable independence. This model is used for the estimation of the expected cell frequencies <b>of</b> <b>agreement</b> tables. It is shown that the interpretation of κ as a <b>measure</b> <b>of</b> degree <b>of</b> <b>agreement</b> is incorrect. The correct interpretation is that κ assesses the degree <b>of</b> <b>agreement</b> beyond that expected based on a statistical model such as the independence or the null model. Based on Goodman’s (1991) distinction between marginal-free and marginal-dependent measures, it is shown that κ is marginal-dependent. It shares this characteristic with the well-known χ-statistic and the correlation coefficient for cross-classifications. In contrast, the odds ratio, the unweighted log-linear interaction, and the percentage <b>of</b> raw <b>agreement</b> are margin-al-free. Therefore, the expectation that marginal-dependent κ would reflect the same data characteristics as some <b>of</b> the marginal-free <b>measures</b> is misguided. It is recommended that researchers report both <b>measures</b> <b>of</b> degree <b>of</b> <b>agreement</b> and <b>measures</b> <b>of</b> <b>agreement</b> beyond some expectation...|$|R
40|$|Agreement between {{measurements}} {{refers to}} the degree of concordance between two (or more) sets of measurements. Statistical methods to test agreement are used to assess inter-rater variability or to decide whether one technique for measuring a variable can substitute another. In this article, we look at statistical <b>measures</b> <b>of</b> <b>agreement</b> for different types of data and discuss the differences between these and those for assessing correlation...|$|R
5000|$|Here, {{reporting}} {{quantity and}} allocation disagreement is informative while Kappa obscures information. Furthermore, Kappa introduces some challenges in calculation and interpretation because Kappa is a ratio. It {{is possible for}} Kappa’s ratio to return an undefined value due to zero in the denominator. Furthermore, a ratio does not reveal its numerator nor its denominator. It is more informative for researchers to report disagreement in two components, quantity and allocation. These two components describe {{the relationship between the}} categories more clearly than a single summary statistic. When predictive accuracy is the goal, researchers can more easily begin to think about ways to improve a prediction by using two components of quantity and allocation, rather than one ratio of Kappa.Some researchers have expressed concern over κ's tendency to take the observed categories' frequencies as givens, which can make it unreliable for measuring agreement in situations such as the diagnosis of rare diseases. In these situations, κ tends to underestimate the agreement on the rare category. For this reason, κ is considered an overly conservative <b>measure</b> <b>of</b> <b>agreement.</b> Others contest the assertion that kappa [...] "takes into account" [...] chance agreement. To do this effectively would require an explicit model of how chance affects rater decisions. The so-called chance adjustment of kappa statistics supposes that, when not completely certain, raters simply guess—a very unrealistic scenario.|$|E
5000|$|After the Battle of Stalingrad, {{the pace}} of Goerdeler's conspiratorial {{activities}} gathered speed. Between November 1942 and November 1943, Goerdeler was in regular contact with his friends, the Wallenberg family of Sweden whom he used as middle-men {{in his efforts to}} make contact with the British and American governments. On January 22, 1943 at the home of Peter Yorck von Wartenburg Goerdeler met with the Kreisau Circle, during which he argued and debated forcefully about the social and economic policies to be pursued by a post-Nazi government. Only with some difficulty were Ulrich von Hassell and Fritz-Dietlof von der Schulenburg able to patch up a <b>measure</b> <b>of</b> <b>agreement</b> between the Kreisau Circle and Goerdeler. Those present at the meeting of January 22 were Goerdeler, Hassell, General Beck, Johannes Popitz and Jens Jessens for the conservative faction and von der Schulenburg, Yorck von Wartenburg, Eugen Gerstenmaier, Adam von Trott zu Solz and Helmuth James Graf von Moltke for the left-learning Kreisau Circle. [...] In March 1943, Goerdeler wrote a letter addressed to several German Army officers appealing to them to overthrow the Nazis and demanding that just one line divide Germans: [...] "...that between decent and non-decent." [...] Goerdeler went on to write: [...] "How is it possible that so basically decent a people as the Germans can put up for so long with such an intolerable system? Only because all offences against law and decency are carried out under the protection of secrecy and under the pressures of terror" [...] Goerdeler argued that if only a situation were created [...] "in which, if only for twenty-four hours, it were possible for the truth to be spoken again", then the Nazi regime would collapse like a house of cards. In May 1943, Goerdeler submitted a memo to the Wallenbergs, which he asked them to pass on to the Anglo-Americans outlining his thoughts on the German-Polish border. In the same memo, Goerdeler called for a [...] "European community" [...] comprising a German-dominated confederation, which in turn was to be sub-divided into an Eastern European confederation consisting of Poland, Lithuania, Latvia and Estonia, a confederation of the Scandinavian states, a South European confederation, and a Balkan confederation. The [...] "European confederation" [...] was to be one economic unit with one military ruled over by a Council consisting of two representatives from every state, who would elect a European President for a four-year term. Helping the Council and the President was to be a Federal Assembly to which each of the various confederations would send five to ten members based on their populations. Finally, the European confederation was to serve as the nucleus of a [...] "World Confederation of Nations" [...] that would banish war everywhere, and promote peace and prosperity.|$|E
3000|$|In edge detection, A 0 may {{be defined}} as a <b>measure</b> <b>of</b> <b>agreement</b> between true and {{detected}} edges. The definition of A [...]...|$|E
50|$|Bennett, Alpert & Goldstein’s S is a {{statistical}} <b>measure</b> <b>of</b> inter-rater <b>agreement.</b> It {{was created by}} Bennett et al. in 1954.|$|R
40|$|Cancer {{screening}} and diagnostic tests often are classified using a binary outcome such as diseased or not diseased. Recently large-scale {{studies have been}} conducted to assess agreement between many raters. <b>Measures</b> <b>of</b> <b>agreement</b> using the class of generalized linear mixed models were implemented efficiently in four recently introduced R and SAS packages in large-scale agreement studies incorporating binary classifications. Simulation studies were conducted to compare the performance across the packages and apply the agreement methods to two cancer studies...|$|R
40|$|Epidemiologists {{sometimes}} collect bivariate {{continuous data}} {{on a number of}} subjects, compute the empirical (sample) quantiles of the marginal data, and then use these values to partition the original data into two-way contingency tables. Tables created in this manner have row and column categories defined by the random empirical marginal quantiles rather than by preset cutpoints, so these tables have fixed marginal totals. Hence, instead of the conventional multinomial distribution, these tables have the empirical bivariate quantile-partitioned (EBQP) distribution. In this paper, the authors demonstrate how to use empirical methods appropriate for EBQP tables to make inferences and construct confidence intervals for three commonly used <b>measures</b> <b>of</b> agreement: kappa, weighted kappa, and another class <b>of</b> <b>measures</b> derived from conditional proportions in the extreme rows of the table. They also show that if one incorrectly applies conventional methods appropriate for multinomial tables to statistics calculated from EBQP tables, one can obtain substantially misleading results. In addition, the authors present alternative parametric methods for estimating these <b>measures</b> <b>of</b> <b>agreement</b> and illustrate corresponding methods of inference and confidence interval construction. Finally, they show that these empirical (EBQP) methods can have low efficiency compared with parametric methods for some <b>of</b> these <b>measures</b> <b>of</b> <b>agreement.</b> Am J Epidemiol 1997; 146 : 520 - 6. epidemiologic methods; nutrition surveys; questionnaires; statistic...|$|R
40|$|Context: The {{method of}} {{combined}} scores is currently {{used to determine}} the score obtained by the Script Concordance Test (SCT). Goal: To propose a novel <b>measure</b> <b>of</b> <b>agreement</b> between the SCT responses given by the candidate and those provided by the panel of experts. Results: The current scoring method of the SCT {{does not take into account}} the fact that agreement between the candidate and the panel of experts may be arbitrary. The proposed measure allows correcting this fact. Conclusion: This article addresses the problem of scores obtained by guessing. The new <b>measure</b> <b>of</b> <b>agreement</b> allows to improve the actual scoring method. Peer reviewe...|$|E
40|$|A major {{deficiency}} in classical test theory is {{the reliance on}} Pearson product- moment (PPM) correlation concepts {{in the definition of}} reliability. PPA measures are totally insensitive to first moment differences in tests which leads to the dubious assumption of essential tan-equivalence. Robinson proposed a <b>measure</b> <b>of</b> <b>agreement</b> that is sensitive to different test difficulty and gives a practical statistic to estimate reliability in the presence of known form variation in difficulty. Robinson's <b>measure</b> <b>of</b> <b>agreement</b> appears to be a useful alternative to the generalizability coefficient, as it provides a more conservative estimate of reliability under conditions of parallel form differences in mean. This is likely to be especially useful when examining inter rater reliability when internal consistency of the raters is poor. Robinson's measure does not seem advantageous for highly reliable parallel tests such as are encountered in standardized testing programs. A simulation study is presented to illustrate the degree of the coefficient's sensitivity to form difficulty variance. Robinson,s <b>measure</b> <b>of</b> <b>agreement</b> and the intraciass correlation are coaputed for each simulation and their values are compared. (author/RL) Reproeluctions supplied by EDRS are the best that can be made from the original document...|$|E
40|$|AIMS AND BACKGROUND: Thyroid carcinomas display several {{pathologic}} features, show different {{behavior and}} necessitate different treatment; thus correct classification is mandatory. METHODS: The kappa statistic {{was used as}} a <b>measure</b> <b>of</b> <b>agreement</b> in a panel of seven pathologists who reviewed 200 cases of thyroid tumors. RESULTS: Overall agreement was 83...|$|E
50|$|In statistics, Andrés and Marzo's Delta is a <b>measure</b> <b>of</b> an <b>agreement</b> {{between two}} {{observers}} used in classifying data. It {{was created by}} Andres & Marzo in 2004.|$|R
40|$|The {{validity}} of peak inspiratory mouth pressure (P. PImax) as a <b>measure</b> <b>of</b> inspiratory muscle strength was investigated by comparing it with sniff Pes {{in patients with}} COPD with respect to (1) learning effect, (2) reproducibility, and (3) <b>measures</b> <b>of</b> <b>agreement.</b> To assess the discriminating capacity of P. PImax, we compared the values in patients with COPD with those of healthy elderly subjects. Thirty-four patients (mean age, 62. 5 years) with severe airways obstruction (FEV(1), 44 % predicted; FEV(1) /IVC, 37 % predicted) and 149 healthy subjects (age {{greater than or equal}} to 55 years) were included. P. PImax was assessed during a maximal static inspiratory maneuver, while sniff Pes was assessed during a maximal sniff maneuver. Both maneuvers were performed from residual volume ten times on the same day. P. PImax showed no learning effect, while the sniff maneuver used seven attempts to obtain a maximal value. The intraindividual coefficients of variation of P. PImax and sniff Pes were 11. 2 % and 6. 0 %, respectively. <b>Measures</b> <b>of</b> <b>agreement</b> showed no significant discrepancies between the mean P. PImax and mean sniff Pes (0. 29 kPa, p = 0. 49). There was a significant correlation (r = 0. 57,...|$|R
40|$|Commonly in {{geomorphology}} measurements {{by different}} methods are compared {{to see how}} far they agree (i. e. are equal), as are predictions from models and corresponding observations. Such assessment usually employs scatter plots, correlation and possibly regression. More appropriate and more effective methods include plotting differences versus means and summary by concordance correlation and other <b>measures</b> <b>of</b> <b>agreement.</b> These methods, some new to geomorphology, are explained and discussed with a variety of examples using fluvial, hillslope, glacial and coastal data...|$|R
30|$|Cell {{viability}} {{was expressed}} {{as a percentage of}} the control values. The intra-class correlation coefficient (ICC) and limits of agreement statistics (Bland and Altman 1986; Bland and Altman 1990; Bland and Altman 1995) were used to compare the scores (Deyo et al. 1991). The limits of agreement statistics were also used as a descriptive <b>measure</b> <b>of</b> <b>agreement.</b>|$|E
40|$|This Memorandum {{describes}} {{work done}} to improve the <b>measure</b> <b>of</b> <b>agreement</b> between a target one-third-octave spectrum and its reproduction via some remote sound source. Digital filters create limited bandwidth data which are modified and finally recombined to produce a new time history. Studies of certain fixed and rotary wing aircraft have shown mean errors significantly reduced by this technique. I...|$|E
40|$|We {{carry out}} a {{preliminary}} calculation of the glueball mass spectrum and the effective string tension in SU(2) lattice gauge theory with 't Hooft's twisted boundary conditions. We make semi-quantitative comparisons with recent analytic results in small volumes and with previous Monte Carlo calculations for periodic boundary conditions. We find a reasonable <b>measure</b> <b>of</b> <b>agreement</b> between the form of our results {{and those of the}} analytic calculations. © 1989...|$|E
30|$|AUC scale-corrected IDIFs by linear Deming regression, which models {{error in}} both variables, {{and by the}} Lin {{concordance}} coefficient [24], which provides a <b>measure</b> <b>of</b> absolute <b>agreement</b> between two estimates.|$|R
40|$|In remote sensing, {{thematic}} map comparison is often undertaken on a per-pixel basis and based upon <b>measures</b> <b>of</b> classification <b>agreement.</b> Here, the degree <b>of</b> <b>agreement</b> between two {{thematic map}}s, {{and so the}} difference between the pair, was evaluated through visual and quantitative analyses for two scenarios. Quantitative assessments were based on basic site-specific <b>measures</b> <b>of</b> <b>agreement</b> that are used widely in accuracy assessment (e. g. the overall percentage of pixels with the same class label in each of the two maps and the kappa coefficient <b>of</b> <b>agreement)</b> as well as an information theory based approach that allows the degree of mutual or shared information to be assessed even if different classification schemes have been used to produce the maps. The results indicated that in the first map comparison scenario, focused on labelling, there was a fair degree of correspondence between the maps but with an overall difference in information content of ? 42 %. In the second comparison scenario, focused on change in time, considerable change had occurred with a change in class label for ? 42 % of the pixels. It was also apparent that global assessments masked local scale changes...|$|R
40|$|There are few {{existing}} or {{widely known}} <b>measures</b> <b>of</b> <b>agreement</b> applicable when data is nominal or categorical. Most such coefficients are applicable only when judges classify objects or subjects {{into a single}} category. A wider range of applications, including those where judges (1) place probabilities on subjects belonging to mutually exclusive and exhaustive nominal categories, or (2) rank order the applicability of categories to subjects, is desirable. A generalized ANOVA model is presented which allows the estimation of various reliability coefficients of interest for all classification tasks described. (Author) ...|$|R
