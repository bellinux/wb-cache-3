151|10000|Public
25|$|Standard deviation: {{the square}} {{root of the}} variance, and hence another <b>measure</b> <b>of</b> <b>dispersion.</b>|$|E
25|$|The {{denominator}} is a <b>measure</b> <b>of</b> <b>dispersion.</b> Replacing the denominator {{with the}} standard deviation we obtain the nonparametric skew.|$|E
25|$|L-estimators {{can also}} be used as {{statistics}} in their own right – for example, the median is a measure of location, and the IQR is a <b>measure</b> <b>of</b> <b>dispersion.</b> In these cases, the sample statistics can act as estimators of their own expected value; for example, the sample median is an estimator of the population median.|$|E
5000|$|The {{following}} statistical <b>measures</b> <b>of</b> <b>dispersion</b> <b>of</b> {{the sample}} ...|$|R
40|$|The second {{moment of}} a random {{variable}} attains {{the minimum value}} when taken around {{the mean of the}} random variable. This property, though elementary at first glance, in fact, characterizes the second moment among all continuous <b>measures</b> <b>of</b> <b>dispersion.</b> A multivariate version of the result is proved. <b>Measures</b> <b>of</b> <b>dispersion</b> Variance Lehmann unbiasedness Loss functions Characterization...|$|R
5000|$|Qualitative variation, for {{a number}} <b>of</b> other <b>measures</b> <b>of</b> <b>dispersion</b> in nominal {{variables}} ...|$|R
25|$|In {{addition}} to being a component of every statistic that uses all elements of the sample, the sample extrema are important parts of the range, a <b>measure</b> <b>of</b> <b>dispersion,</b> and mid-range, a measure of location. They also realize the maximum absolute deviation: {{one of them is the}} furthest point from any given point, particularly a measure of center such as the median or mean.|$|E
25|$|The {{expected}} value {{is a key}} aspect of how one characterizes a probability distribution; it is one type of location parameter. By contrast, the variance is a <b>measure</b> <b>of</b> <b>dispersion</b> of the possible values of the random variable around the {{expected value}}. The variance itself is {{defined in terms of}} two expectations: it is the expected value of the squared deviation of the variable's value from the variable's expected value.|$|E
2500|$|... κ is {{a measure}} of {{concentration}} (a reciprocal <b>measure</b> <b>of</b> <b>dispersion,</b> so 1/κ is analogous to σ2).|$|E
50|$|Some <b>measures</b> <b>of</b> <b>dispersion</b> have {{specialized}} purposes, {{among them}} the Allan variance and the Hadamard variance.|$|R
40|$|The {{first section}} of this paper focusses on the {{descriptive}} properties <b>of</b> two <b>measures</b> <b>of</b> <b>dispersion</b> for nominal variables, the <b>measure</b> <b>of</b> entropy and a new positional measure (which positions a given distribution between the two extremes, the distributions with minimal and maximal dispersion). The two different ways <b>of</b> <b>measuring</b> <b>dispersion</b> are compared graphically. In the following section, estimators for both measures are derived and it is shown that these estimators are asymptotically unbiased and asymptotically normally distributed. Finally approximative tests and confidence intervals are established. <b>Measures</b> <b>of</b> <b>dispersion,</b> nominal variables, estimators, approximative tests and confidence intervals...|$|R
5000|$|... "On <b>Measures</b> <b>of</b> <b>Dispersion</b> for a Finite Distribution." [...] Journal of the American Statistical Association 38, no. 223 (September 1943): 346-352.|$|R
2500|$|Leik's <b>measure</b> <b>of</b> <b>dispersion</b> (D) is {{one such}} index. Let there be K {{categories}} and let p'i be f'i/N where f'i is the number in the ith category and let the categories be arranged in ascending order. Let ...|$|E
2500|$|Material {{dispersion}} {{is often}} characterised by the Abbe number, which gives a simple <b>measure</b> <b>of</b> <b>dispersion</b> {{based on the}} index of refraction at three specific wavelengths. Waveguide dispersion {{is dependent on the}} propagation constant. Both kinds of dispersion cause changes in the group characteristics of the wave, the features of the wave packet that change with the same frequency as the amplitude of the electromagnetic wave. [...] "Group velocity dispersion" [...] manifests as a spreading-out of the signal [...] "envelope" [...] of the radiation and can be quantified with a group dispersion delay parameter: ...|$|E
5000|$|Standard deviation: {{the square}} {{root of the}} variance, and hence another <b>measure</b> <b>of</b> <b>dispersion.</b>|$|E
5000|$|Other <b>measures</b> <b>of</b> <b>dispersion</b> are dimensionless. In other words, {{they have}} no units even if the {{variable}} itself has units. These include: ...|$|R
30|$|As {{mentioned}} earlier, statistical parameters such as <b>measures</b> <b>of</b> <b>dispersion</b> {{have been}} utilized in {{setting of the}} three thresholds in TTSD. The same is discussed below.|$|R
2500|$|... where κ is the kurtosis, var (...) is the {{variance}} and E (...) is the expectation operator. The kurtosis {{can now be}} seen to be a <b>measure</b> <b>of</b> the <b>dispersion</b> <b>of</b> Z2 around its expectation. Alternatively it can {{be seen to be}} a <b>measure</b> <b>of</b> the <b>dispersion</b> <b>of</b> Z around +1 and−1. κ attains its minimal value in a symmetric two-point distribution. In terms of the original variable X, the kurtosis is a <b>measure</b> <b>of</b> the <b>dispersion</b> <b>of</b> X around the two values μ±σ.|$|R
5000|$|CV (coefficient of variation), {{which is}} a {{normalized}} <b>measure</b> <b>of</b> <b>dispersion</b> of a probability distribution.|$|E
5000|$|... κ is {{a measure}} of {{concentration}} (a reciprocal <b>measure</b> <b>of</b> <b>dispersion,</b> so 1/κ is analogous to σ2).|$|E
5000|$|The {{circular}} standard deviation, {{which is}} a useful <b>measure</b> <b>of</b> <b>dispersion</b> for the wrapped Normal distribution and its close relative, the von Mises distribution is given by: ...|$|E
5000|$|... #Caption: Statistics {{distributions}} {{obtained from}} Simon Newcomb {{speed of light}} dataset obtained through bootstrapping: the final result differs between the standard deviation and the median absolute deviation (both <b>measures</b> <b>of</b> <b>dispersion)</b> distributions.|$|R
2500|$|One {{reason for}} the use of the {{variance}} in preference to other <b>measures</b> <b>of</b> <b>dispersion</b> is that the variance of the sum (or the difference) of uncorrelated random variables is the sum of their variances: ...|$|R
5000|$|... where X is {{a random}} variable, μ is {{the mean and}} σ is the {{standard}} deviation. Then [...] where κ is the kurtosis, var (...) is the variance and E (...) is the expectation operator. The kurtosis can now {{be seen to be}} a <b>measure</b> <b>of</b> the <b>dispersion</b> <b>of</b> Z2 around its expectation. Alternatively it can be seen to be a <b>measure</b> <b>of</b> the <b>dispersion</b> <b>of</b> Z around +1 and −1. κ attains its minimal value in a symmetric two-point distribution. In terms of the original variable X, the kurtosis is a <b>measure</b> <b>of</b> the <b>dispersion</b> <b>of</b> X around the two values μ ± σ.|$|R
50|$|The {{appropriate}} statistic {{depends on}} the level of measurement. For nominal variables, a frequency table and a listing of the mode(s) is sufficient. For ordinal variables the median can be calculated as a measure of central tendency and the range (and variations of it) as a <b>measure</b> <b>of</b> <b>dispersion.</b> For interval level variables, the arithmetic mean (average) and standard deviation are added to the toolbox and, for ratio level variables, we add the geometric mean and harmonic mean as measures of central tendency and the coefficient of variation as a <b>measure</b> <b>of</b> <b>dispersion.</b>|$|E
5000|$|Most {{measures}} of dispersion {{have the same}} units as the quantity being measured. In other words, if the measurements are in metres or seconds, so is the <b>measure</b> <b>of</b> <b>dispersion.</b> Examples of dispersion measures include: ...|$|E
5000|$|Leik's <b>measure</b> <b>of</b> <b>dispersion</b> (D) is {{one such}} index. Let there be K {{categories}} and let pi be fi/N where fi is the number in the ith category and let the categories be arranged in ascending order. Let ...|$|E
30|$|The basic aim of any {{detection}} {{process is}} to separate the noisy pixels from the noise-free pixels {{so that they can}} be replaced with a suitable pixel value in the filtering stage. Various existing methods for image processing have utilized some or the other <b>measures</b> <b>of</b> <b>dispersion</b> to identify the outliers. Awad [2] has demonstrated that the detection process can be based on finding the optimum direction by calculating the standard deviation of different directions in the filtering window. The <b>measures</b> <b>of</b> <b>dispersion</b> used in this proposed algorithm for setting different thresholds are the mean, the standard deviation, and the quartile. The reason for choosing three levels of thresholds is explained below.|$|R
40|$|<b>Measures</b> <b>of</b> <b>dispersion</b> for corpus data: an overview, a suggestion, and a {{research}} program II <b>Measures</b> <b>of</b> <b>dispersion</b> for corpus data: an overview, a suggestion, and {{a research}} program II [name anonymized] [affiliation anonymized] In order to adjust observed frequencies of occurrence, {{previous studies have}} suggested a variety <b>of</b> <b>measures</b> <b>of</b> <b>dispersion</b> and adjusted frequencies. In part I of this article, I first summarily reviewed many <b>of</b> these <b>measures</b> {{as well as a}} variety of their shortcomings and then suggested an alternative measure, DP, for deviation of proportions, which I argued to be conceptually simpler, {{but at the same time}} more versatile than many competing measures. I then exemplified this measure on the basis of word frequency data on co-occurrence data of words and construction/patterns. However, in spite of the advantages of DP and in spite of its relevance for virtually all corpus-linguistic work, dispersion is still a very much under-researched topic: to the best of my knowledge, there is not a single study investigating how different measures compare to each other when applied to large datasets. The present article therefore is largely programmatic and exploratory: it sketches out a research program for the investigation <b>of</b> <b>measures</b> <b>of</b> <b>dispersion</b> and adjusted frequencies and takes some initial steps itself. More specifically, this paper addresses the issues of the integration <b>of</b> frequencies and <b>dispersion</b> <b>measures</b> and the quantitative comparison <b>of</b> <b>dispersion</b> <b>measures</b> and adjusted frequencies. Finally, the paper makes available a few online resources that will hopefully stimulate more research in this central area of corpus-linguistic methodology and help other researchers go beyond the first programmatic steps taken here. Key words corpus, frequency of occurrence, frequency <b>of</b> co-occurrence, <b>dispersion,</b> words, constructions/patterns, collocations, collostructions 1. A research program for dispersions and adjusted frequencies 1. ...|$|R
5000|$|... a <b>measure</b> <b>of</b> {{statistical}} <b>dispersion</b> {{like the}} standard deviation ...|$|R
50|$|When {{writing it}} as , {{it is easier}} to see that the {{numerator}} is the average of the upper and lower quartiles (a measure of location) minus the median while the denominator is (Q3-Q1)/2 which (for symmetric distributions) is the MAD <b>measure</b> <b>of</b> <b>dispersion.</b>|$|E
5000|$|Since {{observed}} price changes {{do not follow}} Gaussian distributions, others such as the Lévy distribution are often used. These can capture attributes such as [...] "fat tails".Volatility is a statistical <b>measure</b> <b>of</b> <b>dispersion</b> around the average of any random variable such as market parameters etc.|$|E
5000|$|Entropy: While {{the entropy of}} a {{discrete}} variable is location-invariant and scale-independent, and therefore not a <b>measure</b> <b>of</b> <b>dispersion</b> in the above sense, the entropy of a continuous variable is location invariant and additive in scale: If Hz is the entropy of continuous variable z and y=ax+b, then Hy=Hx+log(a).|$|E
5000|$|Abbe number, <b>measure</b> <b>of</b> glass <b>dispersion</b> defined using Fraunhofer lines ...|$|R
2500|$|Qualitative {{variation}} – other <b>measures</b> <b>of</b> statistical <b>dispersion</b> for nominal distributions ...|$|R
50|$|Statistics of the {{distribution}} of deviations are used as <b>measures</b> <b>of</b> statistical <b>dispersion.</b>|$|R
