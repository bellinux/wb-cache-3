35|46|Public
2500|$|On March 9, 1949, Shannon {{presented}} a paper called “Programming a Digital Computer for Playing Chess.” [...] The paper was {{presented a}}t the National Institute for Radio Engineers Convention in New York. [...] He described how to program a computer to play chess based on position scoring and move selection. [...] He proposed basic strategies for restricting the number of possibilities to be considered in a game of chess. In March 1950, published in Philosophical Magazine, Series 7, Vol. 41 (No. 314, March 1950). This was the first article on computer chess. [...] His process for having the computer decide on which move to make was a <b>minimax</b> <b>procedure,</b> based on an evaluation function of a given chess position. Shannon gave a rough example of an evaluation function in which the value of the black position was subtracted from that of the white position. Material was counted according to the usual chess piece relative value (1 point for a pawn, 3 points for a knight or bishop, 5 points for a rook, and 9 points for a queen). He considered some positional factors, subtracting ½ point for each doubled pawn, backward pawn, and isolated pawn. Another positional factor in the evaluation function was mobility, adding 0.1 point for each legal move available. Finally, he considered checkmate to be the capture of the king, and gave the king the artificial value of 200 points. Quoting from the paper: ...|$|E
5000|$|On March 9, 1949, Claude Shannon, a {{research}} worker at Bell Telephone Laboratories in New Jersey, presented a paper called “Programming a Digital Computer for Playing Chess.” The paper {{was presented at}} the National Institute for Radio Engineers Convention in New York. He described how to program a computer to play chess based on position scoring and move selection. He proposed basic strategies for restricting the number of possibilities to be considered in a game of chess. In March 1950, published in Philosophical Magazine, Series 7, Vol. 41 (No. 314, March 1950). This was the first article on computer chess. [...] His process for having the computer decide on which move to make was a <b>minimax</b> <b>procedure,</b> based on an evaluation function of a given chess position. Shannon gave a rough example of an evaluation function in which the value of the black position was subtracted from that of the white position. Material was counted according to the usual chess piece relative value (1 point for a pawn, 3 points for a knight or bishop, 5 points for a rook, and 9 points for a queen). He considered some positional factors, subtracting ½ point for each doubled pawn, backward pawn, and isolated pawn. Another positional factor in the evaluation function was mobility, adding 0.1 point for each legal move available. Finally, he considered checkmate to be the capture of the king, and gave the king the artificial value of 200 points. Quoting from the paper: ...|$|E
40|$|A new voting {{procedure}} for electing committees, called the <b>minimax</b> <b>procedure,</b> is described. Based on approval voting (AV), it chooses {{the committee that}} minimizes the maximum “Hamming distance ” to all voters (minimax outcome). Such an outcome may be diametrically opposed to the outcome obtained from aggregating votes in the usual manner, which minimizes {{the sum of the}} Hamming distances to all voters (minisum outcome). Computer simulation is used to assess how much minimax and minisum outcomes tend to diverge. The manipulability of the <b>minimax</b> <b>procedure</b> is also investigated. The <b>minimax</b> <b>procedure</b> is applied to the 2003 Game Theory Society (GTS) election of a council of 12 new members from a list of 24 candidates. The 9 th and 10 th biggest vote-getters would have been displaced by the 16 th and 17 th biggest vote-getters if the <b>minimax</b> <b>procedure</b> had been used; there would have been more substantial differences if the size of the council had been made endogenous rather than being fixed at 12. It is argued that when few if any voters cast identical AV ballots, as was true in the GTS election (there were 224 ≈ 16. 8 million possible ballots), a minimax committee will better represent the interests of all voters than a minisum committee. 3 A <b>Minimax</b> <b>Procedure</b> for Electing Committees 1 1...|$|E
40|$|AbstractFor {{nonlinear}} Schrödinger equations in {{the entire}} space we present new results on invariant sets of the gradient flows of the corresponding variational functionals. The structure of the invariant sets will be built into <b>minimax</b> <b>procedures</b> to construct nodal type bound state solutions of nonlinear Schrödinger type equations...|$|R
40|$|AbstractIn this paper, we {{consider}} the existence of positive, negative and sign-changing solutions for some fourth order semilinear elliptic boundary value problems. We present new results on invariant sets of the gradient flows of the corresponding variational functionals. The structure of the invariant sets will be built into <b>minimax</b> <b>procedures</b> to construct the sign-changing solutions...|$|R
40|$|The {{estimation}} {{problem of}} the quantiles of a normal distribution with both parameters unknown, is considered. We construct a class of <b>minimax</b> <b>procedures</b> each of which improves upon the traditional (best equivariant) estimator of a quantile different from the median. For this purpose a differential inequality and a family of its solutions is found. Normal quantiles estimation quadratic loss minimaxness equivariant procedures admissibility differential inequality...|$|R
40|$|A new voting {{procedure}} for electing committees, called the <b>minimax</b> <b>procedure,</b> is described. Based on approval balloting, it chooses {{the committee that}} minimizes the maximum Hamming distance to voters' ballots, where these ballots are weighted by their proximity to other voters' ballots. This minimax outcome may be diametrically opposed to the outcome obtained by aggregating approval votes in the usual manner, which minimizes {{the sum of the}} Hamming distances and is called the minisum outcome. The manipulability of these procedures, and their applicability when election outcomes are restricted in various ways, are also investigated. The <b>minimax</b> <b>procedure</b> is applied to the 2003 Game Theory Society election of a council of 12 new members from a list of 24 candidates. The composition of the council would have changed by 4 members; there would have been more substantial differences between minimax and minisum outcomes if the number of candidates to be elected had been endogenous rather than being fixed at 12. The <b>minimax</b> <b>procedure,</b> which renders central voters more influential but does not antagonize any voter too much, may produce a committee that better represents the interests of all voters than a minisum committee...|$|E
40|$|For non convex Hamiltonians, the {{viscosity}} {{solution and}} the more geometric minimax solution of the Hamilton-Jacobi equation do not coincide in general. They are nevertheless related: we show that iterating the <b>minimax</b> <b>procedure</b> during shorter and shorter time intervals one recovers the viscosity solution. Comment: 27 page...|$|E
30|$|Much {{interest}} has arisen in problems involving critical exponents, {{starting from the}} celebrated paper by Brezis and Nirenberg [1]. For example, Li and Zou [2] obtained infinitely many solutions with odd nonlinearity. Chen and Li [3] obtained the existence of infinitely many solutions by using <b>minimax</b> <b>procedure.</b> For more related results, we refer the interested readers to [4]–[10] and references therein.|$|E
5000|$|In the 20th century, {{interest}} was reignited by Abraham Wald's 1939 paper {{pointing out that}} the two central procedures of sampling-distribution-based statistical-theory, namely hypothesis testing and parameter estimation, are special cases of the general decision problem. Wald's paper renewed and synthesized many concepts of statistical theory, including loss functions, risk functions, admissible decision rules, antecedent distributions, Bayesian <b>procedures,</b> and <b>minimax</b> <b>procedures.</b> The phrase [...] "decision theory" [...] itself was used in 1950 by E. L. Lehmann.|$|R
40|$|The risk {{involved}} in a trial to compare two medical treatments is shared by patients who receive the inferior treatment during the experimental phase and those remaining after the experiment who might all receive the inferior treatment if the results are misleading. We consider the maximum of this risk {{with respect to the}} unknown probabilities of success and seek allocation rules that minimise this quantity, for a given total of patients. It needs extensive computations to find such <b>minimax</b> <b>procedures,</b> but there are simple and almost equally effactive allocation rules based on a truncated sequential probability ratio test...|$|R
40|$|AbstractThe paper {{presents}} an efficient solution to decision problems where direct partial {{information on the}} distribution of the states of nature is available, either by observations of previous repetitions of the decision problem or by direct expert judgements. To process this information we use a recent generalization of Walley’s imprecise Dirichlet model, allowing us also to handle incomplete observations or imprecise judgements, including missing data. We derive efficient algorithms and discuss properties of the optimal solutions with respect to several criteria, including Gamma-maximinity and E-admissibility. In the case of precise data and pure actions the former surprisingly leads us to a frequency-based variant of the Hodges–Lehmann criterion, which was developed in classical decision theory as a compromise between Bayesian and <b>minimax</b> <b>procedures...</b>|$|R
40|$|In {{this paper}} we compare a minisum and a <b>minimax</b> <b>procedure</b> as {{suggested}} by Brams et al. for selecting committees from a set of candidates. Using a general geometric framework as developed by Don Saari for preference aggregation, we show that antipodality of a unique maximin and a unique minisum winner can occur {{for any number of}} candidates larger than two. ...|$|E
30|$|Much {{interest}} has grown on problems involving critical exponents, {{starting from the}} celebrated paper by Brezis and Nirenberg [1]. For example, Li and Zou [2] obtained infinitely many solutions with odd nonlinearity. Chen and Li [3] obtained the existence of infinitely many solutions by using the <b>minimax</b> <b>procedure.</b> For more related results, we refer the interested reader to [4 – 9] and references therein.|$|E
40|$|Random minimaxing, {{introduced}} by Beal and Smith [1], {{is the process}} of using a random static evaluation function for scoring the leaf nodes of a full width game tree and then computing the best move using the standard <b>minimax</b> <b>procedure.</b> The experiments carried out by Beal and Smith, using random minimaxing in Chess, showed that the strength of play increases as the depth of the lookahead is increased. We investigate random minimaxing from a combinatorial point of view in order {{to gain a better understanding}} of the utility of the <b>minimax</b> <b>procedure</b> and thus obtain a theoretical justification for the results of Beal and Smith's experiments. The concept of domination is central to our theory. Intuitively, a move by white dominates another move when this move gives less choice for black when it is black's turn to move, and subsequently more choice for white when it is white's turn to move. We view domination as a measure of mobility and show that when one move dominates another then its pro [...] ...|$|E
40|$|In {{past two}} decades, the Japanese {{successful}} experience in using Just-In-Time (JIT) production {{has received a}} great deal of attention. The underlying goal of JIT is to eliminate waste. This can be achieved through various efforts, such as shortening lead time and improving quality. In this paper, we investigate the impact of investing in quality improvement and lead time reduction on the integrated vendor-buyer inventory model with partial backorders. We assume that the lead time demand probability distribution is unknown, while the mean and standard deviation are known and finite. The <b>minimax</b> distribution-free <b>procedure</b> is applied to solve this problem. Also, numerical examples are given to illustrate the results. Integrated inventory model, quality improvement, lead time reduction, partial backorders, <b>minimax</b> distribution-free <b>procedure...</b>|$|R
40|$|We {{propose a}} <b>minimax</b> scaling <b>procedure</b> for second order {{polynomial}} matrices {{that aims to}} minimize the backward errors incurred in solving a particular linearized generalized eigenvalue problem. We give numerical examples to illustrate that it can significantly improve the backward errors of the computed eigenvalue-eigenvector pairs...|$|R
40|$|Let X|μ∼Np(μ,vxI) and Y|μ∼Np(μ,vyI) be {{independent}} p-dimensional multivariate normal vectors with common unknown mean μ. Based on only observing X=x, {{we consider the}} problem of obtaining a predictive density p̂(y|x) for Y that is close to p(y|μ) as measured by expected Kullback–Leibler loss. A natural procedure for this problem is the (formal) Bayes predictive density p̂U(y|x) under the uniform prior πU(μ) ≡ 1, which is best invariant and minimax. We show that any Bayes predictive density will be minimax if it is obtained by a prior yielding a marginal that is superharmonic or whose square root is superharmonic. This yields wide classes of <b>minimax</b> <b>procedures</b> that dominate p̂U(y|x), including Bayes predictive densities under superharmonic priors. Fundamental similarities and differences with the parallel theory of estimating a multivariate normal mean under quadratic loss are described...|$|R
40|$|There are {{two kinds}} of {{solutions}} of the Cauchy problem of first order, the viscosity solution and the more geometric minimax solution and in general they are different. The aim {{of this article is to}} show how they are related: iterating the <b>minimax</b> <b>procedure</b> during shorter and shorter time intervals one approaches the viscosity solution. This can be considered as an extension to the contact framework of the result of Q. Wei in the symplectic case...|$|E
40|$|We {{prove the}} {{existence}} of a minimal action nodal solution for the quadratic Choquard equation (Formula presented), where Iα is the Riesz potential of order α ∈ (0,N). The solution is constructed as the limit of minimal action nodal solutions for the nonlinear Choquard equations (Formula presented) when p (symbol found) 2. The existence of minimal action nodal solutions for p > 2 can be proved using a variational <b>minimax</b> <b>procedure</b> over a Nehari nodal set. No minimal action nodal solutions exist when p < 2...|$|E
40|$|We study an {{eigenvalue}} {{problem in}} the framework of double phase variational integrals and we introduce a sequence of nonlinear eigenvalues by a <b>minimax</b> <b>procedure.</b> We establish a continuity result for the nonlinear eigenvalues with respect to the variations of the phases. Furthermore, we investigate the growth rate of this sequence and get a Weyl-type law consistent with the classical law for the $p$-Laplacian operator when the two phases agree. Comment: 42 pages, typos corrected, final version, to appear in Ann. Mat. Pura App...|$|E
40|$|Let X| μ∼ N_p(μ,v_xI) and Y| μ∼ N_p(μ,v_yI) be {{independent}} p-dimensional multivariate normal vectors with common unknown mean μ. Based on only observing X=x, {{we consider the}} problem of obtaining a predictive density p̂(y| x) for Y that is close to p(y| μ) as measured by expected Kullback [...] Leibler loss. A natural procedure for this problem is the (formal) Bayes predictive density p̂_U(y| x) under the uniform prior π_U(μ) ≡ 1, which is best invariant and minimax. We show that any Bayes predictive density will be minimax if it is obtained by a prior yielding a marginal that is superharmonic or whose square root is superharmonic. This yields wide classes of <b>minimax</b> <b>procedures</b> that dominate p̂_U(y| x), including Bayes predictive densities under superharmonic priors. Fundamental similarities and differences with the parallel theory of estimating a multivariate normal mean under quadratic loss are described. Comment: Published at [URL] in the Annals of Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|R
40|$|The paper {{presents}} an efficient solution to decision problems where direct partial {{information on the}} distribution of the states of nature is available, either by observations of previous repetitions of the decision problem or by direct expert judgements. To process this information we use a recent generalization of Walley’s imprecise Dirichlet model, allowing us also to handle incomplete observations or imprecise judgements, including missing data. We derive efficient algorithms and discuss properties of the optimal solutions with respect to several criteria, including Gammamaximinity and E-admissibility. In the case of precise data and pure actions the former surprisingly leads us to a frequency-based variant of the Hodges-Lehmann criterion, which was developed in classical decision theory as a compromise between Bayesian and <b>minimax</b> <b>procedures.</b> We also briefly glance at the representation invariance principle (RIP) in the context of decision making. Key words: Belief functions, coarse data, decision making, E-admissibility, imprecise Dirichlet model (IDM), imprecise probabilities, incomplete data, interval probability, interval statistical models, missing data, predictive probabilities, set-valued statistical data...|$|R
40|$|<b>Minimax</b> {{estimation}} <b>procedures</b> for {{the mean}} vector of a distribution on a compact set under squared error type loss functions are considered. In particular, a Dirichlet process prior {{is used to}} show that a linear function of. X is a minimax estimator in the class of all measurable estimators and all possible distributions. This effort extends some earlier work of BOHLMANN to a more general setting...|$|R
40|$|We {{prove the}} {{existence}} of a minimal action nodal solution for the quadratic Choquard equation -Δ u + u = (I_α∗ |u|^ 2) u in R^N, where I_α is the Riesz potential of order α∈(0,N). The solution is constructed as the limit of minimal action nodal solutions for the nonlinear Choquard equations -Δ u + u = (I_α∗ |u|^p) |u|^p- 2 u in R^N when p 2. The existence of minimal action nodal solutions for p> 2 can be proved using a variational <b>minimax</b> <b>procedure</b> over Nehari nodal set. No minimal action nodal solutions exist when p< 2. Comment: 11 page...|$|E
40|$|AbstractThis work {{studies the}} notion of “minimax invariant” of a G-space. Instead of earlier ideas of the G-genus, G-geometrical index, or G-cohomological indexes [1, 2, 8, 15] we develop a purely topological construction. The main idea follows the {{original}} approach of Lusternik and Schnirelman. It is very simple, and has all the properties necessary for the <b>minimax</b> <b>procedure.</b> By using the G-category we can extend the classical Krasnosielski theorem. Namely, we show that the orbit space X/G has Lusternik-Schnirelman category equal to n + 1 if a finite group G acts freely on a Z-cohomological n-sphere X...|$|E
40|$|Abstract: This {{paper is}} {{concerned}} with existence of bound states for Schrödinger systems which have appeared as several models from mathematical physics. We establish multiplicity results of bound states for both small and large interactions. This is done by different approaches depending upon the sizes of the interaction parameters in the systems. For small interactions we give {{a new approach to}} deal with multiple bound states. The novelty of our approach lies in establishing a certain type of invariant sets of the associated gradient flows. For large interactions we use a <b>minimax</b> <b>procedure</b> to distinguish solutions by analyzing their Morse indices. 1...|$|E
40|$|Approved {{for public}} release; {{distribution}} unlimited. Single-period inventory {{problems such as}} the newspaper boy problem having quadratic cost functions for either or both shortages and overages are examined to determine the optimal order level under various principles of choice such as minimum expected cost, aspiration level, and <b>minimax</b> regret. <b>Procedures</b> for finding the optimum order levels are developed for both continuous and discrete demand patterns. [URL] Republic of Korea Arm...|$|R
40|$|Abstract: The linear {{stochastic}} differential {{system with}} uncertain intensity of noises in dynamics and observations is considered. For this system the <b>minimax</b> filtering <b>procedure</b> is proposed. The filter is optimal {{in terms of}} integral criterion. The obtained filtering equations depend on the dual optimization problem solution, which {{can be obtained by}} means of provided numerical procedure. The convergence of the numerical procedure is also considered. Some numerical results are described. Copyright c© 2005 IFA...|$|R
40|$|Abstract. The <b>minimax</b> fitting <b>procedure</b> is {{employed}} in some robust parameter estimation {{in the normal}} linear model. In this paper, we shall reformulate the procedure by using the deviance residual measures {{instead of the usual}} residual measures for the robust fitting procedure. This extends the range of application from the normal distribution to a more general exponential family of distributions. Special cases of the exact solution for the single parameter case and its algorithm will be discussed...|$|R
40|$|Abstract. In 1985, for {{detecting}} {{a change in}} distribution Pollak introduced a minimax criterion and a randomized Shiryaev-Roberts procedure that starts off a random variable sampled from the quasistationary distribution of the Shiryaev-Roberts statistic. Pollak proved that this procedure is asymptotically almost optimal as the mean time to false alarm becomes large. The question whether Pollak’s procedure is strictly minimax has been open {{for more than two}} decades. We provide a counterexample which shows that Pollak’s procedure is not optimal {{and that there is a}} strictly <b>minimax</b> <b>procedure</b> which is nothing but the Shiryaev-Roberts procedure that starts with a specially designed deterministic point...|$|E
40|$|In this paper, {{we propose}} a new simple design {{technique}} for constructing digital PID compensators for the robust control of sampled-data systems, {{based on a}} <b>minimax</b> <b>procedure.</b> First, we formulate the robust control problems for sampled-data systems, including digital PID compensators and disturbances of system parameters. Next, we propose the minimax-based design method with the PID compensators for the robust control problems of sampled-data systems, where we develop a new minimax algorithm which is reliable, stable and simple in computation. Finally, we apply the proposed method to a realistic problem in order to demonstrate its effectiveness...|$|E
40|$|ABSTRACT {{function}} {{is a function}} which estimates what resulting value Two-person, perfect information, constant sum games have been studied in Artificial Intelligence. This paper opens {{up the issue of}} playing n-person games and proposes a pro-cedure for constant sum or non-constant sum games. It is proved that a procedure, max”, locates an equilibrium point given the entire game tree. The <b>minimax</b> <b>procedure</b> for 2 -person games using look ahead finds a saddle point of approximations, while maxn finds an equilibrium point of the values of the evaluation function for n-person games using look ahead. Maz ” is further analyzed with respect to some pruning schemes. ...|$|E
40|$|Let X | µ ∼ Np(µ, vxI) and Y | µ ∼ Np(µ, vyI) be {{independent}} p-dimensional multivariate normal vectors with common unknown mean µ, and let p(x|µ) and p(y |µ) denote the conditional densities of X and Y. Based on only observing X = x, {{we consider the}} problem of obtaining a predictive density p(y | x) for Y that is close to p(y | µ) as measured by expected Kullback-Leibler loss. The natural straw man for this problem is the Bayesian predictive density pU (y |x) under the uniform prior piU (µ) ≡ 1, which is best invariant and minimax. We show that any Bayes predictive density will be minimax if it is obtained by a prior yielding a marginal that is superharmonic or whose square root is superharmonic. This yields wide classes of <b>minimax</b> <b>procedures</b> that dominate pU (y | x) including Bayes predictive densities under superharmonic priors. Such procedures can be constructed to adaptively shrink pU (y | x) towards an arbitrary point or subspace, and can be further combined for minimax multiple shrinkage prediction. Fundamental similarities and differences with the parallel theory of estimating a multivariate normal mean under quadratic loss are described throughout...|$|R
40|$|AbstractThe {{efficiency}} of the αβ-algorithm as a <b>minimax</b> search <b>procedure</b> {{can be attributed to}} its effective pruning at the so-called cut-nodes; ideally only one move is examined there to establish the minimax value. This paper explores the benefits of investing additional search effort at cut-nodes by also expanding some of the remaining moves. Our results show a strong correlation between the number of promising move alternatives at cut-nodes and a new principal variation emerging. Furthermore, a new forward-pruning method is introduced that uses this additional information to ignore potentially futile subtrees. We also provide experimental results with the new pruning method in the domain of chess...|$|R
40|$|We {{study the}} problem of {{detection}} of a high-dimensional signal function in the white Gaussian noise model. As well as a smoothness assumption on the signal function, we assume an additive sparse condition on the latter. The detection problem is {{expressed in terms of}} a nonparametric hypothesis testing problem and it is solved according to the asymptotical minimax approach. The <b>minimax</b> test <b>procedures</b> are adaptive in the sparsity parameter for high sparsity case. We extend to the functional case the known results in the detection of sparse high-dimensional vectors. In particular, our asymptotic detection boundaries are derived from the same asymptotic relations as in the vector case...|$|R
