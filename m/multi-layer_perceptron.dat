1522|541|Public
25|$|Polikar, R., Udpa, L., Udpa, S., and Honavar, V. (2001). Learn++: An Incremental Learning Algorithm for <b>Multi-Layer</b> <b>Perceptron</b> Networks. IEEE Transactions on Systems, Man, and Cybernetics. Vol. 31, No. 4. pp.497–508.|$|E
25|$|A {{sufficiently}} {{complex and}} accurate {{model of the}} neurons is required. A traditional artificial neural network model, for example <b>multi-layer</b> <b>perceptron</b> network model, is not considered as sufficient. A dynamic spiking neural network model is required, which reflects that the neuron fires only when a membrane potential reaches a certain level. It {{is likely that the}} model must include delays, non-linear functions and differential equations describing the relation between electrophysical parameters such as electrical currents, voltages, membrane states (ion channel states) and neuromodulators.|$|E
2500|$|Each block {{consists}} of a simplified <b>multi-layer</b> <b>perceptron</b> (MLP) with a single hidden layer. The hidden layer h has logistic sigmoidal units, and the output layer has linear units. Connections between these layers are represented by weight matrix U; input-to-hidden-layer connections have weight matrix W. Target vectors t form the columns of matrix T, and the input data vectors x form the columns of matrix X. The matrix of hidden units is [...] Modules are trained in order, so lower-layer weights W are known at each stage. The function performs the element-wise logistic sigmoid operation. Each block estimates the same final label class y, and its estimate is concatenated with original input X to form the expanded input for the next block. Thus, the input to the first block contains the original data only, while downstream blocks' input adds the output of preceding blocks. Then learning the upper-layer weight matrix U given other weights in the network can be formulated as a convex optimization problem: ...|$|E
40|$|We {{investigate}} the network complexity of <b>multi-layered</b> <b>perceptrons</b> for solving exactly a given problem. We limit our study {{to the class}} of combinatorial optimization problems. It is shown how these problems can be reformulated as binary classification problems {{and how they can}} be solved by <b>multi-layered</b> <b>perceptrons.</b> Keywords: Combinatorial Optimization, Classification, Complexity, Exact Network Configurations, <b>Multi-Layered</b> <b>Perceptrons,</b> Neural Networks...|$|R
5000|$|... scikit-neuralnetwork - <b>Multi-layer</b> <b>perceptrons</b> as a wrapper for Pylearn2 ...|$|R
30|$|<b>Multi-layered</b> <b>perceptrons</b> {{with more}} than three layers, which use more hidden layers [21, 26]. <b>Multi-layered</b> <b>perceptrons</b> {{correspond}} the input units to the output units by a specific nonlinear mapping [50]. The most important application of <b>multi-layered</b> <b>perceptrons</b> is their ability in function approximation [15]. From Kolmogorov existence theorem, we know that a three-layered perceptron with n(2 n +  1) nodes can compute any continuous function of n variables [22, 31]. The accuracy of the approximation depends {{on the number of}} neurons in the hidden layer and does not depend on the number of the hidden layers [27].|$|R
5000|$|... 1990 Phonetic {{classification}} {{and recognition}} using the <b>multi-layer</b> <b>perceptron</b> ...|$|E
50|$|The {{existence}} of this linear solution means that unlike <b>multi-layer</b> <b>perceptron</b> (MLP) networks, RBF networks have a unique local minimum (when the centers are fixed).|$|E
50|$|Fully {{connected}} layers connect every neuron in {{one layer}} to every neuron in another layer. It is in principle {{the same as}} the traditional <b>multi-layer</b> <b>perceptron</b> neural network (MLP).|$|E
50|$|Torch: The Torch library {{can create}} complex {{machines}} like TDNN’s through combining several built-in <b>Multi-layer</b> <b>Perceptrons</b> (MLP) modules.|$|R
40|$|Because of the {{combination}} of classification, association, adaptation, and pattern recognition capabilities, neural networks are shown to be suitable for solving problems in production planning with uncertain and non-stationary demand. We demonstrate that a properly designed and trained <b>multi-layered</b> <b>perceptron</b> outperforms traditional algorithms for the rolling horizon version of the dynamic lotsizing problem. Formal arguments are supported by numerical experiments. Keywords: Lotsizing, <b>Multi-Layered</b> <b>Perceptrons,</b> Neural Networks, Pattern Recognition, Production Planning, Uncertainty...|$|R
40|$|This paper {{investigates the}} {{possibility}} of describing vowels phonetically using an automated method. Models of the phonetic dimensions of the vowel space are built using two <b>multi-layer</b> <b>perceptrons</b> trained using eight cardinal vowels. The paper aims to improve the positioning of vowels in the open-close dimension by experimenting with a parameter in the model # which is the parameter which controls {{the slope of the}} sigmoid function employed in the <b>multi-layer</b> <b>perceptrons.</b> 1...|$|R
5000|$|Polikar, R., Udpa, L., Udpa, S., and Honavar, V. (2001). Learn++: An Incremental Learning Algorithm for <b>Multi-Layer</b> <b>Perceptron</b> Networks. IEEE Transactions on Systems, Man, and Cybernetics. Vol. 31, No. 4. pp. 497-508.|$|E
50|$|Generally, a Recurrent <b>Multi-Layer</b> <b>Perceptron</b> (RMLP) {{consists}} of cascaded subnetworks, {{each of which}} contains of multiple layers of nodes. Each of these subnetworks is feed-forward except for the last layer, which can have feedback connections. Each of these subnets is connected only by feed forward connections.|$|E
50|$|The {{universal}} approximation theorem for {{neural networks}} states that every continuous function that maps intervals of real numbers to some output interval of real numbers can be approximated arbitrarily closely by a <b>multi-layer</b> <b>perceptron</b> {{with just one}} hidden layer. This result holds {{for a wide range}} of activation functions, e.g. for the sigmoidal functions.|$|E
5000|$|RPROP&minus; {{is defined}} at Advanced Supervised Learning in <b>Multi-layer</b> <b>Perceptrons</b> [...] - [...] From Backpropagation to Adaptive Learning Algorithms. Backtracking {{is removed from}} RPROP+.|$|R
40|$|Neural {{networks}} as {{a general}} mechanism for learning and adaptation became increasingly popular in recent years. Mainly due {{to the development of}} the backpropagation learning procedure which allowed to train <b>Multi-Layer</b> <b>Perceptrons.</b> Unfortunately, back propagation is well-known for its particularly low learning speed. Thus, it is desirable to have more efficient learning mechanisms. In this paper a new learning algorithm for <b>multi-layer</b> <b>Perceptrons</b> is presented. It will be argued that by using a four-layer Perceptron instead of a three-layer Perceptron substantial improvements in the learning speed can be obtained. 1 Introduction In the past few years there was a strongly increasing interest in neural network-like models of computation ([5]). One of the most important algorithms in today's neural networks is the back propagation algorithm for <b>multi-layer</b> <b>Perceptrons.</b> Unfortunately, back propagation is very slow with respect to its convergence toward an unknown target classification fu [...] ...|$|R
40|$|Contents 1 Introduction 4 1. 1 Neural Networks................................ 4 1. 2 <b>Multi-Layer</b> <b>Perceptrons............................</b> 4 1. 3 <b>Multi-Layer</b> <b>Perceptrons</b> in PAW........................ 5 1. 4 Limitations, {{results on}} di#erent platforms.................. 6 1. 5 Organisation of this manual.......................... 6 1. 6 Acknowledgments................................ 6 2 Tutorial 7 2. 1 Classification from Ntuples........................... 7 2. 2 A note on {{defining}} examples from column-wise Ntuples........... 10 2. 3 Fitting a function on a 1 d or 2 d histogram.................. 11 2. 4 Reading and writing ASCII files........................ 12 2. 5 Using the modified vec/fit comman...|$|R
50|$|Nonlinear PCA (NLPCA) uses {{backpropagation}} {{to train}} a <b>multi-layer</b> <b>perceptron</b> (MLP) {{to fit to}} a manifold. Unlike typical MLP training, which only updates the weights, NLPCA updates both the weights and the inputs. That is, both the weights and inputs are treated as latent values. After training, the latent inputs are a low-dimensional representation of the observed vectors, and the MLP maps from that low-dimensional representation to the high-dimensional observation space.|$|E
50|$|The {{approach}} is {{strongly related to}} density networks which use importance sampling and a <b>multi-layer</b> <b>perceptron</b> to form a non-linear latent variable model. In the GTM the latent space is a discrete grid of points which {{is assumed to be}} non-linearly projected into data space. A Gaussian noise assumption is then made in data space so that the model becomes a constrained mixture of Gaussians. Then the model's likelihood can be maximized by EM.|$|E
50|$|A {{sufficiently}} {{complex and}} accurate {{model of the}} neurons is required. A traditional artificial neural network model, for example <b>multi-layer</b> <b>perceptron</b> network model, is not considered as sufficient. A dynamic spiking neural network model is required, which reflects that the neuron fires only when a membrane potential reaches a certain level. It {{is likely that the}} model must include delays, non-linear functions and differential equations describing the relation between electrophysical parameters such as electrical currents, voltages, membrane states (ion channel states) and neuromodulators.|$|E
40|$|Characterization of {{financial}} crisis with hybridHMC-MLP models Violent turbulences are often striking {{the financial markets}} and an Index of Market Shocks (IMS) was recently introduced in the attempt of quantifying these turbulences. Regime switching linear models have already been used in modelling the conditional volatility of returns. In this paper, we propose {{a description of the}} IMS with hybrid models integrating <b>multi-layer</b> <b>perceptrons</b> and hidden Markov chains. After studying the prediction performance of the models, we focus on the series separation and the index behaviour subject to hidden states. financial time series, hidden Markov models, <b>multi-layer</b> <b>perceptrons,</b> index of market shocks, financial crisis...|$|R
5000|$|NeuralBandit {{algorithm}}: In this algorithm several {{neural networks}} {{are trained to}} modelize the value of rewards knowing the context, and it uses a multi-experts approach to choose online the parameters of <b>multi-layer</b> <b>perceptrons.</b>|$|R
40|$|Abstract — This paper {{proposed}} a novel neural network model, named <b>multi-layer</b> <b>perceptrons</b> with embedded feature selection (MLPs-EFS), where feature selection is {{incorporated into the}} training procedure. Compared with the classical MLPs, MLPs-EFS add a preprocessing step where each feature of the samples is multiplied by the corresponding scaling factor. By applying a truncated Laplace prior to the scaling factors, feature selection is integrated {{as a part of}} MLPs-EFS. Moreover, a variant of MLPs-EFS, named EFS+MLPs is also given, which perform feature selection more flexibly. Application in cancer classification validates the effectiveness of the {{proposed a}}lgorithms. Key words — <b>Multi-layer</b> <b>perceptrons</b> (MLPs), neural networks, Laplace prior, feature selection, cancer classification. I...|$|R
50|$|Spiking neural {{networks}} (SNNs) {{fall into the}} third generation of neural network models, increasing the level of realism in a neural simulation.In addition to neuronal and synaptic state, SNNs also incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not fire at each propagation cycle (as it happens with typical <b>multi-layer</b> <b>perceptron</b> networks), but rather fire only when a membrane potential - an intrinsic quality of the neuron related to its membrane electrical charge - reaches a specific value. When a neuron fires, it generates a signal which travels to other neurons which, in turn, increase or decrease their potentials in accordance with this signal.|$|E
5000|$|Each block {{consists}} of a simplified <b>multi-layer</b> <b>perceptron</b> (MLP) with a single hidden layer. The hidden layer h has logistic sigmoidal units, and the output layer has linear units. Connections between these layers are represented by weight matrix U; input-to-hidden-layer connections have weight matrix W. Target vectors t form the columns of matrix T, and the input data vectors x form the columns of matrix X. The matrix of hidden units is [...] Modules are trained in order, so lower-layer weights W are known at each stage. The function performs the element-wise logistic sigmoid operation. Each block estimates the same final label class y, and its estimate is concatenated with original input X to form the expanded input for the next block. Thus, the input to the first block contains the original data only, while downstream blocks' input adds the output of preceding blocks. Then learning the upper-layer weight matrix U given other weights in the network can be formulated as a convex optimization problem: ...|$|E
5000|$|Although the {{perceptron}} initially seemed promising, it {{was quickly}} proved that perceptrons {{could not be}} trained to recognise many classes of patterns. This caused the field of neural network research to stagnate for many years, before it was recognised that a feedforward neural network with two or more layers (also called a multilayer perceptron) had far greater processing power than perceptrons with one layer (also called a single layer perceptron).Single layer perceptrons are only capable of learning linearly separable patterns; in 1969 a famous book entitled Perceptrons by Marvin Minsky and Seymour Papert showed {{that it was impossible}} for these classes of network to learn an XOR function. It is often believed that they also conjectured (incorrectly) that a similar result would hold for a <b>multi-layer</b> <b>perceptron</b> network. However, this is not true, as both Minsky and Papert already knew that multi-layer perceptrons were capable of producing an XOR function. (See the page on Perceptrons (book) for more information.) Three years later Stephen Grossberg published a series of papers introducing networks capable of modelling differential, contrast-enhancing and XOR functions. (The papers were published in 1972 and 1973, see e.g.:). Nevertheless, the often-miscited Minsky/Papert text caused a significant decline in interest and funding of neural network research. It took ten more years until neural network research experienced a resurgence in the 1980s. This text was reprinted in 1987 as [...] "Perceptrons - Expanded Edition" [...] where some errors in the original text are shown and corrected.|$|E
40|$|Neural Networks are {{a set of}} {{mathematical}} methods and computer programs designed to simulate the information process and the knowledge acquisition of the human brain. In last years its application in chemistry is increasing significantly, due the special characteristics for model complex systems. The basic principles of two types of neural networks, the <b>multi-layer</b> <b>perceptrons</b> and radial basis functions, are introduced, as well as, a pruning approach to architecture optimization. Two analytical applications based on near infrared spectroscopy are presented, the first one for determination of nitrogen content in wheat leaves using <b>multi-layer</b> <b>perceptrons</b> networks {{and second one for}} determination of BRIX in sugar cane juices using radial basis functions networks...|$|R
40|$|Image {{denoising}} can {{be described}} as the problem of mapping from a noisy image to a noise-free image. In another paper, we show that <b>multi-layer</b> <b>perceptrons</b> can achieve outstanding image denoising performance for various types of noise (additive white Gaussian noise, mixed Poisson-Gaussian noise, JPEG artifacts, salt-and-pepper noise and noise resembling stripes). In this work we discuss in detail which trade-offs have to be considered during the training procedure. We will show how to achieve good results and which pitfalls to avoid. By analysing the activation patterns of the hidden units we are able to make observations regarding the functioning principle of <b>multi-layer</b> <b>perceptrons</b> trained for image denoising...|$|R
40|$|The problem Building good {{predictors}} on complex domains means learning complicated functions. These {{are best}} represented by multiple levels of non-linear operations i. e. deep architectures. Deep architectures are an old idea: <b>multi-layer</b> <b>perceptrons.</b> Learning {{the parameters of}} deep architectures proved to be challenging...|$|R
40|$|The <b>multi-layer</b> <b>perceptron</b> is {{investigated}} as a {{new approach}} to the automatic recog-nition of spoken isolated digits. The choice of the parameters for the <b>multi-layer</b> <b>perceptron</b> is discussed and experimental results are reported. A comparison is made with established techniques such as dynamic time-warping and hidden Markov mod-elling applied to the same data. The results, for this particular task, show that the recognition accuracy obtained using the <b>multi-layer</b> <b>perceptron</b> is comparable wit...|$|E
30|$|Also, {{as another}} result, due to low complexity, {{we can use}} a smaller dataset {{which is one of}} the main {{features}} of petroleum datasets (Feldman and Ballard 1982; Jacobs et al. 1991; Jacobs 1995). We compare modular neural network with <b>multi-layer</b> <b>perceptron</b> and the results show that modular neural networks outperform <b>multi-layer</b> <b>perceptron</b> in examined dataset in terms of accuracy and learning time.|$|E
30|$|For {{individual}} credit, Desai et al. 1996 used a <b>multi-layer</b> <b>perceptron</b> {{neural network}} (i.e., {{a mixture of}} expert neural networks) for individual credit and found that neural network models outperform linear discriminant analysis and logistic regression models. Davis et al. 1992 studied the decision accuracy of decision trees and a <b>multi-layer</b> <b>perceptron</b> neural network for individual credit. The authors concluded that a comparable level of decision accuracy was obtained with the decision trees and the <b>multi-layer</b> <b>perceptron</b> neural network methods. Piramuthu 1999 employed a <b>multi-layer</b> <b>perceptron</b> neural network and a neural-fuzzy model to predict credit scoring for individual credit and indicated that neural networks obtain superior results. West 2000 used five neural network models: <b>multi-layer</b> <b>perceptron,</b> mixture-of-experts, radial basis function, learning vector quantization, and fuzzy adaptive resonance for individual credit. The author demonstrated that both the mixture-of–experts and radial basis function neural network models were superior to other models, and logistic regression was the most accurate of the traditional methods. Baesens et al. 2003 a clarified the neural network decisions by explanatory rules that capture the learned knowledge embedded in the networks for individual credit, and concluded that neural network rule extraction and decision tables were powerful management tools to build advanced and userfriendly decision-support systems for credit-risk evaluation. Lee et al. 2006 argued that the {{classification and regression tree}} (CART) and multi-variate adaptive regression splines (MARS) for individual credit outperformed ANNs.|$|E
40|$|Abstract: 2 ̆ 2 There {{has been}} an {{increasing}} interest in the applicability of neural networks in disparate domains. In this paper, we describe the use of <b>multi-layered</b> <b>perceptrons,</b> a type of neural network topology, for financial classification problems, with promising results. Back-propagation, which is the learning algorithm most often used in multilayered perceptrons, however, is inherently an inefficient search procedure. We present improved procedures which have much better convergence properties. Using several financial classification applications as examples, we show the efficacy of using <b>multi-layered</b> <b>perceptrons</b> with improved learning algorithms. The modified learning algorithms have better performance, in terms of classification/prediction accuracies, than the methods previously used in the literature, such as probit analysis and similarity-based learning techniques. 2 ̆...|$|R
50|$|Other typical {{problems}} of the back-propagation algorithm are the speed of convergence {{and the possibility of}} ending up in a local minimum of the error function. Today there are practical methods that make back-propagation in <b>multi-layer</b> <b>perceptrons</b> the tool of choice for many machine learning tasks.|$|R
40|$|Abstract—A {{forecasting}} approach {{based on}} wavelet decomposition is presented. Separate forecasting {{models for the}} decomposed signals are built, after which a final model combines the various forecasts. An ensemble approach is followed in which <b>multi-layer</b> <b>perceptrons,</b> {{radial basis function networks}} and support vector regression models are used. U I...|$|R
