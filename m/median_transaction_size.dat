0|222|Public
40|$|AIM - To analyse the {{composition}} of cannabis retail purchases in {{a representative sample of}} purchases made in Christiania, Copenhagen in 2004. MATERIAL - Transactions (n= 1, 123) were registered along four variables; type (loose resin or joints), quantum (n= 957, grams or number of joints), sex (n= 559, female or male) and payment (n= 707, notes or coins). RESULTS - We found {{that more than half of}} all transactions were for joints only. The <b>median</b> <b>transaction</b> quantum was small, at two joints or three grams of resin, valued at DKK 100. Of the resin transactions, 88 % were three grams or below. Women made 11 % of the purchases. There was no statistically significant difference in the preferences for quantum or type between males and females. CONCLUSIONS - Buyers prefer joints over loose resin despite the higher price, which is interesting. The small <b>median</b> <b>transactions</b> <b>size</b> is consistent with findings in the international literature. Illicit drug buyers appear to prefer small acquisitions across drugs and social context. The share of purchases made by women is 11 %, which is similar to the estimated proportion of women among daily cannabis users. This finding suggests an interesting question for future research. At what point in a cannabis-using career do users purchase their drugs? These findings contribute to the existing research by documenting the proportion of female buyers, and preferences for type and quantum in a sample that is representative of a market and is not based on self-reported purchase...|$|R
50|$|<b>Transaction</b> <b>size</b> cannot exceed 10 MB {{of total}} written keys and values.|$|R
40|$|In {{this paper}} we analyse the eect of a cuto <b>transaction</b> <b>size</b> {{on the average}} {{inventory}} cost in a simple newsboy setting. It is assumed that customers with an order larger than a prespecified cuto <b>transaction</b> <b>size</b> are satisfied in an alternative way, against additional cost. For compound Poisson demand with discrete order sizes, we show how to determine the average cost and an optimal cuto <b>transaction</b> <b>size.</b> Because the computational eort to calculate the exact cost is quite large, we also consider an approximate model. By approximating {{the distribution of the}} total demand during a period by the normal distribution one can determine an expression for the average cost function that solely depends on the cuto <b>transaction</b> <b>size.</b> A significant advantage of this approximation is that we can solve problems of any size. The quality of using the normal approximation is evaluated through a number of numerical experiments, which show that the approximate results are satisfactory. 1...|$|R
40|$|In {{this paper}} we analyse {{the effect of}} {{satisfying}} {{in a different way}} customers with an order larger than a prespecified cutoff <b>transaction</b> <b>size,</b> in a simple newsboy setting. For compound Poisson demand with discrete order sizes, we show how to determine the expected costs and the optimal cutoff <b>transaction</b> <b>size.</b> Moreover, by approximating the distribution of the total demand during a period by the normal distribution one can determine an expression for the average cost function that depends on the cutoff <b>transaction</b> <b>size</b> only. A main advantage of this approximation is that the computational effort is much less. The quality of using the normal approximation is evaluated through a number of numerical experiments, which show that the approximative results are satisfactory. inventory;cutoff transaction size;newsboy model...|$|R
40|$|We address Allan Shampine's {{critiques of}} our study on the costs and {{benefits}} of payment instruments, and review the current state of the literature. We argue that a consensus seems to be emerging in which: (a) different payment instruments appear to be socially efficient at different transaction sizes; (b) cash appears to be efficient for small payments; and (c) debit cards appear to overtake cash as the socially optimal instrument as the <b>transaction</b> <b>size</b> increases. Except for the fact that in our study credit appears to overtake debit at large <b>transaction</b> <b>sizes,</b> our 2006 findings are consistent with the consensus. ...|$|R
40|$|A rational, efficiency-based view of {{acquisitions}} {{implies that}} larger transactions generate greater gains for the acquirer and the seller. We test this prediction {{and find a}} positive relationship between acquirer abnormal returns and <b>transaction</b> <b>size</b> scaled by the acquirer size. This relationship holds for many classes of acquisitions, including asset purchases and mergers that target private firms. We find a similar relationship between total abnormal returns and relative <b>transaction</b> <b>size.</b> The results suggest that, in general, acquisitions help shift capital to more productive owners. Furthermore, we present evidence demonstrating that the average acquirer captures {{a significant portion of the}} total gains generated from an acquisition. ...|$|R
40|$|Frequent itemset mining is {{a popular}} data mining technique. Apriori, Eclat, and FP-Growth {{are among the most}} common {{algorithms}} for frequent itemset mining. Considerable research has been performed to compare the relative performance between these three algorithms, by evaluating the scalability of each algorithm as the dataset size increases. While scalability as data size increases is important, previous papers have not examined the performance impact of similarly sized datasets that contain different itemset characteristics. This paper explores the effects that two dataset characteristics can have on the performance of these three frequent itemset algorithms. To perform this empirical analysis, a dataset generator is created to measure the effects of frequent item density and the maximum <b>transaction</b> <b>size</b> on performance. The generated datasets contain the same number of rows. This provides some insight into dataset characteristics that are conducive to each algorithm. The results of this paper's research demonstrate Eclat and FP-Growth both handle increases in maximum <b>transaction</b> <b>size</b> and frequent itemset density considerably better than the Apriori algorithm. This paper explores the effects that two dataset characteristics can have on the performance of these three frequent itemset algorithms. To perform this empirical analysis, a dataset generator is created to measure the effects of frequent item density and the maximum <b>transaction</b> <b>size</b> on performance. The generated datasets contain the same number of rows. This provides some insight into dataset characteristics that are conducive to each algorithm. The results of this paper's research demonstrate Eclat and FP-Growth both handle increases in maximum <b>transaction</b> <b>size</b> and frequent itemset density considerably better than the Apriori algorithm...|$|R
50|$|On 23 August 2006, IBM {{announced}} {{its intention to}} acquire Internet Security Systems for $1.36 billion and its Japanese subsidiary, ISS KK, for additional $570M. Total acquisition <b>transaction</b> <b>size</b> was $1.93 billion. On 16 October 2006, the deal was approved by ISS shareholders.|$|R
40|$|This {{investigation}} adopts {{the perspective}} of the retailer and incorporates information flow between a retailer and customers. Two new models are considered that differ in terms of information completeness. The first model involves the retailer having incomplete information regarding the state of customers demand, namely extending the model of Dekker et al. (IIE Transactions, 32, 2000, 461) by considering the quadratic concave holding and penalty cost functions based on the law of diminishing marginal cost to fit in with the some practical situations. Meanwhile, the second model involves the retailer having full information on the state of customers demand. Precise expressions are derived for the expected total profit of these two newsboy models with a cutoff <b>transaction</b> <b>size</b> and compound Poisson demand distribution. It is worthwhile to measure the value of information and identify the effect of factors for enterprise decision making regarding whether or not to pay for information that can help increase profits. Moreover, we adopt and modify the golden section search technique (Haftka et al., 1990) to determine an optimal order-up-to level S and a cutoff <b>transaction</b> <b>size</b> q systematically. Finally, numerical examples are given to illustrate the result derived. Cutoff <b>transaction</b> <b>size,</b> newsboy model, concave function...|$|R
40|$|Memory {{controller}} {{design is}} challenging as mixed time-criticality embedded systems feature an increasing diversity of real-time (RT) and non-real-time (NRT) applications with variable <b>transaction</b> <b>sizes.</b> To {{satisfy the requirements}} of the applications, tight bounds on the worst-case response time (WCRT) of memory transactions must be provided to RT applications, while the lowest possible average response time must be given to the remaining applications. Existing real-time memory controllers cannot efficiently achieve this goal as they either bound the WCRT by sacrificing the average response time, or cannot efficiently support variable <b>transaction</b> <b>sizes.</b> In this article, we propose to use dynamic command scheduling, which is capable of efficiently dealing with <b>transactions</b> with variable <b>sizes.</b> The three main contributions of this article are: (1) a memory controller architecture consisting of a front-end and a back-end, where the former uses a TDM arbiter with a new work-conserving policy and the latter has a dynamic command scheduling algorithm that is independent of the front-end, (2) a formalization of the timings of the memory transactions for the proposed algorithm and architecture, and (3) an analysis of WCRT for transactions to capture the behavior of both the front-end and the back-end. This WCRT analysis supports variable <b>transaction</b> <b>sizes</b> and different degrees of bank parallelism. The critical part of the WCRT is the worst-case execution time (WCET) of a transaction, which is the time spent on command scheduling in the back-end. The WCET is bounded by two techniques applied to both fixed and variable <b>transaction</b> <b>sizes,</b> respectively. We experimentally evaluate the proposed memory controller and compare to an existing semi-static approach. The results demonstrate that dynamic command scheduling significantly outperforms the semi-static approach in the average case, while it performs equally well or better in the worst-case with only a few exceptions. The former reduces the average response time for NRT applications, and the latter pertains the WCRT for RT applications...|$|R
40|$|This paper {{presents}} {{a mathematical model}} developed for the synthesis of optimal replenishment policies for items that experience lumpy demands. In order to avoid disrupting the inventory system, a cutoff point of w units is introduced such that the system would only satisfy routinely customer orders with <b>transaction</b> <b>sizes</b> {{less than or equal}} to w units. For customer orders with <b>transaction</b> <b>sizes</b> larger than w units, the system would only supply the cutoff amount (w units). The excess units would be refused. The control discipline is the (s, S) inventory policy with continuous review, and the nature of the customer orders is approximated by a discrete stuttering Poisson distribution. The optimal values of the control parameters, w, s and S, are determined. The theoretical results obtained are illustrated with a numerical example. © 1995. link_to_subscribed_fulltex...|$|R
40|$|This paper {{presents}} {{a mathematical model}} developed for the synthesis of optimal replenishment policies for items which experience lumpy demands. To avoid disrupting the inventory system, a cutoff point of w units is introduced such that the system would only satisfy routinely customer orders with <b>transaction</b> <b>sizes</b> {{less than or equal}} to w units. For customer orders with <b>transaction</b> <b>sizes</b> larger than w units, the system would only supply the cutoff amount (w units). The excess units would be refused. The control discipline is the (s, S) inventory policy with continuous review, and the nature of the demands is approximated by a discrete stuttering Poisson distribution. The optimal values of the control parameters, s, and S, are determined. The theoretical results obtained are illustrated with a numerical example. link_to_subscribed_fulltex...|$|R
50|$|While it is {{possible}} to store any digital file in the blockchain, the larger the <b>transaction</b> <b>size,</b> the larger any associated fees become. Various items have been embedded, including URLs to child pornography, an ASCII art image of Ben Bernanke, material from the Wikileaks cables, prayers from bitcoin miners, and the original bitcoin whitepaper.|$|R
40|$|The {{relationship}} between quantity traded and transaction costs {{has been one}} of the main focuses among financial scholars and practitioners. The purpose of this thesis is to investigate the informational {{relationship between}} these variables. Following insights and results of Milgrom (1981), Feldman (2004), and Feldman and Winer (2004), we use New York Stock Exchange (NYSE) data and kernel estimation methods to construct the distribution of one variable conditional on the other. We then study the information in these conditional distributions: the extent to which they are ordered by first order stochastic dominance (FOSD) and by monotone likelihood ratio property (MLRP). We find that <b>transaction</b> <b>size</b> and effective spread are statistically significantly orrelated. FOSD, a necessary condition for a "separating signaling equilibrium", holds under certain conditions. We start from two-subsample case. We choose a cut-off point in <b>transaction</b> <b>size</b> and categorize the observations with <b>transaction</b> <b>sizes</b> smaller than the cut-off point into group "low". The remaining data is classified as "high". We repeat this procedure for all possible <b>transaction</b> <b>size</b> cut-off points. It turns out that FOSD holds nowhere. However, once we eliminate transactions at the quote midpoint, the "crossings" between exchange members not specialists, FOSD holds for all the cut-off points fewer than 15800 shares. MLRP, a necessary and sufficient condition for the separating equilibrium to hold point by point of the conditional density functions, does not hold but might not be ruled out considering the error in the estimates. We also find that large trades are not necessarily associated with large spread. Instead, it is more likely that larger trades are transacted at the quote midpoint (again, the non-specialist "crossings") than smaller trades. Our results confirm the findings of Barclay and Warner (1993) regarding the informativeness of medium-size transactions: we identify informational relationships between mid-size transactions and spreads but not for trades at the quote midpoint and large-size transactions. That is, we identify two regimes, an informational one and a non-informational/liquidity one...|$|R
50|$|Houlihan {{consolidated}} {{its investment}} banking activities under the Houlihan Smith Capital Markets umbrella. Operating as {{a division of}} Houlihan, the group worked in the middle market and specialized in <b>transaction</b> <b>sizes</b> of $10 million to $250 million. Houlihan was involved in more than 500 mergers and acquisitions, capital restructurings, ESOP advisory engagements, private placements and financial restructurings.|$|R
40|$|Using {{detailed}} trader {{surveys in}} Benin, Madagascar and Malawi this paper investigates {{the presence of}} increasing returns in agricultural trade. After analyzing margins, costs, and value added, we find little evidence of returns to scale. Motorized transport is found more cost effective for large loads on longer distances. But transporter pool quantities from multiple traders. Margin rates show little relationship with <b>transaction</b> <b>size.</b> Personal travel costs {{are a source of}} increasing returns, but the effect is small. Consequently, total marketing costs are nearly proportional to <b>transaction</b> <b>size.</b> Working and network capital are key determinants of value added. Constant returns to scale in all accumulable factors - working capital, labour, and network capital - cannot be rejected. This implies that policies to restrict entry into agricultural trade are neither necessary nor useful. Governments should focus instead on technological and institutional innovations to upgrade agricultural markets. ...|$|R
40|$|In this paper, we {{introduce}} {{and describe}} directed dependency graph-based transaction and concurrency control (DCC) for persistent (stable, single-level) object-based bulk data management systems. The new technique is optimistic and applicable {{across a wide}} range of store <b>sizes,</b> <b>transaction</b> <b>sizes</b> and multi-programming levels. It is also has potential for use in management of transactions in other contexts, for example web services. ...|$|R
40|$|Working paper"Using {{detailed}} trader {{surveys in}} Benin, Madagascar, and Malawi, this paper investigates {{the presence of}} increasing returns in agricultural trade. After analyzing margins, costs, and value added, we find little evidence of returns to scale. Motorized transport is found more cost effective for large loads on longer distances. But transporters pool quantities from multiple traders. Margin rates show little relationship with <b>transaction</b> <b>size.</b> Personal travel costs {{are a source of}} increasing returns, but the effect is small. Consequently, total marketing costs are nearly proportional to <b>transaction</b> <b>size.</b> Working and network capital are key determinants of value added. Constant returns to scale in all accumulable factors [...] working capital, labor, and network capital [...] cannot be projected. This implies that policies to restrict entry into agricultural trade are neither necessary nor useful. Governments should focus instead on technological and institutional innovations to upgrade agricultural markets. " [...] Authors' AbstractIFPRI 5 MTIDNon-PR 44 p...|$|R
40|$|The aim of {{this chapter}} is to glean {{empirical}} evidence about the interconnections between Indian and US markets, in terms of price and volatility spillovers. The analysis also investigates the promptness of the Indian market response to US movements, any asymmetric properties of such response and how the latter is affected by liquidity {{factors such as the}} number of companies traded and <b>transaction</b> <b>size.</b> peer-reviewe...|$|R
40|$|Frequent Flier Programs (FFPs) {{are said}} to impact airline {{consumer}} behaviour such that revenue of sponsoring airlines increases. Prior research relies on aggregate industry data to study FFPs. We {{examine the impact of}} FFPs on individual consumer behaviour in a quasi-natural experimental set-up using a combined discrete choice and count data model. We exploit an unanticipated change in the FFP to avoid self-selection bias. We derive the causal effect of redesigning a frequency reward program into a customer tier program on average <b>transaction</b> <b>size,</b> purchase frequency, revenues of the sponsoring airline, and compensating variation. We find that, on average, revenues increased by 8 $ per member over a 16 month period. The welfare impact is small but positive. We find that, on average, consumer surplus increased by 5 $ per member over a 16 month period. The results vary su bstantially across individuals. In line with previous studies, our results suggest that moderate buyers increase their average <b>transaction</b> <b>size</b> and purchase frequency most due to the introduction of the customer tier program...|$|R
40|$|This {{paper is}} {{concerned}} with the synthesis of control policies for inventory systems in which the items experience lumpy demands. The nature of the customer demands is approximated by a discrete stuttering Poisson distribution and a continuous review (s, S) inventory policy is used to control such items. A cutoff point is also incorporated into the control policy such that customer orders with <b>transaction</b> <b>sizes</b> greater than the cutoff point will be filtered out of the inventory system and satisfied by placing a special replenishment order to a higher echelon. Customer orders with <b>transaction</b> <b>sizes</b> {{less than or equal to}} the cutoff point will be met from stock. It is also specified that if the available inventory is below the order-up-to level S at the time when a special replenishment order is placed, such a replenishment order will also raise the available inventory level to S. A search procedure is presented for determining the optimal values of the control parameters w, s and S. A numerical example is used to illustrate the theoretical results obtined. link_to_subscribed_fulltex...|$|R
40|$|Accepted in 13 th IEEE Symposium on Embedded Systems for Real-Time Multimedia (ESTIMedia 2015), Amsterdam, Netherlands. SDRAM is {{a shared}} {{resource}} in modern multi-core platforms executing multiple real-time (RT) streaming applications. It {{is crucial to}} analyze the minimum guaranteed SDRAM bandwidth {{to ensure that the}} requirements of the RT streaming applications are always satisfied. However, deriving the worst-case bandwidth (WCBW) is challenging because of the diverse memory traffic with variable <b>transaction</b> <b>sizes.</b> In fact, existing RT memory controllers either do not efficiently support variable <b>transaction</b> <b>sizes</b> or do not provide an analysis to tightly bound WCBW in their presence. We propose a new mode-controlled data-flow (MCDF) model to capture the command scheduling dependencies of memory <b>transactions</b> with variable <b>sizes.</b> The WCBW can be obtained by employing an existing tool to automatically analyze our MCDF model rather than using existing static analysis techniques, which in contrast to our model are hard to extend to cover different RT memory controllers. Moreover, the MCDF analysis can exploit static information about known transaction sequences provided by the applications or by the memory arbiter. Experimental results show that 77 % improvement of WCBW can be achieved compared to the case without known transaction sequences. In addition, the results demonstrate that the proposed MCDF model outperforms state-of-the-art analysis approaches and improves the WCBW by 22 % without known transaction sequences...|$|R
40|$|This paper studies empirically the {{determinants}} of Chinese commercial banks' net interest margins from 1996 to 2003. It applies an extension to the Ho and Saunders (1981) model to identify the elements affecting net interest margins. The {{results indicate that the}} {{determinants of}} net interest margins in the Chinese market include market competition structure, average operating costs, degree of risk aversion, <b>transaction</b> <b>size,</b> implicit interest payments, opportunity cost of reserve, and management efficiency. commercial banks, determinants, net interest margin,...|$|R
50|$|In December 2014, ChangeTip {{announced}} that it had raised $3.5 million in venture capital funding led by Pantera Capital. At the time, 50,000 social networking accounts had connected to ChangeTip, with <b>transaction</b> <b>sizes</b> ranging between $0.00001 and $100. In the same month, Walter Isaacson speculated that micropayment platforms such as ChangeTip could {{improve the quality of}} journalism by encouraging news sites to publish high quality content to attract users, instead of relying on click-bait to increase advertising revenue.|$|R
50|$|When many {{hundreds}} or thousands of transactions are being done each day, and whenever there is human input involved, error accounts are necessary to keep the audit trail intact. Error accounts also play a role in improving customer service. GAAP recommends daily or weekly monitoring of error accounts depending on volume and <b>transaction</b> <b>size.</b> It is typically up to the company or applicable government department's accounting department to monitor the error accounts that it has in place.|$|R
50|$|In 2001, Van der Moolen {{launched}} VDM Bonds {{to provide}} fixed income liquidity in less-than-wholesale <b>transaction</b> <b>sizes</b> to {{banks and other}} intermediaries. VDM Bonds was sold to Zions Bancorporation in March 2004 after its management team left the company. In January 2006, VDM acquired Curvalue, a Dutch trading company specialized in derivative trading. In late 2006, VDM acquired the assets of London-based prop trading firm Hills Independent Traders Ltd. In 2008, VDM UK's proprietary equity and futures trading unit was closed.|$|R
40|$|Memory {{controller}} {{design is}} challenging as real-time embedded systems feature an increasing diversity of real-time and non-real-time applications with variable <b>transaction</b> <b>sizes.</b> To {{satisfy the requirements}} of the applications, tight bounds on the worst-case execution time (WCET) of memory transactions must be provided to real-time applications, while the lowest possible average execution time must be given to the rest. Existing real-time memory controllers cannot efficiently achieve this goal as they either bound the WCET by sacrificing the average execution time, or are not scalable to directly support variable <b>transaction</b> <b>sizes,</b> or both. In this paper, we propose to use dynamic command scheduling, which is capable of efficiently dealing with <b>transactions</b> with variable <b>sizes.</b> The three main contributions of this paper are: 1) a back-end architecture for a real-time memory controller with a dynamic command scheduling algorithm, 2) a formalization of the timings of the memory transactions for the proposed architecture and algorithm, and 3) two techniques to bound the WCET of transactions with both fixed and variable sizes, respectively. We experimentally evaluate the proposed memory controller and compare both the worst-case and average-case execution times of transactions to a state-of-the-art semi-static approach. The results demonstrate that dynamic command scheduling outperforms the semi-static approach by 33. 4 % in the average case and performs at least equally well in the worst case. We also show the WCET is tight for transactions with fixed and variable sizes, respectively...|$|R
40|$|The common {{assumption}} of universal behavior in stock market data can sometimes lead to false conclusions. In statistical physics, the Hurst exponents characterizing long-range correlations are often {{closely related to}} universal exponents. We show, {{that in the case}} of time series of the traded value, these Hurst exponents increase logarithmically with company size, and thus are non-universal. Moreover, the average <b>transaction</b> <b>size</b> shows scaling with the mean transaction frequency for large enough companies. We present a phenomenological scaling framework that properly accounts for such dependencies. ...|$|R
40|$|We measure {{consumers}} 2 ̆ 019 use of cash by harmonizing payment diary {{surveys from}} seven countries. The seven diary surveys {{were conducted in}} 2009 (Canada), 2010 (Australia), 2011 (Austria, France, Germany and the Netherlands), and 2012 (the United States). Our paper finds cross-country differences 2 ̆ 013 for example, the level of cash usage differs across countries. Cash has not disappeared as a payment instrument, especially for low-value transactions. We also find {{that the use of}} cash is strongly correlated with <b>transaction</b> <b>size,</b> demographics, and point-of-sale characteristics such as merchant card acceptance and venue...|$|R
40|$|This paper {{investigates the}} role of {{corporate}} governance on regulatory compliance {{and the performance of}} Italian corporate insider trading. Exploiting a unique enforcement and reporting framework, we present three main findings. First, corporate governance does not influence the propensity of firms to comply with insider trading regulation. Firms that have concentrated ownership and control, together with those run by families and cooperatives are most likely to comply with regulation. Second, multiple trading, but not <b>transaction</b> <b>size,</b> affects the performance of corporate insider trading. Third, the market responds less to reporting by compliant and family firms...|$|R
40|$|Summary. The common {{assumption}} of universal behavior in stock market data can sometimes lead to false conclusions. In statistical physics, the Hurst exponents characterizing long-range correlations are often {{closely related to}} universal exponents. We show, {{that in the case}} of time series of the traded value, these Hurst exponents increase logarithmically with company size, and thus are non-universal. Moreover, the average <b>transaction</b> <b>size</b> shows scaling with the mean transaction frequency for large enough companies. We present a phenomenological scaling framework that properly accounts for such dependencies. Key words: econophysics; stock market; fluctuation phenomena...|$|R
5000|$|... {{establish}} a single {{gold standard for}} <b>transactions</b> of all <b>sizes.</b>|$|R
30|$|The major {{common problem}} of {{providing}} all these {{services to the}} poor is that the <b>transaction</b> <b>size</b> is very small. As a result, any processing cost, or transaction cost, becomes {{a high percentage of}} the transaction amount. This makes the product very expensive for commercial banks and formal financial institutions to provide these services to the poor, who therefore remain excluded. A second common problem is that the poor not only lack financial capital but are also often socially excluded and lack bridges to rich people. A third common problem is that they are often uneducated, even illiterate, and excluded from technological innovations.|$|R
40|$|This paper models {{transaction}} costs as the rents that a monopolistic market maker extracts from impatient investors who trade via limit orders. We show that limit orders are American options. The limit prices inducing immediate {{execution of the}} order are functionally equivalent to bid and ask prices and can be solved for various <b>transaction</b> <b>sizes</b> to characterize the market maker's entire supply curve. We find considerable empirical support for the model's predictions in the cross-section of NYSE firms. The model produces unbiased, out-of-sample forecasts of abnormal returns for firms added to the S&P 500 index. Copyright (c) 2008 by The American Finance Association. ...|$|R
40|$|Transactional memory (TM), {{a recent}} {{parallel}} programming concept, aims to simplify parallel programming while simultaneously maintaining performance benefits found in concurrent applications. Consistency checking, {{the manner in}} which memory conflicts are identified in transactional memory, is a critical aspect to TM system performance. We present a theoretical, analytical and empirical view of our novel consistency checking algorithm which is optimized for spatially wide transactional workloads. Initial tests show our algorithm yields super linear performance improvements over other alternatives as <b>transaction</b> <b>size</b> grows, resulting in performance gains between 5 x − 250 x for experimental benchmarks. 1...|$|R
40|$|This {{software}} manual is used {{to provide}} detailed information about RT-MemController, such that users can modify this tool according to their needs. 1 The inputs of RTMemController RTMemController requires as inputs memory traces and memory spec-ifications, which should be specified by the user. Example input files {{are included in the}} folders ”memtraces/ ” and ”memspecs/”, respectively. memtraces/: This folder contains 12 Mediabench [1] application traces, each of which has a <b>transaction</b> <b>size</b> of 32 bytes, 64 bytes or 128 bytes. A trace presents every transaction on a separate line, including the time interval (in cycles) between successive transactions (RequestInterval), the transaction type (AccessType: read or write), logical address in decimal and the <b>transaction</b> <b>size</b> in bytes. There are more fields in the input files, while they are not relevant to this tool. Note that this tool uses these traces {{based on the assumption that}} the SDRAM and the processors running the applications have the same frequency. If the frequencies are different, users have to convert the RequestInterval of their own traces into the SDRAM clock cycles. Users can specify which memory traces are used. By running traces. py, these specified traces representing different requestors are combined with a first-come first-serve (FCFS) mechanism. As a result, a combined trace is generated and given by combinedTrace. dat. For example, users may find the following lines in combinedTrace. dat. Each line provides the information of a transaction in terms of time stamp, type, logical address and size...|$|R
