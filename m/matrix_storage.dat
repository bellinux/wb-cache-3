176|152|Public
2500|$|Another common {{method is}} Platt's {{sequential}} minimal optimization (SMO) algorithm, which breaks the problem down into 2-dimensional sub-problems that are solved analytically, {{eliminating the need}} for a numerical optimization algorithm and <b>matrix</b> <b>storage.</b> This algorithm is conceptually simple, easy to ...|$|E
5000|$|Matrix mode now {{supports}} maximum 4×4 matrices and 4 user-defined <b>matrix</b> <b>storage</b> ...|$|E
5000|$|Intel <b>Matrix</b> <b>Storage</b> Technology {{for setting}} up a RAID 0, 1, 5, or 10 array.|$|E
40|$|Abstract We {{study the}} {{numerical}} {{performance of a}} <b>matrices</b> <b>storage</b> free quasi-Newton method for large-scale optimization, which we call the F-BFGS method. We compare its performance {{with that of the}} limited memory BFGS, L-BFGS methods developed by Nocedal (1980) and the conjugate gradient methods. The F-BFGS method is very competitive due to its low storage requirement and computational labor and also able to solve large-scale problems with 106 variables successfully while other methods fail...|$|R
40|$|We {{introduce}} {{the notion of}} 1 -vertex transfer matrix for near neighbor Potts models with k kinds of particles. We show that the topological entropy (free energy) of this model can be expressed as the limit the logarithm of spectral radii of 1 -vertex transfer <b>matrices.</b> <b>Storage</b> and computations using the 1 -vertex transfer matrix are much smaller than storage and computations needed for the standard transfer matrix that is used. We apply our methods to find the first 15 digits of the entropy of the hard core model on the two dimensional integer grid...|$|R
40|$|The scaling {{behavior}} of different OpenFOAM versions is analyzed on two benchmark problems. Results {{show that the}} applications scale reasonably well up to a thousand tasks. An in-depth profiling identifies the calls to the MPI_Allreduce function in the linear algebra core libraries as the main communication bottleneck. A sub-optimal performance on-core {{is due to the}} sparse <b>matrices</b> <b>storage</b> format that does not employ any cache-blocking mechanism at present. Possible strategies to overcome these limitations are p roposed and analyzed, and preliminary results on prototype implementations are presented. OpenFOA...|$|R
5000|$|Viiv 1.5 {{has updated}} {{features}} including <b>matrix</b> <b>storage,</b> integrated Media Server {{and support for}} Digital Media Adaptors.|$|E
5000|$|In {{scientific}} computing, skyline <b>matrix</b> <b>storage,</b> or SKS, or {{a variable}} band <b>matrix</b> <b>storage,</b> or envelope storage scheme {{is a form}} of a sparse <b>matrix</b> <b>storage</b> format matrix that reduces the storage requirement of a matrix more than banded storage. In banded storage, all entries within a fixed distance from the diagonal (called half-bandwidth) are stored. In column-oriented skyline storage, only the entries from the first nonzero entry to the last nonzero entry in each column are stored. There is also row oriented skyline storage, and, for symmetric matrices, only one triangle is usually stored. Skyline storage has become very popular in the finite element codes for structural mechanics, because the skyline is preserved by Cholesky decomposition (a method of solving systems of linear equations with a symmetric, positive-definite matrix; all fill-in falls within the skyline), and systems of equations from finite elements have a relatively small skyline. In addition, the effort of coding skyline Cholesky is about same as for Cholesky for banded matrices (available for banded matrices, e.g. in LAPACK; for a prototype skyline code, see [...] ).|$|E
50|$|Another common {{method is}} Platt's {{sequential}} minimal optimization (SMO) algorithm, which breaks the problem down into 2-dimensional sub-problems that are solved analytically, {{eliminating the need}} for a numerical optimization algorithm and <b>matrix</b> <b>storage.</b> This algorithm is conceptually simple, easy toimplement, generally faster, and has better scaling properties for difficult SVM problems.|$|E
5000|$|... 4,142,112 Single {{active element}} controlled-inversion {{semiconductor}} storage cell devices and <b>storage</b> <b>matrices</b> employing same ...|$|R
5000|$|BroadcastEngineering [...] "Cambridge Imaging Systems and Object <b>Matrix</b> {{partner for}} <b>storage</b> and archives", 30 August 2012.|$|R
40|$|We {{describe}} a new extension to ScaLAPACK [2] for computing with symmetric (Hermitian) matrices {{stored in a}} packed form. The new code is built upon the ScaLAPACK routines for full dense storage for {{a high degree of}} software reuse. The original ScaLAPACK stores a symmetric matrix as a full matrix but accesses only the lower or upper triangular part. The new code enables more efficient use of memory by storing only the lower or upper triangular part of a symmetric (Hermitian) <b>matrix.</b> The packed <b>storage</b> scheme distributes the matrix by block column panels. Within each panel, the matrix is stored as a regular ScaLAPACK <b>matrix.</b> This <b>storage</b> arrangement simplifies the subroutine interface and code reuse. Routines PxPPTRF#PxPPTRS implement the Cholesky factorization and solution for symmetric (Hermitian) linear systems in packed storage. Routines PxSPEV#PxSPEVX (PxHPEV#PxHPEVX) implement the computation of eigenvalues and eigenvectors for symmetric (Hermitian) <b>matrices</b> in packed <b>storage.</b> Routines PxSPGVX #PxHPGVX# implement the expert driver for the generalized eigenvalue problem for symmetric (Hermitian) <b>matrices</b> in packed <b>storage.</b> Performance results on the Intel Paragon suggest that the packed storage scheme incurs only a small time overhead over the full storage scheme...|$|R
50|$|Linux {{supports}} Matrix RAID through device mapper (DM-RAID) for RAID 0, 1 and 10, and Linux MD RAID for RAID 0, 1, 10, and 5. Set {{up of the}} RAID volumes must be done {{by using}} the ROM option in the <b>Matrix</b> <b>Storage</b> Manager, then further configuration {{can be done in}} DM-RAID or MD-RAID.|$|E
5000|$|... {{for some}} vector p. With sparse <b>matrix</b> <b>storage,</b> {{it is in}} general {{practical}} to store the rows of [...] in a compressed form (eg, without zero entries), making a direct computation of the above product tricky due to the transposition. However, if one defines ci as row i of the matrix , the following simple relation holds ...|$|E
5000|$|The Intel <b>Matrix</b> <b>Storage</b> Manager (IMSM) option ROM {{is a part}} of Matrix RAID {{that has}} {{to be used in the}} BIOS to create new RAID arrays. Intel uses [...] "Rapid Storage Technology" [...] -"Option Rom"- on its new chipsets, {{dropping}} the [...] "Matrix" [...] name.An Intel document notes that Matrix RAID storage changed to RST (Rapid Storage Technology) beginning with version 9.5.|$|E
30|$|Coal is {{characterized}} by two distinct porosity systems: micropores or matrix and macropores or cleats. The <b>matrix</b> is <b>storage</b> medium where coal seam gas (primarily CH 4 and CO 2) is mainly stored by sorption and moves by molecular diffusion. The cleats constitute a natural fracture network and provide permeability and connectivity to the reservoir but very limited storage volume as free gas.|$|R
40|$|The {{notion of}} a 1 -vertex {{transfer}} matrix for multi-dimensional codes is introduced. It is shown that the capacity of such codes, or the topological entropy, can be expressed as the limit of the logarithm of spectral radii of 1 -vertex transfer <b>matrices.</b> <b>Storage</b> and computations using the 1 -vertex transfer matrix are much smaller than storage and computations needed for the standard transfer matrix. The method is applied to estimate the first 15 digits of the entropy of the 2 -dimensional (0, 1) run length limited channel. In order to compare the computational cost of the new method with the standard transfer matrix and have rigorous bounds to compare the estimates with a large scale computation of eigenvalues for the (0, 1) run length limited channel in 2 and 3 dimensions have been carried out. This in turn leads to improvements on the best previous lower and upper bounds for that channel...|$|R
40|$|Quadtree <b>matrices</b> using Morton-order <b>storage</b> provide natural {{blocking}} {{on every}} level of a memory hierarchy. Writing the natural recursive algorithms {{to take advantage of}} this blocking results in code that honors the memory hierarchy without the need for transforming the code. Furthermore, the divide-and-conquer algorithm breaks problems down into independent computations. These independent computations can be dispatched in parallel for straightforward parallel processing. Proof-of-concept is given by an algorithm for QR factorization based on Givens rotations for quadtree <b>matrices</b> in Morton-order <b>storage.</b> The algorithms deliver positive results, competing with and even beating the LAPACK equivalent. Categories and subject descriptors...|$|R
50|$|A {{powerful}} {{feature of}} ARPACK {{is its ability}} to use any <b>matrix</b> <b>storage</b> format. This is possible because it doesn't operate on the matrices directly, but instead when a matrix operation is required it returns control to the calling program with a flag indicating what operation is required. The calling program must then perform the operation and call the ARPACK routine again to continue. The operations are typically matrix-vector products, and solving linear systems.|$|E
50|$|Gutenhamer {{began his}} {{pioneering}} {{work on the}} usage of electronic networks to model complex informational systems and solve equations in the late 1930s. In the mid-1940s, he oversaw {{the development of the}} first analog computing machines. In 1950, he spearheaded the creation of an electronic computing machine that used contactless electromagnetic relays running on ferrite-diode cells. In 1954, he presented the first LEM-1 machine and in 1956, he published a scientific paper on the successful usage of <b>matrix</b> <b>storage</b> for data retention.|$|E
5000|$|The episode {{identifies}} Missy, who {{was previously}} mostly {{shown at the}} end of each episode and usually interacting with a character who just died and arrived in the Nethersphere, such as the Half-Face Man from [...] "Deep Breath". This is the first episode where the Doctor, Clara, and Missy interact directly. The <b>Matrix</b> <b>storage</b> system first appeared in the early Doctor Who serials The Deadly Assassin and The Ultimate Foe, both of which had the Master utilizing it. The scene with the Cybermen emerging from St. Paul's Cathedral is an homage to a similar shot from the 1968 serial The Invasion, while the tomb-filled mausoleum references the 1967 serial The Tomb of the Cybermen.|$|E
5000|$|A packed <b>storage</b> <b>matrix,</b> {{also known}} as packed matrix, is a term used in {{programming}} for representing an [...] matrix. It is a more compact way than an m-by-n rectangular array by exploiting a special structure of the matrix.|$|R
50|$|At {{the heart}} of the system is the HiStar E-Network, a network {{crossbar}} switch <b>matrix.</b> This <b>storage</b> platform is made up of different technologies than USP and USP V. The connectivity to back-end disks is via 6Gbit/s SAS links instead of 4Gbit/s Fibre Channel loop. The internal processors are now Intel multi-core processors, and in addition to 3.5-inch drives support has been added for 2.5 inch small-form factor HDDs. The VSP supports SSD, SAS and SATA drives.|$|R
40|$|We {{consider}} an iterative preconditioning technique for non-convex large scale optimization. First, {{we refer to}} the solution of large scale indefinite linear systems by using a Krylov subspace method, and describe the iterative construction of a preconditioner which does not involve matrices products or <b>matrices</b> <b>storage.</b> The set of directions generated by the Krylov subspace method is used, as by product, to provide an approximate inverse preconditioner. Then, we experience our preconditioner within Truncated Newton schemes for large scale unconstrained optimization, where we generalize the truncation rule by Nash-Sofer (Oper. Res. Lett. 9 : 219 - 221, 1990) to the indefinite case, too. We use a Krylov subspace method to both approximately solve the Newton equation and to construct the preconditioner to be used at the current outer iteration. An extensive numerical experience shows that the proposed preconditioning strategy, compared with the unpreconditioned strategy and PREQN (Morales and Nocedal in SIAM J. Optim. 10 : 1079 - 1096, 2000), {{may lead to a}} reduction of the overall inner iterations. Finally, we show that our proposal has some similarities with the Limited Memory Preconditioners (Gratton et al. in SIAM J. Optim. 21 : 912 - 935, 2011) ...|$|R
40|$|Nowadays {{major problem}} is energy consumtion in {{portable}} devices {{which has a}} battery. In this job we have evaluated energy consumption for Pocket PC. We wanted to see memory and processor influence in battery energy consumption. We have created a program which can do matrix multiplication and sparse matrix „storage by columns“ multiplication. During multiplication program takes battery information and saves it into the file. After that I have investigated the result and saw, that sparse <b>matrix</b> <b>storage</b> by columns multiplication is much more effectived than normal matrix multiplication. Sparce <b>matrix</b> <b>storage</b> by columns multiplication take less memory and more processor commands then normal matrix multiplication. We suggest to use sparse <b>matrix</b> <b>storage</b> by columns model instead simple model, because you can save much more operation time, battery resources and memory...|$|E
40|$|Abstract. To {{solve the}} {{deficiency}} of algorithm distributed association rules based on MapReduce, this paper introduces global pruning strategy to increase algorithm efficiency, adopts frequent <b>matrix</b> <b>storage</b> {{to reduce the}} consumption of internal storage, and puts forward MFMDAP of frequent <b>matrix</b> <b>storage</b> of MapReduce calculation model. Experiments show that the algorithm in the paper elevates the algorithm efficiency and saves the usage amount of internal storage, which {{is in favor of}} the calculation and storage of big granularity data. The effectiveness of algorithm has been approved in experiments...|$|E
40|$|Two direct Choleski {{equation}} solvers and two iterative preconditioned {{conjugate gradient}} (PCG) equation solvers {{used in a}} large structural analysis software system are described. The two direct solvers are implementations of the Choleski method for variable-band <b>matrix</b> <b>storage</b> and sparse <b>matrix</b> <b>storage.</b> The two iterative PCG solvers include the Jacobi conjugate gradient method and an incomplete Choleski conjugate gradient method. The performance of the direct and iterative solvers is compared by solving several representative structural analysis problems. Some key factors affecting {{the performance of the}} iterative solvers relative to the direct solvers are identified...|$|E
40|$|Distributed storage {{systems are}} {{becoming}} more and more popular with the rapidly increasing demand for large-scale data storage. To increase the capacity and I/O performance of a distributed storage system, scaling it up is a common method. Regenerating Codes are a class of distributed storage codes that offer good reliability through encoding and provide good bandwidth cost on failed nodes repairing. This paper studies the scaling problem of E-MSR codes based distributed storage systems with fixed number of redundancy codes. We generate the encoding <b>matrices</b> of an <b>storage</b> system carefully from the encoding <b>matrices</b> of an <b>storage</b> system to minimize the changes of encoded blocks when scaling. Therefore the system can be scaled up with relatively low bandwidth cost and computation cost...|$|R
40|$|In {{this article}} the {{question}} of determination the dynamic <b>matrix</b> of the <b>storage</b> process which characterized its the parametrical space and matrix parametrical transmissive function which defines the functional structure of the storage process are studied. When you are citing the document, use the following link [URL]...|$|R
40|$|LINSOL is an {{iterative}} linear solver package (solving Ax = b) {{with direct}} solvers as preconditioners. The program package {{is adapted to}} the application of sparse matrices, but can be efficiently applied to full matrices, too. It contains presently eight iterative methods and a polyalgorithm of generalized Conjugate Gradient (CG) methods. The used direct solver is the Gaussian algorithm working on the skyline of the matrix of the linear system. Nine different data structures are supported by LINSOL to ease the embedding of LINSOL into an application {{as well as the}} mapping of arbitrary sparse <b>matrices</b> to <b>storage</b> patterns...|$|R
40|$|Sparse {{matrices}} are occasionally encountered during {{solution of}} various problems {{by means of}} numerical methods, particularly the finite element method. ELLPACK sparse <b>matrix</b> <b>storage</b> scheme, {{one of the most}} widely used methods due to its implementation ease, is investigated in this study. The scheme uses excessive memory due to its definition. For the conventional finite element method, where the node elements are used, the excessive memory caused by redundant entries in the ELLPACK sparse <b>matrix</b> <b>storage</b> scheme becomes negligible for large scale problems. On the other hand, our analyses show that the redundancy is still considerable for the occasions where facet or edge elements have to be used...|$|E
3000|$|... 2) matrix {{and solve}} at each {{subsequent}} time point a corresponding full linear system. In the next section, we therefore develop and justify an approximation where matrix fill-in is controlled {{so that the}} benefits of sparse <b>matrix</b> <b>storage</b> and computation are recovered.|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimitedLarge <b>matrix</b> <b>storage</b> constitutes a limitation on {{the applicability of}} most numerical techniques including the Finite Element Method, when very accurate results are required. This is particularly true when dealing with Boundary Value Problems. In order to surpass this difficulty a new method to solve these problems has been devised which does not require <b>matrix</b> <b>storage</b> while still providing the possibility of accuracy improvement. Although restricted to one-dimensional, linear differential equations of the form Y(n) (x) = f (x) this new approximating technique gives acceptable results. The method will perform equally well for problems with exact or non-exact integrable forcing functions, continuous or discontinuous, or functions existing only {{as a set of}} values at discrete points. [URL] Ecuadorian Nav...|$|E
40|$|The linear optics of the TLS (Taiwan Light Source) {{storage ring}} at SRRC (Synchrotron Radiation Research Center) have been {{experimentally}} determined using the measured orbit response <b>matrix.</b> The <b>storage</b> ring with insertion devices open {{was found to}} have small beta beating with the linear optics very close to the design. The analysis revealed some incorrect wiring of the orbit steering magnets and significant variation in the gains of BPMs. Both of these problems were subsequently corrected. Analysis of the orbit response matrix data also can be used to correct beta beating caused by the insertion devices...|$|R
40|$|This {{research}} is directed towards {{the use of}} polyoxoanions of the early transition metals (primarily tungsten) as possible sequestrants and <b>storage</b> <b>matrices</b> for lanthanide, actinide, and technetium species. The latter substances are important radioactive components of tank wastes from spent commercial nuclear fuel, but are present in low proportion by mass. Technetium is a particularly troublesome component because it is highly mobile in groundwater and is volatilized in vitrification processes currently under examination for long-term storage. Scientific goals: synthesis and characterization of new and selective polyoxotungstate complexes of Ln{sup 3 +}, An{sup 4 +}, UO{sub 2 }{sup 2 +}; exploration of stable polyoxoanions containing Tc (using, in the first instance, Re as a nonradioactive surrogate); thermal conversion of polytungstate complexes to tungsten bronze materials for their evaluation as inert <b>storage</b> <b>matrices.</b> This report summarizes the results after 20 months of a 3 -year project. ...|$|R
3000|$|... (i= 1, 2,…,l) is {{the number}} of {{cognitive}} users inside each CH sub-network. The trust value of call users can be stored in the FC in two forms: Form 1 The number of users in each CH is different. The matrix V is a cell matrix: the number of elements in each row is distinct. FC should reserve a dynamic space to store the trust value <b>matrix.</b> This <b>storage</b> style could save the storage space, on condition that extra users’ number configuration overhead is needed. Because the storage is simple, we set the trust value matrix row size as the same κ = max{k 1,k 2,…,k [...]...|$|R
