40|123|Public
2500|$|In 1969, the RC 4000 <b>Multiprogramming</b> <b>System</b> {{introduced}} the system design philosophy {{of a small}} nucleus [...] "upon which operating systems for different purposes could be built in an orderly manner", what would be called the microkernel approach.|$|E
5000|$|RC 4000 Software: <b>Multiprogramming</b> <b>System</b> (Complete), Regnecentralen, Copenhagen, Denmark (1969) ...|$|E
5000|$|Testing a <b>multiprogramming</b> <b>system,</b> Software—Practice and Experience 3, 2 (April-June), 145-150 ...|$|E
5000|$|In <b>multiprogramming</b> <b>systems,</b> {{the running}} task keeps running until it {{performs}} {{an operation that}} requires waiting for an external event (e.g. reading from a disk or tape, receiving a message from a network, reading from a terminal or other human input device) or until the computer's scheduler forcibly swaps the running task out of the CPU. <b>Multiprogramming</b> <b>systems</b> are designed to maximize CPU usage.|$|R
50|$|<b>Multi{{programming}}</b> <b>systems</b> (Production programmers) - Used {{for programming}} devices in high volumes by Electronics manufacturers and programming centers.|$|R
5000|$|Real-Time <b>Multiprogramming</b> Operating <b>System</b> (RTMOS) was {{a process}} control {{operating}} system {{developed in the}} 1960s by General Electric that supported both real-time computing and multiprogramming. [...] <b>Multiprogramming</b> operating <b>systems</b> are now considered obsolete, having been replaced by multitasking.|$|R
5000|$|THE <b>multiprogramming</b> <b>system</b> for the Electrologica X8 (software based {{virtual memory}} without {{hardware}} support) ...|$|E
5000|$|RC 4000 Software: <b>Multiprogramming</b> <b>System,</b> Part I General Description, Regnecentralen, Copenhagen, Denmark (1969) 13-52 ...|$|E
5000|$|The {{nucleus of}} a <b>multiprogramming</b> <b>system,</b> Communications of the ACM 13, 4 (April 1970), 238-242 ...|$|E
40|$|The {{study of}} the {{queueing}} system presented in this note was motivated by its possible application to a <b>multiprogramming</b> computer <b>system.</b> The paper considers a closed network with two service centres with feedback. The steady-state behaviour of the passage time, the random times for a job to traverse {{a portion of the}} network, is examined through simulation. The model can be applied to a twin-processor <b>multiprogramming</b> computer <b>system.</b> 2000 Mathematics Subject Classification: 60 K 25...|$|R
40|$|Abstract:Producing in the <b>multiprogramming</b> <b>systems</b> of the {{demanding}} paging style,thrashing {{is an extreme}} status coming from doing page replacement. Based on some research results, the cause of thrashing is discussed with describing the feature of it. A policy of "preventing first " is presented {{to hold on to}} refrain from the thrashing,which sets forth the concrete method of preventing and ending the thrashing. Some relating conclusions are also fit for page replacements of the computer designing in other fields...|$|R
40|$|In <b>multiprogramming</b> <b>systems,</b> {{parallel}} processes {{compete for}} access to shared resources and cooperate by exchanging information. Semaphores are a useful means for controlling competition and synchronizing execution and inter-process messages are useful for communication. Neither semaphores nor inter-process message, however, are natural for solving both problems. This paper introduces a new approach, message classes, which combines and extends features of both semaphores and message passing. Using message classes, numerous mutual exclusion, producer/consumer, process communication, and resource allocation problems can be readily solved...|$|R
50|$|Remote {{procedure}} calls used {{in modern}} operating systems trace their roots {{back to the}} RC 4000 <b>multiprogramming</b> <b>system,</b> which used a request-response communication protocol for process synchronization.|$|E
5000|$|IEEE Computer Pioneer Award, for {{pioneering}} {{development in}} operating systems and concurrent programming exemplified by {{work on the}} RC 4000 <b>multiprogramming</b> <b>system,</b> monitors, and Concurrent Pascal, 2002 ...|$|E
5000|$|The {{design of}} the THE <b>multiprogramming</b> <b>system</b> is {{significant}} for its use of a layered structure, in which [...] "higher" [...] layers only depend on [...] "lower" [...] layers: ...|$|E
40|$|The common {{features}} of third generation operating systems are surveyed from a general view, {{with emphasis on}} the common abstractions that constitute at least {{the basis for a}} &quot;theory &quot; of operating systems. Properties of specific systems are not discussed except where examples are useful. The technical aspects of issues and concepts are stressed, the nontechnical aspects mentioned only briefly. A perfunctory knowledge of third generation systems is presumed. Key words and phrases: <b>multiprogramming</b> <b>systems,</b> operating systems, supervisory systems, time-sharing systems, programming, storage allocation, memory allocation, processes, concurrency, parallelism, resource allocation, protection CR categories: 1. 3, 4. 0, 4. 30, 6. 20 It has been the custom to divide the era of electronic computing into &quot;generations&quot; whose approximate dates are...|$|R
40|$|In <b>multiprogramming</b> <b>systems</b> i. e. {{in systems}} where several {{processes}} reside in memory, organization of processes {{is very important}} so that CPU always has one to execute. CPU scheduling is the base of <b>multiprogramming</b> operating <b>systems.</b> CPU executes one process {{at a time and}} switches between processes to improve CPU utilization. CPU scheduling strategies help in selecting the next process to be executed by the CPU. CPU scheduling {{is one of the most}} important activities performed by operating system which helps in increasing the throughput of the computer system therefore if the performance of scheduling will improve then our computer system will become more productive. In this paper, CPU scheduling algorithm with improved performance has been proposed. The technique which is used for increasing the speed up factor is ‘Pipelining’. This technique can be applied to any CPU scheduling algorithm to improve its performance. The analysis shows that the proposed algorithm is better than the existing scheduling algorithms. The performance is improved by 40 - 50 %...|$|R
40|$|The author selects classic papers {{written by}} the {{computer}} scientists who made the major breakthroughs in concurrent programming. These papers cover the pioneering era of the field from the semaphores of the mid 1960 s to the remote procedure calls of the late 1970 s. The author summarizes the classic papers and puts them in historical perspective. A PROGRAMMING REVOLUTION This {{is the story of}} one of the major revolutions in computer programming: the invention of concurrent programming. Tom Kilburn and David Howarth pioneered the use of interrupts to simulate concurrent execution of several programs on the Atlas computer (Kilburn 1961). This programming technique became known as multiprogramming. The early <b>multiprogramming</b> <b>systems</b> were programmed in assembly language without any conceptual foundation. The slightest programming mistake could make these systems behave in a completely erratic manner that made program testing nearly impossible. By the end of the 1960 s <b>multiprogrammed</b> operating <b>systems</b> had become so huge and unreliable that their designers spoke openly of a software crisi...|$|R
5000|$|In 2002, Brinch Hansen {{was awarded}} the IEEE Computer Pioneer Award “For {{pioneering}} development in operating systems and concurrent programming exemplified by work on the RC 4000 <b>multiprogramming</b> <b>system,</b> monitors, and Concurrent Pascal.” ...|$|E
50|$|In {{the summer}} of 1967, Brinch Hansen left Regnecentralen’s {{hardware}} group to become head of RC 4000 software development, where he led a team including Jørn Jensen, Peter Kraft and Søren Lauesen in defining a general-purpose RC 4000 <b>multiprogramming</b> <b>system,</b> with a goal to avoid developing a custom real-time control operating system for every RC 4000 installation, and to support batch processing and time-sharing, as well. The resulting system was not a complete operating system, but a small kernel providing the mechanisms upon which operating systems for different purposes could be built. By the spring of 1969, a well-documented, reliable version of the RC 4000 <b>multiprogramming</b> <b>system</b> was running.|$|E
5000|$|In 1969, the RC 4000 <b>Multiprogramming</b> <b>System</b> {{introduced}} the system design philosophy {{of a small}} nucleus [...] "upon which operating systems for different purposes could be built in an orderly manner", what would be called the microkernel approach.|$|E
40|$|The goal of {{this work}} is to {{facilitate}} efficient use of concurrent shared objects in asynchronous, shared-memory multiprocessors. Shared objects are usually implemented using locks in such systems. However, lock-based implementations result in substantial inefficiency in <b>multiprogrammed</b> <b>systems,</b> where processes are frequently subject to delays due to preemption. This inefficiency arises because processes can be delayed while holding a lock, thereby delaying other processes that require the lock. In contrast, lock-free and wait-free implementations guarantee that the delay of one process will not delay another process. We show that lock-free and wait-free implementations for shared objects provide {{a viable alternative to}} lock-based ones, and that they can provide a significant performance advantage over lock-based implementations in <b>multiprogrammed</b> <b>systems.</b> Lock-free and wait-free implementations are notoriously difficult to design and to verify as correct. Universal constructions alleviate this problem by generating lock-free and wait-free shared object implementations using sequential implementations. However, previous universal constructions either require significant creative {{effort on the part of}} the programmer for each object, or result in objects that have high space and time overhead due to excessive copying. We present lock-free and wait-free universal constructions that achieve low space and time overhead for a wide spectrum of important objects, while not placing any extra burden on the object programmer. We also show that the space and time overhead of thes...|$|R
40|$|In <b>multiprogrammed</b> <b>systems,</b> {{synchronization}} often {{turns out}} to be a performance bottleneck and the source of poor fault-tolerance. Wait-free and lock-free algorithms can do without locking mechanisms, and therefore do not suffer from these problems. We present an efficient almost wait-free algorithm for parallel accessible hashtables, which promises more robust performance and reliability than conventional lock-based implementations. Our solution is as efficient as sequential hashtables. It can easily be implemented using C-like languages and requires on average only constant time for insertion, deletion or accessing of elements. The algorithm allows the hashtables to grow and shrink when needed. A tru...|$|R
50|$|In {{computer}} science, a semaphore is {{a variable}} or {{abstract data type}} used to control access to a common resource by multiple processes in a concurrent system such as a <b>multiprogramming</b> operating <b>system.</b>|$|R
5000|$|Coenraad Bron (2 August 1937 [...] - [...] 15 August 2006) was a Dutch {{computer}} scientist. He {{worked with}} Edsger W. Dijkstra on the THE <b>multiprogramming</b> <b>system.</b> Together with Joep Kerbosch he invented the Bron-Kerbosch algorithm for the clique problem.|$|E
50|$|The RC 4000 <b>multiprogramming</b> <b>system</b> {{introduced}} the now-standard {{concept of an}} operating system kernel and the separation of policy and mechanism in operating system design. Modern microkernel architectures trace their roots to the extensible nucleus architecture of the RC 4000. Improving microkernel performance was a major theme in operating system research for three decades after the RC 4000.|$|E
50|$|The ICT 1302, used similar {{technology}} to the 1300/1301 but was a <b>multiprogramming</b> <b>system</b> capable of running three programs {{in addition to the}} Executive. It also used the 'Standard Interface' for the connection of peripherals allowing much more flexibility in peripheral configuration. The 'Standard Interface' was originally prototyped on the 1301 and went on to be used on the 1900 series.|$|E
50|$|The {{result was}} the {{development}} of an operating system known as the Laboratory Automation Basic Supervisor for the System/7 (LABS/7) and an application development language. LABS/7 was a real-time multitasking, <b>multiprogramming</b> operating <b>system.</b>|$|R
40|$|Queueing network {{models of}} <b>multiprogramming</b> <b>systems</b> with memory {{constraints}} and multiple classes of jobs {{are important in}} representing large conm~rcial computer systems. Typically, an exact analytical solution of such models is unavailable, and, given {{the size of their}} state space, the solution of models of this type is approached through simulation and/or approximation techniques. Recently, a c(m~utationally efficient iterative technique has been proposed by Brandwajn, Lazowska and Zahorjan for models of systems in which each job is subject to a separate memory constraint, i. e., has its own memory domain. In some important applications, it is not unusual, however, to have several jobs o...|$|R
50|$|GCOS is a multithreading, <b>multiprogramming</b> {{operating}} <b>system</b> originally oriented towards batch processing, although later versions incorporated enhancements for timesharing {{and online}} transaction processing environments. Systems running GCOS today use it mainly for batch and OLTP, or as a backend enterprise server.|$|R
5000|$|Per Brinch Hansen {{introduced}} {{the concept of}} separation of policy and mechanism in operating systems in the RC 4000 <b>multiprogramming</b> <b>system.</b> Artsy and Livny, in a 1987 paper, discussed an approach for an operating system design having an [...] "extreme separation of mechanism and policy". In a 2000 article, Chervenak et al. described the principles of mechanism neutrality and policy neutrality.|$|E
50|$|The {{system is}} most notable as the target {{processor}} for Edsger Dijkstra's {{implementation of the}} THE <b>multiprogramming</b> <b>system.</b> This includes the invention of semaphores, enabled by a specific instruction in the X8 instruction set. Semaphores were used {{not only as a}} synchronization mechanism within the THE operating system, but also in the request and response data structures for I/O requests processed by the CHARON coprocessor.|$|E
50|$|Habermann's {{research}} included programming languages, operating systems, {{and development}} of large software systems. He {{was known for his}} work on inter-process communication, process synchronization and deadlock avoidance, and software verification, but particularly for the computer languages ALGOL 60, BLISS, Pascal, and Ada. He also contributed to new operating systems such as Edsger Dijkstra's THE <b>multiprogramming</b> <b>system,</b> the Family of Operating Systems (FAMOS) at Carnegie Mellon, Berlin's Dynamically Adaptable System (DAS), and UNIX.|$|E
40|$|The base {{entity in}} {{computer}} programming {{is the process}} or task. The parallelism {{can be achieved by}} executing multiple processes on different processors. Distributed systems are managed by distributed operating systems that represent the extension for multiprocessor architectures of multitasking and <b>multiprogramming</b> operating <b>systems.</b> ...|$|R
40|$|Abstract. The {{field of}} {{multiple}} processor computer systems has experienced extensive growth in recent years. In order to operate these systems practically, an operating system or extensions {{must be developed}} to adequately handle multiple processors. In this paper, several versions of a <b>multiprogramming</b> multiprocessor <b>system</b> are simulated...|$|R
40|$|The {{notion of}} profile {{scheduling}} {{was first introduced}} by Ullman in 1975 in the complexity analysis of deterministic scheduling algorithms. In such a model, the number of processors available {{to a set of}} tasks may vary in time. Since the last decade, this model has been used to deal with systems subject to processor failures, <b>multiprogrammed</b> <b>systems,</b> or dynamically reconfigured systems. The aim {{of this paper is to}} overview optimal polynomial solutions for scheduling a set of partially ordered tasks in these systems. Particular attentions are given to a class of algorithms referred to as list scheduling algorithms. The objective of the scheduling problem is to minimize either the maximum lateness or the makespan. Results on preemptive and nonpreemptive deterministic scheduling, and on preemptive stochastic scheduling, are presented...|$|R
