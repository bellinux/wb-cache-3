62|0|Public
40|$|This paper {{presents}} a prediction model for software services availability {{measured by the}} mean-time-to-repair (MTTR) and <b>mean-time-to-failure</b> (MTTF) of a service. The prediction model {{is based on the}} experimental identification of probabilistic prediction for variables that affect MTTR/MTTF, based on monitoring service data collected at runtime...|$|E
30|$|Alhad et al. (2008) made {{known that}} the {{production}} system, sometimes, is void of satisfactory overall availability because of downtime initiated by non-conformance to set quality standards or excessive machine/component failures. Initial single station’s <b>mean-time-to-failure</b> (MTTF) and mean-time-to-repair (MTTR) assessment could not ascertain the overall system performance, and dynamic resourcing and customers’ satisfaction were not addressed in old total productive maintenance (TPM) and manufacturing execution systems (MES).|$|E
40|$|The {{intent of}} this paper is to discuss a {{reliability}} calculation technique, using Markov modeling of a parallel redundant system which can be repairable or non-repairable. In this paper, we will use Markov Modeling technique to provide the derivation for the <b>mean-time-to-failure</b> (non-repairable system) or mean-time-between-failure (repairable system) of a parallel redundant system, with different unit failure or repair rate, to evaluate the reliability and dependability of a parallel operative redundant system...|$|E
40|$|Abstract —This paper {{presents}} a Markov model for reliability using {{different types of}} Sensors and spares that replace sensors in case failure occurs. The primary idea in {{this paper is to}} address and analyze the reliability issues to device a reliable and fault tolerance model for a sensor network system. We analyzed the model in terms of reliability and MTTF (<b>Mean-Time-To-Failure).</b> Our research work focus on the mechanism for providing an alternative of a redundant network by replacing the faulty sensor with the available spares. ...|$|E
40|$|The {{effect of}} {{sulphate}} ion concentration on electrochemical migration (ECM) of silver was investigated by applying an in-situ optical and electrical inspection system. It {{was found that}} dendrites grow not only in an electrolyte solution with low sulphate ion concentration but also in electrolytes with medium and high or even saturated sulphate ion concentrations. According to the <b>Mean-Time-To-Failure</b> (MTTF) values, the migration susceptibility was decreased {{with the increase of}} sulphate ion concentration in case of low and medium concentration levels. However, the ECM susceptibility was increased at saturated concentration level...|$|E
40|$|Mathematical {{expressions}} {{are presented}} for the probability distributions and related statistical parameters, such as mean value and standard deviation, of system downtime and resulting {{loss of production}} caused by irregular equipment failure and repair. The expressions have been derived assuming exponential failure and repair distributions for the different pieces of equipment, with a <b>mean-time-to-failure</b> which is {{much larger than the}} mean-time-for-repair. The descriptions can be applied to a variety of system configurations of varying degrees of complexity, i. e. single units, units with standbys, units in parallel, any of these configurations in series and networks of series configurations...|$|E
40|$|Statistical {{inference}} {{methods for}} the Weibull parameters and their functions usually depend on extensive tables, and hence are rather inconvenient for the practical applications. In this paper, we propose a general method for constructing confidence intervals for the Weibull parameters and their functions, which {{eliminates the need}} for the extensive tables. The method is applied to obtain confidence intervals for the scale parameter, the <b>mean-time-to-failure,</b> the percentile function, and the reliability function. Monte-Carlo simulation shows that these intervals possess excellent finite sample properties, having coverage probabilities very close to their nominal levels, irrespective of the sample size and the degree of censorship...|$|E
40|$|A {{successful}} {{development of}} a very high performance and reliable 0. 1 m m power PHEMT MMIC technology is reported. An output power as high as 550 mW (0. 46 W/mm) with 23. 5 % power added efficiency has been demonstrated and a <b>mean-time-to-failure</b> (MTTF) of 1 x 107 hours at a channel temperature of 120 ºC has also been projected from the MMIC at 60 GHz. The developed solid-state power amplifier (SSPA) technology, for the first time, supports the crosslink applications at V-band – the area dominated by TWTAs at all power levels except at a few watts and below where IMPATT diodes have been used...|$|E
40|$|In recent years, Grid Computing {{has emerged}} as a new and {{powerful}} paradigm in the field of high performance computing. However, the running time of a majority of important computational science applications is more than the <b>mean-time-to-failure</b> of these grids. It is important, therefore, to provide some measure of fault tolerance for compute intensive applications, for them to truly harness the power of computational grids. In this paper we describe the design of Merlin, a tool for instrumenting Fortran/MPI programs to make them fault tolerant. Merlin provides application level check pointing in these programs, which is independent of the MPI library being used on heterogeneous nodes...|$|E
40|$|The {{effect of}} Na 2 SO 4 {{concentration}} on electrochemical migration (ECM) of copper and tin was investigated applying an in-situ optical and real-time electrical inspection system. According to the <b>Mean-Time-To-Failure</b> (MTTF) values, the ECM susceptibility of copper has increased {{at low concentration}} levels. However, the ECM susceptibility of copper has decreased at the medium and stopped at the high and even saturated concentration levels. On the other hand, the ECM susceptibility of tin has increased at low levels. Afterwards the ECM ability of tin was hindered and even stopped at medium level. Interestingly, the ECM susceptibility of tin was reappeared at high concentration levels...|$|E
40|$|Separate {{confinement}} single-quantum-well lasers with 100 - 120 A-thick strained Gal -. In,As/GaAs active layers {{have been}} grown on (100) GaAs substrates by metalorganic chemical vapour deposition. Ten-stripe proton-implanted arrays with 90 pm-wide aperture and 250 pm cavity length emit 200 mW CW optical power at wavelengths 0. 87 I i, 5 0. 95 pm. Lifetest data on an uncoated device emitting 90 mW/facet at 50 °C and I = 0. 95 pm suggest a <b>mean-time-to-failure</b> {{in excess of}} 2500 h at room temperature. The performance of lasers with strained Ga,-,In,As quantum wells is {{comparable to that of}} unstrained AI,Gal -,As/GaAs quantum-well lasers without facet coating...|$|E
40|$|This study {{deals with}} the {{performance}} modeling and reliability analysis of a redundant machining system composed of several functional machines. To analyze the more realistic scenarios, the concepts of switching failure and geometric reneging are included. The time-to-breakdown and repair time of operating and standby machines are assumed to follow the exponential distribution. For the quantitative assessment of the machine interference problem, various performance measures such as <b>mean-time-to-failure,</b> reliability, reneging rate, etc. have been formulated. To show the practicability of the developed model, a numerical illustration has been presented. For the practical justification {{and validity of the}} results established, the sensitivity analysis of reliability indices has been presented by varying different system descriptors...|$|E
40|$|This paper {{turns the}} concept of input {{distributions}} on its head to exploit inverse input distributions. Although such distributions are not always true mathematical inverses, they do capture an intuitive property: inputs that have high frequencies in the original distribution will have low frequencies in the inverse distribution, and vice versa. We can use the inverse distribution in several di erent quality checks during development. Here, we will provide a fault-based (fault-injection) method to determine minimum-timeto-failure and <b>mean-time-to-failure</b> for software systems under normal operational and non-normal operational conditions (meaning rare but legal events). In our calculations, we will consider how various programmer faults, design errors, and incoming hardware failures are expected to impact the observability of the software system...|$|E
40|$|The Mu 2 e {{experiment}} at Fermilab will {{search for}} the coherent μ→ e conversion on aluminum atoms. The detector system consists of a straw tube tracker and a crystal calorimeter. A pre-production of 150 Silicon Photomultiplier arrays for the Mu 2 e calorimeter has been procured. A detailed quality assur- ance {{has been carried out}} on each SiPM for the determination of its own operation voltage, gain, dark current and PDE. The measurement of the <b>mean-time-to-failure</b> for a small random sample of the pro-production group has been also completed as well as the determination of the dark current increase {{as a function of the}} ioninizing and non-ioninizing dose. Comment: 4 pages, 10 figures, conference proceeding for NSS-MIC 201...|$|E
40|$|International audienceIn {{the present}} paper, {{different}} analytical lifetime {{models have been}} assessed on porous low-k dielectrics (in CMOS 28 nm structures) to predict lifetimes. Quantitative comparisons are made thanks to over-one-year-long low-field reliability tests. The lucky electron model (fitted at high fields) shows its higher ability to predict the dielectric <b>mean-time-to-failure</b> (MTTF) at low-field. Then the fitting parameters of this model (alpha = 32 et gamma = 14. 5) have been determined independently from this MTTF fitting procedure: these new values (alpha = 31 et gamma = 14), extracted from the analysis of both Poole-Frenkel conduction and initial leakage current, are in very good agreement with the MTTF-fitted ones. The consequences of this model are discussed. (C) 2013 Elsevier B. V. All rights reserved...|$|E
40|$|Racetrack {{memory is}} an {{emerging}} non-volatile memory based on spintronic domain wall technology. It can achieve ultra-high storage density. Also, its read/write speed is com-parable {{to that of}} SRAM. Due to the tape-like structure of its storage cell, a “shift ” operation is introduced to access racetrack memory. Thus, prior research mainly focused on minimizing shift latency/energy of racetrack memory while leveraging its ultra-high storage density. Yet the reliability issue of a shift operation, however, is not well addressed. In fact, racetrack memory suffers from unsuccessful shift due to domain misalignment. Such a problem is called “position error ” in this work. It can significantly reduce <b>mean-time-to-failure</b> (MTTF) of racetrack memory to an intolerable lev-el. Even worse, conventional error correction codes (ECCs) ...|$|E
40|$|As {{the number}} of {{processors}} in today’s high performance computers continues to grow, the <b>mean-time-to-failure</b> of these computers are becoming significantly shorter than the execution time of many current high performance computing applications. Although today’s architectures are usually robust enough to survive node failures without suffering complete system failure, most today’s high performance computing applications can not survive node failures and, therefore, whenever a node fails, have to abort themselves and restart from the beginning or a stable-storage-based checkpoint. This paper explores {{the use of the}} floating-point arithmetic coding approach to build fault survivable high performance computing applications so that they can adapt to node failures without aborting themselves. Despite the use of erasure codes over Galois field has been theoreticall...|$|E
40|$|Abstract — Internet service {{providers}} need {{ways to improve}} the dependability of their email services. Existing single-site and multi-site, synchronously replicated message stores have low availability. Recovery of multi-site, asynchronously replicated stores is slow and inconsistent. This paper’s two-dimensional Markov analysis shows that the availability of an email service is fundamentally limited by site <b>mean-time-to-failure</b> (MTTF), not by the message store MTTF. Replication of the message store at two and three sites can improve the service MTTF by two and four orders of magnitude respectively. One order of magnitude improvement in the storage mean-time-to-repair for two- and three-site replication improves the service MTTF by one and two orders of magnitude respectively. Improvements in the MTTF of current storage technology will not significantly improve email service availability. Keywords- email, availability, distributed hash tables, Markov analysi...|$|E
40|$|We model a {{gracefully}} degrading multiprocessor soft real-time {{system which}} {{is subject to}} both dynamic (software) and static (hardware) failures, without recovery, using a Markov process. We derive the traditional reliability performance measure and <b>mean-time-to-failure</b> (MTTF) of such a system with respect to both failure types and introduce a parameter, expected-deadlinemissed, which is the expected number of tasks that miss their deadlines per time unit at every processor, due to dynamic failures {{in the presence of}} faulty hardware components. Based on this parameter, we define three performance measures suitable for analysis of real-time systems, deadline-missed-reliability, deadline-missed-beforefailure, and deadline-missed-time-reliability. Keywords: real-time systems, performance evaluation, dynamic failure, static failure 1 Introduction Traditional analyses of computer systems are based on measures such as throughput, response time, utilization, availability, and reliability (f [...] ...|$|E
40|$|Main Memory Map Reduce (M 3 R) {{is a new}} {{implementation}} of the Hadoop Map Reduce (HMR) API targeted at online analytics on high <b>mean-time-to-failure</b> clusters. It does not support resilience, and supports only those workloads which can fit into cluster memory. In return, it can run HMR jobs unchanged – including jobs produced by compilers for higher-level languages such as Pig, Jaql, and SystemML and interactive front-ends like IBM BigSheets – while providing significantly better performance than the Hadoop engine on several workloads (e. g. 45 x on some input sizes for sparse matrix vector multiply). M 3 R also supports extensions to the HMR API which can enable Map Reduce jobs to run faster on the M 3 R engine, while not affecting their performance under the Hadoop engine. 1...|$|E
40|$|Abstract—This paper {{presents}} an efficient algorithm for {{the placement of}} power supply pads in flip-chip packaging for high-performance VLSI circuits. The placement problem is formulated as a mixed-integer linear program (MILP), subject to the constraints on <b>mean-time-to-failure</b> (MTTF) for the pads and the voltage drop in the power grid. To improve {{the performance of the}} optimizer, the pad placement problem is solved based on the divide-and-conquer principle, and the locality properties of the power grid are exploited by modeling the distant nodes and sources coarsely, following the coarsening stage in multigrid-like approach. An accurate electromigration (EM) model that captures current crowding and Joule heating effects is developed and integrated with our C 4 placement approach. The effectiveness of the proposed approach is demonstrated on several designs adapted from publicly released benchmarks. I...|$|E
40|$|Exascale {{computers}} {{will enable the}} unraveling of significant scientific mysteries. Predictions are that 2019 will be the year of exascale, with millions of compute nodes and billions of threads of execution. The current architecture of high-end computing systems is decades-old and has persisted as we scaled from gigascales to petascales. In this architecture, storage is completely segregated from the compute resources and are connected via a network interconnect. This approach will not scale several orders of magnitude in terms of concurrency and throughput, and will thus prevent the move from petascale to exascale. At exascale, basic functionality at high concurrency levels will suffer poor performance, and combined with system <b>mean-time-to-failure</b> in hours, {{will lead to a}} performance collapse for large-scale heroic applications. Storage has the potential to b...|$|E
40|$|This paper {{describes}} a software assessment method {{that is being}} implemented to quantitatively assess information system security and survivability. Our approach [...] which we call Adaptive Vulnerability Analysis [...] exercises software (in source-code form) by simulating incoming malicious and non-malicious attacks that fall under various threat classes. A quantitative metric is computed by determining whether the simulated threats undermine {{the security of the}} system as defined by the user according to the application program. This approach stands in contrast to common security assurance methods that rely on black-box techniques for testing completely-installed systems. AVA does not provide an absolute metric, such as <b>mean-time-to-failure,</b> but instead provides a relative metric, allowing a user to compare the security of different versions of the same system, or to compare non-related systems with similar functionality...|$|E
40|$|Abstract. As {{computational}} clusters {{increase in}} size, their <b>mean-time-to-failure</b> reduces. Typically checkpointing {{is used to}} minimize the loss of computation. Most checkpointing techniques, however, require a central storage for storing checkpoints. This severely limits the scalability of checkpointing. We propose a scalable replication-based MPI checkpointing facility {{that is based on}} LAM/MPI. We extend the existing state of fault-tolerant MPI with asynchronous replication, eliminating the need for central or network storage. We evaluate centralized stor-age, SAN-based solutions, and a commercial parallel file system, and show that they are not scalable, particularly beyond 64 CPUs. We demonstrate the low over-head of our replication scheme with the NAS Parallel Benchmarks and the High Performance LINPACK benchmark with tests up to 256 nodes while demonstrat-ing that checkpointing and replication can be achieved with much lower overhead than that provided by current techniques. ...|$|E
40|$|Because of {{increasing}} {{hardware and software}} complexity, the running time of many computational science applications is now more than the <b>mean-time-to-failure</b> of highpeformance computing platforms. Therefore, computational science applications need to tolerate hardware failures. In this paper, {{we focus on the}} stopping failure model in which a faulty process hangs and stops responding {{to the rest of the}} system. We argue that tolerating such faults is best done by an approach called application-level coordinated non-blocking checkpointing, and that existing faulttolerance protocols in the literature are not suitable for implementing this approach. In this paper, we present a suitable protocol, and show how it can be used with a precompiler that instruments C/MPI programs to save application and MPI library state. An advantage of our approach is that it is independent of the MPI implementation. We present experimental results that argue that the overhead of using our system can be small. ...|$|E
40|$|It {{is often}} {{assumed that the}} failure and repair rates of {{components}} are exponentially distributed. This hypothesis is testable for failure rates, though the process of gathering the necessary data and reducing it to a usable form can be difficult. While no amount of testing can prove that a sample is drawn from an exponential distribution, the hypothesis that a population distribution is exponential can in many cases be rejected with confidence. For this study, {{data were collected from}} as many hosts as was feasible using only data that could be obtained via the Internet with no special privileges or added monitoring facilities. The Internet was used to poll over 100; 000 hosts to determine the length of time that each had been up, and again polled after several months to determine average host availability. A surprisingly rich collection of information was gathered in this fashion, allowing estimates of availability, <b>mean-time-to-failure</b> (MTTF) and mean-time-to-repair (MTTR) to be deri [...] ...|$|E
40|$|Abstract—As device {{feature size}} {{continues}} to shrink, reliability becomes a severe issue due to process variation, particle-induced transient errors, and transistor wear-out/stress such as Negative Bias Temperature Instability (NBTI). Unless {{this problem is}} addressed, chip multi-processor (CMP) systems face low yields and short <b>mean-time-to-failure</b> (MTTF). This paper proposes a new design framework for multi-core system that includes device wear-out impact. Based on device fractional NBTI model, we propose a new NBTI aware system workload model, and develop new dynamic tile partition (DTP) algorithm to balance workload among active cores while relaxing stressed ones. Experimental results on 64 cores show that by allowing {{a small number of}} cores (around 10 %) to relax in a short time period (10 second), the proposed methodology improves CMP system yield. We use the percentage of core failure to represent the yield improvement. The new strategy improves the core failure number by 20 %, and extend MTTF by 30 % with little degradation in performance (less than 6 %). I...|$|E
40|$|High {{availability}} of internet {{systems can be}} achieved either through high <b>mean-time-to-failure</b> (MTTF) or low mean-time-to-recovery (MTTR). Traditionally, system designers have focused on maximizing MTTF {{as a way of}} providing high availability. However, recent work in recovery-oriented computing has emphasized recovery from failures, rather than failure avoidance [1]. In this paper, we investigate the impact of MTTR and MTTF, as well as user retry rates, on the user-perceived {{availability of}} internet systems through simulation experiments. Previous work suggests that between two systems with a given availability, the one with lower MTTR always yields higher user-perceived availability [3]. Our results show that while this is true for certain ranges of system parameters, it is not always the case. This leads us to propose a method for determining whether improving MTTR or MTTF will yield greater user-visible benefits for a given system, allowing a system administrator to invest in system improvements that yield maximum benefits. I...|$|E
40|$|Cosmic-ray induced soft {{errors in}} cache {{memories}} {{are becoming a}} major threat to the reliability of microprocessor-based systems. In this paper, we present a new method to accurately estimate the reliability of cache memories. We have measured the MTTF (<b>Mean-Time-To-Failure)</b> of unprotected first-level (L 1) caches for twenty programs taken from SPEC 2000 benchmark suite. Our results show that a 16 KB first-level cache possesses a MTTF of at least 400 years (for a raw error rate of 0. 002 FIT/bit.) However, this MTTF is significantly reduced for higher error rates and larger cache sizes. Our results show that for selected programs, a 64 KB first-level cache is more than 10 times as vulnerable to soft errors versus a 16 KB cache memory. Our work also illustrates that the reliability of cache memories is highly application-dependent. Finally, we present three different techniques to reduce the susceptibility of first-level caches to soft errors by two orders of magnitude. Our analysis shows how to achieve a balanc...|$|E
40|$|Degradation {{models are}} widely {{used to assess the}} {{lifetime}} information for highly reliable products with quality characteristics whose degradation over time can be related to reliability. The performance of a degradation model largely depends on an appropriate model description of the product's degradation path. The cross-platform package iDEMO (integrated degradation models) is developed in R and the interface is built using the Tcl/Tk bindings provided by the tcltk and tcltk 2 packages included with R. It is a tool to build a linear degradation model which can simultaneously consider the unit-to-unit variation, time-dependent structure and measurement error in the degradation paths. The package iDEMO provides the maximum likelihood estimates of the unknown parameters, <b>mean-time-to-failure</b> and q-th quantile, and their corresponding confidence intervals based on the different information matrices. In addition, degradation model selection and goodness-of-fit tests are provided to determine and diagnose the degradation model for the user's current data by the commonly used criteria. By only enabling user interface elements when necessary, input errors are minimized...|$|E
40|$|We {{describe}} four improvements we {{have implemented}} in {{a version of}} the genetic linkage analysis programs in the LINKAGE package: subdivision of recombination classes, better handling of loops, better coordination between the optimization and output routines, and a checkpointing facility. The unifying theme for all the improvements is to store a small amount of data to avoid expensive recomputation of known results. The subdivision of recombination classes improves on a method of Lathrop and Lalouel [Amer. J. Hum. Genetics 42 (1988), pp. 498 [...] 505]. The new method of handling loops extends a proposal of Lange and Elston [Hum. Hered. 25 (1975), pp. 95 [...] 105] for loopless pedigrees with multiple nuclear families at the earliest generation. From a practical point of view, the most important improvement may be the checkpointing facility which allows the user to carry out linkage computations that are much longer than the <b>mean-time-to-failure</b> of the underlying computer...|$|E
40|$|Resistance {{degradation}} in PZT thin-film capacitors {{has been}} studied {{as a function of}} applied voltage, temperature, and film composition. It is found that the <b>mean-time-to-failure</b> (life-time or t{sub f}) of the capacitors shows a power law dependence on applied voltage of he form t{sub f} {proportional_to} V{sup {minus}n} (n {approximately} 4 [...] 5). The capacitor life-time also exhibits a temperature dependence of the form t{sub f} {proportional_to} exp(E{sub a}/kT), with an activation energy of {approximately} 0. 8 eV. The steady-state leakage current in these samples appears to be bulk controlled. The voltage, temperature, and polarity dependence of the leakage current collectively suggest a leakage current mechanism most similar to a Frenkel-Poole process. The life-time and leakage current of the Nb-doped PZT films are superior to the undoped PZT films. This result can be explained based on the point-defect chemistry of the PZT system. Finally, the results indicate that the Nb-doped PZT films meet the essential requirements for decoupling capacitor applications...|$|E
40|$|Modeling the {{reliability}} of distributed systems requires {{a good understanding of}} {{the reliability}} of the components. Careful modeling allows highly fault-tolerant distributed data applications to be constructed at the least cost. Failure and repair rates of components are often as-sumed to be exponentially distributed. This hypothesis is testable for failure rates, though the process of gathering and reducing the data to a usable form can be difficult. By applying an appropriate test statistic, some samples were found to have a realistic chance of being drawn from an exponential distribution, while others can be confidently classed as non-exponential. Data were collected from a large number of hosts via the Internet. Almost all of the visible Internet (over 350; 000 hosts) were considered, and more than 68; 000 of these that were judged likely to respond were queried. These hosts were sampled several times to obtain up-times, and finally to determine average host availability. Estimates of availability, <b>mean-time-to-failure</b> (MTTF) and mean-time-to-repair (MTTR) were derived. The results reported here correspond with those commonly seen in practice. ...|$|E
40|$|Abstract-During the {{back-end}} {{manufacturing process}} of IC, intervention of spot defects induces extra and missing material of interconnects causing circuit failures. In this paper, {{a new type}} of spot defects called interconnect “narrowing defect ” is defined. Interconnect narrowing occurs when spot defects induce missing material of interconnects without resulting in a complete cut. The narrow sites of defective interconnects favor electromigration that makes narrow interconnects more likely to induce a chip failure than regular interconnects. In this paper, a layout sensitivity model accounting for narrowing defects is derived. A methodology for predicting the probability of narrow interconnects using the sensitivity model is then proposed. The layout sensitivity model for narrow interconnects is tested and compared to actual and simulated data. Our layout sensitivity model for narrow interconnects predicts the probability of narrowing with 3. 1 % error, on average. The model is then combined with electromigration constraints to predict <b>mean-time-to-failure</b> of chips manufactured in future technology down to 32 nm node. The paper concludes with some other possible applications of the narrow interconnect predictive model. 1...|$|E
40|$|Abstract. Starting {{with the}} meaning of the word quality, diverse {{concepts}} connoted by the term are examined. Instead of a bathtub curve, the desirable shape of a failure rate covering the entire life of a good product, which might be called hockey-stick line, is introduced. From the hockey-stick line and the definition of reliability, two measurements are extracted. The terms r-reliability (failure rate) and durability (product life) are explained. The conceptual analysis of failure mechanics explains that reliability technology pertains to design area. The desirable shape of hazard rate curve of electronic items, hockey-stick line, clarifies that <b>Mean-Time-to-failure</b> (MTTF) as the inverse of failure rate can be regarded a nominal life. And Bx life, different from MTTF, is explained. Reliability relationships between components and set products are explained. Reshaped definitions of r-reliability and durability are recommended. The procedure to improve reliability and the reasons for failing to identify failure mode are clarified in order to search right solutions. And generalized Life-Stress failure model is recommended for the calculation of acceleration factor...|$|E
40|$|Abstract—Continuous {{transistor}} scaling due to {{improvements in}} CMOS devices and manufacturing technologies is increasing processor power densities and temperatures; thus, creating challenges {{when trying to}} maintain manufacturing yield rates and devices which will be reliable throughout their lifetime. New microarchitectures require new reliabilityaware design methods that can face these challenges without significantly increasing cost and performance. In this paper we present a complete analysis of reliability for the register file architecture of the Leon 3 processor. The analysis conducted {{is supported by the}} use of an accurate HW/SW FPGAbased emulation platform that enables a complete design space exploration of thermal and reliability metrics during the execution of an extended set of benchmarks, in a very limited amount of time. The effect of various compiler optimizations and register assignments on the reliability of the register file is then analyzed. Our results quantify the respective effects of these different factors and enable us to design a reliabilityaware register file assignment policy that consistently improves the <b>Mean-Time-To-Failure</b> figure (20 % on average) for the various types of applications. I...|$|E
