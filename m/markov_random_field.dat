2923|10000|Public
50|$|A hidden <b>Markov</b> <b>random</b> <b>field</b> is a {{generalization}} of a hidden Markov model. Instead {{of having an}} underlying Markov chain, hidden Markov random fields have an underlying <b>Markov</b> <b>random</b> <b>field.</b>|$|E
50|$|When {{the joint}} {{probability}} {{density of the}} random variables is strictly positive, it is {{also referred to as}} a Gibbs random field, because, according to the Hammersley-Clifford theorem, it can then be represented by a Gibbs measure for an appropriate (locally defined) energy function. The prototypical <b>Markov</b> <b>random</b> <b>field</b> is the Ising model; indeed, the <b>Markov</b> <b>random</b> <b>field</b> was introduced as the general setting for the Ising model.In the domain of artificial intelligence, a <b>Markov</b> <b>random</b> <b>field</b> is used to model various low- to mid-level tasks in image processing and computer vision.|$|E
50|$|In {{the domain}} of physics and probability, a <b>Markov</b> <b>random</b> <b>field</b> (often {{abbreviated}} as MRF), Markov network or undirected graphical model {{is a set of}} random variables having a Markov property described by an undirected graph. In other words, a random field is said to be <b>Markov</b> <b>random</b> <b>field</b> if it satisfies Markov properties.|$|E
40|$|A {{branch of}} the {{computer}} vision research deals with statistical methods to model specific problems. <b>Markov</b> <b>Random</b> <b>Fields</b> are a fundamental tool to do so. The solution of a such modeled problem is achieved by maximum a-posteriori inference, which is a hard problem, especially for higher order <b>Markov</b> <b>Random</b> <b>Fields.</b> Therefore pairwise <b>Markov</b> <b>Random</b> <b>Fields</b> have been used up to now mainly. Graph Cut methods {{are one of the}} most efficient inference techniques. Higher order <b>Markov</b> <b>Random</b> <b>Fields</b> allow to build more detailed models to obtain better results. Due to the above mentioned reasons, higher order <b>Markov</b> <b>Random</b> <b>Fields</b> become more and more important today. This master thesis inspects, whether Graph Cut techniques can be used reasonably in higher order <b>Markov</b> <b>Random</b> <b>Fields.</b> Furthermore an overview of <b>Markov</b> <b>Random</b> <b>Fields</b> and their application to model low level vision problems will be given. Graph cut techniques will be explained in detail...|$|R
30|$|Main {{methods for}} image {{segmentation}} {{are based on}} edge information [19, 159], fragment-based approaches [36, 79], point-wise repetition [182], tree partitioning under a normalized cut criterion [160], a nonparametric Bayesian model [115], a geometric active contour model [180], <b>Markov</b> <b>random</b> <b>fields</b> with region growing [123], <b>Markov</b> <b>random</b> <b>fields</b> and graph cut [25], and local Chan–Vese (LCV) model [163].|$|R
40|$|We {{introduce}} {{a set of}} morphologically specified <b>Markov</b> <b>random</b> <b>fields,</b> that extends the standard set of models by using the operators of mathematical morphology. These models capture important morphological features in images and this is illustrated by Monte Carlo simulations. <b>Markov</b> <b>random</b> <b>fields</b> Image analysis Mathematical morphology Shape Monte Carlo simulation...|$|R
5000|$|A <b>Markov</b> <b>random</b> <b>field,</b> or Markov network, may be {{considered}} to be a generalization of a Markov chain in multiple dimensions. In a Markov chain, state depends only on the previous state in time, whereas in a <b>Markov</b> <b>random</b> <b>field,</b> each state depends on its neighbors in any of multiple directions. A <b>Markov</b> <b>random</b> <b>field</b> may be visualized as a field or graph of random variables, where the distribution of each random variable depends on the neighboring variables with which it is connected. More specifically, the joint distribution for any random variable in the graph can be computed as the product of the [...] "clique potentials" [...] of all the cliques in the graph that contain that random variable. Modeling a problem as a <b>Markov</b> <b>random</b> <b>field</b> is useful because it implies that the joint distributions at each vertex in the graph may be computed in this manner.|$|E
5000|$|A Boltzmann {{machine is}} a type of {{stochastic}} recurrent neural network (and <b>Markov</b> <b>Random</b> <b>Field).</b>|$|E
5000|$|Any <b>Markov</b> <b>random</b> <b>field</b> (with a {{strictly}} positive density) {{can be written}} as log-linear model with feature functions [...] such that the full-joint distribution can be written as ...|$|E
40|$|In {{this report}} we are {{interested}} in processes capable of modeling the dynamics of systems containing a large number of components. We especially focus on <b>Markov</b> <b>random</b> <b>fields,</b> a spatial process whose equilibrium probability has a product form structure. <b>Markov</b> <b>random</b> <b>fields</b> are used in various domain such as image processing for error recovery of lost data, artificial intelligence as the underlying mathematical model for Bayesian networks, statistical physics, biology, etc. This report summarizes the some of the mathematical properties of Gibbs measures and <b>Markov</b> <b>Random</b> <b>Fields.</b> November 9, 1994. Progress report [...] - version 1. Working document. Tractable Spatial Processes: Gibbs Measures and <b>Markov</b> <b>Random</b> <b>Fields</b> Jean-Fran¸cois Huard Department of Electrical Engineering and Center for Telecommunications Research Columbia University, New York, NY 10027 - 6699 1. Introduction In this report, {{we are interested}} in processes capable of modeling systems containing a large (but finite) num [...] ...|$|R
5000|$|Classification: K-means, SVM, <b>Markov</b> <b>random</b> <b>fields</b> {{and access}} to all OpenCV machine {{learning}} algorithms ...|$|R
40|$|It was {{recently}} shown that {{there exists a}} family Z <b>Markov</b> <b>random</b> <b>fields</b> which are K but are not isomorphic to Bernoulli shifts [4]. In this paper we show that most distinct members of this family are not isomorphic. This implies {{that there is a}} two parameter family of Z <b>Markov</b> <b>random</b> <b>fields</b> of the same entropy, no two of which are isomorphic...|$|R
5000|$|... where xa is {{the vector}} of {{neighboring}} variable nodes to the factor node a. Any Bayesian network or <b>Markov</b> <b>random</b> <b>field</b> {{can be represented}} as a factor graph.|$|E
5000|$|Given an {{undirected graph}} , {{a set of}} random {{variables}} [...] indexed by [...] form a <b>Markov</b> <b>random</b> <b>field</b> with respect to [...] if they satisfy the local Markov properties: ...|$|E
50|$|A <b>Markov</b> <b>random</b> <b>field,</b> {{also known}} as a Markov network, is a model over an undirected graph. A {{graphical}} model with many repeated subunits can be represented with plate notation.|$|E
5000|$|James Laurie Snell, Ross Kindermann: <b>Markov</b> <b>Random</b> <b>Fields</b> and Their Applications, Amer Mathematical Society, 1980, , ...|$|R
50|$|Yadrenko {{developed}} the spectral theory of homogeneous and isotropic <b>random</b> <b>fields</b> in Euclidean, Hilbert, and Lobachevskii spaces. These results {{were used by}} Yadrenko and his disciples for the solution of important problems of linear prediction and filtration of <b>random</b> <b>fields.</b> He founded the theory of <b>Markov</b> <b>random</b> <b>fields,</b> which represented a new direction {{in the theory of}} <b>random</b> <b>fields.</b> Later, the theory of <b>Markov</b> <b>random</b> <b>fields</b> was further developed in works related to problems of statistical physics and quantum field theory.|$|R
40|$|We {{present a}} PTAS for {{computing}} the maximum a posteriori assignment on Pairwise <b>Markov</b> <b>Random</b> <b>Fields</b> with non-negative weights in planar graphs. This algorithm is practical and {{not far behind}} state-of-the-art techniques in image processing. MAP on Pairwise <b>Markov</b> <b>Random</b> <b>Fields</b> with (possibly) negative weights cannot be approximated unless P = NP, even on planar graphs. We also show via reduction that this yields a PTAS for one scoring function of Correlation Clustering in planar graphs...|$|R
5000|$|A multivariate normal {{distribution}} forms a <b>Markov</b> <b>random</b> <b>field</b> {{with respect to}} a graph [...] if the missing edges correspond to zeros on the precision matrix (the inverse covariance matrix): ...|$|E
5000|$|Several {{kinds of}} random fields exist, {{among them the}} <b>Markov</b> <b>random</b> <b>field</b> (MRF), Gibbs random field (GRF), {{conditional}} random field (CRF), and Gaussian random field. An MRF exhibits the Markovian property ...|$|E
5000|$|Suppose that {{we observe}} a random {{variable}} , where [...]Hidden Markov random fields {{assume that the}} probabilistic nature of [...] is determinedby the unobservable <b>Markov</b> <b>random</b> <b>field</b> , [...]That is, given the neighbors [...] of , is independent of all other [...] (Markov property).The main difference with a hidden Markov model is that neighborhood is not defined in 1 dimensionbut within a network, i.e. [...] is allowed {{to have more than}} the two neighborsthat it would have in a Markov chain.The model is formulated {{in such a way that}} given , [...] are independent (conditional independence of the observable variables given the <b>Markov</b> <b>random</b> <b>field).</b>|$|E
40|$|A new {{framework}} for color image segmentation is in-troduced generalizing {{the concepts of}} point-based and spatially-based methods. This framework is based on <b>Markov</b> <b>Random</b> <b>Fields</b> using a Continuous Gibbs Sampler. The <b>Markov</b> <b>Random</b> <b>Fields</b> approach allows for a rigor-ous computational framework where local and global spatial constraints can be globally optimized. Using a Continuous Gibbs Sampler enables the algorithm to adapt continuous-valued regional prototypes in a manner analogous to vector quantization while the discrete Gibbs Sampler is used to ad-just region boundaries. 1...|$|R
40|$|International audienceWe {{consider}} pairwise <b>Markov</b> <b>random</b> <b>fields</b> {{which have}} a number of important applications in statistical physics, image processing and machine learning such as Ising model and labeling problem to name a couple. Our own motivation comes from the need to produce synthetic models for social networks with attributes. First, we give conditions for rapid mixing of the associated Glauber dynamics and consider interesting particular cases. Then, for pairwise <b>Markov</b> <b>random</b> <b>fields</b> with submodular energy functions we construct monotone perfect simulation...|$|R
40|$|Snakes {{have been}} used {{extensively}} in locating object boundaries. However, in the medical imaging field, many organs in close proximity have similar intensity values, limiting the usefulness of snakes in segmentation of abdominal organs. In this paper, we use the gradient vector flow snake to test the benefits of running snakes on texture features from a <b>Markov</b> <b>Random</b> <b>Fields</b> model {{as opposed to the}} pixel intensity. Our results show that using <b>Markov</b> <b>Random</b> <b>Fields</b> has a positive effect on the effectiveness of snake. 1...|$|R
5000|$|Standard <b>Markov</b> <b>random</b> <b>field</b> (MRF): Associate {{a penalty}} to disagreeing pixels by {{evaluating}} {{the difference between}} their segmentation label (crude measure {{of the length of}} the boundaries). See Boykov and Kolmogorov ICCV 2003 ...|$|E
50|$|A <b>Markov</b> <b>random</b> <b>field</b> extends this {{property}} {{to two or}} more dimensions or to random variables defined for an interconnected network of items. An example of a model for such a field is the Ising model.|$|E
50|$|The {{approximation}} resolution methods do {{not require}} any user involvement in the disambiguation process. They can all {{require the use of}} some theories, such as fuzzy logic, <b>Markov</b> <b>random</b> <b>field,</b> Bayesian networks and hidden Markov models.|$|E
40|$|<b>Random</b> <b>fields</b> are {{stochastic}} processes indexed by {{a multidimensional}} parameter. They possess some interesting properties, e. g. isotropy and the Markov property, and satisfy laws {{of large numbers}} and weak convergence theorems under fairly general conditions. As such, <b>random</b> <b>fields</b> provide {{a powerful tool for}} modelling spatial phenomena in physics, biology, economics, and other social sciences. <b>Markov</b> <b>random</b> <b>fields,</b> <b>random</b> <b>fields,</b> spatial processes...|$|R
5000|$|BGSLibrary {{includes}} the original LBP implementation for motion detection {{as well as}} a new LBP operator variant combined with <b>Markov</b> <b>Random</b> <b>Fields</b> with improved recognition rates and robustness.|$|R
40|$|A {{combinatorial}} {{random variable}} is a {{discrete random variable}} defined over a combinatorial set (e. g., a power set of a given set). In this paper we introduce combinatorial <b>Markov</b> <b>random</b> <b>fields</b> (Comrafs), which are <b>Markov</b> <b>random</b> <b>fields</b> {{where some of the}} nodes are combinatorial random variables. We argue that Comrafs are powerful models for unsupervised learning by showing their relationship with two existing models. We then present a Comraf model for semi-supervised clustering that demonstrates superior results in comparison to an existing semi-supervised scheme (constrained optimization). 1...|$|R
50|$|In statistics, {{iterated}} conditional modes is a deterministic algorithm {{for obtaining}} a configuration {{of a local}} maximum of the joint probability of a <b>Markov</b> <b>random</b> <b>field.</b> It does this by iteratively maximizing the probability of each variable conditioned on the rest.|$|E
5000|$|... #Caption: An {{example of}} a <b>Markov</b> <b>random</b> <b>field.</b> Each edge {{represents}} dependency. In this example: A depends on B and D. B depends on A and D. D depends on A, B, and E. E depends on D and C. C depends on E.|$|E
50|$|Graph cuts, or max-flow/min-cut, is {{a generic}} method for {{minimizing}} a {{particular form of}} energy called <b>Markov</b> <b>random</b> <b>field</b> (MRF) energy. The Graph cuts method {{has been applied to}} image segmentation as well, and it sometimes outperforms the level set method when the model is MRF or can be approximated by MRF.|$|E
40|$|In this paper, we combine two {{previous}} works, the first being {{by the first}} author and K. Nelander, and the second by J. van den Berg and the second author, to show (1) that one can carry out a Propp [...] Wilson exact simulation for all <b>Markov</b> <b>random</b> <b>fields</b> on Z d satisfying a certain high noise assumption, and (2) that all such <b>random</b> <b>fields</b> are a finitary image of a finite state i. i. d. process. (2) is a strengthening of the previously known fact that such <b>random</b> <b>fields</b> are so-called Bernoulli shifts. 1 Introduction A <b>random</b> <b>field</b> with finite state space S indexed by the integer lattice Z d is a random mapping X : Z d ! S, or it can equivalently {{be seen as a}} random element of S Z d. Here we focus on so-called <b>Markov</b> <b>random</b> <b>fields,</b> characterized by having a dependency structure which only propagates via interactions between nearest neighbors in Z d. We specialize further to <b>Markov</b> <b>random</b> <b>fields</b> satsifying a certain high noise assumption, which says that these interactions shou [...] ...|$|R
50|$|Ising {{models are}} now {{considered}} to be a special case of <b>Markov</b> <b>random</b> <b>fields,</b> which find widespread application in various fields, including linguistics, robotics, computer vision, and artificial intelligence.|$|R
5000|$|A cost {{function}} {{that is to}} be minimized to estimate the image coefficient vector. Often this {{cost function}} includes some form of regularization. Sometimes the regularization is based on <b>Markov</b> <b>random</b> <b>fields.</b>|$|R
