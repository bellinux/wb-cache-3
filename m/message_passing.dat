7733|955|Public
5|$|Concurrent {{programming}} languages, libraries, APIs, {{and parallel}} programming models (such as algorithmic skeletons) {{have been created}} for programming parallel computers. These can generally be divided into classes based on the assumptions they make about the underlying memory architecture—shared memory, distributed memory, or shared distributed memory. Shared memory programming languages communicate by manipulating shared memory variables. Distributed memory uses <b>message</b> <b>passing.</b> POSIX Threads and OpenMP {{are two of the}} most widely used shared memory APIs, whereas <b>Message</b> <b>Passing</b> Interface (MPI) is the most widely used message-passing system API. One concept used in programming parallel programs is the future concept, where one part of a program promises to deliver a required datum to another part of a program at some future time.|$|E
5|$|DragonFly's kernel is a hybrid, {{containing}} {{features of}} both monolithic and microkernels, {{such as the}} <b>message</b> <b>passing</b> capability of microkernels enabling larger portions of the OS to benefit from protected memory, as well as retaining the speed of monolithic kernels for certain critical tasks. The messaging subsystem being developed is {{similar to those found}} in microkernels such as Mach, though it is less complex by design. DragonFly's messaging subsystem has the ability to act in either a synchronous or asynchronous fashion, and attempts to use this capability to achieve the best performance possible in any given situation.|$|E
5|$|In November 2006, first-generation {{symmetric}} multiprocessing (SMP) clients were publicly released for open beta testing, {{referred to as}} SMP1. These clients used <b>Message</b> <b>Passing</b> Interface (MPI) communication protocols for parallel processing, as {{at that time the}} GROMACS cores were not designed to be used with multiple threads. This was the first time a distributed computing project had used MPI. Although the clients performed well in Unix-based operating systems such as Linux and macOS, they were troublesome under Windows. On January24, 2010, SMP2, the second generation of the SMP clients and the successor to SMP1, was released as an open beta and replaced the complex MPI with a more reliable thread-based implementation.|$|E
40|$|In {{this paper}} we present algorithms, which given a {{circular}} arrangement of n uniquely numbered processes, determine {{the maximum number}} in a distributive manner. We begin with a simple unidirectional algorithm, in which the number of <b>messages</b> <b>passed</b> is bounded by 2 n log n + 0 (n). By making several improvements to the simple algorithm, we obtain a unidirectional algorithm in which the number of <b>messages</b> <b>passed</b> is bounded by 1. 5 n logn + 0 (n). These algorithms disprove Hirschberg and Sinclair's'conjecture that 0 (n²) is a lower bound {{on the number of}} <b>messages</b> <b>passed</b> in undirectional algorithms for this problem. At the end of the paper we indicate how our methods can be used to improve an algorithm due to Peterson, to obtain a unidirectional algorithm using at most 1. 356 n log n + 0 (n) messages. This is the best bound so far on the number of <b>messages</b> <b>passed</b> in both the bidirectional and unidirectional cases...|$|R
50|$|The Penguin's Strategy -Creating Influence with <b>Messages</b> <b>Passed</b> on From Person to Person (Conecta 2011).|$|R
3000|$|Here, {{an ideal}} {{error-free}} <b>message</b> <b>pass</b> is considered. The algorithm may begin with each factor node v(M [...]...|$|R
5|$|The {{project has}} pioneered {{the use of}} {{graphics}} processing units (GPUs), PlayStation3s, <b>Message</b> <b>Passing</b> Interface (used for computing on multi-core processors), and some Sony Xperia smartphones for distributed computing and scientific research. The project uses statistical simulation methodology that is a paradigm shift from traditional computing methods. As part of the client–server model network architecture, the volunteered machines each receive pieces of a simulation (work units), complete them, and return them to the project's database servers, where the units are compiled into an overall simulation. Volunteers can track their contributions on the Folding@home website, which makes volunteers' participation competitive and encourages long-term involvement.|$|E
25|$|The {{entities}} {{communicate with}} each other by <b>message</b> <b>passing.</b>|$|E
25|$|<b>Message</b> <b>passing</b> is {{the only}} way for {{processes}} to interact.|$|E
30|$|Over erasure channels, DE becomes one-dimensional, and {{it allows}} to analyze and even to {{construct}} capacity-achieving codes [20]. It works by recursively tracking the erasure probability <b>messages</b> <b>passed</b> {{around the edges of}} the graph during IT decoding. Roughly speaking, this means that it recursively computes the fraction of erased <b>messages</b> <b>passed</b> during the IT decoding. Using this technique, the decoding threshold of codes is defined as the supremum value of ε (that is, the worst channel condition) that allows transmission with an arbitrary small error probability assuming N goes to infinity [19].|$|R
30|$|Run one {{iteration}} of the {{sum-product algorithm}} {{to obtain the}} extrinsic LLR <b>messages</b> <b>passed</b> from each LDGM and RCM check nodes to the VN, and obtain their empirical conditional PDFs.|$|R
50|$|Thus, {{the message}} can be decoded iteratively. For other channel models, the <b>messages</b> <b>passed</b> between the {{variable}} nodes and check nodes are real numbers, which express probabilities and likelihoods of belief.|$|R
25|$|Odd-even sort is a {{parallel}} version of bubble sort, for <b>message</b> <b>passing</b> systems.|$|E
25|$|Moreover, a {{parallel}} algorithm {{can be implemented}} either in {{a parallel}} system (using shared memory) or in a distributed system (using <b>message</b> <b>passing).</b> The traditional boundary between parallel and distributed algorithms (choose a suitable network vs. run in any given network) does not lie {{in the same place}} as the boundary between parallel and distributed systems (shared memory vs. <b>message</b> <b>passing).</b>|$|E
25|$|Prolog-MPI is an {{open-source}} SWI-Prolog extension for {{distributed computing}} over the <b>Message</b> <b>Passing</b> Interface. Also {{there are various}} concurrent Prolog programming languages.|$|E
50|$|The {{semantics}} of {{the programming}} language {{are defined by}} defining each program construct as an Actor with its own behavior. Execution is modeled by having Eval <b>messages</b> <b>passed</b> among program constructs during execution.|$|R
50|$|White {{matter is}} the tissue through which <b>messages</b> <b>pass</b> between {{different}} areas of gray matter within the central nervous system. The white matter is white because of the fatty substance (myelin) that surrounds the nerve fibers (axons). This myelin is found in almost all long nerve fibers, and acts as an electrical insulation. This {{is important because it}} allows the <b>messages</b> to <b>pass</b> quickly from place to place.|$|R
6000|$|... "Yes! The {{youth in}} fisherman's oilskins, into whose hands that <b>message</b> <b>passed</b> last night, is Miss Catherine Abbeway. The young lady has {{referred}} me {{to you for}} some explanation as to its being in her possession." ...|$|R
25|$|A {{computer}} program that {{runs in a}} distributed system is called a distributed program, and distributed programming {{is the process of}} writing such programs. There are many alternatives for the <b>message</b> <b>passing</b> mechanism, including pure HTTP, RPC-like connectors and message queues.|$|E
25|$|Distributed {{computing}} also {{refers to}} the use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers, which communicate with each other by <b>message</b> <b>passing.</b>|$|E
25|$|The Amiga {{multitasking}} kernel {{was also}} one of the first to implement a microkernel OS methodology based on a real-time <b>message</b> <b>passing</b> (inter-process communication) core known as Exec (for executive) with dynamically loaded libraries and devices as optional modules around the core.|$|E
3000|$|... which {{maximize}} {{the rate of}} the code under BP decoding, subject to Gaussian approximation (GA) for the <b>messages</b> <b>passed</b> in the decoder[13]. The code design in this case is a linear programming problem of the form ([17], Ch. 4): [...]...|$|R
5000|$|There are {{two main}} senses of the word [...] "message" [...] in computing: {{messages}} between the human users of computer systems that are delivered by those computer systems, and <b>messages</b> <b>passed</b> between programs or between components of a single program, for their own purposes.|$|R
50|$|Compute Cluster Server {{uses the}} Microsoft <b>Messaging</b> <b>Passing</b> Interface v2 (MS-MPI) to {{communicate}} between the processing nodes on the cluster network. It ties nodes {{together with a}} powerful inter-process communication mechanism which can be complex because of communications between hundreds or even thousands of processors working in parallel.|$|R
25|$|While threads require {{external}} library {{support in}} most languages, Erlang provides language-level features for creating and managing processes {{with the aim}} of simplifying concurrent programming. Though all concurrency is explicit in Erlang, processes communicate using <b>message</b> <b>passing</b> instead of shared variables, which removes the need for explicit locks (a locking scheme is still used internally by the VM).|$|E
25|$|Quasi-opportunistic {{supercomputing}} aims {{to provide}} a higher quality of service than opportunistic resource sharing. The quasi-opportunistic approach enables the execution of demanding applications within computer grids by establishing grid-wise resource allocation agreements; and fault tolerant <b>message</b> <b>passing</b> to abstractly shield against {{the failures of the}} underlying resources, thus maintaining some opportunism, while allowing a higher level of control.|$|E
25|$|CSP uses {{explicit}} {{channels for}} <b>message</b> <b>passing,</b> whereas actor systems transmit messages to named destination actors. These approaches {{may also be}} considered duals of each other, {{in the sense that}} processes receiving through a single channel effectively have an identity corresponding to that channel, while the name-based coupling between actors may be broken by constructing actors that behave as channels.|$|E
40|$|This is {{a report}} on {{research}} in which the solution of Maxwell's equations is computed using the finite difference time domain method. The client server paradigm is used to develop a distributed solution. The <b>messaging</b> <b>passing</b> harness PVM is used to implement communication between the nodes I the system...|$|R
50|$|While {{a member}} of Kris Kristofferson's backing band at the Isle of Wight Festival 1970, he made a brief reunion with John Sebastian; Sebastian had been (apparently) unaware of Yanovsky's presence, and was made aware of that by a <b>message</b> <b>passed</b> through the crowd, written on a toilet roll.|$|R
5000|$|In 2003, {{during the}} severe-acute-respiratory-syndrome (SARS) outbreak, a dozen Chinese were {{reportedly}} arrested for sending text messages about SARS. Skype {{reported that it}} was required to filter <b>messages</b> <b>passing</b> through its service for words like [...] "Falun Gong" [...] and [...] "Dalai Lama" [...] before being allowed to operate in China.|$|R
25|$|It wasn't {{long before}} the frame {{communities}} and the rule-based researchers {{realized that there was}} synergy between their approaches. Frames were good for representing the real world, described as classes, subclasses, slots (data values) with various constraints on possible values. Rules were good for representing and utilizing complex logic such as the process to make a medical diagnosis. Integrated systems were developed that combined Frames and Rules. One of the most powerful and well known was the 1983 Knowledge Engineering Environment (KEE) from Intellicorp. KEE had a complete rule engine with forward and backward chaining. It also had a complete frame based knowledge base with triggers, slots (data values), inheritance, and <b>message</b> <b>passing.</b> Although <b>message</b> <b>passing</b> originated in the object-oriented community rather than AI it was quickly embraced by AI researchers as well in environments such as KEE and in the operating systems for Lisp machines from Symbolics, Xerox, and Texas Instruments.|$|E
25|$|In {{computer}} science, communicating {{sequential processes}} (CSP) is a formal language for describing patterns of interaction in concurrent systems. It {{is a member}} of the family of mathematical theories of concurrency known as process algebras, or process calculi, based on <b>message</b> <b>passing</b> via channels. CSP was highly influential in the design of the occam programming language, and also influenced the design of programming languages such as Limbo, RaftLib, Go, Crystal, and Clojure's core.async.|$|E
25|$|The NUPACK web {{application}} is programmed within the Ruby on Rails framework, employing Ajax and the Dojo Toolkit to implement dynamic features and interactive graphics. Plots and graphics are generated using NumPy and matplotlib. The site is supported on current {{versions of the}} web browsers Safari, Chrome, and Firefox. The NUPACK library of analysis and design algorithms is written in the programming language C. Dynamic programs are parallelized using <b>Message</b> <b>Passing</b> Interface (MPI).|$|E
5000|$|Tennessees battery, counter battery, {{and fire}} support {{played a major}} role in the success of the {{invasion}} of Okinawa as reflected by the <b>messages</b> <b>passed</b> through the chain of command from COs on the ground to Tennessee. On 18 April, Rear Admiral Reifsnider, commanding Task Group 51.12, told Captain Heffernan: ...|$|R
40|$|Unlimited asynchronism is {{intolerable}} in real physically distributed computer systems. Such systems, synchronous or not, use clocks and timeouts. Therefore the magnitudes of elapsed {{absolute time}} in the system need to satisfy the axiom of Archimedes. Under this restriction of asynchronicity logically time-independent solutions can be derived which are nonetheless better (in number of <b>message</b> <b>passes)</b> than is possible otherwise. The use of clocks by the individual processors, in elections in a ring of asynchronous processors without central control, allows a deterministic solution which requires but a linear number of <b>message</b> <b>passes.</b> To obtain the result {{it has to be}} assumed that the clocks measure finitely proportional absolute time-spans for their time units, that is, the magnitudes of elapsed {{time in the}} ring network satisfy the axiom of Archimedes. As a result, some basic subtilities associated with distributed computations are highlighted. For instance, the known nonlinear lower bound on the required number of <b>message</b> <b>passes</b> is cracked. For the synchronous case, in which the necessary assumptions hold a fortiori, the method is -asymptotically- the most efficient one yet, and of optimal order of magnitude. The deterministic algorithm is of -asymptotically- optimal bit complexity, and, in the synchronous case, also yields an optimal method to determine the ring size. All of these results improve the known ones...|$|R
30|$|Implementation of TOSC policy {{requires}} two steps: gathering network state information, {{which includes}} queue length and channel condition; and computing the optimal activation vector. Messages incurred when gathering network state information are called <b>messages</b> <b>passing</b> overhead. The complexity {{to calculate the}} activation vector at the BS and the RS is complexity of TOSC.|$|R
