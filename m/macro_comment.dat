0|12|Public
30|$|Paragraphs [1] and [5] give {{introductory}} priming {{sentences and}} a rounding-off paragraph, {{but the other}} sections describe {{the components of the}} balloon and their properties [2], identify the buoyancy and weight forces with an explanation of how the balloon rises by expanding and losing air [3], and explaining how the balloon falls by opening the vent causing denser air from the atmosphere to increase its weight [4]. These <b>Macro</b> <b>comments</b> note the main functions of the paragraphs, but the Micro sentences detail the descriptive properties of the balloon components, show the explanatory cause-effect-consequence links of the processes which cause the balloon to rise and to fall.|$|R
5000|$|The {{following}} fragment gives {{a simple}} example that could {{form part of}} a library for generating HTML code. It defines a <b>commented</b> <b>macro</b> to number sections automatically: ...|$|R
40|$|Reverse {{engineers}} {{depend on}} the automatic extraction of information from source code. Some useful kinds of information [...] -source models [...] -are wellknown: call graphs, file dependences, etc. Predicting every kind of source model that a reverse engineer may need is impossible. We have developed a lightweight approach for generating flexible and tolerant source model extractors from lexical specifications. The approach is lightweight in that the specifications are relatively small and easy to write. It is flexible in that there are few constraints on {{the information in the}} source that can be extracted (e. g., we can extract from <b>macros,</b> <b>comments,</b> etc.). It is tolerant in that information can be extracted from source that cannot necessarily be compiled or linked. In essence, we scan for source constructs that contribute to the specified source model while ignoring constructs that do not contribute to that source model. We have developed tools to support this approach and applied the tools to [...] ...|$|R
40|$|We {{quantize}} the Reissner-Nordström {{black hole}} using {{an adaptation of}} Kuchař's canonical decomposition of the Kruskal extension of the Schwarzschild black hole. The Wheeler-DeWitt equation turns into a functional Schroedinger equation in Gaussian time by coupling the gravitational field to a reference fluid or dust. The physical phase space of the theory is spanned by the mass, M, the charge, Q, the physical radius, R, the dust proper time, τ, and their canonical momenta. The exact solutions of the functional Schroedinger equation imply that {{the difference in the}} areas of the outer and inner horizons is quantized in integer units. This agrees in spirit, but not precisely, with Bekenstein's proposal on the discrete horizon area spectrum of black holes. We also compute the entropy in the microcanonical ensemble and show that the entropy of the Reissner-Nordström black hole is proportional to this quantized difference in horizon areas. Comment: 31 pages, 3 figures, PHYZZX <b>macros.</b> <b>Comments</b> on the wave-functional in the interior and one reference added. To appear in Phys. Rev. ...|$|R
40|$|Reverse {{engineers}} {{depend on}} the automatic extrac-t ion of information from source code. Some use-ful kinds of information+ource models—are well-known: call graphs, file dependence, etc. Predict-ing every kind of source model that a reverse engi-neer may need is impossible. We have developed a lightweight approach for generating flexible and tol-erant source model extractors from lexical specifica-t ions. The approach is lightweight in that the speci-fications are relatively small and easy to write. It is flexible in that there are few constraints on the in-formation in the source that can be extracted (e. g., we can extract from <b>macros,</b> <b>comments,</b> etc.). It is tolerant in that information can be extracted from source that cannot necessarily be compiled or linked. In essence, we scan for source constructs that con-tribute to the specified source model while ignor-ing constructs that do not contribute to that source model. We have developed tools to support this ap-proach and applied the tools to the extraction {{of a number of}} different source models (file dependence, event interactions, call graphs) from systems imple-mented in a variety of programming languages (C, C++, CLOS, Eiffel). We discuss our approach and describe its application to extract source models not available using existing systems; for example, we com-pute the invoked-by relation over Field tools. We compare and contrast our approach to the conven-tional approach of generating source models from a program database...|$|R
40|$|We {{would like}} to thank Ed Prescott and Igor Livshits for helpful discussions, the National Science Foundation and the Federal Reserve Bank of Minneapolis for {{financial}} support, and seminar participants at the 2003 Summer Workshop on Income and Productivity Disparity Across Countries " in Rio, Brazil, the Federal Reserve Bank of Minneapolis, and the Stanford <b>Macro</b> Lunch for <b>comments.</b> Comments are welcome...|$|R
40|$|Karl Walentin, {{as well as}} {{participants}} at the ISOM and the Greater Stockholm <b>Macro</b> Group for <b>comments,</b> to Regis Barnichon and Francesco Zanetti for help with the data, and to Volker Lindenthal for excellent research assistance. The views expressed in this paper are solely {{the responsibility of the}} authors and should not be interpreted as reflecting the views of the Executive Board of Sveriges Riksbank or the National Bureau of Economic Research. NBER working papers are circulated for discussion and comment purposes. They have not been peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies official NBER publications...|$|R
40|$|A {{method is}} {{presented}} for parsing syntactic constructs that {{are permitted to}} appear independently anywhere in a program. Some examples include <b>comments,</b> <b>macros,</b> and constructs for conditional compilation. Each such construct is defined by its own grammar and parsed by a separate coroutine. The coroutine model of parsing allows the program text to be parsed in one pass despite the syntactic inconsistencies among the program text and the additional constructs. The usefulness of the model is demonstrated by showing how a production language parsing method is extended to handle multiple syntax specifications. The implementation of conditional compilation by carrying along two parses in a coroutine manner is also given. The utility of the model is further demonstrated by showing its adaptation to a recursive descent parser...|$|R
40|$|We thank Michel Juillard for {{his help}} with {{computational}} issues and Larry Christiano, Dirk Krueger, and participants at the Penn <b>Macro</b> lunch for <b>comments.</b> Beyond the usual disclaimer, we must note that any views expressed herein {{are those of the}} authors and not necessarily those of the Federal Reserve Bank of Atlanta or the Federal Reserve System. Finally, we also thank the NSF for financial support. The views expressed herein are those of the author(s) and do not necessarily reflect the views of the National Bureau of Economic Research. NBER working papers are circulated for discussion and comment purposes. They have not been peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies official NBER publications...|$|R
40|$|Quasi-Poisson and {{negative}} binomial regression models have {{equal numbers of}} parameters, and either {{could be used for}} overdispersed count data. While they often give similar results, there can be striking differences in estimating the effects of covariates. We explain when and why such differences occur. The variance of a quasi-Poisson model is a linear function of the mean while the variance of a negative binomial model is a quadratic function of the mean. These variance relationships affect the weights in the iteratively weighted least-squares algorithm of fitting models to data. Because the variance {{is a function of the}} mean, large and small counts get weighted differently in quasi-Poisson {{and negative}} binomial regression. We provide an example using harbor seal counts from aerial surveys. These counts are affected by date, time of day, and time relative to low tide. We present results on a data set that showed a dramatic difference on estimating abundance of harbor seals when using quasi- Poisson vs. negative binomial regression. This difference is described and explained in light of the different weighting used in each regression method. A general understanding of weighting can help ecologists choose between these two methods. Two supplemental files are attached below: The NBvsPoi_FINAL SAS program uses a SAS macro to analyze the data in SSEAK 98 _FINAL. txt. The SAS program and <b>macro</b> are <b>commented</b> for further explanation...|$|R
40|$|We {{describe}} how the coherence function, a Fourier frequency-dependent {{measure of the}} linear correlation between time series measured simultaneously in two energy channels, {{can be used in}} conjunction with energy spectra, power spectra, and time delays between energy channels to constrain models of the spectrum and variability of x-ray binaries. Here we present a procedure for estimating the coherence function in the presence of counting noise. We apply this method to the black hole candidates Cyg X [...] 1 and GX 339 [...] 4, and find that the near perfect coherence between low and high energy x-ray photons rules out a wide range of models that postulate: spatially extended fluctuating emission, thermal flares, and overlapping shot-noise. Comment: Latex file (emulateapj <b>macro</b> included, see <b>comments</b> at beginning of file), 1 eps figure. To be published in ApJ Letters, Jan. 1, 199...|$|R
40|$|This text {{is visible}} because the {{document}} has been L ATEXed with comments enabled. Consequently, four kinds of comment—examples below—appear, {{and all of}} the references in the bibliographic database are listed in the references section (provided that you have re-run BibTEX, of course). Also, a frame showing the text area appears on the next page. (There would have been some text in this frame, had not L ATEX resisted all my efforts to put it there.) The text dimensions are a compromise intended to be suitable for either American letter size or European A 4 paper. ] [The <b>comment</b> <b>macros</b> are intended for short notes only; they should not contain paragraph breaks, environments, etc. This is an anonymous comment. ] [TBD: Something to be done. ] [BHS: A comment by Brian. ] [PDG: A comment by Peter. ] i...|$|R

