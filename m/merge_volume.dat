0|44|Public
40|$|Abstract. This paper {{presents}} two visualization techniques {{suitable for}} huge oceanography time-varying volume datasets on high-performance graphics workstations. We first propose an off-line parallel rendering algorithm that <b>merges</b> <b>volume</b> ray-casting and on-the-fly isocontouring. This hybrid technique is quite effective in producing fly-through movies of high resolution. We also describe an interactive rendering algorithm that exploits multi-piped graphics hardware. Through this technique, {{it is possible}} to achieve interactive-time frame rates for huge time-varying volume data streams. While both techniques have been originally developed on an SGI visualization system, they can be also ported to commodity PC cluster environments with great ease. ...|$|R
40|$|The {{measurement}} of object volumes is of large importance for many sectors in industry, including agriculture, transportation, production, and forestry. In this paper, we investigate {{the feasibility of}} using commercial depth-sensing devices based on structured light such as the Kinect camera for volume {{measurement of}} objects of medium size. Using a fixed set-up, depth data are acquired for different views of the object and <b>merged.</b> <b>Volumes</b> are carved using a volume-intersection approach, which is computationally simple, and, most importantly, model-free. The performance of the method is evaluated using ground-truth volumes of a benchmark data set of selected objects, and volume-measurement errors are reported {{for a set of}} household objects. Peer ReviewedPostprint (published version...|$|R
50|$|The {{legacies}} of both papers are acknowledged {{on the editorial}} pages today, {{with the names of}} the Sentinels Solomon Juneau and the Journals Lucius Nieman and Harry J. Grant listed below their respective newspaper's flags. The <b>merged</b> paper's <b>volume</b> and edition numbers follow those of the Journal.|$|R
40|$|For {{three-dimensional}} reconstruction of single par-ticles from cryoelectron microscope images, {{one has to}} carefully check directions of projections sampled by the image data set. A gap in angular coverage might produce a stretching of the structure (i. e., missing-cone artifact, when using the random conical tilt series reconstruction technique). This problem may be solved by <b>merging</b> <b>volumes</b> obtained from various types of electron microscope views or by {{taking into account the}} possible point-group symmetry of the particle. In case of very low signal-to-noise ratio an oversampling in one direction of projection may produce a similar effect. A fast solution, termed topological selection, is to remove overabundant images to regain the even angular distribution, but other solutions allowing one to keep the entire data set for 3 D reconstruction are discussed. These situations are simulated on a test volume and are described in experimental examples. Key Words: Cryoelectron microscopy, {{three-dimensional reconstruction}}, single particle molecular architecture. *Address for correspondence...|$|R
40|$|Trabajo presentado al 16 th Catalan Conference on Artificial Intelligence celebrado del 23 al 25 de octubre de 2013 en Vic. Más información en [URL] {{measurement}} of object volumes is of large importance for many sectors in industry, including agriculture, transportation, production, and forestry. In this paper, we investigate {{the feasibility of}} using commercial depth-sensing devices based on structured light such as the Kinect camera for volume {{measurement of}} objects of medium size. Using a fixed set-up, depth data are acquired for different views of the object and <b>merged.</b> <b>Volumes</b> are carved using a volume-intersection approach, which is computationally simple, and, most importantly, model-free. The performance of the method is evaluated using ground-truth volumes of a benchmark data set of selected objects, and volume-measurement errors are reported {{for a set of}} household objects. This work received support from the CSIC project MVOD under project no. 201250 E 028. B. D. thanks the Spanish Ministry for Science and Innovation for support through a Ramon y Cajal program. Peer Reviewe...|$|R
40|$|Abstract — We {{present a}} novel vector {{quantization}} method for pattern classification tasks. The input space is quantized into volume regions by code-vectors formed by weights of neurons. During training, the <b>volume</b> regions are <b>merged</b> and split, {{depending upon the}} ambiguity in classification, measured using Kullback-Leibler divergence. The heuristic followed is to split ambiguous regions, and <b>merge</b> two <b>volume</b> regions if they contain predominant populations of the same class. The neural network forms a generalized Delaunay graph, whose topology changes dynamically with the merging and splitting. The simulation results indicate {{the utility of the}} proposed method. I...|$|R
40|$|Abstract: The aim of {{this work}} is the {{development}} of a non-invasive technique for efficient and accurate volume quantization of the cerebellum of mice. This enables an in-vivo study {{on the development of the}} cerebellum in order to define possible alterations in cerebellum volume of transgenic mice. We concentrate on a semi-automatic segmentation procedure to extract the cerebellum from 3 D magnetic resonance data. The proposed technique uses a 3 D variant of Vincent and Soille's immersion based watershed algorithm which is applied to the gradient magnitude of the MR data. The algorithm results in a partitioning of the data in volume primitives. The known drawback of the watershed algorithm, over-segmentation, is strongly reduced by a priori application of an adaptive anisotropic diffusion filter on the gradient magnitude data. In addition, over-segmentation is a posteriori contingently reduced by properly <b>merging</b> <b>volume</b> primitives, based on the minimum description length principle. The outcome of the preceding image processing step is presented to the user for manual segmentation. The first slice which contains the object of interest is quickly segmented by the user through selection of basic image regions. In the sequel, the subsequent slices are automatically segmented. The segmentation results are contingently manually corrected. The technique is tested on phantom objects, where segmentation errors less than 2 % were observed. Three-dimensional reconstructions of the segmented data are shown for the mouse cerebellum and the mouse brains in toto...|$|R
40|$|Dynamic merge control, or {{junction}} control, regulates or closes specific lanes upstream of an interchange. Agencies can modify access {{based on}} traffic demand from two entering roadways. Control strategies improve {{the operation of}} roads that have more lanes entering the merge area than leaving. A potential U. S. application of this technique would be at a two-lane entrance ramp where the left lane of the entrance ramp merges with the outside lane of the freeway. With dynamic merge control, either the outside freeway lane or the left lane of the entrance ramp would be closed upstream of the merge (depending on the traffic volume). The intent is to provide smoother traffic merging and higher speeds, which will result in more reliable travel times for the higher traffic <b>volume.</b> Dynamic <b>merge</b> control can be a permanent application at known bottlenecks, {{or it can be}} used temporarily for special events or until a downstream roadway is widened. It is a practical approach to handling varying traffic demand on the main lanes and the merging lanes to effectively utilize existing capacity. Target Market � Freeways or roads experiencing frequent congestion and significant <b>merging</b> <b>volumes.</b> Facilities with available capacity on main lanes upstream of an interchange. Roads where traffic volumes on two connecting roads peak at different times. How Will This Help? Dynamic merge control can delay the onset of congestion. By increasing capacity for the higher volume flow and encouraging more uniform speeds, traffic flows more smoothly and efficiently resulting in improved travel time reliability...|$|R
40|$|High speed Optical Coherence Tomography (OCT) {{has made}} it {{possible}} to rapidly capture densely sampled 3 D volume data. One key application is the acquisition of high quality in vivo volumetric data sets of the human retina. Since the volume is acquired in a few seconds, eye movement during the scan process leads to distortion, which limits the accuracy of quantitative measurements using 3 D OCT data. In this paper, we present a novel software based method to correct motion artifacts in OCT raster scans. Motion compensation is performed retrospectively using image registration algorithms on the OCT data sets themselves. Multiple, successively acquired volume scans with orthogonal fast scan directions are registered retrospectively in order to estimate and correct eye motion. Registration is performed by optimizing a large scale numerical problem as given by a global objective function using one dense displacement field for each input volume and special regularization based on the time structure of the acquisition process. After optimization, each volume is undistorted and a single <b>merged</b> <b>volume</b> is constructed that has superior signal quality compared to the input volumes. Experiments were performed using 3 D OCT data from the macula and optic nerve head acquired with a high-speed ultra-high resolution 850 nm spectral OCT as well as wide field data acquired with a 1050 nm swept source OCT instrument. Evaluation of registration performance and result stability as well as visual inspection shows that the algorithm can correct for motion in all three dimensions and on a per A-scan basis. Corrected volumes do not show visible motion artifacts. In addition, merging multiple motion corrected and registered volumes leads to improved signal quality. These results demonstrate that motion correction and merging improves image quality and should also improve morphometric measurement accuracy from volumetric OCT data...|$|R
40|$|Surface {{and volume}} plasmons excited {{in a metal}} cluster by moving {{electron}} and corresponding inelastic scattering spectra are studied based on the hydrodynamic approach. Along with the bulk losses traditionally taken into account, the surface and radiative ones are also considered as the physical mechanisms responsible for the plasmon damping. The second and third mechanisms {{are found to be}} essential for the surface plasmons and depend very differently on the multipole mode order. The differential equations are obtained which describe the temporal evolution of every particular mode as that one of a linear oscillator excited by the given external force, and the electron energy loss spectra are calculated. The changes in spectrum shape with the impact parameter and with the electron passage time are analyzed and found to be in good enough agreement with the data of scanning transmission electron microscopy (STEM) experiments. It is shown that, in the general case, a pronounced contribution to the formation of the loss spectrum is given by the both surface and volume plasmons with low and high multipole indices. In particular, at long electron passage time, the integral loss spectrum which is calculated for the free-electron cluster model contains two main peaks: a broad peak from merging of many high-order multipole resonances of the surface plasmons and a narrower peak of nearly the same height from <b>merged</b> <b>volume</b> plasmons excited by the electrons that travel through the central region of the cluster. Comparatively complex dependences of the calculated excitation coefficients and damping constants of various plasmons on the order of the excited multipole result in wide diversity of possible types of the loss spectrum even for the same cluster material and should be taken into account in interpretation of corresponding electron energy loss spectroscopy (EELS) experiments. Comment: 16 pages, 3 figure...|$|R
50|$|She & Him's second album, Volume Two, was {{released}} on <b>Merge</b> in 2010. <b>Volume</b> Two peaked at #6 on Billboard's Top 200, outperforming their first release. A Very She & Him Christmas {{was released}} October 25, 2011 on Merge Records, peaking at #12. After a break, She & Him released their third album of original songs, Volume 3 in 2013 followed by a collection of standards, Classics, in 2014. Both albums also charted and were released on Merge.|$|R
40|$|The aim of {{this work}} is the {{development}} of a semiautomatic segmentation technique for e#cient and accurate volume quantization of Magnetic Resonance #MR# data. The proposed technique uses a 3 D variant of Vincent and Soilles immersion-based watershed algorithm which is applied to the gradient magnitude of the MR data and which produces small volume primitives. The known drawback of the watershed algorithm, oversegmentation, is strongly reduced by a priori application of a 3 D adaptive anisotropic di#usion #lter to the MR data. Furthermore, oversegmentation is a posteriori reduced by properly <b>merging</b> small <b>volume</b> primitives whichhave similar gray level distributions. The outcome of the preceding image processing steps is presented to the user for manual segmentation. Through selection of volume primitives, the user quickly segments the #rst slice which contains the object of interest. Afterwards, the subsequent slices are automatically segmented by extrapolation. Segmentation results are [...] ...|$|R
40|$|Droplet {{evaporation}} under confinement is ubiquitous to {{multitude of}} {{applications such as}} microfluidics, surface patterning, and ink-jet printing. However, the rich physics governing the universality in the underlying dynamics remains grossly elusive. Here, we bring out hitherto unexplored universal features of the evaporation dynamics of a sessile droplet entrapped in a 3 D confined fluidic environment. We show, through extensive set of experiments and theoretical formulations, that the evaporation timescale for such a droplet can be represented by a unique function of the initial conditions. Moreover, using same theoretical considerations, {{we are able to}} trace and universally <b>merge</b> the <b>volume</b> evolution history of the droplets along with evaporation lifetimes, irrespective of the extent of confinement. We also showcase the internal flow transitions caused by spatio-temporal variation of evaporation flux due to confinement. These findings may be of profound importance in designing functionalized droplet evaporation devices for emerging engineering and biomedical applications. Published by AIP Publishing...|$|R
40|$|AbstractTraffic {{incidents}} {{are a major}} source of congestion and travel time uncertainty. Traditionally, extensive attention has been given to accidents in the view of safety when studying occurrence frequency. The regularity of incident frequency, however, deserves equal attention by practitioners and researchers, especially on urban expressways with dense ramps and high traffic volume. The objective {{of this study was to}} have a thorough exploration of environmental and traffic-related causative factors of incident rate on three urban expressways in central Shanghai City, including disability incidents and crash incidents. Incident data obtained by CCTV-monitoring system were used, which contain large quantities of minor and short-duration incidents. The disaggregation of expressway sections and time intervals of this study was rare in its scope: disability frequency is analyzed on an hourly basis and segment-hour aggregation is applied for crash frequency. To account for temporal correlation among different time intervals, Generalized Estimation Equation procedure was used in this paper. In particular, the effects of traffic interaction features on incident occurrence were analyzed by considering segment length, <b>merging</b> and diverging <b>volume.</b> Results showed that temporal correlation of crash incident occurrence was larger than that of disability occurrence. There is a significant relationship of disability rate with rain and temperature, and there was more risk of vehicle disability in dense-traffic and low-speed condition. It also pointed out that the regularity of crash incident occurrence is quite different from that of accidents on highways or rural freeways: Environmental factors exert little impact on crash occurrence except from visibility; Short segment, high <b>merging</b> and diverging <b>volume</b> increased crash rate remarkably...|$|R
40|$|Abstract—In this paper, {{we propose}} a Bayesian {{approach}} to video object segmentation. Our method {{consists of two}} stages. In the first stage, we partition the video data into a set of 3 D watershed volumes, where each watershed volume {{is a series of}} corresponding 2 D image regions. These 2 D image regions are obtained by applying to each image frame the marker-controlled watershed segmentation, where the markers are extracted by first generating a set of initial markers via temporal tracking and then refining the markers with two shrinking schemes: the iterative adaptive erosion and the verification against a pre-simplified watershed segmentation. Next, in the second stage, we use a Markov random field to model the spatio-temporal relationship among the 3 D watershed volumes that are obtained from the first stage. Then, the desired video objects can be extracted by <b>merging</b> watershed <b>volumes</b> having similar motion characteristics within a Bayesian framework. A major advantage of this method is that it can take into account the global motion information contained in each watershed volume. Our experiments have shown that the proposed method has potential for extracting moving objects from a video sequence...|$|R
40|$|Abstract Introduction The {{mechanical}} competence parameter (MCP) of the trabecular bone is a parameter that <b>merges</b> the <b>volume</b> fraction, connectivity, tortuosity and Young modulus of elasticity, {{to provide}} a single measure of the trabecular bone structural quality. Methods As the MCP is estimated for 3 D images and the Young modulus simulations are quite consuming, in this paper, an alternative approach to estimate the MCP based on artificial neural network (ANN) is discussed considering as the training set a group of 23 in vitro vertebrae and 12 distal radius samples obtained by microcomputed tomography (&# 956;CT), and 83 in vivo distal radius magnetic resonance image samples (MRI). Results It is shown that the ANN was able to predict with very high accuracy the MCP for 29 new samples, being 6 vertebrae and 3 distal radius bones by &# 956;CT and 20 distal radius bone by MRI. Conclusion There is a strong correlation (R 2 = 0. 97) between both techniques and, despite {{the small number of}} testing samples, the Bland-Altman analysis shows that ANN is within the limits of agreement to estimate the MCP...|$|R
40|$|Volume {{reconstruction}} is {{a technique}} for visualization of a biological specimen which {{is greater than the}} field of view of a used optical instrument - a confocal laser scanning microscope in our case. The first step of volume reconstruction is acquisition of sets of digital volume images (spatial tiles which overlap) from all studied physical slices. The second step is horizontal merging of overlapping spatial tiles of the same physical slice (mosaicking). The third reconstruction step is vertical <b>merging</b> of digital <b>volumes</b> of successive physical slices of the specimen. The resulting large digital volumes are visualized using a VolumePro hardware board that offers real-time 3 D volume rendering. In this paper we show a reconstruction of a chick embryonic kidne...|$|R
40|$|This paper {{discusses}} how {{an ecological}} dynamics framework {{can be implemented}} to interpret data, design practice tasks and interpret athletic performance in collective sports, exemplified here by research ideas within the Augmented peRCeption ANalysis framEwork for Football (ARCANE) project promoting an augmented perception of football teams for scientists and practitioners. An ecological dynamics rationale can provide an interpretation of athletes’ positional and physiological data during performance, using new methods to assess athletes’ behaviours in real-time and, to some extent, predict health and performance outcomes. The proposed approach signals practical applications for coaches, sports analysts, exercise physiologists and practitioners through <b>merging</b> a large <b>volume</b> of data into a smaller set of variables, resulting in a deeper analysis than typical measures of performance outcomes of competitive games...|$|R
50|$|The label's {{first album}} {{to reach the}} USA Billboard 200 was Arcade Fire's Funeral, a 2004 release. Arcade Fire gave the label its then highest-charting release with their follow-up, 2007's Neon Bible, which debuted at #2 on the Billboard 200, and, later, {{reaching}} #1 with their third album, 2010's The Suburbs. Other Billboard Top Ten releases include Spoon's Ga Ga Ga Ga Ga and Transference, along with She & Him's (actress/musician Zooey Deschanel along with M. Ward, a popular <b>Merge</b> folk musician) <b>Volume</b> Two. Other notable Merge releases include Neutral Milk Hotel's In the Aeroplane Over the Sea, The Magnetic Fields's 69 Love Songs, Caribou's Polaris Prize-winning Andorra, M. Ward's Hold Time, Camera Obscura's Let's Get Out of This Country, and She & Him's Volume One.|$|R
40|$|Abstract—In this letter, {{we propose}} a Bayesian {{approach}} to video object segmentation. Our method {{consists of two}} stages. In the first stage, we partition the video data into a set of three-dimen-sional (3 -D) watershed volumes, where each watershed volume {{is a series of}} corresponding two-dimensional (2 -D) image regions. These 2 -D image regions are obtained by applying to each image frame the marker-controlled watershed segmentation, where the markers are extracted by first generating a set of initial markers via temporal tracking and then refining the markers with two shrinking schemes: the iterative adaptive erosion and the veri-fication against a presimplified watershed segmentation. Next, in the second stage, we use a Markov random field to model the spatio-temporal relationship among the 3 -D watershed volumes that are obtained from the first stage. Then, the desired video objects can be extracted by <b>merging</b> watershed <b>volumes</b> having similar motion characteristics within a Bayesian framework. A major advantage of this method is that it can take into account the global motion information contained in each watershed volume. Our experiments have shown that the proposed method has potential for extracting moving objects from a video sequence. Index Terms—Markov random field, three-dimensional (3 -D) watershed volume, video object segmentation, watershed segmen-tation. I...|$|R
40|$|We apply volume {{reconstruction}} for {{visualization of}} a biological specimen {{greater than the}} field of view of a confocal laser scanning microscope. Prior to the volume reconstruction, large specimens are cut into thin physical slices. The first step of volume reconstruction is acquisition of digital volume images (spatial tiles which overlap) from all studied physical slices. The second step is horizontal merging of overlapping spatial tiles of the same physical slice using a registration algorithm based on a mutual information and translation. The third reconstruction step is vertical <b>merging</b> of digital <b>volumes</b> of successive physical slices using an elastic registration algorithm based on B-splines. The resulting large digital volumes are visualized by a VolumePro hardware board that provides volume rendering in real-time. In this paper we show a reconstruction of a chick embryonic kidney...|$|R
40|$|The {{purpose of}} this study is to develop a {{technique}} for reducing the number of false positives affecting lung nodule computer–aided detection in computed tomography (CT) images. Contiguous 2 D regions of interest found on segmented lung areas from sections of a CT scan are <b>merged</b> to form <b>volumes</b> of interest (VOIs). Feature vectors are then computed by submitting each VOI to the 3 D ranklet transform, i. e., a non–parametric, orientation–selective and multi–resolution transform developed and evaluated herein. Finally, a support vector machine classifier is used to discriminate VOIs containing nodules from those containing normal tissue. The proposed approach is evaluated on data consisting of 25 nodules marked by experienced thoracic radiologists and 1048 non–nodules randomly selected within the segmented lung volume of healthy patients. By achieving 96...|$|R
40|$|Liquid and gas {{interactions}} often contain bubbles {{surrounded by}} thin liquid films. Simulation of these liquid films is challenging since they quickly become thinner than the grid resolution, {{which leads to}} premature bursting or merging of the bubbles. We prevent this thinning process by applying a disjoining force to the film, obtaining bubbles that last much longer without bursting or merging. The surface tension on the liquid film is the next diffuculty. Since the level set is not differentiable {{at the center of}} the thin liquid film, the curvature computed from the level set gradient is noisy, and the thin liquid film ruptures quickly. To prevent this, we compute the surface tension from the local isosurface, obtaining long-lasting liquid films. However, since bubbles stay longer without bursting or <b>merging,</b> the <b>volume</b> loss of each bubble is noticeable. To solve this problem, we modify the pressure projection to produce a velocity field whose divergence is controlled by the proportional and integral feedback. This allows us to preserve the volume or, if desired, to inflate or deflate the bubbles. In addition to premature bursting and volume change, another difficulty is the complicated liquid surface, which increases memory and computational costs. To reduce storage requirement, we collocate the velocity and pressure to simplify the octree mesh. To reduce the computational complexity of the pressure projection, we use a multigrid method...|$|R
40|$|Digital volume {{reconstruction}} is {{a technique}} for rendering and visualization of a biological specimen which {{is greater than the}} field of view of a used optical instrument - a confocal laser scanning microscope in our case. Prior to the volume reconstruction, large biological specimens are cut to thin physical slices. The first step of volume reconstruction is acquisition of sets of digital volume images (spatial tiles which overlap) from all studied physical slices. The second step is composition of neighbouring spatial tiles of the same physical slice. The third reconstruction step is registration and <b>merging</b> of digital <b>volumes</b> of neighbouring physical slices of the specimen. The resulting large digital volumes are rendered and visualized using a VolumePro hardware board that offers real-time 3 D volume rendering. In this paper we show a reconstruction of a chick embryonic kidne...|$|R
40|$|Satellite {{remote sensing}} systems provide a {{tremendous}} {{source of data}} flow to the Earth science community. These systems provide scientists with data of types and on a scale previously unattainable. Looking forward to the capabilities of Space Station and the Earth Observing System (EOS), the full realization of the potential of satellite remote sensing will be handicapped by inadequate information systems. There is a growing emphasis in Earth science research to ask questions which are multidisciplinary in nature and global in scale. Many of these research projects emphasize the interactions of the land surface, the atmosphere, and the oceans through various physical mechanisms. Conducting this research requires large and complex data sets and teams of multidisciplinary scientists, often working at remote locations. A review {{of the problems of}} <b>merging</b> these large <b>volumes</b> of data into spatially referenced and manageable data sets is presented...|$|R
40|$|Any {{modeling}} {{scheme for}} gaseous phenomena in graphics has to capture three aspects: the fuzzy {{geometry of the}} gas, the dynamics, characterized {{by the presence of}} vortices, and the interaction of light with the gaseous volume. We represent the gaseous volume as a particle system and apply vortex element methods (VEM) to model the dynamics. A Lagrangian formulation that is gridless and hence ideal for unbounded flows is used. A gridless approach to ray tracing the particle systems is developed using particle maps. These maps are used to estimate densities within a gaseous volume analogous to the way volume photon maps are used to estimate radiance during Monte Carlo ray tracing. A technique is proposed to <b>merge</b> particle and <b>volume</b> photon maps to obtain an effective method for simulating multiple scattering in a dynamic inhomogeneous participating medium. Our method for modeling and rendering gaseous phenomena is conceptually simple and grid free. Particle maps play an effective role, as the nearest neighbor information obtained during the rendering phase is exploited during the dynamics computation. We present results that demonstrate the effectiveness of our approach...|$|R
40|$|It {{is often}} the case that multiplications of whole spectra, {{component}} by component, must be carried out, for example when light reflects from or is transmitted through materials. This leads to particularly taxing calculations, especially in spectrally based ray tracing or radiosity in graphics, making a full-spectrum method prohibitively expensive. Nevertheless, using full spectra is attractive because of the many important phenomena that can be modeled only by using all the physics at hand. We apply to the task of spectral multiplication a method previously used in modeling RGB-based light propagation. We show that we can often multiply spectra without carrying out spectral multiplication. In previous work [J. Opt. Soc. Am. A 11, 1553 (1994) ] we developed a method called spectral sharpening, which took camera RGBs to a special sharp basis that was designed to render illuminant change simple to model. Specifically, in the new basis, one can effectively model illuminant change by using a diagonal matrix rather than the 3 × 3 linear transform that results from a three-component finite-dimensional model [G. Healey and D. Slater, J. Opt. Soc. Am. A 11, 3003 (1994) ]. We apply this idea of sharpening to the set of principal components vectors derived from a representative set of spectra that might reasonably be encountered in a given application. With respect to the sharp spectral basis, we show that spectral multiplications can be modeled as the multiplication of the basis coefficients. These new product coefficients applied to the sharp basis serve to accurately reconstruct the spectral product. Although the method is quite general, we show how to use spectral modeling by taking advantage of metameric surfaces, ones that match under one light but not another, for tasks such as volume rendering. The use of metamers allows a user to pick out or <b>merge</b> different <b>volume</b> structures in real time simply by changing the lighting...|$|R
40|$|At {{the end of}} the Nineties the Emilia-Romagna updates its twenty-year {{conservation}} and restoration policy launching the LR 19 / 98 on urban regeneration; then {{at the beginning of the}} Millennium among the first regions it legislate about architectural quality (LR 16 / 02), which increases the possibility of intervention for urban spaces of historic and artistic interest, but also introduces funding competitions for the design, construction of contemporary architecture, inclusion of works of art in public buildings, studies and research on historical and contemporary architectural heritage, elimination of incongruous works. In addition to several significant interventions, the Law 16 / 02 develop important research activities: the survey, <b>merged</b> in the <b>volume</b> “Quale e Quanta” (2005), about the quality of architecture of the late twentieth century, which identifies more than one thousand significant buildings in the Region; a survey of works of art pursuant to the “law of 2 percent” (L. 717 / 49) in public buildings, accompanied by a proposal for a regional law on the same subject; the initiative “Selezione Architettura Emilia-Romagna”, biennial event that chooses the best of architectural production from 2001 onwards...|$|R
40|$|We derive {{the close}} pair {{fractions}} and volume merger rates for {{galaxies in the}} GAMA survey with − 23 < Mr < − 17 (ΩM = 0. 27, ΩΛ = 0. 73, H 0 = 100 km s− 1 Mpc− 1) at 0. 01 < z < 0. 22 (lookback time of < 2 Gyr). The merger fraction is approximately 1. 5 % per Gyr at all luminosities (assuming 50 % of pairs <b>merge)</b> and the <b>volume</b> merger rate is ≈ 3. 5 × 10 − 4 Mpc− 3 Gyr− 1. We examine how the merger rate varies by luminosity and morphology. Dry mergers (between red/spheroidal galaxies) {{are found to be}} uncommon and to decrease with decreasing luminosity. Fainter mergers are wet, between blue/disky galaxies. Damp mergers (one of each type) follow the average of dry and wet mergers. In the brighter luminosity bin (− 23 < Mr < − 20) the merger rate evolution is flat, irrespective of colour or morphology, out to z ∼ 0. 2. The makeup of the merging population does not appear to change over this redshift range. Galaxy growth by major mergers appears comparatively unimportant and dry mergers are unlikely to be significant in the buildup of the red sequenc...|$|R
40|$|This paper {{presents}} a novel framework for matching video sequences using the spatiotemporal segmentation of videos. Instead of using appearance features for region matching across frames, we use interest point trajectories to generate video volumes. Point trajectories, which are generated using the SIFT operator, are clustered to form motion segments by analyzing their motion and spatial properties. The temporal {{correspondence between the}} estimated motion segments is then established based on most common SIFT correspondences. A two pass correspondence algorithm is used to handle splitting and <b>merging</b> regions. Spatiotemporal <b>volumes</b> are extracted using the consistently tracked motion segments. Next, a set of features including color, texture, motion, and SIFT descriptors are extracted to represent a volume. We employ an Earth Mover’s Distance (EMD) based approach for the comparison of volume features. Given two videos, a bipartite graph is constructed by modeling the volumes as vertices and their similarities as edge weights. Maximum matching of this graph produces volume correspondences between the videos, and these matching scores are used to compute the video matching score. Experiments for video retrieval were performed {{on a variety of}} videos obtained from different sources including BBC Motion Gallery and promising results were achieved. We present qualitative and quantitative analysis of retrieval along with a comparison with two baseline methods...|$|R
40|$|Using a two-band double-exchange {{model with}} Jahn-Teller lattice distortions and super-exchange interactions, {{supplemented}} by quenched disorder, at electron density n= 0. 65, we explicitly demonstrate the coexistence of the n = 1 / 2 -type (π, π) charge-ordered and the ferromagnetic nanoclusters above the ferromagnetic transition temperature T_ c in colossal magnetoresistive (CMR) manganites. The resistivity increases {{due to the}} enhancement of the volume fraction of the charge-ordered and the ferromagnetic nanoclusters with decreasing the temperature down to T_ c. The ferromagnetic nanoclusters start to grow and <b>merge,</b> and the <b>volume</b> fraction of the charge-ordered nanoclusters decreases below T_ c, leading to the sharp drop in the resistivity. By applying a small external magnetic field h, we show that the resistivity above T_ c increases, {{as compared with the}} case when h= 0, a fact which further confirms the coexistence of the charge-ordered and the ferromagnetic nanoclusters. In addition, we show that the volume fraction of the charge-ordered nanoclusters decreases with increasing the bandwidth and consequently the resistivity hump diminishes for large bandwidth manganites, in good qualitative agreement with experiments. The obtained insights from our calculations provide a complete pathway to understand the phase competition in CMR manganites. Comment: 6 pages, 6 figure...|$|R
40|$|The {{messenger}} molecule {{nitric oxide}} (NO) {{is a key}} mediator of memory formation that can diffuse in the brain over tens of micrometres. It would seem therefore that NO derived from many individual neurones may <b>merge</b> into a <b>volume</b> signal that is inevitably ambiguous, relatively unspecific and thus unreliable. Here {{we report on the}} neuronal architecture that supports the NO-cyclic GMP signalling pathway in the mushroom body of an insect brain, the key centre for associative learning. We show that, in the locust (Schistocerca gregaria), parallel axons of intrinsic neurones (Kenyon cells) form tubular NO-producing zones surrounding central cores of NO-receptive Kenyon cell axons, which do not produce NO. This segregated architecture requires NO to spread at physiological concentrations up to 60 µm from the tube walls into the central NO-receptive cores. By modelling NO diffusion we show that a segregated architecture, which requires NO to act at a distance, affords significant advantages over a system where the same sources and targets intermingle. Segregation enhances the precision of NO volume signals by reducing noise and ambiguity, achieving a reliable integration of the activity of thousands of NO-source neurones. In a neural structure that forms NO-dependent associations, these properties of the segregated architecture may reduce the likelihood of forming spurious memories...|$|R
40|$|To perform {{environmental}} monitoring of deep-sea drill sites (e. g., gas leakage and seafloor deformation) and for optimal placement of monitoring equipment on the seafloor, the exact fault trace and any possible fault offsets need to known. We mapped the structures around a growth fault near the gas hydrate drill site. Then, {{based on the}} contour lines, we derived individual two-dimensional (2 D) chirp data and <b>merged</b> 3 D <b>volumes.</b> The chirp data revealed a clear growth fault slightly south (about 210 m) of drill site. The fault was mapped across all inlines of the 3 D volumes. Fault offsets were determined for five individual layers and a clear change in offset with depth was observed. A reduced offset was noted below layer L 2 (~ 1 m) but it was to 2. 5 m for both underlying layers L 3 and L 4. Based on five piston cores across the survey area between North Zone and South Zone, the linear sedimentation rate for the Holocene showed almost no difference (16. 5 – 16. 7 cm/kyr). The rate below the Holocene showed {{a difference between the}} North Zone (lower; 12. 6 – 12. 8 cm/kyr) and the South Zone (higher; 14. 8 – 16. 1 cm/kyr). This change is explained by a sudden reduction of fault activity during the Holocene...|$|R
40|$|Using liquid slugs as microreactors and microvessels enable precise {{control over}} the {{conditions}} of their contents on short-time scales {{for a wide variety}} of applications. Particularly for screening applications, there is a need for control of slug parameters such as size and composition. We describe a new microfluidic approach for creating slugs in air, each comprising a size and composition that can be selected individually for each slug. Two-component slugs are formed by first metering the desired volume of each reagent, <b>merging</b> the two <b>volumes</b> into an end-to-end slug, and propelling the slug to induce mixing. Volume control is achieved by a novel mechanism: two closed chambers on the chip are initially filled with air, and a valve in each is briefly opened to admit one of the reagents. The pressure of each reagent can be individually selected and determines the amount of air compression, and thus the amount of liquid that is admitted into each chamber. We describe the theory of operation, characterize the slug generation chip, and demonstrate the creation of slugs of different compositions. The use of microvalves in this approach enables robust operation with different liquids, and also enables one to work with extremely small samples, even down to a few slug volumes. The latter is important for applications involving precious reagents such as optimizing the reaction conditions for radiolabeling biological molecules as tracers for positron emission tomography...|$|R
40|$|We derive {{the close}} pair {{fractions}} and volume merger rates {{as a function}} of luminosity and morphology for galaxies in the GAMA survey with - 23 < M(r) < - 17 at 0. 01 < z < 0. 22. The merger fraction is about 0. 015 at all luminosities (assuming 1 / 2 of pairs <b>merge)</b> and the <b>volume</b> merger rate is about 0. 00035 per cubic Mpc per Gyr. Dry mergers (between red or spheroidal galaxies) are uncommon and decrease with decreasing luminosity. Fainter mergers are wet, between blue or disky galaxies. Damp mergers (one of each type) follow the average of dry and wet mergers. In the brighter luminosity bin (- 23 < M(r) < - 20) the merger rate evolution is flat, irrespective of colour or morphology. The makeup of the merging population does not change since z = 0. 2. Major mergers and dry mergers appear comparatively unimportant in the buildup of the red sequence over the past 2 Gyr. We compare the colour, morphology, environmental density and degree of activity of galaxies in pairs to those of more isolated objects in the same volume. Galaxies in close pairs tend to be both redder and slightly more spheroid-dominated. This may be due to "harassment" in multiple previous passes prior to the current interaction. Galaxy pairs do not appear to prefer significantly denser environments. There is no evidence of an enhancement in the AGN fraction in pairs, compared to other galaxies in the same volume. Comment: 14 pages, 8 figures, MNRAS accepte...|$|R
40|$|Background: Orthopedic trauma care {{relies on}} {{two-dimensional}} radiograms {{both before and}} during the operation. Understanding the three-dimensional nature of complex fractures on plain radiograms is challenging. Modern fluoroscopes can acquire three-dimensional volume datasets even during an operation, but the device limitations constrain the acquired volume to a cube of only 12 -cm edge. However, viewing the surrounding intact structures is important to comprehend the fracture in its context. We suggest <b>merging</b> a fluoroscope’s <b>volume</b> scan into a generic bone model to form a composite full-length 3 D bone model. Methods: Materials consisted of one cadaver bone and 20 three-dimensional surface models of human femora. Radiograms and computed tomography scans were taken before and after applying a controlled fracture to the bone. A 3 D scan of the fracture was acquired using a mobile fluoroscope (Siemens Siremobil). The fracture was fitted into the generic bone models by rigid registration using a modified least-squares algorithm. Registration precision was determined and a clinical appraisal of the composite models obtained. Results: Twenty composite bone models were generated. Average registration precision was 2. 0  mm (range 1. 6 to 2. 6). Average processing time on a laptop computer was 35  s (range 20 to 55). Comparing synthesized radiograms with the actual radiograms of the fractured bone yielded clinically satisfactory results. Conclusion: A three-dimensional full-length representation of a fractured bone can reliably be synthesized from a short scan of the patient’s fracture and a generic bone model. This patient-specific model can subsequently be used for teaching, surgical operation planning, and intraoperative visualization purposes...|$|R
