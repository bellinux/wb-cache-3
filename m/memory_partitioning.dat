54|219|Public
50|$|No ARINC 653 {{services}} are {{provided for the}} memory management of partitions. Each partition has to handle its own memory (still under the constraints of <b>memory</b> <b>partitioning</b> enforced by ARINC 653).|$|E
50|$|Local page {{replacement}} assumes {{some form}} of <b>memory</b> <b>partitioning</b> that determines how many pages are to be assigned to a given process {{or a group of}} processes. Most popular forms of partitioning are fixed partitioning and balanced set algorithms based on the working set model. The advantage of local page replacement is its scalability: each process can handle its page faults independently, leading to more consistent performance for that process. However global page replacement is more efficient on an overall system basis.|$|E
40|$|Behavioral {{synthesis}} tools {{have made}} {{significant progress in}} compiling high-level programs into register-transfer level (RTL) specifications. But manually rewriting code is still {{necessary in order to}} obtain better quality of results in memory system optimization. In recent years different automated memory optimization techniques have been proposed and implemented, such as data reuse and <b>memory</b> <b>partitioning,</b> but the problem of integrating these techniques into an applicable flow to obtain a better performance has become a challenge. In this paper we integrate data reuse, loop pipelining, <b>memory</b> <b>partitioning,</b> and memory merging into an automated optimization flow (AMO) for FPGA behavioral synthesis. We develop memory padding to help in the <b>memory</b> <b>partitioning</b> of indices with modulo operations. Experimental results on Xilinx Virtex- 6 FPGAs show that our integrated approach can gain an average 5. 8 x throughput and 4. 55 x latency improvement compared to the approach without <b>memory</b> <b>partitioning.</b> Moreover, memory merging saves up to 44. 32 % of block RAM (BRAM) ...|$|E
5000|$|Five <b>memory</b> <b>partitions</b> {{instead of}} three. Later {{releases}} increased this to seven.|$|R
5000|$|... 16 GB of NAND <b>memory,</b> <b>partitioned</b> as 1 GB {{internal}} storage and 15 GB [...] "USB storage".|$|R
50|$|Partitioned {{allocation}} divides {{primary memory}} into multiple <b>memory</b> <b>partitions,</b> usually contiguous areas of <b>memory.</b> Each <b>partition</b> might contain {{all the information}} for a specific job or task. Memory management consists of allocating a partition to a job when it starts and unallocating it when the job ends.|$|R
40|$|Abstract—Memcached is {{a popular}} {{component}} of modern Web architectures, which allows fast response times – a fundamental performance index for measuring the Quality of Experience of end-users – for serving popular objects. In this work, we study how <b>memory</b> <b>partitioning</b> in Memcached works and how it affects system performance in terms of hit rate. Memcached divides the memory into different classes proportionally to the percentage of requests for objects of different sizes. Once all the available memory has been allocated, reallocation is not possible or limited, a problem called “calcification”. Calcification constitutes a symp-tom indicating that current <b>memory</b> <b>partitioning</b> mechanisms require a more careful design. Using an experimental approach, we show {{the negative impact of}} calcification on an important performance metric, the hit rate. We then proceed to design and implement a new <b>memory</b> <b>partitioning</b> scheme, called PSA, which replaces that of vanilla Memcached. With PSA, Memcached achieves a higher hit rate than what is obtained with the default <b>memory</b> <b>partitioning</b> mechanism, {{even in the absence of}} calcification. Moreover, we show that PSA is capable of “adapting ” to the dynamics of clients ’ requests and object size distributions, thus defeating the calcification problem. I...|$|E
40|$|A massively {{scalable}} architecture for decoding low-density parity-check codes {{is presented}} in this paper. This novel architecture uses hardware scaling and <b>memory</b> <b>partitioning</b> to achieve a throughput of 100 Gbps. Simulation results show that this throughput is achieved without significant bit-error performance degradation...|$|E
30|$|Xen {{hypervisor}} {{acts as a}} basic abstraction layer for guest {{operating system}} and hardware. It controls execution of virtual machine in common sharing environment. It also performs the task of CPU scheduling and <b>memory</b> <b>partitioning</b> of various virtual machines running on same hardware.|$|E
50|$|This phone offers only 512 mb {{internal}} memory {{but it can}} {{be increased}} to greater extent by using various <b>memory</b> <b>partition</b> tools like RAMEXPANDER app.|$|R
30|$|The VBSME {{accelerator}} processes {{a search}} window by accessing its rows (top to bottom) one cycle at a time. Accesses {{to the last}} 15 rows of the search window, coming from <b>Memory</b> <b>Partition</b> B (Fig. 12), are always overlapped with the 33 rows being accessed from <b>Memory</b> <b>Partition</b> A [34]. Thus 33 cycles are all that is required to finish a vertical sweep of the search-window. However, {{not all of the}} 64 columns of the pixels within a 64 × 48 search-window are read per each row access.|$|R
50|$|The {{slightly}} enhanced POWER6+ {{was introduced}} in April 2009, but had been shipping in Power 560 and 570 systems since October 2008. It added more memory keys for secure <b>memory</b> <b>partition,</b> a feature taken from IBM's mainframe processors.|$|R
40|$|<b>Memory</b> <b>partitioning</b> {{has proved}} to be a {{promising}} solution to reduce energy consumption in complex SoCs. <b>Memory</b> <b>partitioning</b> comes in different flavors, depending on the specific domain of usage and design constraints to be met. In this paper, we consider a technique that allows us to customize the architecture of physically partitioned SRAM macros according to the given application to be executed. We present design solutions for the various components of the partitioned memory architecture, and develop a memory generator for automatically generating layouts and schematics of the optimized memory macros. Experimental results, collected for two different case studies, demonstrate the efficiency of the architecture and the usability of the prototype memory generator. In fact, the achieved energy savings w. r. t. implementations featuring monolithic architectures, are around 43 % for a memory macro of 1 KByte, and around 45 % for a memory macro of 8 KByte...|$|E
40|$|An {{energy-efficient}} reconfigurable distributed-arithmetic FIR filter for multi-mode {{wireless communication}} is fabricated in 7 M 1 P 90 nm CMOS and occupies 1. 5 mm 2. A 6 -way parallel, 2 -way time-multiplexed architecture with circuits for memory offset binary coding and <b>memory</b> <b>partitioning</b> enable input wordlength and tap configurability with 1 – 190 MSample/s throughput and 10 – 130 mW total power measured at 1. 1 V, 25 ° C...|$|E
40|$|Conventional {{operating}} systems, like Silicon Graphics' IRIX and IBM's AIX, adopt {{a single}} Memory Management algorithm. The choice of this algorithm is usually {{based on its}} good performance {{in relation to the}} set of programs executed in the computer. Some approximation of LRU (least­recently used) is usually adopted. This choice can take to certain situations in that the computer presents a bad performance due to its bad behavior for certain programs. A possible solution for such cases is to enable each program to have a specific Management algorithm (local strategy) that is adapted to its Memory access pattern. For example, programs with sequential access pattern, such as SOR, should be managed by the algorithm MRU (most­recently used) because its bad performance when managed by LRU. In this strategy {{it is very important to}} decide the <b>Memory</b> <b>partitioning</b> strategy among the programs in execution in a multiprogramming environment. Our strategy named CAPR (Compiler­Aided Page Replacement) analyze the pattern of Memory references from the source program of an application and communicate these characteristics to the operating system that will make the choice of the best Management algorithm and <b>Memory</b> <b>partitioning</b> strategy. This paper evaluates the influence of the Management algorithms and <b>Memory</b> <b>partitioning</b> strategy in the global system performance and in the individual performance of each program. It is also presented a comparison of this local strategy with the classic global strategy and the viability of the strategy is analyzed. The obtained results showed a difference of at least an order of magnitude in the number of page faults among the algorithms LRU and MRU in the global strategy. After that, starting from the analysis of the intrinsic behavior of each application in relation to its Memory access pattern and of the number of page faults, an optimization procedure of Memory system performance was developed for multiprogramming environments. This procedure allows to decide system performance parameters, such as <b>Memory</b> <b>partitioning</b> strategy among the programs and the appropriate Management algorithm for each program. The results showed that, with the local Management strategy, it was obtained a reduction of at least an order of magnitude in the number of page faults and a reduction in the mean Memory usage of about 3 to 4 times in relation to the global strategy. This performance improvement shows the viability of our strategy. It is also presented some implementation aspects of this strategy in traditional operating systems. Sistemas Distribuidos - Redes Concurrenci...|$|E
40|$|<b>Partitioning</b> a <b>memory</b> {{into many}} blocks and {{catching}} {{a certain amount}} of data in main memory from disk where blocks allocated for process are sufficient to execute process is very useful mechanism provide multiprogramming and cpu utilization. However creating appropriate allocation and replacement algorithm are daunting tasks. This paper proposed an on demand <b>memory</b> <b>partition</b> mechanism for scalable and efficient memory management. This paper propose that process start with different size. The allocation algorithm then gradually divides a process into many blocks as main memory receives more requests from process from memory. It catches them in new blocks from available once (free blocks). Consequently: <b>memory</b> <b>partition</b> done on demand where user's task determines the appropriate size of memory. I...|$|R
50|$|Note {{also that}} the storage {{required}} for the multiple processes came from the system's memory pool as needed. There was no having to do SYSGENs on Burroughs systems as with competing systems in order to preconfigure <b>memory</b> <b>partitions</b> in which to run tasks.|$|R
3000|$|A {{processor}} {{is considered}} optimal if {{it requires a}} minimum of <b>memory</b> <b>partitions,</b> is shuffle free, meaning the absence of clock times used uniquely for shuffling, and produces an ordered output given an ordered input. It is shown in Corinthios (1994) that p [...]...|$|R
30|$|Indeed, {{the issue}} of <b>memory</b> <b>partitioning</b> and the {{reordering}} techniques described in Section 4 are linked to each other: whenever the CNUs are in idle, only one reading is performed. Therefore, an overall system optimization aiming at minimizing the iteration latency {{and the amount of}} memory redundancy at the same time could be pursued; however, due to the huge optimization space, this task is almost unfeasible and is not considered in this work.|$|E
40|$|Power {{consumption}} and speed {{are the largest}} costs for a virtual memory system in handheld computers. This paper describes a method of trading off computation and useable physical memory to reduce disk I/O. The design uses a compression cache, keeping some virtual memory pages in compressed form rather than sending them to the backing store. Efficiency is managed by a log-structured circular buffer, supporting dynamic <b>memory</b> <b>partitioning,</b> diskless operation, and disk spin-down...|$|E
40|$|This project {{presents}} {{the implementation of}} 64 x 64 multi-port dynamically configured SRAM in VHDL (VHSIC hardware description language). It employs isolation nodes and dynamic <b>memory</b> <b>partitioning</b> algorithm to facilitate simultaneous multi-port accesses without duplicating bit-lines. VHDL test-bench is developed to verify the functionality of the dynamically configured memory. Results demonstrate that critical memory operations such as "read miss", "write miss" and "write bypass" can be performed using newly proposed low power, area efficient dynamically configured memory...|$|E
40|$|We {{describe}} an abstract interpretation based framework for proving relationships between sizes of <b>memory</b> <b>partitions.</b> Instances of this framework can prove traditional properties such as memory safety and program termination {{but can also}} establish upper bounds on usage of dynamically allocated memory. Our framework also stands out {{in its ability to}} prove properties of programs manipulating both heap and arrays which is considered a difficult task. Technically, we define an abstract domain that is parameterized by an abstract domain for tracking <b>memory</b> <b>partitions</b> (sets of <b>memory</b> locations) and by a numerical abstract domain for tracking relationships between cardinalities of the partitions. We describe algorithms to construct the transfer functions for the abstract domain in terms of the corresponding transfer functions of the parameterized abstract domains. A prototype of the framework was implemented and used to prove interesting properties of realistic programs, including programs that could not have been automatically analyzed before...|$|R
40|$|Many {{architectures}} today, especially embedded systems, {{have multiple}} <b>memory</b> <b>partitions,</b> each with potentially different performance and energy characteristics. To meet the strict time-to-market requirements of systems containing these chips, compilers require retar-getable algorithms for effectively assigning values to the <b>memory</b> <b>partitions.</b> Furthermore, embedded system designers need a methodology for quickly evaluating {{the performance of}} a candidate memory hierarchy on an application without relying on time-consuming simulation. This dissertation presents algorithms and techniques to effectively meet these needs. First, EMBARC is presented. EMBARC is the first algorithm to realize a comprehensive, retargetable algorithm for effective partition assignment of variables in an arbitrary memory hierarchy. It supports a wide variety of memory models including on-chip SRAMs, multiple layers of caches, and even uncached DRAM partitions. Even though it is designed to handle a wide range of memory hierarchies, EMBARC is capable of generating partition assignments of similar quality to algorithms designed for specific memory hierarchies. ...|$|R
50|$|Virtual <b>memory</b> systems <b>partition</b> the computer's {{main storage}} into even larger units, {{traditionally}} called pages.|$|R
40|$|Abstract—Achieving optimal {{throughput}} by extracting parallel-ism {{in behavioral}} synthesis often exaggerates memory bottleneck issues. Data partitioning {{is an important}} technique for increasing memory bandwidth by scheduling multiple simultaneous memory accesses to different memory banks. In this paper we present a vertical <b>memory</b> <b>partitioning</b> and scheduling algorithm that can generate a valid partition scheme for arbitrary affine memory inputs. It does this by arranging non-conflicting memory accesses across the border of loop iterations. A mixed <b>memory</b> <b>partitioning</b> and scheduling algorithm is also proposed to com-bine {{the advantages of the}} vertical and other state-of-art algo-rithms. A set of theorems is provided as criteria for selecting a valid partitioning scheme. This is followed by an optimal and scalable memory scheduling algorithm. By utilizing the property of constant strides between memory addresses in successive loop iterations, an address translation optimization technique for an arbitrary partition factor is proposed to improve performance, area and energy efficiency. Experimental results show that on a set of real-world medical image processing kernels, the proposed mixed algorithm with address translation optimization can gain speed-up, area reduction and power savings of 15. 8 %, 36 % and 32. 4 % respectively, compared to the state-of-art memory parti-tioning algorithm...|$|E
40|$|Conventional {{operating}} systems, like Silicon Graphics ’ IRIX and IBM’s AIX, adopt {{a single}} memory management algorithm. The choice of this algorithm is usually {{based on its}} good performance {{in relation to the}} set of programs executed in the computer. Some approximation of LRU (least-recently used) is usually adopted. This choice can take to certain situations in that the computer presents a bad performance due to its bad behavior for certain programs. A possible solution for such cases is to enable each program to have a specific management algorithm (local strategy) that is adapted to its memory access pattern. For example, programs with sequential access pattern, such as SOR, should be managed by the algorithm MRU (most-recently used) because its bad performance when managed by LRU. In this strategy {{it is very important to}} decide the <b>memory</b> <b>partitioning</b> strategy among the programs in execution in a multiprogramming environment. Our strategy named CAPR (Compiler-Aided Page Replacement) analyze the pattern of memory references from the source program of an application and communicate these characteristics to the operating system that will make the choice of the best management algorithm and <b>memory</b> <b>partitioning</b> strategy...|$|E
40|$|We present our tool SpC {{which enables}} the {{synthesis}} of C behavioral models with pointers and complex data structures. For both analysis and synthesis, memory is represented by location sets. During <b>memory</b> <b>partitioning,</b> these location sets are either mapped to simple variables or arrays. Pointers are encoded and loads/stores are replaced by assignments in which data are directly accesses. Finally, dynamic memory allocation and deallocation are performed within user-defined memory segments by an optimized hardware allocator instantiated from a library...|$|E
5000|$|... vector {{performance}} acceleration for substantially faster querying {{and more}} efficient use of CPU, <b>memory,</b> and <b>partitioning</b> ...|$|R
30|$|Each {{processing}} element at {{each step}} of the algorithm thus accesses from memory its p input operands and writes into memory those of its output operands. The algorithm, while providing an arbitrary, generalised, level of parallelism up to the ultimate massive parallelism, produces optimal multiprocessing machine architecture minimizing addressing, the number of <b>memory</b> <b>partitions</b> as well as the number of required shuffles. Meanwhile it produces virtually wired-in pipelined architecture and properly ordered output.|$|R
50|$|Halcyon {{was based}} around the Z80 microprocessor, with its 64K <b>memory</b> <b>partitioned</b> out to ROM and RAM. A {{separate}} speech recognition computer provided the additional power needed to recognize human speech. Its firmware was proprietary, and its chief communications with the Z80 were indications of what word it had recognized, and what probability of confidence it {{calculated for the}} match. Other functions this subsystem provided were non-volatile memory storage, and speech recognition training.|$|R
40|$|Memory {{bottleneck}} {{has become}} a limiting factor in satisfying the explosive demands on performance and cost in modern embedded system design. Selected computation kernels for acceleration are usually captured by nest loops, which are optimized by state-of-the-art techniques like loop tiling and loop pipelining. However, memory bandwidth bottlenecks prevent designs from reaching optimal throughput with respect to available parallelism. In this paper we present an automatic <b>memory</b> <b>partitioning</b> technique which can efficiently improve throughput and reduce energy consumption of pipelined loop kernels for given throughput constraints and platform requirements. Also, our proposed algorithm can handle general array access beyond affine array references. Our partition scheme consists of two steps. The first step considers cycle accurate scheduling information to meet the hard constraints on memory bandwidth requirements specifically for synchronized hardware designs. An ILP formulation is proposed to solve the <b>memory</b> <b>partitioning</b> and scheduling problem optimally for small designs, followed by a heuristic algorithm which is more scalable and equally effective for solving large scale problems. Experimental results show an average 6 × throughput improvement {{on a set of}} real-world designs with moderate area increase (about 45 % on average), given that less resource sharing opportunities exist with higher throughput in optimized designs. The second step further partitions the memory bank...|$|E
40|$|<b>Memory</b> <b>partitioning</b> is an eective {{approach}} to memory energy optimization in embedded systems. Spatial local-ity {{of the memory}} address prole is the key property that partitioning exploits to determine an eÆcient multi-bank memory architecture. This paper presents an approach, called address clustering, for increasing the locality of a given memory access prole, and thus improving the eÆciency of partitioning. Results obtained on several embedded applications running on an ARM 7 core show average energy reductions of 25 % (maximum 57 %) w. r. t. a partitioned memory architecture synthesized without resorting to address clustering. ...|$|E
40|$|Scratch Pad Memories (SPMs) have {{received}} considerable attention lately as on-chip memory building blocks. The main characteristic that distinguishes an SPM from a conventional cache memory {{is that the}} data flow is controlled by software. The main focus {{of this paper is}} the management of an SPM space shared by multiple applications that can potentially share data. The proposed approach has three major components; a compiler analysis phase, a runtime space partitioner, and a local partitioning phase. Our experimental results show that the proposed approach leads to minimum completion time among all alternate <b>memory</b> <b>partitioning</b> schemes tested. Copyright © 2009, Inderscience Publishers...|$|E
50|$|OS/360 MFT (Multitasking with a Fixed {{number of}} Tasks) {{provided}} multitasking: several <b>memory</b> <b>partitions,</b> each of a fixed size, {{were set up}} when the operating system was installed and when the operator redefined them. For example, {{there could be a}} small partition, two medium partitions, and a large partition. If there were two large programs ready to run, one would {{have to wait until the}} other finished and vacated the large partition.|$|R
5000|$|... #Caption: Copland runtime architecture. The purple boxes show {{threads of}} control, while the heavy lines show {{different}} <b>memory</b> <b>partitions.</b> In {{the upper left}} is the blue box, running a number of System 7 applications (blue) and the toolbox code supporting them (green). Two “headless” applications are also running in their own spaces, providing file and web services. At the bottom are the OS servers, running in the same memory space as the kernel, indicating co-location.|$|R
40|$|Many {{architectures}} today, especially embedded systems, {{have multiple}} <b>memory</b> <b>partitions,</b> each with potentially different performance and energy characteristics. To meet the strict time-to-market requirements of systems containing these chips, compilers require retargetable alogrithms for effectively assigning values to the <b>memory</b> <b>partitions.</b> The EMBARC algorithm {{described in this}} paper is the first algorithm to attempt to realize a comprehensive, retargetable algorithm for effective partition assignment of variables in an arbitrary memory hierarchy. It supports {{a wide variety of}} memory models including on-chip SRAMs, multiple layers of caches, and even uncached DRAM partitions. Even though it is designed to handle such a range of memory hierarchies, EMBARC is capable of generating partition assignments of similar quality to algorithms designed for specific memory hierarchies. We use a large range of benchmarks and memory models to demonstrate the effectiveness of the EMBARC algorithm. We found that EMBARC can achieve 99 % of the improvement of a dedicated algorithm for cacheless systems without SRAM. Also, for cacheless systems with SRAM, EMBARC generated the optimal partition assignment for benchmarks that were simple enough to hand-generate an optimal partition assignment. As further proof of EMBARC’s generality, we also show how EMBARC can be used to generate partition assignments for a memory hierarchies with two on-chip caches that can be accessed in parallel...|$|R
