2|46|Public
5000|$|... where [...] is {{a pattern}} for agents, [...] is a {{sequence}} of conditions on the agents, [...] {{is a set of}} patterns for events that can activate the agents, and [...] is a sequence of actions performed by the agents when they are activated. When the event pattern [...] together with the enclosing braces is missing, an action rule degenerates into a <b>matching</b> <b>clause.</b>|$|E
50|$|A <b>matching</b> <b>clause</b> {{is a form}} of {{a clause}} where the determinacy and input/output unifications are denoted explicitly. The {{compiler}} translates matching clauses into matching trees and generates indexes for all input arguments. The compilation of matching clauses is much simpler than that of normal Prolog clauses because no complex program analysis or specialization is necessary; and the generated code tends to be more compact and faster. The B-Prolog compiler and most of the library predicates are written in matching clauses.|$|E
40|$|Pattern {{matching}} is {{an important}} feature in various functional programming languages such as SML, Caml, Haskell, etc. In these languages, unreachable or redundant <b>matching</b> <b>clauses,</b> which {{can be regarded as}} a special form of dead code, are a rich source for program errors. Therefore, eliminating unreachable <b>matching</b> <b>clauses</b> at compiletime can significantly enhance program error detection. Furthermore, this can also lead to significantly more efficient code at run-time. We present a novel approach to eliminating unreachable <b>matching</b> <b>clauses</b> through the use of the dependent type system of DML, a functional programming language that enriches ML with a restricted form of dependent types. We then prove the correctness of the approach, which consists of the major technical contribution of the paper. In addition, we demonstrate the applicability of our approach to dead code elimination through some realistic examples. This constitutes a practical application of dependent types to functional [...] ...|$|R
50|$|Here the {{function}} is defined as, if given two terms, and the terms are same {{then the first}} <b>clause</b> <b>matches</b> and produces True.else the second <b>clause</b> <b>matches</b> and produces False.|$|R
5000|$|... where [...] is {{an atomic}} formula, [...] and [...] are two {{sequences}} of atomic formulas. [...] {{is called the}} head, [...] the guard, and [...] {{the body of the}} clause. No call in [...] can bind variables in [...] and all calls in [...] must be in-line tests. In other words, the guard must be flat. The following gives an example predicate in <b>matching</b> <b>clauses</b> that merges two sorted lists: ...|$|R
40|$|Haskell is a {{functional}} programming language with nominally non-strict semantics, implying that evaluation of a Haskell expression proceeds by demand-driven reduc-tion. However, Haskell also provides pattern matching on arguments of functions, in let expressions and in the <b>match</b> <b>clauses</b> of case expressions. Pattern-matching requires data-driven reduction {{to the extent necessary}} to evaluate a pattern match or to bind variables introduced in a pattern. In this paper we provide both an ab-stract semantics and a logical characterization of pattern-matching in Haskell and the reduction order that it entails. ...|$|R
5000|$|... (Using '_' {{in place}} of space char {{so as to make}} the {{function}} call clear.)The first <b>clause</b> <b>matches</b> whenever the function Squeeze encounters doubleblanks in its input expression, and replaces it with a single blank.The second <b>clause</b> <b>matches</b> only when the first one did not, and returns theresultant value which is the current expression.|$|R
40|$|We {{introduce}} a formalism for <b>clause</b> <b>matching</b> in concurrent committed-choice languages {{based on the}} construction of <b>clause</b> <b>matching</b> automata, a heuristic for the compilation of <b>clause</b> <b>matching,</b> and a technique for more efficient implementation of matches. The formalism is notable for its generality and simplicity, the heuristic for combining important advantages of several existing heuristics. These include good typical-case time and space performance, a minimal number of suspensions, which can lead to tremendous efficiency benefits, and incremental restart after suspensions, which eliminates repeated tests. This paper presented at the First International Symposium on Parallel Symbolic Computation, Linz, Austria, September 1994. Department of Computer and Information Science University of Oregon Contents 1 Introduction 1 2 Background 1 3 Related Work 3 4 <b>Clause</b> <b>Matching</b> Automata 5 5 A Heuristic For Efficient CMA Construction 8 6 Disjuncts of Conjuncts 11 7 Conclusions 12 References 1 [...] ...|$|R
40|$|B-Prolog is a {{high-performance}} {{implementation of the}} standard Prolog language with several extensions including <b>matching</b> <b>clauses,</b> action rules for event handling, finite-domain constraint solving, arrays and hash tables, declarative loop constructs, and tabling. The B-Prolog system {{is based on the}} TOAM architecture which differs from the WAM mainly in that (1) arguments are passed old-fashionedly through the stack, (2) only one frame is used for each predicate call, and (3) instructions are provided for encoding matching trees. The most recent architecture, called TOAM Jr., departs further from the WAM in that it employs no registers for arguments or temporary variables, and provides variable-size instructions for encoding predicate calls. This paper gives an overview of the language features and {{a detailed description of the}} TOAM Jr. architecture, including architectural support for action rules and tabling. Comment: 30 page...|$|R
40|$|The {{relational}} model of data incorporates fundamental assertions for entity integrity and referential integrity. Recently, these so-called relational invariants were more precisely {{specified by the}} new SQL 2 standard. Accordingly, {{they have to be}} guaranteed by a relational DBMS to its users and, therefore, all issues of semantics and implementation became very important. The specification of referential integrity embodies quite a number of complications including the <b>MATCH</b> <b>clause</b> and a collection of referential actions. In particular, MATCH PARTIAL turns out to be hard to understand and, if applied, difficult and expensive to maintain. In this paper, we identify the functional requirements for preserving referential integrity. At a level free of implementational considerations, the number and kinds of searches necessary for referential integrity maintenance are derived. Based on these findings, our investigation is focused on the question of how the [...] ...|$|R
5000|$|B-Prolog is a {{high-performance}} {{implementation of the}} standard Prolog language with several extended features including <b>matching</b> <b>clauses,</b> action rules for event handling, finite-domain constraint solving, arrays and hash tables, declarative loops, and tabling. First released in 1994, B-Prolog is now a widely used CLP system. The constraint solver of B-Prolog was ranked top in two categories in the Second International Solvers Competition, and it also {{took the second place}} in P class in the second ASP solver competition [...] and the second place overall in the third ASP solver competition. B-Prolog underpins the PRISM system, a logic-based probabilistic reasoning and learning system. B-Prolog is a commercial product, but it can be used for learning and non-profit research purposes free of charge (since version 7.8 for individual users, including commercial individual users, B-Prolog is free of charge [...] ).|$|R
5000|$|Accept The player remains {{with his}} current team on a {{contract}} identical {{to that of the}} offer sheet. With the exception that the current team does not have to <b>match</b> any <b>clauses</b> restricting their ability to trade or reassign the player like a [...] "no trade clause". The team is not allowed to trade the player for one year.|$|R
40|$|A {{refutation}} tree is {{a standard}} two-dimensional representation of resolution derivations. Given a logic program, a computation rule and a goal, different resolution derivations result from selecting different <b>matching</b> <b>clauses</b> in the program. These derivations can be visualized as separate branches in the refutation tree. This process can also be reversed: given an refutation tree where leaf nodes are labelled with either succeed (for the empty clause) or fail, and neither other nodes nor links are labeled, one can construct a program, a computation rule and a goal that match this abstract tree. Thus we can introduce in a visual way basic logic programming concepts such as resolution strategies and finite failure. This might facilitate learning logic programming and Prolog. We are actually using here the computational efficiency for students of diagrams to represent logic programming proof concepts. Also, {{the concept of an}} abstract refutation tree (art) opens the quest for proof theoret [...] ...|$|R
40|$|Abstract. The {{relational}} model of data incorporates fundamental assertions for entity integrity and referential integrity. Recently, these so-called relational invariants were more precisely {{specified by the}} new SQL 2 standard. Accordingly, {{they have to be}} guaranteed by a relational DBMS to its users and, therefore, all issues of semantics and implementation became very important. The specification of referential integrity embodies quite a number of complications including the <b>MATCH</b> <b>clause</b> and a collection of referential actions. In particular, MATCH PARTIAL turns out to be hard to understand and, if applied, difficult and expensive to maintain. In this paper, we identify the functional requirements for preserving referential integrity. At a level free of implementational considerations, the number and kinds of searches necessary for referential integrity maintenance are derived. Based on these findings, our investigation is focused on the question of how the functional requirements can be supported by implementation concepts in an efficient way. We determine the search cost for referential integrity maintenance (in terms of page references) for various possible access path structures. Our main result is that a combined access path structure is the most appropriate for checking the regular MATCH option, whereas MATCH PARTIAL requires very expensive and complicated check procedures. If it cannot be avoided at all, the best support is achieved by a combination of multiple B ∗-trees...|$|R
5|$|Cardiff {{accepted}} two {{bids for}} Campbell, one from Leicester City {{and one from}} Crystal Palace. On 24 July 2014, Crystal Palace completed the signing of Campbell on a three-year deal after <b>matching</b> the release <b>clause</b> in his contract, {{believed to be in}} the region of £900,000. He was released from his contract on 30 June 2017.|$|R
40|$|It is {{generally}} assumed that ellipsis requires certain parallelism between the clause containing the ellips is and some antecedent clause. We {{argue that the}} parallelism requirement generated by ellipsis must be applied in accordance with discourse structure: a <b>matching</b> antecedent <b>clause</b> must be found that locally c-commands the clause containing the ellipsis in the discourse tree. We show that the claim makes several correct predictions concerning the interpretation of ellipsis, {{both in terms of}} the selection of the antecedent (in Sluicing and Verb Phrase Ellipsis), {{and in terms of the}} possible readings given a particular antecedent (in the "many-clause" puzzle and in Antecedent-Contained Deletion) ...|$|R
5|$|In August 2013, Costa {{was heavily}} linked with {{a move to}} Liverpool, who {{allegedly}} <b>matched</b> his release <b>clause</b> of €25million and offered him three times his salary at Atlético. Costa, however, chose {{to stay at the}} club and renewed his contract until 2018, while also doubling his wages; a few days after this, in the first match of the new season on 19 August, he scored a brace in a 3–1 win at Sevilla.|$|R
40|$|In {{this paper}} we address a method to align English-Chinese bilingual news reports from China News Service, {{combining}} both lexical and satistical approaches. Because of the sentential structure differences between English and Chinese, matching at the sentence level as in many other works may result in frequent matching of several sentences en masse. In view of this, the current work also attempts to create shorter alignment pairs by permitting finer <b>matching</b> between <b>clauses</b> from both texts if possible. The current method is based on statiscal correlation between sentence or clause length of both texts {{and at the same time}} uses obvious anchors such as numbers and place names appearing frequently in the news reports as lexcial cues. Comment: 9 pages, Postscript only. In the Proceedings of International Conference on Chinese Computing' 9...|$|R
5000|$|Furthermore, Barker {{states that}} DP partitive constructions cannot be {{headed by a}} {{definite}} determiner without being modified by a relative clause, {{that there is some}} inherent indefiniteness in partitives according to their property of anti-uniqueness. [...] This explains why 4b) is ill-formed, since it is unclear which of John’s friends is being singled out, yet can be made to take a definite determiner by adding context, such as in 4c), which now refers to a single specific friend of John <b>matching</b> the modifier <b>clause.</b>|$|R
2500|$|The code has {{a method}} for every type of clause in a scenario. JBehave will {{identify}} which method goes with which clause {{through the use of}} annotations and will call each method in order while running through the scenario. The text in each clause in the scenario is expected to match the template text given in the code for that clause (for example, a Given in a scenario is expected to be followed by a clause of the form [...] "a X by Y game"). JBehave supports the <b>matching</b> of <b>clauses</b> to templates and has built-in support for picking terms out of the template and passing them to methods in the test code as parameters. The test code provides an implementation for each clause type in a scenario which interacts with the code that is being tested and performs a test based on the scenario. In this case: ...|$|R
40|$|With the {{worldwide}} {{adoption of the}} Internet as a ubiquitous platform for exchanging information, new opportunities are available for sharing educational experiences and material. Thousands of institutes worldwide offer access to courses, lessons, seminars, and exercises, typically stored in repositories and offered to users {{under a variety of}} formats. Due to their educational nature, these contents are often referred to as Learning Objects. This article describes the "ProLearn Query Language", a query language that we have developed for repositories of learning objects. PLQL is primarily a query interchange format, used by source applications (or PLQL clients) for querying repositories (or PLQL servers). In defining PLQL, we have combined exact search, used for selecting learning objects by means of queries on their metadata, and approximate search, used for locating learning objects by means of keyword-based search. A query in PLQL contains both "exact clauses" and "approximate clauses", where each clause is syntactically well-defined; exact <b>clauses</b> <b>match</b> keywords against metadata with a known structure, while approximate <b>clauses</b> <b>match</b> keywords freely, either against metadata or against indexes based upon the documents' contents. In this article, we give a precise description of the semantics of PLQL, concerning both kinds of clauses and their mutual relationship and describe two experimentation efforts around PLQL: one involving the ARIADNE repository and the other the EUN Learning Resource Exchange initiative...|$|R
2500|$|He {{was then}} matched up against old foe Barry Windham at Chi-Town Rumble winning his second NWA United States Heavyweight Championship from him. He {{teamed up with}} Michael P.S. Hayes against Barry and Kendall Windham in a match, {{televised}} on March 18, 1989, which saw Hayes turn on Luger, setting himself as a contender to the U.S. Title. Hayes defeated Luger for the US title at [...] when a surprise appearance by Hayes's ex-Freebird teammate Terry Gordy helped cost Luger the match. Luger regained the U.S. Title from Hayes in a rematch {{a couple of weeks}} later when he broke the rules by pulling Hayes's tights while pinning Hayes to win the match. On the June 14 [...] of Clash of the Champions, Luger attacked the popular Ricky [...] "The Dragon" [...] Steamboat after Steamboat had defeated Terry Funk by disqualification. Luger and Steamboat faced each other at The Great American Bash in July with Luger winning by disqualification after Luger refused to wrestle Steamboat until the <b>match's</b> no-disqualification <b>clause</b> had been waived.|$|R
40|$|In Complex Event Processing (CEP) {{applications}} such as supply chain management and financial data analysis, the capability to match patterns over data sequences is increasingly becoming an important need. This not only involves finding event patterns of interest on live data streams but also requires a similar functionality over archived sequences of streams for historical analysis, verification, and correlation. The goal of this thesis is to extend a relational database system with the capability to match patterns over contiguous sequences of rows stored in a database table. More specif-ically, we have implemented a major subset of the 2007 ANSI standard proposal on adding <b>MATCH</b> RECOGNIZE <b>clause</b> to standard SQL {{on top of the}} MySQL open-source database engine. We have done this in a way to leverage the existing MySQL architecture and process-ing model as much as possible, {{while at the same time}} carefully identifying the parts where brand new extensions were necessary. Thus, one of the main contributions of this thesis is that it clearly shows what it takes in general to add pattern matching capability to any relationa...|$|R
5000|$|He {{was then}} matched up against old foe Barry Windham at Chi-Town Rumble winning his second NWA United States Heavyweight Championship from him. He {{teamed up with}} Michael P.S. Hayes against Barry and Kendall Windham in a match, {{televised}} on March 18, 1989, which saw Hayes turn on Luger, setting himself as a contender to the U.S. Title. Hayes defeated Luger for the US title at WrestleWar 1989: Music City Showdown when a surprise appearance by Hayes's ex-Freebird teammate Terry Gordy helped cost Luger the match. Luger regained the U.S. Title from Hayes in a rematch {{a couple of weeks}} later when he broke the rules by pulling Hayes's tights while pinning Hayes to win the match. On the June 14 edition of Clash of the Champions, Luger attacked the popular Ricky [...] "The Dragon" [...] Steamboat after Steamboat had defeated Terry Funk by disqualification. Luger and Steamboat faced each other at The Great American Bash in July with Luger winning by disqualification after Luger refused to wrestle Steamboat until the <b>match's</b> no-disqualification <b>clause</b> had been waived.|$|R
40|$|Zurich German (ZG) {{relative}} clauses are remarkable from a Germanic {{point of}} view in that resumptive pronouns are employed instead of relative pronouns. Reconstruction effects and Strong Crossover violations show that movement is involved in the derivation of ZG relative <b>clauses.</b> <b>Matching</b> effects sensitive to case and preposition provide crucial evidence that the distribution of resumptives is determined by general licensing condi-tions on oblique case and prepositions. The matching/non-matching dichotomy is mod-eled as an instance of Distributed Deletion, which is claimed to be independently avail-able in the language. Matching is furthermore sensitive to the actual surface form and thus favors a late insertion approach to morphology. * 1...|$|R
40|$|During {{the last}} decade, {{the amount of}} {{research}} published in biomedical journals has grown significantly and at an accelerating rate. To fully explore all of this literature, new tools and techniques are needed for both information retrieval and processing. One such tool is the identification and extraction of key claims. In an e ort to work toward claim-extraction, we aim to identify the key areas {{in the body of}} the article referred to by text in the abstract. In this project, our work is preliminary to that goal in that we attempt to <b>match</b> specific <b>clauses</b> in the abstract with the section of the article body to which they refer. For our data, we use journal articles from PubMed with structured abstracts. Our technique is based on the cosine-measure of feature vectors using a bag-of-words approach. We refine our technique through the application of five di erent experimental variables: feature-weighting, word and bi-gram based feature-sets, text pre-processing, fixedexpression filtering, and di erent classifier heuristics. We found that the choice of classifier dominates all other considerations, and while their performance with feature-weighting is synergistic, other variables were found to have little or no e ffect...|$|R
40|$|Abstract. We study several {{complexity}} {{parameters for}} first order formulas and their suitability for first order learning models. We {{show that the}} standard notion of size is not captured by sets of parameters {{that are used in}} the literature and thus they cannot give a complete characterization in terms of learnability with polynomial resources. We then identify an alternative notion of size and a simple set of parameters that are useful for first order Horn Expressions. These parameters are the number of clauses in the expression, the maximum number of distinct terms in a clause, and the maximum number of literals in a <b>clause.</b> <b>Matching</b> lower bounds derived using the Vapnik Chervonenkis dimension complete the picture showing that these parameters are indeed crucial...|$|R
40|$|Stefan Szeider Institute of Discrete Mathematics, Austrian Academy of Sciences, Sonnenfelsgasse 19, A [...] 1010 Vienna, Austria, stefan. szeider@oeaw. ac. at Abstract A CNF {{formula is}} "matched" if its {{associated}} bipartite graph (whose vertices are clauses and variables) has a matching which covers all <b>clauses.</b> <b>Matched</b> CNF formulas are always satisfiable, {{and can be}} recognized e#ciently by matching algorithms. We generalize this concept and cover clauses by collections of bicliques (complete bipartite graphs). It turns out that such generalizations indeed give raise to larger classes of satisfiable CNF formulas (which we term "biclique satisfiable"). We show, however, that recognition of biclique satisfiable CNF formulas is NP-complete, and remains NP-hard if the size of bicliques is bounded by a fixed integer...|$|R
5000|$|The {{result of}} a left outer join (or simply left join) for tables A and B always {{contains}} all rows of the [...] "left" [...] table (A), even if the join-condition does not find any matching row in the [...] "right" [...] table (B). This means that if the [...] <b>clause</b> <b>matches</b> 0 (zero) rows in B (for a given row in A), the join will still return a row in the result (for that row)—but with NULL in each column from B. A left outer join returns all the values from an inner join plus all values in the left table that do not match to the right table, including rows with NULL (empty) values in the link column.|$|R
40|$|In a {{touch screen}} paradigm, we {{recorded}} 3 - to 7 -year-olds’ (N = 108) accuracy and response times {{to assess their}} comprehension of two-clause sentences containing before and after. Children were influenced by order: performance was most accurate when the presentation order of the two <b>clauses</b> <b>matched</b> the chronological order of events: ‘She drank the juice, before she walked in the park’ (chronological order) vs ‘Before she walked in the park, she drank the juice’ (reverse order). Differences in response times for correct responses varied by sentence type: accurate responses were made more speedily for sentences that afforded an incremental processing of meaning. An independent measure of memory predicted this pattern of performance. We discuss these findings in relation to children’s knowledge of connective meaning and the processing requirements of sentences containing temporal connectives...|$|R
40|$|Five {{experiments}} {{investigate the}} scope of conceptual and grammatical encoding during spoken sentence production. An online picture description task is employed in which parti-cipants generate a variety of sentences {{in response to an}} array of moving pictured objects. Experiment 1, demonstrates longer onset latencies for single clause sentences beginning with a complex phrase (e. g. The dog and the kite move above the house) than for <b>matched</b> single <b>clause</b> sentences beginning with a simple phrase (e. g. The dog moves above the kite and the house). This ®nding suggests that more time is dedicated to the processing of the ®rst phrase of an utterance than the remainder prior to speech onset. Experiments 2 and 3, compare the production of single and double clause sentences. The main effect of Experiment 1 is repli-cated. However, the data also suggest that some time is dedicated to the processing of elements within the second clause prior to speech onset. In Experiment 4, when participants are allowed to preview pictures prior to movement and timer onset the effect of initial phrase complexity is signi®cantly reduced indicating that the latency effects observed previously primarily re¯ect lemma access. Finally, Experiment 5 demonstrates that this reduction is greater for nouns within the ®rst phrase than for nouns beyond it. We conclude from these experiments that, prior to speech onset, lemma access is completed for the ®rst phrase of an utterance and that high level processing is initiated but not completed for the remainder of...|$|R
30|$|Syntax-based {{question}} generation systems {{work through}} three steps: (1) delete the identified target concept, (2) place a determined question key {{word on the}} first position of the question, and (3) convert the verb into a grammatically correct form considering auxiliary and model verbs. For example, the question generation system of Varga and Le [3] uses a set of transformation rules for question formation. For subject–verb–object clauses whose subject {{has been identified as}} a target concept, a “Which Verb Object” template is selected and <b>matched</b> against the <b>clause.</b> The question word “Which” then replaces the target concept in the selected clause. For key concepts that are in the object position of a subject–verb–object, the verb phrase is adjusted (i.e., an auxiliary verb is used). Varga and Le reported that generated questions achieved a score of 2.45 (2.85) with respect to relevance and a score of 2.85 (3.1) with respect to syntactic correctness and fluency given a scale between from 1 to 4, with 1 being the best score. Values outside and inside in the brackets indicate ratings of the 1 st and 2 nd human rater.|$|R
3000|$|... • The {{continuous}} SPARQL query {{engine is}} a software component which supports situation assessment by taking as input the continuous RDF data streams {{generated by the}} triplification engine and evaluating them against pre-registered continuous SPARQL queries. By registering appropriate SPARQL query against a data stream, {{we are able to}} detect critical situations – for example, service failures, high response time from services, overloaded message queues and network request time outs – with minimal delay: the continuous SPARQL engine will trigger as soon as RDF triples in the stream <b>match</b> the WHERE <b>clause</b> of any registered query. Using SPARQL and RDF triples in this way also makes it possible to benefit from inference capabilities – in addition to querying data and detecting complex event patterns, we are able to perform run-time analysis by reasoning over RDF triples [35]. Employing existing RDF streaming engines with “on-the-fly” analysis of constantly flowing observations from hundreds of sensors is expected to help us achieve near real-time behaviour [36] of the adaptation framework (as opposed to “static” approaches where monitored data is first stored on the hard drive before being analysed) – a key requirement when developing an adaptation mechanism.|$|R
50|$|At club level, Ziege {{played for}} Bayern Munich (1990-97), Milan (1997-99) and Middlesbrough (1999-2000). In summer 2000 Liverpool F.C. made a £5.5m bid which exactly <b>matched</b> a get-out <b>clause</b> in Ziege's contract. Middlesbrough insisted they had {{received}} offers in excess of £8m for Ziege, but were forced contractually to allow Ziege to talk to Liverpool, who then signed him. He made his debut for Liverpool in a 3-2 home win over Manchester City on 9 September 2000, replacing Steven Gerrard in the second half. A combination of knee injuries and the improving form of Jamie Carragher, meant he was transferred to Tottenham Hotspur {{at the end of}} that season. He scored two goals during his spell at Liverpool; against Leeds in the league and Stoke in the League Cup. Ziege also contributed to their treble in the 2000-01 season. He came on as an extra time substitute in the 2001 Football League Cup Final and scored a penalty in the shootout as Liverpool defeated Birmingham City, but he was not part of the match days squads for either the 2001 FA Cup Final or 2001 UEFA Cup Final.|$|R
40|$|In a {{joint effort}} of the Peshitta Institute Leiden and the Werkgroep Informatica of the Vrije Universiteit Amsterdam an {{electronic}} database of Syriac texts is being developed. Percy van Keulen and I have been assigned the Books of Kings, which we have analyzed from morpheme level up through clause-level parsing. Using the Hebrew material already available in the Werkgroep Informatica database, a synopsis of the Masoretic text and the Peshitta has been made at clause level. On {{the basis of the}} synop-sis, clause constituents have been matched, providing a basis for <b>matching</b> phrases within <b>clauses,</b> and for <b>matching</b> words within phrases. One of the products is an elec¬tronic translation concordance with lists of translation correspondences occur¬ring within Kings, which was introduced at the 2005 ISLP meeting in Philadelphia. The lexical items occurring at corresponding points in the two texts need not necessarily be lexicon-based semantic translations of one another, but they are what do occur at that point in the two texts. In this manner, both similarities and differences are brought to light. The occurrences of the two cognate verbs sym and swm within Kings are illustrative of the factors at work during the process of translation...|$|R
40|$|This {{document}} {{constitutes an}} outline specification {{for a new}} tracer for Prolog, the design {{of which has been}} guided by an evaluation of the relative strengths and weaknesses of a number of existing Prolog tracers. The new tracer, known as the `Textual Tree Tracer' (or `TTT' for short) will produce a `sideways tree' representation of the execution of a goal, using only textual output, i. e. it will not require the use of any specialised graphics. Its key features include the following: a compact and yet very informative basic form of output, which distinguishes <b>clause</b> <b>matching</b> events, and several different goal failure modes; clear display of the structure of computation and the flow of control, via the use of a tree representation; extensive use of default controls to limit the quantity of trace output produced; the facility of retrospective inspection of earlier parts of the trace, in order to obtain more detailed information; and a specialised `database window' which facilitates correlation of the trace with the source code, and shows dynamically any changes to the database resulting from the assertion or retraction of clauses...|$|R
