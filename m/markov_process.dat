4850|9903|Public
5|$|A Markov {{chain is}} a type of <b>Markov</b> <b>process</b> that has either {{discrete}} state space or discrete index set (often representing time), but the precise definition of a Markov chain varies. For example, it is common to define a Markov chain as a <b>Markov</b> <b>process</b> in either discrete or continuous time with a countable state space (thus regardless of the nature of time), but it is also common to define a Markov chain as having discrete time in either countable or continuous state space (thus regardless of the state space).|$|E
5|$|Markov {{processes}} are stochastic processes, traditionally in discrete or continuous time, {{that have the}} Markov property, which means the next value of the <b>Markov</b> <b>process</b> depends on the current value, but it is conditionally independent of the previous values of the stochastic process. In other words, {{the behavior of the}} process in the future is stochastically independent of its behavior in the past, given {{the current state of the}} process.|$|E
25|$|The {{adjective}} Markovian is used {{to describe}} something that is related to a <b>Markov</b> <b>process.</b>|$|E
40|$|We provide basic {{results for}} the {{development}} of a theory of possibilistic <b>Markov</b> <b>processes.</b> We define and study possibilistic <b>Markov</b> <b>processes</b> and possibilistic <b>Markov</b> chains, and derive a possibilistic analogon of the Chapman-Kolmogorov equation. We also show how possibilistic <b>Markov</b> <b>processes</b> can be constructed using one-step transition possibilities. ...|$|R
50|$|Andrey <b>Markov</b> studied <b>Markov</b> <b>processes</b> in {{the early}} 20th century, {{publishing}} his first paper on the topic in 1906, but earlier uses of <b>Markov</b> <b>processes</b> already existed. Random walks on the integers and the Gambler's ruin problem are examples of <b>Markov</b> <b>processes</b> and were studied hundreds of years earlier. Two important examples of <b>Markov</b> <b>processes</b> are the Wiener process, {{also known as the}} Brownian motion process, and the Poisson process, which are considered the most important and central stochastic processes in the theory of stochastic processes, and were discovered repeatedly and independently, both before and after 1906, in various settings. These two <b>processes</b> are <b>Markov</b> <b>processes</b> in continuous time, while random walks on the integers and the Gambler's ruin problem are examples of <b>Markov</b> <b>processes</b> in discrete time.|$|R
40|$|Since {{the seminal}} work of Lamperti {{there is a lot}} of {{interest}} in the understanding of the general structure of self-similar <b>Markov</b> <b>processes.</b> Lamperti gave a representation of positive self-similar <b>Markov</b> <b>processes</b> with initial condition strictly larger than 0 which subsequently was extended to zero initial condition. For real self-similar <b>Markov</b> <b>processes</b> (rssMps) there is a generalization of Lamperti's representation giving a one-to-one correspondence between <b>Markov</b> additive <b>processes</b> and rssMps with initial condition different from the origin. We develop fluctuation theory for <b>Markov</b> additive <b>processes</b> and use Kuznetsov measures to construct the law of transient real self-similar <b>Markov</b> <b>processes</b> issued from the origin. The construction gives a pathwise representation through two-sided <b>Markov</b> additive <b>processes</b> extending the Lamperti-Kiu representation to the origin. Comment: 38 page...|$|R
25|$|Monopoly as a <b>Markov</b> <b>Process,</b> by R. Ash and R. Bishop, Mathematics Magazine, vol. 45 (1972) pp.26–29.|$|E
25|$|A <b>Markov</b> <b>process</b> is a {{stochastic}} process which satisfies the Markov property {{with respect to}} its natural filtration.|$|E
25|$|Hidden Markov Model (HMM) is a {{statistical}} Markov {{model in which}} the system being modeled {{is assumed to be}} a <b>Markov</b> <b>process</b> with unobserved (i.e. hidden) states.|$|E
40|$|We {{establish}} Lamperti representations for semi-stable <b>Markov</b> <b>processes</b> in locally compact groups. We also {{study the}} particular cases of processes with values in {{and under the}} hypothesis {{that they do not}} visit 0. These Lamperti representations yield some properties of these semi-stable <b>Markov</b> <b>processes.</b> Semi-stable <b>Markov</b> <b>processes</b> Multiplicative Lévy processes Lamperti representation Time-change Feller processes...|$|R
40|$|A {{growing body}} of recent works have been devoted {{to the study of}} the {{favorite}} points of various concrete <b>Markov</b> <b>processes.</b> We contribute to this subject by showing that for a large class of recurrent strongly symmetric <b>Markov</b> <b>processes,</b> singletons are polar for the most visited site(s). Keywords most visited site, local times, symmetric <b>Markov</b> <b>processes...</b>|$|R
40|$|Abstract. The Baum-Welch {{algorithm}} {{is a technique}} for the maximum likelihood parameter estimation of probabilistic functions of <b>Markov</b> <b>processes.</b> We apply this technique to nonstationary <b>Markov</b> <b>processes</b> and explore {{a relationship between the}} Baum-Welch algorithm and the BCJR algorithm. Furthermore, we apply the Baum-Welch algorithm to two nonstationary <b>Markov</b> <b>processes</b> and obtain the turbo decoding algorithm. ...|$|R
25|$|In {{recursive}} Bayesian estimation, {{the true}} state {{is assumed to}} be an unobserved <b>Markov</b> <b>process,</b> and the measurements are the observed states of a hidden Markov model (HMM).|$|E
25|$|A Markov {{chain is}} a type of <b>Markov</b> <b>process</b> that has either {{discrete}} state space or discrete index set (often representing time), but the precise definition of a Markov chain varies. For example, it is common to define a Markov chain as a <b>Markov</b> <b>process</b> in either discrete or continuous time with a countable state space (thus regardless of the nature of time), but it is also common to define a Markov chain as having discrete time in either countable or continuous state space (thus regardless of the state space).|$|E
25|$|Methods of {{statistics}} {{may be used}} predicatively in performance art, as in a card trick based on a <b>Markov</b> <b>process</b> that only works some of the time, the occasion {{of which can be}} predicted using statistical methodology.|$|E
5000|$|Writing in 1931, Andrei Kolmogorov {{started from}} {{the theory of}} {{discrete}} time <b>Markov</b> <b>processes,</b> which are described by the Chapman-Kolmogorov equation, and sought to derive a theory of continuous time <b>Markov</b> <b>processes</b> by extending this equation. He found {{that there are two}} kinds of continuous time <b>Markov</b> <b>processes,</b> depending on the assumed behavior over small intervals of time: ...|$|R
40|$|Abstract. The {{nonlinear}} <b>Markov</b> <b>processes</b> are measure-valued dynamical systems which pre-serve positivity. They can {{be represented}} as the law of large numbers limits of general Markov models of interacting particles. In physics, the kinetic equations allow Lyapunov functionals (entropy, free energy, etc.). This may {{be considered as a}} sort of inheritance of the Lyapunov functionals from the microscopic master equations. We study nonlinear <b>Markov</b> <b>processes</b> that inherit thermodynamic properties from the microscopic linear <b>Markov</b> <b>processes.</b> We develop the thermodynamics of nonlinear <b>Markov</b> <b>processes</b> and analyze the asymptotic assumption, which are sufficient for this inheritance...|$|R
40|$|This paper {{considers}} {{basic questions}} regarding <b>Markov</b> random <b>processes.</b> It shows that continuous-time, continuous-valued, wide-sense stationary, <b>Markov</b> <b>processes</b> that have absolutely continuous second-order distribution and finite second moment are not bandlimited. It {{also shows that}} continuous-time, stationary, <b>Markov</b> <b>processes</b> that are continuous-valued or discrete-valued and satisfy additional mild conditions cannot be recovered from uniform sampling. Further it shows that continuous-time, continuous-valued, stationary, <b>Markov</b> <b>processes</b> that have absolutely continuous second-order distributions and are continuous almost surely, cannot be recovered without error after quantization. Finally, it provides necessary and sufficient conditions for stationary, discrete-time, <b>Markov</b> <b>processes</b> to have zero entropy rate, and relates this to information singularity...|$|R
25|$|Operations {{research}} provides {{techniques for}} solving practical problems in engineering, business, and other fields — {{problems such as}} allocating resources to maximize profit, or scheduling project activities to minimize risk. Operations research techniques include linear programming and other areas of optimization, queuing theory, scheduling theory, network theory. Operations research also includes continuous topics such as continuous-time <b>Markov</b> <b>process,</b> continuous-time martingales, process optimization, and continuous and hybrid control theory.|$|E
25|$|Markov chains {{have many}} {{applications}} as statistical models of real-world processes, such as studying cruise control systems in motor vehicles, queues or lines of customers arriving at an airport, exchange rates of currencies, storage {{systems such as}} dams, and population growths of certain animal species. The algorithm known as PageRank, which was originally proposed for the internet search engine Google, {{is based on a}} <b>Markov</b> <b>process.</b> Furthermore, Markov processes are the basis for general stochastic simulation methods known as Gibbs sampling and Markov Chain Monte Carlo, are used for simulating random objects with specific probability distributions, and have found extensive application in Bayesian statistics.|$|E
2500|$|A <b>Markov</b> <b>process</b> {{is called}} a {{reversible}} <b>Markov</b> <b>process</b> or reversible Markov chain precisely if it satisfies the detailed balance equations. These equations require that the transition probability matrix, P, for the <b>Markov</b> <b>process</b> possess a stationary distribution (i.e. equilibrium probability distribution) π such that ...|$|E
40|$|<b>Markov</b> <b>processes</b> {{are popular}} {{mathematical}} models, studied by theoreticians for their intriguing properties, and applied by practitioners for their flexible structure. With this book we teach how to model and analyze <b>Markov</b> <b>processes.</b> We classify <b>Markov</b> <b>processes</b> {{based on their}} structural properties, which in turn determine which analytic methods are required for solving them. In doing so, we start in each chapter with specific examples that naturally lead up to general theory and general methods. In this way the reader learns about <b>Markov</b> <b>processes</b> on the job. By studying this book, the reader becomes acquainted with the basic analytic methods that come into play when systems are modeled as structured <b>Markov</b> <b>processes.</b> These basic methods will likely prove useful, in real-time when studying the examples at hand, but more importantly for future encounters with <b>Markov</b> <b>processes</b> not covered in this book. Methods {{are more important than}} examples. The methods have a large scope of application, even outside the scope of <b>Markov</b> <b>processes,</b> in areas like probability theory, industrial engineering, mechanical engineering, physics and financial mathematics. Comment: This first version of the monograph is missing exercises. We will add these in the next version...|$|R
5000|$|Diffusions, <b>Markov</b> <b>processes,</b> and martingales, Wiley 1979; 2nd. edn. with L. C. G. Rogers: Diffusions, <b>Markov</b> <b>processes,</b> and martingales, Volume One: Foundations, Wiley 1995; reprinting of 2nd edn. Cambridge University Press 2000 ...|$|R
2500|$|<b>Markov</b> studied <b>Markov</b> <b>processes</b> in {{the early}} 20th century, {{publishing}} his first paper on the topic in 1906. Random walks on integers and the gambler's ruin problem are examples of <b>Markov</b> <b>processes.</b> [...] Some variations of these processes were studied hundreds of years earlier {{in the context of}} independent variables. Two important examples of <b>Markov</b> <b>processes</b> are the Wiener process, also known as the Brownian motion process, and the Poisson process, which are considered the most important and central stochastic processes in the theory of stochastic processes, and were discovered repeatedly and independently, both before and after 1906, in various settings. These two <b>processes</b> are <b>Markov</b> <b>processes</b> in continuous time, while random walks on the integers and the gambler's ruin problem are examples of <b>Markov</b> <b>processes</b> in discrete time.|$|R
2500|$|In the {{stochastic}} {{interpretation is}} not possible to define velocities for particles, i.e. the paths are not smooth. Moreover, to know the motion of the particles at any moment, {{you have to know what}} the <b>Markov</b> <b>process</b> is. However, once we know the exactly initial conditions and the <b>Markov</b> <b>process,</b> the theory is in fact a realistic interpretation of quantum mechanics.|$|E
2500|$|... {{which has}} the same form as the {{diffusion}} equation, with diffusion coefficient [...] In that case, the diffusivity yields the De Broglie relation {{in accordance with the}} <b>Markov</b> <b>process.</b>|$|E
2500|$|... {{and because}} the Kalman filter {{describes}} a <b>Markov</b> <b>process,</b> all relevant information from previous observations is contained in the current state estimate [...] Thus the marginal likelihood is given by ...|$|E
40|$|We give {{necessary}} and sufficient conditions {{in order that}} exponentials of additive functionals of <b>Markov</b> <b>processes</b> have finite expectations. Furthermore, we obtain sharp estimates for these expectations. More precisely, we investigate both the Stieltjes exponential and the ordinary exponential of right-continuous additive functionals of general right-continuous, time-inhomogenous <b>Markov</b> <b>processes.</b> The well-known Khas'minskii Lemma (1959, Probab. Appl. 4, 309 - 318) follows as a corollary. <b>Markov</b> <b>processes</b> Additive functionals Khas' minskii Lemma...|$|R
40|$|This paper {{surveys the}} recent {{progresses}} {{made in the}} field of unstable denumerable <b>Markov</b> <b>processes.</b> Emphases are laid upon methodology and applications. The important tools of Feller transition functions and Resolvent Decomposition Theorems are highlighted. Their applications particularly in unstable denumerable <b>Markov</b> <b>processes</b> with a single instantaneous state and <b>Markov</b> branching <b>processes</b> are illustrated...|$|R
2500|$|<b>Markov</b> <b>processes</b> {{can also}} be used to {{generate}} superficially real-looking text given a sample document. <b>Markov</b> <b>processes</b> are used in a variety of recreational [...] "parody generator" [...] software (see dissociated press, Jeff Harrison, Mark V Shaney ...|$|R
2500|$|Definition: A {{stationary}} <b>Markov</b> <b>process</b> is time reversible if (in {{the steady}} state) {{the amount of}} change from state [...] to [...] {{is equal to the}} amount of change from [...] to , (although the two states may occur with different frequencies). [...] This means that: ...|$|E
2500|$|The word {{stochastic}} is used {{to describe}} other terms and objects in mathematics. Examples include a stochastic matrix, which describes a stochastic process known as a <b>Markov</b> <b>process,</b> and stochastic calculus, which involves differential equations and [...] integrals based on stochastic processes such as the Wiener process, also called the Brownian motion process.|$|E
2500|$|On {{the real}} line, the Poisson {{process is a}} type of continuous-time <b>Markov</b> <b>process</b> known as a birth-death process (with just births and [...] zero deaths) and is called a pure [...] or simple birth process. More {{complicated}} processes with the Markov property, such as Markov arrival processes, have been defined where the Poisson process is a special case.|$|E
40|$|Continuous-time <b>Markov</b> <b>processes</b> can be {{characterized}} conveniently by their infinitesimal generators. For such processes there exist forward and reverse-time generators. The authors show {{how to use these}} generators to construct moment conditions implied by stationary <b>Markov</b> <b>processes.</b> Generalized method of moments estimators and tests can be constructed using these moment conditions. The resulting econometric methods are designed to be applied to discrete-time data obtained by sampling continuous-time <b>Markov</b> <b>processes.</b> Copyright 1995 by The Econometric Society. ...|$|R
5|$|The Brownian motion {{process and}} the Poisson process (in one dimension) are both {{examples}} of <b>Markov</b> <b>processes</b> in continuous time, while random walks on the integers and the gambler's ruin problem are examples of <b>Markov</b> <b>processes</b> in discrete time.|$|R
40|$|Mathematics Subject Classification: 80 A 30 / 60 J 25 / 60 J 60 / 60 J 75 / 82 B 40 The {{nonlinear}} <b>Markov</b> <b>processes</b> are measure-valued dynamical systems which preserve positivity. They can {{be represented}} as the law of large numbers limits of general Markov models of interacting particles. In physics, the kinetic equations allow Lyapunov functionals (entropy, free energy, etc.). This may {{be considered as a}} sort of inheritance of the Lyapunov functionals from the microscopic master equations. We study nonlinear <b>Markov</b> <b>processes</b> that inherit thermodynamic properties from the microscopic linear <b>Markov</b> <b>processes.</b> We develop the thermodynamics of nonlinear <b>Markov</b> <b>processes</b> and analyze the asymptotic assumption, which are sufficient for this inheritance. Peer-reviewedPublisher Versio...|$|R
