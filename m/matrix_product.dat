1422|1341|Public
25|$|In other words: the <b>matrix</b> <b>product</b> is the {{description}} in coordinates {{of the composition}} of linear functions.|$|E
500|$|... where [...] is {{the matrix}} {{containing}} the coefficients of the given equations, [...] is the vector , [...] denotes the <b>matrix</b> <b>product,</b> and [...] is the zero vector. In a similar vein, the solutions of homogeneous linear differential equations form vector spaces. For example, ...|$|E
500|$|Multiplication of two {{matrices}} {{is defined}} if {{and only if}} the number of columns of the left matrix {{is the same as the}} number of rows of the right matrix. If A is an m-by-n matrix and B is an n-by-p matrix, then their <b>matrix</b> <b>product</b> AB is the m-by-p matrix whose entries are given by dot product of the corresponding row of A and the corresponding column of B: ...|$|E
5000|$|The <b>matrix</b> <b>products</b> {{giving us}} the term and {{document}} correlations then become ...|$|R
50|$|This {{involves}} <b>matrix</b> <b>products</b> {{and explicit}} inversion, thus limiting the practical block size.|$|R
5000|$|The medical {{technology}} company ACell is developing extracellular <b>matrix</b> <b>products</b> derived from pig bladder.|$|R
500|$|Determining the {{complexity}} of an algorithm means finding upper bounds or estimates of how many elementary operations such as additions and multiplications of scalars are necessary to perform some algorithm, for example, multiplication of matrices. For example, calculating the <b>matrix</b> <b>product</b> of two n-by-n matrix using the definition given above needs n3 multiplications, since {{for any of the}} n2 entries of the product, n multiplications are necessary. The Strassen algorithm outperforms this [...] "naive" [...] algorithm; it needs only n2.807 multiplications. A refined approach also incorporates specific features of the computing devices.|$|E
500|$|Matrices and matrix {{multiplication}} {{reveal their}} essential features when related to linear transformations, {{also known as}} linear maps. A real m-by-n matrix A {{gives rise to a}} linear transformation Rn → Rm mapping each vector x in Rn to the (<b>matrix)</b> <b>product</b> Ax, which is a vector in Rm. Conversely, each linear transformation f: Rn → Rm arises from a unique m-by-n matrix A: explicitly, the [...] of A is the ith coordinate of f(ej), where ej = (0,...,0,1,0,...,0) is the unit vector with 1 in the jth position and 0 elsewhere. The matrix A is said to represent the linear map f, and A is called the transformation matrix of f.|$|E
2500|$|The {{quaternion}} product AC can now {{be represented}} as the <b>matrix</b> <b>product</b> ...|$|E
5000|$|Similarity transformations {{involving}} similar <b>matrices</b> are <b>matrix</b> <b>products</b> of {{the three}} square matrices, in the form: ...|$|R
2500|$|... including, {{shortest}} path, traveling salesman, knapsack, false coin, egg dropping, {{bridge and}} torch, replacement, chained <b>matrix</b> <b>products,</b> and critical path problem.|$|R
50|$|In addition, as of August 2015, NutraDried started {{developing}} protein <b>matrix</b> <b>products,</b> such {{as sports}} nutrients, protein bars, complete meal replacement snacks, and dietary products.|$|R
2500|$|Now, one {{can expand}} the <b>matrix</b> <b>product</b> in our {{equation}} by bilinearity ...|$|E
2500|$|The {{determinant}} of a <b>matrix</b> <b>product</b> of square matrices equals {{the product of}} their determinants: ...|$|E
2500|$|If , {{then one}} {{can take the}} <b>matrix</b> <b>product</b> the other way, {{yielding}} a scalar (or [...] matrix): ...|$|E
30|$|The {{matrices}} E_i and E_i' have {{a natural}} relation to decimation of matrices. The proof of the next proposition is a straightforward computation of <b>matrix</b> <b>products.</b>|$|R
30|$|Establish {{analytical}} formulas for {{calculating the}} maximum and minimum {{ranks of the}} multiple <b>matrix</b> <b>products</b> in (3.65)-(3.108), and characterize the performances of these products via the rank formulas.|$|R
40|$|The {{principal}} {{result in}} this paper {{is concerned with the}} derivative of a vector with respect to a block vector or matrix. This is applied to the asymptotic Fisher information matrix (FIM) of a stationary vector autoregressive and moving average time series process (VARMA). Representations which can be used for computing the components of the FIM are then obtained. In a related paper [1], the derivative is taken with respect to a vector. This is obtained by vectorizing the appropriate <b>matrix</b> <b>products</b> whereas {{in this paper}} the corresponding <b>matrix</b> <b>products</b> are left unchanged...|$|R
2500|$|Representing [...] and [...] as column matrices, {{the cross}} product can be {{expressed}} as a <b>matrix</b> <b>product</b> ...|$|E
2500|$|These {{relations}} are a direct {{consequence of the}} basic properties of determinants: evaluation of the [...] entry of the <b>matrix</b> <b>product</b> on the left gives the expansion by column [...] of the determinant of the matrix obtained from [...] by replacing column [...] by a copy of column , which is [...] if [...] and zero otherwise; the <b>matrix</b> <b>product</b> on the right is similar, but for expansions by rows.|$|E
2500|$|When this <b>matrix</b> <b>product</b> is {{interpreted}} as i j = k, then one obtains a subgroup {{of the matrix}} group that is isomorphic to the quaternion group. Consequently, ...|$|E
50|$|Generic {{applications}} can {{be written}} in a natural notation, e.g. , while the library dispatches to the appropriate algorithms: <b>matrix</b> vector <b>products</b> vs. <b>matrix</b> <b>products</b> vs. vector scalar products etcetera. The goal is to encapsulate performance issues inside the library and provide scientists an intuitive interface. MTL4 is used in different finite element and finite volume packages, e.g. the FEniCS Project.|$|R
40|$|In this article, {{first we}} generalize the Rudin-Shapiro {{sequence}} {{by means of}} counting digit pattern. Next show that, if the generalized Rudin-Shapiro sequence is not periodic, then no arithmetic subsequence of the generalized Rudin-Shapiro sequence is periodic. We give the simultaneous recursively word definition of the generalized Rudin-Shapiro sequence. We apply the transcendence measures criterion established by Adamczewski-Bugeaud [AB 2] to find that, the series generated by an arithmetic subsequence of the non-periodic generalized Rudin-Shapiro sequence gives a Liouville number or an $S$-number or $T$-number. We also define two infinite <b>matrix</b> <b>products</b> from the simultaneous recursively word definition of the generalized Rudin-Shapiro sequence. We can regard the two infinite <b>matrix</b> <b>products</b> as generalizations of the infinite product studied in [AmV 1], [AmV 2], [Ta]. We study the transcendence of the power series generated by the two infinite <b>matrix</b> <b>products</b> by using the generalized Rudin-Shapiro sequence's properties. Comment: 25 page. Major revision (Change the results and title), add the section...|$|R
40|$|We {{introduce}} representations for { 1, 2, 3 }, { 1, 2, 4 }-inverses {{in terms}} of <b>matrix</b> <b>products</b> involving theMoore-Penrose inverse. We also use representations of { 2, 3 } and { 2, 4 }-inverses of a prescribed rank, introduced in [6] and [9]. These representations can be computed {{by means of the}} modification of the hyper-power iterative process which is used in computing <b>matrix</b> <b>products</b> involving the Moore-Penrose inverse, introduced in [8]. Introduced meth-ods have arbitrary high orders q ≥ 2. A few comparisons with the known modification of the hyper-power method from [17] are presented. 1...|$|R
2500|$|The {{stress vector}} [...] across a surface with normal vector [...] with {{coordinates}} [...] is then a <b>matrix</b> <b>product</b> [...] (where T in upper index is transposition) (look on Cauchy stress tensor), that is ...|$|E
2500|$|The <b>matrix</b> <b>product</b> of [...] and [...] is not hermitian, {{but has a}} {{real and}} {{imaginary}} part. The real part is one half the symmetric expression , while the imaginary part {{is proportional to the}} commutator ...|$|E
2500|$|Moreover, since [...] for any {{square matrix}} A, the {{following}} identities {{can be derived}} (they are obtained from two different coefficients of the <b>matrix</b> <b>product,</b> and one may easily deduce the second one from the first one by changing [...] into [...] ), ...|$|E
40|$|Abstract. In 1999, Wei [M. Wei, Reverse order {{laws for}} {{generalized}} inverse of multiple <b>matrix</b> <b>products,</b> Linear Algebra Appl., 293 (1999), pp. 273 - 288] studied reverse order laws for generalized inverses of multiple <b>matrix</b> <b>products</b> and derived some necessary and sufficient conditions for An{ 1 }An− 1 { 1 }···A 1 { 1 } ⊆(A 1 A 2 ···An) { 1 } by using P-SVD (Product Singular Value Decomposition). In this paper, using the maximal rank of the generalized Schur complement, a new simpler equivalent condition is obtained {{in terms of}} only {{the ranks of the}} known matrices for this inclusion...|$|R
40|$|AbstractAn {{efficient}} algorithm, {{based on}} the LDL∗ factorization, for computing { 1, 2, 3 } and { 1, 2, 4 } inverses and the Moore–Penrose inverse of a given rational matrix A, is developed. We consider <b>matrix</b> <b>products</b> A∗A and AA∗ and corresponding LDL∗ factorizations in order to compute the generalized inverse of A. By considering the <b>matrix</b> <b>products</b> (R∗A) †R∗ and T∗(AT∗) †, where R and T are arbitrary rational matrices with appropriate dimensions and ranks, we characterize classes A{ 1, 2, 3 } and A{ 1, 2, 4 }. Some evaluation times for our algorithm are compared with corresponding times for several known algorithms for computing the Moore–Penrose inverse...|$|R
40|$|AbstractThis {{paper is}} {{concerned}} with {{a long list of}} <b>matrix</b> <b>products</b> for partitioned and non-partitioned matrices, including Kronecker, Khatri–Rao of first kind, Hadamard, Tracy–Singh, Khatri–Rao, block Kronecker, block Hadamard,and box products (sums). Our contribution is to gather the most useful connections of the <b>matrix</b> <b>products</b> from various sources and present them in one place. Several new attractive connections are also added to this collection. We give, as an example, two applications in order to show that these connections {{play a central role in}} many applications. These applications are: A block diagonal least-squares problem and a generalization of matrix inequalities involving Khatri–Rao products of positive definite matrices...|$|R
2500|$|We {{are looking}} for a kernel vector a = [...] such that the <b>matrix</b> <b>product</b> of M on a yields the zero vector [...] The {{dimensional}} matrix as written above is in reduced row echelon form, so one can read off a kernel vector within a multiplicative constant: ...|$|E
2500|$|The dual {{quaternion}} product ÂĈ = (A, B)(C, D) = (AC, AD+BC) can be formulated as {{a matrix}} operation as follows. [...] Assemble {{the components of}} Ĉ into the eight dimensional array Ĉ = (C1, C2, C3, c0, D1, D2, D3, d0), then ÂĈ is given by the 8x8 <b>matrix</b> <b>product</b> ...|$|E
2500|$|The inverse {{of every}} {{orthogonal}} matrix is again orthogonal, {{as is the}} <b>matrix</b> <b>product</b> of two orthogonal matrices. In fact, the set of all [...] orthogonal matrices satisfies all the axioms of a group. It is a compact Lie group of dimension , called the orthogonal group and denoted by [...]|$|E
50|$|Computing <b>matrix</b> <b>products</b> is both {{a central}} {{operation}} in many numerical algorithms and potentially time consuming, {{making it one of}} the most well-studied problems in numerical computing. Various algorithms have been devised for computing , especially for large matrices.|$|R
40|$|An {{approach}} that efficiently solves for a desired parameter {{of a system}} or device that can include both electrically large fast multipole method (FMM) elements, and electrically small QR elements. The system or device is setup as an oct-tree structure that can include regions of both the FMM type and the QR type. An iterative solver is then used to determine a first <b>matrix</b> vector <b>product</b> for any electrically large elements, and a second <b>matrix</b> vector <b>product</b> for any electrically small elements that {{are included in the}} structure. These <b>matrix</b> vector <b>products</b> for the electrically large elements and the electrically small elements are combined, and a net delta for a combination of the <b>matrix</b> vector <b>products</b> is determined. The iteration continues until a net delta is obtained that is within predefined limits. The <b>matrix</b> vector <b>products</b> that were last obtained are used to solve for the desired parameter...|$|R
3000|$|The {{complexities}} {{of these two}} algorithms are analyzed in terms of floating-point operation (FLOP) and we count each complex operation as one FLOP [35]. The complexity mainly comes from <b>matrix</b> <b>products</b> and EVDs. In (10), U_k,elU_k,el^H needs (2 r [...]...|$|R
