10|24|Public
40|$|We {{present a}} {{definition}} of unsigned magnification in gravitational lensing valid on arbitrary convex normal neighborhoods of time oriented Lorentzian manifolds. This definition is a function defined at any two points along a null geodesic that lie in a convex normal neighborhood, and foregoes the usual notions of lens and source planes in gravitational lensing. Rather, it makes essential use of the van Vleck determinant, which we present via the exponential map, and Etherington's definition of luminosity distance for arbitrary spacetimes. We then specialize our definition to spacetimes, like Schwarzschild's, in which the lens is compact and isolated, and show that our <b>magnification</b> <b>function</b> is monotonically increasing along any geodesic contained within a convex normal neighborhood. Comment: 16 pages; v 2 : minor typos correcte...|$|E
40|$|We propose and {{experimentally}} {{demonstrate a}} refractive index (RI) sensor based on cascaded microfiber knot resonators (CMKRs) with Vernier effect. Deriving from high proportional evanescent field of microfiber and spectrum <b>magnification</b> <b>function</b> of Vernier effect, the RI sensor shows high sensitivity {{as well as}} high detection resolution. By using the method named "Drawing-Knotting-Assembling (DKA) ", a compact CMKRs is fabricated for experimental demonstration. With the assistance of Lorentz fitting algorithm on the transmission spectrum, sensitivity of 6523 nm/RIU and detection resolution up to 1. 533 x 10 - 7 RIU are obtained in the experiment which show good agreement with the numerical simulation. The proposed all-fiber RI sensor with high sensitivity, compact size and low cost can be widely used for chemical and biological detection, {{as well as the}} electronic/magnetic field measuremen...|$|E
40|$|Participants in a {{real-time}} groupware conference {{require a}} sense of awareness about other people's interactions within a large shared workspace. Fisheye views can afford this awareness by assigning a focal point to each participant. The fisheye effect around these multiple focal points provides peripheral awareness by showing people's location in the global context, and by magnifying the area around their work to highlight interaction details. An adjustable <b>magnification</b> <b>function</b> lets people customize the awareness information to fit their collaboration needs. A fisheye text editor illustrates how this can be accomplished. Keywords Groupware, fisheye views, awareness, visualization. INTRODUCTION Real-time distributed groupware typically provides a shared virtual workspace where people can see and manipulate work artifacts. Many systems now follow a relaxed "whatyou -see-is-what-I-see" (relaxed-WYSIWIS) model, where people can have different viewports into the workspace. The problem [...] ...|$|E
40|$|Abstract:- By {{interposing}} a layer {{with low}} horizontal stiffness but with high damping characteristics between {{the structure and}} his foundation, an aseismic isolation system partly decouples the building structure from the horizontal components of the earthquake ground motion and thus diminishes the structural demand. As {{a result of the}} lateral flexibilization, the natural period of the former fixed-base structure undergoes a jump and the new base-isolation structure has a new and larger natural period. This “period-shift ” can extract a structure away from the dominant period of the earthquake ground motion and thus can avoid the destructive effects given by the system resonance. The dynamic behavior of the materials and devices of the isolating layer governs the performance of base-isolation system. The dynamic properties, such as horizontal stiffness and damping capacity determine the filtering role of the isolating layer and, finally, the structural dynamic response. But all of materials and devices used in an isolating layer systems exhibit, more or less, a nonlinear behavior. In addition, all site soils materials have the well-known nonlinear mechanical characteristics, which affect the dynamic structural response. Thus, in the dynamic response evaluation of a base-isolated system the nonlinear behavior of both isolator and site layers must taken into account. This paper presents a method for the necessary period-shift determination by using the dynamic linear or/and nonlinear <b>magnification</b> <b>functions.</b> For the nonlinear <b>magnification</b> <b>functions</b> determination we shall use a nonlinear Kelvin-Voigt model (NKV model) for base-isolated structure, which is able to model the effects of the soils and isolating layer nonlinearity on the shape and resonant magnitude of the <b>magnification</b> <b>functions.</b> Thus, we shall obtain a proper tool for modeling the resonant peak dispersion, which is a very important condition for a correct period-shift evaluation and therefore for a correct isolation design...|$|R
5000|$|... modern {{submarine}} periscopes incorporate lenses for <b>magnification</b> and <b>function</b> as telescopes. They typically employ prisms {{and total}} internal reflection instead of mirrors, because prisms, which do not require coatings on the reflecting surface, are much more rugged than mirrors. They may have additional optical capabilities such as range-finding and targeting. The mechanical systems of submarine periscopes typically use hydraulics {{and need to be}} quite sturdy to withstand the drag through water. The periscope chassis may also support a radio or radar antenna.|$|R
40|$|Lensing {{effects on}} light rays from point light sources, such like Type Ia supernovae, are {{simulated}} in a clumpy universe model. In our universe model, {{it is assumed}} that all matter in the universe takes the form of randomly distributed objects each of which has finite size and is transparent for light rays. Monte-Carlo simulations are performed for several lens models, and we compute probability distribution <b>functions</b> of <b>magnification.</b> In the case of the lens models that have a smooth density profile or the same degree of density concentration as the spherical NFW (Navarro-Frenk-White) lens model at the center, the so-called gamma distributions fit well the <b>magnification</b> probability distribution <b>functions</b> if the size of lenses is sufficiently larger than the Einstein radius. In contrast, the gamma distributions do not fit the <b>magnification</b> probability distribution <b>functions</b> {{in the case of the}} SIS (Singular Isothermal Sphere) lens model. We find, by using the power law cusp model, that the <b>magnification</b> probability distribution <b>function</b> is fitted well using the gamma distribution only when the slope of the central density profile is not very steep. These results suggest that we may obtain information about the slope of the central density profiles of dark matter halo from the lensing effect of Type Ia supernovae. Comment: 25 pages, 12 figures, PTP accepted versio...|$|R
40|$|A picture {{which is}} a sample of random {{contrast}} noise is generated. The noise amplitude spectrum in each region of the picture is inversely proportional to spatial frequency contrast sensitivity for that region, assuming the observer fixates {{the center of the}} picture and is the appropriate distance from it. In this case, the picture appears to have approximately the same contrast everywhere. To the extent that contrast detection thresholds are determined by visual system noise, this picture can be regarded as a picture of the noise of that system. There is evidence that, at different eccentricities, contrast sensitivity functions differ only by a magnification factor. The picture was generated by filtering a sample of white noise with a filter whose frequency response is inversely proportional to foveal contrast sensitivity. It was then stretched by a space-varying <b>magnification</b> <b>function.</b> The picture summmarizes a noise linear model of detection and discrimination of contrast signals by referring the model noise to the input picture domain...|$|E
40|$|In {{this paper}} a filter is modeled, {{based on the}} visual system of mammals. In the filter the retinal {{ganglion}} receptive fields and simple cortical receptive fields are used as described by neurophysiologists. The functionality of a ganglion cell and a simple cell are modeled with a gaussian and Gabor function respectively. In the filter also the linear <b>magnification</b> <b>function</b> is included which gives {{the size of the}} receptive field with retinal eccentricity. With modeling the filter we hope to get more insights in the human visual system and in the primary visual cortex especially. 1 Introduction In the past forty years much work on sensory areas has been done by neurophysiologists, so parts in the primary visual cortex (of the macaque monkey) are now well understood. Kuffler was the first one who recorded the activity from the axons of the retinal ganglion cells that make up the optic nerve [10]. His experiments revealed the type of receptive fields the retinal ganglion cells poss [...] ...|$|E
40|$|We have {{measured}} {{visual field}} maps in several distinct visual areas of anaesthetized macaque monkeys using functional {{magnetic resonance imaging}} (fMRI). The maps were measured using phase-encoded retinotopic methods that have also been applied to human subjects. Expanding ring and rotating wedge phase-encoded measurements were acquired (8 -shot EPI, TE= 20 ms, and TR= 750, 16 plane, voxel 1 x 1 x 2 mm). In addition, higher resolution anatomical scans were obtained (3 D MDEFT; 0. 5 x 0. 5 x 0. 5 mm). The anatomical scans were used to register data from different functional scans. Also, these anatomical scans were segmented into white matter and gray matter to produce flat maps of the activation data in order to visualize retinotopic maps. Strong signals were observed in area V 1 and several other areas known to be connected directly to V 1. The retinotopic maps produced clear definitions of the boundary between areas V 1 and V 2 in all animals. These maps parallel closely the results in humans using similar methods. In addition, retinotopic organization in a region on the posterior bank of the STS, probably corresponding to area MT, could be observed. The cortical <b>magnification</b> <b>function</b> in V 1 was in close agreement with the function obtained in similar human fMRI measurements. In different animals, the foveal representation of V 1 and V 2 fell at significantly different positions with respect to the main anatomical landmarks...|$|E
40|$|Motivation: The {{performance}} of classifiers is often assessed using ROC (or AC) curves and the corresponding areas under the curves (AUCs). However, in many fundamental problems ranging from information retrieval to drug discovery, only {{the very top of}} the ranked list of predictions is of any interest and ROCs and AUCs are not very useful. New metrics, visualizations, and optimization tools are needed to address this “early retrieval ” problem. Results: To address the early retrieval problem, we develop the general CROC (Concentrated ROC) framework. In this framework, any relevant portion of the ROC (or AC) curve is magnified smoothly by an appropriate continuous transformation of the coordinates with a corresponding magnification factor. Appropriate families of <b>magnification</b> <b>functions</b> confined to the unit square are derived and their properties are analyzed together with the resulting CROC curves. The area under the CROC curve (AUC[CROC]) can be used to assess early retrieval. The general framework is demonstrated on a drug discovery problem and used to discriminate more accurately the early retrieval {{performance of}} five different predictors. From this framework, we propose a novel metric and visualization—the CROC(exp), an exponential transform of the ROC curve—as an alternative to other methods. The CROC(exp) provides a principled, flexible, and effective way for measuring and visualizing early retrieval performance with excellent statistical power. Corresponding methods for optimizing early retrieval are also described in the Appendix. Availability: Datasets are publicly available. Python code and command-line utilities implementing CROC curves and metrics is available a...|$|R
40|$|Lensing {{effects on}} light rays from point light sources in a clumpy {{universe}} model are simulated. In our universe model, {{it is assumed}} that all of the matter in the universe takes the form of randomly distributed objects each of which has finite size and is transparent for light rays. Further, we assume that each of the lens objects is axially symmetric and the mass ML and the “size ” L are related as L / √ ML =constant for each lens object. Then, the results of our simulation depend on the only value of L / √ ML if we fix the lens model. Monte-Carlo simulations are performed for several lens models, and we compute probability distribution <b>functions</b> of <b>magnification.</b> In the case of the lens models which have smooth density profile, the so called gamma distributions fit well the <b>magnification</b> probability distribution <b>functions</b> if L √ H 0 /ML is sufficiently large, where H 0 is the Hubble constant. The result is the same in the lens model which has the same degree of density concentration as the spherical NFW lens model at the center. In contrast, the gamma distributions do not fit the <b>magnification</b> probability distribution <b>functions</b> {{in the case of the}} SIS(Singular Isothermal Sphere) lens model. These facts suggest that we can distinguish the NFW lens model from the SIS lens model using the observation of the <b>magnification</b> probability distribution <b>function</b> of Type Ia supernovae. Subject headings: gravitational lensing – supernovae – dark matter – substructure...|$|R
40|$|We {{study the}} {{gravitational}} lensing magnification {{produced by the}} intervening cosmological matter distribution, as deduced from three different hierarchical models (SCDM, LCDM, CHDM) on very high redshift sources, particularly supernovae in protogalactic (Pop III) objects. By means of ray-shooting numerical simulations we find that caustics are more intense and concentrated in SCDM models. The <b>magnification</b> probability <b>function</b> presents a moderate degree of evolution up to z≈ 5 (CHDM) and z≈ 7 (SCDM/LCDM). All models predict that statistically large magnifications, μ 20 are achievable, with a probability {{of the order of}} a fraction of percent, the SCDM model being the most efficient magnifier. All cosmologies predict that above z≈ 4 there is a 10...|$|R
40|$|Performance {{in visual}} tasks {{can often be}} equated across eccentricities by proper scaling. The scaling or inverse <b>magnification</b> <b>function</b> (EMF), {{describes}} the ratio of peripheral to foveal stimulus size required to equate performance. Tasks and visual brain regions have different IMFs. It is argued in this thesis that IMFs may average out when a methodology insensitive {{to the presence of}} multiple IMFs is used. This fact is demonstrated through simulations. The present thesis introduces a data fitting technique that detects the presence of multiple IMFs in a psychophysical task. These are revealed as an interaction between stimulus configuration and eccentricity. These new techniques were used to investigate the percept of subjective contours (SC) defined by offset gratings which are thought to be encoded through a cooperation of V 1 and V 2 cells, two brain areas described by different IMFs. Five participants discriminated the orientation of a SC presented foveally (monocularly or binocularly) and at four eccentricities. SC length and carrier grating wavelength were adjusted until performance converged on 81 % correct. There was an interaction between eccentricity and stimulus configuration, F (20, 80) = 2. 063, p =. 0124, which was accounted for only if two IMFs were assumed. It was found that SC length (V 2) scaled faster than the wavelength (V 1) as a function of eccentricity. This qualitatively agrees with anatomical measures of V 1 and V 2 IMFs. The method developed here provides a more informative and more objective measure of eccentricity-dependent performance limitations than other commonly-used methods...|$|E
40|$|Some of the {{difficulties}} in determining the underlying physical properties that are relevant for observed anomalies in microlensing light curves, such as the mass and separation of extrasolar planets orbiting the lens star, or the relative source-lens parallax, are already anchored in factors that {{limit the amount of}} information available from ordinary microlensing events and in the way these are being parametrized. Moreover, a real-time detection of deviations from an ordinary light curve while these are still in progress can only be done against a known model of the latter, and such is also required for properly prioritizing ongoing events for monitoring in order to maximize scientific returns. Despite the fact that ordinary microlensing light curves are described by an analytic function that only involves a handful of parameters, modelling these is far less trivial than one might be tempted to think. A well-known degeneracy for small impacts, and another one for the initial rise of an event, makes an interprediction of different phases impossible, while in order to determine a complete set of model parameters, the fundamental characteristics of all these phases need to be properly assessed. While it is found that the wing of the light curve provides valuable information about the time-scale that absorbs the physical properties, the peak flux of the event can be meaningfully predicted only after {{about a third of the}} total magnification has been reached. Parametrizations based on observable features not only ease modelling by bringing the covariance matrix close to diagonal form, but also allow good predictions of the measured flux without the need to determine all parameters accurately. Campaigns intending to infer planet populations from observed microlensing events need to invest some fraction of the available time into acquiring data that allows to properly determine the <b>magnification</b> <b>function.</b> Key words: gravitational lensing – planetary systems. ...|$|E
40|$|It is very {{important}} to understand human visual characteristics in the wide-view condition for traffic safety. In the present study, we focus on human basic visual characteristics of retinotopic mapping by using functional Magnetic Resonance Imaging (fMRI), and provide basic research results for traffic safety. Previous fMRI studies on human visual retinotopic mapping are typically used for central and/or peri-central visual field stimulus. The retinotopic characteristics on human peripheral vision are still not well known. In this study, we developed a new visual presentation system widest view (60 degrees of eccentricity). The wide-view visual presentation system was made from nonmagnetic optical fibers and a contact lens, so it can used in general clinical fMRI conditions and the cost is lower. In the present study, by using the newly developed wide view visual presentation system, we have firstly been able to gain success in the world to identify the human primary visual cortex (V 1) with an eccentricity of 60 degrees and the quantitative relationship between V 1 and peripheral visual field (eccentricity up to 60 degrees) by fMRI. In addition, we estimated the mean area for the V 1 between 0 – 60 degree eccentricities, and acquired the areal cortical <b>magnification</b> <b>function</b> for V 1 between 0 – 60 degree eccentricities was Mareal = 272 /(E+ 1. 44) 2. The mean cortical surface area of V 1 between the 0 – 60 degree eccentricities was about 2229 mm 2. Our results have good agreement with using physiological, patient and anatomical measurements. From {{the results of the present}} study, it is explained why traffic accidences easily take place on crossings. In the human brain visual cortex (V 1), only a small cortical area is used to process visual information from the peripheral visual field. The basic results suggest that drivers are unable to recognise the dangerous information from both sides of peripheral vision...|$|E
40|$|We {{revisit the}} {{gravitational}} lensing phenomenon {{using a new}} visualization technique. It consists in projecting the observers sky into the source plane, what {{gives rise to a}} folded and stretched surface. This provides a clear graphical tool to visualize some interesting well-known effects, such as the development of multiple images of a source, the structure of the caustic curves, the parity of the images and their <b>magnification</b> as a <b>function</b> of the source position. ...|$|R
3000|$|..., the tangent changes {{orientation}} {{under increasing}} <b>magnification.</b> <b>Functions</b> {{such as the}} Weierstrass function cannot be differentiated (a whole number of times). But {{it turns out that}} these fractal functions can be differentiated a fractional number of times, and the fractional calculus is important for studying these differentiability properties. Fractals are characterized by scaling laws and the fractional derivative at a point can reveal this law. In recent research, scientists at the Mount Sinai School of Medicine have shown that the surfaces of breast cells are fractals and they have found clear differences in the scaling laws for benign cells and malignant cells. The different scaling laws have enabled accurate diagnosis of breast cancers. Another important new discovery that has brought fractional calculus into prominence is that many physical processes are modeled by fractional differential equations. Obviously, the importance of a mathematical model is that {{it can be used to}} make predictions and to give insight into the physical process that underlies the behavior. One area where mathematical models have been employed extensively is that of diffusion and transport processes. For example, the dispersion of pollutants in the ocean and the motion of electronic charges in conductors are diffusion processes. Here, a probabilistic description leads to a (whole number) differential equation which can be solved to predict average properties of the system. Similar types of equations are used by financial analysts to model stock prices. It has recently been discovered that processes governed by diffusion which is enhanced or hindered in some fashion are better modeled by FDEs than by integer-order differential equations. These FDEs are finding numerous applications in areas ranging from financial mathematics to ocean-atmosphere dynamics to mathematical biology [16].|$|R
40|$|In {{the article}} on the {{material}} of urbanonymy of Kiev demonstrated changes in creating and using a symbolic space for {{the approval of the}} state ideology. The author identifies five chronological periods: the pre-Soviet (before 1917), pre-war (1917 - 1941,), post-war (1941 - 1955), during the thaw and stagnation (1955 - 1986) and perestroika and independence (after 1986). It is demonstrated that in the twentieth century, the creation and modification urbanonymy are not spontaneously, but by administrative means. Urbanonymy in Soviet period is rarely associated with local toponymy. The names of streets and other urban facilities are formed from the names of the revolutionaries, military, authorities figures - and symbolic space of the city is like the necropolis. Also a very common use of geographical names with the UkrSSR and the entire Soviet Union. This results in a reduction informative and identifying the functions of urbanonymy and <b>magnification</b> ideological <b>function.</b> The city’s population is alienated from own streets, own place of life, is formed an abstract patriotism, but any local is condemned. In independent Ukraine we can seen the revival of forgotten names artistic figures, local toponymy, and new heroes are glorified in the names of streets. This is like a modern strategy of «back to basics» and «Pantheon of heroes»...|$|R
40|$|The goal of {{this thesis}} is to scan a ship hull with high 3 D {{accuracy}} and resolution using an underwater stereo camera so as to enable the future autonomous detection of invasive biofouling organisms with autonomous underwater vehicles (AUVs). However, turbidity in most harbours necessitates being within a metre of the hull and thus requires ultra wide-angle camera lenses. But such ultra wide-angle lenses embedded in an underwater housing with a flat port lead to significant distance dependent image distortions. Prior {{research in this area}} has only considered narrower fields of view and so has not solved for the significant image distortions arising from wide-angle high resolution flat port underwater cameras. This thesis proposes a solution to modelling and calibrating the underwater camera for accurate 2 D imaging and 3 D reconstruction, and additionally demonstrates an accurate underwater real-time pose estimation system required for future ship hull relative AUV navigation. In this thesis an ultra wide-angle, short-baseline stereo camera is used, which is embedded in a flat port underwater housing. Flat port underwater housings represent a cost efficient way to use arbitrary in-air cameras underwater. However, the flat port of the underwater housing is subject to light refraction and causes distance dependent distortion, which is particularly visible at the large angles of the ultra wide-angle stereo camera used. To incorporate the effects of refraction, the thesis uses the well-known and accurate physics-based refractive underwater camera model. In contrast to the perspective camera-based underwater camera model, the refractive underwater camera model accurately describes the distance dependency of distortion. In the beginning of this thesis, the effects of refraction caused by a thick flat port underwater housing are summarised and extended. In this context, the fundamental <b>magnification</b> <b>function</b> is proposed, which enables the description of numerous known and also newly discovered effects. An additional quantitative analysis is carried out in which the importance to model the thickness of the port and the wavelength of light is revealed. In refractive geometry with a thick flat port, refractive forward projection represents a fundamental operation and describes where a 3 D object point is observed in a 2 D camera image. Refractive forward projection is required in numerous applications, such as refractive calibration, bundle adjustment, simultaneous localisation and mapping (SLAM) or image restoration. Unlike perspective projection in air, this operation is non-linear and computationally more expensive. This thesis compares existing and proposes new refractive forward projection methods and shows in contrast to previous research that refractive forward projection is efficient enough for real-time applications. The thesis also investigates the impact of the port and the impact of the indices of refraction on the camera's projection and reconstruction accuracy. A novel investigation shows that the water pressure, water salinity, water temperature, air pressure and the wavelength of light significantly affect the projection and reconstruction accuracy of wide-angle flat port underwater stereo cameras and should not be neglected by standard refractive indices. Moreover, this thesis proposes an accurate and efficient calibration method for thick flat port underwater stereo cameras. The proposed calibration method mainly achieves its high accuracy by the use of a significantly higher number of calibration images. In contrast to prior research, the computation of the reprojection error does not represent a bottleneck if the proposed refractive forward projection method is used. In this way, the calibration is similar to standard in-air camera calibration techniques and minimises the reprojection error. In combination with the proposed more accurate indices of refraction and refractive calibration, the underwater reconstruction accuracy of the novel configuration of a wide-angle flat port underwater short baseline stereo camera is evaluated under real-world conditions. In this context, a method is proposed, which enables the evaluation of the accuracy of the reconstructed 3 D object space. Both chromatic aberration and pincushion distortion are effects of refraction and are particularly visible at the large angles of wide-angle underwater cameras. In order to obtained distortion-free images with minimised chromatic aberration to texturize reconstructed 3 D ship hull surfaces, this thesis proposes accurate real-time methods to minimise chromatic aberration and to correct the distortion in the underwater camera images. The refractive camera model is based on image coordinates of images, which are distortion-free in air. But these in-air undistorted images are strongly distorted in-water by refraction, particularly at the large angles of wide-angle flat port underwater cameras. Image correspondence in these images is difficult. For that reason, this thesis proposes pseudo rectified images in which these distortions are minimised. Moreover, an accurate and efficient representation of epipolar curves is presented, which enables, for example, real-time constrained correspondence search or dense stereo. This thesis concludes with the demonstration of a pose estimation system for future ship hull relative navigation. The proposed pose estimation system is the first underwater SLAM and visual odometry system, which is based on the more accurate refractive underwater camera model. This thesis shows that the proposed pose estimation system is very accurate in a water tank experiment and efficiently works in real-time, and thus is superior to prior underwater SLAM research, which is based on the less accurate perspective camera-based underwater camera model...|$|E
40|$|Rapid {{advances}} in communications and computer technologies {{in recent years}} have provided users with greater access to large volumes of data from computer-based information systems. The issue of the relatively small window through which an information space can be viewed brings with it two associated problems: presentation and navigation. This research is based on an approach called the Bifocal Display proposed by Spence and Apperley to address these inherent difficulties common in large information spaces in modern computing environments. The essence of this presentation technique is to provide the user with detailed local content as well as a global context to facilitate navigation. In this research, the original one-dimensional Bifocal Display concept has been extended in two-dimensional form to deal with two fundamental types of large information spaces: those with a high information density, for example, large databases and spreadsheets, and those with inherent spatial relationships, such as topographic maps and networks. An experimental study has been carried out to study the usability of the Bifocal Display and other presentation techniques based on various implementations of the London Underground map. Results have shown that the Bifocal Display is a usable and effective approach for the presentation of large information spaces. Presentation techniques can be broadly classified into distortion-oriented and non-distortion-oriented; the former generally requires more computational resources than the latter. With the increasing processing power of personal computers, researchers have developed a variety of novel distortion-oriented presentation techniques. Unfortunately, the distorting appearance resulting from the application of these techniques, coupled with the growing number of new terminologies used by researchers, has caused some confusion to the graphical user interface designer. A taxonomy of distortion-oriented techniques based on their <b>magnification</b> <b>functions</b> has been proposed to facilitate the identification of the similarities and differences of these techniques. A conceptual model has also been put forward to unveil the underlying principles which govern their operations. Despite the variety of novel presentation techniques currently available, the choice of a technique in a particular application remains very subjective; there is a general lack of selection guidelines or methodologies. An evaluation framework E 3 has been developed to provide a basis for the comparison of different presentation techniques, given the nature and characteristics of the data to be presented, and the interpretation required. E 3 focuses on three aspects of graphical data presentation: expressiveness, efficiency and effectiveness. This framework lays the foundation {{for the development of a}} set of metrics to facilitate an objective assessment of presentation techniques. A general visualisation tool, the InfoLens, has been designed based on the theoretical framework of this research. The design of the InfoLens has further demonstrated that the Bifocal Display is an effective approach to visualising large information spaces...|$|R
40|$|The {{instantaneous}} {{resolution and}} accuracy {{that can be}} expected from two inversion techniques {{to be used in}} the Earth Radiation Budget Experiment (ERBE) are examined. It is shown that measurement errors are magnified by the numerical filter and that this <b>magnification</b> is a <b>function</b> of the magnitude of the inversion factor. By using singular value decomposition, these magnitudes can be reduced to improve the estimates. A simulation of the estimation process using an albedo field derived from scanning radiometer data shows that retaining 6 of the 13 singular values gives the best results. If there are no bidirectional model errors the medium-field-of-view data give the best estimates. However, when bidirectional model errors are considered, the wide-field-of-view measurements give better estimates since they are less sensitive to these errors...|$|R
40|$|We {{present the}} {{analytical}} formulae for computing the magnification probability caused by cosmologically distributed galaxies. The galaxies {{are assumed to}} be singular, truncated-isothermal spheres without both evolution and clustering in redshift. We find that, for a fixed total mass, extended galaxies produce a broader shape in the magnification probability distribution and hence are less efficient as gravitational lenses than compact galaxies. The high-magnification tail caused by large galaxies is well approximated by an A exp - 3 form, while the tail by small galaxies is slightly shallower. The mean <b>magnification</b> as a <b>function</b> of redshift is, however, found to be independent {{of the size of the}} lensing galaxies. In terms of the flux conservation, our formulae for the isothermal galaxy model predict a mean magnification to within a few percent with the Dyer-Roeder model of a clumpy universe...|$|R
40|$|We {{study the}} weak {{gravitational}} lensing effects {{caused by a}} stochastic distribution of dark matter halos. We develop a simple approach to calculate the <b>magnification</b> probability distribution <b>function</b> which allows us to easily compute the magnitude bias and dispersion for an arbitrary data sample and a given universe model. As an application we consider the effects of single-mass large-scale cosmic inhomogeneities to the SNe magnitude-redshift relation, and conclude that such structures could bias the PDF enough to affect the extraction of cosmological parameters from the limited size of present-day SNe data samples. We also release turboGL, a simple and very fast (<= 1 s) Mathematica code based on the method here presented. Comment: 20 pages, 11 figures; replaced to match the version accepted for publication in Phys. Rev. D. The turboGL package [v 0. 3] can be downloaded at [URL]...|$|R
40|$|Employing {{asymmetric}} Bragg reflection at the monochromator we {{obtain a}} wide and practically parallel synchrotron‐radiation beam which, {{when combined with}} the use of a CCD detector, enables us to perform chemically specific, high‐speed and high‐resolution tomography. We present recent results obtained with this new method. The actual resolution achieved was determined from the measured line‐spread function of the complete detection system which includes the CCD, the fluorescent screen, and the lens used for optical <b>magnification.</b> The modulation‐transfer <b>function</b> (MTF) calculated from the line‐spread function shows that with only twofold magnification and at 3 % contrast detectability about 28 line pairs per mm are resolved. Extrapolating from this result we find that with an optical magnification of 7 : 1 about 100 line pairs per mm should be resolved. Ways to optimize the method further are discussed...|$|R
40|$|We {{study the}} {{gravitational}} lensing magnification {{produced by the}} intervening cosmological matter distribution, as deduced from three different hierarchical models (SCDM, LCDM, CHDM) on very high redshift sources, particularly supernovae in protogalactic (Pop III) objects. By means of ray-shooting numerical simulations we find that caustics are more intense and concentrated in SCDM models. The <b>magnification</b> probability <b>function</b> presents a moderate degree of evolution up to z ≈ 5 (CHDM) and z ≈ 7 (SCDM/LCDM). All models predict that statistically large magnifications, µ> ∼ 20 are achievable, with a probability {{of the order of}} a fraction of percent, the SCDM model being the most efficient magnifier. All cosmologies predict that above z ≈ 4 there is a 10 % chance to get magnifications larger than 3. We have explored the observational perspectives for Pop III SNe detection with NGST when gravitational magnification is taken into account. We find that NGST should be able to detect and confirm spectroscopically Type II SNe up to a redshift of z ≈ 4 in the J band (for TSN = 25000 K); this limit could be increased up to z ≈ 9 in the K band, allowing for a relatively moderate magnification. Possibly promising strategies to discriminate among cosmological models using their GL magnification predictions and very high-z SNe are sketched. Finally, we outline and discuss the limitations of our study. Subject headings: Cosmology: theory – dark matter – gravitational lensing – supernovae – galaxy evolution – methods: numerical – statistical – 3 – 1...|$|R
40|$|International audienceWe {{theoretically}} and experimentally {{investigate the}} design of an all-optical noiseless <b>magnification</b> and sampling <b>function</b> free from any active gain medium and associated high-power continuous wave pump source. The proposed technique {{is based on the}} co-propagation of an arbitrary shaped signal together with an orthogonally polarized intense fast sinusoidal beating within a normally dispersive optical fiber. Basically, the strong nonlinear phase shift induced by the sinusoidal pump beam on the orthogonal weak signal through cross-phase modulation turns the defocusing regime into localized temporal focusing effects. This periodic focusing is then responsible for the generation of a high-repetition-rate temporal comb upon the incident signal whose amplitude is directly proportional to its initial shape. This internal redistribution of energy leads to a simultaneous sampling and magnification of the signal intensity profile. This process allows us to experimentally demonstrate a 40 -GHz sampling operation as well as an 8 -dB magnification of an arbitrary shaped nanosecond signal around 1550 nm in a 5 -km long normally dispersive fiber. The experimental observations are in quantitative agreement with numerical and theoretical analysis...|$|R
40|$|We {{discuss the}} {{amplification}} dispersion in the observed luminosity of standard candles, like supernovae (SNe) of type Ia, induced by gravitational lensing in a Universe with dark energy (quintessence). We derive {{the main features}} of the <b>magnification</b> probability distribution <b>function</b> (pdf) of SNe {{in the framework of}} on average Friedmann-Lemaître-Robertson-Walker (FLRW) models for both lensing by large-scale structures and compact objects. Analytic expressions, in terms of hypergeometric functions, for luminosity distance–redshift relations in a flat Universe with homogeneous dark energy have been corrected for the effects of inhomogeneities in the pressureless dark matter (DM). The magnification pdf is strongly dependent on the equation of state, wQ, of the quintessence. With no regard to the nature of DM (microscopic or macroscopic), the dispersion increases with the redshift of the source and is maximum for dark energy with very large negative pressure; the effects of gravitational lensing on the magnification pdf, i. e. the mode biased towards de-amplified values and the long tail towards large magnifications, are reduced for both microscopic DM and quintessence with a...|$|R
40|$|Femtolensing is a {{gravitational}} lensing {{effect in}} which the <b>magnification</b> is a <b>function</b> {{not only of the}} positions and sizes of the source and lens, but also of the wavelength of light. Femtolensing is the only known effect of (10 − 13 − 10 − 16 M⊙) dark-matter objects and may possibly be detectable in cosmological gamma-ray burst spectra. We present a new and efficient algorithm for femtolensing calculations in general potentials. The physical-optics results presented here differ at low frequencies from the semi-classical approximation, {{in which the}} flux is attributed to a finite number of mutually coherent images. At higher frequencies, our results agree well with the semi-classical predictions. Applying our method to a point-mass lens with external shear, we find complex events that have structure at both large and small spectral resolution. In this way, we show that femtolensing may be observable for lenses up to 10 − 11 solar masses, much larger than previously believed. Additionally, we discuss the possibility of a search for femtolensing of white dwarfs in the LMC at optical wavelengths. Subject headings: dark matter — gamma rays: bursts — gravitational lensing — methods: numerical – 3...|$|R
40|$|We discuss weak lensing {{characteristics}} in the gravitational field of a compact {{object in the}} low-energy approximation of fourth order f(R) gravity theory. The particular solution {{is characterized by a}} gravitational strength parameter σ and a distance scale r_c much larger than the Schwarzschild radius. Above r_c gravity is strengthened and as a consequence weak lensing features are modified compared to the Schwarzschild case. We find a critical impact parameter (depending upon r_c) for which the behavior of the deflection angle changes. Using the Virbhadra-Ellis lens equation we improve the computation of the image positions, Einstein ring radii, magnification factors and the magnification ratio. We demonstrate that the <b>magnification</b> ratio as <b>function</b> of image separation obeys a power-law depending on the parameter σ, with a double degeneracy. No σ≠ 0 value gives the same power as the one characterizing Schwarzschild black holes. As the magnification ratio and the image separation are the lensing quantities most conveniently determined by direct measurements, future lensing surveys will be able to constrain the parameter σ based on this prediction. Comment: 14 pages, 8 figures, published version. Appendix added on the energy conditions for the modifications from GR, interpreted as an effective energy-momentum tenso...|$|R
40|$|We {{introduce}} {{a technique to}} measure gravitational lensing magnification using the variability of type I quasars. Quasars' variability amplitudes and luminosities are tightly correlated, on average. Magnification due to gravitational lensing increases the quasars' apparent luminosity, while leaving the variability amplitude unchanged. Therefore, the mean magnification of an ensemble of quasars can be measured through the mean shift in the variability-luminosity relation. As a proof of principle, we use this technique to measure the magnification of quasars spectroscopically identified in the Sloan Digital Sky Survey, due to gravitational lensing by galaxy clusters in the SDSS MaxBCG catalog. The Palomar-QUEST Variability Survey, reduced using the DeepSky pipeline, provides variability data for the sources. We measure the average quasar <b>magnification</b> as a <b>function</b> of scaled distance (r/R 200) from the nearest cluster; our measurements are consistent with expectations assuming NFW cluster profiles, particularly after accounting for the known uncertainty in the clusters' centers. Variability-based lensing measurements are a valuable complement to shape-based techniques because their systematic errors are very different, and also because the variability measurements are amenable to photometric errors of a few percent and to depths seen in current wide-field surveys. Given the data volume expected from current and upcoming surveys, this new technique {{has the potential to}} be competitive with weak lensing shear measurements of large scale structure. Comment: Accepted for publication in Ap...|$|R
40|$|We {{discuss the}} {{amplification}} dispersion in the observed luminosity of standard candles, like supernovae (SNe) of type Ia, induced by gravitational lensing in a Universe with dark energy (quintessence). We derive {{the main features}} of the <b>magnification</b> probability distribution <b>function</b> (pdf) of SNe {{in the framework of}} on average Friedmann-Lemaitre-Robertson-Walker (FLRW) models for both lensing by large-scale structures and compact objects. The magnification pdf is strongly dependent on the equation of state, w_Q, of the quintessence. The dispersion increases with the redshift of the source and is maximum for dark energy with very large negative pressure; the effects of gravitational lensing on the magnification pdf, i. e. the mode biased towards de-amplified values and the long tail towards large magnifications, are reduced for both microscopic DM and quintessence with an intermediate w_Q. Different equations of state of the dark energy can deeply change the dispersion in amplification for the projected observed samples of SNe Ia by future space-born missions. The "noise" in the Hubble diagram due to gravitational lensing strongly affects the determination of the cosmological parameters from SNe data. The errors on the pressureless matter density parameter, Ω_M, and on w_Q are maximum for quintessence with not very negative pressure. The effect of the gravitational lensing is of the same order of the other systematics affecting observations of SNe Ia. Due to the lensing by large-scale structures, in a flat Universe with Ω_M = 0. 4, at z= 1 a cosmological constant (w_Q=- 1) can be interpreted as dark energy with w_Q <- 0. 84 (at 2 -σ confidence limit) ...|$|R
40|$|Several {{types of}} {{algorithms}} are generally used to process digital imagery such as Landsat data. The {{most commonly used}} algorithms perform the task of registration, compression, and classification. Because there are different techniques available for performing registration, compression, and classification, imagery data users need a rationale for selecting a particular approach to meet their particular needs. This collection of registration, compression, and classification algorithms was developed so that different approaches could be evaluated and the best approach for a particular application determined. Routines are included for six registration algorithms, six compression algorithms, and two classification algorithms. The package also includes routines for evaluating the effects of processing on the image data. This collection of routines should be useful to anyone using or developing image processing software. Registration of image data involves the geometrical alteration of the imagery. Registration routines available in the evaluation package include image <b>magnification,</b> mapping <b>functions,</b> partitioning, map overlay, and data interpolation. The compression of image data involves reducing the volume of data needed for a given image. Compression routines available in the package include adaptive differential pulse code modulation, two-dimensional transforms, clustering, vector reduction, and picture segmentation. Classification of image data involves analyzing the uncompressed or compressed image data to produce inventories and maps of areas of similar spectral properties within a scene. The classification routines available include a sequential linear technique and a maximum likelihood technique. The choice of the appropriate evaluation criteria is quite important in evaluating the image processing functions. The user is therefore given a choice of evaluation criteria with which to investigate the available image processing functions. All of the available evaluation criteria basically compare the observed results with the expected results. For the image reconstruction processes of registration and compression, the expected results are usually the original data or some selected characteristics of the original data. For classification processes the expected result is the ground truth of the scene. Thus, the comparison process consists of determining what changes occur in processing, where the changes occur, how much change occurs, and the amplitude of the change. The package includes evaluation routines for performing such comparisons as average uncertainty, average information transfer, chi-square statistics, multidimensional histograms, and computation of contingency matrices. This collection of routines is written in FORTRAN IV for batch execution and has been implemented on an IBM 360 computer with a central memory requirement of approximately 662 K of 8 bit bytes. This collection of image processing and evaluation routines was developed in 1979...|$|R
40|$|MACS J 0717 is {{the most}} massive and {{extended}} of the Hubble Frontier Field clusters. It {{is one of the}} more difficult clusters to model, and we argue that this is in part due to the line of sight structure (LoS) at redshifts beyond 2. We show that the Grale mass reconstruction based on sources at 3 <z_s< 4. 1 has fewx 10 ^{ 13 }M_sun more mass than that based on nearby sources, z_s< 2. 6, and attribute the excess mass to a putative LoS, which is at least 75 " from the cluster center. Furthermore, the lens-model fitted z_s's of the recent Kawamata et al. reconstruction are biased systematically low compared to photometric z_s's, and the bias is a function of images' distance from the cluster center. We argue that these mimic the effect of LoS. We conclude that even in the presence of 100 - 200 images, lens-model adjusted source redshifts can conceal the presence of LoS, demonstrating the existence of degeneracies between z_s and (sub) structure. Also, a very good fit to image positions is not a sufficient condition for having a high fidelity mass map: Kawamata et al. obtain an rms of 0. 52 " for 173 images of 60 sources; our Grale reconstruction of the exact same data yields a somewhat different map, but similarly low rms, 0. 62 ". In contrast, a Grale model that uses reasonable, but fixed z_s gives a worse rms of 1. 28 " for 44 sources with 126 images. Unaccounted for LoS can bias the mass map, affecting the <b>magnification</b> and luminosity <b>function</b> estimates of high redshift sources. Comment: 13 pages, 8 figures, submitted to MNRA...|$|R
40|$|To {{elucidate}} {{the formation and}} early evolution of galaxies, {{it is necessary to}} search for and study galaxies in their infancy at redshifts approaching z~ 10. The ongoing Hubble Frontier Fields (HFF) program images six massive galaxy clusters that gravitationally magnify background galaxies to reach depths unrivalled by standard deep field imaging. In order to derive the intrinsic luminosities, sizes, and space densities of the lensed background galaxies, the magnification provided by the clusters must be quantified. Determining the <b>magnification</b> as a <b>function</b> of position requires constructing accurate cluster mass models. Constructing accurate mass models for the HFF clusters is particularly challenging because these clusters {{are in the midst of}} major mergers and therefore far from relaxed. In my thesis, I model the mass distribution of the first cluster completed in the HFF program, Abell 2744, using a free-form approach that makes minimal assumptions about the cluster-scale distribution of dark matter. Specifically, the mass distribution is modeled with a uniform grid on the cluster scale, and with NFW-parameterized components on scales of individual cluster galaxies. We find that the reconstructed mass distribution on the cluster scale not only smoothly traces the overall distribution of cluster galaxies, but also exhibits structures that coincide with bright peaks in the X-ray emitting intracluster gas. To assess the robustness of the lens model, I show that the centroids of multiply lensed images can be generally reproduced to within 1 " - a testament to the internal consistency of the model. I also show that the lens model generally reproduces internal structures seen in the lensed images with the correct distortion and orientations. Most importantly, I show that the predicted relative magnifications of multiple images agree very well with the observed relative fluxes to within ~ 0. 25 mag (25 %), the first time that such a test has been applied to any cluster lens model. The predicted absolute magnification at a single position in the cluster, however, is in slight discrepancy (1 σ) with the magnification inferred recently for a lensed Type Ia supernova discovered following the publication of our work (Lam et al. 2014). The minor inconsistency between the predicted and inferred magnification of the Type Ia supernova motivated a number of refinements that I then made to the modeling method. The first modification reduces the arbitrariness in the NFW-parameterized components by replacing it with the stellar light profile. The second modification improves the quality of the constraints by imposing stricter selection criteria for the lensed images. The refined lens model has a similar image plane dispersion, but predicts an absolute magnification that is in agreement within the uncertainties with the supernova-inferred value. These results demonstrate the significant progress I have made in reliably deriving the magnification of galaxy clusters, even though the cluster modeled is far from relaxed. To make further strides forward, it essential to model simulated cluster lenses to better assess the systematic errors, strengths and weaknesses of the method I have used, in the hope of identifying better approaches to remedy the weaknesses. published_or_final_versionPhysicsMasterMaster of Philosoph...|$|R

