72|156|Public
2500|$|... {{the most}} common choice for {{function}} h being either the absolute value (in which case it is known as <b>Markov</b> <b>inequality),</b> or the quadratic function (respectively Chebyshev's inequality).|$|E
5000|$|Now {{one can use}} the <b>Markov</b> <b>inequality</b> {{to bound}} the desired probability: ...|$|E
5000|$|... {{the most}} common choice for {{function}} h being either the absolute value (in which case it is known as <b>Markov</b> <b>inequality),</b> or the quadratic function (respectively Chebyshev's inequality).|$|E
2500|$|<b>Markov's</b> <b>inequality</b> {{states that}} for any real-valued random {{variable}} Y and any positive number a, we have Pr(|Y|>a) ≤ E(|Y|)/a. One {{way to prove}} Chebyshev's inequality is to apply <b>Markov's</b> <b>inequality</b> to the random variable [...] with a = (kσ)2.|$|R
2500|$|... {{using the}} <b>Markov's</b> <b>inequality</b> and the {{expectation}} derived previously.|$|R
5000|$|<b>Markov's</b> <b>inequality,</b> chain, partition, Markovian process - Andrey Markov ...|$|R
50|$|It is then {{helpful to}} rewrite the main {{difference}} as The first term can be bounded by the value at x of the maximal function for f &minus; g, denoted here by :The second term disappears in the limit since g is a continuous function, and the third term is bounded by |f(x) &minus; g(x)|. For the absolute value of the original difference to be greater than 2α in the limit, {{at least one of}} the first or third terms must be greater than α in absolute value. However, the estimate on the Hardy-Littlewood function says thatfor some constant An depending only upon the dimension n. The <b>Markov</b> <b>inequality</b> (also called Tchebyshev's inequality) says that whence Since ε was arbitrary, it can be taken to be arbitrarily small, and the theorem follows.|$|E
30|$|This follows {{immediately}} from (2.20) and the <b>Markov</b> <b>inequality.</b>|$|E
30|$|Inequalities play an {{important}} role in estimating the range of a variable. The <b>Markov</b> <b>inequality,</b> the Chebyshev inequality, and the Jensen inequality have been introduced to probability theory and uncertainty theory, and they have found many applications. In this section, we will consider the <b>Markov</b> <b>inequality,</b> the Chebyshev inequality, and the Jensen inequality in the framework of uncertain set theory.|$|E
5000|$|Then for , <b>Markov's</b> <b>inequality</b> and the {{independence}} of [...] implies: ...|$|R
30|$|In {{the next}} lemma we prove <b>Markov’s</b> <b>inequality</b> from the axioms.|$|R
30|$|Proof.[*]By using <b>Markov’s</b> <b>inequality</b> and Theorem 2, {{the proof}} is straightforward.|$|R
40|$|We {{prove that}} a compact {{subset of the}} complex plane {{satisfies}} a local <b>Markov</b> <b>inequality</b> {{if and only if}} it satisfies a Kolmogorov type inequality. This result generalizes a theorem established by Bos and Milman in the real case. We also show that every set satisfying the local <b>Markov</b> <b>inequality</b> is a sum of Cantor type sets which are regular {{in the sense of the}} potential theory...|$|E
3000|$|In this setting, {{this choice}} {{can be applied}} to Tests 3 and 5, since the <b>Markov</b> <b>inequality</b> implies that (5) or (9) are {{satisfied}} with [...]...|$|E
40|$|AbstractFor {{a compact}} set K⊆Rd {{we present a}} rather easy {{construction}} of a linear extension operator E:E(K) →C∞(Rd) for the space of Whitney jets E(K) which satisfies linear tame continuity estimates sup{|∂αE(f) (x) |:|α|⩽m,x∈Rd}⩽Cm,ε‖f‖(r+ε) m, where ‖⋅‖s denotes the s-th Whitney norm. The construction {{turns out to be}} possible if and only if the local <b>Markov</b> <b>inequality</b> LMI(s) introduced by Bos and Milman holds for every s>r on K. In particular, E(K) admits a tame linear extension operator if and only if the local <b>Markov</b> <b>inequality</b> LMI(s) holds on K for some s⩾ 1...|$|E
50|$|This {{identity}} {{is used in}} a simple proof of <b>Markov's</b> <b>inequality.</b>|$|R
5000|$|Chebyshev's <b>inequality</b> {{follows from}} <b>Markov's</b> <b>inequality</b> by {{considering}} the random variable ...|$|R
2500|$|For a nonnegative random {{variable}} [...] and , the <b>Markov's</b> <b>inequality</b> states that ...|$|R
30|$|The {{following}} so-called <b>Markov</b> <b>inequality</b> is {{an important}} tool to prove inverse theorems in approximation theory. See, for example, Duffin and Schaeffer [1], Devore and Lorentz [2], and Borwein and Erdelyi [3].|$|E
40|$|Itiswellknownthatcomputingrelativeapproximationsofweightedcountingqueries {{such as the}} {{probability}} of evidence in a Bayesian network, the partition function of a Markov network, {{and the number of}} solutions of a constraint satisfaction problem is NP-hard. In this paper, we settle therefore on an easier problem of computing highconfidence lower bounds and propose an algorithm based on importance sampling and <b>Markov</b> <b>inequality</b> for it. However, a straight-forward application of <b>Markov</b> <b>inequality</b> often yields poor lower bounds because it uses only one sample. We therefore propose several new schemes that extend it to multiple samples. Empirically, we show that our new schemes are quite powerful, often yielding substantially higher (better) lower bounds than state-of-the-art schemes. ...|$|E
40|$|It is {{well known}} that the problem of {{computing}} relative approximations of weighted counting queries such as the probability of evidence in a Bayesian network, the partition function of a Markov network, and the number of solutions of a constraint satisfaction problem is NP-hard. In this paper, we settle therefore on an easier problem of computing high-confidence lower bounds. We propose to use importance sampling and <b>Markov</b> <b>inequality</b> for solving it. However, a straight-forward application of the <b>Markov</b> <b>inequality</b> often yields poor lower bounds. We therefore propose several new schemes for improving its performance in practice. Empirically, we show that our new schemes are quite powerful, often yielding substantially higher (better) lower bounds than all state-of-the-art schemes. ...|$|E
5000|$|<b>Markov's</b> <b>inequality</b> {{requires}} only {{the following information}} on a random variable X: ...|$|R
5000|$|... #Caption: The {{region of}} {{interest}} for which <b>Markov's</b> <b>inequality</b> gives a lower bound.|$|R
5000|$|For a non-negative {{continuous}} {{random variable}} having an expectation, <b>Markov's</b> <b>inequality</b> states that ...|$|R
40|$|We {{consider}} testing {{about the}} slope parameter β when Y - X β {{is assumed to}} be an exchangeable process conditionally on X. This framework encompasses the semi-parametric linear regression model. We show that the usual Fisher's procedure have non trivial exact rejection bound under the null hypothesis R β = ϒ. This bound derives from the <b>Markov</b> <b>inequality</b> and a close inspection of multivariate moments of self-normalized, self-centered, exchangeable processes. Improvement by higher order versions of the <b>Markov</b> <b>inequality</b> are also presented. The bounds do not require the existence of any moment, so they remain valid even if TCL do not apply. We generalize the framework to multivariate and order- 1 auto-regressive models with exogenous variables...|$|E
30|$|In this paper, we proved some inequalities in the {{framework}} of uncertain set theory including the <b>Markov</b> <b>inequality,</b> the Chebyshev inequality, the Jensen inequality, the Hölder inequality, and the Minkowski inequality. These inequalities were applied to the area of representing incomplete uncertain knowledge.|$|E
30|$|On {{the basis}} of the {{inequality}} presented in Theorem  6, the well-known <b>Markov</b> <b>inequality</b> and the Chebyshev inequality in probability theory are proved in the context of fuzzy rough theory as follows, which can be seen as special cases of Theorem  6.|$|E
5000|$|The proofs {{are based}} on an {{application}} of <b>Markov's</b> <b>inequality</b> to the random variable ...|$|R
25|$|The term Chebyshev's {{inequality}} {{may also}} refer to <b>Markov's</b> <b>inequality,</b> {{especially in the}} context of analysis.|$|R
2500|$|The proof {{follows from}} a simple {{application}} of <b>Markov's</b> <b>inequality</b> (applied to second moment of [...]|$|R
30|$|It is {{well known}} that there have been some {{improvements}} of Markov-type inequality when the coefficients of polynomial are restricted; see, for example, [3 – 7]. In [5], Borwein and Erdélyi restricted the coefficients of polynomials and improved the <b>Markov</b> <b>inequality</b> as in following form.|$|E
40|$|Computing the {{probability}} of evidence even with known error bounds is NP-hard. In this paper we address this hard problem by settling on an easier problem. We propose an approximation which provides high confidence lower bounds on probability of evidence but {{does not have any}} guarantees in terms of relative or absolute error. Our proposed approximation is a randomized importance sampling scheme that uses the <b>Markov</b> <b>inequality.</b> However, a straight-forward application of the <b>Markov</b> <b>inequality</b> may lead to poor lower bounds. We therefore propose several heuristic measures to improve its performance in practice. Empirical evaluation of our scheme with state-of- the-art lower bounding schemes reveals the promise of our approach. Comment: Appears in Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence (UAI 2007...|$|E
30|$|Some inequalities, {{including}} the <b>Markov</b> <b>inequality,</b> the Chebyshev inequality, the Hölder inequality, the Minkowski inequality, and the Jensen inequality, analogous {{to those in}} probability theory, have been proved to hold both for fuzzy variables and rough variables by Liu [18]. In this section, these inequalities are proved for fuzzy rough variables.|$|E
50|$|The term Chebyshev's {{inequality}} {{may also}} refer to <b>Markov's</b> <b>inequality,</b> {{especially in the}} context of analysis.|$|R
5000|$|Chebyshev's {{inequality}} {{can be seen}} as {{a special}} case of the generalized <b>Markov's</b> <b>inequality</b> when [...]|$|R
5000|$|The proof {{follows from}} a simple {{application}} of <b>Markov's</b> <b>inequality</b> (applied to second moment of [...]|$|R
