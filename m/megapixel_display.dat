13|7|Public
500|$|While chairman, Jobs visited {{university}} {{departments and}} faculty members to sell Macintosh. Jobs met Paul Berg, a Nobel Laureate in chemistry, at a luncheon held in Silicon Valley to honor François Mitterrand, then President of France. Berg was frustrated by the expense of teaching students about recombinant DNA from textbooks instead of in wet laboratories, used for the testing and analysis of chemicals, drugs, and other materials or biological matter. Wet labs were prohibitively expensive for lower-level courses and were too complex to be simulated on personal computers of the time. Berg suggested to Jobs to use his influence at Apple to create a [...] "3M computer" [...] workstation for higher education, featuring more than one megabyte of random-access memory (RAM), a <b>megapixel</b> <b>display</b> and megaFLOP performance, hence the name [...] "3M".|$|E
5000|$|... #Caption: The NeXT <b>MegaPixel</b> <b>Display,</b> {{sitting on}} top of a NeXTstation with the {{original}} keyboard & mouse ...|$|E
50|$|The NeXT <b>MegaPixel</b> <b>Display</b> was a {{range of}} CRT-based {{computer}} monitors manufactured and sold by NeXT for the NeXTcube and NeXTstation workstations, designed by Hartmut Esslinger/Frog Design Inc.|$|E
5000|$|When the NeXTstation Color and the NeXTdimension {{board was}} released, NeXT sold rebranded color {{monitors}} as <b>MegaPixel</b> Color <b>Display</b> in either 17" [...] or 21" [...] and all connections where {{provided by a}} separate box (the NeXT Soundbox) ...|$|R
5|$|The NeXT Computer {{was based}} on the new 25MHz Motorola 68030 central {{processing}} unit (CPU). The Motorola 88000 RISC chip was originally considered, but was not available in sufficient quantities. It included between 8 and 64 MB of random-access memory (RAM), a 256 MB magneto-optical (MO) drive, a 40 MB (swap-only), 330 MB, or 660 MB hard disk drive, 10BASE2 Ethernet, NuBus and a 17-inch <b>MegaPixel</b> grayscale <b>display</b> measuring 1120 by 832 pixels. In 1989 a typical new PC, Macintosh, or Amiga computer included a few megabytes of RAM, a 640×480 16-color or 320x240 4000-color display, a 10 to 20 megabyte hard drive and few networking capabilities.|$|R
40|$|Tabletop {{displays}} {{are mostly}} used for casual applications {{that do not}} require intricate graphics or precise manipulation. Browsing photographs and maps are common applications. A higher resolution is required to support work involving detailed graphics and text. A display the size of a desk, with the resolution of a typical LCD monitor, will have around 14 <b>megapixels.</b> Tabletop <b>displays</b> are usually constructed from projectors, {{and the only way to}} achieve this size and resolution is to combine multiple projectors. We present techniques from multiprojector display walls and adapt them for tabletops. These high-resolution displays also require high-resolution input, and although touch is simple and natural, better accuracy can generally be achieved using a pen. We also review technologies for pen input on tabletops...|$|R
5000|$|The {{original}} <b>MegaPixel</b> <b>Display</b> was a monochrome 17" [...] monitor displaying 4 brightness levels (black, dark gray, light {{gray and}} white) in a fixed resolution of 1120 x 832 at 92 DPI (just shy {{of a true}} Megapixel at 931,840 total pixels) at 68 Hz.|$|E
50|$|But {{typically}} only {{a limited}} set of brightness levels was provided to save display memory which was very expensive in the '70s and '80s. Either normal/bright or normal/dim (1 bit) per character as in the VT100 or black/white per pixel in the Macintosh 128K or black, dark gray, light gray, white (2bit) per pixel like the NeXT <b>MegaPixel</b> <b>Display.</b>|$|E
5000|$|... 3M was a goal first {{proposed}} in the early 1980s by Raj Reddy {{and his colleagues at}} Carnegie Mellon University (CMU) as a minimum specification for academic/technical workstations: at least a megabyte of memory, a <b>megapixel</b> <b>display</b> and a million instructions per second (MIPS) processing power. It was also often said that it should cost no more than a [...] "megapenny" [...] ($10,000).|$|E
50|$|Stallion is a 328 <b>megapixel</b> tiled <b>display</b> system, {{with over}} 150 times the {{resolution}} of a standard HD display, it is among the highest pixel count displays in the world. The cluster provides users {{with the ability to}} display high-resolution visualizations on a large 16x5 tiled display of 30-inch Dell monitors. This configuration allows for an exploration of visualizations at an extremely high level of detail and quality compared to a typical moderate pixel count projector. The cluster allows users access to over 82GB of graphics memory, and 240 processing cores. This configuration enables the processing of datasets of a massive scale, and the interactive visualization of substantial geometries. A 36TB shared file system is available to enable the storage of tera-scale size datasets.|$|R
50|$|The NeXT Computer {{was based}} on the new 25 MHz Motorola 68030 central {{processing}} unit (CPU). The Motorola 88000 RISC chip was originally considered, but was not available in sufficient quantities. It included between 8 and 64 MB of random-access memory (RAM), a 256 MB magneto-optical (MO) drive, a 40 MB (swap-only), 330 MB, or 660 MB hard disk drive, 10BASE2 Ethernet, NuBus and a 17-inch <b>MegaPixel</b> grayscale <b>display</b> measuring 1120 by 832 pixels. In 1989 a typical new PC, Macintosh, or Amiga computer included a few megabytes of RAM, a 640×480 16-color or 320x240 4000-color display, a 10 to 20 megabyte hard drive and few networking capabilities.It also was the first computer to ship with a general-purpose DSP chip (Motorola 56001) on the motherboard. This was used to support sophisticated music and sound processing, including the Music Kit software.|$|R
40|$|Abstract — Larger, higher {{resolution}} displays {{can be used}} to increase the scalability of information visualizations. But just how much can scalability increase using larger displays before hitting human perceptual or cognitive limits? Are the same visualization techniques that are good on a single monitor also the techniques that are best when they are scaled up using large, high-resolution displays? To answer these questions we performed a controlled experiment on user performance time, accuracy, and subjective workload when scaling up data quantity with different space-time-attribute visualizations using a large, tiled display. Twelve college students used small multiples, embedded bar matrices, and embedded time-series graphs either on a 2 <b>megapixel</b> (Mp) <b>display</b> or with data scaled up using a 32 Mp tiled display. Participants performed various overview and detail tasks on geospatially-referenced multidimensional time-series data. Results showed that current designs are perceptually scalable because they result in a decrease in task completion time when normalized per number of data attributes along with no decrease in accuracy. It appears that, for the visualizations selected for this study, the relative comparison between designs is generally consistent between display sizes. However, results also suggest that encoding is more important on a smaller display while spatial grouping is more important on a larger display. Some suggestions for designers are provided based on our experience designing visualizations for large displays. Index Terms—Information visualization, large displays, empirical evaluation. ...|$|R
5000|$|In {{the early}} 1980s, a {{high-end}} workstation {{had to meet}} the three Ms. The so-called [...] "3M computer" [...] had a Megabyte of memory, a <b>Megapixel</b> <b>display</b> (roughly 1000×1000), and a [...] "MegaFLOPS" [...] compute performance (at least one million floating point operations per second). As limited as this seems today, {{it was at least}} an order of magnitude beyond the capacity of the personal computer of the time; the original 1981 IBM Personal Computer had 16 KB memory, a text-only display, and floating-point performance around 1 kiloFLOPS (30 kiloFLOPS with the optional 8087 math coprocessor). Other desirable features not found in desktop computers at that time included networking, graphics acceleration, and high-speed internal and peripheral data buses.|$|E
5000|$|While chairman, Jobs visited {{university}} {{departments and}} faculty members to sell Macintosh. Jobs met Paul Berg, a Nobel Laureate in chemistry, at a luncheon held in Silicon Valley to honor François Mitterrand, then President of France. Berg was frustrated by the expense of teaching students about recombinant DNA from textbooks instead of in wet laboratories, used for the testing and analysis of chemicals, drugs, and other materials or biological matter. Wet labs were prohibitively expensive for lower-level courses and were too complex to be simulated on personal computers of the time. Berg suggested to Jobs to use his influence at Apple to create a [...] "3M computer" [...] workstation for higher education, featuring more than one megabyte of random-access memory (RAM), a <b>megapixel</b> <b>display</b> and megaFLOP performance, hence the name [...] "3M".|$|E
50|$|Although MIT {{already had}} many computers, {{they were mostly}} used for {{research}} and administration, not by students. The project intended to extend computer power into fields of study outside computer science and engineering, such as foreign languages, economics, and political science. To implement these goals, MIT decided to build a Unix-based distributed computing system. Unlike those at Carnegie Mellon University, which also received the IBM and DEC grants, students {{did not have to}} own their own computer; MIT built computer labs for their users, although the goal was to put networked computers into each dormitory. Students were required to learn FORTRAN and Lisp, and would have access to sophisticated graphical workstations, capable of 1 million instructions per second and with 1 megabyte of RAM and a 1 <b>megapixel</b> <b>display.</b> Upon logging into a workstation, they would have immediate access to a universal set of files and programs via central services. Because the workstation used a thin client model, the user interface would be consistent despite the use of different hardware vendors for different workstations. A small staff could maintain hundreds of clients.|$|E
40|$|We {{present the}} Scalable Adaptive Graphics Environment (SAGE), a {{graphics}} streaming architecture for supporting collaborative scientific visualization environments with potentially hundreds of <b>megapixels</b> of contiguous <b>display</b> resolution. In collaborative scientific visualization {{it is crucial}} to share high resolution visualizations as well as high definition video among groups of collaborators at local or remote sites. Our network-centered architecture allows collaborators to simultaneously run multiple visualization applications on local or remote clusters and share the visualizations by streaming the pixels of each application over ultra high speed networks to large tiled displays. This streaming architecture is designed such that the output of arbitrary M by N pixel rendering cluster nodes can be streamed to X by Y pixel display screens allowing for userdefinable layouts on the display. This dynamic pixel routing capability of our architecture allows users to freely move and resize each application’s imagery over the tiled displays in runtime, tightly synchronizing the multiple visualization streams to form a single stream. Experimental results show that our architecture can support visualization at multi-ten-megapixel resolution with reasonable frame rates using gigabit networks...|$|R
40|$|Having {{to carry}} input devices can be {{inconvenient}} when interacting with wall-sized, high-resolution tiled displays. Such displays are typically {{driven by a}} cluster of computers. Running existing games on a cluster is non-trivial, and the performance attained using software solutions like Chromium is not good enough. This paper presents a touch-free, multi-user, humancomputer interface for wall-sized displays that enables completely device-free interaction. The interface is built using 16 cameras and a cluster of computers, and is integrated with the games Quake 3 Arena (Q 3 A) and Homeworld. The two games were parallelized using two different approaches in order to run on a 7 x 4 tile, 21 <b>megapixel</b> <b>display</b> wall with good performance. The touch-free interface enables interaction with a latency of 116 ms, where 81 ms are due to the camera hardware. The rendering performance of the games is compared to their sequential counterparts running on the display wall using Chromium. Parallel Q 3 A’s framerate is {{an order of magnitude}} higher compare...|$|E
40|$|As {{increasingly}} large displays {{are integrated}} into personal workspaces, mouse-based interaction becomes more problematic. Users must repeatedly “clutch ” the mouse for long distance movements [61]. The visibility of the cursor is also problematic in large screens, since {{the percentage of}} the screen space that the cursor takes from the whole display gets smaller. We test multi-scale approaches to mouse interaction that utilize dynamic speed and size techniques to grow the cursor larger and faster for long movements. Using Fitts ’ Law methods, we experimentally compare different implementations to optimize the mouse design for large displays and to test how they scale to large displays. We also compare them to techniques that integrate absolute pointing with head tracking. Results indicate that with some implementation level modifications the mouse device can scale well up to even a 100 <b>megapixel</b> <b>display</b> with lower mean movement times as compared to integrating absolute pointing techniques to mouse input while maintaining fast performance of the typical mouse configuration on small screens for short distance movements. Designs that have multiple acceleratio...|$|E
40|$|The vast {{volume of}} {{scientific}} data produced today requires tools that can enable scientists to explore {{large amounts of}} data to extract meaningful information. One such tool is interactive visualization. The amount of data that can be simultaneously visualized on a computer display {{is proportional to the}} display’s resolution. While computer systems in general have seen a remarkable increase in performance the last decades, display resolution has not evolved at the same rate. Increased resolution can be provided by tiling several displays in a grid. A system comprised of multiple displays tiled in such a grid is referred to as a display wall. Display walls provide orders of magnitude more resolution than typical desktop displays, and can provide insight into problems not possible to visualize on desktop displays. However, their distributed and parallel architecture creates several challenges for designing systems that can support interactive visualization. One challenge is compatibility issues with existing software designed for personal desktop computers. Another set of challenges include identifying characteristics of visualization systems that can: (i) Maintain synchronous state and display-output when executed over multiple display nodes; (ii) scale to multiple display nodes without being limited by shared interconnect bottlenecks; (iii) utilize additional computational resources such as desktop computers, clusters and supercomputers for workload distribution; and (iv) use data from local and remote compute- and data-resources with interactive performance. This dissertation presents Network Accessible Compute (NAC) resources and Network Accessible Display (NAD) resources for interactive visualization of data on displays ranging from laptops to high-resolution tiled display walls. A NAD is a display having functionality that enables usage over a network connection. A NAC is a computational resource that can produce content for network accessible displays. A system consisting of NACs and NADs is either push-based (NACs provide NADs with content) or pull-based (NADs request content from NACs). To attack the compatibility challenge, a push-based system was developed. The system enables several simultaneous users to mirror multiple regions from the desktop of their computers (NACs) onto nearby NADs (among others a 22 <b>megapixel</b> <b>display</b> wall) without requiring usage of separate DVI/VGA cables, permanent installation of third party software or opening firewall ports. The system has lower performance than that of a DVI/VGA cable approach, but increases flexibility such as the possibility to share network accessible displays from multiple computers. At a resolution of 800 by 600 pixels, the system can mirror dynamic content between a NAC and a NAD at 38. 6 frames per second (FPS). At 1600 x 1200 pixels, the refresh rate is 12. 85 FPS. The bottleneck of the system is frame buffer capturing and encoding/decoding of pixels. These two functional parts are executed in sequence, limiting the usage of additional CPU cores. By pipelining and executing these parts on separate CPU cores, higher frame rates can be expected and by a factor of two in the best case. To attack all presented challenges, a pull-based system, WallScope, was developed. WallScope enables interactive visualization of local and remote data sets on high-resolution tiled display walls. The WallScope architecture comprises a compute-side and a display-side. The compute-side comprises a set of static and dynamic NACs. Static NACs are considered permanent to the system once added. This type of NAC typically has strict underlying security and access policies. Examples of such NACs are clusters, grids and supercomputers. Dynamic NACs are compute resources that can register on-the-fly to become compute nodes in the system. Examples of this type of NAC are laptops and desktop computers. The display-side comprises of a set of NADs and a data set containing data customized for the particular application domain of the NADs. NADs are based on a sort-first rendering approach where a visualization client is executed on each display-node. The state of these visualization clients is provided by a separate state server, enabling central control of load and refresh-rate. Based on the state received from the state server, the visualization clients request content from the data set. The data set is live in that it translates these requests into compute messages and forwards them to available NACs. Results of the computations are returned to the NADs for the final rendering. The live data set is close to the NADs, both in terms of bandwidth and latency, to enable interactive visualization. WallScope can visualize the Earth, gigapixel images, and other data available through the live data set. When visualizing the Earth on a 28 -node display wall by combining the Blue Marble data set with the Landsat data set using a set of static NACs, the bottleneck of WallScope is the computation involved in combining the data sets. However, the time used to combine data sets on the NACs decreases by a factor of 23 when going from 1 to 26 compute nodes. The display-side can decode 414. 2 megapixels of images per second (19 frames per second) when visualizing the Earth. The decoding process is multi-threaded and higher frame rates are expected using multi-core CPUs. WallScope can rasterize a 350 -page PDF document into 550 megapixels of image-tiles and display these image-tiles on a 28 -node display wall in 74. 66 seconds (PNG) and 20. 66 seconds (JPG) using a single quad-core desktop computer as a dynamic NAC. This time is reduced to 4. 20 seconds (PNG) and 2. 40 seconds (JPG) using 28 quad-core NACs. This shows that the application output from personal desktop computers can be decoupled from the resolution of the local desktop and display for usage on high-resolution tiled display walls. It also shows that the performance can be increased by adding computational resources giving a resulting speedup of 17. 77 (PNG) and 8. 59 (JPG) using 28 compute nodes. Three principles are formulated based on the concepts and systems researched and developed: (i) Establishing the end-to-end principle through customization, is a principle stating that the setup and interaction between a display-side and a compute-side in a visualization context can be performed by customizing one or both sides; (ii) Personal Computer (PC) – Personal Compute Resource (PCR) duality states that a user’s computer is both a PC and a PCR, implying that desktop applications can be utilized locally using attached interaction devices and display(s), or remotely by other visualization systems for domain specific production of data based on a user’s personal desktop install; and (iii) domain specific best-effort synchronization stating that for distributed visualization systems running on tiled display walls, state handling can be performed using a best-effort synchronization approach, where visualization clients eventually will get the correct state after a given period of time. Compared to state-of-the-art systems presented in the literature, the contributions of this dissertation enable utilization of a broader range of compute resources from a display wall, {{while at the same time}} providing better control over where to provide functionality and where to distribute workload between compute-nodes and display-nodes in a visualization context...|$|E
40|$|Papers number 2 - 7 and {{appendix}} B and C of {{this thesis}} {{are not available}} in Munin: 2. Hagen, T-M. S., Johnsen, E. S., Stødle, D., Bjorndalen, J. M. and Anshus, O. : 'Liberating the Desktop', First International Conference on Advances in Computer-Human Interaction (2008), pp 89 - 94. Available at [URL] 3. Tor-Magne Stien Hagen, Oleg Jakobsen, Phuong Hoai Ha, and Otto J. Anshus: 'Comparing the Performance of Multiple Single-Cores versus a Single Multi-Core' (manuscript) 4. Tor-Magne Stien Hagen, Phuong Hoai Ha, and Otto J. Anshus: 'Experimental Fault-Tolerant Synchronization for Reliable Computation on Graphics Processors' (manuscript) 5. Tor-Magne Stien Hagen, Daniel Stødle and Otto J. Anshus: 'On-Demand High-Performance Visualization of Spatial Data on High-Resolution Tiled Display Walls', Proceedings of the International Conference on Imaging Theory and Applications and International Conference on Information Visualization Theory and Applications (2010), pages 112 - 119. Available at [URL] 6. Bård Fjukstad, Tor-Magne Stien Hagen, Daniel Stødle, Phuong Hoai Ha, John Markus Bjørndalen and Otto Anshus: 'Interactive Weather Simulation and Visualization on a Display Wall with Many-Core Compute Nodes', Para 2010 – State of the Art in Scientific and Parallel Computing. Available at [URL] 7. Tor-Magne Stien Hagen, Daniel Stødle, John Markus Bjørndalen, and Otto Anshus: 'A Step towards Making Local and Remote Desktop Applications Interoperable with High-Resolution Tiled Display Walls', Lecture Notes in Computer Science (2011), Volume 6723 / 2011, 194 - 207. Available at [URL] The vast volume of scientific data produced today requires tools that can enable scientists to explore large amounts of data to extract meaningful information. One such tool is interactive visualization. The amount of data that can be simultaneously visualized on a computer display {{is proportional to the}} display’s resolution. While computer systems in general have seen a remarkable increase in performance the last decades, display resolution has not evolved at the same rate. Increased resolution can be provided by tiling several displays in a grid. A system comprised of multiple displays tiled in such a grid is referred to as a display wall. Display walls provide orders of magnitude more resolution than typical desktop displays, and can provide insight into problems not possible to visualize on desktop displays. However, their distributed and parallel architecture creates several challenges for designing systems that can support interactive visualization. One challenge is compatibility issues with existing software designed for personal desktop computers. Another set of challenges include identifying characteristics of visualization systems that can: (i) Maintain synchronous state and display-output when executed over multiple display nodes; (ii) scale to multiple display nodes without being limited by shared interconnect bottlenecks; (iii) utilize additional computational resources such as desktop computers, clusters and supercomputers for workload distribution; and (iv) use data from local and remote compute- and data-resources with interactive performance. This dissertation presents Network Accessible Compute (NAC) resources and Network Accessible Display (NAD) resources for interactive visualization of data on displays ranging from laptops to high-resolution tiled display walls. A NAD is a display having functionality that enables usage over a network connection. A NAC is a computational resource that can produce content for network accessible displays. A system consisting of NACs and NADs is either push-based (NACs provide NADs with content) or pull-based (NADs request content from NACs). To attack the compatibility challenge, a push-based system was developed. The system enables several simultaneous users to mirror multiple regions from the desktop of their computers (NACs) onto nearby NADs (among others a 22 <b>megapixel</b> <b>display</b> wall) without requiring usage of separate DVI/VGA cables, permanent installation of third party software or opening firewall ports. The system has lower performance than that of a DVI/VGA cable approach, but increases flexibility such as the possibility to share network accessible displays from multiple computers. At a resolution of 800 by 600 pixels, the system can mirror dynamic content between a NAC and a NAD at 38. 6 frames per second (FPS). At 1600 x 1200 pixels, the refresh rate is 12. 85 FPS. The bottleneck of the system is frame buffer capturing and encoding/decoding of pixels. These two functional parts are executed in sequence, limiting the usage of additional CPU cores. By pipelining and executing these parts on separate CPU cores, higher frame rates can be expected and by a factor of two in the best case. To attack all presented challenges, a pull-based system, WallScope, was developed. WallScope enables interactive visualization of local and remote data sets on high-resolution tiled display walls. The WallScope architecture comprises a compute-side and a display-side. The compute-side comprises a set of static and dynamic NACs. Static NACs are considered permanent to the system once added. This type of NAC typically has strict underlying security and access policies. Examples of such NACs are clusters, grids and supercomputers. Dynamic NACs are compute resources that can register on-the-fly to become compute nodes in the system. Examples of this type of NAC are laptops and desktop computers. The display-side comprises of a set of NADs and a data set containing data customized for the particular application domain of the NADs. NADs are based on a sort-first rendering approach where a visualization client is executed on each display-node. The state of these visualization clients is provided by a separate state server, enabling central control of load and refresh-rate. Based on the state received from the state server, the visualization clients request content from the data set. The data set is live in that it translates these requests into compute messages and forwards them to available NACs. Results of the computations are returned to the NADs for the final rendering. The live data set is close to the NADs, both in terms of bandwidth and latency, to enable interactive visualization. WallScope can visualize the Earth, gigapixel images, and other data available through the live data set. When visualizing the Earth on a 28 -node display wall by combining the Blue Marble data set with the Landsat data set using a set of static NACs, the bottleneck of WallScope is the computation involved in combining the data sets. However, the time used to combine data sets on the NACs decreases by a factor of 23 when going from 1 to 26 compute nodes. The display-side can decode 414. 2 megapixels of images per second (19 frames per second) when visualizing the Earth. The decoding process is multi-threaded and higher frame rates are expected using multi-core CPUs. WallScope can rasterize a 350 -page PDF document into 550 megapixels of image-tiles and display these image-tiles on a 28 -node display wall in 74. 66 seconds (PNG) and 20. 66 seconds (JPG) using a single quad-core desktop computer as a dynamic NAC. This time is reduced to 4. 20 seconds (PNG) and 2. 40 seconds (JPG) using 28 quad-core NACs. This shows that the application output from personal desktop computers can be decoupled from the resolution of the local desktop and display for usage on high-resolution tiled display walls. It also shows that the performance can be increased by adding computational resources giving a resulting speedup of 17. 77 (PNG) and 8. 59 (JPG) using 28 compute nodes. Three principles are formulated based on the concepts and systems researched and developed: (i) Establishing the end-to-end principle through customization, is a principle stating that the setup and interaction between a display-side and a compute-side in a visualization context can be performed by customizing one or both sides; (ii) Personal Computer (PC) – Personal Compute Resource (PCR) duality states that a user’s computer is both a PC and a PCR, implying that desktop applications can be utilized locally using attached interaction devices and display(s), or remotely by other visualization systems for domain specific production of data based on a user’s personal desktop install; and (iii) domain specific best-effort synchronization stating that for distributed visualization systems running on tiled display walls, state handling can be performed using a best-effort synchronization approach, where visualization clients eventually will get the correct state after a given period of time. Compared to state-of-the-art systems presented in the literature, the contributions of this dissertation enable utilization of a broader range of compute resources from a display wall, {{while at the same time}} providing better control over where to provide functionality and where to distribute workload between compute-nodes and display-nodes in a visualization context...|$|E

