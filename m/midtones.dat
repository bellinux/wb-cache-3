28|15|Public
25|$|Still, Ōkyo's works remain Japanese. Unlike European painting, Ōkyo's {{images have}} very few <b>midtones.</b> Moreover, he follows the Eastern {{tradition}} in depicting objects with very little setting; often his pictures feature a single subject on a plain background. The result is a more immediate naturalism with a decorative and reflective feel. This was achieved through skillful brush handling; Ōkyo painted with a broad, flat brush, which he would load with more paint on one side. This created broad strokes that vary in paint coverage. Nature was not his only subject; many works by Ōkyo depict normal scenes from life in Kyoto's commercial area.|$|E
500|$|Jean-Clement Sorret was {{the digital}} colorist for the film, this {{being one of}} the few {{instances}} in which a colorist received a spot in the opening credits. The film contains many coloring effects and [...] "density shifts" [...] of lighting to reflect the moods of the characters. For example, when Hayley gets angry, the colors would be edited to be of lower frequency. One effect used which, as far as the director is aware, had not been done in cinema before, was to brighten the lighting in filming and correct everything down in post-production. This allowed for facial details to be visible even while having a darkened atmosphere. According to the DVD extras, the process required a custom-built digital intermediate to be made and proved to be extremely difficult, with corrections having to be made frame-by-frame in some instances. This technique, known as ETTR, is a standard procedure in digital photography and cinematography to minimize the amount of noise in shadows and <b>midtones.</b> Early working titles of the script were Vendetta and Snip Snip. The producer wanted a title with a [...] "sugar and spice combination and a mixture of harsh roughness, innocence, and vulnerability", and settled on the title Hard Candy.|$|E
5000|$|... 2013 April / Mai Kunstclub Hamburg [...] "blanc et noir", group {{exhibition}} (The {{best work}} of 18 artists {{with a lot}} contrast and <b>midtones)</b> ...|$|E
30|$|A {{picture is}} a {{halftone}} image; {{on the other}} hand, a photo is a continuous-tone image. We observe that smooth regions near <b>midtone</b> are most affected by the halftone noise. Therefore, we use these regions to distinguish between a picture and a photo.|$|R
40|$|Printing quality can {{be defined}} as {{conformance}} to specifications. ISO 12647 - 2 (2004), the standard for offset lithographic printing, specifies solid, TVI and mid-tone spread as conformance metrics. The revision of ISO 12647 - 2 (2010) embraces ΔCh as a new metric for grey reproduction assessment. In order to complete the revision, both the test method and the tolerance must be specified. In his presentation at TC 130 in April 2011, Chung proposed a 2 ̆ 2 three triplets 2 ̆ 7 method as input to the grey reproduction assessment. He also proposed the use of G 7 pass/fail criteria as the criteria to determine the tolerance in terms of ΔCh. This research extends Chung 2 ̆ 7 s work by using TVI and <b>midtone</b> spread tolerances as the criteria to determine the grey reproduction tolerance in terms of ΔCh. Because these tolerances are based on TVI and <b>midtone</b> spread, they should be better in line with the foundation of ISO 12647. Two hundred fifty six simulations of TVI drift from four different calibrated press conditions were used to develop proposed ΔCh tolerances. Berns 2 ̆ 7 methodology was used to minimize disagreement between ΔCh tolerances and the existing <b>midtone</b> spread tolerance of 5. The resulting ΔCh tolerances exhibited nearly 90...|$|R
5000|$|Brown, {{chocolate}} and liver {{are the most}} common terms used to refer to the bb-dilution of black pigment to a dark brown. Depending on breed and exact shade, terms such as mahogany, <b>midtone</b> brown, grey-brown, blackish brown are used. Sedge and deadgrass are used to describe the desired Chesapeake Bay Retriever color that resembles [...] "that of its working surroundings" [...] as closely as possible.|$|R
50|$|The oil {{on canvas}} {{measures}} 140 x 200 centimeters (decorated in rich colors and using <b>midtones</b> and subdued colors) It is {{in the collection of}} the National Museum in Warsaw.|$|E
50|$|Contrast masking can be {{considered}} to be the opposite of gamma correction, which adjusts the <b>midtones</b> of an image. Effects similar to contrast masking can be achieved by adjusting the response curves of an image.|$|E
50|$|Van den Broek often uses {{saturated}} and high-key colours. In his work, {{space is}} made by its borders and demarcations, and light is evoked with shadow, without <b>midtones.</b> This lends to his work often a graphic character, with pure colours.|$|E
5000|$|Robber {{now works}} as an {{underground}} DJ and film-maker. He records under the names Robber Byker and Surfin' Bernard. Robber Byker and Zoe Reynolds have released several Wonky Techno records on Shed Records, and remixes on <b>Midtone.</b> In 2013, with Robber on lead guitar alongside Porkbeast, Vom and Anderson from Crazyhead, a new band was formed, entitled Swamp Delta. The band has released a number of singles to date and played at the Bearded Theory and Rebellion festivals {{during the summer of}} 2014. They plan to release more new music and play more festivals in the summer of 2015.|$|R
40|$|This {{dissertation}} studies {{image quality}} {{problems associated with}} rendering images in devices like printing or displaying. It mainly includes two parts: clustered-dot periodic halftone screen design, and color table compression. ^ Screening is a widely used halftoning method. As {{a consequence of the}} lower resolution of digital presses and printers, the number of printer-addressable dots or holes in each microcell may be too few to provide the requisite number of tone lev- els between paper white and full colorant coverage. To address this limitation, the microcells can be grouped into supercells. The challenge is then to determine the desired supercell shape and the order in which dots are added to the microcell. Using DBS to determine this order results in a very homogeneous halftone pattern. To simplify the design and implementation of supercell halftone screens, it is common to repeat the supercell to yield a periodically repeating rectangular block called the basic screen block (BSB). While applying DBS to design a dot-cluster growth order- ing for the entire BSB is simpler to implement than is the application of DBS to the single non-rectangular supercell, it is computationally very inefficient. To achieve a more efficient way to apply DBS to determine the microcell sequence, we describe a procedure for design of high-quality regular screens using the non-rectangular super- cell. A novel concept the Elementary Periodicity Set is proposed to characterize how a supercell is developed. After a supercell is set, we use DBS to determine the micro-cell sequence within the supercell. We derive the DBS equations for this situation, and show that it is more efficient to implement. ^ Then, we mainly focus on the regular and irregular screen design. With digital printing systems, the achievable screen angles and frequencies are limited by the finite addressability of the marking engine. In order for such screens to generate dot clusters in which each cluster is identical, the elements of the periodicity matrix must be integer-valued, when expressed in units of printer-addressable pixels. Good approximation of the screen sets result in better printing quality. So to achieve a better approximation to the screen sets used for commercial offset printing, irregular screens can be used. With an irregular screen, the elements of the periodicity matrix are rational numbers. In this section, first we propose a procedure to generate regular screens starting from <b>midtone</b> level. And then we describe a procedure for design of high-quality irregular screens based on the regular screen design method. We then propose an algorithm to determine how to add dots from <b>midtone</b> to shadow and how to remove dots from <b>midtone</b> to highlight. We present experimental results illustrating the quality of the halftones resulting from our design procedure by comparing images halftoned with irregular screens using our approach and a template-based approach. We also present the evaluation of the smoothness and improvement of the proposed methods. ^ In the next part, we study another quality problem: ICC profile color table compression. ICC profiles are widely used to provide transformations between different color spaces in different devices. The color look-up tables (CLUTs) in the profiles will increase the file sizes when embedded in color documents. In this chapter, we discuss compression methods that decrease the storage cost of the CLUTs. For DCT compression method, a compressed color table includes quantized DCT coefficients for the color table, the additional nodes with large color difference, and the coefficients bit assignment table. For wavelet-based compression method, a compressed color table includes output of the wavelet encoding method, and the additional nodes with large color difference. These methods support lossy table compression to minimize the network traffic and delay, and also achieves relatively small maximum color difference. ...|$|R
40|$|A {{common problem}} with CCD sensors is their {{inefficiency}} to accurately record gray level {{information in the}} shadow range of an image. This problem is magnified on original copy having important detail in the <b>midtone</b> to shadow range. Consequently, when this type of an image is scanned gray levels in the shadow range of the reproduction are not easily discerned. This study examines two areas, the recording consistency across the scanning sensor and the gray level recording for a desktop six bit scanner. The former determined how consistently density information is recorded across the sensor, while the latter determines the amount of gray level discerned by the scanner. The result from scanning a three step scale in the consistency test showed that the scanner was not capable of consistently recording across the imaging sensor. These inconsistencies were more pronounced in the medium and dark steps in the test scale reproduction. The medium step reproduced ten distinct gray levels, while the dark step reproduced four distinct gray levels. The test results from the gray level recording capability test showed that 38 out of 64 possible gray levels were recorded by the scanner. This resulted in 59...|$|R
5000|$|... where , {{the shadow}} {{critical}} area fraction, {{is the area}} fraction on the form at which the halftone pattern just appears solid on the print. This model, while simple, has dots with relatively small perimeter (in the shadows) exhibiting greater gain than dots with relatively larger perimeter (in the <b>midtones).</b>|$|E
50|$|Not all {{halftone}} dots {{show the}} same amount of gain. The area of greatest gain is in <b>midtones</b> (40-60%); above this, as the dots contact one another, the perimeter available for dot gain is reduced. Dot gain becomes more noticeable with finer screen ruling, {{and is one of the}} factors affecting the choice of screen.|$|E
50|$|It also {{supports}} several more advanced capabilities, such as batch editing/saving/renaming, fine-tuning of <b>midtones,</b> highlights, and shadows, and red-eye removal. It can adjust brightness, contrast, hue, and saturation, including automatic adjustments that {{can sometimes be}} quite helpful. It also has easy-to-use features such as one-click image compression, and resizing to a user's own choice. It does not however, offer any sort of actual drawing or text-editing tools.|$|E
40|$|In {{computer}} graphics, {{the process}} of {{improving the quality of}} a digitally stored image by manipulating the image with software is called image enhancement. Advanced image enhancement software also supports many filters for altering images in various ways. We may be forced to apply one or more features of the image enhancement to the low quality photos to improve them for further photogrammetric use. This could be due to some of the unexpected photographic environment, which affect the images quality and needs something to be done to enhance the images for a better view. Applying such features may affect the accuracy in close range photogrammetric measurements. To find out the effect of these features in the close range photogrammetric measurements accuracy, a theoretical study on applying some of the image enhancement features were studied using CAD environment (3 D Studio software), which can be used as an excellent tool for accuracy studies. Eight of Image enhancement feature were studied, re-size, sharpen, blur, <b>midtone,</b> contrast, highlight, shadow, and brightness. The original images were created using 3 D studio MAX as a CAD environment, which enable us to get errorless image for a computer model as a tes...|$|R
40|$|Printer {{identification}} {{based on}} a printed document has many desirable forensic applications. In the electrophotographic process (EP) quasiperiodic banding artifacts {{can be used as}} an effective intrinsic signature. However, in text only document analysis, the absence of large <b>midtone</b> areas makes it difficult to capture suitable signals for banding detection. Frequency domain analysis based on the projection signals of individual characters does not provide enough resolution for proper printer identification. Advanced pattern recognition techniques and knowledge about the print mechanism can help us to device an appropriate method to detect these signatures. We can get reliable intrinsic signatures from multiple projections to build a classifier to identify the printer. Projections from individual characters {{can be viewed as a}} high dimensional data set. In order to create a highly effective pattern recognition tool, this high dimensional projection data has to be represented in a low dimensional space. The dimension reduction can be performed by some well known pattern recognition techniques. Then a classifier can be built based on the reduced dimension data set. A popular choice is the Gaussian Mixture Model where each printer can be represented by a Gaussian distribution. The distributions of all the printers help us to determine the mixing coefficient for the projection from an unknown printer. Finally, the decision making algorithm can vote for the correct printer. In this paper we will describe different classification algorithms to identify an unknown printer. We will present the experiments based on several different EP printers in our printer bank. The classification results based on different classifiers will be compared ∗...|$|R
40|$|With the {{advanced}} technology in this information requisite world, {{the way of}} communication has been changed into using multiple media. People use different media to publish their ideas, not just on paper. The same information may be presented in different formats because of different purposes or situations. However, {{each of the four}} major media (print, Web, CD-ROM, and video) has different capabilities for displaying image information. Images optimized for one will not necessarily work well with the others. It happens all the time that people spend much time remanipulating an image which has already been made for another media. For example, it is necessary to reduce the color depth and resolution of an image once it is moving from paper output to the World Wide Web because of the monitor limitation and the need for faster transfer rate. The problem exists when switching images among different output media such as video, multimedia CD-ROM, the World Wide Web and print. Quality and performance are the criteria for good publishing. This thesis project would establish guidelines for the designer, which would show the way to manipulate images once and have them work well in all media, including how to optimize images for print, the World Wide Web, CD-ROM, and video. The parameters which would be explored are: resolution, gamma correction, and color depth. After tests have been run on, an image format with 266 pixel per inch, 24 -bit color depth, and no gamma correction could be established to satisfy the needs for image quality of print, Web, CD-ROM, and video publishing with extra gamma correction of 70 percent at <b>midtone</b> of 50 percent for video production...|$|R
50|$|In {{cases where}} the scene DR is within the sensor DR but much wider than the DR of the JPG engine, ETTR results in the JPG preview image exposed for the {{highlights}} and appearing dark, because the <b>midtones</b> range of the scene is recorded in the shadows range of the JPG preview, and the shadows range of the scene is not recorded in the JPG preview. Wide-DR-capable raw processing external software is required (e.g., Adobe Camera Raw / Adobe Photoshop Lightroom, DxO OpticsPro, Capture One, Raw Therapee).|$|E
50|$|Still, Ōkyo's works remain Japanese. Unlike European painting, Ōkyo's {{images have}} very few <b>midtones.</b> Moreover, he follows the Eastern {{tradition}} in depicting objects with very little setting; often his pictures feature a single subject on a plain background. The result is a more immediate naturalism with a decorative and reflective feel. This was achieved through skillful brush handling; Ōkyo painted with a broad, flat brush, which he would load with more paint on one side. This created broad strokes that vary in paint coverage. Nature was not his only subject; many works by Ōkyo depict normal scenes from life in Kyoto's commercial area.|$|E
50|$|Another magpie {{sits on a}} rock at {{the base}} of the gallows, near the skull of an animal. The only people occupy the left foreground: a man defecates in the shadows to the left, while others watch the three dancers. To the right stands a cross with a {{watermill}} behind. The background opens on to a view of a river valley, with a town to the left and castle on a rocky crag above, and a tower on a rock outcrop to the right, and distant hills and the sky beyond. Behind the dancers rise two intertwined trees, a motif used by Bruegel in an earlier drawing of bears playing in a forest. An impression of depth is given by the progression from dense brown tones dominating of the foreground, through green <b>midtones</b> in the middle distance, to light blues and greys for the background.|$|E
40|$|Auto-exposure(AE) control {{automatically}} calculates and {{adjusts the}} exposure of {{the tone of the}} subject to the <b>midtone</b> of the photograph. In the case of real-time performance, this is usually controlled by the sensor gain for consecutive input images. However, unsuitable sensor gain control methods invariably cause oscillation of the average luminance values for continuous input images, resulting in flickering. Also, in mobile phone cameras, only simple information, such as the average luminance value, can be utilized to calculate the sensor gain, due to coarse performance and cost. Therefore, this paper presents a new real-time AE control method using a look up table(LUT) based on S(Scene) · L(Luminance) curves to avoid the generation of flickering. Prior to the AE control, a LUT is constructed using S·L curve information, which illustrates the characteristic of the output average luminance for input gray patches with corresponding sensor gains. The AE control is then performed by estimating a current scene as a gray patch using the proposed LUT as the first process. This estimation is feasible because the average luminance is the only factor that can be obtained from an input image in a mobile phone camera. That is, the spatial information of images or patches is meaningless. Therefore, a gray patch and scene with the same average luminance are considered the same in a mobile phone camera. A new sensor gain is then estimated using a transposed LUT with the previously estimated patch. The entire estimation process is performed using linear interpolation to achieve real-time execution. Based on experimental results, the proposed AE control method demonstrated real-time, flicker-free exposure estimation. 1...|$|R
40|$|Rapid access {{processing}} is {{a photographic}} processing method that combines high temperature processing and high energy developing agents to obtain very short induction periods, and thus, reduced processing times. In {{order to obtain}} maximum quality from the process, the rapid access halftone percent dot areas must be correctly evaluated according to established aim points. Traditional evaluation methods, either visual or instrumental, produce errors. These errors are the partial result of the unique dot characteristics of the process. These characteristics include soft, fringed dots; dots with low Dmax, found particularily in the shadow areas; loss of dot area on {{the tips of the}} halftone dots during plate exposure^ and high fog in the highlight areas. The illustration of the differences in dot fringe characteristics of a rapid access and a conventional lith halftone imaged with main exposures only were determined visually by the use of microphotographs illuminated with oblique illumination, and quanitatively by microdensitometer traces and film contacting the original halftone films, In all cases, the rapid access halftone dots: had a more highly fringed area when compared to the conventional lith halftone. The test designed to compare percent dot area of first generation rapid access halftones by zero referencing the dot area meter on the identified ghost dot to the percent dot area of their second generation hard dot contact films produced a poor correlation between the two sets of films. Of the three first generation halftones imaged with a main, main plus flash and main plus hump exposure; the main plus bump exposure produced the poorest correlation, particularly in the <b>midtone</b> area of the halftone scale. The compensation method designed to determine the effective percent dot area of first generation rapid access halftones by applying percent dot area correction factors found under various: halfone exposure conditions effectively reduced percent dot area error...|$|R
40|$|In this document, we {{describe}} three different research topics: characterization of red-green and blue-yellow opponent channels, autonomous print quality (PQ) defect diagnosis, {{and design of}} stochastic green-noise masks using dual-metric (DBS and a measure of compactness). ^ For the first work of research, we explore a method to characterize opponent channel in human visual system. Responses of opponent-channels have been modeled {{in the past as}} a linear transformation of cone absorption values L, M, S. We asked two related questions: (i) which of these transformations is psychologically most plausible; (ii) is a linear transformation the right model, in the first place. We tested positions of pure colors for seven subjects in a xy chromaticity diagram as well as in a Boynton-MacLeod chromaticity diagram in log-coordinates. The results showed that neither of the two opponent channels can be adequately approximated by a single straight line. The red-green channel can be approximated by two straight lines. The blue-yellow channel can be approximated by a quadratic function, whose middle section coincides closely with the daylight locus. These results represent a violation of the necessary condition for any linear model to be an adequate description of opponent channels. We further show that the properties of the red-green channel (blue-yellow equilibrium line) measured in our experiment correspond to the properties of parvocellular-pathway cells in the visual system. Our further analysis showed that there was a correlation between the red and the green directions. ^ The goal of the second project is to develop an image analysis tool to automatically identify LJ 9500 PQ defects from scans of the printed PQ test pages. We perform the detection algorithm which correctly identify each type of defect {{in the presence of a}} wide range of variation in the actual appearance of each type of defect. ^ In the third project, we propose a new approach to digital halftoning that generates stochastic dispersed-dot patterns in highlight/shadow textures and aperiodic clustered-dot patterns in <b>midtone</b> textures using dual-metric. ...|$|R
50|$|When facing higher-DR scenes, {{metering}} on most cameras {{tends to}} aim for the <b>midtones,</b> often clipping both extreme highlights and shadows of the JPG preview image (seen as spikes at {{the right and left}} edge of the JPG histogram, respectively). One might expect that this would automatically lead towards a raw histogram in which the right (highlights) range is neatly occupied as with ETTR. Unfortunately, metering algorithms in different cameras may produce very different results in such conditions. It is not uncommon that when shooting a scene with DR within the sensor DR, the default camera metering results in a raw image with blown highlights and ample empty space at {{the left side of the}} histogram. In such case, the ETTR principle of maximizing exposure to the point where the raw histogram is aligned to its right edge, requires a seemingly counterintuitive negative exposure compensation. This may be explained with the ETTR principle: to make best use of the additional DR, available with wide-DR sensors, the extra DR is used to expand the shadows end of the recording range, without increasing the raw highlight headroom.|$|E
5000|$|Jean-Clement Sorret was {{the digital}} colorist for the film, this {{being one of}} the few {{instances}} in which a colorist received a spot in the opening credits. The film contains many coloring effects and [...] "density shifts" [...] of lighting to reflect the moods of the characters. For example, when Hayley gets angry, the colors would be edited to be of lower frequency. One effect used which, as far as the director is aware, had not been done in cinema before, was to brighten the lighting in filming and correct everything down in post-production. This allowed for facial details to be visible even while having a darkened atmosphere. According to the DVD extras, the process required a custom-built digital intermediate to be made and proved to be extremely difficult, with corrections having to be made frame-by-frame in some instances. This technique, known as ETTR, is a standard procedure in digital photography and cinematography to minimize the amount of noise in shadows and <b>midtones.</b> Early working titles of the script were Vendetta and Snip Snip. The producer wanted a title with a [...] "sugar and spice combination and a mixture of harsh roughness, innocence, and vulnerability", and settled on the title Hard Candy.|$|E
40|$|Most run {{on paper}} offset {{newspapers}} have not shown {{the capability of}} producing color reproductions without having the prints subject to a darkening or a muddy look. This problem is basically caused by the absorbency of the newsprint. Since the newsprint is so absorbent, the ink can only be printed to a low solid ink density and coarse screens must be used so the dots do not fill-in. To eliminate this problem, {{it is believed that}} if the <b>midtones</b> are reduced on the gray scale during the separation process, and then the wanted colors 2 ̆ 7 dot areas are restored to their normal values, a reduction in the unwanted colors will occur. This should produce a cleaner, less contaminated looking print. To test this theory, a control separation was made using a normal newsprint tone reproduction curve. Then, variations in the separation process were performed to both under and over correct for the amount of contamination in the reproductions. These separations were printed on a Solna four-color 25 -inch press while manipulating this press 2 ̆ 7 s printing characteristics to make it similar to a Goss Metro, a widely used offset newspaper printing press. The reproductions were shown to 30 randomly selected newspaper observers who rated each printed separation on a scale from 1 (totally unsatisfactory) to 9 (most satisfactory). The results of this experimentation proved that newspaper observers preferred a print with a lightened overall reproduction from the normal newsprint reproduction curve while keeping the apparent visual saturation in the colored areas constant. This is {{based on the fact that}} the prints where the <b>midtones</b> were reduced by 10 percent and 5 percent were rated by the observers as being their first and second choices, respectively. The observers 2 ̆ 7 third and fourth choices, however, were reproductions with the midtone values being the normal newsprint tone reproduction or slightly increased, while the least preferred prints were the ones with the <b>midtones</b> lowered the most. This would therefore seem- to imply that there is a limit as to how low the <b>midtones</b> can be reduced from the normal values and still be preferred. From this study, however, the exact location of this limit can not be stated...|$|E
40|$|This {{research}} has explored {{the possibility of}} determining the optimum black and white photographic print characteristics for paper surface, image color, and contrast to achieve optimum reproduction from that original for a given set of printing press conditions utilizing single impression offset lithography. A random population of 101 observers evaluated the reproduction of eighteen different photographic prints of the same subject from the identical film negative. The prints varied in contrast grades from grade one to grade three; in image color from warm black to neutral black to cool black; in surface characteristics from glossy to semi-lustre to matte; and surface finishes of smooth and textured. The reproductions studied were produced after a tone reproduction analysis was conducted to determine the printing press characteristics. To reduce variablity the same press, paper, inks, and plates were used for all the press tests. The film negatives were generated by scanning each of the eighteen original photographic prints on the state-of-the-art Crosfield Magnascan 640 utilizing a predetermined program derived from the initial press and tone reproduction data. The observers were asked to choose what they {{believed to be the}} best three reproductions and the three most inferior reproductions. To analyze the characteristics which contributed to the findings of certain reproductions being selected superior to others a correlation analysis was performed. With this information, the paper characteristics which yielded the optimum reproduction was determined. These characteristics are: A medium to double weight paper stock A cool black image color A smooth, glossy surface (not ferrotyped) A contrast grade between 1. 5 and 2. 5 when the negative has a density range between 0. 90 and 1. 3 A density range of the original greater than 1. 4 but less than 1. 9 with gradients equal to 12 plus/minus 1 gradient To obtain the appropriate highlight, <b>midtone</b> and shadow densities on the photographic print, a darkroom tool was developed and successfully tested as a three step reference for the photographer to insure proper placement of these predetermined tones for an optimum reproduction. The 2 ̆ 2 visual comparator 2 ̆ 2 is waterproof and is used when the print is wet; however, examination of the photographic print takes place under standard viewing conditions. Another advantage of the comparator is that it is made of the same photographic material as the photograph itself. This study shows that a systems approach {{can be applied to the}} creation of the photographic print to incorporate photography with the reproduction process producing high quality reproductions...|$|R
40|$|The {{investigation}} {{examined the}} printing {{characteristics of the}} association product plate, a continuous tone lithographic process which utilizes a synthetic, light-sensitive coating variably ink-receptive according {{to the degree of}} exposure it receives. The methodology was designed to determine (1) the printing range of the plate, (2) the effective sensitivity of the coating to varying amounts of exposure within this range, and (3) the degree to which adherence to ideal tone reproduction could be achieved through systematic variation of exposure. The plate is not commercially available, therefore it was necessary to consult patent literature and prior research to obtain details of the coating composition and plate manufacture. A number of pre-trial tests were conducted to optimize the manufacturing process and achieve a desirable balance of coating resiliency and printability. After optimizing plate manufacture, equal-interval continuous tone gray scales were printed on a conventional lithographic press to determine the printing characteristics of the plate. The characteristic curves which were generated by plotting printed densities against negative densities revealed low highlight contrast and high <b>midtone</b> and shadow contrast within a relatively narrow log exposure range of. 75 - 1. 10. Faulty tone reproduction can often be corrected by modifying the contrast gradients of the film negative(s) used for plate exposure. In halftone lithography, this procedure selectively alters dot area to achieve desired print reflectance. In the case of continuous tone processes, tone corrected negatives selectively modify the ink receptivity of the plate coating. VIII In order to test the effects of exposure modulation on the tone reproduction characteristics of the plate, an optical matrix was sensitometrically generated from continuous tone panchromatic film. Various film development techniques were used to alter negative contrast and obtain gray scales with diverse equal-interval log E increments within the response range of the plate. The matrix was used for exposing plates of similar manufacture to generate print data for analysis. Printed densities obtained from the matrix were used to construct conventional characteristic curves, as well as response profiles relating printed density differences to log exposure increments. A mathematical model of the Jones tone reproduction diagram was used to calculate tone correction factors in terms of relative log exposure and to reconstruct tone corrected negative scales. Printed densities obtained from the test matrix were used to gauge the effect of the corrected scales on tone reproduction. This method, in essence, emulated the Jones-type approach to tone correction in a single print run, thereby reducing manufacturing and printing variables. The data revealed a high degree of success in correcting mid-tone and shadow reproduction through exposure modification. Although some areas of non-linear plate response were persistent, these were primarily due to shortcomings in the methodology rather than plate failure. Optical tone correction of the low highlight contrast was significantly more problematic. Given the relatively short printing range of the plate, the required negative gradient eliminates many original tonal values. Other means physical and/or chemical are required to lengthen the tonal range and raise highlight contrast in order to improve overall plate performance...|$|R
40|$|The {{advent of}} {{computers}} {{and their impact on}} the graphic arts and printing industry has, and will continue to, change the methodology of working and workflow in prepress operations. The conversion of analog materials (prints, artwork, transparencies, studio work) into a digital format requires the use of scanners or digital cameras, coupled with the knowledge of output requirements as related to client expectations. The chosen input sampling ratio (sampling rate in relation to halftone screening) impacts output quality, as well as many aspects of prepress workflow efficiency. The ability to predict printed results begins with the correct conversion of originals into digital information and then an appropriate conversion into the output materials for the intended press condition. This conversion of originals into digital information can be broken down into four general components. First, the image must be scanned {{to the size of the}} final output. Second, the input sampling ratio must be determined, in relation to the screening requirements of the job. This ratio should be appropriate to the needs of the printing condition for the final press sheet. Third, the highlight, highlight to <b>midtone</b> and shadow placement points must be determined in order to achieve the correct tone reproduction. Fourth, decisions must be made as to the image correction system to be employed in order to obtain consistent digital files from the scanner and prepress workflow. Factors relating to image correction and enhancement include such details as gray balance, color cast correction, dot gain, ink trapping, hue error, unsharp masking, all areas that impact quality. These are generally applied from within software packages that work with the scanner, or from within image manipulation software after the digital conversion is complete. The question of what is the necessary input sampling ratio for traditional AM screening has traditionally been based on the Nyquist Sampling Theorem. The basis for determining input sampling ratio requirements for frequency modulated (FM) screening is less clear. The Nyquist Theorem (originally from electrical engineering and communications research) has been applied to the graphic arts, leading to the general acceptance of a standard 2 : 1 ratio for most prepress scanning work. The ratio means that the sampling rate should be twice the screen frequency. This thesis set out to determine if there are dif ferences in input sampling ratio scanning requirements, based on the screen frequency rx selection (lOOlpi AM, 1751 pi AM and 21 |lFM used in this study), when generating films and/or plates for printing, that might question this interpretation of the Nyquist Sam pling Theorem as it relates to the graphic arts. Five images were tonally balanced over three different screening frequencies and six different sampling ratios. A reference image was generated for each condition using the Nyquist Sampling ratio of 2 : 1. Observers were then asked to rate the images in terms of quality against the standard. Statistical analysis was then applied to the data in order to observe interactions, similarities and differences. A pilot study was first run in order to determine the amount of unsharp masking to use on the images that would be manipulated in the main study. Seven images were pre sented from which four were selected for the final study. Thirty observers were asked for their preference on the amount of sharpening to use. It was found that for this condition (7 images) observers preferred the same amount of sharpening for the 1751 pi AM and 21 u FM screens, but slightly more sharpening for the lOOlpi AM screen. This information was then applied to the main study images. An additional image previously published was added after the pilot study, as it contained elements not found in the other images The unsharp masking applied to this image was the same as at the time of publication. The main study focused on the interaction of image type, screen frequency and varia tions of input scanner sampling ratios as it relates to output. The results indicated that image type, sampling ratio, sampling ratio - frequency interaction were factors, but fre quency alone was not. However, viewing the interaction chart of frequency and sampling ratio for the 1751 pi AM and 21 u FM screens alone, an insignificant difference was indi cated (at a 95 % confidence level). The conclusion can therefore be drawn that at the higher screen frequencies tested in this study, viewer observations showed that the input sampling ratios should be the same for 1751 pi and 21) 1 FM screens. Continuous tone orginals should be scanned at a sam pling ratio of 1. 75 : 1. This answered the question of whether FM screening technology can withstand a reduced input sampling ratio and maintain quality, which this study finds cannot. At the lower screen ruling of lOOlpi the input scanner sampling ratio requirement, based on viewer preferences of the five images presented, can be reduced to a 1. 5 : 1...|$|R
40|$|No {{detectable}} {{functional relationship}} {{was found between}} dot area and dot density within a solid ink density and paper combination over a 65 to 300 line screen range. Dot spread, measured as per cent difference from the per cent dot value of the half tone negative, {{was found to be}} more significant in the <b>midtones</b> and the 300 line screen was observed to increase spread appreciably as well as variability with tints...|$|E
40|$|We {{present a}} {{clustered}} minority pixel error diffusion halftoning algorithm {{for which the}} quantizer threshold is modified based on the past output and a dot activation map. Dot size, dot shape, and dot distribution are more controllable, compared with other clustered dot halftone algorithms such as Levien's algorithm. This method also effectively reduces structured worm-like artifacts in <b>midtones</b> that occur in Levien's algorithm. The dot distribution is further improved by using different error diffusion weights for different input gray levels...|$|E
40|$|The {{thesis is}} an {{evaluation}} of preferred tone reproduction {{as a function of}} reproduction size. When photographs are reproduced photomechanically using the halftone process the main limitation in achieving a facsimile reproduction is the density range of the printing process. Because the printing processes normally used cannot achieve the same high density range as a photograph, tonal compression must occur. The manner in which the tonal compression is achieved has an important effect upon the appearance of the reproduction. The tone reproduction curve is a graphic method used to describe the relationship between the density of the original and the density of the reproduction. A variety of tone reproduction curves can be used to describe a variety of prints whose tone reproduction has been varied. Experimental work was conducted to determine if pre ferred tone reproduction is different for reproductions of varying size. Low key, normal key, and high key original photographs were reproduced using the halftone process with five variations of tone reproduction at a small, medium and large size. A group of twenty-one viewers was used to evaluate the reproductions. The results of the tone reproduction evaluation suggest that preferred tone reproduction varies relative to size primarily with low key scenes. To a lesser extent preferred tone reproduction varies relative to size with normal key scenes. There appears to be no difference in preferred tone reproduction relative to size with high key scenes. The results seem to indicate that with low key originals, viewers prefer small size (32 ̆ 2 x 2 5 / 162 ̆ 2) repro ductions with 2 ̆ 2 open 2 ̆ 2 or lower density <b>midtones</b> and shadows. At the middle size (8 1 / 22 ̆ 2 x 6 5 / 82 ̆ 2), viewers prefer darker <b>midtones</b> and shadows, sacrificing some detail. At the largest size, viewers rated the reproduction with 2 ̆ 2 open 2 ̆ 2 shadows very poor while rating a reproduction with darker <b>midtones</b> and shadows as best. The reproduction with the darkest shadows, losing all detail, was rated the poorest. The only difference in preferred reproduction relative to size in normal key subjects was that at the smallest size the preferred reproduction was a curve that accentuates the contrast in the <b>midtones.</b> At the medium and large sizes the preferred curve was the curve that corresponds most closely to equal visual differences in brightness throughout the range of tones. This is similar to results obtained by Yule in previous studies. The preferred reproductions chosen when using a high key subject were very consistent from small to medium to large. The preferred reproductions for this subject were different than that for low and normal key scenes...|$|E
40|$|High Dynamic Range imaging techniques, used in {{combination}} with gigapixel imaging technology, allow creation of highly-detailed, evenly-exposed panoramic photographs of a space in 360 -degrees. Typically obtaining correct exposure across a 360 -degree plane of view is difficult, leading to blown-out highlights and muddled shadows in {{certain areas of the}} image. High Dynamic Range imaging techniques also allow for extremely saturated rendition of colors, and a compression of <b>midtones</b> resembling drawing or painting. This technique, when applied to the fields of fine art and fashion, yields visually striking results and exceptional design flexibility, as well as output resolution uncommon to both areas...|$|E
30|$|An {{important}} {{concept in}} HDR reconstruction is the weighting function (also called certainty function), which is introduced in [27] and subsequently used in [28 – 30]. The weighting function in [30] {{is defined as}} being proportional to {{the slope of the}} camera response function (CRF), indicating how quickly the output pixel intensity of a camera varies for a given input intensity. Correspondingly, certainty images are computed by applying certainty functions to digital images, revealing which parts of the image are the most “reliable”. It is found that this is the case for the <b>midtones</b> of the image. One of the issues however, is that several choices for weighting functions have been proposed in literature and that it is not clear which function to choose under which conditions.|$|E
40|$|This study {{examines}} microfading {{as a tool}} to predict light induced colour change of water colour paintings. Microfading allows direct measurements of the material. By exposing a small area (0, 2 - 0, 4 mm) to very intense light while simultaneously measure the colour of the area with a spectrometer, a quick determination of the materials lightfastness can be made. Measurements have been executed on Carl Larssons ‘Lisbeth’, 1894 in the collection of Gothenburg museum of art. Further, measurements have been made on samples of indigo in gum arabic on gelatine-sized paper in order to evaluate the impact of spectral distribution of the light source used, the impact of the colour strength of the samples, the impact of angular distribution of the light source and whether reciprocity holds. These factors affect the accuracy of the test. Having them in mind, an estimation of the fading behaviour of the water colour painting can be made. The measured spots on the painting showed a low tendency to light induced colour change, below Blue Wool 3. The most sensitive spots where {{to be found in the}} red <b>midtones.</b> Microfading offers data which can be used together with a lighting policy to limit the degree of light damage to paintings on display. Uppsats för avläggande av filosofie kandidatexamen i Kulturvård, Konservatorprogrammet 15 hp Institutionen för kulturvård Göteborgs universitet 2016 : 3...|$|E
