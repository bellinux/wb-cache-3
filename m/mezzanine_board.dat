26|12|Public
25|$|In {{the last}} {{revision}} of the Mac Mini G4, the internal <b>mezzanine</b> <b>board</b> was upgraded to accommodate the AirPort Wi-Fi and Bluetooth technology onto one chip. In prior models, the Mac Mini included an AirPort Extreme card taped to the <b>mezzanine</b> <b>board</b> and a separate Bluetooth module. This new Wi-Fi card also no longer uses an MMCX-Female connector for the antenna (as the prior models did) but rather a proprietary Apple one.|$|E
5000|$|... #Caption: Pyra {{prototype}} mainboard backside without CPU <b>mezzanine</b> <b>board</b> ...|$|E
50|$|In {{the last}} {{revision}} of the Mac Mini G4, the internal <b>mezzanine</b> <b>board</b> was upgraded to accommodate the AirPort Wi-Fi and Bluetooth technology onto one chip. In prior models, the Mac Mini included an AirPort Extreme card taped to the <b>mezzanine</b> <b>board</b> and a separate Bluetooth module. This new Wi-Fi card also no longer uses an MMCX-Female connector for the antenna (as the prior models did) but rather a proprietary Apple one.|$|E
50|$|The {{following}} <b>mezzanine</b> expansion <b>boards</b> {{are currently}} available.|$|R
50|$|Standardized {{expansion}} buses for peripheral I/O {{have led}} {{to a wide range of}} compatible add-on <b>mezzanine</b> <b>boards</b> that will work across a variety of 96Boards products. Users have access to a wide range of boards with different features at various price points. In addition, some SoC vendors have announced long term availability of the SoC to encourage their use in products with long life cycles.|$|R
40|$|The Trigger Data Serializer (TDS) is {{a custom}} {{designed}} Application Specific Integrated Circuit (ASIC) designed at the University of Michigan {{to be used}} on the ATLAS New Small Wheel (NSW) detector. The TDS is a central hub of the NSW trigger system. It prepares the trigger data for both pad and strip detectors, performs pad-strip matching, and serializes the matched strip data to other circuits {{on the rim of}} the NSW. In total, 6000 TDS chips will be produced. As part of the TDS’ initial production run, a test platform was developed to verify the functionality of each chip before being sent to users. The test platform consisted of multiple FPGA evaluation boards with custom designed <b>mezzanine</b> <b>boards</b> to hold the TDS chip during testing and control software running on a local computer. Of the initial run of 200 chips, 161 chips were tested with the automatic setup of which 158 passed. Detailed description of the TDS and automatic test fixture can be found in this thesis...|$|R
5000|$|A daughterboard, daughtercard, <b>mezzanine</b> <b>board</b> or {{piggyback}} board is an expansion card that attaches {{to a system}} directly. [...] Daughterboards often have plugs, sockets, pins or other attachments for other boards. Daughterboards often have only internal connections within a computer or other electronic devices, and usually access the motherboard directly rather than through a computer bus.|$|E
40|$|We have {{implemented}} the digital {{section of a}} {{wireless local area network}} (WLAN) demodulator in a reconfigurable interface card called the PCI Pamette. The entire baseband section of the demodulator has been implemented in the Pamette and a simple analog to digital <b>mezzanine</b> <b>board.</b> This is the second implementation of the demodulator, the first being a card-based design using a mixture of discrete and reconfigurable logic. The Pamette implementation took far less time to complete than the card-based design. Moreover, the reconfigurable substrate is much more versatile. This paper describes the Pamette implementation and discusses our experiences with the two different design styles and technologies...|$|E
40|$|International audienceWe {{present a}} system {{architecture}} {{made of a}} motherboard with a Xilinx Zynq System on Chip (SoC) and a <b>mezzanine</b> <b>board</b> equipped with an Associative Memory chip (AM). The proposed architecture is designed {{to serve as an}} accelerator of general purpose algorithms based on pipeline processing and pattern recognition. We present the open source software and firmware developed to fully exploit the available communication channels between the ARM CPU and the FPGA using Direct Memory Access (DMA) technique and the AM using Multi-Gigabit Transceivers (MGT). We report the measured performances and discuss potential applications and future developments. The proposed architecture is compact, portable and provide a large communication bandwidth between components...|$|E
40|$|The report {{presents}} recent {{results of}} research and technical work on LLRF control system development for FLASH and XFEL. The report period covers approximately {{the last several months}} before the publication date. The report subject covers some of the chosen contributions by Elhep Lab. Most of the design efforts are supported by measurement results performed either at MTS or directly at the linac. A part of the LLRF system cooperating with RF Gun requires even more stringent parameters in terms of quality. A complete measurement path was presented including I and Q detectors and fpga based, low latency digital controller. The system has standardized DOOCS gui. Input signal calibration procedure was added after practical control tests. An alternative software solution to fpga based controller, supported by matlab was developed to investigate novel firmware implementation. The complex control algorithm is based on nonlinear system identification. The controller spans over the full cryomodule, and calculates vector sum for eight cavities. A modular construction of the LLRF controller system PCB is presented. The module consists of a digital part residing on the base platform and exchangeable analog part positioned on a number of daughter-boards. Functional structure was presented and in particular the FPGA implementation with configuration and extension block for RF <b>mezzanine</b> <b>boards...</b>|$|R
50|$|The {{upper floor}} {{vestibule}} contains two banks of timber cupboards used for storage of robes, under a more recently added <b>mezzanine.</b> The timber <b>boarded</b> ceiling {{is decorated with}} two fine carved timber rosettes. The western wall has three arched diamond-glazed windows with stained glass borders and inset with masonic symbols.|$|R
40|$|The Readout System (ROS) is the ATLAS DAQ {{element that}} {{receives}} the data fragments from the ~ 1600 detector readout links, buffers them and provides them on demand {{to the second}} level trigger processor or to the event building system. The ROS system is implemented with ~ 150 PCs each one housing in average 4 custom-built PCI <b>mezzanine</b> <b>boards</b> (ROBIN) and a 4 -port PCIe NIC. Each PC runs a multithreaded OO-software framework managing the requests for data coming through the NIC and collecting the corresponding fragments from the physical buffers. At LHC luminosity of 10 ^ 33 cm- 2 s- 1, corresponding to an average Level 1 trigger rate of 75 kHz, the ROS has to concurrently service up to approximately 20 kHz of data requests from the Level 2 trigger and up to 3. 5 kHz of requests from event building nodes. The system has been commissioned in 2007 and since then has been working smoothly. For the most of 2008 the main activity has been data taking with cosmics in which the Level 1 trigger rate is much lower with respect to LHC operation. However, the commissioning of the detector requires a much larger data size per event and a very limited rate reduction between Level 1 and Level 2, pushing the ROS to operate in unforeseen conditions. We will describe here the ROS performances under these conditions, analyze them {{in view of the}} 2009 data taking conditions and discuss evolutionary scenarios of the ROS system for improving its flexibility and enh ance its performances, especially in view of the super LHC upgrade...|$|R
40|$|In the RF {{system for}} the VUV-FEL Linac each {{klystron}} supplies RF power to up to 32 cavities. The requirements for gradient and phase stability are therefore of the order 10 - 4 and 0. 1 degree respectively [1]. To meet this requirements a DSP digital LLRF control system was designed. The basic part of this system is 6 U-VME standard board which contains TMS 320 C 6701 floating-point DSP (Texas Instruments Inc.) and 8 Gigalink channels [2]. For communication between DSP and ADC/ DAC boards a Gigalink <b>mezzanine</b> <b>board</b> was developed [3]. BIOS, SOLARIS 2. X driver and C++ library were written for the DSP board. LLRF control algorithm was implemented in DSP firmware and DOOCS server [4]. Hardware and software design of the DSP based LLRF control are presented...|$|E
40|$|SRAM based {{reprogrammable}} FPGAs {{are sensitive}} to radiation-induced Single Event Upsets (SEU), not only in their user flip-flops and memory, {{but also in the}} configuration memory. Appropriate mitigation has to be applied if they are used in space, for example the XTMR scheme implemented by the Xilinx TMRTool and configuration scrubbing. The FLIPPER fault injection platform, described in this paper, allows testing the efficiency of the SEU mitigation scheme. FLIPPER emulates SEU-like faults by doing partial reconfiguration and then applies stimuli derived from HDL simulation (VHDL/Verilog test-bench), while comparing the outputs with the golden pattern, also derived from simulation. FLIPPER has its Device-Under-Test (DUT) FPGA on a <b>mezzanine</b> <b>board,</b> allowing an easy exchange of the DUT device. Results from a test campaign are presented using a design from space application and applying various levels of TMR mitigation. 1...|$|E
40|$|The LHCb sub-detector {{electronics}} {{requires a}} configuration bus {{able to communicate}} fast and properly over an up to 120 -meter line, with a unique master, and up to 32 slaves located on the detector. In such a configuration, the master card will {{be located in the}} control room, which is not exposed to radiation, and the slave close to the detector electronics boards. The slave thus has to be both SEL and SEU immune, in order to work properly in radiation sensitive environments (up to 40 Krad of total dose). The SPECS protocol is a 10 Mbit/s serial link designed for the configuration of remote electronics elements. SPECS is a single-master multi-slave bus. The protocol requires 4 lines in BLVDS technology with pole-zero cancellation at the emitter side. The SPECS frame is composed of a fixed-size header (8 -bit words which contain information relevant to the transfer), a variable-size data block, and a fixed-size trailer with the checksum of the data block :The SPECS master board will host 4 SPECS masters. This board is implemented on a standard 3. 3 V 32 -bit 33 MHz PCI board. The heart of the system is designed as a portable VERILOG code and integrated within an Altera Cyclone FPGA, the PCI interface being performed by a dedicated circuit (PLX 9030). The SPECS slave offers different interfaces: Point to point SPECS interface for long distance interconnections, Multi drop SPECS bus, Local bus parallel interface, 32 configuration I/O lines, JTAG master interface, long distance I 2 C master bus, 24 commands driver to manage 12 independent links. An intermediate <b>mezzanine</b> <b>board</b> of 0. 47 dm² houses the SPECS slave, and provides all the described functionalities using 2 SMC connectors. The <b>mezzanine</b> <b>board</b> also provides most of the necessary service functions for the sub-detector front-end electronics. The goal is indeed to avoid putting any unnecessary electronics in the radiation sensitive area...|$|E
40|$|Submitted in partial {{fulfilment}} of {{requirement in}} degree of Bachelor in Engineering. Nuclear Power {{appears to be}} an inevitable option as future energy source; but disposal of nuclear waste is an important issue of concern in harnessing nuclear energy through “critical reactors”, which needs to be addressed satisfactorily. In India, economic exploitation of nuclear power is considerably dependent on Uranium &Thorium. Accelerator-Driven System (ADS) has the potential to provide an additional route to an efficient use of the available uranium and thorium resources, besides offering a way towards nuclear waste incineration. o The most challenging part of this CW proton accelerator is development of the low-energy injector, typically up to 20 MeV, because the space charge effects are maximal here. Therefore, BARC has initiated a programme {{for the development of a}} Low Energy (20 MeV) High Intensity Proton Accelerator (LEHIPA) as front-end injector of the 1 GeV accelerator for the ADS programme. RF Protection Interlock (RFPI) system has been developed for the protection of high power RF components in LEHIPA. The system consists of six VME 64 X based modules which processes output of different sensors. The system is designed based on a mezzanine card approach with a common base board. All the function specific boards are mounted as <b>mezzanine</b> <b>boards.</b> After detecting any fault, time required for RFPI system to switch off RF power to the cavities is less than 1 us. The system consists of the following modules. 1. Multi trip module- processes the pickup RF signals from directional couplers. 2. Photo Multiplier Tube (PMT) module- processes output signals from PMTs 3. Field Emission Probe (FEP) module- processes output of field emission probes 4. Photo diode module- processes optical signal or arc signals from the system 5. Photo transistor module- processes output of photo transistor which is mounted on RF window 6. System control module-which processes RF leakage signals from wave guide joints and also generates RESET and Video Pulse signals to other modules of the system...|$|R
40|$|Abstract—This paper {{presents}} {{the design of}} a platform for teaching majority of courses in the embedded computer engineering curriculum. The learning platform {{is a result of the}} E 2 LP project whose main idea is to provide a unified platform which will cover a complete process for embedded computer systems learning. The main body of the platform is the base board with FPGA connected to a wide range of interfaces – audio, video, communication, memory and user I/O. Additionally the base board can be connected to one extension board via the standardized <b>Mezzanine</b> interface. Extension <b>boards</b> broaden the range of applications of the platform beyond digital system design and allow the platform to be used in the entire computer engineering curriculum. The first application of the platform in laboratory exercises has shown a positive acceptance of the platform by students. I...|$|R
50|$|The exits at the 181st Street station were {{designed}} {{so as to}} not be sidewalk obstructions like those constructed by Interborough Rapid Transit and Brooklyn-Manhattan Transit, the two other subway operators in the city. To accomplish this goal, the station's entrances were placed on city-owned property near the station-the exit at Overlook Terrace and West 184th Street, and in a building-the exit at Fort Washington Avenue between West 183rd and 185th Streets. The Overlook Terrace entrance consists of a passageway to the mezzanine, while the Fort Washington Avenue entrance, which is across from Bennett Park, consists of three elevators to <b>mezzanine</b> level. While <b>Board</b> of Transportation engineers {{were able to find}} ways to place these subway entrances in places without forming obstructions, {{it was not possible to}} do the following at Fort Washington Avenue at 181st Street. Here, there are four street stairs in total, two to each southern corner of the intersection.|$|R
40|$|We are {{developing}} a Micro TCA Carrier Hub card which provides timing, control and data acquisition functions in a Micro TCA crate for HL-LHC readout electronics. This module may be mounted in the primary or redundant MCH slot in a Micro TCA crate, and distributes low-jitter LHC RF clock and encoded fast timing signals to up to 12 AMC modules. In addition, it receives buffer status signals and DAQ data at up to 600 MBytes/sec from each AMC. The prototype module is built on a commercial MCH base board with a custom <b>mezzanine</b> <b>board</b> stack. The latest Xilinx® Virtex®- 6 FPGA are used to provide a clear upgrade path. Prototype modules {{have been developed for}} a CMS HCAL test beam in summer 2010. We describe the specifications of the module, its application in a Micro TCA system beyond CMS HCAL, and our experience in commissioning the module for the test bea...|$|E
40|$|For {{each of the}} 272 SC {{cavities}} of the LEP 2 RF {{system a}} total of 24 temperatures at various points around the cavity and in the cryostat must be continuously monitored. In the event of certain limit values being exceeded appropriate actions {{must be taken to}} protect valuable equipment. These temperature measurements are most conveniently done within the G 64 based equipment controller (EC) that provides the interface and low level control functions for the cavity. Since reliability and failsafe action is of utmost importance the tempera-ture measurement and equipment protection functions must be completely independent of the other EC tasks, although temperature data and limits status must be available to the EC. A dedicated temperature measure-ment module has been developed to meet these require-ments. The design is based on a general-purpose intelli-gent base acquisition module with an on-board industrial microcontroller. A <b>mezzanine</b> <b>board</b> provides the specific functions such as tempera [...] ...|$|E
40|$|Prometeo is the {{portable}} test-bench {{for the full}} certification of the front-end electronics of the ATLAS Tile calorimeter designed for the upgrade phase-II. It is a high throughput electronics system designed to simultaneously read-out all the samples from 12 channels at the LHC bunch crossing frequency and assess {{the quality of the}} data in real-time. The core of the system is a Xilinx Virtex 7 evaluation board extended with a dual QSFP FMC module to read-out and control the front-end boards. The rest of the functionalities of the system are provided by a HV <b>mezzanine</b> <b>board</b> that to turn on the gain of the photo-multipliers, an LED board that sends light to illuminate them, and a 12 channel ADC board that samples the analog output of the front-end. The system is connected by ethernet to a GUI client from which QA tests are performed on the electronics such as noise measurements and linearity response to an injected charge...|$|E
40|$|We {{present an}} {{overview}} of the 'ICE' hardware and software framework that implements large arrays of interconnected FPGA-based data acquisition, signal processing and networking nodes economically. The system was conceived for application to radio, millimeter and sub-millimeter telescope readout systems that have requirements beyond typical off-the-shelf processing systems, such as careful control of interference signals produced by the digital electronics, and clocking of all elements in the system from a single precise observatory-derived oscillator. A new generation of telescopes operating at these frequency bands and designed with a vastly increased emphasis on digital signal processing to support their detector multiplexing technology or high-bandwidth correlators [...] -data rates exceeding a terabyte per second [...] -are becoming common. The ICE system is built around a custom FPGA motherboard that makes use of an Xilinx Kintex- 7 FPGA and ARM-based co-processor. The system is specialized for specific applications through software, firmware, and custom <b>mezzanine</b> daughter <b>boards</b> that interface to the FPGA through the industry-standard FMC specifications. For high density applications, the motherboards are packaged in 16 -slot crates with ICE backplanes that implement a low-cost passive full-mesh network between the motherboards in a crate, allow high bandwidth interconnection between crates, and enable data offload to a computer cluster. A Python-based control software library automatically detects and operates the hardware in the array. Examples of specific telescope applications of the ICE framework are presented, namely the frequency-multiplexed bolometer readout systems used for the SPT and Simons Array and the digitizer, F-engine, and networking engine for the CHIME and HIRAX radio interferometers. Comment: 20 pages, 8 figures. Submitted to JAI special issue on Digital Signal Processing in Radio Astronomy (2016...|$|R
40|$|The Cherenkov Telescope Array (CTA) is {{the next}} {{generation}} ground-based very high energy gamma-ray observatory. The Large-Sized Telescope (LST) of CTA targets 20 GeV [...] 1 TeV gamma rays and has 1855 photomultiplier tubes (PMTs) installed in the focal plane camera. With the 23 m mirror dish, the night sky background (NSB) rate amounts to several hundreds MHz per pixel. In order to record clean images of gamma-ray showers with minimal NSB contamination, a fast sampling of the signal waveform is required so that the signal integration time can be as short as the Cherenkov light flash duration (a few ns). We have developed a readout board which samples waveforms of seven PMTs per board at a GHz rate. Since a GHz FADC has a high power consumption, leading to large heat dissipation, we adopted the analog memory ASIC "DRS 4 ". The sampler has 1024 capacitors per channel and can sample the waveform at a GHz rate. Four channels of a chip are cascaded to obtain deeper sampling depth with 4096 capacitors. After a trigger is generated in a <b>mezzanine</b> on the <b>board,</b> the waveform {{stored in the capacitor}} array is subsequently digitized with a low speed (33 MHz) ADC and transferred via the FPGA-based Gigabit Ethernet to a data acquisition system. Both a low power consumption (2. 64 W per channel) and high speed sampling with a bandwidth of $>$ 300 MHz have been achieved. In addition, in order to increase the dynamic range of the readout we adopted a two gain system achieving from 0. 2 up to 2000 photoelectrons in total. We finalized the board design for the first LST and proceeded to mass production. Performance of produced boards are being checked with a series of quality control (QC) tests. We report the readout board specifications and QC results. Comment: In Proceedings of the 34 th International Cosmic Ray Conference (ICRC 2015), The Hague, The Netherlands. All CTA contributions at arXiv: 1508. 0589...|$|R
40|$|The ATLAS Fast Tracker is a {{hardware}} processor built to reconstruct tracks {{at a rate}} of up to 100 kHz and provide them to the high level trigger system. The Fast Tracker will allow the trigger to utilize tracking information from the entire detector at an earlier event selection stage than ever before, allowing for more efficient event rejection. The connection of the system from to the detector read-outs and to the high level trigger computing farms are made through custom boards implementing Advanced Telecommunications Computing Technologies standard. The input is processed by the Input Mezzanines and Data Formatter boards, designed to receive and sort the data coming from the Pixel and Semi-conductor Tracker. The Fast Tracker to Level- 2 Interface Card connects the system to the computing farm. The Input <b>Mezzanines</b> are 128 <b>boards,</b> performing clustering, placed on the 32 Data Formatter mother boards that sort the information into 64 logical regions required by the downstream processing units. This necessitates the sharing of data between different data formatters at high bandwidth with low latency over the full-mesh backplane of the host shelf. Each data formatter board contains a custom micro-controller designed to manage the combined system, and provide the ability to download firmware on each field programmable gate array over Ethernet. In the FLIC system, each board receives 8 optical links with a bandwidth of 1 Gbps, re-formats the data to the ATLAS standard record format, performs the conversion from local to global module identifier, and sends the event records out to the High Level Trigger at 2 Gbps with a latency of O(10) microseconds. The Input Mezzanine and Data Formatter system is the first component of the Fast Tracker to be installed and commissioned. Since the start of the installation in December 2015, several boards have already been integrated into ATLAS, successfully taking data during proton-proton and heavy ion collisions. This poster discusses the current status of the installation, as well as the ongoing commissioning of the data flow while running with the ATLAS detector. Included in this talk is the status of the hardware, firmware and software of each of the systems and their microcontroller. The Run Control software required for integration of the system into the ATLAS detector is also discussed...|$|R
40|$|The Fast Tracker is an {{integral}} part of trigger upgrade program for the ATLAS experiment. At LHC Run 2, which started operations in June 2015 at a center of mass energy of 13 TeV, the luminosity could reach up to 2 * 1034 cm^ 2 s^ 1 and an average of 40 - 50 simultaneous proton collisions per beam crossing will be expected. The higher luminosity demands a more sophisticated trigger system with increased use of tracking information. The FTK is a highly-parallel hardware system that rapidly finds and reconstructs tracks in the ATLAS inner-detector at the triggering stage. This paper focuses on the <b>Mezzanine</b> <b>Board</b> that is input module of entire FTK system. The functions of this board are to receive the pixel and micro-strip data from the ATLAS Silicon read-out drivers, perform clustering, and forward the data to its mother board. Mass production and quality control tests of Mezzanine Boards were completed, and staged installation and commissioning are ongoing. Details of its functionality, mass production, quality control tests, and installation status are reported...|$|E
40|$|The Fast Tracker (FTK) is an {{integral}} part of trigger upgrade program for the ATLAS experiment. At LHC Run 2, which started operations in June 2015 at a center-of-mass energy of 13 TeV, the luminosity could reach up to 2 * 1034 cm- 2 s- 1 and an average of 40 - 50 simultaneous proton collisions per beam crossing will be expected. The higher luminosity demands a more sophisticated trigger system with increased use of tracking information. The Fast Tracker is a highly-parallel hardware system that rapidly finds and reconstructs tracks in the ATLAS inner-detector at the triggering stage. This paper focuses on the FTK Input <b>Mezzanine</b> <b>Board</b> that is input module of entire system. The functions of this board are to receive the insertable b-layer, pixel and micro-strip data from the ATLAS Silicon read-out drivers, perform clustering, and forward the data to its mother board. Mass production and quality control tests of Mezzanine Boards were completed, and staged installation and commissioning are ongoing. Details of its functionality, mass production, quality control tests, and installation as well as first results from data taking are reported...|$|E
40|$|A novel rf beam control {{architecture}} {{has been}} successfully tested in the LEIR synchrotron. The design {{is based on a}} VME 64 X carrier board, including a DSP (digital signal processor), into which different daughter cards can be plugged in. The DDC (Digital Down Converter) is one of them. Hardware wise it has the features of a four-channel ADC (analogue-to-digital converter) which outputs drive a powerful FPGA (field programmable logic array); the latter is connected to the DSP on the carrier board via high-speed connectors. Mainly, this unit will acquire rf signals to analyze their phase and amplitude at a specified harmonic of the revolution. The main sampling clock feeding the <b>mezzanine</b> <b>board</b> is at a high harmonic of the particle’s revolution frequency. In the PSB, this frequency is varying along the accelerating cycle and this choice allows analyzing the rf signals from the cavities or from the beam without changing any parameter along the cycle. The sampling clock is tagged at the revolution rate allowing for a synchronous load of new parameters along the accelerating cycle. Synchronous means in phase with the different electronic boards composing the rf beam control. The different signal processing features programmed in the FPGA will be depicted in this note...|$|E
40|$|This User’s Guide {{should be}} used {{together}} with the CCB 2004 Specification [1] and the TTCrx Reference Manual [2]. The TTCrx ASIC can be programmed from the TTC source (four main registers, write only, see Section 1. 1) and over I 2 C serial bus using VME accesses in the CCB 2004 address space (write and read, see Section 1. 2). JTAG access to TTCrx ASIC has not been implemented on the CCB 2004 board. 1. 1 Initialization After power cycling {{make sure that the}} four green LEDs on the front panel indicating active powers as well as “DONE ” LED (FPGA was successfully configured from its EPROM) are “on”. When optical connection between the TTCrq and the source of the TTC clock and commands (TTCvi [3] or TTCci modules) is established, the “TTCRDY” and “QLOCK ” LEDs of the front panel of the CCB 2004 must be “on”. They just repeat the state of the respective LEDs on TTCrq <b>mezzanine</b> <b>board</b> (also visible through the CCB 2004 front panel). If an optical fiber is plugged in from both sides, and both source and destination operate properly, the connection will be established automatically after power cycling. Make sure the “CLK 40 ” LED on the front panel is blinking (~ 7 Hz). Then...|$|E
40|$|The SPS {{accelerator}} {{will be used}} as injector for the LHC and has to {{be adapted}} to the LHC requirements. The tight specification on beam blow-up and bunch spacing in the SPS has required an upgrade program of the SPS injection kicker in order to obtain a reduction of the magnetic field ripple to less than ± 0. 5 % and of the magnet current rise time to less than 145 ns. In this context, the slow control part has been entirely rebuilt on the basis of off-the-shelf industrial components. A hierarchical architecture based on a SIEMENS S 7 - 400 master programmable logic controller interconnected through PROFIBUS-DP to S 7 - 300 deported and decentralised I/Os has been implemented. Integration of in-house specific G- 64 hardware systems inside this industrial environment has been done through a PROFIBUS-DP to G- 64 intelligent interface based on an OEM fieldbus <b>mezzanine</b> <b>board</b> on one side and an FPGA implementing the required functionality on the other. Simultaneously, the fast timing system has been completely reshuffled in order to provide the required SPS multi-cycling functionality and a synchronisation of the 16 magnets to 5 ns. This modular architecture has been successfully integrated inside the new SPS accelerator control infrastructure and will be duplicated in the future for the control of the different SPS extraction channels. ...|$|E
40|$|The Gas Electron Multiplier (GEM) upgrade project aims at {{improving}} {{the performance of}} the muon spectrometer of the Compact Muon Solenoid (CMS) experiment which will suffer from the increase in luminosity of the Large Hadron Collider (LHC). The GEM collaboration proposes to instrument the first muon station with Triple-GEM detectors, a technology which has proven to be resistant to high fluxes of particles. The architecture of the readout system is based {{on the use of the}} microTCA standard hosting FPGA-based Advanced Mezzanine Card (AMC) and of the Versatile Link with the GBT chipset to link the on-detector electronics to the micro-TCA boards. For the front-end electronics a new ASIC, called VFAT 3, is being developed. On the detector, a Xilinx Virtex- 6 FPGA <b>mezzanine</b> <b>board,</b> called the OptoHybrid, has to collect the data from 24 VFAT 3 s and to transmit the data optically to the off-detector micro-TCA electronics, as well as to transmit the trigger data at 40 MHz to the CMS Cathode Strip Chamber (CSC) trigger. The microTCA electronics provides the interfaces from the detector (and front-end electronics) to the CMS DAQ, TTC (Timing, Trigger and Control) and Trigger systems. In this paper, we will describe the DAQ system of the Triple-GEM project and provide results from the latest test beam campaigns done at CERN. SCOPUS: cp. jinfo:eu-repo/semantics/publishe...|$|E
40|$|We {{will present}} the {{electronic}} and DAQ system being developed for TripleGEM detectors which will be installed in the CMS muon spectrometer. The microTCA system uses an Advanced Mezzanine Card equipped with an FPGA and the Versatile Link with the GBT chipset to link the front and back-end. On the detector an FPGA <b>mezzanine</b> <b>board,</b> the OptoHybrid, has to collect {{the data from the}} detector readout chips to transmit them optically to the microTCA boards using the GBT protocol. We will describe the hardware architecture, report on the status of the developments, and present results obtained with the system. In this contribution we will report on the progress of the design of the electronic readout and data acquisition (DAQ) system being developed for Triple-GEM detectors which will be installed in the forward region (1. 5 < eta < 2. 2) of the CMS muon spectrometer during the 2 nd long shutdown of the LHC, planed for the period 2018 - 2019. The architecture of the Triple-GEM readout system is based {{on the use of the}} microTCA standard hosting FPGA-based Advanced Mezzanine Card (AMC) and of the Versatile Link with the GBT chipset to link the front-end electronics to the micro-TCA boards. For the on-detector electronics a new front-end ASIC, called VFAT 3, is being developed for the CMS Triple-GEM system. Its architecture is based on the TOTEM VFAT 2 chip which is currently used to test the CMS Triple-GEM prototypes and the new data acquisition system. On detector, a Xilinx Virtex- 6 FPGA <b>mezzanine</b> <b>board,</b> called the OptoHybrid, has to collect the data from 24 front-end chips and to transmit the data optically to the off-detector micro-TCA electronics as well as to transmit the trigger data at 40 MHz to the CMS Cathode Strip Chamber (CSC) trigger. Two versions of this OptoHybrid have already been designed. They are used to readout the CMS Triple-GEM prototypes equipped with VFAT 2 chips and both have been tested with beam at CERN. The microTCA electronics provides the interfaces from the detector (and front-end electronics) to the CMS DAQ, TTC (Timing, Trigger and Control) and Trigger systems. Each micro-TCA crate can house 12 AMC boards. Currently the GLIB board designed by CERN is used for the system developments. For the final system more powerful boards based on the Virtex- 7 or Kintex- 7 Xilinx FPGA are envisaged. During the LHC yearly extended technical stop of winter 2016 - 2017, 8 Triple-GEM detectors will be installed inside CMS. They will be read-out with the existing VFAT 2 chip and with the data acquisition system described above. To prepare this installation, called slice-test, a dedicated test bench has been set-up at CERN to integrate the GEM with the CSC electronics. This work also includes the development of the DAQ software based on xDAQ and of the detector control system. In this contribution we will describe the hardware architecture and expected performance, report on the status of the developments of the various electronic components and present preliminary results obtained with the microTCA-based readout system developed for the slice-test...|$|E
40|$|A total {{ionizing}} dose (TID) {{test of the}} MDT-ASD, the ATLAS MDT front-end chip [12][13] {{has been}} performed at the Harvard Cyclotron Lab. The MDT-ASD is an 8 -channel drift tube read-out ASIC fabricated in a commercial 0. 5 Pm CMOS process (AMOS 14 TB). The accumulated TID {{at the end of}} the test was 300 krad, delivered by 160 MeV protons at a rate of approximately 70 rad/sec. All 10 irradiated chips retained their full functionality and performance and showed only minuscule and completely insignificant changes in device parameters. As the total accumulated dose is substantially higher than the relevant ATLAS Radiation Tolerance Criteria (RTCtid) [3], the results of this test indicate that MDT-ASD meets the ATLAS TID radiation hardness requirements. In addition, the results of this test correspond well with results of a 30 keV gamma TID irradiation test performed by us on an earlier prototype at the CERN x-ray facility as well as with results of other irradiation test on this process found in literature. 1. Radiation environment The MDT-ASD will reside and work in the Muon Spectrometer part of the ATLAS detector and thus will be exposed to the LHC radiation environment. The worst case location for the ATLAS MDT read-out electronics in terms of radiation levels is the middle end-cap (End-cap 2) close to the beam pipe. The Simulated Radiation Levels (SRLtid) and Safety Factors (SFtid) and the resulting Radiation Tolerance Criteria (RTCtid) are defined and issued by the ATLAS Radiation Hardness Assurance Working Group (RHA-WG) [1]. The relevant numbers for the <b>Mezzanine</b> <b>board</b> on which the MDT-ASD resides are given for the End-cap 2 innermost location in Table 1...|$|E
40|$|Abstract – Large LAr TPCs {{are among}} the most {{powerful}} detectors to address open problems in particle and astro-particle physics, such as CP violation in leptonic sector, neutrino properties and their astrophysical implications, proton decay search etc. The scale of such detector implies severe constraints on their readout and DAQ system. In this article we describe a data acquisition scheme for this new generation of large detectors. The main challenge is to propose a scalable and easy to use solution able to manage a large number of channels at the lowest cost. It {{is interesting to note that}} these constraints are very similar to those existing in Network Telecommunication Industry. We propose to study how emerging technologies like ATCA and µTCA could be used in neutrino experiments. We describe the design of an Advanced <b>Mezzanine</b> <b>Board</b> (AMC) including 32 ADC channels. This board receives 32 analogical channels at the front panel and sends the formatted data through the µTCA backplane using a Gigabit Ethernet link. The gigabit switch of the MCH is used to centralize and to send the data to the event building computer. The core of this card is a FPGA (ARIA-GX from ALTERA) including the whole system except the memories. A hardware accelerator has been implemented using a NIOS II µP and a Gigabit MAC IP. Obviously, in order to be able to reconstruct the tracks from the events a time synchronisation system is mandatory. We decided to implement the IEEE 1588 standard also called Precision Timing Protocol, another emerging and promising technology in Telecommunication Industry. In this article we describe a Gigabit PTP implementation using the recovered clock of the gigabit link. By doing so the drift is directly cancelled and the PTP will be used only to evaluate and to correct the offset. ...|$|E
40|$|This {{document}} specifies {{the prototype}} Clock and Control Board (CCB) for the CMS Endcap Muon electronics {{residing in the}} Sector Processor (SP) crates in the counting room. The standard set of modules in each crate consists of two SP and six Sector Receivers (SR). The general block diagram of the CCB is shown on Fig. 1. 1. TTC Interface The TTC interface {{is based on the}} TTCrx chip [1]. The prototype CCB includes connectors for the TTCrx <b>mezzanine</b> <b>board.</b> The general sequence of L 1 ACC, Reset and BX 0 commands is described in [2]. Particularly, for the Reset sequence, the TTC sends a broadcast command (either system, or user) to the TTCrx indicating that the next L 1 ACC has to be treated as a Reset. After this data is sent, a single L 1 ACC is generated. CCB decoding logic recognizes this command and treats the next incoming L 1 ACC as a RESET signal. After some predetermined interval the next broadcast command is transmitted over the TTC indicating that the next L 1 ACC should be treated as Bunch Crossing 0. In a similar fashion, CCB recognizes this command and treats the next L 1 ACC as a BX 0. After that CCB internal logic enables generation of L 1 A to backplane upon every L 1 ACC from TTC system. We use only one option when BrcstStr 2 and Brcst are synchronized to Clock 40 Des 1. We do not use an option when BrcstStr 2 and Brcst are synchronized to Clock 40 Des 2 The TTCrx chip can be programmed using an I 2 C interface and interface controller PCF 8584 over VME (see Fig. 2). This controller is located on CCB board as well...|$|E
40|$|Talk {{presented}} at the 2009 Real Time Conference, Beijing, May ' 09, submitted to the proceedingsLarge LAr TPCs {{are among the most}} powerful detectors to address open problems in particle and astro-particle physics, such as CP violation in leptonic sector, neutrino properties and their astrophysical implications, proton decay search etc. The scale of such detector implies severe constraints on their readout and DAQ system. In this article we describe a data acquisition scheme for this new generation of large detectors. The main challenge is to propose a scalable and easy to use solution able to manage a large number of channels at the lowest cost. It {{is interesting to note that}} these constraints are very similar to those existing in Network Telecommunication Industry. We propose to study how emerging technologies like ATCA and µTCA could be used in neutrino experiments. We describe the design of an Advanced <b>Mezzanine</b> <b>Board</b> (AMC) including 32 ADC channels. This board receives 32 analogical channels at the front panel and sends the formatted data through the µTCA backplane using a Gigabit Ethernet link. The gigabit switch of the MCH is used to centralize and to send the data to the event building computer. The core of this card is a FPGA (ARIA-GX from ALTERA) including the whole system except the memories. A hardware accelerator has been implemented using a NIOS II µP and a Gigabit MAC IP. Obviously, in order to be able to reconstruct the tracks from the events a time synchronisation system is mandatory. We decided to implement the IEEE 1588 standard also called Precision Timing Protocol, another emerging and promising technology in Telecommunication Industry. In this article we describe a Gigabit PTP implementation using the recovered clock of the gigabit link. By doing so the drift is directly cancelled and the PTP will be used only to evaluate and to correct the offset...|$|E
40|$|The main {{goals of}} {{cryptography}} are the encryption of messages to render them unintelligible to third {{parties and the}} authentication of messages to certify {{that they have not}} been modified. These goals can be accomplished if the sender ("Alice") and recipient ("Bob") both possess a secret random binary digit (bit) known as "key". It is essential that Alice and Bob acquire the key material {{with a high level of}} confidence that any third party ("Eve") does not have even partial information about the random bit sequence. If Alice and Bob communicate solely through classical messages (as opposed to Quantum cryptography), it is impossible for them to generate a certifiably secret key. QKD are the new generation of cryptographic systems which allow two remote parties (Alice and Bob) to generate a secret key with privacy guaranteed by quantum mechanics. They generate a random key securely over an optical fiber connection (also known as Quantum channel). This random key is then used for encryption and decryption of confidential messages, which then can be sent in encrypted form over any non-secure communication channel. In this thesis, we study two fiber-based QKD systems namely "oneway" and "two-way". Both systems have their unique advantage which distinguish them to one another. In one-way, the complexity of the electronic system may reduce. However more attention has to be made on the optical setup due to the requirement of active compensation. In the two-way, the requirement of optical setup may reduce but the attention moves to the electronic system which requires precise and short pulse especially for high speed in Alice system configuration. Our developed prototype is capable to support either one-way and two-way QKD system. We also solved some of the issues from the previous prototype Kumar [2008] which limited the system to be used in high speed. For instance: the synchronization system now uses a single synchronization signal per frame; the frame initialization time delay is reduced to 140 ms per frame; pulse shaping distortion due to current consumption. We also introduced the security perspective for B 92 protocol with uninformative states. This is done by utilizing the security analysis for BB 84 such as entanglement distillation protocol (EDP) [ [...] . ], smooth Ra'©nyi entropy [ [...] . ] and composable security [ [...] . ]. Numerous proposal on smooth Ra'©nyi entropy as general case [ [...] . ] either for finite security analysis [ [...] . ] or in asymptotic limit [ [...] . ] assist us to deduce finite security perspective for B 92 with uninformative states. [ [...] . ] This thesis is organized as follow, initially starts with a general introduction to the cryptography and its relation with quantum cryptography. This is elaborated in Chapter One. In the Second Chapter we will go through the background of quantum mechanics and quantum information and introduce some parameters and theory mostly used in Quantum Key Distribution. These include quantum measurement, state behavior and the security analysis parameters. The second chapter will give the background concepts for the QKD in perspective of quantum information. The Third Chapter will explore more detailed information towards QKD. It starts with the basic architecture algorithm of the quantum cryptography system and details each components of the architecture. Later we focus on the security analysis specifically for the B 92 protocol. Finally in the chapter, we will make some finite element analysis for the B 92 protocol with uninformative states. In Chapter Four and onwards, we are discussing the experimental implementation and analysis. We begin with the heart of our system which is the field programmable gate array (FPGA) system. We explain the detailed architecture of our FPGA system and how the system works. The module that we develop in our FPGA system in order to work inside the QKD system is also explained in detail. We reserve the modification and advanced work of this system for the future by giving the original codes in the Appendix A. Chapter Five is one of the shortest chapters in this thesis. This chapter explains our electronic development and the opto-electronic device which are used in the QKD system. The main device that we develop for the QKD system is our <b>Mezzanine</b> <b>board</b> for FPGA. This <b>mezzanine</b> <b>board</b> supports some functions that are not available from the FPGA in order to make the system functional. Other developments include opto-electronic board and proportional-integral-derivative (PID) controller with the current driver. Chapter Six is the main experimental part. In this Chapter we start to give the introduction to our optical setup. We detail out our configuration of the setup and finally show our results taken from experimental work. Finally in Chapter Seven we conclude our work and purposed future work which actually need to done for the system...|$|E
