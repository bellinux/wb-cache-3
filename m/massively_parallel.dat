7246|37|Public
5|$|Large-scale Atomic/Molecular <b>Massively</b> <b>Parallel</b> Simulator (LAMMPS) is a {{molecular}} dynamics code that simulates particles across {{a range of}} scales, from quantum to relativistic, to improve materials science with potential applications in semi-conductor, biomolecule and polymer development.|$|E
5|$|Fermi, {{learning}} of Ulam's breakthrough, devised an analog computer {{known as the}} Monte Carlo trolley, later dubbed the FERMIAC. The device performed a mechanical simulation of random diffusion of neutrons. As computers improved in speed and programmability, these methods became more useful. In particular, many Monte Carlo calculations carried out on modern <b>massively</b> <b>parallel</b> supercomputers are embarrassingly parallel applications, whose results can be very accurate.|$|E
5|$|In 2005 Stephan C. Schuster at Penn State University {{and colleagues}} {{published}} the first sequences {{of an environmental}} sample generated with high-throughput sequencing, in this case <b>massively</b> <b>parallel</b> pyrosequencing developed by 454 Life Sciences. Another early paper in this area appeared in 2006 by Robert Edwards, Forest Rohwer, and colleagues at San Diego State University.|$|E
5|$|The {{system is}} workload-optimized, {{integrating}} <b>massively</b> <b>parallel</b> POWER7 processors and built on IBM's DeepQA technology, which it uses to generate hypotheses, gather massive evidence, and analyze data. Watson employs {{a cluster of}} ninety IBM Power 750 servers, each of which uses a 3.5GHz POWER7 eight-core processor, with four threads per core. In total, the system has 2,880 POWER7 processor threads and 16 terabytes of RAM.|$|E
5|$|In {{the early}} 1970s, at the MIT Computer Science and Artificial Intelligence Laboratory, Marvin Minsky and Seymour Papert started {{developing}} {{what came to}} be known as the Society of Mind theory which views biological brain as <b>massively</b> <b>parallel</b> computer. In 1986, Minsky published The Society of Mind, which claims that “mind is formed from many little agents, each mindless by itself”. The theory attempts to explain how what we call intelligence could be a product of the interaction of non-intelligent parts. Minsky says that the biggest source of ideas about the theory came from his work in trying to create a machine that uses a robotic arm, a video camera, and a computer to build with children's blocks.|$|E
5|$|The first metagenomic studies {{conducted}} using high-throughput sequencing used <b>massively</b> <b>parallel</b> 454 pyrosequencing. Three other technologies commonly applied to environmental sampling are the Ion Torrent Personal Genome Machine, the Illumina MiSeq or HiSeq and the Applied Biosystems SOLiD system. These techniques for sequencing DNA generate shorter fragments than Sanger sequencing; Ion Torrent PGM System and 454 pyrosequencing typically produces ~400bp reads, Illumina MiSeq produces 400-700bp reads (depending on whether paired end options are used), and SOLiD produce 25-75bp reads. Historically, these read lengths were significantly {{shorter than the}} typical Sanger sequencing read length of ~750bp, however the Illumina technology is quickly coming close to this benchmark. However, this limitation is compensated for by the much larger number of sequence reads. In 2009, pyrosequenced metagenomes generate 200–500megabases, and Illumina platforms generate around 20–50gigabases, but these outputs have increased by orders of magnitude in recent years. An additional advantage to high throughput sequencing is that this technique does not require cloning the DNA before sequencing, removing {{one of the main}} biases and bottlenecks in environmental sampling.|$|E
5|$|Two {{types of}} {{analysis}} {{are used in}} the bioprospecting of metagenomic data: function-driven screening for an expressed trait, and sequence-driven screening for DNA sequences of interest. Function-driven analysis seeks to identify clones expressing a desired trait or useful activity, followed by biochemical characterization and sequence analysis. This approach is limited by availability of a suitable screen and the requirement that the desired trait be expressed in the host cell. Moreover, the low rate of discovery (less than one per 1,000 clones screened) and its labor-intensive nature further limit this approach. In contrast, sequence-driven analysis uses conserved DNA sequences to design PCR primers to screen clones for the sequence of interest. In comparison to cloning-based approaches, using a sequence-only approach further reduces the amount of bench work required. The application of <b>massively</b> <b>parallel</b> sequencing also greatly increases the amount of sequence data generated, which require high-throughput bioinformatic analysis pipelines. The sequence-driven approach to screening is limited by the breadth and accuracy of gene functions present in public sequence databases. In practice, experiments make use of a combination of both functional and sequence-based approaches based upon the function of interest, the complexity of the sample to be screened, and other factors.|$|E
5|$|Regarding {{the primary}} {{function}} of conscious processing, a recurring idea in recent theories is that phenomenal states somehow integrate neural activities and information-processing {{that would otherwise}} be independent. This has been called the integration consensus. Another example has been proposed by Gerald Edelman called dynamic core hypothesis which puts emphasis on reentrant connections that reciprocally link areas of the brain in a <b>massively</b> <b>parallel</b> manner. Edelman also stresses the importance of the evolutionary emergence of higher-order consciousness in humans from the historically older trait of primary consciousness which humans share with non-human animals (see Neural correlates section above). These theories of integrative function present solutions to two classic problems associated with consciousness: differentiation and unity. They show how our conscious experience can discriminate between a virtually unlimited number of different possible scenes and details (differentiation) because it integrates those details from our sensory systems, while the integrative nature of consciousness in this view easily explains how our experience can seem unified as one whole despite all of these individual parts. However, it remains unspecified which kinds of information are integrated in a conscious manner and which kinds can be integrated without consciousness. Nor is it explained what specific causal role conscious integration plays, nor why the same functionality cannot be achieved without consciousness. Obviously not all kinds of information are capable of being disseminated consciously (e.g., neural activity related to vegetative functions, reflexes, unconscious motor programs, low-level perceptual analyses, etc.) and many kinds of information can be disseminated and combined with other kinds without consciousness, as in intersensory interactions such as the ventriloquism effect. Hence it remains unclear why any of it is conscious. For a review of the differences between conscious and unconscious integrations, see the article of E. Morsella.|$|E
25|$|Mitchell Resnick (1994), Turtles, Termites and Traffic Jams: Explorations in <b>Massively</b> <b>Parallel</b> Microworlds, Complex Adaptive Systems series, MIT Press.|$|E
25|$|Connectionist replies:Closely {{related to}} the brain {{simulator}} reply, this claims that a <b>massively</b> <b>parallel</b> connectionist architecture {{would be capable of}} understanding.|$|E
25|$|Because APL's core {{objects are}} arrays, it lends itself well to {{parallel}}ism, parallel computing, <b>massively</b> <b>parallel</b> applications, and very-large-scale integration or VLSI.|$|E
25|$|Analytics Platform System (APS): Formerly Parallel Data Warehouse (PDW) A <b>massively</b> <b>parallel</b> {{processing}} (MPP) SQL Server appliance {{optimized for}} large-scale data warehousing such {{as hundreds of}} terabytes.|$|E
25|$|Breakthroughs in {{high-performance}} computing, {{including the}} development of novel concepts for <b>massively</b> <b>parallel</b> computing and the design and application of computers that can carry out hundreds of trillions of operations per second.|$|E
25|$|The 21164 and 21264 {{processors}} {{were used}} by NetApp in various network-attached storage systems, while the 21064 and 21164 processors {{were used by}} Cray in their T3D and T3E <b>massively</b> <b>parallel</b> supercomputers.|$|E
25|$|Dial-out PCR: {{a highly}} {{parallel}} method for retrieving accurate DNA molecules for gene synthesis. A complex library of DNA molecules is modified with unique flanking tags before <b>massively</b> <b>parallel</b> sequencing. Tag-directed primers then enable the retrieval of molecules with desired sequences by PCR.|$|E
25|$|<b>Massively</b> <b>Parallel</b> Sequencing is a newer {{method that}} can be used to detect mutations. Using this method, up to 17 gigabases can be sequenced at once, as opposed to limited ranges for Sanger {{sequencing}} of only about 1 kilobase. Several technologies are available to perform this test and it is being looked at to be used in clinical applications. When testing for different carcinomas, current methods only allow for looking at one gene at a time. <b>Massively</b> <b>Parallel</b> Sequencing can test for a variety of cancer causing mutations at once as opposed to several specific tests. An experiment to determine the accuracy of this newer sequencing method tested for 21 genes and had no false positive calls for frameshift mutations.|$|E
25|$|Approaches to {{supercomputer}} architecture {{have taken}} dramatic turns since the earliest systems {{were introduced in}} the 1960s. Early supercomputer architectures pioneered by Seymour Cray relied on compact innovative designs and local parallelism to achieve superior computational peak performance. However, in time the demand for increased computational power ushered {{in the age of}} <b>massively</b> <b>parallel</b> systems.|$|E
25|$|Cray {{set up a}} new company, SRC Computers, {{and started}} the design of his own <b>massively</b> <b>parallel</b> machine. The new design {{concentrated}} on communications and memory performance, the bottleneck that hampered many parallel designs. Design had just started when Cray died suddenly {{as a result of a}} car accident. SRC Computers carried on development and specialized in reconfigurable computing.|$|E
25|$|Japan's {{entry into}} {{supercomputing}} {{began in the}} early 1980s. In 1982, Osaka University's LINKS-1 Computer Graphics System used a <b>massively</b> <b>parallel</b> processing architecture, with 514 microprocessors, including 257 Zilog Z8001 control processors and 257 iAPX 86/20 floating-point processors. It was mainly used for rendering realistic 3D computer graphics. It was the world's most powerful computer, as of 1984.|$|E
25|$|During the 1980s, as {{the demand}} for {{computing}} power increased, the trend to a much larger number of processors began, ushering {{in the age of}} <b>massively</b> <b>parallel</b> systems, with distributed memory and distributed file systems, given that shared memory architectures could not scale to a large number of processors. Hybrid approaches such as distributed shared memory also appeared after the early systems.|$|E
25|$|Advances in <b>massively</b> <b>parallel</b> {{sequencing}} {{has led to}} {{the development}} of RNA-Seq technology, that enables a whole transcriptome shotgun approach to characterize and quantify gene expression. Unlike microarrays, which need a reference genome and transcriptome to be available before the microarray itself can be designed, RNA-Seq can also be used for new model organisms whose genome has not been sequenced yet.|$|E
25|$|However, de novo {{resistance}} to endocrine therapy undermines {{the efficacy of}} using competitive inhibitors like tamoxifen. Hormone deprivation {{through the use of}} aromatase inhibitors is also rendered futile. <b>Massively</b> <b>parallel</b> genome sequencing has revealed the common presence of point mutations on ESR1 that are drivers for resistance, and promote the agonist conformation of ERα without the bound ligand. Such constitutive, estrogen-independent activity is driven by specific mutations, such as the D538G or Y537S/C/N mutations, in the ligand binding domain of ESR1 and promote cell proliferation and tumor progression without hormone stimulation.|$|E
25|$|Following {{the release}} of the Radeon R520 and GeForce G70 GPU cores with {{programmable}} shaders, the large floating-point throughput drew attention from academic and commercial groups, experimenting with using then for non-graphics work. The interest led ATI (and Nvidia) to create GPGPU products — able to calculate general purpose mathematical formulas in a <b>massively</b> <b>parallel</b> way — to process heavy calculations traditionally done on CPUs and specialized floating-point math co-processors. GPGPUs were projected to have immediate performance gains of a factor of 10 or more, over compared to contemporary multi-socket CPU-only calculation.|$|E
25|$|Systems with {{a massive}} number of {{processors}} generally take one of two paths: in one approach, e.g., in grid computing the processing power {{of a large number}} of computers in distributed, diverse administrative domains, is opportunistically used whenever a computer is available. In another approach, a large number of processors are used in close proximity to each other, e.g., in a computer cluster. In such a centralized <b>massively</b> <b>parallel</b> system the speed and flexibility of the interconnect becomes very important, and modern supercomputers have used various approaches ranging from enhanced Infiniband systems to three-dimensional torus interconnects.|$|E
25|$|On April 2, 1994, the {{factorization}} of RSA-129 {{was completed}} using QS. It was a 129-digit number, {{the product of}} two large primes, one of 64 digits and the other of 65. The factor base for this factorization contained 524339 primes. The data collection phase took 5000 MIPS-years, done in distributed fashion over the Internet. The data collected totaled 2GB. The data processing phase took 45 hours on Bellcore's (now Telcordia Technologies) MasPar (<b>massively</b> <b>parallel)</b> supercomputer. This was the largest published factorization by a general-purpose algorithm, until NFS was used to factor RSA-130, completed April 10, 1996. All RSA numbers factored since then have been factored using NFS.|$|E
25|$|One {{can perform}} {{high-throughput}} shotgun sequencing from plasma {{of pregnant women}} to obtain about 5 million sequence tags per patient sample. Fan et al. 2008, using this method, {{were able to identify}} aneuploid pregnancies, trisomy detected at gestational ages as early as 14th week. Shotgun sequencing can be done with a Solexa/Illumina platform. Whole fetal genome mapping by parental haplotype analysis using sequencing of cell free fetal DNA was done in 2010. Chiu et al. in 2010 studied 753 pregnant females, using a 2-plex <b>massively</b> <b>parallel</b> maternal plasma DNA sequencing and trisomy was diagnosed with z-score greater than 3. The test was 100% sensitivity, 97.9% specificity, positive predictive value of 96.6%, and negative predictive value of 100%.|$|E
25|$|RNA-Seq {{refers to}} the {{sequencing}} of transcript cDNAs, where abundance {{is derived from the}} number of counts from each transcript. The technique has therefore been heavily influenced by the development of high-throughput sequencing technologies. <b>Massively</b> <b>Parallel</b> Signature Sequencing (MPSS) was an early example based on generating 16-20bp sequences via a complex series of hybridisations, and was used in 2004 to validate the expression of 104 genes in Arabidopsis thaliana. The earliest RNA-Seq work was published in 2006 with 105 transcripts sequenced using the 454 technology. This was sufficient coverage to quantify relative transcript abundance. RNA-Seq began to increase in popularity after 2008 when new Solexa/Illumina technologies allowed 109 transcript sequences to be recorded. This yield is now sufficient for accurate quantitation of entire human transcriptomes.|$|E
25|$|Field Hockey is {{a popular}} sport in India. Until the mid-1970s, India {{dominated}} international field hockey, winning eight Olympic gold medals and won the men's Hockey World Cup held in 1975. Since then, barring {{a gold medal in}} the 1980 Olympics, India's performance in field hockey has been dismal, with other hockey-playing nations such as Australia, Netherlands and Germany improving their standards and catching up with India. Its decline is also due to the change in rules of the game, introduction of artificial turf, and internal politics in Indian field hockey bodies. The popularity of field hockey has also declined <b>massively</b> <b>parallel</b> to the decline of the Indian hockey team. In recent years, the standard of Indian hockey has gone from bad to worse, with the Indian hockey team not qualifying for the 2008 Olympics and finishing last in the 2012 Olympics. Currently, the Indian team is 5th in the rankings of the Fédération Internationale de Hockey sur Gazon (FIH, English:International Hockey Federation), the international governing body of field hockey and indoor field hockey.|$|E
25|$|Aneuploidy, {{which refers}} to {{abnormal}} number of chromosomes, can also be detected using non-invasive prenatal tests. Research previous shown an increase in quantity of cffDNA in maternal plasma for fetal trisomy 13 and trisomy 21, {{and it is not}} elevated in fetal trisomy 18. A number of fetal nucleic acid molecules derived from aneuploid chromosomes can be detected including SERPINEB2 mRNA, clad B, hypomethylated SERPINB5 from chromosome 18, placenta-specific 4 (PLAC4), hypermethylated holocarboxylase synthetase (HLCS) and c21orf105 mRNA from chromosome 12. With complete trisomy, the mRNA alleles in maternal plasma isn't the normal 1:1 ratio, but is in fact 2:1. Allelic ratios determined by epigenetic markers {{can also be used to}} detect the complete trisomies. Massive parallel sequencing and digital PCR for fetal aneuploidy detection can be used without restriction to fetal-specific nucleic acid molecules. Several cell-free fetal DNA and RNA technologies are under development to test a pregnancy for aneuploidy, mostly focusing on Down syndrome testing. Sampling of cffDNA from maternal blood for analysis by <b>massively</b> <b>parallel</b> sequencing (MPSS) is estimated to have a sensitivity of between 96 and 100%, and a specificity between 94 and 100% for detecting Down syndrome. It can be performed at 10 weeks of gestational age. One study in the United States estimated a false positive rate of 0.3% and a positive predictive value of 80% when using cffDNA to detect Down syndrome.|$|E
500|$|Similar models (which also view {{biological}} {{brain as}} <b>massively</b> <b>parallel</b> computer, i.e. {{the brain is}} made up of a constellation of independent or semi-independent agents) were also described by: ...|$|E
500|$|A <b>massively</b> <b>parallel</b> {{processor}} (MPP) is {{a single}} computer with many networked processors. MPPs have {{many of the same}} characteristics as clusters, but MPPs have specialized interconnect networks (whereas clusters use commodity hardware for networking). MPPs also tend to be larger than clusters, typically having [...] "far more" [...] than 100processors. In an MPP, [...] "each CPU contains its own memory and copy of the operating system and application. Each subsystem communicates with the others via a high-speed interconnect." ...|$|E
500|$|The {{failure of}} the Cray-3 was in large {{part due to the}} {{changing}} political and technical climate. The machine was being designed during the collapse of the Warsaw Pact and ending of the cold war, which led to a massive downsizing in supercomputer purchases. At the same time, the market was increasingly investing in <b>massively</b> <b>parallel</b> (MP or MPP) designs. Cray was critical of this approach, and was quoted by the Wall Street Journal as saying that MPP systems had not yet proven their supremacy over vector computers, noting the difficulty many users have had programming for large parallel machines. [...] "I don't think they'll ever be universally successful, at least not in my lifetime".|$|E
500|$|SIMD {{parallel}} computers {{can be traced}} back to the 1970s. The motivation behind early SIMD computers was to amortize the gate delay of the processor's control unit over multiple instructions. In 1964, Slotnick had proposed building a <b>massively</b> <b>parallel</b> computer for the Lawrence Livermore National Laboratory. His design was funded by the US Air Force, which was the earliest SIMD parallel-computing effort, ILLIAC IV. The key to its design was a fairly high parallelism, with up to 256processors, which allowed the machine to work on large datasets in what would later be known as vector processing. However, ILLIAC IV was called [...] "the most infamous of supercomputers", because the project was only one fourth completed, but took 11years and cost almost four times the original estimate. When it was finally ready to run its first real application in 1976, it was outperformed by existing commercial supercomputers such as the Cray-1.|$|E
2500|$|Anton – A specialized, <b>massively</b> <b>parallel</b> {{supercomputer}} {{designed to}} execute MD simulations ...|$|E
2500|$|MUMPS (MUltifrontal <b>Massively</b> <b>Parallel</b> sparse direct Solver), {{written in}} Fortran90, is a frontal solver ...|$|E
2500|$|Its {{centerpiece}} is an 18 rack Blue Gene /L and 2 rack Blue Gene/P <b>massively</b> <b>parallel</b> supercomputer ...|$|E
