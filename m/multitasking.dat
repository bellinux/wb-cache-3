3129|937|Public
5|$|Cooperative <b>multitasking</b> {{made its}} Macintosh debut in March 1985 {{with a program}} called Switcher by Andy Hertzfeld, which allowed the user to launch {{multiple}} applications and switch between them. Many programs and features did not function correctly with Switcher. Also, Switcher did not display windows of other applications beside the selected one. It was not included with the operating system, but was available from Apple for purchase separately. Both System 5 and System 6 had a feature called MultiFinder instead, which was much more mature and widely used in System 6. MultiFinder could be enabled or disabled, with a reboot. If disabled, the Finder would quit when the user launched another application, thus freeing RAM for it. If enabled, the system behaved as in the nowadays familiar <b>multitasking</b> tradition, with the desktop and windows of other applications in the screen's background.|$|E
5|$|Laubenstein {{was raised}} in Barrington, Rhode Island, where a {{childhood}} bout of polio left her paraplegic and using a wheelchair {{for the rest of}} her life. She studied at Barnard College and received her medical degree from New York University School of Medicine, where she specialized in hematology and oncology and went on to become a clinical professor before leaving to focus on treating AIDS patients in her private practice. In addition to her medical work, she was an outspoken AIDS activist and co-founded a non-profit organization, <b>Multitasking,</b> which provided employment to people with AIDS.|$|E
5|$|Most {{versions}} of Windows Mobile have a standard set of features, such as <b>multitasking</b> {{and the ability}} to navigate a file system similar to that of Windows 9x and Windows NT, including support for many of the same file types. Similarly to its desktop counterpart, it comes bundled with a set of applications that perform basic tasks. Internet Explorer Mobile is the default web browser, and Windows Media Player is the default media player used for playing digital media. The mobile version of Microsoft Office, is the default office suite.|$|E
40|$|<b>Multitask</b> Learning is an {{approach}} to inductive transfer that improves learning for one task by using {{the information contained in}} the training signals of other related tasks. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. In this thesis we demonstrate <b>multitask</b> learning for a dozen problems. We explain how <b>multitask</b> learning works and show that there are many opportunities for <b>multitask</b> learning in real domains. We show that in some cases features that would normally be used as inputs work better if used as <b>multitask</b> outputs instead. We present suggestions for how {{to get the most out}} of <b>multitask</b> learning in artificial neuralnets, present an algorithm for <b>multitask</b> learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for <b>multitask</b> learning in decision trees. <b>Multitask</b> learning improves generalization performance, can be applied i...|$|R
40|$|<b>Multitask</b> Learning is an {{inductive}} transfer {{method that}} improves generalization by using domain information {{implicit in the}} training signals of related tasks as an inductive bias. It does this by learning multiple tasks in parallel using a shared representation. <b>Multitask</b> transfer in connectionist nets has already been proven. But questions remain about how often training data for useful extra tasks will be available, and if <b>multitask</b> transfer will work in other learning methods. This paper argues that many real world problems present opportunities for <b>multitask</b> learning {{if they are not}} first overly sanitized. We present eight prototypical applications of <b>multitask</b> transfer where the training signals for related tasks are available and can be leveraged. We also outline algorithms for <b>multitask</b> transfer in decision trees and k-nearest neighbor. We conclude that <b>multitask</b> transfer has broad utility. 1 Introduction The goal of inductive transfer is to leverage additional sources of infor [...] ...|$|R
40|$|This paper {{presents}} a new <b>multitask</b> learning framework that learns a shared representation among the tasks, incorporating both task and feature clusters. The jointly-induced clusters yield a shared latent subspace where task relationships are learned more effectively and more generally than in state-of-the-art <b>multitask</b> learning methods. The proposed general framework enables the derivation of more specific or restricted state-of-the-art <b>multitask</b> methods. The paper also proposes a highly-scalable <b>multitask</b> learning algorithm, {{based on the}} new framework, using conjugate gradient descent and generalized Sylvester equations. Experimental results on synthetic and benchmark datasets show that the proposed method systematically outperforms several state-of-the-art <b>multitask</b> learning methods...|$|R
5|$|System 6 (also {{referred}} to as System Software 6) is a graphical user interface-based operating system for Macintosh computers. It was released in 1988 by Apple Computer and {{was part of the}} classic Mac OS line of operating systems. System 6 was shipped with various Macintosh computers until it was succeeded by System 7 in 1991. The boxed version of System 6 cost $49 when introduced. System 6 is classed as a monolithic operating system. It featured an improved MultiFinder, which allowed for co-operative <b>multitasking.</b>|$|E
5|$|It was {{confirmed}} on June 8, 2015, at Apple's WWDC that the iPad 2 would support iOS 9. This makes the iPad 2 the first iPad and only iOS device to support six major versions of iOS. As with previous releases, though, many headline features are unavailable on the iPad 2, including predictive Siri, translucency effects, split-view, slide-over and picture-in-picture <b>multitasking,</b> Low Power Mode and the Health app. iOS 9 {{is said to}} feature performance improvements that may help the aging device function more smoothly, and initial tests suggest {{that it did not}} significantly impact available space. Other A5-based devices will also support iOS 9 including the iPhone 4S (five major iOS versions), the iPad Mini (four major iOS versions) and iPod Touch 5G (four major iOS versions).|$|E
5|$|Parallel {{computing}} {{is closely}} related to concurrent computing—they are frequently used together, and often conflated, though the two are distinct: it is possible to have parallelism without concurrency (such as bit-level parallelism), and concurrency without parallelism (such as <b>multitasking</b> by time-sharing on a single-core CPU). In parallel computing, a computational task is typically broken down in several, often many, very similar subtasks that can be processed independently and whose results are combined afterwards, upon completion. In contrast, in concurrent computing, the various processes often do not address related tasks; when they do, as is typical in distributed computing, the separate tasks may have a varied nature and often require some inter-process communication during execution.|$|E
40|$|<b>Multitask</b> Learning is an {{approach}} to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate <b>multitask</b> learning in three domains. We explain how <b>multitask</b> learning works, and show {{that there are many}} opportunities for <b>multitask</b> learning in real domains. We present an algorithm and results for <b>multitask</b> learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for <b>multitask</b> learning in decision trees. Because <b>multitask</b> learning works, can [...] ...|$|R
40|$|Abstract. <b>Multitask</b> Learning is an {{approach}} to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate <b>multitask</b> learning in three domains. We explain how <b>multitask</b> learning works, and show {{that there are many}} opportunities for <b>multitask</b> learning in real domains. We present an algorithm and results for <b>multitask</b> learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for <b>multitask</b> learning in decision trees. Because <b>multitask</b> learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems...|$|R
40|$|A <b>multitask</b> {{learning}} framework is developed for discriminative classification and regression where multiple large-margin linear classifiers are estimated for different prediction problems. These classifiers {{operate in a}} common input space but are coupled as they recover an unknown shared representation. A maximum entropy discrimination (MED) framework is used to derive the <b>multitask</b> algorithm which involves only convex optimization problems that are straightforward to implement. Three <b>multitask</b> scenarios are described. The first <b>multitask</b> method produces multiple support vector machines that learn a shared sparse feature selection over the input space. The second <b>multitask</b> method produces multiple support vector machines that learn a shared conic kernel combination. The third <b>multitask</b> method produces a pooled classifier as well as adaptively specialized individual classifiers. Furthermore, extensions to regression, graphical model structure estimation and other sparse methods are discussed. The maximum entropy optimization problems are implemented via a sequential quadratic programming method which leverages recent progress in fast SVM solvers. Fast monotonic convergence bounds are provided by bounding the MED sparsifying cost function with a quadratic function and ensuring only a constant factor runtime increase above standard independent SVM solvers. Results are shown on <b>multitask</b> data sets and favor <b>multitask</b> learning over single-task or tabula rasa methods...|$|R
5|$|Cognitive deficits {{that can}} follow TBI include {{impaired}} attention; disrupted insight, judgement, and thought; reduced processing speed; distractibility; and deficits in executive {{functions such as}} abstract reasoning, planning, problem-solving, and <b>multitasking.</b> Memory loss, the most common cognitive impairment among head-injured people, occurs in 20–79% of people with closed head trauma, depending on severity. People who have suffered TBI may also have difficulty with understanding or producing spoken or written language, or with more subtle aspects of communication such as body language. Post-concussion syndrome, a set of lasting symptoms experienced after mild TBI, can include physical, cognitive, emotional and behavioral problems such as headaches, dizziness, difficulty concentrating, and depression. Multiple TBIs may have a cumulative effect. A young person who receives a second concussion before symptoms from another one have healed {{may be at risk}} for developing a very rare but deadly condition called second-impact syndrome, in which the brain swells catastrophically after even a mild blow, with debilitating or deadly results. About one in five career boxers is affected by chronic traumatic brain injury (CTBI), which causes cognitive, behavioral, and physical impairments. Dementia pugilistica, the severe form of CTBI, affects primarily career boxers years after a boxing career. It commonly manifests as dementia, memory problems, and parkinsonism (tremors and lack of coordination).|$|E
5|$|The {{developers}} {{tried to}} redefine Lara Croft's actions by questioning {{what they felt}} the character was capable of. While previous games used hand-animated movement for the character, Underworld introduced motion capture-based animation to display more fluid, realistic movement and facial expressions. Stuntwoman and Olympic Gold medalist Heidi Moneymaker was the motion capture actress, and advised the designers on practical movements. Animators adjusted and blended the recorded animation to create seamless transitions between the separate moves and their simultaneous combinations. The blends and additional animations give the character more flexible movement. Actions were overlapped to allow for <b>multitasking,</b> such as aiming at two separate targets and shooting {{with one hand while}} the other holds an object collected from the environment. Other additions include more melee attacks, as well as contextual offensive and climbing manoeuvres. Crystal Dynamics sought to make the visual appearance of the Xbox 360 and PlayStation 3 versions identical, although the systems use different techniques to achieve this. In response to Underworlds lackluster sales figures, Eidos reportedly considered altering the character's appearance to appeal more to female fans.|$|E
25|$|On many {{single user}} {{operating}} systems cooperative <b>multitasking</b> is perfectly adequate, as home computers generally run {{a small number}} of well tested programs. The AmigaOS is an exception, having preemptive <b>multitasking</b> from its very first version. Windows NT was the first version of Microsoft Windows which enforced preemptive <b>multitasking,</b> but it didn't reach the home user market until Windows XP (since Windows NT was targeted at professionals).|$|E
40|$|We propose <b>multitask</b> Laplacian {{learning}}, a {{new method}} for jointly learning clusters of closely related tasks. Unlike standard <b>multitask</b> methodologies, {{the graph of}} relations among the tasks is not assumed to be known a priori, but is learned by the <b>multitask</b> Laplacian algorithm. The algorithm builds on kernel based methods and exploits an optimization approach for learning a continuously parameterized kernel. It involves solving a semidefinite program of a particu- lar type, for which we develop an algorithm based on Douglas-Rachford split- ting methods. <b>Multitask</b> Laplacian learning can find application in many cases in which tasks are related {{with each other to}} varying degrees, some strongly, oth- ers weakly. Our experiments highlight such cases in which <b>multitask</b> Laplacian learning outperforms independent learning of tasks and state of the art <b>multitask</b> learning methods. In addition, they demonstrate that our algorithm partitions the tasks into clusters each of which contains well correlated tasks...|$|R
40|$|Massively <b>multitask</b> neural {{architectures}} {{provide a}} learning framework for drug discovery that synthesizes information from many distinct biological sources. To train these architectures at scale, we gather {{large amounts of}} data from public sources to create a dataset of nearly 40 million measurements across more than 200 biological targets. We investigate several aspects of the <b>multitask</b> framework by performing a series of empirical studies and obtain some interesting results: (1) massively <b>multitask</b> networks obtain predictive accuracies significantly better than single-task methods, (2) the predictive power of <b>multitask</b> networks improves as additional tasks and data are added, (3) {{the total amount of}} data and the total number of tasks both contribute significantly to <b>multitask</b> improvement, and (4) <b>multitask</b> networks afford limited transferability to tasks not in the training set. Our results underscore the need for greater data sharing and further algorithmic innovation to accelerate the drug discovery process. Comment: Preliminary work. Under review by the International Conference on Machine Learning (ICML...|$|R
40|$|<b>Multitask</b> {{learning}} is {{an approach to}} machine learning, in which algorithm learns to solve multiple related problems. It tries to find one common model instead of building multiple separate models. Such a model is usually smaller {{than the sum of}} separate models, easier to understand and less likely to overfit training data. In prediction stage the algorithm predicts values for several problems at the same time. Problems that are learned together must be related, so that learning of one problem can improve learning of other problems. Currently this approach is used with tree models for either multiple classification or multiple regression tasks. In this work we extend the approach to mixed classification and regression tasks. During construction of trees different attribute selection methods are used in regression and classification. The returned scores are not directly comparable, so in our scenario we rank attributes for each task and choose the attribute that is best ranked in total. We implement <b>multitask</b> regression and classification tree, <b>multitask</b> bagging and <b>multitask</b> random forest based on rankings of attributes. We compare these algorithms with their single task variants, with regular <b>multitask</b> tree and with <b>multitask</b> neural network. We propose task relatedness measure based on ranking of attributes. In this way we can find related tasks in a dataset and use them together in <b>multitask</b> approach. On one dataset implemented <b>multitask</b> random forest works statistically significantly better than single-task version. On some datasets implemented algorithms work worse than single-task versions...|$|R
25|$|The Control Center {{is visible}} in the <b>multitasking</b> window on iPads.|$|E
25|$|Several {{additional}} {{instructions were}} introduced in protected mode of 80286, which are helpful for <b>multitasking</b> operating systems.|$|E
25|$|Coroutines are {{functions}} that can yield control {{to each other}} - a form of co-operative <b>multitasking</b> without threads.|$|E
40|$|This paper {{introduces}} self-paced task {{selection to}} <b>multitask</b> learning, where instances from {{more closely related}} tasks are selected in a progression of easier-to-harder tasks, to emulate an effective human education strategy, but applied to <b>multitask</b> machine learning. We develop the mathematical foundation for {{the approach based on}} iterative selection of the most appropriate task, learning the task parameters, and updating the shared knowledge, optimizing a new bi-convex loss function. This proposed method applies quite generally, including to <b>multitask</b> feature learning, <b>multitask</b> learning with alternating structure optimization, etc. Results show that in each of the above formulations self-paced (easier-to-harder) task selection outperforms the baseline version of these methods in all the experiments...|$|R
40|$|Abstract—Protein {{subcellular}} localization {{is concerned}} with predicting {{the location of a}} protein within a cell using computational methods. The location information can indicate key functionalities of proteins. Thus, accurate prediction of subcellular localizations of proteins can help the prediction of protein functions and genome annotations, as well as the identification of drug targets. Machine learning methods such as Support Vector Machines (SVMs) have been used in the past for the problem of protein subcellular localization, but have been shown to suffer from a lack of annotated training data in each species under study. To overcome this data sparsity problem, we observe that because some of the organisms may be related to each other, there may be some commonalities across different organisms that can be discovered and used to help boost the data in each localization task. In this paper, we formulate protein subcellular localization problem as one of <b>multitask</b> learning across different organisms. We adapt and compare two specializations of the <b>multitask</b> learning algorithms on 20 different organisms. Our experimental results show that <b>multitask</b> learning performs much better than the traditional single-task methods. Among the different <b>multitask</b> learning methods, we found that the <b>multitask</b> kernels and supertype kernels under <b>multitask</b> learning that share parameters perform slightly better than <b>multitask</b> learning by sharing latent features. The most significant improvement in terms of localization accuracy is about 25 percent. We find that if the organisms are very different or are remotely related from a biological point of view, then jointly training the multiple models cannot lead to significant improvement. However, if they are closely related biologically, the <b>multitask</b> learning can do much better than individual learning. Index Terms—Protein subcellular localization; <b>multitask</b> learning. ...|$|R
40|$|<b>Multitask</b> {{clustering}} {{tries to}} improve the clustering performance of multiple tasks simultaneously by taking their relationship into account. Most existing <b>multitask</b> clustering algorithms fall into the type of generative clustering, and none are formulated as convex optimization problems. In this paper, we propose two convex Discriminative <b>Multitask</b> Clustering (DMTC) algorithms to address the problems. Specifically, we first propose a Bayesian DMTC framework. Then, we propose two convex DMTC objectives within the framework. The first one, which {{can be seen as}} a technical combination of the convex <b>multitask</b> feature learning and the convex Multiclass Maximum Margin Clustering (M 3 C), aims to learn a shared feature representation. The second one, which {{can be seen as a}} combination of the convex <b>multitask</b> relationship learning and M 3 C, aims to learn the task relationship. The two objectives are solved in a uniform procedure by the efficient cutting-plane algorithm. Experimental results on a toy problem and two benchmark datasets demonstrate the effectiveness of the proposed algorithms...|$|R
25|$|A single-tasking {{system can}} only run one {{program at a}} time, while a multi-tasking {{operating}} system allows more than one program to be running in concurrency. This is achieved by time-sharing, dividing the available processor time between multiple processes that are each interrupted repeatedly in time slices by a task-scheduling subsystem of the operating system. Multi-tasking may be characterized in preemptive and co-operative types. In preemptive <b>multitasking,</b> the operating system slices the CPU time and dedicates a slot {{to each of the}} programs. Unix-like operating systems, e.g., Solaris, Linux, as well as AmigaOS support preemptive <b>multitasking.</b> Cooperative <b>multitasking</b> is achieved by relying on each process to provide time to the other processes in a defined manner. 16-bit versions of Microsoft Windows used cooperative multi-tasking. 32-bit versions of both Windows NT and Win9x, used preemptive multi-tasking.|$|E
25|$|Consumer Reports {{named the}} S4 {{as the top}} {{smartphone}} as of May 2013 due to its screen quality, <b>multitasking</b> support, and built-in IR blaster.|$|E
25|$|Introduction of {{operating}} systems with preemptive <b>multitasking</b> and windowing capabilities and menu-driven user interfaces (with Digital Research): MP/M, Concurrent CP/M, Concurrent DOS, DOS Plus, FlexOS, GEM, ViewMAX.|$|E
40|$|Abstract Background Quantitative structure-activity {{relationship}} (QSAR) models have become popular tools {{to help identify}} promising lead compounds in anticancer drug development. Few QSAR studies have investigated <b>multitask</b> learning, however. <b>Multitask</b> learning is an approach that allows distinct but related data sets {{to be used in}} training. In this paper, a suite of three QSAR models is developed to identify compounds that are likely to (a) exhibit cytotoxic behavior against cancer cells, (b) exhibit high rat LD 50 values (low systemic toxicity), and (c) exhibit low to modest human oral clearance (favorable pharmacokinetic characteristics). Models were constructed using Kernel <b>Multitask</b> Latent Analysis (KMLA), an approach that can effectively handle a large number of correlated data features, nonlinear relationships between features and responses, and <b>multitask</b> learning. <b>Multitask</b> learning is particularly useful when the number of available training records is small relative to the number of features, {{as was the case with}} the oral clearance data. Results <b>Multitask</b> learning modestly but significantly improved the classification precision for the oral clearance model. For the cytotoxicity model, which was constructed using a large number of records, <b>multitask</b> learning did not affect precision but did reduce computation time. The models developed here were used to predict activities for 115, 000 natural compounds. Hundreds of natural compounds, particularly in the anthraquinone and flavonoids groups, were predicted to be cytotoxic, have high LD 50 values, and have low to moderate oral clearance. Conclusion <b>Multitask</b> learning can be useful in some QSAR models. A suite of QSAR models was constructed and used to screen a large drug library for compounds likely to be cytotoxic to multiple cancer cell lines in vitro, have low systemic toxicity in rats, and have favorable pharmacokinetic properties in humans. </p...|$|R
30|$|In <b>multitask</b> organizations, the {{incentives}} are “low-powered” (Holmstrom and Milgrom 1991). “High-powered” relative performance incentives {{are suitable for}} lower-level employees with simple tasks rather than higher-level employees who have <b>multitasks.</b> Many research works focus on political elites in the Chinese government; although reasonable, this focus leaves many problems unaddressed.|$|R
40|$|We {{discuss a}} general method to learn data {{representations}} from multiple tasks. We provide {{a justification for}} this method in both settings of <b>multitask</b> learning and learning-to-learn. The method is illustrated in detail in the special case of linear feature learning. Conditions on the theoretical advantage offered by <b>multitask</b> representation learning over independent task learning are established. In particular, focusing on the important example of half-space learning, we derive the regime in which <b>multitask</b> representation learning is beneficial over independent task learning, {{as a function of}} the sample size, the number of tasks and the intrinsic data dimensionality. Other potential applications of our results include <b>multitask</b> feature learning in reproducing kernel Hilbert spaces and multilayer, deep networks. Comment: To appear in Journal of Machine Learning Research (JMLR). 31 page...|$|R
25|$|Mutual {{recursion}} {{can also}} implement a finite-state machine, with one function for each state, and single recursion in changing state; this requires tail call optimization {{if the number}} of state changes is large or unbounded. This {{can be used as a}} simple form of cooperative <b>multitasking.</b> A similar approach to <b>multitasking</b> is to instead use coroutines which call each other, where rather than terminating by calling another routine, one coroutine yields to another but does not terminate, and then resumes execution when it is yielded back to. This allows individual coroutines to hold state, without it needing to be passed by parameters or stored in shared variables.|$|E
25|$|PCBoard {{supported}} the 16C550 UARTS (universal asynchronous receiver transmitter), such as 16550 UART ("Fifo"), 16554 UART and 16650 UART, {{which made it}} possible to run multiple nodes of the BBS on a single (<b>multitasking)</b> computer using either using IBM OS/2 or the DOS <b>multitasking</b> tool DESQview in combination with the memory manager QEMM. Some sysops tried to run PCBoard on the (then) new Windows 95 operating system by Microsoft and reported mixed results. Stability was critical for a BBS, which was usually running 24/7, and the early version of the Microsoft 32-bit operating system lacked it. Windows 95 was never officially supported by CDC.|$|E
25|$|The Amiga <b>multitasking</b> kernel {{was also}} one of the first to {{implement}} a microkernel OS methodology based on a real-time message passing (inter-process communication) core known as Exec (for executive) with dynamically loaded libraries and devices as optional modules around the core.|$|E
40|$|Abstract—Multitask {{clustering}} {{tries to}} improve the clustering performance of multiple tasks simultaneously by taking their relationship into account. Most existing <b>multitask</b> clustering algorithms fall into the type of generative clustering, and none are formulated as convex optimization problems. In this paper, we propose two convex Discriminative <b>Multitask</b> Clustering (DMTC) algorithms to address the problems. Specifically, we first propose a Bayesian DMTC framework. Then, we propose two convex DMTC objectives within the framework. The first one, which {{can be seen as}} a technical combination of the convex <b>multitask</b> feature learning and the convex Multiclass Maximum Margin Clustering (M 3 C), aims to learn a shared feature representation. The second one, which {{can be seen as a}} combination of the convex <b>multitask</b> relationship learning and M 3 C, aims to learn the task relationship. The two objectives are solved in a uniform procedure by the efficient cutting-plane algorithm. Experimental results on a toy problem and two benchmark datasets demonstrate the effectiveness of the proposed algorithms. Index Terms—Convex optimization, cutting-plane algorithm, discriminative clustering, unsupervised <b>multitask</b> learning F...|$|R
40|$|Deep <b>multitask</b> networks, {{in which}} one neural network {{produces}} multiple predictive outputs, are more scalable and often better regularized than their single-task counterparts. Such advantages can potentially lead to gains in both speed and performance, but <b>multitask</b> networks are also difficult to train without finding the right balance between tasks. We present a novel gradient normalization (GradNorm) technique which automatically balances the <b>multitask</b> loss function by directly tuning the gradients to equalize task training rates. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting over single networks, static baselines, and other adaptive <b>multitask</b> loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter α. Thus, {{what was once a}} tedious search process which incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we hope to demonstrate that gradient manipulation affords us great control over the training dynamics of <b>multitask</b> networks and {{may be one of the}} keys to unlocking the potential of <b>multitask</b> learning. Comment: 10 pages, 6 figures, submitted to ICLR 2018. Revised to include more detail and for improved clarit...|$|R
40|$|Background: The lack of {{sufficient}} training data is the limiting factor for many Machine Learning applications in Computational Biology. If data {{is available for}} several different but related problem domains, <b>Multitask</b> Learning algorithms {{can be used to}} learn a model based on all available information. In Bioinformatics, many problems can be cast into the <b>Multitask</b> Learning scenario by incorporating data from several organisms. However, combining information from several tasks requires careful consideration of the degree of similarity between tasks. Our proposed method simultaneously learns or refines the similarity between tasks along with the <b>Multitask</b> Learning classifier. This is done by formulating the <b>Multitask</b> Learning problem as Multiple Kernel Learning, using the recently published q-Norm MKL algorithm. Results: We demonstrate the performance of our method on two problems from Computational Biology. First, we show that our method is able to improve performance on a splice site dataset with given hierarchical task structure by refining the task relationships. Second, we consider an MHC-I dataset, for which we assume no knowledge about the degree of task relatedness. Here, we are able to learn the task similarities ab initio along with the <b>Multitask</b> classifiers. In both cases, we outperform baseline methods that we compare against. Conclusions: We present a novel approach to <b>Multitask</b> Learning that is capable of learning task similarity alon...|$|R
