6764|210|Public
5|$|PyPy is a fast, {{compliant}} {{interpreter of}} Python 2.7 and 3.5. Its just-in-time compiler brings a significant speed improvement over CPython. A version {{taking advantage of}} <b>multi-core</b> processors using software transactional memory is being created.|$|E
5|$|Folding@home {{can use the}} {{parallel}} computing abilities of modern <b>multi-core</b> processors. The ability to use several CPU cores simultaneously allows completing the full simulation far faster. Working together, these CPU cores complete single work units proportionately faster than the standard uniprocessor client. This method is scientifically valuable because it enables much longer simulation trajectories to be performed in {{the same amount of}} time, and reduces the traditional difficulties of scaling a large simulation to many separate processors. A 2007 publication in the Journal of Molecular Biology relied on <b>multi-core</b> processing to simulate the folding of part of the villin protein approximately 10 times longer than was possible with a single-processor client, in agreement with experimental folding rates.|$|E
5|$|Parallel {{computers}} can be roughly classified {{according to the}} level at which the hardware supports parallelism, with <b>multi-core</b> and multi-processor computers having multiple processing elements within a single machine, while clusters, MPPs, and grids use multiple computers to work on the same task. Specialized parallel computer architectures are sometimes used alongside traditional processors, for accelerating specific tasks.|$|E
40|$|Challenges in the {{research}} and development of uniprocessors have led to the rise of <b>multi-cores.</b> However, <b>multi-cores</b> introduce new challenges to software development: applications will need to be developed using concurrent programming techniques and highly scalable to improve performance on successive generations of <b>multi-cores,</b> i. e. as the number of cores increases. Traditionally concurrent programming has employed locks to safeguard concurrent access to shared data, but these are known to be challenging to use, and only a minority of developers have the expertise to write correct, let alone highly scalable, lock-based code. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|In {{wireless}} sensor networks (WSNs), sensor nodes {{are deployed}} for collecting and analyzing data. These nodes use limited energy batteries for easy deployment and low cost. The use of limited energy batteries {{is closely related to}} the lifetime of the sensor nodes when using {{wireless sensor networks}}. Efficient-energy management is important to extending the lifetime of the sensor nodes. Most effort for improving power efficiency in tiny sensor nodes has focused mainly on reducing the power consumed during data transmission. However, recent emergence of sensor nodes equipped with <b>multi-cores</b> strongly requires attention to be given to the problem of reducing power consumption in <b>multi-cores.</b> In this paper, we propose an energy efficient scheduling method for sensor nodes supporting a uniform <b>multi-cores.</b> We extend the proposed T-Ler plane based scheduling for global optimal scheduling of a uniform <b>multi-cores</b> and multi-processors to enable power management using dynamic power management. In the proposed approach, processor selection for a scheduling and mapping method between the tasks and processors is proposed to efficiently utilize dynamic power management. Experiments show the effectiveness of the proposed approach compared to other existing methods...|$|R
40|$|In {{symbolic}} computation, polynomial multiplication is {{a fundamental}} operation akin to matrix multiplication in numerical computation. We present efficient implementation strategies for FFT-based dense polynomial multiplication targeting <b>multi-cores.</b> We show that balanced input data can maximize parallel speedup and minimize cache complexity for bivariate multiplication. However, unbalanced input data, which are common in symbolic computation, are challenging. We provide efficient techniques, that we call contraction and extension,to reduce multivariate (and univariate) multiplication to balanced bivariate multiplication. Our implementation in Cilk++ demonstrates good speedup on <b>multi-cores...</b>|$|R
5|$|CAPS {{entreprise}} and Pathscale {{are also}} coordinating {{their effort to}} make hybrid <b>multi-core</b> parallel programming (HMPP) directives an open standard called OpenHMPP. The OpenHMPP directive-based programming model offers a syntax to efficiently offload computations on hardware accelerators and to optimize data movement to/from the hardware memory. OpenHMPP directives describe remote procedure call (RPC) on an accelerator device (e.g. GPU) or more generally a set of cores. The directives annotate C or Fortran codes to describe two sets of functionalities: the offloading of procedures (denoted codelets) onto a remote device and the optimization of data transfers between the CPU main memory and the accelerator memory.|$|E
5|$|Development for {{multiple}} platforms is profitable, but difficult. Optimizations needed for one platform architecture {{do not necessarily}} translate to others. Individual platforms such as the Sega Genesis and PlayStation 3 are seen as difficult to develop for compared to their competitors, and developers are not yet fully accustomed to new technologies such as <b>multi-core</b> processors and hyper-threading. Multi-platform releases are increasingly common, but not all differences between editions on multiple platforms can be fully explained by hardware alone, and there remain franchise stalwarts that exist solely on one system. Developers for new platforms such as handheld and mobile systems {{do not have to}} operate under the pressure of $20 million budgets and the scrutiny of publishers' marketing experts.|$|E
5|$|Parallel {{computing}} {{is a type}} of computation {{in which}} many calculations or the execution of processes are carried out simultaneously. Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has been employed for many years, mainly in high-performance computing, but interest in it has grown lately due to the physical constraints preventing frequency scaling. As power consumption (and consequently heat generation) by computers has become a concern in recent years, parallel computing has become the dominant paradigm in computer architecture, mainly in the form of <b>multi-core</b> processors.|$|E
40|$|Single-ISA {{heterogeneous}} <b>multi-cores</b> {{consisting of}} small (e. g., in-order) and big (e. g., out-of-order) cores dramatically improve energy- and power-efficiency by scheduling workloads {{on the most}} appropriate core type. A significant body of recent work has focused on improving system throughput through scheduling. However, none of the prior work has looked into fairness. Yet, guaranteeing that all threads make equal progress on heterogeneous <b>multi-cores</b> is of utmost importance for both multi-threaded and multi-program workloads to improve performance and quality-of-service. Furthermore, modern operating systems affinitize workloads to cores (pinned scheduling) which dramatically affects fairness on heterogeneous <b>multi-cores.</b> In this paper, we propose fairness-aware scheduling for single-ISA heterogeneous <b>multi-cores,</b> and explore two flavors for doing so. Equal-time scheduling runs each thread or workload on each core type for an equal fraction of the time, whereas equal-progress scheduling strives at getting equal amounts of work done on each core type. Our experimental results demonstrate an average 14 % (and up to 25 %) performance improvement over pinned scheduling through fairness-aware scheduling for homogeneous multi-threaded workloads; equal-progress scheduling improves performance by 32 % on average for heterogeneous multi-threaded workloads. Further, we report dramatic improvements in fairness over prior scheduling proposals for multi-program workloads, while achieving system throughput comparable to throughput-optimized scheduling, and an average 21 % improvement in throughput over pinned scheduling...|$|R
40|$|Abstract — In {{symbolic}} computation, polynomial multiplication is {{a fundamental}} operation akin to matrix multiplication in numerical computation. We present efficient implementation strategies for FFT-based dense polynomial multiplication targeting <b>multi-cores.</b> We show that balanced input data can maximize parallel speedup and minimize cache complexity for bivariate multiplication. However, unbalanced input data, which are common in symbolic computation, are challenging. We provide efficient techniques, what we call contraction and extension, to reduce multivariate (and univariate) multiplication to balanced bivariate multiplication. Our implementation in Cilk++ demonstrates good speedup on <b>multi-cores.</b> Keywords- parallel symbolic computation; parallel polynomial multiplication; parallel multi-dimensional FFT/TFT; Cilk++; multi-core; I...|$|R
40|$|In {{the last}} few years, the {{traditional}} ways to keep the increase of hardware performance at the rate predicted by Moore's Law have vanished. When uni-cores were the norm, hardware design was decoupled from the software stack thanks to a well defined Instruction Set Architecture (ISA). This simple interface allowed developing applications without worrying {{too much about the}} underlying hardware, while hardware designers were able to aggressively exploit instruction-level parallelism (ILP) in superscalar processors. With the irruption of <b>multi-cores</b> and parallel applications, this simple interface started to leak. As a consequence, the role of decoupling again applications from the hardware was moved to the runtime system. Efficiently using the underlying hardware from this runtime without exposing its complexities to the application has been the target of very active and prolific research in the last years. Current <b>multi-cores</b> are designed as simple symmetric multiprocessors (SMP) on a chip. However, we believe that this is not enough to overcome all the problems that <b>multi-cores</b> already have to face. It is our position that the runtime has to drive the design of future <b>multi-cores</b> to overcome the restrictions in terms of power, memory, programmability and resilience that <b>multi-cores</b> have. In this paper, we introduce a first approach towards a Runtime-Aware Architecture (RAA), a massively parallel architecture designed from the runtime's perspective. This work has been partially supported by the European Research Council under the European Union’s 7 th FP, ERC Grant Agreement number 321253, by the Spanish Ministry of Science and Innovation under grant TIN 2012 - 34557 and by the HiPEAC Network of Excellence. M. Moreto has been partially supported by the Ministry of Economy and Competitiveness under Juan de la Cierva postdoctoral fellowship number JCI- 2012 - 15047, and M. Casas is supported by the Secretary for Universities and Research of the Ministry of Economy and Knowledge of the Government of Catalonia and the Co-fund programme of the Marie Curie Actions of the 7 th R&D Framework Programme of the European Union (Contract 2013 BP B 00243). Peer ReviewedPostprint (published version...|$|R
5|$|Perfect Dark Zero is {{also one}} of the first games that uses the Havok's HydraCore physics engine, which was {{specifically}} designed for <b>multi-core</b> video game systems such as the Xbox 360. The game's renderer engine employs more advanced graphic technologies than was possible in the previous console generation, including parallax mapping, ambient occlusion, subsurface scattering, and high dynamic range. The soundtrack of the game was primarily composed by David Clynick, who worked with Grant Kirkhope on the original Nintendo 64 game's score. New York based group MorissonPoe contributed two songs to the score, while DJs Kepi and Kat composed the game's nightclub theme.|$|E
5|$|The {{fluorides}} of tantalum can be {{used for}} its separation from niobium. Tantalum forms halogen compounds in the oxidation states of +5, +4, and +3 of the type , , and , although <b>multi-core</b> complexes and substoichiometric compounds are also known. Tantalum pentafluoride (TaF5) is a white solid with a melting point of 97.0°C and tantalum pentachloride (TaCl5) is a white solid with a melting point of 247.4°C. Tantalum pentachloride is hydrolyzed by water and reacts with additional tantalum at elevated temperatures by forming the black and highly hygroscopic tantalum tetrachloride (TaCl4). While the trihalides can be obtained by reduction of the pentahalides with hydrogen, the dihalides do not exist. A tantalum-tellurium alloy forms quasicrystals. Tantalum compounds with oxidation states as low as −1 have been reported in 2008.|$|E
5|$|The {{project has}} pioneered {{the use of}} {{graphics}} processing units (GPUs), PlayStation3s, Message Passing Interface (used for computing on <b>multi-core</b> processors), and some Sony Xperia smartphones for distributed computing and scientific research. The project uses statistical simulation methodology that is a paradigm shift from traditional computing methods. As part of the client–server model network architecture, the volunteered machines each receive pieces of a simulation (work units), complete them, and return them to the project's database servers, where the units are compiled into an overall simulation. Volunteers can track their contributions on the Folding@home website, which makes volunteers' participation competitive and encourages long-term involvement.|$|E
40|$|In {{the last}} few years, the {{traditional}} ways to keep the increase of hardware performance to the rate predicted by the Moore's Law have vanished. When uni-cores were the norm, hardware design was decoupled from the software stack thanks to a well defined Instruction Set Architecture (ISA). This simple interface allowed developing applications without worrying {{too much about the}} underlying hardware, while hardware designers were able to aggressively exploit instruction-level parallelism (ILP) in superscalar processors. With the irruption of <b>multi-cores</b> and parallel applications, this simple interface started to leak. As a consequence, the role of decoupling again applications from the hardware was moved to the runtime system. Efficiently using the underlying hardware from this runtime without exposing its complexities to the application has been the target of very active and prolific research in the last years. Current <b>multi-cores</b> are designed as simple symmetric multiprocessors (SMP) on a chip. However, we believe that this is not enough to overcome all the problems that <b>multi-cores</b> already have to face. It is our position that the runtime has to drive the design of future <b>multi-cores</b> to overcome the restrictions in terms of power, memory, programmability and resilience that <b>multi-cores</b> have. In this talk, we introduce a first approach towards a Runtime-Aware Architecture (RAA), a massively parallel architecture designed from the runtime's perspective. The BSC is the Spanish national supercomputing lab and does research into Computer, Life, Earth and Engineering Sciences. BSC provides supercomputing services to Spanish and European scientists and generates knowledge and technology to transfer to business and society. BSC is a Severo Ochoa Centre of Excellence and has a long record of carrying our excellent research through competitively funded projects, particularly in Europe and with industry. BSC works with the most important IT companies and with many others in different application areas, particularly energy and medicine. BSC tries to promote collaboration between Europe and Latin America and is committed to education and training in the field of HPC. Universidad de Málaga. Campus de Excelencia Internacional Andalucía Tech...|$|R
40|$|Graduation date: 2009 The Advent of <b>multi-cores</b> allows {{programs}} {{to be executed}} much faster than before. Cryptoalgorithms use long-bit words thus parallelizing these operations on <b>multi-cores</b> will achieve significant performance improvement. However, not all long-bit word operations in cryptosystems are suitable for parallel execution on <b>multi-cores.</b> In particular, long-bit words used in Elliptic Curves Cryptography (ECC) do not efficiently divide by the system word size. This causes some of the cores to be idle, which makes it vulnerable for attackers to guess how many operations occurred and thus what field size is being used. Multiplication {{is the most important}} part of public key cryptosystems. Long-bit word multiplication operations are needed for encryption and decryption. J. Fan et al. proposed using Montgomery multiplication on <b>multi-cores</b> using GF(2 ²⁵⁶) [25, 26], which is suitable for comput-er systems with 16 -bit or 32 -bit word size. Fan‟s Montgomery multiplication is suitable for most RSA. However, in ECC, some GFs will cause idle cores. For example, suppose GF(2 ¹³¹) is used (which is one of the recommended word size by NIST) on a quad-core with a 32 -bit word size, which requires [132 / 32] = 5 iterations with the last iteration requiring just a 3 -bit operation. This cause three of the cores to be idle during this time causing needless power consumption. The most general and the easiest way to make side channel attacks difficult is to insert dummy instructions to cover the idle processors. However, dummy instructions result in extra workloads that lead to performance degradation and increases in power consumption. In this thesis, we will present a multiplier adjuster technique to improve the execution time and the power consumption for the last unbalanced iteration. By appropriately applying dummy instructions between point-addition and point-doubling operations, a balanced point operation can be achieved in ECC. The performance and power-efficiency of the proposed method on <b>multi-cores</b> are analyzed for each GF used in ECC...|$|R
40|$|Future {{large-scale}} <b>multi-cores</b> {{will likely}} be best suited for use within high-performance computing (HPC) domains. A large fraction of HPC workloads employ the message-passing interface (MPI), yet <b>multi-cores</b> continue to be optimized for shared-memory workloads. In this position paper, we put forth {{the design of a}} unique chip that is optimized for MPI workloads. It introduces specialized hardware to optimize the transfer of messages between cores. It eliminates most aspects of on-chip cache coherence to not only reduce complexity and power, but also improve shared-memory producer-consumer behavior and the efficiency of buffer copies used during message transfers. We also consider two optimizations (caching of read-only and private blocks) that alleviate the negative performance effects of a coherence-free system...|$|R
5|$|A Folding@home {{participant}} installs {{a client}} program {{on their personal}} computer. The user interacts with the client, which manages the other software components in the background. Through the client, the user may pause the folding process, open an event log, check the work progress, or view personal statistics. The computer clients run continuously in the background at a very low priority, using idle processing power so that normal computer use is unaffected. The maximum CPU use can be adjusted via client settings. The client connects to a Folding@home server and retrieves a work unit and may also download the appropriate core for the client's settings, operating system, and the underlying hardware architecture. After processing, the work unit is returned to the Folding@home servers. Computer clients are tailored to uniprocessor and <b>multi-core</b> processor systems, and graphics processing units. The diversity and power of each hardware architecture provides Folding@home {{with the ability to}} efficiently complete many types of simulations in a timely manner (in a few weeks or months rather than years), which is of significant scientific value. Together, these clients allow researchers to study biomedical questions formerly considered impractical to tackle computationally.|$|E
25|$|The {{record for}} a <b>multi-core</b> fiber as of January 2013 was 1.05 petabits per second.|$|E
25|$|NEC and Corning Inc. {{develop a}} <b>multi-core</b> {{fiber optic cable}} that can {{transfer}} a record-breaking petabit of data per second.|$|E
40|$|Shared memory {{architectures}} {{are widely}} taking place. Following the structured parallel programming approach, a cost model is fundamental for performance portability and predictability. This thesis gives a contribution about cost models for multiprocessors and <b>multi-cores</b> {{taking into account}} important characteristics of new generation shared memory architectures, e. g. hierarchical shared memory, and with particular focus {{on the impact of}} the parallel application...|$|R
40|$|A fast ring finding {{algorithm}} {{is a crucial}} point to allow the use of RICH in on-line trigger selection. The present algorithms are either too slow (with respect to the incoming data rate) or need the information coming from a tracking system. Digital image techniques, assuming limited computing power (as for example Hough transform), are not perfectly robust for what concerns the noise immunity. We present a novel technique based on Ptolemy's theorem for multi-ring pattern recognition. Starting from purely geometrical considerations, this algorithm (also known as "Almagest") allows fast and trackless rings reconstruction, with spatial resolution comparable with other offline techniques. Almagest is particularly suitable for parallel implementation on <b>multi-cores</b> machines. Preliminary tests on GPUs (<b>multi-cores</b> video card processors) show that, thanks to an execution time smaller than 10 μs per event, this algorithm could be employed for on-line selection in trigger systems. The user case of the NA 62 RICH trigger, based on GPU, will be discussed...|$|R
40|$|Federated {{scheduling}} is {{a promising}} approach to schedule parallel real-time tasks on <b>multi-cores,</b> where each heavy task exclusively executes {{on a number}} of dedicated processors, while light tasks are treated as sequential sporadic tasks and share the remaining processors. However, federated scheduling suffers resource waste since a heavy task with processing capacity requirement x + ϵ (where x is an integer and 0 < ϵ < 1) needs x + 1 dedicated processors. In the extreme case, almost half of the processing capacity is wasted. In this paper we propose the semi-federate scheduling approach, which only grants x dedicated processors to a heavy task with processing capacity requirement x + ϵ, and schedules the remaining ϵ part together with light tasks on shared processors. Experiments with randomly generated task sets show the semi-federated scheduling approach significantly outperforms not only federated scheduling, but also all existing approaches for scheduling parallel real-time tasks on <b>multi-cores...</b>|$|R
25|$|Extensions for {{software}} parallelism (xSP), aimed at speeding up programs to enable multi-threaded and <b>multi-core</b> processing, announced in Technology Analyst Day 2007. One of the initiatives being discussed since August 2007 is the Light Weight Profiling (LWP), providing internal hardware monitor with runtimes, to observe information about executing process {{and help the}} re-design of software to be optimized with <b>multi-core</b> and even multi-threaded programs. Another one is the extension of Streaming SIMD Extension (SSE) instruction set, the SSE5.|$|E
25|$|Snow Leopard also {{featured}} new 64-bit technology capable of supporting greater amounts of RAM, improved support for <b>multi-core</b> processors through Grand Central Dispatch, and advanced GPU performance with OpenCL.|$|E
25|$|Cell is a <b>multi-core</b> {{microprocessor}} microarchitecture {{that combines}} a general-purpose Power Architecture core of modest performance with streamlined coprocessing elementss Multicore Architecture|publisher=IEEE|accessdate=2007-03-22|format=PDF}} which greatly accelerate multimedia and vector processing applications, {{as well as}} many other forms of dedicated computation.|$|E
40|$|Image {{reconstruction}} is {{a process}} of obtaining the original image from corrupted data. Applications of image reconstruction include Computer Tomography, radar imaging, weather forecasting etc. Recently steering kernel regression method has been applied for image reconstruction [1]. There are two major drawbacks in this technique. Firstly, it is computationally intensive. Secondly, output of the algorithm suffers form spurious edges (especially in case of denoising). We propose a modified version of Steering Kernel Regression called as Median Based Parallel Steering Kernel Regression Technique. In the proposed algorithm the first problem is overcome by implementing it in on GPUs and <b>multi-cores.</b> The second problem is addressed by a gradient based suppression in which median filter is used. Our algorithm gives better output than that of the Steering Kernel Regression. The results are compared using Root Mean Square Error(RMSE). Our algorithm has also shown a speedup of 21 x using GPUs and shown speedup of 6 x using <b>multi-cores...</b>|$|R
30|$|Work {{distribution}} {{is a fundamental}} step to assure a perfect exploitation of <b>multi-cores</b> architecture's potential. We'll start by recalling briefly some basic notion of distribution techniques then we introduce our minimal distribution approach that is particularly suitable for topological recursive algorithms where simple point characterization is necessary. Our approach is general and applicable to shared memory parallel machines. Critical cases are also introduced and discussed.|$|R
40|$|Abstract—This work {{proposed}} a {{design of a}} processor that unifies the execution of Graphic Processing Units and a general purpose processor. This design is evolved from a simple Graphic Processing softcore where all cores execute the same instruction. The discussion of programming model of vectorised instructions and the extension to allow <b>multi-cores</b> to run independently is presented. This design is suitable for embedded applications...|$|R
25|$|QuickTime 7.7.x on Windows {{fails to}} encode H.264 on <b>multi-core</b> systems {{with more than}} {{approximately}} 20 threads, e.g. HP Z820 with 2× 8-core CPUs. A suggested solution is to disable hyper-threading/limit CPU cores. Encoding speed and stability depends on the scaling of the player window.|$|E
25|$|<b>Multi-core</b> {{support is}} {{available}} with all versions of PSQL v11: 32- and 64-bit Windows and Linux Servers, and 32-bit Workgroup. Internal testing at Pervasive documented performance increases of 300% when comparing PSQL v10 to PSQL v11 on an 8-core server running Microsoft 2008 Enterprise Server SP2(64-bit).|$|E
25|$|Supreme Commander makes {{extensive}} use of two technologies relatively unused in video games prior to its release, namely multi core processing and multi monitor displays. When detecting a <b>multi-core</b> processor, the game assigns a specific task, such as AI calculations, to each core, splitting the load between them. Supreme Commander {{is one of the}} first games to specifically support dual and quad core processors in the game.|$|E
40|$|International audiencePower and {{programming}} challenges make heterogeneous <b>multi-cores</b> composed of cores and ASICs an attractive alternative to homogeneous <b>multi-cores.</b> Recently, multi-purpose loop-based generated accelerators {{have emerged as}} an especially attractive accelerator option. They have several assets: short design time (automatic generation), ﬂexibility (multi-purpose) but low conﬁguration and routing overhead (unlike FPGAs), computational performance (operations are directly mapped to hardware), and a focus on memory throughput by leveraging loop constructs. However, with multiple streams, the memory behavior of such accelerators can become at least as complex as that of superscalar processors, while they still need to retain the memory ordering predictability and throughput efﬁciency of DMAs. In this article, we show how to design a memory interface for multi-purpose accelerators which combines the ordering predictability of DMAs, retains key efﬁciency features of memory systems for complex processors, and requires {{only a fraction of}} their cost by leveraging the properties of streams references. We evaluate the approach with a synthesizable version of the memory interface for an example 9 -task generated loopbased accelerato...|$|R
30|$|Exploiting the {{knowledge}} about distinguishable operating modes {{in a system}} is tempting and thus modal systems are an increasingly popular subject in research. Traditionally, the research focused on single-processor systems [12] or, more recently, homogeneous bus-based <b>multi-cores</b> [13]. As the contemporary microcontrollers dedicated to the automotive industry, such as Infineon TriCore, follow these architecture principles, the schedulability analysis presented in [14] may be directly applied to them when modal applications are considered.|$|R
40|$|Abstract. This paper {{describes}} a new fast and implicitly parallel approach to neighbour-finding in multi-resolution Smoothed Particle Hydrodynamics (SPH) simulations. This new approach {{is based on}} hierarchical cell decompositions and sorted interactions, within a task-based formulation. It is shown to be faster than traditional tree-based codes, and to scale better than domain decomposition-based approaches on hybrid shared/distributed-memory parallel architectures, e. g. clusters of <b>multi-cores,</b> achieving a 40 × speedup over the Gadget- 2 simulation code...|$|R
