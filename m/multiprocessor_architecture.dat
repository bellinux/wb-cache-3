396|639|Public
5000|$|... #Subtitle level 3: Streaming <b>Multiprocessor</b> <b>Architecture</b> (SMX) ...|$|E
50|$|Sudhakar Yalamanchili {{from the}} Georgia Institute of Technology, Atlanta, GA was named Fellow of the Institute of Electrical and Electronics Engineers (IEEE) in 2014 for {{contributions}} to high-performance <b>multiprocessor</b> <b>architecture</b> and communication.|$|E
50|$|Babak Falsafi {{from the}} EPFL Ecole Polytechnique Federale de Lausanne, Lausanne, Switzerland was named Fellow of the Institute of Electrical and Electronics Engineers (IEEE) in 2012 for {{contributions}} to <b>multiprocessor</b> <b>architecture</b> and memory systems.|$|E
40|$|Abstract — In {{the field}} of {{distributed}} and parallel computing the common term is <b>multiprocessor</b> <b>architectures.</b> The <b>multiprocessor</b> <b>architectures</b> may have centralized shared memory or distributed shared memory. These systems may not be useful {{when the number of}} processors is large as the bandwidth on the memory becomes excessive and this produces a bottleneck. Significant reduction with the problem of memory bandwidth can be resolved by inclusion of large caches with processors. But inclusion of caches with processors creates the problem of cache coherence. This paper presents cache coherence problem and solution on it, in <b>multiprocessor</b> <b>architectures</b> with various protocols of cache coherence. It compares and discuses benefits and limitations of protocols. Keywords—Cache coherence, Distributed Share...|$|R
40|$|Directed graphs {{are widely}} used to model data flow and {{execution}} dependencies in streaming applications. This enables the utilization of graph partitioning algorithms for the problem of parallelizing computation for <b>multiprocessor</b> <b>architectures.</b> However due to resource restrictions, an acyclicity constraint on the partition is necessary when mapping streaming applications to an embedded multiprocessor. Here, we contribute a multi-level algorithm for the acyclic graph partitioning problem. Based on this, we engineer an evolutionary algorithm to further reduce communication cost, {{as well as to}} improve load balancing and the scheduling makespan on embedded <b>multiprocessor</b> <b>architectures...</b>|$|R
50|$|As of 2013, most <b>multiprocessor</b> <b>architectures</b> support CAS in hardware, and the compare-and-swap {{operation}} {{is the most}} popular synchronization primitive for implementing both lock-based and non-blocking concurrent data structures.|$|R
50|$|SmartSockets was {{the main}} product of Talarian. It is a {{real-time}} message-oriented middleware (MOM) which is scalable and fault tolerant. Its programming model is built specifically to offer high-speed interprocess communication (IPC) for <b>multiprocessor</b> <b>architecture,</b> scalability and reliability.|$|E
5000|$|Burroughs {{collaborated with}} University of Illinois on a <b>multiprocessor</b> <b>architecture</b> {{developing}} the ILLIAC IV {{computer in the}} early 1960s. The ILLIAC had up to 128 parallel processors while the B6700 & B7700 only accommodated a total of 7 CPUs and/or IO units (the 8th unit was the memory tester).|$|E
50|$|On today's machines, {{the layout}} of {{processors}} andmemory, {{the layout of}} data in memory, the communication load on thevarious elements of the <b>multiprocessor</b> <b>architecture</b> all influence performance.Furthermore, there is a tension between correctness and performance: algorithmic enhancements that seek to improve performance often {{make it more difficult}} to design and verify a correctdata structure implementation.|$|E
40|$|The Distributed Computing Column {{covers the}} theory of systems that are {{composed}} {{of a number of}} interacting computing elements. These include problems of communication and networking, databases, distributed shared memory, <b>multiprocessor</b> <b>architectures,</b> operating systems, veri cation, internet, and the web...|$|R
50|$|The {{family was}} {{announced}} on October 7, 1996.The project was code named Lego, and {{also known as}} SN0, to indicate {{the first in a}} series of scalable node architectures, contrasting with previous symmetric <b>multiprocessor</b> <b>architectures</b> in the SGI Challenge series.|$|R
40|$|The Distributed Computing Column {{covers the}} theory of systems that are {{composed}} {{of a number of}} interacting computing elements. These include problems of communication and networking, databases, distributed shared memory, <b>multiprocessor</b> <b>architectures,</b> operating systems, verification, Internet, and the Web. This issue consists of...|$|R
50|$|The cores of Knights Corner {{are based}} on a {{modified}} version of P54C design, used in the original Pentium. The basis of the Intel MIC architecture is to leverage x86 legacy by creating an x86-compatible <b>multiprocessor</b> <b>architecture</b> that can use existing parallelization software tools. Programming tools include OpenMP, OpenCL, Cilk/Cilk Plus and specialised versions of Intel's Fortran, C++ and math libraries.|$|E
50|$|Sun-4d: (D for Dragon, the codename of the SPARCcenter 2000) A {{high-end}} <b>multiprocessor</b> <b>architecture,</b> {{based on}} the XDBus processor interconnect, scalable up to 20 processors. The only Sun-4d systems produced by Sun were the SPARCserver 1000 and SPARCcenter 2000 series. The Cray CS6400 was also nominally a Sun-4d machine (sun4d6), although it required a custom version of Solaris. Supported by Solaris 2.2 to 8.|$|E
50|$|The {{project to}} produce a GPU retail product {{directly}} from the Larrabee research project was terminated in May 2010. The Intel MIC <b>multiprocessor</b> <b>architecture</b> announced in 2010 inherited many design elements from the Larrabee project, but does not function as a graphics processing unit; the product is intended as a co-processor for high performance computing. The prototype card was named Knights Ferry, a production card built at a 22 nm process named Knights Corner was planned for production in 2012 or later.|$|E
40|$|The base {{entity in}} {{computer}} programming {{is the process}} or task. The parallelism {{can be achieved by}} executing multiple processes on different processors. Distributed systems are managed by distributed operating systems that represent the extension for <b>multiprocessor</b> <b>architectures</b> of multitasking and multiprogramming operating systems. ...|$|R
40|$|Typical telecom {{applications}} apply a planar architecture pattern {{based on}} the processing requirements of each subsystem. In a symmetric multiprocessing environment all applications share the same hardware resources. However, currently embedded hardware platforms are being designed with asymmetric <b>multiprocessor</b> <b>architectures</b> to improve separation and increase performance of noninterfering tasks. These asymmetric <b>multiprocessor</b> <b>architectures</b> allow different planes to be separated and assign dedicated hardware for each responsibility. While planes are logically separated, some hardware is still shared and creates cross-plane influence effects which will impact {{the performance of the}} system. The aim of this report is to evaluate, in an embedded environment, the performance of a typical symmetric multiprocessing architecture compared to its asymmetric multiprocessing variant, applied on a telecom application...|$|R
40|$|Heterogeneous {{embedded}} <b>multiprocessor</b> <b>architectures</b> {{are becoming}} more prominent as a key design solution to today's microelectronics design problems. These application-specific architectures integrate multiple software programmable processors and dedicated hardwarecomponentstogetheron to a single cost-efficient IC. In contrast to general-purpose computer systems, embedded systems are designed and optimized to provide specific functionality, using possibly a combination of different classes of processors (e. g. DSPs, microcontrollers) from different vendors. While these customized heterogeneous <b>multiprocessor</b> <b>architectures</b> offer designers new possibilities to tradeoff programmability, processing performance, power dissipation, and design turnaround time, there is currently a lack of tools to support the programming of these architectures. In this paper, we consider the problem of providing real-time kernel support for managing the concurrent software tasks that are distributed over a set [...] ...|$|R
5000|$|The POWER7 superscalar {{symmetric}} <b>multiprocessor</b> <b>architecture</b> was {{a substantial}} evolution from the POWER6 design, focusing more on power efficiency through multiple cores and simultaneous multithreading (SMT). [...] The POWER6 architecture was built {{from the ground}} up to maximize processor frequency at the cost of power efficiency. It achieved a remarkable 5 GHz. While the POWER6 features a dual-core processor, each capable of two-way simultaneous multithreading (SMT), the IBM POWER 7 processor has up to eight cores, and four threads per core, for a total capacity of 32 simultaneous threads.|$|E
5000|$|The AN/UYK-7 was the {{standard}} 32-bit computer of the United States Navy for surface ship and submarine platforms, starting in 1970. It {{was used in}} the Navy's Aegis combat system and U.S. Coast Guard, and the navies of U.S. allies. [...] It was also used by the U.S. Army. [...] Built by UNISYS, it used integrated circuits, had 18-bit addressing and could support multiple CPUs and I/O controllers (three CPUs and two I/O controllers were a common configuration). Its <b>multiprocessor</b> <b>architecture</b> was based upon the UNIVAC 1108. [...] An airborne version, the UNIVAC 1832, was also produced.|$|E
5000|$|A {{parallel}} database system {{seeks to}} improve performance through parallelization of various operations, such as loading data, building indexes and evaluating queries. Although data may be stored in a distributed fashion, the distribution is governed solely by performance considerations. Parallel databases improve processing and input/output speeds by using multiple CPUs and disks in parallel. Centralized and client-server database systems are not powerful enough to handle such applications. In parallel processing, many operations are performed simultaneously, as opposed to serial processing, in which the computational steps are performed sequentially. Parallel databases can be roughly divided into two groups, {{the first group of}} architecture is the <b>multiprocessor</b> <b>architecture,</b> the alternatives of which are the following: ...|$|E
40|$|ABSTRACT: In {{this report}} {{we deal with}} a {{definition}} of the architecture modelling technique. We define representation, methods, and models related to the symbolic program exploration approach. We also show the reasons (motivation) why we choose this particular approach for exploration of different streaming <b>multiprocessor</b> <b>architectures...</b>|$|R
40|$|This chapter {{presents}} a generic <b>architecture</b> model for <b>multiprocessor</b> embedded system-on-chip design. The {{use of this}} model as a template in a system design environment allows for efficient generation of <b>multiprocessor</b> <b>architectures.</b> The key characteristics of this model are its great modularity, flexibility and scalability whic...|$|R
40|$|The Extended Hypercube {{is a new}} {{approach}} in <b>multiprocessor</b> <b>architectures,</b> which reduces the communication burden on the processor elements. We propose a scheme for implementing such an architecture using INMOS transputers as the processor and controller elements to achieve a very high computation to communication ratio...|$|R
50|$|Also in 1978, L. J. Sevins and Steve Goings from Mostek visited Brinch Hansen at USC, {{where he}} {{outlined}} a low-cost <b>multiprocessor</b> <b>architecture.</b> Mostek began {{a project to}} implement such a multiprocessor, with Brinch Hansen working as a consultant. Brinch Hansen developed a new concurrent programming language, Edison, for the project. As with the RC 4000 project, Edison was also used as a formal specification language for the hardware. Mostek got an initial 4-node multiprocessor working and Brinch Hansen wrote a portable Edison compiler on a PDP 11/55, but shortly after, United Technologies acquired Mostek and cancelled the project. In 1982, Brinch Hansen moved the Edison system to an IBM PC, and then published his third book, Programming a Personal Computer.|$|E
50|$|Larrabee is the codename for a {{cancelled}} GPGPU {{chip that}} Intel was developing separately {{from its current}} line of integrated graphics accelerators. It is named after Larrabee State Park in Whatcom County, Washington, {{near the town of}} Bellingham. The chip was to be released in 2010 as the core of a consumer 3D graphics card, but these plans were cancelled due to delays and disappointing early performance figures. The project to produce a GPU retail product directly from the Larrabee research project was terminated in May 2010. The Intel MIC <b>multiprocessor</b> <b>architecture</b> announced in 2010 inherited many design elements from the Larrabee project, but does not function as a graphics processing unit; the product is intended as a co-processor for high performance computing.|$|E
5000|$|The LEAP (Low Power Energy Aware Processing) {{has been}} {{developed}} by D. McIntire, K. Ho, B. Yip, A. Singh, W. Wu, and W.J. Kaiser at University of California Los Angeles {{to make sure the}} embedded network sensor systems are energy optimized for their applications. The LEAP system as described in reference [...] offers a detailed energy dissipation monitoring and sophisticated power control scheduling for all subsystems including the sensor systems. LEAP is a <b>multiprocessor</b> <b>architecture</b> based on hardware and software system partitioning. It is an independent energy monitoring and power control method for each individual subsystem. The goal of LEAP is to control microprocessors to achieve the lowest per task operating energy. Many modern embedded networked sensors are required to do many things like image processing, statistical high performance computing and communication. To make sure all of these applications are working efficiently a real-time energy monitoring and scheduling feature is required and LEAP can offer this feature for those systems.|$|E
40|$|Trace driven {{simulation}} is a {{well known}} technique for performance evaluation of single processor computers. However, trace driven simulation introduces distortions when used to simulate <b>multiprocessor</b> <b>architectures.</b> Execution driven simulation is the only technique that gives accurate simulation results for <b>multiprocessor</b> <b>architectures</b> though {{it is difficult to}} implement. This paper presents SPAM, a simulation kernel that simplifies the construction of execution driven simulators for shared memory multiprocessors. The kernel provides a tracing tool and a set of primitives which allow the execution, tracing and simulation of shared memory parallel applications on a single processor computer. The performance of the kernel allows the simulation of real sized parallel applications in a reasonnable time. Key-words: execution driven simulation, parallel address traces, shared memory multiprocessors (R'esum'e : tsvp) email :Alain. Gefflaut@irisa. fr, Philippe. Joubert@irisa. fr Centre Natio [...] ...|$|R
40|$|The growing {{complexity}} of electronic systems {{has resulted in}} the development of large <b>multiprocessor</b> <b>architectures.</b> Many advanced consumer products such as mobile phones, PDAs and media players are based on System on Chip (SoC) solutions. These solutions consist of a highly integrated chip and associated software. SoCs combine hardware IP core...|$|R
40|$|Abstract-This work {{faces the}} problem of the HW/SW {{co-design}} of dedicated digital electronic systems based on heterogeneous <b>multiprocessor</b> <b>architectures.</b> In particular, it describes the customization of a general system-level methodology to realize a prototypal SystemC-based co-design environment. The description of the adopted methodology and the related tools, supported by a reference case study, represents the core of the paper...|$|R
5000|$|The Kepler {{architecture}} {{employs a}} new Streaming <b>Multiprocessor</b> <b>Architecture</b> called [...] "SMX". SMXs {{are the reason}} for Kepler's power efficiency as the whole GPU uses a single unified clock speed. Although SMXs usage of a single unified clock increases power efficiency {{due to the fact}} that multiple lower clock Kepler CUDA Cores consume 90% less power than multiple higher clock Fermi CUDA Core, additional processing units are needed to execute a whole warp per cycle. Doubling 16 to 32 per CUDA array solve the warp execution problem, the SMX front-end are also double with warp schedulers, dispatch unit and the register file doubled to 64K entries as to feed the additional execution units. With the risk of inflating die area, SMX PolyMorph Engines are enhanced to 2.0 rather than double alongside the execution units, enabling it to spurr polygon in shorter cycles. Dedicated FP64 CUDA cores are also used as all Kepler CUDA cores are not FP64 capable to save die space. With the improvement Nvidia made on the SMX, the results include an increase in GPU performance and efficiency. With GK110, the 48KB texture cache are unlocked for compute workloads. In compute workload the texture cache becomes a read-only data cache, specializing in unaligned memory access workloads. Furthermore, error detection capabilities have been added to make it safer for workloads that rely on ECC. The register per thread count is also doubled in GK110 with 255 registers per thread.|$|E
5000|$|The Kepler {{architecture}} {{employs a}} new Streaming <b>Multiprocessor</b> <b>Architecture</b> called SMX. The SMX {{are the key}} method for Kepler's power efficiency as the whole GPU uses a single [...] "Core Clock" [...] rather than the double-pump [...] "Shader Clock". The SMX usage of a single unified clock increases the GPU power efficiency {{due to the fact}} that two Kepler CUDA Cores consume 90% power of one Fermi CUDA Core. Consequently, the SMX needs additional processing units to execute a whole warp per cycle. Kepler also needed to increase raw GPU performance as to remain competitive. As a result, it doubled the CUDA Cores from 16 to 32 per CUDA array, 3 CUDA Cores Array to 6 CUDA Cores Array, 1 load/store and 1 SFU group to 2 load/store and 2 SFU group. The GPU processing resources are also double. From 2 warp schedulers to 4 warp schedulers, 4 dispatch unit became 8 and the register file doubled to 64K entries as to increase performance. With the doubling of GPU processing units and resources increasing the usage of die spaces, The capability of the PolyMorph Engine aren't double but enhanced, making it capable of spurring out a polygon in 2 cycles instead of 4. With Kepler, Nvidia not only worked on power efficiency but also on area efficiency. Therefore, Nvidia opted to use eight dedicated FP64 CUDA cores in a SMX as to save die space, while still offering FP64 capabilities since all Kepler CUDA cores are not FP64 capable. With the improvement Nvidia made on Kepler, the results include an increase in GPU graphic performance while downplaying FP64 performance.|$|E
40|$|A {{fault-tolerant}} <b>multiprocessor</b> <b>architecture</b> is reported. This architecture, {{together with}} a comprehensive information system architecture, has important potential for future aircraft applications. A preliminary definition and assessment of a suitable <b>multiprocessor</b> <b>architecture</b> for such applications is developed...|$|E
40|$|Current {{simulators}} for shared-memory <b>multiprocessor</b> <b>architectures</b> {{involve a}} large tradeoff between simulation speed and accuracy. Most simulators assume much simpler processors {{than the current}} generation of processors that aggressively exploit instruction-level parallelism (ILP). This can result in large simulation inaccuracies. A few newer simulators model current ILP processors more accurately, but are about ten times slower...|$|R
40|$|This report {{consists}} of two papers that treat reliability of the multiprocessor systems at CMU. The first paper discusses the <b>multiprocessor</b> <b>architectures,</b> reliability features (hardware and software), and measured reliability data. The second paper presents hard failure data {{from one of the}} systems, calibrates a hard failure rate model, and analytically models the reliability of the three systems...|$|R
40|$|International audienceThis paper {{presents}} {{the design and}} analysis of multimedia applications such as the JPEG encoder on <b>multiprocessor</b> <b>architectures.</b> Abstract clocks are considered {{to deal with the}} correctness of system behaviors and to find the most suitable execution platform configurations regarding performance and energy consumption. Our approach offers a rapid and reliable design space analysis, which is crucial when implementing complex systems...|$|R
