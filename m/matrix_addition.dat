138|1293|Public
5|$|There are {{a number}} of basic {{operations}} that can be applied to modify matrices, called <b>matrix</b> <b>addition,</b> scalar multiplication, transposition, matrix multiplication, row operations, and submatrix.|$|E
25|$|The set of all n by n {{matrices}} over a given ring, with <b>matrix</b> <b>addition</b> or matrix multiplication as the operation.|$|E
25|$|Given {{again the}} finite-dimensional case, if bases have been chosen, then the {{composition}} of linear maps corresponds to the matrix multiplication, the addition of linear maps corresponds to the <b>matrix</b> <b>addition,</b> and the multiplication of linear maps with scalars corresponds to the multiplication of matrices with scalars.|$|E
40|$|A {{simple and}} direct pole-placement {{algorithm}} is introduced for dynamical systems having a block companion matrix A. The algorithm utilizes well-established properties of matrix polynomials. Pole placement {{is achieved by}} appropriately assigning coefficient matrices of the corresponding matrix polynomial. This involves only <b>matrix</b> <b>additions</b> and multiplications without requiring matrix inversion. A numerical example is given {{for the purpose of}} illustration...|$|R
40|$|By use of {{a simple}} identity, the product of two complex {{matrices}} can be formed with three real matrix multiplications and five real <b>matrix</b> <b>additions,</b> instead of the four real matrix multiplications and two real <b>matrix</b> <b>additions</b> required by the conventional approach. This alternative method reduces the number of arithmetic operations, even for small dimensions, achieving a saving of up to 25 %. The numerical stability of the method is investigated. The method {{is found to be}} less stable than conventional multiplication but stable enough to warrant practical use. Issues involved in the choice of method for complex matrix multiplication are discussed, including the relative efficiency of real and complex arithmetic and the backward stability of block algorithms. Key words. Matrix multiplication, complex matrix, Strassen's method, Winograd's identity, numerical stability, error analysis, level 3 BLAS. AMS subject classifications. primary 65 F 05, 65 G 05. 1. Introduction. How many real mult [...] ...|$|R
3000|$|... shrink(x,λ) is a {{nonlinear}} {{function that}} applies a soft-thresholding rule at level λ to the singular {{values of the}} input matrix. The key property here is that for large values of τ, the sequence Xk converges to a solution which very nearly minimizes (3). Hence, at each step, one only needs to compute at most one singular value decomposition and perform a few elementary <b>matrix</b> <b>additions.</b>|$|R
500|$|<b>Matrix</b> <b>addition</b> {{is defined}} for two {{matrices}} {{of the same}} dimensions. The sum of two m × n (pronounced [...] "m by n") matrices A and B, denoted by , is again an [...] matrix computed by adding corresponding elements: ...|$|E
500|$|Arthur Cayley {{published}} a treatise on geometric transformations using matrices {{that were not}} rotated versions of the coefficients being investigated as had previously been done. Instead he defined operations such as addition, subtraction, multiplication, and division as transformations of those matrices and showed the associative and distributive properties held true. Cayley investigated and demonstrated the non-commutative property of matrix multiplication {{as well as the}} commutative property of <b>matrix</b> <b>addition.</b> [...] Early matrix theory had limited the use of arrays almost exclusively to determinants and Arthur Cayley's abstract matrix operations were revolutionary. He was instrumental in proposing a matrix concept independent of equation systems. In 1858 Cayley published his A memoir on the theory of matrices in which he proposed and demonstrated the Cayley-Hamilton theorem.|$|E
2500|$|Figure 11 shows two {{identical}} such networks {{connected in}} series-series. [...] The total z-parameters predicted by <b>matrix</b> <b>addition</b> are; ...|$|E
30|$|The {{application}} of the generalized covariance <b>matrix</b> in <b>addition</b> to the Gibbs sampling really helps to consistently obtain high-quality results.|$|R
5000|$|Matrix: 3 {{editable}} tables, preset 2x2 and 3x3 identity <b>matrices,</b> <b>matrix</b> arithmetic (<b>addition,</b> subtraction, scalar/vector multiplication, matrix-vector multiplication (vector {{interpreted as}} column)) ...|$|R
40|$|This paper {{presents}} a highly efficient decomposition scheme {{and its associated}} Mathematica notebook {{for the analysis of}} complicated quantum circuits comprised of single/multiple qubit and qudit quantum gates. In particular, this scheme reduces the evaluation of multiple unitary gate operations with many conditionals to just two <b>matrix</b> <b>additions,</b> regardless of the number of conditionals or gate dimensions. This improves significantly the capability of a quantum circuit analyser implemented in a classical computer. This is also the first efficient quantum circuit analyser to include qudit quantum logic gates...|$|R
2500|$|<b>Matrix</b> <b>addition</b> and {{multiplication}} {{make the}} set of all n [...] n matrices into an associative algebra and hence there is a corresponding representation theory of associative algebras.|$|E
2500|$|Addition and {{multiplication}} of split-complex {{numbers are}} then given by <b>matrix</b> <b>addition</b> and multiplication. The modulus of z {{is given by}} the determinant of the corresponding matrix. In this representation, split-complex conjugation corresponds to multiplying on both sides by the matrix ...|$|E
2500|$|When two-ports are {{connected}} in a parallel-parallel configuration {{as shown in}} figure 13, the best choice of two-port parameter is the y-parameters. [...] The y-parameters of the combined network are found by <b>matrix</b> <b>addition</b> of the two individual y-parameter matrices.|$|E
40|$|This paper gives a brief {{description}} about some routing protocols like EEE LEACH, LEACH and DirectTransmission protocol (DTx) in Wireless Sensor Network (WSN) and a comparison study of theseprotocols based on some performance <b>matrices.</b> <b>Addition</b> to this an attempt is done to calculate theirtransmission time and throughput. To calculate these, MATLAB environment is used. Finally, on the basisof the obtained results from the simulation, the above mentioned three protocols are compared. Thecomparison results show that, the EEE LEACH routing protocol has a greater transmission time thanLEACH and DTx protocol and with smaller throughpu...|$|R
30|$|The {{enhancement}} in the ε´ of the NBR filled compounds {{could be}} due to the high polarity nature of NBR <b>matrix</b> in <b>addition</b> to effective filler dispersion within the matrix.|$|R
30|$|We give tight bounds for the {{logarithmic}} mean. We {{also give}} new Frobenius norm inequalities for two positive semidefinite <b>matrices.</b> In <b>addition,</b> we give some matrix inequalities on the matrix power mean.|$|R
2500|$|With the {{operations}} of <b>matrix</b> <b>addition</b> and matrix multiplication, this set satisfies the above ring axioms. The element [...] is the multiplicative identity of the ring. [...] If [...] and , then [...] while this example shows that the ring is noncommutative.|$|E
2500|$|For any ring R and any {{natural number}} n, {{the set of}} all square n-by-n {{matrices}} with entries from R, forms a ring with <b>matrix</b> <b>addition</b> and matrix multiplication as operations. For n = 1, this matrix ring is isomorphic to R itself. [...] For n > 1 (and R not the zero ring), this matrix ring is noncommutative.|$|E
2500|$|Three of {{the most}} {{fundamental}} operations which can be performed on square matrices are <b>matrix</b> <b>addition,</b> multiplication by a scalar, and matrix multiplication. [...] These are exactly those operations necessary for defining a polynomial function of an n × n matrix [...] [...] If we recall from basic calculus that many functions can be written as a Maclaurin series, then we can define more general functions of matrices quite easily. [...] If [...] is diagonalizable, that is ...|$|E
40|$|GPGPU ??????? ?? ????????? ???????? ??? ????????? (?????????, ????????, ???????? ?? ??????). ??????????? ???????????? ?????? ????????? ????????? ???????? ?? ?? ?? ?? ?????? ??????? ?????????. ??? ??? ?????? ???? ????????? ???????? ??????? ??????? ?????, ?????, ??? ????? ???? ???????? ?????????? ?? ?????????, ??? ? ??????? ?????????????? ??????????. ?????????, ??? ????????????????? ??? ??????????? ???????????? ????????, ?????????? ?? ?????????? ??? ????????, ? ????? ????????? ??????????? ?? ???????????. ?? ???????????? ????????? ??????????, ?? ???????????? ???????????? ???????? ??? ?????????, ???????? ????????, ???? ????, ??? ???? ????? ? ???????? ??? ?????? ????????? ???????????. This article {{discusses}} GPGPU {{approach to}} performing algebraic operations on <b>matrices</b> (<b>addition,</b> multiplication, multiplication of matrix and scalar). A comparative analysis of computation speed on CPU and GPU {{of the same}} price category was also performed. During the work several input data models were introduced to model calculations of low and high computation complecity. Algorithms that were applied for modeling the algebraic operations were based on those operations' definitions, and no additional optimizations were performed. Based on the algorithms performance results all conclusions about best circumstances for executing algorithms either on CPU or GPU were made. ? ?????? ??????????????? ?????????? GPGPU ??????? ? ?????????? ???????? ??? ????????? (????????, ????????????, ???????????? ?? ??????). ??????????? ????????????? ?????? ???????? ?????????? ???????? ?? ?? ? ?? ????? ??????? ?????????. ?? ????? ?????? ???? ????????? ????????? ??????? ??????? ??????, ?????, ????? ????? ???? ????????? ??????? ??? ?????????, ??? ? ??????? ?????????????? ?????????. ?????????, ??????? ??????????? ??? ????????????? ?????????????? ????????, ???????????? ?? ??????????? ???? ????????, ? ??????? ?????????????? ??????????? ?? ???????????. ?? ??????????? ?????????? ??????????, ??????????? ??????????? ???????? ??? ?????????, ??????? ?????? ?? ????????, ??? ??????? ?????? ?? ???????? ???? ????? ??????? ???????? ????????...|$|R
40|$|The Shannon {{capacity}} limit can {{be reached}} by less than 0. 27 dB at a BER of 10 ^- 5 by applying long but simple Hamming codes as component codes to an iterative `turbo'-decoding scheme. To our knowledge, such a performance has not been achieved by any other (de) coding scheme until now. In general, the complexity of soft-in/soft-out decoding of binary block codes is rather high. However, the application of a neurocomputer in combination with a parallelization of the decoding rule facilitates an implementation of the decoding algorithm in the logarithmic domain which requires only <b>matrix</b> <b>additions</b> and multiplications...|$|R
40|$|Strassen’s {{algorithm}} has practical performance {{benefits for}} architectures with simple memory hierarchies, because it trades computationally expensive matrix multiplications (MM) with cheaper <b>matrix</b> <b>additions</b> (MA). However, it presents no advantages for high-performance architectures with deep memory hierarchies, because MAs exploit limited data reuse. We present an easy-to-use adaptive algorithm combining Strassen’s recursion and high-tuned version of ATLAS MM. In fact, we introduce a last {{step in the}} ATLAS-installation process that determines whether Strassen’s may achieve any speedup. We present a recursive algorithm achieving up to 30 % speed-up versus ATLAS alone. We show experimental results for 14 different systems...|$|R
2500|$|... modules {{over these}} {{abstract}} algebraic structures. In essence, a representation makes an abstract algebraic object more concrete by describing its elements by matrices and the algebraic operations {{in terms of}} <b>matrix</b> <b>addition</b> and matrix multiplication. The algebraic objects amenable to such a description include groups, associative algebras and Lie algebras. The most prominent of these (and historically the first) is the representation theory of groups, in which elements of a group are represented by invertible matrices {{in such a way}} that the group operation is matrix multiplication.|$|E
2500|$|... {{together}} with the operations of ordinary <b>matrix</b> <b>addition</b> and multiplication of a matrix by a number, forms a vector space over the real numbers. The generators [...] form a basis set of V, and {{the components of the}} axis-angle and rapidity vectors, , are the coordinates of a Lorentz generator with respect to this basis. + yey + zez}} is expressed as a linear combination of the Cartesian unit vectors [...] which form a basis, and the Cartesian coordinates [...] are coordinates with respect to this basis.|$|E
2500|$|... modules {{over these}} {{abstract}} algebraic structures. In essence, a representation makes an abstract algebraic object more concrete by describing its elements by matrices and the algebraic operations {{in terms of}} <b>matrix</b> <b>addition</b> and matrix multiplication, which is non-commutative. The algebraic objects amenable to such a description include groups, associative algebras and Lie algebras. The most prominent of these (and historically the first) is the representation theory of groups, in which elements of a group are represented by invertible matrices {{in such a way}} that the group operation is matrix multiplication.|$|E
2500|$|... combination. [...] It has a <b>matrix</b> {{structure}} in <b>addition</b> to coordinate ...|$|R
30|$|The 2005 Fukushima Regional IO Table {{provides}} inverse <b>matrices</b> in <b>addition</b> {{to basic}} input–output matrices. However, this study uses new inverse matrices {{that are consistent}} with the trade and import coefficients derived from analysis.|$|R
30|$|Each {{transition}} {{is called a}} step. Any matrix satisfying Eqs. (2), (3) and (4) {{is referred to as}} a stochastic <b>matrix.</b> In <b>addition</b> if ∑_ip_ij = 1 then it is called a doubly stochastic matrix.|$|R
5000|$|Applying further {{simplification}} {{and basic}} rules of <b>matrix</b> <b>addition</b> yields ...|$|E
5000|$|Figure 11 shows two {{identical}} such networks {{connected in}} series-series. The total z-parameters predicted by <b>matrix</b> <b>addition</b> are; ...|$|E
50|$|Using that , {{it follows}} that {{addition}} and composition of matrices obey the usual rules for <b>matrix</b> <b>addition</b> and matrix multiplication.|$|E
50|$|For instance, {{consider}} <b>matrix</b> multiplication and <b>addition</b> in a sequential {{manner as}} discussed in the example.|$|R
40|$|Siloxane-containing {{addition}} polyimides yield toughened high-temperature adhesives and <b>matrix</b> resins. <b>Addition</b> polyimide made by {{reaction of}} aromatic tetracarboxylic acid dianhydride with aromatic diamine in presence of ethynyl-substituted aromatic monoamine. Acetylene-terminated siloxane imide cured by heating to yield acetylene-terminated polyimide siloxane...|$|R
50|$|Platinum Equity {{acquired}} {{the assets of}} Trinsic Communications (formerly Z-Tel) in June 2007 and integrated them into <b>Matrix,</b> This <b>addition</b> deepened the residential customer base within Matrix and broadened its national footprint of services and Trinsic, Powered by Matrix was born.|$|R
