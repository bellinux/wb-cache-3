28|7691|Public
40|$|The Kalkulator is a rapid ex-ante impact {{assessment}} tool with <b>minimum</b> <b>data</b> <b>requirements</b> that {{allows users to}} benchmark and explore multiple environmental impacts of interventions (e. g. productivity-enhancing agricultural practices, soil protection measures) on a farm using four indicators: greenhouse gas emissions, soil nitrogen balance, soil erosion, and on-farm calorie production...|$|E
40|$|Following the {{endorsement}} of the Millennium Development Goals, there is an increasing demand for methods to track poverty regularly. This paper develops an economically intuitive and inexpensive methodology {{to do so in the}} absence of regular, comparable data on household consumption. The <b>minimum</b> <b>data</b> <b>requirements</b> for the methodology are the availability of a household budget survey and a series of surveys with a comparable set of asset data also contained in the budget survey. The methodology is illustrated using a series of Demographic Health Surveys from Kenya...|$|E
40|$|Following the {{endorsement}} {{by the international}} community of the Millennium Development Goals, there has been an increasing demand for practical methods for steadily tracking poverty. An economically intuitive and inexpensive methodology is explored for doing so in the absence of regular, comparable data on household consumption. The <b>minimum</b> <b>data</b> <b>requirements</b> for this methodology are the availability of a household budget survey and a series of surveys with a comparable set of asset data also contained in the budget survey. This method is illustrated using a series of Demographic and Health Surveys for Kenya. Copyright The Author 2007. Published by Oxford University Press on behalf of the International Bank for Reconstruction and Development / the world bank. All rights reserved. For permissions, please e-mail: journals. permissions@oxfordjournals. org, Oxford University Press. ...|$|E
40|$|The time, space, {{and data}} {{complexity}} of an optimally data efficient isomorphism identification algorithm are presented. The data complexity, {{the amount of}} data required for an inference algorithm to terminate, is analyzed and shown to be the minimum possible for all possible isomorphism inference algorithms. The <b>minimum</b> <b>data</b> <b>requirement</b> is shown to be ⌈log 2 (n) ⌉, and a method for constructing this minimal sequence of data is presented. The average <b>data</b> <b>requirement</b> is shown to be approximately 2 log 2 (n). The time complexity is O(n 2 log 2 (n)) and the space requirement is O(n 2...|$|R
40|$|Abstract: In this report, a novel {{qualitative}} model learning (QML) framework named QML-Morven is presented. QML-Morven is an extensible {{framework and}} currently includes three QML subsystems, which employ either symbolic or evolutionary approaches as their learning strategies. QML-Morven uses the formalism of Morven, a fuzzy qualitative simulator, to represent and reason about qualitative models, {{and it also}} utilises Morven to verify candidate models. Based on this framework, a series ofexperiments were designed and carried out to: (1) verify the results obtained by the previous QML system ILP-QSI; (2) investigate factors that influence the learning precision and <b>minimum</b> <b>data</b> <b>requirement</b> for successful learning; (3) address the scalability issue of QML systems...|$|R
25|$|For IT purposes, this is {{commonly}} expressed as the <b>minimum</b> application and <b>data</b> <b>requirements</b> {{and the time}} in which the minimum application and application data must be available.|$|R
40|$|This {{paper is}} {{describing}} {{the issues of}} data collection in power distribution networks. It discusses the posibilities of data communication over wide area networks using the communication protocol IEC 60870 - 5 - 104, used in power distribution systems for transmission of information over IP networks. Thesis presents 4 technologies, suitable for data collection, {{with respect to the}} use of existing infrastructure of the utility. It focuses on design of appropriate data types in correspondence with used IEC 60870 - 5 - 104 protocol, and estimates the <b>minimum</b> <b>data</b> <b>requirements</b> for transmission, through proposed hierarchical network, with collecting data concentrators. For verification of given design, simulations are carried out based on proposed data loads with subsequent analysis of network load and transmission delays. Consequently, the results are analyzed and selected parts of network optimized for improvement od selected results, of which causes of formation are discussed in debate...|$|E
40|$|Bio-irrigation {{is often}} {{quantified}} through incubations where an inert tracer {{is added to}} the overlying water of a core or a benthic chamber, and subsequently following the tracer distribution in either the overlying water or the porewater. The interpretation is based on fitting data with a model containing several unknown parameters such as the enhancement over molecular diffusion or non-local exchange. In this paper, we test under what conditions the results obtained through this fitting are robust. We first use identifiability analysis to investigate the <b>minimum</b> <b>data</b> <b>requirements</b> for two types of sediments, representative for deep-sea and shallow-water settings. We then use two different representative data-sets to estimate uncertainties of the fitted parameters, based on a Bayesian technique, the Markov Chain Monte Carlo. Using only the concentration change in the overlying water, {{it is not possible to}} constrain both the rate and the mechanism of bio-irrigation, thus, sampling the porewaters at the end of the incubation is a necessity. ...|$|E
40|$|This article {{summarizes}} the framework that translated data from multiple disciplines into a bio-economic decision tool for modeling {{the costs and}} benefits of improving fish welfare in commercial aquaculture. This decision tool formed the basis of a recent EU research project, BENEFISH which was funded via the European Commission's Sixth Framework (FP 6) initiative. The bio-economic decision model can incorporate biological data, productivity data, micro (farm) and macro (industry) level economic data, and consumer marketing and business to business data. It can identify areas for potential added value that might be achieved by improving fish welfare across a range of species and husbandry systems within European aquaculture. This article provides a brief overview of the <b>minimum</b> <b>data</b> <b>requirements</b> for successfully modeling the bio-economic impacts of improvements in farmed fish welfare using the model developed during the BENEFISH project. It also highlights potential bottlenecks and the minimum prerequisites for each potential data set to be used for successful modeling...|$|E
40|$|Abstract—The {{quantified}} {{residence time}} distribution (RTD) provides a numerical characterization of mixing in a reactor, thus allowing the process engineer {{to better understand}} mixing performance of the reactor. This paper discusses computational studies to investigate flow patterns in a two impinging streams cyclone reactor(TISCR). Flow in the reactor was modeled with computational fluid dynamics (CFD). Utilizing the Eulerian-Lagrangian approach, implemented in FLUENT (V 6. 3. 22), particle trajectories were obtained by solving the particle force balance equations. From simulation results obtained at different ∆ts, the mean residence time (t m) and the mean square deviation (σ 2) were calculated. a good agreement can be observed between predicted and experimental data. Simulation {{results indicate that the}} behavior of complex reactor systems can be predicted using the CFD technique with <b>minimum</b> <b>data</b> <b>requirement</b> for validation...|$|R
40|$|Due to {{the limited}} amount of memory {{resources}} in embedded systems, minimizing the memory requirement is an important goal of software synthesis. This paper presents a set of techniques to reduce the code and data sizes for software synthesis from graphical DSP programs based on the synchronous dataflow (SDF) model. Observing that the required code and buffer sizes of the generated code depend on coding style as well as scheduling result, we develop an optimization procedure of determining the best coding style of each block. Two techniques are proposed to improve the performance over the previous works in which a single appearance schedule with <b>minimum</b> <b>data</b> <b>requirement</b> is used assuming inline coding style. By sharing the kernel code among multiple instances of a block with a shared function, we can further reduce the code size below the single appearance schedule. And, a systematic approach is devised {{to give up the}} single appearance schedule for reducing the <b>data</b> buffer <b>requirement.</b> [...] ...|$|R
30|$|For the {{analytical}} {{calculation of the}} user performance and the optimization of the SNR thresholds of the applied modulation schemes shown in Section “The SNR threshold problem”, {{it was assumed that}} the user serving vector ϑ was already given. In the following, it is shown how to determine ϑ such that the average system data rate is maximized while all users fulfill the target BER and the <b>minimum</b> <b>data</b> rate <b>requirements.</b>|$|R
40|$|Recent {{advances}} in 13 C-Metabolic flux analysis (13 C-MFA) have increased its capability to accurately resolve fluxes using a genome-scale model with narrow confidence intervals without pre-judging the activity or inactivity of alternate metabolic pathways. However, the necessary precautions, computational challenges, and <b>minimum</b> <b>data</b> <b>requirements</b> for successful analysis remain poorly established. This review aims {{to establish the}} necessary guidelines for performing 13 C-MFA at the genome-scale for a compartmentalized eukaryotic system such as yeast in terms of model and data requirements, while addressing key issues such as statistical analysis and network complexity. We describe the various approaches used to simplify the genome-scale model {{in the absence of}} sufficient experimental flux measurements, the availability and generation of reaction atom mapping information, and the experimental flux and metabolite labeling distribution measurements to ensure statistical validity of the obtained flux distribution. Organism-specific challenges such as the impact of compartmentalization of metabolism, variability of biomass composition, and the cell-cycle dependence of metabolism are discussed. Identification of errors arising from incorrect gene annotation and suggested alternate routes using MFA are also highlighted...|$|E
40|$|The International Commission for Irrigation and Drainage (ICID) and Food and Agriculture Organization of the United Nations (FAO) have {{proposed}} using the Penman-Monteith method {{as the standard}} method for estimating reference evapotranspiration (ET 0), and for evaluating other methods. The FAO- 56 Penman-Monteith (FAO- 56 PM) method requires the numerous weather data that are not available in {{the most of the}} stations. The objectives of this study were: first, to estimate errors that can arise if some weather data are not available and have to be estimated; second, to compare the FAO- 56 PM ET 0 values computed under various levels of data availability; and third, to determine minimum weather data requirements for estimating ET 0 without decreasing the acceptable accuracy. For this study, full weather data sets were collected from six humid weather stations from Serbia (Southeast Europe). The main conclusion is that the minimum and maximum air temperature and "local default" value of wind speed are the <b>minimum</b> <b>data</b> <b>requirements</b> necessary to apply the FAO- 56 PM method in humid climate...|$|E
40|$|Background: Despite {{indications}} that infection-related mortality in sub-Saharan Africa may be decreasing and {{the burden of}} non-communicable diseases increasing, the overwhelming reality is that health information systems across most of sub-Saharan Africa remain too weak to track epidemiological transition in a meaningful and effective way. Proposals: We propose a minimum dataset {{as the basis of}} a functional health information system in countries where health information is lacking. This would involve continuous monitoring of cause-specific mortality through routine civil registration, regular documentation of exposure to leading risk factors, and monitoring effective coverage of key preventive and curative interventions in the health sector. Consideration must be given as to how these <b>minimum</b> <b>data</b> <b>requirements</b> can be effectively integrated within national health information systems, what methods and tools are needed, and ensuring that ethical and political issues are addressed. A more strategic approach to health information systems in sub-Saharan African countries, along these lines, is essential if epidemiological changes are to be tracked effectively for the benefit of local health planners and policy makers. Conclusion: African countries have a unique opportunity to capitalize on modern information and communications technology in order to achieve this. Methodological standards need to be established an...|$|E
30|$|Vrba and Zaporozec (1994) {{distinguished}} intrinsic (or natural) vulnerability from specific (or integrated) vulnerability; {{the first}} term defined solely {{as a function of}} hydrogeological factors and the latter term defined by the potential impacts of specific land uses and contaminants. In other words, specific vulnerability integrates the contamination risk placed upon aquifers by human activities (Stigter et al. 2006). The intrinsic vulnerability of groundwater to contaminants takes into account the geological, pedological, hydrological and hydrogeological characteristics of an area (Vrba and Zaporozec 1994). Several methods have been developed to evaluate groundwater vulnerability, and choosing an appropriate one depends on many factors such as scale of the study area, data availability, and desired results (Al-Hanbali and Kondoh 2008). The most widely used method of vulnerability evaluation is DRASTIC model (Aller et al. 1987) due to its ease to use, <b>minimum</b> <b>data</b> <b>requirement,</b> and clear explanation of groundwater vulnerability.|$|R
5000|$|... 1 November - SATIN IV was {{effectively}} terminated by Congress. The restructured program was renamed SAC Digital Network (SACDIN), and was formulated to meet SAC's <b>minimum</b> essential <b>data</b> communications <b>requirements,</b> but {{also had the}} capability to grow in a modular fashion.|$|R
40|$|It is {{well known}} that {{effective}} prediction of project cost related factors is an important aspect of software engineering. Unfortunately, despite extensive research over more than 30 years, this remains a significant problem for many practitioners. A major obstacle is the absence of reliable and systematic historic data, yet this is a sine qua non for almost all proposed methods: statistical, machine learning or calibration of existing models. In this paper we describe our sparse data method (SDM) based upon a pairwise comparison technique and Saaty's Analytic Hierarchy Process (AHP). Our <b>minimum</b> <b>data</b> <b>requirement</b> is a single known point. The technique is supported by a software tool known as DataSalvage. We show, for data from two companies, how our approach — based upon expert judgement — adds value to expert judgement by producing significantly more accurate and less biased results. A sensitivity analysis shows that our approach is robust to pairwise comparison errors. We then describe the results of a small usability trial with a practising project manager. From this empirical work we conclude that the technique is promising and may help overcome some of the present barriers to effective project prediction...|$|R
40|$|AIM: To {{produce a}} {{user-friendly}} list of metastatic spinal cord compression (MSCC) Red Flags for non-specialist 'generalist' front-line clinicians working in primary-care settings. BACKGROUND: The issue of identifying MSCC early to prevent serious long-term disability {{was a key}} theme identified by the Task and Finish Group at Greater Manchester and Cheshire Cancer Network (GMCCN) in 2009. It was this group who initially brokered and then coordinated the current development {{as part of their}} strategic approach to improving care for MSCC patients. METHODS: A consensus-building approach that considered the essential <b>minimum</b> <b>data</b> <b>requirements</b> to raise the index of suspicion suggestive of MSCC was adopted. This followed a model of cross-boundary working to facilitate the mutual sharing of expertise across a variety of relevant clinical specialisms. RESULT: A guideline aimed at helping clinicians to identify the early signs and symptoms of MSCC was produced {{in the form of a}} credit card. This credit card includes key statements about MSCC, signposting to key sources of additional information and a user-friendly list of Red Flags which has been developed into an eight-item Red Flag mnemonic. To date, an excess of 120, 000 cards have been printed by a variety of organisations and the distribution of the cards is ongoing across the United Kingdom and the Republic of Ireland...|$|E
40|$|The {{objective}} of this manual is to inform the network of National Focal Centers (NFCs) about the requirements of methodologies for the dynamic modelling of geochemical processes in soils in particular. This information is necessary to support European air quality policies with knowledge on time delays of ecosystem damage or recovery caused by changes, in time, of acidifying deposition. This manual has been requested by bodies under the Convention on Long-range Transboundary Air Pollution (CLRTAP) to support {{the extension of the}} European critical load database with dynamic modelling parameters. A Very Simple Dynamic (VSD) model is described to encourage NFCs to meet <b>minimum</b> <b>data</b> <b>requirements</b> upon engaging in the extension of national critical load databases. This manual can be consulted in combination with a running version of VSD which is available on www. rivm. nl/cce. The manual also provides an overview of existing dynamic models which generally have more complex input data requirements. Finally, the manual tentatively describes possible linkages between dynamic modelling results and integrated air pollution assessment modelling. This linkage is necessary for use in the near future support of the review of the 1999 CLRTAP Protocol to Abate Acidification, Eutrophication and Ground-level Ozone (the "Gothenburg Protocol") and the 2001 EU National Emission Ceiling directive...|$|E
40|$|Following the {{endorsement}} {{by the international}} community of the Millennium Development Goals, there has been an increasing demand for practical methods for steadily tracking poverty. An economically intuitive and inexpensive methodo-logy is explored for doing so in the absence of regular, comparable data on household consumption. The <b>minimum</b> <b>data</b> <b>requirements</b> for this methodology are the availability of a household budget survey and a series of surveys with a com-parable set of asset data also contained in the budget survey. This method is illustrated using a series of Demographic and Health Surveys for Kenya. JEL codes: C 81, I 32 The worldwide endorsement of the Millennium Development Goals and the shift to results-based lending in supporting developing countries have intensi-fied the importance of being able to reliably gauge the evolution of poverty. The common approach to measuring poverty is anchored in utility theory and is empirically based on household consumption or income measures, which are usually derived from nationally representative household budget surveys (Ravallion 1996 a; Deaton 2003). Obtaining reliable measures of household consumption presents a series of challenges in practice. 1 These challenges increase when comparing poverty over time. David Stifel is an assistant professor at Lafayette College; his email address is stifeld@lafayette. edu. Luc Christiaensen (corresponding author) is a senior economist in the East Asia Rural Development an...|$|E
3000|$|In this paper, we analyze {{in which}} way users of {{different}} service classes should be assigned in a heterogeneous scenario, thereby extending ideas from [3, 4]. Users request either a fixed <b>minimum</b> <b>data</b> rate, for example, as needed for voice services, or unconstrained best-effort (BE) data services. We formulate the user assignment as a utility maximization problem which is {{constrained by the}} resources (such as power or bandwidth) of the individual base stations (BSs) as well as users' <b>minimum</b> <b>data</b> rate <b>requirements.</b> The utilities represent quality of service (QoS) indicators of the BE users and, by choosing appropriate utility functions, give operators the freedom to tune the operation point of the heterogeneous system. It {{is important to note}} that although our model holds for general concave utility functions we will adopt the concept of [...]...|$|R
3000|$|Finally, a {{relatively}} low throughput is obtained when using the MM algorithm {{with respect to the}} other schemes. This {{can be explained by the}} fact that this scheme minimizes the energy consumption of the small cell network at all costs, while satisfying the <b>minimum</b> <b>data</b> rate <b>requirements</b> of users. The throughput performance is at its minimum when the number of users is very low, since in this case almost all small cells are in sleep mode (see Fig. 13) and most of the traffic is handled by the macro cell. In effect, the MM algorithm undertakes an aggressive version of the proposed heuristic where t [...]...|$|R
40|$|We {{present the}} {{theoretical}} foundation, implementation, and experimental {{evaluation of a}} novel power-saving mechanism for wireless transmission from multiple-input multiple-output (MIMO) transceivers, called RF chain management. RF chain management seeks to minimize the energy per bit for MIMO transmission, via adaptively choosing the optimal RF chain configuration, and satisfies the <b>minimum</b> <b>data</b> rate <b>requirement</b> at the same time. Our simulation shows that up to 45 % and averagely 23 % energy per bit reduction can be achieved. We have also built a prototype based on the WARP platform, and our experimental results have proved the feasibility of RF chain management in real systems and under realistic channels...|$|R
40|$|Fuelled by {{the raising}} {{importance}} of cephalopod fisheries in Europe, {{there have been}} demands from scientists and stakeholders for their assessment and management. However, little {{has been done to}} improve the data collection in order to analyse cephalopod populations under the EU Data Collection Framework (DCF). While the DCF allows member states to design flexible national sampling programmes, it establishes the <b>minimum</b> <b>data</b> <b>requirements</b> (MDR) each state is obliged to fulfil. In this study, it was investigated whether such MDR currently set by the DCF allow the application of depletion models (DMs) to assess European cephalopod stocks. Squid and cuttlefish fisheries from the western Mediterranean were used as a case study. This exercise sheds doubt on the suitability of the MDR to properly assess and manage cephalopod stocks by means of DMs. Owing to the high plasticity of life-history traits in cephalopod populations, biological parameters should be estimated during the actual depletion period of the fished stocks, in contrast with the triennial sampling established by the DCF. In order to accurately track the depletion event, the rapid growth rates of cephalopods implies that their populations should be monitored at shorter time scales (ideally weekly or biweekly) instead of quarterly as required by the DCF. These measures would not demand additional resources of the ongoing DCF, but a redistribution of sampling efforts during the depletion period. Such changes in the sampling scheme could be designed and undertaken by the member states or directly integrated as requirements...|$|E
40|$|Agro-ecological {{indicators}} (AEIs) allow evaluating sustainability for a {{large number}} of farms. The SITPAS Information System developed for the agricultural park &ldquo;Parco Agricolo Sud Milano&rdquo; (northern Italy) contains detailed farming and cropping systems information for 731 farms {{that can be used for}} these analyses. We used the SITPAS database to evaluate N management with an AEI and to evaluate the suitability of the SITPAS data model for this type of applications. The AEI (soil surface N balance) was calculated for each crop at field scale, as the difference between the sum of N inputs (atmospheric depositions, biological fixation, fertilisers, residues from previous crop) and crop N uptake; the results were aggregated at rotation and farm levels. The farming systems with the highest surplus (&gt; 300 kg N ha- 1) are dairy, cattle and pig farms, in which chemical N fertilisers are used in addition to animal manures. The crops with the highest surplus are Italian ryegrass and maize (183 and 172 kg N ha- 1, respectively), while rice and wheat have the lowest surplus (87 and 85 kg N ha- 1). The data model allowed to store and analyse complex information not manageable otherwise; its main limitation was the excessive flexibility, requiring a complicated procedure for the calculations of this example, and the exclusion of most data at the farming systems level (corresponding to 82 % of the studied area) for missing, incomplete, out-of-range or inconsistent data. These results suggest to promote actions towards better N management in cropping systems in the Park and to develop simple data models based on <b>minimum</b> <b>data</b> <b>requirements</b> when sustainability evaluations are to be conducted...|$|E
40|$|This report informs parties {{under the}} Convention on Long-range Transboundary Air Pollution (CLRTAP) and its network of National Focal Centres {{on the recent}} update of the European {{critical}} loads database. In 2003 this database underwent a first time extension with dynamic modelling variables. This information is necessary to support European air quality policies with knowledge on time delays of ecosystem damage or recovery caused by changes, in time, of acidifying deposition. The database was updated and extended following {{the request of the}} Working Group on Effects at its 21 st session (Geneve, 28 - 30 augustus 2002). Nineteen countries submitted data on critical loads for acidity and eutrophication. Ten countries also submitted the requested dynamic modelling parameters. Other countries indicated their intentions to prepare data for submission for the next call planned for the autumn of 2003. It is concluded that <b>minimum</b> <b>data</b> <b>requirements</b> for dynamic modelling can be met. The report contains national reports on the methods applied to contribute to the European critical loads data base. The report includes an analysis of the data to verify cross border consistency. A comparison is made with the database used to support negotiations of the 1999 CLRTAP Protocol to Abate Acidification, Eutrophication and Ground-level Ozone (the "Gothenburg Protocol") and the 2001 EU National Emission Ceiling directive. The results described in this report are important for {{the next step in the}} support of the revision process of these European agreements expected in 2004 / 2005. The report will be presented at the 22 nd session of the Working Group on Effects (Geneva, 3 - 5 September 2003) ...|$|E
40|$|The {{factors that}} {{determine}} <b>data</b> volume <b>requirements</b> {{in a typical}} wind tunnel test are identified. It is suggested that productivity in wind tunnel testing can be enhanced by managing the inference error risk associated with evaluating residuals in a response surface modeling experiment. The relationship between <b>minimum</b> <b>data</b> volume <b>requirements</b> and the factors upon which they depend is described and certain simplifications to this relationship are realized when specific model adequacy criteria are adopted. The question of response model residual evaluation is treated and certain practical aspects of response surface modeling are considered, including inference subspace truncation. A wind tunnel test plan developed by using the Modern Design of Experiments illustrates the advantages of an early estimate of <b>data</b> volume <b>requirements.</b> Comparisons are made with a representative One Factor At a Time (OFAT) wind tunnel test matrix developed to evaluate a surface to air missile...|$|R
40|$|In this paper, {{we propose}} a cross-layer {{scheduling}} algorithm that achieves a throughput "epsilon-close" to the optimal throughput in multi-hop wireless networks with a tradeoff of O(1 /epsilon) in delay guarantees. The algorithm aims {{to solve a}} joint congestion control, routing, and scheduling problem in a multi-hop wireless network while satisfying per-flow average end-to-end delay guarantees and <b>minimum</b> <b>data</b> rate <b>requirements.</b> This problem has been solved for both backlogged as well as arbitrary arrival rate systems. Moreover, we discuss {{the design of a}} class of low-complexity suboptimal algorithms, the effects of delayed feedback on the optimal algorithm, and the extensions of the proposed algorithm to different interference models with arbitrary link capacities...|$|R
3000|$|The third paper titled [...] "CDIT-based {{constrained}} {{resource allocation}} for mobile WiMAX systems" [...] by F. Brah, J. Louveaux, and L. Vandendorpe addresses {{the problem of}} subchannel assignment and power allocation for mobile WiMAX systems. The authors consider a fast fading environment, where the transmitter has only the channel distribution information (CDI) instead of the full instantaneous channel state information. The objective is to maximize the ergodic weighted sum rate under long-term fairness, <b>minimum</b> <b>data</b> rate <b>requirement,</b> and power budget constraints. The authors formulate the problem as a nonlinear stochastic constrained optimization problem and provide an efficient analytical solution based on Lagrange dual decomposition framework. For the proposed CDIT-based resource allocation framework, the trade-off between reduction in computational complexity and performance degradation is analyzed.|$|R
40|$|The European Food Safety Authority (EFSA) {{requested}} the Panel on Plant Health {{to develop a}} methodology for assessing the environmental risks posed by harmful organisms that may enter, establish and spread in the European Union. To do so, the Panel first reviewed the methods for assessing the environmental risks of plant pests that have previously been used in pest risk assessment. The limitations identified by the review led the Panel to define the new methodology for environmental risk assessment which is described in this guidance document. The guidance is primarily addressed to the EFSA PLH Panel and has been conceived as an enhancement of the relevant parts of the “Guidance on a harmonised framework for pest risk assessment and the identification and evaluation of pest risk management options by EFSA”. Emphasizing the importance of assessing the consequences on both the structural (biodiversity) and the functional (ecosystem services) aspects of the environment, this new approach includes methods for assessing both aspects {{for the first time}} in a pest risk assessment scheme. A list of questions has been developed for the assessor to evaluate the consequences for structural biodiversity and for ecosystem services in the current area of invasion and in the risk assessment area. To ensure the consistency and transparency of the assessment, a rating system has also been developed based on a probabilistic approach with an evaluation of the degree of uncertainty. Finally, an overview of the available risk reduction options for pests in natural environments is presented, <b>minimum</b> <b>data</b> <b>requirements</b> are described, and a glossary to support the common understanding of the principles of this opinion is provided...|$|E
40|$|Fuelled by the {{increasing}} importance of cephalopod fisheries in Europe, scientists and stakeholders have demanded their assessment and management. However, {{little has been}} done to improve the data collection under the EU Data Collection Framework (DCF) in order to analyse cephalopod populations. While the DCF allows member states to design flexible national sampling programmes, it establishes the <b>minimum</b> <b>data</b> <b>requirements</b> (MDR) each state is obliged to fulfil. This study was performed to investigate whether such MDR currently set by the DCF allow the application of depletion models (DMs) to assess European cephalopod stocks. Squid and cuttlefish fisheries from the western Mediterranean were used as a case study. This study sheds doubt on the suitability of the MDR to properly assess and manage cephalopod stocks by means of DMs. Owing to the high plasticity of life-history traits in cephalopod populations, biological parameters should be estimated during the actual depletion period of the fished stocks, rather than performing triennial sampling as established by the DCF. In order to accurately track the depletion event, the rapid growth rates of cephalopods implies that their populations should be monitored at shorter time scales (ideally weekly or biweekly) instead of quarterly as specified by the DCF. These measures would not require additional resources of the ongoing DCF but a redistribution of sampling efforts during the depletion period. Such changes in the sampling scheme could be designed and undertaken by the member states or directly integrated as requirements. This study was performed under the Data Collection Framework (cofunded by the EU and the Spanish Institute of Oceanography, IEO) and the CONFLICT project (CGL 2008 - 958; funded by the Spanish Ministry of Science and Innovation). SK was financed by an IEO-FPI grant (2011 / 11) Peer Reviewe...|$|E
40|$|In {{the last}} three decades many {{sophisticated}} tools have been developed that can accurately predict the dynamics of flooding. However, due to the paucity of adequate infrastructure, this technological advancement did not benefit ungauged flood-prone regions in the developing countries in a major way. The overall research theme of this dissertation is to explore the improvement in methodology that is essential for utilising recently developed flood prediction and management tools in the developing world, where ideal model inputs and validation datasets do not exist. This research addresses important issues related to undertaking inundation modelling at different scales, particularly in data-sparse environments. The results indicate that in order to predict dynamics of high magnitude stream flow in data-sparse regions, special attention is required on the choice of the model in relation to the available data and hydraulic characteristics of the event. Adaptations are necessary to create inputs for the models that have been primarily designed for areas with better availability of data. Freely available geospatial information of moderate resolution can often meet the <b>minimum</b> <b>data</b> <b>requirements</b> of hydrological and hydrodynamic models if they are supplemented carefully with limited surveyed/measured information. This thesis also explores the issue of flood mitigation through rainfall-runoff modelling. The purpose of this investigation is {{to assess the impact of}} land-use changes at the sub-catchment scale on the overall downstream flood risk. A key component of this study is also quantifying predictive uncertainty in hydrodynamic models based on the Generalised Likelihood Uncertainty Estimation (GLUE) framework. Detailed uncertainty assessment of the model outputs indicates that, in spite of using sparse inputs, the model outputs perform at reasonably low levels of uncertainty both spatially and temporally. These findings have the potential to encourage the flood managers and hydrologists in the developing world to use similar data sets for flood management. ...|$|E
40|$|We propose {{an online}} {{algorithm}} for energy minimization of transmitting stations in IEEE 802. 11 wireless local area networks (WLANs). Our algorithm configures radio frequency (RF) parameters {{such as the}} number of transmitting antennas based on the real-time traffic demand and channel measurements so that the power consumption of the device is reduced and the <b>minimum</b> <b>data</b> rate <b>requirements</b> of stations at a target error rate are satisfied. The transition between different RF configurations is theoretically investigated by employing a 2 D Markov model. Using an 802. 11 ac hardware testbed, we experimentally demonstrate that the power consumption of a transmitting station can be reduced up to 33 %. We also validate that the experimental and the theoretical results closely matc...|$|R
40|$|Conventional {{designs on}} OFDM-based {{underlay}} cognitive radio (CR) networks mainly focus on interference avoidance and spectral efficiency (SE) improvement. As green radio becomes increasingly important, this paper investigates energy efficient power allocation. Our {{aim is to}} maximize energy efficiency (EE), subject to the constraints on the total transmit power, the peak interference power, and the <b>minimum</b> <b>data</b> rate <b>requirement.</b> We first analyze the relationship between SE and EE and solve this optimization problem {{with the help of}} bisection search technique. However, the accuracy of the power allocation solution is dependent on the number of iterations. In order to achieve the exact optimal solution, a new energy efficient power allocation scheme is proposed to balance the tradeoff between SE and EE. Simulation results are provided to demonstrate the effectiveness of the proposed schemes...|$|R
40|$|The book mainly {{introduces}} {{readers to}} the development and current status of water quality criteria (WQC) in China and other countries or areas, and proposes a <b>minimum</b> toxicity <b>data</b> <b>requirement</b> (MTDR; i. e., six species from three phyla) as the principle metric for deriving WQC in China. Further, ten model species from 4 different phyla are recommended as domestic test species, and the methodologies for deriving aquatic life criteria, sediment criteria, ecocriteria and nutrient criteria in China are also described in detail. In order to demonstrate the methods, several representative chemical pollutants and aquatic environments are highlighted as examples. The book provides important references for future WQC-related research in China, which will make it {{of great interest to}} researchers and graduate students in the fields of environmental science, ecology and aquatic science etc...|$|R
