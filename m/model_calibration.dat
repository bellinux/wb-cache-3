2899|3472|Public
2500|$|Anderson, Mary P., Woessner, William W., & Hunt, Randall J., 2015, Applied Groundwater Modeling, 2nd Edition, Academic Press. — Updates the 1st edition {{with new}} examples, new {{material}} {{with respect to}} <b>model</b> <b>calibration</b> and uncertainty, and online Python scripts (https://github.com/Applied-Groundwater-Modeling-2nd-Ed).|$|E
5000|$|Beven, K.J. and Binley, A.M., 1992. The {{future of}} {{distributed}} models: <b>model</b> <b>calibration</b> and uncertainty prediction, Hydrological Processes, 6, pp. 279-298.|$|E
5000|$|... 133.Antoshechkina, P., Wolf, A., Hamecher, E., Asimow, P., Ghiorso, M. (2015) Improved Thermodynamic <b>Model</b> <b>Calibration</b> with Bayesian Methods. Goldschmidt Abstracts, 2015 94 ...|$|E
30|$|We applied two {{different}} strategies for suitability modeling under {{past and future}} climate conditions. <b>Model</b> <b>calibrations</b> for projections to LGM climate conditions (only for CWR) were carried out at 2.5 arc minute resolution using only WorldClim climate layers (Hijmans et al. 2005) as explanatory variables. <b>Model</b> <b>calibrations</b> intended for projections to future climate scenarios (period 2040 – 2069; referred to as 2050 s), were carried out at 30 arc sec resolution, using aside from climate layers also altitude, slope, aspect, terrain roughness, direction of water flow and soil type (FAO/IIASA/ISRIC/ISS-CAS/JRC 2012). Collinear explanatory variables were removed based on iterative calculations of variance inflation factors (VIF), retaining only variables with VIFs smaller than 5. The resulting sets of explanatory variables, as well as presence, background and absence points used for <b>model</b> <b>calibrations</b> are given in Additional file 15 : Table S 7.|$|R
30|$|Background points (an overall {{maximum of}} 10, 000 and maximum one per grid cell) for LGM <b>model</b> <b>calibrations</b> were {{randomly}} selected from the area enclosed by a convex hull polygon constructed around all presence points and extended with a buffer corresponding to 10 % of the polygon’s largest axis. For future <b>model</b> <b>calibrations</b> we additionally limited the selection of background points to areas of the extended convex hull intersected by the vegetation units from Olson et al. (Olson et al. 2001) with at least one species presence.|$|R
30|$|The Marx Creek {{model was}} {{manually}} calibrated to hydraulic head values {{observed in the}} monitor wells and stream discharge values collected from upper Marx Creek. All of the <b>model’s</b> <b>calibration</b> simulations were run under steady-state conditions.|$|R
5000|$|B.B. Park and J.D. Schneeberger, Microscopic Simulation <b>Model</b> <b>Calibration</b> and Validation: Case Study of VISSIM Simulation Model for a Coordinated Actuated Signal System. Transportation Research Record 1856:185-192, 2003. PDF ...|$|E
50|$|The <b>Model</b> <b>Calibration</b> {{option is}} based on a process where {{measured}} data from a real device is used to tune parameterssuch that the simulation results are in good agreement with the measured data.|$|E
50|$|As {{stated by}} Malcolm Kemp in Chapter {{five of his}} book Market Consistency: <b>Model</b> <b>Calibration</b> in Imperfect Markets, the risk-free rate means {{different}} things to different people and there is no consensus on how to go about a direct measurement of it.|$|E
50|$|Although NumXL is {{intended}} as an analytical add-in for Excel, it extends Excel’s user-interface (UI) and offers many wizards, menus and toolbars to automate the mundane phases of time series analysis. The features include summary statistics, test of hypothesis, correlogram analysis, <b>modeling,</b> <b>calibration,</b> residuals diagnosis, back-testing and forecast.|$|R
50|$|Standard {{wind tunnel}} models, {{also known as}} {{reference}} <b>models,</b> <b>calibration</b> <b>models</b> (maquettes d'étalonnage) or test check-standards are objects of relatively simple and precisely defined shapes, having known aerodynamic characteristics, that are tested in wind tunnels. Standard models are used in order to verify, by comparison of wind tunnel test results with previously published results, the complete measurement chain in a wind tunnel, including wind tunnel structure, quality of the airstream, model positioning, transducers and force balances, data acquisition system and data reduction software.|$|R
40|$|AbstractThis work {{describes}} the simulation <b>models</b> <b>calibration</b> method called <b>Model</b> Output <b>Calibration.</b> In order to verify its effectiveness, presents {{the application of}} the MOC in weather forecast correction generated by the Eta 15 Km model at CPTEC/INPE. Eta is a regional model for numerical weather prediction. The results of the statistical correction of Eta forecast were positive, with satisfactory improvements in the variables tested (temperature and relative humidity). The use of this approach shows the possibility of gains in the results of simulation models of crops and diseases that use as predictive variables the variables generated by weather forecast models...|$|R
50|$|Dynamic {{optimization}} problems, including optimal control, trajectory optimization, parameter optimization and <b>model</b> <b>calibration</b> can be formulated and solved using JModelica.org. The Optimica extension enables high-level {{formulation of}} dynamic optimization problems based on Modelica models. The mintOC project provides {{a number of}} benchmark problems encoded in Optimica.|$|E
5000|$|Kubinger, K.D., Hohensinn, C., Hofer, S., Khorramdel, L., Frebort, M., Holocher-Ertl, S., Reif, M. & Sonnleitner, P. (2011). Designing {{the test}} {{booklets}} for Rasch <b>model</b> <b>calibration</b> {{in a large}} scale assessment with reference to numerous moderator variables and several ability dimensions. Educational Research and Evaluation, 17, 483-495.|$|E
5000|$|In addition, [...] "calibration" [...] {{is used in}} {{statistics}} {{with the}} usual general meaning of calibration. For example, <b>model</b> <b>calibration</b> can be also {{used to refer to}} Bayesian inference about the value of a model's parameters, given some data set, or more generally to any type of fitting of a statistical model.|$|E
25|$|All {{problems}} are treated identically. No <b>modeling</b> or <b>calibration</b> inputs are required.|$|R
30|$|This work is {{organized}} as an introduction plus six sections: the second presents {{a brief description}} of LSA, the third presents the corpus used in the work, the fourth presents the LSA model, the fifth presents the <b>model’s</b> <b>calibration</b> process, the sixth presents a discussion of the results, and the seventh presents conclusions and future research.|$|R
40|$|All <b>model</b> <b>calibrations</b> {{are subject}} to uncertainty. Even when an {{optimization}} procedure is used, the specific calibration period strongly influences the parameter values and causes uncertainty in streamflow prediction. The present paper aims to study the influence that the dimension and {{the position of the}} sampling period have on the uncertainty of the response of a daily conceptual shot noise model. The simulations have been conducted using continuous daily series of discharges recorded for 76 years on the river basin of Oreto (Palermo, Italy). Many continuous sub-series for simulations have been obtained from the historical series available. In the first place many <b>model</b> <b>calibrations</b> have been carried out for variable dimension temporal windows, through an optimization procedure, obtaining different set of parameters. Consequently runoff predictions have been carried out, through Monte Carlo simulations. The proposed methodology contributes to determine the streamflow uncertainty bands for fixed quantile...|$|R
50|$|Drag {{induced by}} the neutral-atmosphere density is the major {{perturbation}} on satellites in low earth orbit. True density deviates as much as 21% from model predictions, introducing error into crucial government and private space operations with applications to situational awareness, space surveillance, laser communications, re-entry prediction, rendezvous and proximity ops. A need exists to measure physical or 'true' density, quantify density variations, and to provide in-situ <b>model</b> <b>calibration</b> data.|$|E
50|$|There {{is a wide}} {{spectrum}} of terminology associated with space mapping: ideal model, coarse model, coarse space, fine model, companion model, cheap model, expensive model, surrogate model, low fidelity (resolution) model, high fidelity (resolution) model, empirical model, simplified physics model, physics-based model, quasi-global model, physically expressive model, device under test, electromagnetics-based model, simulation model, computational model, tuning <b>model,</b> <b>calibration</b> model, surrogate model, surrogate update, mapped coarse model, surrogate optimization, parameter extraction, target response, optimization space, validation space, neuro-space mapping, implicit space mapping, output space mapping, port tuning, predistortion (of design specifications), manifold mapping, defect correction, model management, multi-fidelity models, variable fidelity/variable complexity, multigrid method, coarse grid, fine grid, surrogate-driven, simulation-driven, model-driven, feature-based modeling.|$|E
5000|$|For traffic {{modelling}} {{work in the}} [...] "baseline" [...] scenario, a GEH of {{less than}} 5.0 is considered a good match between the modelled and observed hourly volumes (flows of longer or shorter durations should be converted to hourly equivalents to use these thresholds). According to DMRB, 85% of the volumes in a traffic model should have a GEH less than 5.0. GEHs {{in the range of}} 5.0 to 10.0 may warrant investigation. If the GEH is greater than 10.0, there is a high probability {{that there is a problem}} with either the travel demand model or the data (this could be something as simple as a data entry error, or as complicated as a serious <b>model</b> <b>calibration</b> problem).|$|E
40|$|Recently {{three-dimensional}} global seismic velocity {{models of}} crust and mantle {{have been developed}} and location improvements have been demonstrated with regional (CUB 1 and CUB 2 models) and teleseismic (J 362 <b>model)</b> <b>calibrations,</b> respectively (Shapiro and Ritzwoller, 2002; Ritzwoller et al., 2002; Antolik et al., 2003). In this study, we validated event location improvements from these regional and teleseismic models, separated an...|$|R
40|$|We {{introduce}} a new calibration methodology that allows perfect fitting of the displaced diffusion LIBOR market model to caplets and co-terminal swaptions, whilst avoiding global optimizations. The approach works by regarding a forward rate as a difference of swap rates and then bootstrapping through rates one by one. LIBOR market <b>models,</b> <b>Calibration</b> of deterministic volatility, Computational finance, Interest rate derivatives, American style derivative securities,...|$|R
40|$|We {{propose a}} {{flexible}} linear <b>calibration</b> <b>model</b> with errors from RS (Ramberg & Schmeiser, 1974) generalized lambda distribution ($Glambda D$). We demonstrate the derivation {{of the maximum}} likelihood estimates of RS $Glambda D$ parameters and examine the estimation performance using a simulation study for sample sizes ranging from 30 to 200. The use of RS $Glambda D$ <b>calibration</b> <b>model</b> not only provides statistical modeller with a richer range of distributional shapes, but can also provide more precise parameter estimates compared to the standard Normal <b>calibration</b> <b>model</b> or skewed Normal <b>calibration</b> <b>model</b> proposed by Figueiredoa, Bolfarinea, Sandovala and Limab (2010) ...|$|R
50|$|Data on the abundance, species composition, {{and size}} {{structure}} of vegetation {{is critically important}} for {{a wide array of}} sub-disciplines in ecology, conservation, natural resource management, and global change biology. However, addressing many of the pressing questions in these disciplines will require that terrestrial biosphere and hydrologic models are able to assimilate the large amount of long-tail data that exists but is largely inaccessible. The Brown Dog team in cooperation with researches from Dietze's lab will facilitate the capture of a huge body of smaller research-oriented vegetation data sets collected over many decades and historical vegetation data embedded in Public Land Survey data dating back to 1785. This data will be used as initial conditions for models, to make sense of other large data sets and for <b>model</b> <b>calibration</b> and validation.|$|E
5000|$|HYDRUS models {{may be used}} {{to analyze}} water and solute {{movement}} in unsaturated, partially saturated, or fully saturated homogeneous of layered media. The codes incorporates hysteresis by assuming that drying scanning curves are scaled from the main drying curve, and wetting scanning curves from the main wetting curve. Root water uptake can be simulated as a function of both water and salinity stress, and can be either compensated or uncompensated. The HYDRUS software packages additionally implement a Marquardt-Levenberg type parameter estimation technique for inverse estimation of soil hydraulic and/or solute transport and reaction parameters from measured transient or steady-state flow and/or transport data. The programs are for this purpose written {{in such a way that}} almost any application that can be run in a direct mode can equally well be run in an inverse mode, and thus for <b>model</b> <b>calibration</b> and parameter estimation.|$|E
50|$|<b>Model</b> <b>calibration</b> is {{achieved}} by adjusting any available parameters in order to adjust how the model operates and simulates the process. For example, in traffic simulation, typical parameters include look-ahead distance, car-following sensitivity, discharge headway, and start-up lost time. These parameters influence driver behavior such as when {{and how long it}} takes a driver to change lanes, how much distance a driver leaves between his car and {{the car in front of}} it, and how quickly a driver starts to accelerate through an intersection. Adjusting these parameters has a direct effect on the amount of traffic volume that can traverse through the modeled roadway network by making the drivers more or less aggressive. These are examples of calibration parameters that can be fine-tuned to match characteristics observed in the field at the study location. Most traffic models have typical default values but they may need to be adjusted to better match the driver behavior at the specific location being studied.|$|E
40|$|AbstractThe <b>calibration</b> <b>model</b> of {{infrared}} spectra analysis is established combined with support vector machine, a new information processing method. As the model parameters {{have an impact}} on the analysis, selection of SVM <b>calibration</b> <b>model</b> parameters is researched through experimental studies. Beginning with the study of SVM <b>calibration</b> <b>model</b> parameters and spectra data sample parameters, the effect on the analysis results by the parameters like types of SVM <b>calibration</b> <b>model,</b> spectrometer scanning interval, spectra analysis band, kernel function, penalty factor C etc is researched after the infrared (IR) spectra data is preprocessed by the use of normalization-expansion method. The experimental results show that as the SVM <b>calibration</b> <b>model</b> is determined, a reasonable selection of parameters can improve the accuracy of the analysis results, and it has a practical application value...|$|R
40|$|In {{epidemiologic}} studies, {{measurement error}} in dietary variables often attenuates association between dietary intake and disease occurrence. To adjust for the attenuation caused by error in dietary intake, regression calibration is commonly used. To apply regression calibration, unbiased reference measurements are required. Short-term reference measurements for {{foods that are}} not consumed daily contain excess zeroes that pose challenges in the <b>calibration</b> <b>model.</b> We adapted twopart regression <b>calibration</b> <b>model,</b> initially developed for multiple replicates of reference measurements per individual to a single-replicate setting. We showed how to handle excess zero reference measurements by two-step modeling approach, how to explore heteroscedasticity in the consumed amount with variance-mean graph, how to explore nonlinearity with the generalized additive modeling (GAM) and the empirical logit approaches, and how to select covariates in the <b>calibration</b> <b>model.</b> The performance of two-part <b>calibration</b> <b>model</b> was compared with the one-part counterpart. We used vegetable intake and mortality data from European Prospective Investigation on Cancer and Nutrition (EPIC) study. In the EPIC, reference measurements were taken with 24 -hour recalls. For {{each of the three}} vegetable subgroups assessed separately, correcting for error with an appropriately specified two-part <b>calibration</b> <b>model</b> resulted in about three fold increase in the strength of association with all-cause mortality, as measured by the log hazard ratio. Further found is that the standard way of including covariates in the <b>calibration</b> <b>model</b> can lead to over fitting the two-part <b>calibration</b> <b>model.</b> Moreover, the extent of adjusting for error is influenced by the number and forms of covariates in the <b>calibration</b> <b>model.</b> For episodically consumed foods, we advise researchers to pay special attention to response distribution, nonlinearity, and covariate inclusion in specifying the <b>calibration</b> <b>model...</b>|$|R
30|$|Evaluation of the <b>model</b> {{includes}} <b>calibration,</b> validation sensitivity, {{and uncertainty}} analysis to establish its reliability for intervention analysis.|$|R
50|$|Liden {{applied the}} HBV model to {{estimate}} the riverine transport of three different substances, nitrogen, phosphorus and suspended sediment in four different countries: Sweden, Estonia, Bolivia and Zimbabwe. The relation between internal hydrological model variables and nutrient transport was assessed. A model for nitrogen sources was developed and analysed in comparison with a statistical method. A model for suspended sediment transport in tropical and semi-arid regions was developed and tested. It was shown that riverine total nitrogen could be well simulated in the Nordic climate and riverine suspended sediment load could be estimated fairly well in tropical and semi-arid climates. The HBV model for material transport generally estimated material transport loads well. The main conclusion {{of the study was}} that the HBV model can be used to predict material transport on the scale of the drainage basin during stationary conditions, but cannot be easily generalised to areas not specifically calibrated. In a different work, Castanedo et al. applied an evolutionary algorithm to automated watershed <b>model</b> <b>calibration.</b>|$|E
5000|$|Impetus {{to derive}} a {{quantitative}} prediction model arose from a trend of historically decreasing river flow rates coupled with jurisdictional and tribal conflicts over water rights {{as well as}} concern for river biota. When expansion of the Reno-Sparks Wastewater Treatment Plant was proposed, the EPA decided to fund a large scale research effort to create simulation software and a parallel program to collect field data in the Truckee River and Pyramid Lake. For river stations water quality measurements {{were made in the}} benthic zone as well as the topic zone; in the case of Pyramid Lake boats were used to collect grab samples at varying depths and locations. Earth Metrics conducted the software development for the first generation computer model and collected field data on water quality and flow rates in the Truckee River. After <b>model</b> <b>calibration,</b> runs were made to evaluate impacts of alternative land use controls and discharge parameters for treated effluent.|$|E
50|$|AGARD-B is a {{standard}} wind tunnel <b>model</b> (<b>calibration</b> model) {{that is used to}} verify, by comparison of test results with previously published data, the measurement chain in a wind tunnel.Together with its derivative AGARD-C it belongs to a family of AGARD standard wind tunnel models. Its origin dates to the year 1952, and the Second Meeting of the AGARD Wind Tunnel and Model Testing Panel in Rome, Italy, when it was decided to define two standard wind tunnel model configurations (AGARD-A and AGARD-B) to be used for exchange of test data and comparison of test results of same models tested in different wind tunnels. The idea was to establish standards of comparison between wind tunnels and improve the validity of wind tunnel tests. Among the standard wind tunnel models, AGARD model configuration B (AGARD-B) has become by far the most popular. Initially intended for the supersonic wind tunnels, the AGARD-B configuration has since been tested in many wind tunnels at a wide range of Mach numbers, from low subsonic (Mach 0.1), through transonic (Mach 0.7 to 1.4) to hypersonic (Mach 8 and above). Therefore, a considerable database of test results is available.|$|E
30|$|The {{accuracy}} of any bioanalytical method {{depends on the}} selection of an appropriate <b>calibration</b> <b>model.</b> The most commonly used <b>calibration</b> <b>model</b> is the unweighted linear regression, where the response (y-axis) is plotted against the corresponding concentration (x-axis). The degree of association between these two variables is {{expressed in terms of}} correlation coefficient (r 2). However, the satisfactory r 2 alone is not adequate to accept the <b>calibration</b> <b>model.</b> The wide <b>calibration</b> curve range used in the bioanalytical methods is susceptible to the heteroscedasticity of the calibration curve data. The use of weighted linear regression with an appropriate weighting factor reduces the heteroscedasticity and improves the accuracy over the selected concentration range.|$|R
40|$|There are {{currently}} no <b>calibration</b> <b>models</b> that allow whole body density in professional footballers to be estimated. As such, {{there is a}} need to develop practical <b>calibration</b> <b>models</b> in order to make sound body composition judgements. The aim of this thesis is threefold. Firstly, to examine the measurement reliability of a range of anthropometric measures, residual lung volume, air displacement plethysmography and hydrostatic weighing. Secondly, to establish reliability and precision of body composition measures used within existing <b>calibration</b> <b>models</b> which estimate whole body density from the criterion of hydrostatic weighing. Thirdly, to develop and cross-validate new <b>calibration</b> <b>models</b> for professional footballers. Further details are given in the full abstract above...|$|R
40|$|In this paper, the Azimuth cut-off {{procedure}} is first {{extended to the}} X-band and properly calibrated over COSMO-SkyMed© SAR measurements. The data set consists of 100 X-band COSMO-SkyMed© SAR data, collected in the Mediterranean Sea on 2009 - 2010 and gathered in different acquisition modes, i. e. StripMap HImage and ScanSAR Huge Region ones. Experimental results demonstrate that different <b>model</b> <b>calibrations</b> of the Azimuth cut-off procedure are needed for the StripMap and the ScanSAR mode...|$|R
