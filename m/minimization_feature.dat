2|26|Public
40|$|Abstract—Feature {{selection}} and feature transformation, {{the two main}} ways to reduce dimensionality, are often presented separately. In this paper, a feature selection method is proposed by combining the popular transformation based dimensionality reduction method Linear Discriminant Analysis (LDA) and spar-sity regularization. We impose row sparsity on the transformation matrix of LDA through ℓ 2; 1 -norm regularization to achieve feature selection, and the resultant formulation optimizes for selecting the most discriminative features and removing the redundant ones simultaneously. The formulation is extended to the ℓ 2;p-norm regularized case: which {{is more likely to}} offer better sparsity when 0 < p < 1. Thus the formulation is a better approximation to the feature selection problem. An efficient algorithm is developed to solve the ℓ 2;p-norm based optimization problem and it is proved that the algorithm converges when 0 < p 2. Systematical experiments are conducted to under-stand the work of the proposed method. Promising experimental results on various types of real-world data sets demonstrate the effectiveness of our algorithm. Index Terms—Feature selection, Linear discriminant analysis, ℓ 2;p-norm <b>minimization,</b> <b>Feature</b> redundancy. I...|$|E
40|$|The aim of {{this report}} is to {{introduce}} {{the concept of a}} centralized control strategy for an offshore wind farm and demonstrate it as an interesting solution for the offshore wind farms through showing its feasibility, analyzing its advantages and evaluating it from an energy production point of view. In this thesis work, first of all, a model of the wind turbine including the induction generator, the non-stiff shaft with a gear-box and the wind energy conversion is derived and is implemented in MATLAB/Simulink and PSCAD/EMTDC. In the next step, a control strategy based on the induction machine V/Hz control principles is derived which is aimed to maximize the aerodynamic conversion efficiency through setting an appropriate frequency for the offshore wind farm and also to minimize the induction generator and the transformer losses through setting an appropriate voltage level for the offshore wind farm. The latter case involves decreasing the fluxes in the generators and the transformers of the wind turbines at low wind speeds in order to decrease the losses in the cores of the wind turbine generators and transformers. The derived strategy is also implemented in the both simulation software and verified. The mentioned loss <b>minimization</b> <b>feature</b> is derived with expandability in mind so that it may be a base for the future work and also may be applied in systems not related to the wind systems. The convergence of the nonlinear centralized speed control algorithm is shown in MATLAB and the effectiveness of the derived control strategy in controlling the flux and the speed is demonstrated. In addition, the losses due to the removal of an individual control system in each wind turbine is also calculated and shown as percentage of the annual wind farm production. In the end of the report, it is concluded that in the offshore wind farms where an HVDC transmission is used, it is possible to remove the individual control systems along with the corresponding converters and replace them with a single larger converter in the HVDC station. In this case, a more advanced control strategy should be used for controlling the converter in the HVDC station. In this thesis work, it is shown {{that it is possible to}} derive a centralized control strategy. The control strategy that the controller follows should involve the maximization of the energy absorbed from the wind by the wind turbines through setting the correct reference grid frequency and thus the correct reference rotational speed for the whole wind farm and also compensate for the slip so that the aerodynamic efficiencies become maximum and also may involve minimizing the lost energy by increasing the efficiencies through setting the correct reference voltage level. It is shown in this work that the energy input to the wind turbine can be maximized by setting a correct rotational speed as the reference and the lost energy in the system can be minimized by lowering the flux magnitude at low wind speeds. It is also shown that in addition to being feasible, the percentage of the annual lost energy due to using this centralized strategy is less than 2 % in the worst case and moreover such losses decrease in the areas where the wind is less gusty...|$|E
50|$|Many SGML {{features}} {{relate to}} markup <b>minimization.</b> Other <b>features</b> relate to concurrent (parallel) markup (CONCUR), to linking processing attributes (LINK), and to embedding SGML documents within SGML documents (SUBDOC).|$|R
40|$|Current {{minimization}} {{programs do}} not permit full control over different aspects of minimization algorithm such as distance or probability measures and may not allow for unequal allocation ratios. This article describes the implementation of “MinimPy” an open-source minimization program in Python programming language, which provides full customizetion of <b>minimization</b> <b>features.</b> MinimPy supports naive and biased coin minimization together with various new and classic distance measures. Data syncing is provided to facilitate minimization of multicenter trial over the network. MinimPy can easily be modified to fit special needs of clinical trials and in particular change it to a pure web application, though it currently supports network syncing of data in multi-center trials using network repositories...|$|R
40|$|Decision {{trees for}} {{classification}} {{can be constructed}} using mathematical programming. Within decision tree algorithms, the <b>feature</b> <b>minimization</b> problem is to construct accurate decisions using as few features or attributes within each decision as possible. <b>Feature</b> <b>minimization</b> is {{an important aspect of}} data mining since it helps identify what attributes are important and helps produce accurate and interpretable decision trees. In <b>feature</b> <b>minimization</b> with bounded accuracy, we minimize the number of features using a given misclassification error tolerance. This problem can be formulated as a parametric bilinear program and is shown to be NP-complete. A parametric FrankWolfe method is used to solve the bilinear subproblems. The resulting minimization algorithm produces more compact, accurate, and interpretable trees. Computational results compare favorably with a popular greedy feature elimination method as well as with a linear programming method of tree construction. Key Words: Data mi [...] ...|$|R
40|$|A novel {{route is}} {{presented}} enabling <b>minimization</b> of <b>feature</b> sizes via laser ablative micro-patterning in highly porous silica aerogel monoliths and subsequent viscous sintering. Vitrification yields isotropically contracted silica solids preserving their original stereometric forms. The contraction depends on aerogel structure and porosity and we demonstrate here the first realization of a 3 : 1 ratio. Surface relief and void micropatterns {{inscribed in the}} monolith also undergo isotropic contraction and <b>feature</b> <b>minimization</b> beyond the spatial resolution of their original recording. Experimental results provide clear evidence that embedded void structures undergo contraction larger than the nominal stereometric scaling. This is a demonstration of a generic principle that enables fundamental physical resolution limits to be surpassed, leading to new avenues in micro- and nano-fabrication technologies. </span...|$|R
40|$|Abstract: In this paper, we {{proposed}} a new bundle adjustment algorithm based on the distance <b>minimization</b> of <b>feature</b> matches, to eliminate the accumulated errors over a sequence for seamless mosaic. The new feature based image mosaic approach improved {{the stability of the}} global registration among images. Our experiments show that the new mosaic method is robust. Key–Words: Feature based image mosaic, Bundle adjustment, Global registration...|$|R
40|$|Decision {{trees for}} {{classification}} {{can be constructed}} using mathematical programming. Within decision tree algorithms, the <b>feature</b> <b>minimization</b> problem is to construct accurate decisions using as few features or attributes within each decision as possible. <b>Feature</b> <b>minimization</b> is {{an important aspect of}} data mining since it helps identify what attributes are important and helps produce accurate and interpretable decision trees. In <b>feature</b> <b>minimization</b> with bounded accuracy, we minimize the number of features using a given misclassification error tolerance. This problem can be formulated as a parametric bilinear program and is shown to be NP-complete. A parametric FrankWolfe method is used to solve the bilinear subproblems. The resulting minimization algorithm produces more compact, accurate, and interpretable trees. This procedure can be applied to many di#erent error functions. Formulations and results for two error functions are given. One method, FM RLP-P, dramatically reduced the number of features of one dataset from 147 to 2 while maintaining an 83. 6 % testing accuracy. Computational results compare favorably with the standard univariate decision tree method, C 4. 5, as well as with linear programming methods of tree construction. Key Words: Data mining, machine learning, <b>feature</b> <b>minimization,</b> decision trees, bilinear programming. # Knowledge Discovery and Data Mining Group, Department of Mathematical Sciences, Rensselaer Polytechnic Institute, Troy, NY 12180. Email bredee@rpi. edu, bennek@rpi. edu. Telephone (518) 276 - 6899. FAX (518) 276 - 4824. This material is based on research supported by National Science Foundation Grant 949427. ...|$|R
40|$|We {{propose a}} 3 D mesh curving method that {{converts}} a straight-sided mesh to an optimal-quality curved high-order mesh that interpolates a CAD boundary representation. The main {{application of this}} method is the generation of discrete approximations of curved domains that are valid for simulation analysis with unstructured high-order methods. We devise the method as follows. First, the boundary of a straight-sided high-order mesh is curved to match the curves and surfaces of a CAD model. Second, the method minimizes the volume mesh distortion {{with respect to the}} coordinates of the inner nodes and the parametric coordinates of the curve and surface nodes. The proposed <b>minimization</b> <b>features</b> untangling capabilities and therefore, it repairs the invalid elements that may arise from the initial curving step. Compared with other mesh curving methods, the only goal of the proposed residual system is to minimize the volume mesh distortion. Furthermore, it is less constrained since the boundary nodes are free to slide on the CAD curves and surfaces. Hence, the proposed method is well suited to generate curved high-order meshes of optimal quality from CAD models that contain thin parts or high-curvature entities. To illustrate these capabilities, we generate several curved high-order meshes from CAD models with the implementation detailed in this work. Specifically, we detail a node-by-node non-linear iterative solver that minimizes the proposed objective function in a block Gauss-Seidel manner. Peer ReviewedPostprint (author's final draft...|$|R
3000|$|... 2 norm <b>minimization</b> {{that have}} <b>featured</b> among speaker {{recognition}} studies {{is that the}} discriminative nature of sparse regression techniques implies an indirect manipulation of the weights in the entries of the regression solution to ensure sparseness of the solution {{in such a way}} that this manipulation could be disrupting the original or intrinsic weightings of the individual speakers. As an example, for the case of ℓ [...]...|$|R
40|$|AbstractWe {{present a}} method for {{computing}} a posteriori error bounds for piecewise linear nonconforming approximate solution of elliptic equations. The upper error estimator is a quadratic function of free parameters. Optimal error bounds are obtained by solving a quadratic <b>minimization</b> problem. The <b>feature</b> of the method is that error estimates do not require the high regularity for the exact solution of the original problem...|$|R
40|$|Selective area epitaxy (SAE) is {{emerging}} as an important technology in the fabrication of optoelectronic integrated circuits. This paper reviews the current growth technologies for their applicability {{to the process of}} SAE. It discusses in detail the interaction on the masked area with the adjoining epitaxial window. The <b>minimization</b> of the <b>features</b> formed by this interaction whilst optimizing selectivity is seen as the main aim of the growth processes...|$|R
40|$|The {{alternating}} direction {{method of}} multipliers (ADMM) is a flexible method {{to solve a}} large class of convex <b>minimization</b> problems. Particular <b>features</b> are its unconditional convergence {{with respect to the}} involved step size and its direct applicability. This article deals with the ADMM with variable step sizes and devises an adjustment rule for the step size relying on the monotonicity of the residual and discusses proper stopping criteria. The numerical experiments show significant improvements over established variants of the ADMM...|$|R
40|$|We {{propose a}} linear {{regression}} method for estimating Weibull parameters from life tests. The method uses stochastic {{models of the}} unreliability at each failure instant. As a result, a heteroscedastic regression problem arises that is solved by weighted least squares <b>minimization.</b> The main <b>feature</b> of our method is an innovative s-normalization of the failure data models, to obtain analytic expressions of centers and weights for the regression. The method has been Monte Carlo contrasted with Benard?s approximation, and Maximum Likelihood Estimation; {{and it has the}} highest global scores for its robustness, and performance...|$|R
40|$|In {{the setting}} of antiplane linearized elasticity, we show the {{existence}} of quasistatic evolutions of cracks in brittle materials by using a vanishing viscosity approach, thus taking into account local <b>minimization.</b> The main <b>feature</b> of our model is that the path followed by the crack need not be prescribed a priori: indeed, it is found as the limit (in the sense of Hausdorff convergence) of curves obtained by an incremental procedure. The result {{is based on a}} continuity property for the energy release rate in a suitable class of admissible cracks...|$|R
40|$|The feature {{selection}} {{problem can be}} formulated as a multi-objective optimization (MOO) problem, as it involves the <b>minimization</b> of the <b>feature</b> subset cardinality and the misclassification error. In this chapter, a comparison of MOO algorithms applied to {{feature selection}} is presented. The used MOO methods are: Nondominated Sorting Genetic Algorithm II (NSGA-II), Archived Multi Objective Simulated Annealing (AMOSA), and Direct Multi Search (DMS). To test the feature subset solutions, Takagi- Sugeno fuzzy models are used as classifiers. To solve the feature selection problem, AMOSA was adapted to deal with discrete optimization. The multi-objective methods are applied to four benchmark datasets used in the literature and the obtained results are compared and discussed. info:eu-repo/semantics/publishedVersio...|$|R
40|$|Abstract—Feature {{selection}} {{plays an}} important role in pattern recognition and machine learning. Feature evaluation and classifi-cation complexity estimation arise as key issues in the construction of selection algorithms. To estimate classification complexity in different feature subspaces, a novel feature evaluation measure, called the neighborhood decision error rate (NDER), is proposed, which is applicable to both categorical and numerical features. We first introduce a neighborhood rough-set model to divide the sample set into decision positive regions and decision boundary regions. Then, the samples that fall within decision boundary regions are further grouped into recognizable and misclassified subsets based on class probabilities that occur in neighborhoods. The percentage of misclassified samples is viewed as the estimate of classification complexity of the corresponding feature subspaces. We present a forward greedy strategy for searching the feature subset, which minimizes the NDER and, correspondingly, mini-mizes the classification complexity of the selected feature subset. Both theoretical and experimental comparison with other feature selection algorithms shows that the proposed algorithm is effective for discrete and continuous features, as well as their mixture. Index Terms—Continuous <b>feature,</b> decision error <b>minimization,</b> discrete <b>feature,</b> feature selection, neighborhood, rough sets. I...|$|R
40|$|Phrase-based {{language}} {{models have}} been recognized to have an advantage over word-based language models since they allow us to capture long spanning dependencies. Class based language {{models have been}} used to improve model generalization and overcome problems with data sparseness. In this paper, we present a novel approach for combining the phrase acquisition with class construction process to automatically acquire phrase-grammar fragments from a given corpus. The phrase-grammar learning is decomposed into two sub-problems, namely the phrase acquisition and feature selection. The phrase acquisition is based on entropy <b>minimization</b> and the <b>feature</b> selection. is driven by the entropy reduction principle. We further demonstrate that the phrasegrammar based n-gram language model significantly outperforms a phrase-based n-gram language model in an end-to-end evaluation of a spoken language application...|$|R
40|$|Robust pattern {{recognition}} within the Bayesian framework for scene segmentation/boundary detection is oftentimes {{hampered by the}} presence of textures within natural images. In order to improve segmentation/boundary detection on natural images, it is necessary to combine multiple features effectively. This paper introduces two algorithms for combining both color and texture features to assist boundary detection processes. One is to combine features through the surface processes and the other through the line processes. The algorithms can be generalized for combining any number of feature sets. Keywords: boundary detection, Bayesian model, texture, <b>features,</b> <b>minimization</b> 1 Introduction In this paper, color features are defined as features obtained by pixel-wise operations such as scaling and mean subtraction on the original data, while texture features are defined as features obtained by combination of pixel-wise operations and local operations such as averaging and linear filters. The [...] ...|$|R
40|$|When {{training}} perceptrons (linear classifiers) {{not only}} the performance on the training set is important but also the number of features that are actually needed. For instance, if {{only a small fraction}} of features suffice to correctly classify all given examples, a training algorithm that also minimizes the number of nonzero weights would require far fewer examples to achieve a given level of accuracy in the testing phase than any algorithm that does not try to discard irrelevant features. To investigate the computational complexity of the problem of designing compact networks, we first focus on linearly separable training sets and we improve the nonapproximability bound for the corresponding <b>feature</b> <b>minimization</b> problem. In particular, we disprove a conjecture by van Horn and Martinez and show that no polynomial time algorithm is guaranteed to yield a weight vector that correctly classifies all training examples and whose number of nonzero weights is within a logarithmic factor of the [...] ...|$|R
40|$|This paper {{presents}} a semi-supervised learning (SSL) approach to find similarities of images using statistics of local matches. SSL algorithms {{are well known}} for leveraging {{a large amount of}} unlabeled data as well as a small amount of labeled data to boost classification performance. Our approach proposes to formulate the problem of matching two images as an SSL based classification problem of image pairs with a minimal amount of labeled pairs. We apply a Gaussian random field model to represent each image pair as vertices in a weighted graph and the optimal configuration of the field is obtained by harmonic energy <b>minimization.</b> A symmetrical <b>feature</b> selection criterion is first introduced to select robust matches of local keypoints between two images. The Mallows distance is then adopted to combine multiple cues from statistics of local matches. Our experiments confirm that our SSL based approach not only boost classification performance but also improve robustness of the learned category model using only simple local keypoint features...|$|R
40|$|International audienceIn {{this paper}} we discuss {{foreground}} detection and human body silhouette extraction and tracking in monocular video systems designed for human motion analysis applications. Vision algorithms face many challenges {{when it comes to}} analyze human activities in non-controlled environments. For instance, issues like illumination changes, shadows, camouflage and occlusions make the detection and the tracking of a moving person a hard task to accomplish. Hence, advanced solutions are required to analyze the content of video sequences. We propose a real-time, two-level foreground detection, enhanced by body parts tracking, designed to efficiently extract person silhouette for monocular video-based human motion analysis systems. We aim to find solutions for different non-controlled environment challenges, which make the detection and the tracking of a moving person a hard task to accomplish. On the first level, we propose an enhanced Mixture of Gaussians, built on both chrominance-luminance and chrominance-only spaces, which handles global illumination changes. On the second level, we improve segmentation results, in interesting areas, by using statistical foreground models updated by a high-level tracking of body parts. Each body part is represented with a set of template characterized by a feature vector built in an initialization phase. Then, high level tracking is done by finding blob-template correspondences via distance <b>minimization</b> in <b>feature</b> space. Correspondences are then used to update foreground models, and a graph cut algorithm, which minimizes a Markov random field energy function containing these models, is used to refine segmentation. We were able to extract a refined silhouette in the presence of light changes, noise and camouflage. Moreover, the tracking approach allowed us to infer information about the presence and the location of body parts even in the case of partial occlusion...|$|R
40|$|Computer vision [...] ACCV 2016 : 13 th Asian Conference on Computer Vision, Taipei, Taiwan, November 20 - 24, 2016 Speckle {{reduction}} {{is a crucial}} prerequisite of many computer-aided ultrasound diagnosis and treatment systems. However, most of existing speckle reduction filters concentrate the blurring near features and introduced the hole artifacts, making the subsequent processing procedures complicated. Optimization-based methods can globally distribute such blurring, leading to better feature preservation. Motivated by this, we propose a novel optimization framework based on L 0 L 0 <b>minimization</b> for <b>feature</b> preserving ultrasound speckle reduction. We observed that the GAP, which integrates gradient and phase information, is extremely sparser in despeckled images than in speckled images. Based on this observation, we propose the L 0 L 0 minimization framework to remove speckle noise and simultaneously preserve features in ultrasound images. It seeks for the L 0 L 0 sparsity of the GAPGAP values, and such sparsity is achieved by reducing small GAPGAP values to zero in an iterative manner. Since features have larger GAPGAP magnitudes than speckle noise, the proposed L 0 L 0 minimization is capable of effectively suppressing the speckle noise. Meanwhile, the rest of GAPGAP values corresponding to prominent features are kept unchanged, leading to better preservation of those features. In addition, we propose an efficient and robust numerical scheme to transform the original intractable L 0 L 0 minimization into several sub-optimizations, from which we can quickly find their closed-form solutions. Experiments on synthetic and clinical ultrasound images demonstrate that our approach outperforms other state-of-the-art despeckling methods in terms of noise removal and feature preservation. School of Nursing 2016 - 2017 > Academic research: not refereed > Chapter in an edited book (author) bcw...|$|R
40|$|Tracking {{a target}} in {{infrared}} (IR) sequences is a challenging task {{because of low}} resolution, low signal-to-noise ratios, occlusion, and poor target visibility. For many civil and military applications, the realtime requirement is always a key factor for tracking algorithms to be used. This undoubtedly makes tracking in IR sequences more difficult. This paper presents a real-time IR target tracking under complex conditions based on l 1 <b>minimization</b> and compressive <b>features.</b> First, we adopt a sparse measurement matrix to project the high-dimensional Harr-like features to low-dimensional features that are applied to the appearance modeling. This appearance model allows significant reduction in the computational cost of the target-tracking phase. Then, the appearance model is introduced into {{the framework of the}} popular l 1 tracker. Each IR target candidate is represented by the appearance template based on the structure of sparse representation. Finally, the candidate that has the minimum reconstruction error is selected as the tracking result. The proposed tracking method can combine the real-time advantages of the compressive tracking and the robustness of the l 1 tracker. Experimental results on challenging IR image sequences including both aerial targets and ground targets show that the proposed algorithm has better robustness and real-time performance in comparison with two state-of-the-art tracking algorithms. authorsversionPeer reviewe...|$|R
40|$|In {{this paper}} {{computational}} intelligence, referring {{here to the}} synergy of neural networks and genetic algorithms, is deployed {{in order to determine}} a near-optimal neural network for the classification of dark formations in oil spills and look-alikes. Optimality is sought in the framework of a multi-objective problem, i. e. the <b>minimization</b> of input <b>features</b> used and, at the same time, the maximization of overall testing classification accuracy. The proposed method consists of two concurrent actions. The first is the identification of the subset of features that results in the highest classification accuracy on the testing data set i. e. feature selection. The second parallel process is the search for the neural network topology, in terms of number of nodes in the hidden layer, which is able to yield optimal results with respect to the selected subset of features. The results show that the proposed method, i. e. concurrently evolving features and neural network topology, yields superior classification accuracy compared to sequential floating forward selection as well as to using all features together. The accuracy matrix is deployed to show the generalization capacity of the discovered neural network topology on the evolved sub-set of features. JRC. G. 6 -Sensors, radar technologies and cybersecurit...|$|R
40|$|Automatic image {{registration}} {{is the process}} of aligning two or more images of approximately the same scene with minimal human assistance. Wavelet-based automatic registration methods are standard, but sometimes are not robust to the choice of initial conditions. That is, if the images to be registered are too far apart relative to the initial guess of the algorithm, the registration algorithm does not converge or has poor accuracy, and is thus not robust. These problems occur because wavelet techniques primarily identify isotropic textural features and are less effective at identifying linear and curvilinear edge features. We integrate the recently developed mathematical construction of shearlets, which is more effective at identifying sparse anisotropic edges, with an existing automatic wavelet-based registration algorithm. Our shearlet features algorithm produces more distinct features than wavelet features algorithms; the separation of edges from textures is even stronger than with wavelets. Our algorithm computes shearlet and wavelet features for the images to be registered, then performs least squares <b>minimization</b> on these <b>features</b> to compute a registration transformation. Our algorithm is two-staged and multiresolution in nature. First, a cascade of shearlet features is used to provide a robust, though approximate, registration. This is then refined by registering with a cascade of wavelet features. Experiments across a variety of image classes show an improved robustness to initial conditions, when compared to wavelet features alone...|$|R
40|$|Heterogeneous face {{recognition}} (HFR) aims to match facial images acquired from different sensing modalities with mission-critical applications in forensics, security and commercial sectors. However, HFR {{is a much}} more challenging problem than traditional {{face recognition}} because of large intra-class variations of heterogeneous face images and limited training samples of cross-modality face image pairs. This paper proposes a novel approach namely Wasserstein CNN (convolutional neural networks, or WCNN for short) to learn invariant features between near-infrared and visual face images (i. e. NIR-VIS face recognition). The low-level layers of WCNN are trained with widely available face images in visual spectrum. The high-level layer is divided into three parts, i. e., NIR layer, VIS layer and NIR-VIS shared layer. The first two layers aims to learn modality-specific features and NIR-VIS shared layer is designed to learn modality-invariant feature subspace. Wasserstein distance is introduced into NIR-VIS shared layer to measure the dissimilarity between heterogeneous feature distributions. So W-CNN learning aims to achieve the minimization of Wasserstein distance between NIR distribution and VIS distribution for invariant deep feature representation of heterogeneous face images. To avoid the over-fitting problem on small-scale heterogeneous face data, a correlation prior is introduced on the fully-connected layers of WCNN network to reduce parameter space. This prior is implemented by a low-rank constraint in an end-to-end network. The joint formulation leads to an alternating <b>minimization</b> for deep <b>feature</b> representation at training stage and an efficient computation for heterogeneous data at testing stage. Extensive experiments on three challenging NIR-VIS face recognition databases demonstrate the significant superiority of Wasserstein CNN over state-of-the-art methods...|$|R
40|$|Turbulence is a {{ubiquitous}} {{phenomenon that}} occurs throughout the universe, in both neutral fluids and plasmas. For collisionless plasmas, kinetic effects, which alter the nonlinear dynamics {{and result in}} small-scale dissipation, are still not well understood {{in the context of}} turbulence. This work uses direct numerical simulations (DNS) and observations of Earth 2 ̆ 7 s magnetosphere to study plasma turbulence. Long-time relaxation in magnetohydrodynamic (MHD) turbulence is examined using DNS with particular focus on the role of magnetic and cross helicity and symmetries of the initial con-figurations. When strong symmetries are absent or broken through perturbations, flows evolve towards states predicted by statistical mechanics with an energy <b>minimization</b> principle, which <b>features</b> two main regimes; one magnetic helicity dominated and one with quasi-equipartition of kinetic and magnetic energy. The role of the Hall effect, which contributes to the dynamics of collisionless plasmas, is also explored numerically. At scales below the ion inertial length, a transition to a magnetically dominated state, associated with advection becoming subdominant to dissipation, occurs. Real-space current, vorticity, and electric fields are examined. Strong current structures are associated with alignment between the current and magnetic field, which may be important in collisionless plasmas where field-aligned currents can be unstable. Turbulence within bursty bulk ow braking events, thought to be associated with near-Earth magnetotail reconnection, are then studied using the THEMIS spacecraft. It is proposed that strong field-aligned currents associated with turbulent intermittency destabilize into double layers, providing a collisionless dissipation mechanism for the turbulence. Plasma waves may also radiate from the region, removing energy from the turbulence and potentially depositing it in the aurora. Finally, evidence for turbulence in the Kelvin-Helmholtz instability (KHI) on the Earth 2 ̆ 7 s magnetopause is found using data from the Magnetospheric Multiscale (MMS) mission. With MMS, spatial properties, including spatial intermittency and anisotropy, can be examined along with temporal properties and ion and electron velocity spectra can be examined observationally into the kinetic scales. Quasi-two-dimensional anisotropy perpendicular to the magnetic field is found. Field-aligned current instabilities and wave radiation may also be relevant in the KHI...|$|R

