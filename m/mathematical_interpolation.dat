33|32|Public
50|$|Peter Biľak's Karloff is a {{sophisticated}} revival designed partly through <b>mathematical</b> <b>interpolation.</b> Biľak's group digitally inverted {{the contrast of}} a conventional Didone font: this allowed Biľak to create a normal and matching reverse-contrast font, together with a low-contrast slab serif design all with the same basic structure, named Karloff Positive, Negative and Neutral with an upper and lower case. Village Type's Arbor like Karloff adds a lower-case, while Match & Kerosene's Slab Sheriff is caps-only, with a 'A' featuring the conventional stress on the right. Other unreleased revivals have reportedly been made for private use by Paul Barnes and Justin Howes.|$|E
50|$|GSI3D utilises {{a digital}} {{elevation}} model, surface geological linework and downhole borehole and geophysical data {{to enable the}} geologist to construct cross sections by correlating boreholes and the outcrops to produce a geological fence diagram. <b>Mathematical</b> <b>interpolation</b> between the nodes along the drawn sections {{and the limits of}} the units produces a solid model comprising a stack of triangulated objects each corresponding to one of the geological units present. Scientists draw their sections based on facts such as borehole logs correlated by intuition - the shape 'looks right' to a geologist. This 'looks right' element pulls on the geologists' wealth of understanding of earth processes, examination of exposures and theoretical knowledge gathered over a career in geology. GSI3D enables the efficient capture of tacit and implicit knowledge which was until now trapped in geologist's heads.|$|E
40|$|This {{article is}} devoted {{to the study of the}} ASARCO {{demolition}} seismic data. Two different classes of modeling techniques are explored: First, <b>mathematical</b> <b>interpolation</b> methods and second statistical smoothing approaches for curve fitting. We estimate the characteristic parameters of the propagation medium for seismic waves with multiple mathematical and statistical techniques, and provide the relative advantages of each approach to address fitting of such data. We conclude that <b>mathematical</b> <b>interpolation</b> techniques and statistical curve fitting techniques complement each other and can add value to the study of one dimensional time series seismographic data: they can be use to add more data to the system in case the data set is not large enough to perform standard statistical tests...|$|E
40|$|A ground {{penetrating}} radar survey {{has been}} aplied to an archeological site before its excavation. GPR survey has been very useful to localize and characterize {{the remains of the}} Augustinian monastery of Fraga (Huesca). The presence of metallic remains at the surface and topographic anomalies hasn´t allowed us to use <b>mathematical</b> <b>interpolations</b> to produce 3 D anomaly maps. The characterization has been made manually with the definition of several degrees of reliability. These degrees allow us to make excavation guides very comprenhensive where we can express all the information furnished by the GPR profile...|$|R
5000|$|In {{the field}} of <b>mathematical</b> analysis, an <b>interpolation</b> {{inequality}} is an inequality of the form ...|$|R
5000|$|In a <b>mathematical</b> context, {{bilinear}} <b>interpolation</b> is {{the problem}} of finding a function f(x,y) of the form ...|$|R
40|$|Fix a {{internal}} thread high CPU occupied problem Add a data fitting module using LeastSquares method in dll Microsoft. VisualBasic. Data. Bootstrapping. Fittings. dll Add various spline interpolation methods in namespace Microsoft. VisualBasic. <b>Mathematical.</b> <b>Interpolation</b> csv serialization module improvements: add custom cell parser in column attribute, you can implements your object parser by implement the interface Microsoft. VisualBasic. Data. csv. StorageProvider. Reflection. IParser All possible system states character by using monte-carlo method in module: Microsoft. VisualBasic. Data. Bootstrapping. MonteCarlo. StatesCharacter...|$|E
40|$|Add Darwinism {{computing}} modules, these module includes: Genetic Algorithm Framework Difference Evolution module you can {{found these}} module in namespace: Microsoft. VisualBasic. DataMining. Darwinism Add two curve spline Interpolation module in namespace Microsoft. VisualBasic. <b>Mathematical.</b> <b>Interpolation,</b> includes: b-spline and cubic-spline Add a new random generator {{based on the}} random number table, which is avaliable at Microsoft. VisualBasic. Mathematical. Randomizer A data parameter estimates method based on the genetic algorithm, protocol is avaliable in Microsoft. VisualBasic. Data. Bootstrapping. Darwinism Renames the odes solver as Microsoft. VisualBasic. Mathematical. ODEsSolver. dl...|$|E
40|$|In {{this chapter}} we present Kriging— {{also known as}} a Gaussian process (GP) model— which is a <b>mathematical</b> <b>interpolation</b> method. To select the input {{combinations}} to be simulated, we use Latin hypercube sampling (LHS); we allow uniform and non-uniform distributions of the simulation inputs. Besides deterministic simulation we discuss random simulation, which requires adjusting the design and analysis. We discuss sensitivity analysis of simulation models, using "functional analysis of variance" (FANOVA) — also known as Sobol sensitivity indexes. Finally, we discuss optimization of the simulated system, including "robust" optimization...|$|E
3000|$|An {{underlying}} <b>mathematical</b> {{model for}} <b>interpolation</b> {{must also be}} specified to describe what happens for intermediary values x∈]X [...]...|$|R
40|$|Over {{the past}} decade, {{a large number}} of {{numerical}} models have been developed to predict heat and moisture transfer within building envelopes. In these models, the moisture transfer mechanism has been described and correlated by reference to the various transport phenomena and corresponding theories, viz. heat transfer and fluid flow. However, predicting the coupled heat and moisture performance of a building construction has never been a straightforward task, since a steady state situation hardly ever occurs and the transport properties (heat and moisture) of a material vary with moisture content and temperature. This paper discusses the transport phenomenon and the various numerical algorithms used in the discretization equations and how different algorithms affect the modelled results. Computer simulations have been conducted for different building materials and material combinations and comparisons have been made to evaluate the selection of discretized transport properties. Discrepancies in results are demonstrated between different <b>mathematical</b> <b>interpolations,</b> namely the Resistance (R) type formula and Linear (L) interpolation. Recommendations are given as guidance towards applying the most appropriate formulations for a given modelling scenario...|$|R
40|$|The {{demographic}} data of a territory, or society, can give valuable information of an analysed group of interest, but appropriate {{demographic data}} must be available. This {{is not the}} case of the Crown of Aragon in the medieval period. Censuses or other statistical data are limited. They do not allow us to do population estimates for the whole period of the Late Middle Ages. The objective {{of this paper is to}} present a <b>mathematical</b> tool, <b>interpolation,</b> which allows to estimate the late-medieval population of the Crown of Aragon from fiscal censuses and published projections and estimates. The demographic data of a territory, or society, can give valuable information of an analysed group of interest, but appropriate demographic data must be available. This {{is not the case}} of the Crown of Aragon in the medieval period. Censuses or other statistical data are limited. They do not allow us to do population estimates for the whole period of the Late Middle Ages. The objective of this paper is to present a <b>mathematical</b> tool, <b>interpolation,</b> which allows to estimate the late-medieval population of the Crown of Aragon from fiscal censuses and published projections and estimates...|$|R
40|$|Summary: The {{dramatic}} increase of applying mathematical concepts and computational techniques to food microbiology questions has {{lead to a}} discipline called “predictive microbiology”. It is focussing mainly on the description of microbial responses to food environments by mathematical models. Its aim {{is more than the}} mere collection and computational representation of microbial observations, possibly <b>mathematical</b> <b>interpolation.</b> With the accumulation of data and experience, qualitative features are becoming constraints for the mathematical models to be created, thus moving towards mechanistic modelling. In this development, both mathematical models and microbiology databases have played crucial roles...|$|E
40|$|Algebraic grid {{generation}} is the direct {{expression of a}} physical coordinate system {{as a function of}} a uniform grid in a rectangular computational coordinate system. Algebraic grid {{generation is}} based on <b>mathematical</b> <b>interpolation</b> and is presented in general terms of multivariate transfinite interpolation. The multisurface method and the two-boundary technique are described as univariate procedures that can be applied within the context of transfinite interpolation. A technique for grid clustering is described. Problems that are commonly encountered in three-dimensional grid generation are discussed and approaches for dealing with complex physical domains using multiple computational grid blocks are presented...|$|E
40|$|One of {{the major}} {{approaches}} to numerical grid generation is the explicit algebraic expression of a physical grid {{as a function of}} a uniform grid in a rectangular computational coordinate system. The algebraic methods are based on <b>mathematical</b> <b>interpolation,</b> and the primary advantages are speed and directness. The relation between interpolation and grid generation is described. For three-dimensional grid generation, transfinite interpolation using the coordinate control processes developed in the multisurface method and two-boundary technique are advocated. Grid singularities encountered in three dimensions are discussed, and the exploration of multiple overlapping grids is proposed. Some aspects of interactive algebraic grid computation in three dimensions are discussed...|$|E
40|$|The {{problem of}} C 2 {{interpolation}} of a discrete {{set of data}} on the interval [a,b] representing the function f using quartic splines is investigated. An explicit scheme of interpolation is obtained using different quartic splines on even and odd subintervals of <b>interpolation.</b> <b>Mathematical</b> Subject Classification: 41 A 05, 41 A 1...|$|R
5000|$|In {{the field}} of <b>mathematical</b> analysis, an <b>interpolation</b> space is a space which lies [...] "in between" [...] two other Banach spaces. The main {{applications}} are in Sobolev spaces, where spaces of functions that have a noninteger number of derivatives are interpolated from the spaces of functions with integer number of derivatives.|$|R
40|$|Abstract. An {{overview}} of the essential features of groundwater transport of radio-active contaminants in a saturated porous media is presented and used in an inte-grated bi-dimensional phenomenological model of transport and fate. The conception and the assumptions implicit in the model are described. The output results are then compared with values estimated by different <b>mathematical</b> space <b>interpolation</b> techniques applied to experimental sample measurements obtained in the surroundings of a contaminated site. These interpolation methods allowed evaluating the spatial variability of the contamination, defining the contour of the plume. These values are then compared to those produced by the transport and fate model. This methodology was applied to uranium and radium, due to their special environmental concern...|$|R
40|$|The digital {{surface model}} (DSM) {{is used for}} several {{purposes}} in photogrammetry and remote sensing such as orthoimage production, contours derivation, extraction of high information such as buildings and trees, and used in GIS. Creation of a DSM from 3 -D sparse points that {{can be derived from}} stereo imagery, range data (e. g. laser data) can be done with several <b>mathematical</b> <b>interpolation</b> models. In this paper, thin plate spline is used for digital surface modeling. Determination of suitable weight is an important problem in thin plate function for a surface. The Voronoi algorithm has been proposed as a method for determination of the weights in thin plate spline. In this paper, we tested the method for different surfaces. The results shown that the thin plate spline can be independent of weight. TS 26 Positioning and Measurement Technologies and Practices I...|$|E
40|$|Use of <b>mathematical</b> <b>interpolation</b> in Digital Signal Processing {{applications}} {{often seems}} to be a remarkable solution when applied for noise reduction. In the last two decades advancement in the Elasticity imaging of tissue is worth mentioning. Two dimensional spline technique for generating Elastograms is fairly a new approach for generating ultrasonic Elastograms. In the way of analyzing imaging modalities generally three parameters are taken into account resolution, SNRe and CNRe. In ultrasound elastography spline based method for axial strain estimation is well-established in the literature. In this paper we have shown the possibilities of 2 D spline which mainly works on a plate of experimental data considering both axial and the lateral directions. We have also analyzed the improvement of performance while utilizing this method comparing with other wellestablished techniques such as simple 1 D Smoothing Spline and the Adaptive Strain Estimation technique. Keywords...|$|E
40|$|When {{dealing with}} the {{observation}} with missing values, {{we used to get}} them by means of <b>mathematical</b> <b>interpolation.</b> Compared with the traditional methods for parametric interpolation including linear interpolation, spline interpolation, kriging interpolation, etc., which sometimes export so paradoxical results that there are quite a lot of debates on the reliability of rationale and application, the non-parametric methods {{are becoming more and more}} popular to interpolate the missing values for the cross sectional dataset. In this paper, a non-parametric method is introduced and its feasibility of filling in missing values of per capita GDP data at county level for China is illustrated and verified. The results indicate that the non-parametric method produces essentially unbiased estimates by using kernel density function based on a sample drawn from all the observations. So it appears that the actual performance of non-parametric model can be quite helpful to fill in the missing values with a large sample of observation and the non-parametric extrapolation methods tested in this empirical study could be applied in other similar studies...|$|E
40|$|The Langley Research Center and Virginia Institute of Marine Science wave {{refraction}} computer model {{was applied to}} the Baltimore Canyon region of the mid-Atlantic continental shelf. Wave refraction diagrams {{for a wide range of}} normally expected wave periods and directions were computed by using three bottom topography approximation techniques: quadratic least squares, cubic least squares, and constrained bicubic <b>interpolation.</b> <b>Mathematical</b> or physical interpretation of certain features appearing in the computed diagrams is discussed...|$|R
40|$|Abstract — This paper {{introduces}} a novel method of approximate calculation of delay in voice over IP systems. The proposed method relies on <b>mathematical</b> operations like <b>interpolation</b> and integration {{to produce a}} continuous function. This continuous function shows the delay in different times. The paper shows a detailed example of applying this method successfully to a sample call. The resulting delay function can help in {{better understanding of the}} variations of delay in different times and how it is affected by network load. Keywords- delay, ip, voice over ip, voip I...|$|R
50|$|In <b>mathematical</b> logic, Craig's <b>interpolation</b> theorem is {{a result}} about the {{relationship}} between different logical theories. Roughly stated, the theorem says that if a formula φ implies a formula ψ, and the two have at least one atomic variable symbol in common, then there is a third formula ρ, called an interpolant, such that every nonlogical symbol in ρ occurs both in φ and ψ, φ implies ρ, and ρ implies ψ. The theorem was first proved for first-order logic by William Craig in 1957. Variants of the theorem hold for other logics, such as propositional logic. A stronger form of Craig's theorem for first-order logic was proved by Roger Lyndon in 1959; the overall result is sometimes called the Craig - Lyndon theorem.|$|R
40|$|Abstract Although {{local and}} {{regional}} instrumental recordings of the devastating 26, January 2001, Bhuj earthquake are sparse, the distribution of macroseismic ef-fects can provide important constraints on the mainshock ground motions. We com-piled available news accounts describing damage and other effects and interpreted them to obtain modified Mercalli intensities (MMIs) at 200 locations throughout the Indian subcontinent. These values are then used to map the intensity distribution throughout the subcontinent using a simple <b>mathematical</b> <b>interpolation</b> method. Al-though preliminary, the maps reveal several interesting features. Within the Kachchh region, the most heavily damaged villages are concentrated toward {{the western edge of}} the inferred fault, consistent with western directivity. Significant sediment-induced amplification is also suggested at a number of locations around the Gulf of Kachchh to the south of the epicenter. Away from the Kachchh region, intensities were clearly amplified significantly in areas that are along rivers, within deltas, or on coastal alluvium, such as mudflats and salt pans. In addition, we use fault-rupture parameters inferred from teleseismic data to predict shaking intensity at distances of 0 – 1000 km...|$|E
40|$|This thesis {{deals with}} the {{technology}} of the high pressure hydrolysis with nitric acid (HNO 3) in biogass production from the hay. The theoretical part {{is focused on the}} basic information about the acid hydrolisis. Much attention is ingaged in the lignocellulosic materials and methods of their treatment. The hay was crushed, pelleted and subsequently subjected the acid hydrolysis in the high-pressure hydrolyzer (UV CZ 21314) at pressures (0, 475 MPa, 0, 934 MPa, 1, 611 MPa). As the hydrolysis reagent was used highly concentrated (65 %) nitric acid (HNO 3). The pressure was achieved in the hydrolyzer by the steam in temparature of 190 °C, the residence time of the phytomass in the machine was 500 s. Based on the mapping process with wide CO 2 production were selected the interesting areas, which were subsequently carried out detailed mapping process using batch simulations at CH 4 production. After the <b>mathematical</b> <b>interpolation</b> of the maxima the values were used in the economic analysis, that fully respects the technological possibilities and legislative constraints...|$|E
40|$|The Survey of Israel (SOI) has 849 anchor-points, where “orthometric ” and “ellipsoidal” heights {{are known}} [the {{quotation}} marks indicate low accuracy for these terms]. On this basis, a kriging process established the Israel Undulation Model (currently ILUM 1. 2), by purely <b>mathematical</b> <b>interpolation.</b> A research project {{was funded by}} SOI- to improve this official (statutory) model by gravimetric interpolation (parts A+C+D+E). The method chosen is Remove/Restore (R/R), whereby all known contributions to the undulations are accounted for. Data sources are: observed gravity in Israel, measured free-air anomalies over the seas (Med and Red), and Bouguer anomalies in neighboring areas. All data were converted to free-air anomalies on a 1 -minute grid, on the WGS 84 ellipsoid. The global gravity model EGM 2008 was utilized as a reference for the process – for both undulations and gravity anomalies. A Stokes integration, to 2 -degrees (beyond Israel borders) was performed. The full process includes: remove model undulations, Stokes contributions (of residual anomalies, known minus model), and Indirect Effect, at anchor-points – to obtain residua...|$|E
40|$|Abstract – Modern {{building}} technologies (for example {{prestressed concrete}} to produce beams, floors, bridges, or installation, testing and monitoring of ground anchors) {{require the use}} of accurate force machines (hydraulic jacks). One component of successful building works is a correct and accurate calibration procedure of hydraulic jack for tensioning cables or anchors. This article analyzes two different calibration methods: By means of hollow load cell and thread bar and calibration in the closed frame. We analyze these methods using different accessories and their influence on the uncertainty of calibration results. As {{a very important part of}} calibration procedure is a correct choice of the calibration <b>mathematical</b> model (<b>interpolation</b> curve force-hydraulic pressure and hydraulic pressureforce), there is a need to use a special software. Such is the developed software FORCE- 401 -S which permits to communicate the measurement line “load cell-amplifiercomputer”; to choose the correct interpolation polynom; enable to compute the errors and uncertainty values and build the table of calibration results. The calibration procedure is performed according to the standard ISO 7500 - 1 "Verification of static uniaxial testing machines- Part 1 : Tension/compression testing machines- Verification and calibration of the forcemeasuring system". We recommend to widen the definitions of this standard and to include calibration of non-force units scaled devices (as an example pressure gauges) ...|$|R
30|$|The {{historical}} wildfire {{data were}} analyzed for {{better understanding of the}} distribution pattern of anthropogenically-initiated wildfire events in the southeast Mississippi region. Analysis focused on: (1) causes of wildfires; (2) yearly variations; (3) wildfire sizes; and (4) monthly and seasonal variations. The data analysis consisted of a topological analysis of the vector or raster objects to understand their spatial structure or correlation. For this purpose the following topological tools available in ArcGIS were used: (1) adjacency—which is the zone of influence on either side of an element, for example, calculation of buffer zone; (2) distance—which is the Euclidean distance between two points of interest; (3) neighborhood—which interpolates data features to raster objects in order to obtain a continuous response surface; and (4) map algebra functions to calculate complex <b>mathematical</b> functions. <b>Interpolation</b> was performed using moving window/kernel density estimation. The size of the window was chosen using an iterative method to maintain local variation without over-generalizing the density estimation following Cooke et al. (2007). For this purpose, kernel sizes from 200  m to 10  km were tested. The large kernel sizes produced a near uniform response surface and failed to capture the local variation patterns. On the other hand, small regional patterns were lost for smaller kernel sizes and lead to regions of zero road-density as the roads are sparsely distributed. The optimal window size was found to be 4  km, which is used in the analysis.|$|R
40|$|Abstract：High {{resolution}} {{data has}} become {{an important source of}} data, before they can be integrating into a GIS database, It requires processing for ortho-rectification to generate image map with high accuracy and low cost。 Using surface splines interpolation for rectification quite different with traditional grid method in photogrammetric, To introduce surface splines is important. In fact the actual named is mechanical surface splines because it must add mechanical conditions to form the formula. The main advantages of the surface splines are that the coordinate of the known points are not located in a rectangular array and the function may be differentiated in find slopes. Surface splines are a <b>mathematical</b> tool to <b>interpolation</b> a function of two variables. It base upon small deflection equation of an infinite plate， its originally developed for interpolation wing deflection of aircraft 1972 by Harderaed and Desmarais contributed. An example to generate 1 : 500 image map only five control points for rectification, the map size is 40 cm× 200 cm...|$|R
40|$|Abstract. The {{weather is}} a key factor for {{outbreaks}} of plant diseases. Therefore, the monitoring of climate is essential to any intelligent system of cultivation. Usually, such monitoring is carried through agrometeorological stations that acquire data like temperature, humidity, wind speed and leaf wetness. Unfortunately, {{the high cost of}} stations limits the number of monitoring points within the same area. This forces the use of tools of <b>mathematical</b> <b>interpolation.</b> However, such procedure does not lead to good results in the estimation of precipitation and leaf wetness. In this work was developed a monitoring system, composed for a set of microstations of data acquisition. They presented low cost and highest autonomy. Therefore, they can be installed throughout the area of cultivation, collecting data like temperature and leaf wetness. The microstations communicate with each other through wireless communication. Thus, daily, the data collected are transferred to a central station, where become available for analysis. The system made it possible to monitor the entire area of cultivation, providing reliable data, which make it possible to characterize the microclimate. This makes it possible to, for example, the efficient use of pesticides, reducing production costs and generating healthy foods...|$|E
40|$|In this work, we applied {{enhanced}} geophysical {{techniques to}} detect new prospecting zones at the Puerto Colón oil field. The easier-to-produce hydrocarbons {{are currently being}} or have been extracted. In order to extract harder-to-produce hydrocarbons, we need to better define the Caballos formation characteristics. We obtained an acceptable match between the rock-physics laboratory measurements and the petrophysical properties estimated {{through the use of}} seismic data. We used well logs to guide the seismic measurements in the estimation of both porosity and gamma-ray response (from seismic attributes), and acoustic impedance (via seismic inversion), using a neural network approach. We applied a probabilistic neural network (PNN) because of its particular characteristics of 1) mapping non linear relationships between seismic and well log data; 2) incrementing both accuracy and resolution when performing inversion, as compared to conventional inversion, and; 3) using a <b>mathematical</b> <b>interpolation</b> scheme not implemented as a black box. Poisson and Vp/Vs ratio provide a means to discriminate between high and low reservoir-rock quality at the Caballos formation. Finally, we analyzed three angle gather stacks (0 °- 10 °, 11 °- 20 ° and 21 °- 30 °) through elastic inversion...|$|E
40|$|The paper {{presents}} {{a set of}} on-line and off-line measuring methods for the dielectric parameters of the electric insulation {{as well as the}} method of results interpretation aimed to determine the occurence of a damage and to set up the its speed of evolution. These results lead finally to the determination of the life time under certain imposed safety conditions. The interpretation of the measurement results is done based on analytical algorithms allowing also the calculation of the index of correlation between the real results and the <b>mathematical</b> <b>interpolation.</b> It is performed a comparative analysis between different measuring and interpretation methods. There are considered certain events occurred during the measurement performance including their causes. The working-out of the analytical methods has been improved during the during the dielectric measurements performance for about 25 years at a number of 140 turbo and hydro power plants. Finally it is proposed a measurement program to be applied and which will allow the correlation of the on-line and off-line dielectric measurement obtaining thus a reliable technology of high accuracy level for the estimation of the available lifetime of electrical insulation...|$|E
40|$|The radial basis {{function}} (RBF) and quasi Monte Carlo (QMC) {{methods are}} two very promising schemes to handle high-dimension problems with complex and moving boundary geometry {{due to the fact}} that they are independent of dimensionality and inherently meshless. The two strategies are seemingly irrelevant and are so far developed independently. The former is largely used to solve partial differential equations (PDE), neural network, geometry generation, scattered data processing with <b>mathematical</b> justifications of <b>interpolation</b> theory [1], while the latter is often employed to evaluate high-dimension integration with the Monte Carlo method (MCM) background [2]. The purpose of this communication is to try to establish their intrinsic relationship on the grounds of numerical integral. The kernel function of integral equation is found the key to construct efficient RBFs. Some significant results on RBF construction, error bound and node placement are also presented. It is stressed that the RBF is here established on integral analysis rather than on the sophisticated interpolation and native space analysis...|$|R
40|$|The {{fracture}} toughness of SLG filled phenolic composites have been determined by short bar tests. It is expensive {{to prepare the}} samples for the tests. Therefore, {{it is necessary to}} develop a mathematical model that will predict the {{fracture toughness}} of particulate filled phenolic composites. Mathematical models for tensile strength, Young’s modulus are available but not for impact strength and fracture toughness. There is no sign that it can be built up from simple <b>mathematical</b> model; polynomial <b>interpolation</b> using Lagrange’s method was therefore employed to generate the fracture toughness model using the data obtained from experiments. From experiments, {{it was found that the}} trend of the fracture toughness of the samples cured conventionally was similar to that cured in microwaves; it is therefore possible to predict the fracture toughness of the samples cured in microwaves from shifting the mathematical model generated for fracture toughness of samples post-cured in conventional oven. The shifted model represented the fracture toughness of the samples cured in microwaves vey well...|$|R
30|$|RSA-analyses: RSA-analyses were {{performed}} in our department at workstation using a validated [11] RSA-software program (WinRSA ver. 4.0, Tilly Medical Products, Sweden). The method {{for determining the}} position of the tibial implant in the global coordinate system arises from the kinematic model where the tantalum markers in the tibial implant and the proximal tibia bone define two rigid bodies (segments). The tantalum markers in the calibration knee cage define the global coordinate system. The proximal tibia bone acted as a reference segment for the tibial implant segment. The tantalum markers in both segments and the calibration knee cage were detected manually on the X-ray images in the two planes. It is of crucial importance that the corresponding marker is detected in the two planes and that a minimum of three corresponding markers are detectable in each segment. By <b>mathematical</b> transformation (<b>interpolation</b> of marker coordinates in the two planes) into the 3 D laboratory coordinate system the RSA-software calculated the 3 D position of the segments. Subsequently the migration of the tibial implant over time (according to follow-up schedule) was calculated with the postoperative examination as reference. Manual detection of markers is time consuming, and the mean time spent on one RSA examination and subsequent analysis of the RSA-image pair was 120  min. The unit for translations was millimeters (mm) and for rotations it was degrees (°). In our study the tibial implant was defined stable if the translation between two examinations were less than 0.2  mm. In RSA examinations and analyses several factors influence the reliability of the results [10, 11, 13]. Two important parameters that affect the results of RSA analysis are the condition number and rigid body error.|$|R
