3|22|Public
30|$|Recent {{releases}} of mobile phones, {{announced by the}} manufacturers and the media as “smartphones”, have ever more high technology embedded, and are true portable computers, where capabilities such as data processing and virtual <b>memory</b> <b>increment</b> with every new release. It is notorious also that other objective and subjective aspects critical to the full user satisfaction, studied in detail by Han et al. (2004), as weight, shape, finish, elegance, simplicity, among others, are still present in making the decision to purchase or {{not one of these}} devices.|$|E
40|$|Most {{implementations}} of a radix- 2 fast Fourier transform {{on large}} scientific computers use algorithms that involve memory accesses whose strides are powers of two. (The term str/de means the <b>memory</b> <b>increment</b> between successive elements stored or fetched). Such strides are unacceptable for recently developed supercomputers, particularly the Cray- 2, because of serious difficulties with memory bank conflicts. This paper describes an algorithm {{for evaluating the}} fast Fourier transform that avoids this difficulty and thus could {{provide the basis for}} implementations that more fully utilize the power of the Cray- 2. A Fortran program implementing this algorithm is included, and timing comparisons with the Cray assembly-coded library subroutine are shown...|$|E
40|$|Ref: Journal of Supercomputing, vol. 1, no. 1 (July 1987), pg. 43 { 60 Most {{implementations}} of a radix- 2 fast Fourier transform {{on large}} scienti c computers use algorithms that involve memory accesses whose strides are powers of two. (The term stride means the <b>memory</b> <b>increment</b> between successive elements stored or fetched). Such strides are unacceptable for recently developed supercomputers, particularly the Cray- 2, because of serious di culties with memory bank con icts. This paper describes an algorithm {{for evaluating the}} fast Fourier transform that avoids this di culty andthus could {{provide the basis for}} implementations that more fully utilize the power of the Cray- 2. A Fortran program implementing this algorithm is included, and timing comparisons with the Cray assembly-coded library subroutine are shown...|$|E
40|$|Recent {{studies of}} {{measurements}} from packet networks {{have demonstrated that}} the arrival process of packets (packet network traffic) is self similar or long memory. In this paper, statistical models for packet network traffic are presented. They provide a physical explanation for the occurrence of self similarity based on new convergence results for processes that exhibit high variability. The key result states that the superposition of many strictly alternating ON/OFF sources whose ON and/or OFF periods exhibit the Noah Effect (i. e. have high variability or infinite variance) produces (aggregate) network traffic that exhibits the Joseph Effect (i. e. is self similar with long <b>memory</b> <b>increments).</b> The implications of self similarity on performance evaluation are presented. Statistical inference {{in the presence of}} self similarity is described...|$|R
50|$|<b>Memory</b> size <b>increments</b> for all SDS/XDS/Xerox {{computers}} are stated in kWords, not kBytes. For example, the Sigma 5 base memory is 16K 32-Bit words (64K Bytes). Maximum memory {{is limited by}} the length of the instruction address field of 17 bits, or 128K Words (512K Bytes). Although this is a trivial amount of memory in today's technology, Sigma systems performed their tasks exceptionally well, and few were deployed with, or needed, the maximum 128K Word memory size.|$|R
50|$|The 1301 was {{the main}} machine in the line. Its main <b>memory</b> came in <b>increments</b> of 400 words of 48 bits (12 decimal digits) plus two parity bits. The maximum size was 2,000 words. It was the first ICT machine to use core memory.|$|R
40|$|Disk {{scheduling}} problem has theoretical interest and practical importance since, the processor speed and memory capacity have been progressing several {{times faster than}} disk speed. More efficient disk usage methods have been needed because of the slow evolution in disk speed technology. Although disk and <b>memory</b> capacity <b>increment</b> is much higher versus disk speed improvement, little studies {{have been made to}} develop disk usage algorithms in more efficient manner. In this work, a new approach based on ant colony algorithm is proposed for disk {{scheduling problem}} and its performance is evaluated...|$|R
50|$|An {{interesting}} use of ANTIC's DMA memory scan behavior {{permits a}} strategy for apparently long horizontal scrolling distances, but uses {{a fraction of the}} real memory required. As discussed in Horizontal Coarse Scrolling ANTIC's automatic <b>memory</b> scan <b>increment</b> from Mode line to Mode line conflicts with the idea that the rows of screen data are wider than the display. Using memory actually organized as a long, horizontal, contiguous series of bytes requires an LMS modifier for every Display List Text or Map Mode instruction in the scrolling region.|$|R
40|$|AbstractThe {{systemic}} administration of lipopolysaccharide (LPS) induces time-dependent behavioral alterations, which {{are related to}} sickness behavior and depression. The time-course effects of LPS on prepulse inhibition (PPI) remain unknown. Furthermore, the time-dependent effects of LPS on central nitrite content had not been investigated. Therefore, we studied alterations induced by single LPS (0. 5 mg/kg, i. p.) administration to mice on parameters, such as PPI, depressive- and anxiety-like behaviors, working memory, locomotor activity and motor coordination, 1. 5 and 24 h post-LPS administration. IL- 1 β and TNFα in the blood and brain as well as brain nitrite levels were evaluated in the prefrontal cortex (PFC), hippocampus (HC) and striatum (ST). An overall hypolocomotion was observed 1. 5 h post-LPS, along with depressive-like behaviors and deficits in working <b>memory.</b> <b>Increments</b> in IL- 1 β content in plasma and PFC, TNFα in plasma and decreases in nitrite levels in the ST and PFC were also verified. Twenty-four hours post-LPS treatment, depressive-like behaviors and working memory deficits persisted, while PPI levels significantly reduced along with increases in IL- 1 β content in the PFC {{and a decrease in}} nitrite levels in the HC, ST and PFC. Our data demonstrate that a delayed increase (i. e., 24 h post-LPS) in PPI levels ensue, which may be useful behavioral parameter for LPS-induced depression. A decrease in nitrergic neurotransmission was associated with these behavioral findings...|$|R
50|$|Horizontal coarse {{scrolling}} {{requires a}} little more effort than vertical scrolling. While horizontal scrolling is expected to present {{the illusion of a}} view port moving left and right across a wide panoramic scene made of screen memory, ANTIC's automatic <b>memory</b> scan <b>increment</b> conflicts with this idea that the rows of screen data is wider than the display. Presenting screen memory as long horizontal lines requires an LMS modifier for every Display List Text or Map Mode instruction in the scrolling region. A horizontal step is accomplished by incrementing or decrementing all the LMS addresses of the scrolling region.|$|R
40|$|The {{scanning}} ntuple classifier is {{an efficient}} and accurate classifier for handwriting recognition. One {{of the major}} difficulties in implementing this scheme is its demand for a very large memory space, thus making it unsuitable for resource constrained systems such as embedded applications. This paper proposes some modifications to the basic sntuple algorithm which eliminates the necessity of normalizing the chain-code length, by adjusting the <b>memory</b> cell <b>increments</b> as an inverse function the chain length. The resulting system performance is shown to be superior to the standard sntuple configuration in both speed and accuracy when smaller and fewer sntuples are used, a configuration which also reduces the demand for memory...|$|R
30|$|Scopolamine induced <b>memory</b> {{impairment}} causes <b>increment</b> in AchE {{action and}} oxidative stress in brain. Furthermore, {{it is believed}} that scopolamine has got tendency to hinder the neurogenesis process of the brain which causes impairment in cognition [21]. Many studies have shown that scopolamine alters the process of receiving, collecting and remembrance of information. Drugs which have potential of increasing the cholinergic neuronal activity, are effective against memory impairment. Physostigmine is one of the drugs which increases cholinergic neuronal activity in brain and can combat the memory impairment [30].|$|R
5000|$|The {{language}} utilises a 64K {{block of}} memory, and 2 pointers - a memory pointer and an instruction pointer. The l33t interpreter tokenizes {{all the words}} in the source to create a sequence of numerical opCodes, and places them in order into the memory block, starting at byte 0. The instruction pointer will keep incrementing until it encounters an END. The memory pointer starts at the first byte after the instructions. <b>Memory</b> [...] "wraps": <b>incrementing</b> the <b>memory</b> and the instruction pointer past 64K will cause it to run around to byte 0, and vice versa.|$|R
40|$|We {{introduce}} an any{space algorithm for exact inference in Bayesian networks, called recursive conditioning. On one extreme, recursive conditioning takes O(n) {{space and}} O(n exp(w log n)) time|where n {{is the size}} of a Bayesian network and w is the width of a given elimination order|therefore, establishing a new complexity result for linear{space inference in Bayesian networks. On the other extreme, recursive conditioning takes O(n exp(w)) space and O(n exp(w)) time, therefore, matching the complexity of state{of{the{art algorithms based on clustering and elimination. In between linear and exponential space, recursive conditioning can utilize <b>memory</b> at <b>increments</b> of X-bytes, where X is the number of bytes needed to store a oating point number in a cache. Moreover, the algorithm is equipped with a formula for computing its average running time under any amount of space, hence, providing a valuable tool for time{space tradeos in demanding applications. Recursive conditioning is therefore the rst algorithm for exact inference in Bayesian networks to oer a smooth tradeo between time and space, and to explicate a smooth, quantitative relationship between these two important resources. ...|$|R
40|$|AbstractWe {{introduce}} an any-space algorithm for exact inference in Bayesian networks, called recursive conditioning. On one extreme, recursive conditioning takes O(n) {{space and}} O(nexp(wlogn)) time—where n {{is the size}} of a Bayesian network and w is the width of a given elimination order—therefore, establishing a new complexity result for linear-space inference in Bayesian networks. On the other extreme, recursive conditioning takes O(nexp(w)) space and O(nexp(w)) time, therefore, matching the complexity of state-of-the-art algorithms based on clustering and elimination. In between linear and exponential space, recursive conditioning can utilize <b>memory</b> at <b>increments</b> of X-bytes, where X is the number of bytes needed to store a floating point number in a cache. Moreover, the algorithm is equipped with a formula for computing its average running time under any amount of space, hence, providing a valuable tool for time–space tradeoffs in demanding applications. Recursive conditioning is therefore the first algorithm for exact inference in Bayesian networks to offer a smooth tradeoff between time and space, and to explicate a smooth, quantitative relationship between these two important resources...|$|R
30|$|Recent {{releases}} of mobile phones, {{announced by the}} media as “smartphones” have ever more high technology embedded and they are true handheld computers, where the characteristics of information processing, physical and virtual <b>memory</b> capabilities have <b>incremented</b> with each new release. It is also remarkable that other objective and subjective critical aspects to the total satisfaction of the user are still present when making the decision to buy or {{not one of these}} devices. Therefore, we present, using the approach of data envelopment analysis (DEA), an exploratory study considering the main requirements to examine different Smartphone alternatives with respect to characteristics related to the user and the product.|$|R
30|$|The neuropsychological {{batteries}} comprised cognitive {{tests that}} were standardised {{to the general}} population, psychometrically sound and widely used in this patient population (Strauss et al. 2006; Mitrushina et al. 2005). The Bergen n-back test was excluded from the review due to weak construct validity (Kane et al. 2007; Jaeggi et al. 2010). The continuous visual execution task and semantic <b>memory</b> with associative <b>increment</b> test were also {{not included in the}} analysis {{due to the lack of}} information available on test description and normative data to verify that the test was standardised as per inclusion criteria. The tests used across the studies covered the following cognitive domains: processing speed, attention, learning and memory, visuospatial orientation, executive functioning and intelligence. A list of all the cognitive tests representing the aforementioned domains is available as Additional file 1 : Table S 1.|$|R
30|$|One {{potential}} {{limitation of}} this study is that it uses a qualitative research paradigm to analyze the effectiveness of GIGAME. While qualitative case study researchers have a strong case for how qualitative research paradigms still serve as rich sources of data (e.g., Baxter and Jack 2008), especially when applied to constructivist research questions (e.g., Yin 2003), critics of qualitative case study methods often cite subjectivity in the analyses, and the effectiveness that is assessed in the current study can only be regarded as perceived effectiveness, rather than real effectiveness. Further studies should be conducted to supplement the findings of this study. An important contribution of this case study is that it documents the process of game conceptualization and makes a reasoned and valid suggestion for how educational games may be developed and evaluated. Future research should involve controlled experimental designs, which include treatment and control groups, to rigorously determine the actual effectiveness of the use of GIGAME and other educational games. Important dependent variables, including <b>memory</b> retention, knowledge <b>increments,</b> and actual academic performance, can then be assessed, thereby strengthening this research.|$|R
40|$|We {{introduce}} the self-excited multifractal (SEMF) model, defined {{such that the}} amplitudes of the increments of the process are expressed as exponentials of a long <b>memory</b> of past <b>increments.</b> The principal novel feature of the model lies in the self-excitation mechanism combined with exponential nonlinearity, i. e. the explicit dependence of future values of the process on past ones. The self- excitation captures the microscopic origin of the emergent endogenous self-organization properties, such as the energy cascade in turbulent flows, the triggering of aftershocks by previous earthquakes and the "reflexive" interactions of financial markets. The SEMF process has all the standard stylized facts found in financial time series, which are robust to the specification of the parameters and {{the shape of the}} memory kernel: multifractality, heavy tails of the distribution of increments with intermediate asymptotics, zero correlation of the signed increments and long-range correlation of the squared increments, the asymmetry (called "leverage" effect) of the correlation between increments and absolute value of the increments and statistical asymmetry under time reversal...|$|R
50|$|In 1979 McCormack was {{employed}} by NCR {{right out of}} college, and they had developed a Bit slicing implementation of the p-code machine using the Am2900 chip set. This CPU had a myriad of timing and performance problems so McCormack proposed a total redesign of the processor using a programmable logic device based Microsequencer. McCormack left NCR to start a company called Volition Systems but continued {{the work on the}} CPU as a contractor.The new CPU used an 80-bit wide microword, so parallelism in the microcode was radically enhanced. There were several loopsin the microcode that were a single instruction long and many of the simpler p-code ops took 1 or 2 microcode instructions. With the wide microword and the way the busses were carefully arranged, as well as <b>incrementing</b> <b>memory</b> address registers, the cpu could execute operations inside the ALU while transferring a memory word directly to the onboard stack, or feed one source into the ALU while sending a previously computed register to the destination bus in a single microcycle.|$|R
50|$|Some models {{relax the}} program order even further by {{relaxing}} even the ordering constraints between writes to different locations. The SPARC V8 Partial Store Ordering model (PSO) {{is the only}} {{example of such a}} model. The ability to pipeline and overlap writes to different locations from the same processor is the key hardware optimisation enabled by PSO. PSO is similar to TSO in terms of atomicity requirements, in that, it allows a processor to read the value of its own write and preventing other processors from reading another processor’s write before the write is visible to all other processors. Program order between two writes is maintained by PSO using an explicit STBAR instruction. The STBAR is inserted in a write buffer in implementations with FIFO write buffers. A counter is used to determine when all the writes before the STBAR instruction have been completed, which triggers a write to the <b>memory</b> system to <b>increment</b> the counter. A write acknowledgement decrements the counter, and when the counter becomes 0, it signifies that all the previous writes are completed.|$|R
40|$|Flash memory {{promises}} to revolutionize storage systems {{because of its}} massive performance gains, ruggedness, large decrease in power usage and physical space requirements, {{but it is not}} a direct replacement for magnetic hard disks. Flash memory possesses fundamentally different characteristics and in order to fully utilize the positive aspects of flash memory, we must engineer around its unique limitations. The primary limitations are lack of in-place updates, the asymmetry between the sizes of the write and erase operations, and the limited endurance of flash memory cells. This leads to the need for efficient methods for block cleaning, combating write amplification and performing wear leveling. These are fundamental attributes of flash memory and will always need to be understood and efficiently managed to produce an efficient and high performance storage system. Our goal in this work is to provide analysis and algorithms for efficiently managing data storage for endurance in flash memory. We present update codes, a class of floating codes, which encodes data updates as flash <b>memory</b> cell <b>increments</b> that results in reduced block erases and longer lifespan of flash memory, and provides a new algorithm for constructing optimal floating codes. We also analyze the theoretically possible limits of write amplification reduction and minimization by using offline workloads. We give an estimation of the minimal write amplification by a workload decomposition algorithm and find that write amplification can be pushed to zero with relatively low over-provisioning. Additionally, we give simple, efficient and practical algorithms that are effective in reducing write amplification and performing wear leveling. Finally, we present a quantitative model of wear levels in flash memory by constructing a difference equation that gives erase counts of a block with workload, wear leveling strategy and SSD configuration as parameters...|$|R
40|$|We study {{a family}} of memory-based {{persistent}} random walks and we prove weak convergences after space-time rescaling. The limit processes are not only Brownian motions with drift. We have obtained a continuous but non-Markov process (Zt) which can be easely {{expressed in terms of}} a counting process (Nt). In a particular case the counting process is a Poisson process, and (Zt) permits to represent the solution of the telegraph equation. We study in detail the Markov process ((Zt, Nt); t ≥ 0). 1 The setting of persistent random walks. 1) The simplest way to present and define a persistent random walk with value in Z is to introduce the process of its increments (Yt, t ∈ IN). In the classical symmetric random walk case, this process is just a sequence of independent random variables satisfying IP(Yt = 1) = for any t ≥ 0. Here we shall introduce some short range <b>memory</b> in these <b>increments</b> in order to create the persistence phenomenon. Namely (Yt) is a {− 1, 1 }-valued Markov chain: the law of Yt+ 1 given Ft = σ(Y 0, Y 1, [...] ., Yt) depends only on the value of Yt. This dependence is represented by the transition probability π(x,y) = IP(Yt+ 1 = y|Yt = x) with (x, y) ∈ {− 1, 1 } 2 : IP(Yt = − 1) = 1 2 1 − α α π = β 1 − β 0 < α < 1, 0 < β < 1. The persistent random walk is the corresponding process of partial sums...|$|R
40|$|Jacob Hyllested-Winge, 1 Thomas Sparre, 2 Line Kynemund Pedersen 2 1 Novo Nordisk Pharma Ltd, Tokyo, Japan; 2 Novo Nordisk A/S, Søborg, Denmark Abstract: The {{introduction}} of insulin pen devices has provided easier, well-tolerated, and more convenient treatment regimens {{for patients with}} diabetes mellitus. When compared with vial and syringe regimens, insulin pens offer a greater clinical efficacy, improved quality of life, and increased dosing accuracy, particularly at low doses. The portable and discreet nature of pen devices reduces {{the burden on the}} patient, facilitates adherence, and subsequently contributes to the improvement in glycemic control. NovoPen Echo® is one of the latest members of the NovoPen® family that has been specifically designed for the pediatric population and is the first to combine half-unit increment (= 0. 5 U of insulin) dosing with a simple memory function. The half-unit increment dosing amendments and accurate injection of 0. 5 U of insulin are particularly beneficial for children (and insulin-sensitive adults/elders), who often require small insulin doses. The memory function can be used to record the time and amount of the last dose, reducing the fear of double dosing or missing a dose. The memory function also provides parents with extra confidence and security that their child is taking insulin at the correct doses and times. NovoPen Echo is a lightweight, durable insulin delivery pen; it is available in two different colors, which may help to distinguish between different types of insulin, providing more confidence for both users and caregivers. Studies have demonstrated a high level of patient satisfaction, with 80 % of users preferring NovoPen Echo to other pediatric insulin pens. Keywords: NovoPen Echo®, <b>memory</b> function, half-unit <b>increment</b> dosing, adherence, children, adolescents ...|$|R

