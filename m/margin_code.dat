6|14|Public
40|$|We {{present the}} first fully {{convolutional}} end-to-end solution for instance-aware semantic segmentation task. It inherits all {{the merits of}} FCNs for semantic segmentation and instance mask proposal. It performs instance mask prediction and classification jointly. The underlying convolutional representation is fully shared between the two sub-tasks, {{as well as between}} all regions of interest. The proposed network is highly integrated and achieves state-of-the-art performance in both accuracy and efficiency. It wins the COCO 2016 segmentation competition by a large <b>margin.</b> <b>Code</b> would be released at ...|$|E
40|$|Matching local {{geometric}} {{features on}} real-world depth images is a challenging task {{due to the}} noisy, low-resolution, and incomplete nature of 3 D scan data. These difficulties limit the performance of current state-of-art methods, which are typically based on histograms over geometric properties. In this paper, we present 3 DMatch, a data-driven model that learns a local volumetric patch descriptor for establishing correspondences between partial 3 D data. To amass training data for our model, we propose a self-supervised feature learning method that leverages the millions of correspondence labels found in existing RGB-D reconstructions. Experiments show that our descriptor is not only able to match local geometry in new scenes for reconstruction, but also generalize to different tasks and spatial scales (e. g. instance-level object model alignment for the Amazon Picking Challenge, and mesh surface correspondence). Results show that 3 DMatch consistently outperforms other state-of-the-art approaches by a significant <b>margin.</b> <b>Code,</b> data, benchmarks, and pre-trained models are available online at [URL] To appear at the Conference on Computer Vision and Pattern Recognition (CVPR) 2017. Project webpage: [URL]...|$|E
40|$|This paper {{considers}} {{the task of}} matching images and sentences. The challenge consists in discriminatively embedding the two modalities onto a shared visual-textual space. Existing work in this field largely uses Recurrent Neural Networks (RNN) for text feature learning and employs off-the-shelf Convolutional Neural Networks (CNN) for image feature extraction. Our system, in comparison, differs in two key aspects. Firstly, we build a convolutional network amenable for fine-tuning the visual and textual representations, where the entire network only contains four components, i. e., convolution layer, pooling layer, rectified linear unit function (ReLU), and batch normalisation. End-to-end learning allows the system to directly learn from the data and fully utilise the supervisions. Secondly, we propose instance loss according to viewing each multimodal data pair as a class. This works with a large margin objective to learn the inter-modal correspondence between images and their textual descriptions. Experiments on two generic retrieval datasets (Flickr 30 k and MSCOCO) demonstrate that our method yields competitive accuracy compared to state-of-the-art methods. Moreover, in language person retrieval, we improve {{the state of the}} art by a large <b>margin.</b> <b>Code</b> is available at [URL] com/layumi/Image-Text-EmbeddingComment: 12 pages, 13 figure...|$|E
40|$|Deep {{learning}} {{has become a}} ubiquitous technology to improve machine intelligence. However, most of the existing deep models are structurally very complex, making them difficult to be deployed on the mobile platforms with limited computational power. In this paper, we propose a novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning. Unlike the previous methods which accomplish this task in a greedy way, we properly incorporate connection splicing into the whole process to avoid incorrect pruning and {{make it as a}} continual network maintenance. The effectiveness of our method is proved with experiments. Without any accuracy loss, our method can efficiently compress the number of parameters in LeNet- 5 and AlexNet by a factor of 108 × and 17. 7 × respectively, proving that it outperforms the recent pruning method by considerable <b>margins.</b> <b>Code</b> and some models are available at [URL] Accepted by NIPS 201...|$|R
40|$|Introduction. Therapeutic mammaplasty (TM) is {{a useful}} {{technique}} in the armamentarium of the oncoplastic breast surgeon (OBS). There is limited guidance on patient selection, technique, coding, and management of involved margins. The practices of OBS in England remain unknown. Methods. Questionnaires were sent to all OBS involved with the Training Interface Group. We assessed the number of TM cases performed per surgeon, criteria for patient selection, pedicle preference, contralateral symmetrisation, use of routine preoperative MRI, management of involved <b>margins,</b> and clinical <b>coding.</b> Results. We had an overall response rate of 43 %. The most common skin resection technique utilised was wise pattern followed by vertical scar. Superior-medial pedicle was preferred {{by the majority of}} surgeons (62 %) followed by inferior pedicle (34 %). Twenty percent of surgeons would always proceed to a mastectomy following an involved margin, whereas the majority would offer reexcision based on several parameters. The main absolute contraindication to TM was tumour to[*]breast ratio > 50 %. One in five surgeons would not perform TM in smokers and patients with multifocal disease. Discussion. There is a wide variation in the practice of TM amongst OBS. Further research and guidance would be useful to standardise practice, particularly management of involved <b>margins</b> and <b>coding</b> for optimal reimbursement...|$|R
40|$|The goal of {{predictive}} sparse coding is {{to learn}} a representation of examples as sparse linear combinations of elements from a dictionary, such that a learned hypothesis linear in the new representation performs well on a predictive task. Predictive sparse coding algorithms recently have demonstrated impressive performance {{on a variety of}} supervised tasks, but their generalization properties have not been studied. We establish the first generalization error bounds for predictive sparse coding, covering two settings: 1) the overcomplete setting, where the number of features k exceeds the original dimensionality d; and 2) the high or infinite-dimensional setting, where only dimension-free bounds are useful. Both learning bounds intimately depend on stability properties of the learned sparse encoder, as measured on the training sample. Consequently, we first present a fundamental stability result for the LASSO, a result characterizing the stability of the sparse codes with respect to perturbations to the dictionary. In the overcomplete setting, we present an estimation error bound that decays as Õ(sqrt(d k/m)) with respect to d and k. In the high or infinite-dimensional setting, we show a dimension-free bound that is Õ(sqrt(k^ 2 s / m)) with respect to k and s, where s is an upper bound on the number of non-zeros in the sparse code for any training data point. Comment: Sparse Coding Stability Theorem from version 1 has been relaxed considerably using a new notion of <b>coding</b> <b>margin.</b> Old Sparse <b>Coding</b> Stability Theorem still in new version, now as Theorem 2. Presentation of all proofs simplified/improved considerably. Paper reorganized. Empirical analysis showing new <b>coding</b> <b>margin</b> is non-trivial on real dataset...|$|R
40|$|Table-to-text {{generation}} aims {{to generate}} a description for a factual table which {{can be viewed as}} a set of field-value records. To encode both the content and the structure of a table, we propose a novel structure-aware seq 2 seq architecture which consists of field-gating encoder and description generator with dual attention. In the encoding phase, we update the cell memory of the LSTM unit by a field gate and its corresponding field value in order to incorporate field information into table representation. In the decoding phase, dual attention mechanism which contains word level attention and field level attention is proposed to model the semantic relevance between the generated description and the table. We conduct experiments on the WIKIBIO dataset which contains over 700 k biographies and corresponding infoboxes from Wikipedia. The attention visualizations and case studies show that our model is capable of generating coherent and informative descriptions based on the comprehensive understanding of both the content and the structure of a table. Automatic evaluations also show our model outperforms the baselines by a great <b>margin.</b> <b>Code</b> for this work is available on [URL] Accepted by AAAI 201...|$|E
40|$|Arguably, Deformable Part Models (DPMs) {{are one of}} {{the most}} {{prominent}} approaches for face alignment with im-pressive results being recently reported for both controlled lab and unconstrained settings. Fitting in most DPM meth-ods is typically formulated as a two-step process during which discriminatively trained part templates are first cor-related with the image to yield a filter response for each landmark and then shape optimization is performed over these filter responses. This process, although computation-ally efficient, is based on fixed part templates which are assumed to be independent, and has been shown to result in imperfect filter responses and detection ambiguities. To address this limitation, in this paper, we propose to jointly optimize a part-based, trained in-the-wild, flexible appear-ance model along with a global shape model which results in a joint translational motion model for the model parts via Gauss-Newton (GN) optimization. We show how signif-icant computational reductions can be achieved by build-ing a full model during training but then efficiently opti-mizing the proposed cost function on a sparse grid using weighted least-squares during fitting. We coin the proposed formulation Gauss-Newton Deformable Part Model (GN-DPM). Finally, we compare its performance against the state-of-the-art and show that the proposed GN-DPM out-performs it, in some cases, by a large <b>margin.</b> <b>Code</b> for our method is available fro...|$|E
40|$|Cascaded {{regression}} {{approaches have}} been recently shown to achieve state-of-the-art performance for many computer vision tasks. Beyond its connection to boosting, cascaded regression {{has been interpreted}} as a learning-based approach to iterative optimization methods like the Newton’s method. However, in prior work, the connection to optimization theory is limited only in learning a mapping from image features to problem parameters. In this paper, we consider the problem of facial de-formable model fitting using cascaded regression and make the following contributions: (a) We propose regression to learn a sequence of averaged Jacobian and Hessian matri-ces from data, and from them descent directions in a fashion inspired by Gauss-Newton optimization. (b) We show that the optimization problem in hand has structure and devise a learning strategy for a cascaded regression approach that takes the problem structure into account. By doing so, the proposed method learns and employs a sequence of aver-aged Jacobians and descent directions in a subspace or-thogonal to the facial appearance variation; hence, we call it Project-Out Cascaded Regression (PO-CR). (c) Based {{on the principles of}} PO-CR, we built a face alignment system that produces remarkably accurate results on the challeng-ing iBUG data set outperforming previously proposed sys-tems by a large <b>margin.</b> <b>Code</b> for our system is available fro...|$|E
40|$|M. Ed. The {{focus of}} this {{investigation}} is on the perceptions of non-educators and school principals {{with regard to the}} implementation of the policies of employment equity and affirmative action in public schools. There were gross inequalities that were perpetuated by a policy of employment segregation prior to the 1994 democratic government that skewed human and physical resources in favour of the white people. This adversely affected the employment status of non-educators in public schools. To this end, public schools lacked credibility and legitimacy and service delivery became inefficient on account of unfair employment policies and practices. Therefore, the promulgation of the policies of employment equity and affirmative action became a necessity through which equalisation of employment opportunities could be realised. The {{purpose of this study is}} to explore and understand the perceptions of non-educators and principals with regard to the implementation of the aforesaid policies and how they address their employment barriers that were legislated during the apartheid era in the South African public schools. The perceptions of non-educators and school principals with regard to the implementation of employment policies were investigated. Furthermore, an investigation on the implications for the management of non-educators in public schools were contextually explored, observed and described in a natural setting, via in-depth semi-structured interviews thus allowing an indepth and qualitative description of their views and beliefs. The raw data was transcribed verbatim from tape recorder and videotape. Data was analysed by <b>margin</b> <b>coding</b> and by clustering data into categories and sub-categories. Conclusions were drawn and recommendations made for further research. The research study revealed that there is a need for practical implementation of the policies of employment equity and affirmative action in public schools. In order to implement the policies appropriately, it was discovered that training and development should occur alongside monitoring and evaluation, and a participatory management style that encompasses noneducators in public schools...|$|R
40|$|Abstract—Necessary {{conditions}} for asymptotically optimal sliding-block or stationary codes for source coding and rate-constrained simulation of memoryless sources are presented {{and used to}} motivate a design technique for trellis-encoded source coding and rate-constrained simulation. The code structure has intuitive similarities to classic random coding arguments {{as well as to}} “fake process ” methods and alphabet-constrained methods. Experimental evidence shows that the approach pro-vides comparable or superior performance in comparison with previously published methods on common examples, sometimes by significant <b>margins.</b> Index Terms—Source <b>coding,</b> simulation, rate-distortion, trellis source encodin...|$|R
30|$|For {{the current}} study of cases treated by mastectomy, all {{pathology}} reports were manually reviewed to confirm the diagnosis of pure DCIS and to obtain data on resection margin status and width. The following data were abstracted from original pathology reports: nuclear grade (low, intermediate, high, unreported), comedo necrosis (present, absent), multifocality (present, absent). Resection <b>margin</b> width was <b>coded</b> as ≤ 2  mm if DCIS was at or within 2  mm from the margin, > 2  mm if the closest distance from tumor to the inked resection margin was more than 2  mm or unreported if the closest margin was not reported. Comedo necrosis and multifocality were coded as absent if not indicated in the pathology report. Tumor size was not reported for > 20 % of cases and therefore {{was not included in}} the analysis.|$|R
40|$|Search {{for higher}} {{safety of a}} tug-barge convoy (TBC) in towing necessitates risk {{reduction}} of towline failure. That requires determination of the static and dynamic forces on the towline at. A mathematical design model of the complete towing system has been built where the rope pull of the tug and overall tension on the towline determine the tow equilibrium, e. g. the maximum attainable speed. The model computes both static and dynamic forces induced by tug and marge motions as well as catenary’s depth and scope in given weather conditions to help the TBC personnel to operate the convoy with an acceptable safety <b>margin</b> The implemented <b>code</b> was applied to an AGIP convoy operated around the Kashagan oil field in the North-Eastern Caspian Sea, where it is mandatory to evaluate also the effects of extreme shallow water conditions on the risk of towline failure and actual maximum towing speed...|$|R
40|$|OFDMA) {{refers to}} aradio {{transmission}} technique based on dividing the frequency channelinto narrowband sub-channels. In this dissertation studied that <b>margin</b> adaptive- superposition <b>coding</b> (MA-SC) algorithm and rate adaptive- superposition coding (RA-SC) algorithm, where at most two users can share each subcarrier {{as compared to}} MA and RA algorithm, where each subcarrier is shared by only single user without SC scheme. Then apply SC scheme over MA and RA to achieve maximum system throughput with separate power constraint for real and non-real time users, ensuring that QoS requirement for real time and proportional fairness among non-real time users is satisfied. The overall computational complexity of RA-SC algorithm is same as RA algorithm. In MA-SC, complexity increases {{in some of the}} steps due to addition of SC scheme over MA algorithm, but overall complexity remains the same. Matlab simulations are being carried out to show that the performance of algorithm in terms of power required and throughput. Index Term-MA-SC, RA-SC, OFDM Superposition coding. I...|$|R
40|$|This work {{presents}} a statistical {{study on the}} variability of the mechanical properties of hardened self-compacting concrete, including the compressive strength, splitting tensile strength and modulus of elasticity. The comparison of the experimental results with those derived from several codes and recommendatio ns allows evaluating if the hardened behaviour of self- compacting concrete can be appropriately predicted by the existing formulations. The variables analyzed include the maximum size aggregate, paste and gravel content. Results from the analyzed self-compacting concretes presented variability measures in the same range than the expected for conventional vibrated concrete, with all the results within a confidence level of 95 %. From several formulations for conventional concrete considered in this study, {{it was observed that}} a safe estimation of the modulus of elasticity can be obtained from the value of compressive strength; with lower strength self-compacting concretes presenting higher safety <b>margins.</b> However, most <b>codes</b> overestimate the material tensile strength. Peer ReviewedPostprint (author’s final draft...|$|R
40|$|The paper {{summarizes}} a {{feasibility study}} of a multibeam FDMA satellite system operating in the 30 / 20 GHz band. The system must accommodate a very high volume of traffic within the restrictions of a 5 kW solar cell array and a 2. 5 GHz bandwidth. Multibeam satellite operation reduces the DC power demand and allows reuse of the available bandwidth. Interferences among the beams are brought to acceptable levels by appropriate frequency assignments. A transponder design is presented; it is greatly simplified by {{the application of a}} regional concept. System analysis shows that MSK modulation is appropriate for a high-capacity system because it conserves the frequency spectrum. Rain attenuation, a serious problem in this frequency band, is combatted with sufficient power <b>margins</b> and with <b>coding.</b> Link budgets, cost analysis, and weight and power calculations are also discussed. A satellite-routed FDMA system compares favorably in performance and cost with a satellite-switched TDMA system...|$|R
40|$|Recently {{there has}} been a renewed {{interest}} on mobile communications via satellite. Therefore, the DVB-S 2 /-RCS standard has been updated with some countermeasures to combat fading effects of the mobile scenario. The focus of this paper is on multimedia traffic management on the DVB-S 2 forward link to collective terminals on trains. In our study, we consider ACM, PHY <b>margins,</b> and LL-FEC <b>coding</b> to counteract the time-varying propagation conditions in Ku-band that are very critical for the railway scenario. A multi-layer optimization study is performed in order to maximize TCP goodput by jointly tuning LL-FEC coding and the PHY layer margin. It is shown how, using appropriate parameters, LL-FEC coding can improve the performance of the system. Moreover, the performance of the proposed UBMT scheduler is evaluated in managing CBR and FTP traffic flows. This study has permitted to determine the maximum capacity of CBR flows (guaranteeing a given quality of service level) in the presence of FTP flows...|$|R
40|$|The ASME Boiler and Pressure Vessel Code {{provides}} {{rules for}} the construction of nuclear power plant components. The Code specifies fatigue design curves for structural materials. However, the effects of light water reactor (LWR) coolant environments are not explicitly addressed by the Code design curves. Existing fatigue strain-vs. -life ({var_epsilon}-N) data illustrate potentially significant effects of LWR coolant environments on the fatigue resistance of pressure vessel and piping steels. This report provides an overview of the existing fatigue {var_epsilon}-N data for carbon and low-alloy steels and wrought and cast austenitic SSs to define the effects of key material, loading, and environmental parameters on the fatigue lives of the steels. Experimental data are presented on the effects of surface roughness on the fatigue life of these steels in air and LWR environments. Statistical models are presented for estimating the fatigue {var_epsilon}-N curves {{as a function of the}} material, loading, and environmental parameters. Two methods for incorporating environmental effects into the ASME Code fatigue evaluations are discussed. Data available in the literature have been reviewed to evaluate the conservatism in the existing ASME Code fatigue evaluations. A critical review of the <b>margins</b> for ASME <b>Code</b> fatigue design curves is presented...|$|R
40|$|Mutations in the CINCINNATA (CIN) gene in Antirrhinum majus and its {{orthologs}} in Arabidopsis {{result in}} crinkly leaves {{as a result}} of excess growth towards the leaf <b>margin.</b> CIN homologs <b>code</b> for TCP (TEOSINTE-BRANCHED 1, CYCLOIDEA, PROLIFERATING CELL FACTOR 1 AND 2) transcription factors and are expressed in a broad zone in a growing leaf distal to the proliferation zone where they accelerate cell maturation. Although a few TCP targets are known, the functional basis of CIN-mediated leaf morphogenesis remains unclear. We compared the global transcription profiles of wild-type and the cin mutant of A. majus to identify the targets of CIN. We cloned and studied the direct targets using RNA in situ hybridization, DNA-protein interaction, chromatin immunoprecipitation and reporter gene analysis. Many of the genes involved in the auxin and cytokinin signaling pathways showed altered expression in the cin mutant. Further, we showed that CIN binds to genomic regions and directly promotes the transcription of a cytokinin receptor homolog HISTIDINE KINASE 4 (AmHK 4) and an IAA 3 /SHY 2 (INDOLE- 3 -ACETIC ACID INDUCIBLE 3 /SHORT HYPOCOTYL 2) homolog in A. majus. Our results suggest that CIN limits excess cell proliferation and maintains the flatness of the leaf surface by directly modulating the hormone pathways involved in patterning cell proliferation and differentiation during leaf growth. 10. 1111 /(ISSN) 1469 - 8137 </do...|$|R
40|$|Background: In {{recent years}} {{there has been}} a growing {{awareness}} of the quality of breast cancer care. In the Netherlands the completeness of breast conserving surgery (BCS) was introduced as a quality parameter, and hospitals are obliged to report on the proportion of patients with positive margins after first BCS since 2007. Increasing national mastectomy rates during the last decade for the treatment of early breast cancer have recently been published from the US. This study describes trends in BCS over time in relation to positive margin rates after BCS. Materials and Methods: All breast cancer patients T 1 – 2, any N, M 0 diagnosed between July 1, 2008, and December 31, 2012 who underwent surgical resection were selected from the Netherlands Cancer Registry. Type of first surgery was coded as BCS or mastectomy. <b>Margin</b> status was <b>coded</b> as clear, focally positive margins (tumor in a limited area of the inked surface, i. e. one or two foci of tumor, with a maximum of 4 [*]mm), more than focally positive margins (MFP) or unknown. BCS rates were available in the NCR for the period 1995 – 2012. Results: The percentage of BCS as first surgery increased over time (48 % and 64 % in 1995 and 2012 respectively; χ 2 -test: p[*]=[*] 0. 000), with a temporary decline in the period 2008 – 2009. Of the 49, 570 included patients over the period 2008 – 2012, 62 % (30, 790 patients) received BCS in 89 hospitals in the Netherlands. The percentage of MFP-margins significantly decreased since the introduction of the indicator; 9. 7 % in 2008 versus 5. 4 % in 2012 (Table 1; χ 2 -test: p[*]=[*] 0. 000). After case mix correction for age, tumor size, grade, lobular subtype, multifocality, hormone receptor status and HER 2 status, hospital variance was substantial: corrected MFP-margin rates varied from 0 % to 19 %. Thirty-seven hospitals (42 %) had margin rates significantly lower than 10 %, while 3 % showed significantly higher rates. Conclusions: The percentage of patients with positive surgical margins after first BCS for breast cancer decreased between 2008 and 2012. This decrease in positive margins was accompanied by a increase in the national BCS rates in the Netherlands during this period...|$|R

