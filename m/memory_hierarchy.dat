1872|452|Public
5|$|The Itanium {{processors}} show {{a progression}} in capability. Merced was a proof of concept. McKinley dramatically improved the <b>memory</b> <b>hierarchy</b> and allowed Itanium to become reasonably competitive. Madison, with {{the shift to}} a 130nm process, allowed for enough cache space to overcome the major performance bottlenecks. Montecito, with a 90nm process, allowed for a dual-core implementation and a major improvement in performance per watt. Montvale added three new features: core-level lockstep, demand-based switching and front-side bus frequency of up to 667MHz.|$|E
25|$|The {{development}} of time-sharing systems {{led to a}} number of problems. One was that users, particularly at universities where the systems were being developed, seemed to want to hack the system to get more CPU time. For this reason, security and access control became a major focus of the Multics project in 1965. Another ongoing issue was properly handling computing resources: users spent most of their time staring at the terminal and thinking about what to input instead of actually using the resources of the computer, and a time-sharing system should give the CPU time to an active user during these periods. Finally, the systems typically offered a <b>memory</b> <b>hierarchy</b> several layers deep, and partitioning this expensive resource led to major developments in virtual memory systems.|$|E
500|$|... {{experiments}} {{have shown that}} some of these methods may be an improvement on radix sorting for data with 128 or more bits per key. Additionally, for large data sets, the near-random memory access patterns of many integer sorting algorithms can handicap them compared to comparison sorting algorithms that have been designed with the <b>memory</b> <b>hierarchy</b> in mind.|$|E
5000|$|Reorganize {{algorithms}} {{to reduce}} communication across all <b>memory</b> <b>hierarchies.</b>|$|R
40|$|We {{introduce}} a new data structure, the String B-tree, intended to search in unstructured data. We were focused on comparing searching in the String B-tree, with searching in Suffix Trees and String Dictionarys. More specifically we focused on efficency use of the <b>memory</b> <b>hierarchies.</b> The results shows, that the String B-tree is the fastest in searching, and has the most efficiency use of the <b>memory</b> <b>hierarchies...</b>|$|R
5000|$|C V Ramamoorthy and K Mani Chandy. Optimization of <b>Memory</b> <b>Hierarchies</b> in Multi- {{programmed}} Systems. J. ACM (...) , 17(3):426-445, 1970.|$|R
5000|$|The {{number of}} {{levels in the}} <b>memory</b> <b>hierarchy</b> and the {{performance}} at each level has increased over time. For example, the <b>memory</b> <b>hierarchy</b> of an Intel Haswell Mobile [...] processor circa 2013 is: ...|$|E
5000|$|OpenCL {{defines a}} four-level <b>memory</b> <b>hierarchy</b> for the compute device: ...|$|E
5000|$|A <b>memory</b> <b>hierarchy</b> with {{multi-level}} caches has unpredictable {{memory access}} latencies.|$|E
5000|$|<b>Memory</b> <b>hierarchies</b> {{have grown}} taller. The {{cost of a}} CPU cache miss is far more expensive. This exacerbates the {{previous}} problem.|$|R
40|$|We {{present a}} general {{deterministic}} sorting strategy that is applicable {{to a wide}} variety of parallel <b>memory</b> <b>hierarchies</b> with parallel processors. The simplest incarnation of the strategy is an optimal deterministic algorithm called Balance Sort for external sorting on multiple disks with a single CPU. Balance Sort was the topic of a previous report. This report shows how to adapt Balance Sort to sort deterministically in parallel <b>memory</b> <b>hierarchies.</b> The algorithms so derived will be optimal for all parallel <b>memory</b> <b>hierarchies</b> for which an optimal algorithm is known for a single hierarchy. In the case of D disks, P processors, block size B, and internal memory size M, they are optimal in terms of I/Os for any P _ M log rain{M/B, log M}/log M and log M/B: o(1 % M) ...|$|R
40|$|The paper {{presents}} a task allocation scheme for system-level synthesis of multirate real-time tasks on multiprocessors with <b>memory</b> <b>hierarchies.</b> The allocation algorithm {{is the first}} {{to take into account the}} effect of <b>memory</b> <b>hierarchies</b> at task level and optimizes for it. The algorithm is based on a task-level model of hierarchical memories first proposed in our previous work [1]. Caches are essential for modern RISC embedded cores to obtain sustained high performance. However, caches have received limited use in priority-driven preemptive real-time systems due to the unpredictability of caches [...] average-case improvements are of no use in systems with hard deadlines. Our task-level model of performance in the presence of <b>memory</b> <b>hierarchies</b> provides an efficient means to bound the guaranteed memory performance of tasks running in a multi-rate, multitasking environment. Our system synthesis algorithm uses cache partitioning and reservation techniques to guarantee cache hits for some tas [...] ...|$|R
5000|$|CMOx memory {{technology}} {{stretches the}} Flash {{space in the}} <b>memory</b> <b>hierarchy</b> ...|$|E
5000|$|Predicting {{where in}} the <b>memory</b> <b>hierarchy</b> the data resides is difficult.|$|E
5000|$|... #Subtitle level 2: Properties of the {{technologies}} in the <b>memory</b> <b>hierarchy</b> ...|$|E
40|$|Strassen’s {{algorithm}} has practical performance {{benefits for}} architectures with simple <b>memory</b> <b>hierarchies,</b> because it trades computationally expensive matrix multiplications (MM) with cheaper matrix additions (MA). However, it presents no advantages for high-performance architectures with deep <b>memory</b> <b>hierarchies,</b> because MAs exploit limited data reuse. We present an easy-to-use adaptive algorithm combining Strassen’s recursion and high-tuned version of ATLAS MM. In fact, we introduce a last {{step in the}} ATLAS-installation process that determines whether Strassen’s may achieve any speedup. We present a recursive algorithm achieving up to 30 % speed-up versus ATLAS alone. We show experimental results for 14 different systems...|$|R
30|$|The results {{presented}} for this application revealed that by using <b>memory</b> <b>hierarchies</b> with a two-level cache, {{we can improve}} not only the performance but also the energy consumption needed to run the application.|$|R
40|$|Certain {{architectural}} features either constrain or inhibit compiler optimizations. We suggest three hardware changes {{aimed to}} improve the situation, from a compiler's perspective. These changes involve redesigns of translation lookaside buffers, communication in <b>memory</b> <b>hierarchies,</b> and page mapping hardware for caches...|$|R
5000|$|In {{computer}} architecture, the <b>memory</b> <b>hierarchy</b> separates {{computer storage}} into a hierarchy based on response time. Since response time, complexity, and capacity are related, the levels {{may also be}} distinguished by their performance and controlling technologies. <b>Memory</b> <b>hierarchy</b> affects performance in computer architectural design, algorithm predictions, and lower level programming constructs involving locality of reference.|$|E
50|$|Modern {{machines}} tend to read {{blocks of}} lower memory {{into the next}} level of the <b>memory</b> <b>hierarchy.</b> If this displaces used memory, the operating system tries to predict which data will be accessed least (or latest) and move it down the <b>memory</b> <b>hierarchy.</b> Prediction algorithms tend to be simple to reduce hardware complexity, though they are becoming somewhat more complicated.|$|E
5000|$|.....the {{location}} in the <b>memory</b> <b>hierarchy</b> dictates the time required for the prefetch to occur.|$|E
40|$|Frigo, Leiserson, Prokop and Ramachandran in 1999 {{introduced}} the ideal-cache model as a formal model of computation for developing algorithms in environments with multiple levels of caching, and coined the terminology of cache-oblivious algorithms. Cache-oblivious algorithms {{are described as}} standard RAM algorithms with only one memory level, i. e. without any knowledge about <b>memory</b> <b>hierarchies,</b> but are analyzed in the two-level I/O model of Aggarwal and Vitter for an arbitrary memory and block size and an optimal off-line cache replacement strategy. The result are algorithms that automatically apply to multi-level <b>memory</b> <b>hierarchies.</b> This paper gives {{an overview of the}} results achieved on cache-oblivious algorithms and data structures since the seminal paper by Frigo et al...|$|R
40|$|Flash, phase-change, spin-torque, and {{resistive}} {{memories are}} rapidly transforming how system designers think about <b>memory</b> devices, <b>memory</b> <b>hierarchies,</b> processor architectures, storage systems, operating systems, and applica-tions. We have gathered a comprehensive {{survey of the}} (at last count) 340 non-volatile memory technology papers pub...|$|R
40|$|Abstract. Multiprocessor {{chips are}} now {{commonly}} supplied by IC man-ufacturers. Real-time applications {{based on this}} kind of execution plat-forms are difficult to develop for many reasons: complexity, design space, unpredictability, [...] . The allocation of the multiple hardware resources to the software entities could have a great impact on the final result. Then, if the interest to represent the set of hardware computing resources inside an architectural model is obvious, we argue that other hardware elements like shared buses or <b>memory</b> <b>hierarchies</b> must be included in the models if analyze of the timing behavior is expected to be performed. This article gives guidelines to represent, with AADL, shared-memory multiprocessing systems and their <b>memory</b> <b>hierarchies</b> while keeping a high-level model of the hardware architecture...|$|R
50|$|In this manner, this {{recursive}} definition can {{be extended}} throughout all layers of the <b>memory</b> <b>hierarchy.</b>|$|E
50|$|In {{order to}} avoid the problem of read/write order {{described}} above, the write buffer can be treated as a fully associative cache and added into the <b>memory</b> <b>hierarchy</b> of the device in which it is implemented.Adding complexity slows down the <b>memory</b> <b>hierarchy</b> so this technique is often only used for memory which does not need strong ordering (always correct) like the frame buffers of video cards.|$|E
5000|$|... {{this formula}} can be {{expanded}} further and used recursively {{for all the}} further levels in the <b>memory</b> <b>hierarchy</b> to get the AMAT.|$|E
40|$|Field {{programmable}} gate arrays (FPGAs) {{are fundamentally}} different to fixed processors architectures because their <b>memory</b> <b>hierarchies</b> can {{be tailored to}} the needs of an algorithm. FPGA compilers for high level languages are not hindered by fixed <b>memory</b> <b>hierarchies.</b> The constraint when compiling to FPGAs is the availability of resources. In this paper we describe how the dataflow intermediary of our declarative FPGA image processing DSL called RIPL (Rathlin Image Processing Language) enables us to constrain memory. We use five benchmarks to demonstrate that memory use with RIPL is comparable to the Vivado HLS OpenCV library without the need for language pragmas to guide hardware synthesis. The benchmarks also show that RIPL is more expressive than the Darkroom FPGA image processing language...|$|R
40|$|In {{this talk}} we examine how high {{performance}} computing {{has changed over}} the last 10 -year and look toward the future in terms of trends. These changes have had and will continue to {{have a major impact on}} our software. A new generation of software libraries and algorithms are needed for the effective and reliable use of (wide area) dynamic, distributed and parallel environments. Some of the software and algorithm challenges have already been encountered, such as management of communication and <b>memory</b> <b>hierarchies</b> through a combination of compile–time and run–time techniques, but the increased scale of computation, depth of <b>memory</b> <b>hierarchies,</b> range of latencies, and increased run–time environment variability will make these problems much harder. We will focus on the redesign of software to fit multicore architectures...|$|R
40|$|We address {{programming}} of accelerator-based heterogeneous multiprocessors in {{the context}} of computational science. Specifically, we consider stream architectures with explicitly man-aged <b>memory</b> <b>hierarchies.</b> In this paper we present a programming approach which supports program development for such multiprocessors. The programming approach is based on a coordination model which allows a programmer explicitly to control parallel activities and to manage <b>memory</b> <b>hierarchies.</b> Accelerators are only beneficial, if one succeeds to map the computational kernel efficiently onto the non-general-purpose hardware. Since the tar-get architecture of our programming system are stream multiprocessors, namely Cell/BE, streaming abstractions are provided to improve programmability of stream kernels and to enable profitable compiler optimizations. Parallelization techniques for code generation are presented. First experiences back up the approach. ...|$|R
5000|$|One of {{the main}} ways to {{increase}} system performance is minimising how far down the <b>memory</b> <b>hierarchy</b> one {{has to go to}} manipulate data.|$|E
5000|$|Latency and {{bandwidth}} are two metrics {{associated with}} caches and. Neither {{of them is}} uniform, but is specific to a particular component of the <b>memory</b> <b>hierarchy.</b>|$|E
50|$|Many programmers assume {{one level}} of memory. This works fine until the {{application}} hits a performance wall. Then the <b>memory</b> <b>hierarchy</b> will be assessed during code refactoring.|$|E
40|$|Scientific and {{numerical}} applications rely on multi-dimensional {{array data}} accessed in nested loops. These regular data access patterns {{can benefit from}} explic-itly managed local memories, such as the local stores of the Cell processor. We present Strider, a runtime library framework which helps programming and opti-mization of multi-dimensional data accesses in nested loops, on multi-core processors with explicitly managed <b>memory</b> <b>hierarchies.</b> Strider automatically schedules strided data accesses based on a high-level descrip-tion of loops and arrays provided by programmers. We present the design and implementation of Strider on Cell, followed by a preliminary performance evaluation of the framework. Our evaluation illustrates that Strider outperforms by significant margins existing parallel pro-gramming environments for multi-core processors with explicitly managed <b>memory</b> <b>hierarchies.</b> 1...|$|R
30|$|Once a {{model is}} constructed, various {{scenarios}} can be explored using simulation. Parameters such as inputs, data rates, <b>memory</b> <b>hierarchies,</b> and speed can be varied and by analyzing simulation results engineers can study the various trade-offs {{until they reach}} an optimal solution or an optimized design.|$|R
40|$|Increasing {{disparity}} between processor and memory speeds {{has been a}} motivation for designing systems with deep <b>memory</b> <b>hierarchies.</b> Most data-dominated multimedia applications do not use their cache e ciently and {{spend much of their}} time waiting for memory accesses [1]. This also implies a signi cant additiona...|$|R
