10|10000|Public
40|$|International audienceWe {{consider}} {{the problem of}} parameter estimation of Markovian models where the exact computation of the partition function is not possible or computationally too expensive with MCMC methods. The <b>main</b> <b>idea</b> <b>is</b> <b>then</b> to approximate {{the expression of the}} likelihood by a simpler one where we can either have an analytical expression or compute it more efficiently. We consider two approaches: Variational Bayes Approximation (VBA) and Mean Field Approximation (MFA) and study the properties of such approximations and their effects on the estimation of the parameters...|$|E
3000|$|... [...]) to c (numeric or categorical) {{attribute}} values. The {{values of}} each feature {{can be represented}} in a separate n×n matrix. As an important special case of dyad-attributed networks, we study multiplex networks. In these networks, all dyad features are integer-valued. Thus, each feature {{can be interpreted as}} (or can be derived from) a separate multigraph over the same set of nodes. In our setting, the <b>main</b> <b>idea</b> <b>is</b> <b>then</b> to try and explain the occurrence of a multiset of edges E in one multigraph G with nodes V by using other multigraphs Ĝ on the same node set.|$|E
40|$|In {{this paper}} we study how global {{optimization}} methods (like genetic algorithms) {{can be used}} to train neural networks. These methods are useful when local (for example gradient-based methods) do not work well. We introduce the notion of regularity for studying properties of the error function. If regularities are present in the error function, then they expand the search space in an artificial way. Regularities are used to generate constraints on the weights of the network. By the introduction of constraints we avoid the expansion of the search space. The <b>main</b> <b>idea</b> <b>is</b> <b>then</b> to consider the training of the network as a constrained optimization problem. Often there are other constraints on the weights of the network (for example domain constraints, shared weights). In order to find a satisfiable set of constraints we use a constraint logic programming system. We also relate the notion of regularity to so-called network transformations...|$|E
50|$|This {{piece is}} in the form of a polonaise; its {{principal}} section contains three <b>main</b> <b>ideas</b> which <b>are</b> <b>then</b> overcome by the persistent rhythms of the Trio section.|$|R
40|$|Decolonizing the Viking Age 1 {{argues that}} the Scandinavian “Viking Age” {{can be seen as}} a system of {{knowledge}} constructed in the late 19 th century and in its basic structures maintained up to the present day. This system of knowledge was heavily influenced by the nationalistic and evolutionary ideas of its time of making and may be described as a colonialism of the past. The book follows the making of the Viking Age from the start, through the most influential academic studies of the 20 th century and up to the most authorative recent works. A deconstruction of its <b>main</b> <b>ideas</b> <b>is</b> <b>then</b> suggested. In the second half of the book, a study of south-east Scandinavia is presented. This study is based upon discussions of “Old Norse” semantics of cultural landscapes, temporality and of the important connection between collective death rituals and the community of large groups of people. The results of this study are found to be incompatible with the knowledge structures of the “Viking Age” and, in a third and concluding part of the book, ways of “decolonization” and of reaching beyond the Viking Age are suggested. Decolonizing the Viking Age 1 is the first part of a dissertation in archaeology in two parts. The second volume, Death Rituals in South-East Scandinavia AD 800 – 1000. Decolonizing the Viking Age 2, is an archaeological work that creates the empirical foundation for the study of south-east Scandinavia found in the second part of the present volume. Death Rituals in South-East Scandinavia AD 800 – 1000 is an archaeological study of burials and cemeteries datable to the “Viking Age” in the present South Scandinavian provinces of Scania, Blekinge, Halland, Bornholm, Öland and south Småland. While burials of the time studied here have previously been thought of as components of a similar pan-Scandinavian Viking Age culture with more or less the same traditions everywhere, the main argument of this work is that mortuary rituals of this time actually followed the specific and rather different cultural norms of a whole number of smaller and larger “ritual systems”. These ritual systems were in some cases specific to small and geographically bounded settlement districts and in other cases to larger areas made up of several geographical regions. Death Rituals in South-East Scandinavia AD 800 – 1000 makes up the second part of Decolonizing the Viking Age, which is a dissertation in archaeology. While being an independent study of death rituals, the book at the same time creates the empirical base for archaeological discussions in Decolonizing the Viking Age 1...|$|R
30|$|The <b>main</b> <b>idea</b> <b>is</b> to {{construct}} a subsolution of the equation.|$|R
40|$|This diploma {{thesis is}} {{concerned}} with the question of the right conceptual approach towards consciousness. It opens up with the thesis that the crucial characteristic of consciousness - its subjective aspect - is profoundly elusive. To understand the nature of this elusiveness we get a loose inspiration from Karl Jaspers (of the continental tradition) and his idea of "subject-object dichotomy" whose main point is a realisation that the conscious subject is in principle unobjectifiable and can never be properly grasped by objectifying thinking. This <b>main</b> <b>idea</b> <b>is</b> <b>then</b> applied to various modern theories of consciousness (coming from the analytical tradition) in order to explore and demonstrate to what extend each of the theories misses or acknowledges the specific irreducibility of consciousness to objectively describable phenomena. Thus we observe that J. J. C. Smart omits subjectivity from his identity theory altogether since he understands reality as objectively graspable in all its aspects. Colin McGinn comes with an interesting explanation of our problems with grasping consciousness as part of the physical world and asserts that we are "cognitively closed" with respect to the solution of the mind-body problem. However, he concludes that a possible solution delivered in objectifying terms exists [...] ...|$|E
40|$|Within {{the frame}} of {{machinery}} maintenance, spectral analysis is a helpful tool. Therefore, an automatic spectral analysis tool, capable to identify each component of a measured signal would be of interest. This paper studies a new spectral analysis strategy for detecting, characterizing and classifying all spectral components of an unknown process. Indeed, any vibration signal {{can be considered as}} a mixture of components, a component being either a sinusoidal wave, or a narrow band one. We assume that a sum of an unknown number of these components is embedded in an unknown colored noise. The complete methodology we propose provides a way to feature each component in the spectral domain. The first idea is not to choose one specific spectral analysis method but, rather, to concatenate the results of complementary algorithms. For each one, the noise spectrum is estimated by a nonlinear filter and spectral component detection is managed with a local Bayesian hypothesis testing. This test is defined in frequency and takes account of the noise spectrum estimator. Thanks to a matching with the corresponding spectral window, each component detected is classified into one of the following four classes: Pure Frequency, Narrow Band, Alarm and Noise. The second <b>main</b> <b>idea</b> <b>is</b> <b>then</b> to propose a fusion of the classification results, leading to a complete description of each spectral component present in the signal. This spectral classification is particularly interesting within the context of condition monitoring. Examples are given on real vibratory signals and show the performance of the proposed automatic method, which is particularly well adapted to signals having a high number of components...|$|E
40|$|In 1953, the {{physicists}} E. Inonü and E. P. Wigner {{introduced the}} concept of deformation of a Lie algebra by claiming that the limit 1 /c → 0, when c is the speed of light, of the composition law (u,v) → (u+v) /(1 +(uv/c^ 2)) of speeds in special relativity (Poincaré group) should produce the composition law (u,v) → u + v used in classical mechanics (Galilée group). However, the dimensionless composition law (u'=u/c,v'=v/c) → (u'+v') /(1 +u'v') does not contain any longer a perturbation parameter. Nevertheless, this idea brought {{the birth of the}} " deformation theory of algebraic structures", culminating {{in the use of the}} Chevalley-Eilenberg cohomology of Lie algebras and one of the first applications of computer algebra in the seventies. One may also notice that the main idea of general relativity is to deform the Minkowski metric of space-time by means of the small dimensionless parameter ϕ/c^ 2 where ϕ=GM/r is the gravitational potential at a distance r of a central attractive mass M with gravitational constant G. A few years later, a " deformation theory of geometric structures " on manifolds of dimension n was introduced and one may quote riemannian, symplectic or complex analytic structures. Though often conjectured, the link between the two approaches has never been exhibited and the aim of this paper is to provide the solution of this problem by new methods. The key tool is made by the " Vessiot structure equations " (1903) for Lie groups or Lie pseudogroups of transformations, which, contrary to the " Cartan structure equations ", are still unknown today and contain " structure constants " which, like in the case of constant riemannian curvature, have in general nothing to do with any Lie algebra. The <b>main</b> <b>idea</b> <b>is</b> <b>then</b> to introduce the purely differential Janet sequence 0 →Θ→ T → F_ 0 → F_ 1 → [...] . → F_n → 0 as a resolution of the sheaf Θ⊂ T of infinitesimal transformations and to induce a purely algebraic " deformation sequence " with finite dimensional vector spaces and linear maps, even if Θ is infinite dimensional. The infinitesimal equivalence problem for geometric structures has to do with the local exactness at F_ 0 of the Janet sequence while the deformation problem for algebraic structures has to do with the exactness of the deformation sequence at the invariant sections of F_ 1, that is ONE STEP FURTHER ON in the sequence and this unexpected result explains why the many tentatives previously quoted have not been successful. Finally, we emphasize through examples the part that could be played by computer algebra in any explicit computation. Comment: This paper is an extended version of an invited lecture at the international conference SCA (Symbolic Computation and its applications) 2012, held in Aachen (Aix-la-Chapelle), Germany, may 17 - 20, 201...|$|E
5000|$|... the <b>main</b> <b>idea</b> <b>is</b> to get {{the king}} through to capture {{opposing}} pawns ...|$|R
50|$|The <b>main</b> <b>idea</b> <b>is</b> {{to smooth}} image {{brightness}} between {{series of the}} same scene frames.|$|R
50|$|The <b>main</b> <b>idea</b> <b>was</b> {{to unite}} all {{organizations}} {{through the years}} into one Ultras Organization.|$|R
40|$|It {{is both an}} {{experimental}} and theoretical fact that imaging of scatterers using bandlimited signals results in {{what is known as}} a diffraction-limited image. As a consequence, the best possible resolution obtained from a diffraction-limited system is about half a wavelength of the illuminating wavefield. In the near-field imaging systems used in optics this problem has been overcome by measuring the contribution of evanescent wavefields. However, in case of seismics, the sources and receivers are placed {{more than three times the}} wavelength away from the target and the evanescent wavefields are highly attenuated and thus fall below the noise level. Nevertheless, super-resolution imaging (i. e. imaging beyond the diffraction-limit) is possible by utilizing the time-reversal MUltiple SIgnal Classification (MUSIC) algorithm. The <b>main</b> <b>idea</b> <b>is</b> <b>then</b> to perform a Singular Value Decomposition (SVD) of the Multistatic Response (MSR) matrix to obtain the source and receiver side singular vectors which transforms the active experiment into a purely passive one. These singular vectors contain interaction information about the different scatterers, which is the key to obtain super-resolution. Though this algorithm can provide a super-resolved localization of point targets, it is also highly noise sensitive. In this thesis, we propose a phase-coherent time-reversal MUSIC (PC-MUSIC) algorithm, which utilizes the band of frequencies present in the measured data and exhibits a phase-coherent nature of the time-reversal operator. The noise present in the resulting monochromatic time-reversal MUSIC image can now be minimized by averaging over a smaller band of frequencies. The robustness and super-resolution ability of PC-MUSIC has been demonstrated employing both experimental ultrasonic data and numerical simulations based on the Foldy-Lax interaction model. Both time-reversal MUSIC and its phase-coherent version, PC-MUSIC, are originally designed to localize point like targets. In seismic or Ground Penetrating Radar (GPR) signals, the contributions from point like targets are carried by the diffracted wavefield. However, diffracted signals both in seismic and GPR are often much weaker than the specular reflections making it difficult to utilize them for super-resolution imaging. In this thesis, we propose to separate the diffracted signals from the reflected ones using two parameterized diffraction traveltime approximations. The first technique is based on a modified version of the Common Reflection Surface (CRS) technique. The second diffraction traveltime approximation is based on the REplacement Medium (REM) approach derived in this thesis for applications in a laterally smooth velocity field. The actual diffraction enhancement (or separation) is then carried out by stacking the data along the two approximate diffraction traveltime surfaces with optimal parameters determined using a coherency measure. As possible coherency measure candidates we tested both conventional Semblance and higher-resolution coherency measures like MUSIC, Eigen Vector (EV) and Minimum Variance (MV). The higher-resolution coherency measures, originally developed for narrowband Direction Of Arrival (DOA) estimation, were extended to handle the highly correlated and wideband seismic and GPR signals. From this extensive testing, employing both controlled data (Marmousi) as well as field data (both GPR and seismic), we concluded that the MUSIC coherency measure provides the most optimal diffraction traveltime parameters. After separating the diffractions from the reflections by stacking along the optimized diffraction traveltime surfaces, we performed diffraction imaging using both conventional Kirchhoff migration and a new high-resolution MUSIC like imaging algorithm known as Semblance balanced MUSIC (SB-MUSIC). This new algorithm outperformed classical migration when applied to various controlled and field data...|$|E
60|$|INFORMATION {{is what we}} want, {{clothed in}} the {{peculiar}} western style of the character we want to present. The <b>main</b> <b>idea</b> <b>is</b> to be NATURAL, DIRECT, AND CONCISE.|$|R
50|$|Helen Herz Cohen founded The <b>Main</b> <b>Idea</b> at Camp Walden in 1968. The <b>Main</b> <b>Idea</b> <b>is</b> {{a nonprofit}} camp for {{economically}} disadvantaged girls. Camp Walden hosts the <b>Main</b> <b>Idea</b> on its property {{for ten days}} in August every year, {{at the close of}} the regular camping season. Every <b>Main</b> <b>Idea</b> camper attends for free. The <b>Main</b> <b>Idea's</b> budget <b>is</b> funded primarily through private donations from Camp Walden campers and alumnae.|$|R
3000|$|The <b>main</b> <b>idea</b> <b>is</b> to linearize the Dirichlet to Neumann map at {{constant}} boundary data {{equal to}} the parameter [...]...|$|R
50|$|The <b>main</b> <b>idea</b> <b>is</b> {{that the}} riskiness of one portfolio's returns is being {{adjusted}} for comparison to another portfolio's returns.|$|R
5000|$|... #Caption: An {{example of}} the food/drink offered. The <b>main</b> <b>idea</b> <b>is</b> tea, but coffee, cookies, {{pastries}} with jam, etc. are also offered.|$|R
3000|$|Now, we {{are going}} to build the dual {{interior}} proximal step for solving (GVI). The <b>main</b> <b>idea</b> <b>is</b> to construct a sequence [...]...|$|R
30|$|The {{next two}} results {{are based on}} {{different}} conditions from Hypothesis B. The <b>main</b> <b>idea</b> <b>is</b> inspired by Sung [5]. Here we consider stochastic domination, not identical distribution.|$|R
3000|$|The {{exhaustive}} {{will be used}} as {{a reference}} and an upper bound for the achievable sum rate. The <b>main</b> <b>idea</b> <b>is</b> to check all possible subcarrier assignments [...]...|$|R
50|$|June is {{the time}} for Acoustic music festival. Its <b>main</b> <b>idea</b> <b>is</b> to popularize {{acoustic}} music traditions. During the festival there are ancient musical instrument workshops and seminars.|$|R
50|$|Plains {{about the}} College of Dunaújváros’s {{predecessor}} is from 1952. The <b>main</b> <b>idea</b> <b>was</b> {{to make a}} technicum, a dormitory and a workshop. Latter was ready in 1956.|$|R
5000|$|June is {{the time}} for Acoustic music festival. The <b>main</b> <b>idea</b> <b>is</b> to popularize {{acoustic}} music traditions. During the festival there are ancient musical instrument workshops and seminars.|$|R
3000|$|... can {{be fixed}} during word-length selection. The right side of Figure 2 shows basic blocks for word-length selection. The <b>main</b> <b>idea</b> <b>is</b> to iterate trying {{different}} word-length (i.e., [...]...|$|R
50|$|The <b>main</b> <b>idea</b> <b>is</b> that if {{a member}} sends a {{postcard}} {{he or she will}} receive at least one postcard back from a random postcrosser somewhere in the world.|$|R
50|$|The <b>main</b> <b>idea</b> <b>was</b> {{to develop}} a sculptural {{building}} that will integrate into the urban grain, yet preserving high visual impact, designed {{on the principles of}} contemporary minimalistic architecture.|$|R
30|$|With {{the ranking}} scores of keyterms, next we extract the keyphrases. The <b>main</b> <b>idea</b> <b>is</b> to {{generate}} candidate phrases {{first and then}} rank them based on the scores of keyterms.|$|R
30|$|The <b>main</b> <b>idea</b> <b>is</b> for {{a network}} to {{determine}} {{participants in a}} sensor collaboration by dynamically optimizing the utility function of data for a given cost of communication and computation.|$|R
3000|$|As {{described}} earlier, the <b>main</b> <b>idea</b> <b>is</b> to fit {{the mean}} motion velocity between two consecutive frames against the magnitude model of the scene. It gives a probability for running [...]...|$|R
50|$|The <b>main</b> <b>idea</b> <b>was</b> to {{eliminate}} dynamic forces. This was successfully exhibited when the locomotive was hung on chains. It silently simulated {{a run at}} maximum speed (60 km/h) without swinging.|$|R
5000|$|Miss Herz's goal in {{establishing}} The <b>Main</b> <b>Idea</b> <b>was</b> [...] "to {{try to give}} campers a new window in life, see something different, inspire them {{to do something with}} their lives." ...|$|R
5000|$|The {{algorithm}} {{for determining}} pose estimation {{is based on}} the Iterative Closest Point algorithm. The <b>main</b> <b>idea</b> <b>is</b> to determine the correspondences between 2D image features and points on the 3D model curve.|$|R
3000|$|... (cf. (Acciaio and Penner 2011)). The <b>main</b> <b>idea</b> <b>is</b> {{to replace}} the {{equality}} in (17) by an inequality. The term middle acceptance or middle rejection is used depending on {{the direction of the}} inequality.|$|R
30|$|Our <b>main</b> <b>idea</b> <b>is</b> to inscribe privacy {{protection}} into any analytical process by design, {{so that the}} analysis incorporates the relevant privacy requirements from the very start, evoking the concept of privacy-by-design discussed above.|$|R
50|$|In 2008, the Saint Petersburg State Academy of Art and Design {{housed the}} designer's final {{graduation}} collection. Its <b>main</b> <b>idea</b> <b>was</b> consumerism and the 'wrapping of a person' as {{means of identification}} and self-defense.|$|R
