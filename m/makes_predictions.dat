543|10000|Public
25|$|Gadagkar {{devised a}} unified model that <b>makes</b> <b>predictions</b> about what {{proportion}} of the population of R. marginata “should opt for a selfish solitary nesting strategy and what proportion should opt for an altruistic worker strategy” (853). From this, he was able to predict that 5% should opt for the selfish solitary nesting strategy while 95% should opt for the altruistic worker strategy.|$|E
25|$|Although {{neutrino}} oscillations {{have been}} experimentally confirmed, the theoretical foundations are still controversial, {{as it can}} be seen in the discussion related to sterile neutrinos. This <b>makes</b> <b>predictions</b> of possible Lorentz violations very complicated. It is generally assumed that neutrino oscillations require a certain finite mass. However, oscillations could also occur as a consequence of Lorentz violations, so there are speculations as to how much those violations contribute to the mass of the neutrinos.|$|E
25|$|Theorems in {{mathematics}} and theories in science are fundamentally different in their epistemology. A scientific theory cannot be proved; its key attribute {{is that it is}} falsifiable, that is, it <b>makes</b> <b>predictions</b> about the natural world that are testable by experiments. Any disagreement between prediction and experiment demonstrates the incorrectness of the scientific theory, or at least limits its accuracy or domain of validity. Mathematical theorems, on the other hand, are purely abstract formal statements: the proof of a theorem cannot involve experiments or other empirical evidence in the same way such evidence is used to support scientific theories.|$|E
30|$|Use {{pictures}} to <b>make</b> <b>predictions</b> about content.|$|R
50|$|Examination of the {{internal}} organs to <b>make</b> <b>predictions</b> is known as extispicy.|$|R
50|$|Tortoise shells {{were used}} by ancient Chinese as oracle bones to <b>make</b> <b>predictions.</b>|$|R
25|$|Sometimes a {{conjecture}} {{is called}} a hypothesis when it is used frequently and repeatedly as an assumption in proofs of other results. For example, the Riemann hypothesis is a conjecture from number theory that (amongst other things) <b>makes</b> <b>predictions</b> about the distribution of prime numbers. Few number theorists doubt that the Riemann hypothesis is true. In anticipation of its eventual proof, some have proceeded to develop further proofs which are contingent on {{the truth of this}} conjecture. These are called conditional proofs: the conjectures assumed appear in the hypotheses of the theorem, for the time being.|$|E
25|$|Relativity is a {{falsifiable}} theory: It <b>makes</b> <b>predictions</b> {{that can}} be tested by experiment. In the case of special relativity, these include the principle of relativity, the constancy {{of the speed of}} light, and time dilation. The predictions of special relativity have been confirmed in numerous tests since Einstein published his paper in 1905, but three experiments conducted between 1881 and 1938 were critical to its validation. These are the Michelson–Morley experiment, the Kennedy–Thorndike experiment, and the Ives–Stilwell experiment. Einstein derived the Lorentz transformations from first principles in 1905, but these three experiments allow the transformations to be induced from experimental evidence.|$|E
25|$|Under unified neutral theory, complex {{ecological}} {{interactions are}} permitted among individuals of an ecological community (such as competition and cooperation), providing all individuals obey the same rules. Asymmetric phenomena such as parasitism and predation are ruled {{out by the}} terms of reference; but cooperative strategies such as swarming, and negative interaction such as competing for limited food or light are allowed, so long as all individuals behave the same way. The theory <b>makes</b> <b>predictions</b> that have implications for the management of biodiversity, especially the management of rare species. It predicts the existence of a fundamental biodiversity constant, conventionally written θ, that appears to govern species richness {{on a wide variety of}} spatial and temporal scales.|$|E
40|$|Part 2 : Full PapersInternational audienceAnnotations {{obtained}} by Cultural Heritage institutions {{from the crowd}} need to be automatically assessed for their quality. Machine learning using graph kernels is an effective technique to use structural information in datasets to <b>make</b> <b>predictions.</b> We employ the Weisfeiler-Lehman graph kernel for RDF to <b>make</b> <b>predictions</b> {{about the quality of}} crowdsourced annotations in Steve. museum dataset, which is modelled and enriched as RDF. Our results indicate that we could predict quality of crowdsourced annotations with an accuracy of 75  %. We also employ the kernel to understand which features from the RDF graph are relevant to <b>make</b> <b>predictions</b> about different categories of quality...|$|R
30|$|Step 3 <b>Make</b> <b>prediction</b> by {{the trained}} INW–ESN {{based upon the}} dynamic multi-steps strategy.|$|R
5000|$|Validated {{relationships}} {{are applied to}} <b>make</b> <b>predictions</b> about future events in the learning environment.|$|R
2500|$|In Mad Magazine's section {{entitled}} the [...] "strip club" [...] a comic strip entitled Middle School Nostradamus appears every so often. Nostradamus {{is depicted as}} a preteen in wizard garb who <b>makes</b> <b>predictions</b> of impending despair for the people he is around at inopportune times.|$|E
2500|$|Benford's law, {{also called}} the first-digit law, is an {{observation}} about the frequency distribution of leading digits in many real-life sets of numerical data. The law states that in many naturally occurring collections of numbers, the leading significant digit {{is likely to be}} small. For example, in sets that obey the law, the number [...] appears as the most significant digit about 30% of the time, while [...] appears as the most significant digit less than 5% of the time. By contrast, if the digits were distributed uniformly, they would each occur about 11.1% of the time. Benford's law also <b>makes</b> <b>predictions</b> about the distribution of second digits, third digits, digit combinations, and so on.|$|E
2500|$|Mackerras {{has been}} a regular {{commentator}} on Australian elections in print, {{on radio and television}} on most federal and state elections. He has become well known for his predictions of electoral outcomes. He claims a [...] "win" [...] ratio of two in three and adds, [...] "at least I'm not boring. The election analyst who <b>makes</b> <b>predictions</b> is far more interesting than one who doesn't. And if I collect egg on my face, then so be it." [...] An example of an incorrect prediction was one he made in The Australian of 1 November 2004. [...] Mackerras said that John Kerry would defeat George W. Bush in a [...] "landslide" [...] in the U.S. presidential election the following day, and specifically predicted that Kerry would carry Florida, Ohio, Nevada and Missouri.|$|E
40|$|Most {{proposed}} Web prefetching techniques <b>make</b> <b>predictions</b> {{based on}} the historical references to requested objects. In contrast, this paper examines the accuracy of predicting a user’s next action based on analysis {{of the content of}} the pages requested recently by the user. Predictions are made using the similarity of a model of the user’s interest to the text in and around the hypertext anchors of recently requested Web pages. This approach can <b>make</b> <b>predictions</b> of actions that have never been taken by the user and potentially <b>make</b> <b>predictions</b> that reflect current user interests. We evaluate this technique using data from a full-content log of Web activity and find that textual similarity-based predictions outperform simpler approaches...|$|R
50|$|Task force 1: Mapping the {{arterial}} hypertension prevalence {{on the continent}} and <b>make</b> <b>predictions</b> for near future.|$|R
5000|$|We cannot {{explain the}} {{relationships}} we discover; we can <b>make</b> <b>predictions</b> {{only about the}} foreign policy behavior.|$|R
2500|$|In 1983, Crichton wrote Electronic Life, a {{book that}} {{introduces}} BASIC programming to its readers. The book, written like a glossary, with entries such as [...] "Afraid of Computers (everybody is)", [...] "Buying a Computer", and [...] "Computer Crime", was intended to introduce the idea of personal computers to a reader who might be faced with the hardship of using them at work or at {{home for the first}} time. It defined basic computer jargon and assured readers that they could master the machine when it inevitably arrived. In his words, being able to program a computer is liberation; [...] "In my experience, you assert control over a computer—show it who's the boss—by making it do something unique. That means programming it....If you devote a couple of hours to programming a new machine, you'll feel better about it ever afterwards". In the book, Crichton predicts a number of events in the history of computer development, that computer networks would increase in importance as a matter of convenience, including the sharing of information and pictures that we see online today which the telephone never could. He also <b>makes</b> <b>predictions</b> for computer games, dismissing them as [...] "the hula hoops of the '80s", and saying [...] "already there are indications that the mania for twitch games may be fading." [...] In a section of the book called [...] "Microprocessors, or how I flunked biostatistics at Harvard", Crichton again seeks his revenge on the medical school teacher who had given him abnormally low grades in college. Within the book, Crichton included many self-written demonstrative Applesoft (for Apple II) and BASICA (for IBM PC compatibles) programs.|$|E
2500|$|In the 1960s, Chomsky {{introduced}} two central ideas {{relevant to}} the construction and evaluation of grammatical theories. The first was the distinction between competence and performance. Chomsky noted the obvious fact that people, when speaking in the real world, often make linguistic errors (e.g., starting a sentence and then abandoning it midway through). He argued that these errors in linguistic performance were ir{{relevant to the}} study of linguistic competence (the knowledge that allows people to construct and understand grammatical sentences). Consequently, the linguist can study an idealised version of language, greatly simplifying linguistic analysis (see the [...] "Grammaticality" [...] section below). The second idea related directly to the evaluation of theories of grammar. Chomsky distinguished between grammars that achieve descriptive adequacy and those that go further and achieved explanatory adequacy. A descriptively adequate grammar for a particular language defines the (infinite) set of grammatical sentences in that language; that is, it describes the language in its entirety. A grammar that achieves explanatory adequacy has the additional property that it gives an insight into the underlying linguistic structures in the human mind; that is, it does not merely describe the grammar of a language, but <b>makes</b> <b>predictions</b> about how linguistic knowledge is mentally represented. For Chomsky, the nature of such mental representations is largely innate, so if a grammatical theory has explanatory adequacy it must be able to explain the various grammatical nuances of the languages of the world as relatively minor variations in the universal pattern of human language. Chomsky argued that, even though linguists were still a long way from constructing descriptively adequate grammars, progress in terms of descriptive adequacy will only come if linguists hold explanatory adequacy as their goal. In other words, real insight into the structure of individual languages can only be gained through comparative study {{of a wide range of}} languages, on the assumption that they are all cut from the same cloth.|$|E
5000|$|If {{the agent}} <b>makes</b> <b>predictions,</b> be very sure {{to see if}} they become true ...|$|E
3000|$|Finding {{a way to}} <b>make</b> <b>predictions</b> {{that take}} into account both user, virtual and {{physical}} resources variations, [...]...|$|R
5000|$|Collaborative Filtering- {{contains}} {{a collection of}} applications used to <b>make</b> <b>predictions</b> about users interests and factorize large matrices.|$|R
25|$|SportsZone has a {{group of}} panelists discuss the latest sports news where they <b>make</b> <b>predictions</b> and cover campus athletics.|$|R
50|$|The Unified Theory also <b>makes</b> <b>predictions</b> {{that have}} {{profound}} {{implications for the}} management of biodiversity, especially the management of rare species.|$|E
5000|$|Baghdad’s Dead {{tells the}} story of a superman who {{researches}} the genetic modification of individuals and ethnic groups and <b>makes</b> <b>predictions</b> about their biological and social futures.|$|E
5000|$|Given a {{time series}} [...] with values [...] at times [...] {{and a model}} that <b>makes</b> <b>predictions</b> for those values , then the {{directional}} symmetry (DS) statistic is defined as ...|$|E
5000|$|We can <b>make</b> <b>predictions</b> about {{preparations}} {{leading to}} the result 'n' by using an expression similar to Born's rule: ...|$|R
40|$|The aim of {{semantic}} {{science is}} {{to allow for the}} publications of ontologies, observation data, and hypotheses/theories. Hypotheses <b>make</b> <b>predictions</b> on data and on new cases. Those hypotheses that fit the available evidence are called theories. This paper considers how thoeries can be used for predictions in new cases. Theories are typically very narrow and not all of the inputs to a theory are observed, so to <b>make</b> <b>predictions</b> on a particular case, many theories need to be used. Without any global design, the available theories do not necessarily fit together nicely. This paper explains how theories can be combined into theory ensembles to <b>make</b> <b>predictions</b> on a particular case. This is needed to evaluate theories, and to <b>make</b> useful <b>predictions.</b> We motivate and give desiderata for theory ensembles for level 1, feature-based, semantic science, which assumes that the data and the theories can be described in terms of features (random variables) ...|$|R
40|$|International audienceAn {{advanced}} interface {{for sports}} tournament predictions uses direct manipulation to allow users to <b>make</b> nonlinear <b>predictions.</b> Unlike previous interface designs, this proposed technique better matches {{the way people}} actually <b>make</b> <b>predictions,</b> such as first choosing the winner and then filling {{up the rest of}} the bracket...|$|R
50|$|A simple {{mathematical}} {{theory of}} hitches {{has been proposed}} by Bayman and extended by Maddocks and Keller. It <b>makes</b> <b>predictions</b> that are approximately correct when tested empirically. No similarly successful theory has been developed for knots in general.|$|E
50|$|Models {{are only}} as good as the {{assumptions}} on which they are based. If a model <b>makes</b> <b>predictions</b> which are out of line with observed results and the mathematics is correct, the initial assumptions must change to make the model useful.|$|E
50|$|In {{addition}} to protein secondary structure, JPred also <b>makes</b> <b>predictions</b> of solvent accessibility and coiled-coil regions. The JPred service runs up to 134 000 jobs per month and has carried out over 2 million predictions in total for users in 179 countries.|$|E
30|$|Anticipate: {{being able}} to <b>make</b> <b>predictions</b> {{to know what to}} expect, and {{being able to}} act on {{developments}} further into the future.|$|R
50|$|Analysis of maturation involves {{assessing}} the thermal {{history of the}} source rock in order to <b>make</b> <b>predictions</b> of the amount and timing of hydrocarbon generation and expulsion.|$|R
50|$|The Oracle at Delphi {{was also}} famous for trances {{in the ancient}} Greek world; priestesses there would <b>make</b> <b>predictions</b> about the future in {{exchange}} for gold.|$|R
