57|985|Public
40|$|Abstract. Based on the 2006 {{edition of}} the Model Size Metrics workshop, we believe that counts are undervalued as useful <b>model</b> <b>metrics.</b> In this {{position}} paper, we provide arguments from the literature so as to consider counts as important metrics for the model measurement. We then state associated issues and sketch a model-driven framework to raise the abstraction level of the implementation of <b>model</b> <b>metrics,</b> starting with count metrics. ...|$|E
40|$|This paper {{presents}} quality {{goals for}} models {{and provides a}} state-of-the-art analysis regarding <b>model</b> <b>metrics.</b> While model-based software development often requires assessing the quality of models at different abstraction and precision levels and developed for multiple purposes, existing work on <b>model</b> <b>metrics</b> do not reflect this need. Model size metrics are descriptive and {{may be used for}} comparing models but their relation to model quality is not welldefined. Code metrics are proposed to be applied on models for evaluating design quality while metrics related to other quality goals are few. Models often consist of a significant amount of elements, which allows a large amount of metrics to be defined on them. However, identifying useful <b>model</b> <b>metrics,</b> linking them to model quality goals, providing some baseline for interpretation of data, and combining metrics with other evaluation models such as inspections requires more theoretical and empirical work...|$|E
40|$|In this {{position}} {{paper we discuss}} a number of issues relating to <b>model</b> <b>metrics,</b> with particular emphasis on metrics for UML models. Our discussion is presented as a series of nine observations where we examine some of the existing work on applying metrics to UML models, present some of our own work in this area, and specify some topics for future research that we regard as important. Furthermore, we identify three categories of challeges for <b>model</b> <b>metrics</b> and describe how our nine observations can be partitioned into these categories...|$|E
50|$|Live {{testing is}} {{the final stage of}} {{development}} and requires the developer to compare actual live trades with both the backtested and forward tested <b>models.</b> <b>Metrics</b> compared include percent profitable, profit factor, maximum drawdown and average gain per trade.|$|R
5000|$|System Engineering Standards, <b>Modeling,</b> Paradigms, <b>Metrics,</b> Testing, Management, Optimization, Simulation, Scheduling, Reliability and Fault Tolerant ...|$|R
50|$|<b>Models</b> and <b>Metrics</b> is {{the third}} studio album by American jam band Family Groove Company {{released}} in 2008.|$|R
40|$|Abstract: The {{assessment}} of quality in a software development process {{is vital for}} the quality of the final system. A number of approaches exist, which can be used to determine such quality properties. In a model-driven development process models are the primary artifacts. Novel technologies are needed in order to assess the quality of those artifacts. Often, the Object Constraint Language is used to formulate <b>model</b> <b>metrics</b> and to compute them automatically afterwards. This paper describes an approach for the generation of <b>model</b> <b>metrics</b> expressed as OCL statements based on a set of generic rules. These rules can be applied on any domain specific modeling languages for creating a basic set of metrics which can be tailored for the specific needs of a development process. The paper also briefly describes a prototype of a tool for the generation, computation, and management of these <b>model</b> <b>metrics</b> by using the Software Metrics Meta-model- SMM...|$|E
40|$|Abstract. The paper {{describes}} {{a conceptual framework}} for model-driven development based on concise application of UML and modeling tool functionality. A case study of modeling software for library management is presented as an illustration of how to apply the proposed framework. Modeling tool features such as model transformations, code generation cartridges, model validation, dependency matrix, <b>model</b> <b>metrics,</b> model comparison, and model refactoring are presented as enablers for efficient model-driven development. The presented ideas and samples are based on industrial experience of authors who work as trainers and consultants for the modeling tool MagicDraw UML...|$|E
40|$|Although water {{temperature}} {{is important to}} stream biota, {{it is difficult to}} collect in a spatially and temporally continuous fashion. We used remotely-sensed Land Surface Temperature (LST) data to estimate mean daily stream temperature for every confluence-to-confluence reach in the John Day River, OR, USA for a ten year period. Models were built at three spatial scales: site-specific, subwatershed, and basin-wide. Model quality was assessed using jackknife and cross-validation. <b>Model</b> <b>metrics</b> for linear regressions of the predicted vs. observed data across all sites and years: site-specific r 2 = 0. 95, Root Mean Squared Error (RMSE) = 1. 25 °C; subwatershed r 2 = 0. 88, RMSE = 2. 02 °C; and basin-wide r 2 = 0. 87, RMSE = 2. 12 °C. Similar analyses were conducted using 2012 eight-day composite LST and eight-day mean stream temperature in five watersheds in the interior Columbia River basin. Mean <b>model</b> <b>metrics</b> across all basins: r 2 = 0. 91, RMSE = 1. 29 °C. Sensitivity analyses indicated accurate basin-wide models can be parameterized using data from as few as four temperature logger sites. This approach generates robust estimates of stream temperature through time for broad spatial regions for which there is only spatially and temporally patchy observational data, and may be useful for managers and researchers interested in stream biota...|$|E
40|$|Metrics {{are gaining}} {{importance}} and acceptance in corporate sectors as organizations grow, mature and strive to improve enterprise qualities. Measurement {{of a test}} process is a required competence for an effective software test manager for designing and evaluating a cost effective test strategy. Effective management of any process requires quantification, measurement and <b>modeling.</b> Software <b>Metrics</b> provide quantitative approach to the development and validation of the software process <b>models.</b> <b>Metrics</b> help organization to obtain the information it needs to continue to improve its productivity, reduce errors and improve acceptance of processes, products and services and achieve the desired Goal. This paper, focusing on metrics lifecycle, various software testing metrics, need for having metrics, evaluation process and arriving at ideal conclusion have also been discussed in the present paper...|$|R
40|$|Abstract—Metrics {{are gaining}} {{importance}} and acceptance in corporate sectors as organizations grow, mature and strive to improve enterprise qualities. Measurement {{of a test}} process is a required competence for an effective software test manager for designing and evaluating a cost effective test strategy. Effective management of any process requires quantification, measurement and <b>modeling.</b> Software <b>Metrics</b> provide quantitative approach to the development and validation of the software process <b>models.</b> <b>Metrics</b> help organization to obtain the information it needs to continue to improve its productivity, reduce errors and improve acceptance of processes, products and services and achieve the desired Goal. This paper, focusing on metrics lifecycle, various software testing metrics, need for having metrics, evaluation process and arriving at ideal conclusion have also been discussed in the present paper. Keywords- Software Testing, Software Testing Metrics I...|$|R
40|$|Abstract—Indirect <b>metrics</b> {{in quality}} <b>models</b> define {{weighted}} integrations of direct metrics to provide higher-level quality indicators. This paper presents {{a case study}} that investigates to what degree quality models depend on statistical assumptions about the distribution of direct metrics values when these are integrated and aggregated. We vary the normalization used by the quality assessment efforts of three companies, while keeping quality <b>models,</b> <b>metrics,</b> metrics implementation and, hence, metrics values constant. We find that normalization has a considerable impact on the ranking of an artifact (such as a class). We also investigate how normalization affects the quality trend and find that normalizations have a considerable effect on quality trends. Based on these findings, we find it questionable to continue to aggregate different metrics in a quality model as we do today. I...|$|R
40|$|As {{with other}} {{software}} development processes, model-driven engineering of real time software systems include quality assurance and measurement. Model-driven engineering (MDE) supports {{the development of}} real-time software systems {{by means of a}} set of languages, processes, methods and tools. To measure the models, a dedicated measurement software has to be developed, which is costly. In this paper, we propose a framework to concisely define and automatically implement an open-ended family of metrics for real time software systems. The overall contribution of this approach is to give an instant, reliable and low cost implementation of <b>model</b> <b>metrics</b> seamlessly integrated into modeling tools. ...|$|E
40|$|Abstract—This paper {{investigates the}} interrelationship among various {{measured}} {{characteristics of a}} software project, ranging from project model, size, and metrics used to govern {{the administration of the}} project. By analyzing various dimensions of project characteristics based on the underlying <b>model,</b> <b>metrics</b> and project technicality such as language and development paradigm, our findings reveal that certain metrics and models are not suitable for small project since they possess insufficient information to extract and analyze the inherent characteristics of the project. As such, project managers should pay attention to proper selection of project parameters that are conducive toward accurate estimations. I...|$|E
40|$|International audienceAs {{with other}} {{software}} development processes, model-driven engineering of real time software systems include quality assurance and measurement. Model-driven engineering (MDE) supports {{the development of}} real-time software systems {{by means of a}} set of languages, processes, methods and tools. To measure the models, a dedicated measurement software has to be developed, which is costly. In this paper, we propose a framework to concisely define and automatically implement an open-ended family of metrics for real-time software systems. The overall contribution of this approach is to give an instant, reliable and low cost implementation of <b>model</b> <b>metrics</b> seamlessly integrated into modeling tool...|$|E
40|$|A {{software}} quality model {{and its associated}} attributes are defined and used {{as the model for}} the basis for a discussion on risk. Specific quality goals and attributes are selected based on their importance to a software development project and their ability to be quantified. Risks that can be determined by the <b>model's</b> <b>metrics</b> are identified. A core set of metrics relating to the software development process and its products is defined. Measurements for each metric and their usability and applicability are discussed...|$|R
40|$|Software metrics {{provide an}} {{effective}} method for characterizing software. Metrics {{have traditionally been}} composed through the definition of an equation. This approach {{is limited by the}} fact that all the interrelationships among all the parameters be fully understood. This paper explores an alternative, neural network approach to <b>modeling</b> <b>metrics.</b> Experiments performed on two widely accepted metrics, McCabe and Halstead, indicate that the approach is sound, thus serving as the groundwork for further exploration into the analysis and design of software metrics...|$|R
40|$|This paper {{attempts}} to characterize and present {{a state of}} the art view of several quantitative <b>models</b> and <b>metrics</b> of the software life cycle. These <b>models</b> and <b>metrics</b> can be used to aid in managing and engineering software projects. They deal with various aspects of the software process and product, including resources allocation and estimation, changes and errors, size, complexity and reliability. Some indication is given {{of the extent to which}} the various models have been used and the success they have achieved...|$|R
40|$|Abstract. Successful {{supply chain}} {{management}} becomes essential for the ultimate success of corporations. Companies today seek an effective performance measurement (PM) system to maximize the bottom line. Unfortunately, performance measurement in the supply-chain field has not kept pace with today’s world of interdependent business relationships. What companies need is a new PM system that unifies different business elements, concepts, technologies and tools. In this paper, the architecture of such a pervasive PM system is introduced. The main system elements such as process <b>model,</b> <b>metrics</b> and data warehouse are described. Finally, a specialized PM web portal which enables proactive performance monitoring and fosters the improvement and optimization is presented...|$|E
40|$|Abstract. X 3 D-UML utilises X 3 D (eXtensible 3 D) {{to enable}} standards-based {{advanced}} 3 D UML visualisations. Using X 3 D-UML, 3 D UML State Machine Diagrams {{have been evaluated}} against actual user tasks and data, using the Sequential Evaluation methodology. The results of User Task Analysis, Heuristic Evaluation and Formative Evaluation phases provide clear evidence {{that the use of}} UML extended with 3 D is a practical solution for visualising complex system behaviour. RoseRT <b>model</b> <b>metrics</b> show between 56 %- 90 % of state machine diagram work would benefit from such 3 D UML extensions; hence the 3 D improvement can deliver considerable benefit to organisations...|$|E
40|$|Abstract: The {{importance}} of a business process model to be understandable to its reader is widely acknowledged. In this vein, several approaches to assess and improve understandability exist, such as theoretical quality frameworks, modeling guidelines and process <b>model</b> <b>metrics.</b> In this paper we propose to investigate the issue of under-standability from the angle of cognitive psychology. To this end, we discuss how the cognitive process of inference acts as a central process of problem solving. In particu-lar, we illustrate in how far chunking, computational offloading and external memory might {{have an impact on}} the understandability of process models. Our propositions are theory-based so far and will serve as basis for planned empirical investigations, as discussed in the research agenda...|$|E
40|$|We {{consider}} several viscous <b>models</b> with <b>metrics</b> FRW (k = 0) {{but with}} variable G,c and Λ. We find trivially {{a set of}} solutions through Dimensional Analysis. 1 Introduction. Recently have been studied several <b>models</b> with <b>metrics</b> FRW where the ”constants ” G and Λ ([3]) are considered as dependent functions on time t ([1]). Most recently this type of models have been generalized by Arbab ([2]) who considers a viscous fluid. Other authors ([3]) study models with c and...|$|R
40|$|A {{feasibility}} {{study has been}} worked out for an integrated advisory system for precision agriculture on the production of forage (grass and maize) for dairy farming in the Netherlands. Technology is not restrictive anymore to apply precision agriculture on important cultivation measures. It is now about achieving an effective integration of sensor data, <b>models,</b> <b>metrics</b> and equipment in a comprehensive advice system. It has been advised how a practical advice system for operational decisions can be realized on the dairy farm in the foreseeable future...|$|R
40|$|Measuring {{intranet}} overall value contributions {{based on}} a corporation’s critical business requirements. A Intranet <b>Model</b> and <b>Metrics</b> lthough many corporations store {{a great deal of}} information in their corporate intranets, few have a reliable means of measuring the effectiveness of their intranet portals to use this information to meet specific business needs. Turning infor-mation into knowledge capital that corpora-tions can leverage quickly for competitive advantage requires a <b>model</b> and <b>metrics</b> that tractably support it. Most intranet portal measurements are based almost exclusively on usage statistics—with lit...|$|R
30|$|For a {{commercial}} application {{it is possible}} to improve prediction metrics by creating a separate model for each power line, increasing the feature space during feature selection process and adding additional information to the feature space, such as the historical energy consumption properties and the weather forecast. These multimodal data sources are out of the scope of this research result, but in fact improve the <b>model</b> <b>metrics.</b> The cost of this improvement is an increase in computational complexity of each model, that could be efficiently parallelized in the cloud or by an efficient use of high performance computing (HPC) infrastructures, which usually exist in telecommunication and energy companies. All the computations we propose could be done in batch mode and do not require real-time processing.|$|E
40|$|Abstract: The {{paradigm}} of model-based software development {{has become more}} and more popular, since it promises an increase in the efficiency and quality of software development. Following this paradigm, models become primary artifacts in the soft-ware development process where quality assurance of the overall software product considerably relies on the quality assurance of involved software models. In this pa-per, we concentrate on the syntactical dimension of model quality which is analyzed and improved by <b>model</b> <b>metrics,</b> model smells, and model refactorings. We propose an integration of these model quality assurance techniques in a predefined quality assur-ance process being motivated by specific industrial needs. By means of a small case study, we illustrate the model quality assurance techniques and discuss Eclipse-based tools which support the main tasks of the proposed model quality assurance process. ...|$|E
30|$|A {{combined}} {{measure of}} perceived video quality for the H. 264 /AVC compression is proposed using no reference <b>model.</b> <b>Metrics</b> were implemented in a C/C++ environment {{as part of}} JM software of H. 264. The objective modeling of subjective quality parameters {{was derived from the}} defined standard model. The results are analyzed for correctness with the actual content quality for a given encoding scenario which shows that the values are highly correlated to the users viewing experience. Also these results are compared against a standard full reference model and verified using comparison methods as mentioned in VQEG for a set of training and test vectors. Based on these results, video impairment analysis based quality model which is relatively low computational requirements compared to full reference method was providing better quality indication is evident.|$|E
30|$|This section {{provides}} background {{about the}} regression methods, <b>model’s</b> performance <b>metrics,</b> and a metric {{for measuring the}} relative importance of the prediction factors used in the models.|$|R
50|$|There are {{a variety}} of <b>models</b> and <b>metrics</b> to measure the {{security}} of a system. These are a few methods {{that can be used to}} measure the security of software systems.|$|R
30|$|As power {{allocation}} in AF system critically {{depends on}} the SNR and power constraints of relays, accurate estimation of average SNR, which is sensitive to hop count and individual hop distributions, {{is essential for the}} effective implementation of the system. Hence, an expression for the average SNR is derived (in terms of relay amplification factor). It is then used to simulate a two-hop AF relay transmission system and the bit error performance is analysed for different compound channel <b>models.</b> <b>Metrics</b> like CV and average SNR derived can be used to select appropriate branches in a selection or switched diversity combining systems [9, 10].|$|R
40|$|His thesis deals with. {{the issues}} of {{measuring}} and managing projects In the first part the author describes the methodics of IS/ICT management, such as ITIL, COBIT, MMDIS and BSC and also some methologies of IT project management as PMBoK, PRINCE 2, or ValIT MMDISŘíP After analysis of this metodology, there was constructed metrics model, {{which is based on}} these methodologies and uses metrics that are contained in methodologies, or the metrics are logically derived from these methodologies. In the second part the author describes the characteristics and usage of this model. For application were used data of the 20 projects from the course 4 IT 414 IS/ICT Project Management. The conclusion contains an analysis and evaluation of data generated by this model. Keywords: Project methodology, Methodology for IS/ICT Management, Metric, <b>Model</b> <b>metrics,</b> project evaluation...|$|E
40|$|Despite {{the large}} body of {{knowledge}} on formal analysis techniques for process models, in particular Petri nets, {{there has been a}} notable gap of empirical research into verification. In this paper we compare the few studies that report results from applying verification techniques to real-world process model collections. For this comparison we are particularly interested in the different approaches, their computational performance, and the number of errors found. Our comparison reveals that most of the samples have error rates of 10 % to 20 %. Some of the studies have established a connection between error probability and process <b>model</b> <b>metrics,</b> as well as between model understanding and both metrics and modeling competence of the model reader. Based on these results, we discuss implications and directions for future research...|$|E
40|$|This paper {{investigates the}} {{performance}} of Vehicle-to-Infrastructure (V 2 I) services over Vehicular Networks (VANETs) that are assisted by Road Side Units (RSU). More specifically, an analytical study of RSU dimensioning and a respective module is designed and developed in a simulated VANET environment. Two V 2 I application scenarios (e. g. car crash, spot weather) are considered in order to evaluate the impact of RSUs, vehicles’ size and speed and car crash start time and duration on applications’ performance. It is shown that the VANET network metrics (Packet Loss and Packet Delivery Ratio) {{are affected by the}} available MAC Bit rates and application scenarios. Mobility <b>model</b> <b>metrics</b> (Total Busy Time and Total CO 2 Emissions) are also affected by the different application scenarios, number and type of vehicles...|$|E
40|$|The {{main goal}} of diploma thesis is {{to propose a}} <b>model</b> for <b>metrics</b> {{description}} and metrics themselves, which {{will be used for}} different project areas effectiveness and performance evaluation during core banking systems implementation. The second goal is to describe metrics using MBI (Management of Business Informatics) model and compare results. The thesis has the next structure: - theoretical part, where main terms and research works which addressed to the same topic are described; -analytical part, where <b>models</b> for <b>metrics</b> description and metrics will be introduced; - conclusion, where results will be evaluated and models will be compared...|$|R
5000|$|Part 2: [...] Metric Model: Defines a <b>Metrics</b> <b>Model</b> - under {{development}} ...|$|R
40|$|In {{order to}} {{alleviate}} the inherent problems of early multicast architecture, two protocols have been proposed for the inter-domain multicast deployment: MSDP and BGMP. In this paper we evaluate the overhead and performance of these two protocols using analytic and simulation <b>models.</b> <b>Metrics</b> include state and communication overhead, network resource utilization, join latency and end-to-end delay. The simulations are performed on a realistic model of the Internet AS topology. We discuss the overhead and performance results {{in the light of}} protocol complexity and manageability. Finally, we suggest several protocol improvements, and argue that MSDP may prevail as the inter-domain protocol solution due to its simplicity and sucient scalability...|$|R
