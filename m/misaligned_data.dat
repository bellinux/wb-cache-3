36|17|Public
50|$|A {{computer}} accesses memory by {{a single}} memory word at a time. As long as the memory word size {{is at least as}} large as the largest primitive data type supported by the computer, aligned accesses will always access a single memory word. This may not be true for <b>misaligned</b> <b>data</b> accesses.|$|E
30|$|To {{show the}} effect of {{projection}} misalignment, we use an ImageJ macro to shift each projection by a random distance with flat probability distribution between − 1 and + 1 pixels along its length, using bicubic interpolation {{in the case of}} non-integer shifts. The <b>misaligned</b> <b>data</b> is then reconstructed as per the previous example.|$|E
40|$|Binary Translation (BT) {{has been}} {{commonly}} used to migrate application software across Instruction Set Architectures (ISAs). Some architectures, such as X 86, allow <b>Misaligned</b> <b>Data</b> Accesses (MDAs), while most modern architectures require natural data alignments. In a binary translation system, where the source ISA allows MDA and the target ISA does not, memory operations must be carefully translated. Naive translation may cause frequent <b>misaligned</b> <b>data</b> access traps to occur at runtime on the target machine and severely slow down the migrated application. This article evaluates different approaches in handling MDA in a binary translation system including how to identify MDA candidates and how to translate such memory instructions. This article also proposes some new mechanisms to more effectively deal with MDAs. Extensive measurements based on SPEC CPU 2000 and CPU 2006 benchmarks show that the proposed approaches are more effective than existing methods and getting close to the performance upper bound of MDA handling. Categories and Subject Descriptors: D. 3. 4 [Programming Languages]: Processors—Code generation; optimization; run-time environment...|$|E
40|$|Abstract − A hidden Markov model based {{classifier}} {{is proposed}} {{in this paper}} to perform automatic speech recognition using myoelectric signals from the muscles of vocal articulation. The classifier's resilience to temporal variance is compared to a linear discriminant analysis classifier {{that was used in}} a pervious study. Speech recognition was performed, using five channels of myoelectric signals, on isolated words from a 10 word vocabulary. Temporal variance was induced by temporally <b>misaligning</b> <b>data</b> from the test set, with respect to the training set. When compared to the LDA classifier, the hidden Markov model classifier demonstrated a markedly lower variation in classification error due to the temporal misalignment. Characteristics of the hidden Markov model MES classifier suggest that it would effectively complement a conventional acoustic speech recognizer, in a multi-modal speech recognition system...|$|R
40|$|Peer-reviewed {{publications}} 1. Li, P., Banerjee, S., Hanson, T. A. and McBean, A. M. (2014). Nonparametric hierarchical modeling {{for detecting}} boundaries in areally referenced spatial datasets. Statistica Sinica (in press). 2. Finley, A. O., Banerjee, S. and Gelfand, A. E. (2014). spBayes: for large univariate and multivariate point-referenced spatio-temporal data models. Journal of Statistical Software (in press). 3. Monteiro, J. V., Banerjee, S. and Ramachandran, G. (2014). Bayesian modeling for physical processes in industrial hygiene using <b>misaligned</b> workplace <b>data.</b> Technometrics (in press). 4. Finley, A. O., Banerjee, S. and Cook, B. D. (2014). Bayesian hierarchical models for spatially misaligne...|$|R
40|$|We {{consider}} inference for <b>misaligned</b> multivariate functional <b>data</b> {{that represents}} the same underlying curve, but where the functional samples have systematic differences in shape. In this paper we introduce {{a new class of}} generally applicable models where warping effects are modeled through nonlinear transformation of latent Gaussian variables and systematic shape differences are modeled by Gaussian processes. To model cross-covariance between sample coordinates we introduce a class of low-dimensional cross-covariance structures suitable for modeling multivariate functional data. We present a method for doing maximum-likelihood estimation in the models and apply the method to three data sets. The first data set is from a motion tracking system where the spatial positions {{of a large number of}} body-markers are tracked in three-dimensions over time. The second data set consists of height and weight measurements for Danish boys. The third data set consists of three-dimensional spatial hand paths from a controlled obstacle-avoidance experiment. We use the developed method to estimate the cross-covariance structure, and use a classification setup to demonstrate that the method outperforms state-of-the-art methods for handling <b>misaligned</b> curve <b>data.</b> Comment: 44 pages in total including tables and figures. Additional 9 pages of supplementary material and reference...|$|R
40|$|Abstract: While {{modeling}} environmental processes, {{a researcher}} has often {{to cope with}} spatially <b>misaligned</b> <b>data.</b> In this paper {{we are interested in}} a situation where there is misalignment between explanatory variables and response one. In particular we consider PM 10 concentration levels measured by the monitoring network of Siracusa (Sicily) in 2003, with the aim to model their spatial and temporal variability accounting for weather variables. Four solutions will be proposed to cope with spatial misalignment between PM 10 concentration and weather data in a multilevel linear model framework...|$|E
40|$|Residential radon {{exposure}} {{is a serious}} public health concern, and as such appears in the recommendations of European Code Against Cancer. The objective {{of this study was}} to assess the association between residential radon levels and mortality due to different types of cancer, using <b>misaligned</b> <b>data</b> analysis techniques. Mortality data (observed cases) for each of the 313 Galician municipalities were drawn from the records of the National Statistics Institute for the study period (1999 ? 2008). Expected cases were computed using Galician mortality rates for 14 types of malignant tumors as reference, with a total of 56, 385 deaths due to the tumors analyzed. The effect estimates of indoor radon (3371 sampling points) were adjusted for sociodemographic variables, altitude, and arsenic topsoil levels (1069 sampling points), using spatial/geostatistical models fitted with stochastic partial differential equations and integrated nested Laplace approximations. Thesemodels are capable of processing <b>misaligned</b> <b>data.</b> The results showed a statistical association between indoor radon and lung, stomach and brain cancer inwomen in Galicia. Apart fromlung cancer (relative risk (RR) = 1. 09), inwhich a twofold increase in radon exposure led to a 9 % rise inmortality, the association was particularly relevant in stomach (RR= 1. 17) and brain cancer (RR= 1. 28). Further analytical epidemiologic studies are needed to confirm these results, and an assessment should be made of the advisability of implementing interventions targeting such exposure in higher-risk areas...|$|E
40|$|Geographic {{information}} systems have proven instrumental in assessing environmental impacts on individual and community health, but numerous methodological challenges {{are associated with}} analyses of highly localized phenomena in which spatially <b>misaligned</b> <b>data</b> are used. In a case study based on child care facility and traffic data for the Los Angeles metropolitan area, we assessed the extent of facility misclassification with spatially unreconciled data from 3 different governmental agencies {{in an attempt to}} identify child care centers in which young children are at risk from high concentrations of toxic vehicle-exhaust pollutants. Relative to geographically corrected data, unreconciled information produced a modest bias in terms of aggregated number of facilities at risk and a substantial number of false positives and negatives...|$|E
40|$|Baete K., Nuyts J., Van Laere K., Van Paesschen W., Ceyssens S., De Ceuninck L., Gheysens O., Kelles A., Van den Eynden J., Suetens P., Dupont P., ''Evaluation {{of anatomy}} based {{reconstruction}} for partial volume correction in brain FDG-PET'', NeuroImage, vol. 23, no. 1, pp. 305 - 317, September 2004. FDG-PET {{contributes to the}} diagnosis and management of neurological diseases. In some of these diseases, pathological gray matter (GM) areas may have a reduced FDG uptake. Detection of these regions can be difficult and some remain undiscovered using visual assessment. The main reason for this detection problem is the relatively small thickness of GM compared to the spatial resolution of PET, known as the partial volume effect. We have developed an anatomy-based maximum-a-posteriori reconstruction algorithm (A-MAP) which corrects for this effect during the reconstruction using segmented magnetic resonance (MR) data. Monte-Carlo based 3 -D brain software phantom simulations were used to investigate {{the influence of the}} strength of anatomy-based smoothing in GM, the influence of <b>misaligned</b> MR <b>data,</b> and the effect of local segmentation errors. A human observer study was designed to assess the detection performance of A-MAP versus post-smoothed maximum-likelihood (ML) reconstruction. We demonstrated the applicability of A-MAP using real patient data. The results for A-MAP showed improved recovery values and robustness for local segmentation errors. <b>Misaligned</b> MR <b>data</b> reduced the recovery values towards those obtained by post-smoothed ML, for small registration errors. In the human observer study, detection accuracy of hypometabolic regions was significantly improved using A-MAP, compared to post-smoothed ML (P < 0. 004). The patient study confirmed the applicability of A-MAP in clinical practice. Conclusion: A-MAP is a promising technique for voxel-based partial volume correction of FDG-PET of the human brain. status: publishe...|$|R
40|$|Abstract- In microprocessors, {{reducing}} the cache ac-cess {{time and the}} pipeline stall is critical to improve the system performance. To overcome the pipeline stall caused by the <b>misaligned</b> multi-words <b>data</b> or multi cycle accesses of prefetch codes which are placed over two cache lines, we proposed the Separated Word-line Decoding (SE W D) cache. SEWD cache {{makes it possible to}} access misaligned multi-ple words as well as aligned words in one clock cycle. This feature is invaluable in most microprocessors because the branch target address is usually misaligned, and many of <b>data</b> accesses are <b>misaligned.</b> 8 K-byte SEWD cache chip consists of 489, 000 transistors on a die size of 0. 853 x 0. 827 cm 2 and is implemented in 0. 8 pm DLM CMOS process op-erating at 60 MHz. I...|$|R
40|$|Abstract. We explore two {{characteristic}} {{features of}} x-ray computed tomography inversion formulas {{in two and}} three dimensions that are dependent on π-lines. In such formulas the data from a given source position contribute only to the reconstruction of f(x) for x in a certain region, called the region of backprojection. The second characteristic is a certain small artifact in the reconstruction, called a comet tail artifact. We propose that the comet tail artifact {{is closely related to}} the boundary of the region of backprojection and make this relationship precise, developing a general theory of the region of backprojection, its boundary, and the location of the artifact in helical and fan-beam tomography. This theory is applied to a number of specific examples and confirmed by numerical experiments. Furthermore it is demonstrated that a strong comet tail artifact appears in numerical reconstructions from <b>misaligned</b> fan-beam <b>data.</b> A numerical method to use the artifact to find the correct alignment is suggested...|$|R
40|$|In this paper, {{the problem}} of {{combining}} information from different data sources is considered. We focus our attention on spatially <b>misaligned</b> <b>data,</b> where available information (typically counts or rates from administrative sources) refers to spatial units that {{are different from the}} ones of interest. A hierarchical Bayesian perspective is considered, as proposed by Mugglin et al. in 2000, to provide a fully model-based approach in an inferential, and not only descriptive, sense. In particular, explanatory covariates are arranged to be modeled according to spatial correlations through a conditionally autoregressive prior structure. In order to assess model performance and its robustness we generate artificial data inspired by a real study and a simulation exercise is then carried out...|$|E
40|$|We propose novel {{methods for}} {{predictive}} (sparse) PCA with spatially <b>misaligned</b> <b>data.</b> These methods identify principal component loading vectors that explain as much {{variability in the}} observed data as possible, while also ensuring the corresponding principal component scores can be predicted accurately by means of spatial statistics at locations where air pollution measurements are not available. This {{will make it possible}} to identify important mixtures of air pollutants and to quantify their health effects in cohort studies, where currently available methods cannot be used. We demonstrate the utility of predictive (sparse) PCA in simulated data and apply the approach to annual averages of particulate matter speciation data from national Environmental Protection Agency (EPA) regulatory monitors. Comment: 43 pages, 5 figures, and 6 table...|$|E
40|$|Synchronization {{interfaces}} in a network-on-chip (NoC) {{are becoming}} vulnerable points {{that need to}} be safeguarded against link delay variations and signal misalignments. This paper addresses the challenge of designing a process variation and layout mismatch tolerant link for GALS NoCs by implementing a self-calibration mechanism. A variation detector senses the variability-induced misalignment between data lines with themselves and with the transmitter clock routed with data in source synchronous links. Then, a suitable delayed replica of the transmitter clock is selected for safe sampling of <b>misaligned</b> <b>data.</b> The paper proves correct operation of the GALS link augmented with the variation detector and compares its reliability with that of a detector-less link, beyond proving robustness with respect to the delay variability affecting the detector itself...|$|E
40|$|The {{class of}} Gaussian copula {{regression}} models provides a unified modeling framework to accommodate various marginal distributions and flexible dependence structures. In {{the presence of}} missing data, the Expectation-Maximization (EM) algorithm plays {{a central role in}} parameter estimation. This classical method is greatly challenged by multilevel correlation, large dimension of model parameters, and <b>misaligned</b> missing <b>data</b> mechanism. This dissertation develops a series of new methodologies to enhance the effectiveness of the EM algorithm in dealing with complex correlated data analysis via a combination of new concepts, estimation approaches, and computing procedures. Project 1 is focus on the development of an EM algorithm in Gaussian copula regression models with missing values, in which univariate location-scale family distributions are utilized for marginal regression models and Gaussian copula for dependence. To improve the implementation of the EM algorithm, we establish an effective peeling procedure in the M-step to sequentially maximize the observed log-likelihood with respect to regression parameters and dependence parameters. In addition, the Louis formula is provided for the calculation of the Fisher information. Project 2 is a critical extension of Project 1, where the assumption of structured correlation structure is relaxed, so the resulting model and algorithm can be applied to deal with complex correlated data with missing values. The key new contribution in the extension concerns the development of EM algorithm for composite likelihood estimation in the presence of <b>misaligned</b> missing <b>data.</b> We propose the complete-case composite likelihood to handle both point-identifiable and partially identifiable parameters in the Gaussian copula regression model. Estimation of a partially identifiable correlation parameter is given by an estimated interval. Both estimation properties and algorithmic convergences are discussed. Motivated by an electroencephalography (EEG) data, Project 3 concerns the regression analysis of multilevel correlated data. We develop a class of parametric regression models using Gaussian copulas and implement the maximum likelihood estimation. The proposed model is very flexible; in the aspect of regression model, it can accommodate continuous outcomes, or outcomes of mixed types; and in the aspect of dependence, it can allow temporal, spatial, clustered, or combined dependence structures...|$|R
40|$|A novel on-axis general sun-tracking formula {{has been}} {{integrated}} in the algorithm of an open-loop sun-tracking system {{in order to}} track the sun accurately and cost effectively. Sun-tracking errors due to installation defects of the 25 m 2 prototype solar concentrator have been analyzed from recorded solar images {{with the use of}} a CCD camera. With the recorded <b>data,</b> <b>misaligned</b> angles from ideal azimuth-elevation axes have been determined and corrected by a straightforward changing of the parameters' values in the general formula of the tracking algorithm to improve the tracking accuracy to 2. 99 mrad, which falls below the encoder resolution limit of 4. 13 mrad...|$|R
40|$|This paper {{proposes a}} graph-theoretic model that {{supports}} the design and analysis of data flow within digital musical instruments (DMIs). The {{state of the art}} in DMI design does not provide standards for the scheduling of computations within a DMI’s data flow. Without a theoretical framework, analysis of different scheduling protocols and their impact on the DMI’s performance is extremely difficult. As a result, the mapping between the DMI’s sensory inputs and sonic outputs is classically treated as a black box. DMI builders are forced to design and schedule the flow of data through this black box on their own. Improper design of the data flow can produce undesirable results, ranging from overflowing buffers that cause system crashes to <b>misaligned</b> sensory <b>data</b> that result in strange or disordered sonic events. In this paper, we attempt to remedy this problem by providing a framework for the design and analysis of the DMI data flow closely modeled after a framework for digital signal processing. We also propose the use of a scheduling algorithm built upon that framework, and prove that it guarantees desirable properties for the resulting DMI...|$|R
40|$|The PowerPC 604 e {{microprocessor}} is a lower power, higher performance {{extension of}} the PowerPC 604 TM microprocessor. The 604 e doubles the cache size and tunes the performance of memory accesses compared to the original 604. The 604 e has also added hardware support for <b>misaligned</b> <b>data</b> accesses when using little-endian byte ordering. The branch processing microarchitecture of the 604 e has been somewhat enhanced. To assist sofnvare writers in tuning their code, the 604 e has significantly enhanced its hardware performance monitor. These enhancements along with the process migration were done using the Somerset design methodology. This combination of enhancements along with changing to a faster, lower voltage silicon process have made the 604 e the highest performance PowerPCTM microprocessor on the desktop. ...|$|E
40|$|This paper proposes an {{approach}} to improve both the target speaker’s individuality {{and the quality of}} the converted speech by preparing the training data. In mixture Gaussian spectral mapping (MGM) based voice conversion, spectral features representations are analyzed to obtain the right feature associations between the source and target characteristics. A voiced and unvoiced (V/UV) decision scheme for time-alignment is provided to obtain the right data for training mixture Gaussian spectral mapping function while removing the <b>misaligned</b> <b>data.</b> Experiments are conducted in terms of the applications of spectral representation methods and V/UV decisions strategies to the MGM functions. When linear predictive cepstral coefficients (LPCC) are used for time-alignment and the V/UV decisions are adopted for removing bad data, results show that the conversion function can get a better accuracy and the proposed method can effectively improve the overall performance of voice conversion. 1...|$|E
40|$|Abstract—We {{present a}} novel factor {{analysis}} method {{that can be}} applied to the discovery of common factors shared among trajecto-ries in multivariate time series data. These factors satisfy a prece-dence-ordering property: certain factors are recruited only after some other factors are activated. Precedence-ordering arise in ap-plications where variables are activated in a specific order, which is unknown. The proposed method is based on a linear model that accounts for each factor’s inherent delays and relative order. We present an algorithm to fit the model in an unsupervised manner using techniques from convex and nonconvex optimization that en-force sparsity of the factor scores and consistent precedence-order of the factor loadings. We illustrate the order-preserving factor analysis (OPFA) method for the problem of extracting precedence-ordered factors from a longitudinal (time course) study of gene ex-pression data. Index Terms—Dictionary learning, genomic signal processing, <b>misaligned</b> <b>data</b> processing, structured factor analysis. I...|$|E
40|$|This is the publisher’s final pdf. The {{published}} {{article is}} copyrighted by SIAM (Society for Industrial and Applied Mathematics) {{and can be}} found at: [URL] explore two characteristic features of x-ray computed tomography inversion formulas in two and three dimensions that are dependent on π-lines. In such formulas the data from a given source position contribute only to the reconstruction of ƒ(x) for x in a certain region, called the region of backprojection. The second characteristic is a certain small artifact in the reconstruction called a comet tail artifact. We propose that the comet tail artifact is closely related to the boundary of the region of backprojection and make this relationship precise, developing a general theory of the region of backprojection, its boundary, and the location of the artifact in helical and fan-beam tomography. This theory is applied to a number of specific examples and confirmed by numerical experiments. Furthermore it is demonstrated that a strong comet tail artifact appears in numerical reconstructions from <b>misaligned</b> fan-beam <b>data.</b> A numerical method for using the artifact to find the correct alignment is suggested. Key words. computed tomography, π-lines, helica...|$|R
40|$|Bayesian dynamic process {{convolution}} models {{provide an}} appealing approach for modeling both univariate and multivariate spatial temporal data. Their structure can be exploited to {{significantly reduce the}} dimensionality of a complex spatial temporal process. This results in efficient Markov chain Monte Carlo (MCMC) algorithms required for full Bayesian inference. In addition, the dynamic process convolution framework readily handles both missing <b>data</b> and <b>misaligned</b> multivariate space-time <b>data</b> {{without the need for}} imputation. We review the dynamic process convolution framework and discuss these and other computational advantages of the approach. We present an application involving the modeling of air pollutants to demonstrate how this approach can be used to effectively model a space-time process and provide predictions along with corresponding uncertainty statements...|$|R
40|$|Interdisciplinarity {{is a mixed}} blessing. An {{attempted}} union {{between two}} disciplines can cook up a couscous of conflicting assumptions and theories, <b>misaligned</b> methodological and <b>data</b> analytic strategies, and disconcertingly divergent validation philosophies. The chimerical offspring of this union will be amorphous, uncompelling, and unusable. Yet sometimes such a union can {{turn out to be}} a marriage made in heaven. The union of social psychology and motivation is a case in point, with the self as master of ceremonies. The self acts as a methodological point of contact between contemporary social psychological and traditional motivational approaches. The former approach is nomothetic, experimental, and laboratory-based, whereas the latter is idiographic, naturalistic, and questionnaire-based. More importantly, the self acts as a theoretical point of contact between the two approaches. The role °f the self as the facilitator of this doubly harmonious union is illustrated though a brief exposition to the self-evaluation literature. SELF-EVALUATIO...|$|R
40|$|We {{propose a}} class of <b>misaligned</b> <b>data</b> models for {{addressing}} typical small area estimation (SAE) problems. In particular, we extend hierarchical Bayesian atom-based models for spatial misalignment to the SAE context enabling use of auxiliary covariates, which are available on areal partitions non nested with the small areas of interest, along with planned domains survey estimates also misaligned with these small areas. We model the latent characteristic of interest at atom level as a Poisson variate with mean arising {{as a product of}} population size and incidence. Spatial random effects are introduced either using a CAR model or a process specification. For the latter, incidence is a function of a Gaussian process model for the spatial point pattern over the entire region. Atom counts are driven by integrating the point process over atoms. In the proposed class of models benchmarking to large area estimates is automatically satisfied. A simulation study examines the capability of the proposed models to improve on traditional SAE model estimates...|$|E
40|$|This paper investigates, {{from sources}} to biomarkers, the {{pathways}} of human exposure to arsenic. We use a multi-scale (individual level, county level) hierarchical Bayesian model (HBM) that has explicit stages for pollutant sources, global and local environmental levels, personal exposures, and biomarkers. By analyzing these stages simultaneously, we provide {{an analysis of}} exposure pathways from the sources of toxic substances in the environment to biomarker levels observed in individuals. The complexity of our approach, in terms of levels of hierarchy, variety of (<b>misaligned)</b> <b>data</b> sources, and computational requirements, illustrates what is possible using hierarchical Bayesian modeling. Our HBM draws on individual-specific measurements from the National Human Exposure Assessment Survey (NHEXAS) Phase I, supplemented by arsenic-concentration measurements in topsoil and stream sediments. We focus on arsenic and its air, soil, water, and food pathways of exposure for individuals in the US Environmental Protection Agency 2 ̆ 7 s Region 5 (Illinois, Indiana, Michigan, Minnesota, Ohio, and Wisconsin). © 2007 Elsevier B. V. All rights reserved...|$|E
40|$|We {{address the}} problem of object {{modelling}} from 3 D and 4 D sparse data acquired as different sequences which are misaligned with respect to each other. Such data may result from various imaging modalities and can there-fore present very diverse spatial configurations and appearances. We focus on medical tomographic data, made up of sets of 2 D slices having arbitrary positions and orientations, and which may have different gains and contrasts even within the same dataset. The analysis of such tomographic data is essential for establishing a diagnosis or planning surgery. Modelling from sparse and <b>misaligned</b> <b>data</b> requires solving the three in-herently related problems of registration, segmentation, and interpolation. We propose a new method to integrate these stages in a level set framework. Registration is particularly challenging by the limited number of intersections present in a sparse dataset, and interpolation has to handle images that may have very different appearances. Hence, registration and interpolation ex...|$|E
40|$|We {{present the}} results of a transcontinental {{campaign}} to observe the 2009 June 5 transit of the exoplanet HD 80606 b. We report the first detection of the transit ingress, revealing the transit duration to be 11. 64 +/- 0. 25 hr and allowing more robust determinations of the system parameters. Keck spectra obtained at midtransit exhibit an anomalous blueshift, giving definitive evidence that the stellar spin axis and planetary orbital axis are <b>misaligned.</b> The Keck <b>data</b> show that the projected spin-orbit angle is between 32 - 87 deg with 68. 3 % confidence and between 14 - 142 deg with 99. 73 % confidence. Thus the orbit of this planet is not only highly eccentric (e= 0. 93), but is also tilted away from the equatorial plane of its parent star. A large tilt had been predicted, {{based on the idea that}} the planet's eccentric orbit was caused by the Kozai mechanism. Independently of the theory, it is noteworthy that all 3 exoplanetary systems with known spin-orbit misalignments have massive planets on eccentric orbits, suggesting that those systems migrate differently than lower-mass planets on circular orbits. Comment: ApJ, in press [13 pg...|$|R
40|$|In {{this paper}} we specify, within a {{hierarchical}} Bayesian setting, appropriate atom-based models {{to solve the}} following small area estimation (SAE) questions: (i) combining auxiliary covariates which are available on non nested areal partitions (misaligned areal regression problem); (ii) providing small area estimates by using planned domains <b>data</b> (<b>misaligned</b> areal interpolation problem). To illustrate our approach we consider the problem of estimating the number of unemployed at Local Labour Market area (small area or target zone) level by using two misaligned source data: auxiliary information available on different administrative partitions; reliable estimates of unemployed on Labour Force Survey planned domains. Thus we explore the close connection that typical SAE issues show to have with spatial misalignment problems. Ob ject of SAE is, in fact, inference on survey non-planned “minor domains” (the so called small areas) : based on direct domain data (when available), it leads to estimates of poor quality. Thereby models are set up for borrowing strength from indirectly related data sources. Similarly, spatial misalignment models are set up whenever “target zones” for which data are needed are different from source zones on which data are available...|$|R
40|$|We {{propose a}} Bayesian dynamic factor process {{convolution}} model for multivariate spatial temporal pro-cesses and illustrate {{the utility of}} this approach in modeling large air quality monitoring data. Key advan-tages of this modeling framework are a descriptive parametrization of the cross-covariance structure of the space-time processes and dimension reduction features that allow full Bayesian inference procedures to re-main computationally tractable for large data sets. These features result from modeling space-time data as realizations of linear combinations of underlying space-time elds. The underlying latent components are constructed by convolving temporally-evolving processes dened on a grid covering the spatial domain and include both trend and cyclical components. We argue that mixtures of such components can realistically describe a variety of space-time environmental processes and are especially applicable to air pollution pro-cesses that have complex space-time dependencies. In addition to computational benets that arise from the dimension reduction features of the model, the process convolution structure permits <b>misaligned</b> and missing <b>data</b> {{without the need for}} imputation when tting the model. This advantage is especially useful when constructing models for data collected at monitoring stations that have misaligned sampling schedules and that are frequently out of service for long stretches of time. We illustrate the modeling approach using a multivariate pollution dataset taken from the EPA's CASTNet database...|$|R
40|$|Source {{synchronous}} links {{for use in}} multi-synchronous networks-on-chip (NoCs) {{are becoming}} the most vulnerable points for correct network operation and must be safeguarded against intra-link delay variations and signal misalignments. The intricacy of matching link net attributes during placement and routing and the growing role of process parameter variations in nanoscale silicon technologies are the root causes for this. This article addresses the challenge of designing a process variation and layout mismatch tolerant link for synchronizer-based GALS NoCs by implementing a self-calibration mechanism. A variation detector senses the variability-induced misalignment between data lines with themselves and with the transmitter clock routed with data in source synchronous links. A suitable delayed replica of the transmitter clock is then selected for safe sampling of <b>misaligned</b> <b>data.</b> The manuscript proves robustness of the link in isolation {{with respect to a}} detector-less link, but also assesses integration issues with the downstream synchronizer and switch architecture, proving the benefits in a realistic experimental setting for cost-effective NoCs...|$|E
40|$|In the {{ultrasonic}} community {{there is}} a growing use of models to simulate inspection processes [1, 2]. One necessary input to such models is knowledge of the transducer’s radiation pattern. The radiation pattern of a commercial transducer often approximates that of an ideal, focused, piston transducer with appropriately chosen parameters (element dimensions and focal lengths), and these“ideal probe” parameters often serve as model inputs. In this paper we demonstrate beam mapping methods for determining these parameters. For probes with circularly symmetric beam cross-sections, an axial scan of the beam suffices. For more general probes, C-scan data are acquired to map out the beam cross section at several different waterpaths. In each case, the transducer parameters are determined by adjusting their values to minimize the discrepancy between the measured and model amplitudes. A technique for handling <b>misaligned</b> <b>data</b> is also described. Measured and fitted fields are compared for a variety of transducers (spherically, cylindrically, and bi-cylindrically focused) including one with a presumably damaged element. In addition, the axial scan and C-scan methods are compared for one circular, spherically-focused transducer...|$|E
40|$|The {{long-term}} {{objective of}} this research project is to characterize multi-pollutant (ar-senic, lead, cadmium, and chromium) human exposures by linking sources to biomarkers using a multi-scale (individual level, county level) hierarchical Bayesian model (HBM) that describes how multi-media pathways contribute to direct routes of exposure. Our approach {{is to use a}} statistical model that has explicit stages for pollutant sources, global and local environmental levels, personal exposures, and biomarkers. By analyzing these stages simultaneously, we provide an analysis of exposure pathways from the sources of toxic substances in the environment to biomarker levels observed in individuals. The complexity of our approach, in terms of levels of hierarchy, variety of (<b>misaligned)</b> <b>data</b> sources, and computational requirements, illustrates what is now possible using hierar-chical Bayesian models. Our HBM draws on individual-specic measurements from the National Human Exposure Assessment Survey (NHEXAS) Phase I, supplemented by ar-senic concentration measurements in topsoil and stream sediments. We focus on arsenic and its air, soil, water, and food pathways of exposure for individuals in the U. S. Environ...|$|E
40|$|When a {{computer}} code {{is used to}} simulate a complex system, a fundamental task is to assess {{the uncertainty of the}} simulator. In the case of computationally expensive simulators, this is often accomplished via a surrogate statistical model, a statistical output emulator. An effective emulator is one that provides good approximations to the computer code output for wide ranges of input values. In addition, an emulator should be able to handle large dimensional simulation output for a relevant number of inputs; it should flexibly capture heterogeneities in the variability of the response surface; it should be fast to evaluate for arbitrary combinations of input parameters; and it should provide an accurate quantification of the emulation uncertainty. In this work, we develop Bayesian adaptive spline methods for emulation of computer models that output functions. We introduce modifications to traditional Bayesian adaptive spline approaches that allow for fitting large amounts of data and allow for more efficient Markov chain Monte Carlo sampling. We develop a functional approach to sensitivity analysis that can be performed using this emulator. We present a sensitivity analysis of {{a computer}} model of the deformation of a protective plate used in pressure driven experiments. This example serves as an illustration of the ability of Bayesian adaptive spline emulators to fulfill all the necessities of computability, flexibility and reliable calculation on relevant measures of sensitivity. We extend the methods to emulation of an atmospheric dispersion simulator that outputs a plume in space and time based on inputs detailing the characteristics of the release, some of which are categorical. We achieve accurate emulation using Bayesian adaptive splines to model weights on empirical orthogonal functions. We extend the adaptive spline methodology to allow for categorical inputs. We use this emulator as well as appropriately identifiable simulator discrepancy and observational error models to calibrate the simulator using a dataset from an experimental release of particles from the Diablo Canyon Nuclear Power Plant in Central California. Since the release was controlled, these characteristics are known, allowing us to compare our findings to the truth. We further extend the methods to emulate a computer model that outputs <b>misaligned</b> functional <b>data.</b> We do this by modeling the aligned, or warped, data as well as the warping functions, using separate Bayesian adaptive spline models. We explore inference methods that treat these models jointly and separately, and establish methods to ensure that the warping functions are non-decreasing. These methods are applied to a high-energy-density physics model that outputs a curve representing energy as a function of time...|$|R
40|$|Aim: Combined whole-body (WB) PET/CT imaging {{provides}} better overall co-registration {{compared to}} separate CT and PET However, in clinical routine local PET-CT mis-registration cannot be avoided. Thus, the reconstructed PET tracer distribution may be biased {{when using the}} <b>misaligned</b> CT transmission <b>data</b> for CT-based attenuation correction (CT-AC). We investigate the feasibility of retrospective co-registration techniques to align CT and PET images prior to CTA C, thus improving potentially the quality of combined PET/CT imaging in clinical routine. Methods: First, using a commercial software registration package CT images were aligned to the uncorrected PET data by rigid and non-rigid registration methods. Co-registration accuracy of both alignment approaches was assessed by reviewing the PET tracer uptake patterns (visual, linked cursor display) following attenuation correction based on the original and co-registered CT Second, we investigated non-rigid registration based on a prototype ITK implementation of the B-spline algorithm on a similar targeted MR-CT registration task, there showing promising results. Results: Manual rigid, landmark-based co-registration introduced unacceptable misalignment, in particular in peripheral areas of the whole-body images. Manual, non-rigid landmark-based co-registration prior to CTA C was successful with minor loco-regional distortions. Nevertheless, neither rigid nor non-rigid automatic co-registration based on the Mutual Information image to image metric succeeded in co-registering the CT and no A C-PET images. In contrast to widely available commercial software registration our implementation of an alternative automated, non-rigid B-spline co-registration technique yielded promising results in this setting with MR-CT data. Conclusion: In clinical PET/CT imaging, retrospective registration of CT and uncorrected PET images may {{improve the quality of}} the A C-PET images. As of today no validated and clinically viable commercial registration software is in routine use. This has triggered our efforts in pursuing new approaches to a validated, non-rigid co-registration algorithm applicable to whole-body PET/CT imaging of which first results are presented here. This approach appears suitable for applications in retrospective WB-PET/CT alignment...|$|R
40|$|Malaria {{pandemic}} (MP) {{has been}} linked to a range of serious health problems including premature mortality. The main objective of this research is to quantify uncertainties about impacts of malaria on mortality. A multivariate spatial regression model was developed for estimation of the risk of mortality associated with malaria across Ogun State in Nigeria, West Africa. We characterize different local governments in the data and model the spatial structure of the mortality data in infants and pregnant women. A flexible Bayesian hierarchical model was considered for a space-time series of counts (mortality) by constructing a likelihood-based version of a generalized Poisson regression model that combines methods for point-level <b>misaligned</b> <b>data</b> and change of support regression. A simple two-stage procedure for producing maps of predicted risk is described. Logistic regression modeling was used to determine an approximate risk on a larger scale, and geo-statistical ("Kriging") approaches were used to improve prediction at a local level. The results suggest improvement of risk prediction brought about in the second stage. The advantages and shortcomings of this approach highlight the need for further development of a better analytical methodology...|$|E
