10000|10000|Public
25|$|Mean squared {{error is}} used for obtaining {{efficient}} estimators, a widely used class of estimators. Root <b>mean</b> <b>square</b> <b>error</b> is simply the square root of mean squared error.|$|E
2500|$|The great body of {{available}} statistics {{show us that}} the deviations of a human measurement from its mean follow very closely the Normal Law of Errors, and, therefore, that the variability may be uniformly measured by the standard deviation corresponding to the square root of the <b>mean</b> <b>square</b> <b>error.</b> When there are two independent causes of variability capable of producing in an otherwise uniform population distributions with standard deviations [...] and , {{it is found that}} the distribution, when both causes act together, has a standard deviation [...] [...] It is therefore desirable in analysing the causes of variability to deal with the square of the standard deviation as the measure of variability. [...] We shall term this quantity the Variance...|$|E
50|$|<b>Mean</b> <b>square</b> <b>error</b> or {{mean squared error}} (abbreviated MSE) {{and root}} <b>mean</b> <b>square</b> <b>error</b> (RMSE) refer to the amount by which the values {{predicted}} by an estimator differ from the quantities being estimated (typically outside the sample from which the model was estimated).|$|E
40|$|We {{obtain a}} {{conditional}} prediction <b>mean</b> <b>squared</b> <b>error</b> {{for a state}} space model with estimated parameters. An important application of our results is the derivation of conditional forecast and interpolation <b>mean</b> <b>squared</b> <b>errors</b> for autoregressive-moving average models with estimated parameters. We also obtain the conditional <b>mean</b> <b>squared</b> <b>error</b> for filtered and smoothed estimates of the state vector...|$|R
40|$|Summary Total planar {{area can}} be {{estimated}} based on sampling by a lattice of figures (e. g. point patterns, line segments, quadrats). General formulae are provided for the approximation of <b>mean</b> <b>squared</b> <b>errors.</b> The approximation formulae are products of the boundary length and of a parameter that depends only on the sampling scheme. An R package {{is provided by the}} authors for the numerical computation of the <b>mean</b> <b>squared</b> <b>error</b> formulae. The speed of convergence of the <b>mean</b> <b>squared</b> <b>error</b> approximation is assessed on the basis of several simulations. Several sampling schemes are compared in view of the approxi-mated <b>mean</b> <b>squared</b> <b>errors...</b>|$|R
50|$|Forecast errors can be {{evaluated}} {{using a variety}} of methods namely mean percentage <b>error,</b> root <b>mean</b> <b>squared</b> <b>error,</b> <b>mean</b> absolute percentage <b>error,</b> <b>mean</b> <b>squared</b> <b>error.</b> Other methods include tracking signal and forecast bias.|$|R
50|$|This {{choice of}} filter {{parameters}} minimizes the <b>mean</b> <b>square</b> <b>error.</b>|$|E
5000|$|... so {{that the}} linear minimum <b>mean</b> <b>square</b> <b>error</b> {{estimator}} is given by ...|$|E
5000|$|... #Subtitle level 3: The Karhunen-Loève {{expansion}} {{minimizes the}} total <b>mean</b> <b>square</b> <b>error</b> ...|$|E
40|$|Small area is an {{area with}} {{insufficient}} sample for direct estimation. Limited survey objects, cause direct estimation can not produce better parameter estimates. Based on this, an indirect estimation method called empirical Bayes is used to obtain a better estimate. This study will compare <b>means</b> <b>squared</b> <b>error</b> by  direct estimation method and empirical Bayes method {{to find a better}} method on a small area. Jackknife is used to get the <b>means</b> <b>squared</b> <b>error</b> in the empirical Bayes. The results is, empirical Bayes methods give a better parameters based on <b>mean</b> <b>squared</b> <b>errors.</b> Empirical Bayes can produce a smaller <b>mean</b> <b>squared</b> <b>error</b> more than direct estimation in small area. </p...|$|R
40|$|Several authors {{consider}} the optimization of linear combinations of independent estimators {{with respect to}} <b>mean</b> <b>squared</b> <b>error.</b> The minimization of variance for convex combinations of estimators having a known correlation coe±cient is also considered in the literature. We unify and generalize the results pertaining to these two problems by minimizing <b>mean</b> <b>squared</b> <b>error</b> for linear combinations of dependent estimators. We examine {{the role of the}} correlation coe±cient in establishing the optimal weights for these combinations and uncover a relationship between these optimal weights and those provided in the literature for minimizing the <b>mean</b> <b>squared</b> <b>error</b> of a single estimator. Key Words: Weighted estimator, coe±cient of variation, <b>mean</b> <b>squared</b> <b>error.</b> ...|$|R
40|$|The minimum {{discrimination}} information (MDI) {{procedure for}} lowering the <b>mean</b> <b>squared</b> <b>error</b> (MSE) {{of the minimum}} variance unbiased estimator (MVUE) of the normal mean is considered. The procedure is employed to shrink the MVUE toward a preliminary conjectured interval under the information measure of Kullback and Leibler (1951). MDI estimator and its <b>mean</b> <b>squared</b> <b>error</b> are derived. The suggested estimator compares favorably with the previously proposed estimators in terms of <b>mean</b> <b>squared</b> <b>error</b> efficiency. 1...|$|R
5000|$|... where [...] is an {{estimate}} of [...] that minimizes the <b>mean</b> <b>square</b> <b>error.</b>|$|E
50|$|A Bayesian analog is a Bayes estimator, {{particularly}} with minimum <b>mean</b> <b>square</b> <b>error</b> (MMSE).|$|E
50|$|An {{estimator}} that minimises {{the bias}} {{will not necessarily}} minimise the <b>mean</b> <b>square</b> <b>error.</b>|$|E
50|$|The use of <b>mean</b> <b>squared</b> <b>error</b> without {{question}} has been criticized by the decision theorist James Berger. <b>Mean</b> <b>squared</b> <b>error</b> is the negative of the expected value of one specific utility function, the quadratic utility function, {{which may not be}} the appropriate utility function to use under a given set of circumstances. There are, however, some scenarios where <b>mean</b> <b>squared</b> <b>error</b> can serve as a good approximation to a loss function occurring naturally in an application.|$|R
40|$|We {{propose a}} kernel-based {{multi-stage}} conditional median predictor for [alpha]-mixing time series of Markovian structure. <b>Mean</b> <b>squared</b> <b>error</b> properties of single-stage and multi-stage conditional medians are derived and discussed. [alpha]-mixing Conditional median Kernel Markovian <b>Mean</b> <b>squared</b> <b>error</b> Multi-stage predictor Single-stage predictor Time series...|$|R
50|$|There {{are several}} basic fitness {{functions}} for evaluating model performance, {{with the most}} common being based on the error or residual between the model output and the actual value. Such functions include the <b>mean</b> <b>squared</b> <b>error,</b> root <b>mean</b> <b>squared</b> <b>error,</b> <b>mean</b> absolute <b>error,</b> relative <b>squared</b> <b>error,</b> root relative <b>squared</b> <b>error,</b> relative absolute error, and others.|$|R
5000|$|Lastly, {{the error}} {{covariance}} and minimum <b>mean</b> <b>square</b> <b>error</b> achievable by such estimator is ...|$|E
5000|$|... thus {{providing}} an expression for the coefficients [...] {{of the minimum}} <b>mean</b> <b>square</b> <b>error</b> estimator.|$|E
50|$|Efficiencies {{are often}} defined using the {{variance}} or <b>mean</b> <b>square</b> <b>error</b> as {{the measure of}} desirability.|$|E
50|$|Two {{naturally}} desirable {{properties of}} estimators are {{for them to}} be unbiased and have minimal <b>mean</b> <b>squared</b> <b>error</b> (MSE). These cannot in general both be satisfied simultaneously: a biased estimator may have lower <b>mean</b> <b>squared</b> <b>error</b> (MSE) than any unbiased estimator; see estimator bias.|$|R
40|$|When climate {{forecasts}} {{are highly}} uncertain, the optimal <b>mean</b> <b>squared</b> <b>error</b> {{strategy is to}} ignore them. When climate forecasts are highly certain, the optimal <b>mean</b> <b>squared</b> <b>error</b> strategy is to use them as is. In between these two extremes there are climate forecasts with an intermediate level of uncertainty for which the optimal <b>mean</b> <b>squared</b> <b>error</b> strategy {{is to make a}} compromise forecast. We present two new methods for making such compromise forecasts, and show, using simulations, that they improve on previously published methods...|$|R
40|$|International audienceThis paper {{presents}} a density estimator {{based upon a}} histogram computed on a fuzzy partition. We prove the consistency of this estimator in the <b>Mean</b> <b>Squared</b> <b>Error</b> (MSE). We give the optimal bin width of the estimator which minimizes the Asymptotic Integrated <b>Mean</b> <b>Squared</b> <b>Error</b> (AIMSE) ...|$|R
50|$|The {{following}} {{is one way}} to find the minimum <b>mean</b> <b>square</b> <b>error</b> estimator by using the orthogonality principle.|$|E
5000|$|One measure {{which is}} used to try to reflect both types of {{difference}} is the <b>mean</b> <b>square</b> <b>error,</b> ...|$|E
5000|$|... gave another proof, {{and also}} showed that most {{probabilistic}} models of primes incorrectly predict the <b>mean</b> <b>square</b> <b>error</b> ...|$|E
40|$|This article {{contains}} the derivations of expressions for the <b>mean</b> <b>squared</b> <b>errors</b> for two estimators of the intensity {{function of the}} power law process, a nonhomogeneous Poisson process with intensity function ([beta]/[theta]) (t/[theta]) [beta]- 1. <b>mean</b> <b>squared</b> <b>error</b> efficiency Weibull process power law process nonhomogeneous Poisson process...|$|R
5000|$|Minimum <b>mean</b> <b>squared</b> <b>error</b> (MMSE), {{also known}} as Bayes least <b>squared</b> <b>error</b> (BLSE) ...|$|R
40|$|Abstract. To let {{introduce}} {{concept of}} error into statistics a central value (unknown true value) of error distribution is required then <b>mean</b> <b>squared</b> <b>error</b> {{can not be}} based (present status) on (true) random variable. In this paper <b>mean</b> <b>squared</b> <b>error</b> of <b>mean</b> estimation is introduced and <b>mean</b> <b>squared</b> <b>error</b> of estimation is dismissed. 1. Origin Let us consider (e. g. for j = n + 1) variance of the difference Rj of two random variables: Vj and ˆ Vj, where E{Vj} = E { ˆ Vj} = m, {{in the terms of}} covarianc...|$|R
50|$|Important {{potential}} {{properties of}} statistics include completeness, consistency, sufficiency, unbiasedness, minimum <b>mean</b> <b>square</b> <b>error,</b> low variance, robustness, and computational convenience.|$|E
5000|$|... {{which has}} an {{expected}} value equal to the true mean μ (so it is unbiased) and a <b>mean</b> <b>square</b> <b>error</b> of ...|$|E
5000|$|The <b>mean</b> <b>square</b> <b>error</b> [...] is {{minimized}} by {{updating the}} filter weights {{in a manner}} to converge to the optimum filter weight.|$|E
40|$|In this paper, {{we examine}} the small sample {{properties}} of the pre-test iterative variance estimator in regression. The explicit formula of MSE is derived, and it is shown that the pre-test iterative variance estimator with an appropriate critical value dominates the iterative variance estimator without pre-testing in terms of MSE. We also compare the MSE performances of the pre-test iterative variance estimators using the Stein-rule, minimum <b>mean</b> <b>squared</b> <b>error,</b> and adjusted minimum <b>mean</b> <b>squared</b> <b>error</b> estimators by numerical evaluations. Iterative variance estimator <b>Mean</b> <b>squared</b> <b>error</b> Pre-test Regression error variance...|$|R
40|$|In this article, we {{consider}} the risk performance of an iterative feasible minimum <b>mean</b> <b>squared</b> <b>error</b> estimator of the regression disturbance variance under the LINEX loss function. This loss is a generalisation of the quadratic loss function allowing for asymmetry. Notwithstanding the justification for using the feasible minimum <b>mean</b> <b>squared</b> <b>error</b> estimator in estimating the regression coefficients, {{it is found that}} the corresponding estimator of the disturbance variance does not, in general, improve over a class of conventional estimators commonly used in practice. Error variance LINEX loss Minimum <b>mean</b> <b>squared</b> <b>error</b> Risk...|$|R
40|$|We {{consider}} a superposition {{of an unknown}} number of independent homogeneous Poisson processes in which the source of each event can be identified. After observing the system for a fixed time t, the total rate, U(t), of the unobserved processes is to be estimated. We prove that a uniformly minimum <b>mean</b> <b>squared</b> <b>error</b> estimate of U(t) does not exist and all unbiased estimators of U(t) are negatively correlated with U(t) and derive the minimum <b>mean</b> <b>squared</b> <b>error</b> estimator among all unbiased estimators. <b>Mean</b> <b>squared</b> <b>error</b> Poisson process software reliability unbiasedness...|$|R
