19|951|Public
40|$|We {{present a}} {{framework}} that connects three interesting classes of groups: the twisted groups (also known as Suzuki-Ree groups), the mixed groups and the exotic pseudo-reductive groups. For a given characteristic p, we construct categories of twisted and <b>mixed</b> <b>schemes.</b> Ordinary schemes are a full subcategory of the <b>mixed</b> <b>schemes.</b> <b>Mixed</b> <b>schemes</b> arise from a twisted scheme by base change, although not every mixed scheme arises this way. The group objects in these categories are called twisted and mixed group schemes. Our main theorems state: (1) The twisted Chevalley groups ^ 2 B_ 2, ^ 2 G_ 2 and ^ 2 F_ 4 arise as rational points of twisted group schemes. (2) The mixed groups {{in the sense of}} Tits arise as rational points of mixed group schemes over mixed fields. (3) The exotic pseudo-reductive groups of Conrad, Gabber and Prasad are Weil restrictions of mixed group schemes. Comment: 68 pages, comments and suggestions are warmly welcome...|$|E
40|$|In this paper, amiable <b>mixed</b> <b>schemes</b> are {{presented}} for two variants of fourth order curl equations. Specifically, mixed formulations {{for the problems}} are constructed, which are well-posed in Babuska-Brezzi's sense and admit stable discretizations by finite element spaces of low smoothness and of low degree. The regularities of the mixed formulations and thus equivalently the primal problems are established, and some finite elements examples are given which can exploit the regularity of the solutions to an optimal extent...|$|E
40|$|Alexa [1] and Ivrissimtzis et al [13] have {{proposed}} a classification mechanism for bivariate subdivision schemes. Alexa considers triangular primal schemes, Ivrissimtzis et al generalise this both to quadrilateral and hexagonal meshes and to dual and <b>mixed</b> <b>schemes.</b> I summarise this classification and then proceed to analyse {{it in order to}} determine which classes of subdivision scheme are likely to contain useful members. My aim is to ascertain whether there are any potentially useful classes which have not yet been investigated or whether we can say, with reasonable confidence, that all of the useful classes have already been considered...|$|E
40|$|Blends {{of natural}} rubber (NR) and low {{molecular}} weight natural rubber (LMWNR) were compounded using three different <b>mixing</b> <b>schemes</b> by adopting the semi-efficient sulphur vulcanization compounding formulation. In scheme 1, the natural rubber and LMWNR were first mixed before adding the compounding ingredients. In scheme 2, the compounding ingredients were first mixed with the NR before adding the LMWNR and in scheme 3 the compounding ingredients were first mixed with the LMWNR before adding the NR. Properties of the vulcanizates from the three <b>mixing</b> <b>schemes</b> viz-a-viz: chemical resistance, ageing and physico-mechanical properties were investigated and compared. The physico-mechanical results of all the vulcanizates {{were found to be}} within the accepted level for NR compounds. The ageing results from all the <b>mixing</b> <b>schemes</b> were found impressing. Vulcanizates from <b>mixing</b> <b>scheme</b> 1 restricted penetration of petroleum fuels and organic solvents best, followed by scheme 2 and the least was scheme 3. The activation energy and free energy change were found to be highest with scheme 1, thereby making <b>mixing</b> <b>scheme</b> 1 being more technologically advantageous than other <b>mixing</b> <b>schemes...</b>|$|R
40|$|QCD sum-rules {{are used}} to {{calculate}} the ρ̂(1 ^-+) →πη, πη' decay widths of the exotic hybrid in two different η-η' <b>mixing</b> <b>schemes.</b> In the conventional flavour octet-singlet <b>mixing</b> <b>scheme,</b> the decay widths are both found to be small, while in the recently-proposed quark <b>mixing</b> <b>scheme,</b> the decay width Γ_ρ̂→ηπ≈ 250 MeV is large compared with the decay width Γ_ρ̂→η^'π≈ 20 MeV. These results provide some insight into η-η' mixing and hybrid decay features. Comment: latex 2 e, 11 pages with 4 embedded eps figures. v 2 corrects reference [5] and minor error in equation (11...|$|R
50|$|Originally {{proposed}} {{as a candidate}} lepton mixing matrix, and actively studied as such (and even as a candidate quark mixing matrix), trimaximal mixing is now definitively ruled-out as a phenomenologically viable lepton <b>mixing</b> <b>scheme</b> by neutrino oscillation experiments, especially the CHOOZ reactor experiment, in favour of the no longer tenable (related) tribimaximal <b>mixing</b> <b>scheme.</b>|$|R
40|$|This paper aims at {{analysing}} {{the impact}} of prospective payment schemes on cost efficiency of acute care hospitals in Switzerland. We study a panel of 121 public hospitals subject to one of four payment schemes. While several hospitals are still reimbursed on a per diem basis {{for the treatment of}} patients, most face flat per-case rates—or <b>mixed</b> <b>schemes,</b> which combine both elements of reimbursement. Thus, unlike previous studies, we are able to simultaneously analyse and isolate the cost-efficiency effects of different payment schemes. By means of stochastic frontier analysis, we first estimate a hospital cost frontier. Using the two-stage approach proposed by Battese and Coelli (Empir Econ 20 : 325 – 332, 1995), we then analyse {{the impact of}} these payment schemes on the cost efficiency of hospitals. Controlling for hospital characteristics, local market conditions in the 26 Swiss states (cantons), and a time trend, we show that, compared to per diem, hospitals which are reimbursed by flat payment schemes perform better in terms of cost efficiency. Our results suggest that <b>mixed</b> <b>schemes</b> create incentives for cost containment as well, although to a lesser extent. In addition, our findings indicate that cost-efficient hospitals are primarily located in cantons with competitive markets, as measured by the Herfindahl–Hirschman index in inpatient care. Furthermore, our econometric model shows that we obtain biased estimates from frontier analysis if we do not account for heteroscedasticity in the inefficiency term...|$|E
40|$|Planning {{and housing}} {{policies}} in the UK have recently adopted the principle of developing <b>mixed</b> <b>schemes,</b> whereby a mixing of tenures stands in for a mixing of income groups. A series of recent research studies has informed the future production of mixed income new communities. This article draws on these wider studies to consider the issue of social mixing and design within developments in detail. In particular the concept of "tenure blind" development is critically investigated with regard to three mature case studies, combining design analysis with social research. Conclusions are drawn {{about the dangers of}} over-specific prescriptions towards design and the continuing relevance of urban design theory...|$|E
40|$|This thesis {{focuses on}} the {{performance}} of terrestrial communication systems that use channel assignment schemes to allocate base stations in a scenario that implements the coexistence of mixed terrestrial communication systems based on cognitive radio technology. Interaction and coexistence of different channel assignment schemes is investigated. Reinforcement learning is applied into multicast downlink transmission with power adjustment to develop the intelligence of cognitive radio. We focus on investigating channel assignment schemes that select channels based on optimizing the coverage area supported by a terrestrial network. Four channel assignment schemes are developed and compared individually followed by an interaction of <b>mixed</b> <b>schemes.</b> It was found that for <b>mixed</b> <b>schemes,</b> different combinations will affect performance, either delivering better coexistence or more interference. It is shown in this thesis that the dynamic channel assignment used in different situations can efficiently improve the performance of spectrum management. We investigate how channel assignment in multicast terrestrial communication systems with distributed channel occupancy detection can be improved using intelligence based on reinforcement learning and transmitter power adjustment. A weighting factor is used to determine the highest priority channels and help in controlling the performance of the system. It is shown how such schemes significantly reduce the number of reassignments and improve the dropping probability at the expense of increased blocking. It is found that using different minimum quality of service threshold percentages can partly control and improve performance in place of the more traditional SINR (Signal to Interference plus Noise Ratio) threshold levels. We also show how a power adjustment technique is developed, that significantly reduces the level of overlap between adjacent base stations and further reduces interference and transmitter power. ...|$|E
40|$|The vector meson ω-ϕ mixing is {{studied in}} two {{alternative}} scenarios with different numbers of mixing angles, i. e., the one-mixing-angle scenario and the two-mixing-angle scenario, {{in both the}} octect-singlet <b>mixing</b> <b>scheme</b> and the quark flavor <b>mixing</b> <b>scheme.</b> Concerning the reproduction of experimental data and the Q^ 2 behavior of transition form factors, one-mixing-angle scenario in the quark flavor scheme performs better than that in the octet-singlet scheme, while the two-mixing-angle scenario works well for both <b>mixing</b> <b>schemes.</b> The {{difference between the two}} mixing angles in the octet-singlet scheme is bigger than that in the quark flavor scheme. Comment: 16 pages, 7 figures, final version to appear in PR...|$|R
40|$|There {{are many}} <b>mixing</b> <b>schemes</b> based upon flavor symmetries that predict a {{vanishing}} θ_ 13. These <b>mixing</b> <b>schemes</b> need corrections or modifications {{to account for}} recent experimental measurements of non-zero θ_ 13. We propose new parameterizations for the lepton mixing matrix to quantify the minimal modifications needed in these <b>mixing</b> <b>schemes.</b> The parameterizations can be factorized in two parts: U_ 0 (a,b) and R(θ,ϕ). The first factor {{can be viewed as}} a zeroth order mixing matrix coming from some flavor symmetry. It reproduces the popular <b>mixing</b> <b>schemes</b> based upon flavor symmetries for suitable values of a and b. The second factor can be interpreted as a minimal modification to the mixing matrix and is responsible for non zero θ_ 13, non-maximal θ_ 23 and CP violation. We also find the experimentally allowed parameter space for the parameters a and b and compare it with the symmetry based values for these parameters. Comment: 8 pages, 1 figure, version to appear in Physical Review...|$|R
40|$|Flavour models may {{display a}} {{relation}} between the CP-violating asymmetry for leptogenesis and low-energy parameters. If the flavour symmetry produces an exact mass independent lepton <b>mixing</b> <b>scheme</b> at leading order (with type I see-saw) the CP-violating asymmetry would vanish {{in the absence of}} corrections. We present a model displaying the link between deviations from the <b>mixing</b> <b>scheme</b> and leptogenesis. Comment: 4 pages, 5 figures, conference proceedings for Corfu Summer Institute 201...|$|R
40|$|It {{is widely}} {{recognised}} that optimal tax/transfer schemes will generally involve {{elements of both}} 'tagging' (the use of categorical benefits) and 'means-testing' (Income-relation of benefits). This paper explores the optimal design of such <b>mixed</b> <b>schemes.</b> Simulations suggest a striking qualitative dissimilarity between the group-specific schedules optimally imposed on the poorer and richer groups: broadly speaking, the optimal marginal tax rate decreases with income amongst the latter, but increases with income amongst the former. This latter observation, potentially important for policy, runs counter to the conventional wisdom from previous simulations; the reconciliation, we argue, lies in the role played in optimal tax design by the revenue constraint. The simulations also suggest that gains from the appropriate use of categorical information can plausibly be substantial. ...|$|E
40|$|Gradient schemes is a {{framework}} which enables the unified convergence analysis {{of many different}} methods [...] such as finite elements (conforming, non-conforming and mixed) and finite volumes methods [...] for 2 ^ nd order diffusion equations. We show in this work that the gradient schemes framework can be extended to variational inequalities involving mixed Dirichlet, Neumann and Signorini boundary conditions. This extension allows us to provide error estimates for numerical approximations of such models, recovering known convergence rates for some methods, and establishing new convergence rates for schemes not previously studied for variational inequalities. The general framework we develop also enables us to design a new numerical method for the obstacle and Signorini problems, based on hybrid mimetic <b>mixed</b> <b>schemes.</b> We provide numerical results that demonstrate the accuracy of these schemes, and confirm our theoretical rates of convergence...|$|E
40|$|Aim To {{assess the}} {{proportion}} of people with diabetes screened for retinopathy according to provision of screening services. Methods Twenty-five health authorities in England and Wales were sampled after stratification by type of screening provision for diabetic retinopathy. Nine {{did not have a}} population-based screening scheme, six had an optometry scheme, six had a camera scheme and four had schemes with more than one method of screening ('mixed schemes'). Within each authority general practices were randomly sampled, 129 in total, and in each the records of a sample of diabetic patients examined. Results Of the 9200 records examined, 5812 (63. 2 %) had a record of one or more retinal examinations from any source in the year before the survey. This proportion did not differ significantly according to type of screening provision. The proportion of people with one or more retinal examinations by an 'expert' (defined as ophthalmologist, diabetologist, optometrist or screening scheme) in the last year was 44. 7 % where there was no screening scheme and 62. 2 %, 59. 4 %, and 61. 6 %, respectively, where optometry, camera and <b>mixed</b> <b>schemes</b> were present. Adjusted relative odds (95 % confidence interval) for a retinal examination from any source in the last year compared with areas with no screening schemes were 1. 19 (0. 73, 1. 93), 1. 26 (0. 80, 1. 98), and 1. 19 (0. 77, 1. 84) for camera, optometry and <b>mixed</b> <b>schemes,</b> respectively. Equivalent figures for an expert retinal examination were 2. 30 (1. 51, 3. 49), 1. 86 (1. 25, 2. 78) and 2. 13 (1. 32, 3. 45). Coverage by schemes themselves did not differ according to type of scheme. Highest coverage rates, including examinations by screening schemes, were achieved in those treated with insulin, and the lowest rates found in those treated with diet alone. Conclusions Screening schemes have had a small impact on overall retinal examinations, but a higher impact on the coverage of examinations performed by experts...|$|E
40|$|In this paper, we {{calculate}} the branching ratios for B^+→ D_s^+η, B^+→ D_s^+η^', B^+→ D_s^*+η and B^+→ D_s^*+η^' decays by employing the perturbative QCD (pQCD) factorization approach. Under the {{two kinds of}} η-η^' <b>mixing</b> <b>schemes,</b> the quark-flavor <b>mixing</b> <b>scheme</b> and the singlet-octet <b>mixing</b> <b>scheme,</b> {{we find that the}} calculated branching ratios are consistent with the currently available experimental upper limits. We also considered the so called "f_D_s puzzle", by using two groups of parameters about the D^(*) _s meson decay constants, that is f_D_s= 241 MeV, f_D^*_s= 272 MeV and f_D_s= 274 MeV, f_D^*_s= 312 MeV, to {{calculate the}} branching ratios for the considered decays. We find that the results change 30 % by using these two different groups of paramters. Comment: 12 pages, 1 figure. Typos removed, minor correction...|$|R
40|$|In {{this paper}} {{we make a}} {{systematic}} study of the semileptonic decays B/B_s → (η,, G) (l^+l^-,lν̅,νν̅) by employing the perturbative QCD (pQCD) factorization approach. The next-to-leading-order (NLO) contributions to the relevant form factors are included, and the ordinary η- <b>mixing</b> <b>scheme</b> and the η [...] G <b>mixing</b> <b>scheme</b> are considered separately, where G denotes a pseudoscalar glueball. The numerical results and the phenomenological analysis indicate that (a) the NLO contributions to the relevant form factors provide 25...|$|R
50|$|A more {{accurate}} {{depiction of the}} <b>mixing</b> <b>scheme</b> used in AC'97 compatible sound cards {{can be seen in}} Figure 17 of the AC'97 spec.|$|R
40|$|A new {{technique}} of residual-type a posteriori error analysis is {{developed for the}} lowest-order Raviart-Thomas mixed finite element discretizations of convection-diffusion-reaction equations in two- or three-dimension. Both centered mixed scheme and upwind-weighted mixed scheme are considered. The a posteriori error estimators, derived for the stress variable error plus scalar displacement error in $L^{ 2 }$-norm, can be directly computed with the solutions of the <b>mixed</b> <b>schemes</b> without any additional cost, and are robust {{with respect to the}} coefficients in the equations. Local efficiency dependent on local variations in coefficients is obtained without any saturation assumption, and holds from the cases where convection or reaction is not present to convection- or reaction-dominated problems. The main tools of analysis are the postprocessed approximation of scalar displacement, abstract error estimates, and the property of modified Oswald interpolation. Numerical experiments are reported to support our theoretical results and to show the competitive behavior of the proposed posteriori error estimates...|$|E
40|$|A {{general theory}} of <b>mixed</b> <b>schemes</b> of finite element method has been {{developed}} for the solution of thermo-mechanical boundary problems of inhomogeneous media, in particular nonlinear problems that describe the non-isothermal processes of elasto-plastic deformation using curved trajectories with small radius of curvature. Using the device for functional analysis the correctness of mixed projection-mesh algorithms is studied and based on it the conditions that provide stability and convergence of mixed approximation for stresses, strains and displacements have been laid down. It is found that the mixed method results in more accurate computational distribution of stresses and strains compared with the classical method of displacements. Special finite elements have been constructed, which ensure stability and convergence of the proposed mixed approximations. The sys-tem of equations for mixed method with consideration of specific fulfillment of static boundary conditions on the body surface has been obtained. The efficient and stable step-iteration computational algorithms have been proposed for the solution of this system...|$|E
40|$|Hybrid methods {{represent}} a classic discretization paradigm for ellip-tic equations. More recently, hybrid {{methods have been}} formulated for convection-diffusion problems, in particular compressible fluid flow. In [25], we have introduced a hybrid mixed method for the compressible Navier-Stokes equations as {{a combination of a}} hybridized DG scheme for the convective terms, and an H(div,Ω) -method for the diffusive part. Since hybrid methods are based on Galerkin’s principle, the adjoint of a given hybrid discretization may be used for PDE-constraint optimal control problems, or error estimation, provided that the discretization is adjoint consistent. In the present paper, we extend the adjoint consis-tency analysis, previously reported for many DG schemes to the more complex hybrid methods. We prove adjoint consistency for a class of Hybrid <b>Mixed</b> <b>schemes,</b> which includes the hybridized DG schemes pro-posed by [19], as well as our recently proposed method ([25]). Hybrid Mixed discretizations, Hybridized Discontinuous Galerkin discretizations, Adjoint Consistency, compressible Navier-Stokes equation...|$|E
40|$|We {{propose a}} new eta-eta' <b>mixing</b> <b>scheme</b> where we {{start from the}} quark flavor basis and assume that the decay {{constants}} in that basis follow the pattern of particle state mixing. On exploiting the divergences of the axial vector currents - which embody the axial vector anomaly - all basic parameters are fixed to first order of flavor symmetry breaking. That approach naturally leads to a mass matrix, quadratic in the masses, with specified elements. We also test our <b>mixing</b> <b>scheme</b> against experiment and determine corrections to the first order values of the basic parameters from phenomenology. Finally, we generalize the <b>mixing</b> <b>scheme</b> to include the eta(c). Again the divergences of the axial vector currents fix the mass matrix and, hence, mixing angles and the charm content of the eta and eta'. Comment: 14 pages, uses feynmp. st...|$|R
40|$|We derive the {{relation}} between the amplitudes of short-baseline appearance and disappearance oscillations in 3 +$N_{s}$ neutrino <b>mixing</b> <b>schemes</b> which is the origin of the appearance-disappearance tension that is found from the analysis of the existing data in any 3 +$N_{s}$ neutrino <b>mixing</b> <b>scheme.</b> We illustrate the power of {{the relation}} to reveal the appearance-disappearance tension in the cases of 3 + 1 and 3 + 2 mixing using the results of global fits of short-baseline neutrino oscillation data. Comment: 6 pages; final version to be published in Mod. Phys. Lett. ...|$|R
40|$|In {{frames of}} the Ising model, we analyze {{self-organization}} of aqueous acetone (AC) and tetramethyl urea (TMU), caused by lyotropic phase transformations. Using the lattice Monte Carlo simulations, we confirmed the statements by Koga and co-workers {{on the tree}} content dependent <b>mixing</b> <b>schemes</b> in AC and TMU and found the values of molar fractions of surfactants, corresponding to the transitions between the mesophases of these <b>mixing</b> <b>schemes.</b> So we may compare these results with another concepts on the water structures. Comment: minor revision, partially submit to Revue Roumaine de Chimi...|$|R
40|$|Since {{the early}} 70 's, mixed finite {{elements}} {{have been the}} object of a wide and deep study by the mathematical and engineering communities. The fundamental role of this method for many application fields has been worldwide recognized and its use has been introduced in several commercial codes. An important feature of mixed finite elements is the interplay between theory and application. Discretization spaces for <b>mixed</b> <b>schemes</b> require suitable compatibilities, so that simple minded approximations generally do not work and the design of appropriate stabilizations gives rise to challenging mathematical problems. This volume collects the lecture notes of a C. I. M. E. course held in Summer 2006, when some of the most world recognized experts in the field reviewed the rigorous setting of mixed finite elements and revisited it after more than 30 years of practice. Applications, in this volume, range from traditional ones, like fluid-dynamics or elasticity, to more recent and active fields, like electromagnetism...|$|E
40|$|Calculations of the {{energetics}} of rare-earth incorporation in SrTiO 3 {{and other}} perovskite materials using classical potential models are widely {{featured in the}} literature. However, the standard incorporation mechanisms are often simpliﬁed and many do {{not account for the}} generation of oxygen vacancies. In this work, we use two mixed defect schemes that account for the introduction of rare-earth dopants at both the A- and B-sites of the perovskite structure and oxygen vacancies. An overall assessment of rare-earth doping in SrTiO 3 using the standard dopant incorporation modes with respect to dopant ionic radii is also given. Although the energies for our proposed mixed mechanisms are somewhat higher than the energies for the standard mechanisms, they are more realistic when compared to real samples, as they incorporate a range of different intrinsic defects, unlike the idealized standard schemes. Strong binding energies are reported throughout, in agreement with previous studies. A comparative study of these <b>mixed</b> <b>schemes</b> in BaTiO 3 and SrTiO 3 reveals that {{they are more likely to}} be active in Ba TiO 3. status: publishe...|$|E
40|$|The {{cattle and}} sheep populations {{as well as}} the {{production}} systems involved in the E. E. C. meat industry can vary considerably according to the wide geographical differences in physical and human characteristics. The optimal use of the existing animal populations can be achieved by. developing crossbreeding systems in dairy herds or flocks and, in the case of other herds or flocks, between dam populations and sires from specialised paternal strains or breeds. The breeding schemes for sires (male lines) are developed more and more according to the future use of their female progeny; paternal strains (slaughter cattle), maternal breeds (maximum use for breeding) and synthetic breeds. These schemes are being expanded in different ways, chiefly according to the percentage of artificial inseminations in a country or area:- traditional stratified systems for pedigree breeders,- integrated schemes, where recording facilities and decisions on breeding are thoroughly planned,- intermediate schemes (<b>mixed</b> <b>schemes.</b> combining the two conditions). Due to the small size of the herds and flocks, it is necessary to organise these schemes on a co-operative basis. The French example is mainly given here to illustrate this situation. I...|$|E
40|$|This paper {{continues}} the development, begun in part I, {{of the relationship}} between the simple genetic algorithm and the Walsh transform. The <b>mixing</b> <b>scheme</b> (comprised of crossover and mutation) is essentially "triangularized" when expressed in terms of the Walsh basis. This leads to a formulation of the inverse of the expected next generation operator. The fixed points of the <b>mixing</b> <b>scheme</b> are also determined, and a formula is obtained giving the fixed point corresponding to any starting population. Geiringer's theorem follows from these results in the special case corresponding to zero mutation. ...|$|R
40|$|We present further {{tests and}} {{applications}} of the new eta-eta' <b>mixing</b> <b>scheme</b> recently proposed by us. The particle states are decomposed into orthonormal basis vectors in a light-cone Fock representation. Because of flavor symmetry breaking the mixing of the decay constants can be identical to the mixing of particle states at most for a specific choice of this basis. Theoretical and phenomenological considerations show that the quark flavor basis has this property and allows, therefore, for a reduction {{of the number of}} mixing parameters. A detailed comparison with other <b>mixing</b> <b>schemes</b> is also presented. Comment: 9 page...|$|R
40|$|We analyse {{the effects}} of semi-convection and {{overshooting}} on the predicted surface abundances after {{the first and second}} dredge-ups in 15 and 20 M⊙ Pop. I stars. Overshooting is applied either to the core boundary or to the boundaries of all convective zones. It is shown that the surface abundances are sensitive to the <b>mixing</b> <b>scheme</b> adopted in the interior. The models including semi-convection lead to lower 12 C/ 13 C ratios than the other <b>mixing</b> <b>schemes,</b> while models with overshooting predict higher enhancements of sodium at the surface. © 1994 Kluwer Academic Publishers. SCOPUS: ar. jinfo:eu-repo/semantics/publishe...|$|R
40|$|Existing {{models of}} {{hospital}} financing advocate <b>mixed</b> <b>schemes</b> which include both lump-sum and cost-based payments. The doctor is generally the unique decision maker, which is unrealistic {{in a hospital}} setting where both managers and doctors are involved. This paper develops a model in which managers and doctors are responsible for different decisions within the hospital. In this model, public authorities who provide the financing, hospital managers who allocate resources within the hospital, and doctors who assign patients to either a low-tech or a high-tech therapy have information of increasing quality on the casemix of patients. The public authorities sign with hospital managers contracts specifying some lump-sum financing and some size of a high-tech equipment. In turn, managers, who know the broad mix of patients in the hospital, sign with hospital doctors contracts that specify the non-medical resources allocated to this facility {{as well as some}} remuneration. Doctors, who know each patient’s illness severity, select the patients to be treated by the high-tech facility, and receive from public authorities some fee-for-service payment that is differentiated according to the low- or high-tech treatment used for curing their patients. What emerges is a two-stage agency problem in which contracts are designed to elicit information in the most efficient way...|$|E
40|$|Emissions {{trading is}} a hot issue. At {{national}} as well as supranational levels, proposals for introduction of emissions trading schemes have been made. This paper assesses alternative emissions trading schemes at domestic level: (1) schemes where the total level of emissions is fixed (absolute cap-and-trade), (2) schemes where the allowable level of emissions per firm is related to some firm-specific indicator (relative cap-and-trade), and (3) <b>mixed</b> <b>schemes</b> which combine elements of the above alternatives. We present a quantitative assessment of these alternatives for climate change policy in the Netherlands. It is concluded that while relative cap-and-trade would avoid negative effects on competitiveness, it would not reduce emissions at the lowest costs. Besides, {{the addition of a}} trade system to existing relative standards does not result in additional emission reduction; it should be combined with other policy measures, such as energy taxes, in order to realise further reduction. Absolute cap-and-trade leads to efficient emissions reduction, but, implemented at the national level, its overall macroeconomic costs may be significant. The mixed scheme has as drawback that it treats firms unequal, which leads to high administrative costs. We conclude that none of the trading schemes is an advisable instrument for domestic climate policy. © 2003 Elsevier Science Ltd. All rights reserved...|$|E
40|$|The paper {{deals with}} the {{application}} features of the finite element technologies {{to solve the problems}} of elasticity with one-sided constraints. On the one hand, the area of this study is determined by the fact that many critical parts and assemblies of mechanical and power engineering constructions have a significant contact within some given surface. To assess the strength and the life of these parts and assemblies, reliable stress-strain state data are demandable. Data on the stress-strain state can be obtained using the contemporary mathematical modeling means, e. g., finite element technology. To solve the problems of the theory of elasticity with one-sided constraints, a method of finite elements in a traditional classical form can be used, but it is necessary to consider some of its shortcomings. The most significant one is an approximation of the tensile stress and strain, as well as a considerably lower order of convergence of the approximation for stresses and strains as compared to displacements. Improving the accuracy through increasing a density of the finite element models and/or the transition to more complex approximations is not always optimal, because increasing a dimension of the discrete problem leads to a significant computational cost and demand for expensive computing resources. One of the alternatives in numerical analysis of contact problems of the elasticity theory is to use the mixed variational formulations of the finite element method in which stresses and/or strains appear in the resolving equations along with displacements as equal unknown. A major positive factor when using the mixed formulations of the finite element method is reduction of the approximation error of stress and strain, which leads to a more accurate assessment of the stress-strain state in comparison with the classical approach of the finite element method {{in the form of the}} method of displacements. Besides, <b>mixed</b> <b>schemes</b> of the finite element method enable us to ensure continuous approximation of not only displacements, but also stresses and strains. <b>Mixed</b> <b>schemes</b> to solve the boundary value problems lead to the saddle-point problems. Their solutions use various iterative techniques. One of the most effective techniques is a modified SSOR (MSSOR) method, based on the SOR (Successive Over Relaxation) one. The paper considers one of the options of the finite element method in the framework of mixed scheme that uses a Reissner functional. The procedures of the algorithm proposed in the paper are used to solve the problem of contact interaction when an elastic body of the finite dimensions, being under a load of the external forces, relies on the absolutely rigid half-space. The contact occurs with the distinguished contact surface, which in the general case can change its size during thermo-mechanical loading. The algorithm is implemented as an application software complex. The numerical study of the one-sided contact interaction between the elastic plate and the perfectly rigid half-space has shown a fairly high efficiency of the developed algorithm and the code that implements it. </p...|$|E
40|$|A coding {{error in}} the s-Coordinate Primitive Equation Model SPEM has led to {{misleading}} statements about the behaviour of the Mellor-Yamada level 2 parameterization of vertical mixing. It has been claimed that the scheme removes static instability only very slowly and preserves statically unstable stratifications for an unrealistic long time. This note corrects this statement by demonstrating that the Mellor-Yamada <b>mixing</b> <b>scheme,</b> if implemented correctly, tends to overestimate rather than underestimate vertical mixing in seasonally ice-covered seas. Similar to other <b>mixing</b> <b>schemes</b> with the same behaviour, this leads to spurious open ocean deep convection, an unrealistic homogenization of the water column, and a significant reduction of sea ice volume...|$|R
40|$|Inconsistencies {{can arise}} in ocean {{circulation}} models when {{part of the}} physical processes responsible for vertical mixing is described in the usual differential form and part is formulated as adjustment processes. Examples for the latter class are explicit convective adjustment and Kraus–Turner type models of the surface mixed layer. Implicit convective adjustment as well as various representations of interior-ocean mixing are normally described in differential form. All these <b>schemes</b> <b>mix</b> density, with a mixing intensity that itself depends on stratification. This requires that information concerning static stability is passed through the individual mixing routines in a consistent sequence. It is shown that inconsistencies can arise when coupling a Kraus–Turner type model of wind-induced mixing with both a standard implicit convective adjustment {{as well as with}} an isopycnal <b>mixing</b> <b>scheme.</b> This leads to considerably overestimated mixed layer depths, for example, by hundreds of meters in the subpolar North Atlantic. The problem is eliminated first by ensuring that dissipation of potential energy during convection is included in the <b>mixing</b> <b>scheme,</b> even when considering wind-induced turbulence only, and second, by either calling the mixed layer routine before the differential vertical <b>mixing</b> <b>scheme</b> or tapering the vertical diffusivities to zero within the surface mixed layer...|$|R
40|$|The paper, "Latitude-dependent {{vertical}} mixing and the tropical thermocline {{in a global}} OGCM", was revised and published in Geophysical Research Letters. It treats the new GISS <b>mixing</b> <b>scheme</b> which includes the latitudinal dependence of the interior ocean turbulence field reported by Gregg, Sanford & Winkel. When implemented in the 3 x 3 degree NCAR CSMl OGCM [NCOMl] the new <b>mixing</b> <b>scheme</b> produces an improved, sharper equatorial thermoclines in both the Atlantic and the Pacific while simultaneously maintaining the realistic meridional overturning and northward heat transports found already with the previous GISS scheme. Also the paper "Diagnostics of the oceanic thermohaline circulation in a coupled climate model" describing earlier work on the grany was published...|$|R
