15|12|Public
5000|$|Scanning Projects Clearinghouse. This project {{sought to}} create a [...] "union list" [...] of <b>map</b> <b>digitization</b> {{projects}} among libraries in the principal region, so as to increase knowledge and accessibility to these projects and reduce duplication of effort.|$|E
40|$|Abstract. Historical maps contain rich cartographic {{information}}, such as road networks, {{but this}} information is “locked ” in images and inaccessible to a geographic information system (GIS). Manual <b>map</b> <b>digitization</b> requires intensive user effort and cannot handle {{a large number of}} maps. Previous approaches for automatic map processing generally require expert knowledge in order to fine-tune parameters of the applied graphics recognition techniques and thus are not readily usable for non-expert users. This paper presents an efficient and effective graphics recognition technique that employs interactive user intervention procedures for processing historical raster maps with limited graphical quality. The interactive procedures are performed on color-segmented preprocessing results and are based on straightforward user training processes, which minimize the required user effort for <b>map</b> <b>digitization.</b> This graphics recognition technique eliminates the need for expert users in digitizing map images and provides opportunities to derive unique data for spatiotemporal research by facilitating timeconsuming <b>map</b> <b>digitization</b> efforts. The described technique generated accurate road vector data from a historical map image and reduced the time for manual <b>map</b> <b>digitization</b> by 38 %...|$|E
40|$|This {{presentation}} {{highlights the}} key steps and decision points essential to completing a successful <b>map</b> <b>digitization</b> project. Topics {{to be covered}} include: overcoming the challenges of scanning large-scale materials (including file sizes and encapsulation), descriptive metadata for map collections, copyright and privacy issues for geographic materials, adding geographic coordinates to map collections, image viewer and interface options for online maps, and methods to track the impact of <b>map</b> <b>digitization</b> with users. Using University of Nevada Las Vegas Libraries’ digital project Southern Nevada: History in Maps as a case study, the authors will discuss the challenges inherent in <b>map</b> <b>digitization</b> and suggest strategies to overcome these obstacles. In addition to walking the audience through <b>map</b> <b>digitization</b> workflow, the poster will highlight the University of Nevada Las Vegas’ spatial search tool, ISIS, that allows users to search maps in CONTENTdm collections without text-based queries. Poster session attendees will learn from one institution’s experience with a successful map digital collection and leave with the knowledge and confidence to pursue their own project...|$|E
30|$|The {{required}} {{data were}} divided into physiographic and climate categories. The topographical data of the area under study were obtained through the digitization of maps  1 : 50, 000 courtesy of the Greek Geographical Army Service. Through <b>maps</b> <b>digitization,</b> the specification of {{the boundaries of the}} watershed was achieved.|$|R
30|$|Development of a 3 D fault <b>map</b> {{begins with}} <b>digitization</b> of fault traces from the 2 D {{geologic}} map, digitizing fault profiles from 2 D cross sections, and digitizing fault interpretations from subsurface well data (or importing these from a geographic information system).|$|R
40|$|An ongoing {{program to}} {{preserve}} approximately seven hundred oversized, canvas-backed, coal mining maps from the CONSOL Energy Mining Map Collection was {{initiated by the}} University of Pittsburgh (Pitt) in 2007, supported by funding from the United States Department of the Interior Office of Surface Mining and Reclamation (OSM) and the Pennsylvania Department of Environmental Protection (PA-DEP). The main goal of this project is to stabilize and clean the mining <b>maps</b> for <b>digitization</b> at the OSM National Mine Map Repository (NMMR) located in Pittsburgh, Pennsylvania. The digitized data of the underground mines will be incorporated into Geographical Information Systems relative to mine safety, land reclamation, current mining operations, and new development...|$|R
40|$|This {{program is}} aimed at archivists and other special {{collections}} staff who have published maps as opposed to manuscript maps {{as part of their}} collections but do not have much expertise in map librarianship. The program includes information on kinds of maps, the basic parts of a map including those found mainly on pre- 19 th century maps, how to store and preserve maps, why they should be cataloged, how cataloging rare maps differs from cataloging current maps, why maps should be classified with a standard classification system, how Library of Congress call numbers can be used to locate certain kinds of maps for certain geographical areas, and how to date a map. Information will also be provided on how to do map exhibits and <b>map</b> <b>digitization</b> projects and how the information in bibliographic records for maps can be used in metadata for <b>map</b> <b>digitization</b> projects. UNLV’s <b>map</b> <b>digitization</b> project will be shown as an example and some online historical map collections will be discussed that patrons can be referred to and that may be useful in finding more information on maps in a library or museum’s collection. Examples will be provided of how various archival collections organize and store their maps. Discussion will be included on the ethical and cataloging issues of acquiring sheets that have been removed from atlases...|$|E
40|$|In this paper, a least-squares based {{cadastral}} parcel area adjustment in geographic information systems (GIS) is developed based on (1) both the areas and coordinates {{being treated as}} observations with errors; and (2) scale parameters being introduced to take the systematic effect into account {{in the process of}} cadastral <b>map</b> <b>digitization.</b> The area condition equation for {{cadastral parcel}} considerations of scale parameters and geometric constraints is first constructed. The effects of the scale error on area adjustment results are then derived, and statistical hypothesis testing is presented to determine the significance of the scale error. Afterwards, Helmert's variance component estimation based on least-squares adjustment using the condition equation with additional parameters is proposed to determine the weight between the coordinate and area measurements of the parcel. Practical tests are conducted to illustrate the implementation of the proposed methods. Four schemes for solving the inconsistencies between the registered areas and the digitized areas of the parcels are studied. The analysis of the results demonstrates {{that in the case of}} significant systematic errors in cadastral <b>map</b> <b>digitization,</b> the accuracies of the adjusted coordinates and areas are improved by introducing scale parameters to reduce the systematic error influence in the parcel area adjustment. Meanwhile, Helmert's variance component estimation method determines more accurate weights of the digitized coordinates and parcel areas, and the least-squares adjustment solves the inconsistencies between the registered areas and the digitized areas of the parcels. Department of Land Surveying and Geo-Informatic...|$|E
40|$|This thesis {{proposes to}} combine {{methods and data}} from two rather distant fields of {{language}} science – dialectology and human language technology – into a system that automatically transforms Standard German words and sentences into multiple Swiss German dialects. Our work is inspired by previous research in generative dialectology and computational linguistics, which attempts to derive multiple dialect systems from a single reference system {{with the help of}} hand-written transformation rules. We propose to call such rules 'georeferenced', {{in the sense that they}} are linked to probability maps that specify their area of validity. These probability maps are extracted by interpolation from existing dialectological atlases. Finally, as a consequence of our <b>map</b> <b>digitization</b> efforts, we are able to present original dialectometrical results for the Swiss German dialect landscape...|$|E
40|$|This study {{aimed to}} map phytophysiognomies {{of an area}} of Ombrophilous Dense Forest at Parque Estadual da Serra do Mar and {{characterize}} their floristic composition. Photointerpretation of aerial photographs in scale of 1 : 35, 000 was realized in association with field work. Thirteen physiognomies were mapped and they were classified as Montane Ombrophilous Dense Forest, Alluvial Ombrophilous Dense Forest or Secondary System. Three physiognomies identified at Casa de Pedra streamlet's basin were studied with more details. Riparian forest (RF), valley forest (VF), and hill forest (HF) presented some floristic distinction, as confirmed by Detrended Correspondence Analysis (DCA) and Indicator Species Analysis (ISA) conducted here. Anthropic or natural disturbances and heterogeneity of environmental conditions may be the causes of physiognomic variation in the vegetation of the region. The results presented here {{may be useful to}} decisions related to management and conservation of Núcleo Santa Virgínia forests, in general. The authors would like to acknowledge the Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq) for the scholarship awarded to first author; the Fundação de Amparo à Pesquisa do Estado de São Paulo (FAPESP) for the funding for the projects "Composição florística, estrutura e funcionamento da Floresta Ombrófila Densa dos Núcleos Picinguaba e Santa Virgínia do Parque Estadual da Serra do Mar" (Process number 03 / 12595 - 7), under coordination of Dr. Carlos Alfredo Joly (IB / UNICAMP) and "O balanço de carbono sobre uma floresta de Mata Atlântica com medidas micrometeorológicas e biométricas" (07 / 57465 - 4), under the coordination of Dr. Humberto Ribeiro da Rocha (IAG / USP); all colleagues who helped in field work; all taxonomists who helped with the identification of the material collected and to Amanda Catarucci for the help with the initial <b>maps</b> <b>digitization.</b> ...|$|R
40|$|We call "natural" image any {{photograph}} of an outdoor or indoor scene {{taken by a}} standard camera. We discuss the physical generation process of natural images as a combination of occlusions, transparencies and contrast changes. This description fits to the phenomenological description of Gaetano Kanizsa according to which visual perception tends to remain stable with respect to these basic operations. We define a contrast invariant presentation of the digital image, the topographic map, where the subjacent occlusion-transparency structure is put into evidence by the interplay of level lines. We prove that each topographic map represents a class of images invariant with respect to local contrast changes. Several visualization strategies of the topographic map are proposed and implemented and mathematical arguments are developed to establish stability properties of the topographic <b>map</b> under <b>digitization.</b> 1 Introduction What are the basic, computable elements from which the analysis of an [...] ...|$|R
40|$|The conference co-ordinator and an {{invited speaker}} of the International Conference on Digital Libraries (ICDL) {{which took place in}} New Delhi, India, February 2004 provide an {{overview}} of the conference – one of a growing series of digital library conferences that bring together computer scientists and librarians. The objectives of this conference were to bridge the knowledge gaps between developing and developed countries; initiate capacity building activities in digital libraries; evolve a road <b>map</b> for the <b>digitization</b> of archives, manuscripts and libraries; provide a forum for facilitating interaction among participants; and formulate recommendations on digitization technologies and policies. Outlines the themes and topics covered and provides the main points of the inaugural address to the conference by the President of India as well as the three keynote addresses. Peer Reviewe...|$|R
40|$|Academic {{libraries}} {{are playing}} a role in the digitization of Canadian government documents, but maps tend to be excluded from these activities due to their unique dimensions and display requirements. Using a topographic <b>map</b> <b>digitization</b> project as a case study, this paper presents a collaborative approach to map scanning, georeferencing, and metadata creation across several Ontario universities. Collectively, the 21 institutions making up the Ontario Council of University Libraries (OCUL) possess and maintain large volumes of Canadian topographic maps. However, few OCUL universities hold complete sets of these map series. While the Canadian government’s most recent topographic maps are now available online, older editions of these maps have not been digitized. This project, currently underway at several participating universities, will enable us to share digital versions of some of our most-requested historical map series with the public at large...|$|E
40|$|Many cartographic systems {{currently}} rely on raster-scan digitizing {{to convert}} analog source material to digital form. Raster technology is very effective at rapid and accurate digitization of {{large volumes of}} cartographic data. At the same time, existing raster-to-vector (R-V) conversion processes rely on an inordinately large amount of human post-scan editing to coherently sort and combine the short, unattributed lineal segments into single cartographic spatial entities. The Automatic Feature Tracking (AFT) system addresses {{one of the major}} causes of this bottleneck in the digitization process—skeletonization. This is accomplished by directly converting symbolized linear features on a raster map image into sets of x,y coordinates. The system relies on Template Matching and Feature Tracking techniques to locate feature centerlines. This paper briefly reviews the history of <b>map</b> <b>digitization</b> techniques, illustrates inadequacies of those past approaches, and presents the AFT system {{as an alternative to the}} R-V conversion routines in existing raster digitization systems...|$|E
40|$|In {{this paper}} is {{investigating}} {{the possibility of the}} application of panchromatic image, with spatial analysis 1 meter, from the satellite IKONOS- 2 in the Hellenic Cadastre for suburban regions. This possibility is estimated via the technical specifications of the Hellenic Cadastre. For the achievement of this aim, two orthoimages were produced with two different methods. The first method led to an orthoimage production by using control points that were received from <b>map</b> <b>digitization</b> in scale 1 : 5000, while the second method led to an orthoimage production using control points that were received from measurements, which were realized with the Global Positioning System (GPS). The application aim of these two methods was on the improvement of orthoimage accuracy with the second method as well as the two methods evaluation. Moreover, the two orthoimages ’ accuracy was estimated based on the technical specifications of the Hellenic Cadastre for suburban regions. Finally the two methods were evaluated taking into consideration the results for the orthoimages accuracy and the technical specifications of Hellenic Cadastre for suburban regions. 1...|$|E
40|$|In {{this paper}} we discuss the {{efficient}} implementation of pseudochaotic piecewise linear <b>maps</b> with high <b>digitization</b> accuracies, taking the R'enyi chaotic map as a reference. The proposed digital architectures {{are based on a}} novel algorithmic approach that uses carry save adders for the nonlinear arithmetic modular calculations arising when computing piecewise linear maps with a finite precision. As a result, the system can be implemented by digital circuits obtaining high throughputs, which are not dependent on the digital resolution while involving a hardware complexity linearly proportional to the number of bits used for representing the discretized state. The proposed solutions result to be particularly suitable for the implementation of pseudorandom number generators based on pseudochaos, or for the definition of efficient digital blocks that can be integrated in most of the pseudochaotic cyphers proposed in the literature...|$|R
40|$|AbstractThe {{present study}} {{illustrates}} {{an approach to}} map a landscape element typical of the Mediterranean rural landscape by integrating <b>digitization</b> from topographic <b>maps</b> and photo-interpretation on recent Google Heart imagery. The approach was applied to {{a case study in}} Greece. The location of water open reservoirs used for crop irrigation, taken as a typical element of the rural landscape in the area, was <b>mapped</b> through (i) <b>digitization</b> of Hellenic Military Geographical Service (HMGS) maps scaled 1 : 25, 000 referring to late- 1970 s and (ii) interpretation of Google Heart Imagery referring to early- 2010 s with the support of ancillary data sources. Maps have been included into a Geographic Information System (GIS) incorporating layers, which describe land-use and population distribution. The cartography was processed to develop landscape indicators by using spatial analysis tools. The integration of digitization techniques with geographic information systems and spatial analysis represents an original approach for landscape assessment in areas suffering from the lack of digital information on the environment...|$|R
40|$|In this paper, modern {{definition}} of island {{established by the}} IHO has been accepted, and classification of islands, islets, rocks and rocks awash has been proposed according to their areas. The coastline of the Croatian part of the Adriatic Sea was digitized from topographic maps produced at the scale of 1 : 25 000 (TM 25). Topographic <b>maps</b> used for <b>digitization</b> are more precise than the maps {{that were used in}} earlier works and consequently the data on the number of islands and their coastline lengths and areas are more precise. Polygons of islands were closed in GIS package AutoCAD Map 2000, and each was given its name. From the obtained database and classification of islands, islets and rocks, in the coastal sea area of the Republic of Croatia 79 islands, 525 islets, and 642 rocks and rocks awash, or a total of 1246 have been recorded. Furthermore, it has been established that on TM 25 the island of Cres has the largest area (405. 70 km 2), although in literature so far (including atlases) the island of Krk was most often cited as the largest island in the Adriatic Sea. The island of Pag has the longest coastline length of 302. 47 km...|$|R
40|$|Proyecto de Graduación (Licenciatura en Ingeniería Forestal). Instituto Tecnológico de Costa Rica, Escuela de Ingeniería Forestal, 2010. This {{study is}} an initial {{proposal}} for restoring the area knows as the Diques (flooded area) of the Reventado River National Reserve, Cartago, Costa Rica. For this {{it was used}} the software ArcGIS 9. 3 to conduct the photo interpretation and <b>map</b> <b>digitization.</b> The main orthophotomaps sheets {{used in this study}} where Ochomogo 3445 - IV- 17 and Tejar 3445 -IV- 22, scale 1 : 10 000, and acquired by the Greater Metropolitan Area PRUGAM Project for land use zoning. Different land uses were identified within the study area of 129. 7 ha such as: grasses (51. 5 %), urban (50. 4 %), secondary forest (18. 2 %), forest plantations (7. 3 %), extraction of materials such as stone and sand (1. 9 %) and runway model airplane (0. 7 %). A potential area of 45. 15 hectares was identified for restoration with three tree suitable species for reforestation such as Citharexylum donnell-smithii (Dama), Acnistus arborescens (Güitite) and Acacia angustissima (Carboncillo) with densities of 100 trees / ha...|$|E
40|$|Digitization of the {{geologic}} maps {{is confusing}} and time consuming process. If {{the same person}} is digitizing the data and converting it into the GIS, the way is not so important, but the result is. However, for parallel digitization, which means digitization {{of two or more}} geological maps by different users at the same time, user intervention brings confusion problems while converting digitized data into the GIS. For this reason, before initiating the project, the standards of geological <b>map</b> <b>digitization</b> should be well performed. A geological map can contain limited number of common features such as faults, folds, types of formation boundaries and dip-strike measurements. They should be represented with common attributes in all maps. For that reason a task bar is prepared for use in any digitization software that standardizes the attributes of digitized data. This task bar provides the attributes of common geological features. While digitizing, user finds the icon of the feature on the task bar and pushes that button. The attribute is copied into the PC’s ram and user pastes it into the necessary field. One advantage of this method is to simplify the digitization process and so it is much faster than non standardized and unorganized digitization. Another advantage is the digitizing may be done by people having no geological background. Using this method, 80 pieces of 1 / 50, 000 scaled geological map are converted into the GIS for about 4 months with two digitizing person. 1...|$|E
40|$|Present-day <b>map</b> <b>digitization</b> methods produce {{data that}} is semantically opaque; {{that is to}} a machine, a {{digitized}} map is merely a collection of bits and bytes. The area it depicts, the places it mentions, any text contained within legends or written on its margins remain unknown - unless a human appraises the image and manually adds this information to its metadata. This problem is especially severe {{in the case of}} old maps: these are typically handwritten, may contain text in varying orientations and sizes, and can be in a bad condition due to varying levels of deterioration or damage. As a result, searching for the contents of these documents remains challenging, which makes them hard to discover for users, unusable for machine processing and analysis, and thus effectively lost to many forms of public, scientific or commercial utilization. Fully automatic detection and transcription of place names and legends is, likely, not achievable with today's technology. We argue, however, that semi-automated methods can eliminate much of the tedious effort required to annotate map scans entirely by hand. In this paper, we showcase early work on semi-automatic place name annotation. In our experiment, we utilize open source tools to identify potential locations on the map representing toponyms. We present how, in next steps, we aim to extend our experiment by exploiting the spatial layout of identified candidates to deduce possible place names based on existing toponym lists. Ultimately, or goal is to combine this work with a toolset for manual image annotation into a convenient online environment. This will allow curators, researchers, and potentially also the general public “tag” and annotate toponyms on digitized maps rapidly...|$|E
40|$|In the State Archive of Como, Northern Italy, about 15000 {{historical}} {{cadastral maps}} corresponding to 246 current municipalities of Como and Lecco districts are preserved. These maps belong to different cadastral productions: the Theresian cadastre, promoted in 1718 by Emperor Carl VI and come into force in 1760 {{during the reign}} of Maria Teresa; the Lombardo- Veneto cadastre, started in 1854 and completed, with continuous updates during the time, {{at the end of the}} century; and finally some maps of 1905 belonging to the New Lands Cadastre, the first national geometric cadastre after Italian unification of 1861. Maps have not only a considerable artistic value but mostly a cultural and historical one, since they constitute a great source to derive an accurate representation of the territory and its evolutions. For these reasons, the old maps represent nowadays a valuable instrument for historians, scholars and professionals working both in the historical research field and in the urban and territorial planning. The project Web C. A. R. T. E. (Web Catalogo e Archivio delle Rappresentazioni del Territorio e delle sue Evoluzioni), sponsored by the Fondazione Provinciale della Comunità Comasca Onlus, has been started to enhance the immense cartographic heritage of the State Archive of Como using the most recent technologies of map processing and web services. After the <b>maps</b> <b>digitization</b> step, performed by the State Archive in agreement with the interested municipalities, a georeferencing and warping procedure is needed to place the cadastral maps in the actual Italian reference system, thus making it possible to overlap them to the current cartography. Being the most of the maps divided in sheets, that have been surveyed and drawn independently from each other, the preliminary step has been to combine the sheets in a single map by applying to them a roto- translation with a scale variation. The georeferencing of unified maps has then been performed and tested in different software and GIS packages to determine the optimal solution. Finally PCI Geomatica OrthoEngine has been chosen, thanks to its variety of implemented mathematical models and to the possibility of inserting not only Ground Control Points (points of known coordinates, both in the actual cartography and in the historical map, that are used to compute the mathematical model) but also Check Points, points with known coordinates that are not included in the transformation and can therefore be used to check the model accuracy. The residuals of the transformation have then been used to determine the best georeferencing model for each cadastral map, confirming the choice with statistical techniques. The following step has been the documentation of georeference d maps in terms of metadata, a series of information needed to precisely identify the data and get information about their content, accuracy, accessibility and usage constrains. Metadata schema are currently defined by national and international standards: at the Italian level, the CNIPA (Centro Nazionale per l’Informatica nella Pubblica Amministrazione) proposed in 2006 a standard which is in agreement with the European Directive INSPIRE and defines a common set of metadata related to all kinds of geographic information used by national Public Administrations. Metadata for the historical georeferenced maps have therefore been compiled according to the Italian standard; last step has been their publication on the Internet through GeoNetwork, an open source web geo-catalogue that allows users to immediately identify a data and derive (from its metadata) information about language, spatial extent, reference system, responsible person or agency, accessibility, possible limitations on the usage, data origin and production process, and other features. Digitized and georeferenced maps, accompanied by their metadata, can finally be visualized and navigated online through the implementation of a dedicated webGIS. The realization of this viewing service implies the usage of software and tools both from the server and the client-side. Applying FOSS (Free and Open Source Software) solutions, a system with interactive functionalities and able to manage large raster maps has been developed. The entire service is currently in a test phase to verify its fulfilment of specific requests and needs expressed by experts from the State Archive; for this reason it may be possible that new and improved solutions will be introduced in the future...|$|R
40|$|Includes bibliographical {{references}} (pages [85]- 86) Tax {{map data}} were converted from hardcopy to digital form using three tax <b>map</b> conversion methods: <b>digitization</b> and transformation; digitization, transformation, and rubbersheeting; recompilation, digitization, and transformation. Conversion methods were independently applied to two coordinate control data sets. The first was constructed from survey data collected using traditional and {{global positioning system}} (GPS) surveying techniques. The second was constructed from map features on USGS 7. 5 -minute maps. The accuracy of the products from these conversion methods was evaluated along with the influence coordinate control data have on this accuracy and the replicability of the conversion methods. The accuracy of the conversion methods {{and the influence of}} the control data were measured by comparing conversion products with two parcel map control data sets. One parcel map was constructed from the survey coordinate control data. The second was constructed from the USGS map feature control data. Public Land Survey System (PLSS) comer positions, parcel comer positions, parcel dimensions, and parcel areas were used in this comparison. These parcel characteristics were identically coded across the conversion method products and parcel maps. The Euclidean distance metric was used to calculate positional differences in PLSS and parcel comer positions. Absolute differences were calculated for parcel lines and parcel areas. Arithmetic means, standard deviations, and coefficients of variation were calculated for these differences and used to analyze the accuracy of the conversion method products and the influence of the coordinate control data. The conversion method which used rubbersheeting and survey control data produced the best positional, dimensional, and areal accuracies. Recompilation produced the worst regardless of coordinate control data. The remainder produced similar results. The influence of the coordinate control data was seen in all aspects of parcels described by aliquot part descriptions. The coordinate control data influenced only the positional aspects of metes and bounds described parcels. The coordinate control data did not influence the replicability of the conversion methods, but did influence the accuracy of the results from those methods. The study area for this research is located in Kingston Township, DeKalb County, Illinois. It consists of four PLSS sections which remain well defined on the landscape. The primary land use within the study area is agriculture. The terrain is low to moderately rolling with elevation differences throughout the study area of approximately 330 feet. M. S. (Master of Science...|$|R
40|$|Large scale {{maps and}} image mosaics are {{representative}} geospatial data {{that can be}} extracted from UAV images. Map drawing using UAV images can be performed either by creating orthoimages and digitizing them, or by stereo plotting. While <b>maps</b> generated by <b>digitization</b> may serve the need for geospatial data, many institutions and organizations require map drawing using stereoscopic vision on stereo plotting systems. However, there are several aspects to be checked for UAV images to be utilized for stereo plotting. The first aspect is the accuracy of exterior orientation parameters (EOPs) generated through automated bundle adjustment processes. It {{is well known that}} GPS and IMU sensors mounted on a UAV are not very accurate. It is necessary to adjust initial EOPs accurately using tie points. For this purpose, we have developed a photogrammetric incremental bundle adjustment procedure. The second aspect is unstable shooting conditions compared to aerial photographing. Unstable image acquisition may bring uneven stereo coverage, which will result in accuracy loss eventually. Oblique stereo pairs will create eye fatigue. The third aspect is small coverage of UAV images. This aspect will raise efficiency issue for stereo plotting of UAV images. More importantly, this aspect will make contour generation from UAV images very difficult. This paper will discuss effects relate to these three aspects. In this study, we tried to generate 1  :  1, 000 scale map from the dataset using EOPs generated from software developed in-house. We evaluated Y-disparity of the tie points extracted automatically through the photogrammetric incremental bundle adjustment process. We could confirm that stereoscopic viewing is possible. Stereoscopic plotting work was carried out by a professional photogrammetrist. In order to analyse the accuracy of the map drawing using stereoscopic vision, we compared the horizontal and vertical position difference between adjacent models after drawing a specific model. The results of analysis showed that the errors were within the specification of 1  :  1, 000 map. Although the Y-parallax can be eliminated, it is still necessary to improve the accuracy of absolute ground position error in order to apply this technique to the actual work. There are a few models in which the difference in height between adjacent models is about 40  cm. We analysed the stability of UAV images by checking angle differences between adjacent images. We also analysed the average area covered by one stereo model and discussed the possible difficulty associated with this narrow coverage. In the future we consider how to reduce position errors and improve map drawing performances from UAVs...|$|R
40|$|In this paper, {{we report}} on recent {{digitization}} efforts of the linguistic atlas of German-speaking Switzerland (Sprachatlas der deutschen Schweiz, henceforth SDS). The SDS project was initiated in the 1930 s, its data collection (in 557 locations) {{took place in the}} 1940 s and 1950 s, and the publication of the resulting eight volumes (totalling about 1500 hand-drawn maps) occurred between 1962 and 1997. While the SDS is often considered to have pioneered the concept of small-scale linguistic atlas (Kleinraumatlas), its achievement before the digital era prevented it from being used for quantitative analyses and computational work, and no digitization efforts had been undertaken until recently. Here, we would like to report on the advancement of our SDS digitization project and of some potential uses of the resulting dataset. Because of time and resource constraints, we had to restrict the SDS data in three ways: (1) selection of a subset of the maps, (2) removal of 8 inquiry points located on Italian territory, (3) removal of rarely used variants in each map. We will discuss the linguistic and dialectological reasonings behind these choices. We will also present the technical process of <b>map</b> <b>digitization</b> that we found to be most productive. In the first phase, 216 maps were digitized by one of the co-authors. In the second phase, some of the 216 existing maps were refined and 20 additional maps were digitized by one of the co-authors. The digitized maps are made available on an interactive web site, and can be downloaded freely as ArcGIS shapefiles for research purposes. We will also briefly present briefly some of our application of this material, such as a multi-dialectal morphological generator, a machine translation system into the various Swiss German dialects, as well as dialectometrical analyses...|$|E
40|$|Most Natural Language Processing (NLP) {{applications}} {{focus on}} standardized, written language varieties. From a practical point of view, this focus is understandable: such systems {{are most likely}} to be used on written data of a standard variety, and this kind of data is also most easily available for training and parametrizing NLP systems. However, in many regions of the world, the linguistic reality is somewhat more complex: many speakers use some kind of non-standard language variety [...] mostly in speech, but sometimes also in writing. Non-standard lects are subject to continuous variation along the dialectal and sociolinguistic level. From a methodological as well as a practical point of view, it is therefore interesting to include findings of variational linguistics in existing NLP methods. Our work focuses on Swiss German dialects. The German-speaking part of Switzerland has been subject to more than a century of dialectological research that has resulted in dialect atlases, grammars and lexicons. Today, the dialects represent the default variety of oral communication (Standard German is only used for writing). Recently, dialect writing has also become popular in electronic media. This evolution justifies the development of dialect NLP tools, and at the same time provides us with data to validate them. We will present two prototypes of NLP applications: machine translation from Standard German to Swiss German dialects, and Swiss German dialect parsing. Both applications share two basic assumptions. First, they largely rely on existing Standard German models to capitalize on the larger resource pool of a written, standardized language. Second, they conceive Swiss German neither as one homogeneous language variety, nor as a finite number of distinct dialects, but rather as a continuum of varieties that share some characteristics. NLP applications traditionally consist of a set of rules [...] grammar rules in parsing, transfer rules in machine translation. In our models, these rules are probabilistic, and the probability of a rule not only depends on the grammatical context, but also on the geographical location of the dialect to be treated. Therefore, each rule is associated with a map that defines its probability distribution over the Swiss German dialect landscape. This conception leads to some practical issues of <b>map</b> <b>digitization</b> and interpolation that will also be discussed...|$|E

