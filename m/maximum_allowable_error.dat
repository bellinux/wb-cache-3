23|3124|Public
30|$|Real-time (RT) high {{priority}} services {{are used for}} applications such as video conferencing and streaming entailing QoS guarantees on <b>maximum</b> <b>allowable</b> <b>error</b> rate, minimum throughput, and maximum delay.|$|E
30|$|Non-real-time (nRT) {{services}} entail {{applications such}} as file transfers (FTP). They do not impose any constraint on delays but, {{in addition to a}} <b>maximum</b> <b>allowable</b> <b>error</b> rate, they require sustained throughput guarantees.|$|E
30|$|Best effort (BE) low {{priority}} services with a prescribed <b>maximum</b> <b>allowable</b> <b>error</b> rate but without specific requirements on rate or delay guarantees. Examples of best-effort services include {{applications such as}} e-mail or HTTP web browsing.|$|E
40|$|A test/analysis {{correlation}} {{criterion for}} the analytical model accuracy requirement has been developed. It {{is based on}} the principle of equal errors from the model inaccuracy and the uncertainties of dynamic environments. The forcing functions are idealized to establish the base for uncertainty definition and the <b>maximum</b> <b>allowable</b> <b>errors</b> from these uncertainties are obtained. Then the model accuracy requirement is established by comparing the responses due to the model errors to those due to the forcing function uncertainties...|$|R
40|$|I {{discuss the}} {{accuracy}} requirements on numerical relativity calculations of inspiraling compact object binaries whose extracted gravitational waveforms {{are to be}} used as templates for matched filtering signal extraction and physical parameter estimation in modern interferometric gravitational wave detectors. Using a post-Newtonian point particle model for the pre-merger phase of the binary inspiral, I calculate the <b>maximum</b> <b>allowable</b> <b>errors</b> for the mass and relative velocity and positions of the binary during numerical simulations of the binary inspiral. These <b>maximum</b> <b>allowable</b> <b>errors</b> are compared to the errors of state-of-the-art numerical simulations of multiple-orbit binary neutron star calculations in full general relativity, and are found to be smaller by several orders of magnitude. A post-Newtonian model for the error of these numerical simulations suggests that adaptive mesh refinement coupled with second order accurate finite difference codes {{will not be able to}} robustly obtain the accuracy required for reliable gravitational wave extraction on Terabyte-scale computers. I conclude that higher order methods (higher order finite difference methods and/or spectral methods) combined with adaptive mesh refinement and/or multipatch technology will be needed for robustly accurate gravitational wave extraction from numerical relativity calculations of binary coalescence scenarios. Comment: One figure added, various clarifications and references added; accepted to Phys. Rev. ...|$|R
40|$|A {{nonlinear}} {{simulation of}} the NASA Generic Transport Model {{was used to}} investigate the effects of errors in sensor measurements, mass properties, and aircraft geometry on the accuracy of dynamic models identified from flight data. Measurements from a typical system identification maneuver were systematically and progressively deteriorated and then used to estimate stability and control derivatives within a Monte Carlo analysis. Based on the results, recommendations were provided for <b>maximum</b> <b>allowable</b> <b>errors</b> in sensor measurements, mass properties, and aircraft geometry to achieve desired levels of dynamic modeling accuracy. Results using other flight conditions, parameter estimation methods, and a full-scale F- 16 nonlinear aircraft simulation were compared with these recommendations...|$|R
40|$|We {{present a}} smooth, every-where C k, {{analytic}} surface representation for closed surfaces of arbitrary topology. We demonstrate fitting this representation to meshes of varying resolutions and sampling quality. The fitting process is adaptive and provides controls {{for both the}} average and the <b>maximum</b> <b>allowable</b> <b>error.</b> The representation is suitable for applications which require consistent parameterizations across different surfaces...|$|E
30|$|The {{characteristics}} of renal lithiasis {{indicate that the}} <b>maximum</b> <b>allowable</b> <b>error</b> for urinary pH is 0.1 pH units. None of the pH measurements made using the new electronic device differed from those made by the pH meter by more than 0.1 pH units. However, more than 70 % {{of the differences between}} the dipstick and the pH meter were above 0.1 pH units, and more than 40 % had an absolute difference of more than 0.2 pH units.|$|E
40|$|The {{impact of}} the mismodelling in the Earth's {{geopotential}} coefficient C 22 on the detection of the general relativistic Lense-Thirring and gravitomagnetic clock effects is investigated. The systematical error induced on the Lense-Thirring LAGEOS experiment amounts to almost 1 %. Regarding the gravitomagnetic clock effect, the systematical error induced on the satellite's azimuthal location is 1 - 2 orders of magnitude larger than the <b>maximum</b> <b>allowable</b> <b>error</b> {{in order to make}} feasible the measurement of this general relativistic feature...|$|E
30|$|The data {{length is}} 24  h and the {{sampling}} interval is 5  min (24  ×  12  =  288) in this example. The curves {{used to predict}} the wind generation by eight wind farms and the actual power curves are shown in Appendix 1, and assume that the <b>maximum</b> <b>allowable</b> prediction <b>error</b> ΔP is ± 0.05  p.u.|$|R
40|$|We {{calculate}} analytically {{the most}} relevant nongravitational perturbations on the mean anomaly of LAGEOS type satellites in order to investigate the constraints they pose on the feasibility of the proposed general relativistic gravitomagnetic clock effect. The gravitoelectric contributions to the proper times induced by the perturbations on the Keplerian mean motions of two counter-orbiting satellites dedicated to this goal can be canceled out if a couple of LAGEOS type satellites with identical geometrical and physical features is assumed. However, there are relevant perturbations growing linearly and quadratically in time which could prevent to meet the stringent requirements on the <b>maximum</b> <b>allowable</b> <b>errors</b> on the radial and azimuthal satellites' locations needed to make feasible the detection of the gravitomagnetic clock effect...|$|R
30|$|Repeat step 2 {{until the}} value of the {{objective}} function is less than the <b>allowable</b> <b>maximum</b> <b>error</b> or the iteration number is the maximum.|$|R
30|$|Inclusion {{criterion}} {{for this study}} was pregnant women attending the ANC outpatient clinic at FUH. Exclusion criteria were having acute varicella infection and undergoing immunosuppressive therapy. Three hundred thirty-three participants were recruited to participate in the study. The sample size was calculated, using a single proportion formula with <b>maximum</b> <b>allowable</b> <b>error</b> set at 5 %, a proportion of positive seroprevalence of VZV antibodies at 76.5 % (6), and at 5 % significance level. The required number was 277. To adjust for 10 % nonresponse rate, the estimated sample was increased to 305; through this research, 333 participants were recruited to participate in the study.|$|E
40|$|Abstract—Current {{processor}} designs have {{a critical}} operating point that sets a hard limit on voltage scaling. Any scaling beyond the critical voltage results in exceeding the <b>maximum</b> <b>allowable</b> <b>error</b> rate, i. e., {{there are more}} timing errors than can be effectively and gainfully detected or corrected by an error-tolerance mechanism. This limits the effectiveness of voltage scaling as a knob for reliability/power tradeoffs. In this paper, we present power-aware slack redistribution, a novel design-level approach to allow voltage/reliability tradeoffs in processors. Techniques based on power-aware slack redistribution reapportion timing slack of the frequently-occurring, nearcritical timing paths of a processor in a power- and area-efficient manner, such that we increase the range of voltages over which the incidence of operational (timing) errors is acceptable. This results in soft architectures- designs that fail gracefully, allowing us to perform reliability/power tradeoffs by reducing voltage {{up to the point}} that produces maximum allowable errors for our application. The goal of our optimization is to minimize the voltage at which a soft architecture encounters the <b>maximum</b> <b>allowable</b> <b>error</b> rate, thus maximizing the range over which voltage scaling is possible and minimizing power consumption for a given error rate. Our experiments demonstrate 23 % power savings over the baseline design at an error rate of 1 %. Observed power reductions are 29 %, 29 %, 19 %, and 20 % for error rates of 2 %, 4 %, 8 %, and 16 % respectively. Benefits are higher in the face of error recovery using Razor. Area overhead of our techniques is up to 2. 7 %. I...|$|E
40|$|Abstract. In this {{abstract}} {{we present}} a rigorous numerical algorithm which solves initial-value problems (IVPs) defined with polynomial dif-ferential equations (i. e. IVPs of the type y ′ = p(t, y), y(t 0) = y 0, where p is a vector of polynomials) for any value of t. The inputs of the algorithm are the data defining the initial-value problem, the time T at which we want to compute {{the solution of the}} IVP, and the <b>maximum</b> <b>allowable</b> <b>error</b> ε> 0. Using these inputs, the algorithm will output a value ỹT such that ‖ỹT − y(T) ‖ ≤ ε in time polynomial in T, − log ε, and in several quantities related to the polynomial IVP. ...|$|E
30|$|The most {{important}} quantitative attibute is the diameter at breast height (dbh) of sample trees, which is measured using a diameter tape. The breast height {{is defined as}} 1.3  m, and the minimum dbh is 5.0  cm measured over bark. The <b>maximum</b> <b>allowable</b> measurement <b>error</b> (tolerance) for dbh measurement is 3  mm or 1.5  % (for trees with a dbh exceeding 20  cm).|$|R
40|$|The NASA Generic Transport Model (GTM) {{nonlinear}} simulation {{was used}} to investigate the effects of errors in sensor measurements, mass properties, and aircraft geometry on the accuracy of identified parameters in mathematical models describing the flight dynamics and determined from flight data. Measurements from a typical flight condition and system identification maneuver were systematically and progressively deteriorated by introducing noise, resolution errors, and bias errors. The data were then used to estimate nondimensional stability and control derivatives within a Monte Carlo simulation. Based on these results, recommendations are provided for <b>maximum</b> <b>allowable</b> <b>errors</b> in sensor measurements, mass properties, and aircraft geometry to achieve desired levels of dynamic modeling accuracy. Results using additional flight conditions and parameter estimation methods, {{as well as a}} nonlinear flight simulation of the General Dynamics F- 16 aircraft, were compared with these recommendation...|$|R
40|$|It {{is shown}} here that during normal on-orbit {{operations}} the TOPEX low-earth orbiting satellite {{is subjected to}} an impulsive disturbance torque caused by rapid heating of its solar array when entering and exiting the earth's shadow. Error budgets and simulation results are used to demonstrate that this sunrise/sunset torque disturbance is the dominant Normal Mission Mode (NMM) attitude error source. The detailed thermomechanical modeling, analysis, and simulation of this torque is described, and the predicted on-orbit performance of the NMM attitude control system {{in the face of}} the sunrise/sunset disturbance is presented. The disturbance results in temporary attitude perturbations that exceed NMM pointing requirements. However, they are below the <b>maximum</b> <b>allowable</b> pointing <b>error</b> which would cause the radar altimeter to break lock...|$|R
40|$|To {{replace the}} {{traditional}} anthropometric data collection processes with the 3 D acquiring system {{it is important}} that the validity of the data is not compromised. To do this, a validation study, based on the guideline of ISO 20685, can be performed. This paper presents the results of a comparison between traditional measurements and measurements taken with a 3 D acquiring system using only four Kinect sensors. The results obtained were then compared with the <b>maximum</b> <b>allowable</b> <b>error</b> indicated in ISO 20685, concluding that this system cannot give sufficiently reliable data that can substitute the manual procedures. FEDER funds through the Competitive Factors Operational Program (COMPETE) and by national funds through FCT (Portuguese Foundation for Science and Technology) with the projects PEst- C/CTM/U 10264 and ID/CEC/ 00319 / 201...|$|E
40|$|AbstractDuring the {{production}} of silicon solar cells crack detection systems can help to sort out damaged wafers and reduce wafer breakage before they enter {{the production}} line. In order to be cost effective, the crack detection system needs to minimize false detections as much as possible. False detections in crack detection systems occur when bad wafers are not detected or when good wafers are falsely detected as bad. The first error leads {{to an increase in}} cell breakage, the second error raises cell costs because non-damaged wafers are sorted out prior to cell processing. In this work a model has been developed to calculate the <b>maximum</b> <b>allowable</b> <b>error</b> rates of crack detection systems in order to achieve a cost per wafer benefit. Therefore a breakage rate dependent throughput calculation, based on manufacturing data, has been implemented. A sensitivity analysis shows that avoiding a high sorting out rate is crucial to favor the implementation of a crack detection system...|$|E
40|$|Artificial Intelligence Lab, Department of MIS, University of ArizonaThis paper {{presents}} a neural network approach to document semantic indexing. A Hopfield net algorithm {{was used to}} simulate human associative memory for concept exploration {{in the domain of}} computer science and engineering. INSPEC, a collection of more than 320, 000 document abstracts from leading journals, was used as the document testbed. Benchmark tests confirmed that three parameters (maximum number of activated nodes, E - <b>maximum</b> <b>allowable</b> <b>error,</b> and maximum number of iterations) were useful in positively influencing network convergence behavior without negatively impacting central processing unit performance. Another series of benchmark tests was performed to determine the effectiveness of various filtering techniques in reducing the negative impact of noisy input terms. Preliminary user tests confirmed our expectation that the Hopfield net algorithm is potentially useful as an associative memory technique to improve document recall and precision by solving discrepancies between indexer vocabularies and end-user vocabularies...|$|E
40|$|This thesis {{examines}} {{a hybrid}} DS/FH-CDMA system employing FSK based modulation schemes. The proposed modulation schemes are: non-coherent MFSK, {{a combination of}} MFSK and DPSK (called MFSK-DPSK) and wideband multitone (MT) FSK. In each case, the signal to be transmitted is modulated by a high rate BPSK signal (the PN sequence) and then it is hopped at a rate higher than the symbol rate into different frequency bins that are sufficiently spaced so that the fading in each channel {{appears to be an}} independent process. The main difference between each system is that a DS/FH-CDMA system employing wideband MT-FSK must employ fewer frequency bins to have the same (or comparable) bandwidth as systems employing MFSK or MFSK-DPSK. However, the advantage gained by using MT-FSK is the inherent diversity of the modulation scheme. The bit error rate performance of each modulation scheme in a Rayleigh fading channel is found (in some cases, upper bounds are used). Both frequency-nonselective and frequency-selective fading are considered. A hybrid DS/FH-CDMA system is presented. The multiple access interference is modelled as additional white Gaussian noise. Spectral efficiency expressions are obtained for the system employing the different modulation and coding schemes considered. It is shown that the spectral efficiency of the system is inversely proportional to the bit energy to noise spectral density ratio required to achieve the <b>maximum</b> <b>allowable</b> bit <b>error</b> rate. Furthermore, it is shown that coded MFSK and coded MFSK-DPSK can guarantee lower bit energy to noise spectral density ratio required to achieve the <b>maximum</b> <b>allowable</b> bit <b>error</b> rate of 10 - 3 than coded MT-FSK, and thus lower bounds on the spectral efficiency of DS/FH-CDMA systems employing coded MFSK or MFSK-DPSK are higher than DS/FH-CDMA systems employing coded MT-FSK...|$|R
30|$|The {{prediction}} error of Dolphin Imaging VTO was analyzed by tabulating the error frequency of subjects {{within the range}} of acceptable error in both the X-axis (Table  2) and Y-axis (Table  3). Three categories (0.5, 1.0, and 2.0  mm) were used to analyze the data based on increasing allowance of error. Two millimeter has been cited as the <b>maximum</b> <b>error</b> <b>allowable</b> before it does not have any value to the patient or clinician [5].|$|R
40|$|Current noninvasive blood {{pressure}} (BP) measurement methods estimate the systolic and diastolic {{blood pressure}} (SBP and DBP) at two random instants in time. The BP variability and its serious consequences on the measurement are not recognized by most physicians. The standard for automated BP devices sets a <b>maximum</b> <b>allowable</b> system <b>error</b> of +/- 5 mmHg, even though natural BP variability often exceeds these limits. This thesis characterizes the variability of SBP and DBP and proposes {{a new approach to}} augment the conventional noninvasive measurement using simultaneous recordings of the oscillometric and continuous arterial pulse waveforms by providing: 1) The mean SBP (or DBP) over the measurement interval, 2) Their respective standard deviations, and 3) An indicator {{as to whether or not}} the oscillometric reading is an outlier. Recordings with healthy subjects showed that the approach has prominent potential and does not suffer from bias relative to the conventional method...|$|R
40|$|For {{a country}} that {{is in the process of}} {{integration}} into EU structures, reducing the corruption is a good sign for attracting foreign investment and developing the economic environment. The paper estimates the parameters of a simultaneous equation model based on data sets obtained at a sample of employees in public administration. Statistical sample consist in 407 people and the <b>maximum</b> <b>allowable</b> <b>error</b> was estimated at ± 2. 5 %. For the effective development of the statistical questionnaire we identified major themes of public administration that are directly related to the problem of corruption: managing the institution, the civil service, transparency in the system, the decentralization process, causes and effects of corruption and the quality of the reform in the public administration. Based on the questionnaire we defined primary and secondary variables that have been used to define the model with simultaneous equations. For the variables in the model they have been divided into endogenous and exogenous...|$|E
40|$|During the {{production}} of silicon solar cells crack detection systems can help to sort out damaged wafers and reduce wafer breakage before they enter {{the production}} line. In order to be cost effective, the crack detection system needs to minimize false detections as much as possible. False detections in crack detection systems occur when bad wafers are not detected or when good wafers are falsely detected as bad. The first error leads {{to an increase in}} cell breakage, the second error raises cell costs because non-damaged wafers are sorted out prior to cell processing. In this work a model has been developed to calculate the <b>maximum</b> <b>allowable</b> <b>error</b> rates of crack detection systems in order to achieve a cost per wafer benefit. Therefore a breakage rate dependent throughput calculation, based on manufacturing data, has been implemented. A sensitivity analysis shows that avoiding a high sorting out rate is crucial to favor the implementation of a crack detection system...|$|E
40|$|Transport Modelling is {{capital for}} {{transportation}} planning. All-or-Nothing Traffic Assignment is used quite a lot. The model {{can not be}} considered valid without passing through validation and calibration process. A validation and calibration method, for the all-or-nothing traffic assignment, need to be developed. The research produced simple practical validation and calibration method. The calculated model traffic volumes are compared against the real traffic volumes. Simple validation method was developed by just setting a <b>maximum</b> <b>allowable</b> <b>error,</b> measured in precentage. Simple calibration method consists of correcting the model traffic volume by correcting the corresponding OD Matrix cell values. The calibration consists of 4 basic tasks : 1. identifying the traffic volume need to be calibrated, 2. identifying corresponding OD matrix cells corresponding to the traffic volume need to be calibrated, 3. distributing traffic volume error to the coresponding OD matrix cells, and 4. developing the calibrated OD matrix. Validation and Calibration are a pair of an iterative process...|$|E
40|$|Digital {{data hiding}} is a {{technology}} being developed for multimedia services, where {{significant amounts of}} secure data is invisibly hidden inside a host data source by the owner, for retrieval only by those authorized. The hidden data should be recoverable even after the host has undergone standard transformations such as compression. In this work, we present a source and channel coding framework for data hiding, allowing any trade-off between the visibility of distortions introduced, the amount of data embedded, {{and the degree of}} robustness to noise. The secure data is source coded by vector quantization, and the indices obtained in the process are embedded in the host video using orthogonal transform domain vector perturbations. Transform coefficients of the host are grouped into vectors and perturbed using noise-resilient channel codes derived from multidimensional lattices. The perturbations are constrained by a <b>maximum</b> <b>allowable</b> mean-squared <b>error</b> that can be introduced in the ho [...] ...|$|R
40|$|The {{far-field}} {{beam pattern}} and the power-collection efficiency are calculated for a multistage laser-diode-array amplifier consisting of about 200, 000 5 -W laser diode arrays with random distributions of phase and orientation errors and random diode failures. From the numerical calculation {{it is found}} that the far-field beam pattern is little affected by random failures of up to 20 percent of the laser diodes with reference of 80 percent receiving efficiency in the center spot. The random differences in phases among laser diodes due to probable manufacturing errors is allowed to about 0. 2 times the wavelength. The <b>maximum</b> <b>allowable</b> orientation <b>error</b> is about 20 percent of the diffraction angle of a single laser diode aperture (about 1 cm). The preliminary results indicate that the amplifier could be used for space beam-power transmission with an efficiency of about 80 percent for a moderate-size (3 -m-diameter) receiver placed at a distance of less than 50, 000 km...|$|R
40|$|AbstractA {{simulated}} {{field test}} {{was designed to}} determine whether the Holmes–Wright A lantern (HWA) is a valid color vision test for the rail industry. The simulation replicated viewing rail signal lights at 0. 8 km distance under daylight conditions. Using the worst-normal as the <b>maximum</b> number of <b>allowable</b> <b>errors</b> on the simulation, 94 % of the color-defectives failed both tests on the first trial and 92 % failed at the second session. The HWA had a higher false negative rate than a false alarm rate. The majority of individuals who had discrepancies on the two tests were mild deutans. Results from the Ishihara test were marginally better at predicting performance on the simulation...|$|R
40|$|Several {{satellite}} latent fault modes {{can cause}} errors {{that may not}} be removed by pseudorange corrections or be detected unless special augmentation-based integrity monitors are employed. The error classes resulting from satellite fault modes are distorted signal waveform, uncorrectable ephemeris error, excessive clock acceleration, excessive code-carrier divergence, and low signal power. The paper derives the integrity monitoring parameters for these errors through mathematical analyses and error bounding rationales. The integrity parameters are the <b>maximum</b> <b>allowable</b> <b>error</b> in the pseudorange corrections and the probability of missed detection of this error. The inputs to the analyses are the integrity monitoring operational requirements contained in the Minimum Aviation System Performance Standards for the Local Area ∗ The contents of this material reflect the views of the authors. Neither the Federal Aviation Administration nor the Department of Transportation makes any warranty or guarantee, or promise, expressed or implied concerning the content or accuracy of the views expressed herein. 1 Augmentation System (LAAS). The results of the analyses are used as the basic performance requirements for ranging source integrity monitoring in the FAA’s LAAS Ground Facility specification. The paper includes model derivations and an example application to the signal waveform distortion error class...|$|E
40|$|Abstract—Modern digital IC designs have a {{critical}} operating point, or “wall of slack”, that limits voltage scaling. Even with an error-tolerance mechanism, scaling voltage below {{a critical}} voltage- so-called overscaling- results in more timing errors {{than can be}} effectively detected or corrected. This limits the effectiveness of voltage scaling in trading off system reliability and power. We propose a design-level approach to trading off reliability and voltage (power) in, e. g., microprocessor designs. We increase the range of voltage values at which the (timing) error rate is acceptable; we achieve this through techniques for power-aware slack redistribution that shift the timing slack of frequently-exercised, near-critical timing paths in a power- and area-efficient manner. The resulting designs heuristically minimize the voltage at which the <b>maximum</b> <b>allowable</b> <b>error</b> rate is encountered, thus minimizing power consumption for a prescribed maximum error rate and allowing the design to fail more gracefully. Compared with baseline designs, we achieve a maximum of 32. 8 % and an average of 12. 5 % power reduction at an error rate of 2 %. The area overhead of our techniques, as evaluated through physical implementation (synthesis, placement and routing), {{is no more than}} 2. 7 %. I...|$|E
40|$|Abstract The <b>maximum</b> <b>allowable</b> <b>error</b> rate, Qmax ' is {{the value}} of In the {{entangled}} translucent eavesdropping scenario of key the error rate in the Alice-Bob channel, for which the mu-generation in quantum cryptography, I demonstrate that tual information in the Bob-Eve channel is unity, namely the unsafe error rate based on standard mutual informa- [1], tion comparisons is equivalent to the maximum allowable Qmax = Q such that IBE = 1. (3) error rate based on perfect mutual information for the This corresponds to perfect information for the eaves-eavesdropper. In this case, the unsafe error rate is not in dropper. fact overly conservative, as is commonly supposed. I recently obtained new closed-form algebraic expressions Introduction for the error rates and mutual information, expressed only in terms of the POVM receiver error rate, Q, and the angle In a popular scheme for entangled translucent eavesdrop- Obetween the carrier polarization states [3, 4]. To do this, ping in quantum cryptography, the key generation proce- I employed the quantum mechanical unitarity conditions dure involves the transmission, interception, and recep- that must be satisfied by the eavesdropping device param...|$|E
40|$|In {{this work}} the authors {{deal with the}} problem {{regarding}} tolerance design of the journal – bearing kinematic joint, in which hydrodynamic lubrication conditions are realised. In particular, they propose a simplified numerical approach to analyse consequences of macro geometric variations on the performances of this kind of bearings. The final result is numerical formulation of new abacuses {{to be used in the}} design of such kinematic joints that show how the kinematic joint performances change when the functional feature has not the ideal shape. The designer can use the abacus starting from a performance objective and evaluating the <b>maximum</b> <b>allowable</b> macro geometric <b>error</b> (i. e. the tolerance value) that satisfies the functional requirements...|$|R
50|$|Per the IEC standard, {{accuracy}} classes for {{various types of}} measurement are defined in IEC 61869-1 as Classes 0.1, 0.2s, 0.2, 0.5, 0.5s, 1 and 3. The class designation is an approximate measure of the CT's accuracy. The ratio (primary to secondary current) error of a Class 1 CT is 1% at rated current; the ratio error of a Class 0.5 CT is 0.5% or less. Errors in phase are also important especially in power measuring circuits. Each class has an <b>allowable</b> <b>maximum</b> phase <b>error</b> for a specified load impedance.|$|R
40|$|In this paper, {{we present}} human pose {{estimation}} and gesture recognition algorithms that use only depth information. The proposed methods {{are designed to}} be operated with only a CPU (central processing unit), so that the algorithm can be operated on a low-cost platform, such as an embedded board. The human pose estimation method is based on an SVM (support vector machine) and superpixels without prior knowledge of a human body model. In the gesture recognition method, gestures are recognized from the pose information of a human body. To recognize gestures regardless of motion speed, the proposed method utilizes the keyframe extraction method. Gesture recognition is performed by comparing input keyframes with keyframes in registered gestures. The gesture yielding the smallest comparison error is chosen as a recognized gesture. To prevent recognition of gestures when a person performs a gesture that is not registered, we derive the <b>maximum</b> <b>allowable</b> comparison <b>errors</b> by comparing each registered gesture with the other gestures. We evaluated our method using a dataset that we generated. The experiment results show that our method performs fairly well and is applicable in real environments...|$|R
