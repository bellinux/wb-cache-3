74|299|Public
50|$|The AVS is {{one kind}} of {{collocated}} <b>multiple</b> <b>microphone</b> array, it makes use of a <b>multiple</b> <b>microphone</b> array approach for estimating the sound directions by multiple arrays and then finds the locations by using reflection information such as where the direction is detected where different arrays cross.|$|E
5000|$|... #Subtitle level 4: Learning how {{to apply}} <b>Multiple</b> <b>Microphone</b> Array ...|$|E
5000|$|The {{location}} of a sound source is determined through three-dimensional sound localization using <b>multiple</b> <b>microphone</b> arrays, binaural hearing methods, and HRTF (head-related transfer function).|$|E
30|$|In contrast, in this contribution, we want {{to focus}} on {{distributed}} microphones, where the arrangement is not limited to fixed geometries but where each speaker in the car cabin has a dedicated microphone close to his position. In the case at hand with <b>multiple</b> <b>microphones</b> and <b>multiple</b> speakers to be supported, the sensor signals have to be combined in a beneficial way. In the literature, it is often focussed on setups where <b>multiple</b> <b>microphones</b> are used to capture the speech signal of one single speaker. In this case, <b>multiple</b> spatially distributed <b>microphones</b> may be mounted in the direct vicinity of just one speaker in order to search for the optimal microphone position.|$|R
50|$|Voxeet's {{technology}} reproduces {{these natural}} mechanisms using audio {{picked up by}} <b>multiple</b> <b>microphones.</b> Voxeet builds a virtual 3D audio space that mimics natural sounds without extra effort from the brain.|$|R
40|$|In {{this paper}} {{we present a}} new method of signal {{processing}} for robust speech recognition using <b>multiple</b> <b>microphones.</b> The method, loosely based on the human binaural hearing system, consists of passing the speech signals detected by <b>multiple</b> <b>microphones</b> through bandpass filtering and nonlinear rectification operations, and then cross-correlating the outputs from each channel within each frequency band. These operations provide {{an estimate of the}} energy contained in the speech signal in each frequency band, and provides rejection of off-axis jamming noise sources. We demonstrate that this method increases recognition accuracy for a multi-channel signal compared to equivalent processing of a monaural signal. 1...|$|R
5000|$|... xCORE-VOICE - Launched in April 2016. xCORE-VOICE {{processors}} are {{a combination}} {{of one or more}} xCORE-200 processors with software that enables capture of <b>multiple</b> <b>microphone</b> signals, and aggregate those into a single signal.|$|E
50|$|The Levelator {{adjusts the}} audio levels within an audio segment by {{combining}} traditional discrete compression, normalization and limiting processing. By taking a global {{view of the}} data in various time segments (both long and short), the Levelator automatically balances various audio levels, such as <b>multiple</b> <b>microphone</b> levels in an interview or panel discussion, or segments combined from multiple sessions that were recorded at different levels. The Levelator can read and process PCM audio files of many sample rates and resolutions.|$|E
50|$|Many {{telephones}} have {{an integrated}} speakerphone function {{which can be}} activated by pushing a single button. This button transfers the sound input and output from the handset to the ambient microphone and loudspeaker. Devices designed specifically for speakerphone use often have <b>multiple</b> <b>microphone</b> inputs arranged radially around the device to maximize sound input, such as may occur around a conference table. The most sophisticated units allow the connection of additional satellite microphones that can be placed {{some distance from the}} main unit.|$|E
25|$|Automixer {{hardware}} or software uses {{a variety}} of methods that allow increased gain before feedback for live sound reinforcement as well as reducing comb filtering between <b>multiple</b> <b>microphones</b> for recorded and broadcast applications.|$|R
40|$|In {{this paper}} {{we present a}} new method of signal {{processing}} for robust speech recognition using <b>multiple</b> <b>microphones.</b> The method, based on human binaural hearing, consists of passing the speech signals detected by <b>multiple</b> <b>microphones</b> through bandpass filtering and nonlinear rectification operations, and then cross-correlating the outputs from each channel within each frequency band. These operations provide {{an estimate of the}} energy contained in the speech signal in each frequency band, and provides rejection of off-axis jamming noise sources. We demonstrate that this method increases recognition accuracy for a multi-channel signal compared to equivalent processing of a monaural signal, and compared to processing using simple delay-and-sum beamforming. 1...|$|R
5000|$|... xCORE Voice is {{a series}} of {{products}} produced by XMOS that contain all the algorithms necessary to aggregrate data from <b>multiple</b> <b>microphones,</b> and process the signal to produce an audio signal suitable for the humans of automated speech recognition systems.|$|R
50|$|EAW {{developed}} a digital mixing console prototype in 2005, the UMX.96; a console which incorporated SmaartLive 5 internally. Any selected channel on the mixer {{could be used}} as a source for Smaart analysis, displaying, for instance, the real-time results of channel equalization. The console could be configured to send <b>multiple</b> <b>microphone</b> inputs to Smaart, and it offered constant metering of sound pressure level in decibels. When it was put into production in 2007, band engineer Don Dodge took the mixer out on a world tour with Foreigner, the first concert mixed in March 2007. With its 15-inch touchscreen able to serve both audio control and Smaart analysis functions, Dodge continued to mix Foreigner on it throughout 2007 and 2008.|$|E
3000|$|... samp is a {{sampling}} frequency. To improve {{the performance of the}} original CSP method, we used a peak-hold process [31] and noise component suppression, which sets the cross-power spectrum to zero when the estimated signal-to-noise ratio (SNR) is below 0 dB [5]. Synchronous addition of <b>multiple</b> <b>microphone</b> pair-wise CSP coefficients reduces the noise influence [32].|$|E
40|$|Copyright Â© 2005 Acoustical Society of AmericaThis paper {{builds on}} earlier {{work by the}} same authors to derive {{expressions}} for the time-averaged acoustic energy density in the frequency domain using the auto- and cross-spectral densities of <b>multiple</b> <b>microphone</b> elements. Expressions for the most common three-dimensional geometric arrangements are derived. Simplified expressions for use with two channel spectrum analysers are also presented. Ben S. Cazzolato and Justin Gha...|$|E
25|$|Windows Vista {{includes}} integrated {{microphone array}} support which {{is intended to}} increase {{the accuracy of the}} speech recognition feature and allow a user to connect <b>multiple</b> <b>microphones</b> to a system so that the inputs can be combined into a single, higher-quality source.|$|R
50|$|Category 2 {{would include}} those live signals {{obtained}} using <b>multiple</b> <b>microphones</b> {{to capture the}} acoustic properties of a room.The signals oriented for generate acoustic fields, obtained by a linear grouping of microphones, belong to this group. Such signalshave a very high cross correlation between all channels.|$|R
40|$|The {{separation}} of speech signals measured at <b>multiple</b> <b>microphones</b> in noisy and reverberant environ-ments using only the audio modality has limitations {{because there is}} generally insufficient information to discriminate fully the different sound sources. Humans mitigate this problem by exploiting the visual modality which is insensitive to background noise and can provide contextual information about the audi...|$|R
40|$|This paper {{presents}} a minimum mean-square error spectral phase estimator for speech enhancement in the distributed <b>multiple</b> <b>microphone</b> scenario. The estimator uses Gaussian models {{for both the}} speech and noise priors under the assumption of a diffuse incoherent noise field representing ambient noise in a widely dispersed microphone configuration. Experiments demonstrate significant benefits of using the optimal multichannel phase estimator {{as compared to the}} noisy phase of a reference channel...|$|E
40|$|Time delay {{estimation}} (TDE) is {{an important}} part of sound source localization in hands-free communication systems. TDE in room acoustic environments is usually affected by reverberation. To improve the robustness of time delay estimators to reverberation, a novel TDE algorithm is proposed from information theory perspective. The non-mutual information among <b>multiple</b> <b>microphone</b> signals is extracted to estimate the time delay. Superiority of the proposed algorithm is demonstrated in reverberant environments...|$|E
30|$|Time {{difference}} of arrival (TDOA) is another widely used TDE-based source localization method. The method is a two-step procedure. In the first stage, the time {{difference of}} signal arrival between a pair of microphones is estimated. With {{the knowledge of the}} propagation velocity of sound, the estimated TDOA measurement is transformed into range difference measurement from which hyperbolic range difference equation is formed. The second stage utilizes efficient algorithms to produce an unambiguous solution to the hyperbolic equations obtained from <b>multiple</b> <b>microphone</b> pairs. The solution produced by these algorithms result in the estimated source location.|$|E
50|$|Amongst {{their various}} alumni {{are a number}} of {{recording}} engineers. QSound was developed by Dan Lowe after experimenting in a recording session that involved <b>multiple</b> <b>microphones</b> set up around a studio. The technology was used on recordings in the 80s by Pink Floyd, Sting, Madonna and other noted artists and is currently being used in cellphones.|$|R
40|$|In {{this paper}} {{we present a}} new method of signal {{processing}} for robust speech recognition using <b>multiple</b> <b>microphones.</b> The method, loosely based on the human binaural hearing system, consists of passing the speech signals detected by <b>multiple</b> <b>microphones</b> through bandpass filtering and nonlinear halfwave rectification operations, and then cross-correlating the outputs from each channel within each frequency band. These operations provide rejection of off-axis interfering signals. These operations are repeated (in a non-physiological fashion) for the negative of the signal, and {{an estimate of the}} desired signal is obtained by combining the positive and negative outputs. We demonstrate that the use of this approach provides substantially better recognition accuracy than delay-and-sum beamforming using the same sensors for target signals in the presence of additive broadband and speech maskers. Improvements in reverberant environments are tangible but more modest. Index Terms: robust speech recognition, binaural hearing, auditory processing, speech enhancemen...|$|R
40|$|When <b>multiple</b> <b>microphones</b> are {{available}} estimates of inter-channel delay, which characterise a speakerâs location, {{can be used}} as features for speaker diarization. Background noise and reverberation can, however, lead to noisy features and poor performance. To ameliorate these problems, this paper presents a new approach to the discriminant analysis of delay features for speaker diarization. This novel and nonetheless unsupervised approach aims to increase speaker separability in delay-space. We assess the approach on subsets of four standard NIST RT datasets and demonstrate a relative improvement in diarization error rate of 25 % on a separate evaluation set using delay features alone. Index Terms â Speaker diarization, <b>multiple</b> distant <b>microphones</b> 1...|$|R
3000|$|The paper {{entitled}} [...] "Rate-constrained beamforming in {{binaural hearing}} aids" [...] (S. Srinivasan et al.) addresses hearing aid systems, where {{the left and}} right ear devices collaborate with each other. Binaural beamforming for hearing aids requires an exchange of microphone signals between the two devices over a wireless link. In this contribution, two issues are investigated: which <b>multiple</b> <b>microphone</b> signals to transmit from one ear to the other, and at what bit-rate. Obviously, the second problem is relevant as the capacity of the wireless link is limited by stringent power consumption constraints.|$|E
40|$|A {{new method}} to assess noise {{reduction}} algorithms {{with respect to}} their ability to enhance the perceived quality of speech is pre-sented. Such algorithms consist of both single-microphone sys-tems and <b>multiple</b> <b>microphone</b> systems. Tests of the presented method show a higher correlation with subjective assessments than any other objective system known by the authors. It is be-lieved that this method is suitable to improve the comparability between noise reduction algorithms. Another area of applica-tion could be the optimization of parameters in a noise reduction algorithm, as well as the optimization of the geometric micro-phone positioning. 1...|$|E
40|$|A <b>multiple</b> <b>microphone</b> {{time varying}} filter {{that is an}} {{extension}} of the dual-microphone speech enhancement technique of [7] is proposed and experimentally analyzed. The technique utilizes information regarding the locations of the speech source of interest and the microphones to compute a time varying filter that results in substantial noise reduction over other speech enhancement techniques such as delayand -sum beamformingand superdirective beamforming. For example, digit recognition results in an environment with two speakers and a reverberation time of 0. 1 s show a recognition accuracy rate increase of 25. 2 % over delay-and-sum beamforming and an increase of 26. 5 % over superdirective beamforming using six microphones...|$|E
40|$|In {{this paper}} we {{introduce}} a novel method for the visualization of speech disorders. We demonstrate the method with disor-dered speech and a control group. However, {{both groups were}} recorded using two different microphones. The projection of the patient data using a single microphone yields significant corre-lations between the coordinates on the map and certain criteria of the disorder which were perceptually rated. However, projec-tion of data from <b>multiple</b> <b>microphones</b> reduces this correlation. Usually, the acoustical mismatch between the microphones {{is greater than the}} mismatch between the speakers, i. e., not the disorders but the microphones form clusters in the visualiza-tion. Based on an extension of the Sammon mapping, we are able to create a map which projects the same speakers onto the same position even if <b>multiple</b> <b>microphones</b> are used. Further-more, our method also restores the correlation between the map coordinates and the perceptual assessment. Index Terms: visualization, robustness, speech processing. 1...|$|R
40|$|Abstract:- The {{benefits}} {{of a network of}} cooperative multi-modal sensors are presented in the contexts of three different human-computer interaction systems. First, a robust automatic teleconferencing system with <b>multiple</b> <b>microphones</b> and cameras is used to illustrate the {{benefits of}} multi-modal sensor fusion. The integrated speaker localization system doubles the original accuracy of the individual sensors and functions correctly at signal-to-noise ratios as low as 0. 5 dB. Next, <b>multiple</b> <b>microphones</b> are used to improve speaker independent speech recognition, enabling a word recognition rate of approximately 90 % at signal-to-noise ratios as low as â 20 dB. Finally, several dynamic microphone arrays (DMA), which, when attached to a Personal Digital Assistant (PDA) or other appropriate processing device, can roam freely in the environment, use the active speakers in the environment as beacons in order to localize and track their own locations and that of their corresponding PDA. Key-Words:- human computer interaction, sensor fusion, multi-modal perception...|$|R
40|$|In this paper, {{we examine}} the {{challenging}} problem of detecting acoustic events and voice activity in smart indoors environments, equipped with <b>multiple</b> <b>microphones.</b> In particular, we focus on channel combination strategies, aiming {{to take advantage of}} the <b>multiple</b> <b>microphones</b> installed in the smart space, capturing the potentially noisy acoustic scene from the far-field. We propose various such approaches that can be formulated as fusion at the signal, feature, or at the decision level, as well as combinations of the above, also including multi-channel training. We apply our methods on two multi-microphone databases: (a) one recorded inside a small meeting room, containing twelve classes of isolated acoustic events; and (b) a speech corpus containing interfering noise sources, simulated inside a smart home with multiple rooms. Our multi-channel approaches demonstrate significant improvements, reaching relative error reductions over a single-channel baseline of 9. 3 % and 44. 8 % in the two datasets, respectively. Â© 2014 EURASIP...|$|R
40|$|We {{propose a}} novel {{principle}} based on Complex Spectrum Circle Centroid (CSCC) for restoring complex {{spectrum of the}} target signal from <b>multiple</b> <b>microphone</b> input signals in a noisy environment. If noise arrives at multiple microphones with different time delays relative to the target signal, the observed noisy signals lie on a circle in the complex spectrum plane from which the target signal is restored by finding the centroid of the circle. Unlike most of existing methods for noise reduction such as ICA, AMNOR and beamforming, this nonlinear operation is applicable to any type of noise including non-stationary, moving, signal-correlated, nonplanar, and spoken noises, without identifying the noise direction and training parameters...|$|E
40|$|We {{propose a}} spatial diffuseness feature for {{deep neural network}} (DNN) -based {{automatic}} speech recognition to improve recognition accuracy in reverberant and noisy environments. The feature is computed in real-time from <b>multiple</b> <b>microphone</b> signals without requiring knowledge or estimation of the direction of arrival, and represents the relative amount of diffuse noise in each time and frequency bin. It is shown that using the diffuseness feature as an additional input to a DNN-based acoustic model leads to a reduced word error rate for the REVERB challenge corpus, both compared to logmelspec features extracted from noisy signals, and features enhanced by spectral subtraction. Comment: accepted for ICASSP 201...|$|E
40|$|We propose an {{improved}} time-domain Blind Source Separa-tion method {{and apply it}} to speech signal enhancement using <b>multiple</b> <b>microphone</b> recordings. The improvement consists in utilization of fuzzy clustering instead of a hard one, which is verified by experiments where real-world mixtures of two au-dio signals are separated from two microphones. Performance of the method is demonstrated by recognizing mixed and sepa-rated utterances from the Czech part of the European broadcast news database using our Czech LVCSR system. The separation allows significantly better recognition, e. g., by 32 % when the jammer signal is a Gaussian noise and the input signal-to-noise ratio is 10 dB...|$|E
30|$|We {{evaluated}} the medium-sized vocabulary continuous speech recognition {{task of the}} REVERB challenge in order to validate the effectiveness of single-channel dereverberation and multi-channel beamforming techniques and discriminative training of acoustic model and feature transformation in reverberant environments. For speech enhancement, experiments show the effectiveness of dereverberation of the late reverberation components, and beamforming using <b>multiple</b> <b>microphones</b> that enhances direct sounds compared to the reflected sounds.|$|R
25|$|Shure {{introduced}} the Automatic Microphone System (AMS) in 1983, {{one of the}} first automatic, high-quality mixer system using directional gating for installations utilizing <b>multiple</b> <b>microphones.</b> In 1987, Shure SCM810 Automatic Mixer installations begin at the United States Capitol, and by 1997, the US Capitol was one of the largest Shure automatic mixer installations in the world. In 2008, Shure {{introduced the}} Microflex microphone line specifically designed for conference room applications.|$|R
40|$|AbstractâWhen {{performing}} speaker diarization on recordings from meetings, <b>multiple</b> <b>microphones</b> {{of different}} qualities are usually available and distributed around the meeting room. Although several approaches {{have been proposed}} {{in recent years to}} take advantage of <b>multiple</b> <b>microphones,</b> they are either too computationally expensive and not easily scalable or they can not outperform the simpler case of using the best single microphone. In this work the use of classic acoustic beamforming techniques is proposed together with several novel algorithms to create a complete frontend for speaker diarization in the meeting room domain. New techniques we are present include blind reference-channel selection, two-step Time Delay of Arrival (TDOA) Viterbi postprocessing, and a dynamic output signal weighting algorithm, together with using such TDOA values in the diarization to complement the acoustic information. Tests on speaker diarization show a 25 % relative improvement on the test set compared to using a single most centrally located microphone. Additional experimental results show improvements using these techniques in a speech recognition task. Index Termsâacoustic beamforming, speaker diarization, speaker segmentation and clustering, meetings processing. I...|$|R
