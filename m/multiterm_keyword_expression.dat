0|72|Public
25|$|Some Lisp control {{structures}} are special operators, equivalent to other languages' syntactic <b>keywords.</b> <b>Expressions</b> using these operators {{have the same}} surface appearance as function calls, but differ in that the arguments are not necessarily evaluated—or, {{in the case of}} an iteration expression, may be evaluated more than once.|$|R
25|$|The {{placement}} syntax adds {{an expression}} list {{immediately after the}} new <b>keyword.</b> This <b>expression</b> list is the placement. It can contain any number of expressions.|$|R
40|$|International audienceTo {{organize}} data {{resulting from}} the phenotypic characterization of a library of 30, 000 T-DNA enhancer trap (ET) insertion lines of rice (Oryza sativa L cv. Nipponbare), we developed the Oryza Tag Line (OTL) database ([URL] OTL structure facilitates forward genetic search for specific phenotypes, putatively resulting from gene disruption, and/or for GUSA or GFP reporter gene expression patterns, reflecting ET-mediated endogenous gene detection. In the latest version, OTL gathers the detailed morpho-physiological alterations observed during field evaluation and specific screens in a first set of 13, 928 lines. Detection of GUS or GFP activity in specific organ/tissues in {{a subset of the}} library is also provided. Search in OTL can be achieved through trait ontology category, organ and/or developmental stage, <b>keywords,</b> <b>expression</b> of reporter gene in specific organ/tissue as well as line identification number. OTL now contains the description of 9721 mutant phenotypic traits observed in 2636 lines and 1234 GUS or GFP expression patterns. Each insertion line is documented through a generic passport data including production records, seed stocks and FST information. 8004 and 6101 of the 13, 928 lines are characterized by at least one T-DNA and one Tos 17 FST, respectively that OTL links to the rice genome browser OryGenesDB...|$|R
40|$|Realistic facial {{animation}} {{transfer from}} one individual to others {{has been a}} persistent challenge. In this paper, we present an effective method that transfers facial animation from 2 D videos onto 3 D faces in a visually pleasing manner. Our method {{is based on a}} Laplacian deformation framework. We represent the facial animation with the displacements of a set of feature points. By the assumption that the feature points move only in the X-Y directions, we can map the displacements of the feature points from a 2 D video to a 3 D face. These displacements are used to drive the Laplacian deformation and calculate the deformed positions of the non-feature points on the 3 D face. The approach produces accurate, realistic and smooth transfer. Furthermore, the method is efficient and practical, and the interface is intuitive. The proposed technique outperforms previous methods based on machine learning and anatomy in terms of speed and applicability. Our method is useful {{for a wide range of}} applications, such as, avatars, character animation for 3 D films, computer games, and online chatting. The versatility of our approach is demonstrated by some special effects, such as expression exaggeration and <b>expression</b> imitation. <b>Keywords</b> <b>Expression</b> and speech animation transfer, Laplacian deformation, facial animation, motion capture and retargeting, performance-driven animation 1...|$|R
40|$|Background: In {{order to}} {{generate}} Google search strategies, we used NLP techniques and two separate corpuses to build semantic markers for identifying Argentinean scientists abroad. The first corpus was {{extracted from the}} Web of Science (WoS) : using the bibliographical records of this database we identified the co-authors of papers, their institutional addresses, their subject area and country of residence. A second corpus of 50 documents was manually built {{in order to identify}} <b>keywords</b> and <b>expressions</b> for detecting people’s biographical information on the Web. Results: Using queries built exclusively from bibliographic data, 74 % of the scientists in our test population were found, at an average cost of 21. 3 search engine queries per scientist. Using biographic <b>keywords</b> and <b>expressions,</b> we got better recall (83 %), but at a higher query cost (an average of 252. 2 queries per scientist). Conclusions: This paper reports on work aimed at constantly improving the cost efficiency of combining bibliographic data with biographic keywords when searching for scientist abroad on the Web. A combination of these two type of data seems the best compromise between a maximum scientist recall at a minumum query cost. Background The “semantics of involvement ” question lies at the...|$|R
40|$|To {{organize}} data {{resulting from}} the phenotypic characterization of a library of 30 000 T-DNA enhancer trap (ET) insertion lines of rice (Oryza sativa L cv. Nipponbare), we developed the Oryza Tag Line (OTL) database ([URL] OTL structure facilitates forward genetic search for specific phenotypes, putatively resulting from gene disruption, and/or for GUSA or GFP reporter gene expression patterns, reflecting ET-mediated endogenous gene detection. In the latest version, OTL gathers the detailed morpho-physiological alterations observed during field evaluation and specific screens in a first set of 13 928 lines. Detection of GUS or GFP activity in specific organ/tissues in {{a subset of the}} library is also provided. Search in OTL can be achieved through trait ontology category, organ and/or developmental stage, <b>keywords,</b> <b>expression</b> of reporter gene in specific organ/tissue as well as line identification number. OTL now contains the description of 9721 mutant phenotypic traits observed in 2636 lines and 1234 GUS or GFP expression patterns. Each insertion line is documented through a generic passport data including production records, seed stocks and FST information. 8004 and 6101 of the 13 928 lines are characterized by at least one T-DNA and one Tos 17 FST, respectively that OTL links to the rice genome browser OryGenesD...|$|R
40|$|In {{this paper}} we discuss the aspects of {{designing}} facial expressions for Virtual Humans with a specific culture. We review psychological experiments on cross-cultural perception of emotional facial expressions and look at case studies with Virtual Humans. By identifying the culturally critical issues of data collection and interpretation with both real and virtual humans, we aim at providing a methodological reference and inspiration for further research. Author <b>Keywords</b> Facial <b>expressions,</b> embodied agents, cultural difference, adaptation. ACM Classification Keywords H 5. m. Information interfaces and presentation (e. g., HCI) ...|$|R
40|$|For a data {{management}} system with {{information storage and retrieval}} capabilities a language is needed {{by which a}} user of the system can specify the records he wishes to retrieve and the operations he wishes to perform on these records. The Command and Query Language under discussion was developed to meet these needs for the extended {{data management}} system. Its development was divided into two spheres of responsibility. The first sphere, referred to as the Assembler, centers on the routines needed for accepting and translating user requests. The second sphere centers on those routines needed for executing the translated requests. These routines are called collectively the Interpreter. The design of the Command and Query Language and the implementation of the Assembler is the topic of this report. Basically, the Language enables the user to specify the records by means of the logical and arithmetic <b>expression</b> of <b>keywords.</b> Since program names may be keywords, the user can specify operations (to be performed on records) with <b>keyword</b> <b>expressions</b> as well The design of the Language involves the following steps: (1) Define the requirements of the Language. (2) Define the (external) syntax and semantics of the Language. (3) Design an internal form of the Language to allow efficient processing by the Interpreter. The design and implementation of the Assembler will result in the necessary routines which can check the syntax of the Language and transform the Language from its external syntax to internal form...|$|R
40|$|As search {{applications}} keep gathering new {{and diverse}} information sources, presenting relevant information anchored in time becomes more important. Temporal {{information is available}} in every document either explicitly, e. g., {{in the form of}} temporal expressions, or implicitly in the form of metadata. Recognizing such temporal information and exploiting it for document retrieval and presentation purposes are important features that can significantly improve the functionality of search applications. In this paper, we present an exploratory search interface that uses timelines to present and explore search results. We also describe a prototypical implementation that illustrates the main ideas of our approach. Author <b>Keywords</b> Temporal <b>expressions,</b> temporal document retrieval, use...|$|R
40|$|DNA microarrays {{can be used}} {{to measure}} the {{expression}} levels of thousands of genes simultaneously. In this paper, the gene expression data were processed by a signal processing method. A discrete wavelet transform (DWT) based feature extraction method for cancer classification was introduced, by which micro-array data are transformed into time-scale domain and used as classification features. Finally, some test and comparison experiments for the feature extraction method have been made by using the weighted voting classification scheme[1]. Experiment results show that the correct rate is over 90 % in tumor vs normal classification by using the feature extraction method. <b>Keywords</b> gene <b>expression,</b> discrete wavelet transform, feature extraction, tissue classification...|$|R
40|$|Emotions are {{an often}} {{overlooked}} aspect of work {{since they are}} not included in formal work models. However, they con-tinue provide critical information as well as be part of a rich social context for action. The following study focuses on the expression of emotions {{within the context of a}} particular work environment – an emergency room – and highlights how it is used, why it is invisible in the work, and how it continues to persist through workarounds. These work-arounds provide indications towards the design of sociotech-nical systems to continue to support the expression of invisi-ble emotions. Author <b>Keywords</b> Emotion <b>expressions,</b> documentation, ER, healthcare, articu-lation wor...|$|R
40|$|In {{this paper}} we propose tree {{expressions}} which express transition diagrams of finite dynamical systems by algebraic formula. And we consider the cartesian product of dynamical systems and get the product formula of tree expressions. By the product formula we easily get the tree expression of the product of dynamical systems represented by tree <b>expressions.</b> <b>Keywords</b> : Tree <b>Expression,</b> Product Formula, Dynamical System 1 Introduction In nature there exist many natural dynamical systems and many researchers investigate them. Many artificial dynamical systems are devised by engineers and scientists. They investigate dynamical systems and applied them to many fields. Generally dynamical systems are represented by a pair (X; f) of an non empty set X and a transition function f on X. It is difficult to understand behaviors of a dynamical system (X; f). So {{in order to understand}} behaviors of dynamical systems more easily we describe their transition diagrams which are graphs showing their t [...] ...|$|R
40|$|Abstract. We {{present a}} simple and {{computationally}} feasible method to perform automatic emotional classification of facial expressions. We propose the use of 10 characteristic points (that {{are part of the}} MPEG 4 feature points) to extract relevant emotional information (basically five distances, presence of wrinkles and mouth shape). The method defines and detects the six basic emotions (plus the neutral one) in terms of this information and has been fine-tuned with a data-base of 399 images. For the moment, the method is applied to static images. Application to sequences is being now developed. The extraction of such information about the user is of great interest for the development of new multimodal user interfaces. <b>Keywords.</b> Facial <b>Expression,</b> Multimodal Interface. ...|$|R
50|$|The {{monitoring}} system developed by China {{is not confined}} to the Great Firewall, monitoring is also built into social networks, chat services and VoIP. Private companies are directly responsible to the Chinese authorities for surveillance of their networks to ensure banned messages are not circulated. The QQ application, owned by the firm Tencent, allows the authorities to monitor in detail exchanges between Internet users by seeking certain <b>keywords</b> and <b>expressions.</b> The author of each message can be identified by his or her user number. The QQ application is effectively a giant Trojan horse. And since March 2012, new legislation requires all new users of micro-blogging sites to register using their own name and telephone number.|$|R
5000|$|... class person{ std::string name; int age;public: person (...) : age(5) { } void print (...) const;};void person::print (...) const{ cout << name << [...] ":" [...] << age << endl; /* [...] "name" [...] and [...] "age" [...] are {{the member}} variables. The [...] "this" [...] <b>keyword</b> is an <b>expression</b> whose value is {{the address of}} the object for which the member was invoked. Its type is [...] const person*, because the {{function}} is declared const. */} ...|$|R
40|$|This article {{concerns}} {{the struggle between}} artistic expression and technological innovation. The perspective that is articulated is drawn from the work and of the Interactive Cinema group at the MIT Media Laboratory. Situated at the boundary of evolving technologies and media storytelling, research of the group iterates between shaping and presenting cinematic expressions using emerging technologies and developing the required tools and platforms to support its creation and delivery. This dynamic is integral to collaborative expression on large-scale projects, {{as well as in}} more individual research endeavors such as a current investigation which conjoins new tangible display technologies with interactive stories. <b>Keywords</b> Artistic <b>expression,</b> technological innovation, creativity, elastic movies, tangible displays, research laboratory. &quot;Interactive Cinema reflects the longing of cinema to become something new, something more complex, and something more personal, as if in conversation with an audience. &quot...|$|R
40|$|Short Abstract — How cells sense their {{environment}} using signal transduction pathways {{and respond to}} environmental changes by regulating gene expression is a key problem in systems biology. Here we investigate the coupling dynamics between signal transduction and gene expression in Saccharomyces cerevisiae yeast cells. The research focus on the high-osmolarity glycerol (HOG) signal transduction pathway, {{which is one of}} the mitogen-activated protein kinase (MAPK) pathways in bakers yeast. We are studying single yeast cells to understand variability in signal transduction and gene <b>expression.</b> <b>Keywords</b> — mitogen-activated protein kinase (MAPK...|$|R
40|$|As the Internet {{currently}} {{represents the}} dominating information resource, {{the need for}} more reliable web search engines gets increasing research focus. While many investigations have explored the use of implicit feedback to improve the user search results, no study has yet examined a more direct approach of the user’s mental state disclosure, his facial expressions while examining these results. This paper tries to answer the following question: can we predict document relevance from the user’s facial expressions? The intuition is that the user mental state reflected in his facial expression while examining a relevant document is different from his expression while examining a non-relevant one. A conducted feasibility study revealed that the detected facial expressions corresponding to relevant and non-relevant documents were distinguishable by a trained neural network classifier but many collected data are required to predict the relevance of future documents. <b>Keywords</b> Facial <b>expressions,</b> feature detection and extraction, implicit user feedback, search engines, information retrieval systems. 1...|$|R
40|$|This paper {{focuses on}} the tourism-related opinion mining, {{including}} tourism-related opinion detection and tourist attraction target identification. The experimental data are blog articles labeled as in the domestic tourism category in a blogspace. Annotators were asked to annotate the opinion polarity and the opinion target for every sentence. Different strategies and features have been proposed to identify opinion targets, including tourist attraction <b>keywords,</b> coreferential <b>expressions,</b> tourism-related opinion words, a 2 -level classifier, and so on. We used machine learning methods to train classifiers for tourism-related opinion mining. A retraining mechanism was proposed to obtain the system decisions of preceding sentences as a new feature. The precision and recall scores of tourism-related opinion detection were 55. 98 % and 59. 30 %, respectively, and the scores of tourist attraction target identification among known tourism-related opinionated sentences were 90. 06 % and 89. 91 %, respectively. The overall precision and recall scores were 51. 30 % and 54. 21 %, respectively...|$|R
40|$|Work on analogy {{has been}} done {{from a number of}} {{disciplinary}} perspectives throughout the history of Western thought. This work is a multidisciplinary guide to theorizing about analogy. It contains 1, 406 references, primarily to journal articles and monographs, and primarily to English language material. classical through to contemporary sources are included. The work is classified into eight different sections (with a number of subsections). A brief introduction to each section is provided. <b>Keywords</b> and key <b>expressions</b> of importance to research on analogy are discussed in the introductory material. Electronic resources for conducting research on analogy are listed as well...|$|R
40|$|The {{present study}} {{sets out to}} examine the subject of {{corruption}} and publicity. It states that: 1. publicity can help to uncover and prevent corruption, and take sanctions against it, 2. business and political interests might hinder the media’s effective anti-corruption activities; 3. the media are able to influence the social judgement of the extent and frequency of corruption. The study proposes to investigate, with empirical research, the links between the level of corruption, its media presentation and its social perception. Methods: the prevalence of some <b>keywords</b> and <b>expressions</b> (corruption, bribe, slush-fund, severance pay) were examined in two leading daily political newspapers published in 2009 and 2010. The results were compared to Transparency International’s Corruption Perception Index, published six months later and the corruption ranking, based on it. Results: the number of media reports on corruption increased between 2009 and 2010, {{as well as its}} social perception. Conclusions: if corruption cases are uncovered and presented by the media, thus reducing the chance of corruption, its social perception will increase. corruption, media, social perception...|$|R
40|$|Motivation: Assignment of {{putative}} protein functional annotation by {{comparative analysis}} using pre-defined experimental annotations is performed routinely by molecular biologists. The number and statistical {{significance of these}} assignments remains a challenge {{in this era of}} high-throughput proteomics. A combined statistical method that enables robust, automated protein annotation by reliably expanding existing annotation sets is described. An existing clustering scheme, based on relevant experimental information (e. g., sequence identity, <b>keywords,</b> or gene <b>expression</b> data) is required. The method assigns new proteins to these clusters with a measure of reliability. It can also provide human reviewers with a reliability score for both new and previously classified proteins...|$|R
40|$|The paper {{presents}} a support method for affect analysis of utterances in Japanese. One {{of the problems}} in the system for affect analysis developed by us before was confusing the valence of emotion types in the final stage of analysis. The cause of this problem was extracting from the utterance only the emotive <b>expression</b> <b>keyword</b> without its grammatical context. To solve this problem we enhance the emotion types extraction procedure in the baseline system with grammatical analysis using Contextual Valence Shifters (CVS). CVS are words, or phrases such as "not", "very much " "not quite", which determine the semantic orientation of the valence of emotive expressions...|$|R
40|$|Searching for {{relevant}} text documents {{has traditionally}} been based on <b>keywords</b> and Boolean <b>expressions</b> of them. Often the search results show high recall and low precision, or vice versa. Considerable efforts {{have been made to}} develop alternative methods, but their practical applicability has been low. Powerful methods are needed for the exploration of miscellaneous document collections. The WEBSOM method organizes a document collection on a map display that provides an overview of the collection and facilitates interactive browsing. Interesting documents can be retrieved by a content addressable search of interesting map locations. The interesting locations could also be marked as filters for collecting interesting new documents...|$|R
40|$|The {{expression}} {{delimited by}} [and] (which specifies {{the size of}} an array) shall be an integral constant expression that has a value greater than zero. Support for the optional type qualifiers, the <b>keyword</b> static, the <b>expression</b> not having to be constant, and support for * between [and] in a declarator is new in C 99. C++ Support for the optional type qualifiers, the <b>keyword</b> static, the <b>expression</b> not having to be constant, and * between [and] in a declarator is new in C 99 and is not specified in the C++ Standard. If they delimit an expression (which specifies {{the size of an}} array), the expression shall have an integer type. Commentary The expression is usually thought of, by developers, in terms of specifying the number of elements, not the size. Other Languages Many languages require the value of the expression to be known at translation time, and some (e. g., Ada) allow execution time evaluation of the array bounds, while a few (e. g., APL, Perl, and Common Lisp) support dynamically resizeable arrays. In some languages (e. g., Fortran) there is an implied lower bound of one. The value given in the array declaration is the upper bound and equals the number of elements. Languages in the Pascal family require that a type be given. The minimum and maximum values of this type specifying the lower and upper bounds of the array. This type also denotes the type of the expression that must be used to index that array. For instance, in...|$|R
40|$|DNA microarrays are {{becoming}} a standard tool for determining the role of genes in the regulation of any biological process in an organism. The application of this technology {{for the analysis of}} gene expression creates enormous opportunities for accelerating the pace towards the understanding of living systems and for the identification of target genes and pathways for drug development. However, equally efficient methods need to be developed for upgrading the information content of the large amounts of data generated by microarray experiments. A procedure for extracting patterns of gene expression through the analysis of the architecture of an associative memory neural network is described. Such patterns contain critical information about the gene-networking relationships observed during changes in cell physiology and the onset of diseases. The proposed method has been tested on two different microarray data sets, namely DeRisi's experiment on yeast cultures [10] and Golub's analysis of acute human leukemia molecular profiles [17]. Using these data sets, the neural network structure has been examined to extract relationships among different genes involved in major metabolic pathways and to relate specific genes to different classes of leukemia. <b>Keywords</b> Gene <b>expression</b> data, cDNA microarrays, neural networks, pattern recognition, data mining 1...|$|R
40|$|Traditional {{hierarchical}} text clustering methods {{assume that}} the documents are represented only by “technical information”, i. e., <b>keywords,</b> phrases, <b>expressions</b> and named entities that can be directly extracted from the texts. However, in many scenarios there is an additional and valuable information about the documents which is usually disregarded during the clustering task, such as user-validated tags, annotations and comments from experts, dictionaries and domain ontologies. Recently, Vapnik introduced a new learning paradigm, called LUPI - Learning Using Privileged Information, which allows the incorporation of this additional (privileged) information in a supervised learning setting. We investigated the incorporation of privileged information in unsupervised setting. The key idea in our proposed approach is to extract important relationships among documents represented in the privileged information dimensional space to learn a more accurate metric for text clustering in the technical information space. A thorough experimental evaluation indicates that the incorporation of privileged information through metric learning significantly improves the hierarchical clustering accuracy. São Paulo Research Foundation (FAPESP) (grants 2010 / 20564 - 8, 2011 / 17366 - 2, 2011 / 19850 - 9, 2012 / 13830 - 9, 2013 / 16039 - 3, 2013 / 22547 - 1) PROPP/UFMSCAPESCNP...|$|R
40|$|Available {{computer}} {{sign language}} training aids use for signs demonstration film-loops. It {{is not possible}} now to create user defined sign messages and, accordingly, available sign language training aids are not equipped with education control means. The discussed project is proposing an avatar for signs fixing. This method open the way to collect signs via Internet, to demonstrate it, to choose most popular sign form for some sentences. It is proposed to create special server and Internet site to make available the sign languages bank for common access to view and enrich it. The linguistic studies of sign languages are forming essential part of this project. To translate text to signs is supposed to use text descriptions of sign usage to eliminate the ambiguity. Using this mechanism {{it is possible to}} systematize the signs, study the grammars and conduct other studies of various sign languages. There is demonstrated that solution of sign language translation problem needs same means that classical computing linguistics — natural language queries to full-text databases for different data domain, verbal input and output, the coherent text generation using <b>keywords</b> and <b>expressions.</b> Projected sign speech bank data will be used in creation of multimedia teaching aids and in the system for deaf distant education...|$|R
40|$|This paper {{propose a}} semi {{supervised}} clustering model TPC-IRBM(Two phase clustering-Integrated rule based model) for clustering large data set such as gene expression data. TPC-IRBM works in two phases to cluster the gene expression data set. The proposed model {{is based on}} rule based models CRT,C 5,CHAID and QUEST. In {{the first phase of}} the model 30 % data(which may vary) is extracted to prepare training, testing and validation data (TTV data) using suitable heuristic or neural network based clustering techniques. The output of first phase is used as build the models and generate the rule base fitting to TTV data using aforesaid models. The proposed model is then constructed by selecting and integrating the quality rules of various models using qualifying criteria corresponding to every cluster. The number of quality rules in proposed model is much more compared to that of CRT,C 5,CHAID and QUEST. The performance in terms of accuracy is better compared to the models. Although in some cases Neural Network based models performance is slightly better but a very high cost of complexity for very large data set. <b>Keywords</b> Gene <b>expression</b> clustering,semi supervised clustering, integrated rule based model, two phase clustering,CA 1 region gene expression clustering of rat hippocampus. 1...|$|R
40|$|CafePie is {{a visual}} {{programming}} environment for CafeOBJ, an algebraic specification language based on term rewriting. CafePie shows term rewriting directly by using two types of visualizations: animated cartoon-like and Obi-shaped. A more abstract visualization schema is necessary instead of program understanding at the programming language level. Therefore we investigate the visualization schema, which uses more realistic expressions. Here we visualize term rewriting with more realistic expressions by using figures, pictures and images. In CafeOBJ, rewriting rules are called "equations. " An equation is composed of operators and variables. We map operators to realistic expressions so that equations are expressed as transformations of realistic expressions. We use visual transformation rules which give the program pictorial <b>expressions.</b> <b>Keywords</b> Visual Programming, Specification Languages, Computer Human Interaction 1. INTRODUCTION A visual programming system (VPS) visualizes the str [...] ...|$|R
5000|$|Data is {{classified}} as structured or unstructured. Structured data resides in fixed fields within a file such as a spreadsheet, while unstructured data refers to free-form text or media as in text documents, PDF files and video. An estimated 80% of all data is unstructured and 20% structured. Data classification is divided into content analysis, focused on structured data and contextual analysis which looks at the place of origin or the application or system that generated the data. [...] Methods for describing sensitive content are abundant. They {{can be divided into}} precise and imprecise methods.Precise methods involve content registration and trigger almost zero false positive incidents.All other methods are imprecise and can include: <b>keywords,</b> lexicons, regular <b>expressions,</b> extended regular expressions, meta data tags, bayesian analysis and statistical analysis techniques such as Machine Learning, etc.|$|R
40|$|Text mining (TM) and {{computational}} linguistics (CL) are computationally intensive fields where many tools are becoming available to study large text corpora and exploit {{the use of}} corpora for various purposes. In this chapter we will {{address the problem of}} building conversational agents or chatbots from corpora for domain-specific educational purposes. After addressing some linguistic issues relevant to the development of chatbot tools from corpora, a methodology to systematically analyze large text corpora about a limited knowledge domain will be presented. Given the Artificial Intelligence Markup Language as the assembly language for the artificial intelligence conversational agents we present a way of using text corpora as seed from which a set of source files can be derived. More specifically we will illustrate how to use corpus data to extract relevant <b>keywords,</b> multiword <b>expressions,</b> glossary building and text patterns in order to build an AIML knowledge base that could be later used to build interactive conversational systems. The approach we propose does not require deep understanding techniques for the analysis of text. As a case study it will be shown how to build the knowledge base of an English conversational agent for educational purpose from a child story that can answer question about characters, facts and episodes of the story. A discussion of the main linguistic and methodological issues and further improvements is offered in the final part of the chapter...|$|R
30|$|We {{believe that}} joining this task using micro-blogs {{would also be}} a very good test case for our {{unsupervised}} segmentation system (Magistry and Sagot 2012). In our segmentation system, MWE-like chunks are extracted instead of words, and the polarity of each chunk is directly learned from the training data. This method turns out to be particularly efficient as relevant <b>keywords</b> or key <b>expressions</b> for discriminating polarities within context may well be corpus specific. Therefore, in this case, the chunks with automatically learned polarities should be used in this task only but not getting collected into generic sentiment lexica lists. As data sparsity and dynamics remain a challenging issue under this approach, despite the extracted chunks, we also tried to enrich the lexica using other sources. In this section, materials and detailed procedures of the study are presented below.|$|R
30|$|Regular {{language}} {{search is}} productively effective for this system. This makes the system {{be the first}} of its type, {{to the best of our}} knowledge. It is undeniable that SSE (e.g., [10]) usually enjoys better efficiency in data searching compared to the public key-based searchable encryption. However, this novel system can support any arbitrary alphabet/regular language search, so that it is more human-friendly readable for search keyword design. Besides, the system provides verifiable (data integrity) check for system users (due to public-key-based feature). Moreover, the system does not need to require a data owner to pick up some special keywords before constructing keyword index structures, e.g., least frequent keyword, but also it only leverages a DFA structure to embed flexible search expressiveness, e.g., “AND, OR, NOT,” unlike that of only limited in “a <b>keyword</b> AND (formula)” <b>expression.</b>|$|R
40|$|We have {{designed}} and experimentally implemented {{a tool for}} developing a natural language systems that can accept extrasgrammatical <b>expressions,</b> <b>keyword</b> sequences, and linguistic fragments, as well as ordi nary natural language queries. The key to this tooUs efficiency is its effective use of a simple keyword analyzer iu combination with a conventional case-based parser. The keyword analyzer performs a majority of those queries which are simple data retrievals. Since it uses only keywords in any query, this analyzer is robust with regard to extra-grammatical expressions. Since little labor is required of tile application designer in using the keyword analyzer portion of the tool, and since the case-based parser processes only those queries which the keyword analyzer fails to interpret, total labor required of the designer is less than that for a tool which employs a couventional case-based passer alone...|$|R
40|$|Growth is {{a concept}} of {{particular}} interest for economic discourse. This paper sets out to explore a small corpus of economic growth, which consists of articles from The Economist. The corpus software {{used in this study}} is a web-based tool Wmatrix, an automatic tagging software able to assign semantic field (domain) tags, and to permit the extraction of key words and key semantic domains by applying the keyness calculation to tag frequency lists. The results show that at 99 % confidence (or p < 0. 01), the cut-off of 6. 63 (log likelihood value) indicates that there are 1051 positive <b>keywords</b> (including multiword <b>expressions)</b> and 80 key semantic domains generated from the corpus. It is evident that BRICs or the emerging economies/markets, in particular China and India, are a big concern regarding economic growth over the past years. A number of examples of possible ways forward in teaching methodology are presented...|$|R
