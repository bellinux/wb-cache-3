107|40|Public
25|$|According {{to social}} {{exchange}} theory, relationships {{are based on}} rational choice and cost-benefit analysis. If one partner's costs begin to outweigh his or her benefits, that person may leave the relationship, especially if there are good alternatives available. This theory {{is similar to the}} <b>minimax</b> <b>principle</b> proposed by mathematicians and economists (despite the fact that human relationships are not zero-sum games). With time, long term relationships tend to become communal rather than simply based on exchange.|$|E
5000|$|The Courant <b>minimax</b> <b>principle</b> gives a {{condition}} for finding the eigenvalues for a real symmetric matrix. The Courant <b>minimax</b> <b>principle</b> is as follows: ...|$|E
5000|$|For simplicity, {{suppose that}} m ≤ q(x) ≤ M on 0,π with Dirichlet {{boundary}} conditions.The <b>minimax</b> <b>principle</b> shows that ...|$|E
40|$|In {{this paper}} {{we show that}} a {{generally}} nonsmooth locally Lipschitz function which satisfies the nonsmooth C-condition (nonsmooth Cerami condition) and is bounded from below, is coercive. The Cerami condition is a weak form of the well-known Palais-Smale condition, which suffices to prove <b>minimax</b> <b>principles...</b>|$|R
40|$|A semilinear {{elliptic}} equation {{with strong}} resonance at infinity {{and with a}} nonsmooth potential is studied. Using nonsmooth critical point theory and developing some abstract <b>minimax</b> <b>principles</b> which complement and extend results in the literature, two results on existence are obtained. © 2006 Wuhan Institute of Physics and Mathematics...|$|R
40|$|A {{worst-case}} estimator for econometric models containing unobservable components, {{based on}} <b>minimax</b> <b>principles</b> for optimal selection of parameters, is proposed. Worst-case estimators are robust against the averse effects of unobservables. Computing worstcase estimators involves solving a minimax continuous problem, {{which is quite}} a challenging task. Large sample theory is considered, and a Monte Carlo study of finite-sample properties is conducted. A financial application is consideredPublicad...|$|R
50|$|In mathematics, the Courant <b>minimax</b> <b>principle</b> {{gives the}} {{eigenvalues}} {{of a real}} symmetric matrix. It is named after Richard Courant.|$|E
50|$|On {{the other}} hand, the {{distribution}} of the zeros of the entire functionω(λ) is already known from the <b>minimax</b> <b>principle.</b>|$|E
50|$|The <b>minimax</b> <b>principle</b> also generalizes to {{eigenvalues}} {{of positive}} self-adjoint operators on Hilbert spaces, {{where it is}} commonly used to study the Sturm-Liouville problem.|$|E
40|$|Abstract A semilinear {{elliptic}} equation {{with strong}} resonance at infinity {{and with a}} nonsmooth potential is studied. Using nonsmooth critical point theory and developing some abstract <b>minimax</b> <b>principles</b> which complement and extend results in the literature, two results on existence are obtained. Key words Strong resonance, hemivariational inequality, Laplacian, principal eigen-value, locally Lipschitz function, Clarke subdifferential, nonsmooth critical point theory 2000 MR Subject Classification 35 J 85...|$|R
40|$|In {{this paper}} we develop a {{critical}} point theory for nonsmooth locally Lipschitz functionals defined on a closed, convex set extending this way the work of Struwe (Variational Methods, Springer, Berlin, 1990). Through a deformation result, we obtain <b>minimax</b> <b>principles</b> producing critical points. Then we use the theory to obtain positive and negative solutions of nonlinear and semilinear hemivariational inequalities. In this context we improve a result on positive solutions for semilinear elliptic problems due to Nirenberg (Variational methods in nonlinear problems, in: Topics in Calculus of Variations, Lecture Notes in Mathematics, vol. 1365, Springer, Berlin, 1987). (c) 2005 Elsevier Ltd. All rights reserved...|$|R
40|$|AbstractIt is {{well known}} that all the {{eigenvalues}} of the linear eigenvalue problemΔu=(q−λr) u,in Ω⊂RN, can (under appropriate conditions on q, r and Ω) be characterized by <b>minimax</b> <b>principles,</b> but it has been a long-standing question whether that remains true for analogous equations involving the p-Laplacian Δp. It will be shown that there are corresponding nonlinear eigenvalue problemsΔpu=(q−λr) |u|p− 1 sgnu,in Ω⊂RN, with 1 0 on Ω¯, for which not all eigenvalues are of variational type. As far as we know, this is the first observation of such a phenomenon, and examples will be given for one- and higher-dimensional equations. The question of exactly which eigenvalues are variational is also discussed when N= 1...|$|R
50|$|Courant's name is also {{attached}} to the finite element method, with his numerical treatment of the plain torsion problem for multiply-connected domains, published in 1943.This method {{is now one of}} the ways to solve partial differential equations numerically. Courant is a namesake of the Courant-Friedrichs-Lewy condition and the Courant <b>minimax</b> <b>principle.</b>|$|E
50|$|Although {{this article}} {{is focused on the}} Proximity aspect of the Principles of Attraction, it is {{important}} to note other principles. These are not in any speciﬁc order, but they are an important to consider to fully understand the principles of attraction. The other principles are, The Elaboration Principle, The Similarity Principle, The Complementarity Principle, The Reciprocity Principle, and the <b>Minimax</b> <b>Principle.</b>|$|E
50|$|The Courant <b>minimax</b> <b>principle</b> is {{a result}} of the maximum theorem, which says that for q(x) = <Ax,x>, A being a real {{symmetric}} matrix, the largest eigenvalue is given by λ1 = max||x||=1q(x) = q(x1), where x1 is the corresponding eigenvector. Also (in the maximum theorem) subsequent eigenvalues λk and eigenvectors xk are found by induction and orthogonal to each other; therefore, λk = max q(xk) with <xj,xk> = 0, j < k.|$|E
40|$|Abstract In {{probability}} {{proportional to}} size sampling, including prism cruising {{and other forms}} of point and line sampling, calculation of an exact expansion factor requires that size be recorded exactly. When sizes are binned or recorded by class, this information is lost. While several alternatives for calculating the expansion factor have been proposed, theoretical attention has been lacking. A decision-theoretic perspective helps distinguish between the alternatives and offers some support {{to the use of the}} arithmetic mean size in calculating the expansion factor, a choice which had previously come under some criticism. However, consistency arguments strongly favor estimators based on squared error loss or <b>minimax</b> <b>principles.</b> Some new alternatives are suggested when prior information about the diameter distribution in a stratum is available...|$|R
40|$|AbstractThis {{paper is}} {{concerned}} with the existence of solutions for the boundary value problem{−(|u′|p− 2 u′) ′+ɛ|u|p− 2 u=∇F(t,u),in (0,T),((|u′|p− 2 u′) (0),−(|u′|p− 2 u′) (T)) ∈∂j(u(0),u(T)), where ɛ⩾ 0, p∈(1,∞) are fixed, j:RN×RN→(−∞,+∞] is a proper, convex and lower semicontinuous function and F:(0,T) ×RN→R is a Carathéodory mapping, continuously differentiable with respect to the second variable and satisfies some usual growth conditions. Our approach is a variational one and relies on Szulkin's critical point theory [A. Szulkin, <b>Minimax</b> <b>principles</b> for lower semicontinuous functions and applications to nonlinear boundary value problems, Ann. Inst. H. Poincaré Anal. Non Linéaire 3 (1986) 77 – 109]. We obtain the existence of solutions in a coercive case as well as the existence of nontrivial solutions when the corresponding Euler–Lagrange functional has a “mountain pass” geometry...|$|R
40|$|Abstract. This paper {{provides}} a framework for deriving necessary conditions, {{in the form of}} a maximum <b>principle,</b> for <b>minimax</b> optimal control problems. The distinguishing feature of these problems is that the data depends on a vector α of unknown parameters, and “optimality ” is defined on a worst case basis, as α ranges over the parameter set A. The centerpiece, a <b>minimax</b> maximum <b>principle,</b> is a set of optimality conditions for such problems. Here, the parameter set A is taken to be an arbitrary compact metric space and the hypotheses imposed on the dynamics and endpoint constraints are of an unrestrictive nature. The <b>minimax</b> maximum <b>principle</b> captures as special cases necessary conditions for optimal control problems with minimax costs, for problems involving “semi-infinite ” endpoint constraints, and also a maximum principle for state constrained optimal control problems...|$|R
50|$|According {{to social}} {{exchange}} theory, relationships {{are based on}} rational choice and cost-benefit analysis. If one partner's costs begin to outweigh his or her benefits, that person may leave the relationship, especially if there are good alternatives available. This theory {{is similar to the}} <b>minimax</b> <b>principle</b> proposed by mathematicians and economists (despite the fact that human relationships are not zero-sum games). With time, long term relationships tend to become communal rather than simply based on exchange.|$|E
5000|$|The Courant <b>minimax</b> <b>principle,</b> {{as well as}} {{the maximum}} principle, can be visualized by imagining that if ||x|| = 1 is a {{hypersphere}} then the matrix A deforms that hypersphere into an ellipsoid. When the major axis on the intersecting hyperplane are maximized [...] - [...] i.e., the length of the quadratic form q(x) is maximized [...] - [...] this is the eigenvector, and its length is the eigenvalue. All other eigenvectors will be perpendicular to this.|$|E
50|$|In {{computational}} complexity theory, Yao's principle or Yao's <b>minimax</b> <b>principle</b> {{states that}} the expected cost of a randomized algorithm on the worst case input, is no better than a worst-case random probability distribution of the deterministic algorithm which performs best for that distribution. Thus, to establish a lower bound {{on the performance of}} randomized algorithms, it suffices to find an appropriate distribution of difficult inputs, and to prove that no deterministic algorithm can perform well against that distribution. This principle is named after Andrew Yao, who first proposed it.|$|E
40|$|A precise {{description}} of the convexity of Gaussian measures is provided by sharp Brunn-Minkowski type inequalities due to Ehrhard and Borell. We show that these are manifestations of a game-theoretic mechanism: a <b>minimax</b> variational <b>principle</b> for Brownian motion. As an application, we obtain a Gaussian improvement of Barthe's reverse Brascamp-Lieb inequality. Comment: 23 page...|$|R
40|$|We {{consider}} {{a model of}} a price setting monopolistic firm that has to decide on a price adjustment in light of some new information about the uncertain demand for its product. It is assumed that the firm applies the so-called <b>minimax</b> adjustment <b>principle</b> for this purpose. This optimality principle is based on the notion that human decision makers tend to be reluctant to change their behaviour and prefer to take the current price as a benchmark rather than to compute optimal new prices all the time. We show that the current price is maintained as long as it lies in some set that is derived from the firm's degree of uncertainty and its degree of conservatism as regards changing behaviour. Otherwise, the price is adjusted but kept {{as close as possible to}} the current price. Hence, the model provides an explanation for the existence of price rigidities which is not based on any technical adjustment or menu costs but simply on how human decision makers may deal with uncertainty. Price rigidities, <b>minimax</b> adjustment <b>principle,</b> fuzzy information...|$|R
40|$|A risk measure, {{expected}} opportunity loss (EOL), {{is introduced}} {{to quantify the}} potential loss of making an incorrect choice in risk-based decision making. Different from Savage's (1951) <b>minimax</b> regret <b>principle,</b> EOL can account for the unbounded continuous random outcomes of alternatives and decision makers' acceptable risk. This article studies {{the effects of the}} forms of loss function, correlation among outcomes, and the acceptable risk on the ranking results by considering the loss function in the power form. The results show that the loss functions and the outcomes correlations can significantly influence the rankings of alternatives in risk-based decision making...|$|R
50|$|There {{are many}} other search methods, or metaheuristics, which are {{designed}} {{to take advantage of}} various kinds of partial knowledge one may have about the solution. Heuristics {{can also be used to}} make an early cutoff of parts of the search. One example of this is the <b>minimax</b> <b>principle</b> for searching game trees, that eliminates many subtrees at an early stage in the search. In certain fields, such as language parsing, techniques such as chart parsing can exploit constraints in the problem to reduce an exponential complexity problem into a polynomial complexity problem. In many cases, such as in Constraint Satisfaction Problems, one can dramatically reduce the search space by means of Constraint propagation, that is efficiently implemented in Constraint programming languages.The search space for problems can also be reduced by replacing the full problem with a simplified version. For example, in computer chess, rather than computing the full minimax tree of all possible moves for the remainder of the game, a more limited tree of minimax possibilities is computed, with the tree being pruned at a certain number of moves, and the remainder of the tree being approximated by a static evaluation function.|$|E
5000|$|Info-gap is propounded (e.g. Ben-Haim 2001, 2006) {{as a new}} non-probabilistic {{theory that}} is radically {{different}} from all current decision theories for decision under uncertainty. So, {{it is imperative to}} examine in this discussion in what way, if any, is info-gap's robustness model radically different from Maximin. For one thing, there is a well-established assessment of the utility of Maximin. For example, Berger (Chapter 5) suggests that even in situations where no prior information is available (a best case for Maximin), Maximin can lead to bad decision rules and be hard to implement. He recommends Bayesian methodology. And as indicated above,It should also be remarked that the <b>minimax</b> <b>principle</b> even if it is applicable leads to an extremely conservative policy. However, quite apart from the ramifications that establishing this point might have for the utility of info-gaps' robustness model, the reason that it behooves us to clarify the relationship between info-gap and Maximin is the centrality of the latter in decision theory. After all, this is a major classical decision methodology. So, any theory claiming to furnish a new non-probabilistic methodology for decision under severe uncertainty would be expected to be compared to this stalwart of decision theory. And yet, not only is a comparison of info-gap's robustness model to Maximin absent from the three books expounding info-gap (Ben-Haim 1996, 2001, 2006), Maximin is not even mentioned in them as the major decision theoretic methodology for severe uncertainty that it is. [...] Elsewhere in the info-gap literature, one can find discussions dealing with similarities and differences between these two paradigms, as well as discussions on the relationship between info-gap and worst-case analysis,However, the general impression is that the intimate connection between these two paradigms has not been identified. Indeed, the opposite is argued. For instance, Ben-Haim (2005) argues that info-gap's robustness model is similar to Maximin but, is not a Maximin model.|$|E
30|$|From the definition, we {{give the}} {{following}} general <b>minimax</b> <b>principle</b> for the critical values of a locally Lipschitz function φ.|$|E
40|$|Images {{of natural}} scenes contain a rich variety of visual patterns. To learn and {{recognize}} these patterns from natural images, {{it is necessary}} to construct statistical models for these patterns. In this review article we describe three statistical principles for modeling image patterns: the sparse coding <b>principle,</b> the <b>minimax</b> entropy <b>principle,</b> and the meaningful alignment principle. We explain these three principles and their relationships in the context of modeling images as compositions of Gabor wavelets. These three principles correspond to three regimes of composition patterns of Gabor wavelets, and these three regimes are connected by changes in scale or resolution. KEY WORDS...|$|R
40|$|Using {{annual data}} on mergers for 35 leading German {{companies}} from 1870 to 1913, my study {{tries to explain}} the first merger wave that emerged 1898. My panel probit model that accounted for economies of scale, macroeconomic conditions, success of former mergers, and market structure revealed that previous mergers made subsequent mergers more likely. The propensity to merge was higher for larger companies that increased their market power. In the banking industry, managers imitated mergers, although these mergers were not successful, and hence followed the <b>minimax</b> regret <b>principle.</b> Rational information-based herding caused the serial dependency of mergers in other industrie...|$|R
40|$|Model {{selection}} {{is central to}} statistics, and many learning problems can be formulated as model selection problems. In this paper, we treat the problem of selecting a maximum entropy model given various feature subsets and their moments, as a model selection problem, and present a minimum description length (MDL) formulation to solve this problem. For this, we derive normalized maximum likelihood (NML) codelength for these models. Furthermore, we prove that the <b>minimax</b> entropy <b>principle</b> is a special case of maximum entropy model selection, where one assumes that complexity of all the models are equal. We apply our approach to gene selection problem and present simulation results. Comment: 9 pages, 3 figures, 4 tables, submitted to Uncertainty in Artificial Intelligenc...|$|R
30|$|From {{the above}} general <b>minimax</b> <b>principle,</b> a nonsmooth {{version of the}} {{mountain}} pass theorem, the saddle point theorem, and the generalized mountain pass theorem are available by choosing the link sets appropriately (see [10], [14]).|$|E
30|$|In 2006, Balaj [27] applied this {{to obtain}} two minimax inequalities in G-convex spaces which extend and improve {{a large number}} of generalizations of the Ky Fan minimax {{inequality}} and of the von Neumann-Sion <b>minimax</b> <b>principle.</b>|$|E
30|$|In this paper, some {{existence}} theorems {{are obtained}} for subharmonic solutions of second-order Hamiltonian systems with linear part under non-quadratic conditions. The {{approach is the}} <b>minimax</b> <b>principle.</b> We consider some new cases and obtain some new existence results.|$|E
40|$|An {{important}} {{way to make}} large training sets is to gather noisy labels from crowds of nonexperts. We propose a <b>minimax</b> entropy <b>principle</b> {{to improve the quality}} of these labels. Our method assumes that labels are generated by a probability distribution over workers, items, and labels. By maximizing the entropy of this distribution, the method naturally infers item confusability and worker expertise. We infer the ground truth by minimizing the entropy of this distribution, which we show minimizes the Kullback-Leibler (KL) divergence between the probability distribution and the unknown truth. We show that a simple coordinate descent scheme can optimize minimax entropy. Empirically, our results are substantially better than previously published methods for the same problem. ...|$|R
40|$|In {{this paper}} we propose a novel inhomogeneous Gibbs model by the <b>minimax</b> entropy <b>{{principle}},</b> and apply it to face modeling. The maximum entropy principle generalizes the statistical properties of the observed samples and results in the Gibbs distribution, while the minimum entropy principle makes the learnt distribution close to the observed one. To capture the fine details of a face, an inhomogeneous Gibbs model is derived to learn the local statistics of facial feature points. To alleviate the high dimensionality problem of face models, we propose to learn the distribution in a subspace reduced by principal component analysis or PCA. We demonstrate that our model effectively captures important and subtle non-Gaussian face patterns and efficiently generates good face models. 1...|$|R
40|$|The {{paper is}} devoted to the study of two-parametric {{families}} of Dirichlet problems for systems of equations with p, q-Laplacians and indefinite nonlinearities. Continuous and monotone curves Γ_f and Γ_e on the parametric plane λ×μ, which are the lower and upper bounds for a maximal domain of existence of weak positive solutions are introduced. The curve Γ_f is obtained by developing our previous work BobkovIlyasov and it determines a maximal domain of the applicability of the Nehari manifold and fibering methods. The curve Γ_e is derived explicitly via <b>minimax</b> variational <b>principle</b> of the extended functional method. Comment: The proof of statement (3) of Theorem 2. 3 in the previous version of the article was not correct. The accents of the article have been changed. Exposition have been improved for easier reading. 15 pages, 2 figure...|$|R
