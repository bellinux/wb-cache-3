292|150|Public
500|$|The setlist {{has been}} well {{received}} by critics, stating that the setlist reads like [...] "the quintessential 'Best Of' track list for the band". Matt Helgeson of Game Informer reviewed the soundtrack as [...] "the best hit to <b>miss</b> <b>ratio</b> of any music game to date".|$|E
500|$|Reviews have {{primarily}} {{praised the}} game for a [...] "stellar" [...] set list that [...] " [...] like the quintessential 'Best Of' track list for the band", and consider [...] "the best hit to <b>miss</b> <b>ratio</b> of any music game to date". The difficulty of the songs was also warmly received, with reviews noting that Metallica's songs can [...] " [...] really well to a plastic guitar", and that [...] "the songs here are a treat on any skill level". However, the game still provides a difficult challenge for experienced players, {{and the introduction of}} the Expert+ difficulty for drums and the Drum Over mode were seen as good additions. Reviews commented favorably on the new career progression, noting that one can complete the career mode without having to play [...] "Metallica’s earliest, shreddiest, most brutal stuff", and allows the player to [...] "skip right past the early stuff and quickly get to the big tracks".|$|E
50|$|Open {{addressing}} {{has a lower}} cache <b>miss</b> <b>ratio</b> than separate chaining {{when the}} table is mostly empty. However, as the table becomes filled with more elements, open addressing's performance degrades exponentially. Additionally, separate chaining uses less memory in most cases, unless the entries are very small (less than four {{times the size of}} a pointer).|$|E
40|$|Cache misses due to {{coherence}} and directory maintenance {{is a major}} reason for poor performance in shared memory multiprocessors. We show that the relationship between a particular access pattern and cache <b>miss</b> <b>ratios</b> for a class of directory-based, write-invalidate cache coherence protocols can be characterised in a small set of parameters. In order to do this, a reference generator has been designed that, based on parameters automatically extracted from a program, artificially can generate a reference stream that results in the same cold, {{coherence and}} directory replacement <b>miss</b> <b>ratios</b> as an execution of the program. ...|$|R
40|$|Previous {{research}} has shown that the SPEC benchmarks achieve low <b>miss</b> <b>ratios</b> in relatively small instruction caches. This paper presents evidence that current software-development practices produce applications that exhibit substantially higher instruction-cache <b>miss</b> <b>ratios</b> than do the SPEC benchmarks. To represent these trends, we have assembled a collection of applications, called the Instruction Benchmark Suite (IBS), that provides a better test of instruction-cache performance. We discuss the rationale behind the design of IBS and characterize its behavior relative to the SPEC benchmark suite. Our analysis is based on trace-driven and trap-driven simulations and takes into full account both the application and operating-system components of the workloads. This paper then reexamines a collection of previously-proposed hardware mechanisms for improving instruction-fetch performanc...|$|R
40|$|Abstract: With an {{increasing}} requirement of more flexible real-time applications, e. g. multimedia servers in home networks and real-time database servers, a real-time process scheduler using Worst-Case Execution Time (WCET) is inefficient for optimizing performance. Some soft and firm real-time {{models have been}} proposed {{to deal with this}} situation. This paper presents a feedback control approach for scheduling processes with imprecise computation, a firm real-time model to produce approximate result of an acceptable quality when the exact result of the desired quality cannot be obtained in time. By introducing feedback control to process scheduling, our approach aims to bind the deadline <b>missing</b> <b>ratio</b> under a varying system workload to reach a tradeoff between the deadline <b>missing</b> <b>ratio</b> and result precision...|$|R
50|$|Shellsort {{performs}} more {{operations and}} has higher cache <b>miss</b> <b>ratio</b> than quicksort. However, since {{it can be}} implemented using little code and does not use the call stack, some implementations of the qsort function in the C standard library targeted at embedded systems use it instead of quicksort. Shellsort is, for example, used in the uClibc library. For similar reasons, an implementation of Shellsort {{is present in the}} Linux kernel.|$|E
50|$|For CPU caches {{with large}} {{associativity}} (generally >4 ways), the implementation cost of LRU becomes prohibitive. In many CPU caches, a scheme that almost always discards {{one of the}} least recently used items is sufficient. So many CPU designers choose a PLRU algorithm which only needs one bit per cache item to work.PLRU typically has a slightly worse <b>miss</b> <b>ratio,</b> has a slightly better latency, uses slightly less power than LRU and lower overheads compared to LRU.|$|E
5000|$|Reviews have {{primarily}} {{praised the}} game for a [...] "stellar" [...] set list that [...] " [...] like the quintessential 'Best Of' track list for the band", and consider [...] "the best hit to <b>miss</b> <b>ratio</b> of any music game to date". The difficulty of the songs was also warmly received, with reviews noting that Metallica's songs can [...] " [...] really well to a plastic guitar", and that [...] "the songs here are a treat on any skill level". However, the game still provides a difficult challenge for experienced players, {{and the introduction of}} the Expert+ difficulty for drums and the Drum Over mode were seen as good additions. Reviews commented favorably on the new career progression, noting that one can complete the career mode without having to play [...] "Metallica’s earliest, shreddiest, most brutal stuff", and allows the player to [...] "skip right past the early stuff and quickly get to the big tracks".|$|E
40|$|Abstract. In this paper, we {{consider}} flux caches prefetching and a media application. We analyze the MPEG 4 encoder workload with realistic data {{set in a}} scenario representative for the embedded systems domain. Our study shows that different well known data prefetch mechanisms can gain little reduction in the cache <b>miss</b> <b>ratios</b> when applied on the complete MPEG 4 application. Furthermore, we investigate the potential improvement when dedicated prefetching strategies are applied to the sum of absolute differences (SAD) kernels in MPEG 4. We propose a flux cache mechanism that dynamically invokes cache designs with dedicated prefetching engines that can fully utilize the available memory bandwidth. We show that our proposal improves the cache <b>miss</b> <b>ratios</b> by a factor close to 3 x...|$|R
40|$|Applications often under-utilize cache space, {{generating}} unnecessarily high cache <b>miss</b> <b>ratios.</b> Data {{distribution is}} a software technique which could improve cache miss rates for any types of application. There {{is a great}} potential to exploit: data distribution can reduce capacity misses as well as conflict misses...|$|R
40|$|Objective: During the {{analysis}} of ordinal longitudinal datasets, the most frequent problem is missing or incomplete data. In this study, when comparing two independent groups {{for this type of}} datasets, the effects of different <b>missing</b> <b>ratio</b> and different sample size on Type I and Type II error rates were investigated. Material and Methods: The data for different <b>missing</b> <b>ratio</b> and sample sizes (n= 50, 100, 200, 400) using simulation technique were generated. Repeated measurements (at four time points) for each group were generated from multivariate normal distributions using SAS MVN macro and transformed ordinal structure with quintiles method. Completely random monotone missing data for predefined ratios were created. On the comparison of two independent groups using generalized estimating equations (GEE), Type I and Type II error rates were investigated. These simulations were each replicated 1000 times. Results: While the Type I error rate was not affected seriously from missing clusters, the Type II error rate was affected. In the complete dataset for both small and large datasets, Type I error rates were < 0. 0001. While Type II error rates were greater than 0. 10 for small sample sizes (n< 100), this value was greater than 50 % in the presence of missing datasets. Conclusion: The sample size and <b>missing</b> <b>ratio</b> are still important tasks for ordinal longitudinal data both in the planning and analyzing stages of an experimental design...|$|R
40|$|It {{is proven}} {{that under the}} {{assumption}} of independent references, the (apparently analytically and computationally intractable) expected LRU (least recently used) <b>miss</b> <b>ratio</b> with main memory size CAP can be approximated arbitrarily closely by the (analytically and computationally tractable) expected working-set <b>miss</b> <b>ratio</b> with expected working-set size CAP, as {{the size of the}} database goes to infinity. Thier common asymptotic value is given by a tractable formula involving integrals. An immediate corollary of the representation is the asymptotic independence of <b>miss</b> <b>ratio</b> from page size in the independent reference model and in some generalizations of this model. This result also has implications about the effect on <b>miss</b> <b>ratio</b> of variable or fixed partitioning of main memory, in case of multiprogramming. Furthermore, in certain database environments, we can answer the question as to how the size of main memory must vary {{in order to maintain the}} same <b>miss</b> <b>ratio,</b> when the size of the database increases. The methods of this paper are extended to give an asymptotic formula for the <b>miss</b> <b>ratio</b> under VMIN, the optimal variable-space page replacement algorithm under demand paging...|$|E
40|$|In a {{two-level}} {{computer storage}} hierarchy, <b>miss</b> <b>ratio</b> measurements are often {{made from a}} &quot;cold start&quot;, that is, made with the first-level store initially empty. For large capacities {{the effect on the}} measured <b>miss</b> <b>ratio</b> of the misses incurred while filling the first-level store can be significant, even for long reference strings. Use of &quot;warm-start &quot; rather than &quot;cold-start &quot; miss ratios cast doubt on the widespread belief that the observed &quot;S-shape &quot; of lifetime (reciprocal of <b>miss</b> <b>ratio)</b> versus capacity curve indicates a property of behavior of programs that maintain a constant number of pages in main storage. On the other hand, if cold-start miss ratios are measured as a function of capacity and measurement length, then they are useful in studying systems in which operation of a program is periodically interrupted by task switches. It is shown how to obtain, under simple assumptions, the cache <b>miss</b> <b>ratio</b> for multiprogramming from cold-start <b>miss</b> <b>ratio</b> values and how to obtain approximate cold-start miss ratios from warm-start miss ratios...|$|E
40|$|Increasing the {{associativity}} is {{a common}} {{way to reduce the}} performance-detrimental conflicts in a cache. From a dynamic cache power perspective this associativity comes at a high cost. In this paper we present <b>miss</b> <b>ratio</b> performance and dynamic power estimates for a skewed cache and also for the organization proposed in this paper, the elbow cache. We will show that by extending a skewed cache organization with a relocation strategy we can obtain a <b>miss</b> <b>ratio</b> that is comparable to the <b>miss</b> <b>ratio</b> of an 8 -way set-associative cache, while consuming up to 48 % less dynamic power...|$|E
40|$|This thesis evaluates an {{innovative}} cache design called, prime-mapped cache. The performance analysis on various applications and programs {{shows that the}} prime-mapped cache performs better than the conventional cache organizations. The performance gain will increase {{with the increase of}} the speed gap between processors and memories. The exact cache behavior of numerical applications namely: matrix multiplication and SPEC benchmarks is studied by varying the cache parameters such as cachesize, linesize and associativity. Traces are collected from these programs and <b>miss</b> <b>ratios</b> for instructions and data accesses are compared. Based on the experimental results and depending on the algorithm used, the <b>miss</b> <b>ratios</b> of the prime-mapped cache are found to be 50 to 100 % less than for conventional caches. Depending upon the speed difference between processors and memories, with the prime-mapped cache these algorithms can run 30 % to 2 times faster than they do on conventional caches...|$|R
40|$|Reducing {{the size}} of large address traces by "filtering" them through a small direct-mapped cache is a useful {{technique}} for making more efficient use of both secondary storage and processors used for trace-driven simulation. However, when filtered traces are used to drive cache simulators, the distortion introduced by such filtering can produce substantial errors in the <b>miss</b> <b>ratios</b> obtained, despite earlier reports to the contrary. We present {{the results of a}} systematic study of such errors, including a model for compensating for some errors. Introduction Trace-driven simulation has become a very popular method for studying and evaluating computer architectures. For the past two decades, a primary means of cache memory analysis has been the use of traces of memory access patterns to drive simulators that determine the <b>miss</b> <b>ratios</b> of different cache designs. Large traces are required to accurately evaluate large caches [1]. Traces containing a few million references are suitable when [...] ...|$|R
40|$|Tapeworm II is a {{software-based}} simulation {{tool that}} evaluates the cache and TLB performance of multiple-task and operating system intensive workloads. Tapeworm resides in an OS kernel and causes a host machine’s hardware to drive simulations with kernel traps instead of with address traces, as is conventionally done. This allows Tapeworm to quickly and accurately capture complete memory referencing behavior {{with a limited}} degradation in overall system performance. This paper compares trap-driven simulation, as implemented in Tapeworm, with the more common technique of trace-driven memory simulation with respect to speed, accuracy, portability and flexibility. Results: For reasonable <b>miss</b> <b>ratios,</b> Tapeworm simulations are significantly faster than traditional trace-driven simulations. Tapeworm typically slows a system down by less than {{an order of magnitude}} (10 x) when cache <b>miss</b> <b>ratios</b> are under 10 %, and slowdowns approach zero as <b>miss</b> <b>ratios</b> decrease. Tapeworm can employ set sampling techniques to further reduce slowdowns, but at the expense of higher measurement variance. Unlike tracedriven simulations, which typically produce identical results from run to run, trap-driven simulations exhibit greater sensitivity to inherent variations in memory system behavior on a real machine. Less than 5 % of Tapeworm’s code is machine-dependent, enhancing its portability to different machines provided that they support a few essential primitive operations. Although the trap-driven approach is flexible enough to simulate most TLB and cache configurations, other architectural structures, such as write buffers or instruction pipelines cannot be simulated with this approach. Tapeworm implementations currently exist for TLB and instruction cache simulation on MIPS-based DECstations and for TLB simulation on a 486 -based Gateway PC...|$|R
40|$|We {{are proposing}} a cache {{addressing}} {{scheme based on}} hashing intended to decrease the <b>miss</b> <b>ratio</b> of small size caches. The main intention {{is to improve the}} hit ratio for 'random' patterns pointer memory accesses for embedded (special purpose) system applications. We introduce a hashing scheme, denoted as bit juggling, and measure the effect such a scheme has in the cache access <b>miss</b> <b>ratio.</b> It is shown, for the considered benchmark, that 3 -bit bit juggling will reduce the <b>miss</b> <b>ratio</b> for up to 12 %, for associative caches of maximum size of 8 KBytes when compared to usual cache addressing schemes...|$|E
40|$|In a microprocessor, {{the cache}} hit time {{generally}} determines the clock frequency. But for the ten last years, a technological trend is {{the increase of}} the cache miss penalty in terms of instruction issue delays; then maintaining the cache <b>miss</b> <b>ratio</b> {{as low as possible}} is also of particular interest. For a few years, there has been numerous studies focusing on a low cache hit time while maintaining low cache <b>miss</b> <b>ratio.</b> Unfortunately most of the proposed solutions implicitly suppose that the cache is virtually indexed. When using virtually indexed caches, the operating system (or may be some specific hardware) has to manage the consistency of caches and memory. In this paper, we propose the Direct-mapped Access Set-associative Check cache (DASC) for addressing both difficulties. On a DASC cache, the cache array is direct-mapped then the cache hit time is low, but as the tag array is set-associative,the external <b>miss</b> <b>ratio</b> {{is the same as the}} <b>miss</b> <b>ratio</b> of a setassociative cache. When [...] ...|$|E
40|$|Abstract—A two-hop {{neighborhood}} information-based {{routing protocol}} is proposed for real-time wireless sensor networks. The approach of mapping packet deadline to a velocity is adopted as that in SPEED; however, our routing {{decision is made}} {{based on the novel}} two-hop velocity integrated with energy balancing mechanism. Initiative drop control is embedded to enhance energy utilization efficiency, while reducing packet deadline <b>miss</b> <b>ratio.</b> Simulation and comparison show that the new protocol has led to lower packet deadline <b>miss</b> <b>ratio</b> and higher energy efficiency than two existing popular schemes. The result has also indicated a promising direction in supporting real-time quality-of-service for wireless sensor networks. Index Terms—Deadline <b>miss</b> <b>ratio,</b> energy utilization efficiency, quality-of-service (QoS), real-time, two-hop information, wireless sensor networks (WSNs). I...|$|E
40|$|Today, {{embedded}} processors {{are expected}} to be able to run complex, algorithm-heavy applications that were originally designed and coded for general-purpose processors. As a result, traditional methods for addressing performance and determinism become inadequate. This paper explores a new data cache design for use in modern high-performance em-bedded processors that will dynamically improve execution time, power efficiency, and determinism within the system. The simulation results show significant improvement in cache <b>miss</b> <b>ratios</b> and reduction in power consumption of approxi-mately 30 % and 15 %, respectively...|$|R
40|$|Consideration {{is given}} to two major issues {{in the design of}} branch target buffers (BTBs), with the goal of {{achieving}} maximum performance for a given number of bits allocated to the BTB design. The first issue is BTB management; the second is what information to keep in the BTB. A number of solutions to these problems are reviewed, and various optimizations in the design of BTBs are discussed. Design target <b>miss</b> <b>ratios</b> for BTBs are developed, making it possible to estimate the performance of BTBs for real workloads...|$|R
40|$|Copyright © 2013 Roopa PS et al. This is an {{open access}} article {{distributed}} under theCreative CommonsAttribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Objectives. (1) To determine the frequency of maternal near miss, maternal near <b>miss</b> incidence <b>ratio</b> (MNMR), maternal near <b>miss</b> to mortality <b>ratio</b> and mortality index. (2) To compare the nature of near miss events with that of maternal mortality. (3) To se...|$|R
40|$|AnsTRXCT A {{theoretical}} justification {{is given}} to the empirmal observation that in some computing systems with a paged, 2 -level storage hierarchy, long-term <b>miss</b> <b>ratio</b> is roughly independent of page size Let MISS be the expected working+set <b>miss</b> <b>ratio</b> in the independent reference model, with ex-pected working set size CAP pages Now form blocks, by combining the B pages with the highest probabllitms of reference into one block, the B pages with the next-highest probabilities of reference into a second block, and so on Let MISS * be the expected working-set <b>miss</b> <b>ratio</b> when all data are moved m blocks and when the expected working set size is again CAP pages, that is, CAP/B = C blocks. It ~s proved that I MISS [...] MISS * I < (2 /C). -}- (33 /C 2). Thus, ff the expected working-set size (in blocks) is sufficiently large, then the miss ratios in the blocked and unblocked cases are ap-proximately equal This result m used to argue the approximate independence of <b>miss</b> <b>ratio</b> on page size in more realistic models of page references...|$|E
40|$|Recently, {{cheap and}} large {{capacity}} non-volatile memory such as flash memory is rapidly replacing disks in embedded systems. While the access time of flash memory is highly predictable, deadline misses may occur if data objects in flash memory are not properly managed in real-time embedded databases. Buffer cache {{can be used}} to mitigate this problem. However, since the workload of a real-time database cannot be precisely predicted, it may not be feasible to provide enough buffer space to satisfy all timing constraints. Several deadline <b>miss</b> <b>ratio</b> management schemes have been proposed, but they do not consider I/O activities. In this paper, we present an I/O-aware deadline <b>miss</b> <b>ratio</b> management scheme in real-time embedded databases whose secondary storage is flash memory. We propose an adaptive I/O deadline assignment scheme, in which I/O deadlines are derived from up-to-date system status. We also present a deadline <b>miss</b> <b>ratio</b> management architecture where a control theory-based feedback control loop prevents resource overload both in I/O and CPU. A simulation study shows that our approach can effectively cope with both I/O and CPU overload to achieve the desired deadline <b>miss</b> <b>ratio...</b>|$|E
40|$|Microprocessors are {{increasingly}} incorporating {{one or more}} on-chip caches. These caches are occupying {{a greater share of}} chip area, and thus may be the locus of manufacturing defects. Some of these defects will cause faults in cache tag or data memory. These faults can be tolerated by disabling the cache blocks that contain them. This approach lets chips with defects be used without requiring on-chip caches to have redundant row or columns or to use error correcting codes. Disabling blocks, however, typically increases a cache’s <b>miss</b> <b>ratio.</b> This paper investigates how much cache miss ratios increase when blocks are disabled. It shows how the mean <b>miss</b> <b>ratio</b> increase can be characterized {{as a function of the}} miss ratios of related caches, develops an efficient approach for calculating the exact distribution of <b>miss</b> <b>ratio</b> increases from all fault patterns, and applies this approach to the ATUM traces [1]. Results reveal that the mean relative <b>miss</b> <b>ratio</b> increase from a few faults decreases with increasing cache size, and is negligible (< 2 % per defect) unless a set is completely disabled by faults. The maximum relative increase is also acceptable (< 5 % per fault) if no set is entirely disabled...|$|E
40|$|Objectives. (1) To {{determine}} {{the frequency of}} maternal near miss, maternal near <b>miss</b> incidence <b>ratio</b> (MNMR), maternal near <b>miss</b> to mortality <b>ratio</b> and mortality index. (2) To compare the nature of near miss events with that of maternal mortality. (3) To see the trend of near miss events. Design. Audit. Setting. Kasturba Hospital, Manipal University, Manipal, India. Population. Near miss cases & maternal deaths. Methods. Cases were defined based on WHO criteria 2009. Main Outcome Measures. Severe acute maternal morbidity and maternal deaths. Results. There were 7390 deliveries and 131 “near miss” cases during the study period. The Maternal near <b>miss</b> incidence <b>ratio</b> was 17. 8 / 1000 live births, maternal near <b>miss</b> to mortality <b>ratio</b> was 5. 6 [*]:[*] 1, and mortality index was 14. 9 %. A total of 126 cases were referred, while 5 cases were booked at our hospital. Hemorrhage was the leading cause (44. 2 %), followed by hypertensive disorders (23. 6 %) and sepsis (16. 3 %). Maternal mortality ratio (MMR) was 313 / 100000 live births. Conclusion. Hemorrhage and hypertensive disorders are the leading causes of near miss events. New-onset viral infections have emerged as {{the leading cause of}} maternal mortality. As near miss analysis indicates the quality of health care, it is worth presenting in national indices...|$|R
40|$|We {{propose a}} two layer {{protocol}} for tracking fast targets in sensor networks. At the lower layer, the Distributed Spanning Tree Algorithm (DSTA) [12] partitions the network into clusters with controllable diameter and constructs a spanning tree backbone of clusterheads rooted at the sink. At the upper layer, we propose a target tracking algorithm which wakes clusters of nodes {{by using the}} estimated trajectory beforehand, which is different from existing studies [3] in which target can be detected only when the nodes close to the target are awake. We provide the simulation results and show the effect of fore-waking operation by comparing error and <b>miss</b> <b>ratios</b> of existing studies with our proposed target tracking algorithm. 1...|$|R
40|$|Both {{analysis}} and design optimisation of real-time systems has predominantly concentrated on considering hard real-time constraints. For a large class of applications, however, this is both unrealistic {{and leads to}} unnecessarily expensive implementations. This paper addresses the problem of task priority assignment and task mapping {{in the context of}} multiprocessor applications with stochastic execution times and in the presence of constraints on the percentage of missed deadlines. We propose a design space exploration strategy together with a fast method for system performance analysis. Experiments emphasise the efficiency of the proposed analysis method and optimisation heuristic in generating high quality implementations of soft real-time systems with stochastic task execution times and constraints on deadline <b>miss</b> <b>ratios...</b>|$|R
40|$|With the {{proliferation}} of e-businesses, Java ™ Middleware and OLTP applications are gaining importance. As the gap between CPU and memory latencies continues to increase, the performance of these applications running on multiprocessor systems will become further limited by the memory system. This study characterizes the memory behavior of such applications using the SPECjAppServer 2002 and TPC-C benchmarks running on a real multiprocessor system. More specifically, the shared and private L 3 caches with invalidation- and update-based coherence protocols are evaluated using the Programmable Hardware-Assisted Cache Emulator (PHA$E). We found that coherency misses increase with larger private L 3 caches, constituting {{up to more than}} 15 % of all misses for both benchmarks. Additionally, a saturation point was observed at which employing larger private cache yields no further improvement in <b>miss</b> <b>ratio.</b> Conversely, the shared L 3 cache design was observed to be more scalable since it does not suffer from coherence misses. Our limit study shows that the existing Write-Broadcast policy, which updates line copies in other caches during a write on a shared line, has the potential to simultaneously reduce private cache <b>miss</b> <b>ratio</b> and bus traffic. For example, at 64 MB, it reduces the <b>miss</b> <b>ratio</b> by 53 % and 44 % respectively for SPECjAppServer 2002 and TPC-C, while lowering the bus traffic by 18 % and 11 %. In overall, the policy can eliminate the aforementioned saturation point and allows for private cache <b>miss</b> <b>ratio</b> that is comparable with the <b>miss</b> <b>ratio</b> of a shared cache...|$|E
40|$|The {{demand for}} {{real-time}} database services {{has been increasing}} recently. Examples include sensor data fusion, decision support applications, web information services, ecommerce, and data-intensive smart spaces. In these systems, {{it is essential to}} execute transactions in time using fresh (temporally consistent) data. Due to the high service demand and stringent timing/data temporal consistency constraints, real-time databases can be overloaded. As a result, users may suffer poor services. Many transaction deadlines can be missed or transactions may have to use stale data. To address these problems, we present a service differentiation architecture. Transactions are classified into several service classes based on their importance. Under overload, different degrees of deadline <b>miss</b> <b>ratio</b> guarantees are provided among the service classes according to their importance. A certain data freshness guarantee is also provided for the data accessed by timely transactions which finish within their deadlines. Feedback control is applied to support the <b>miss</b> <b>ratio</b> and freshness guarantees. In a simulation study, our service differentiation approach shows a significant performance improvement compared to the baseline approaches. The specified <b>miss</b> <b>ratio</b> and freshness are supported even in the presence of unpredictable workloads and data access patterns. Our approach also achieves a relatively low <b>miss</b> <b>ratio</b> for the less privileged service classes, thereby reducing potential starvation...|$|E
40|$|This paper {{proposes a}} Link Reliability based Two-Hop Routing {{protocol}} for Wireless Sensor Networks (WSNs). The protocol achieves to reduce packet deadline <b>miss</b> <b>ratio</b> (DMR) while considering link reliability, two-hop delay and power efficiency and utilizes memory and computational effective methods for estimating the link metrics. Numerical results provide insights that the protocol {{has a lower}} packet deadline <b>miss</b> <b>ratio</b> and results into longer sensor network lifetime. The {{results show that the}} proposed protocol is a feasible solution to the QoS routing problem in WSNs that support real-time applications. Â© 2013 NICT...|$|E
40|$|The paper {{presents}} a simple Markovian model for estimating coherency traffic for a multiprocessor {{system based on}} a few parameters including cache write/read fraction and read/write <b>miss</b> <b>ratios.</b> In particular, the model allows the estimation of cache invalidation traffic appearing on the bus, implicit writeback traffic and and explicit writeback traffic. The basic model is developed for a symmetric multiprocessor (SMP) system with a single level of processor cache, and then generalized for multilevel caches and clustered systems (i. e., several SMP systems connected via an interconnect that optionally provides coherency traffic filtering). Filtering involves "back invalidations", i. e., cache invalidations forced by the filter; a simple scheme is developed here for estimating the impact of back invalidations...|$|R
40|$|We {{investigated}} how {{operating system}} design should be adapted for multithreaded chip multiprocessors (CMT) – {{a new generation}} of processors that exploit thread-level parallelism to mask the memory latency in modern workloads. We determined that the L 2 cache is a critical shared resource on CMT and that an insufficient amount of L 2 cache can undermine the ability to hide memory latency on these processors. To use the L 2 cache as efficiently as possible, we propose an L 2 -conscious scheduling algorithm and quantify its performance potential. Using this algorithm it is possible to reduce <b>miss</b> <b>ratios</b> in the L 2 cache by 25 - 37 % and improve processor throughput by 27 - 45 %. 1...|$|R
40|$|Cache {{coherence}} {{activities with}} writeinvalidate protocol in Symmetric Multiprocessors not only incur overhead but may increase cache <b>miss</b> <b>ratios</b> due to unnecessary invalidations. Under software synchronization models, a lazy cache coherence protocol delays write invalidations and permits inconsistent {{copies of the}} same cache line existing in different caches. In this paper, we propose a demand-driven two-phase deferred cache coherence model which further delays writes to be observed by other processors until a processor requests the new data after certain synchronization instructions. Data dependence can be maintained by identifying when the new data must be fetched and reconciled. Cycle-by-cycle execution-driven simulation of SPLASH- 2 workload shows that the two-phase deferred coherence protocol can out-perform the eager protocol up to 30 % for some workload...|$|R
