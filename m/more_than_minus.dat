1|10000|Public
40|$|Housing is {{typically}} the greatest investment, {{and the most}} valuable asset, of any household. Overall, it dominates the household portfolios and is crucial in the accumulation of wealth over time. Since housing assets can serve as collateral, people are granted large mortgages, and thus even modest returns yield great wealth boosts due to the sheer magnitudes of the investments. Naturally, owning a home also produces housing benefits of great value to the owner-occupier. Everyone needs {{a place to live}} and owning a home secures a steady flow of dwelling services that would otherwise have had to be bought in the rental market. It is no surprise then that Norwegian households' asset portfolios are undiversified: using data from the Income Distribution Survey 2002 from Statistics Norway, we find that the average portfolio has a housing-to-wealth ratio of 1. 175, a stocks-to-wealth ratio of 0. 089, and a debt-to-wealth ratio of 0. 264. These figures mirror findings in similar surveys of other economies. Breaking the average holdings down to age cohorts shows that young households hold much more housing and debt relative to their wealth than older households. It is also evident that wealth accumulates a great deal over the life-cycle: when population average wealth is normalised to unity, the cohort relative wealth goes from 0. 074 for the youngest households, via 2. 233 for 55 - 66 year olds, and finally down to 1. 177 for households with head older than 80. The absence of equity holdings are further notable throughout the study: even the portfolios of the wealthiest quartile contain only 10 % stocks. Our key motivation is then the apparent question; is this behaviour optimal and can the allocations be rationalised by formal models? If not, what are then the theoretical recommendations? For our model experiments, the return on risk-free holdings (i. e., bank deposits, loans and bonds) is estimated by an average of Norwegian real bond rates over 1992 - 2006 less 28 % tax, and amounts to a rate of 0. 03 annually. Stock return is set to the price appreciation (less 28 % tax) plus the dividend rate on the Oslo stock exchange, less inflation, over the same time period. Housing return is similar as it also consists of a capital gain component and a dividend stream. While the former is easily observed by the house price index from Statistics Norway, the latter is not readily available due to lack of observations. However, with justification in rental data and in theory of the benefit to the owner-occupier, we settle on the before-tax, real risk-free rate as proxy for housing dividend. We further elect to follow advice from the literature on the riskiness of single-home housing investments, and set the standard deviation equal to 60 % of that in the stock market. The latter is found be 0. 2 annually for the 1992 - 2006 period. The reason for not simply using the risk exhibited by the house price index as our measure is that this aggregates market transactions and thus cannot capture the risk of buying a single house. We find mean rates of returns from housing and stocks of 0. 11 and 0. 188 respectively, with a correlation of 0. 33. Real prices on stocks and housing have appreciated tremendously since the early 1990 s. In fact, real prices were fairly stable [...] or even decreasing [...] over most of the 20 th century, before going through a boom-bust period in the mid-to-late eighties, and then literally taking off around 1993. Since then, real prices have grown by roughly 490 % and 260 % respectively, and the returns to equity- and house owners have clearly been enormous over these years. Our first attempt at explaining or rationalising the observed household behaviour mentioned above is the employment of a static mean-variance model of portfolio choice. We assume that the investor only cares about the expected return and variance of the portfolio and that the objective is to minimise this variance given a requirement on the expected return (this is of course equivalent to maximising expected return given some risk tolerance). There are two risky assets, housing and stocks, and one risk-free asset, simply called "risk-free. " The two-fund separation theorem ensures that any investor household will choose a combination of the risk-free asset and the market portfolio. While the latter is determined by the properties of the assets available to all investors, each investor will put a weight on the risky- and the risk-free portfolio according to his or her level of risk aversion. The solution to such a setup will then be a set of shares of housing, stocks and risk-free. With return estimates based on observed asset performances over 1992 - 2006 we find that the holdings of stocks and housing should be virtually equal in magnitude (the market portfolio composition of risky assets). The less risk averse the household is, the more stocks, housing and debt should it take on. More risk averse households on the other hand, should hold less of the risky assets and positive amounts of the risk-free asset (i. e., bank deposits or bond holdings). Since this result is very far from the observed holdings, we seek an alternative solution by using different return estimates. Reliable data on equity dividend and on house- and stock prices are available from 1966, so we collect asset properties for the 1966 - 1991 period and calculate a new set of estimates. The resulting portfolio model solution is uplifting: the framework now prescribes far less stocks relative to housing, which we know is closer to actual allocations. For example, with a risk aversion coefficient equal to 3 we find an "optimal" housing share of 1. 129, a share of stocks of 0. 484, and a residual risk-free share of - 0. 613. This is not very far from the observed, average household portfolio barring the weight on stocks (which is much lower in the data). Further, {{there seems to be a}} clear relationship between age and level of risk aversion. That is, for both sets of results, the high-risk aversion portfolios match the observed holdings of older households better, while the model portfolios with lower risk aversion coefficients are better fits with younger households' allocations. We have mentioned that younger households have the largest shares of housing in their overall portfolio, and that the share tends to decline with age. If we now, as an extension of the applied portfolio model, assume that the amount of housing is fixed and determined by the households' demand for housing service consumption, it may be interesting to find the optimal portfolios given the housing share of each cohort. That is, what are the optimal shares of stocks and risk-free when the household is already equipped with a certain expected return and risk from the housing holdings? Unfortunately, our analytical results make no sense in this case due to the absence of a credit constraint: the model prescribes outrageously high portfolio shares of stocks and debt. However, by imposing a limitation on how much the households are allowed to borrow ("no <b>more</b> <b>than</b> <b>minus</b> the housing holdings") in a simple numerical procedure in Excel, we obtain far more reasonable results. But, with the 1992 - 2006 estimates the model recommends an equity share (which is never higher than 0. 12 in the data) between 0. 75 and 1 for all cohorts and a debt share consistently greater than that in the data. When we instead assume that the 1966 - 1991 estimates hold, the match with data is almost perfect: low portfolio shares of stocks throughout and a share of risk-free virtually in sync with the Norwegian observations from 2002. It thus seems like people either disregard the tremendous returns of the nineties and early noughties as extraordinary and unsustainable, or that they enjoy even greater utility from housing than what we have accounted for in our efforts. In any event, the older return estimates are better able to explain the observed 2002 holdings than estimates based on asset returns seen in the years around the observation itself. Our second main exercise is the employment of a dynamic life-cycle model. This approach allows a far richer economic environment to be constructed for the household as we now can consider intertemporal investment-consumption choice and explicitly model the consequences of decisions. It is then assumed that the household receives utility from regular consumption and from housing consumption, and that the objective is, at all stages of life, to maximise the present value of the remaining lifetime utility. While there are still two risky assets and one risk-free asset, we are now adding a stream of labour income which is to be optimally allocated between investment/savings and immediate consumption. As the lifetime optimisation problem is virtually impossible to solve analytically, we resort to the well-known numerical method of value function iterations. This procedure basically entails approximating an unknown function (the present value of remaining lifetime utility [...] the value function) by identifying the values of the choice variables that yield the greatest lifetime utility at any point in time, and for all possible past investment decisions. Using this function we next find consumption- and investment rules that prescribe optimal actions given any previous set of allocations and asset returns, and thus solve the household's problem. It is further assumed that the representative household begins life at age 20, dies at age 80, and that every period in the model corresponds to five years. For computational tractability we abstract from income/human capital uncertainty and simply set the household's income equal to the (normalised) average Norwegian income in 2002 for every period/age. Since the solution is that of a representative household, we simulate the "lives" of 400 individual households, find the average allocations and interpret these as our solution. With the 1992 - 2006 asset return estimates we find that the household should invest quite heavily in housing from the get-go by taking on the maximum amount of debt (relative to the housing investment), while equity investment and regular consumption are chiefly postponed till age 35. From then on however, consumption levels grow tremendously and lie far above labour income throughout. While equity holdings also grow over the life-cycle and stabilise nicely between housing holdings and labour income, debts are responsibly paid down by the simulated households. Moreover, as the stock holdings accumulate and debts are abolished, housing stays fairly constant over time. A possible explanation is that households are using the capital gains from their housing asset to increase consumption, pay down the mortgage and invest in the stock market, rather than reinvesting it back into more housing. This makes perfectly good sense in our model, but may not be too reasonable in the real world. With the 1966 - 1991 asset return estimates we find mostly the same average profiles except for holdings of stocks and housing being consistently greater than with the 1992 - 2006 estimates. This sounds a bit curious since the former asset returns are lower than the latter: why would the households invest more now? One rationale is that the amounts invested in various assets must be higher when expected returns are lower in order for the household to attain a comfortable level of precautionary savings, and to be able to push up future consumption. On the other hand, lower returns on savings also means that future consumption is more expensive, thus households should save less and "eat" more. As our model output shows that regular consumption is pushed forward but attains lower overall levels over the life-cycle compared to the output with the 1992 - 2006 estimates, both above effects are recognised. In the final periods of these exercises, equity investments drop to zero since we assume that there is no value in leaving anything behind. But what if we instead say that the household draws utility, while alive, from bequeathing its end-of-life worth to a younger generation? Solving this modified problem we find, as expected, that the simulated households consume less and save more at the end of their lives. Such a specification is of course much closer to reality since it is commonly assumed that bequest motives exist, and because of the inherent uncertainty of the time of death (which we characteristically have abstracted from in our theoretical escapades). Finally, we compare the performance of the (bequest) model, with the two sets of estimates, to the observed Norwegian life-cycle holdings, surveyed in 2002. We find that the newer estimates allocations lie much closer to the actual holdings than those of the older estimates. This is immediately interesting because the static portfolio model produced the exact opposite conclusion: older estimates were able to better explain observed behaviour than newer ones. Even though predicted stock holdings are still wildly out of tune with observations, the dynamic model solved with the 1992 - 2006 estimates can explain the broad features of households' behaviour...|$|E
5000|$|The {{tower was}} built on a caisson with <b>more</b> <b>than</b> 900 <b>mini</b> piles.|$|R
50|$|A {{government}} bus 39A {{comes to}} Peruvilai from Nagercoil bus stand, via Nagercoil, Parvathipuram.And also <b>more</b> <b>than</b> five <b>mini</b> buses are passing through Peruvilai to Assaripallam (Government Medical College).|$|R
5000|$|... 11. The {{walls of}} the old city. 11 km of walls, <b>more</b> <b>than</b> 20 <b>mini</b> forts within it, 4 {{auxiliary}} doors, only one bridge-fort to connect {{the city to the}} mainland.|$|R
5000|$|Par is a {{scoring system}} used mostly in amateur and club golf. It {{involves}} scoring (+, 0, −) {{based on results}} at each hole. The objective {{is to have an}} end score with <b>more</b> pluses <b>than</b> <b>minuses.</b> The result on each hole is always based on one's handicap-adjusted score.|$|R
5000|$|Universities and {{corporations}} used these compilers {{and a number}} of other software products have been developed in the WATFOR tradition.For example, a version for the COBOL programming language is called WATBOL.Daniel D. McCracken said [...] "it is no exaggeration to suggest that WATFOR revolutionized the use of computers in education." [...] At one point, <b>more</b> <b>than</b> 3,000 <b>mini</b> and mainframe computer licenses and over 100,000 microcomputer licenses were held worldwide for this family of software products.|$|R
5000|$|Most of {{his novels}} are {{published}} {{in the form of}} books. Ghazi, Dosra Janam, Tareekh-e-Khandhar, Mataa-e-Dil-o-Jaan, Masiha, Shaheen Sifat and Muqadas Taboot are best sellers.He won [...] "Dosheza Writer Award" [...] two times in last the ten years, which proves his popularity and reader ship of his novels and stories.Pervez Bilgrami is the only writer from Pakistan who writes mini stories. Mini stories term is used for the stories which consist of one page and are known as [...] "Mini Kahani" [...] or Short Story. He has written <b>more</b> <b>than</b> 150 <b>mini</b> stories which are uploaded on many social websites and Urdu forum.|$|R
50|$|<b>More</b> <b>than</b> a <b>mini</b> ice age in 2013-2041, Hamaker's {{immediate}} {{concern was the}} shortening of the growing season from the coming glacial period, which he believed could be forestalled through rock dusting, resulting in more abundant yields at harvest. He believed the coming glacial period would preceded by an interglacial-to-glacial transition phase already underway since the 1970s, and strongly advocated an intensive global co-operative soil remineralization effort to maintain the quantity of food while improving its quality. To achieve this, he recommended simultaneous remineralization of dying forests and soils, also needed to grow bio-fuels, {{as part of a}} goal to return excessive carbon dioxide to stable interglacial levels of 280 ppm.|$|R
30|$|The {{molecular}} methods {{developed for}} differentiation of C. minus simile and C. minus verum {{can be used}} to rapidly identify these morphotypes from cultures or from infected needles, which reduces the time otherwise required for morphological identification. Analysis using these tools for characterisation of C. minus sensu lato, isolated from P. radiata plantations in New Zealand since 1969, revealed that C. minus simile was <b>more</b> common <b>than</b> C. <b>minus</b> verum.|$|R
40|$|After 20 {{years since}} {{the first wave of}} roundabouts in Slovenia, there are {{currently}} <b>more</b> <b>than</b> 500 roundabouts (<b>mini,</b> single lane, Multi lane and Turbo) installed all over the country. Many more are expected in line with the new Slovenian Roundabout Design Regulations which has provided an opportunity for a fresh look at roundabouts in the Republic of Slovenia. This article outlines the Slovenian experiences with respect to roundabout utilization in urban areas...|$|R
40|$|It is {{sometime}} {{suggested in}} the disaster planning literature that rather than evacuating potential victims of a radiation fallout from a nuclear plant accident, they should instead be taught to take in-place sheltering. This issue is examined with respect to such health care facilities as hospitals and nursing homes. The focus is especially on the perceptions or social psychological factors that would be operative among the major social actors, namely the patients, their relatives, the staff members of the facilities, and other organizational personnel who {{would be involved in}} either of the two processes. The social science research literature on the matter collected at the Disaster Research Center was used to make an evaluation of {{the pros and cons of}} evacuating or sheltering. The data strongly points to the strong probability that there are far <b>more</b> plus <b>than</b> <b>minuses</b> on planning for evacuation than in-place sheltering...|$|R
30|$|In[3], {{the authors}} {{proposed}} a scheduling approach, where the CFP is always splitted into 16 mini slots. The basic {{idea is to}} exploit {{the fact that the}} GTS descriptor has a structure not fully utilized in the standard scheme and to propose a new mapping to represent nine new slots. The nodes in the PAN must be aware of this new mapping for the GTS allocation. Following the same ideas, in[4], the authors presented a more flexible approach and proposed a CFP divided into <b>more</b> <b>than</b> 16 <b>mini</b> time slots for periodic real-time message allocation. This proposal offers tighter delay bounds and improves the GTS utilization through an off-line bandwidth allocation algorithm. With both approaches[3, 4], it is possible to extend the GTS allocation to support applications that require a slightly higher number of slots than the original IEEE 802.15. 4 specification.|$|R
5000|$|Wilson is {{from the}} American South (Atlanta, North Carolina, and Danville California) and was a {{community}} organizer before switching careers. She is a volunteer board member for the West 3rd Street Business Association, has donated <b>more</b> <b>than</b> 100-dozen <b>mini</b> bundts to schools and charities, partnered with nonprofits to help emerging entrepreneurs, hosted interns from local urban high schools to teach them about entrepreneurship, {{and served as a}} volunteer panelist to speak about her experiences in community building and [...] "opening her bakery to inspire young people and adults to chase their dreams and follow their hearts." [...] She has stated that, [...] "I know there aren't very many African-American-owned businesses in the Beverly Hills and West Hollywood areas where my bakery is located," [...] adding that she feels fortunate to have been able get her business opened and established in the community.|$|R
40|$|This paper {{describes}} a dual beam thruster {{that has been}} designed, constructed, and tested. The system is suitable for two-axes attitude control and is comprised of two orthogonal strips, each capable of producing 0. 30 mlb thrust and beam deflections of <b>more</b> <b>than</b> plus or <b>minus</b> 20 deg. The nominal specific impulse for the thruster is 5000 sec, and the thrust level from each strip can be varied from 0 to 100 %. Neutralizer filaments that were developed and life tested over 2000 hours producing <b>more</b> <b>than</b> 40 mA of electron emission per watt of input power are also discussed. The system power required for clean ionizers is approximately 200 W...|$|R
40|$|Very small, {{individual}} polymeric microspheres {{with very}} precise size {{and a wide}} variation in monomer type and properties are produced by deploying a precisely formed liquid monomer droplet, suitably an acrylic compound such as hydroxyethyl methacrylate into a containerless environment. The droplet which assumes a spheroid shape is subjected to polymerizing radiation such as ultraviolet or gamma radiation as it travels through the environment. Polymeric microspheres having precise diameters varying no <b>more</b> <b>than</b> plus or <b>minus</b> 5 percent from an average size are recovered. Many types of fillers including magnetic fillers may be dispersed in the liquid droplet...|$|R
40|$|The {{objective}} of this program is to develop electrical resistance strain gages which will permit the measurement of static strains on nickel and cobalt superalloy parts inside gas turbine engines running on a test stand. The specific goal {{is to develop a}} complete system able to make strain measurements up to plus or minus 2000 mu strain with a total error of no <b>more</b> <b>than</b> plus or <b>minus</b> 10 percent over a 50 hour period at 1250 K. The initial part of this work consisted of a strain gage alloy development effort in which a variety of alloys were evaluated after being prepared by drop-casting or splat cooling...|$|R
5000|$|The film {{had a huge}} hype {{prior to}} its release due to Tamil cinema {{veterans}} K.Balachander and Bharathiraja coming {{together for the first}} time together in a film. The film received poor reviews from almost all reviewers for its mediocre script, amateurish screenplay, acting and non-existing entertainment factor. Sify labelled it as one of the [...] "Biggest Bore" [...] of 2010, The Hindu called it as a different film with <b>more</b> <b>minuses</b> <b>than</b> pluses. The film was one of the biggest flops of 2010 and was a major financial blow for director Shankar and his production house S Pictures.|$|R
40|$|The {{objective}} {{is to develop a}} new thin film resistance strain gage system which will be suitable for use inside gas turbine engines on blades or vanes at temperatures up to 1250 K. These gages are to be capable of making strain measurements to plus or minus 2000 microstrain with total errors of no <b>more</b> <b>than</b> plus or <b>minus</b> 10 percent during a 50 hour period. In addition to survival and stability in this hostile environment, attaining a low temperature coefficient of resistance, of the order of 20 ppm/K or less, is an important goal. This requirement arises from the presently unavoidable uncertainties in the measurement of exact temperatures inside gas turbine engines for use in making corrections for apparent strain...|$|R
5000|$|... the {{grazing fee}} [...] "equals the $1.23 base {{established}} by the 1966 Western Livestock Grazing Survey multiplied by {{the result of the}} Forage Value Index (computed annually from data supplied by the Statistical Reporting Service) added to the Combined Index (Beef Cattle Price Index minus the Prices Paid Index) and divided by 100; provided, that the annual increase or decrease in such fee for any given year shall be limited to not <b>more</b> <b>than</b> plus or <b>minus</b> 25 percent of the previous year's fee, and provided further, that the fee shall not be less than $1.35 per animal unit month." [...] Apart from the $1.35 floor, which took effect on February 14, 1986, this {{is the same as the}} fee calculation that applied for the grazing years 1979 through 1985.|$|R
40|$|The native form of Drosophila melanogaster DNA topoisomerase II was {{purified}} from Schneider's S 3 {{tissue culture}} cells and studied with two supercoiled minicircle preparations, mini and mini-CG, 354 bp and 370 bp in length, respectively. Mini-CG contains a d(CG) 7 insert which assumes a left-handed Z-DNA conformation in negative supercoiled topoisomers {{with a negative}} linking number difference - delta Lk {{greater than or equal}} to 2. The interactions of topoisomerase II with topoisomer families of mini and mini-CG were studied by band-shift gel electrophoresis in which the individual topoisomers and their discrete or aggregated protein complexes were resolved. A monoclonal anti-Z-DNA IgG antibody (23 B 6) bound and aggregated only mini-CG, thereby confirming the presence of Z-DNA. Topoisomerase II bound and relaxed mini-CG <b>more</b> readily <b>than</b> <b>mini.</b> In both cases, there was a preference for more highly negatively supercoiled topoisomers. The topoisomerase II inhibitor VM- 26 induced the formation of stable covalent DNA-protein intermediates. In addition, the non-hydrolyzable GTP analogue GTP gamma S inhibited the binding and relaxation activities. Experiments to detect topoisomerase cleavage sites failed to elicit specific loci on either minicircle preparation. We conclude that Drosophila topoisomerase II is able to bind and process small minicircles with lengths as short as 360 bp and negative superhelix densities, - sigma, which can exceed 0. 1. Furthermore, the enzyme has a preferential affinity for topoisomers containing Z-DNA segments and relaxes these molecules, presumably by cleavage external to the inserts. Thus, a potentially functional relationship between topoisomerase II, an enzyme regulating the topological state of DNA-chromatin in vivo, and left-handed Z-DNA, a conformation stabilized by negative supercoiling, has been established...|$|R
5000|$|DataPlay, a {{proprietary}} write-once mini optical disc format which is even smaller <b>than</b> <b>mini</b> CD media ...|$|R
40|$|How is {{fuzzy logic}} usually formalized? There are many {{seemingly}} reasonable requirements that a logic should satisfy: e. g.,   ¢¡¤£ since and £¥¡¤ are the same, the corresponding and-operation should be commutative. Sim-ilarly, since  ¦¡ ¤  means the same as, {{we should expect}} that the and-operation should also satisfy this property, etc. It {{turns out to be}} impossible to satisfy all these seemingly natural requirements, so usually, some requirements are picked as absolutely true (like commutativity or associativity), and others are ignored if they contradict to the picked ones. This idea leads to a neat mathematical theory, but the analysis of real-life expert reasoning shows that all the requirements are only approximately satisfied. we should require all of these requirements to be satisfied to some extent. In this paper, we show the preliminary results of analyzing such operations. In particular, we show that non-associative operations explain the empirical §©¨� � law in psychology according to which a person can normally distinguish between no <b>more</b> <b>than</b> 7 plus <b>minus</b> 2 classes...|$|R
40|$|Results on the {{performance}} of solar cells fabricated on wafers from multiple silicon ingots of large diameter, grown by using a single crucible and a sequential melt replenishment Czochralski (CZO) technique are presented. Samples were analyzed for resistivity, dislocation density and impurity content. Solar cells were fabricated from the seed, center and tang end of each ingot to evaluate the growth reproducibility and material quality. The cell efficiency within a given wafer varies by no <b>more</b> <b>than</b> plus or <b>minus</b> 5 % of the average value. A small but consistent decrease in the cell efficiency is observed from the first to the fourth ingot grown from a single crucible. This decrease may be related to an increase in impurity content or dislocation density or a combination of both. The efficiency of the cells fabricated from the tang end of the fourth ingot is about 10 % lower than that of the control cell. An impurity effects model is employed to correlate this decrease in efficiency with the impurity build-up in the residual melt...|$|R
2500|$|A {{frequency}} response diagram plots the microphone sensitivity in decibels over {{a range of}} frequencies (typically 20Hz to 20kHz), generally for perfectly on-axis sound (sound arriving at 0° to the capsule). Frequency response may be less informatively stated textually like so: [...] "30Hz–16kHz±3dB". This is interpreted as meaning a nearly flat, linear, plot between the stated frequencies, with variations in amplitude of no <b>more</b> <b>than</b> plus or <b>minus</b> 3dB. However, one cannot determine from this information how smooth the variations are, nor in what parts of the spectrum they occur. Note that commonly made statements such as [...] "20Hz–20kHz" [...] are meaningless without a decibel measure of tolerance. Directional microphones' {{frequency response}} varies greatly with distance from the sound source, and with the geometry of the sound source. IEC60268-4 specifies that frequency response should be measured in plane progressive wave conditions (very {{far away from the}} source) but this is seldom practical. Close talking microphones may be measured with different sound sources and distances, but there is no standard and therefore no way to compare data from different models unless the measurement technique is described.|$|R
5000|$|A {{frequency}} response diagram plots the microphone sensitivity in decibels over {{a range of}} frequencies (typically 20 Hz to 20 kHz), generally for perfectly on-axis sound (sound arriving at 0° to the capsule). Frequency response may be less informatively stated textually like so: [...] "30 Hz-16 kHz ±3 dB". This is interpreted as meaning a nearly flat, linear, plot between the stated frequencies, with variations in amplitude of no <b>more</b> <b>than</b> plus or <b>minus</b> 3 dB. However, one cannot determine from this information how smooth the variations are, nor in what parts of the spectrum they occur. Note that commonly made statements such as [...] "20 Hz-20 kHz" [...] are meaningless without a decibel measure of tolerance. Directional microphones' {{frequency response}} varies greatly with distance from the sound source, and with the geometry of the sound source. IEC 60268-4 specifies that frequency response should be measured in plane progressive wave conditions (very {{far away from the}} source) but this is seldom practical. Close talking microphones may be measured with different sound sources and distances, but there is no standard and therefore no way to compare data from different models unless the measurement technique is described.|$|R
30|$|Dummy {{variable}} for an output gap lower <b>than</b> <b>minus</b> 4 % of GDP: This variable is generated {{based on the}} European Commission’s estimate of the output gap (source: AMECO Database, European Commission, DG ECFIN).|$|R
40|$|In this study, {{irradiation}} copolymer {{of natural}} rubber latexmethyl methacrylate (NRL-co-MMA) was added into recovery synthetic and mineral lubricant oil as samples at concentration of 0. 25 %, 0. 70 %, 1 %, 3 %, 5 %, 7 %, 10 %. The viscosity index and kinematic viscosity of samples {{were compared to}} commercial lubricant oils. The results shown that viscosity index of recovery sinthetyc lubricant oil increased <b>more</b> <b>than</b> 50 % at 0. 25 % addition of NRL-co-MMA. At similar addition of NRL-co-MMA, the viscosity index of mineral lubricant oil increased up to 11 %. Pour point of samples were less <b>than</b> <b>minus</b> 36, means fulfill the Society of Automotive Engineers (SAE) standard. The total base number of lubricant samples increased significantly by NRL-co-MMA addition...|$|R
40|$|As {{a result}} of {{interest}} by the U. S. Department of Energy, the Bureau of Mines conducted studies on comminution of bituminous coal to the 2 -um particle size range using the Bureau-developed turbomill. Both "plant grind" size (80 pct minus 75 um) and microsize (minus 10 um) coal can be produced in a single step with the Bureau's turbomill. In 15 min, minus 2. 4 -mm coal milled in water with steel shot was reduced tc 65 pct minus 75 m and 26 pct minus 2 m, with an energy requirement of 139 kW^h/mt coal. Plant-grind size coal milled in water, with Ottawa sand as the milling medium, was reduced to <b>more</b> <b>than</b> 45 pct <b>minus</b> 2 m in 15 min, at 175 kW 22 ̆ 0 ac 2 h/mt. Plant-grind coal milled with steel shot was reduced to 57 pct minus 2 um in 15 min, at 138 kW 22 ̆ 0 ac 2 h/mt. When diesel fuel was substituted for water, the milling of the coal was less effective and less energy efficient. Keeping the pulp dispersed was critical. Without an effective dispersant, the slurry became very viscous with increased milling time, resulting in higher energy requirements, low heat dissipation, and poor grinding efficiency...|$|R
40|$|Ceramic {{tantalum}} carbide artifacts with high thermal shock and mechanical erosion resistance {{are provided by}} incorporating tungsten-rhenium and carbon particles in a {{tantalum carbide}} matrix. The mix is sintered by hot pressing to form the ceramic article which has a high fracture strength relative to its elastic modulus and thus has an improved thermal shock and mechanical erosion resistance. The tantalum carbide is preferable less <b>than</b> <b>minus</b> 100 mesh, the carbon particles are preferable less <b>than</b> <b>minus</b> 100 mesh, and the tungsten-rhenium particles are preferable elongate, having a length to thickness ratio of at least 2 / 1. Tungsten-rhenium wire pieces are suitable as well as graphite particles...|$|R
50|$|Compact {{sport utility}} vehicle, {{also known as}} Compact SUV, is a class of small sport utility {{vehicles}} that is larger <b>than</b> <b>mini</b> SUVs but smaller than mid-size SUVs with a length roughly between 4.25 to 4.60 m.|$|R
60|$|Euler’s Algebra, a book {{otherwise}} {{of great}} merit, but full, to overflowing, of logical errors {{in respect to}} {{the foundation of the}} science, contains the following argument to prove that minus multiplied by minus gives plus, a doctrine the opprobrium of all mere mathematicians, and which Euler had not a glimpse of the true method of proving. He says minus multiplied by minus can not give minus; for minus multiplied by plus gives minus, and minus multiplied by minus can not give the same product as minus multiplied by plus. Now one is obliged to ask, why minus multiplied by minus must give any product at all? and if it does, why its product can not be {{the same as that of}} minus multiplied by plus? for this would seem, at the first glance, not <b>more</b> absurd <b>than</b> that <b>minus</b> by minus should give the same as plus by plus, the proposition which Euler prefers to it. The premise requires proof, as much as the conclusion; nor can it be proved, except by that more comprehensive view of the nature of multiplication, and of algebraic processes in general, which would also supply a far better proof of the mysterious doctrine which Euler is here endeavoring to demonstrate.|$|R
5000|$|If r {{is greater}} than one or less <b>than</b> <b>minus</b> one {{the terms of the}} series become larger and larger in magnitude. The sum of the terms also gets larger and larger, and the series has no sum. (The series diverges.) ...|$|R
40|$|About 4. 5 {{revolutions}} {{of laser}} altimetry {{were obtained by}} Apollo 15. This altimetry indicates a 2 -km displacement {{of the center of}} mass from the center of figure toward the earthside. The terrae are quite rough, with frequent changes of 1 km or more in successive altitudes at about 33 -km intervals. The mean altitude of terrae above maria is about 3 km with respect to the center of mass, indicating a thickness of about 24 km for a high-alumina crust. The maria are extremely level, with elevations varying not <b>more</b> <b>than</b> plus or <b>minus</b> 150 m about the mean over some stretches of 200 to 600 km. However, different maria have considerably different mean elevations. The largest unanticipated feature found is a 1400 km wide depression centered at about 180 deg longitude, and 2 km deep with respect to a 1737 -km sphere (about 6 km deep with respect to the surrounding terrae). This basin has the appearance of typical terrae, although there are indications of a ring structure of about 600 -km radius in the Orbiter photography. Altitudes across circum-Orientale features suggest that Mare Orientale is also a deep basin. The data appear to corroborate a model of early large-scale differentiation of a crust, followed a considerable time later by short intense episodes of mare filling with low viscosity lavas...|$|R
5000|$|The Kiewit Construction Company, {{based in}} Omaha, Nebraska, {{was awarded the}} {{contract}} for phase two, redesigning the Anton Anderson Tunnel. Kiewit began planning the tunnel design in June 1998, and began work on the project sometime around September. The {{first part of the}} tunnel construction involved vertically and horizontally expanding the existing rock walls. Beginning from the western entrance, Kiewit drilled away several feet of the rock face {{from the top of the}} tunnel and installed a net to prevent any potential rockfalls. They then drilled sideways, clearing space for the nine vehicle turnaround areas. However, work on the tunnel was hindered by several different events. While crews were working on the tunnel, a drunken Whittier resident drove his or her truck into the tunnel and got it stuck on the rails. On October 23, a thirteen-car train derailed at the western entrance. Although no workers were injured, a substantial amount of the equipment was destroyed. In addition to the accidents, crews had to work in extreme weather. Kiewit claims that workers had to deal with [...] "winds of <b>more</b> <b>than</b> 120 mph, <b>minus</b> 40 degree temperatures and snow up to 43 feet deep" [...] and wind chills that would drop to around -80 F. An avalanche also at one point halted construction for four days.|$|R
50|$|In {{the fluid}} flow field around a body {{there will be}} points having {{positive}} pressure coefficients up to one, and negative pressure coefficients including coefficients less <b>than</b> <b>minus</b> one, but nowhere will the coefficient exceed plus one because the highest pressure {{that can be achieved}} is the stagnation pressure.|$|R
50|$|The {{show itself}} {{has been on the}} air for <b>more</b> <b>than</b> sixteen years and {{originally}} started as a half-hour show (less <b>than</b> 20 minutes <b>minus</b> commercials) on Saturday nights on New York City's 77 WABC, and has since broadcast on other heritage stations such as 710 WOR in New York City and KLSX in Los Angeles.|$|R
