57|341|Public
5000|$|Metadata: {{provides}} a <b>metadata</b> <b>service</b> provider interface and implementation that includes dependency registration and caching ...|$|E
50|$|AMGA as a <b>{{metadata}}</b> <b>service</b> {{allows users}} to attach metadata information to files stored on the Grid, where metadata can be any relationally organized data typically stored in a relational database system (RDBMS). In addition, the metadata in AMGA can also be stored independently of any associated files, which allows AMGA {{to be used as}} a general access tool to relational databases on the Grid. AMGA features a simple to learn metadata access language, which has been very useful for the adoption of AMGA in smaller Grid applications, as it considerably lowers the technical hurdle to make use of relational data. Access via SQL92 is also supported.|$|E
40|$|The Belle II {{experiment}} {{is expected to}} produce 50 times more data than the existing Belle experiment. Such huge data production requires not only scalability {{with respect to the}} storage service but also scalability regarding the <b>metadata</b> <b>service.</b> There has already been a <b>metadata</b> <b>service</b> at the Belle experiment, but it is not proper for the Belle II experiment because it has scalability problems and it is not intended to be used in a distributed grid environment. To deal with these issues, we designed an advanced <b>metadata</b> <b>service</b> system based on AMGA, which provides eﬃcient and scalable metadata searching. We have built testbed sites to test the correctness, performance and scalability of the advanced <b>metadata</b> <b>service</b> system, and it has been proved to be able to provide eﬃcient metadata searching for the Belle II experimen...|$|E
40|$|Survey {{instrument}} {{administered by}} the University of North Texas (UNT) Libraries Cataloging and <b>Metadata</b> <b>Services</b> Department from October 27 through November 17, 2014. The purpose of this research study is to assess how Public Service employees rate the importance of Cataloging and <b>Metadata</b> <b>Services</b> activities, and how satisfied Public Service employees are with Cataloging and <b>Metadata</b> <b>Services</b> activities...|$|R
40|$|Poster {{presentation}} for the Association of College and Research Libraries (ACRL) Annual Conference. This poster {{summarizes the}} results of a survey administered by the University of North Texas (UNT) Libraries Cataloging and <b>Metadata</b> <b>Services</b> Department from October 27 through November 17, 2014. The purpose of the research study was to assess how Public Service employees rate the importance of Cataloging and <b>Metadata</b> <b>Services</b> activities, and how satisfied Public Service employees are with Cataloging and <b>Metadata</b> <b>Services</b> activities...|$|R
5000|$|<b>Metadata</b> <b>services,</b> which {{handle all}} {{metadata}} related to entities, relationships and mappings.|$|R
40|$|Abstract. The {{imperative}} {{demand on}} {{the description of}} semantic metadata and the processing of real-time data presents unique challenge to Grid <b>Metadata</b> <b>Service.</b> Grid Monitoring Architecture (GMA), which is a framework for dynamic data management, is limited by its conventional interface of relational database and therefore fails {{to address the problem}} of interoperability. Faced with the problem of metadata publishing in GMA, we present a new publishharvest protocol for semantic metadata called GMA-PSMH (Grid Monitoring Architecture-Protocol for Semantic Metadata Harvesting) by modifying the OAI-PMH metadata harvest framework. As part of the Semantic <b>Metadata</b> <b>Service</b> Project in Peking University, the associated dynamic metadata management framework is then implemented according to the above protocol. At the end, we make the conclusion and overview the future work. ...|$|E
40|$|Based on the {{analysis}} and research on the current geographic information sharing and metadata service，we design, develop and deploy a distributed <b>metadata</b> <b>service</b> system based on GeoNetwork covering more than 30 nodes in provincial units of China [...] By identifying the advantages of GeoNetwork, we design a distributed <b>metadata</b> <b>service</b> system of national surveying and mapping results. It consists of 31 network nodes, a central node and a portal. Network nodes are the direct system metadata source, and are distributed arround the country. Each network node maintains a <b>metadata</b> <b>service</b> system, responsible for metadata uploading and management. The central node harvests metadata from network nodes using OGC CSW 2. 0. 2 standard interface. The portal shows all metadata in the central node, provides users {{with a variety of}} methods and interface for metadata search or querying. It also provides management capabilities on connecting the central node and the network nodes together. There are defects with GeoNetwork too. Accordingly, we made improvement and optimization on big-amount metadata uploading, synchronization and concurrent access. For metadata uploading and synchronization, by carefully analysis the database and index operation logs, we successfully avoid the performance bottlenecks. And with a batch operation and dynamic memory management solution, data throughput and system performance are significantly improved; For concurrent access,, through a request coding and results cache solution, query performance is greatly improved. To smoothly respond to huge concurrent requests, a web cluster solution is deployed. This paper also gives an experiment analysis and compares the system performance before and after improvement and optimization. Design and practical results have been applied in national <b>metadata</b> <b>service</b> system of surveying and mapping results. It proved that the improved GeoNetwork service architecture can effectively adaptive for distributed deployment requirements, performance improvement and optimization of the system guarantee its continuous and stable running on the internet...|$|E
40|$|Most recent {{distributed}} file {{systems have}} adopted architecture with an independent metadata server cluster. However, potential multiple hotspots and flash crowds access patterns often cause a <b>metadata</b> <b>service</b> that violates performance Service Level Objectives. To maximize the throughput of the <b>metadata</b> <b>service,</b> an adaptive request load balancing framework is critical. We present a distributed cache framework above the distributed metadata management schemes to manage hotspots rather than managing all metadata to achieve request load balancing. This benefits the metadata hierarchical locality {{and the system}} scalability. Compared with data, metadata has its own distinct characteristics, such as small size and large quantity. The cost of useless metadata prefetching is much less than data prefetching. In light of this, we devise a time period-based prefetching strategy and a perfecting-based adaptive replacement cache algorithm to improve {{the performance of the}} distributed caching layer to adapt constantly changing workloads. Finally, we evaluate our approach with a hadoop distributed file system cluster. © 2013 IEEE. Most recent distributed file systems have adopted architecture with an independent metadata server cluster. However, potential multiple hotspots and flash crowds access patterns often cause a <b>metadata</b> <b>service</b> that violates performance Service Level Objectives. To maximize the throughput of the <b>metadata</b> <b>service,</b> an adaptive request load balancing framework is critical. We present a distributed cache framework above the distributed metadata management schemes to manage hotspots rather than managing all metadata to achieve request load balancing. This benefits the metadata hierarchical locality and the system scalability. Compared with data, metadata has its own distinct characteristics, such as small size and large quantity. The cost of useless metadata prefetching is much less than data prefetching. In light of this, we devise a time period-based prefetching strategy and a perfecting-based adaptive replacement cache algorithm to improve the performance of the distributed caching layer to adapt constantly changing workloads. Finally, we evaluate our approach with a hadoop distributed file system cluster. © 2013 IEEE...|$|E
40|$|Presentation for the 2015 Texas Library Association (TLA) Annual Conference. This {{presentation}} {{discusses the}} results of a survey that was administered from October 27 through November 17, 2014. The survey assessed how Public Services employees rate the importance of Cataloging and <b>Metadata</b> <b>Services</b> activities, and how satisfied Public Services employees are with Cataloging and <b>Metadata</b> <b>Services</b> activities...|$|R
5000|$|... abcde - a commandline tool, {{also for}} Linux, that also frontends popular {{libraries}} and <b>metadata</b> <b>services.</b>|$|R
50|$|Earlier in 2014, Moodagent {{announced}} {{a deal with}} Universal Music Publishing Group for the delivery of <b>metadata</b> <b>services.</b>|$|R
40|$|Abstract: This paper {{presents}} {{the design and}} implementation of a distributed data repository for Grid environments that supports secure sharing of possibly confidential data by members of ad-hoc created groups. The system is composed of three independent services- <b>metadata</b> <b>service,</b> replica locator service and storage service. The group-based access control is achieved through augmentation of the user credentials required by the back-end storage service using a chain of authorization assertions added by anticipating services. The <b>metadata</b> <b>service</b> introduces a very flexible system for storing and querying the metadata. The metadata schemas define a variable length vector of attributes {{that can be easily}} modified by introducing new versions without any intervention of the system administrator. In addition, the user may organize the metadata into a tree hierarchy enabling the creation of custom views of data collections that are independent of the actual organization of the storage...|$|E
40|$|The cost of Byzantine Fault Tolerant (BFT) {{storage is}} the main concern {{preventing}} its adoption in practice. This cost stems from {{the need to maintain}} at least 3 t+ 1 replicas in different storage servers in the asynchronous model, so that t Byzantine replica faults can be tolerated. In this paper, we present MDStore, the first fully asynchronous read/write BFT storage protocol that reduces the number of data replicas to as few as 2 t+ 1, maintaining 3 t+ 1 replicas of metadata at (possibly) different servers. At the heart of MDStore store is its <b>metadata</b> <b>service</b> that is built upon a new abstraction we call timestamped storage. Timestamped storage both allows for conditional writes (facilitating the implementation of a <b>metadata</b> <b>service)</b> and has consensus number one (making it implementable wait-free in an asynchronous system despite faults). In addition to its low data replication factor, MDStore offers very strong guarantees implementing multi-writer multi-reader atomic wait-free semantics and tolerating any number of Byzantine readers and crash-faulty writers. We further show that MDStore data replication overhead is optimal; namely, we prove a lower bound of 2 t+ 1 on the number of data replicas that applies even to crash-tolerant storage with a fault-free <b>metadata</b> <b>service</b> oracle. Finally, we prove that separating data from metadata for reducing the cost of BFT storage is not possible without cryptographic assumptions. However, our MDStore protocol uses only lightweight cryptographic hash functions...|$|E
40|$|PowerPoint {{slides and}} text of {{presentation}} {{given at the}} 2007 Oregon Library Association Conference on April 19, {{as part of a}} session co-presented with Mary Grenci, Interim Head of <b>Metadata</b> <b>Service</b> & Digital Projects, University of Oregon Libraries. An analysis of three key factors in the development of successful public interfaces to online digital collections: Redundancy, Interoperability, and Creation of Structure through Metadata...|$|E
40|$|XML-based <b>metadata</b> {{information}} <b>services</b> are {{a crucial}} core service needed by Problem Solving Environments built over emerging service-based, globally-scaled distributed systems, as {{envisioned by the}} Open Grid Services Architecture and the Semantic Web. Developing user interfaces and service bindings for manipulating instances of particular schemas is thus extremely important {{and needs to be}} made as simple as possible. In this paper we describe procedures for automating the creation of Web Service environments {{that can be used to}} simplify the creation and deployment of schema-based <b>metadata</b> <b>services...</b>|$|R
40|$|In 2011, {{one of the}} authors, a {{staff member}} of the <b>Metadata</b> <b>Services</b> Department at the University of Maryland, College Park, created an {{electronic}} resources cataloging management database (ERCM) to manage the details of MARC record set loads to the online catalog. After attending the NISO Standards update session entitled “The NISO ERM Data Standards and Best Practices Review” presentation at the 2012 Annual Conference of the American Library Association, at which cataloging workflow support {{was referred to as}} a problem area in electronic resources management, she decided to follow up with an investigation {{of the nature of the}} problem and to explore its relevancy to the ERCM. This article will inform <b>metadata</b> <b>services</b> departments about the management of constantly changing electronic resources cataloging workflows and also discuss cataloging workflow as it pertains to Electronic Resources Management System (ERMS) development...|$|R
40|$|High {{availability}} {{data storage}} systems {{are critical for}} many applications as research and business become more data-driven. Since metadata management is essential to system availability, multiple <b>metadata</b> <b>services</b> are used to improve the availability of distributed storage systems. Past research focused on the active/standby model, where each active service {{has at least one}} redundant idle backup. However, interruption of service and even some loss of service state may occur during a fail-over depending on the used replication technique. In addition, the replication overhead for multiple <b>metadata</b> <b>services</b> can be very high. The research in this paper targets the symmetric active/active replication model, which uses multiple redundant service nodes running in virtual synchrony. In this model, service node failures do not cause a fail-over to a backup and there is no disruption of service or loss of service state. A fast delivery protocol is further discussed to reduce the latency of the...|$|R
40|$|Abstract:- After concise {{introduction}} of Software test and its significance, an integration environment of Distributed Test Platform for software testing is proposed and its two departments, including Test Server and Test Driver, are described subsequently. The composition of Test Server based on <b>Metadata</b> <b>Service</b> is presented. The Metadata Service’s element and its runtime principle {{are described in}} details. Finally summarization is listed...|$|E
40|$|Abstract—In cluster file systems, the {{metadata}} {{management is}} critical to the whole system. Past researches mainly focus on journaling which alone is not enough to provide high-available <b>metadata</b> <b>service.</b> Some others try to use replication, but the extra latency accompanied is a main problem. To guarantee both availability and efficiency, we propose a mechanism for building highly available metadata servers based on replication, which integrates Paxos algorithm effectively into <b>metadata</b> <b>service.</b> The Packed Multi-Paxos is proposed to reduce the latency brought by replication, which is self-adaptive and can make the replication to achieve high throughput under heavy client load and low latency under light client load. By designing efficient architecture and coordination mechanism, all replica server nodes simultaneously provide metadata read-access service. This high-available mechanism could decrease the impact of server failures and there is no interruption of service. The performance results show that the latency caused by replication and redundancy is well under control, and the performance of metadata read operation gains improvement. I...|$|E
40|$|Abstract This paper {{sketches}} {{the design}} of the Eliot File System(Eliot), a mutable filesystem that maintains the pure immutability of its peer-to-peer (P 2 P) substrate by isolatingmutation in an auxiliary <b>metadata</b> <b>service.</b> The immutability of address-to-content bindings has sev-eral advantages in P 2 P systems. However, mutable filesystems are desirable because they allow clients to update ex-isting files; a necessary property for many applications. In order to facilitate modifications, the filesystem mustprovide some atom of mutability. Since this atom of mutability is a fundamental characteristic of the filesystem and notthe underlying storage substrate, {{it is a mistake to}} violate the integrity of the substrate with special cases for mutabil-ity. Instead, Eliot employs a separate, generalized <b>metadata</b> <b>service</b> that isolates all mutation and client state in an aux-iliary replicated database. Eliot provides fine-granularity file updates with either AFS open-close or NFS-like con-sistency semantics. Eliot builds a mutable filesystem on a global resource bed of purely immutable P 2 P block storage...|$|E
40|$|With {{funding from}} the Andrew W. Mellon Foundation, the University of Rochester and partner institutions, the University of Rochester River Campus Libraries is {{developing}} open-source software to provide libraries with an alternative way to reveal their collections to users. The eXtensible Catalog (XC) will run alongside a library's current Integrated Library System to provide more intuitive access to resources, a customizable user interface, and a platform for managing a variety of <b>metadata</b> <b>services.</b> This presentation will include {{an overview of the}} XC Project and then will focus upon how using XC software will allow libraries to take control of their legacy metadata to enable it to be used in new web environments and in combination with other metadata. This is accomplished through the use of the XC Schema, which applies aspects of linked data, FRBR, and RDA to legacy metadata; and through XC's <b>Metadata</b> <b>Services</b> Toolkit, which provides a platform for scheduling and managing iterative <b>metadata</b> <b>services</b> to normalize, transform, and aggregate metadata. Jennifer Bowen is a co-principal investigator for the eXtensible Catalog Project at the University of Rochester. She has spent her career as a librarian working in the areas of cataloging and metadata, initially as a specialist in music cataloging and more recently as a manager of cataloging and technical services. In recent years she has focused her attention upon metadata standards development (RDA), the FRBR data model, and the use of legacy library metadata in next-generation discovery systems...|$|R
5000|$|SAML {{actors are}} Identity Providers (IdP), Service Providers (SP), Discovery <b>Services,</b> ECP Clients, <b>Metadata</b> <b>Services,</b> or Broker/IDP-proxy. This table shows the {{capability}} of products according to Kantara Initiative testing. Claimed capabilities are in column [...] "other". Each mark denotes {{that at least one}} interoperability test was passed. Detailed results with product and test procedure versions are available at the Kantara/Liberty site given below.|$|R
40|$|In 2015, the Cataloging and <b>Metadata</b> <b>Services</b> {{department}} of Rice University’s Fondren Library developed {{a process to}} reconcile four years of authority headings against an internally developed thesaurus. With a goal of immediate cleanup {{as well as an}} ongoing maintenance procedure, staff developed a “hack” of OpenRefine’s normal Reconciliation function that ultimately yielded 99. 6 % authority reconciliation and a stable process for monthly data verification...|$|R
40|$|An efficient, {{secure and}} {{interoperable}} data platform solution {{has been developed}} in the TESSA project to provide fast navigation {{and access to the}} data stored in the data archive, as well as a standard-based metadata management support. The platform mainly targets scientific users and the situational sea awareness high-level services such as the decision support systems (DSS). These datasets are accessible through the following three main components: the Data Access Service (DAS), the <b>Metadata</b> <b>Service</b> and the Complex Data Analysis Module (CDAM). The DAS allows access to data stored in the archive by providing interfaces for different protocols and services for downloading, variables selection, data subsetting or map generation. <b>Metadata</b> <b>Service</b> {{is the heart of the}} information system of the TESSA products and completes the overall infrastructure for data and metadata management. This component enables data search and discovery and addresses interoperability by exploiting widely adopted standards for geospatial data. Finally, the CDAM represents the back-end of the TESSA DSS by performing on-demand complex data analysis tasks...|$|E
40|$|Lack of {{a highly}} {{scalable}} and parallel <b>metadata</b> <b>service</b> is the Achilles heel for many cluster file system deployments in both the HPC world and the Internet services world. This is because most cluster file systems have focused on scaling the data path, i. e. providing high bandwidth parallel I/O to files that are gigabytes in size. But with proliferation of massively parallel applications that produce metadata-intensive workloads, such as large number of simultaneous file creates and large-scale storage management, cluster file systems also need to scale metadata performance. To realize these goals, this paper makes a case for a scalable <b>metadata</b> <b>service</b> middleware that layers on existing cluster file system deployments and distributes file system metadata, including the namespace tree, small directories and large directories, across many servers. Our key idea is to effectively synthesize a concurrent indexing technique to distribute metadata with a tabular, on-disk representation of all file system metadata...|$|E
40|$|This paper {{sketches}} {{the design}} of the Eliot File System (Eliot), a mutable filesystem that maintains the pure immutability of its peer-to-peer (P 2 P) substrate by isolating mutation in an auxiliary <b>metadata</b> <b>service.</b> The immutability of address-to-content bindings has several advantages in P 2 P systems. However, mutable filesystems are desirable because they allow clients to update existing files; a necessary property for many applications. In order to facilitate modifications, the filesystem must provide some atom of mutability. Since this atom of mutability is a fundamental characteristic of the filesystem and not the underlying storage substrate, {{it is a mistake to}} violate the integrity of the substrate with special cases for mutability. Instead, Eliot employs a separate, generalized <b>metadata</b> <b>service</b> that isolates all mutation and client state in an auxiliary replicated database. Eliot provides fine-granularity file updates with either AFS open–close or NFS-like consistency semantics. Eliot builds a mutable filesystem on a global resource bed of purely immutable P 2 P block storage. 1...|$|E
40|$|Abstract. There {{has been}} {{considerable}} interest {{recently in the}} use of highly-available configuration management services based on the Paxos family of algorithms to address long-standing problems in the manage-ment of large-scale heterogeneous distributed systems. These problems include providing distributed locking services, determining group mem-bership, electing a leader, managing configuration parameters, etc. While these services are finding their way into the management of distributed middleware systems and data centers in general, there are still areas of applicability that remain largely unexplored. One such area is the man-agement of metadata in distributed file systems. In this paper we show that a Paxos-based approach to building <b>metadata</b> <b>services</b> in distributed file systems can achieve high availability without incurring a performance penalty. Moreover, we demonstrate that it is easy to retrofit such an ap-proach to existing systems (such as PVFS and HDFS) that currently use different approaches to availability. Our overall approach is based on the use of a general-purpose Paxos-compatible component (the embed-ded Oracle Berkeley database) along with a methodology for making it interoperate with existing distributed file system <b>metadata</b> <b>services.</b> ...|$|R
40|$|The {{volume and}} {{diversity}} of metadata in an experiment of the {{size and scope of}} ATLAS are considerable. Even the definition of metadata may seem context-dependent: data that are primary for one purpose may be metadata for another. ATLAS <b>metadata</b> <b>services</b> must integrate and federate information from inhomogeneous sources and repositories, map metadata about logical or physics constructs to deployment and production constructs, provide a means to associate metadata at one level of granularity with processing or decision-making at another, offer a coherent and integrated view to physicists, and support both human use and programmatic access. In this paper we consider ATLAS <b>metadata,</b> <b>metadata</b> <b>services,</b> and <b>metadata</b> flow principally from the illustrative perspective of how disparate metadata are made available to executing jobs and, conversely, how metadata generated by such jobs are returned. We describe how metadata are read, how metadata are cached, and how metadata generated by jobs and the tasks of which they are a part are communicated, associated with data products, and preserved. We also discuss the principles that guide decision-making about metadata storage, replication, and access. ...|$|R
40|$|International audienceThere {{has been}} {{considerable}} interest {{recently in the}} use of highly-available configuration management services based on the Paxos family of algorithms to address long-standing problems in the management of large-scale heterogeneous distributed systems. These problems include providing distributed locking services, determining group membership, electing a leader, managing configuration parameters, etc. While these services are finding their way into the management of distributed middleware systems and data centers in general, there are still areas of applicability that remain largely unexplored. One such area is the management of metadata in distributed file systems. In this paper we show that a Paxos-based approach to building <b>metadata</b> <b>services</b> in distributed file systems can achieve high availability without incurring a performance penalty. Moreover, we demonstrate that it is easy to retrofit such an approach to existing systems (such as PVFS and HDFS) that currently use different approaches to availability. Our overall approach is based on the use of a general-purpose Paxos-compatible component (the embedded Oracle Berkeley database) along with a methodology for making it interoperate with existing distributed file system <b>metadata</b> <b>services...</b>|$|R
40|$|This paper {{presents}} a novel metadata management mechanism on the metadata server (MDS) for parallel and distributed file systems. In this technique, the client file system backs up the sent metadata requests, {{which have been}} handled by the metadata server, so that the MDS {{does not need to}} log metadata changes to nonvolatile storage for achieving highly available <b>metadata</b> <b>service,</b> as well as better performance improvement in metadata processing. As the client file system backs up certain sent metadata requests in its memory, the overhead for handling these backup requests is much smaller than that brought by the metadata server, while it adopts logging or journaling to yield highly available <b>metadata</b> <b>service.</b> The experimental results show that this newly proposed mechanism can significantly improve the speed of metadata processing and render a better I/O data throughput, in contrast to conventional metadata management schemes, that is, logging or journaling on MDS. Besides, a complete metadata recovery can be achieved by replaying the backup logs cached by all involved clients, when the metadata server has crashed or gone into nonoperational state exceptionally...|$|E
40|$|As MPEG- 7 gets extended, DVB systems {{find the}} need to {{transport}} metadata over MPEG- 2. This paper gives a view of a real implementation in an MHP application framework of the amendment ISO/IEC 138181 : 2000 /FDAM 1, which standardizes the carriage of MPEG- 7 metadata over MPEG 2. The paper focuses mainly on the transport of metadata and how to synchronize metadata with the video content by means of Normal Play Time (NPT). The paper also presents some ways of broadcasting a Dynamic <b>Metadata</b> <b>Service</b> in a MHP application framework...|$|E
40|$|Abstract—To link {{multiple}} varying Grids together, {{there is}} a need for an Information System which will act as a translator for accessing information from other Grid’s Information Services. This study discusses principles and experiences in building a novel architecture of a Hybrid Service, which provides unification, federation and interoperability of different XML metadata services, and presents a performance evaluation of the prototype implementation. The results indicate that the Hybrid Service achieves information integration with negligible processing overheads while preserving persistency of information. Index Terms—Information services, information integration, information federation, hybrid services, <b>metadata</b> <b>service...</b>|$|E
30|$|Institutional and noninstitutional {{maps are}} indexed by an {{off-line}} procedure through their metadata, managed by a <b>metadata</b> catalogue <b>service.</b> <b>Metadata</b> allow decision makers to query flexibly and retrieve susceptibility/hazard and risk maps (via the query engine) {{on the basis}} of relevant user-defined criteria (mainly referring to static geo-environmental variables and/or the dynamic meteorological-climatic parameters).|$|R
40|$|Members of the CUL <b>Metadata</b> <b>Services</b> {{group will}} present {{projects}} {{from the past}} year. We will {{provide an overview of}} major initiatives such as managing research data and cataloging image resources. We will provide brief descriptions of individual projects and show examples of solutions we devised to solve problems. We intend for the presentation to accessible to the entire CUL community, answering the age-old question, "What exactly do the Metadata Librarians do?...|$|R
5000|$|ECHO Reverb - {{the next}} {{generation}} <b>metadata</b> and <b>service</b> discovery tool, which has replaced the former Warehouse Inventory and Search Tool (WIST); ...|$|R
