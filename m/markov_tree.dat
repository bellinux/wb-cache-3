170|192|Public
25|$|In Dempster–Shafer theory, {{each state}} {{equation}} or observation {{is considered a}} special case of a linear belief function and the Kalman filter is a special case of combining linear belief functions on a join-tree or <b>Markov</b> <b>tree.</b> Additional approaches include belief filters which use Bayes or evidential updates to the state equations.|$|E
5000|$|... #Subtitle level 3: The hidden <b>Markov</b> <b>tree</b> (HMT) {{model for}} the contourlet {{transform}} ...|$|E
50|$|The {{resulting}} distributions often {{displayed a}} dependence structure {{that could not}} be captured as a <b>Markov</b> <b>tree.</b>|$|E
40|$|We study {{algorithms}} {{for learning}} Mixtures of <b>Markov</b> <b>Trees</b> for density estimation. There are two approaches to build such mixtures, which both exploit the interesting scaling properties of <b>Markov</b> <b>Trees.</b> We investigate whether the maximum likelihood and the variance reduction approaches {{can be combined}} together by building a two level Mixture of <b>Markov</b> <b>Trees.</b> Our experiments on synthetic data sets show that this two-level model outperforms the maximum likelihood one. Peer reviewe...|$|R
40|$|<b>Markov</b> <b>trees,</b> a {{probabilistic}} {{graphical model}} for density estimation, can be expanded {{in the form}} of a weighted average of <b>Markov</b> <b>Trees.</b> Learning these mixtures or ensembles from observations can be performed to reduce the bias or the variance of the estimated model. We propose a new combination of both, where the upper level seeks to reduce bias while the lower level seeks to reduce variance. This algorithm is evaluated empirically on datasets generated from a mixture of <b>Markov</b> <b>trees</b> and from other synthetic densities. Peer reviewe...|$|R
40|$|A new {{graphical}} model, {{called a}} vine, for dependent random variables is introduced. Vines generalize the <b>Markov</b> <b>trees</b> {{often used in}} modelling high-dimensional distributions. They differ from <b>Markov</b> <b>trees</b> and Bayesian belief nets in {{that the concept of}} conditional independence is weakened to allow for various forms of conditional dependence...|$|R
5000|$|An {{important}} feature of vines {{is that they can}} add conditional dependencies among variables on top of a <b>Markov</b> <b>tree</b> which is generally too parsimonious to summarize the dependence among variables.|$|E
50|$|In Dempster-Shafer theory, {{each state}} {{equation}} or observation {{is considered a}} special case of a linear belief function and the Kalman filter is a special case of combining linear belief functions on a join-tree or <b>Markov</b> <b>tree.</b> Additional approaches include belief filters which use Bayes or evidential updates to the state equations.|$|E
5000|$|A tree-structured graph, {{where each}} node is {{a subset of}} variables, each pair of neighbors has {{non-empty}} intersection, and the intersection of two distinct nodes is contained in every node on the path connecting the two distinct nodes. Such a graph is also sometimes called a join tree. In mathematical notations, a <b>Markov</b> <b>tree</b> is a graph [...] that satisfies the following: (1) [...] is a hypergraph; (2) if , then and (3) if [...] and [...] are distinct vertices of , and , then [...] is in every vertex on the path from [...] to [...]|$|E
5000|$|Dependence {{information}} for such studies {{had been captured}} with <b>Markov</b> <b>trees,</b> ...|$|R
40|$|A new {{graphical}} model, {{called a}} vine, for dependent random variables is introduced. Vines generalize the <b>Markov</b> <b>trees</b> {{often used in}} modelling high-dimensional distributions. They differ from <b>Markov</b> <b>trees</b> and Bayesian belief nets in {{that the concept of}} conditional independence is weakened to allow for various forms of conditional dependence. Electrical Engineering, Mathematics and Computer Scienc...|$|R
40|$|An {{important}} {{problem in}} multiresolution analysis of signals and images consists in estimating hidden random variables (r. v.) x = {xs}s∈S from observed ones y = {ys}s∈S. This is done classically {{in the context}} of Hidden <b>Markov</b> <b>Trees</b> (HMT). In particular, a smoothing Kalman-like algorithm has been proposed by Chou et al. in the linear Gaussian case. In this paper we extend this algorithm to the more general framework of Pairwise <b>Markov</b> <b>Trees</b> (PMT). 1...|$|R
5000|$|There are two simple ways {{to obtain}} a new Markov triple from an old one (x, y, z). First, one may permute the 3 numbers x,y,z, so in {{particular}} one can normalize the triples so that x ≤ y ≤ z. Second, if (x, y, z) is a Markov triple then by Vieta jumping so is (x, y, 3xy &minus; z). Applying this operation twice returns the same triple one started with. Joining each normalized Markov triple to the 1, 2, or 3 normalized triples one can obtain from this gives a graph starting from (1,1,1) as in the diagram. This graph is connected; in other words every Markov triple can be connected to (1,1,1) by a sequence of these operations. [...] If we start, as an example, with (1, 5, 13) we get its three neighbors (5, 13, 194), (1, 13, 34) and (1, 2, 5) in the <b>Markov</b> <b>tree</b> if z is set to 1, 5 and 13, respectively. For instance, starting with (1, 1, 2) and trading y and z before each iteration of the transform lists Markov triples with Fibonacci numbers. Starting with that same triplet and trading x and z before each iteration gives the triples with Pell numbers.|$|E
40|$|The <b>Markov</b> <b>Tree</b> {{model is}} a discrete-time option pricing model that {{accounts}} for short-term memory of the underlying asset. In this work, we compare the empirical performance of the <b>Markov</b> <b>Tree</b> model against that of the Black-Scholes model and Heston’s stochastic volatility model. Leveraging {{a total of five}} years of individual equity and index option data, and using three new methods for fitting the <b>Markov</b> <b>Tree</b> model, we find that the <b>Markov</b> <b>Tree</b> model makes smaller out-of-sample hedging errors than competing models. This comparison includes versions of <b>Markov</b> <b>Tree</b> and Black-Scholes models in which volatilities are strike- and maturity-dependent. Visualizing the errors over time, we find that the <b>Markov</b> <b>Tree</b> model yields more accurate and less risky single instrument hedges than Heston’s stochastic volatility model. A statistical resampling method indicates that the <b>Markov</b> <b>Tree</b> model’s superior hedging performance is due to its robustness with respect to noise in option data...|$|E
40|$|A {{trinomial}} <b>Markov</b> <b>tree</b> {{model is}} studied for pricing options {{in which the}} dynamics of the stock price are modeled by the first-order Markov process. Firstly, we construct a trinomial <b>Markov</b> <b>tree</b> with recombining nodes. Secondly, we give an algorithm for estimating the risk-neutral probability and provide the condition for the existence of a validation risk-neutral probability. Thirdly, we propose a method for estimating the volatilities. Lastly, we analyze the convergence and sensitivity of the pricing method implementing trinomial <b>Markov</b> <b>tree.</b> The result shows that, compared to binomial <b>Markov</b> <b>tree,</b> the proposed model is a natural combining tree and, while changing the probability of the node, it is still combining, so the computation is very fast and very easy to be implemented...|$|E
40|$|This work {{deals with}} the {{statistical}} restoration of a hidden signal using Pairwise <b>Markov</b> <b>Trees</b> (PMT). PMT have been introduced recently {{in the case of}} a discrete hidden signal. We first show that PMT can perform better than the classical Hidden <b>Markov</b> <b>Trees</b> (HMT) when applied to unsupervised image segmentation. We next consider a PMT in a linear Gaussian model with continuous hidden data, and we give formulas of an original extension of the classical Kalman filter...|$|R
40|$|This work {{deals with}} the {{statistical}} restoration of a hidden signal using Pairwise <b>Markov</b> <b>Trees</b> (PMT). The latter PMT, recently introduced {{in the case of}} discrete hidden signal, are here applied to unsupervised image segmentation and it is showed that they work better then the classical Hidden <b>Markov</b> <b>Trees</b> (HMT). Further, considering a PMT in a linear Gaussian model with continuous hidden data, which is new, we give the formulas of an original extension of the classical Kalman filter...|$|R
40|$|An {{important}} {{problem in}} multiresolution analysis of signals or images consists in estimating hidden random variables x fxsg s 2 S from observed ones y fy sg s 2 S. This is done classically {{in the context}} of hidden <b>Markov</b> <b>trees</b> (HMT). HMT have been extended recently to the more general context of pairwise <b>Markov</b> <b>trees</b> (PMT). In this note, we propose an adaptive filtering algorithm which is an extension to PMT of the Kalman filter (KF). r 2005 Elsevier B. V. All rights reserved...|$|R
40|$|AbstractMarkov {{trees and}} clique trees are the {{alternative}} representations of valuation networks and belief networks {{that are used}} by local computational techniques for efficient reasoning. However, once the <b>Markov</b> <b>tree</b> has been created, the existing techniques can only compute the marginals for the vertices of the <b>Markov</b> <b>tree</b> or for a subset of variables which is contained in one vertex. This paper presents a method for computing the marginal for a subset {{which may not be}} contained in one vertex, but is a subset of the union of several vertices. The proposed method allows us to change the <b>Markov</b> <b>tree</b> to include a vertex containing the new subset without changing any information in the original vertices, thus avoiding possible repeated computations. Moreover, it can compute marginals for any subsets from the marginal representation in the <b>Markov</b> <b>tree.</b> By using the presented method, we can easily update belief for some variables given some observations...|$|E
40|$|Abstract Dempster's rule of {{evidence}} combination is computational expensive. This paper presents a parallel approach to evidence combination on a qualitative <b>Markov</b> <b>tree.</b> Binarization algorithm transforms a qualitative <b>Markov</b> <b>tree</b> into a binary tree based on the computational workload in nodes for an exact implementation {{of evidence}} combination. A binary tree is then partitioned into clusters with each cluster being assigned to a processor in a parallel environment. The parallel implementation improves the computational efficiency of evidence combination...|$|E
40|$|We {{consider}} algorithms {{for generating}} Mixtures of Bagged Markov Trees, for density estimation. In problems deﬁned over many variables and when few observations are available, those mixtures generally outperform a single <b>Markov</b> <b>tree</b> maximizing the data likelihood, but {{are far more}} expensive to compute. In this paper, we describe new algorithms for approximating such models, {{with the aim of}} speeding up learning without sacriﬁcing accuracy. More speciﬁcally, we propose to use a ﬁltering step obtained as a by-product from computing a ﬁrst <b>Markov</b> <b>tree,</b> so as to avoid considering poor candidate edges in the subsequently generated trees. We compare these algorithms (on synthetic data sets) to Mixtures of Bagged Markov Trees, as well as to a single <b>Markov</b> <b>tree</b> derived by the classical Chow-Liu algorithm and to a recently proposed randomized scheme used for building tree mixtures. Peer reviewe...|$|E
40|$|International audienceAn {{important}} {{problem in}} multiresolution analysis of signals and images consists in estimating continuous hidden random variables x = {x_s}_s ∈ S from observed ones y = {y_s}_s ∈ S. This is done classically {{in the context}} of hidden <b>Markov</b> <b>trees</b> (HMTs). In this note we deal with the recently introduced pairwise <b>Markov</b> <b>trees</b> (PMTs). We first show that PMTs are more general than HMTs. We then deal with the linear Gaussian case, and we extend from HMTs with independent noise (HMT-IN) to PMT a smoothing Kalman-like recursive estimation algorithm which was proposed by Chou et al., as well as an algorithm for computing the likelihoo...|$|R
40|$|AbstractWe {{extend the}} <b>Markov</b> Chain <b>Tree</b> Theorem to general {{commutative}} semirings, and we generalize the State Reduction Algorithm to general commutative semifields. This {{leads to a}} new universal algorithm, whose prototype is the State Reduction Algorithm which computes the <b>Markov</b> chain <b>tree</b> vector of a stochastic matrix...|$|R
40|$|We {{extend the}} <b>Markov</b> chain <b>tree</b> theorem to general {{commutative}} semirings, and we generalize the state reduction algorithm to commutative semifields. This {{leads to a}} new universal algorithm, whose prototype is the state reduction algorithm which computes the <b>Markov</b> chain <b>tree</b> vector of a stochastic matrix. Comment: 13 page...|$|R
40|$|The hidden <b>Markov</b> <b>tree</b> {{models were}} {{introduced}} by Crouse, Nowak and Baraniuk in 1998 for modeling nonindependent, non-Gaussian wavelet transform coefficients. In their article, they developed {{the equivalent of}} the forward-backward algorithm for hidden <b>Markov</b> <b>tree</b> models and termed it the “upward-downward algorithm”. This algorithm is subject to the same numerical limitations as the forward-backward algorithm for hidden Markov chains. In this paper, adapting the ideas of Devijver from 1985, we propose a new “upward-downward ” algorithm, which is a true smoothing algorithm and is immune to numerical underflow. Furthermore, we propose a Viterbi-like algorithm for global restoration of the hidden state tree. The contribution of those algorithms as diagnosis tools is illustrated through the modeling of statistical dependencies between wavelet coefficients with a special emphasis on local regularity changes. Keywords: hidden <b>Markov</b> <b>tree</b> model, EM algorithm, hidden state tree restoration, upward-downward algorithm, wavelet decomposition, scaling laws, change detection...|$|E
40|$|Wavelet-domain hidden <b>Markov</b> <b>tree</b> (HMT) {{modeling}} {{provides a}} powerful approach {{to capture the}} underlying statistics of the wavelet coefficients. We develop a mutual information-based information-theoretic approach to quantify the interactions between the wavelet coefficients within a wavelet tree. This graphical method enables {{the design of a}} context-specific hidden <b>Markov</b> <b>tree</b> (HMT) by adding or deleting links from the traditional tree structure. The performance of the model is demonstrated on segmenting two-dimensional synthetic textures having intricate substructures, although the method can be used for signals of arbitrary dimensions. 1...|$|E
40|$|In this thesis, {{we propose}} the <b>Markov</b> <b>tree</b> option pricing model and subjectit to {{large-scale}} empirical tests against market options and equity data toquantify its pricing and hedging performances. We begin by proposing a tree model that explicitly {{accounts for the}} dependenceobserved in the log-returns of underlying asset prices. The dynamics of theMarkov tree model is explained together with implementation notes that enableexact calculation of the probability mass function of the <b>Markov</b> <b>tree</b> process. We also show that the tree model operates {{in the framework of}} arbitragefree option pricing. Next, we show how the discrete <b>Markov</b> <b>tree</b> process {{can be viewed as a}} generalized persistentrandom walk and demonstrate how to approximate it by a mixture of two normals. Thisderivation enables us to obtain a closed form pricing formula for the Europeancall option allowing for faster calibration using market option data. We then empirically test both the pricing as well as the hedging performance ofthe <b>Markov</b> <b>tree</b> model against the Black-Scholes and the Heston's stochastic volatilitymodels establishing its superior hedging performance. Additionally, we alsoanalyze different regression based techniques to estimate the parameters in theMarkov tree model that obtain increasingly better hedging results. We alsolay down statistical procedures to rigorously analyze the hedgingperformance of any option pricing model. We then generalize the <b>Markov</b> <b>tree</b> process and explore its relation withthe generalized delayed random walk. In doing so, we develop a spectral method for computingthe probability density function for delayed random walks; for such problems,the spectral method we propose is exact to machine precision and faster than existing methods. In conjunction with step function approximation and the weak Euler-Maruyamadiscretization, the spectral method can be applied to nonlinear stochasticdelay differential equations. We carry out tests for a particular nonlinear SDDE that showsthat this method captures the solution without the need for Monte Carlo sampling...|$|E
40|$|A vine {{is a new}} {{graphical}} model for dependent random variables. Vines generalize the <b>Markov</b> <b>trees</b> often used in modeling multivariate distributions. They differ from <b>Markov</b> <b>trees</b> and Bayesian belief nets in {{that the concept of}} conditional independence is weakened to allow for various forms of conditional dependence. A general formula for the density of a vine dependent distribution is derived. This generalizes the well-known density formula for belief nets based on the decomposition of belief nets into cliques. Furthermore, the formula allows a simple proof of the Information Decomposition Theorem for a regular vine. The problem of (conditional) sampling is discussed, and Gibbs sampling is proposed to carry out sampling from conditional vine dependent distributions. The so-called lsquocanonical vinesrsquo built on highest degree trees offer the most efficient structure for Gibbs sampling...|$|R
40|$|ABSTRACT. Multiresolution {{signal and}} image {{analysis}} and multiscale algorithms {{are of interest}} in many fields. In particular, efficient Bayesian restoration algorithms have been proposed for some treestructured Markovian models. In this paper we show that Bayesian filtering and prediction can be performed exactly, with complexity linear in time index, in a particular class of Triplet <b>Markov</b> <b>Trees.</b> ...|$|R
40|$|A new {{graphical}} model, {{called a}} vine, for dependent random variables is introduced. Vines generalize the <b>Markov</b> <b>trees</b> {{often used in}} modelling high-dimensional distributions. They differ from <b>Markov</b> <b>trees</b> and Bayesian belief nets in {{that the concept of}} conditional independence is weakened to allow for various forms of conditional dependence. Vines can be used to specify multivariate distributions in a straightforward way by specifying various marginal distributions {{and the ways in which}} these marginals are to be coupled. Such distributions have applications in uncertainty analysis where the objective is to determine the sensitivity of a model output with respect to the uncertainty in unknown parameters. Expert information is frequently elicited to determine some quantitative characteristics of the distribution such as (rank) correlations. We show that it is simple to construct a minimum information vine distribution, given such expert information. Sampling from minimum information distr [...] ...|$|R
40|$|We {{develop a}} hierarchical, nonparametric {{statistical}} model for wavelet representations of natural images. Extending previous work on Gaussian scale mixtures, wavelet coefficients are marginally distributed according to infinite, Dirichlet process mixtures. A hidden <b>Markov</b> <b>tree</b> is {{then used to}} couple the mixture assignments at neighboring nodes. Via a Monte Carlo learning algorithm, the resulting hierarchical Dirichlet process hidden <b>Markov</b> <b>tree</b> (HDP-HMT) model automatically adapts {{to the complexity of}} different images and wavelet bases. Image denoising results demonstrate the effectiveness of this learning process. Index Terms — hidden Markov trees, hierarchical Dirichlet processes, nonparametric Bayesian methods, wavelet transforms, image denoising. 1...|$|E
40|$|Abstract. Binary join {{trees have}} been a popular {{structure}} to compute the impact of multiple belief functions initially assigned to nodes of trees or networks. Shenoy has proposed two alternative methods to transform a qualitative <b>Markov</b> <b>tree</b> into a binary tree. In this paper, we present an alternative algorithm of transforming a qualitative <b>Markov</b> <b>tree</b> into a binary tree based on the computational workload in nodes for an exact implementation of evidence combination. A binary tree is then partitioned into clusters with each cluster being assigned to a processor in a parallel environment. These three types of binary trees are examined to reveal the structural and computational differences. ...|$|E
40|$|Abstract: The wavelet-domain Hidden <b>Markov</b> <b>Tree</b> Model can {{properly}} {{describe the}} dependence and correlation of fun-dus angiographic images ’ wavelet coefficients among scales. Based {{on the construction}} of the fundus angiographic im-ages Hidden <b>Markov</b> <b>Tree</b> Models and Gaussian Mixture Models, this paper applied expectation-maximum algorithm to estimate the wavelet coefficients of original fundus angiographic images and the Bayesian estimation to achieve the goal of fundus angiographic images denoising. As is shown in the experimental result, compared with the other algorithms as mean filter and median filter, this method effectively improved the peak signal to noise ratio of fundus angiographic im-ages after denoising and preserved the details of vascular edge in fundus angiographic images...|$|E
40|$|Abstract. This paper {{presents}} a novel texture segmentation method using Bayesian estimation and neural networks. Multi-scale wavelet coefficients and the context information extracted from neighboring wavelet coefficients {{were used as}} input for the neural networks. The output was modeled as a posterior probability. The context information was obtained by HMT (Hidden <b>Markov</b> <b>Trees)</b> model. The proposed segmentation method shows performed better than ML (Maximum Likelihood) segmentation using the HMT model. ...|$|R
40|$|We {{consider}} randomization schemes of the Chow-Liu algorithm from weak (bagging, of quadratic complexity) {{to strong}} ones (full random sampling, of linear complexity), for learning probability density {{models in the}} form of mixtures of <b>Markov</b> <b>trees.</b> Our empirical study on high-dimensional synthetic problems shows that, while bagging is the most accurate scheme on average, some of the stronger randomizations remain very competitive in terms of accuracy, specially for small sample sizes. Peer reviewe...|$|R
40|$|We study {{consistent}} {{collections of}} random fragmentation trees with random integervalued edge lengths. We prove several equivalent necessary and sufficient {{conditions under which}} Geometrically distributed edge lengths can be consistently assigned to a <b>Markov</b> branching <b>tree.</b> Among these conditions is a characterization by a unique probability measure, which plays a role similar to the dislocation measure for homogeneous fragmentation processes. We discuss this and other connections to previous work on <b>Markov</b> branching <b>trees</b> and homogeneous fragmentation processes...|$|R
