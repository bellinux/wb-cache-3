80|22|Public
5000|$|... 1971: Sinfonia <b>minimale</b> {{for youth}} orchestra, op.16: 4 minutes ...|$|E
5000|$|<b>Minimal(e)</b> String(en)z (1989; premiered Oldenburg, 1991), {{for three}} violins, cello, and systhesiser/keyboard.|$|E
5000|$|... #Caption: The {{statement}} of the monokini made by <b>Minimale</b> Animale on the runways of the Mercedes Benz Fashion Week ...|$|E
5000|$|Essais fragiles d’aplomb (essai), Verticales, coll. <b>Minimales,</b> 2002 ...|$|R
40|$|International audienceForeword This {{document}} {{presents a}} translation for historical {{perspective of the}} paper: " B. Courcelle, G. Kahn, and J. Vuillemin. Algorithmes d'´ equivalence et de réduction a des expressions <b>minimales</b> dans une classe d'´ equations récursives simples, in, Jacques Loeckx, editor, Automata, Languages and Programming, volume Abstract In this paper, we describe an algorithm for deciding equivalence in a domain whose objects are defined by uninterpreted fixpoint equations. The algorithm is then applied to finding minimal representations of those objects...|$|R
40|$|AbstractWe {{will prove}} that {{isolated}} singularities of sections with prescribed mean curvature of a Riemannian submersion fibered by geodesics of a vertical Killing field, are removable. Also we obtain {{information on the}} growth of the difference of two sections u,v:Ω→M¯, having the same prescribed mean curvature and u=v on ∂Ω. This generalizes Theorem 2 of [P. Collin, R. Krust, Le problème de Dirichlet l'équation des surfaces <b>minimales</b> sur des domaines non bornés, Bull. S. M. F. 119 (4) (1991) 443 – 462]...|$|R
50|$|Araeopteron <b>minimale</b> is {{a species}} of moth of the family Erebidae. It {{is found in the}} Seychelles on Mahé and Félicité Island.|$|E
50|$|Kutzsche A (1962). Die <b>minimale</b> Hemmkonzentration von Sulfanilamiden Minimum Inhibitory Concentration of Sulfanilamides. Das Ärztliche Laboratorium: Zeitschrift für den Laboratoriumsarzt und die ärztliche Praxis (= Clinical Laboratory), 8: 330-332.|$|E
5000|$|Nasty Gal’s {{original}} label {{launched in}} 2012 {{and consists of}} limited-edition styles. In September 2012, Nasty Gal debuted its first Nasty Gal Fall/Winter 2012 Collection - Weird Science- during New York Fashion Week. [...] Since then, the company has continued to launch various collections timed to major fashion seasons throughout the year. In addition, Nasty Gal launched its first-ever footwear collection, Shoe Cult by Nasty Gal, in August 2013. In 2014, Nasty Gal debuted three additional in-house collections: the vintage-inspired Nasty Gal Denim Collection, Nasty Gal Swimwear, and Nasty Gal Lingerie. Nasty Gal also collaborated on a swimwear line with <b>Minimale</b> Animale in 2014. Additionally, Nasty Gal collaborated with M∙A∙C Cosmetics on a capsule collection of lipsticks and nail polish in December 2014.|$|E
40|$|International audienceWe give {{a compact}} {{expression}} {{for the number}} of factorizations of any permutation into a minimal number of transpositions of the form (1 i). Our result generalizes earlier work of Pak (Reduced decompositions of permutations in terms of star transpositions, generalized catalan numbers and k-ary trees, Discrete Math. 204 : 329 ― 335, 1999) in which substantial restrictions were placed on the permutation being factored. Nous présentons une expression compacte pour le nombre de factorisations <b>minimales</b> d'une permutation arbitraire de transposition de la forme (1 i). Ce résultat généralise le travail passé de Pak (Reduced decompositions of permutations in terms of star transpositions, generalized catalan numbers and k-ary trees, Discrete Math. 204 : 329 ― 335, 1999) dans lequel des restrictions substantielles sont imposées sur la permutation étant factorisée...|$|R
40|$|Este trabajo trata sobre espacios foliados equicontinuos, considerados como generalizaciones de las foliaciones riemannianas introducidas por Reinhart [40]. Especialmente, se consideran espacios foliados equicontinuos compactos que son <b>minimales</b> en el sentido de que sus hojas son densas. The {{main goal}} of this thesis is to prove the {{structural}} theorems of Molino’s theory of Riemannian foliations in the topological context; in other words, for compact equicontinuous foliated spaces. This is achieved for the case where the leaves are dense (minimal foliated space), describing such foliated space {{in terms of a}} foliated space whose transverse dynamics is given by local left translations in a local group G (a G-foliated space). Then this description is used to study the growth of the leaves in the spirit of Carriére and Breuillard- Gelander...|$|R
40|$|Pour réunir à la fois les {{structures}} stables et menues, nous introduisons les structures fines et mettons en évidence plusieurs caractéristiques communes aux structures (faiblement) <b>minimales,</b> stables ou menues : une condition de chaîne uniforme et locale et une notion de presque phi-stabilisateur local. Nous en déduissons qu'un corps infini dont la théorie est fine n'a pas de sous-groupes additifs ni multiplicatifs d'indice fini, ni d'extension d'Artin-Schreier. We investigate {{some common}} points between stable and weakly small structures and define a structure M to be "fine" if the topological space S_ϕ(dcl^eq(A)) has an ordinal Cantor-Bendixson rank for every formula phi and finite subset A of M. By definition, a theory is "fine" if every of its models is so. Weakly minimal, small, and stable structures are all examples of fine structures. For {{any of its}} finite subset A, a fine structure has local descending chain conditions on the algebraic closure acl(A) of A for subgroups uniformly definable over acl(A). An infinite field with fine theory has no additive or multiplicative proper subgroup of finite index, and no Artin-Schreier extension...|$|R
40|$|Minimum {{inhibitory}} (MIC) {{and minimum}} microbicidal concentration (MMC) of polihexanide and triclosan against antibiotic sensitive and resistant Staphylococcus aureus and Escherichia coli strains <b>Minimale</b> Hemm-Konzentration (MHK) und <b>minimale</b> bakterizide Konzentration (MBK) von Polihexanid und Triclosan gegen Antibiotika-empfindliche und resistente Staphylococcus aureus- und Escherichia coli-Stämme Background: An in-vitro {{study was conducted}} investigating the anti...|$|E
40|$|Die natuurlike metaboliet van estradiol, naamlik 2 -metoksieëstradiol (2 ME 2) oefen antiproliferatiewe en anti-tumor effekte {{in vitro}} en in vivo uit met <b>minimale</b> of geen toksisiteit nie. In vitro changes in {{mitochondrial}} potential, aggresome formation and caspase activity by a novel 17 -beta-estradiol analog in mcf- 7 breast adenocarcinoma cells. After a 24 hour exposure time, cells both apoptosis and autophagy were induced. </div...|$|E
40|$|Among the {{normalized}} metrics on a graph, we {{show the}} existence and the uniqueness of an entropy-minimizing metric, and give explicit formulas for the minimal volume entropy and the metric realizing it. Parmi les distances normalisées sur un graphe, nous montrons l’existence et l’unicité d’une distance qui minimise l’entropie, et nous donnons des formules explicites pour l’entropie volumique <b>minimale</b> et la distance qui la réalise. 1. Introduction Let (X, g) be a compact connected Riemannian manifold of nonpositive curvature. It was shown by A. Manning [Man] that the topological entropy htop(g) of the geodesic flow {{is equal to the}} volume entropy hvol(g) of the manifold 1 hvol(g) = lim log(vol(B(x, r))) ...|$|E
40|$|Environmental Resources Management in the Soudanian Savanah Area of Northern Cameroon: Assessment of Alternatives Using a "modèle de normes <b>minimales</b> de sécurité". The {{world is}} more and more {{concerned}} with the everwidening gap between available natural resources and the world's population growth. Two important international meetings testify this concern: the International Conference on the Environment (3) and the one held in Cairo in 1994 on Population and Development. Improved land-use planning at the national level should be undertaken as an important step in the biodiversity conservation process. People need to use natural resources, so ways must be found to use those resources in the least destructive manner. Biodiversity conservation planners need to consider the interactions between population needs and natural resources conservation. The comparison of the economic value of the conservation of protected areas with other alternatives of lands used indicated that: the economic value of the conservation is twenty times greater than the economic value of extensive livestock traditional bovine breeding. Although all the elements were not included, the estimated economic value of land for conservation is about 50 % of the land's economic value for agriculture. Recommendations have been made from these different results...|$|R
40|$|Consider a set {{of mobile}} robots with minimal {{capabilities}} placed over distinct nodes of a discrete anonymous ring. They operate {{on the basis of}} the so called Look-Compute-Move cycle. Asynchronously, each robot takes a snapshot of the ring, determining which nodes are either occupied by robots or empty. Based on the observed configuration, it decides whether to move to one of its adjacent nodes or to stay idle. In the first case, it performs the computed move, eventually. The computation also depends on the required task. In this paper, we solve both the well-known Searching and Gathering tasks. In the literature, most contributions are restricted to a subset of initial configurations. Here, we design two different algorithms and provide a full characterization of the initial configurations that permit the resolution of the problems under minimal assumptions. Nous considérons un ensemble de robots mobiles qui sont placés sur distincts sommets d'un réseau en anneau. Le réseau est anonyme et les robots ont des aptitudes <b>minimales.</b> Ils opérent par des cycles Observer-Calculer-Bouger. Nous résolvons les problémes de la réunion et du nettoyage de graphe dans ce modéle...|$|R
40|$|Ab initio MO calculations, {{using both}} minimal (sTo- 3 G) and {{extended}} (Roos-Siegbahn) basis sets arereported for the systems methanethiol-imidazole, methanethiol-imidazole-formaldehyde, and methanethiol-imidazole-formamide, which, {{together with a}} point-change representation of a long a-helix,form models {{for the active site}} of papain. It is shown that the large electric field exerted by the helixin the active-site region is responsible for the presence of the essential residues Cys 25 and His 159 {{in the form of an}} ion pair RS- [...] . ImH+, which is crucial for a recently proposed mechanism forthecatalytic action of the enzyme. Also, an explanation is given for the anomalies in measured pK valuesfor these residues. Detailed studies on the (sub) systems show thatminimal basis sets lack the flexibilitynecessary for describing the type of proton transfer involved. We conclude that a-helices are essentialparts of enzymes and that they playa significant role in the catalytic process. Onprésente des calculs MO ab initio avec soit des bases <b>minimales</b> (sTo- 3 G), soit des bases élargies (Roos-Siegbahn) pour les systèmes méthanéthiole-imidazole, méthanéthiole-imidazole-formaldé-hyde,et méthanéthiole-imidazole-formamide, qui avec une représentation de charges ponctuelles d'unelongue hélice a, forment des modèles pour Ie site actif de la papaïne. Ii est démontré, que Ie champelectrique important déployé par I'hélice dans la région du site actif est responsable pour l...|$|R
40|$|The {{objective}} of these experiment is {{to calculate the}} <b>minimale</b> capacity of paddle from techno-economical stand point. Technically, {{the quality of the}} liming product by paddle were better quality than the leather processed by using drum. Usually paddle machine were made of teac wood or brick wall. Based on economic evaluation, the production price of the paddle that were made of teac wood (1000 dm 3 and 1500 dm 3 scale) were Rp. 334, 46 and Rp. 285, 45 respectivelly, made of brick wall (1000 dm 3 and 1500 dm 3 scale) were Rp. 317, 83 and Rp. 273, 56 and used of drum Rp. 353, 56. Therefore the paddle machine made of brick wall in 1500 dm 3 scale was the most economic machine...|$|E
40|$|Abstract – The {{objective}} {{of this research was}} to determine the effect of days dry (DD) on actual milk yield and to identify the minimum dry period length needed to maximize milk yield in the sub-sequent lactation. Field data collected through the U. S. dairy herd improvement association from January, 1997 to December, 2003 and extracted from the Animal Improvement Programs Laboratory national database were used for analysis. Actual lactation records calculated from test-day yields using the test-interval method were used in this study. The model for analysis included herd-year of calving, year-state-month of calving, previous lactation milk yield, age at calving, and DD as a cat-egorical variable. Interactions were added to this model to determine if the effects of DD on subse-quent lactation milk yield depended on previous lactation milk yield, age at calving, somatic cell score, or days open. Milk yield in the subsequent lactations was generally maximized with a 60 to 65 d dry period, regardless of parity. Days dry effects on milk yield were, for the most part, consistent across lactations, although dry periods < 35 d are somewhat more detrimental to milk yield after first lactation than after second and later lactations. Dry periods less than 20 d result in very pro-nounced losses in subsequent lactation yield. A short dry period (< 40 d) for high producing cows that bred back early in lactation proved to be the worst combination in terms of maximizing subse-quent lactation milk yield. days dry / lactational milk yield Résumé – Période de tarissement <b>minimale</b> pour maximiser la production laitière pendant la lactation suivante. L’objectif de cette recherche a été de déterminer les effets de la durée de la période de tarissement (PT) sur la production totale de lait pendant la lactation suivant le tarissement, et d’identifier la période <b>minimale</b> nécessaire pour maximiser cette production. Les données utilisée...|$|E
40|$|The authors {{discuss a}} class of {{likelihood}} functions involving weak assumptions on data generating mechanisms. These likelihoods may be appropriate when it is di#cult to propose models for the data. The properties of these likelihoods are given and it is shown {{how they can be}} computed numerically by use of the Blahut-Arimoto algorithm. The authors then show how these likelihoods can give useful inferences using a data set for which no plausible physical model is apparent. The plausibility of the inferences is enhanced by the extensive robustness analysis these likelihoods permit. Resume Les auteurs montrent comment il est possible, en l'absence de modele naturel pour des observations, de construire une classe de fonctions de vraisemblance a partir d'hypotheses tres faibles concernant l'origine des donnees. Ils presentent les proprietes de ces vraisemblances a information <b>minimale</b> et expliquent comment les calculer a l'aide de l'algorithme de Blahut-Arimoto. Ils illustrent la faisabil [...] ...|$|E
40|$|A {{standard}} {{yield per recruit}} (YPR) analysis is used to analyse the effects of minimum size regulations on bluefin tuna from the Eastern stock (East Atlantic and Mediterranean). Analyses {{were based on a}} selectivity vector computed over the period 1990 - 1994 for which data were more reliable and VPA (performed during the 2002 stock assessment) showed a relatively good convergence. This selectivity vector was modified according to different minimum size regulations and implementation error levels. It is concluded that: (i) perfect implementation of the size limit regulation adopted in 2004 would increase YPR by 4. 3 % relative to the size limit adopted in 2002, and by 8. 8 % relative to the pattern realized in the early 1990 s; (ii) YPR is however expected to decrease for some fleets, especially those targeting small BFT in the East Atlantic; and, (iii) an implementation error of 50 % could cancel the gains expected due to the last size limit regulation adopted by the Commission. RÉSUMÉ Une analyse du rendement par recrue (RPR) est menée pour étudier les réglementations sur les tailles <b>minimales</b> du thon rouge de l’Atlantique Est et Méditerranée. Le vecteur de sélectivité a été calculé sur la période 1990 - 1994, pour laquelle les données de captures sont plus fiables e...|$|R
40|$|ABSTRACT. The {{importance}} of adequate selenium in diets of native wild herbivores canonly {{be inferred from}} data for beef cattle where minimum dietary concentrations range from 50 to 100 ppb. Concern about possible selenium deficiencies in wild herbivores {{is based on a}} few reports of symp-toms in wildlife, a paucity of data on selenium in their forages, and the idea that excessive atmospheric sulfur may increase the incidence of selenium deficiencies in herbivores. Concentrations of selenium in sedges, Carex spp., and reedgrasses, calamagrosris spp., the main food plants of bison, Bison bison, in northwestern Canada, varied from 9 to 800 ppb in samples collected at three lowland locations. However, approximately three-quarters of all the samples of plant species consumed by bison were dietarily deficient by the beef cattle standard. Key words: Carex, calamagrostis, bison, selenium concentration &UMfi. L’importance d’un quantit 6 adkquate de sklbium dans les rCgimes des herbivores sauvages locaux ne peut &re infCr 6 e que d’apks les donnh portant sur le Mtail bovin, dans lequel les concentrations <b>minimales</b> varient entre 50 et 100 ppb. La question d’insuffisance possible en dlenium chez les herbivores sauvages fut lev & par rapport h quelques etudes des sympt 8 mes d’animaux sauvages, la p 6 nurie de donnh sur le dlCnium dans leur rkgime et h la hypothhe selon laquelle un ex & de souffre dans I’atmosphZxe pourrait augmenter I’incidence d’insuffisance e...|$|R
40|$|Los grafos perfectos fueron definidos por Claude Berge en 1960. Un grafo G es perfecto cuando para todo subgrafo inducido H de G, el número cromático de H es igual al tamaño de un subgrafo completo máximo de H. Los grafos perfectos son de gran interés desde el punto de vista algoritmo: si bien los problemas de determinar la clique máxima y el número cromático de un grafo son NP-completos, éstos se resuelven en tiempo polinomial para grafos perfectos. Desde entonces, fueron definidas y estudiadas gran cantidad de variantes de los grafos perfectos. Entre ellas, los grafos clique-perfectos. Una clique en un grafo es un subgrafo completo maximal con respecto a la inclusión. Un {{transversal}} de las cliques de un grafo G es un subconjunto de vértice que interseca a todas las cliques de G. Un conjunto de cliques independientes es un conjunto de cliques disjuntas dos a dos. Un grafo G es clique-perfecto si el tamaño de un transversal de las cliques mínimo coincide con el de un conjunto de cliques independientes máximo, para cada subgrafo inducido de G. El término "clique-perfecto" fue introducido por Guruswami y Pandu Rangan en 2000, pero la igualdad de esos parámetro fue estudiada previamente por Berge en el contexto de hipergrafos balanceados. En 2002, Chudnovsky, Robertson, Seymour y Thomas demostraron una caracterización de los grafos perfectos por subgrafos prohibidos <b>minimales,</b> cerrando una conjetura abierta durante 40 años. También durante el año 2002 fueron presentados dos trabajos, uno de ellos de Chudnovsky y Seymour, y el otro de Cornuéjols, Liu y Vuskovic, que mostraban que el reconocimiento de esta clase era polinomial, resolviendo otro problema abierto formulado mucho tiempo atrás. La lista de subgrafos prohibidos <b>minimales</b> para la clase de grafos clique-perfectos no se conoce aún, y También es una pregunta abierta la complejidad del problema de reconocimiento. En esta tesis presentamos resultados parciales en estas direcciones, es decir, caracterizamos los grafos cliqueperfectos por subgrafos prohibidos <b>minimales</b> dentro de ciertas clases de grafos, a saber,Perfect graphs {{were defined}} by Claude Berge in 1960. A graph G is perfect whenever for every induced subgraph H of G, the chromatic number of H equals the cardinality of a maximum complete subgraph of H. Perfect graphs are very interesting from an algorithmic point of view: while determining the clique {{number and the}} chromatic number of a graph are NP-complete problems, they are solvable in polynomial time for perfect graphs. Since then, many variations of perfect graphs were defined and studied, including the class of clique-prefect graphs. A clique in a graph is a complete subgraph maximal under inclusion. A clique-transversal of a graph G is a subset of vertices meeting all the cliques of G. A clique-independent set {{is a collection of}} pairwise vertex-disjoint cliques. A graph G is clique-perfect if the sizes of a minimum clique-transversal and a maximum clique-independent set are equal for every induced subgraph of G. The term -perfect" was introduced by Guruswami and Pandu Rangan in 2000, but the equality of these parameters had been previously studied by Berge in the context of balanced hypergraphs. A characterization of perfect graphs by minimal forbidden subgraphs was recently proved by Chudnovsky, Robertson, Seymour and Thomas, and a polynomial time recognition algorithm for this class of graphs has been developed by Chudnovsky, Cornuéjols, Liu, Seymour and Vusković. The list of minimal forbidden induced subgraphs for the class of clique-perfect graphs is not known. Another open question concerning cliqueperfect graphs is the complexity of the recognition problem. In this thesis, we present partial results in these directions, that is, we characterize clique-perfect graphs by a restricted list of forbidden induced subgraphs when the graph is either a line graph, or claw-free hereditary clique-Helly, or diamond-free, or a Helly circular-arc graph. Almost all of these characterizations lead to polynomial time recognition algorithms for clique-perfection in the corresponding class of graphs. Berge defined a hypergraph to be balanced if its vertex-edge incidence matrix is balanced, that is, if it does not contain the vertex-edge incidence matrix of an odd cycle as a submatrix. In 1998, Dahlhaus, Manuel and Miller consider this concept applied to graphs, defining a graph to be balanced when its vertex-clique incidence matrix is balanced. Balanced graphs are an interesting subclass in the intersection of perfect and clique-perfect graphs. We give two new characterizations of this class, the first one by forbidden subgraphs and the second one by clique subgraphs. Using domination properties we define four subclasses of balanced graphs. Two of them are characterized by 0 - 1 matrices and can be recognized in polynomial time. Furthermore, we propose polynomial time combinatorial algorithms for the stable set problem, the clique-independent set problem and the clique-transversal problem in one of these subclasses. Finally, we analyze the behavior of balanced graphs and these four subclasses under the clique graph operator. Fil:Bonomo, Flavia. Universidad de Buenos Aires. Facultad de Ciencias Exactas y Naturales; Argentina...|$|R
40|$|We {{investigate}} the minimization of Newton's functional {{for the problem}} of the body of minimal resistance with maximal height M > 0 [4] in the class of convex developable functions dened in a disc. This class is a natural candidate to nd a (non-radial) minimizer in accordance with the results of [9]. We prove that the minimizer in this class has a minimal set {{in the form of a}} regular polygon with n sides centered in the disc, where the natural number n 2 is a non-decreasing function of M. The corresponding functions all achieve a lower value of the functional than the optimal radially symmetric function with the same height M. Resume Nous examinons la minimisation de la fonctionnelle de Newton pour le probleme de resistance <b>minimale</b> [4] d'un corps de hauteur maximale M > 0 dans la classe des fonctions convexes developpables denies sur un disque. D'apres les resultats de [9], cette classe est un candidat naturel pour la recherche d'un minimiseur non-radial. Nous demontro [...] ...|$|E
40|$|In his {{keynote speech}} {{held at the}} Opening Conference of the Research Center (SFB) 700 in February 2007, Robert O. Keohane deals with the {{normative}} dimensions of institutional legitimacy {{from the standpoint of}} liberal democratic theory, interpreted in a consequentialist way. Having formulated a number of normative standards for the legitimacy of institutions, he gives examples on how these should be applied to institutions on a global level and in situations of limited statehood. Keohane concludes by suggesting that minimal moral acceptability, comparative benefit, integrity and epistemic quality all seem relevant for evaluating the legitimacy of governance institutions. Robert O. Keohane beschäftigt sich in seiner Rede, die er bei der Eröffnungskonferenz des SFB 700 im Februar in Berlin hielt, mit den normativen Dimensionen institutioneller Legitimität aus der Perspektive liberaler demokratischer Theorie, die er konsequentialistisch interpretiert. Keohane nennt eine Reihe von normativen Standards für die Legitimität von Institutionen, die er auf Beispiele von Institutionen in Räumen begrenzter Staatlichkeit sowie im internationalen Bereich anwendet. Abschließend formuliert Keohane, dass die Kriterien <b>minimale</b> moralische Akzeptanz, komparativer Vorteil, Integrität und epistemische Qualität alle von hoher Bedeutung scheinen, um die Legitimität von Governance-Institutionen zu bewerten...|$|E
40|$|INTRODUCTION 3 2. VUE D'ENSEMBLE 3 2. 1. HISTORIQUE 4 2. 2. TERMINOLOGIE 5 2. 3. LE MODELE FTP 9 3. FONCTIONS DE TRANSFERT DE DONNEES 10 3. 1. REPRESENTATION DES DONNEES ET STOCKAGE 11 3. 1. 1. TYPES DE DONNEES 11 3. 1. 2. STRUCTURES DE DONNEES 14 3. 2. ETABLISSEMENT DU CANAL DE DONNEES 17 3. 3. GESTION DU CANAL DE DONNEES 18 3. 4. MODES DE TRANSMISSION 18 3. 4. 1. MODE "FLUX" 19 3. 4. 2. MODE BLOC 19 3. 4. 3. MODE COMPRESSE 21 3. 5. RECUPERATION D'ERREUR ET REPRISE DE TRANSMISSION 22 4. FONCTIONS DE TRANFERT DE FICHIERS 23 4. 1. COMMANDES FTP 23 4. 1. 1. COMMANDES DE CONTROLE D'ACCES 24 4. 1. 2. COMMANDES DE PARAMETRAGE DU TRANSFERT 26 4. 1. 3. COMMANDES DE SERVICE FTP 28 4. 2. REPONSES FTP 32 4. 2. 1 CODES DE REPONSE PAR GROUPES DE FONCTIONS 36 4. 2. 2 CODES REPONSE PAR ORDRE NUMERIQUE 37 5. SPECIFICATIONS DECLARATIVES 39 5. 1. IMPLEMENTATION <b>MINIMALE</b> 39 5. 2. CONNEXIONS 39 5. 3. COMMANDES 40 5. 3. 1. COMMANDES FTP 41 5. 3. 2. ARGUMENTS DE COMMANDES FTP 42 5. 4. SEQUENCEMENT DES COMMANDES ET REPONSES 42 6. DIAGRAMMES D'...|$|E
40|$|URL des Documents de travail : [URL] - soumis à Journal of Empirical Finance. Documents de travail du Centre d'Economie de la Sorbonne 2008. 47 - ISSN : 1955 - 611 XIn this paper, {{we provide}} a new dynamic asset pricing model for plain vanilla options and we discuss {{its ability to}} produce minimum mispricing errors on equity option books. The data set is the daily log returns of the French CAC 40 index, on the period January 2, 1988, October 26, 2007. Under the {{historical}} measure, we adjust, on this data set, an EGARCH model with Generalized Hyperbolic innovations. We have shown (Chorro, Guégan and Ielpo, 2008) that when the pricing kernel is an exponential affine function of the state variables, the risk neutral distribution is unique and implies again a Generalized Hyperbolic dynamic, with changed parameters. Thus, using this theoretical result associated to Monte Carlo simulations, we compare our approach to natural competitors {{in order to test}} its efficiency. More generally, our empirical investigations analyze the ability of specific parametric innovations to reproduce market prices {{in the context of the}} exponential affine specification of the stochastic discount factor. Dans ce papier on propose une nouvelle méthode pour pricer des options et on discute sa capacité à produire des prix d'options avec des erreurs <b>minimales.</b> Les données utilisées correspondent au CAC 40, entre le 2 janvier 1988 et le 26 octobre 2007. Sous la probabilité historique on ajuste sur les données un modèle EGARCH avec des innovations hyperboliques. En utilisant l'approche théorique développée dans Chorro, Guégan et Ielpo (2008), on fournit les prix d'options correspondant à l'aide de simulations de Monte Carlo. Notre méthode est comparée avec plusieurs autres méthodes...|$|R
40|$|This work {{deals with}} the {{verification}} of behavioural specifications for parallel programs, and, more precisely, with the design of efficient algorithms for the comparison of two labelled transition systems modulo a simulation or a bisimulation relation. First, we recall {{the principle of the}} classical decision procedures, based on partition refinement algorithms. This approach requires to previously build the transition relations of the two systems before the comparison phase, which constitutes a practical limitation. Consequently, we propose an original algorithm, based on a depth-first traversal of a synchronous product of the two systems, which allows to perform the comparison ``on the fly'', without explicitly building or storing the two transition relations. This ``on the fly'' comparison algorithm has been implemented within the Aldebaran verification tool with for various relations: strong bisimulation, observational equivalence, tau*a-bisimulation, delay bisimulation and branching bisimulation, as well as safety equivalence and preorder. Its application to the verification of several Lotos programs confirms the interest of this approach in comparison with the more classical ones. Finally, we are also concerned with diagnostic generation when the two labelled transitions systems are not equivalent: the decision procedures implemented within Aldebaran provide a set of discriminating execution sequences, which are minimal with respect to a given order relation. Nous rappelons tout d'abord le principe des procedures de decision classiques,qui reposent sur des algorithmes de raffinement de partitions. Cette approche necessite de construire au prealable les relations de transition des deux systemes a comparer, ce qui constitue une limitation en pratique. Nous proposons par consequent un algorithme original, base sur un parcours en profondeur du produit synchrone des deux systèmes, qui permet d'effectuer la comparaison ``a la volee'', sans jamais construire ni mémoriser explicitementles deux relations de transition. L'algorithme de comparaison ``a la volée'' a ete mis en œuvre au sein du logiciel de verification Aldebaran dans le cas de différentes relations : la bisimulation forte, l'équivalence observationnelle, la tau*a-bisimulation, la delay bisimulation et la bisimulation de branchement, ainsi que le preordre et l'equivalence de surete. Son application a la verification de plusieurs programmes Lotos de taille realiste a confirme l'interet pratique de notre approche par rapport aux methodes classiques. Enfin, nous nous interessons egalement a la generation d'un diagnostic lorsque les deux systemes de transitions etiquetees a comparer ne sont pas equivalents : les procedures de decision implementees dans Aldebaran fournissent le cas echeant un ensemble de sequences d'execution discriminantes, <b>minimales</b> pour une relation d'ordre donnee...|$|R
40|$|La nappe aquifère de Hesbaye, logée dans les craies du Crétacé, est sollicitée à raison de trente {{millions}} de mètres cubes par an. Bien que naturellement protégée par une épaisseur de 5 à 20 mètres de limons, de nombreux indices montrent une dégradation de la qualité des eaux souterraines, notamment par les nitrates. Les concentrations en nitrates atteignent 15 à 25 mg. l- 1 dans la partie semi-captive de la nappe et sont systématiquement supérieures à 35 mg. l- 1 dans la partie libre. Malgré de fortes fluctuations temporelles, les teneurs augmentent en moyenne de 0, 35 mg. l- 1 à 0, 7 mg. l- 1 par an selon la situation semi-captive ou libre de la nappe. La détermination des paramètres hydrodynamiques et de transport de la craie par plus de 35 traçages répartis sur 11 sites, a permis de réaliser un modèle local (10 km 2) de transport simulant la propagation des nitrates dans la nappe. Le modèle a montré que cette dernière est, malgré une certaine homogénéisation, très sensible aux apports de surface engendrant une très forte variation spatiale des concentrations. La nappe réagit de manière très différente selon que les apports de surface sont d'origine ponctuelle ou diffuse. Pour les pollutions ponctuelles, les concentrations fluctuent rapidement avec des valeurs maximales et <b>minimales</b> observées respectivement en périodes de basses eaux et de hautes eaux. Cette situation est liée à un phénomène de dilution de la pollution par les eaux en provenance de l'amont. En cas de suppression d'une pollution ponctuelle, la qualité de la nappe s'améliore rapidement (délai de 1 à 2 ans). Pour les pollutions diffuses, les concentrations <b>minimales</b> s'observent en période de rabattement de la nappe : le front de nitrates migre plus lentement (environ 1 à 2 m par an) que les vitesses de rabattement de la nappe (jusqu'à 5 m par an) et les intrants restent nuls durant des périodes pouvant aller jusqu'à 3 ans. Différentes simulations mathématiques ont montré que si la quantité d'intrants d'origine diffuse diminue de manière permanente, la nappe mettra une vingtaine d'années pour se rééquilibrer. Ces constatations sont primordiales {{dans le cadre de}} la mise en œuvre de mesures de protection puisque, si les résultats de la suppression des pollutions ponctuelles sont rapidement mais localement observés, ceux liés à la diminution des pollutions d'origine diffuse sont observés dans des délais nettement plus longs (une à deux décennies). Ces résultats montrent clairement que toute gestion qualitative des aquifères doit être basée sur des actions à long terme. The Hesbaye area {{is located}} in the northeastern part of Belgium. The aquifer formations consist of chalk deposits. Groundwater provides about 80, 000 m 3 d- 1. Despite 5 to 20 meters of superficial loess deposits, the groundwater quality is threatened by increasing nitrate concentrations of 0. 35 mg×L- 1 per year in the semi-confined part of the aquifer to 0. 7 mg×L- 1 in the unconfined aquifer. Presently, nitrate concentrations are between 15 and 25 mg×L- 1 in the semi-confined part of the aquifer but are more than 35 mg×L- 1 (reaching locally 150 mg×L- 1) in the unconfined part that covers 95 % of the area. Nitrate concentrations have such a high spatial variation that various statistical treatments (such as kriging used to draw iso-concentration maps) have failed. This failure {{is due to the fact}} that the concentrations are highly influenced by surface land use (grass land, culture land, villages, point source pollutants, etc.). In addition, nitrate content in the aquifer varies vertically with decreasing values at depth (gradient of 0. 7 mg×L- 1 ×m- 1). Aquifer parameters were determined by 38 pumping and tracer tests conducted in radial convergent or cylindrical flow at 11 sites. Results showed that hydraulic conductivity values ranged from 1 × 10 - 6 m×s- 1 to 4 × 10 - 2 m×s- 1 and effective porosities from 0. 5 % to 7 %, showing that the aquifer was heterogeneous. Dispersivity values were affected by scale effects and varied according to chalk weathering or fracture zones. They ranged from less than 5 m in fractures to more than 60 m in weathered chalk (as in the upper part of the aquifer) and in the chalk matrix. In the chalk, transport processes were influenced by the immobile water effect due to diffusive transfer from the moving to the non-moving fluid. Non-effective porosity filled by non-moving fluid was estimated between 8 to 42 %. The transfer constant ranged from 0. 98 × 10 - 7 s- 1 to 10 × 10 - 7 s- 1. The determination of the transport parameters allowed simulation of nitrate transport at a regional scale. The SUFT 3 D (Saturated and Unsaturated Flow and Transport Model), developed by the Hydrogeology Section of the Georesources, Geotechnologies and Building Materials Department of Liege University was used. The modelled groundwater zone was defined as a 2. 0 x 4. 5 km rectangle of 10 km 2. The aquifer was subdivided into 6 layers of 3350 cells (50 x 50 m wide and 3 to 15 m thick). Boundary flow conditions were defined as a prescribed head (Dirichlet conditions) to the north and the south of the area modelled. As the model simulations run for a time period of 30 years, the northern Dirichlet conditions had to be adapted to the regional and seasonal water table fluctuations that were observed during this period. At the south boundary, as the aquifer is drained by the river Geer, the water table is fixed at the river bed altitude. The eastern and western boundaries were, according to the regional piezometry, assumed to be impermeable. For the transport boundary conditions, prescribed flux (Cauchy conditions) was used for the aquifer top. Elsewhere Neumann conditions were usedSimulations were run for the period from 1963 to 1992. Nitrate inputs were averaged yearly and estimated according to actual input conditions. These conditions were calculated by simulation of nitrate flows through the non-saturated part of the aquifer using the EPIC-Model and taking into account the amount of nitrate fertilisers used by farmers (given by the Belgian government Statistical Institute). Initial conditions were calculated according to the 1963 nitrate inputs. Simulations demonstrated that it is important to distinguish the origin of the pollution as either point or non-point (diffuse) sources. For point source pollutants (such as contaminated infiltration basins), aquifer nitrate concentrations increased during low water level periods due to weaker dilution linked with a poor regional water gradient. During high groundwater levels, dilution is more important and the nitrate concentration decreases. If a point source pollutant is suppressed, aquifer quality is improved within one to two years. This demonstrates the importance of protective actions that could be applied within the framework of the protection zones around collecting galleries and pumping fields. For diffuse contamination the mean input over the area (10 m depth below cropped areas) increased from 1. 32 × 10 - 7 mg×m- 2 ×s- 1 in 1963 to 5. 14 × 10 - 7 mg×m- 2 ×s- 1 (i. e., a factor of four). According to these values, concentrations ranged from 11 mg×L- 1 to 22 mg×L- 1 (i. e., increasing by 0. 5 mg×L- 1 per year) between 1963 and 1992. Predictive simulations, using 1992 input, show that it will take more or less 30 years for the aquifer to be in equilibrium with the 1992 input. At that time the mean concentration value will be around 30 mg×L- 1. The main results of the simulations clearly show that if actions are taken to decrease nitrate inputs, even if the aquifer nitrate contents rapidly react to the new input, nitrate levels will decrease slowly and take about 30 years to be in equilibrium with the new inputs. This long delay is due to the immobile water effect that is characteristic of the chalk aquifer. Thus it is important to inform environmentalists who work on action programs (such as the water directive imposed by the European Community in the vulnerable zones) that the effects of their actions must be based on 10 to 20 year scenarios. To this estimation, based on the reaction time of the aquifer to a new input, one must also add the time transfer of the pollutant through the unsaturated part of the aquifer...|$|R
40|$|ABSTRACT. Two {{continental}} glaciations, both considered Wisconsin in age {{have been}} noted in the eastern Mealy Mountains. The earlier ice sheet reached a mini-mum elevation of 3, 000 feet (925 m.) and may have overtopped the summit plateaus of 4, 000 feet (1, 240 m.). The ice moved from west to east. The vertical limit of the later ice sheet varied from 2, 300 feet (710 m.) {{in the west to}} 1, 800 feet (555 m.) in the east. Cirques were carved prior to both glaciations. The development of active glaciers with pronounced end moraines, subsequent to the second and later glaciation, was restricted to cirques oriented towards the east and southeast. Cirque glacier re-advances have probably not occurred within the last few hundred years. R ~ S U M É. Histoire glaciaire des monts Mealy orientaux, Labrador méridional. Dans les monts Mealy orientaux, on a distingué deux glaciations continenta-les, toutes deux considérées comme d’âge wisconsinien. La plus ancienne calotte atteignait une altitude <b>minimale</b> de 3000 pieds (925 m) et a peut-être recou-vert les plateaux sommitaux de 4000 pieds (1240 m) d’altitude. La glace se déplaçait d‘ouest en est. La limite verticale de la calotte plus récente variait de 2300 pied...|$|E
40|$|This {{thesis is}} based on the article "Entropie <b>minimale</b> et rigiditès des espaces localement symétriques de courbure strictement négative" of G. Besson, G. Courtois and S. Gallot. We {{consider}} a compact oriented n-Rimennian manifold M that supports a locally symmetric metric of negative sectional curvature. The aim of the thesis is to show that an invariant characterizes this metric. The invariant that we consider is the volume entropy of a metric g on a compact Riemannian manifold. It measures the growth of the volume of the balls B(x,r) of the covering manifold of M endowed with the cover metric when r tends to infinity. The entropy of a metric g is denoted by h(g). The quantity ent(g) =h(g) ^n*Vol(M,g), where Vol(M,g) is the volume of M calculated with respect to g, is invariant under homotheties. The main result of the thesis is that the locally symmetric metric of negative curvature on M minimizes the quantity ent(g), where g varies among all the Riemannian metrics on M. Moreover it is unique in the following sense: if g is a Riemannian metric that minimizes the functional ent, then, up to homotheties, the metric g is isometric to the locally symmetric one. This result give us some rigidity theorems on Riemannian manifolds that support a locally symmetric metric of negative curvature. For example a corollary of the theorem above is the well known Mostow rigidity theorem...|$|E
40|$|This paper {{presents}} an experimental approach {{to determine the}} {{coefficient of thermal expansion}} of concrete at early age, in which a group of three sealed concrete prisms (75 x 75 x 295 mm 3) are subjected to 25 - 300 C temperature cycles in an environmental chamber from set time to an age of at least seven days. The concrete deformation is measured with high-precision displacement sensors and the temperature is measured by thermocouples embedded in the concrete prisms. The coefficient of thermal expansion is determined as a function of time by direct calculation on the measured data. For the high-performance concrete used in this study, {{it was found that the}} coefficient of thermal expansion decreased towards a minimum value of 6 x 10 - 6 / 0 C one day after the setting of concrete, and then increased linearly until a more stable value of 8 x 10 - 6 / 0 C was reached at the age of 4 days. Pour le b 9 ton 0 hautes performances utilis 9 dans le cadre de cette 9 tude, on a d 9 couvert que le coefficient de dilatation thermique diminue vers une valeur <b>minimale</b> de 6 x 10 - 6 / 0 C un (1) jour apr 8 s la prise du b 9 ton, pour augmenter ensuite lin 9 airement jusqu' 0 une valeur plus stable de 8 x 10 - 6 / 0 C 0 l' 2 ge de 4 jours. Peer reviewed: YesNRC publication: Ye...|$|E
40|$|In {{this thesis}} are {{exploited}} several instances {{of the relationship}} between convex Cauchy surfaces S in flat Lorentzian (2 + 1) -dimensional maximal globally hyperbolic manifolds M and the tangent bundle of Teichmüller space T(S) of the topological surface S. This relationship was first pointed out by Geoffrey Mess in the case of closed surfaces. The first case presented is the case of simply connected surfaces, and M is a domain of dependence in R^ 2, 1. We prove a classification of entire surfaces of constant curvature in R^ 2, 1 in terms of Zygmund functions on the circle, which represent tangent vectors of universal Teichmüller space T() at the identity. An important ingredient is the solvability of Minkowski problem for Cauchy surfaces in any domain of dependence M contained in the future cone over some point of R^ 2, 1, which is proved by analyzing the Dirichlet problem for the Monge-Ampère equation D^ 2 u(z) =(1 /ψ(z)) (1 -|z|^ 2) ^- 2 on the disc, where ψ is a smooth positive function. Moreover, when S is a surface of constant curvature, the principal curvatures are bounded if and only if φ is in the Zygmund class. The situation of S a closed surface, and M is a maximal globally hyperbolic flat spacetime diffeomorphic to S×R, is next discussed. We provide an explicit relation between the embedding data of any strictly convex Cauchy surface in M and the holonomy of M, which was used by Mess to parametrize the moduli space of manifolds M as above by means of the tangent bundle of T(S). The techniques used in this thesis are amenable to be extended to the case of globally hyperbolic flat spacetimes with n> 0 particles, namely cone singularities along timelike lines, where the cone angle is assumed in (0, 2 π). The analogue of Mess' parametrization is then proved, showing that the corresponding moduli space is parametrized by the tangent bundle of Teichmüller space of the closed surface S with n punctures. The above connections can be regarded as an infinitesimal version of the relation of Teichmüller space T(S) and universal Teichmüller space T() with surfaces in maximal globally hyperbolic Anti-de Sitter manifolds (either with the topological type of a closed surface, or with trivial topology) and in quasi-Fuchsian hyperbolic manifolds (or in H^ 3 itself). In {{the last part of the}} thesis this perspective is discussed, and the behavior of zero mean curvature surfaces in H^ 3 and AdS^ 3 close to the Fuchsian locus is discussed. The main result in hyperbolic space is a sublinear estimate of the supremum of principal curvatures of a minimal embedded disc in H^ 3 spanning a quasicircle Γ in the boundary at infinity in terms of the norm of Γ in the sense of universal Teichmüller space, provided Γ is sufficiently close to being the boundary of a totally geodesic plane. As a by-product, there is a universal constant C such that if the Teichmüller distance between the ends of a quasi-Fuchsian manifold M is at most C, then M is almost-Fuchsian, independently of the genus. In Anti-de Sitter space, an estimate is proved for the principal curvatures of any maximal surface with boundary at infinity the graph of a quasisymmetric homeomorphism ϕ of the circle. The supremum of the principal curvatures is estimated again in a sublinear way, in terms of the cross-ratio norm of ϕ. This also provides a bound on the maximal distortion of the quasiconformal minimal Lagrangian extension to the disc of a given quasisymmetric homeomorphism. Dans ma thèse doctorale, j'ai étudié principalement les plongements de surfaces dans des 3 -variétés Riemanniennes et Lorentziennes de courbure constante. J'ai écrit deux articles avec mon directeur de thèse Francesco Bonsante sur le cas des variétés Lorentziennes plates, l'un sur les surfaces convexes dans les espace-temps plats maximaux globalement hyperboliques, également quand on permet l'existence de singularités de type temps, et l'autre sur les surfaces convexes dans l'espace de Minkowski (qui est l'analogue Lorentzien de l'espace Euclidien) en relation avec la courbure Gaussienne. D'autre part, pendant mon séjour à l'Université du Luxembourg, j'ai commencé l'étude des surfaces à courbure moyenne nulle dans 3 -variétés à courbure sectionnelle constante et négative. J’ai écrit un article qui concerne les surfaces <b>minimales</b> dans l'espace hyperbolique et un autre article concernent des surfaces maximales dans l'espace Anti-de Sitter, qui peut être considéré comme l'analogue Lorentzien de l'espace hyperbolique. Dans plusieurs cas, il y a une forte relation entre les surfaces plongées et la théorie de Teichmüller, en particulier la théorie des applications entre surfaces...|$|R
40|$|The {{following}} {{thesis is}} concerned with the elucidation of structural changes of RNA molecules during the time course of dynamic processes that are commonly denoted as folding reactions. In contrast to the field of protein folding, the concept of RNA folding comprises not only folding reactions itself but also refolding- or conformational switching- and assembly processes (see chapter III). The method in this thesis to monitor these diverse processes is high resolution liquid-state NMR spectroscopy. To understand the reactions is of considerable interest, because most biological active RNA molecules function by changing their conformation. This can be either an intrinsic property of their respective sequence or may happen in response to a cellular signal such as small molecular ligand binding (like in the aptamer and riboswitch case), protein or metal binding. The first part of the thesis (chapters II & III) provides a general overview over the field of RNA structure and RNA folding. The two chapters aim at introducing the reader into the current status of research in the field. Chapters II is structured such that primary structure is first described then secondary and tertiary structure elements of RNA structure. A special emphasis is given to bistable RNA systems that are functionally important and represent models to understand fundamental questions of RNA conformational switching. RNA folding in vitro as well as in vivo situations is discussed in Chapter III. The following chapters IV and V also belong to the introduction part and review critically the NMR methods that were used to understand the nature and the dynamics of the conformational/structural transitions in RNA. A general overview of NMR methods quantifying dynamics of biomolecules is provided in chapter IV. A detailed discussion of solvent exchange rates and time-resolved NMR, as the two major techniques used, follows. In the final chapter V of the first part the NMR parameters used in structure calculation and structure calculation itself are conferred. The second part of the thesis, which is the cumulative part, encompasses the conducted original work. Chapter VI reviews the general NMR techniques applied and explains their applicability in the field of RNA structural and biochemical studies in several model cases. Chapter VII describes the achievement of a complete resonance assignment of an RNA model molecule (14 mer cUUCGg tetral-loop RNA) and introduces a new technique to assign quaternary carbon resonances of the nucleobases. Furthermore, it reports on a conformational analysis of the sugar backbone in this RNA hairpin molecule in conjunction with a parameterization of 1 J scalar couplings. Achievements: • Establishment of two new NMR pulse-sequences facilitating the assignment of quaternary carbons in RNA nucleobases • First complete (99. 5 %) NMR resonance assignment of an RNA molecule (14 mer) including 1 H, 13 C, 15 N, 31 P resonances • Description of RNA backbone conformation by a complete set of NMR parameters • Description of the backbone conformational dependence in RNA of new NMR parameters (1 J scalar couplings) Chapters VII & VIII summarize the real-NMR studies that were conducted to elucidate the conformational switching events of several RNA systems. Chapter VIII gives an overview on the experiments that were accomplished on three different bistable RNAs. These molecules where chosen to be good model systems for RNA refolding reactions and so consequently served as reporters of conformational switching events of RNA secondary structure elements. Achievements: • First kinetic studies of RNA refolding reactions with atomic resolution by NMR • Application of [new] RT-NMR techniques either regarding the photolytic initiation of the reaction or regarding the readout of the reaction • Discovery of different RNA refolding mechanisms for different RNA molecules Deciphering of a general rule for RNA refolding methodology to conformational switching processes of RNA tertiary structure elements. The models for these processes were a) the guanine-dependent riboswitch RNA and b) the minimal hammerhead ribozyme. Achievements: • NMR spectroscopic assignment of imino-resonances of the hypoxanthine bound guanine-dependent riboswitch RNA • Application of RT-NMR techniques to monitor the ligand induced conformational switch of the aptamer domain of the guanine-dependent riboswitch RNA at atomic resolution • Translation of kinetic information into structural information • Deciphering a folding mechanism for the guanine riboswitch aptamer domain • Application of RT-NMR techniques to monitor the reaction of the catalytically active mHHR RNA at atomic resolution In the appendices the new NMR pulse-sequences and the experimental parameters are described, which are not explicitly treated in the respective manuscripts. Die vorliegende Doktorarbeit beschäftigt sich mit den strukturellen Änderungen in RNA Molekülen während dynamischer konformationeller Änderungen, die gemeinhin als RNA-Faltung bezeichnet werden. Im Gegensatz zur Proteinfaltung sind RNA-Faltungsprozesse nicht exklusiv als die Faltung einer definierten Konformation aus einem Ensemble an ungefalteten, d. h. ausgehend von unstrukturierten Molekülen, zu verstehen. RNA-Faltung beinhaltet vielmehr die strukturelle Umwandlung verschiedener stabiler Konformationen (die als RNA-Umfaltung benannt wird) und den Aufbau von molekularen Komplexen aus mehreren Molekülen (siehe Kapitel III). Die experimentelle Technik, die hier zur Untersuchung dieser Prozesse genutzt wurde, ist die hochauflösende Flüssig-NMR-Spektroskopie. Das Verständnis der strukturellen und biophysikalischen Grundlagen solcher Umfaltungsreaktionen von RNA ist essentiell, da solche konformationellen Änderungen die biologische Funktion der Moleküle modulieren. Dabei ist zu bemerken, dass eine Umfaltungsreaktion eine intrinsische Eigenschaft einer gegebenen RNA-Sequenz sein kann oder die Antwort auf ein externes zelluläres Signal, wie die Bindung eines niedermolekularen Liganden (z. B. in Aptameren und in Riboswitch RNAs), eines Proteins oder eines Metall-Ions. Der erste Teil dieser Doktorarbeit (Kapitel I & II) hält einen Überblick über die Themengebiete RNA-Struktur und RNA-Faltung bereit. Beide Kapitel führen in den derzeitigen Stand der Forschung ein. Kapitel II führt dabei entlang der hierarchischen Ordnung von RNA Molekülen und diskutiert die Eigenschaften von Primär-, Sekundär- und Tertiär-Strukturelementen. Ein besonderes Augenmerk wird dabei auf bistabile RNA Systeme gelegt; ihre wichtige biologische Funktionalität wird dargestellt, ebenso wird das Potential ausgeleuchtet, diese funktionale Klasse von RNA Molekülen als Modellsysteme zu nutzen, um fundamentale Fragen zu konformationellen Übergängen in RNA zu beantworten. In Kapitel III folgt sodann die Diskussion über RNA-Faltung in in vitro Experimenten als auch im zellulären Kontext (in vivo). Die Kapitel IV und V besprechen die NMR-spektroskopischen Techniken, die genutzt werden, um die Art und die dynamischen Eigenschaften von konformationellen/strukturellen Umwandlungen in RNA zu untersuchen. Hierbei wird der Schwerpunkt auf die verwendeten Techniken des Wasseraustauschs an labilen Protonen und der zeitaufgelösten NMR-Spektroskopie gelegt. Der zweite Teil der Doktorarbeit fasst kumulativ die durchgeführten Studien zusammen. Kapitel VI bespricht hierbei die grundlegenden NMR Techniken, die zur Strukturaufklärung von RNA Molekülen angewendet werden und zeigt deren Anwendungsmöglichkeiten an unterschiedlichen Beispielen von strukturellen und biochemischen Studien. Das folgende Kapitel VII beschreibt die komplette Resonanzzuordnung eines RNA Modell-Moleküls (14 mer cUUCGg tetra-loop RNA) und stellt eine neue Pulstechnik vor, die zur Zuordnung der Resonanzen von quatären Kohlenstoffen in Purinbasen benützt werden kann. Weiterhin schließt sich ein Report an, wie die Konformation des Zuckerrückgrates in RNA-Molekülen bestimmt wird und schlägt mittels einer an oben genanntem Modellsystem durchgeführte Parametrisierung 1 J skalare Kopplungen als neue Strukturparameter vor. Kapitel VII & VIII fassen die hierzu durchgeführten RT-NMR Studien zusammen. Kapitel VIII gibt hierbei einen Überblick über die Untersuchungen an drei bistabilen RNA-Systemen. Diese Moleküle wurden ausgewählt, da sie als Modelle für RNA-Umfaltungsreakionen dienen. Das finale Kapitel IX behandelt die Anwendung der oben ausgeführten neuen Methodologie auf konformationelle Umwandlungen von RNA Tertiär-Strukturelementen: a) Guanin-abhängige Riboswitch RNA (GSW) und b) <b>Minimales</b> "hammerhead" Ribozym (mHHR) ...|$|R
40|$|Over {{the past}} 25 years, search engines have {{become one of}} the most important, if not the entry point of the World Wide Web. This {{development}} has been primarily due to the continuously increasing amount of available documents, which are highly unstructured. Moreover, the general trend is towards classifying search results into categories and presenting them in terms of semantic information that answer users' queries without having to leave the search engine. With the growing amount of documents and technological enhancements, the needs of users as well as search engines are continuously evolving. Users want to be presented with increasingly sophisticated results and interfaces while companies have to place advertisements and make revenue to be able to offer their services for free. To address the above needs, it is more and more important to provide highly usable and optimized search engine results pages (SERPs). Yet, existing approaches to usability evaluation are often costly or time-consuming and mostly rely on explicit feedback. They are either not efficient or not effective while SERP interfaces are commonly optimized primarily from a company's point of view. Moreover, existing approaches to predicting search result relevance, which are mostly based on clicks, are not tailored to the evolving kinds of SERPs. For instance, they fail if queries are answered directly on a SERP and no clicks need to happen. Applying Human-Centered Design principles, we propose a solution to the above in terms of a holistic approach that intends to satisfy both, searchers and developers. It provides novel means to counteract exclusively company-centric design and to make use of implicit user feedback for efficient and effective evaluation and optimization of usability and, in particular, relevance. We define personas and scenarios from which we infer unsolved problems and a set of well-defined requirements. Based on these requirements, we design and develop the Search Interaction Optimization toolkit. Using a bottom-up approach, we moreover define an eponymous, higher-level methodology. The Search Interaction Optimization toolkit comprises a total of six components. We start with INUIT [1], which is a novel minimal usability instrument specifically aiming at meaningful correlations with implicit user feedback in terms of client-side interactions. Hence, it serves as a basis for deriving usability scores directly from user behavior. INUIT has been designed based on reviews of established usability standards and guidelines as well as interviews with nine dedicated usability experts. Its feasibility and effectiveness have been investigated in a user study. Also, a confirmatory factor analysis shows that the instrument can reasonably well describe real-world perceptions of usability. Subsequently, we introduce WaPPU [2], which is a context-aware A/B testing tool based on INUIT. WaPPU implements the novel concept of Usability-based Split Testing and enables automatic usability evaluation of arbitrary SERP interfaces based on a quantitative score that is derived directly from user interactions. For this, usability models are automatically trained and applied based on machine learning techniques. In particular, the tool is not restricted to evaluating SERPs, but can be used with any web interface. Building on the above, we introduce S. O. S., the SERP Optimization Suite [3], which comprises WaPPU as well as a catalog of best practices [4]. Once it has been detected that an investigated SERP's usability is suboptimal based on scores delivered by WaPPU, corresponding optimizations are automatically proposed based on the catalog of best practices. This catalog has been compiled in a three-step process involving reviews of existing SERP interfaces and contributions by 20 dedicated usability experts. While the above focus on the general usability of SERPs, presenting the most relevant results is specifically important for search engines. Hence, our toolkit contains TellMyRelevance! (TMR) [5] — the first end-to-end pipeline for predicting search result relevance based on users’ interactions beyond clicks. TMR is a fully automatic approach that collects necessary information on the client, processes it on the server side and trains corresponding relevance models based on machine learning techniques. Predictions made by these models can then be fed back into the ranking process of the search engine, which improves result quality and hence also usability. StreamMyRelevance! (SMR) [6] takes the concept of TMR one step further by providing a streaming-based version. That is, SMR collects and processes interaction data and trains relevance models in near real-time. Based on a user study and large-scale log analysis involving real-world search engines, we have evaluated the components of the Search Interaction Optimization toolkit as a whole—also to demonstrate the interplay of the different components. S. O. S., WaPPU and INUIT have been engaged in the evaluation and optimization of a real-world SERP interface. Results show that our tools are able to correctly identify even subtle differences in usability. Moreover, optimizations proposed by S. O. S. significantly improved the usability of the investigated and redesigned SERP. TMR and SMR have been evaluated in a GB-scale interaction log analysis as well using data from real-world search engines. Our findings indicate that they are able to yield predictions that are better than those of competing state-of-the-art systems considering clicks only. Also, a comparison of SMR to existing solutions shows its superiority in terms of efficiency, robustness and scalability. The thesis concludes with a discussion of the potential and limitations of the above contributions and provides an overview of potential future work. Im Laufe der vergangenen 25 Jahre haben sich Suchmaschinen zu einem der wichtigsten, wenn nicht gar dem wichtigsten Zugangspunkt zum World Wide Web (WWW) entwickelt. Diese Entwicklung resultiert vor allem aus der kontinuierlich steigenden Zahl an Dokumenten, welche im WWW verfügbar, jedoch sehr unstrukturiert organisiert sind. Überdies werden Suchergebnisse immer häufiger in Kategorien klassifiziert und in Form semantischer Informationen bereitgestellt, die direkt in der Suchmaschine konsumiert werden können. Dies spiegelt einen allgemeinen Trend wider. Durch die wachsende Zahl an Dokumenten und technologischen Neuerungen wandeln sich die Bedürfnisse von sowohl Nutzern als auch Suchmaschinen ständig. Nutzer wollen mit immer besseren Suchergebnissen und Interfaces versorgt werden, während Suchmaschinen-Unternehmen Werbung platzieren und Gewinn machen müssen, um ihre Dienste kostenlos anbieten zu können. Damit geht die Notwendigkeit einher, in hohem Maße benutzbare und optimierte Suchergebnisseiten – sogenannte SERPs (search engine results pages) – für Nutzer bereitzustellen. Gängige Methoden zur Evaluierung und Optimierung von Usability sind jedoch größtenteils kostspielig oder zeitaufwändig und basieren meist auf explizitem Feedback. Sie sind somit entweder nicht effizient oder nicht effektiv, weshalb Optimierungen an Suchmaschinen-Schnittstellen häufig primär aus dem Unternehmensblickwinkel heraus durchgeführt werden. Des Weiteren sind bestehende Methoden zur Vorhersage der Relevanz von Suchergebnissen, welche größtenteils auf der Auswertung von Klicks basieren, nicht auf neuartige SERPs zugeschnitten. Zum Beispiel versagen diese, wenn Suchanfragen direkt auf der Suchergebnisseite beantwortet werden und der Nutzer nicht klicken muss. Basierend auf den Prinzipien des nutzerzentrierten Designs entwickeln wir eine Lösung in Form eines ganzheitlichen Ansatzes für die oben beschriebenen Probleme. Dieser Ansatz orientiert sich sowohl an Nutzern als auch an Entwicklern. Unsere Lösung stellt automatische Methoden bereit, um unternehmenszentriertem Design entgegenzuwirken und implizites Nutzerfeedback für die effizienteund effektive Evaluierung und Optimierung von Usability und insbesondere Ergebnisrelevanz nutzen zu können. Wir definieren Personas und Szenarien, aus denen wir ungelöste Probleme und konkrete Anforderungen ableiten. Basierend auf diesen Anforderungen entwickeln wir einen entsprechenden Werkzeugkasten, das Search Interaction Optimization Toolkit. Mittels eines Bottom-up-Ansatzes definieren wir zudem eine gleichnamige Methodik auf einem höheren Abstraktionsniveau. Das Search Interaction Optimization Toolkit besteht aus insgesamt sechs Komponenten. Zunächst präsentieren wir INUIT [1], ein neuartiges, <b>minimales</b> Instrument zur Bestimmung von Usability, welches speziell auf sinnvolle Korrelationen mit implizitem Nutzerfeedback in Form Client-seitiger Interaktionen abzielt. Aus diesem Grund dient es als Basis für die direkte Herleitung quantitativer Usability-Bewertungen aus dem Verhalten von Nutzern. Das Instrument wurde basierend auf Untersuchungen etablierter Usability-Standards und -Richtlinien sowie Experteninterviews entworfen. Die Machbarkeit und Effektivität der Benutzung von INUIT wurden in einer Nutzerstudie untersucht und darüber hinaus durch eine konfirmatorische Faktorenanalyse bestätigt. Im Anschluss beschreiben wir WaPPU [2], welches ein kontextsensitives, auf INUIT basierendes Tool zur Durchführung von A/B-Tests ist. Es implementiert das neuartige Konzept des Usability-based Split Testing und ermöglicht die automatische Evaluierung der Usability beliebiger SERPs basierend auf den bereits zuvor angesprochenen quantitativen Bewertungen, welche direkt aus Nutzerinteraktionen abgeleitet werden. Hierzu werden Techniken des maschinellen Lernens angewendet, um automatisch entsprechende Usability-Modelle generieren und anwenden zu können. WaPPU ist insbesondere nicht auf die Evaluierung von Suchergebnisseiten beschränkt, sondern kann auf jede beliebige Web-Schnittstelle in Form einer Webseite angewendet werden. Darauf aufbauend beschreiben wir S. O. S., die SERP Optimization Suite [3], welche das Tool WaPPU sowie einen neuartigen Katalog von „Best Practices“ [4] umfasst. Sobald eine durch WaPPU gemessene, suboptimale Usability-Bewertung festgestellt wird, werden – basierend auf dem Katalog von „Best Practices“ – automatisch entsprechende Gegenmaßnahmen und Optimierungen für die untersuchte Suchergebnisseite vorgeschlagen. Der Katalog wurde in einem dreistufigen Prozess erarbeitet, welcher die Untersuchung bestehender Suchergebnisseiten sowie eine Anpassung und Verifikation durch 20 Usability-Experten beinhaltete. Die bisher angesprochenen Tools fokussieren auf die generelle Usability von SERPs, jedoch ist insbesondere die Darstellung der für den Nutzer relevantesten Ergebnisse eminent wichtig für eine Suchmaschine. Da Relevanz eine Untermenge von Usability ist, beinhaltet unser Werkzeugkasten daher das Tool TellMyRelevance! (TMR) [5], die erste End-to-End-Lösung zur Vorhersage von Suchergebnisrelevanz basierend auf Client-seitigen Nutzerinteraktionen. TMR ist einvollautomatischer Ansatz, welcher die benötigten Daten auf dem Client abgreift, sie auf dem Server verarbeitet und entsprechende Relevanzmodelle bereitstellt. Die von diesen Modellen getroffenen Vorhersagen können wiederum in den Ranking-Prozess der Suchmaschine eingepflegt werden, was schlussendlich zu einer Verbesserung der Usability führt. StreamMyRelevance! (SMR) [6] erweitert das Konzept von TMR, indem es einen Streaming-basierten Ansatz bereitstellt. Hierbei geschieht die Sammlung und Verarbeitung der Daten sowie die Bereitstellung der Relevanzmodelle in Nahe-Echtzeit. Basierend auf umfangreichen Nutzerstudien mit echten Suchmaschinen haben wir den entwickelten Werkzeugkasten als Ganzes evaluiert, auch, um das Zusammenspiel der einzelnen Komponenten zu demonstrieren. S. O. S., WaPPU und INUIT wurden zur Evaluierung und Optimierung einer realen Suchergebnisseite herangezogen. Die Ergebnisse zeigen, dass unsere Tools in der Lage sind, auch kleine Abweichungen in der Usability korrekt zu identifizieren. Zudem haben die von S. O. S. vorgeschlagenen Optimierungen zu einer signifikanten Verbesserung der Usability der untersuchten und überarbeiteten Suchergebnisseite geführt. TMR und SMR wurden mit Datenmengen im zweistelligen Gigabyte-Bereich evaluiert, welche von zwei realen Hotelbuchungsportalen stammen. Beide zeigen das Potential, bessere Vorhersagen zu liefern als konkurrierende Systeme, welche lediglich Klicks auf Ergebnissen betrachten. SMR zeigt gegenüber allen anderen untersuchten Systemen zudem deutliche Vorteile bei Effizienz, Robustheit und Skalierbarkeit. Die Dissertation schließt mit einer Diskussion des Potentials und der Limitierungen der erarbeiteten Forschungsbeiträge und gibt einen Überblick über potentielle weiterführende und zukünftige Forschungsarbeiten...|$|R
