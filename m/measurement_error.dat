8450|7710|Public
5|$|The {{extended}} line {{of research}} that culminated in Astronomia nova (A New Astronomy)—including the first two laws of planetary motion—began with the analysis, under Tycho's direction, of Mars' orbit. Kepler calculated and recalculated various approximations of Mars' orbit using an equant (the mathematical tool that Copernicus had eliminated with his system), eventually creating a model that generally agreed with Tycho's observations to within two arcminutes (the average <b>measurement</b> <b>error).</b> But he was {{not satisfied with the}} complex and still slightly inaccurate result; at certain points the model differed from the data by up to eight arcminutes. The wide array of traditional mathematical astronomy methods having failed him, Kepler set about trying to fit an ovoid orbit to the data.|$|E
25|$|Because {{pressure}} <b>measurement</b> <b>error</b> can {{be caused}} by more than just CCT (i.e., corneal hydration, elastic properties, etc.), it is impossible to 'adjust' pressure measurements based only on CCT measurements. The frequency doubling illusion {{can also be used to}} detect glaucoma with the use of a frequency doubling technology perimeter.|$|E
25|$|If x and y are {{results of}} {{measurements}} that contain <b>measurement</b> <b>error,</b> the realistic limits on the correlation coefficient are not −1 to +1 but a smaller range. For {{the case of a}} linear model with a single independent variable, the coefficient of determination (R squared) is the square of r, Pearson's product-moment coefficient.|$|E
40|$|In this paper, {{we propose}} a new {{empirical}} {{version of the}} Fama and French Model based on the Hausman (1978) specification test and aimed at discarding <b>measurement</b> <b>errors</b> in the variables. The proposed empirical framework is general enough {{to be used for}} correcting other financial and accounting models of <b>measurement</b> <b>errors.</b> Removing <b>measurement</b> <b>errors</b> is important at many levels as information disclosure, corporate governance and protection of investors. Asset pricing, portfolio selection, <b>errors</b> in variables, <b>measurement</b> <b>errors,</b> higher moments, instrumental variables, Specification test, corporate governance, protection of investors. ...|$|R
40|$|This article {{presents}} {{the problem of}} estimating the population mean using auxiliary information {{in the presence of}} <b>measurement</b> <b>errors.</b> A numerical study is made among the proposed estimator, the exponential ratio estimator, Singh and Solanki (2012) estimator and the mean per unit estimator in the presence of <b>measurement</b> <b>errors.</b> Key words: Population mean, Study variate, Auxiliary variates, Mean squared <b>error,</b> <b>Measurement</b> <b>errors,</b> Efficienc...|$|R
40|$|In this paper, we {{investigate}} {{the implications of}} <b>measurement</b> <b>errors</b> in the daily published stock prices on the creation and management of efficient portfolios. Using stochastic simulation techniques and the Markowitz Mean Variance approach {{in the creation of}} the weights of the various stocks of a portfolio, we conclude that <b>measurement</b> <b>errors</b> have significant implications on the efficiency of the management of a stock portfolio. Markowitz Mean Variance, <b>Measurement</b> <b>Errors</b> in Returns, Stochastic Simulation. ...|$|R
25|$|Surgery is {{indicated}} by the Society on Scoliosis Orthopaedic and Rehabilitation Treatment (SOSORT) at 45 degrees to 50 degrees and by the Scoliosis Research Society (SRS) at a Cobb angle of 45 degrees. SOSORT uses the 45-degree to 50-degree threshold {{as a result of}} the well-documented, plus or minus five degrees <b>measurement</b> <b>error</b> that can occur while measuring Cobb angles.|$|E
25|$|The Human Development Index {{has been}} criticized {{on a number of}} grounds, {{including}} alleged lack of consideration of technological development or contributions to the human civilization, focusing exclusively on national performance and ranking, lack of attention to development from a global perspective, <b>measurement</b> <b>error</b> of the underlying statistics, and on the UNDP's changes in formula which can lead to severe misclassification in the categorisation of 'low', 'medium', 'high' or 'very high' human development countries.|$|E
25|$|New {{research}} is surfacing {{that suggests that}} ability EI measures might be measuring personality in addition to general intelligence. These studies examined the multivariate effects of personality and intelligence on EI and also corrected estimates for <b>measurement</b> <b>error</b> (which is often not done in some validation studies). For example, a study by Schulte, Ree, Carretta (2004), showed that general intelligence (measured with the Wonderlic Personnel Test), agreeableness (measured by the NEO-PI), as well as gender could reliably be used to predict the measure of EI ability.|$|E
40|$|Kalman {{filters are}} {{tracking}} and prediction algorithms based on Gaussian <b>measurement</b> <b>errors</b> and structural models. The Kalman filter performance may degrade if the <b>measurement</b> <b>errors</b> {{come from a}} thicker-tailed-than Gaussian distribution. In this report non-linear procedures are described {{which are based on}} Kalman-type models, but work with student-t <b>measurement</b> <b>errors.</b> Keywords: Kalman filter; Student-t measurement errors; Iterative reweighting procedure; Nonlinear filter; Biweight; Robust estimationPrepared for: Chief of Naval Research[URL] provided by the Chief of Naval Research, Arlington, VA...|$|R
40|$|Revealed {{preference}} {{tests are}} widely used in empirical applications of consumer rationality. These are static tests, and consequently, lack ability to handle <b>measurement</b> <b>errors</b> in the data. This paper extends and generalizes existing procedures that account for <b>measurement</b> <b>errors</b> in revealed preference tests. In particular, it introduces a very efficient method to implement these procedures, which make them operational for large data sets. The paper illustrates the new method for both classical and Berkson <b>measurement</b> <b>errors</b> models...|$|R
40|$|<b>Measurement</b> <b>errors</b> in {{economic}} data are pervasive and nontrivial in size. The presence of <b>measurement</b> <b>errors</b> causes biased and inconsistent parameter estimates {{and leads to}} erroneous conclusions to various degrees {{in economic}} analysis. While linear errors-in-variables models are usually handled with well-known instrumental variable methods, this article {{provides an overview of}} recent research papers that derive estimation methods that provide consistent estimates for nonlinear models with <b>measurement</b> <b>errors.</b> We review models with both classical and nonclassical <b>measurement</b> <b>errors,</b> and with misclassification of discrete variables. For each of the methods surveyed, we describe the key ideas for identification and estimation, and discuss its application whenever it is currently available. (JEL C 20, C 26, C 50) ...|$|R
25|$|Deletion of outlier data is a {{controversial}} practice frowned upon by many scientists and science instructors; while mathematical criteria provide an objective and quantitative method for data rejection, {{they do not}} make the practice more scientifically or methodologically sound, especially in small sets or where a normal distribution cannot be assumed. Rejection of outliers is more acceptable in areas of practice where the underlying model of the process being measured and the usual distribution of <b>measurement</b> <b>error</b> are confidently known. An outlier resulting from an instrument reading error may be excluded but it is desirable that the reading is at least verified.|$|E
25|$|Outliers {{can occur}} by chance in any distribution, {{but they often}} {{indicate}} either <b>measurement</b> <b>error</b> or that the population has a heavy-tailed distribution. In the former case one wishes to discard them or use statistics that are robust to outliers, while {{in the latter case}} they indicate that the distribution has high skewness and that one should be very cautious in using tools or intuitions that assume a normal distribution. A frequent cause of outliers is a mixture of two distributions, which may be two distinct sub-populations, or may indicate 'correct trial' versus 'measurement error'; this is modeled by a mixture model.|$|E
25|$|The {{discovery}} of Ceres led Gauss {{to his work}} on a theory of the motion of planetoids disturbed by large planets, eventually published in 1809 as Theoria motus corporum coelestium in sectionibus conicis solem ambientum (Theory of motion of the celestial bodies moving in conic sections around the Sun). In the process, he so streamlined the cumbersome mathematics of 18th century orbital prediction that his work remains a cornerstone of astronomical computation. It introduced the Gaussian gravitational constant, and contained an influential treatment of the method of least squares, a procedure used in all sciences to this day to minimize the impact of <b>measurement</b> <b>error.</b>|$|E
40|$|Abstract: This paper {{describes}} the basic data process of antenna surface geometrical measurement {{and the quality}} evaluation indexes, which are surface accuracy and its uncertainty. The peak to peak value of normal deviation is used to characterize the surface accuracy,and its deviation is defined as its uncertainty. The <b>measurement</b> <b>errors</b> are given under repeated measuring conditions. Under repeated measuring conditions,the simulation method is that adding the <b>measurement</b> <b>errors</b> calculated by repeated measurement to theoretical coordinates. Under single measuring conditions, {{the first step is}} to convert the measurement data from the measuring coordinate system to the design coordinate system to get the transformation parameters;then use these parameters to convert theoretical coordinates to the measurement coordinate system;and then add <b>measurement</b> <b>errors</b> to transformed coordinates for simulation. The key point is adding <b>measurement</b> <b>errors</b> to the data which do not contain <b>measurement</b> <b>errors.</b> The simulation and actual measurement experiments of a φ 0. 7 meter antenna were conducted,which showed that the method is correct...|$|R
30|$|If <b>measurement</b> <b>errors</b> are detected, the {{procedure}} for calculating the rigidity measures tries to purge them from the wage change distribution by computing a new distribution, named as the “true” or “error-corrected” distribution, which replaces the observed distribution in Equation 1. The detection of <b>measurement</b> <b>errors</b> {{is based on the}} analysis of the autocorrelation of wage changes: positive changes followed by negative changes are taken {{as a sign of the}} existence of <b>measurement</b> <b>errors</b> (for further details, see (Dickens and Goette 2005)).|$|R
40|$|The {{effects of}} <b>measurement</b> <b>errors</b> on usual linear {{regression}} estimator are exam-ined. A comparative study is made among the linear regression estimator, the mean per unit estimator and the ratio estimator {{in the presence}} of <b>measurement</b> <b>errors.</b> Key words: Bias; efficiency; mean square error; observational error. ...|$|R
25|$|There {{are several}} {{important}} analyser characteristics. The mass resolving {{power is the}} measure of the ability to distinguish two peaks of slightly different m/z. The mass accuracy is the ratio of the m/z <b>measurement</b> <b>error</b> to the true m/z. Mass accuracy is usually measured in ppm or milli mass units. The mass range is the range of m/z amenable to analysis by a given analyzer. The linear dynamic range is the range over which ion signal is linear with analyte concentration. Speed refers to the time frame of the experiment and ultimately is used to determine the number of spectra per unit time that can be generated.|$|E
25|$|After {{the initial}} report of {{apparent}} superluminal velocities of neutrinos, most physicists {{in the field}} were quietly skeptical of the results, but prepared to adopt a wait-and-see approach. Experimental experts {{were aware of the}} complexity and difficulty of the measurement, so an extra unrecognized <b>measurement</b> <b>error</b> was still a real possibility, despite the care taken by the OPERA team. However, because of the widespread interest, several well-known experts did make public comments. Nobel laureates Steven Weinberg, George Smoot III, and Carlo Rubbia, and other physicists not affiliated with the experiment, including Michio Kaku, expressed skepticism about the accuracy of the experiment on the basis that the results challenged a long-held theory consistent with the results of many other tests of special relativity. Nevertheless, Ereditato, the OPERA spokesperson, stated that no one had an explanation that invalidated the experiment's results.|$|E
25|$|Subsequent to {{this the}} ICC {{received}} data from laboratory based analyses, {{on the basis that}} these measurement environments are more controlled, involving more sophisticated measurement technologies such as the Vicon Motion Analysis system. These were subject to less <b>measurement</b> <b>error.</b> Data was provided by the Australian Institute of Sport, the University of Western Australia and the Motion Analysis Corporation system from the University of Auckland. The ICC also carried out further video based three-dimensional analyses on all bowlers during the 2004 Champions Trophy in England. Regardless of the biomechanical measurement protocol used, a strikingly similar pattern emerged: the normal biomechanics of cricket bowling, whether it be spin or pace, features an element of elbow extension. The average extension of a normal, seemingly legal delivery was 8-10 degrees for all bowler types. There were virtually zero instances of no elbow extension at all in accordance with the original laws.|$|E
40|$|This paper {{examines}} {{the problem of}} systematic <b>measurement</b> <b>errors</b> in optical triangulation when some light sheets that illuminate the measured surface exhibit non-negligible bending (curvature). The problem is demonstrated experimentally by triangulation measurement of two reference bodies whose geometry reveals systematic <b>measurement</b> <b>errors</b> due to light sheet curvature. To correct these errors a triangulation model is developed which assumes parabolic light sheet shape and allows exact solution of system equations. Test measurements show that the model successfully compensates for systematic <b>measurement</b> <b>errors</b> originating from the curvature of light sheets...|$|R
40|$|The paper {{considers}} {{the effect of}} additive and multiplicative <b>measurement</b> <b>errors</b> on the estimation of linear models. We assume that such <b>measurement</b> <b>errors</b> have been applied to the micro data by purpose {{in order to protect}} them against re-identification. In particular <b>measurement</b> <b>errors</b> with a bimodal mixture distribution are analyzed. First the case of cross-section data is assumed. Then for panel data both the "naive' estimator ("within estimator", mixed effects estimator) and IV estimators are considered. In particular the effect of autocorrelation of regressors in short panels is discussed. bimodal mixture distribution...|$|R
40|$|Abstract. The {{measurements}} of redundant IMU {{are difficult to}} fuse, and traditional data fusion method can’t take each measurement into account. This method is an improving fusion method. A dodecahedron non-orthogonal redundant IMU configuration was selected as model. The correlation between <b>measurement</b> <b>errors</b> and fusion errors was derived and an effective calculation method for <b>measurement</b> <b>errors</b> was proposed. The method considered the differences between projection of fusion vector and measurements, and then made a conversion from projection <b>errors</b> to <b>measurement</b> <b>errors.</b> <b>Measurement</b> <b>errors</b> was calculated and used to generate an optimal weighted matrix in optimal weighted least square method. Result from optimal least square method is an optimal fusion vector whose errors are limited a minimum. Simulations also proved that the fusion result of this method is more accurate than the result of traditional method...|$|R
25|$|A later {{study from}} 2000-2003 showed that bowling actions that looked normal {{to the naked}} eye in many of the world's elite fast bowlers had, on average, 9 degrees of elbow {{extension}} during the bowling action. Some recorded elbow extension measuring between 10-15 degrees, yet none of these bowlers had ever had a problem regarding the legality of their bowling action. This testing showed that a zero tolerance threshold, and the tiered thresholds implemented in the late 1990s, had no or little scientific merit. The study, conducted by the Australian Institute of Sport Biomechanics department, led by cricket biomechanist Dr. Marc Portus, involved taking three-dimensional video based biomechanical analyses during tour, test and one-day international matches in Melbourne, Sydney and Brisbane. Results from this work indicated that video based <b>measurement</b> <b>error</b> in such a scenario, using best practice methodologies, was 3 degrees. This report was submitted to the ICC in 2003, which instigated the review of the illegal action definition and processes.|$|E
25|$|Composition is {{calculated}} from three primary sources: albedo, surface spectrum, and density. The last {{can only be}} determined accurately by observing the orbits of moons the asteroid might have. So far, every asteroid with moons {{has turned out to}} be a rubble pile, a loose conglomeration of rock and metal that may be half empty space by volume. The investigated asteroids are as large as 280km in diameter, and include 121 Hermione (268×186×183km), and 87 Sylvia (384×262×232km). Only half a dozen asteroids are larger than 87 Sylvia, though none of them have moons; however, some smaller asteroids are thought to be more massive, suggesting they may not have been disrupted, and indeed 511 Davida, the same size as Sylvia to within <b>measurement</b> <b>error,</b> is estimated to be two and a half times as massive, though this is highly uncertain. The fact that such large asteroids as Sylvia can be rubble piles, presumably due to disruptive impacts, has important consequences for the formation of the Solar System: Computer simulations of collisions involving solid bodies show them destroying each other as often as merging, but colliding rubble piles are more likely to merge. This means that the cores of the planets could have formed relatively quickly.|$|E
2500|$|The normal-ogive model {{derives from}} the {{assumption}} of normally distributed <b>measurement</b> <b>error</b> and is theoretically appealing on that basis. Here [...] is, again, the difficulty parameter. [...] The discrimination parameter is , the standard deviation of the <b>measurement</b> <b>error</b> for item i, and comparable to 1/.|$|E
40|$|In this paper, we {{estimate}} a transition model {{that allows for}} <b>measurement</b> <b>errors</b> in the data. The <b>measurement</b> <b>errors</b> arise because the survey design is partly retrospective, so that individuals sometimes forget or misclassify their past labor market transitions. The observed data are adjusted for errors via a measurement-error mechanism. The parameters {{of the distribution of}} the true data, and those of the measurement-error mechanism are estimated by a two-stage method. The results, based on the 1990 - 1992 French labor force survey, show that neglecting <b>measurement</b> <b>errors</b> leads to an underestimation of the average durations spent in labor market states. The estimates of some important transition probabilities between states are also biased by the <b>measurement</b> <b>errors.</b> © 1999 by the President and Fellows of Harvard College and the Massachusetts Institute of Technology...|$|R
40|$|This {{presentation}} addresses {{users of}} standard Vector Network Analyzers (VNA) • Many users believe their very flat S-parameter <b>measurements,</b> but <b>errors</b> {{are still there}} • Reflection <b>measurements</b> have larger <b>errors</b> • Only worst case estimations of S-parameter <b>measurement</b> <b>errors</b> H. Heuermann, A. Rumiantsev...|$|R
5000|$|... #Subtitle level 3: <b>Measurement</b> <b>errors</b> due to {{bottom-hole}} assembly ...|$|R
2500|$|In statistics, errors-in-variables models or <b>measurement</b> <b>error</b> models ...|$|E
2500|$|Since the <b>measurement</b> <b>error</b> vk is {{uncorrelated}} {{with the}} other terms, this becomes ...|$|E
2500|$|The {{standard}} deviation we obtain by sampling a distribution is itself not absolutely accurate, both for mathematical reasons (explained {{here by the}} confidence interval) and for practical reasons of measurement (<b>measurement</b> <b>error).</b> The mathematical effect can be described by the confidence interval or CI.|$|E
5000|$|Willem Egbert (Wim) Saris (born 8 July 1943) is a Dutch {{sociologist}} and Emeritus Professor of Statistics and Methodology especially {{known for}} his work on [...] "Causal modelling in nonexperimental research". and on <b>measurement</b> <b>errors</b> (MTMM analyses, development of the SQP program, and of procedures to correct for <b>measurement</b> <b>errors).</b>|$|R
25|$|<b>Measurement</b> <b>errors</b> in {{physical}} experiments are often modeled by a normal distribution. This {{use of a}} normal distribution {{does not imply that}} one is assuming the <b>measurement</b> <b>errors</b> are normally distributed, rather using the normal distribution produces the most conservative predictions possible given only knowledge about the mean and variance of the errors.|$|R
40|$|I {{develop a}} novel {{diagnostic}} procedure {{to estimate the}} associations between <b>measurement</b> <b>errors</b> of expected returns proxies and …rm characteristics. Application to GLS, a popular implementation of the implied cost of capital ("ICC"), yields the …rst direct empirical evidence that ICC <b>measurement</b> <b>errors</b> i) are persistent, ii) {{can be associated with}} …rms’risk or growth characteristics, and therefore iii) can lead to spurious inferences in regressions. I devise a novel methodology to account for the in‡uence of ICCs <b>measurement</b> <b>errors</b> in regression settings, and show that its application i) can explain some puzzling associations between GLS and …rm characteristics and ii) can improve upon GLS, by forming new ICCs that better sort realized returns. Together, the innovations of this paper allow researchers to better understand ICC <b>measurement</b> <b>errors</b> and provide a robust empirical strategy for future research...|$|R
