4443|298|Public
5|$|Computer {{architectures}} {{in which}} each element of <b>main</b> <b>memory</b> can be accessed with equal latency and bandwidth are known as uniform memory access (UMA) systems. Typically, {{that can be achieved}} only by a shared memory system, in which the memory is not physically distributed. A system that does not have this property is known as a non-uniform memory access (NUMA) architecture. Distributed memory systems have non-uniform memory access.|$|E
5|$|On December 26, 2006, {{a person}} using the alias muslix64 {{published}} a utility named BackupHDDVD and its source code on the DVD decryption forum at the website Doom9. BackupHDDVD {{can be used}} to decrypt AACS protected content once one knows the encryption key. muslix64 claimed to have found title and volume keys in <b>main</b> <b>memory</b> while playing HD DVDs using a software player, and that finding them is not difficult.|$|E
5|$|Thirty-two such modules {{were then}} stacked and wired {{together}} with a mass of twisted-pair wires into a single processor. The basic cycle time was 2.11ns, or 474MHz, allowing each processor to reach about 0.948 GFLOPS, and a 16 processor machine a theoretical 15.17 GFLOP. Key to the high performance was the high-speed access to <b>main</b> <b>memory,</b> which allowed each process to burst up to 8 GB/s.|$|E
25|$|An equally {{important}} {{reason was that}} <b>main</b> <b>memories</b> were quite slow (a common type was ferrite core memory); by using dense information packing, one could reduce {{the frequency with which}} the CPU had to access this slow resource. Modern computers face similar limiting factors: <b>main</b> <b>memories</b> are slow compared to the CPU and the fast cache memories employed to overcome this are limited in size. This may partly explain why highly encoded instruction sets have proven to be as useful as RISC designs in modern computers.|$|R
5000|$|Besides local GDDR3 <b>memory,</b> <b>main</b> XDR <b>memory</b> can be {{accessed}} by RSX too, which is limited to either: ...|$|R
40|$|Abstract—Hybrid <b>main</b> <b>memories</b> {{composed}} of DRAM as a cache to scalable non-volatile memories such as phase-change memory (PCM) can provide much larger storage capacity than traditional <b>main</b> <b>memories.</b> A key challenge for enabling high-performance and scalable hybrid memories, though, is efficiently managing the metadata (e. g., tags) for data cached in DRAM at a fine granularity. Based on {{the observation that}} storing metadata off-chip in the same row as their data exploits DRAM row buffer locality, this paper reduces the overhead of fine-granularity DRAM caches by only caching the metadata for recently accessed rows on-chip using a small buffer. Leveraging the flexibility and efficiency of such a fine-granularity DRAM cache, we also develop an adaptive policy to choose the best granularity when migrating data into DRAM. On a hybrid memory with a 512 MB DRAM cache, our proposal using an 8 KB on-chip buffer can achieve within 6 % of the performance of, and 18 % better energy efficiency than, a conventional 8 MB SRAM metadata store, even when the energy overhead due to large SRAM metadata storage is not considered. Index Terms—Cache memories, tag storage, non-volatile <b>memories,</b> hybrid <b>main</b> <b>memories.</b> ...|$|R
5|$|CAPS {{entreprise}} and Pathscale {{are also}} coordinating {{their effort to}} make hybrid multi-core parallel programming (HMPP) directives an open standard called OpenHMPP. The OpenHMPP directive-based programming model offers a syntax to efficiently offload computations on hardware accelerators and to optimize data movement to/from the hardware memory. OpenHMPP directives describe remote procedure call (RPC) on an accelerator device (e.g. GPU) or more generally a set of cores. The directives annotate C or Fortran codes to describe two sets of functionalities: the offloading of procedures (denoted codelets) onto a remote device and the optimization of data transfers between the CPU <b>main</b> <b>memory</b> and the accelerator memory.|$|E
5|$|<b>Main</b> <b>memory</b> in a {{parallel}} computer is either shared memory (shared between all processing elements in a single address space), or distributed memory (in which each processing element has its own local address space). Distributed memory refers {{to the fact that}} the memory is logically distributed, but often implies that it is physically distributed as well. Distributed shared memory and memory virtualization combine the two approaches, where the processing element has its own local memory and access to the memory on non-local processors. Accesses to local memory are typically faster than accesses to non-local memory.|$|E
5|$|The {{original}} Mac Pro's <b>main</b> <b>memory</b> uses 667MHz DDR2 ECC FB-DIMMs; {{the early}} 2008 model uses 800MHz ECC DDR2 FB-DIMMS, the 2009 and onward Mac Pro use 1066MHz DDR3 ECC DIMMs {{for the standard}} models, and 1333MHz DDR3 ECC DIMMs for systems configured with 2.66GHz or faster CPUs. In the original and 2008 models, these modules are installed in pairs, one each on two riser cards. The cards have 4 DIMM slots each, allowing a total of 32GB of memory (8 × 4GB) to be installed. Notably, due to its FB-DIMM architecture, installing more RAM in the Mac Pro will improve its memory bandwidth, but may also increase its memory latency. With a simple installation of a single FB-DIMM, the peak bandwidth is 8GB/s, but this can increase to 16GB/s by installing two FB-DIMMs, one {{on each of the}} two buses, which is the default configuration from Apple. While electrically the FB-DIMMs are standard, for pre-2009 Mac Pro models Apple specifies larger-than-normal heatsinks on the memory modules. Problems have been reported by users who have used third party RAM with normal size FB-DIMM heatsinks. (see notes below). 2009 and later Mac Pro computers do not require memory modules with heatsinks.|$|E
40|$|With recent {{technological}} advances, {{shared memory}} parallel machines {{have become more}} scalable, and oer large <b>main</b> <b>memories</b> and high bus bandwidths. They are emerging as good platforms for data warehousing and data mining. In this paper, we focus on shared memory parallelization of data mining algorithms...|$|R
50|$|Integrated {{circuits}} such as 1K-bit RAMs, calculator chips, and {{the first}} microprocessors, that began to be manufactured in moderate quantities in the early 1970s, had under 4,000 transistors. True LSI circuits, approaching 10,000 transistors, began to be produced around 1974, for computer <b>main</b> <b>memories</b> and second-generation microprocessors.|$|R
40|$|This {{paper is}} a work in {{progress}} study of the operating system services required to manage on-chip memories. We are evaluating different CMP on-chip memories configurations. Chip-MultiProcessors (CMP) architectures integrating multiple computing and memory elements presents different problems (coherency, latency, [...] .) that must be solved. On-chip local memories are directly addressable and their latency is much shorter than off-chip <b>main</b> <b>memories.</b> Since memory latency is a key factor for application performance, we study how the OS can help. Postprint (author’s final draft...|$|R
25|$|The Kindle 2 {{features}} a Freescale 532MHz, ARM-11 90nm processor, 32MB <b>main</b> <b>memory,</b> 2GB flash memory and a 3.7V 1,530mAh lithium polymer battery.|$|E
25|$|The PCI {{standard}} is discouraging {{the use of}} I/O space in new devices, preferring {{that as much as}} possible be done through <b>main</b> <b>memory</b> mapping.|$|E
25|$|Tries can be slower in {{some cases}} than hash tables for looking up data, {{especially}} if the data is directly accessed on a hard disk drive or some other secondary storage device where the random-access time is high compared to <b>main</b> <b>memory.</b>|$|E
5000|$|<b>Main</b> {{physical}} <b>memory</b> (often dynamic RAM) - this operates somewhat {{slower than}} the CPU.|$|R
40|$|This paper {{describes}} a join algorithm suitable for deductive and relational databases which are accessed by computers with large <b>main</b> <b>memories.</b> Using multi-key hashing and appropriate buffering, joins {{can be performed}} on very large relations more eficiently than with existing methods. Furthermore, this algorithm jirs naturally into top-down Prolog computations and can be made very jlexible by incorporating additional Prolog features...|$|R
25|$|As of 2012, {{there are}} {{attempts}} to use flash <b>memory</b> as the <b>main</b> computer <b>memory,</b> DRAM.|$|R
25|$|A bootloader, {{for example}} GNU GRUB, LILO, SYSLINUX, or Gummiboot. This {{is a program}} that loads the Linux kernel into the computer's <b>main</b> <b>memory,</b> by being {{executed}} by the computer when it is turned on and after the firmware initialization is performed.|$|E
25|$|The BBC B+ and {{the later}} Master {{provided}} 'shadow modes', where the 1–20KB frame buffer was stored in an alternative RAM bank, freeing the <b>main</b> <b>memory</b> for user programs. This feature was requested by setting bit 7 of the mode variable, i.e. by requesting modes 128–135.|$|E
25|$|For 64-bit PowerPC processors, {{the virtual}} address {{resides in the}} {{rightmost}} 64 bits of a pointer while it was 48 bits in the S/38 and CISC AS/400. The 64-bit address space references <b>main</b> <b>memory</b> and disk as a single address set which is the single-level storage concept.|$|E
25|$|An {{important}} force encouraging complexity {{was very}} limited <b>main</b> <b>memories</b> (on {{the order of}} kilobytes). It was therefore advantageous for the code density—the density of information held in computer programs—to be high, leading to features such as highly encoded, variable length instructions, doing data loading as well as calculation (as mentioned above). These issues were of higher priority than the ease of decoding such instructions.|$|R
40|$|Virtual {{memory was}} {{conceived}} {{as a way to}} automate overlaying of program segments. Modern computers have very large <b>main</b> <b>memories,</b> but need automatic solutions to the relocation and protection problems. Virtual memory serves this need as well and is thus useful in computers of all sizes. The history of the idea is traced, showing how it has become a widespread, little noticed feature of computers today...|$|R
40|$|Proposed {{scheme for}} {{buffering}} data on intensities of picture elements (pixels) of image increases rate or processing beyond that attainable when data read, one pixel at time, from <b>main</b> image <b>memory.</b> Scheme applied in design of specialized image-processing circuitry. Intended to optimize performance of processor in which electronic equivalent of address-lookup table used {{to address those}} pixels in <b>main</b> image <b>memory</b> required for processing...|$|R
25|$|The {{system allows}} for the GPU to {{directly}} read data produced by the CPU without going to <b>main</b> <b>memory.</b> In this specific case of data streaming, called Xbox procedural synthesis (XPS), the CPU is effectively a data decompressor, generating geometry on-the-fly for consumption by the GPU 3D core.|$|E
25|$|By the mid-1960s, binary {{addressing}} {{had become the}} standard architecture in most computer designs, and <b>main</b> <b>memory</b> sizes were most commonly powers of two. This is the most natural configuration for memory, as all combinations of their address lines map to a valid address, allowing easy aggregation into a larger block of memory with contiguous addresses.|$|E
25|$|OS/360 also {{pioneered the}} concept that the {{operating}} system keeps track {{of all of the}} system resources that are used, including program and data space allocation in <b>main</b> <b>memory</b> and file space in secondary storage, and file locking during update. When the process is terminated for any reason, all of these resources are re-claimed by the operating system.|$|E
50|$|An {{important}} force encouraging complexity {{was very}} limited <b>main</b> <b>memories</b> (on {{the order of}} kilobytes). It was therefore advantageous for the code density—the density of information held in computer programs—to be high, leading to features such as highly encoded, variable length instructions, doing data loading as well as calculation (as mentioned above). These issues were of higher priority than the ease of decoding such instructions.|$|R
5000|$|Drum memory a {{magnetic}} data storage device {{used as the}} <b>main</b> working <b>memory</b> in many early computers ...|$|R
50|$|For {{economic}} reasons, {{the large}} (<b>main)</b> <b>memories</b> found in personal computers, workstations, and non-handheld game-consoles (such as PlayStation and Xbox) normally consist of dynamic RAM (DRAM). Other {{parts of the}} computer, such as cache memories and data buffers in hard disks, normally use static RAM (SRAM). However, since SRAM has high leakage power and low density, die-stacked DRAM has recently been used for designing multi-megabyte sized processor caches.|$|R
25|$|Big FFTs: With the {{explosion}} of big data in fields such as astronomy, the need for 512k FFTs has arisen for certain interferometry calculations. The data collected by projects such as MAP and LIGO require FFTs of {{tens of billions of}} points. As this size does not fit into <b>main</b> <b>memory,</b> so called out-of-core FFTs are an active area of research.|$|E
25|$|When I/O {{transfer}} is complete or an error is detected, the controller communicates with the CPU through the channel using an interrupt. Since the channel has {{direct access to}} the <b>main</b> <b>memory,</b> it is also often referred to as DMA controller (where DMA stands for direct memory access), although that term is looser in definition and is often applied to non-programmable devices as well.|$|E
25|$|Divide-and-conquer {{algorithms}} naturally {{tend to make}} {{efficient use}} of memory caches. The reason is that once a sub-problem is small enough, it and all its sub-problems can, in principle, be solved within the cache, without accessing the slower <b>main</b> <b>memory.</b> An algorithm designed to exploit the cache {{in this way is}} called cache-oblivious, because it does not contain the cache size as an explicit parameter.|$|E
40|$|A new {{parallel}} search algorithm {{running on a}} large computer cluster solves a popular board game by efficiently computing the best moves from all reachable positions. As such, the algorithm uses the <b>main</b> <b>memories</b> for frequently and randomly accessed data and stores terbytes of less frequently acessed intermediate results on disks. All processors repeatably inform each other about positions' intermediate values, generating more than a petabit of interprocessor communication as well as terabytes of disk I/O...|$|R
5000|$|... #Caption: [...] "Figurative {{system of}} human knowledge", the {{structure}} that the Encyclopédie organised knowledge into. It had three <b>main</b> branches: <b>memory,</b> reason, and imagination ...|$|R
40|$|We {{explore the}} {{possibilities}} to organize a query data structure in the <b>main</b> <b>memories</b> or hard disks of a cluster computer. The query data structure serves to improve {{the performance of a}} parallel algorithm for the computation of the structure of a graph induced by a random function. Tradeoffs between different organizations using main mem-ory or hard disks are developed and quantified with param-eters. Thus, for concrete cluster systems with concrete pa-rameter values, the best organization can be selected. 1...|$|R
