2069|3540|Public
25|$|A 1 to 80 scale section {{model of}} the deck in the {{erection}} stage, and a 1 to 400 scale full aeroelastic {{model of the}} entire bridge were constructed. It is a <b>Monte-Carlo</b> <b>simulation</b> of the typhoon wind climate. The full model was tested in different stages of construction in turbulent boundary layer flow, complete with the local topography to model the wind conditions at the site. The model tests identified critical stages of erection that allowed the construction schedule of the bridge to be tailored to avoid the typhoon season. The comparison of model test results and the full scale monitoring will assist engineers {{to better understand the}} behaviour of long span bridges in wind and to improve current design methods.|$|E
2500|$|More fundamentally, {{investors are}} stuck with {{estimating}} key parameters from past market data because MPT attempts to model risk {{in terms of}} the likelihood of losses, but says nothing about why those losses might occur. The risk measurements used are probabilistic in nature, not structural. [...] This is a major difference as compared to many engineering approaches to risk management. As estimation errors are critical in MPT, appropriate modeling approaches must be applied. In an MPT or mean-variance optimization framework, accurate estimation of the variance–covariance matrix is paramount. [...] Thus, forecasting with <b>Monte-Carlo</b> <b>simulation</b> with the Gaussian copula and well-specified marginal distributions are effective. [...] Allowing the modeling process to allow for empirical characteristics in stock returns such as auto-regression, asymmetric volatility, skewness, and kurtosis is important. [...] Not accounting for these attributes can lead to severe estimation error in the correlation and variance–covariance matrices that have negative biases (as much as 70% of the true values).|$|E
5000|$|... #Subtitle level 2: <b>Monte-Carlo</b> <b>simulation,</b> {{trees and}} lattices ...|$|E
5000|$|Industrial Engineering: {{discrete}} {{event and}} <b>Monte-Carlo</b> <b>simulations</b> (for logistics and manufacturing systems for example), queueing networks, mathematical optimization ...|$|R
40|$|The {{design of}} Quantum Turbo Codes (QTCs) {{typically}} {{relies on the}} analysis of their distance spectra, followed by <b>Monte-Carlo</b> <b>simulations.</b> By contrast, {{in this paper we}} appropriately adapt the conventional non-binary EXtrinsic Information Transfer (EXIT) charts for quantum turbo codes by exploiting the intrinsic quantum-to-classical isomorphism. The EXIT chart analysis not only allows us to dispense with the time-consuming <b>Monte-Carlo</b> <b>simulations,</b> but also facilitates the design of near-capacity codes without resorting to the analysis of their distance spectra. We have demonstrated that our EXIT chart predictions are in line with the <b>Monte-Carlo</b> <b>simulations</b> results. We have also optimized the entanglement-assisted QTC using EXIT charts, which outperforms the existing distance spectra based QTCs. More explicitly, the performance of our optimized QTC is as close as 0. 3 dB to the corresponding hashing bound. Comment: 10 page...|$|R
3000|$|... can be {{beneficial}} but not mandatory. These facts are also supported {{with the help}} of <b>Monte-Carlo</b> <b>simulations</b> presented in the next section.|$|R
50|$|The method may be {{implemented}} using <b>Monte-Carlo</b> <b>simulation,</b> or in a simplified, approximate form (the DM range option).|$|E
5000|$|Rubinstein R.Y. and D.P. Kroese, [...] "The Cross-Entropy Method: a Unified Approach to Combinatorial Optimization, <b>Monte-Carlo</b> <b>Simulation</b> and Machine Learning", Springer, 2004.|$|E
50|$|Monte-Carlo is {{a method}} to pseudo-randomly sample a large space of variables.The {{importance}} sampling technique used to select the gauge configurations in the <b>Monte-Carlo</b> <b>simulation</b> imposes the use of Euclidean time, by a Wick rotation of spacetime.|$|E
30|$|Using the {{expressions}} obtained in (7), {{we are able}} to calculate (8) using a numerical Gaussian quadrature instead of <b>Monte-Carlo</b> <b>simulations</b> as shown in [2].|$|R
50|$|Historically, {{basic methods}} of {{pseudo-random}} number sampling were developed for <b>Monte-Carlo</b> <b>simulations</b> in the Manhattan project; {{they were first}} published by John von Neumann in the early 1950s.|$|R
5000|$|It {{is known}} that {{from the work of}} Bhatia et al. thatThe value of this {{constant}} was estimated using <b>Monte-Carlo</b> <b>simulations</b> by Mustonen and Rajesh to be [...]|$|R
5000|$|Full {{calculation}} of CVA is done via <b>Monte-Carlo</b> <b>simulation</b> of all risk factors {{which is very}} computationally demanding.There exists a simple approximation for CVA which consists in buying just one default protection (Credit Default Swap) for amount of NPV of netted set of derivatives for each counterparty.http://www.pricederivatives.com/en/simple-cva-calculation-example-credit-valuation-adjustment-excel/ ...|$|E
50|$|In a {{mean-variance}} optimization framework, accurate {{estimation of}} the variance-covariance matrix is paramount. Quantitative techniques that use <b>Monte-Carlo</b> <b>simulation</b> with the Gaussian copula and well-specified marginal distributions are effective. Allowing the modeling process to allow for empirical characteristics in stock returns such as auto-regression, asymmetric volatility, skewness, and kurtosis is important. Not accounting for these attributes lead to severe estimation error in the correlations, variances and covariances that have negative biases (as much as 70% of the true values). Other optimization strategies that focus on minimizing tail-risk (e.g., value at risk, conditional value at risk) in investment portfolios are popular amongst risk averse investors. To minimize exposure to tail risk, forecasts of asset returns using <b>Monte-Carlo</b> <b>simulation</b> with vine copulas to allow for lower (left) tail dependence (e.g., Clayton, Rotated Gumbel) across large portfolios of assets are most suitable.|$|E
50|$|Neutron-acceptance diagram shading (NADS) is a beam {{simulation}} technique. Unlike <b>Monte-Carlo</b> <b>simulation</b> codes like McStas, NADS {{does not}} trace individual neutrons but traces linearly-related bunches in a reduced-dimensionality phase space. Bunches are subdivided where necessary to follow accurately a simplified surface reflectivity model. This makes jnads results equivalent to Monte-Carlo simulations but about 5 {{orders of magnitude}} faster for difficult modelling tasks.|$|E
40|$|International audienceRipley’s K {{function}} is the classical tool {{to characterize the}} spatial structure of point patterns. It is widely used in vegetation studies. Testing its values against a null hypothesis usually relies on <b>Monte-Carlo</b> <b>simulations</b> since {{little is known about}} its distribution. We introduce a statistical test against complete spatial randomness (CSR). The test returns the p-value to reject the null hypothesis of independence between point locations. It is more rigorous and faster than classical <b>Monte-Carlo</b> <b>simulations.</b> We show how to apply it to a tropical forest plot. The necessary R code is provided...|$|R
40|$|International audienceIn {{order to}} assess {{investments}} plans, economic indicators need to be quantified. These indicators describe the expected gain {{as well as the}} economic risks. <b>Monte-Carlo</b> <b>simulations</b> are often used in this context. However, they require a large computational time to obtain accurate results. As our goal is to find an optimal strategy, <b>Monte-Carlo</b> <b>simulations</b> are not appropriate. Indeed, the Monte-Carlo method would require a too long computational time within an optimization algorithm. Here we propose to use quasi Monte-Carlo methods as an alternative, which provide accurate results more quickly than the Monte-Carlo method...|$|R
40|$|We analyze by <b>Monte-Carlo</b> <b>simulations</b> and {{analytically}} {{spin dynamics}} of {{two-dimensional electron gas}} (2 DEG) interacting with short-range scatterers in nonquantizing magnetic fields. It is shown that the spin dynamics is non-Markovian with the exponential spin relaxation followed by the oscillating tail due to the electrons residing on the closed trajectories. The tail relaxes on a long time scale due to an additional smooth random potential and inelastic processes. The developed analytical theory and <b>Monte-Carlo</b> <b>simulations</b> are in the quantitative agreement with each other. Comment: 6 pages, 3 figure...|$|R
5000|$|... #Caption: Simplified <b>Monte-Carlo</b> <b>simulation</b> of the photo-Dember {{effect in}} semiconductors. Electrons {{are assumed to}} have a {{mobility}} 3 times larger than holes (for visualisation purposes). It can be observed how electrons diffuse away from the surface faster than holes shifting the [...] "centre of negative charge" [...] deeper into the semiconductor while the holes ("centre of positive charge") remain closer to the surface, thus forming a dipole.|$|E
50|$|Operator overloading, {{for both}} forward and reverse accumulation, can be well-suited to {{applications}} where the objects are vectors of real numbers rather than scalars. This {{is because the}} tape then comprises vector operations; this can facilitate computationally efficient implementations where each vector operation performs many scalar operations. Vector adjoint algorithmic differentiation (vector AAD) techniques may be used, for example, to differentiate values calculated by <b>Monte-Carlo</b> <b>simulation.</b>|$|E
5000|$|... #Caption: Figure 9. Ion-ion {{recombination}} reaction constants (α) for [...] + [...] + Ne → [...] + Ne as {{a function}} of the temperature (T) at prssure P = 294.2 kPa shown on the figure as a continuous line; results were obtained from Flannery's equation by Christov et al. (∆) is the result obtained by <b>Monte-Carlo</b> <b>simulation.</b> Work was carried out by Bardsley et al. (○).|$|E
40|$|Ripley’s 8353 e {{function}} is the classical tool {{to characterize the}} spatial structure of point patterns. It is widely used in vegetation studies. Testing its values against a null hypothesis usually relies on <b>Monte-Carlo</b> <b>simulations</b> since {{little is known about}} its distribution. We introduce a statistical test against complete spatial randomness (CSR). The test returns the 83543 value to reject the null hypothesis of independence between point locations. It is more rigorous and faster than classical <b>Monte-Carlo</b> <b>simulations.</b> We show how to apply it to a tropical forest plot. The necessary R code is provided...|$|R
40|$|International audienceIn this paper, {{we propose}} a {{multiscale}} coupling approach to perform <b>Monte-Carlo</b> <b>simulations</b> on systems described at the atomic scale {{and subjected to}} random phenomena. The method {{is based on the}} Arlequin framework, developed to date for deterministic models involving coupling a region of interest described at a particle scale with a coarser model (continuum model). The new method can result in a dramatic {{reduction in the number of}} degrees of freedom necessary to perform <b>Monte-Carlo</b> <b>simulations</b> on the fully atomistic structure. The focus here is on the construction of an equivalent stochastic continuum model and its coupling with a discrete particle model through a stochastic version of the Arlequin method. Concepts from the Stochastic Finite Element Method, such as the Karhünen–Loeve expansion and Polynomial Chaos, are extended to multiscale problems so that <b>Monte-Carlo</b> <b>simulations</b> are only performed locally in subregions of the domain occupied by particles. Preliminary results are given for a 1 D structure with harmonic interatomic potentials...|$|R
40|$|FPGAs {{have been}} {{successfully}} used to accelerate many computationally bound applications, such as highperformance <b>Monte-Carlo</b> <b>simulations,</b> but the amount of programmer effort required in development, testing, and tuning is also very high, requiring a new custom design for each application. This paper presents Contessa, a pure-functional continuation-based language for describing path-based <b>Monte-Carlo</b> <b>simulations,</b> and a completely automated method for turning platform-independent Contessa programs into high-performance hardware implementations. Our approach exploits the large degree of thread-based parallelism available in <b>Monte-Carlo</b> <b>simulations,</b> allowing data-dependent control-flow and loopcarried dependencies to be expressed, while retaining highperformance. The Contessa toolchain is evaluated using five different simulation kernels, in comparison to both software and manually described hardware. When compared to an existing FPGA implementation, Contessa requires {{a quarter of the}} Handel-C source-code length, and doubles the clock rate to over 300 MHz while requiring a similar number of resources, and also provides a 35 times speedup over a C++ implementation using an Opteron 2. 2 GHz. ...|$|R
50|$|However, valuing vanilla {{instruments}} such as {{caps and}} swaptions is useful primarily for calibration. The real {{use of the}} model is to value somewhat more exotic derivatives such as bermudan swaptions on a lattice, or other derivatives in a multi-currency context such as Quanto Constant Maturity Swaps, as explained for example in Brigo and Mercurio (2001). The efficient and exact <b>Monte-Carlo</b> <b>simulation</b> of the Hull-White model with time dependent parameters can be easily performed, see Ostrovski (2013) and (2016).|$|E
50|$|The raw {{speed of}} NADS {{makes it a}} {{particularly}} attractive tool for beam modelling where evolutionary algorithms are used. Tests on the C++ prototype engine could calculate the on-sample flux of a SANS instrument in 55 milliseconds on a single 2 GHz intel core 2 core. The java release (jnads) performs the same calculation in 0.8 seconds on the same hardware. A <b>Monte-Carlo</b> <b>simulation</b> of the same instrument would take 25 hours to complete with 1% statistical errors.|$|E
5000|$|In a {{mean-variance}} optimization framework, accurate {{estimation of}} the variance-covariance matrix is paramount. Thus, forecasting with <b>Monte-Carlo</b> <b>simulation</b> with the Gaussian copula and well-specified marginal distributions are effective. [...] Allowing the modeling process to allow for empirical characteristics in stock returns such as auto-regression, asymmetric volatility, skewness, and kurtosis is important. Not accounting for these attributes lead to severe estimation error in the correlations and variances that have negative biases (as much as 70% of the true values).|$|E
40|$|The way {{computer}} programs play strategy games {{is quite different}} from the way humans play. In perfect-information games like chess and checkers, a game-tree search is the core technique in a computer program’s arsenal, augmented by good evaluation functions and clever secondary strategies. In other perfect-information games such as go and clobber, there is very little intuition as to how good a position is, and consequently constructing a good evaluation function is not easy. Furthermore, go has a high branching factor. It turns out that <b>Monte-Carlo</b> <b>simulations,</b> i. e. producing repeated random samples and considering their average in making a decision, work surprisingly well in these games. In imperfect-information games such as bridge and scrabble (the latter game has inherent randomness associated with it as well), <b>Monte-Carlo</b> <b>simulations</b> once again turn out to be useful. This paper examines the use of <b>Monte-Carlo</b> <b>simulations</b> in bridge, scrabble, go, clobber, and backgammon, and reports on how this technique impacts each of these games. 1...|$|R
50|$|The {{limiting}} {{distribution of}} this test statistic is a weighted sum of chi-squared random variables, however in practice {{it is more}} convenient to compute the sample quantiles using the <b>Monte-Carlo</b> <b>simulations.</b>|$|R
40|$|We use replica {{exchange}} <b>Monte-Carlo</b> <b>simulations</b> {{to measure}} the equilibrium equation of state of the disordered fluid state for a binary hard sphere mixture up to very large densities where standard <b>Monte-Carlo</b> <b>simulations</b> do not easily reach thermal equilibrium. For the moderate system sizes we use (up to N= 100), we find {{no sign of a}} pressure discontinuity near the location of dynamic glass singularities extrapolated using either algebraic or simple exponential divergences, suggesting they do not correspond to genuine thermodynamic glass transitions. Several scenarios are proposed for the fate of the fluid state in the thermodynamic limit. Comment: 10 pages, 8 fig...|$|R
50|$|The {{interpretation}} of the divergence is that the field fluctuations cannot stay centered around a mean. If you start {{at a point where}} the field has the value 1, the divergence tells you that as you travel far away, the field is arbitrarily far from the starting value. This makes a two dimensional massless scalar field slightly tricky to define mathematically. If you define the field by a <b>Monte-Carlo</b> <b>simulation,</b> it doesn't stay put, it slides to infinitely large values with time.|$|E
5000|$|The FAST method {{originated}} in study of coupled chemical reaction systems in 1973 and the {{detailed analysis of}} the computational error was presented latter in 1975. Only the first order sensitivity indices referring to “main effect” were calculated in the original method. A FORTRAN computer program capable of analyzing either algebraic or differential equation systems was published in 1982. In 1990s, the relationship between FAST sensitivity indices and Sobol’s ones calculated from <b>Monte-Carlo</b> <b>simulation</b> was revealed in the general framework of ANOVA-like decomposition [...] and an extended FAST method able to calculate sensitivity indices referring to “total effect” was developed.|$|E
50|$|A 1 to 80 scale section {{model of}} the deck in the {{erection}} stage, and a 1 to 400 scale full aeroelastic {{model of the}} entire bridge were constructed. It is a <b>Monte-Carlo</b> <b>simulation</b> of the typhoon wind climate. The full model was tested in different stages of construction in turbulent boundary layer flow, complete with the local topography to model the wind conditions at the site. The model tests identified critical stages of erection that allowed the construction schedule of the bridge to be tailored to avoid the typhoon season. The comparison of model test results and the full scale monitoring will assist engineers {{to better understand the}} behaviour of long span bridges in wind and to improve current design methods.|$|E
40|$|In {{this work}} we address {{the nature of}} broken {{ergodicity}} in the low temperature phase of Ising spin glasses by examining spectral properties of spin correlation functions C_ij≡. We argue {{that more than one}} extensive (i. e., O(N)) eigenvalue in this matrix signals replica symmetry breaking. <b>Monte-Carlo</b> <b>simulations</b> of the infinite-range Ising spin-glass model, above and below the Almeida-Thouless line, support this conclusion. Exchange <b>Monte-Carlo</b> <b>simulations</b> for the short-range model in four dimensions find a single extensive eigenvalue and a large subdominant eigenvalue consistent with droplet model expectations. Comment: 4 pages, 3 figures, accepted for publication in Phys. Rev. Let...|$|R
40|$|Beam dump {{experiments}} {{have been used}} to search for new particles with null results interpreted in terms of limits on masses m_ϕ and coupling constants ϵ. However these limits have been obtained by using approximations [including the Weizsäcker-Williams (WW) approximation] or <b>Monte-Carlo</b> <b>simulations.</b> We display methods, using a new scalar boson as an example, to obtain the cross section and the resulting particle production numbers without using approximations or <b>Monte-Carlo</b> <b>simulations.</b> We show that the approximations cannot be used to obtain accurate values of cross sections. The corresponding exclusion plots differ by substantial amounts when seen on a linear scale. In the event of a discovery, we generate pseudodata (assuming given values of m_ϕ and ϵ) in the currently allowed regions of parameter space. The use of approximations to analyze the pseudodata for the future experiments is shown to lead to considerable errors in determining the parameters. Furthermore, a new region of parameter space can be explored without using one of the common approximations, m_ϕ≫ m_e. Our method {{can be used as a}} consistency check for <b>Monte-Carlo</b> <b>simulations.</b> Comment: 21 pages, 8 figure...|$|R
30|$|In this section, {{we propose}} two {{applications}} of the obtained analytical expressions. For instance, we derive analytical and numerically tractable (compared to exhaustive <b>Monte-Carlo</b> <b>simulations)</b> expressions, for both the constrained capacity and the coded BER.|$|R
