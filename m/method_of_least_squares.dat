584|10000|Public
5|$|The {{experimental}} determination of pKa values is commonly performed {{by means of}} titrations, in a medium of high ionic strength and at constant temperature. A typical procedure would be as follows. A solution of the compound in the medium is acidified with a strong acid {{to the point where}} the compound is fully protonated. The solution is then titrated with a strong base until all the protons have been removed. At each point in the titration pH is measured using a glass electrode and a pH meter. The equilibrium constants are found by fitting calculated pH values to the observed values, using the <b>method</b> <b>of</b> <b>least</b> <b>squares.</b>|$|E
25|$|The <b>method</b> <b>of</b> <b>least</b> <b>squares</b> {{is often}} used to {{generate}} estimators and other statistics in regression analysis.|$|E
25|$|The first {{clear and}} concise {{exposition}} of the <b>method</b> <b>of</b> <b>least</b> <b>squares</b> {{was published by}} Legendre in 1805. The technique is described as an algebraic procedure for fitting linear equations to data and Legendre demonstrates the new method by analyzing the same data as Laplace for {{the shape of the}} earth. The value of Legendre's <b>method</b> <b>of</b> <b>least</b> <b>squares</b> was immediately recognized by leading astronomers and geodesists of the time.|$|E
40|$|Aspects {{of neutral}} network {{construction}} and its learning for increasing accuracy of gas concentration reduction by measuring data of CO 2 -laser trass gas analyzer have been considered. Accuracy of reducing atmospheric gas (Н 2 О, СО 2 и О 3) concentration by neural network method {{in comparison with}} traditionally used <b>method</b> <b>of</b> <b>least</b> <b>square</b> is give...|$|R
5000|$|Important {{papers of}} his include On series {{expansions}} {{determined by the}} <b>methods</b> <b>of</b> <b>least</b> <b>squares,</b> and Investigations <b>of</b> the number of primes less than a given number. The mathematical method that bears his name, the Gram-Schmidt process, {{was first published in}} the former paper, in 1883. [...] Gram's theorem and the Gramian matrix are also named after him.|$|R
40|$|This paper {{presents}} {{a new approach}} for the optimization of GARCH parameters estimation. Firstly, we propose a method for the localization of the maximum. Thereafter, using the <b>methods</b> <b>of</b> <b>least</b> <b>squares,</b> we make a local approximation for the projection of the likelihood function curve on two dimensional planes by a polynomial of order two which {{will be used to}} calculate an estimation of the maximum. Comment: 10 pages, 4 tables, 3 figure...|$|R
25|$|When the {{observations}} {{come from an}} exponential family and mild conditions are satisfied, least-squares estimates and maximum-likelihood estimates are identical. The <b>method</b> <b>of</b> <b>least</b> <b>squares</b> can also be derived {{as a method of}} moments estimator.|$|E
25|$|The fourth {{chapter of}} this {{treatise}} includes an exposition of the <b>method</b> <b>of</b> <b>least</b> <b>squares,</b> a remarkable testimony to Laplace's command over {{the processes of}} analysis. In 1805 Legendre had published the <b>method</b> <b>of</b> <b>least</b> <b>squares,</b> making no attempt to tie it {{to the theory of}} probability. In 1809 Gauss had derived the normal distribution from the principle that the arithmetic mean of observations gives the most probable value for the quantity measured; then, turning this argument back upon itself, he showed that, if the errors of observation are normally distributed, the least squares estimates give the most probable values for the coefficients in regression situations. These two works seem to have spurred Laplace to complete work toward a treatise on probability he had contemplated as early as 1783.|$|E
25|$|In 1809 Carl Friedrich Gauss {{published}} his method of calculating {{the orbits of}} celestial bodies. In that work {{he claimed to have}} been in possession of the <b>method</b> <b>of</b> <b>least</b> <b>squares</b> since 1795. This naturally led to a priority dispute with Legendre. However, to Gauss's credit, he went beyond Legendre and succeeded in connecting the <b>method</b> <b>of</b> <b>least</b> <b>squares</b> with the principles of probability and to the normal distribution. He had managed to complete Laplace's program of specifying a mathematical form of the probability density for the observations, depending on a finite number of unknown parameters, and define a method of estimation that minimizes the error of estimation. Gauss showed that arithmetic mean is indeed the best estimate of the location parameter by changing both the probability density and the method of estimation. He then turned the problem around by asking what form the density should have and what method of estimation should be used to get the arithmetic mean as estimate of the location parameter. In this attempt, he invented the normal distribution.|$|E
40|$|In this study, eight {{different}} leg motions are classified using two single-axis gyroscopes {{mounted on the}} right leg of a subject {{with the help of}} several pattern recognition techniques. The <b>methods</b> <b>of</b> <b>least</b> <b>squares,</b> Bayesian decision, k-nearest neighbor, dynamic time warping, artificial neural networks and support vector machines are used for classification and their performances are compared. This study comprises the preliminary work for our future studies on motion recognition with a much wider scope. © 2009 IEEE...|$|R
40|$|The most ambigueus {{factor in}} solving {{concrete}} field problems is {{the estimation of}} the boundary values. In this paper, the inverse method for estimating the boundary condition from measured values is proposed as the application <b>of</b> finite element <b>method</b> and the <b>method</b> <b>of</b> <b>least</b> <b>square,</b> and an error analysis is carried out for simple problems. The method is applied to heat condution problem in the piston model ofellgine {{and the effects of}} estimating parameters are clarified...|$|R
40|$|The {{nonlinear}} <b>methods</b> <b>of</b> <b>Least</b> <b>Squares</b> and Maximum Likelihood estimation are {{the main}} methods {{in the theory of}} nonlinear regression analysis. The serious disadvantage <b>of</b> these <b>methods</b> is that they can be applied only when the distribution of errors is known or is similar to the normal distribution. Using the optimal design theory, we propose an original approach, which is independent of assumptions on errors distribution. This approach is applied to a parameter estimation problem in chemical equilibrium analysis...|$|R
25|$|The {{discovery}} of Ceres led Gauss {{to his work}} on a theory of the motion of planetoids disturbed by large planets, eventually published in 1809 as Theoria motus corporum coelestium in sectionibus conicis solem ambientum (Theory of motion of the celestial bodies moving in conic sections around the Sun). In the process, he so streamlined the cumbersome mathematics of 18th century orbital prediction that his work remains a cornerstone of astronomical computation. It introduced the Gaussian gravitational constant, and contained an influential treatment of the <b>method</b> <b>of</b> <b>least</b> <b>squares,</b> a procedure used in all sciences to this day to minimize the impact of measurement error.|$|E
500|$|The Wiener {{process or}} Brownian motion process has {{its origins in}} {{different}} fields including statistics, finance and physics. In 1880, [...] Thorvald Thiele {{wrote a paper on}} the <b>method</b> <b>of</b> <b>least</b> <b>squares,</b> where he used the process to study the errors of a model in time-series analysis. The work is now considered as an early discovery of the statistical method known as Kalman filtering, but the work was largely overlooked. It is thought that the ideas in Thiele's paper were too advanced to have been understood by the broader mathematical and statistical community at the time.|$|E
500|$|A {{significant}} indirect {{influence was}} Thomas Simpson, who achieved {{a result that}} closely resembled de Moivre's. According to Simpsons' work's preface, his own work depended greatly on de Moivre's; the latter in fact described Simpson's work as an abridged version of his own. Finally, Thomas Bayes wrote an essay discussing theological implications of de Moivre's results: his solution to a problem, namely that of determining the probability of an event by its relative frequency, was taken as a proof {{for the existence of}} God by Bayes. Finally in 1812, Pierre-Simon Laplace published his Théorie analytique des probabilités in which he consolidated and laid down many fundamental results in probability and statistics such as the moment generating function, <b>method</b> <b>of</b> <b>least</b> <b>squares,</b> inductive probability, and hypothesis testing, thus completing the final phase in the development of classical probability. [...] Indeed, in light of all this, there is good reason Bernoulli's work is hailed as such a seminal event; not only did his various influences, direct and indirect, set the mathematical study of combinatorics spinning, but even theology was impacted.|$|E
40|$|Abstract: We study nonparametric {{estimation}} of convex regression and density functions by <b>methods</b> <b>of</b> <b>least</b> <b>squares</b> (in the regression and density cases) and maximum likelihood (in the density estimation case). We provide characterizations of these estimators, {{prove that they}} are consistent, and establish their asymptotic distributions at a xed point of positive curvature of the functions estimated. The asymptotic distribution theory relies {{on the existence of}} a function " for integrated two-sided Brownian motion + t 4 which is established in th...|$|R
3000|$|Surface profile {{describes}} the intersection profile {{that a certain}} surface truncated the actual material surface (Fig.  9). The calculation <b>method</b> <b>of</b> surface profile parameters R_a and RS_m were given, as shown in Eqs. (29) to (31). Datum line is the contour line based on the <b>method</b> <b>of</b> <b>least</b> <b>square</b> <b>method,</b> which makes {{the sum of the}} squares of the distances from each point on the contour line to the datum line have minimum value. The computational formulae of the surface profile parameters are shown as follows: [...]...|$|R
40|$|Abstract: "Ve study nonparametric {{estimation}} of convex regression and density functions <b>methods</b> <b>of</b> <b>least</b> <b>squares</b> (in the and density and maximum likelihood (in the density estimation. provide characterizations of these estimators, {{prove that they}} are consistent, and establish their distributions at a fixed point of positive curvature of the functions estimated. The asymptotic distribution theory relies {{on the existence of}} a "invelope function " for integrated two-sided Brownian motion t 4 which is established in the companion paper GROENEBOOM, JONGBLOED, AND "VELLNER (2000). a discussion ll...|$|R
2500|$|... 1805– Adrien-Marie Legendre {{introduces}} the <b>method</b> <b>of</b> <b>least</b> <b>squares</b> for fitting a curve {{to a given}} set of observations.|$|E
2500|$|... are the residuals, {{which are}} {{observable}} {{estimates of the}} unobservable values of the error termε'ij. [...] Because {{of the nature of}} the <b>method</b> <b>of</b> <b>least</b> <b>squares,</b> the whole vector of residuals, with ...|$|E
2500|$|Adrien-Marie Legendre (1805) {{developed}} the <b>method</b> <b>of</b> <b>least</b> <b>squares,</b> and introduced {{it in his}} Nouvelles méthodes pour la détermination des orbites des comètes (New Methods for Determining the Orbits of Comets). In ignorance of Legendre's contribution, an Irish-American writer, Robert Adrain, editor of [...] "The Analyst" [...] (1808), first deduced the law of facility of error, ...|$|E
40|$|Abstract. The random errors {{exist in}} {{measuring}} with touch trigger probe. These errors are directly introduced into workpiece coordinate system with conventional alignment method. An enhanced measuring method is proposed {{in this paper}} to improve the accuracy of alignment for five-axis CNC machine tools. By using the <b>method</b> <b>of</b> <b>least</b> <b>square,</b> an error reference frame is constructed, {{and the relationship between}} the alignment parameters and the error reference frame is derived. An example of aircraft structural parts is presented to validate the presented method...|$|R
5000|$|The <b>method</b> <b>of</b> {{ordinary}} <b>least</b> <b>squares</b> {{can be used}} to find {{an approximate}} solution to overdetermined systems. For the system [...] the <b>least</b> <b>squares</b> formula is obtained from the problem ...|$|R
25|$|In 1810, {{after reading}} Gauss's work, Laplace, after proving the central limit theorem, {{used it to}} give a large sample {{justification}} for the <b>method</b> <b>of</b> <b>least</b> <b>square</b> and the normal distribution. In 1822, Gauss was able to state that the least-squares approach to regression analysis is optimal {{in the sense that}} in a linear model where the errors have a mean of zero, are uncorrelated, and have equal variances, the best linear unbiased estimator of the coefficients is the least-squares estimator. This result is known as the Gauss–Markov theorem.|$|R
2500|$|The <b>method</b> <b>of</b> <b>least</b> <b>squares</b> is a {{standard}} approach in regression analysis to the approximate solution of overdetermined systems, i.e., sets of equations {{in which there are}} more equations than unknowns. [...] "Least squares" [...] means that the overall solution minimizes the sum of the squares of the residuals made in the results of every single equation.|$|E
2500|$|When {{the name}} is used, the [...] "Gaussian distribution" [...] was named after Carl Friedrich Gauss, who {{introduced}} the distribution in 1809 {{as a way of}} rationalizing the <b>method</b> <b>of</b> <b>least</b> <b>squares</b> as outlined above. Among English speakers, both [...] "normal distribution" [...] and [...] "Gaussian distribution" [...] are in common use, with different terms preferred by different communities.|$|E
2500|$|The <b>method</b> <b>of</b> <b>least</b> <b>squares</b> {{grew out}} of the fields of {{astronomy}} and geodesy, as scientists and mathematicians sought to provide solutions to the challenges of navigating the Earth's oceans during the Age of Exploration. [...] The accurate description of the behavior of celestial bodies was the key to enabling ships to sail in open seas, where sailors could no longer rely on land sightings for navigation.|$|E
40|$|We study nonparametric {{estimation}} of convex regression and density functions by <b>methods</b> <b>of</b> <b>least</b> <b>squares</b> (in the regression and density cases) and maximum likelihood (in the density estimation case). We provide characterizations of these estimators, {{prove that they}} are consistent, and establish their asymptotic distributions at a fixed point of positive curvature of the functions estimated. The asymptotic distribution theory relies {{on the existence of}} a "invelope function" for integrated two-sided Brownian motion + t 4 which is established in the companion paper Groeneboom, Jongbloed, and Wellner (2000) ...|$|R
50|$|In 1810, {{after reading}} Gauss's work, Laplace, after proving the central limit theorem, {{used it to}} give a large sample {{justification}} for the <b>method</b> <b>of</b> <b>least</b> <b>square</b> and the normal distribution. In 1822, Gauss was able to state that the least-squares approach to regression analysis is optimal {{in the sense that}} in a linear model where the errors have a mean of zero, are uncorrelated, and have equal variances, the best linear unbiased estimator of the coefficients is the least-squares estimator. This result is known as the Gauss-Markov theorem.|$|R
30|$|This measure {{does not}} include {{personnel}} absence cost, i.e. the value of employees’ foregone productive contribution. Equation (2) has been estimated via the <b>method</b> <b>of</b> ordinary <b>least</b> <b>squares</b> (OLS) for both (sub-)samplesk.|$|R
2500|$|Its {{mathematical}} foundations {{were laid}} in the 17th century {{with the development of}} the probability theory by Gerolamo Cardano, Blaise Pascal and Pierre de Fermat. Mathematical probability theory arose from the study of games of chance, although the concept of probability was already examined in medieval law and by philosophers such as Juan Caramuel. The <b>method</b> <b>of</b> <b>least</b> <b>squares</b> was first described by [...] Adrien-Marie Legendre in 1805.|$|E
2500|$|Usually in curve fitting, {{a set of}} {{data points}} is fitted with a curve defined by some {{mathematical}} function. For example, common types of curve fitting use a polynomial or a set of exponential functions. When there is no theoretical basis for choosing a fitting function, the curve may be fitted with a spline function composed of a sum of B-splines, using the <b>method</b> <b>of</b> <b>least</b> <b>squares.</b> Thus, the objective function for least squares minimization is, for a spline function of degree k, ...|$|E
2500|$|The {{first person}} to {{describe}} the mathematics behind Brownian motion was Thorvald N. Thiele in a paper on the <b>method</b> <b>of</b> <b>least</b> <b>squares</b> published in 1880. This was followed independently by Louis Bachelier in 1900 in his PhD thesis [...] "The theory of speculation", in which he presented a stochastic analysis of the stock and option markets. The Brownian motion model {{of the stock market}} is often cited, but Benoit Mandelbrot rejected its applicability to stock price movements in part because these are discontinuous.|$|E
5000|$|Vexler, A. and Konev, V. (1996). On {{precision}} of sequential estimations by the <b>method</b> <b>of</b> the <b>least</b> <b>squares</b> <b>of</b> the autoregression parameters. Journal of Communication Technology and Electronics. V.41, N7, 623- 630 ...|$|R
30|$|In this section, we {{investigate}} widely used fuzzy regression <b>methods</b> <b>of</b> Fuzzy <b>Least</b> <b>Squares</b> (FLS), General Fuzzy <b>Least</b> <b>Squares</b> (GFLS), Sakawa–Yano (SY), Hojati–Bector–Smimou (HBS), Approximate-Distance Fuzzy <b>Least</b> <b>Squares</b> (ADFLS) and Interval-Distance Fuzzy <b>Least</b> <b>Squares</b> (IDFLS).|$|R
30|$|In the ERT inversions, diverse setup {{constraints}} (mainly regularizations) are applied. These {{include the}} minimization <b>methods</b> <b>of</b> <b>least</b> <b>squares</b> (L 2) or robust blocky normalization (L 1), and initial models of a constant homogeneous resistivity or an approximate inverse model (e.g., Claerbout and Muir 1973). Two synthetic data sets (generated {{before and after}} adding 3 % random noise) are inverted with incorporating mapping data of subsurface stratigraphy from prior (seismic) surveys (see next sections). Each of these two data sets was inverted twice, once by incorporating layer interfaces and once by fixing resistivity regions, both are outside the reservoir layer.|$|R
