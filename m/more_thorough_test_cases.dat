0|10000|Public
30|$|We {{tested the}} effect of the choice of the algorithms’ {{parameters}} {{on the quality of the}} model, to eventually select one or several candidates for <b>more</b> <b>thorough</b> <b>tests.</b>|$|R
50|$|On March 15, 2010, {{a new and}} <b>more</b> <b>thorough</b> <b>test</b> was introduced. This test {{is based}} on a longer 63-page guide called Discover Canada. This gives {{immigrants}} a richer picture on Canada's history, culture, law and politics. At the same time, immigrants are required to memorize more facts for the test.|$|R
5000|$|Cannot include styles from a rule {{into another}} rule: CSS styles often must be {{duplicated}} in several rules {{to achieve a}} desired effect, causing additional maintenance and requiring <b>more</b> <b>thorough</b> <b>testing.</b> Some new CSS features were proposed to solve this, but (as of February, 2016) are not yet implemented anywhere.|$|R
40|$|Debates on {{opportunities}} for social mobility and {{the integration of}} people with a relatively weak position in society, such as unskilled people, long-term unemployed and single mothers, should {{take into account that}} a multi-dimensional and multi-level approach is required. Policies aimed at improving opportunities also require a <b>more</b> <b>thorough</b> <b>testing</b> of the basic assumptions that underpin the...|$|R
50|$|Assembly {{was done}} at Martin-Marietta's Baltimore plant {{so as not to}} {{interfere}} with missile work at the Denver facility, although it also saved the former from a planned shutdown. As with the Mercury-Atlas launch vehicles, a high degree of workmanship was stressed as well as <b>more</b> <b>thorough</b> <b>testing</b> of components and improved handling procedures compared with Titans designed for unmanned flight.|$|R
40|$|Risk is {{anything}} that threatens the successful achievement of a project’s goals. The fundamental principle of risk-based testing {{is to do}} <b>more</b> <b>thorough</b> <b>testing</b> to {{those parts of the}} software system that present the highest risk. In this fast abstract, we introduce risk-based testing and discuss applying risk analysis to select <b>test</b> <b>cases</b> for regression <b>testing</b> which is essential to ensure software quality. We provide a method of risk-based <b>test</b> <b>case</b> selection. This approach is a specificationbased method. Therefore, it does not have scalability problems as does code-based techniques. It is also easy to implement with test tools, thus, making the regression test process more automated...|$|R
5000|$|In a <b>more</b> <b>thorough</b> <b>test</b> of archosaurian {{relationships}} {{published in}} 2011 by Sterling Nesbitt, [...] "rauisuchians" [...] {{were found to}} be paraphyletic, with Poposauroidea {{at the base of the}} clade Paracrocodylomorpha, and the rest of the [...] "rauisuchians" [...] forming a grade within the clade Loricata. Nesbitt noted that no previous study of [...] "rauisuchian" [...] relationships had ever included a wide variety of supposed [...] "rauisuchians" [...] as well as a large number of non-"rauisuchian" [...] taxa as controls.|$|R
50|$|The Block III {{spacecraft}} {{would be}} stripped {{down to a}} minimum of instrumentation with the eight scientific instruments on Rangers 3-5 removed so that more space could be devoted to redundant systems hardware. This caused some protests from the scientific community that the gamma ray and other measurements were far more valuable than simply returning photography of the Moon. Jet Propulsion Laboratory began <b>more</b> <b>thorough</b> <b>testing</b> of Ranger components and a second review board was put to work evaluating {{the reliability of the}} Atlas-Agena.|$|R
30|$|According to the {{strategies}} of exploring the programs, fuzzers {{could be classified as}} directed fuzzing and coverage-based fuzzing. A directed fuzzer aims at generation of testcases that cover target code and target paths of programs, and a coverage-based fuzzer aims at generation of testcases that cover as much code of programs as possible. Directed fuzzers expect a faster test on programs, and coverage-based fuzzers expect a <b>more</b> <b>thorough</b> <b>test</b> and detect as more bugs as possible. For both directed fuzzers and coverage-based fuzzers, how to extract the information of executed paths is a key problem.|$|R
5000|$|Specification-based testing aims to {{test the}} {{functionality}} of software according to the applicable requirements. This level of <b>testing</b> usually requires <b>thorough</b> <b>test</b> <b>cases</b> to be provided to the tester, who then can simply verify that for a given input, the output value (or behavior), either [...] "is" [...] or [...] "is not" [...] {{the same as the}} expected value specified in the <b>test</b> case.Test <b>cases</b> are built around specifications and requirements, i.e., what the application is supposed to do. It uses external descriptions of the software, including specifications, requirements, and designs to derive <b>test</b> <b>cases.</b> These <b>tests</b> can be functional or non-functional, though usually functional.|$|R
30|$|The {{challenge}} of low code coverage. Higher code coverage represents {{for a higher}} coverage of program execution states, and a <b>more</b> <b>thorough</b> <b>testing.</b> Previous work has proved that better coverage results in a higher probability of finding bugs. However, most testcases only cover the same few paths, {{while most of the}} code could not be reached. As a result, it’s not a wise choice to achieve high coverage only through large amounts of testcase generation and throwing into testing resources. Coverage-based fuzzers try to solve the problem with the help of program analysis techniques, like program instrumentation. We will introduce the detail in next section.|$|R
40|$|Brennan and Hamlin (1998) {{predict that}} moderates {{are more likely}} to be {{expressive}} rather than instrumental voters, but do not test this hypothesis. Greene and Nelson (2002) claim to reject this, by finding that extremists are as likely to vote as moderates. We argue that Greene and Nelson's study was not a complete test of Brennan and Hamlin's hypothesis and we extend their analysis to provide a <b>more</b> <b>thorough</b> <b>test.</b> Our results imply that there is some evidence to suggest that extremist non-voters are less likely to be instrumentally motivated, providing some support for the predictions of Brennan and Hamlin. Copyright Springer Science+Business Media, LLC 2007 Voting, Expressiveness, Extremists,...|$|R
40|$|The kill curve for Phanerozoic {{marine species}} {{is used to}} {{investigate}} large-body impact {{as a cause of}} species extinction. Current estimates of Phanerozoic impact rates are combined with the kill curve to produce an impact-kill curve, which predicts extinction levels from crater diameter, on the working assumption that impacts are responsible for all "pulsed" extinctions. By definition, pulsed extinction includes the approximately 60 % of Phanerozoic extinctions that occurred in short-lived events having extinction rates greater than 5 %. The resulting impact-kill curve is credible, thus justifying <b>more</b> <b>thorough</b> <b>testing</b> of the impact-extinction hypothesis. Such testing is possible but requires an exhaustive analysis of radiometric dating of Phanerozoic impact events...|$|R
40|$|Abstract. Although {{high-temperature}} measurements show {{a dramatic}} {{reduction in the}} bias-temperature stress-induced threshold-voltage instability of present state-of-the-art devices, a <b>more</b> <b>thorough</b> <b>test</b> methodology shows that several different conclusions may actually be drawn. The particular conclusion depends on the specific post-BTS measurement technique employed. Immediate room-temperature measurements suggest that significant oxide-trap activation may still be occurring. A significant, yet rapid, post-BTS recovery is observed as well. These results underline the importance of making both high-temperature and room-temperature measurements, {{as a function of}} stress and recovery time, to better ensure that the full effect of the BTS is observed. Initial AC BTS results suggest a similar level of device degradation as occurs from a DC BTS...|$|R
40|$|This paper {{describes}} and evaluates {{an objective}} measurement that grades {{the quality of}} a complex musical signal. The authors have previously identified a potential correlation between inter-band dynamics and the subjective quality of produced music excerpts. This paper describes the previously presented Inter-Band Relationship (IBR) descriptor and extends this work by conducting <b>more</b> <b>thorough</b> <b>testing</b> with real-world music excerpts and a greater number of listening subjects. A high degree of correlation is observed between the Mean Subject Scores (MSS) and the objective IBR descriptor suggesting it could be used as an additional model output variable (MOV) to describe produced music quality. The method lends itself to real-time implementation and therefore can be exploited within mixing, mastering and monitoring tools...|$|R
40|$|Major changes {{support for}} Isabelle {{components}} (71209 ab, 25 cd 288, 455 baf 4) components can be injected when initializing an Environment components are automatically discovered {{when they are}} in the classpath libisabelle now takes control of $ISABELLE_HOME/etc/components some breaking changes in bootstrapping; e. g. Configurations are not created from Resources; see Hello_PIDE. scala for an up-to-date example attempting to use a System after disposal will result in an error (d 5 f 8 fc 7) a failed System startup will automatically clean up a potentially running Isabelle instance (9057 fe 1, regressed in # 58) Minor changes removed dependency on java 7 -fs-more (12109 ee) bumped dependencies (9 b 45 e 00) <b>more</b> <b>thorough</b> <b>testing</b> on Windows (7095 a 7 d...|$|R
40|$|Computer systems which {{interact}} with human users to collect, update or provide information are growing more complex. Additionally, users are demanding <b>more</b> <b>thorough</b> <b>testing</b> of all computer systems. Because {{of the complexity}} and thoroughness required, automation of interactive systems testing is desirable, especially for functional testing. Many currently available testing tools, like program proving, are impractical for testing large systems. The solution presented here {{is the development of}} an automated test system which simulates human users. This system incorporates a high-level programming language, ATLIS. ATLIS programs are compiled and interpretively executed. Programs are selected for execution by operator command, and failures are reported to the operator's console. An audit trail of all activity is provided. This solution provides improved efficiency and effectiveness over conventional testing methods...|$|R
40|$|This slide {{presentation}} {{reviews the}} failure {{analysis of a}} fractured poppet from a flow control valve (FCV) used on the space shuttle. This presentation {{has focused on the}} laboratory analysis of the failed hardware. The use of Scanning electron fractography during the investigation led {{to the conclusion that the}} poppet failed due to fatigue cracking that, most likely, occurred under changing loading conditions. The initial investigation led to a <b>more</b> <b>thorough</b> <b>test</b> of poppets that had been retired, this testing led to the conclusion that the thumbnail cracks in the flight hardware had existed for the life of the shuttle program. This led to a program to develop an eddy current technique that was capable of detecting small very tight cracks...|$|R
40|$|The thesis {{tries to}} verify {{a model for}} dynamic {{disconnection}} of sunspots from their magnetic roots proposed in the publication "The dynamical dis- connection of sunspots from their magnetic roots"(Schüssler & Rempel, 2005, Astron. Astrophys. 441, 337). In order to accomplish this task I conducted a numerical simulation, including a computation of a quiet Sun model using the OPAL opacity and equation of state tables. While simulating the time evolution of sunspot we retained the steps as are in the reffered article. The quiet Sun model corresponded well with other quiet Sun models, which {{are considered to be}} state-of-the-art. However, I was not able to reproduce the results fully as I didn't observe the dynamic disconnection. I suggest a <b>more</b> <b>thorough</b> <b>testing</b> of the presented code. ...|$|R
40|$|Appropriate {{analysis}} of simulation output {{is important to}} the success of a simulation study. Many users, however, do not have the skills required to perform such analyses. One way of overcoming this problem is to provide automated tools for analyzing simulation output. An Excel based automated "Analyser" is described that performs an {{analysis of}} a single scenario. The Analyser links to a commercial simulation software package, SIMUL 8, and provides recommendations on warm-up, number of replications and run-length. Various standard procedures are used in the Analyser with some adaptations to make them suitable for automation. This research demonstrates the potential of the approach. A requirement for further development is <b>more</b> <b>thorough</b> <b>testing</b> of the analysis procedures, particularly for their robustness and generality in use. Further adaptation of the procedures for automation may also be required...|$|R
40|$|Context: One of the {{strategies}} for managing large volumes of data is distributed and parallel computing. Among the tools that allow applying these characteristics are some Data Base Management Systems (DBMS), such as Oracle, DB 2, and SQL Server. Method: In this paper we present a case study where we evaluate the performance of an SQL query in two of these DBMS. The evaluation is done through various forms of data distribution in a computer network with different degrees of parallelism. Results: The tests of the SQL query evidenced the performance {{differences between the two}} DBMS analyzed. However, <b>more</b> <b>thorough</b> <b>testing</b> and a wider variety of queries are needed. Conclusions: The differences in performance between the two DBMSs analyzed show that when evaluating this aspect, it is necessary to consider the particularities of each DBMS and the degree of parallelism of the queries...|$|R
40|$|In this paper, {{we propose}} a text mining system to extract {{and use the}} {{information}} in radiology reports. The system consists of three main modules: medical finding extractor, report and image retriever. The medical finding extraction module automatically extracts medical findings and associated modifiers to structure radiology reports. The structuring of the free text reports bridges the gap between users and report database, makes {{the information contained in}} the reports readily accessible. It also serves as intermediate result to other components of the system. The retrieval module analyzes user’s query and returns the reports and images that match the query. The overallevaluation results are satisfactory, though <b>more</b> <b>thorough</b> <b>testing</b> and evaluation are needed. Our future work includes improving the current system performance and implementing the radiology report generation system using statistical machine translation approach, for which we have designed the general architecture...|$|R
40|$|Testing of {{database}} applications {{is crucial}} for ensuring high software quality as undetected faults can result in unrecoverable data corruption. The problem of database application testing can be broadly partitioned into the problems of <b>test</b> <b>cases</b> generation, <b>test</b> data preparation and test outcomes verification. Among the three problems, the problem of <b>test</b> <b>cases</b> generation directly affects the effectiveness of testing. Conventionally, database application testing is based upon {{whether or not the}} application can perform a set of predefined functions. While it is useful to achieve a basic degree of quality by considering the application to be a black box in the testing process, white box testing is required for <b>more</b> <b>thorough</b> <b>testing.</b> However, the semantics of the Structural Query Language (SQL) statements embedded in database applications are rarely considered in conventional white box testing techniques. In this paper, we propose to complement white box techniques with the inclusion of the SQL semantics. Our approach is to transform the embedded SQL statements to procedures in some general-purpose programming language and thereby generate <b>test</b> <b>cases</b> using conventional white box testing techniques. Additional <b>test</b> <b>cases</b> that are not covered in traditional white box testing are generated to improve the effectiveness of database application testing. The steps of both SQL statements transformation and <b>test</b> <b>cases</b> generation will be explained and illustrated using an example adapted from a course registration system. We successfully identify additional faults involving the internal states of databases. ...|$|R
50|$|In {{the medical}} sphere, it is {{important}} to have products to measure patients color vision. While medically professional vision tests are available, a Munsell Vision Test is often an informal and relevant test to determine a potential need for <b>more</b> <b>thorough</b> vision <b>testing</b> at the hands of pro staff or optometry experts. As previously mentioned, the Munsell-Farnsworth D15 Color Vision Test is a capable and professional method to test an individual.|$|R
50|$|In {{the early}} 1950s, after several cases were {{reported}} of sickness {{in children who}} had eaten Halloween candy colored with the dye, the FDA conducted new, <b>more</b> <b>thorough</b> and rigorous <b>testing</b> on food dyes. Orange 1 was outlawed for food use in 1956.|$|R
40|$|Abstract—In this paper, {{we propose}} a text mining system to extract {{and use the}} {{information}} in radiology reports. The system consists of three main modules: medical finding extractor, report and image retriever. The medical finding extraction module automatically extracts medical findings and associated modifiers to structure radiology reports. The structuring of the free text reports bridges the gap between users and report database, makes {{the information contained in}} the reports readily accessible. It also serves as intermediate result to other components of the system. The retrieval module analyzes user’s query and returns the reports and images that match the query. The overall evaluation results are satisfactory, though <b>more</b> <b>thorough</b> <b>testing</b> and evaluation are needed. Our future work includes improving the current system performance and implementing the radiology report generation system using statistical machine translation approach, for which we have designed the general architecture. Keywords-component; Text Mining, Radiology reports I...|$|R
40|$|Software {{testing is}} by far the most popular {{activity}} currently used by developers to ensure high software quality. Testing of database applications is particularly crucial as undetected faults can result in unrecoverable data corruption. The problem of database application testing can be broadly partitioned into the problems of <b>test</b> <b>cases</b> generation, <b>test</b> data preparation and test outcomes verification. Among the three problems, the problem of <b>test</b> <b>cases</b> generation directly affects the effectiveness of testing. Conventionally, database application testing is based upon whether or not the application can perform a set of predefined functions. The database application is largely considered as a black box in the testing process. While this is useful to achieve a basic degree of quality, white box testing is required for <b>more</b> <b>thorough</b> <b>testing</b> of database applications. However, the semantics of the Structural Query Language (SQL) statements embedded in database applications is rarely considered in conventional white box testing techniques. In this paper, we propose to complement white box techniques with the inclusion of the SQL semantics. Our approach is to transform the embedded SQL statements to procedures in some general-purpose programming language and thereby generate <b>test</b> <b>cases</b> using conventional white box testing techniques. The steps of transformations and <b>test</b> <b>cases</b> generation will be explained in detail and illustrated using an example adapted from a course registration system. This leads to the identification of additional faults involving the internal states of databases...|$|R
40|$|In {{spite of}} its {{importance}} in software reliability, testing is labor intensive and expensive. It {{has been found that}} software testing without a good strategy may not be more effective than testing the system with random data. Obviously, the effectiveness of testing relies heavily on how well the test suite-the set of <b>test</b> <b>cases</b> actually used-is generated. This is because the comprehensiveness of the test suite will affect the scope of testing and, hence, the chance of revealing software faults. There are two main approaches to generating test suites: specification-based and code-based. The former generates a test suite from information derived from the specification, without requiring the knowledge of the internal structure of the program. The latter approach, on the other hand, generates a test suite based on the source code of the program. Neither of these approaches is sufficient; they are complementary to one another. In software development, the requirements have to be established before implementation, and the specification should exist prior to coding. In this respect, the specification-based approach to test suite generation is particularly useful because <b>test</b> <b>cases</b> can be generated before coding has been completed. This facilitates software development phases to be performed in parallel, thus allowing time for preparing <b>more</b> <b>thorough</b> <b>test</b> plans and yet shortening the length of the whole process. © 2010 ACM. postprin...|$|R
40|$|Major changes (since 0. 6. 6) {{support for}} Isabelle {{components}} (71209 ab, 25 cd 288, 455 baf 4) components can be injected when initializing an Environment components are automatically discovered {{when they are}} in the classpath libisabelle now takes control of $ISABELLE_HOME/etc/components some breaking changes in bootstrapping; e. g. Configurations are not created from Resources; see Hello_PIDE. scala for an up-to-date example attempting to use a System after disposal will result in an error (d 5 f 8 fc 7) a failed System startup will automatically clean up a potentially running Isabelle instance (9057 fe 1, regressed in # 58) extended isabellectl (4 eb 2493) [...] afp flag pulls in the AFP as a component default for [...] version Minor changes (since 0. 6. 6) removed dependency on java 7 -fs-more (12109 ee) bumped dependencies (9 b 45 e 00) <b>more</b> <b>thorough</b> <b>testing</b> on Windows (7095 a 7 d) Documentation an example project demonstrating the combined use of libisabelle and sbt-libisabelle is availabl...|$|R
40|$|This paper {{describes}} and evaluates {{an objective}} measurement that profiles a complex musical signal {{over time in}} terms of identification of dynamic content and overall perceived quality. The authors have previously identified a potential correlation between inter-band dynamics and the subjective quality of produced music excerpts. This paper describes the previously presented Inter-Band Relationship (IBR) descriptor and extends this work by conducting <b>more</b> <b>thorough</b> <b>testing</b> of its relationship to perceived punch and clarity over varying time resolutions. A degree of correlation is observed between subjective test scores and the objective IBR descriptor suggesting {{it could be used}} as an additional model output variable (MOV) to describe punch and clarity with a piece of music. Limitations have been identified in the measure however and further consideration is required with regards to the choice of threshold adopted based on the range of dynamics detected within the musical extract and the possible inclusion of a gate as utilised in some loudness algorithms...|$|R
50|$|It is {{commonly}} {{performed in the}} United States and Japan, whereas the practice varies among South East Asia and mainland European countries. In Japan it is required by law for regular working employees to check once a year, with a much <b>more</b> <b>thorough</b> battery of <b>tests</b> than other countries.|$|R
40|$|The {{objective}} is to develop and demonstrate two technologies for the placement of coal combustion by-products in abandoned underground coal mines, and to assess {{the environmental impact of}} these technologies for the management of coal combustion by-products. The two technologies for the underground placement that will be developed and demonstrated are: (1) pneumatic placement using virtually dry coal combustion by-products, and (2) hydraulic placement using a paste mixture of combustion by-products with about 70 % solids. Phase 2 of the overall program began April 1, 1996. The principal objective of Phase 2 is to develop and fabricate the equipment for both the pneumatic and hydraulic placement technologies, and to conduct a limited, small-scale shakedown test of the pneumatic and hydraulic placement equipment. The shakedown test originally was to take place on the surface, in trenches dug for the tests. However, after a thorough study it was decided, with the concurrence of DOE-METC, to drill additional injection wells and conduct the shakedown tests underground. This will allow a <b>more</b> <b>thorough</b> <b>test</b> of the placement equipment...|$|R
40|$|The {{quality of}} low {{frequency}} audio reproduction in small spaces {{has always been}} problematic. For some time, methods have been suggested in order to optimise this reproduction. Many such methods have been based upon objective metrics which remain unproven from a subjective perspective. Whilst perception has been studied, this thesis identifies a research gap for <b>more</b> <b>thorough</b> <b>testing.</b> A series of listening tests has been conducted, with virtual rooms auralised and presented over headphones in order to isolate specific modal parameters and allow e cient collection of subjective response from many listening environments. The work presented searches for optimal values and perceptual thresholds of three parameters- modal spacing, density and decay. Results show that optimal spacings and densities may only be defined where assumptions are made which are not valid in realistic listening spaces. Thresholds of modal decay 1 have been defined, which are considered valid regardless of stimuli or replay level. These are around 0. 2 seconds for frequencies above 100 Hz, and increase sharply below this point to around 0. 85 seconds at 32 Hz...|$|R
40|$|Spur-of-the-moment {{planetary}} exploration {{missions are}} within our reach. Complex systems and complex missions usually take years {{of planning and}} force launches to become incredibly expensive. The longer the planning and the more expensive the mission, the more catastrophic if it fails. Always the remedies have been thought to be ever better planning, <b>more</b> redundancy, <b>more</b> <b>thorough</b> <b>testing,</b> and higher-quality components. We argue here for cheap, fast missions using large numbers of mass produced simple autonomous robots that are small by today's standards, perhaps 1 to 2 kg. Let loose upon a planet and out of control of ground-based mission planners, we argue that such robots enable the time between mission conception and implementation to be radically reduced, launch mass to be slashed, totally autonomous robots to be more reliable than ground-controlled robots, and large numbers of robots to change the tradeoff between reliability of individual components and overall mission success. Lastly, we suggest that {{within a few years}} it will be possible, at modest cost, to invade a planet with millions of tiny robots...|$|R
40|$|Ray-Tracing {{software}} tools {{have been widely}} used in the optical design of solar concentrating collectors. In spite of the ability of these tools to assess the geometrical and material aspects impacting the optical performance of concentrators, their use in combination with experimental measurements in the framework of collector testing procedures as not been implemented, to the date, in none of the current solar collector testing standards. In the latest revision of ISO 9806 an effort was made to include linear focusing concentrating collectors but some practical and theoretical difficulties emerged. A Ray-Tracing analysis could provide important contributions to overcome these issues, complementing the experimental results obtained through thermal testing and allowing the achievement of <b>more</b> <b>thorough</b> <b>testing</b> outputs with lower experimental requirements. In order to evaluate different available {{software tools}} a comparison study was conducted. Taking as representative technologies for line-focus concentrators the Parabolic Trough Collector and the Linear Fresnel Reflector Collector, two exemplary cases with predefined conditions - geometry, sun model and material properties - were simulated with different software tools. This work was carried out within IEA/SHC Task 49 "Solar Heat Integration in Industrial Processes"...|$|R
40|$|In {{our modern}} society, time is important. Technology {{is used in}} countless ways to faster get us from one point to another. The purpose of this report is to inquire {{whether there is a}} way to {{implement}} an option for personnel at Swedish hospitals to claim priority on an elevator call, and to do this in an efficient way. An ordinary elevator system has been simulated with an implementation of Nearest Car algorithm, an algorithm which calculates the most suitable elevator car to serve a specific call considering the distance between the call floor and the car and the direction. Along with this algorithm, different ways to handle priority calls has been implemented to see how they affect the mean transport time for both passengers with priority and regular passengers in the elevator. This has been made to decide if any of these algorithms are suitable for using in practice. It is established that the mean time for priority calls are substantially lower in all tested algorithms and that the ordinary calls mean time are not affected very much. However, it is stated that <b>more</b> <b>thorough</b> <b>tests</b> are required before a conclusion can be made about the implementations effectivity. ...|$|R
