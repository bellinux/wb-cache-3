0|10000|Public
50|$|One {{drawback}} to this <b>method,</b> aside <b>from</b> <b>requiring</b> Cp2Ti(CH3)2, is {{the difficulty of}} separating product from unreacted starting reagent.|$|R
40|$|AbstractIn {{this paper}} we {{consider}} the concept of orthogonality with respect to infinitely many inner products. We describe geometric properties related to this concept of orthogonality in certain Köthe sequence spaces (power series spaces), spaces of holomorphic functions in one and several variables and spaces of infinitely differentiable functions. The <b>methods</b> are <b>required</b> <b>from</b> a mixture of functional analysis (theory of bases), theory of functions of one complex variable, Fourier analysis and interpolation theory...|$|R
5000|$|Some <b>methods</b> <b>require</b> {{preference}} information <b>from</b> the DM {{throughout the}} solution process. These {{are referred to}} as interactive methods or <b>methods</b> that <b>require</b> [...] "progressive articulation of preferences." [...] These methods have been well-developed for both the multiple criteria evaluation (see for example Geoffrion, Dyer and Feinberg, 1972, and Köksalan and Sagala, 1995 [...] ) and design problems (see Steuer, 1986).|$|R
50|$|Other {{methods that}} are not so widely used at present can measure the {{diffusing}} capacity. These include the steady state diffusing capacity that is performed during regular tidal breathing, or the rebreathing <b>method</b> that <b>requires</b> rebreathing <b>from</b> a reservoir of gas mixtures.|$|R
3000|$|... ([36], Chapter 3). Note {{that this}} {{numerical}} approximation can be also applied assuming a deterministic path gain, i.e., {{in the absence}} of shadow fading effect, by simply setting a 0 =a 0 (1) (i.e., ψ= 1). In this case, the <b>method</b> <b>requires</b> drawing only <b>from</b> the random variables related to the noise ([...] [...]...|$|R
2500|$|In {{computer}} circuitry, {{this method}} is no faster than the [...] "complement and add one" [...] method; both <b>methods</b> <b>require</b> working sequentially <b>from</b> right to left, propagating logic changes. The method of complementing and adding one can be sped up by a standard carry look-ahead adder circuit; the LSB towards MSB method can be sped up by a similar logic transformation.|$|R
3000|$|... [16]. Accurate {{determination}} of absorbed dose by the CV <b>method</b> <b>requires</b> scintigraphic data <b>from</b> multiple time points over a reasonably long time interval following {{administration of the}} radiopharmaceutical, accurate calibration of the gamma camera system, appropriate corrections for background activity, attenuation, and scatter, and reliable estimation of organ masses. If possible, less time-consuming routine methods should be defined with adequate accuracy in activity quantification and absorbed dose determination.|$|R
40|$|Quantization of {{classical}} integrable models by the Quantum Inverse Scattering <b>Method</b> <b>requires</b> transition <b>from</b> classical r -matrices to the quantum ones. The twists are the special {{elements in the}} algebra of observables, which help to build new classical and quantum r -matrices. In this thesis we develop an approach to explicit derivation of quasiclassical twists for higher dimensional analogs of Jordanian r -matrices. The twists are obtained as limits of more general quantum twists which allow a simple description. The considered class of r -matrices includes the skew-symmetric Cremmer-Gervais r -matrices {{as well as the}} extended Jordanian ones. The quantum analogs for both twists are obtained...|$|R
40|$|Subset Simulation is an {{adaptive}} simulation method that efficiently solves structural reliability problems with many random variables. The <b>method</b> <b>requires</b> sampling <b>from</b> conditional distributions, which is achieved through Markov Chain Monte Carlo (MCMC) algorithms. This paper discusses different MCMC algorithms proposed for Subset Simulation and introduces {{a novel approach}} for MCMC sampling in the standard normal space. Two variants of the algorithm are proposed: A basic variant, which is simpler than existing algorithms with equal accuracy and efficiency, and a more efficient variant with adaptive scaling. It is demonstrated that the proposed algorithm improves the accuracy of Subset Simulation, {{without the need for}} additional model evaluations...|$|R
40|$|While {{using the}} Kahn Precipitation Test and {{especially}} the quantitative procedure, the possibility suggested itself of applying similar principles of concentration to the demonstration of agglu-tinins. If an immediate agglutination reaction could be obtained by an accurate, quantitative method, the advantages would at once be apparent. Working along these lines, a rapid method for the macroscopic agglutination test has been developed. The method involves concentration of the antigen, the use of small amounts of both antigen and serum, together with thorough mixing. Results read within five minutes after adding serum to suspension correspond to those obtained by a standard <b>method</b> <b>requiring</b> <b>from</b> eighteen to twenty-four hours. EXPERIMENTAL The first experiments were made with 0. 15 cc. of varying dilu-tions of an antityphoid serum (1 : 20 to 1 : 1000) and 0. 05 cc. of a very heavy killed suspension of B. typhosus. These were mixed in small test tubes (75 by 13 mm.), shaken vigorously for two min-utes, 0. 5 cc. saline added and read-thus following the quantities and mechanics of the Kahn Precipitation Test. There was complete agglutination throughout, with no clump-ing in the control (0. 15 cc. saline plus 0. 05 cc. suspension). This experiment was repeated with B. typhosus and B. paratyphosu...|$|R
5000|$|In 1985, as Jean Charles {{was getting}} ready to take his turn running the company, the French wine world was {{undergoing}} a revolution of sort. Wines from the New World had just made a spectacular entry on to the international market and were challenging French supremacy. New efforts, new thinking and new <b>methods</b> were <b>required</b> <b>from</b> the French winemaking industry to face this challenge. Jean Charles decided {{it was time for a}} major upgrade of the Duval-Leroy facilities and line of wines. He laid down the plans for modernizing the winery, and started thinking about a new prestige cuvee, which in time would become [...] "Femme de Champagne".|$|R
40|$|Abstract The trans-lycopene {{content of}} fresh tomato homogenates was {{assessed}} {{by means of the}} laser photoacoustic spectroscopy, the laser optothermal window, micro-Raman spectroscopy, and colorimetry; none of these <b>methods</b> <b>require</b> the extraction <b>from</b> the product matrix prior to the analysis. The wet chemistry method (highperformance liquid chromatography) was used as the absolute quantitative method. Analytical figures of merit for all methods were compared statistically; best linear correlation was achieved for the chromaticity index a * and chroma C*...|$|R
40|$|This paper {{discusses}} {{some aspects}} of the centralised version of the supply chain coordination method that uses the so-called Alternating Direction Method (ADM) presented by Jeong (2012, A centralized/decentralized design of a full return contract for a risk-free manufacturer and a risk-neutral retailer under partial information sharing. International Journal of Production Economics 136 (1), 110 - 115). We show that the <b>method</b> <b>requires</b> both <b>from</b> the retailer and the manufacturer to faithfully follow the proposed algorithm, without any attempt to follow their own interests in gaining higher profits. We also warn that the condition of the information privacy is violated also in partial information sharing model. Furthermore, we correct an error in one of the equations...|$|R
40|$|Recently Møller, Pettitt, Berthelsen and Reeves [17] {{introduced}} a new MCMC methodology for drawing samples from a posterior distribution when the likelihood function is only specified up to a normalising constant. We illustrate the method {{in the setting of}} Bayesian inference for Markov point processes; more specifically we consider a likelihood function given by a Strauss point process with priors imposed on the unknown parameters. The method relies on introducing an auxiliary variable specified by a normalised density which approximates the likelihood well. For the Strauss point process we use a partially ordered Markov point process as the auxiliary variable. As the <b>method</b> <b>requires</b> simulation <b>from</b> the “unknown ” likelihood, perfect simulation algorithms for spatial point processes become useful...|$|R
30|$|The {{virtualization}} technology {{provides the}} ability to transfer virtual machine from one physical host to another using virtual machine live/offline migration. In order to keep transfer time and overhead minimum, efficient virtual machine live migration <b>methods</b> are <b>required.</b> Apart <b>from</b> this, other issues are also there which {{need to be addressed}} to make the migration method applicable on most of the virtual environments. This section identifies and discusses these key issues.|$|R
30|$|As {{wireless}} networks {{became more}} popular, {{the use of}} a wireless lossy channel presents new issues to TCP engineers. Considerable efforts have been made to improve TCP efficiency over wireless channels [5 – 7]. Some researchers attempted to modify congestion control in order to retain the congestion window during wireless loss [8 – 10]. Others have differentiated congestion losses and wireless channel losses [11 – 13]. However, the results of these studies were not very promising because the developed <b>methods</b> <b>require</b> help <b>from</b> other network elements such as base stations or routers, and the proposed differentiation algorithms did not perform well. Other TCP variants such as TCP-RR [14], TCP-PR [15], and TCP-DCR [16] address the issue of packet reordering or persistent congestion.|$|R
30|$|The genetic {{algorithm}} <b>requires</b> <b>methods</b> derived <b>from</b> {{the principles of}} biological evolution to produce a new generation from the initial generation. These methods are operators that affect {{the members of the}} initial generation and evolve them for evolution of next generations. The operators used in the research include crossover and mutation operators. They will briefly explain.|$|R
40|$|Abstract—This paper {{presents}} a new method for real-time monitoring of available bandwidth over a network path. The method uses active probing with trains of probe packets, and produces estimates in real-time using Kalman filtering. It improves the estimate for each new {{measurement of the}} strain of the inter-packet time intervals in a probe packet train. We have tested the method with good accuracy and agreement, both in simulation and in a physical test network. The <b>method</b> <b>requires</b> no communication <b>from</b> receiver to sender and only minor processing and memory space...|$|R
40|$|These {{lecture notes}} for a {{graduate}} course cover generalized derivative concepts useful in deriving necessary optimality conditions and numerical algorithms for nondifferentiable optimization problems in inverse problems, imaging, and PDE-constrained optimization. Treated are convex functions and subdifferentials, Fenchel duality, monotone operators and resolvents, Moreau [...] Yosida regularization, proximal point and (some) first-order splitting methods, Clarke subdifferentials, and semismooth Newton <b>methods.</b> The <b>required</b> background <b>from</b> functional analysis and {{calculus of variations}} is also briefly summarized. Comment: Lecture note...|$|R
40|$|Understanding and {{following}} musical improvisations {{with a computer}} <b>requires</b> <b>methods</b> different <b>from</b> those needed when following score-based musical performances. This paper discusses questions related to interactive music systems for the recognition and accompaniment of tonal improvisations. This includes discussion about rhythmic parsing and harmonic analysis with a recursive Bayesian classifier. Finally, a real-time MIDI system for identifying {{and following}} tonal improvisations is described...|$|R
40|$|Hydro-meteorological data is an {{important}} asset that can enhance management of water resources. But existing data often contains gaps, leading to uncertainties and so compromising their use. Although many methods exist for infilling data gaps in hydro-meteorological time series, many of these <b>methods</b> <b>require</b> inputs <b>from</b> neighbouring stations, which are often not available, while other methods are computationally demanding. Computing techniques such Artificial Intelligence {{can be used to}} address this challenge. Self-Organizing Maps (SOMs), which are a type of Artificial Neural Network, was used for infilling gaps in a hydro-meteorological time series in a Sudano-Sahel catchment. The coefficients of determination obtained were all above 0. 75 and 0. 65 while the average topographic error was 0. 008 and 0. 02 for rainfall and river discharge time series respectively. These results further indicate that SOMs are a robust and efficient method for infilling missing gaps in hydro-meteorological time series...|$|R
40|$|High-throughput {{screening}} (HTS) {{of chemical}} libraries {{is often used}} for the unbiased identification of compounds interacting with G protein-coupled receptors (GPCRs), the largest family of therapeutic targets. However, current HTS <b>methods</b> <b>require</b> removing GPCRs <b>from</b> their native environment, which modifies their pharmacodynamic properties and biases the screen toward false positive hits. Here, we developed and validated a molecular imaging (MI) agent, NIR-mbc 94, which emits near infrared (NIR) light and selectively binds to endogenously expressed cannabinoid CB(2) receptors, a recognized target for treating autoimmune diseases, chronic pain and cancer. The precision and ease of this assay allows for the HTS of compounds interacting with CB(2) receptors expressed in their native environment...|$|R
40|$|This thesis {{focuses on}} {{analysis}} {{of changes in}} the area of film editing, resulting from the introduction of the modern electronic technologies. By comparison of the classical cinematographical workflows based on a traditional film stock with the new opportunities brought by digital systems, it aims to specify the new working <b>methods</b> as <b>required</b> <b>from</b> the professional film editor. Consequently, this thesis defines the advantages and point out the possible issues arising from displacement of the classical film stock with an electronic media as a fundamental changeover in the whole cinematography history. Furthermore, it explores the recording options of new digital cameras with a view to evaluate the impact on the profession of the film editor. It ponders on the situation of a film editor as he/she is being overloaded with new technologies and seeks the way out from the accumulation of duties and responsibilities which an editor has to manage nowadays...|$|R
5000|$|The atomic-scale {{complexity}} presents additional {{challenges to}} computational modelling of high-entropy alloys. Thermodynamic modelling using the CALPHAD <b>method</b> <b>requires</b> extrapolating <b>from</b> binary and ternary systems. Most commercial thermodynamic databases are designed for, and {{may only be}} valid for, alloys consisting primarily of a single element. Thus, they require experimental verification or additional ab initio calculations such as density functional theory (DFT). However, DFT modeling of complex, random alloys has its own challenges, as the <b>method</b> <b>requires</b> defining a fixed-size cell, which can introduce non-random periodicity. This is commonly overcome using the method of [...] "special quasirandom structures," [...] designed to most closely approximate the radial distribution function of a random system, combined with the Vienna Ab-initio Simulation Package. Using this method, {{it has been shown}} that results of a 4-component equiatomic alloy begins to converge with a cell as small as 24 atoms. The exact muffin-tin orbital method with the coherent potential approximation has also been employed to model HEAs. Other techniques include the 'multiple randomly populated supercell' approach, which better describes the random population of a true solid solution (although is far more computationally demanding). This method has also been used to model glassy/amorphous (including bulk metallic glasses) systems without a crystal lattice.|$|R
40|$|An {{important}} {{problem in}} epidemiology and medical {{research is the}} estimation of a causal effect of a treatment action at a single point in time on the mean of an outcome within a population defined by strata {{of some of the}} observed covariates. Marginal structural models (MSM) are models for marginal distributions of treatment-specific counterfactual outcomes, possibly conditional on a subset of the baseline covariates, and are therefore precisely modelling such causal effects. These models were introduced by Robins (e. g., Robins (2000 a), Robins (2000 b), van der Laan and Robins (2002)). Inverse probability of treatment weighted estimators, double robust inverse probability of treatment weighted estimators, and G-computation (likelihood) -based estimators, have been developed and studied in detail (Robins (2000 b), Neugebauer and van der Laan (2004), Yu and van der Laan (2003), van der Laan and Robins (2002)). These <b>methods</b> <b>require</b> <b>from</b> the user specification of a parametric model (i. e., a marginal structural model) for the treatment specific mean as function of the treatment and adjustment covariates, and a model for the nuisance parameter representing either the treatment mechanism (IPTW) and/or regression of the outcome on treatment and all baseline covariates (DR-IPTW, G-comp). In this article we develop and implement a general data adaptive loss-based estimation (as in machine learning) methodology, involving cross-validation to data adaptively select model complexities for the marginal structural model, as well as the nuisance parameter model. We implemented our data adaptive methodology in a publicly available R-package, and illustrate its practical performance with an extensive simulation study. In addition, we provide an application involving the estimation of the effect of lung function on survival in an elderly population. Causal inference, confounding, counterfactual, cross-validation, double robust estimation, G-computation estimation, inverse probability of treatment, weighted estimation, loss function, risk,...|$|R
30|$|Comparing the {{importance}} sampling with the MCMC method, {{we see that}} both can obtain samples from a PDF that is known to us up to a normalizing constant. Nevertheless, some key differences between them should be noted. In {{the importance}} sampling, no iterations are involved, but the samples obtained are associated with different weights, which implies a lower computational efficiency since particles with different weights take up {{the same amount of}} computational resources. By contrast, all the samples obtained are equally weighted in the MCMC method. However, the MCMC <b>method</b> <b>requires</b> iterative sampling <b>from</b> the proposal distribution to ensure that the invariant distribution is finally reached, which can be time prohibitive in some real-time applications.|$|R
40|$|SummaryHigh-throughput {{screening}} (HTS) {{of chemical}} libraries {{is often used}} for the unbiased identification of compounds interacting with G protein-coupled receptors (GPCRs), the largest family of therapeutic targets. However, current HTS <b>methods</b> <b>require</b> removing GPCRs <b>from</b> their native environment, which modifies their pharmacodynamic properties and biases the screen toward false positive hits. Here, we developed and validated a molecular imaging (MI) agent, NIR-mbc 94, which emits near infrared (NIR) light and selectively binds to endogenously expressed cannabinoid CB 2 receptors, a recognized target for treating autoimmune diseases, chronic pain and cancer. The precision and ease of this assay allows for the HTS of compounds interacting with CB 2 receptors expressed in their native environment...|$|R
40|$|Fiber {{composite}} materials {{have a great}} practical importance in lightweight construction, in particular in aerospace applications. They are made of unidirectional fiber-reinforced synthetic resin layers. The orientations of the unidirectional layers relative to a common reference direction, layer thicknesses {{and the number of}} layers are variables, among others, in the design of optimum fiber composite components. As a rule they are discretely variable, e. g. for production reasons. So their mathematical optimization requires the use of discrete optimization methods. Since {{this is a matter of}} generally nonlinear problems with frequently implicitly given problem functions, there is not a single most appropriate method. A range of <b>methods</b> is <b>required,</b> <b>from</b> which the suitable method for a concrete case of application is to be selected. This paper reports on the method of H. M. Amir and T. Hasegawa, its translation into a subroutine, and first experience gathered with the same. (orig.) SIGLEAvailable from TIB Hannover: RN 4165 (1995, 09) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekDEGerman...|$|R
30|$|Recent {{improvements}} in 2 D methods {{have made them}} comparable to 3 D methods in terms of quality. For instance, {{the use of an}} L 1 -norm optimization can generate a camera path that follows cinematographic rules in order to consider separately constant, linear, and parabolic motion [24]. A mesh-based model, in which multiple trajectories are calculated at different locations of the video, proved to be efficient in dealing with parallax without the use of 3 D methods [23]. A semi-automatic 2 D <b>method,</b> which <b>requires</b> assistance <b>from</b> the user, is proposed to adjust problematic frames [46].|$|R
40|$|One of {{the most}} {{difficult}} aspects of using the Gibbs sampler in practice is knowing when to stop the algorithm. In order to answer this we need to have some method which will tell us when we have completed enough iterations for the chain to have converged sufficiently. In this paper I will {{look at some of the}} methods that have been suggested in the literature. Most of these <b>methods</b> <b>require</b> input <b>from</b> the user throughout the length of the chain. This aspect of the diagnostics extends the length of time that it takes for the algorithm to terminate and is quite tedious for the user. Ideally one would like to have an automatic algorithm which would test for convergence and stop the Gibbs sampler when it is sufficiently close to convergence. I will look at some of the issues involved in finding such a diagnostic. 1 Introduction Markov Chain Monte Carlo (MCMC) methods have recently become very popular tools for the analysis of Bayesian posterior distributions of relatively high dimension. T [...] ...|$|R
40|$|Isolated {{power plants}} with well {{characterized}} emissions {{serve as an}} ideal test case of methods to estimate emissions using satellite data. In this study we evaluate the Exponentially-Modified Gaussian (EMG) method and the box model method based on mass balance for estimating known NOx emissions from satellite retrievals made by the Ozone Monitoring Instrument (OMI). We consider 29 power plants in the USA which have large NOx plumes that do not overlap with other sources and which have emissions data from the Continuous Emission Monitoring System (CEMS). This enables us to identify constraints <b>required</b> by the <b>methods,</b> such as which wind data to use and how to calculate background values. We found that the lifetimes estimated by the methods are too short to {{be representative of the}} chemical lifetime. Instead, we introduce a separate lifetime parameter to account for the discrepancy between estimates using real data and those that theory would predict. In terms of emissions, the EMG <b>method</b> <b>required</b> averages <b>from</b> multiple years to give accurate results, whereas the box model method gave accurate results for individual ozone seasons...|$|R
40|$|The {{methodology}} of Green’s function retrieval by cross-correlation {{has led to}} many interesting applications for passive and controlled-source acoustic measurements. In all applications, a virtual source is created at {{the position of a}} receiver. Here a method is discussed for Green’s function retrieval from controlled-source reflection data, which circumvents the requirement of having an actual receiver at the position of the virtual source. The <b>method</b> <b>requires,</b> apart <b>from</b> the reflection data, an estimate of the direct arrival of the Green’s function. A single-sided three-dimensional (3 D) Marchenko equation underlies the method. This equation relates the reflection response, measured at one side of the medium, to the scattering coda of a so-called focusing function. By iteratively solving the 3 D Marchenko equation, this scattering coda is retrieved from the reflection response. Once the scattering coda has been resolved, the Green’s function (including all multiple scattering) can be constructed from the reflection response and the focusing function. The proposed methodology has interesting applications in acoustic imaging, properly accounting for internal multiple scattering. Geoscience & EngineeringCivil Engineering and Geoscience...|$|R
30|$|For {{calibration}} of a 2 -D {{laser line}} scanner to a camera, Zhang et al. [10] proposed {{a method that}} makes use of checkerboards for aligning the coordinate systems of both modalities. The <b>method</b> <b>requires</b> multiple acquisitions <b>from</b> different positions to establish sufficiently many constraints for nonlinear optimization. More recently, Kassir et al. [11] propose an automatic toolbox that builds {{on top of the}} well-known Camera Calibration Toolbox for Matlab. The toolbox is extended by detection algorithms for both checkerboards in the camera images and lines in the laser scanner data. In an iterative process, the spatial relation is optimized such that the detected lines match the planes of the calibration pattern. Zhou [12] presented a numerically more stable approach that also uses plane-line correspondences to constrain the estimation. This <b>method</b> also <b>requires</b> fewer plane-line pairs than the method by Zhang et al. [10].|$|R
40|$|Historical {{mission of}} {{economy of the}} XXI century is {{increase}} of life quality. Due to this significance of factors that determine life quality increases. Extreme inequality of income becomes a factor of destructive effect, especially recently. In order to analyse influence of inequality of income upon life quality, the article uses a system approach; to detect direct and backward connections between the inequality and life quality – dialectical method; to identify the character of interconnection between the economic growth and inequality and its decomposition into components: positive and excessive – comparative and statistical <b>methods.</b> Extreme inequality <b>requires</b> <b>from</b> scientists and experts to actively search for new mechanisms of fair re-distribution of the economic growth effect. Increase of life quality is impossible without efficient solution of this problem...|$|R
40|$|Presented in Partial Fulfillment of the Requirements for The Master of Special Education Degree in the College of {{education}} and Human Service Professions, University of Minnesota Duluth, 2011 Committee names: Trudie Hughes (Chair), Gerry Nierengarten. The survey designed {{for this study was}} meant to examine teacher's satisfaction with their current method for determining eligibility for learning disabilities as well as gaining more insight into what interventions they currently take part in for this process. The current process being used by the schools participating is the severe discrepancy method in which a student of evaluated is given an IQ assessment as well as a formal academic assessment {{to see if there is}} a large enough difference between the two assessments to make the statement that a severe discrepancy exists. The survey also addressed the same teacher's knowledge of RtI and their thoughts on what this <b>method</b> would <b>require</b> <b>from</b> them. All participants that participated in this study are from one school district in southern Minnesota that is currently at the beginning stages of implementing RtI in their schools. University of Minnesota, Duluth. College {{of education}} and Human Service Profession...|$|R
40|$|In {{this article}} we {{investigate}} a finite element formulation of strongly monotone quasi-linear elliptic PDEs {{in the context of}} fixed-point iterations. As opposed to Newton's <b>method,</b> which <b>requires</b> information <b>from</b> the previous iteration in order to linearise the iteration matrix (and thereby to recompute it) in each step, the alternative method used in this article exploits the monotonicity properties of the problem, and only needs the iteration matrix calculated once for all iterations of the fixed-point method. We outline the a priori and a posteriori error estimates for iteratively obtained solutions, and show both theoretically as well as numerically how the number of iterations of the fixed-point method can be restricted in dependence of the mesh size, or of the polynomial degree, to obtain optimal convergence...|$|R
