0|141|Public
25|$|He mistook {{an editor}} and re-publisher for the {{original}} author. Sutton cites Wilkin (1852) which is an edited collected works, within which the <b>matching</b> <b>phrase</b> occurs, in Browne (1658).|$|R
40|$|We {{present a}} system that enables {{flexible}} and efficient <b>phrase</b> <b>matching</b> in XML documents. Since XML allows structured and unstructured information to be interleaved, <b>phrase</b> <b>matching</b> in XML raises new challenges. Our system, named PIX, permits <b>phrase</b> <b>matching</b> in XML documents that contain "mixed content". A key feature of PIX is that users can specify which element and content to ignore when <b>matching</b> a <b>phrase.</b> PIX uses inverted indices and an efficient evaluation algorithm to compute the set of matches and returns answers where phrases, ignored tags and content are highlighted. In addition, query answers are sorted using a ranking function. PIX is implemented {{as an extension of}} GALAX, a full-fledged XQuery engine. The functionality of PIX is fully integrated into XQuery and permits a natural combination of XPath-based structure <b>matching</b> with <b>phrase</b> <b>matching...</b>|$|R
50|$|Graph Search {{operated}} {{by use of}} a search algorithm similar to traditional search engines such as Google. However, the search feature is distinguished as a semantic search engine, searching based on intended meaning. Rather than returning results based on matching keywords, the search engine is designed to <b>match</b> <b>phrases,</b> as well as objects on the site.|$|R
5000|$|For example, {{a search}} {{could be used}} to find [...] "red brick house", and <b>match</b> <b>phrases</b> such as [...] "red house of brick" [...] or [...] "house made of red brick". By {{limiting}} the proximity, these <b>phrases</b> can be <b>matched</b> while avoiding documents where the words are scattered or spread across a page or in unrelated articles in an anthology.|$|R
40|$|Document {{clustering}} techniques mostly rely on single term {{analysis of}} the document data set, such as the Vector Space Model. To better capture the structure of documents, the underlying data model {{should be able to}} represent the phrases in the document as well as single terms. We present a novel data model, the Document Index Graph, which indexes web documents based on phrases, rather than single terms only. The semi-structured web documents help in identifying potential <b>phrases</b> that when <b>matched</b> with other documents indicate strong similarity between the documents. The Document Index Graph captures this information, and finding significant <b>matching</b> <b>phrases</b> between documents becomes easy and efficient with such model. The similarity between documents is based on both single term weights and <b>matching</b> <b>phrases</b> weights. The combined similarities are used with standard document clustering techniques to test their effect on the clustering quality. Experimental results show that our phrase-based similarity, combined with single-term similarity measures, enhances web document clustering quality significantly. 1...|$|R
5000|$|Phonosemantic <b>matching,</b> finding <b>phrases</b> which combine {{both the}} meaning and sound of the neologism. Examples: ...|$|R
5000|$|Phrasing {{and form}} (movement and parts are {{structured}} to <b>match</b> the <b>phrasing</b> of the music) ...|$|R
50|$|Negative {{keywords}} {{are often}} necessary for paid search campaigns that contain keywords on either broad <b>match</b> or <b>phrase</b> <b>match.</b> These match types will display your ad for additional search queries (i.e. search queries {{other than the}} actual keyword that {{was added to the}} account). Thus, removing irrelevant terms often becomes necessary.|$|R
40|$|This paper {{introduces}} a novel journal splitting algorithm. It takes {{full advantage of}} various kinds of information such as text match, layout and page numbers. The core procedure is a highly efficient text-mining algorithm, which detects the <b>matched</b> <b>phrases</b> between the content pages and the title pages of individual articles. Experiments show that this algorithm is robust and able to split {{a wide range of}} journals, magazines and books. 1...|$|R
40|$|This paper {{presents}} a partial matching strategy for phrase-based {{statistical machine translation}} (PBSMT). Source phrases which do {{not appear in the}} training corpus can be translated by word substitution according to partially <b>matched</b> <b>phrases.</b> The advantage of this method is that it can alleviate the data sparseness problem if the amount of bilingual corpus is limited. We incorporate our approach into the state-of-the-art PBSMT system Moses and achieve statistically significant improvements on both small and large corpora. ...|$|R
50|$|Putting hyperlinks where {{visitors}} {{will not see}} them to increase link popularity. Highlighted link text can help rank a webpage higher for <b>matching</b> that <b>phrase.</b>|$|R
40|$|Abstract. Document {{clustering}} techniques mostly rely on single term {{analysis of}} text, {{such as the}} vector space model. To better capture the structure of documents, the underlying data model {{should be able to}} represent the phrases in the document as well as single terms. We present a novel data model, the Document Index Graph, which indexes Web documents based on phrases rather than on single terms only. The semistructured Web documents help in identifying potential <b>phrases</b> that when <b>matched</b> with other documents indicate strong similarity between the docu-ments. The Document Index Graph captures this information, and finding significant <b>matching</b> <b>phrases</b> between documents becomes easy and efficient with such model. The model is flexi-ble in that it could revert to a compact representation of the vector space model if we choose not to index phrases. However, using phrase indexing yields more accurate document similar-ity calculations. The similarity between documents is based on both single term weights and <b>matching</b> <b>phrase</b> weights. The combined similarities are used with standard document cluster-ing techniques to test their effect on the clustering quality. Experimental results show that our phrase-based similarity, combined with single-term similarity measures, gives a more accurate measure of document similarity and thus significantly enhances Web document clustering qual-ity...|$|R
5000|$|The Flute Concerto has a {{duration}} of roughly 15 minutes and is composed in one continuous movement. Tower briefly described the {{piece in the}} score program notes, writing, [...] "The 15-minute work starts with the low register of the flute alone before the orchestra comes. As the flute gets more active, the chamber-size orchestra provides competitive tension which is <b>matched</b> <b>phrase</b> by phrase as the piece heads relentlessly towards to a finale where the [...] "music blows wide open" [...] (Wincenc) in a virtuosic display of flute scales and arpeggios." ...|$|R
40|$|This paper {{presents}} a noun phrase driven two-level {{statistical machine translation}} system. Noun phrases (NPs) are used as the unit of decomposition to build a two level hierarchy of phrases. English noun phrases are identified using a parser. The corresponding translations are induced using a statistical word alignment model. Identified noun phrase pairs in the training corpus are replaced with a tag to produce a NP tagged corpus. This corpus is then used to extract phrase translation pairs. Both NP translations and NP-tagged phrases are used in a two-level translation decoder: NP translations tag NPs in the first level, where NP-tagged <b>phrases</b> <b>match</b> across NPs to produce translations in the second level. The two-level system shows significant improvements over a baseline SMT system. It also produces longer <b>matching</b> <b>phrases</b> due to the generalization introduced by tagging NPs. 1...|$|R
40|$|<b>Phrase</b> <b>matching</b> is {{a common}} IR {{technique}} to search text and identify relevant documents in a document collection. <b>Phrase</b> <b>matching</b> in XML presents new challenges as text may be interleaved with arbitrary markup, thwarting search techniques that require strict contiguity or close proximity of keywords. We present a technique for <b>phrase</b> <b>matching</b> in XML that permits dynamic specification of both the <b>phrase</b> to be <b>matched</b> and the markup to be ignored. We develop an effective algorithm for our technique that utilizes inverted indices on phrase words and XML tags. We describe experimental results comparing our algorithm to an indexed-nested loop algorithm that illustrate our algorithm's efficiency...|$|R
40|$|In natural {{language}} question answering (QA) systems, questions often contain terms and phrases that are critically important for retrieving or finding answers from documents. We present a learnable {{system that can}} extract and rank these terms and <b>phrases</b> (dubbed mandatory <b>matching</b> <b>phrases</b> or MMPs), and demonstrate their utility in a QA system on Internet discussion forum data sets. The system relies on deep syntactic and semantic analysis of questions only and is independent of relevant documents. Our proposed model can predict MMPs with high accuracy. When used in a QA system features derived from the MMP model improve performance significantly over a state-of-the-art baseline. The final QA system was the best performing system in the DARPA BOLT-IR evaluation. ...|$|R
40|$|The {{fundamental}} {{function of}} an information retrieval {{system is to}} retrieve texts or documents from a database {{in response to a}} user’s request for information, such that the content of the retreived documents will be relevant to the user’s original information need. This is accomplished through matching the user’s information request against the texts in the database in order to estimate which texts are relevant. In this thesis I propose a method for using current natural language processing techniques {{for the construction of a}} text representation to be used in an information retrieval system. In order to support this proposal I have designed a matching algorithm specifically for performing the retrieval task of matching user queries against texts in a database, using the proposed text representation. Having designed this text representation and matching algorithm, I then constructed an experiment to investigate the effectiveness of the algorithm at <b>matching</b> <b>phrases.</b> This experiment involved the use of standard statistical methods to compare the <b>phrase</b> <b>matching</b> capabilities of the proposed matching algorithm to a sample of information retrieval users performing the same task. The results of this evaluation experiment allow me to comment first of all on the effectiveness of the <b>phrase</b> <b>matching</b> algorihtm that I have designed and more generally, on the usefulness of incorporating natural language processing techniques into information retrieval systems...|$|R
40|$|UIC) {{participate}} in the robust track, which is a traditional ad hoc retrieval task. The emphasis is based on average effectiveness {{as well as individual}} topic effectiveness. Noun phrases in the query are identified and classified into 4 types: proper names, dictionary phrases, simple phrases and complex phrases. A document has a phrase if all content words in a phrase are within a window of a certain size. The window sizes for different types of phrases are different. We consider phrases to be more important than individual terms. As a consequence, documents in response to a query are ranked with <b>matching</b> <b>phrases</b> given a higher priority. WordNet is used to disambiguate word senses and bring in useful synonyms and hyponyms once the correct senses of the words in a query have been identified. The usual pseudo-feedback process is modified so that the documents are also ranked according to phrase and word similarities with <b>phrase</b> <b>matching</b> having a higher priority. Five runs which use either title or title and description have been submitted. 1...|$|R
40|$|Online resources, such as Wiktionary, {{provide an}} {{accurate}} but incomplete source of idiomatic phrases. In this paper, we study {{the problem of}} automatically identifying idiomatic dictionary entries with such resources. We train an idiom classifier on a newly gathered corpus of over 60, 000 Wiktionary multi-word definitions, incorporating features that model whether phrase meanings are constructed compositionally. Experiments demonstrate that the learned classifier can provide high quality idiom labels, more than doubling the number of idiomatic entries from 7, 764 to 18, 155 at precision levels of over 65 %. These gains also translate to idiom detection in sentences, by simply using known word sense disambiguation algorithms to <b>match</b> <b>phrases</b> to their definitions. In a set of Wiktionary definition example sentences, the more complete set of idioms boosts detection recall by over 28 percentage points. ...|$|R
40|$|The {{problem of}} {{manually}} modifying the lexicon appears with any {{natural language processing}} program. Ideally, a program {{should be able to}} acquire new lexieal entries from context, the way people learn. We address the problem of acquiring entire phrases, specifically Jigurative phr~es, through augmenting a phr~al lezico~ Facilitating such a self-extending lexicon involves (a) disambiguation-selection of the intended phrase from a set of <b>matching</b> <b>phrases,</b> (b) robust parsin~-comprehension of partially-matching phrases, and (c) error analysis [...] -use of errors in forming hy-potheses about new phrases. We have designed and im-plemented a program called RINA which uses demons to implement functional-qrammar principles. RINA receives new figurative phrases in context and through the appli-cation of a sequence of failure-driven rules, creates and refines both the patterns and the concepts which hold syntactic and semantic information about phrases...|$|R
40|$|In a {{joint effort}} of the Peshitta Institute Leiden and the Werkgroep Informatica of the Vrije Universiteit Amsterdam an {{electronic}} database of Syriac texts is being developed. Percy van Keulen and I have been assigned the Books of Kings, which we have analyzed from morpheme level up through clause-level parsing. Using the Hebrew material already available in the Werkgroep Informatica database, a synopsis of the Masoretic text and the Peshitta has been made at clause level. On {{the basis of the}} synop-sis, clause constituents have been matched, providing a basis for <b>matching</b> <b>phrases</b> within clauses, and for <b>matching</b> words within <b>phrases.</b> One of the products is an elec¬tronic translation concordance with lists of translation correspondences occur¬ring within Kings, which was introduced at the 2005 ISLP meeting in Philadelphia. The lexical items occurring at corresponding points in the two texts need not necessarily be lexicon-based semantic translations of one another, but they are what do occur at that point in the two texts. In this manner, both similarities and differences are brought to light. The occurrences of the two cognate verbs sym and swm within Kings are illustrative of the factors at work during the process of translation...|$|R
40|$|Verbal {{command and}} control systems are fairly common; almost all {{off-the-shelf}} speech recognition packages come {{with a way to}} perform various tasks through a voice command. Unfortunately, these systems require that the user utter the commands precisely in the format that it is expecting. These systems have a small number of grammar rules defined that are used to match against incoming utterances. Here, we present a method of using these same grammar rules to expand the capabilities of {{command and control}} engines to include semantically similar utterances. Latent Semantic Analysis (LSA) is used to connect specific grammar rules with the meanings underlying <b>matching</b> <b>phrases</b> resulting in utterances being matched to grammar rules even though the exact <b>phrase</b> did not <b>match</b> any specific rule. Experiments are described that {{determine the extent to which}} this method can be used and how accurate it is...|$|R
40|$|Generally, speech {{recognition}} engines can employ two different grammar methods, rule and dictation, to recognize an utterance. The {{purpose of these}} grammars is to constrain the search space {{in a way that}} anticipates the speaker's utterance. The research described in this paper attempts to maintain the accuracy of a rule grammar without limiting the speaker to rigorous phraseology. Latent Semantic Analysis (LSA) is used to connect specific grammar rules with the meanings underlying <b>matching</b> <b>phrases</b> resulting in utterances being matched to knowledge base elements even though the exact <b>phrase</b> did not <b>match</b> any grammar rule. A separate knowledge base is used to dynamically add or remove grammar rules in the {{speech recognition}} engine as the conversation context changes. Finally, a learning technique is used to create new regular expressions based on utterances that matched semantically through LSA...|$|R
2500|$|The phrase [...] "No surrender" [...] is {{occasionally}} sung in {{the bridge}} before [...] "Send her victorious" [...] by England football fans at <b>matches.</b> The <b>phrase</b> [...] "no surrender" [...] is {{also associated with}} Combat 18, a white supremacist group. The phrase is also associated with Ulster loyalism and can sometimes be heard {{at the same point}} before Northern Ireland football matches.|$|R
50|$|Sparse binary {{polynomial}} hashing (SBPH) is a {{generalization of}} Bayesian spam filtering that can <b>match</b> mutating <b>phrases</b> {{as well as}} single words. SBPH {{is a way of}} generating a large number of features from an incoming text automatically, and then using statistics to determine the weights for each of those features in terms of their predictive values for spam/nonspam evaluation.|$|R
5000|$|The phrase [...] "No surrender" [...] is {{occasionally}} sung in {{the bridge}} before [...] "Send her victorious" [...] by England football fans at <b>matches.</b> The <b>phrase</b> [...] "no surrender" [...] is {{also associated with}} Combat 18, a white supremacist group. The phrase is also associated with Ulster loyalism and can sometimes be heard {{at the same point}} before Northern Ireland football matches.|$|R
5000|$|Several {{arrangements}} of the tune {{are often used}} for the ballroom Paso Doble dance (to the point that, among ballroom dancers, it is known as [...] "the paso doble song" [...] as it is very commonly played in competition due to the common custom for the choreography to <b>match</b> the <b>phrasing</b> and accents of the music for the full effect of the dance).|$|R
40|$|Noun phrases in {{queries are}} {{identified}} and classified into four types: proper names, dictionary phrases, simple phrases and complex phrases. A document has a phrase if all content {{words in the}} phrase are within a window of a certain size. The window sizes for different types of phrases are different and are determined using a decision tree. Phrases {{are more important than}} individual terms. Consequently, documents in response to a query are ranked with <b>matching</b> <b>phrases</b> given a higher priority. We utilize WordNet to disambiguate word senses of query terms. Whenever the sense of a query term is determined, its synonyms, hyponyms, words from its definition and its compound words are considered for possible additions to the query. Experimental results show that our approach yields between 23 % and 31 % improvements over the best-known results on the TREC 9, 10 and 12 collections for short (title only) queries, without using Web data...|$|R
40|$|The current paper {{focuses on}} the {{development}} of a 3 D character generation and multilingual talking emulation software application. Traditional lip sync techniques in 3 D graphics productions require a careful (and painful) design at the pre-render phase, as well as the selection of the mostly <b>matching</b> <b>phrases</b> during audio dubbing. This paper proposes a completely different approach, whereas graphic designers are not worried about facial animation for lip sync purposes. When the scene’s sequence modeling is finished, rendering starts just after an automatic modification procedure where faces are morphed according to the driving sound. In case of a multilingual production, rendering for each different language is required, but the content of the dialogues is not depended on the original facial expressions formed according to the original language. Based on our experimentation, we suggest a “standardized face ” format for 3 d graphic popular platforms in combination with the developed algorithm for automated lip sync, translation and rendering...|$|R
50|$|Instead {{of searching}} for an address or a street name, the {{application}} can also search by name, for example guide the user to a nearby restaurant by being given {{the name of the}} restaurant. The application can also take phrases such as “a place with burgers” and suggest nearby destinations that <b>match</b> the <b>phrase.</b> The application can receive a voice input instead of typing the destination on the device.|$|R
40|$|We have {{developed}} an experimental Arabic-to-English example-based machine translation (EBMT) system, which exploits a bilingual corpus to find examples that match fragments of the input source-language text [...] Modern Standard Arabic (MSA), in our case [...] and imitates its translations. Translation examples were extracted from a collection of parallel, sentencealigned, unvocalized Arabic-English documents, taken from several corpora published by the Linguistic Data Consortium. The system is non-structural: translation examples are stored as textual strings, with some additional inferred linguistic features. In working with a highly inflected language, finding an exact match for an input phrase with reasonable precision presumably requires a very large parallel corpus. Since we are interesting in studying the use of relatively small corpora for translation, <b>matching</b> <b>phrases</b> to the corpus is done on a spectrum of linguistic levels, so that not only exact phrases are discovered, but also related ones. In this work, we investigate particularly the effect of matching synonymous verbs. To explore the possibility of matching fragments based on source-language synonyms, we created a thesaurus for Arabic verbs, based on Arabic comparable corpus. A comparabl...|$|R
40|$|This paper proposes Deep 2 Q, a novel {{search engine}} that {{proactively}} transforms query forms of Deep Web sources into phrase queries, constructs query evaluation plans, and caches results for popular queries offline. Then at query time, keyword queries are simply <b>matched</b> with <b>phrase</b> queries to retrieve results. Deep 2 Q embodies a novel dual-ranking framework for query answering and novel solutions for discovering frequent attributes and queries. Preliminary experiments show the great potentials of Deep 2 Q...|$|R
40|$|In this paper, we {{show that}} certain phrases {{although}} not present in a given question/query, play {{a very important role}} in answering the question. Exploring the role of such phrases in answering questions not only reduces the dependency on <b>matching</b> question <b>phrases</b> for extracting answers, but also improves the quality of the extracted answers. Here <b>matching</b> question <b>phrases</b> means phrases which co-occur in given question and candidate answers. To achieve the above discussed goal, we introduce a bigram-based word graph model populated with semantic and topical relatedness of terms in the given document. Next, we apply an improved version of ranking with a prior-based approach, which ranks all words in the candidate document with respect to a set of root words (i. e. non-stopwords present in the question and in the candidate document). As a result, terms logically related to the root words are scored higher than terms that are not related to the root words. Experimental results show that our devised system performs better than state-of-the-art for the task of answering Why-questions. Comment: Got accepted in NLDB- 2013; as Paper ID: 23; Title: "Exploring the Role of Logically Related Non-Question Phrases for Answering Why-Questions", Withdraw...|$|R
40|$|In this thesis, {{we propose}} a {{framework}} that uses multiple-domains and multi-modal techniques to disambiguate a variety of natural human input modes. This system {{is based on the}} input needs of pervasive computing users. The work extends the Galaxy architecture developed by the Spoken Language Systems group at MIT. Just as speech recognition disambiguates an input wave form by using a grammar to find the best <b>matching</b> <b>phrase,</b> we use the same mechanism to disambiguate other input forms, T 9 in particular. A skeleton version of the framework was implemented to show this framework is possible and to explore {{some of the issues that}} might arise. The system currently works for both T 9 and Speech modes. The framework also includes potential for any other type of input for which a recognizer can be built such as graffiti input. by Shalini Agarwal. Thesis (M. Eng.) [...] Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2002. Includes bibliographical references (leaves 51 - 53). This electronic version was submitted by the student author. The certified thesis is available in the Institute Archives and Special Collections...|$|R
40|$|The NASA Lexical Dictionary (NLD), {{a system}} that {{automatically}} translates input subject terms to those of NASA, was developed in four phases. Phase One provided <b>Phrase</b> <b>Matching,</b> a context sensitive word-matching process that <b>matches</b> input <b>phrase</b> words with any NASA Thesaurus posting (i. e., index) term or Use reference. Other Use references {{have been added to}} enable the matching of synonyms, variant spellings, and some words with the same root. Phase Two provided the capability of translating any individual DTIC term to one or more NASA terms having the same meaning. Phase Three provided NASA terms having equivalent concepts for two or more DTIC terms, i. e., coordinations of DTIC terms. Phase Four was concerned with indexer feedback and maintenance. Although the original NLD construction involved much manual data entry, ways were found to automate nearly all but the intellectual decision-making processes. In addition to finding improved ways to construct a lexical dictionary, applications for the NLD have been found and are being developed...|$|R
40|$|In this paper, we {{describe}} ideas and related experiments of Tsinghua University IR group in TREC 2004 QA track. In this track, our system consists three components: Question analysis, Information retrieval, and Answer extraction. Question analysis component extracts Query Term and answer type. Information retrieval component retrieves candidate documents from index set based on paragraph level and re-ranks {{them to find}} more relevant documents. And then Answer extraction component <b>matches</b> empirical <b>phrases</b> according to answer type to extract final answer. 1...|$|R
