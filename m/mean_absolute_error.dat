1998|7763|Public
2500|$|... 2.	<b>Mean</b> <b>Absolute</b> <b>Error</b> (MAE) and {{standard}} deviation (SD) in prediction.|$|E
2500|$|The <b>mean</b> <b>absolute</b> <b>error</b> {{of a real}} {{variable}} c {{with respect}} to the random variablenbsp&X is ...|$|E
2500|$|Provided {{that the}} {{probability}} distribution of X {{is such that}} the above expectation exists, then m is a median of [...] X {{if and only if}} m is a minimizer of the <b>mean</b> <b>absolute</b> <b>error</b> with respect to X. In particular, m is a sample median if and only if m minimizes the arithmetic mean of the absolute deviations.|$|E
30|$|Both {{types of}} <b>mean</b> <b>absolute</b> <b>errors</b> (second and third column of Table  1), {{on the other}} hand, show {{encouraging}} results in all cases.|$|R
3000|$|..., {{which are}} the {{remaining}} training data after the sampling, we perform the estimation of prediction error degrees and calculate their <b>mean</b> <b>absolute</b> <b>errors.</b>|$|R
30|$|These {{models have}} been {{developed}} and tested using a common incident data set, including 237 incident events, for allowing a direct comparison of the models’ prediction ability in the various incident situations. The <b>Mean</b> <b>Absolute</b> <b>Errors</b> (MAE), the Root Mean Squared Errors (RMSE) and the <b>Mean</b> <b>Absolute</b> Percentage <b>Errors</b> (MAPE) were adopted to estimate the models’ accuracy.|$|R
5000|$|Forecast {{skill for}} single-value {{forecasts}} is commonly represented {{in terms of}} metrics such as correlation, root mean squared error, <b>mean</b> <b>absolute</b> <b>error,</b> relative <b>mean</b> <b>absolute</b> <b>error,</b> bias, and the Brier score, among others.|$|E
5000|$|... 2. <b>Mean</b> <b>Absolute</b> <b>Error</b> (MAE) and {{standard}} deviation (SD) in prediction.|$|E
5000|$|As {{the name}} suggests, the <b>mean</b> <b>{{absolute}}</b> <b>error</b> is {{an average of}} the absolute errors , where [...] is the prediction and [...] the true value. Note that alternative formulations may include relative frequencies as weight factors. The <b>mean</b> <b>absolute</b> <b>error</b> uses the same scale as the data being measured. This {{is known as a}} scale-dependent accuracy measure and therefore cannot be used to make comparisons between series using different scales. The <b>mean</b> <b>absolute</b> <b>error</b> is a common measure of forecast error in time series analysis, where the terms [...] "mean absolute deviation" [...] is sometimes used in confusion with the more standard definition of mean absolute deviation. The same confusion exists more generally.|$|E
5000|$|The <b>mean</b> <b>absolute</b> scaled <b>error</b> has the {{following}} desirable properties: ...|$|R
30|$|Table  1 {{summarizes}} the main {{results of our}} study. In particular, the upper third of Table  1 presents averages of <b>mean</b> <b>absolute</b> percentage <b>error</b> (MAPE) estimates from M = 50 replications of the same out-of-sample prediction exercises for six benchmark procedures. The parenthesized numbers are empirical standard deviations of these <b>mean</b> <b>absolute</b> percentage <b>error</b> estimates.|$|R
5000|$|For a non-seasonal time series, the <b>mean</b> <b>absolute</b> scaled <b>error</b> is {{estimated}} by ...|$|R
50|$|The <b>mean</b> <b>absolute</b> <b>error</b> {{of a real}} {{variable}} c {{with respect}} to the random variable X isProvided that the probability distribution of X is such that the above expectation exists, then m is a median of X if and only if m is a minimizer of the <b>mean</b> <b>absolute</b> <b>error</b> with respect to X. In particular, m is a sample median if and only if m minimizes the arithmetic mean of the absolute deviations.|$|E
5000|$|In statistics, <b>mean</b> <b>absolute</b> <b>error</b> (MAE) is {{a measure}} of {{difference}} between two continuous variables. Assume X and Y are variables of paired observations that express the same phenomenon. Examples of Y versus X include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement. Consider a scatter plot of n points, where point i has coordinates (xi, yi). <b>Mean</b> <b>Absolute</b> <b>Error</b> (MAE) is the average vertical distance between each point and the Y=X line, which is also known as the One-to-One line. MAE is also the average horizontal distance between each point and the Y=X line.|$|E
50|$|Where a {{prediction}} model {{is to be}} fitted using a selected performance measure, {{in the sense that}} the least squares approach is related to the mean squared error, the equivalent for <b>mean</b> <b>absolute</b> <b>error</b> is least absolute deviations.|$|E
3000|$|Specifically in Table 1, we {{show the}} <b>mean</b> <b>absolute</b> <b>errors</b> (MAE) of the {{estimated}} diffusion source parameters obtained using the centralized and both distributed (unquantized and quantized communication links) algorithms. To obtain the reported statistics, we simulate the field induced by a single source (M= 1), with c [...]...|$|R
40|$|The paper {{analyses}} {{the performance}} {{results of the}} recently developed short-term forecasting suit for the Latvian power system. The system load and wind power are forecasted using ANN and ARIMA models, respectively, and the forecasting accuracy is evaluated in terms of <b>errors,</b> <b>mean</b> <b>absolute</b> <b>errors</b> and <b>mean</b> <b>absolute</b> percentage <b>errors.</b> The investigation of influence of additional input variables on load forecasting errors is performed. The interplay of hourly loads and wind power forecasting errors is also evaluated for the Latvian power system with historical loads (the year 2011) and planned wind power capacities (the year 2023) ...|$|R
5000|$|For a {{seasonal}} time series, the <b>mean</b> <b>absolute</b> scaled <b>error</b> is estimated {{in a manner}} similar to the method for non-seasonal time series: ...|$|R
5000|$|Although scoring {{rules are}} {{introduced}} in probabilistic forecasting literature, the definition is general enough to consider non-probabilistic {{measures such as}} <b>mean</b> <b>absolute</b> <b>error</b> or mean square error as some specific scoring rules. The main characteristic of such scoring rules is [...] is just {{a function of the}} expected value of [...] (i.e., [...] ).|$|E
50|$|There are {{normally}} occurring uncertainties in building design and building energy assessment. Yezioro, Dong and Leite developed an artificial intelligence approach towards assessing building performance simulation results {{and found that}} more detailed simulation tools have the best simulation performance in terms of heating and cooling electricity consumption within 3% of <b>mean</b> <b>absolute</b> <b>error.</b>|$|E
5000|$|The main {{difference}} with the method for non-seasonal time series, {{is that the}} denominator is the <b>mean</b> <b>absolute</b> <b>error</b> of the one-step [...] "seasonal naive forecast method" [...] on the training set, which uses the actual value from the prior season as the forecast: Ft = Yt−m, where m is the seasonal period.|$|E
5000|$|Symmetric <b>mean</b> <b>absolute</b> {{percentage}} <b>error</b> (SMAPE or sMAPE) is {{an accuracy}} measure based on percentage (or relative) errors. It is usually defined as follows: ...|$|R
40|$|We {{discuss and}} compare {{measures}} of accuracy of univariate time series forecasts. The methods {{used in the}} M-competition and the M 3 -competition, {{and many of the}} measures recommended by previous authors on this topic, are found to be inadequate, {{and many of them are}} degenerate in commonly occurring situations. Instead, we propose that the <b>mean</b> <b>absolute</b> scaled <b>error</b> become the standard measure for comparing forecast accuracy across multiple time series. Forecast accuracy, Forecast evaluation, Forecast <b>error</b> measures, M-competition, <b>Mean</b> <b>absolute</b> scaled <b>error.</b> ...|$|R
2500|$|There are {{alternative}} optimality criteria, which {{attempt to}} cover cases where MISE {{is not an}} appropriate measure. The equivalent L1 measure, <b>Mean</b> Integrated <b>Absolute</b> <b>Error,</b> is ...|$|R
50|$|There {{are several}} basic fitness {{functions}} for evaluating model performance, {{with the most}} common being based on the error or residual between the model output and the actual value. Such functions include the mean squared error, root mean squared error, <b>mean</b> <b>absolute</b> <b>error,</b> relative squared error, root relative squared error, relative absolute error, and others.|$|E
50|$|The {{two most}} popular {{measures}} of accuracy that incorporate the forecast error are the <b>Mean</b> <b>Absolute</b> <b>Error</b> (MAE) and the Root Mean Squared Error (RMSE). Thus these measures {{are considered to be}} scale-dependent, that is, they are on the same scale as the original data. Consequently, these cannot be used to compare models of differing scales.|$|E
5000|$|Like variance, {{mean squared error}} has the {{disadvantage}} of heavily weighting outliers. [...] This {{is a result of}} the squaring of each term, which effectively weights large errors more heavily than small ones. This property, undesirable in many applications, has led researchers to use alternatives such as the <b>mean</b> <b>absolute</b> <b>error,</b> or those based on the median.|$|E
5000|$|In statistics, the <b>mean</b> <b>absolute</b> scaled <b>error</b> (MASE) is {{a measure}} of the {{accuracy}} of forecasts [...] It was proposed in 2005 by statistician Rob J. Hyndman and Professor of Decision Sciences Anne B. Koehler, who described it as a [...] "generally applicable measurement of forecast accuracy without the problems seen in the other measurements." [...] The <b>mean</b> <b>absolute</b> scaled <b>error</b> has favorable properties when compared to other methods for calculating forecast errors, such as root-mean-square-deviation, and is therefore recommended for determining comparative accuracy of forecasts.|$|R
5000|$|Forecast {{accuracy}} {{in the supply}} chain is typically measured using the <b>Mean</b> <b>Absolute</b> Percent <b>Error</b> or MAPE. Statistically MAPE {{is defined as the}} average of percentage errors.|$|R
5000|$|Its {{mathematical}} analysis is considerably {{more difficult than}} the MISE ones. In practise, the gain appears not to be significant. The L∞ norm is the <b>Mean</b> Uniform <b>Absolute</b> <b>Error</b> ...|$|R
50|$|The <b>mean</b> <b>absolute</b> <b>error</b> {{is one of}} {{a number}} of ways of {{comparing}} forecasts with their eventual outcomes. Well-established alternatives are the mean absolute scaled error (MASE) and the mean squared error. These all summarize performance in ways that disregard the direction of over- or under- prediction; a measure that does place emphasis on this is the mean signed difference.|$|E
50|$|Following this {{procedure}} <b>mean</b> <b>absolute</b> <b>error</b> (MAE) {{can be calculated}} for all values of all ranges from below Table. It assumed that the material properties are independent of each other. Therefore, each materialproperty will be varied at a time, leaving the others constant at the default values (from EnergyPlus)and measured the <b>mean</b> <b>absolute</b> <b>error</b> (MAE) between the real indoor and the simulated temperatures. The range of material properties was given by an expert.The specific room understudy {{has a lot of}} fenestration, so it is not so surprising to see that the influence of the solar transmittance of the windows is the most influential of all material properties analyzed. The next influential factor is the conductivity of the bricks, followed by the thermal absorptance and the specific heat of the bricks.The most influential properties of the materials analyzed (bricks and glasses) are the solar transmittance of the glasses and the conductivity of the bricks.|$|E
5000|$|... {{where the}} {{numerator}} et is the forecast error {{for a given}} period, defined as the actual value (Yt) minus the forecast value (Ft) for that period: et = Yt &minus; Ft, and the denominator is the <b>mean</b> <b>absolute</b> <b>error</b> of the one-step [...] "naive forecast method" [...] on the training set, which uses the actual value from the prior period as the forecast: Ft = Yt−1 ...|$|E
5000|$|Scale invariance: The <b>mean</b> <b>absolute</b> scaled <b>error</b> is {{independent}} of {{the scale of the}} data, so can be used to compare forecasts across data sets with different scales.|$|R
30|$|When {{there are}} more than one model developed, a {{comparative}} analysis can be made by calculating the accuracy of each model and comparing them. The accuracy of any predictive model can be determined only by choosing appropriate error measures. In this paper, different models are compared using two <b>error</b> measuring parameters, <b>Mean</b> <b>Absolute</b> Percentage <b>Error</b> (MAPE) and Symmetric <b>Mean</b> <b>Absolute</b> Percentage <b>Error</b> (SMAPE) which can be defined as shown below. MAPE is a relative measure which reflects error as the percentage of actual data. Hence, the accuracy of the model can be easily judged.|$|R
40|$|We {{describe}} a monotone classification algorithm called MOCA {{that attempts to}} minimize the <b>mean</b> <b>absolute</b> prediction <b>error</b> for classification problems with ordered class labels. We first find a monotone classifier with minimum L 1 loss on the training sample, and then use a simple interpolation scheme to predict the class labels for attribute vectors not present in the training data. We compare MOCA to the Ordinal Stochastic Dominance Learner (OSDL), on artificial as well as real data sets. We show that MOCA often outperforms OSDL with respect to <b>mean</b> <b>absolute</b> prediction <b>error.</b> ...|$|R
