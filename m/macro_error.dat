1|33|Public
40|$|This article reviews on-shell {{methods for}} {{analytic}} computation of loop amplitudes, emphasizing techniques based on unitarity cuts. Unitarity techniques are formulated generally {{but have been}} especially useful for calculating one-loop amplitudes in massless theories such as Yang-Mills theory, QCD, and QED. Comment: 34 pages. Invited review for a special issue of Journal of Physics A devoted to "Scattering Amplitudes in Gauge Theories. " v 2 : typesetting <b>macro</b> <b>error</b> fixe...|$|E
50|$|First, {{because there}} isn't {{any form of}} <b>macros</b> <b>error</b> {{checking}} (as there is for C or assembly language), {{it is possible to}} make macros which will not work.Indeed, for the C language, the syntax of each macro is replaced by what has been declared by the preprocessor. Only after that does the compiler check the code.|$|R
5000|$|... errno.h, a header file in C {{that defines}} <b>macros</b> for {{reporting}} <b>errors</b> ...|$|R
40|$|Abstract—The {{widely used}} C {{preprocessor}} (CPP) {{is generally considered}} a source of difficulty for understanding and maintaining C/C++ programs. The main reason for this difficulty is CPP’s purely lexical semantics, i. e., its treatment of both input and output as token streams. This can easily lead to errors {{that are difficult to}} diagnose, and {{it has been estimated that}} up to 20 % of all macros are erroneous. To reduce such errors, more restrictive, replacement languages for CPP have been proposed to limit expanded macros to be valid C syntactic units. However, there is no practical tool that can effectively validate CPP macros in legacy applications. In this paper, we introduce a novel, general characterization of inconsistent macro usage as a strong indicator of <b>macro</b> <b>errors.</b> Our key insight is that all applications of the same macro should behave similarly. In particular, we map each macro call c in a source file f to c’s normalized syntactic constructs within the abstract syntax tree (AST) for f’s preprocessed source, and use syntactic similarity as the basis for comparing macro calls of the same macro definition. Utilizing this characterization, we have developed an efficient algorithm to statically validate macro usage in C/C++ programs. We have implemented the algorithm; evaluation results show that our tool is effective in detecting common macro-related errors and reports few false positives, making it a practical tool for validating macro usage. Keywords-preprossing, <b>macro</b> <b>errors,</b> inconsistencies I...|$|R
50|$|It {{has been}} shown that {{integration}} of linkage maps can aid de novo assemblies with long range, chromosome scale recombination data, without which, assemblies can be subject to <b>macro</b> ordering <b>errors.</b> Optical mapping is the process of immobilizing the DNA on a slide and digesting it with restriction enzymes. The fragment ends are then fluorescently tagged and stitched back together.For the last two decades, optical mapping has been prohibitively expensive, but recent advances in technology have reduced cost significantly.|$|R
500|$|The {{stage was}} {{converted}} {{to resemble the}} band's 1986 [...] "Reign in Pain" [...] tour, which featured the Slayer eagle and inverted crosses {{as part of the}} lighting rig. The stage was modified to absorb the [...] "blood" [...] and have it recirculate back down upon the band, which allowed for easier clean-up and lowered the chance of injury by slipping. [...] The DVD was recorded in 1.85:1 video, which caused <b>macro</b> blocking <b>errors</b> such as aliasing and a murky stage when fully lit, and the audio featured English Dolby Digital 5.1 and 2.0 stereo, with no subtitles.|$|R
40|$|Multi-component seismic data is {{required}} to obtain information on the angle-dependent P-P, P-S, S-P and S-S reflectivity of the subsurface. It is generally accepted that wave field decomposition should form part of the multi-component processing sequence, aiming at retrieving this reflectivity information. However, the question whether decomposition should take place before or after downward extrapolation {{has not yet been}} unambiguously solved. In this paper we argue that, although both procedures are theoretically equivalent, decomposition before downward extrapolation has a number of practical advantages, one of the reasons being the robustness with respect to <b>macro</b> model <b>errors...</b>|$|R
5000|$|The {{stage was}} {{converted}} {{to resemble the}} band's 1986 [...] "Reign in Pain" [...] tour, which featured the Slayer eagle and inverted crosses {{as part of the}} lighting rig. The stage was modified to absorb the [...] "blood" [...] and have it recirculate back down upon the band, which allowed for easier clean-up and lowered the chance of injury by slipping. [...] The DVD was recorded in 1.85:1 video, which caused <b>macro</b> blocking <b>errors</b> such as aliasing and a murky stage when fully lit, and the audio featured English Dolby Digital 5.1 and 2.0 stereo, with no subtitles.|$|R
40|$|The {{widely used}} C {{preprocessor}} (CPP) {{is generally considered}} a source of difficulty for understanding and maintaining C/C++ programs. The main reason for this difficulty is CPP’s purely lexical semantics, i. e., its treatment of both input and output as token streams. This can easily lead to errors {{that are difficult to}} diagnose, and {{it has been estimated that}} up to 20 % of all macros are erroneous. To reduce such errors, more restrictive, replacement languages for CPP have been proposed to limit expanded macros to be valid C syntactic units. However, there is no practical tool that can effectively validate CPP macros in legacy applications. In this paper, we introduce a novel, general characterization of inconsistent macro usage as a strong indicator of <b>macro</b> <b>errors.</b> Our key insight is that all applications of the same macro should behave similarly. In particular, we map each macro call c in a source file f to c’s normalized syntactic constructs within the abstract syntax tree (AST) for f’s preprocessed source, and use syntactic similarity as the basis for comparing macro calls of the same macro definition. Utilizing this characterization, we have developed an efficient algorithm to statically validate macro usage in C/C++ programs. We have implemented the algorithm; evaluation results show that our tool is effective in detecting common macro-related errors and reports few false positives, making it a practical tool for validating macro usage...|$|R
40|$|In {{this work}} the authors {{deal with the}} problem {{regarding}} tolerance design of the journal – bearing kinematic joint, in which hydrodynamic lubrication conditions are realised. In particular, they propose a simplified numerical approach to analyse consequences of macro geometric variations on the performances of this kind of bearings. The final result is numerical formulation of new abacuses {{to be used in the}} design of such kinematic joints that show how the kinematic joint performances change when the functional feature has not the ideal shape. The designer can use the abacus starting from a performance objective and evaluating the maximum allowable <b>macro</b> geometric <b>error</b> (i. e. the tolerance value) that satisfies the functional requirements...|$|R
50|$|Javelin {{encourages}} viewing {{data and}} algorithms in various self-documenting ways, including simultaneous multiple synchronized views. For example, users can {{move through the}} connections between variables on a diagram while seeing the logical roots and branches of each variable. This {{is an example of}} what is perhaps its primary contribution—the concept of traceability of a users logic or model structure through its twelve views. Among its dynamically linked views were: diagram, formulas, table, chart, QuickGraph, worksheet, notes, <b>errors,</b> <b>macro,</b> graph. A complex model can be dissected and understood by others who had no role in its creation, and this remains unique even today.|$|R
40|$|The {{detonation}} velocity {{is one of}} the principal characteristics of energetic materials. Therefore, computer search for structures of materials with high {{detonation velocity}} is a very urgent problem. We formulated the following task : to elaborate a concept of computer design and subsequent screening of target structures with high detonation velocities. The present state of this concept and the operation of the program package are illustrated by computer generation of novel energetic compounds, including caged skeletons. Using some optimum gross formulas, we use methods of mathematical chemistry for computer generation of isomeric structural formulas, which may correspond to potentially interesting substances with high detonation velocity. Then, using different methods (mostly original) for estimating the properties of these compounds, we calculate their physicochemical characteristics and recommend some of them for synthesis. We also analyzed the possible sources of errors during calculation of some detonation characteristics and determined the possible error ranges for the calculated properties at the micro level (enthalpies of formation and molecular crystal densities). As a result, the total (<b>macro</b> level) <b>errors</b> in calculations of detonation parameters were determined...|$|R
40|$|This report {{gives an}} {{overview}} and motivates {{the design of}} a C++ framework for object recognition using channel-coded feature maps. The code was produced in connection to the work on my PhD thesis Channel-Coded Feature Maps for Object Recognition and Machine Learning. The package contains algorithms ranging from basic image processing routines to specific complex algorithms for creating channel-coded feature maps through piecewise polynomials. Much emphasis has been put in creating a flexible framework using virtual interfaces. This makes it easy e. g. ~to switch between different image primitives detectors or learning methods in an object recognizer. Some common design choices include an image class with a convenient but fast pixel access, a configurable assert <b>macro</b> for <b>error</b> handling and a common base class for object ownership management. The main computer vision algorithms are channel-coded feature maps (CCFMs) including their derivatives, single-sided colored lines, object detection using an abstract hypothesize-verify framework and tracking and pose estimation using locally weighted regression and CCFMs. The code is considered as having alpha status at best. It is available under the GNU General Public License (GPL) and is mainly intended for future research on the subject...|$|R
40|$|A pair of macros was {{developed}} to enhance user analytic process management in interactive SAS sessions. %_mStart(), the preprocessing macro, creates a relative path system independent from hard-coded absolute paths and is executable without program modifications when copied to a different location. In addition, %_mStart() can back up SAS source code automatically while the program is submitted for executing. The backed-up SAS program will have date and time stamp added to the original file name, serving the purpose of version control which is not directly available in a typical analytical SAS environment. Following the end of execution of the program, a post-processing <b>macro</b> %_mEnd() detects <b>error</b> and warning messages in a SAS log window and displays them in a pop-up window...|$|R
40|$|In {{this paper}} we propose a {{description}} language for specifying motions for humanoid robots and for allowing humanoid robots to acquire motor skills. Locomotion greatly increases our ability to interact with our environments, which in turn increases our mental abilities. This principle also applies to humanoid robots. However, there are great difficulties to specify humanoid motions and to represent motor skills, which in most cases require four-dimensional space representations. We propose a representation framework that includes the following attributes: motion description layers, egocentric reference system, progressive quantized refinement, and automatic constraint satisfaction. We also outline strategies for acquiring new motor skills by learning from trial and <b>error,</b> <b>macro</b> approach, and programming. Then, we outline {{the development of a new}} humanoid motion description language called Cybele...|$|R
500|$|Yet another private {{joke was}} the hidden {{character}} Noob Saibot, who {{has appeared in}} various versions of the game starting with Mortal Kombat II. The character's name derived from two of the series' creators' surnames, Ed Boon and John Tobias, spelled backwards. In addition, a counter for ERMACS on the game's audits screen (ERMACS being short for <b>error</b> <b>macros),</b> was generally considered by some players to be {{a reference to a}} hidden character. The development team decided to turn the rumor into reality, introducing Ermac in Ultimate Mortal Kombat 3 as an unlockable secret character. The character Mokap, introduced in Mortal Kombat: Deadly Alliance, is a tribute to Carlos Pesina, who played Raiden in MK and MKII and has served as a motion capture actor for subsequent titles in the series.|$|R
40|$|We {{investigate}} (1, 0) -superconformal Toda theories {{based on}} simple Lie algebras {{and find that}} the classical integrability properties of the underlying bosonic theories do not survive. For several models based on algebras of low rank, we show explicitly {{that none of the}} conserved W-algebra generators can be generalized to the supersymmetric case. Using these results we deduce that at least one W-algebra generator fails to generalize in any model based on a classical Lie algebra. This argument involves a method for relating the bosonic Toda theories and their conserved currents within each classical series. We also scrutinize claims that the (1, 0) -superconformal models actually admit (1, 1) supersymmetry and find that they do not. Our results are consistent with the belief that all integrable Toda models with fermions arise from Lie superalgebras. Comment: plain TeX file, 11 pages output, <b>macros</b> included; minor <b>errors</b> corrected, extra references adde...|$|R
40|$|Abstract- This paper apply {{stenography}} algorithm {{in a video}} stream. In previous stenography methods spatial or transformeddomain is {{used for}} data hiding. But in proposed method we are using compressed video to hide the secrete data; we use motion vectors to encode and reconstruct both the forward predictive (P) -frame and bidirectional (B) -frames in compress video. Motion vectors are calculated using <b>macro</b> block prediction <b>error,</b> which {{is different from the}} approaches based on the motion vectorattributes such as the magnitude and phase angle, etc. A greedy adaptive threshold is searched for every frame to achieve robustnesswhile maintaining a low prediction error level. Secrete Message bits are replace with the least significant bits of motion vectors. Thisalgorithm was tested on different types of videos. Based on the aforementioned criteria, the proposed method is found to performwell and is compared to a motion vector attributebased method from the literature...|$|R
40|$|Abstract — There {{are many}} {{researches}} {{that have been}} proposed for hiding the data into digital videos. Most of those schemes uses the attributes of motion vectors like amplitude, phase angle etc. This paper deals with hiding data in compressed video where motion vectors are used to encode and reconstruct both the forward predictive (P-) frame and bidirectional (B-) frames in the compressed video. The subset of motion vectors are chosen based their associated <b>macro</b> block prediction <b>error.</b> Pertinent features will be collected from the motion in between the frames as {{in the form of}} the vectors in association with macro blocks and depending on the motion message is going to be hidden. To achieve the robustness a adaptive threshold is searched and low predictive error level is retained. Secret message bits are hidden in Least significant bit of both components of candidate motion vector. The evaluation will be based on two criteria: minimum distortion to reconstructed video and minimum overhead on compressed video size...|$|R
40|$|We {{develop the}} general {{formalism}} {{for the study}} of neutrino propagation in presence of stochastic media. This formalism allows the systematic derivation of evolution equations for averaged quantities as survival probabilities and higher order distribution moments. The formalism applies equally to any finite dimensional Schroedinger equation in presence of a stochastic external force. New integro-differential equations valid for finite correlated processes are obtained for the first time. For the particular case of exponentially correlated processes a second order ordinary equation is obtained. As a consequence, the Redfield equation valid for Gaussian delta-correlated noise is rederived in a simple way. The formalism, together with the quantum correlation theorem is applied to the computation of higher moments and correlation functions of practical interest in forthcoming high precision neutrino experiments. It is shown that equal and not equal time correlators follow similar differential equations. Comment: 12 pags., Latex. 1 fig., ps. epsfig <b>macro.</b> minor typo <b>errors</b> correcte...|$|R
40|$|There {{are many}} {{researches}} {{that have been}} proposed for hiding the data into digital videos. Most of those schemes uses the attributes of motion vectors like amplitude, phase angle etc. This paper deals with hiding data in compressed video where motion vectors are used to encode and reconstruct both the forward predictive (P-) frame and bidirectional (B-) frames in the compressed video. The subset of motion vectors are chosen based their associated <b>macro</b> block prediction <b>error.</b> Pertinent features will be collected from the motion in between the frames as {{in the form of}} the vectors in association with macro blocks and depending on the motion message is going to be hidden. To achieve the robustness a adaptive threshold is searched and low predictive error level is retained. Secret message bits are hidden in Least significant bit of both components of candidate motion vector. The evaluation will be based on two criteria: minimum distortion to reconstructed video and minimum overhead on compressed video size...|$|R
40|$|Abstract: [...] In {{this paper}} we target the motion vectors used to encode and {{reconstruct}} both the forward predictive (P) -frame and bidirectional (B) -frames in compressed video. The choice of candidate subset of these motion vectors {{are based on}} their associated <b>macro</b> block prediction <b>error,</b> which {{is different from the}} approaches based on the motion vector attributes such as the magnitude and phase angle, etc. A greedy adaptive threshold is searched for every frame to achieve robustness while maintaining a low prediction error level. The secret message bit stream is embedded in the least significant bit of both components of the candidate motion vectors. The method is implemented and tested for hiding data in natural sequences of multiple groups of pictures and the results are evaluated. The evaluation is based on two criteria: minimum distortion to the reconstructed video and minimum overhead on the compressed video size. Based on the aforementioned criteria, the proposed method is found to perform well and is compared to a motion vector attribute-based method from the literature...|$|R
50|$|While {{many games}} {{have been subject}} to urban legends about secret {{features}} and unlockable content, these kinds of myths were particularly rampant among the dedicated fan community of the Mortal Kombat series. The game's creators did little to dispel the rumors, some of which were even made reality in subsequent games. The most notable of these myths spawned from an audit-menu listing titled ERMACS (<b>error</b> <b>macro)</b> on the game's diagnostics screen, which led players to believe that another secret character, a red ninja named Ermac, existed in the game, followed by reports of a glitch where the sprites of either Scorpion or Reptile would flash red during gameplay. While both rumors were false, they proved relevant enough that Midway included the character as a playable in Ultimate Mortal Kombat 3 and subsequent titles. According to UGO.com, it was also rumored that the SNES version of the game had a cheat code to re-enable the blood and gore, but such a code existed only on the Mega Drive/Genesis version.|$|R
40|$|Multiply gravitationally lensed {{objects with}} known time delays {{can lead to}} direct {{determinations}} of H_ 0 independent of the distance ladder if the mass distribution of the lens is known. Currently, the double QSO 0957 + 561 is the only lensed object with a precisely known time delay. The largest remaining source of systematic error in the H_ 0 determination results from uncertainty in the mass distribution of the lens which is comprised of a massive galaxy (G 1) and the cluster in which it resides. We have obtained V-band CCD images from CFHT {{in order to measure}} the mass distribution in the cluster from its gravitional distorting effect on the appearance of background galaxes. We use this data to constuct a two-dimensional mass map of the field. A mass peak is detected at the 4. 5 σ level, offset from, but consistent with, the position of G 1. Simple tests reveal no significant substructure and the mass distribution is consistent with a spherical cluster. The peak in the number density map of bright galaxies is offset from G 1 similarly to the mass peak. We constructed an azimuthally averaged mass profile centered on G 1 out to 2 (400 h^- 1 kpc). It is consistent with an isothermal mass distribution with a small core (r_c ≈ 5 = 17 h^- 1 kpc). The inferred mass within 1 Mpc is consistent with the dynamical mass estimate but 2 σ higher than the upper limits from a ROSAT X-ray study. We discuss implications for H_ 0 in a future paper. Comment: LaTeX, aas version 4 <b>macros.</b> Calibration <b>error</b> in original led to overestimate of cluster mass. Seven out of twelve figures included. Complete paper is available at: [URL]...|$|R
40|$|The C {{language}} {{is among the}} most widely used in the world, particularly for critical infrastructure software. C programs depend upon macros processed using the C preprocessor, but these macros are difficult to analyze and are often error-prone [4]. Existing tools that analyze and transform C source code have rudimentary support for the preprocessor, leading to obscure error messages and difficulty refactoring. We present a three part solution: (1) a replacement macro language, ASTEC, that addresses the most important important deficiencies of the preprocessor and that eliminates many of the errors it introduces; (2) a translator, Macroscope, that converts existing code into ASTEC semi-automatically; and (3), an ASTEC-aware refactoring tool that handles preprocessor constructs naturally. ASTEC’s primary benefits are its analyzability and its refactorability. We present several refactorings that are enabled by ASTEC. Additionally, ASTEC eliminates many of the sources of errors that can plague C preprocessor macros; Ernst et al. [4] estimate that more than 20 % of <b>macros</b> may contain <b>errors.</b> In this paper, we describe our translation and refactoring tools and evaluate them on a suite of programs including OpenSSH and the Linux kernel...|$|R
40|$|Abstract This paper {{deals with}} data hiding in {{compressed}} video. Unlike data hiding in images and raw video which operates on the images {{themselves in the}} spatial or transformed domain which are vulnerable to steganalysis, we target the motion vectors used to encode and reconstruct both the forward predictive (P) -frame and bidirectional (B) -frames in compressed video. The choice of candidate subset of these motion vectors are based on their associated <b>macro</b> block prediction <b>error,</b> which {{is different from the}} approaches based on the motion vector attributes such as the magnitude and phase angle, etc. A greedy adaptive threshold is searched for every frame to achieve robustness while maintaining a low prediction error level. The secret message bit stream is embedded in the least significant bit of both components of the candidate motion vectors. The method is implemented and tested for hiding data in natural sequences of multiple groups of pictures and the results are evaluated. The evaluation is based on two criteria: minimum distortion to the reconstructed video and minimum overhead on the compressed video size. Based on the aforementioned criteria, the proposed method is found to perform well and is compared to a motion vector attribute-based method from the literature. Keywords-Steganalysis, Candidate motion vectors (CMV), Huffman Coding, Quantization index modulation (QIM). I...|$|R
40|$|Abstract: Secret {{communication}} {{using the}} Compressed Video file as cover medium and data hiding in compressed video. Unlike data hiding in images and raw video which operates on the images {{themselves in the}} spatial or transformed domain which are vulnerable to steganalysis, we target the motion vectors used to encode and reconstruct both the forward predictive (P) -frame and bidirectional (B) -frames in compressed video. The choice of candidate subset of these motion vectors are based on their associated <b>macro</b> block prediction <b>error,</b> which {{is different from the}} approaches based on the motion vector attributes such as the magnitude and phase angle, etc. A greedy adaptive threshold is searched for every frame to achieve robustness while maintaining a low prediction error level. The secret message bit stream is embedded in the least significant bit of both components of the candidate motion vectors. The method is implemented and tested for hiding data in natural sequences of multiple groups of pictures and the results are evaluated. The evaluation is based on two criteria: minimum distortion to the reconstructed video and minimum overhead on the compressed video size. Based on the aforementioned criteria, the proposed method is found to perform well and is compared to a motion vector attribute-based method from the literature...|$|R
40|$|This proven {{textbook}} guides {{readers to}} {{a thorough understanding}} of the theory and design of operational amplifiers (OpAmps). The core of the book presents systematically the design of operational amplifiers, classifying them into a periodic system of nine main overall configurations, ranging from one gain stage up to four or more stages. This division enables circuit designers to recognize quickly, understand, and choose optimal configurations. Characterization of operational amplifiers is given by <b>macro</b> models and <b>error</b> matrices, together with measurement techniques for their parameters. Definitions are given for four types of operational amplifiers depending on the grounding of their input and output ports. Many famous designs are evaluated in depth, using a carefully structured approach enhanced by numerous figures. In order to reinforce the concepts introduced and facilitate self-evaluation of design skills, the author includes problems with detailed solutions, as well as simulation exercises. Provides textbook coverage of the theory and design of operational amplifiers; Discusses low-voltage rail-to-rail input and output stages for design of low-power OpAmps; Presents frequency compensation techniques for all nine OpAmp configurations and compensation techniques for amplifiers with high capacitive loads; Includes design of µV-offset operational amplifiers and precision instrumentation amplifiers by applying chopping, auto-zeroing, and dynamic element-matching techniques. Provides beyond the rails CM input voltage ranges to OpAmps and InstAmps by the design of capacitive-coupled chopper input stages...|$|R
40|$|Nowadays, Data {{transmission}} over {{internet is}} not at all secure and they are vulnerable to various attacks. So in order to prevent our valuable information we require special security measures. One of the methods for secure data transmission is cryptography where the information is protected by transforming it into unintelligible format. But we can ensure more security if the existences of the hidden message itself is concealed. Steganography is one such method in which the message may be hidden in image, audio, video, text etc. In this paper a sequential hybrid method to integrate cryptography and steganography in order to provide more than one level security is being proposed. And it is proposed to use video steganography in order to store high capacity data. The secret information is encrypted using the standard encryption algorithm AES. Then the encrypted message is embedded in the motion vector of the compressed video. The motion vector for embedding data is selected based on their associated <b>macro</b> block prediction <b>error.</b> Then the secret message is embedded in both horizontal and vertical component of the motion vector. At the same time in each GOP, the control information for data extraction will be embedded in I frame. At the receiving end the control information in I frame should be extracted first then the encrypted message can be extracted from P and B frames accordingly. The original message is retrieved back by applying decryption algorithm. General Terms cryptography, steganography, motion vector, prediction erro...|$|R
40|$|Earnings {{mobility}} {{has been}} studied both at the macro level (how much {{of a certain kind}} of mobility is there in the economy?) and at the micro level (what are the correlates of change in income or position?). Many empirical mobility studies provide estimates of the amount of mobility in a country over time and the correlates of individual mobility within the income distribution. While measurement error is recognized as potentially important at both these levels, very {{little is known about the}} degree to which earnings mobility estimates are affected by measurement error. In this paper, we use a new dataset that contains individually reported total annual labor earnings from the Survey of Income and Program Participation (SIPP) linked to employer-reported total annual labor earnings from the Social Security Administration’s Detailed Earnings Record (DER; these are taken directly from Box 1 on theW- 2 form and are not capped by FICA) to compare micro and macro earnings mobility estimates for the U. S. during the 1990 s using the two different earnings measures. We ask how much difference it makes to mobility estimates to use administrative-based earnings rather than survey-based earnings, and we obtain two major findings. Qualitatively, we find that the results are similar but not identical when administrative-based earnings are used rather than survey-based earnings. Quantitatively, we find that magnitudes are often very different when administrative-based earnings are used rather than survey-based earnings. The administrative-based results are neither systematically larger nor systematically smaller than the survey-based ones. earnings mobility, measurement <b>error,</b> <b>macro</b> mobility, micro mobility. ...|$|R
40|$|International audienceWe study, in this paper, {{the impact}} of {{imperfect}} small cell positioning with respect to traffic hotspots in cellular networks. In order to derive the throughput distribution in macro and small cells, we firstly perform static level analysis of the system considering a non-uniform distribution of user locations. We secondly introduce {{the dynamics of the}} system, characterized by random arrivals and departures of users after a finite service duration, with the service rates and distribution of radio conditions outfitted from {{the first part of the}} work. When dealing with the dynamics of the system, macro and small cells are modeled by multi-class processor sharing queues. Macro and small cells are assumed to be operating in the same bandwidth. Consequently, they are coupled due to the mutual interferences generated by each cell to the other. We derive several performance metrics such as the mean flow throughput and the gain, if any, generated from deploying small cells to manage traffic hotspots. Our results show that in case the hotspot is near the macro BS (Base Station), even a perfect positioning of the small cell will not yield improved performance due to the high interference experienced at macro and small cell users. However, in case the hotspot is located far enough from the <b>macro</b> BS, performing <b>errors</b> in small cell positioning is tolerated (since related results show positive gains) and it is still beneficial in offloading traffic from the congested macrocell. The best performance metrics depend also on several other important factors such as the users' arrival intensity, the capacity of the cell and the size of the traffic hotspo...|$|R
40|$|We study, in this paper, {{the impact}} of {{imperfect}} small cell positioning with respect to traffic hotspots in cellular networks. In order to derive the throughput distribution in macro and small cells, we firstly perform static level analysis of the system considering a non-uniform distribution of user locations. We secondly introduce {{the dynamics of the}} system, characterized by random arrivals and departures of users after a finite service duration, with the service rates and distribution of radio conditions outfitted from {{the first part of the}} work. When dealing with the dynamics of the system, macro and small cells are modeled by multi-class processor sharing queues. Macro and small cells are assumed to be operating in the same bandwidth. Consequently, they are coupled due to the mutual interferences generated by each cell to the other. We derive several performance metrics such as the mean flow throughput and the gain, if any, generated from deploying small cells to manage traffic hotspots. Our results show that in case the hotspot is near the macro BS (Base Station), even a perfect positioning of the small cell will not yield improved performance due to the high interference experienced at macro and small cell users. However, in case the hotspot is located far enough from the <b>macro</b> BS, performing <b>errors</b> in small cell positioning is tolerated (since related results show positive gains) and it is still beneficial in offloading traffic from the congested macrocell. The best performance metrics depend also on several other important factors such as the users' arrival intensity, the capacity of the cell and the size of the traffic hotspot. Comment: This paper is already published in IEEE Transactions on Vehicular Technology 201...|$|R
40|$|The {{purpose of}} this project was to write a Superwylbur macro, called D 50 TD 80, for the Computing Information center at Northern Illinois University. The project was {{completed}} using Superwylbur, an interactive computing environment which enables access to the University’s mainframe computer. Superwylbur macro programming combines Superwylbur commands along {{with a set of}} special instructions for branching and decision making. The D 50 TD 80 macro was designed to assist users in moving data sets from the current 3350 disk packs to the newly installed, more efficient 3380 disk packs. This macro allows the user three options: 1) To create a partitioned data set from sequential data sets, and place on a 3380 disk pack, 2) To move an entire partitioned data set to a 3380 disk pack, and 3) To move a sequential data set to a 3380 disk pack. Options 2 and 3 are executed by calling modified versions of existing macros. Changes to these <b>macros</b> included additional <b>error</b> checking and more informative prompts. The first option begins by asking the user for the name and location of the partitioned data set that they wish to create. The user is next prompted for the names of the sequential data sets to be saved as members of the partitioned data set. The requested data sets are then placed into the specified PDS. The macro was coded to be user friendly by anticipating a variety of user errors, and making the prompts and error messages friendly, yet informative. The method followed to complete the end product was basically a “hands on” approach. Using The Superwylbur Macro Programming Manual as a primary reference, a simple macro was written which can assist a user in uploading and downloading files from Superwylbur. After some practice with the language, the D 50 TD 80 macro was coded. This macro will be available for public use in the Spring of 1989. B. S. (Bachelor of Science...|$|R

