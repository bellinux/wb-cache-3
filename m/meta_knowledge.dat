41|15|Public
40|$|For many tasks, {{such as the}} {{integration}} of knowledge bases in the semantic web, one must not only handle the knowledge itself, but also characterizations of this knowledge, e. g. : (i) where did a knowledge item come from (i. e. provenance), (ii) what level of trust can be assigned to a knowledge item, or (iii) what degree of certainty is associated with it. We refer to all such kinds of characterizations as <b>meta</b> <b>knowledge.</b> Approaches for providing <b>meta</b> <b>knowledge</b> for query answers in relational databases and RDF repositories, based on algebraic operations, exist. As query answering in description logics in general does not boil down to algebraic evaluation of tree shaped query models, these formalizations do not easily carry over. In this {{paper we propose a}} formalization of <b>meta</b> <b>knowledge,</b> which is still algebraic, but allows for the computation of <b>meta</b> <b>knowledge</b> of inferred knowledge in description logics, including reasoning with conflicting and incomplete <b>meta</b> <b>knowledge.</b> We use pinpointing to come up with <b>meta</b> <b>knowledge</b> formulas for description logics, which then can be evaluated algebraically. We describe and evaluate our prototypical implementation...|$|E
40|$|The Semantic Web {{is based}} on {{accessing}} and reusing RDF data from many different sources, which one may assign different levels of authority and credibility. Existing Semantic Web query languages, like SPARQL, have targeted the retrieval, combination and reuse of facts, but have so far ignored all aspects of <b>meta</b> <b>knowledge,</b> such as origins, authorship, recency or certainty of data, to name but a few. In this paper, we present an original, generic, formalized and implemented approach for managing many dimensions of <b>meta</b> <b>knowledge,</b> like source, authorship, certainty and others. The approach re-uses existing RDF modeling possibilities in order to represent <b>meta</b> <b>knowledge.</b> Then, it extends SPARQL query processing {{in such a way}} that given a SPARQL query for data, one may request <b>meta</b> <b>knowledge</b> without modifying the original query. Thus, our approach achieves highly flexible and automatically coordinated querying for data and <b>meta</b> <b>knowledge,</b> while completely separating the two areas of concern...|$|E
40|$|<b>Meta</b> <b>{{knowledge}},</b> i. e. {{knowledge about}} knowledge, {{is important in}} oder to judge the validity or appropriateness of the knowledge. We present a generic framework for managing and querying <b>meta</b> <b>knowledge</b> considering the most widespread paradigms for knowledge representation, i. e. RDF, Logic Programming, and Description Logics. ...|$|E
5000|$|Defined {{architecture}} on SWCI1. Semantic-oriented cleansing2. Semantic-oriented <b>meta</b> management3. Clinical(Medical) <b>knowledge</b> basement4. Semantic-oriented user intelligence ...|$|R
40|$|In a {{manufacturing}} environment, engineers and scientist often need to access and reason about general types of manufacturing methods without complex specifications and numbers. They {{may also want}} to disseminate the general process plans without sensitive details. This general knowledge is typically contained in the structure of knowledge bases which is composed of classes and can be referred to as <b>Meta</b> Level <b>Knowledge.</b> Typical manufacturing knowledge bases are designed for populated knowledge only and do not support capturing and reasoning over <b>Meta</b> level <b>knowledge.</b> Modelling to capture <b>Meta</b> level <b>knowledge</b> has been investigated in broader software community but the same is not true in manufacturing domain particularly in process planning. Moreover, an exploration to capture and reason about such knowledge in formal manufacturing ontologies has not been conducted. In this paper, a new formal ontology is proposed to capture meta level production methods. The ontology makes use of “powertypes” and “clabjects” to treat classes as objects. Meta level production methods can be captured using the proposed ontology. Manufacturability of features within a part family {{can be found at the}} meta level. Various tests are conducted to examine the ability to access, infer and reason about the meta level production methods to show the effectiveness of proposed ontological model...|$|R
40|$|This {{deliverable}} is {{the final}} version of the OTK methodology. Core contributions of this document are: (i) An overview of the On-To-Knowledge building blocks and their relationships, (ii) a methodology for introducing and maintaining ontology based knowledge management solutions into enterprises with a focus on Knowledge Processes and <b>Knowledge</b> <b>Meta</b> Processes and, last but not least, (iii) the illustration of process steps by examples and lessons learned derived from applying the OTK tool suite in the OTK case studies according to the methodology...|$|R
40|$|The {{integration}} of knowledge from heterogeneous infor-mation sources and applications does not only require the conceptual mapping of information structures, {{but it also}} requires the treatment of semantic <b>meta</b> <b>knowledge</b> (i. e. knowledge about knowledge) in a generic manner. We de-scribe a generic mechanism for (i) modeling and (ii) query-ing semantic <b>meta</b> <b>knowledge</b> {{in the context of}} RDF repos-itories. We substantiate our approach by use cases from a project about multi-modal information integration from dif-ferent information sources...|$|E
40|$|In {{recent years}} {{feedback}} approaches {{have been used}} in relating low-level image features with concepts to overcome the subjective nature of the human image interpretation. Generally, in these systems when the user starts with a new query, the entire prior experience of the system is lost. In this paper, we address the problem of incorporating prior experience of the retrieval system to improve the performance on future queries. We propose a semi-supervised fuzzy clustering method to learn class distribution (<b>meta</b> <b>knowledge)</b> in the sense of high-level concepts from retrieval experience. Using fuzzy rules, we incorporate the <b>meta</b> <b>knowledge</b> into a probabilistic feature relevance feedback approach to improve the retrieval performance. Results on synthetic and real databases show that our approach provides better retrieval precision compared to the case when no retrieval experience is used. r 200...|$|E
40|$|Abstract. Meta triples provide {{additional}} information about individual triples, such as the source, the occuring time or place, or the certainty. Integrating such meta triples into semantic knowledge bases would enable the querying and reasoning mechanisms {{to be aware of}} provenance, time, location, or certainty of triples. However, an efficient RDF representation for such <b>meta</b> <b>knowledge</b> of triples remains challenging. The existing reification approach allows such <b>meta</b> <b>knowledge</b> of RDF triples to be expressed using RDF by two steps. The first step is representing the triple by a Statement instance which has subject, predicate, and object indicated separately in three different triples. The second step is creating assertions about that instance as if it is a statement. While reification is simple and intuitive, this approach does not have formal semantics and is not commonly used in practice as described in the RDF Primer. In this paper, we propose a novel approach called Singleton Property for representing meta triples and provide a formal semantics for it. We explain how this singleton property approach fits well with the existing syntax and formal semantics of RDF, and the syntax of SPARQL query language. We also demonstrate the use of singleton property in the representation and querying of <b>meta</b> <b>knowledge</b> in two examples of Semantic Web knowledge bases: YAGO 2 and BKR. This approach, which is also simple and intuitive, can be easily adopted for representing and querying meta triples in other knowledge bases. draft...|$|E
40|$|National audienceCognitive Agent {{communication}} {{is a research}} field in full development. We propose here an extension and an implementation of the STROBE model, which regards the Agents as Scheme interpreters. These Agents are able to interpret messages in a dedicated environment including an interpreter that learns from the current conversation. These interpreters evolve dynamically, progressively with the conversations, and thus represent evolving <b>meta</b> level Agent <b>knowledge.</b> We illustrate this theoretical model by a “teacher-student” dialogue experimentation, where an Agent learns a new performative {{at the completion of}} the conversation. Details of the implementation are not provided here, but are available...|$|R
40|$|Summary. In {{this chapter}} {{we present a}} {{methodology}} for introducing and maintaining ontology based knowledge management applications into enterprises {{with a focus on}} Knowledge Processes and <b>Knowledge</b> <b>Meta</b> Processes. While the former process circles around the usage of ontologies, the latter process guides their initial set up. We illustrate our methodology by an example from a case study on skills management. The methodology serves as a scaffold for Part B “Ontology Engineering” of the handbook. It shows where more specific concerns of ontology engineering find their place and how they are related in the overall process. ...|$|R
40|$|Unlike other revolutions, the {{information}} revolution is not routed in ideology but rather science and technological developments. The episteme of science has an historical legacy {{that is well}} documented by contested and often conflicting accounts. In this paper a thesis for the dis-aggregation of scientific <b>knowledge</b> (<b>meta</b> narratives) is adopted to tease out a framework for contemporary analysis of social spaces and the self. The sub-text is a testing of knowledge acquisition, validation, and representation claims, against the inferences, explanations, and uncertainty beliefs elaborated in recent Textbooks used {{in the teaching of}} information systems. The problem is the present. 3 Knowledge Games and the Information Revolutio...|$|R
40|$|Many ILP systems, such as GOLEM, FOIL, and MIS, take {{advantage}} of user supplied meta-knowledge to restrict the hypothesis space. This meta-knowledge {{can be in the}} form of type information about arguments in the predicate being learned, or it can be information about whether a certain argument in the predicate is functionally dependent on the other arguments (supplied as mode information). This <b>meta</b> <b>knowledge</b> is explicitly supplied to an ILP system in addition to the data. The present paper argues that in many cases the <b>meta</b> <b>knowledge</b> can be extracted directly from the raw data. Three algorithms are presented that learn type, mode, and symmetric meta-knowledge from data. These algorithms can be incorporated in existing ILP systems {{in the form of a}} preprocessor that obviates the need for a user to explicitly provide this information. In many cases, the algorithms can extract metaknowledge that the user is either unaware of, but which information can be used by the ILP system to restrict [...] ...|$|E
40|$|Large {{information}} spaces {{such as the}} WWW {{face the}} problem of how to maintain data warehouses #views# de#ned over Information Sources #ISs# whenever capabilities of these ISs change. Wehave developed a novel solution approach to address this problem, called the Evolvable View Environment#EVE#. In EVE, knowledge of both the capabilities of as well as #partial# containment relationships between ISs is collected in a <b>Meta</b> <b>Knowledge</b> Base #MKB#. The accuracy and consistency of the MKB has great importance to the EVE framework, because the MKB serves as an information pool enabling EVE to #nd alternative replacements for components of views that have become affected by IS capabilitychanges. In this paper, we describe the <b>meta</b> <b>knowledge</b> management problem and focus on issues related to the MKB evolution process. The contributions of this paper are threefold. In this paper, we #rst formally de#ne consistency criteria for the MKB evolution process. Second, we use PC constraintevolution to demons [...] ...|$|E
40|$|Statements about RDF statements, or meta triples, provide {{additional}} information about individual triples, such as the source, the occurring time or place, or the certainty. Integrating such meta triples into semantic knowledge bases would enable the querying and reasoning mechanisms {{to be aware of}} provenance, time, location, or certainty of triples. However, an efficient RDF representation for such <b>meta</b> <b>knowledge</b> of triples remains challenging. The existing standard reification approach allows such <b>meta</b> <b>knowledge</b> of RDF triples to be expressed using RDF by two steps. The first step is representing the triple by a Statement instance which has subject, predicate, and object indicated separately in three different triples. The second step is creating assertions about that instance as if it is a statement. While reification is simple and intuitive, this approach does not have formal semantics and is not commonly used in practice as described in the RDF Primer. In this paper, we propose a novel approach called Singleton Property for representing statements about statements and provide a formal semantics for it. We explain how this singleton property approach fits well with the existing syntax and formal semantics of RDF, and the syntax of SPARQL query language. We also demonstrate the use of singleton property in the representation and querying of <b>meta</b> <b>knowledge</b> in two examples of Semantic Web knowledge bases: YAGO 2 and BKR. Our experiments on the BKR show that the singleton property approach gives a decent performance in terms of number of triples, query length and query execution time compared to existing approaches. This approach, which is also simple and intuitive, can be easily adopted for representing and querying statements about statements in other knowledge bases...|$|E
40|$|The {{search for}} the common {{features}} of “good language learners” has obsessed scholars such as Naiman et al. (1978), Rubin (1975), and Stevick (1989). Regarding those with good writing skill in particular, some (Angelova, 1999; Beare, 2000; Victori, 1995) list some features such as language proficiency, L 1 writing competence, use of cohesive devices, <b>meta</b> cognitive <b>knowledge</b> about the writing task. The {{purpose of this study}} was to find the cognitive and metacognitive strategies of a successful learner in writing skill (considering those suggested by Arndt, 1987; Wenden, 1991). Tina, a 27 year old language learner with a BS degree in architecture, was found the most suitable case based on the teacher`s observation of her good writing and the analysis of Oxford's (1990) strategy inventory for language learning (SILL) administered. The data collected from the observation of her writing, the think-aloud protocol and the interview showed that Tina made use of most of the cognitive and metacognitive strategies listed but there was no evidence of L 1 reliance in her L 2 writing. The data also revealed that she was highly good at using prefabricated phrases and sentences in her writing...|$|R
40|$|This paper {{presents}} {{the development of}} an ontology to represent financial headline news. This ontology is developed using the On-To-Knowledge methodology where {{the focus is on the}} design steps of the <b>Knowledge</b> <b>Meta</b> Process. This development is part of an ongoing project which aims to design a virtual stock market simulator based on multi-agent systems. The proposed ontology has 31 concepts and includes 201 attributes. The testing results conducted on reliable headline news show that 99 % of these headline news can be properly represented by the attributes of the right category in the ontology. Unreliable headline news characterized by news having uncertainties, incompleteness, ill-definition, or imprecision cannot be represented by the proposed ontology. Approaches for representing these unreliable headline news are discussed...|$|R
40|$|Networked {{multimedia}} presently revolutionises {{teaching as}} once did {{the invention of}} blackboard and chalk. Distribution and roll out of complex multimedia objects however suffers from significant efforts any author has to fulfil in production. This paper presents a prototypic implementation of a virtual marketplace for multimedia teaching objects, which by a simple user policy allows for trading knowledge material between teachers. Knowledge entities thereby are considered as adaptable, compound objects consisting of media data as well as annotations, background information and <b>meta</b> descriptors. Our <b>Knowledge</b> Marketplace {{is based on the}} more general Multimedia Information Repository MIR. The MIR system consists of an intelligent media database with Web-authoring tool and is based on a hypermedia data model of reusable object components. Further educational applications of our architecture are presented, as well. ...|$|R
40|$|Classification is {{a machine}} {{learning}} technique {{which is used}} to categorize the different input patterns into different classes. To select the best classifier for a given dataset is one of the critical issues in Classification. Using cross-validation approach, it is possible to apply candidate algorithms on a given dataset and best classifier is selected by considering various evaluation measures of Classification. But computational cost is significant. Meta Learning automates this process by acquiring knowledge in form of Meta-features and performance information of candidate algorithm on datasets and creates a <b>Meta</b> <b>Knowledge</b> Base. Once <b>Meta</b> <b>Knowledge</b> Base is generated, system uses k-Nearest Neighbor as a Meta Learner that identifies the most similar datasets to new dataset. But generation of Meta Example is a costly process due to a large number of candidate algorithms and datasets with different characteristics involved. So Active Learning is incorporated into Meta Learning System that reduces generation of Meta example {{and at the same time}} maintaining performance of candidate algorithms. Once the training phase is completed based on Active Meta Learning approach, ranking is provided based on Success Rate Ratio (SRR) method that considers accuracy as a performance evaluation measure...|$|E
40|$|Ample {{research}} is conducted on ICT, automation and robotics in agriculture and related environmental issues. ICT and Robotics innovations are rapidly emerging {{and have the}} ability to revolutionize future farming through their major impacts on productivity and profitability. Unfortunately human and financial resources and efforts are fragmented and limited. This {{led to the creation of}} the ICT-AGRI ERA-NET that provides a central structured framework. Its main objective is to strengthen and coordinate European research regarding ICT and robotics in agriculture. Besides the creation of the <b>Meta</b> <b>Knowledge</b> Base (MKB), a common European research agenda will be developed and common research calls are launched. The <b>Meta</b> <b>Knowledge</b> Base ([URL] is attempting to map all relevant research and development within the selected research area. To accomplish the mapping, two types of information are collected: research profiles and research postings. To organize the postings, a three-dimensional task-technology oriented framework was designed. The results indicated that the three axes: task, technology and scope seemed insufficient to describe the whole research area. Therefore, an improved framework was developed. By extending the task-technology oriented framework with a process-control&# 8211;information system, a useful framework was designed...|$|E
40|$|In an authorship {{verification}} problem one {{is given}} writ-ing examples from an author A, {{and one is}} asked to de-termine whether or not each text in fact was written by A. In a more general form of the authorship verification prob-lem one is given a single document d only, {{and the question is}} whether or not d contains sections from other authors. The heart of authorship verification is the quantization of an author’s writing style along with an outlier analysis to identify anomalies. Human readers are well-versed in de-tecting such spurious sections since they combine a highly-developed sense for wording with context-dependent <b>meta</b> <b>knowledge</b> in their analysis. The intention of this paper is to compile an overview of the algorithmic building blocks for authorship verifica-tion. In particular, we introduce authorship verification problems as decision problems, discuss possibilities for the use of <b>meta</b> <b>knowledge,</b> and apply meta analysis to post-process unreliable style analysis results. Our meta analysis combines a confidence-based majority decision with the un-masking approach of Koppel and Schler. With this strategy we can improve the analysis quality in our experiments by 33 % in terms of the F-measure...|$|E
40|$|The {{purpose of}} this paper is to {{describe}} a computational model for legal reasoning in criminal law (i. e. trial rea-soning). This logic-programming based model contains seven key components: facts of a new case, old cases, domain <b>knowledge,</b> <b>meta</b> rules, similarity matching re-lations, various implications, and two explicit agents, the plaintiff and the defendant, with opposing goals and reasoning strategies. The argumentation process in this model can be likened to a two-agent game. One agent puts forward an argument. The other agent recognizes the situation, generates candidates to refute the claim, and selects the best one for the next move. The game ends when any one agent can no longer make a move. Certain debate strategies of this model are illustrated in this paper with examples. In addition, the con~puta-tional model presented has been used in the design and development of HELICII- a parallel knowledge-based system for trial reasoning. ...|$|R
40|$|Actionable Analytics {{is one of}} ten {{trends in}} {{information}} technology today. Key elements of actionable analytics are purposefulness and information analysis scenarios. Realization of information analysis scenarios mandates {{the development of a}} general tool for describing the different forms of analytical information system elements interactions. Such a tool provides an implementation of actionable analytics. The usage of the scenario-goal approach to actionable analytics objects analysis gives the possibility to develop such a tool. According to the scenario-goal approach to actionable analytics objects analysis, the process of the information analytical system development is considered as the sequence of building its models. At the first stage, a conceptual model of the information analytical system is built by using a scenario-goal approach. This model describes general notions of functional and information components: the goal, scenario, <b>meta</b> description, and <b>knowledge.</b> The construction of a conceptual model is illustrated using an analysis of key factors of improving {{the quality of life in}} the region as an example. ??????????? ????????? ?? ??????????? ???? ?????? ? ??????? ???????? ?????????????? ?? ????? ?????????????? ??????????. ????????? ?????????? ??????????? ????????? ???????? ?????????????????? ? ???????? ??????? ??????????. ????????????? ?????????? ????????? ??????? ?????????? ???????????? ?????????? ??????????? ??????????? ??? ???????? ????????? ???? ?????????????? ????????? ?????????????-????????????? ??????? ? ??????????? ??????????? ?????????? ??????????? ?????????. ????????????? ????????-???????? ??????? ? ??????? ???????? ??????????? ????????? ???? ??????????? ??????????? ????? ??????????. ??????? ?????????? ?????????????-????????????? ??????? ??????????????? ??? ?????????????????? ?????????? ?? ??????? ??? ?????????? ????????-???????? ??????? ? ??????? ???????? ?????????. ? ???????? ?????? ?????? ???????? ?????????????? ?????? ?????????????-????????????? ??????? ? ?????????????? ????????-???????? ???????, ??????? ?????????? ???????? ??????? ?????????????? ? ?????????????? ???????????? ??????????? ?????????: ????, ????????, ????????????, ??????. ?????????? ?????????????? ?????? ??????????? ?? ??????? ??????? ???????? ???????? ????????? ???????? ????? ? ???????...|$|R
40|$|The {{objective}} of this thesis is {{the design of a}} spatial theory for GIS (consisting of representation, meta data, and transformations) that allows complete integration of data sets that differ in resolution and format. The scope is limited to a discrete view of geographic reality similar to "area-class maps", "categorical coverages", and "nominal fields". The spatial theory consists of representations of resolution-limited spatial <b>knowledge,</b> <b>meta</b> data that describe the knowledge content of representations, and transformations between representations of different type, resolution, format (raster or vector). The spatial theory addresses the following problems: (1) What limitations does limited resolution impose on spatial knowledge that is represented in GIS? (2) How can such resolution-limited knowledge be represented in a way that keeps precise track of the contained spatial knowledge and its limitations? (3) How can the same spatial knowledge be represented in different formats such as raster and vector? (4) How can spatial knowledge be transformed to other representation types, levels of resolution, and formats? The viability of the proposed spatial theory is shown by demonstrating the implementability of representations and transformations. The practical applicability of the proposed resolution concept is shown by relating it to the resolution of sensors and by showing that resolution-limited representations can always be visualized within the limitations of display media...|$|R
40|$|Description Logics (DLs) are {{successful}} knowledge representation formalisms, {{which can be}} used to represent the terminological knowledge of an application domain in a structured and formally well-understood way. They are employed in various application domains, such as natural language processing, configuration, and databases. Sf. In this work we try to use this performance of DL systems to describe a <b>Meta</b> <b>knowledge</b> base defining the classes of the word in Arabic language and relation between them. This can be useful for syntactic categorisation of sentences which is very important for automatic language processing...|$|E
40|$|In {{a lot of}} {{applications}} experts have apart from the pure definitional and assertional knowledge some <b>meta</b> <b>knowledge</b> about what they prefer {{to be in their}} intended worlds. This paper presents a framework for dealing with preferences embedded in ID-Logic, a logic with inductive definitions as the basic concept combined with first order logic sentences. In the obtained framework preference rules are transformed into an inductive definition. In that process the domain expert has to make a number of decisions to obtain the final representation. This ensures the flexibility of the framework. To clarify [...] ...|$|E
40|$|The PEN & PAD {{clinical}} workstation is {{for use by}} clinicians {{for direct}} patient care. It {{is based on a}} highly structured and detailed medical record but is intended to be simple and intuitive to use, allowing clinicians to summarise and view information in many ways, and quickly enter information through structured forms. It also provides mechanisms for tailoring the system to individual users without compromising its overall integrity. This has been achieved by following a process of user centred design and by developing a novel formalism, Structured <b>Meta</b> <b>Knowledge,</b> to represent the terminological knowledge, the medical record, and pragmatic knowledge about clinical practice...|$|E
40|$|It {{has been}} about 25 years now since {{researchers}} first {{became interested in the}} study of metacognition, with the onset of interest marked by the publication of the 1975 metamemory interview study of Kreutzer, Leonard, and Flavell and the seminal theoretical work of John Flavell (1976) and Ann Brown (1978). The early work by developmental psychologists on age-related differences in children 2 ̆ 7 s metacognition captured the attention of researchers concerned with individual differences in academic achievement in children as well as adults. Within academic domains, most of the research has been focused on reading and studying (Baker 2 ̆ 6 Brown, 1984; Forrest Pressley 2 ̆ 6 Waller, 1984; Garner, 1987; Paris, Wasik, 2 ̆ 6 Turner, 1991), but mathematics (Van Haneghan 2 ̆ 6 Baker, 1989), writing (Scardamalia 2 ̆ 6 Bereiter, 1985), and science (Baker, 1991) have also received attention. The consistent finding has been that students who are more successful in a domain exhibit higher levels of <b>meta</b> cognitive <b>knowledge</b> about the domain and are more skilled at regulating their cognitive processes. Clearly, the construct of metacognition has had wide appeal and wide applicability, stimulating a great deal of research across a broad spectrum of psychological problems and issues, as well as a growing amount of intervention work in classrooms. In a 1994 review paper on social influences on metacognitive development, Baker wrote, 2 ̆ 2 The popular appeal of metacognition has led to the widespread adoption and somewhat uncritical acceptance of the construct among educators. This situation is obviously problematic from a scientific standpoint and makes clear the need for further basic research on how metacognition develops, the role of metacognition in cognitive development, and how metacognition may best be fostered 2 ̆ 2 (pp. 202 - 203). The concern about uncritical acceptance is no less apt with regard to measurement; let us therefore amend the final sentence to end with and measured. In this chapter, we address the issue of metacognitive assessment first by examining methods of measuring metacognition used in empirical research, including questionnaires, interviews, think-aloud procedures, error-detection procedures, and various on-line measures. We then examine some of the instruments that have been subjected to tests of reliability and validity by independent investigators; their numbers are few. Next we consider recommendations for assessing metacognition that are published in books and journals for teachers and school psychologists; their numbers are many. Throughout, primary emphasis is on metacognition as it relates to reading and studying, but some reference is made to assessment of meta cognition in other domains as well (e. g., metamemory, problem solving) ...|$|R
40|$|The aim of {{this thesis}} is to explore {{the way in which}} free labour of and user – {{generated}} content are part of the current structure and business model of the e-Sports industry. Specifically the competitive industry of League of Legends helps us to understand the nature and growth of e-Sports as well as the way in which business that are founded by models of the free to play variety have continuously subjected gamers to exploitation. Through the fantasy of professionalisation, the gamer is propelled into a market in which such exploitation is necessary to the potential of becoming a celebrity within the wider gaming community. To make it so that gamers are more willing to participate in such a culture of free labour, professional and competitive league play is had, as well as actual money. Using an archival method, this thesis will seek to explore {{the way in which the}} labour that is given by the gaming community is further exploited by Riot. Through the analysis of build sites, live streaming and the process of the Tribunal, the contributions of the community toward the production and maintenance of the cultural significance of League is significant and worthy of investigation. With my own experience within the gaming field of League of Legends guided general knowledge on how the game was structured and played as well as how the <b>meta</b> worked, my <b>knowledge</b> allows me to further investigate the ways in which the community interacts with Riot and amongst themselves. The outcome of such research helps establish and understanding of how such an understanding of the professionalisation of videogames can increase ou...|$|R
40|$|Metacognition {{refers to}} {{thinking}} about thinking, or more generally, to using higher-level knowledge and strategies to regulate lower level performance. Previous research suggests that metacognition {{is an important part}} of learning among adults (Baker, 1989; Garner 2 ̆ 6 Alexander, 1989; Pressley 2 ̆ 6 Ghatala, 1990) and children (Alexander, Carr, 2 ̆ 6 Schwanenflugel, 1995; Borkowski 2 ̆ 6 Muthukrishna, 1992). Metacognition contributes to learning in several ways, but especially by helping learners to use their attentional resources more efficiently, to process information at a deeper level, and to monitor their performance more accurately. Notwithstanding its importance, there is considerable debate regarding how to measure meta cognition. At the heart of the problem is the elusive nature of metacognitive knowledge itself. Most theorists assume <b>meta</b> cognitive <b>knowledge</b> is highly abstract and cuts across domain-specific boundaries (Brown, 1987; Flavell, 1987; Paris 2 ̆ 6 Byrnes, 1989; Schraw, Dunkle, Bendixen, 2 ̆ 6 Roedel, 1995; Schraw 2 ̆ 6 Moshman, 1995). In contrast, most declarative and procedural knowledge in memory is welded to a specific domain, and can be stated as a declarative fact or demonstrated through a procedure. As a result, declarative and procedural knowledge are much easier to identify, manipulate, and measure than metacognitive knowledge. Added to this is the fact that metacognitive knowledge is acquired gradually over long periods of time, emerges relatively late in development, and often is difficult to explicate even when an individual demonstrates a high degree of metacognitive competence (Brown, 1987; Garner, 1994; Weinert 2 ̆ 6 Kluwe, 1987). Another problem is that metacognitive processes such as planning and evaluation are difficult to measure directly. For this reason, researchers have relied on a variety of indirect measures such as verbal reports, think-alouds, self-report inventories, and subjective measures of performance accuracy. One consequence of the unobservable nature of metacognitive knowledge and regulation is that researchers have focused their attention on several specific aspects of metacognition that are easier to measure than others, especially various forms of monitoring. Most studies have focused on memory monitoring (Cavanaugh 2 ̆ 6 Perlmutter, 1982; Johnson, Hastroudi, 2 ̆ 6 Lindsay, 1994; Lovelace, 1984; Koriat, 1993; Schneider 2 ̆ 6 Pressley, 1989), comprehension monitoring (Glenberg 2 ̆ 6 Epstein, 1985; Leonesio 2 ̆ 6 Nelson, 1990; Weaver, 1990), or performance monitoring (Glenberg, Sanocki, Epstein, 2 ̆ 6 Morris, 1987; Pressley 2 ̆ 6 Ghatala, 1990). This chapter addresses problems related to the measurement of metacognition in greater detail. We believe that some of the more imposing obstacles can be addressed successfully via computer-based testing procedures, but especially those pertaining to the assessment of metacognitive control processes. We will argue that computer based testing provides opportunities for researchers to measure control processes with much greater precision than with non-computerized methodologies. Computer-based testing enables us to do so in an unobtrusive, reliable manner that is less apt to be confounded by pre-experimental knowledge and ability. The remainder of this chapter is divided into six sections. The first of these provides a brief overview of previous research and presents a multilevel model of metacognition that distinguishes between two major components, including knowledge about and regulation of cognitive processes and knowledge. We further distinguish between two sub-components of meta cognitive regulation, including meta cognitive control and monitoring. Control processes are used to select performance goals and guide ongoing cognitive activities. Monitoring processes are used to evaluate the present success of one 2 ̆ 7 s performance and the degree to which one has met one 2 ̆ 7 s long-term performance goals. We assume that control and monitoring are reciprocally linked in a manner that facilitates self-regulation during performance...|$|R
40|$|Abstract: Employee {{knowledge}} {{is a key}} asset of every corporate organization. There-fore organizations strive to preserve this knowledge in its different forms {{in a variety of}} ways. One kind of such {{knowledge is}} the network of social contacts employees de-velop over time, incorporating <b>meta</b> <b>knowledge</b> on who knows what. These networks depend on the active participation of employees and their usefulness is reduced by passive user groups. We present an approach that allows for better integration of pas-sive members in online social networking services. Expanding on existing approaches we propose a tagging mechanism for single elements of a user’s profile as well as an ageing and rating mechanism for these tags. We also outline an implementation of our approach. ...|$|E
40|$|One of {{the main}} tasks in {{content-based}} image retrieval (CBIR) {{is to reduce the}} gap between low-level visual features and high-level human concepts. This paper presents a new semi-supervised EM algorithm (NSS-EM), where the image distribution in feature space is modeled as a mixture of Gaussian densities. Due to the statistical mechanism of accumulating and process-ing <b>meta</b> <b>knowledge,</b> the NSS-EM algorithm with long-term learning of mixture model parameters can deal with the cases where users may mislabel images dur-ing relevance feedback. Our approach that integrates mixture model of the data, relevance feedback and long-term learning helps to improve retrieval performance. The concept learning is incrementally refined with in-creased retrieval experiences. Experiment results on Corel database show the efficacy of our proposed con-cept learning approach. ...|$|E
40|$|The aim of tiffs {{paper is}} to show that when a {{development}} life cycle of representation refinement is utilized, that follows the principles of Newell’s Knowledge-Level, then the system will become self validating. This is illustrated through a rigorous development methodologythat utilizes formal techniques in the specification of the domain knowledge, the cognitive aspects and the representation. The paper introduces the concept of the Meta Knowledge-Level, a variant of Newell’s Knowledge-Level that facilitates the construction of a <b>meta</b> <b>knowledge</b> model. This provides the knowledge ngineer with a dynamic perspective of the system which can be used in conjunctionwith the static aspects found in the intermediate representation, an implementation independent representation that is created {{through the use of a}} knowledge filter. 1...|$|E
40|$|Statements about RDF statements, or meta triples, provide {{additional}} information about individual triples, such as the source, the occurring time or place, or the certainty. In-tegrating such meta triples into semantic knowledge bases would enable the querying and reasoning mechanisms {{to be aware of}} provenance, time, location, or certainty of triples. However, an efficient RDF representation for such meta knowl-edge of triples remains challenging. The existing standard reification approach allows such <b>meta</b> <b>knowledge</b> of RDF triples to be expressed using RDF by two steps. The first step is representing the triple by a Statement instance which has subject, predicate, and object indicated separately in three different triples. The second step is creating assertions about that instance as if it is a statement. While reificatio...|$|E
40|$|<b>Meta</b> <b>{{knowledge}}</b> is {{knowledge about}} knowledge; {{knowledge that is}} not domain specific but is concerned instead with its own internal structure. Several past systems have used meta-knowledge to improve {{the nature of the}} user interface, to maintain the knowledge base, and to control the inference engine. More extensive use of meta-knowledge is probable for the future as larger scale problems are considered. A proposed system architecture is presented and discussed in terms of meta-knowledge applications. The principle components of this system: the user support subsystem, the control structure, the knowledge base, the inference engine, and a learning facility are all outlined and discussed in light of the use of meta-knowledge. Problems with meta-constructs are also mentioned but it is concluded that the use of meta-knowledge is crucial for increasingly autonomous operations...|$|E
40|$|The rapidly {{increasing}} popularity of Web 2. 0 knowledge and content sharing systems and growing amount of shared data make discovering relevant content and finding contacts a difficult enterprize. Typically, folksonomies provide a {{rich set of}} structures and social relationships that can be mined {{for a variety of}} recommendation purposes. In this paper we propose a formal model to characterize users, items, and annotations in Web 2. 0 environments. Our objective is to construct social recommender systems that predict the utility of items, users, or groups based on the multi-dimensional social environment of a given user. Based on this model we introduce recommendation mechanisms for content sharing frameworks. Our comprehensive evaluation shows the viability of our approach and emphasizes the key role of social <b>meta</b> <b>knowledge</b> for constructing effective recommendations in Web 2. 0 applications...|$|E
