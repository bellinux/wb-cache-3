6462|14|Public
5|$|A {{variant of}} the <b>minimax</b> path problem has also been {{considered}} for sets of points in the Euclidean plane. As in the undirected graph problem, this Euclidean <b>minimax</b> path problem can be solved efficiently by finding a Euclidean minimum spanning tree: every path in the tree is a <b>minimax</b> path. However, the problem becomes more complicated when a path is desired that not only minimizes the hop length but also, among paths with the same hop length, minimizes or approximately minimizes the total length of the path. The solution can be approximated using geometric spanners.|$|E
5|$|If all edge weights of an {{undirected graph}} are positive, then the <b>minimax</b> {{distances}} between pairs of points (the maximum edge weights of <b>minimax</b> paths) form an ultrametric; conversely every finite ultrametric space comes from <b>minimax</b> distances in this way. A data structure constructed from the {{minimum spanning tree}} allows the <b>minimax</b> distance between any pair of vertices to be queried in constant time per query, using lowest common ancestor queries in a Cartesian tree. The root of the Cartesian tree represents the heaviest minimum spanning tree edge, {{and the children of}} the root are Cartesian trees recursively constructed from the subtrees of the minimum spanning tree formed by removing the heaviest edge. The leaves of the Cartesian tree represent the vertices of the input graph, and the <b>minimax</b> distance between two vertices equals the weight of the Cartesian tree node that is their lowest common ancestor. Once the minimum spanning tree edges have been sorted, this Cartesian tree can be constructed in linear time.|$|E
5|$|A {{closely related}} problem, the <b>minimax</b> path problem, asks for {{the path that}} {{minimizes}} the maximum weight of any of its edges. It has applications that include transportation planning. Any algorithm for the widest path problem can be transformed into an algorithm for the <b>minimax</b> path problem, or vice versa, by reversing the sense of all the weight comparisons performed by the algorithm, or equivalently by replacing every edge weight by its negation.|$|E
5|$|Von Neumann {{founded the}} field of game theory as a {{mathematical}} discipline. Von Neumann proved his <b>minimax</b> theorem in 1928. This theorem establishes that in zero-sum games with perfect information (i.e. in which players know at each time all moves {{that have taken place}} so far), there exists a pair of strategies for both players that allows each to minimize his maximum losses, hence the name <b>minimax.</b> When examining every possible strategy, a player must consider all the possible responses of his adversary. The player then plays out the strategy that will result in the minimization of his maximum loss.|$|E
5|$|In an undirected graph, a widest path may {{be found}} as the path between the two {{vertices}} in the maximum spanning tree of the graph, and a <b>minimax</b> path {{may be found}} as the path between the two vertices in the minimum spanning tree.|$|E
5|$|A {{solution}} to the <b>minimax</b> path problem between the two opposite corners of a grid graph {{can be used to}} find the weak Fréchet distance between two polygonal chains. Here, each grid graph vertex represents a pair of line segments, one from each chain, and the weight of an edge represents the Fréchet distance needed to pass from one pair of segments to another.|$|E
5|$|Vector spaces {{have many}} {{applications}} as they occur frequently in common circumstances, namely wherever functions with values in some field are involved. They {{provide a framework}} to deal with analytical and geometrical problems, or {{are used in the}} Fourier transform. This list is not exhaustive: many more applications exist, for example in optimization. The <b>minimax</b> theorem of game theory stating the existence of a unique payoff when all players play optimally can be formulated and proven using vector spaces methods. Representation theory fruitfully transfers the good understanding of linear algebra and vector spaces to other mathematical domains such as group theory.|$|E
5|$|Such strategies, which {{minimize}} the maximum loss for each player, are called optimal. Von Neumann showed that their minimaxes are equal (in absolute value) and contrary (in sign). Von Neumann improved and extended the <b>minimax</b> theorem to include games involving imperfect information and games {{with more than}} two players, publishing this result in his 1944 Theory of Games and Economic Behavior (written with Oskar Morgenstern). Morgenstern {{wrote a paper on}} game theory and thought he would show it to von Neumann because of his interest in the subject. He read it and said to Morgenstern that he should put more in it. This was repeated a couple of times, and then von Neumann became a coauthor and the paper became 100 pages long. Then it became a book. The public interest in this work was such that The New York Times ran a front-page story. In this book, von Neumann declared that economic theory needed to use functional analytic methods, especially convex sets and topological fixed-point theorem, rather than the traditional differential calculus, because the maximum-operator did not preserve differentiable functions.|$|E
25|$|Alpha–beta pruning is {{a search}} {{algorithm}} {{that seeks to}} decrease the number of nodes that are evaluated by the <b>minimax</b> algorithm in its search tree. It is an adversarial search algorithm used commonly for machine playing of two-player games (Tic-tac-toe, Chess, Go, etc.). It stops completely evaluating a move when at least one possibility {{has been found that}} proves the move to be worse than a previously examined move. Such moves need not be evaluated further. When applied to a standard <b>minimax</b> tree, it returns the same move as <b>minimax</b> would, but prunes away branches that cannot possibly influence the final decision.|$|E
25|$|This is an {{estimate}} of the number of positions one would have to evaluate in a <b>minimax</b> search to determine the value of the initial position.|$|E
25|$|However, a {{striking}} {{example of the}} line used to divide Dada and Surrealism among art experts is the pairing of 1925's Little Machine Constructed by <b>Minimax</b> Dadamax in Person (Von <b>minimax</b> dadamax selbst konstruiertes maschinchen) with The Kiss (Le Baiser) from 1927 by Max Ernst. The first is generally held to have a distance, and erotic subtext, whereas the second presents an erotic act openly and directly. In the second the influence of Miró and the drawing style of Picasso is visible {{with the use of}} fluid curving and intersecting lines and colour, whereas the first takes a directness that would later be influential in movements such as Pop art.|$|E
25|$|There {{are also}} some {{algorithms}} which naturally have two phases, such as <b>minimax</b> (min and max), and these can be implemented by having each phase in a separate function with mutual recursion, though {{they can also be}} combined into a single function with direct recursion.|$|E
25|$|At {{the same}} time the Llama III-A was {{replaced}} by the Micro-Max. This was similar in style to the <b>Minimax,</b> with a matte black or satin-chrome finish and 3.75-inch barrel. Models imported into the US were chambered for 9mm corto/.380 ACP (7-round capacity) and 7.65mm/.32 ACP (8-round capacity).|$|E
25|$|The depth-first <b>minimax</b> {{strategy}} will use computation time proportional to game's tree-complexity, since it must explore the whole tree, and {{an amount of}} memory polynomial in the logarithm of the tree-complexity, since the algorithm must always store one node of the tree at each possible move-depth, {{and the number of}} nodes at the highest move-depth is precisely the tree-complexity.|$|E
25|$|According {{to social}} {{exchange}} theory, relationships {{are based on}} rational choice and cost-benefit analysis. If one partner's costs begin to outweigh his or her benefits, that person may leave the relationship, especially if there are good alternatives available. This theory {{is similar to the}} <b>minimax</b> principle proposed by mathematicians and economists (despite the fact that human relationships are not zero-sum games). With time, long term relationships tend to become communal rather than simply based on exchange.|$|E
25|$|Since the <b>minimax</b> {{algorithm}} and its variants {{are inherently}} depth-first, a strategy such as iterative deepening is usually {{used in conjunction}} with alpha–beta so that a reasonably good move can be returned even if the algorithm is interrupted before it has finished execution. Another advantage of using iterative deepening is that searches at shallower depths give move-ordering hints, as well as shallow alpha and beta estimates, that both can help produce cutoffs for higher depth searches much earlier than would otherwise be possible.|$|E
25|$|The {{benefit of}} alpha–beta pruning {{lies in the}} fact that {{branches}} of the search tree can be eliminated. This way, the search time can be limited to the 'more promising' subtree, and a deeper search can be performed in the same time. Like its predecessor, it belongs to the branch and bound class of algorithms. The optimization reduces the effective depth to slightly more than half that of simple <b>minimax</b> if the nodes are evaluated in an optimal or near optimal order (best choice for side on move ordered first at each node).|$|E
25|$|Given {{the rules}} of any two-person game with {{a finite number of}} positions, one can always trivially {{construct}} a <b>minimax</b> algorithm that would exhaustively traverse the game tree. However, since for many non-trivial games such an algorithm would require an infeasible amount of time to generate a move in a given position, a game is not considered to be solved weakly or strongly unless the algorithm can be run by existing hardware in a reasonable time. Many algorithms rely on a huge pre-generated database, and are effectively nothing more.|$|E
25|$|The {{linear search}} problem was solved by Anatole Beck and Donald J. Newman (1970) as a two-person zero-sum game. Their <b>minimax</b> {{trajectory}} is {{to double the}} distance on each step and the optimal strategy {{is a mixture of}} trajectories that increase the distance by some fixed constant. This solution gives search strategies that are not sensitive to assumptions concerning the distribution of the target. Thus, it also presents an upper bound for a worst-case scenario. This solution was obtained in the framework of an online algorithm by Shmuel Gal, who also generalized this result to a set of concurrent rays. The best online competitive ratio for the search on the line is 9 but it can be reduced to 4.6 by using a randomized strategy. The online solution with a turn cost is given in.|$|E
500|$|In number theory, the unsolved Gaussian moat problem {{asks whether}} or not <b>minimax</b> paths in the Gaussian prime numbers have bounded or {{unbounded}} <b>minimax</b> length. That is, does there exist a constant [...] such that, for every pair of points [...] and [...] in the infinite Euclidean point set defined by the Gaussian primes, the <b>minimax</b> path in the Gaussian primes between [...] and [...] has <b>minimax</b> edge length at most? ...|$|E
500|$|... {{suggest that}} service {{vehicles}} and emergency vehicles should use <b>minimax</b> paths when {{returning from a}} service call to their base. In this application, the time to return {{is less important than}} the response time if another service call occurs while the vehicle {{is in the process of}} returning. By using a <b>minimax</b> path, where the weight of an edge is the maximum travel time from a point on the edge to the farthest possible service call, one can plan a route that minimizes the maximum possible delay between receipt of a service call and arrival of a responding vehicle. [...] use maximin paths to model the dominant reaction chains in metabolic networks; in their model, the weight of an edge is the free energy of the metabolic reaction represented by the edge.|$|E
500|$|Her {{indirect}} {{influence on}} literature, through her salon and her many literary friendships, {{can be seen}} in the number of writers who have addressed or portrayed her in their works. [...] Claudine S'en Va (Claudine and Annie, 1903) by Colette contains a brief appearance by Barney as [...] "Miss Flossie," [...] echoing the nickname she had earlier been given in de Pougy's novel Idylle Saphique. [...] Renée Vivien wrote many poems about her, as well as a Symbolist novel, Une Femme M'Apparut (A Woman Appeared to Me, 1904), in which she is described as having [...] "eyes ... as sharp and blue as a blade.... The charm of peril emanated from her and drew me inexorably." [...] Remy de Gourmont addressed her in his Letters to the Amazon, and Truman Capote mentioned her in his last, unfinished novel [...] [...] She also appeared in two later novels by writers who never met her: Francesco Rapazzini's Un Soir chez l'Amazone (An Evening with the Amazon, 2004) is a historical novel about Barney's salon, while Anna Livia's <b>Minimax</b> (1991) portrays both her and Renee Vivien as still-living vampires.|$|E
500|$|It is {{possible}} to find maximum-capacity paths and <b>minimax</b> paths with a single source and single destination very efficiently even in models of computation that allow only comparisons of the input graph's edge weights and not arithmetic on them. The algorithm maintains a set [...] of edges that are known to contain the bottleneck edge of the optimal path; initially, [...] is just the set of all [...] edges of the graph. At each iteration of the algorithm, it splits [...] into an ordered sequence of subsets [...] of approximately equal size; the number of subsets in this partition is chosen {{in such a way}} that all of the split points between subsets can be found by repeated median-finding in time [...] The algorithm then reweights each edge of the graph by the index of the subset containing the edge, and uses the modified Dijkstra algorithm on the reweighted graph; based on the results of this computation, it can determine in linear time which of the subsets contains the bottleneck edge weight. It then replaces [...] by the subset [...] that it has determined to contain the bottleneck weight, and starts the next iteration with this new set. The number of subsets into which [...] can be split increases exponentially with each step, so the number of iterations is proportional to the iterated logarithm function, , and the total time is [...] In a model of computation where each edge weight is a machine integer, the use of repeated bisection in this algorithm can be replaced by a list-splitting technique of , allowing [...] to be split into [...] smaller sets [...] in a single step and leading to a linear overall time bound.|$|E
2500|$|In {{imbalanced}} datasets, {{where the}} sampling ratio {{does not follow}} the population statistics, one can resample the dataset in a conservative manner called <b>minimax</b> sampling. The <b>minimax</b> sampling has its origin in Anderson <b>minimax</b> ratio whose value is proved to be 0.5: in a binary classification, the class-sample sizes should be chosen equally. This ratio can be proved to be <b>minimax</b> ratio only under the assumption of LDA classifier with Gaussian distributions. The notion of <b>minimax</b> sampling is recently developed for a general class of classification rules, called class-wise smart classifiers. In this case, the sampling ratio of classes is selected so that the worst case classifier error over all the possible population statistics for class prior probabilities, would be the ...|$|E
2500|$|The pseudo-code for <b>minimax</b> with {{alpha-beta pruning}} is as follows: ...|$|E
2500|$|In this case, {{a family}} of <b>minimax</b> estimators is given for any [...] and [...] as ...|$|E
2500|$|... 1928– John von Neumann begins devising the {{principles}} of game theory and proves the <b>minimax</b> theorem.|$|E
2500|$|The weak Fréchet {{distance}} is {{a variant of}} the classical Fréchet distance without the requirement that the endpoints move monotonically along their respective curves [...] the dog and its owner are allowed to backtrack to keep the leash between them short. Alt and Godau describe a simpler algorithm to compute the weak Fréchet distance between polygonal curves, based on computing <b>minimax</b> paths in an associated grid graph.|$|E
2500|$|Half-way models: these {{consist of}} very special homotopies. This is the {{original}} method, first done by Shapiro and Phillips via Boy's surface, later refined by many others. [...] The original half-way model homotopies were constructed by hand, and worked topologically but weren't minimal. The movie created by Nelson Max, over a seven-year period, and based on Charles Pugh's chicken-wire models (subsequently stolen from the Mathematics Department at Berkeley), was a computer-graphics 'tour de force' for its time, and set the bench-mark for computer animation for many years. A more recent and definitive graphics refinement (1980s) is <b>minimax</b> eversions, which is a variational method, and consist of special homotopies (they are shortest paths with respect to Willmore energy). In turn, understanding behavior of Willmore energy requires understanding solutions of fourth-order partial differential equations, and so the visually beautiful and evocative images belie some very deep mathematics beyond Smale's original abstract proof.|$|E
2500|$|With an (average or constant) {{branching}} factor of b, and a search depth of d plies, {{the maximum number}} of leaf node positions evaluated (when the move ordering is [...] ) is O(b*b*...*b) = O(b'd) – the same as a simple <b>minimax</b> search. If the move ordering for the search is optimal (meaning the best moves are always searched first), the number of leaf node positions evaluated is about O(b*1*b*1*...*b) for odd depth and O(b*1*b*1*...*1) for even depth, or [...] In the latter case, where the ply of a search is even, the effective {{branching factor}} is reduced to its square root, or, equivalently, the search can go twice as deep with the same amount of computation. The explanation of b*1*b*1*... is that all the first player's moves must be studied to find the best one, but for each, only the best second player's move is needed to refute all but the first (and best) first player move—alpha–beta ensures no other second player moves need be considered. When nodes are ordered at random, ...|$|E
2500|$|On March 9, 1949, Shannon {{presented}} a paper called “Programming a Digital Computer for Playing Chess.” [...] The paper was {{presented a}}t the National Institute for Radio Engineers Convention in New York. [...] He described how to program a computer to play chess based on position scoring and move selection. [...] He proposed basic strategies for restricting the number of possibilities to be considered in a game of chess. In March 1950, published in Philosophical Magazine, Series 7, Vol. 41 (No. 314, March 1950). This was the first article on computer chess. [...] His process for having the computer decide on which move to make was a <b>minimax</b> procedure, based on an evaluation function of a given chess position. Shannon gave a rough example of an evaluation function in which the value of the black position was subtracted from that of the white position. Material was counted according to the usual chess piece relative value (1 point for a pawn, 3 points for a knight or bishop, 5 points for a rook, and 9 points for a queen). He considered some positional factors, subtracting ½ point for each doubled pawn, backward pawn, and isolated pawn. Another positional factor in the evaluation function was mobility, adding 0.1 point for each legal move available. Finally, he considered checkmate to be the capture of the king, and gave the king the artificial value of 200 points. Quoting from the paper: ...|$|E
5000|$|The {{difficulty}} {{of determining the}} exact <b>minimax</b> estimator has motivated the study of estimators of asymptotic <b>minimax</b> --- an estimator [...] is called -asymptotic (or approximate) <b>minimax</b> if ...|$|E
50|$|<b>Minimax</b> {{also led}} to three {{successful}} cookbooks: Graham Kerr's Smart Cooking, Graham Kerr's <b>Minimax</b> Cookbook and Graham Kerr's Creative Choices (A <b>Minimax</b> Book) along with corresponding series on public television.|$|E
50|$|Negamax scores match <b>minimax</b> {{scores for}} nodes where player A {{is about to}} play, andwhere player A is the maximizing player in the <b>minimax</b> equivalent.Negamax always {{searches}} for the maximum value for all its nodes.Hence for player B nodes, the <b>minimax</b> score is a negation of its negamax score.Player B is the minimizing player in the <b>minimax</b> equivalent.|$|E
50|$|A is the <b>minimax</b> winner {{within the}} first group of voters and also within the second group of voters. However, both groups {{combined}} elect C as the <b>Minimax</b> winner. Thus, <b>Minimax</b> fails the consistency criterion.|$|E
50|$|Best Node Search is a <b>minimax</b> search algorithm, {{developed}} in 2011. Experiments with random trees {{show it to}} be the most efficient <b>minimax</b> algorithm. This algorithm does tell which move leads to minmax, but does not tell the evaluation of <b>minimax.</b>|$|E
