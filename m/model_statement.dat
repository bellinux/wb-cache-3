16|313|Public
5000|$|In SAS, the GODFREY {{option of}} the <b>MODEL</b> <b>statement</b> in PROC AUTOREG {{provides}} {{a version of}} this test.|$|E
40|$|In 1997 the Instruction Section of ACRL {{created a}} Task Force {{to review the}} 1987 <b>Model</b> <b>Statement</b> of Objectives for Academic Bibliographic Instruction. The 1997 Task Force made twelve recommendations, ranging from the "title should more clearly {{indicate}} the document's content" to the "statement should be more concise. " The Instruction Section subsequently created a Task Force for Revision of the <b>Model</b> <b>Statement</b> of Objectives 1 and charged it to follow those recommendations. The Task Force began its work at ALA Annual in 1998. Concurrently, an ACRL task force was working on information literacy standards for higher education institutions. That task force's document, Information Literacy Competency Standards for Higher Education (herein {{referred to as the}} Competency Standards) were approved in January 2000 and are available at: [URL] The following Objectives for Information Literacy Instruction: A <b>Model</b> <b>Statement</b> for Academic Librarians updates and replaces the older <b>Model</b> <b>Statement.</b> The Objectives will herein be referred to as the IS Objectives for clarity and to indicate that they were written by a Task Force of the Instruction Section (IS), formerly the Bibliographic Instruction Section of ACRL. Translated by Cristóbal Pasadas Ureña...|$|E
40|$|Deception {{research}} regarding {{insurance claims}} is rare but relevant given the financial loss {{in terms of}} fraud. In Study 1, a field study in a large multinational insurance fraud detection company, truth telling mock claimants (N = 19) and lying mock claimants (N = 21) were interviewed by insurance company telephone operators. These operators classified correctly only 50 % of these truthful and lying claimants, but their task was particularly challenging: Claimants said little, and truthful and deceptive statements {{did not differ in}} quality (measured with Criteria-Based Content Analysis [CBCA]) or plausibility. In Study 2, a laboratory experiment, participants in the experimental condition (N = 43) were exposed to an audiotaped truthful and detailed account of an event that was unrelated to insurance claims (a day at the motor races). The number of words, quality of the statement (measured with CBCA), and plausibility of the participants’ accounts were compared with participants who were not given a <b>model</b> <b>statement</b> (N = 40). The participants who had listened to the <b>model</b> <b>statement</b> provided longer statements than control participants, truth tellers obtained higher CBCA scores than liars, and only in the <b>model</b> <b>statement</b> condition did truth tellers sound more plausible than liars. Providing participants with a <b>model</b> <b>statement</b> is thus an innovative and successful tool to elicit cues to deception. Providing such a model has the potential to enhance performance in insurance call interviews, and, as we argue, in many other interview settings...|$|E
40|$|NC binds {{to small}} oligonucleotides {{following}} a complex binding pathway. The model shown in Fig. 3 {{is used for}} analysis of the SPR, FA, and TFQ assays. The mathematical details required {{for the analysis of}} each of these assays differ somewhat, although each analysis includes these core <b>model</b> <b>statements.</b> One implication of th...|$|R
40|$|This minor update fixes the URLs to {{download}} the required VC++ redistributables, {{as the previous}} ones were deleted by Microsoft. It also incorporates the following minor improvements: The "linear" keyword is now always removed from the <b>model</b> <b>statements.</b> The main DynareOBC file now has the documentation as it's top comment, so MATLAB help for DynareOBC displays the documentation...|$|R
40|$|State of Research: Manipulation and Characterization of Nanoparticles {{using the}} Scanning-Tunneling-Microscopy (STM) and Atomic Force Microscopy (AFM) {{reported}} by Reifenberger et al. and Samuelson et al. (1995). Target: Placing of nanoparticles regarding the bottom-up principle {{as a possible}} route to the realization of prototypes of a functional structure with three states. Method: Combination of various preparation techniques for nanoparticles with {{the features of the}} Scanning-Tunneling-Microscopy in a In-situ-procedure; objecttheoretical modeling; bottom-up-technological intention to the objecttheoretical <b>modeling</b> <b>statement</b> (contextdependent particle function). Result: Identification, imaging and positioning of prefabricated particles; contextdependent single-electron-processes; conceptional <b>modeling</b> <b>statement</b> for nano-heterostructures with three-state-function. Fields of application: prototype-structures for the realization of physical phenomena e. g. contextdependent single-electron-processes in a innovative nanotechnology for functional multiple-state-devices. (orig.) SIGLEAvailable from TIB Hannover: F 97 B 136 +a / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekBundesministerium fuer Bildung, Wissenschaft, Forschung und Technologie, Bonn (Germany) DEGerman...|$|R
40|$|This paper {{provides}} a detailed documentation of an applied CGE model of Malawi – {{the first ever}} for Malawi – developed {{in the context of}} the project “Collaborative Research and Capacity Strengthening for Multi-Sector Policy Analysis in Malawi and Southern Africa. ” The purpose of this paper is to serve as a source of background information for analysts using the model {{in the context of the}} current project and in the future. The model is built around a 1998 Social Accounting Matrix (SAM) for Malawi, which was developed in the context of the current project, is based on data from the 1998 Integrated Household Survey of Malawi. The main parts of the paper are a brief, self-contained summary of the model, and a detailed mathematical <b>model</b> <b>statement,</b> presented in a step-by-step fashion. The Appendices present a mathematical <b>model</b> <b>statement</b> in summary form and the 1998 Malawi SAM. Discussion paperNon-PRIFPRI 1 TM...|$|E
40|$|Abstract: <b>Model</b> <b>statement</b> of {{the problem}} of a waking machine {{emergency}} stop is discussed. Plane motion of a machine with weightless legs and the body mass concentrated in its center of masses is investigated. The possibility of the foot slippage is taken into consideration. A special algorithm of the machine stop (similar to rearing of horses) using pulse control actions (forces in the legs joints) is also discussed. It is shown, that such algorithm of the machine stop is less efficient than the emergency braking. Note: Publication language:russia...|$|E
30|$|LTM {{network was}} {{optimized}} using hydrogeological modeling method individually for the Quaternary aquifer and the Tertiary aquifer. In both aquifers, overlying the particle tracks with {{the locations of}} existing monitoring wells shows {{that more than one}} well was located on some of the contaminant flow path lines (Fig.  8). The results from modeled particle tracks depicted that some of the contaminant flow path lines consisted of more than one well, and thence, following the first LTM network optimization <b>model</b> <b>statement</b> relating the redundancy of more than one well on the same path line for specific aquifer type, the redundancy in the existing monitoring network was identified.|$|E
5000|$|Judge {{which of}} two {{competing}} <b>models</b> or <b>statements</b> {{is more likely}} to be true.|$|R
40|$|International audienceThis paper {{provides}} {{a discussion of}} different multiple-valued extensions of logical expressions of analogical proportions that are equivalent in the Boolean case. It is advocated that these extensions may serve different goals, or at least reflect different views. Boolean expressions of analogical proportions, <b>modeling</b> <b>statements</b> of the form A is to B as C is to D, are first restated and commented. An approach to handling the case of binary attributes that may not apply in the Boolean setting is also outlined via a proper encoding...|$|R
40|$|The NEPA Contracting Quality Improvement Team {{identified}} several contracting improvements {{to reduce the}} cost and time for the NEPA process. The team`s February 1995 report recommended a series of steps to achieve the improvements, including issuance of contracting guidance. The guidance will be issued in three phases. This Phase I guidance implements the team`s short-term recommendations. It provides <b>model</b> <b>statements</b> of work and a sample schedule of contractor deliverables, establishes a pilot program for evaluating performance of NEPA support contractors, and describes information resources available on the DOE NEPA Web...|$|R
40|$|We {{show how}} the NLMIXED {{procedure}} {{can be used to}} fit linear models (simple linear regression, multiple linear regression, analysis of variance and analysis of covariance) when data are not normally distributed or contaminated with potential outliers. This is accomplished using PROC NLMIXED’s capability of fitting user defined distributions through the general log likelihood option in the <b>Model</b> <b>statement.</b> This methodology can offer advantages over the M-estimation option in Proc ROBUSTREG which cannot analyze repeated measures and Proc NPAR 1 WAY which only provides p-values and cannot adjust for covariates or handle repeated measures. We provide some the basic theory and directions for specifying the likelihoods needed and then demonstrate the usefulness of the method on several examples...|$|E
40|$|News In Preview This newsletter's Q and A section {{describes}} the main {{factors that can}} be behind Permission denied er-rors. The Easily Overlooked Feature section {{describes the}} Align commands {{that can be used}} to align grid text, attribute text, and analysis text. The first article describes using the Tian method to plot loop gain in AC analysis. The Tian method has an advantage over the Middlebrook method due to it taking into account bilateral feedback paths in its calculations. The second article describes how to model skin effect in a lossy transmission line through the new capa-bility that allows the use of F and S within the lossy transmission line <b>model</b> <b>statement</b> parameters. The third article describes how to measure the crest factor of a waveform by using the performanc...|$|E
40|$|Aiken & West [1] {{emphasize}} it {{is important}} to center continuous variables before creating a product term with them to represent an interaction. This applies to software in which you must create your own interaction terms before running a model (e. g., SAS Proc Reg) or software in which the interaction terms are created for you by specifying the interaction component in the <b>model</b> <b>statement</b> (e. g., SAS Proc Mixed: y = x 1 x 2 x 1 * x 2). This is a demonstration why. First, let’s create some variables (there are a couple of options to creating low correlations between variables) :> x 1 x 2 x 2 cor(x 1, x 2) [1] 0. 1725266 Create the product term and combine variables:> x 1. x 2 unc apply(unc, 2, mean) x 1 x 2 x 1. x...|$|E
50|$|The Balances Mechanics (Saldenmechanik) (from {{balances}} of bookkeeping {{respectively the}} credit system and mechanics {{to characterize the}} strict universal identities) is a work and mean of economics, comparable with Stock-Flow Consistent <b>Modelling.</b> <b>Statements</b> of Balances Mechanics are not based on assumptions and preconditions of a model but are of trivial arithmetric nature, usually shaped as equation and universal without restrictions. Balances Mechanics were developed by Wolfgang Stützel and published in his books Paradoxa der Geld- und Konkurrenzwirtschaft (Paradoxes of Competition-Based Monetary Economies) and Volkswirtschaftliche Saldenmechanik (Balances Mechanics of Economics).|$|R
40|$|Logarithmic scaling {{invariance}} {{is a wide}} distributed {{natural phenomenon}} and was proved in the distributions of physical properties of various processes — in high en- ergy physics, chemistry, seismicity, biology, geology and technology. Based on the Gantmacher-Krein continued fraction method the present paper introduces fractal scal- ing models of resonant oscillations in chain systems of harmonic oscillators. These models generate logarithmic scaling spectra. The introduced models are not based on any statements {{about the nature of}} the link or interaction between the elements of the oscillating system. Therefore the <b>model</b> <b>statements</b> are quite generally, what opens a wide field of possible applications...|$|R
30|$|In {{this article}} we have asked for the {{different}} forms of orientation for the governance of modern societies by futures studies and reflections. The suggested distinction between mode 1, mode 2 and mode 3 orientation is geared to the question if and to which extent the results of futures studies tend to converge or diverge. If they are sufficiently convergent, I refer to mode 1 orientation which is also assumed by the popular decision-theoretical <b>model.</b> <b>Statements</b> about the future are interpreted as a framework into which decisions and actions have to fit as good as possible. According to a common phrase decisions and actions are “derived” from knowledge about the future (Section  2).|$|R
40|$|This {{document}} {{defines a}} logical model that separates control from switching and adaptation functions for voice, video, and data services. The {{objective of the}} MSF’s System Architecture Implementation Agreement is the identification and definition of protocols between the Control Plane and the Switching/Adaptation Planes and the management of this interaction. The functionality of the identified protocols is explained through the functions and reference points of the Functional Architecture. The Functional Architecture {{is the basis for}} Release 1 and future releases Implementation Agreements. The document also gives several examples of physical implementations of this logical <b>model.</b> <b>STATEMENT</b> REGARDING DRAFT IMPLEMENTATION AGREEMENTS The Multiservice Switching Forum (MSF) is responsible for developing Implementation Agreements which can be used by developers and network operators to ensure interoperability between components from different vendors. MSF Implementation Agreements are formally ratified via a Straw Ballot and then a Principal Member Ballot. Draft MSF Implementation Agreements may be published before formal ratification via Straw or Principal Member Ballot. In order for this to take place, the MSF Technical Committee must formally agree that a draft Implementation Agreement should be progressed through the balloting process. A Draft MSF Implementatio...|$|E
40|$|AbstractThe size of today’s {{programs}} {{continues to}} grow, {{as does the}} number of bugs they contain. Testing alone is rarely able to flush out all bugs, and many lurk in difficult-to-test corner cases. An important alternative is static analysis, in which correctness properties of a program are checked without running it. While it cannot catch all errors, static analysis can catch many subtle problems that testing would miss. We propose a new space of abstractions for pointer analysis—an important component of static analysis for C and similar languages. We identify two main components of any abstraction—how to <b>model</b> <b>statement</b> order and how to model conditionals, then present {{a new model of}} programs that enables us to explore different abstractions in this space. Our assign-fetch graph represents reads and writes to memory instead of traditional points-to relations and leads to concise function summaries {{that can be used in}} any context. Its flexibility supports many new analysis techniques with different trade-offs between precision and speed. We present the details of our abstraction space, explain where existing algorithms fit, describe a variety of new analysis algorithms based on our assign-fetch graphs, and finally present experimental results that show our flow-aware abstraction for statement ordering both runs faster and produces more precise results than traditional flow-insensitive analysis...|$|E
40|$|Since　 1979 ，　there　have　been　many　attempts　to　reform　the　management　of　public　services，including　public　library　services　in　the　UK．　ln　this　research，　the　strategies　for　reform　of　thepublic　library　management　which　were　carried　forward　by　the　Conservative　Party　and　those　bythe　Labor　Party　were　compared　and　analysed　to　clarify　their　features．　　　It　was　confirmed　that　the　strategies　by　both　the　Conservative　and　the　Labor　were　set　inaccord　with　the　principles　of　those　for　reform　of　the　whole　public　services．　The　principles　of　theConservative　strategies　were　promotion　of　privatization，　management　rationalization　by　voluntary　effort，　and　creation　of　customer－oriented　libraries．　While　those　of　the　Labor　strategies　werepromotion　of　the　library　management　in　partnership　with　public，　private　and　voluntary　sectors，management　rationalization　at　the　initiative　of　the　central　government，　creation　of　customeroriented　libraries，　and　adaptation　to　information　society．　　　<b>Model</b>　<b>Statement</b>　of　Standants　developed　by　the　Conservative　was　analysed　and　comparedwith　ComPrehensive，　EiEcient　ana　Modern　Public　Libraries　developed　by　the　Labor．　The　resultswere　as　follows：　The　Conservative　administration　had　aimed　at　customer－oriented　libraries，　but，the　government　gave　them　only　the　paratactic　menu　of　the　library　services，　and　demandedthem　to　set　their　own　targets　and　to　achieve　them　through　management　rationalization　byvoluntary　effort．　The　Labor　administration　aimed　at　customer－oriented　libraries　like　the　Conservative，　but　showed　clearly　the　emphasis　placed　on　the　following　services：　book－lendingservice　for　general　adults　and　children，　and　services　related　to　information　and　communication technology．　The　Labor　also　developed　a　system　for　the　library　authorities　to　achieve　thenational　target　set　by　the　government．　　It　was　showed　that　the　principal　aim　of　their　strategies　was　to　improve　nationallylibrarians’　skills　in　management　and　policy－making．　However，　they　still　leave　some　problems　toresolve，　such　as　too　much　dependence　on　customer　satisfaction　and　the　difificulty　of　consideringthe　needs　of　the　coming　generation...|$|E
40|$|Presented is an {{analytic}} microeconomic {{model of}} the temporal price dispersion of homogeneous goods in polypoly markets. This new approach {{is based on the}} idea that the price dispersion has its origin in the dynamics of the purchase process. The price dispersion is determined by the chance that demanded and supplied product units meet in a given price interval. It can be characterized by a fat-tailed Laplace distribution for short and by a lognormal distribution for long time horizons. Taking random temporal variations of demanded and supplied units into account both the mean price and also the standard deviation of the price dispersion are governed by a lognormal distribution. A comparison with empirical investigations confirms the <b>model</b> <b>statements...</b>|$|R
5000|$|Predictive: the <b>model</b> {{must allow}} <b>statements</b> {{to be made}} about the {{research}} object, especially and in particular about its future behaviour.|$|R
40|$|Since decades, basic-block (BB) graphs are the {{state-of-the-art}} {{means for}} representing programs in advanced industrial compiler environments. The usual justification for introducing the intermediate BB-structures {{in the program}} representation is performance: analyses on BB-graphs are generally assumed to outperform their counterparts on single-instruction (SI) graphs, which, undoubtedly, are conceptually much simpler, easier to implement, and more straightforward to verify. In this article, we discuss {{the difference between the}} two program representations and show by means of runtime measurements that, according to the new computer generations, performance is no longer on the side of the more complex BB-graphs. In fact, it turns out that no sensible reason for the BB-structure remains. Rather, we will demonstrate that edge-labeled SI-graphs, which in contrast to the classical flow graphs <b>model</b> <b>statements</b> in their edges instead of in their nodes, are most adequate, both for the theore [...] ...|$|R
40|$|This study {{comparatively}} {{evaluated the}} osteophilic capacity of 17 different surface modifications (i. e. fourteen different chemical modifications via ceramic coatings and three different physical modifications via surface roughness) for titanium (Ti) surfaces. All surface modifications {{were subjected to}} physico-chemical analyses and immersion in simulated body fluid (SBF) for coating stability assessment. Subsequently, a bone conduction chamber cassette model on the goat transverse process was used for comparative in vivo analysis based on bone responses to these different surface modifications after twelve weeks. Histological and histomorphometrical analyses in terms of longitudinal bone-to-implant contact percentage (BIC%), relative bone area (BA%) were investigated within each individual channel and maximum bone height (BH). Characterization of the surface modifications showed significant differences in surface chemistry and surface roughness among the surface modifications. Generally, immersion of the coatings in SBF showed net uptake of calcium by thick coatings (> 50 mum; plasma-sprayed and biomimetic coatings) and no fluctuations in the SBF for thin coatings (< 50 mum). The histomorphometrical data set demonstrated that only plasma-sprayed CaP coatings performed superiorly regarding BIC%, BA% and BH compared to un-coated surfaces, irrespective of surface roughness of the latter. In conclusion, this study demonstrated that the deposition of plasma-sprayed CaP coating with high roughness significantly improves the osteophilic capacity of titanium surfaces in a chamber cassette <b>model.</b> <b>STATEMENT</b> OF SIGNIFICANCE: For the bone implant market, {{a large number of}} surface modifications are available on different types of (dental and orthopedic) bone implants. As the implant surface provides the interface at which the biomaterial interacts with the surrounding (bone) tissue, it is of utmost importance to know what surface modification has optimal osteophilic properties. In contrast to numerous earlier studies on bone implant surface modifications with limited number of comparison surfaces, the manuscript by van Oirschot et al. describes the data of in vivo experiments using a large animal model that allows for direct and simultaneous comparison of a large variety of surface modifications, which included both commercially available and experimental surface modifications for bone implants. These data clearly show the superiority of plasma-sprayed hydroxyapatite coatings regarding bone-to-implant contact, bone amount, and bone height...|$|E
40|$|AbstractResearch {{on the use}} {{of natural}} {{products}} to treat or prevent microbial invasion as alternatives to antibiotic use is growing. Polymorphonuclear leukocytes (PMNL) play a vital role with regard to the innate immune response that affects severity or duration of mastitis. To our knowledge, effect of cold-pressed terpeneless Valencia orange oil (TCO) on bovine PMNL function has not been elucidated. Therefore, the objective {{of this study was to}} investigate the effect of TCO on bovine blood PMNL chemotaxis and phagocytosis capabilities and the expression of genes involved in inflammatory response in vitro. Polymorphonuclear leukocytes were isolated from jugular blood of 12 Holstein cows in mid-lactation and were incubated with 0. 0 or 0. 01 % TCO for 120 min at 37 °C and 5 % CO 2, and phagocytosis (2 × 106 PMNL) and chemotaxis (6 × 106 PMNL) assays were then performed in vitro. For gene expression, RNA was extracted from incubated PMNL (6 × 106 PMNL), and gene expression was analyzed using quantitative PCR. The supernatant was stored at − 80 °C for analysis of tumor necrosis factor-α. Data were analyzed using a general linear mixed model with cow and treatment (i. e., control or TCO) in the <b>model</b> <b>statement.</b> In vitro supplementation of 0. 01 % of TCO increased the chemotactic ability to IL- 8 by 47 %; however, migration of PMNL to complement 5 a was not altered. Treatment did not affect the production of tumor necrosis factor-α by PMNL. Expression of proinflammatory genes (i. e., SELL, TLR 4, IRAK 1, TRAF 6, and LYZ) coding for proteins was not altered by incubation of PMNL with TCO. However, downregulation of TLR 2 [fold change (FC=treatment/control) =− 2. 14], NFKBIA (FC= 1. 82), IL 1 B (FC=− 2. 16), TNFA (FC=− 9. 43), and SOD 2 (FC=− 1. 57) was observed for PMNL incubated with TCO when compared with controls. Interestingly, expression of IL 10, a well-known antiinflammatory cytokine, was also downregulated (FC=− 3. 78), whereas expression of IL 8 (FC= 1. 93), a gene coding for the cytokine IL- 8 known for its chemotactic function, tended to be upregulated in PMNL incubated with TCO. Incubation of PMNL with TCO enhanced PMNL chemotaxis in vitro. The expression of genes involved in the inflammatory response was primarily downregulated. Results showed that 0. 01 % TCO did not impair the function of PMNL in vitro. Future studies investigating the use of TCO as an alternative therapy for treatment of mastitis, including dose and duration, for cows during lactation are warranted...|$|E
40|$|Methane, one of {{the major}} {{greenhouse}} gases is produced primarily from cattle among livestock. Many researches have been conducted to reduce methane production and also to develop methods and/or equations to predict methane production in cattle. The objectives of this study were thus to construct a database containing experimental observations of methane production from cattle and to develop equations that predict methane production by cattle accurately. The database developed in this study contains experimental observations from the research articles published in the Journal of Dairy Science, Journal of Animal Science, Animal Feed Science and Technology, Canadian Journal of Animal Science, International Congress Series and Journal of Nutrition from 1964 till 2009. A total of 350 treatment means from 75 studies were obtained from the scientific journal articles that were found by searching for with methane and cattle as keywords. There were different methods measuring methane production; a chamber system, indirect respiratory hood, Sulfur hexafluoride (SF 6) and stoichiometric calculation. Only measured data were used in the subsequent analysis. Consequently the actual database used for the analysis is composed of a total of 256 treatment means from 57 studies. The types of animal in the database were 110 lactating dairy cows, 12 non-lactating dairy cows, 47 heifers, 65 steers, 10 calves, 10 bulls and 2 mixed. The mean (±SD) methane (g day - 1) methane (Mcal day - 1) and methane (GE%) of the data were 204. 50 (± 104. 22), 2. 76 (± 1. 38) and 5. 56 (± 1. 87), respectively. Among the variables tested, DMI (kg) or NDF intake (NDFI, kg) was the most significant single variable that correlates with methane production. Using a random coefficient model with study as a random effect, researchers obtained - 24. 27 (± 17. 76) + 13. 93 (± 1. 68) DMI (kg) + 0. 57 (± 0. 20) FpDM + 8. 43 (± 4. 16) NDFI (kg) (n = 145, - 2 Res log likelihood = 1434. 9) for predicting methane production (g). Using a simple linear regression, the best equation was CH 4 (g) = – 18. 53 (± 14. 90) + 11. 89 (± 1. 50) DMI (kg) + 0. 49 (± 0. 18) FpDM + 14. 19 (± 3. 77) NDFI (kg) (R 2 = 0. 84, root mean square error = 42. 25). Although, DMI and NDFI are inherently correlated, a single variable was not sufficient to explain the variations in methane production of cattle. When both NDFI and DMI were present in the <b>model</b> <b>statement</b> type of animal or method of methane measurement was no longer significant. The results from this study suggest that methane production from cattle can be predicted accurately with DMI and NDFI. More research however is needed to improve accuracy of the model predictions...|$|E
40|$|Understanding {{financial}} statements is imperative for better manage-ment of corporations, while system dynamics (SD) offers dynamic model-ing and simulation skills for better strategies of management. This paper tries {{to present a}} consolidated principle of accounting system dynamics {{on the basis of}} simple principles from SD and accounting system. It is, then, specifically applied to <b>model</b> corporate financial <b>statements</b> (income state-ment, balance sheet and cash flow statement) described in the book [3]. It is shown that cash flow statement is indispensable for <b>modeling</b> financial <b>statements.</b> At the same time, a limitation of the current accounting sys-tem as a dynamic guidance for management strategies is pointed out. This demonstrates the importance of SD modeling in the field of accounting system. ...|$|R
25|$|The {{theory of}} random graphs is ω categorical, complete, and decidable, and its {{countable}} model {{is called the}} Rado graph. A statement {{in the language of}} graphs is true in this theory if and only if the probability that an n-vertex random graph <b>models</b> the <b>statement</b> tends to 1 in the limit as n goes to infinity.|$|R
40|$|This MS thesis proposes {{the use of}} DYMOLA, an {{object-oriented}} language for modeling hierarchically structured systems, to generate ACSL simulation programs for continuous system analysis. An ACSL model {{described in terms of}} time dependent non-linear differential equations or transfer functions can be generated from a hierarchical model description of the system using DYMOLA. The model description in DYMOLA can be an equation description or a non-linear hierarchical bond graph abstraction to describe the system under investigation. The interface provides an automated method to generate ACSL simulation programs, hence eliminating the need for manual coding. The provision to specify an experiment description for run-time analysis and additional <b>model</b> <b>statements</b> is implemented. The implementation of the compiler's code generator includes parsing, error checking and system dependent file handling routines. Implementation techniques, model and control file specifications, and validation with examples in several application areas are described...|$|R
40|$|We {{present a}} model of steganographic systems which allows to {{evaluate}} their security. We especially want to establish an analogy to the known-plaintext-attack which is commonly used to rate cryptographic systems. This <b>models</b> main <b>statement</b> is that the embedding operation of a steganographic system should work indeterministic from the attackers point of view. This is proved by means of information theory...|$|R
50|$|Apple's iPod {{does not}} {{natively}} support Vorbis {{but through the}} use of Rockbox, an open-source firmware project, is capable of decoding Vorbis files on certain <b>models.</b> Similar <b>statements</b> apply to other devices capable of running Rockbox, as well. The Xiph.Org Foundation wiki has an up-to-date list of Vorbis-supporting hardware, such as portables, PDAs, and microchips. Also see Internet radio device for an overview.|$|R
40|$|Modeling Modeling is {{not merely}} a process of {{behavioral}} mimicry. Highly functional patterns of behavior, which constitute the proven skills and established customs of a culture, may be adopted in essentially the same form as they are exemplified. There is little leeway for 25 improvisation on how to drive automobiles or to perform arithmetic operations. However, in many activities, subskills must be improvised to suit varying circumstances. Modeling influences can convey rules for generative and innovative behavior as well. This higher-level learning is achieved through abstract modeling. Rule-governed behavior differs in specific content and other details but it contains the same underlying rule. For example, the <b>modeled</b> <b>statements,</b> "The dog is being petted," and "the window was opened" refer to different things but the linguistic rule [...] the passive form [...] is the same. In abstract modeling, observers extract the rule embodied in the specific behavior exhibited by others. Once they lear [...] ...|$|R
40|$|Merging of {{mathematical}} models (either manually or assisted by computer pro-grams) {{is an important}} requisite for creating large mathematical models of cells. A kinetic model describes biochemical quantities like concentrations and reaction rates by explicit differential and algebraic equations. We can regard it as a list of <b>model</b> <b>statements,</b> each comprising a biochemical quantity (e. g. a substance concentration), the corresponding mathematical object (e. g. a variable or parameter), and a mathemat-ical equation that allows to compute its numerical value. When two such models are merged, typical conflicts have to be detected and resolved: (i) incompatible names or identifiers; (ii) incompatible physical units; (iii) duplicate elements with contradicting assignments; (iv) conflicting (“semantically dependent”) quantities; (v) cyclic depen-dencies between model equations. To define and judge whether merging algorithms are trustworthy, we need formal criteria for the validity of models; such criteria can be clas-sified into the categories “syntax”, “computation”, “biochemical semantics”, “physical laws and empirical knowledge”, and “model relevance”...|$|R
40|$|A new approach, {{based on}} {{upper and lower}} solutions, was {{recently}} employed by the same authors to unify and generalize existing results for wave fronts in reaction-diffusion equations arising in combustion and genetic <b>models.</b> The <b>statement</b> {{of the problem and}} then main results are here explaned, and relations with existing results are analyzed. Some open problems and directions for futurre research are also indicated...|$|R
40|$|This {{document}} {{describes the}} algorithms and mechanisms of the MODEL Processor, {{which is a}} software system performing a program writing function. It also documents the program structure and procedures of the Processor. The MODEL Processor {{has been designed to}} automate the program design, coding and debugging of software development, based on a non-procedural specifications of a program module in the MODEL language. A program module is formally described and specified in the <b>MODEL</b> language, whose <b>statements</b> are then submitted to the Processor. The set of <b>MODEL</b> <b>statements</b> describing a program module is referred to as a specification. The Processor, performs the analysis (including checking for the completeness and consistency of the entire specification), program module design (including generating a flowchart-like sequence of events for the module), and code generation functions, thus replacing the tasks of an application programmer/coder. The Processor 2 ̆ 7 s capability to process a non-procedural specification language is built on application of graph theory to the analysis of such specification and to the program generation task. Another important function of the Processor is to interact with the specifier to indicate necessary supplements or changes to the submitted statements. The Processor produces a complete PL/ 1 program ready for compilation as well as various reports concerning the specification and the generated program. The Processor output reports include a listing of the specification, a cross-reference report, subscript range report, a flowchart-like report of the generated program, and a listing of the generated program...|$|R
