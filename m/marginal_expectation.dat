6|26|Public
40|$|Based on the {{introduction}} of modern agriculture, relationship between land circulation and modern agriculture is discussed. This relationship is reflected mainly in two aspects. Firstly, land circulation is a necessary requirement {{for the development of}} modern agriculture, which can speed up the adjustment of agricultural structure and realize the scale management, accelerate the transfer of the rural population and improve the agricultural income, enhance the risk resisting ability of agriculture and the response of agriculture to market challenges. Secondly, development of modern agriculture must be based on land circulation. Land circulation can not only provide resource allocation basis for modern agriculture, but also offer economic benefit basis for modern agriculture. 2007 China Statistical Yearbook, agricultural product output (Y), agricultural employees (L) and other relevant data in 19 provinces are obtained. CES production function is used to reflect the agricultural employees (L), agricultural capital investment (K) and agricultural product output (Y), in order to calculate the agricultural production function. Result shows that contribution of labor force to output is negative, while the contribution of capita to output is positive. Meanwhile, the substitution elasticity of capital for labor is 1. 11, indicating that every 1 % increase of labor force will lead to the 1. 11 % enhancement of farmers’ capital. Land circulation, Modern agriculture, <b>Marginal</b> <b>expectation,</b> Scale economy, China, Land Economics/Use,...|$|E
40|$|We {{investigate}} {{the stability of}} a Sequential Monte Carlo (SMC) method applied {{to the problem of}} sampling from a target distribution on R^d for large d. It is well known that using a single importance sampling step one produces an approximation for the target that deteriorates as the dimension d increases, unless the number of Monte Carlo samples N increases at an exponential rate in d. We show that this degeneracy can be avoided by introducing a sequence of artificial targets, starting from a `simple' density and moving to the one of interest, using an SMC method to sample from the sequence. Using this class of SMC methods with a fixed number of samples, one can produce an approximation for which the effective sample size (ESS) converges to a random variable ε_N as d→∞ with 1 <ε_N<N. The convergence is achieved with a computational cost proportional to Nd^ 2. If ε_N≪ N, we can raise its value by introducing a number of resampling steps, say m (where m is independent of d). In this case, ESS converges to a random variable ε_N,m as d→∞ and _m→∞ε_N,m=N. Also, we show that the Monte Carlo error for estimating a fixed dimensional <b>marginal</b> <b>expectation</b> is of order 1 /√(N) uniformly in d. The results imply that, in high dimensions, SMC algorithms can efficiently control the variability of the importance sampling weights and estimate fixed dimensional marginals at a cost which is less than exponential in d and indicate that, in high dimensions, resampling leads to a reduction in the Monte Carlo error and increase in the ESS...|$|E
40|$|AbstractLiang and Zeger {{introduced}} {{a class of}} estimating equations that gives consistent estimates of regression parameters and of their variances {{in the class of}} generalized linear models for longitudinal data. When the response variable in such models is subject to overdispersion, the oerdispersion parameter does not only influence the marginal variance, it may also influence the mean of the response variable. In such cases, the overdispersion parameter plays {{a significant role in the}} estimation of the regression parameters. This raises the necessity for a joint estimation of the regression, as well as overdispersion parameters, in order to describe the <b>marginal</b> <b>expectation</b> of the outcome variable as a function of the covariates. To correct for the effect of overdispersion, we, therefore, exploit a general class of joint estimating equations for the regression and overdispersion parameters. This is done, first, under the working assumption that the observations for a subject are independent and then under the general condition that the observations are correlated. In the former case, both score and quasi-score estimating equations are developed. The score equations are obtained from the marginal likelihood of the data, and the quasi-score equations are derived by exploiting the first two moments of the marginal distribution. This quasi-score equations approach requires a weight matrix, usually referred to as the pseudo-covariance weight matrix, which we construct under the assumption that the observations for a subject (or in a cluster) are independent. In the later case when observations are correlated, quasi-score estimating equations are developed in the manner similar to that of the independence case but the pseudo-covariance weight matrix is constructed from a suitable working covariance matrix of the longitudinal observations, the joint distribution of the observations being unknown. Asymptotic theory is provided for the general class of joint estimators for the regression and overdispersion parameters. The asymptotic distributional results are also applied to develop suitable chi-square test for testing for the regression of the overdispersed data...|$|E
40|$|Includes bibliographical {{references}} (pages [50]) Correlated {{binary data}} occur when measurements {{of two or}} more dichotomous response variables are taken on the same experimental unit. This type of data arises frequently in the medical and statistical literature. The need for methods to analyze and simulate such data is obvious and has attracted substantial interest in the last few years. Procedures have been presented which attempt to simulate random binary variables with predetermined <b>marginal</b> <b>expectations</b> and pair-wise dependence structures. The scope of this thesis is to investigate these attempts, explore their limitations, and suggest modifications to them. M. S. (Master of Science...|$|R
40|$|In {{this paper}} {{we look at}} the {{assumptions}} behind a Cournot model of investment in electricity markets. We analyze how information influences investment, looking at the way common knowledge of <b>marginal</b> costs, <b>expectations</b> on the competitors' <b>marginal</b> costs, <b>expectations</b> on the level and duration of demand, and conjectures on the others' behavior, influence the value of a project. We expose how the results are highly dependent on the assumptions used, and how the investment Nash-Cournot game with perfect and complete information implies such a degree of coordination between players that the outcome of the game would be classified by any regulation law as collusive behavior. Furthermore, we introduce the concept of Nash Value of Complete Information. As an example we use a stylized model of investment in liberalized electricity markets. ...|$|R
40|$|In {{studies of}} complex health conditions, {{mixtures}} of discrete outcomes (event time, count, binary, ordered categorical) are commonly collected. For example, studies of skin tumorigenesis record latency time {{prior to the}} first tumor, increases {{in the number of}} tumors at each week, and the occurrence of internal tumors at the time of death. Motivated by this application, we propose a general underlying Poisson variable framework for mixed discrete outcomes, accommodating dependency through an additive gamma frailty model for the Poisson means. The model has log-linear, complementary log-log, and proportional hazards forms for count, binary and discrete event time outcomes, respectively. Simple closed form expressions can be derived for the <b>marginal</b> <b>expectations,</b> variances, and correlations. Following a Bayesian approach to inference, conditionally-conjugate prior distributions are chosen that facilitate posterior computation via an MCMC algorithm. The methods are illustrated using data from a Tg. AC mouse bioassay study...|$|R
40|$|Liang and Zeger {{introduced}} {{a class of}} estimating equations that gives consistent estimates of regression parameters and of their variances {{in the class of}} generalized linear models for longitudinal data. When the response variable in such models is subject to overdispersion, the oerdispersion parameter does not only influence the marginal variance, it may also influence the mean of the response variable. In such cases, the overdispersion parameter plays {{a significant role in the}} estimation of the regression parameters. This raises the necessity for a joint estimation of the regression, as well as overdispersion parameters, in order to describe the <b>marginal</b> <b>expectation</b> of the outcome variable as a function of the covariates. To correct for the effect of overdispersion, we, therefore, exploit a general class of joint estimating equations for the regression and overdispersion parameters. This is done, first, under the working assumption that the observations for a subject are independent and then under the general condition that the observations are correlated. In the former case, both score and quasi-score estimating equations are developed. The score equations are obtained from the marginal likelihood of the data, and the quasi-score equations are derived by exploiting the first two moments of the marginal distribution. This quasi-score equations approach requires a weight matrix, usually referred to as the pseudo-covariance weight matrix, which we construct under the assumption that the observations for a subject (or in a cluster) are independent. In the later case when observations are correlated, quasi-score estimating equations are developed in the manner similar to that of the independence case but the pseudo-covariance weight matrix is constructed from a suitable working covariance matrix of the longitudinal observations, the joint distribution of the observations being unknown. Asymptotic theory is provided for the general class of joint estimators for the regression and overdispersion parameters. The asymptotic distributional results are also applied to develop suitable chi-square test for testing for the regression of the overdispersed data. Exponential family mixture model overdispersion joint estimating equations marginal likelihood mixed quasi-score equations score equations multivariate Gaussian consistent estimates asymptotic chi-square test (null) ...|$|E
40|$|Conditional {{inference}} is {{an intrinsic}} part of statistical theory, though not routinely of statistical practice. Conditioning has two principal objectives; (i) elimination of nuisance parameters, (ii) ensuring relevance of inference to observed sample data, through conditioning on an ancillary statistic, {{when such a}} statistic exists. Apart from formal difficulties with conditional inference, related to such issues as non-uniqueness of ancillary statistics, practical difficulties often arise, as calculating a conditional sampling distribution is typically not easy. Much interest therefore lies in inference procedures which are stable, that is, {{which are based on}} a statistic which has, to some high order in the sample size, the same repeated sampling behaviour unconditionally and conditional on the value of the appropriate conditioning statistic. Accurate approximation to an exact conditional inference can then be achieved by considering the marginal distribution of the stable statistic, ignoring the relevant conditioning. The principal approach to approximation of an intractable exact conditional inference by this route lies in developments in higher-order small-sample likelihood asymptotics. An alternative approach which we consider in this talk uses marginal simulation of the sampling distribution of an appropriately chosen stable statistic to mimic its conditional distribution. We offer theoretical results and empirical guidance to approximate conditioning by the computer-intensive route, for parametric inference on both scalar and vector interest parameters, in the presence of nuisance parameters. A key context where conditioning is used to eliminate nuisance parameters concerns inference in a multi-parameter exponential family setting. Here, computer-intensive methods yield third-order accuracy in approximation of exact conditional inference, under an appropriate handling of the nuisance parameter in the marginal simulation. In the ancillary statistic context, typically only second-order accuracy can be obtained via the marginal distribution of a stable statistic, though in practice excellent approximation is seen in many settings. We aim to provide general recommendations on the effectiveness of marginal simulation approaches to approximation of conditional inference. For one-sided inference on a scalar interest parameter, very effective approximations are obtained by simulating the marginal distribution of the signed root likelihood ratio statistic. With a vector interest parameter, good results are obtained by simulating the <b>marginal</b> <b>expectation</b> of the likelihood ratio statistic and making an empirical Bartlett correction. These approaches extend readily to more complex model settings, such as those involving high-dimensional parameters and where composite likelihood approaches are necessary...|$|E
40|$|Bibliography : leaves 143 - 153. Over {{the past}} few decades there has been {{increasing}} interest in clustered studies and hence much research has gone into the analysis of data arising from these studies. It is erroneous to treat clustered data, where observations within a cluster are correlated with each other, as one would treat independent data. It has been found that point estimates are not as greatly affected by clustering as are the standard deviations of the estimates. But as a consequence, confidence intervals and hypothesis testing are severely affected. Therefore one has to approach the analysis of clustered data with caution. Methods that specifically deal with correlated data have been developed. Analysis may be further complicated when the outcome variable of interest is binary rather than continuous. Methods for estimation of proportions, their variances, calculation of confidence intervals and a variety of techniques for testing the homogeneity of proportions have been developed over the years (Donner and Klar, 1993; Donner, 1989, and Rao and Scott, 1992). The methods developed within the context of experimental design generally involve incorporating the effect of clustering in the analysis. This cluster effect is quantified by the intracluster correlation and needs {{to be taken into account}} when estimating proportions, comparing proportions and in sample size calculations. In the context of observational studies, the effect of clustering is expressed by the design effect which is the inflation in the variance of an estimate that is due to selecting a cluster sample rather than an independent sample. Another important aspect of the analysis of complex sample data that is often neglected is sampling weights. One needs to recognise that each individual may not have the same probability of being selected. These weights adjust for this fact (Little et al, 1997). Methods for modelling correlated binary data have also been discussed quite extensively. Among the many models which have been proposed for analyzing binary clustered data are two approaches which have been studied and compared: the population-averaged and cluster-specific approach. The population-averaged model focuses on estimating the effect of a set of covariates on the <b>marginal</b> <b>expectation</b> of the response. One example of the population-averaged approach for parameter estimation is known as generalized estimating equations, proposed by Liang and Zeger (1986). It involves assuming that elements within a cluster are independent and then imposing a correlation structure on the set of responses. This is a useful application in longitudinal studies where a subject is regarded as a cluster. Then the parameters describe how the population-averaged response rather than a specific subject's response depends on the covariates of interest. On the other hand, cluster specific models introduce cluster to cluster variability in the model by including random effects terms, which are specific to the cluster, as linear predictors in the regression model (Neuhaus et al, 1991). Unlike the special case of correlated Gaussian responses, the parameters for the cluster specific model obtained for binary data describe different effects on the responses compared to that obtained from the population-averaged model. For longitudinal data, the parameters of a cluster-specific model describe how a specific individuals probability of a response depends on the covariates. The decision to use either of these modelling methods depends on the questions of interest. Cluster-specific models are useful for studying the effects of cluster-varying covariates and when an individual's response rather than an average population's response is the focus. The population-averaged model is useful when interest lies in how the average response across clusters changes with covariates. A criticism of this approach is that there may be no individual with the characteristics of the population-averaged model...|$|E
40|$|This {{study used}} {{simulated}} data {{to evaluate the}} performance of distinct conditional generalized estimating equations (CGEE) {{for the analysis of}} exchangeable correlation for binary data. The CGEE differs from the usual generalized estimating equations (GEE) in that, instead of <b>marginal</b> <b>expectations,</b> the conditional expectations of the responses were used in the estimating equations. The major distinction among the CGEEs compared was the sizes of the conditioning events used in the conditional expectations. The results show that, for the estimation of correlation coefficient, the bias decreases, and the variance increases when more members in a cluster are included in the conditioning event. The increase of variance is, however, only moderate for small intracluster correlation coefficient. On the other hand, for the estimation of regression parameters, the bias and variance of the estimates both increase when the size of the conditioning event increases. The increase, however, is also insignificant when the correlation coefficient is small. Clustered binary data Generalized estimating equations Efficiency Exchangeable...|$|R
40|$|Using {{the theory}} of group action, we first {{introduce}} {{the concept of the}} automorphism group of an exponential family or a graphical model, thus formalizing the general notion of symmetry of a probabilistic model. This automorphism group provides a precise mathematical framework for lifted inference in the general exponential family. Its group action partitions the set of random variables and feature functions into equivalent classes (called orbits) having identical <b>marginals</b> and <b>expectations.</b> Then the inference problem is effectively reduced to that of computing <b>marginals</b> or <b>expectations</b> for each class, thus avoiding the need to deal with each individual variable or feature. We demonstrate the usefulness of this general framework in lifting two classes of variational approximation for MAP inference: local LP relaxation and local LP relaxation with cycle constraints; the latter yields the first lifted inference that operate on a bound tighter than local constraints. Initial experimental results demonstrate that lifted MAP inference with cycle constraints achieved {{the state of the art}} performance, obtaining much better objective function values than local approximation while remaining relatively efficient. ...|$|R
40|$|New quasi-imputation and {{expansion}} strategies for correlated binary responses are proposed by borrowing ideas from random number generation. The core {{idea is to}} convert correlated binary outcomes to multivariate normal outcomes in a sensible way so that re-conversion to the binary scale, after performing multiple imputation, yields the original specified <b>marginal</b> <b>expectations</b> and correlations. This conversion process ensures that the correlations are transformed reasonably which in turn allows us {{to take advantage of}} well-developed imputation techniques for Gaussian outcomes. We use the phrase ‘quasi’ because the original observations are not guaranteed to be preserved. We argue that if the inferential goals are well-defined, {{it is not necessary to}} strictly adhere to the established definition of multiple imputation. Our expansion scheme employs a similar strategy where imputation is used as an intermediate step. It leads to proportionally inflated observed patterns, forcing the data set to a complete rectangular format. The plausibility of the proposed methodology is examined by applying it to a wide range of simulated data sets that reflect alternative assumptions on complete data populations and missing-data mechanisms. We also present an application using a data set from obesity research. We conclude that the proposed method is a promising tool for handling incomplete longitudinal or clustered binary outcomes under ignorable non-response mechanisms...|$|R
40|$|In {{this paper}} we assume a multivariate risk {{model has been}} {{developed}} for a portfolio and its capital derived as a homogeneous risk measure. The Euler (or gradient) principle, then, states that the capital to be allocated to each component of the portfolio has to be calculated as an expectation conditional to a rare event, which can be challenging to evaluate in practice. We exploit the copula-dependence within the portfolio risks to design a Sequential Monte Carlo Samplers based estimate to the <b>marginal</b> conditional <b>expectations</b> involved in the problem, showing its efficiency {{through a series of}} computational examples. 21 page(s...|$|R
40|$|We {{introduce}} a novel multivariate Laplace (MVL) distribution as a sparsity promoting prior for Bayesian source localization {{that allows the}} specification of constraints between and within sources. We represent the MVL distribution as a scale mixture that induces a coupling between source variances instead of their means. Approximation of the posterior <b>marginals</b> using <b>expectation</b> propagation is shown to be very efficient due to properties of the scale mixture representation. The computational bottleneck amounts to computing the diagonal elements of a sparse matrix inverse. Our approach is illustrated using a mismatch negativity paradigm for which MEG data and a structural MRI have been acquired. We show that spatial coupling leads to sources which are active over larger cortical areas as compared with an uncoupled prior. ...|$|R
40|$|In {{longitudinal}} data analysis, our primary {{interest is in}} the regression parameters for the <b>marginal</b> <b>expectations</b> of the longitudinal responses; the longitudinal correlation parameters are of secondary interest. The joint likelihood function for {{longitudinal data}} is challenging, particularly for correlated discrete outcome data. Marginal modeling approaches such as generalized estimating equations (GEEs) have received much attention {{in the context of}} longitudinal regression. These methods are based on the estimates of the first two moments of the data and the working correlation structure. The confidence regions and hypothesis tests are based on the asymptotic normality. The methods are sensitive to misspecification of the variance function and the working correlation structure. Because of such misspecifications, the estimates can be inefficient and inconsistent, and inference may give incorrect results. To overcome this problem, we propose an empirical likelihood (EL) procedure based on a set of estimating equations for the parameter of interest and discuss its characteristics and asymptotic properties. We also provide an algorithm based on EL principles for the estimation of the regression parameters and the construction of a confidence region for the parameter of interest. We extend our approach to variable selection for highdimensional longitudinal data with many covariates. In this situation it is necessary to identify a submodel that adequately represents the data. Including redundant variables may impact the model’s accuracy and efficiency for inference. We propose a penalized empirical likelihood (PEL) variable selection based on GEEs; the variable selection and the estimation of the coefficients are carried out simultaneously. We discuss its characteristics and asymptotic properties, and present an algorithm for optimizing PEL. Simulation studies show that when the model assumptions are correct, our method performs as well as existing methods, and when the model is misspecified, it has clear advantages. We have applied the method to two case examples...|$|R
40|$|This paper {{addresses}} {{the estimation of}} Phillips curve equations for the euro area while employing less stringent assumptions on the functional correspondence between price inflation, inflation <b>expectations</b> and <b>marginal</b> costs. <b>Expectations</b> are not assumed to be an unbiased predictor of actual inflation and instead derived from the European Commission 2 ̆ 019 s Consumer Survey data. The results suggest that expectations drive inflation with a lag of about 6 months, which casts further doubt on {{the validity of the}} New Keynesian Phillips curve. Moreover, the trade off between inflation and real economic activity is not vertical in the short run. Non- and Semiparametric estimates reveal an important nonlinearity in the sense that demand pressure on price inflation is not invariant to {{the state of the economy}} as it increases considerably at times of high economic activity. Conventional linear Phillips curves cannot capture this empirical regularity. Some implications for monetary policy are discussed...|$|R
40|$|Abstract Background Spurious {{associations}} between {{single nucleotide polymorphisms}} and phenotypes are {{a major issue in}} genome-wide association studies and have led to underestimation of type 1 error rate and overestimation of the number of quantitative trait loci found. Many authors have investigated the influence of population structure on the robustness of methods by simulation. This paper is aimed at developing further the algebraic formalization of power and type 1 error rate for some of the classical statistical methods used: simple regression, two approximate methods of mixed models involving the effect of a single nucleotide polymorphism (SNP) and a random polygenic effect (GRAMMAR and FASTA) and the transmission/disequilibrium test for quantitative traits and nuclear families. Analytical formulae were derived using matrix algebra for the first and second moments of the statistical tests, assuming a true mixed model with a polygenic effect and SNP effects. Results The expectation and variance of the test statistics and their <b>marginal</b> <b>expectations</b> and variances according to the distribution of genotypes and estimators of variance components are given {{as a function of the}} relationship matrix and of the heritability of the polygenic effect. These formulae were used to compute type 1 error rate and power for any kind of relationship matrix between phenotyped and genotyped individuals for any level of heritability. For the regression method, type 1 error rate increased with the variability of relationships and with heritability, but decreased with the GRAMMAR method and was not affected with the FASTA and quantitative transmission/disequilibrium test methods. Conclusions The formulae can be easily used to provide the correct threshold of type 1 error rate and to calculate the power when designing experiments or data collection protocols. The results concerning the efficacy of each method agree with simulation results in the literature but were generalized in this work. The power of the GRAMMAR method was equal to the power of the FASTA method at the same type 1 error rate. The power of the quantitative transmission/disequilibrium test was low. In conclusion, the FASTA method, which is very close to the full mixed model, is recommended in association mapping studies. </p...|$|R
40|$|This paper {{analyses}} optimal overhead allocation in {{a simple}} one-period setting with several divisions (production, sales or service departments). At the begin-ning of the period, headquarters has {{to decide on the}} procurement of a common input. The divisions possess private information on their respective <b>marginal</b> profit <b>expectations</b> of the common input. The objective of headquarters is to determine the most efficient overhead allocation mechanism. Different mechan-isms are compared. The conclusion is that the non-allocation of the common costs leads to a presentation of excessive profit expectations by the divisional managers and will thus induce an overinvestment in the common input. The Groves scheme prevents such a misallocation of resources only if collusion is precluded. In contrast, full-cost allocation leads to the first-best solution if allocation is based on a measure that ideally approximates the divisions' pro-portion of marginal profitability of the input. Thus, in the case of homogeneous divisions which benefit equally from the common input an equal allocation is optimal. In the case that divisions with higher profits benefit more from a common input, the allocation of overhead costs according to the divisional profits observable {{at the end of the}} period ('ability-to-bear' principle) can be optimal. ...|$|R
40|$|We {{propose a}} theory of asset prices that {{emphasizes}} heterogeneous information as the main element determining prices of different securities. Our main analytical innovation is in formulating a model of noisy information aggregation through asset prices, which is parsimonious and tractable, yet flexible in the specification of cash flow risks. We show that the noisy aggregation of heterogeneous investor beliefs drives a systematic wedge between the impact of fundamentals on an asset price, and the corresponding impact on cash flow expectations. The key intuition behind the wedge is that {{the identity of the}} marginal trader has to shift for different realization of the underlying shocks to satisfy the market-clearing condition. This identity shift amplifies the impact of price on the <b>marginal</b> trader’s <b>expectations.</b> We derive tight characterization for both the conditional and the unconditional expected wedges. Our first main theorem shows how the sign of the expected wedge (that is, the difference between the expected price and the dividends) depends on the shape of the dividend payoff function and on the degree of informational frictions. Our second main theorem provides conditions under which the variability of prices exceeds the variability for realized dividends. We conclude with two applications of our theory. First, we highlight how heterogeneous information can lead to systematic departures from the Modigliani-Miller theorem. Second, in a dynamic extension of our model we provide conditions under which bubbles arise...|$|R
40|$|We provide {{evidence}} on the fit of the hybrid New Keynesian Phillips curve for selected euro zone countries, the US and the UK. Instead of imposing rational expectations and estimating the Phillips curve by the Generalized Method of Moments, we follow Roberts (1997) and Adam and Padula (2003) and use direct measures of inflation expectations. The data source is the Ifo World Economic Survey, which quarterly polls economic experts about their expected future development of inflation. Our main findings are as follows: (i) In comparison with the rational expectations approach, backward-looking behaviour turns out to more relevant for most countries in our sample. (ii) The use of survey data for inflation expectations yields a positive slope of the Phillips curve when the output gap {{is used as a}} measure for <b>marginal</b> cost. inflation <b>expectations,</b> survey data, euro zone, Phillips curve...|$|R
40|$|Bayesian model {{comparison}} {{involves the}} evaluation of the <b>marginal</b> likelihood, the <b>expectation</b> of the likelihood under the prior distribution. Typically, this high-dimensional integral over all model parameters is approximated using Markov chain Monte Carlo methods. Thermodynamic integration is a popular method to estimate the marginal likelihood by using samples from annealed posteriors. Here we show that there exists a robust and flexible alternative. The new method estimates the density of states, which counts the number of states associated with a particular value of the likelihood. If the density of states is known, computation of the marginal likelihood reduces to a one- dimensional integral. We outline a maximum likelihood procedure to estimate the density of states from annealed posterior samples. We apply our method to various likelihoods and show that it is superior to thermodynamic integration in that it is more flexible with regard to the annealing schedule and the family of bridging distributions. Finally, we discuss the relation of our method with Skilling's nested sampling...|$|R
40|$|We {{consider}} {{the problem of}} improving the Gaussian approximate posterior <b>marginals</b> computed by <b>expectation</b> propagation and the Laplace method in latent Gaussian models and propose methods that are similar in spirit to the Laplace approximation of Tierney and Kadane (1986). We show {{that in the case}} of sparse Gaussian models, the computational complexity of expectation propagation can be made comparable to that of the Laplace method by using a parallel updating scheme. In some cases, expectation propagation gives excellent estimates where the Laplace approximation fails. Inspired by bounds on the correct marginals, we arrive at factorized approximations, which can be applied on top of both expectation propagation and the Laplace method. The factorized approximations can give nearly indistinguishable results from the non-factorized approximations and their computational complexity scales linearly with the number of variables. We experienced that the <b>expectation</b> propagation based <b>marginal</b> approximations we introduce are typically more accurate than the methods of similar complexity proposed by Rue et al. (2009) ...|$|R
40|$|We {{estimate}} a forward-looking New Keynesian Phillips Curve (NKPC) for the U. S. {{using data}} from the Survey of Professional Forecasters as proxy for expected inflation. We obtain significant and plausible estimates for the structural parameters of the NKPC (the discount factor and the share of firms adjusting prices) independent from whether output or unit labor costs are used as a measure of <b>marginal</b> costs. Survey <b>expectations</b> suggest that the usual identification of expectations exploiting orthogonality of forecast errors with respect to output is severely distorted, which explains why the NKPC estimated with survey data performs much better than under the assumption of rational expectations. We also find that lagged inflation enters the price equation significantly, even when controlling for its ability to predict expectations. This suggests a role for lagged inflation beyond that of capturing non-rationalities in expectations. When estimating a Phillips curve where lagged inflation enters due to price indexation by non-reoptimizing firms, we find that rejection of the coefficient restrictions depends on the measure of marginal costs used. JEL Classification: E 31...|$|R
40|$|The {{methodology}} of BayMeth is roughly {{divided into two}} steps: 1) An empirical Bayes procedure to derive parameters for the prior distributions of all parameters in the model. 2) The analytical derivation of the posterior <b>marginal</b> distribution, posterior <b>expectation</b> and variance for the methylation levels. Credible intervals are derived numerically from the posterior marginal distribution. Recall the model formulation provided in the main text: yiS |µi, λi ∼ Poisson f × cni ccn × µi × λi, and yiC |λi ∼ Poisson(λi), Prior specification For λi we assume a gamma prior distribution with parameters α and β: λi | α, β = β α Γ(α) λα− 1 i exp(−βλi), λi> 0, α, β> 0. The methylation level µi has support from zero to one. We consider two groups of prior distributions: • a mixture of beta distributions, i. e., µi ∼ ∑M m= 1 wm Be(am, bm), where in its simplest form M = 1. (The default configuration of BayMeth is M = 1 and (a = am = b = bm = 1), i. e., a uniform distribution from zero to one. ...|$|R
40|$|Many {{interesting}} stochastic {{models can}} be formulated as finite-state vector Markov processes, {{with a state}} characterized by the values {{of a collection of}} random variables. In general, such models suffer from the curse of dimensionality: the size of the state space grows exponentially with the number of underlying random variables, thereby precluding conventional modeling and analysis. A potential cure to this curse is to work with models that allow the propagation of partial information, e. g. <b>marginal</b> distributions, <b>expectations,</b> higher-moments, or cross-correlations, as derived from the joint distribution for the network state. This thesis develops and rigorously investigates the notion of separability, associated with structure in probabilistic models that permits exact propagation of partial information. We show that when partial information can be propagated exactly, it can be done so linearly. The matrices for propagating such partial information share many valuable spectral relationships with the underlying transition matrix of the Markov chain. Separability can be understood from the perspective of subspace invariance in linear systems, though it relates to invariance in a non-standard way. We analyze the asymptotic generality [...] as the number of random variables becomes large-of some special cases of separability that permit the propagation of marginal distributions. Within this discussion of separability, we introduce the generalized influence model, which incorporates as special cases two prominent models permitting the propagation of marginal distributions: the influence model and Markov chains on permutations (the symmetric group). The thesis proposes a potentially tractable solution to learning informative model parameters, and illustrates many advantageous properties of the estimator under the assumption of separability. Lastly, we illustrate separability in the general setting without any notion of time-homogeneity, and discuss potential benefits for inference in special cases. by William J. Richoux. Thesis (Ph. D.) [...] Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2011. Cataloged from PDF version of thesis. Includes bibliographical references (p. 185 - 191) ...|$|R
40|$|This paper {{studies the}} {{determinants}} of college attendance in Mexico. I use subjective quantitative expectations of future earnings to analyze both causes and implications of the steep income gradient in higher-education enrollment. I find that poor individuals require significantly higher expected returns to be induced to attend college. I then test predictions of a simple model of college attendance choice {{in the presence of}} credit constraints, using parental income and wealth as a proxy for the household's (unobserved) interest rate. I find that poor individuals with high expected returns are particularly responsive to changes in direct costs such as tuition, which is consistent with credit constraints playing an important role. To evaluate potential welfare implications of introducing a means-tested student loan program, I apply the Local Instrumental Variables approach of Heckman and Vytlacil to my model. I find that a sizeable fraction of poor individuals would change their decision in response to a reduction in interest rate, and that individuals at the margin have higher expected returns than the individuals already attending college. This suggests that policies such as government student loan programs could lead to large welfare gains. Educational choice, Credit Constraints, Subjective <b>Expectations,</b> <b>Marginal</b> Returns to Schooling, Local Instrumental Variables Approach, Mexico...|$|R
40|$|Abstract. A classical, Neyman-Pearson {{hypothesis}} test {{results in a}} decision (choice of action) justified not by any assessment of sample evidence, but by the pre-specified frequencies with which that procedure generates errors of the two possible types. By applying such a test in auditing, the {{hypothesis test}}ed is accepted or rejected without the auditor having to consider whether the data observed confirms (in any degree), or disconfirms, that hypothesis. In contrast with the classical framework, the Bayesian approach is to evaluate the probability of the hypothesis tested conditional on the data observed, and then {{to make a decision}} on the basis of that revised probability. Decisions are thus evidence-based rather than rule-based. So as to compare the classical and Bayesian programs, a familiar test example is considered, and hypothetical data, which, on a classical view, marginally reject the auditee's stated account balance, are re-interpreted from a Bayesian, evidential perspective. The results of this comparison reveal that classical hypothesis tests in auditing do not have a consistent (from test-to-test) evidential basis, and, in Bayesian terms, are therefore "incoherent". Also, contrary to intuitive <b>expectations,</b> <b>marginal</b> rejection is found to imply evidence in favor of the auditee's stated balance. Asymptotically, an account balance which is rejected only marginally in a classical hypothesis test has an "objective " (not-dependent-on-prior) posterior probability arbitrarily close to one...|$|R
40|$|A classical, Neyman-Pearson {{hypothesis}} test {{results in a}} decision (choice of action) justified not by any assessment of sample evidence, but by the pre-specified frequencies with which that procedure generates errors of the two possible types. By applying such a test in auditing, the {{hypothesis test}}ed is accepted or rejected without the auditor having to consider whether the data observed confirms (in any degree), or disconfirms, that hypothesis. In contrast with the classical framework, the Bayesian approach is to evaluate the probability of the hypothesis tested conditional on the data observed, and then {{to make a decision}} on the basis of that revised probability. Decisions are thus evidence-based rather than rule-based. So as to compare the classical and Bayesian programs, a familiar test example is considered, and hypothetical data, which, on a classical view, marginally reject the auditee 2 ̆ 7 s stated account balance, are re-interpreted from a Bayesian, evidential perspective. The results of this comparison reveal that classical hypothesis tests in auditing do not have a consistent (from test-to-test) evidential basis, and, in Bayesian terms, are therefore 2 ̆ 2 incoherent 2 ̆ 2. Also, contrary to intuitive <b>expectations,</b> <b>marginal</b> rejection is found to imply evidence in favor of the auditee 2 ̆ 7 s stated balance. Asymptotically, an account balance which is rejected only marginally in a classical hypothesis test has an 2 ̆ 2 objective 2 ̆ 2 (not-dependent-on-prior) posterior probability arbitrarily close to one...|$|R
40|$|Dissemination of {{data with}} {{sensitive}} information has an implicit risk of unauthorized disclosure. Several masking {{methods have been}} developed {{in order to protect}} the data without the loss of too much information. One such method is the Post Randomization Method (PRAM) based on perturbations of a categorical variable according to a Markov probability transition matrix. The method has the drawback that it is difficult to find an optimal transition matrix to perform perturbations and maximize data utility. An evolutionary algorithm which generates an optimal probability transition matrix is proposed. Optimality is with respect to a pre-defined fitness function dependent on the aspects of the data that need to be preserved following perturbation. The algorithm embeds two properties: the invariance of the transition matrix to preserve <b>marginal</b> totals in <b>expectation,</b> and the control of diagonal probabilities which determine the amount of perturbation. Experimental results using a real data set are presented in order to illustrate and empirically evaluate the application of this algorithm. © 2014 Elsevier Ireland Ltd. All rights reserved. This work has been carried out under the Ph. D. in Computer Science program of the Universitat Autònoma de Barcelona (UAB). It is also partially supported by the Spanish MECARES-CONSOLIDER INGENIO 2010 CSD 2007 - 00004, and COPRIVACY TIN 2011 - 27076 - 03 - 03. The research was also funded by the European Union’s Seventh Framework infrastructure research grant: 262608, Data Without Boundaries (DwB). Peer Reviewe...|$|R
40|$|This {{paper is}} about one aspect of Britain's {{electricity}} trading system, its advantages and its weaknesses concerning the incentives it provides or fails {{to provide for the}} location of generation. (Similar considerations apply to the location of loads, though these are less responsive to locational influences exerted by the trading system). The optimal location of generation in the short-run is a matter of determining the unit commitment and dispatch of the existing generation park so as to minimise the cost of generation hour by hour, subject to security constraints and taking account of transmission losses. In the long-run, choices of the locational pattern of new plant construction and of the decommissioning of old plant should be influenced by their effects upon the cost of the transmission investment that they entail. In systems with a gross pool, such as in New York, Ireland and New Zealand, there is a central dispatch. This, taking account of transmission losses and constraints, can produce locational <b>marginal</b> prices. <b>Expectations</b> concerning their future levels provide signals relevant to the location of new generation. Thus both in the short-run and in the long-run these systems provide locational incentives. In some of them, where the long run incentives to investment provided by the uncertain prospect of future price spikes are deemed insufficient, capacity requirements are imposed upon (what in Britain are called) "suppliers". These too can embody a locational element, as in the LICAP arrangements in New York and proposed for New England. (cont.) In the British system, there is a net pool, and two ca shout prices rather than one emerge from the Balancing Mechanism. This is Britain's version of what is elsewhere called the spot market, regulation market or real time market. Unit commitment is left to the generators, while National Grid, as system operator re-dispatches so as to preserve balance and to deal with transmission constraints. For the latter purpose, it constrains on here and constrains off there (though such actions may serve other purposes too). This costs it money, providing the occasion for it to weigh up the operating cost of dealing with constraints against the capital cost of removing them. But locational prices do not emerge from this process and no account is taken of locational differences in marginal losses. These are two defects of the short-run locational incentives provided by the British system. On the other hand, the British system scores highly with respect to long-run locational incentives. Instead of providing these by participants' expectations of future locational differences in energy prices, and maybe capacity prices, Britain provides them through locational differences in the transmission costs borne by generators. National Grid's Transmission Use of System Charges vary locationally to reflect the results of an "Incremental Cost" analysis. But although these may be roughly right, National Grid's approach is imperfect, even though it has evolved to meet some past criticism. This paper points to its remaining defects after first tackling the short-run issue of the treatment of losses...|$|R

