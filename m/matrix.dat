10000|10000|Public
5|$|A nonzero scalar {{multiple}} of an identity <b>matrix</b> {{is called a}} scalar <b>matrix.</b> If the <b>matrix</b> entries come from a field, the scalar matrices form a group, under <b>matrix</b> multiplication, that is isomorphic to the multiplicative group of nonzero elements of the field.|$|E
5|$|Matrices {{which have}} a single row are called row vectors, and those {{which have a}} single column are called column vectors. A <b>matrix</b> which has {{the same number of}} rows and columns is called a square <b>matrix.</b> A <b>matrix</b> with an {{infinite}} number of rows or columns (or both) is called an infinite <b>matrix.</b> In some contexts, such as computer algebra programs, it is useful to consider a <b>matrix</b> with no rows or no columns, called an empty <b>matrix.</b>|$|E
5|$|The {{size of a}} <b>matrix</b> {{is defined}} by the number of rows and columns that it contains. A <b>matrix</b> with m rows and n columns is called an m×n <b>matrix</b> or m-by-n <b>matrix,</b> while m and n are called its dimensions. For example, the <b>matrix</b> A above is a 3×2 <b>matrix.</b>|$|E
40|$|Determinants block <b>matrices</b> is {{determinants}} of partitioned (or block) <b>matrices</b> have {{the shape of}} <b>matrices.</b> The value of determinants block <b>matrices</b> can look for, when block <b>matrices</b> from a <b>matrices</b> shape <b>matrices</b> or that <b>matrices</b> is block diagonal <b>matrices.</b> The purpose from final assignment is to proof the theorem-theorem of determinants block <b>matrices</b> and {{to know how to}} look for the value of determinants block matrice...|$|R
40|$|In this paper, we {{deal with}} {K,s+ 1 }-potent <b>matrices.</b> These <b>matrices</b> generalize all the {{following}} classes of matrices: k-potent <b>matrices,</b> periodic <b>matrices,</b> idempotent <b>matrices,</b> involutory <b>matrices,</b> centrosymmetric <b>matrices,</b> mirrorsymmetric <b>matrices,</b> circulant <b>matrices,</b> among others. Several applications of these classes of <b>matrices</b> {{can be found in}} the literature. We develop algorithms in order to compute {K,s+ 1 }-potent <b>matrices</b> and {K,s+ 1 }-potent linear combinations of {K,s+ 1 }-potent <b>matrices.</b> In addition, some examples are presented in order to show the numerical performance of the method...|$|R
40|$|Generally {{a set of}} <b>matrices</b> is not {{commutative}} for binary multiplication properties. It {{is due to the}} multiplication of two <b>matrices</b> {{that must}} consider the rows and columns. This thesis will show about a set of <b>matrices</b> which commutative for multiplication operation properties. It is the set of circulant <b>matrices.</b> Circulant <b>matrices</b> have interacts with the cyclic permutation <b>matrices.</b> The resulted of relation cyclic permutation <b>matrices</b> with circulant <b>matrices</b> is diagonalized of circulant <b>matrices</b> by the Fourier <b>matrices.</b> The results of denationalization circulant <b>matrices</b> can be applied to determine the inverse <b>matrices</b> and prove the properties of the <b>matrices</b> multiplication is commutative. The commutative properties on the set of circulant <b>matrices</b> is an important axiom for build the algebraic structures in the set of circulant <b>matrices,</b> especially in algebraic structures that have two binary operations are addition and multiplication. Furthermore, in this thesis will be indicated that the definition of an algebraic structures field proofed by the set of circulant <b>matrices...</b>|$|R
5|$|A major {{branch of}} {{numerical}} analysis {{is devoted to}} the development of efficient algorithms for <b>matrix</b> computations, a subject that is centuries old and is today an expanding area of research. <b>Matrix</b> decomposition methods simplify computations, both theoretically and practically. Algorithms that are tailored to particular <b>matrix</b> structures, such as sparse matrices and near-diagonal matrices, expedite computations in finite element method and other computations. Infinite matrices occur in planetary theory and in atomic theory. A simple example of an infinite <b>matrix</b> is the <b>matrix</b> representing the derivative operator, which acts on the Taylor series of a function.|$|E
5|$|A {{symmetric}} <b>matrix</b> is positive-definite if {{and only}} if all its eigenvalues are positive, that is, the <b>matrix</b> is positive-semidefinite and it is invertible. The table at the right shows two possibilities for 2-by-2 matrices.|$|E
5|$|An empty <b>matrix</b> is a <b>matrix</b> {{in which}} the number of rows or columns (or both) is zero. Empty {{matrices}} help dealing with maps involving the zero vector space. For example, if A is a 3-by-0 <b>matrix</b> and B is a 0-by-3 <b>matrix,</b> then AB is the 3-by-3 zero <b>matrix</b> corresponding to the null map from a 3-dimensional space V to itself, while BA is a 0-by-0 <b>matrix.</b> There is no common notation for empty matrices, but most computer algebra systems allow creating and computing with them. The determinant of the 0-by-0 <b>matrix</b> is 1 as follows from regarding the empty product occurring in the Leibniz formula for the determinant as 1. This value {{is also consistent with}} the fact that the identity map from any finite dimensional space to itself has determinant1, a fact that is often used as a part of the characterization of determinants.|$|E
40|$|The idea of {{defining}} the generalized band <b>matrices</b> {{is based on}} the recognition that several pattern <b>matrices</b> and their inverses have low rank submatrices in certain domains. Theoretical considerations concerning the generalized band <b>matrices</b> enable us to give uniform treatment for several well known classes of <b>matrices</b> like band <b>matrices,</b> block band <b>matrices,</b> band <b>matrices</b> with low rank corrections, sparse <b>matrices</b> and their inverses. Making use of the new notions of information content and of compact representation of <b>matrices,</b> the concept of proper <b>matrices</b> is extended for generalized band <b>matrices.</b> Some reduction algorithms are presented which help to discover certain hidden structural properties of the generalized band <b>matrices.</b> The theoretical results are enlightened by a large number of figures illustrating numerical examples...|$|R
40|$|AbstractCentrosymmetric Toeplitz-plus-Hankel <b>matrices</b> are {{investigated}} on {{the basis}} of their “splitting property”, which is their similarity to the direct sum of two special Toeplitz-plus-Hankel <b>matrices.</b> These <b>matrices</b> can be considered as Hankel <b>matrices</b> (moment <b>matrices)</b> in bases of Chebyshev polynomials and are called Chebyshev–Hankel <b>matrices.</b> Chebyshev–Hankel <b>matrices</b> have similar properties like Hankel <b>matrices.</b> This concerns inversion formulas and fast algorithms. A superfast algorithm for solving Chebyshev–Hankel and centrosymmetric Toeplitz-plus-Hankel systems is presented that is based on real trigonometric transforms. The main tool of investigation is the interpretation of Chebyshev–Hankel <b>matrices</b> as <b>matrices</b> of restricted multiplication operators with respect to Chebyshev bases...|$|R
40|$|AbstractWe study {{recently}} meet <b>matrices</b> on meet-semilattices as {{an abstract}} generalization of {{greatest common divisor}} (GCD) <b>matrices.</b> Analogously, {{in this paper we}} consider join <b>matrices</b> on lattices as an abstract generalization of least common multiple (LCM) <b>matrices.</b> A formula for the determinant of join <b>matrices</b> on join-closed sets, bounds for the determinant of join <b>matrices</b> on all sets and a formula for the inverse of join <b>matrices</b> on join-closed sets are given. The concept of a semi-multiplicative function gives us formulae for meet <b>matrices</b> on join-closed sets and join <b>matrices</b> on meet-closed sets. Finally, we show what new the study of meet and join <b>matrices</b> contributes to the usual GCD and LCM <b>matrices...</b>|$|R
5|$|Geometrical optics {{provides}} further <b>matrix</b> applications. In this approximative theory, {{the wave}} nature of light is neglected. The {{result is a}} model in which light rays are indeed geometrical rays. If the deflection of light rays by optical elements is small, the action of a lens or reflective element on a given light ray can be expressed as multiplication of a two-component vector with a two-by-two <b>matrix</b> called ray transfer matrix: the vector's components are the light ray's slope and its distance from the optical axis, while the <b>matrix</b> encodes {{the properties of the}} optical element. Actually, {{there are two kinds of}} matrices, viz. a refraction <b>matrix</b> describing the refraction at a lens surface, and a translation <b>matrix,</b> describing the translation of the plane of reference to the next refracting surface, where another refraction <b>matrix</b> applies.|$|E
5|$|If all entries of A {{below the}} main {{diagonal}} are zero, A {{is called a}}n upper triangular <b>matrix.</b> Similarly if all entries of A above the main diagonal are zero, A is called a lower triangular <b>matrix.</b> If all entries outside the main diagonal are zero, A is called a diagonal <b>matrix.</b>|$|E
5|$|In mathematics, a <b>matrix</b> is a {{rectangular}} array of numbers or other data. In physics, a <b>matrix</b> {{model is a}} particular kind of physical theory whose mathematical formulation involves the notion of a <b>matrix</b> in an important way. A <b>matrix</b> model describes the behavior of a set of matrices within the framework of quantum mechanics.|$|E
40|$|The {{implicit}} Q-theorem for Hessenberg <b>matrices</b> is {{a widespread}} and powerful theorem. It {{is used in the}} development of for example implicit QR-algorithms to compute the eigendecomposition of Hessenberg <b>matrices.</b> Moreover it {{can also be used to}} prove the essential uniqueness of orthogonal similarity transformations of <b>matrices</b> to Hessenberg form. The theorem is also valid for symmetric tridiagonal <b>matrices,</b> proving thereby also in the symmetric case its power. Currently there is a growing interest to so-called semiseparable <b>matrices.</b> These <b>matrices</b> can be considered as the inverses of tridiagonal <b>matrices.</b> In a similar way, one can consider Hessenberg-like <b>matrices</b> as the inverses of Hessenberg <b>matrices.</b> In this paper, we formulate and prove an implicit Q-theorem for Hessenberg-like <b>matrices.</b> Similarly, like in the Hessenberg case the notion of unreduced Hessenberg-like <b>matrices</b> is introduced and also a method for transforming <b>matrices</b> via orthogonal transformations to this form is proposed. Moreover, as the theorem is valid for Hessenberg-like <b>matrices</b> it is also valid for symmetric semiseparable <b>matrices.</b> nrpages: 22 status: publishe...|$|R
40|$|The {{circulant}} {{real and}} complex <b>matrices</b> {{are used to}} find new real and complex conference <b>matrices.</b> With them we construct Sylvester inverse orthogonal <b>matrices</b> by doubling the size of inverse complex conference <b>matrices.</b> When the free parameters take values on the unit circle the inverse orthogonal <b>matrices</b> transform into complex Hadamard <b>matrices.</b> The method is used for $n= 6 $ conference <b>matrices</b> {{and in this way}} we find new parametrisations of Hadamard <b>matrices</b> for dimension $ n= 12 $...|$|R
40|$|The {{concept of}} semi-regular sets of <b>matrices</b> was {{introduced}} by J. Seberry in "A new construction for Williamson-type matrices", Graphs and Combinatorics, 2 (1986), 81 - 87. A regular s-set of <b>matrices</b> of order m was first discovered by J. Seberry and A. L. Whiteman in "New Hadamard <b>matrices</b> and conference <b>matrices</b> obtained via Mathon's construction", Graphs and Combinatorics, 4 (1988), 355 - 377. In this paper we study the product of semi-regular sets of <b>matrices</b> and applications in various Williamson-like <b>matrices.</b> Using semi-regular sets of <b>matrices</b> we construct new classes of Willianson type <b>matrices,</b> new classes of complex Hadamard <b>matrices</b> and new Williamson type <b>matrices</b> with additional properties...|$|R
5|$|One {{important}} {{example of}} a <b>matrix</b> model is the BFSS <b>matrix</b> model proposed by Tom Banks, Willy Fischler, Stephen Shenker, and Leonard Susskind in 1997. This theory describes {{the behavior of a}} set of nine large matrices. In their original paper, these authors showed, among other things, that the low energy limit of this <b>matrix</b> model is described by eleven-dimensional supergravity. These calculations led them to propose that the BFSS <b>matrix</b> model is exactly equivalent to M-theory. The BFSS <b>matrix</b> model can therefore be used as a prototype for a correct formulation of M-theory and a tool for investigating the properties of M-theory in a relatively simple setting.|$|E
5|$|The complex {{analogue}} of an orthogonal <b>matrix</b> is {{a unitary}} <b>matrix.</b>|$|E
5|$|The entries a'ii {{form the}} main {{diagonal}} of a square <b>matrix.</b> They {{lie on the}} imaginary line which runs from the top left corner to the bottom {{right corner of the}} <b>matrix.</b>|$|E
5000|$|In {{relativistic}} quantum mechanics, the spinors in {{four dimensions}} are 4 &times; 1 (or 1 &times; 4) <b>matrices.</b> Hence the Pauli <b>matrices</b> or the Sigma <b>matrices</b> operating on these spinors {{have to be}} 4 &times; 4 <b>matrices.</b> They are {{defined in terms of}} 2 &times; 2 Pauli <b>matrices</b> asIt follows from this definition that [...] <b>matrices</b> have the same algebraic properties as [...] <b>matrices.</b>|$|R
5000|$|Among complex <b>matrices,</b> all unitary, Hermitian, and skew-Hermitian <b>matrices</b> are normal. Likewise, among real <b>matrices,</b> all orthogonal, symmetric, and skew-symmetric <b>matrices</b> are normal. However, {{it is not}} {{the case}} that all normal <b>matrices</b> are either unitary or (skew-)Hermitian. For example, ...|$|R
40|$|AbstractA {{selected}} tour of {{the theory}} of identification <b>matrices</b> is offered here. We show that, among other things, shortest-path adjacency <b>matrices</b> are identification <b>matrices</b> for all simple graphs and adjacency <b>matrices</b> are identification <b>matrices</b> for all bipartite graphs. Additionally, we provide an improved proof that augmented adjacency <b>matrices</b> satisfying the circular 1 's property are identification <b>matrices.</b> We also present a characterization of doubly convex bipartite graphs by identification <b>matrices.</b> Based on the theory of identification <b>matrices,</b> we describe an improved method for testing isomorphism between Γ circular arc graphs. The sequential algorithm can be implemented to run in O(n 2) time and is optimal if the graphs are given as (augmented) adjacency <b>matrices,</b> so to speak...|$|R
5|$|The numbers, symbols or {{expressions}} in the <b>matrix</b> {{are called}} its entries or its elements. The {{horizontal and vertical}} lines of entries in a <b>matrix</b> are called rows and columns, respectively.|$|E
5|$|The rank of a <b>matrix</b> A is {{the maximum}} number of linearly {{independent}} row vectors of the <b>matrix,</b> which is the same as {{the maximum number}} of linearly independent column vectors. Equivalently it is the dimension of the image of the linear map represented by A. The rank–nullity theorem states that the dimension of the kernel of a <b>matrix</b> plus the rank equals the number of columns of the <b>matrix.</b>|$|E
5|$|There are {{a number}} of basic {{operations}} that can be applied to modify matrices, called <b>matrix</b> addition, scalar multiplication, transposition, <b>matrix</b> multiplication, row operations, and submatrix.|$|E
40|$|Characterizations {{are given}} for automorphisms of semigroups of nonnegative <b>matrices</b> {{including}} doubly stochastic <b>matrices,</b> row (column) stochastic <b>matrices,</b> positive <b>matrices,</b> and nonnegative monomial <b>matrices.</b> The proofs utilize {{the structure of}} the automorphisms of the symmetric group (realized as the group of permutation <b>matrices)</b> and alternating group. Furthermore, for each of the above (semi) groups of <b>matrices,</b> a larger (semi) group of <b>matrices</b> is obtained by relaxing the nonnegativity assumption. Characterizations are also obtained for the automorphisms on the larger (semi) groups and their subgroups (subsemigroups) as well...|$|R
40|$|In her Ph. D. thesis (Seberry) Wallis {{described}} a method using {{a variation of}} the Williamson array to find suitable <b>matrices,</b> which we will call good <b>matrices,</b> to construct skew Hadamard <b>matrices.</b> Good <b>matrices</b> were designed to plug into the Seberry-Williamson array to give skew-Hadamard <b>matrices.</b> We investigate the properties of good <b>matrices</b> in an effort to find a new, efficient, method to compute these <b>matrices.</b> We give the parameters of the supplementary difference sets (SDS) which give good <b>matrices</b> for use in the Seberry-Williamson array...|$|R
5000|$|Packed <b>matrices,</b> storing {{only half}} the <b>matrices</b> (for {{triangular}} or symmetric <b>matrices).</b>|$|R
5|$|There exists an {{equivalent}} weak interaction <b>matrix</b> for leptons (right {{side of the}} W boson on the above beta decay diagram), called the Pontecorvo–Maki–Nakagawa–Sakata <b>matrix</b> (PMNS <b>matrix).</b> Together, the CKM and PMNS matrices describe all flavor transformations, but the links between the two are not yet clear.|$|E
25|$|In {{numerical}} linear algebra, a convergent <b>matrix</b> is a <b>matrix</b> that converges to {{the zero}} <b>matrix</b> under <b>matrix</b> exponentiation.|$|E
25|$|A <b>matrix</b> {{polynomial}} {{equation is}} an equality between two <b>matrix</b> polynomials, which holds {{for the specific}} matrices in question. A <b>matrix</b> polynomial identity is a <b>matrix</b> polynomial equation which holds for all matrices A in a specified <b>matrix</b> ring Mn(R).|$|E
30|$|For brevity we only {{consider}} square <b>matrices.</b> The generalizations from square <b>matrices</b> to rectangular <b>matrices</b> are obvious, {{and usually}} problems on singular values of rectangular <b>matrices</b> {{can be converted}} to the case of square <b>matrices</b> by adding zero rows or zero columns.|$|R
40|$|We define binet <b>matrices,</b> which furnish {{a direct}} {{generalization}} of totally unimodular network <b>matrices</b> and {{arise from the}} node-edge incidence <b>matrices</b> of bidirected graphs {{in the same way}} as the network <b>matrices</b> do from directed graphs. We develop the necessary theory, give binet representations for interesting sets of <b>matrices,</b> characterize totally unimodular binet <b>matrices</b> and discuss the recognition problem. We also prove that binet <b>matrices</b> guarantee half-integral optimal solutions to linear programs...|$|R
40|$|AbstractComplex <b>matrices</b> {{that are}} {{structured}} {{with respect to}} a possibly degenerate indefinite inner product are studied. Based on earlier works on normal <b>matrices,</b> the notions of hyponormal and strongly hyponormal <b>matrices</b> are introduced. A full characterization of such <b>matrices</b> is given and it is shown how those <b>matrices</b> are related to different concepts of normal <b>matrices</b> in degenerate inner product spaces. Finally, the existence of invariant semidefinite subspaces for strongly hyponormal <b>matrices</b> is discussed...|$|R
