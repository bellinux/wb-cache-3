100|10000|Public
50|$|Because of its {{emphasis}} in using human {{intelligence in the}} information retrieval process, HCIR requires different evaluation models - one that combines evaluation of the IR and HCI components of the system. A key area of research in HCIR involves evaluation of these systems. Early work on interactive information retrieval, such as Juergen Koenemann and Nicholas J. Belkin's 1996 study of different levels of interaction for automatic query reformulation, leverage the standard IR <b>measures</b> <b>of</b> <b>precision</b> and recall but apply them {{to the results of}} multiple iterations of user interaction, rather than to a single query response. Other HCIR research, such as Pia Borlund's IIR evaluation model, applies a methodology more reminiscent of HCI, focusing on the characteristics of users, the details of experimental design, etc.|$|E
5000|$|In 1955, {{he helped}} found the Center for Documentation Communication Research at Western Reserve University. This was [...] "the first {{academic}} program {{in the field of}} mechanized information retrieval, first using cards, then utilizing new reel-to-reel tape technology." [...] In the same year he introduce the <b>measures</b> <b>of</b> <b>precision</b> and recall in [...] In 1959, he wrote an article for Harper's magazine entitled, [...] "A Machine That Does Research" [...] which provided one of the first incites in mainstream media about how Americans lives can change due to electronic information technology. [...] He joined the faculty of the University of Pittsburgh in 1963, where in 1970 he began the Department of Information Science. He retired from the university in 1992. At the time of his death, he was Distinguished Service Professor in the School of Information Sciences at the University of Pittsburgh The school named a scholarship after him.|$|E
3000|$|... [...]. The {{experiments}} {{were carried out}} in a MATLAB Version 7.3. 0.267 (R 2006 b) environment using a personal computer with Intel(R) CPU T 2400, running at 1.83 [*]GHz with 1.99 [*]GB RAM. We measure performance {{in terms of the}} information retrieval <b>measures</b> <b>of</b> <b>precision</b> and recall. We use the following notation: [...]...|$|E
5000|$|Kriging {{provides}} [...] as a <b>measure</b> <b>of</b> <b>precision.</b> However this <b>measure</b> {{relies on}} the correctness of the variogram.|$|R
3000|$|... c is {{considered}} as a <b>measure</b> <b>of</b> <b>precision</b> and accuracy to assess the degree of agreement between two <b>measures</b> <b>of</b> the same variables performed with different reconstruction parameters [35]. In all cases, a perfect concordance gives ρ [...]...|$|R
40|$|The {{problem of}} {{selecting}} {{the type and}} structure of the coastal systems, navigation equipment for certain areas of navigation <b>of</b> vessels. A <b>measure</b> <b>of</b> <b>precision</b> control <b>of</b> the vessel and formulated the problem of designing the optimal structure of the system. ??????????? ?????? ?????? ???? ? ????????? ?????? ?????????? ?????????????? ???????????? ??? ???????????? ??????? ???????? ?????. ????????? ?????????? ???????? ???????? ????? ????? ? ?????????????? ?????? ??????? ??????????? ????????? ???????...|$|R
40|$|We extend Holt and Bishop’s (2002) {{normalized}} quadratic inverse {{demand function}} by obtaining <b>measures</b> <b>of</b> <b>precision</b> of elasticities and consumer welfare loss estimates due to reductions in commercial landings in the U. S. Great Lakes. Even with curvature constraints imposed, confidence intervals are obtained {{by using the}} bootstrap, subsample bootstrap, and subsample jackknife. Key words: quadratic inverse demand function, bootstrap, subsample bootstrap...|$|E
40|$|It {{is often}} useful to {{determine}} the <b>measures</b> <b>of</b> <b>precision</b> of the directly observed quantities in a triangulation net. Provided the net is not strained these measures are unique to {{a particular set of}} observations and weights. Unique measures for the precision of the indirectly observed quantities cannot be found by classical means although several ad hoc approaches can be used to approximate to this measure of the 'inherent strength' of a net. Bjerhammar's theory of generalised matrix inverses can be used to derive <b>measures</b> <b>of</b> <b>precision</b> for the indirectly observed quantities, which may be interpreted as reflecting the inherent strength of the net. The theory of adjustment of a triangulation net by the method of variation of co-ordinates is described, followed by an explanation of the theory bf generalised inverses. Methods for the practical derivation of particular inverses are described, following Mittermayer. The characteristics of Normal, Transnormal and Stochastic Ring inverses in solution of Normal equations BX = R, are described...|$|E
40|$|Abstract. This article {{describes}} how mathematical content items and formulæ are processed, retrieved, and accessed in ActiveMath. Central to the retrieval and access is a search tool {{which allows for}} searching text, attributes, relations and formulæ, and presenting items. The search tool has been evaluated according to the standard <b>measures</b> <b>of</b> <b>precision</b> and recall {{as well as for}} usability. We report results of these evaluations. ...|$|E
30|$|Quantitative {{performance}} was considered acceptable if the SD {{of the percentage}} differences was ±[*] 25 % (<b>measure</b> <b>of</b> <b>precision)</b> and the mean value was within ±[*] 10 % (<b>measure</b> <b>of</b> accuracy). Lin’s CC coefficient assesses both the measurement <b>of</b> <b>precision</b> (a Pearson correlation coefficient) and of accuracy (a bias correction factor, which <b>measures</b> the level <b>of</b> the deviation from a 45 ° line through the origin).|$|R
30|$|To {{investigate}} publication bias, {{we assessed}} funnel plots by visual inspection for asymmetry. In a funnel plot, the treatment effect is plotted against a <b>measure</b> <b>of</b> <b>precision.</b> When a publication {{is less likely}} for smaller and hence less precise studies failing to detect a significant effect, the funnel plot may be asymmetrical.|$|R
40|$|As {{libraries}} ease {{into the}} age of electronic utilities and computerized catalogs based on records read by machine rather than interpreted by humans, a considerably greater <b>measure</b> <b>of</b> <b>precision</b> {{will have to be}} introduced into library work. As one step toward that goal an examination of the structure of publications will be in order...|$|R
40|$|Within the {{pharmaceutical}} industry it is common practice to transfer analytical methods from one laboratory to other laboratories. An experiment or interlaboratory study is performed to estimate the repeatability, the intermediate precision, and the reproducibility of the analytical method. These <b>measures</b> <b>of</b> <b>precision</b> are quantified by appropriate sums of variance components from {{an analysis of variance}} model describing the structure of the data. In the literature, several methods have been described for calculating approximate (closed-form) confidence intervals on sums of variance components, i. e., Welch, Satterthwaite, and modified large-sample (MLS). Comparisons between these methods have been performed for one-way and two-way classification analysis of variance models only. Interlaboratory studies though often need higher order classifications. Therefore, these methods for constructing confidence intervals are compared on the <b>measures</b> <b>of</b> <b>precision</b> from a specific three-way classification analysis of variance model that is frequently used for method transfer studies. Using a simulation study, the coverage probability for these methods is evaluated in situations where variance components may be estimated negatively wit...|$|E
40|$|Evaluations in Information Retrieval are {{dominated}} by <b>measures</b> <b>of</b> <b>precision</b> and recall. Is that enough? Probably not, as it somewhat assumes that all information seeking tasks are equal, and that everyone needs the same thing. In this position paper, we advocate a consumers ’ guide to systems that aim at supporting information seeking tasks. We propose a method that provides guidance in whole-of-system evaluations, explicitly considering all participants and {{both sides of the}} “bang for buck ” equation. 1...|$|E
40|$|This {{information}} is distributed {{solely for the}} purpose of predissemination peer review. It has not been formally disseminated by NOAA. It does not represent any final agency determination or policy. Terms of Reference 1. Compare and contrast the performance with alternative estimators of total discards with respect to precision and accuracy. 2. Evaluate impacts of trimming observations (e. g. large discard events) on the magnitude of bias and <b>measures</b> <b>of</b> <b>precision.</b> Covered by WORKING PAPERS 2 and 4 Available on-line a...|$|E
30|$|This is a <b>measure</b> <b>of</b> <b>precision</b> or {{replicability}} <b>of</b> measurement. The intratester TEM {{was determined}} by taking six measurements: stature, foot length, and foot breadth, repeated by the tester on two participants. The measurements were then repeated “blind.” The six “pairs” of measurement were compared using the equation given as follows: TEM[*]=[*][∑d 2 / 2 n] 0.5.|$|R
50|$|Computer {{representations}} of {{floating point numbers}} typically use a form of rounding to significant figures, but with binary numbers. The number of correct significant figures {{is closely related to}} the notion of relative error (which has the advantage of being a more accurate <b>measure</b> <b>of</b> <b>precision,</b> and is independent of the radix of the number system used).|$|R
40|$|Our {{aim is to}} give a novel geostatistically based {{technique}} to recover any required complete pole figure, which {{was not at all}} or only incompletely measured, from a collection <b>of</b> <b>measured</b> diffraction data and to provide a local <b>measure</b> <b>of</b> <b>precision</b> for the reconstruction. The basic idea is to model the true orientation density, which is a function on the orientation group SO(3) /G, as a random field) (gf with known mean 1. The measured diffraction intensities P(h,r) are mean values of) (gf along geodesic fibers}:{), (rghgrhB = = or corresponding tube shaped blocks in the domain of the random field. We propose a kind of block kriging to interpolate to unmeasured P(h,r). The method surpasses other methods in simplicity and generality with respect to the universatility of input data. It also allows to reconstruct local portions of pole figures from incomplete measurements and gives the kriging error as a <b>measure</b> <b>of</b> <b>precision.</b> 2...|$|R
40|$|Abstract-In current Command and Control system design, {{the concept}} of {{information}} plays a central role. In order to find architectures for situation and threat databases {{making full use of}} all dimensions of information, {{the concept of}} information awareness must be understood. We consider and define some information attributes: <b>measures</b> <b>of</b> <b>precision,</b> quality and usability, and suggest some uses of these concepts. The analysis is Bayesian. A critical point is where subjective Bayesian probabilities of decision makers meet the objective sensor-related Bayesian assessments of the system. This interface must be designed to avoid credibility problems. ...|$|E
40|$|International audienceConstruction of closed-form {{confidence}} intervals on linear combinations of variance components were developed generically for balanced data and studied for simple {{analysis of variance}} models. Satterthwaite approach is generalized to unbalanced data and modified to increase its coverage probability. They are applied on <b>measures</b> <b>of</b> <b>precision</b> in combination with (restricted) maximum likelihood and Henderson III Type 1 & 3 estimation. Simulation studies do not show superiority {{of any of the}} possible combinations of estimation methods and Satterthwaite approaches on three precision measures. However, the modified Satterthwaite approach with Henderson III Type 3 estimation is often preferred above the other combinations...|$|E
40|$|Ground-gathered {{data and}} LANDSAT {{multispectral}} scanner (MSS) digital data from 1981 were analyzed {{to produce a}} classification of Kansas land areas into specific types called land covers. The land covers included rangeland, forest, residential, commercial/industrial, and various types of water. The analysis produced two outputs: acreage estimates with <b>measures</b> <b>of</b> <b>precision,</b> and map-type or photo products of the classification which can be overlaid on maps at specific scales. State-level acreage estimates were obtained and substate-level land cover classification overlays and estimates were generated for selected geographical areas. These products {{were found to be}} of potential use in managing land and water resources...|$|E
40|$|A {{cornerstone}} of the target article is that, in a predictive coding framework, attention can be modelled by weighting prediction error with a <b>measure</b> <b>of</b> <b>precision.</b> We argue {{that this is not}} a complete explanation, especially in the light of ERP (event-related potentials) data showing large evoked responses for frequently presented target stimuli, which thus are predicted. © 2013 Cambridge University Press...|$|R
40|$|The L_ 2 -discrepancy is a {{quantitative}} <b>measure</b> <b>of</b> <b>precision</b> for multivariate quadrature rules. It can be computed explicitly. Previously known algorithms needed O(m" 2) operations, where m {{is the number}} of nodes. In this paper we present algorithms which require O(m(log m) "d) operations. (orig.) Available from TIB Hannover: RN 7281 (267) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekSIGLEDEGerman...|$|R
30|$|Depth {{discrimination}} thresholds {{were used}} as a <b>measure</b> <b>of</b> the <b>precision</b> <b>of</b> relative depth judgments. Depth realism {{might be expected to}} increase as the <b>precision</b> <b>of</b> relative depth judgments increases.|$|R
40|$|CONFERENCE PAPER The ICESat orbit {{has been}} {{determined}} using one of two BlackJack GPS receivers. The accuracy of the radial orbit component has been validated using ground-based Satellite Laser Ranging to an accuracy of 1 - 2 cm, which exceeds the pre-launch requirement of 5 cm [...] Other <b>measures</b> <b>of</b> <b>precision,</b> such as “orbit overlaps, ” yield better than 1 cm precision [...] Although ICESat does not have stringent prediction requirements, the 600 km altitude makes it a suitable object for prediction experiments. As expected, atmospheric density variability is the dominant, and significant, source of prediction error...|$|E
40|$|Monolingual and {{multilingual}} terminology and collocation bases, covering a specific domain, used independently or integrated with other resources, {{have become a}} valuable electronic resource. Building of such resources could be assisted by automatic term extraction tools, combining statistical and linguistic approaches. In this paper, the research on term extraction from monolingual corpus is presented. The corpus consists of publicly accessible English legislative documents. In the paper, results of two hybrid approaches are compared: extraction using the TermeX tool and an automatic statistical extraction procedure followed by linguistic filtering through the open source linguistic engineering tool. The results have been elaborated through statistical <b>measures</b> <b>of</b> <b>precision,</b> recall, and F-measure...|$|E
40|$|Nonprobability {{sampling}} describes any {{method for}} collecting survey data {{which does not}} utilize a full probability sampling design. Nonprobability samples are usually cheaper and easier to collect than probability samples. However, {{there are a number}} of drawbacks. Such methods can be prone to selection bias, and standard design-based methods of inference cannot be used to ensure approximately unbiased estimators of population quantities or to provide associated <b>measures</b> <b>of</b> <b>precision.</b> In this article, some of the more common methods of nonprobability sampling, quota sampling in particular, are introduced. Their advantages and disadvantages are discussed, and a formal framework for assessing the validity of inferences from nonprobability samples is described...|$|E
3000|$|The {{geographic}} grid we use {{is generally}} finer {{than the original}} pixel scale, and we further derive sub-pixel displacement; therefore, the image resolution does not quantize the obtained CMVs. However, it does not guarantee high precision, as sub-images used for template matching could be featureless or could be deformed excessively over time. IH 16 proposed a <b>measure</b> <b>of</b> <b>precision</b> based on the 90 [...]...|$|R
30|$|Different {{from the}} NOEC approach, a CI can be {{calculated}} for ECxx as a <b>measure</b> <b>of</b> <b>precision.</b> The width <b>of</b> the ECxx confidence interval depends, among others, {{on the value of}} xx. In contrast to the NOEC, ECxx itself does not depend on the design of the experiment, which makes interlaboratory comparisons of ECxx more consistent than comparing NOEC values [14], even if different experimental designs are involved.|$|R
40|$|Problem statement: This article {{emphasized}} on {{the construction}} of valid inferential procedures for an estimator &# 952; ^ as a <b>measure</b> <b>of</b> its statistical <b>precision</b> for dependent data structure. Approach: The truncated geometric bootstrap estimates of standard error and other <b>measures</b> <b>of</b> statistical <b>precision</b> such as bias, coefficient of variation, ratio and root mean square error are considered. Results: We extend it to other <b>measures</b> <b>of</b> statistical <b>precision</b> such as bootstrap confidence interval for an estimator &# 952; ^ and illustrate with real geological data. Conclusion/Recommendations: The bootstrap estimates of standard error and other <b>measures</b> <b>of</b> statistical accuracy such as bias, ratio, coefficient of variation and root mean square error reveals the suitability of the method for dependent data structure...|$|R
30|$|In {{the work}} of Honorato and Monard[8], the authors {{developed}} a framework for the extraction of terminology using the hybrid approach for the medical report. This framework, called ‘Term Pattern Discover’ (TP-Discover), in summary, selects words and phrases that occur with a certain Absolute Frequency (statistical method) and, for that purpose, the lemmatisation technique (linguistic method) is applied using the TreeTagger [74] lemmatiser. Then, the terms that follow predefined morphosyntactic patters are selected (for example: term ‘terço distal’ (‘distal third’) follows the N+Adj pattern). As {{we did not have}} access to the exact <b>measures</b> <b>of</b> <b>precision</b> and recall, based in their available results, we assumed that the best F-measure value was 59 %.|$|E
40|$|As {{developers}} work on {{a software}} product they accumu-late expertise, including expertise about the code base of the software product. We call this type of expertise ‘implemen-tation expertise’. Knowing the set of developers who have implementation expertise for a software product has many important uses. This paper presents an empirical evalua-tion of two approaches to determining implementation ex-pertise from the data in source and bug repositories. The expertise sets created by the approaches are compared to those provided by experts and evaluated using the <b>measures</b> <b>of</b> <b>precision</b> and recall. We found that both approaches are good at finding all of the appropriate developers, although they vary in how many false positives are returned. ...|$|E
40|$|Abstract. —Many {{laboratories}} rely on periodic re-reading {{of reference}} collections of scales or otoliths {{to ensure that}} their age readers remain consistent in their age interpretations, both through time and with other age readers. Measures of both systematic difference (bias) and precision are required for this purpose, be-cause measures of bias are not suitable as <b>measures</b> <b>of</b> <b>precision,</b> and vice versa. Using data from an age com-parison study of haddock Melanogrammus aeglefinus for demonstration purposes, we evaluated a variety of graphical and statistical approaches for making paired age comparisons from the standpoint of both detecting age differences and assessing precision. Parametric and nonparametric matched-pair tests, regression analysis, analysis of variance, and age difference plots were al...|$|E
40|$|Standard data {{analysis}} pipelines for digital PCR estimate {{the concentration of}} a target nucleic acid by digitizing the end-point fluorescence of the parallel micro-PCR reactions, using an automated hard threshold. While {{it is known that}} misclassification has a major impact on the concentration estimate and substantially reduces accuracy, the uncertainty of this classification is typically ignored. We introduce a model-based clustering method to estimate the probability that the target is present (absent) in a partition conditional on its observed fluorescence and the distributional shape in no-template control samples. This methodology acknowledges the inherent uncertainty of the classification and provides a natural <b>measure</b> <b>of</b> <b>precision,</b> both at individual partition level and {{at the level of the}} global concentration. We illustrate our method on genetically modified organism, inhibition, dynamic range, and mutation detection experiments. We show that our method provides concentration estimates of similar accuracy or better than the current standard, along with a more realistic <b>measure</b> <b>of</b> <b>precision.</b> The individual partition probabilities and diagnostic density plots further allow for some quality control. An R implementation of our method, called Umbrella, is available, providing a more objective and automated {{data analysis}} procedure for absolute dPCR quantification...|$|R
40|$|Heritability {{is often}} used by plant breeders and geneticists as a <b>measure</b> <b>of</b> <b>precision</b> <b>of</b> a trial or a series of trials. Its main use is for {{computing}} the response to selection. Most formulas proposed for calculating heritability implicitly assume balanced data and independent genotypic effects. Both of these assumptions are often violated in plant breeding trials. This article proposes a simulation-based approach to tackle the problem. The key idea is to directly simulate the quantity of interest, e. g., response to selection, {{rather than trying to}} approximate it using some ad hoc <b>measure</b> <b>of</b> heritability. The approach is illustrated by three examples...|$|R
40|$|The {{concepts}} of the curved exponential family of distributions and ancillarity are applied to estimation problems of a single structural equation in a simultaneous equation model, {{and the effect of}} conditioning on ancillary statistics on the limited information maximum-likelihood (LIML) estimator is investigated. The asymptotic conditional covariance matrix of the LIML estimator conditioned on the second-order asymptotic maximal ancillary statistic is shown to be efficiently estimated by Liu and Breen's formula. The effect of conditioning on a second-order asymptotic ancillary statistic, i. e., the smallest characteristic root associated with the LIML estimation, is analyzed by means of an asymptotic expansion of the distribution as well as the exact distribution. The smallest root helps to give an intuitively appealing <b>measure</b> <b>of</b> <b>precision</b> <b>of</b> the LIML estimator. ...|$|R
