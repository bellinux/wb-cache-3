798|10000|Public
25|$|In 1809 Carl Friedrich Gauss {{published}} his method of calculating {{the orbits of}} celestial bodies. In that work {{he claimed to have}} been in possession of the method of least squares since 1795. This naturally led to a priority dispute with Legendre. However, to Gauss's credit, he went beyond Legendre and succeeded in connecting the method of least squares with the principles of probability and to the normal distribution. He had managed to complete Laplace's program of specifying a mathematical form of the probability density for the observations, depending on a finite number of unknown parameters, and define a <b>method</b> <b>of</b> <b>estimation</b> that minimizes the error of estimation. Gauss showed that arithmetic mean is indeed the best estimate of the location parameter by changing both the probability density and the <b>method</b> <b>of</b> <b>estimation.</b> He then turned the problem around by asking what form the density should have and what <b>method</b> <b>of</b> <b>estimation</b> should be used to get the arithmetic mean as estimate of the location parameter. In this attempt, he invented the normal distribution.|$|E
25|$|In open-angle glaucoma, {{the typical}} {{progression}} from normal vision to complete blindness takes about 25 years to 70 years without treatment, {{depending on the}} <b>method</b> <b>of</b> <b>estimation</b> used. The intraocular pressure can also have an effect, with higher pressures reducing the time until blindness.|$|E
25|$|It {{would appear}} from both these later derivations {{that the only}} {{assumptions}} really needed for the inequality itself (as opposed to the <b>method</b> <b>of</b> <b>estimation</b> of the test statistic) are that {{the distribution of the}} possible states of the source remains constant and the detectors on the two sides act independently.|$|E
50|$|The {{second set}} <b>of</b> <b>methods</b> <b>of</b> <b>estimation</b> <b>of</b> {{heritability}} involves ANOVA and <b>estimation</b> <b>of</b> variance components.|$|R
5000|$|... #Subtitle level 2: Regression/correlation <b>methods</b> <b>of</b> <b>estimation</b> ...|$|R
40|$|The {{interrelation}} between analytic {{functions and}} real-valued functions is formulated in the work. It is shown such an interrelation realizes nonlinear representations for real-valued functions that allow {{to develop new}} <b>methods</b> <b>of</b> <b>estimation</b> for them. These <b>methods</b> <b>of</b> <b>estimation</b> are approved by solving the Cauchy problem for equations of viscous incompressible liquid...|$|R
25|$|The {{development}} of a criterion that can be evaluated to determine when the solution with the minimum error has been achieved. Laplace tried to specify a mathematical form of the probability density for the errors and define a <b>method</b> <b>of</b> <b>estimation</b> that minimizes the error of estimation. For this purpose, Laplace used a symmetric two-sided exponential distribution we now call Laplace distribution to model the error distribution, and used the sum of absolute deviation as error of estimation. He felt these to be the simplest assumptions he could make, and {{he had hoped to}} obtain the arithmetic mean as the best estimate. Instead, his estimator was the posterior median.|$|E
25|$|Research {{published}} in 2016 by Mathieu Ossendrijver based on tablets {{found in the}} British Museum provides evidence that the Babylonians {{even went so far}} as to have a concept of objects in an abstract mathematical space. The tablets date from between 350 and 50 B.C.E., revealing that the Babylonians understood and used geometry even earlier than previously thought. Ossendrijver showed that the Babylonians used a method for estimating the area under a curve by drawing a trapezoid underneath, a technique previously believed to have originated in 14th century Europe. This <b>method</b> <b>of</b> <b>estimation</b> allowed them to, for example, find the distance Jupiter had traveled in a certain amount of time.|$|E
2500|$|The {{method of}} maximum {{likelihood}} estimates θ0 by finding {{a value of}} θ that maximizes [...] This <b>method</b> <b>of</b> <b>estimation</b> defines a maximum likelihood estimator (MLE) of θ0: ...|$|E
5000|$|Scaling law, {{mathematical}} <b>methods</b> <b>of</b> <b>estimation</b> {{and expert}} opinion to estimate necessary data ...|$|R
40|$|The {{main purpose}} of this paper is to obtain {{estimates}} of parameters, reliability and hazard rate functions of a heterogeneous population represented by finite mixture of two general components. The doubly Type II censoring of generalized order statistics scheme is used. Maximum likelihood and Bayes <b>methods</b> <b>of</b> <b>estimation</b> are used for this purpose. The two <b>methods</b> <b>of</b> <b>estimation</b> are compared via a Monte Carlo Simulation study...|$|R
40|$|This article {{addresses}} the different <b>methods</b> <b>of</b> <b>estimation</b> <b>of</b> the probability density function (PDF) and the {{cumulative distribution function}} (CDF) for the Lindley distribution. Following estimation methods are considered: uniformly minimum variance unbiased estimator (UMVUE), maximum likelihood estimator (MLE), percentile estimator (PCE), least square estimator (LSE), weighted least square estimator (WLSE), Cramér-von-Mises estimator (CVME), Anderson-Darling estimator (ADE). Monte Carlo simulations are performed to compare the performances <b>of</b> the proposed <b>methods</b> <b>of</b> <b>estimation...</b>|$|R
2500|$|The final wave, which mainly saw the {{refinement}} {{and expansion}} of earlier developments, emerged from the collaborative work between Egon Pearson and Jerzy Neyman in the 1930s. They introduced the concepts of [...] "Type II" [...] error, power of a test and confidence intervals. Jerzy Neyman in 1934 showed that stratified random sampling was in general a better <b>method</b> <b>of</b> <b>estimation</b> than purposive (quota) sampling.|$|E
2500|$|Lynn and Vanhanen {{base their}} {{analysis}} on [...] selected IQ data from studies which covered 113 nations. For another 79 nations, they estimated the mean IQs {{on the basis}} of the arithmetic means of the measured IQs of neighboring countries. They justify this <b>method</b> <b>of</b> <b>estimation</b> by claiming that the correlation between the estimated national IQs they reported in IQ and the Wealth of Nations and the measured national IQs since obtained is very high (0.91).|$|E
2500|$|Because some {{governments have}} {{strongly}} promoted atheism {{and others have}} strongly condemned it, atheism may be either over-reported or under-reported for different countries. There {{is a great deal}} of room for debate as to the accuracy of any <b>method</b> <b>of</b> <b>estimation,</b> as the opportunity for misreporting (intentionally or not) a category of people without an organizational structure is high. [...] Also, many surveys on religious identification ask people to identify themselves as [...] "agnostics" [...] or [...] "atheists", which is potentially confusing, since these terms are interpreted differently, with some identifying themselves as being agnostic atheists. Additionally, many of these surveys only gauge the number of irreligious people, not the number of actual atheists, or group the two together. For example, research indicates that the fastest growing religious status may be [...] "no religion" [...] in the United States, but this includes all kinds of atheists, agnostics, and theists. Non-religious people make up 9.66%, while one fifth of them are atheists.|$|E
40|$|In {{this paper}} the {{robustness}} of Barro's new classical model of unemployment is examined using both two- and three-equation systems, to various sample periods and <b>methods</b> <b>of</b> <b>estimation.</b> The <b>methods</b> <b>of</b> <b>estimation</b> used are two-step OLS, two-step OLS with correct standard errors {{using the same}} and different numbers of observations for the expectations and structural equations, full information maximum likelihood, and four novel variants of the computationally intensive parametric bootstrap method. Not surprisingly, the empirical results for Barro's model change according to the sample period chosen, the different <b>methods</b> <b>of</b> <b>estimation,</b> and the different ways of computing the covariance matrix for purposes of inference. However, the empirical results and inferences are generally robust to variations in {{any or all of}} these options. ...|$|R
5000|$|Bauer, D.J. & Sterba, S.K. (2011). Fitting {{multilevel}} {{models with}} ordinal outcomes: performance of alternative specifications and <b>methods</b> <b>of</b> <b>estimation.</b> Psychological <b>Methods,</b> 16, 373-390.|$|R
50|$|Estimation {{methods for}} using left-censored data vary, {{and not all}} <b>methods</b> <b>of</b> <b>estimation</b> may be {{applicable}} to, or the most reliable, for all data sets.|$|R
2500|$|PR-STV {{provides}} proportionality by transferring {{votes to}} minimise waste of votes, and therefore also minimises {{the number of}} unrepresented voters. In this way PR-STV provides Droop proportionality - an example STV election using the Droop quota method for 9 seats and with no exhausted preferences would guarantee representation to every distinct group of 10% of the voters, with at most just under 10% of the vote being wasted as unneeded excess. Unlike other proportional representation methods employing party lists, voters in STV do not explicitly state their preferred political party (with the exception where above-the-line voting systems are in place); this in turn can create some difficulty when attempting to analyse how an STV election's results compare with the nationwide partisan makeup. One common method of estimating the party identification of voters is to assume their top-preference on their ballot represents a candidate from their preferred party, however this <b>method</b> <b>of</b> <b>estimation</b> is made more complicated {{by the possibility of}} independent candidates and of cross-party voting. However valid comparisons have and can still be made if sufficient data and information are available. In Victoria, Australia, it is possible to make a direct comparison between the Australian Senate election and the Victorian Upper House elections although individual circumstance will always exist. Voting patterns have shown that most voters stay with their chosen party within a limited percentage range based on local issues and circumstances. The main advantage in Victoria's case is that both systems are similar in design with one being a subset of the other. Victoria held its first multi-member proportional representation election in November 2006 for the Legislative Council ...|$|E
50|$|In statistics, minimum {{chi-square}} estimation is a <b>method</b> <b>of</b> <b>estimation</b> of unobserved quantities {{based on}} observed data.|$|E
50|$|If {{there exists}} a finite mean for the {{posterior}} distribution, then the posterior mean is a <b>method</b> <b>of</b> <b>estimation.</b>|$|E
40|$|Although {{the term}} “antioxidant ” is used very frequently, there are {{problems}} with the definition <b>of</b> antioxidants and <b>estimation</b> <b>of</b> antioxidant activity. The distinction between antioxidant and antiradical activities is not always obvious. This minireview discusses critically the principles, advantages and limitations of the most frequently used <b>methods</b> <b>of</b> <b>estimation</b> <b>of</b> antiradical and antioxidant activities...|$|R
40|$|Expressions for the {{expected}} value and {{the variance of}} the longest matched segment on the surfaces of two random shapes are obtained. Applications of these estimates to antibody-antigen pattern recognition during immune response are considered. <b>Methods</b> <b>of</b> <b>estimation</b> <b>of</b> the involved probabilities are presented with relevant computational results...|$|R
40|$|In {{this paper}} we address the problem <b>of</b> <b>estimation</b> <b>of</b> the {{variance}} of a normal population based on a balanced {{as well as an}} unbalanced ranked set sample (RSS), which is a modification of the original RSS of McIntyre (1952). We have proposed several <b>methods</b> <b>of</b> <b>estimation</b> <b>of</b> variance by combining different unbiased between and within estimators, and compared their performances. link_to_subscribed_fulltex...|$|R
5000|$|The {{method of}} maximum {{likelihood}} estimates θ0 by finding {{a value of}} θ that maximizes [...] This <b>method</b> <b>of</b> <b>estimation</b> defines a maximum likelihood estimator (MLE) of θ0: ...|$|E
5000|$|In statistics, the plug-in {{principle}} [...] is the <b>method</b> <b>of</b> <b>estimation</b> of functionals of {{a population}} distribution by evaluating the same functionals at the empirical distribution based on a sample.|$|E
5000|$|These {{considerations}} do not [...] "invalidate" [...] M-estimation in any way. They merely {{make clear}} that some care is needed in their use, as is true of any other <b>method</b> <b>of</b> <b>estimation.</b>|$|E
40|$|While the two {{exercises}} are independent {{in that they}} purport to answer separate questions, methodologically they are very similar. They both compare the efficiency <b>of</b> different <b>methods</b> <b>of</b> <b>estimation</b> <b>of</b> parameters by the familiar <b>method</b> <b>of</b> ratio F of variances of estimate, stochastically decisive, under very general conditions, when number of observations T tends towards infinity...|$|R
40|$|This paper {{deals with}} an <b>estimation</b> <b>of</b> output gap and {{potential}} output for Russian’s economy. Three <b>methods</b> <b>of</b> <b>estimation</b> {{have been used}} for estimating these two unobservable variables: Hodrick-Prescott filter, production function and SVAR model. All <b>methods</b> <b>of</b> <b>estimation</b> showed very similar course, although obtained values were not identical. Then obtained values of output gap were used to analyse the ability of output gap to forecast inflation. Two simple gap models were used for this purpose. Results showed that output gap could be used as useful indicator of inflation, according Keywords: output gap, HP filter, SVAR model, production function, inflatio...|$|R
40|$|Methods {{derived from}} fractal {{geometry}} {{can be used}} in many areas of neurobiology. For example, fractal dimension is a quantitative measure of cell shape complexity. Three <b>methods</b> <b>of</b> <b>estimation</b> <b>of</b> this parameter are described, i. e. box counting, dilation, and mass-radius. The advantages of fractal analysis and some problems are briefly discussed...|$|R
50|$|Boehm refined the Delphi <b>method</b> <b>of</b> <b>estimation</b> {{to include}} more group iteration, making it more {{suitable}} for certain classes of problems, such as software development. This variant is called the Wideband Delphi method.|$|E
50|$|In 1809 Carl Friedrich Gauss {{published}} his method of calculating {{the orbits of}} celestial bodies. In that work {{he claimed to have}} been in possession of the method of least squares since 1795. This naturally led to a priority dispute with Legendre. However, to Gauss's credit, he went beyond Legendre and succeeded in connecting the method of least squares with the principles of probability and to the normal distribution. He had managed to complete Laplace's program of specifying a mathematical form of the probability density for the observations, depending on a finite number of unknown parameters, and define a <b>method</b> <b>of</b> <b>estimation</b> that minimizes the error of estimation. Gauss showed that arithmetic mean is indeed the best estimate of the location parameter by changing both the probability density and the <b>method</b> <b>of</b> <b>estimation.</b> He then turned the problem around by asking what form the density should have and what <b>method</b> <b>of</b> <b>estimation</b> should be used to get the arithmetic mean as estimate of the location parameter. In this attempt, he invented the normal distribution.|$|E
5000|$|Response Rate 3 (RR3) - {{estimates}} what {{proportion of}} cases of unknown eligibility is actually eligible. Those respondents estimated to be ineligible are excluded from the denominator. The <b>method</b> <b>of</b> <b>estimation</b> *must* be explicitly stated with RR3.|$|E
40|$|Existent {{approaches}} and <b>methods</b> <b>of</b> <b>estimation</b> <b>of</b> competitiveness of enterprise are considered. Thrown out suggestions about level classification of descriptions of competitiveness. Offered to recommendation on perfection <b>of</b> <b>estimation</b> <b>of</b> competitiveness of enterprises of chemical industry {{taking into account}} the specific of their activity...|$|R
30|$|Here, we {{describe}} three <b>methods</b> <b>of</b> <b>estimation</b> <b>of</b> the unknown {{parameters of the}} NW-X family. The methods are: ordinary least square (OLS) estimation, percentile based estimation and maximum likelihood <b>estimation.</b> The performance <b>of</b> these <b>estimation</b> <b>methods</b> are studied through Monte Carlo simulation.|$|R
50|$|There {{are other}} <b>methods</b> <b>of</b> <b>estimation</b> that {{minimize}} the posterior risk (expected-posterior loss) {{with respect to}} a loss function, and these are of interest to statistical decision theory using the sampling distribution ("frequentist statistics").|$|R
