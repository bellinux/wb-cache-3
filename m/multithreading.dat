2056|3075|Public
5|$|Simultaneous <b>multithreading</b> (of which Intel's Hyper-Threading is {{the best}} known) was an early form of pseudo-multi-coreism. A {{processor}} capable of simultaneous <b>multithreading</b> includes multiple execution units in the same processing unit—that is it has a superscalar architecture—and can issue multiple instructions per clock cycle from multiple threads. Temporal <b>multithreading</b> {{on the other hand}} includes a single execution unit in the same processing unit and can issue one instruction at a time from multiple threads.|$|E
5|$|According to Intel, it skips the 45nm process {{technology}} {{and uses a}} 32nm {{process technology}}. It features eight cores and has a 12-wide issue architecture, <b>multithreading</b> enhancements, and new instructions {{to take advantage of}} parallelism, especially in virtualization.|$|E
5|$|Google began {{a project}} named Unladen Swallow in 2009 {{with the aim}} of {{speeding}} up the Python interpreter fivefold by using the LLVM, and of improving its <b>multithreading</b> ability to scale to thousands of cores.|$|E
5000|$|... {{coarse-grained}} <b>multithreaded</b> processor IP {{core and}} later the first fine-grained <b>multithreaded</b> processor IP core ...|$|R
5000|$|Classic. 4K, M14K, 24K, 34K, 74K, 1004K (multicore and <b>multithreaded)</b> and 1074K (superscalar and <b>multithreaded)</b> families.|$|R
40|$|Abstract — Nowadays, <b>multithreaded</b> {{architectures}} {{are becoming}} more and more popular. In fact, many processor vendors have already shipped processors with <b>multithreaded</b> features. Regardless of this push on <b>multithreaded</b> processors, still today there is not a clear procedure that defines how to measure the behavior of a <b>multithreaded</b> processor. This paper presents FAME, a new evaluation methodology aimed to fairly measure the performance of <b>multithreaded</b> processors. FAME can be used in conjunction with any of the metrics proposed for <b>multithreaded</b> processors like IPC throughput, weighted speedup, etc. The idea behind FAME is to reexecute all threads in a <b>multithreaded</b> workload until all of them are fairly represented in the final measurements taken from the workload. Then these measurements will be combined with the corresponding metric to obtain a final value that quantifies the performance of the processor under consideration. I...|$|R
5|$|New {{information}} presents {{improvements in}} <b>multithreading,</b> resilency improvements (Intel Instruction Replay RAS) and few new instructions (thread priority, integer instruction, cache prefetching, and data access hints).|$|E
5|$|Given Intel's {{history of}} {{disclosing}} details about Itanium microprocessors at ISSCC, this paper most likely refers to Poulson. Analyst David Kanter speculates that Poulson {{will use a}} new microarchitecture, with a more advanced form of <b>multithreading</b> that uses up to two threads, to improve performance for single threaded and multithreaded workloads.|$|E
25|$|In 2010, Pervasive Software {{released}} Pervasive PSQL v11, {{which allows}} users {{to take full advantage}} of <b>multithreading</b> for faster database processing.|$|E
40|$|Explicit {{hardware}} {{support for}} <b>multithreaded</b> software, {{either in the}} form of shared-memory chip multiprocessors or hardware <b>multithreaded</b> architectures, is becoming increasingly common. As such support becomes available, application developers are expected to exploit these developments by employing <b>multithreaded</b> programming. But although threads simplify the program’s conceptual design, they also increase programming complexity. In writing sharedmemory <b>multithreaded</b> applications, programmers must ensure that threads interact correctly, and this requires care and expertise. Errors in accessing shared-data object...|$|R
40|$|This {{position}} paper argues for {{an approach to}} bring several techniques successful for (regression) testing of sequential code over to <b>multithreaded</b> code. <b>Multithreaded</b> code is getting increasingly important but remains extremely hard to develop and test. Most recent research on testing <b>multithreaded</b> code focuses solely on finding bugs in one given version of code. While there are many promising results, the tools are fairly slow (as they, conceptually, explore {{a large number of}} schedules) and do not exploit the fact that code evolves over several versions during development and maintenance. Our proposal is to allow explicit specification of relevant schedules (either manually written or automatically generated) for <b>multithreaded</b> tests, which can substantially speed up testing, especially for evolving code. To enable the use of schedules, we propose to design a novel language for specifying schedules in <b>multithreaded</b> tests, and to develop tools for automatic generation of <b>multithreaded</b> tests and for improved regression testing with <b>multithreaded</b> tests. 1...|$|R
40|$|This {{dissertation}} addresses {{operating system}} thread scheduling for chip <b>multithreaded</b> processors. Chip <b>multithreaded</b> processors are becoming mainstream {{thanks to their}} superior performance and power characteristics. Threads running concurrently on a chip <b>multithreaded</b> processor share the processor’s resources. Resource contention, and accordingly performance, depends on characteristics of the co-scheduled threads. The operating system controls thread co-scheduling, and thus affects performance of a chip <b>multithreaded</b> system. This dissertation describes the design and implementation of three new scheduling algorithms for chip <b>multithreaded</b> processors: the nonwork-conserving algorithm, the target-miss-rate algorithm, and the cachefair algorithm. These algorithms target contention for the second-level cache, a recognized performance-critical resource, and pursue several objectives: performance optimization, fairness, and performanc...|$|R
25|$|Multiple {{lines of}} the same {{computer}} program may be simultaneously executed using threads. <b>Multithreading</b> processors are optimized to execute multiple threads efficiently.|$|E
25|$|Intel's Larrabee {{multicore}} architecture project uses {{a processor}} core {{derived from a}} P5 core (P54C), augmented by <b>multithreading,</b> 64-bit instructions, and a 16-wide vector processing unit. Intel's low-powered Bonnell microarchitecture employed in early Atom processor cores also uses an in-order dual pipeline similar to P5.|$|E
25|$|Several {{optional}} extensions {{are also}} available, including MIPS-3D {{which is a}} simple set of floating-point SIMD instructions dedicated to common 3D tasks, MDMX (MaDMaX) which is a more extensive integer SIMD instruction set using the 64-bit floating-point registers, MIPS16e which adds compression to the instruction stream to make programs take up less room, and MIPS MT, which adds <b>multithreading</b> capability.|$|E
50|$|Single and <b>multithread</b> modes: if a {{computer}} supports multi-threading, video creation process is performed faster in <b>multithread</b> mode, {{especially on a}} multi-core system.|$|R
5000|$|... session-information: unless {{using the}} <b>multithreaded</b> server, the {{instance}} stores its session-information in the PGA. In a <b>multithreaded</b> server, the session-information {{goes in the}} SGA.) ...|$|R
40|$|The {{advent of}} {{multicore}} processors has necessitated {{the use of}} parallelism to extract greater software performance. Shared-memory <b>multithreaded</b> programming is currently the dominant parallel programming paradigm. However, <b>multithreaded</b> programs are difficult to get right and are often afflicted by bugs like data races, deadlocks, and atomicity violations which may be triggered only by {{a specific set of}} schedules. <b>Multithreaded</b> programs are also difficult to test. Since the behavior of <b>multithreaded</b> programs can depend on the schedule, developers need to express and enforce schedules in <b>multithreaded</b> tests. However, there exists no reliable, modular, efficient, and intuitive methodology for expressing and enforcing schedules in <b>multithreaded</b> tests. Traditionally, developers enforce schedules with time delays, e. g., using Thread. sleep in Java. Unfortunately, this sleep-based approach can produce false positives or negatives, and can result in unnecessarily long testing time. This dissertation presents a novel framework, called IMUnit, for expressing and enforcing schedules reliably and efficiently in <b>multithreaded</b> tests. IMUnit includes a new language for specifying schedules as constraints on events encountered during test execution and a too...|$|R
25|$|Copland is an {{unreleased}} {{operating system}} prototype for Apple Macintosh computers {{of the late}} 1990s, intended to be released as the modern System 8 successor to the aging but venerable System 7. It introduced protected memory, preemptive multitasking, {{and a number of}} new underlying operating system features, while retaining compatibility with existing Mac applications. Copland's planned successor, codenamed Gershwin, was intended to add advanced features such as application-level <b>multithreading.</b>|$|E
25|$|The {{first few}} {{generations of the}} Alpha chips {{were some of the}} most {{innovative}} of their time. The first version, the Alpha 21064 or EV4, was the first CMOS microprocessor whose operating frequency rivalled higher-powered ECL minicomputers and mainframes. The second, 21164 or EV5, was the first microprocessor to place a large secondary cache on chip. The third, 21264 or EV6, was the first microprocessor to combine both high operating frequency and the more complicated out-of-order execution microarchitecture. The 21364 or EV7 was the first high performance processor to have an on-chip memory controller. The unproduced 21464 or EV8 would have been the first to include simultaneous <b>multithreading,</b> but this version was canceled after the sale of DEC to Compaq. The Tarantula research project, which most likely would have been called EV9, would have been the first Alpha processor to feature a vector unit.|$|E
25|$|Jet 3.0 {{included}} many enhancements, {{including a new}} index structure that reduced storage size and the time that was taken to create indices that were highly duplicated, the removal of read locks on index pages, a new mechanism for page reuse, a new compacting method for which compacting the database resulted in the indices being stored in a clustered-index format, a new page allocation mechanism to improve Jet's read-ahead capabilities, improved delete operations that speeded processing, <b>multithreading</b> (three threads were used to perform read ahead, write behind, and cache maintenance), implicit transactions (users {{did not have to}} instruct the engine to start manually and commit transactions to the database), a new sort engine, long values (such as memos or binary data types) were stored in separate tables, and dynamic buffering (whereby Jet's cache was dynamically allocated at start up and had no limit and which changed from a first in, first out (FIFO) buffer replacement policy to a least recently used (LRU) buffer replacement policy). Jet 3.0 also allowed for database replication.|$|E
40|$|Designing and {{implementing}} thread-safe <b>multithreaded</b> libraries {{can be a}} daunting task as developers of these libraries need {{to ensure that their}} implementations are free from concurrency bugs, including deadlocks. The usual practice involves employing software testing and/or dynamic analysis to detect. deadlocks. Their effectiveness is dependent on well-designed <b>multithreaded</b> test cases. Unsurprisingly, developing <b>multithreaded</b> tests is significantly harder than developing sequential tests for obvious reasons. In this paper, we address the problem of automatically synthesizing <b>multithreaded</b> tests that can induce deadlocks. The key insight to our approach is that a subset of the properties observed when a deadlock manifests in a concurrent execution can also be observed in a single threaded execution. We design a novel, automatic, scalable and directed approach that identifies these properties and synthesizes a deadlock revealing <b>multithreaded</b> test. The input to our approach is the library implementation under consideration and the output is a set of deadlock revealing <b>multithreaded</b> tests. We have implemented our approach as part of a tool, named OMEN 1. OMEN is able to synthesize <b>multithreaded</b> tests on many <b>multithreaded</b> Java libraries. Applying a dynamic deadlock detector on the execution of the synthesized tests results in the detection of a number of deadlocks, including 35 real deadlocks in classes documented as thread-safe. Moreover, our experimental results show that dynamic analysis on <b>multithreaded</b> tests that are either synthesized randomly or developed by third-party programmers are ineffective in detecting the deadlocks...|$|R
40|$|<b>Multithreaded</b> servers, while {{relatively}} simple {{to design and}} to implement, tend not to scale well for large numbers of concurrent users. Event-driven servers, which do scale well, are generally more difficult to design, write, and debug than <b>multithreaded</b> servers. Virtual Threads is a new server programming model in which the programmer writes a <b>multithreaded</b> server and a preprocessor automatically converts the server to a sophisticated event-driven server. We describe our current implementation of Virtual Threads and present benchmarks showing that a <b>multithreaded</b> Web server that uses Virtual Threads scales {{as well as an}} event-driven server. ...|$|R
5000|$|In {{parallel}} computing, work stealing is a scheduling {{strategy for}} <b>multithreaded</b> computer programs. It solves {{the problem of}} executing a dynamically <b>multithreaded</b> computation, one that can [...] "spawn" [...] new threads of execution, on a statically <b>multithreaded</b> computer, with a fixed number of processors (or cores). It does so efficiently {{both in terms of}} execution time, memory usage, and inter-processor communication.|$|R
25|$|The SIMD vector {{processor}} (VMX128) was modified for the Xbox {{to include a}} dot-product instruction. The dot-product instruction took far less latency than discrete instructions. The VMX128 was also modified {{by the addition of}} direct 3D (D3D) compressed data format. This led to an approximate 50 percent savings in required band-width and memory footprint making the CPU having a theoretical peak performance of 115.2GFLOPS, being capable of 9.6 billion dot products per second. Each core of the CPU was capable of simultaneous <b>multithreading</b> and was clocked at 3.2GHz. However, to reduce CPU die size, complexity, cost, and power demands, the processor used in-order execution in contrast to the Intel Coppermine 128-based Pentium III used in the original Xbox, which used more complex out of order execution. The original chip used a 90nm process, although a newer 65nm process SOI revision was implemented on later models, which was in-turn superseded by a 45nm combined CPU and GPU chip. A 21.6GB/s front side bus, aggregated 10.8GB/s upstream and downstream, connected Xenon with the graphics processor/northbridge. Xenon was equipped with an 8th way set associative 1MB Level 2 cache on-die running at half CPU clock speed. This cache was shared amongst the three CPU cores. Each core had separate L1 caches, each containing a two-way set associative 32-Kbyte L1 instruction cache and a four-way set associative 32-Kbyte L1 data cache. The write-through data cache did not allocate cache lines on writes. The CPU also contained ROM storing Microsoft private encrypted keys, used to decrypt game data. The heat sink implemented to cool the Xenon CPU was composed of aluminum fins with a copper base, and a heat pipe. Newer revisions, which had a smaller core, do not feature the heat pipe or copper base. The heat sink was cooled by two 70mm fans {{at the rear of the}} console on original-style consoles, while a single fan mounted on the side of the consoles was used in Xbox 360 S consoles. There were several types of fan used in Xbox 360s, which were produced by Nidec, Sunon and Delta Electronics.|$|E
500|$|PHP {{received}} {{mixed reviews}} due to lacking support for <b>multithreading</b> {{at the core}} language level, though using threads is {{made possible by the}} [...] "pthreads" [...] PECL extension.|$|E
2500|$|Performance enhancements, {{including}} <b>multithreading</b> of texture synthesis and autogen {{to provide}} modest performance improvements on multi-core computers ...|$|E
40|$|The {{architecture}} of future {{high performance computer}} systems will respond to the possibilities offered by technology and to the increasing demand for attention to issues of programmability. <b>Multithreaded</b> processing element architectures are a promising alternative to RISC architecture and its multiple-instruction-issue extensions such as VLIW, superscalar, and superpipelined architectures. This paper presents an overview of <b>multithreaded</b> computer architectures and the technical issues affecting their prospective evolution. We introduce the basic concepts of <b>multithreaded</b> computer architecture and describe several architectures representative of the design space for <b>multithreaded,</b> parallel computers. We review design issues for <b>multithreaded</b> processing elements intended for use as the node processor of parallel computers for scientific computing. These include the question of choosing an appropriate program execution model, {{the organization of the}} processing element to achieve good utilization of major resources, support for fine-grain interprocessor communication and global memory access, compiling machine code for <b>multithreaded</b> processors, and the challenge of implementing virtual memory in large-scale multiprocessor systems...|$|R
40|$|The main aim of {{this study}} is to promote the {{efficiency}} of a control system using a <b>multithread</b> digital control design. In this system, the management of a computer`s input and output information is handled appropriately by the program language. The <b>multithread</b> digital control design is used in the robotic arm`s tracking system. The advantage of this <b>multithread</b> digital control design is to activate each procedure running simultaneously when the transient overload of the information`s input and output in the control system occurs. Therefore, the time run in the <b>multithread</b> system will be shorter than that run in a traditional single thread system in which each procedure is lined up for running. In this study, case studies of <b>multithread</b> application used in image tracking and robot control are introduced. The results reveal that the speed of the tracking system can be improved by using the <b>multithread</b> technique under an immediate procedure plan...|$|R
40|$|This thesis {{presents}} cross-domain {{approaches that}} improve the {{effective use of}} <b>multithreaded</b> architectures. The contributions of the thesis can be classified in three groups. First, we propose several methods for thread assignment of network applications running in <b>multithreaded</b> network servers. Second, we analyze the problem of graph partitioning that {{is a part of}} the compilation process of <b>multithreaded</b> streaming applications. Finally, we present a method that improves the measurement-based timing analysis of <b>multithreaded</b> architectures used in time-critical environments. The following sections summarize each of the contributions. (1) Thread assignment on <b>multithreaded</b> processors: State-of-the-art <b>multithreaded</b> processors have different level of resource sharing (e. g. between thread running on the same core and globally shared resources). Thus, the way that threads of a given workload are assigned to processors' hardware contexts determines which resources the threads share, which, in turn, may significantly affect the system performance. In this thesis, we demonstrate the importance of thread assignment for network applications running in <b>multithreaded</b> servers. We also present TSBSched and BlackBox scheduler, methods for thread assignment of <b>multithreaded</b> network applications running on processors with several levels of resource sharing. Finally, we propose a statistical approach to the thread assignment problem. In particular, we show that running a sample of several hundred or several thousand random thread assignments is sufficient to capture at least one out of 1...|$|R
2500|$|Support for <b>multithreading</b> – run {{multiple}} virtual machines concurrently, each {{in its own}} thread {{for improved}} stability and performance ...|$|E
2500|$|The OS is single-user and employs {{cooperative}} multitasking (CMT). While most current desktop OSes use preemptive multitasking (PMT) and <b>multithreading,</b> [...] remains with a CMT system. By 2003, many users {{had called for}} the OS to migrate to PMT. The OS memory protection is not comprehensive.|$|E
2500|$|Android {{has another}} {{operating}} system, Trusty OS, within it, {{as a part}} of [...] "Trusty" [...] "software components supporting a Trusted Execution Environment (TEE) on mobile devices." [...] "Trusty and the Trusty API are subject to change. [...] Applications for the Trusty OS can be written in C/C++ (C++ support is limited), and they have access to a small C library. [...] All Trusty applications are single-threaded; <b>multithreading</b> in Trusty userspace currently is unsupported. [...] Third-party application development is not supported in" [...] the current version, and software running on the OS and processor for it, run the [...] "DRM framework for protected content. [...] There are many other uses for a TEE such as mobile payments, secure banking, full-disk encryption, multi-factor authentication, device reset protection, replay-protected persistent storage, wireless display ("cast") of protected content, secure PIN and fingerprint processing, and even malware detection." ...|$|E
40|$|Successful {{software}} evolves as developers {{add more}} features, respond to requirements changes, and fix faults. Regression testing {{is widely used}} for ensuring the validity of evolving software. As regression test suites grow over time, it becomes expensive to execute them. The problem is exacerbated when test suites contain <b>multithreaded</b> tests. These tests are generally long running as they explore many different thread schedules searching for concurrencyfaults suchas dataraces, atomicity violations, and deadlocks. While many techniques have been proposed for regression test prioritization, selection, and minimization for sequential tests, {{there is not much}} work for <b>multithreaded</b> code. We present a novel technique, called Change-Aware Preemption Prioritization (CAPP), thatuses information about the changes in software evolution to prioritize the exploration of schedules in a <b>multithreaded</b> regression test. We have implemented CAPP in two frameworks for systematic exploration of <b>multithreaded</b> Java code. We evaluated CAPP on the detection of 15 faults in <b>multithreaded</b> Java programs, including large open-source programs. The results show that CAPP can substantially reduce the exploration required to detect <b>multithreaded</b> regression faults. Categories andSubject Descriptor...|$|R
40|$|With the {{continuing}} emergence of <b>multithreaded</b> computation {{as a powerful}} vehicle for science and engineering, {{the need for an}} introduction to <b>multithreaded</b> programming for scientists and engineers is high. All popular operating systems already support <b>multithreaded</b> programming and the popular POSIX Pthreads standard has been approved. It is the right time to teach students this new technology. This paper presents the problems and difficulties we encountered and a set of comprehensive and flexible course materials for a <b>multithreaded</b> programming course for sophomore and junior students. This paper also presents the design of pedagogical tools for the students to visualize and experiment with various concepts in <b>multithreaded</b> programming. These concepts include program behavior and execution visualization, deadlock and race condition detection, and software metrics for measuring the complexity of students' programs. 1. INTRODUCTION John Hopcroft challenged the computer science community [...] ...|$|R
40|$|Foundational and {{scalable}} {{techniques for}} runtime safety analysis of <b>multithreaded</b> programs are explored in this paper. A technique based on vector clocks {{to extract the}} causal dependency order on state updates from a running <b>multithreaded</b> program is presented, together with algorithms to analyze a <b>multithreaded</b> computation against safety properties expressed using temporal logics. A prototype tool implementing our techniques, is also presented, together with examples where it can predict safety errors in <b>multithreaded</b> programs from successful executions of those programs. This tool is called Java MultiPathExplorer (JMPaX), and available for download on the web. To {{the best of our}} knowledge, JMPaX is the first tool of its kind...|$|R
