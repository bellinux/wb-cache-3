8|3|Public
50|$|The 8086/8088 {{could be}} {{connected}} to a <b>mathematical</b> <b>coprocessor</b> to add hardware/microcode-based floating-point performance. The Intel 8087 was the standard math coprocessor for the 8086 and 8088, operating on 80-bit numbers. Manufacturers like Cyrix (8087-compatible) and Weitek (not 8087-compatible) eventually came up with high-performance floating-point coprocessors that competed with the 8087, {{as well as with}} the subsequent, higher-performing Intel 80387.|$|E
50|$|He {{had moved}} to the University of Chicago in 1949 and his PhD was awarded in 1950. He then joined the Physics Department of the University of Chicago. From 1962 to 1968 he was Director of the University of Chicago Computation Centre. Later he was Professor of Physics and Chemistry at the University of Chicago. Since his retirement, in 1988, he has worked for the Hewlett-Packard Laboratories in Palo Alto, California, where his primary {{contribution}} has been {{in the development of the}} <b>mathematical</b> <b>coprocessor</b> routines for the Itanium chip. His method of analyzing pipeline architecture has been unique and innovative and greatly admired in supercomputer circles around the world.|$|E
40|$|Abstract- This paper {{deals with}} {{the design of a}} <b>mathematical</b> <b>{{coprocessor}}</b> using RISC based approach. The coprocessor can relieve the main processor of large matrix based computations usually used in the field of image processing, cryptography, etc. The protocol for communication of the processor with the computer is also designed in the project. The timing required for the designed processor is checked with a Visual Basic interface. The main processor can transmit matrix values to the coprocessor, which can return the main processor with the desired results. The <b>mathematical</b> <b>coprocessor</b> was implemented on Spartan III Field Programmable Gate Array using VHD...|$|E
50|$|The didactical {{approach}} to floating point calculations, {{which has been}} introduced in a comparable manner already in the early 1940s by Konrad Zuse, is introduced by using elemental sublevel operations for exponent and mantissa involved in the key operations of addition/subtraction and multiplication/division.A set of powerful 32-bit floating point arithmetic commands in mantissa and exponent for the basic operations and elementary analytical functions are provided, as they are realized in todayâ€™s <b>mathematical</b> <b>coprocessors.</b> Here, in the simulation with MikroSim it is ideally assumed that the execution of each supported ALU arithmetic operation requires only a distinct computing duration independent of circuit complexity realistically needed in practice.|$|R
40|$|In {{this paper}} the basic {{structure}} and features of SCNN 2000, a universal simulation system for Cellular Neural Networks (CNN) is presented. Since the rst presentation of SCNN [1] {{the structure of the}} simulation system has been changed to achieve more exibility in simulating CNN. Especially, a wider class of training algorithms including new optimization methods have been implemented. SCNN 2000 also supports several kinds of CNN hardware as <b>mathematical</b> <b>coprocessors.</b> Additionally, a new SCNN control system has been developed, including a new graphical user interface and an integrated SCNN shell to allow a more convenient working with SCNN 2000. In this part of the contribution {{the basic structure}} and features of SCNN 2000 will be discussed, whereas the SCNN control system is presented in a second paper [2]. 1. Introduction Since it's rst presentation in [1] SCNN 1 {{has become one of the}} mostly used simulation systems for CNN [3]. It operates under dierent systems, like AIX-UNIX, [...] ...|$|R
40|$|The data {{analysis}} tool {{of choice for}} many Sun-Earth Connection missions is the Interactive Data Language (IDL) by ITT VIS. The increasing amount of data produced by these missions and the increasing complexity of image processing algorithms requires access to higher computing power. Parallel computing is a cost-effective way {{to increase the speed}} of computation, but algorithms oftentimes have to be modified to take advantage of parallel systems. Enhancing IDL to work on clusters gives scientists access to increased performance in a familiar programming environment. The goal of this project was to enable IDL applications to benefit from both computing clusters as well as graphics processing units (GPUs) for accelerating {{data analysis}} tasks. The tool suite developed in this project enables scientists now to solve demanding data analysis problems in IDL that previously required specialized software, and it allows them to be solved orders of magnitude faster than on conventional PCs. The tool suite consists of three components: (1) TaskDL, a software tool that simplifies the creation and management of task farms, collections of tasks that can be processed independently and require only small amounts of data communication; (2) mpiDL, a tool that allows IDL developers to use the Message Passing Interface (MPI) inside IDL for problems that require large amounts of data to be exchanged among multiple processors; and (3) GPULib, a tool that simplifies the use of GPUs as <b>mathematical</b> <b>coprocessors</b> from within IDL. mpiDL is unique in its support for the full MPI standard and its support of a broad range of MPI implementations. GPULib is unique in enabling users to take advantage of an inexpensive piece of hardware, possibly already installed in their computer, and achieve orders of magnitude faster execution time for numerically complex algorithms. TaskDL enables the simple setup and management of task farms on compute clusters. The products developed in this project have the potential to interact, so one can build a cluster of PCs, each equipped with a GPU, and use mpiDL to communicate between the nodes and GPULib to accelerate the computations on each node...|$|R
40|$|Several {{computer}} subroutines {{have been}} developed that interpolate three types of nonanalytic functions: univariate, bivariate, and map. The routines use data in floating-point form. However, because they are written for use on a 16 -bit Intel 8086 system with an 8087 <b>mathematical</b> <b>coprocessor,</b> they execute as fast as routines using data in scaled integer form. Although all of the routines are written in assembly language, they have been implemented in a modular fashion so as to facilitate their use with high-level languages...|$|E
40|$|The paper {{focuses on}} {{identification}} {{issues of the}} advanced controller ASPECT * that is implemented on a sim-ple PLC platform with an extra <b>mathematical</b> <b>coprocessor</b> and is intended for the advanced control of complex plants. The model of the controlled plant is obtained by means of experimental modelling using an online lear-ning procedure that combines model identification with pre- and post-identification steps that provide reliable ope-ration. It is shown that acceptable performance {{of the system is}} obtained despite difficult conditions that may arise during operation. Key words: programmable logic controllers, fuzzy modelling, identification, nonlinear control, pH control...|$|E
40|$|This thesis delves {{into the}} field of general purpose {{computation}} on graphics processing units (GPGPU). A MATLAB interface for solving numerical linear algebra on the graphics processing unit (GPU), and three algorithms from numerical linear algebra are presented. The algorithms are shown to be faster than the highly efficient ATLAS implementations used in MAT-LAB. In addition, the interface allows background processing on the GPU, enabling it {{to be used as}} a <b>mathematical</b> <b>coprocessor.</b> The computations are shown to be sufficiently accurate, and solving the shallow water equations implicitly is shown where both the CPU and the GPU are both utilized for maximum performance. A comparison of the interface and other high-level languages for GPGPU is also presented. iii Content...|$|E
40|$|Modern {{automatic}} multi-electrode survey instruments {{have made}} it possible to use non-traditional arrays to maximize the subsurface resolution from electrical imaging surveys. Previous studies have shown that one of the best methods for generating optimized arrays is to select the set of array configurations that maximizes the model resolution for a homogeneous earth model. The Sherman-Morrison Rank- 1 update is used to calculate the change in the model resolution when a new array is added to a selected set of array configurations. This method had the disadvantage that it required several hours of computer time even for short 2 -D survey lines. The algorithm was modified to calculate the change in the model resolution rather than the entire resolution matrix. This reduces the computer time and memory required as well as the computational round-off errors. The matrix-vector multiplications for a single add-on array were replaced with matrix-matrix multiplications for 28 add-on arrays to further reduce the computer time. The temporary variables were stored in the double-precision SIMD registers within the CPU to minimize computer memory access. A further reduction in the computer time is achieved by using the computer graphics card GPU as a highly parallel <b>mathematical</b> <b>coprocessor.</b> This makes it possible to carry out th...|$|E
40|$|A {{model and}} a {{computer}} program {{have been developed for}} the prediction of breakthrough curves of a non-isothermal adsorption column, packed with porous spherical particles. At time zero, a step change in the concentration of an adsorbable component is introduced to the flowing stream. Together with this concentration step, the temperature of the flowing stream may be changed. The adsorption column is subjected to axial dispersion, external film diffusion resistance, pore diffusion resistance, heat effects of the adsorption process, axial heat transfer resistance of the solid and the fluid phase, external film heat transfer resistance and heat transfer from the fluid and the solid phase to the wall. Radial gradients, the pressure drop across the bed, mass accumulation in the pores and the heat capacity of the fluid in the pores are neglected. Other assumptions are: plug flow takes place in the bed: heat capacities, densities and the adsorption heat are constant with respect to concentrations, temperatures and time; and the wall temperature is uniform and constant. At the solid surface, the fluid phase and solid phase are assumed to be in equilibrium. This equilibrium must be represented by an isotherm that is linear with respect to concentration and temperature, because for the method of solution, this isotherm must be Laplace transformed. The model equations are solved by Laplace transformation, both with respect to time and place. The inverse Laplace transform with respect to place is calculated analytical by complex integration. The inverse Laplace transform with respect to time is calculated numerically by applying the Fast Fourier Transform. As a result, the computer program calculates concentration and temperature breakthrough curves in a few minutes on a Personal Computer with a 8086 processor and a 8087 <b>mathematical</b> <b>coprocessor.</b> The model describes quantitative the existence of different breakthrough regimes and temperature plateaus. The effects of variations of the process parameters, such as diffusion coefficients and heat transfer resistances are described qualitative. In order to make the program useful for a more general set of fixed-bed processes, future developments should be directed towards a more general adsorption isotherm and the radial components of the model equations. Especially the possibility of non-linear isotherms would make the program very powerful. Chemical Reactor EngineeringDelftChemTechApplied Science...|$|E

