1|6|Public
40|$|Disaster Recovery (DR) is a {{desirable}} feature for all enterprises, and a crucial one for many. However, adoption of DR remains limited {{due to the}} stark tradeoffs it imposes. To recover an application {{to the point of}} crash, one is limited by financial considerations, substantial application overhead, or minimal geographical separation between the primary and recovery sites. In this paper, we argue for cloud-based DR and pipelined synchronous replication as an antidote to these problems. Cloud hosting promises economies of scale and on-demand provisioning that are a perfect fit for the infrequent yet urgent needs of DR. Pipelined synchrony addresses the impact of WAN replication latency on performance, by efficiently overlapping replication with application processing for multi-tier servers. By tracking the consequences of the disk modifications that are persisted to a recovery site all the way to client-directed messages, applications realize forward progress while retaining full consistency guarantees for client-visible state {{in the event of a}} disaster. PipeCloud, our prototype, is able to sustain these guarantees for <b>multi-node</b> <b>servers</b> composed of black-box VMs, with no need of application modification, resulting in a perfect fit for the arbitrary nature of VM-based cloud hosting. We demonstrate disaster failover to the Amazon EC 2 platform, and show that PipeCloud can increase throughput by an order of magnitude and reduce response times by more than half compared to synchronous replication, all while providing the same zero data loss consistency guarantees...|$|E
50|$|The EPA {{released}} Version 1.0 of the Computer Server specifications on May 15, 2009. It covered standalone servers {{with one}} to four processor sockets. A second tier to the specification adding active state power and performance reporting for all qualified servers, as well as blade and <b>multi-node</b> <b>server</b> idle state requirements became effective December 16, 2013.|$|R
50|$|In a <b>multi-node</b> (clustered) <b>server,</b> {{the state}} of the {{upstream}} service will need to be reflected across all the nodes in the cluster. Therefore, implementations may need to use a persistent storage layer, e.g. a network cache such as Memcached or Redis, or local cache (disk or memory based) to record the availability of what is, to the application, an external service.|$|R
40|$|Artículo de publicación ISIThe Geostatistical Software Library (GSLIB) {{has been}} used in the geostatistical {{community}} for more than thirty years. It was designed as a bundle of sequential Fortran codes, and today it is still in use by many practitioners and researchers. Despite its widespread use, few attempts have been reported in order to bring this package to the multi-core era. Using all CPU resources, GSLIB algorithms can handle large datasets and grids, where tasks are compute- and memory-intensive applications. In this work, a methodology is presented to accelerate GSLIB applications using code optimization and hybrid parallel processing, specifically for compute-intensive applications. Minimal code modifications are added decreasing as much as possible the elapsed time of execution of the studied routines. If multi-core processing is available, the user can activate OpenMP directives to speed up the execution using all resources of the CPU. If multi-node processing is available, the execution is enhanced using MPI messages between the compute nodes. Four case studies are presented: experimental variogram calculation, kriging estimation, sequential gaussian and indicator simulation. For each application, three scenarios (small, large and extra large) are tested using a desktop environment with 4 CPU-cores and a <b>multi-node</b> <b>server</b> with 128 CPU-nodes. Elapsed times, speedup and efficiency results are shown...|$|R
50|$|In January, 2010, Panasas {{proposed}} an NFSv4.1 {{based on their}} Parallel NFS (pNFS) technology claiming to improve data-access parallelism capability. The NFSv4.1 protocol defines a method of separating the filesystem meta-data from file data location; it goes beyond the simple name/data separation by striping the data amongst a set of data servers. This differs from the traditional NFS server which holds the names of files and their data under the single umbrella of the server. Some products are <b>multi-node</b> NFS <b>servers,</b> but {{the participation of the}} client in separation of meta-data and data is limited.|$|R
40|$|The {{collaboration}} between SAS ® and Teradata on enterprise class scoring {{has produced the}} SAS ® Scoring Accelerator for Teradata which deploys SAS ® Enterprise Miner ™ scoring models in a highly scalable format to a Teradata server. Once deployed, these models {{have been shown to}} be linearly scalable, completely leveraging the power of the parallel shared-nothing architecture of Teradata. This presentation will cover the model deployment process from start to finish and include performance profiles from a <b>multi-node</b> Teradata <b>server.</b> There will also be a preview of how this technology will be integrated into SAS ® Model Manager 2. 2...|$|R

