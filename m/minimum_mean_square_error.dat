2313|10000|Public
5000|$|... so {{that the}} linear <b>minimum</b> <b>mean</b> <b>square</b> <b>error</b> {{estimator}} is given by ...|$|E
50|$|A Bayesian analog is a Bayes estimator, {{particularly}} with <b>minimum</b> <b>mean</b> <b>square</b> <b>error</b> (MMSE).|$|E
5000|$|Lastly, {{the error}} {{covariance}} and <b>minimum</b> <b>mean</b> <b>square</b> <b>error</b> achievable by such estimator is ...|$|E
40|$|In this article, we {{consider}} the risk performance of an iterative feasible <b>minimum</b> <b>mean</b> <b>squared</b> <b>error</b> estimator of the regression disturbance variance under the LINEX loss function. This loss is a generalisation of the quadratic loss function allowing for asymmetry. Notwithstanding the justification for using the feasible <b>minimum</b> <b>mean</b> <b>squared</b> <b>error</b> estimator in estimating the regression coefficients, {{it is found that}} the corresponding estimator of the disturbance variance does not, in general, improve over a class of conventional estimators commonly used in practice. Error variance LINEX loss <b>Minimum</b> <b>mean</b> <b>squared</b> <b>error</b> Risk...|$|R
5000|$|<b>Minimum</b> <b>mean</b> <b>squared</b> <b>error</b> (MMSE), {{also known}} as Bayes least <b>squared</b> <b>error</b> (BLSE) ...|$|R
40|$|Dash and Mishra [1] {{suggested}} an improved class of estimators without defining the optimum estimator. However, {{they gave the}} wrong Taylor’s series expression of their class of estimator and their <b>minimum</b> <b>mean</b> <b>squared</b> <b>error</b> expressions are also incorrect. Here we show that Ahmed et al. ’s [2] class of chain estimators is more efficient than Dash and Mishra’s [1], with <b>minimum</b> <b>mean</b> <b>squared</b> <b>error...</b>|$|R
5000|$|... thus {{providing}} an expression for the coefficients [...] of the <b>minimum</b> <b>mean</b> <b>square</b> <b>error</b> estimator.|$|E
50|$|The {{following}} {{is one way}} to find the <b>minimum</b> <b>mean</b> <b>square</b> <b>error</b> estimator by using the orthogonality principle.|$|E
50|$|Important {{potential}} {{properties of}} statistics include completeness, consistency, sufficiency, unbiasedness, <b>minimum</b> <b>mean</b> <b>square</b> <b>error,</b> low variance, robustness, and computational convenience.|$|E
3000|$|... [...]. Provided the linearized model (5) {{is a valid}} approximation, {{the optimal}} values, in a <b>minimum</b> <b>mean</b> <b>squared</b> <b>error</b> sense, of [...]...|$|R
30|$|<b>Minimum</b> <b>Mean</b> <b>Squared</b> <b>Error</b> Estimation of the Noise in Unobserved Component Models, Journal of Business & Economic Statistics, 5 (1), 115 – 120, 1987.|$|R
40|$|In this paper, {{we examine}} the small sample {{properties}} of the pre-test iterative variance estimator in regression. The explicit formula of MSE is derived, and it is shown that the pre-test iterative variance estimator with an appropriate critical value dominates the iterative variance estimator without pre-testing in terms of MSE. We also compare the MSE performances of the pre-test iterative variance estimators using the Stein-rule, <b>minimum</b> <b>mean</b> <b>squared</b> <b>error,</b> and adjusted <b>minimum</b> <b>mean</b> <b>squared</b> <b>error</b> estimators by numerical evaluations. Iterative variance estimator <b>Mean</b> <b>squared</b> <b>error</b> Pre-test Regression error variance...|$|R
5000|$|This we can {{recognize}} {{to be the}} same as [...] Thus the <b>minimum</b> <b>mean</b> <b>square</b> <b>error</b> achievable by such a linear estimator is ...|$|E
50|$|Using the MSE as risk, the Bayes {{estimate}} of the unknown parameter is simply {{the mean of the}} posterior distribution, This is known as the <b>minimum</b> <b>mean</b> <b>square</b> <b>error</b> (MMSE) estimator. The Bayes risk, in this case, is the posterior variance.|$|E
5000|$|While the OLS point {{estimator}} remains unbiased, {{it is not}} [...] "best" [...] in {{the sense}} of having <b>minimum</b> <b>mean</b> <b>square</b> <b>error,</b> and the OLS variance estimator [...] does not provide a consistent estimate of the variance of the OLS estimates.|$|E
40|$|We {{consider}} a superposition {{of an unknown}} number of independent homogeneous Poisson processes in which the source of each event can be identified. After observing the system for a fixed time t, the total rate, U(t), of the unobserved processes is to be estimated. We prove that a uniformly <b>minimum</b> <b>mean</b> <b>squared</b> <b>error</b> estimate of U(t) does not exist and all unbiased estimators of U(t) are negatively correlated with U(t) and derive the <b>minimum</b> <b>mean</b> <b>squared</b> <b>error</b> estimator among all unbiased estimators. <b>Mean</b> <b>squared</b> <b>error</b> Poisson process software reliability unbiasedness...|$|R
5000|$|One common {{estimator}} is the <b>minimum</b> <b>mean</b> <b>squared</b> <b>error</b> (MMSE) estimator, which utilizes {{the error}} between the estimated parameters {{and the actual}} value of the parameters ...|$|R
30|$|A note on <b>minimum</b> <b>mean</b> <b>squared</b> <b>error</b> {{estimation}} of signals with unit roots, Journal of Economic Dynamics and Control, 12 (2 – 3), 589 – 593, 1988.|$|R
5000|$|Further, {{while the}} {{corrected}} sample variance {{is the best}} unbiased estimator (<b>minimum</b> <b>mean</b> <b>square</b> <b>error</b> among unbiased estimators) of variance for Gaussian distributions, if the distribution is not Gaussian then even among unbiased estimators, the best unbiased estimator of the variance may not be ...|$|E
5000|$|The {{orthogonality}} {{principle is}} {{most commonly used}} {{in the setting of}} linear estimation. In this context, let x be an unknown random vector which is to be estimated based on the observation vector y. One wishes to construct a linear estimator [...] for some matrix H and vector c. Then, the orthogonality principle states that an estimator [...] achieves <b>minimum</b> <b>mean</b> <b>square</b> <b>error</b> if and only if ...|$|E
50|$|In {{statistics}} and signal processing, the orthogonality principle {{is a necessary}} and sufficient condition for the optimality of a Bayesian estimator. Loosely stated, the orthogonality principle says that the error vector of the optimal estimator (in a mean square error sense) is orthogonal to any possible estimator. The orthogonality principle is most commonly stated for linear estimators, but more general formulations are possible. Since the principle is a necessary and sufficient condition for optimality, {{it can be used}} to find the <b>minimum</b> <b>mean</b> <b>square</b> <b>error</b> estimator.|$|E
3000|$|... are {{computed}} online, {{resulting in}} good performance in nonstationary noise. A maximum likelihood approach is adopted in [25] and a Bayesian <b>minimum</b> <b>mean</b> <b>squared</b> <b>error</b> (MMSE) approach in [26].|$|R
40|$|In this paper, we {{consider}} a linear regression model when relevant regressors are omitted. We derive the explicit formulae for the predictive <b>mean</b> <b>squared</b> <b>errors</b> (PMSE’s) of the Stein-rule (SR) estimator, the positive-part Stein-rule (PSR) estimator, the <b>minimum</b> <b>mean</b> <b>squared</b> <b>error</b> (MMSE) estimator and the adjusted <b>minimum</b> <b>mean</b> <b>squared</b> <b>error</b> (AMMSE) estimator. It is shown analytically that the PSR estimator dominates the SR estimator {{in terms of}} PMSE even when there are omitted relevant regressors. Also, our numerical {{results show that the}} PSR estimator and the AMMSE estimator have much smaller PMSE’s than the OLS estimator even when the relevant regressors are omitted. ...|$|R
25|$|When {{the mean}} is not known, the <b>minimum</b> <b>mean</b> <b>squared</b> <b>error</b> {{estimate}} of the variance of a sample from Gaussian distribution is achieved by dividing by nnbsp&+nbsp&1, rather than nnbsp&−nbsp&1 or nnbsp&+nbsp&2.|$|R
50|$|A {{well-known}} example {{arises in}} {{the estimation of}} the population variance by sample variance. For a sample size of n, {{the use of a}} divisor n &minus; 1 in the usual formula (Bessel's correction) gives an unbiased estimator, while other divisors have lower MSE, at the expense of bias. The optimal choice of divisor (weighting of shrinkage) depends on the excess kurtosis of the population, as discussed at mean squared error: variance, but one can always do better (in terms of MSE) than the unbiased estimator; for the normal distribution a divisor of n + 1 gives one which has the <b>minimum</b> <b>mean</b> <b>square</b> <b>error.</b>|$|E
50|$|The {{goal of the}} Wiener filter is {{to compute}} a {{statistical}} estimate of an unknown signal using a related signal as an input and filtering that known signal to produce the estimate as an output. For example, the known signal might consist of an unknown signal of interest that has been corrupted by additive noise. The Wiener filter {{can be used to}} filter out the noise from the corrupted signal to provide an estimate of the underlying signal of interest. The Wiener filter is based on a statistical approach, and a more statistical account of the theory is given in the <b>minimum</b> <b>mean</b> <b>square</b> <b>error</b> (MMSE) estimator article.|$|E
50|$|Beamforming is {{the method}} used to create the {{radiation}} pattern of the antenna array by adding constructively the phases of the signals {{in the direction of}} the targets/mobiles desired, and nulling the pattern of the targets/mobiles that are undesired/interfering targets.This can be done with a simple Finite Impulse Response (FIR) tapped delay line filter. The weights of the FIR filter may also be changed adaptively, and used to provide optimal beamforming, in the sense that it reduces the <b>Minimum</b> <b>Mean</b> <b>Square</b> <b>Error</b> between the desired and actual beampattern formed. Typical algorithms are the steepest descent, and Least Mean Squares algorithms. In digital antenna arrays with multi channels use the digital beamforming, usually by DFT or FFT.|$|E
50|$|Common {{types of}} {{precoding}} include zero-forcing (ZF), <b>minimum</b> <b>mean</b> <b>squared</b> <b>error</b> (MMSE) precoding, maximum ratio transmission (MRT), and Block Diagonalization. Common types of spatial demultiplexing include ZF, MMSE combining, and successive interference cancellation.|$|R
40|$|Under a {{balanced}} loss function, we derive the explicit formulae {{of the risk}} of the Stein-rule (SR) estimator, the positive-part Stein-rule (PSR) estimator, the feasible <b>minimum</b> <b>mean</b> <b>squared</b> <b>error</b> (FMMSE) estimator, and the adjusted feasible <b>minimum</b> <b>mean</b> <b>squared</b> <b>error</b> (AFMMSE) estimator in a linear regression model with multivariate t errors. The results show that the PSR estimator dominates the SR estimator under the balanced loss and multivariate t errors. Also, our numerical results show that these estimators dominate the ordinary least squares (OLS) estimator when the weight of precision of estimation is larger than about half, and vice versa. Furthermore, the AFMMSE estimator dominates the PSR estimator in certain occasions...|$|R
3000|$|... (i)We derive {{the optimal}} joint {{design of the}} BS and RS filter {{matrices}} that achieves the <b>minimum</b> <b>mean</b> <b>squared</b> <b>error</b> (MMSE) for both downlink and uplink of the multiuser MIMO relay systems at the absence of direct path.|$|R
50|$|In {{statistics}} and signal processing, a <b>minimum</b> <b>mean</b> <b>square</b> <b>error</b> (MMSE) estimator is an estimation method which minimizes the {{mean square error}} (MSE), which is a common measure of estimator quality, of the fitted values of a dependent variable. In the Bayesian setting, the term MMSE more specifically refers to estimation with quadratic loss function. In such case, the MMSE estimator is given by the posterior mean of the parameter to be estimated. Since the posterior mean is cumbersome to calculate, {{the form of the}} MMSE estimator is usually constrained to be within a certain class of functions. Linear MMSE estimators are a popular choice since they are easy to use, calculate, and very versatile. It has given rise to many popular estimators such as the Wiener-Kolmogorov filter and Kalman filter.|$|E
3000|$|... and near-far {{resistance}} {{than the}} corresponding <b>minimum</b> <b>mean</b> <b>square</b> <b>error</b> (MMSE) filters with block memory.|$|E
3000|$|There {{are many}} {{techniques}} {{to estimate the}} channel state information exploiting pilot blocks, such as leastsquares (LS), <b>minimum</b> <b>mean</b> <b>square</b> <b>error</b> (MMSE), and linear <b>minimum</b> <b>mean</b> <b>square</b> <b>error</b> (LMMSE). A detailed description of channel estimation {{can be found in}} [21]. In this paper, we use LS channel estimation, which is the simplest method. Starting from system model of OFDM given in (9) the LS estimation of [...]...|$|E
50|$|When {{the mean}} is not known, the <b>minimum</b> <b>mean</b> <b>squared</b> <b>error</b> {{estimate}} of the variance of a sample from Gaussian distribution is achieved by dividing by n + 1, rather than n − 1 or n + 2.|$|R
40|$|This paper⇤ {{addresses}} {{the design of}} MSE-optimal preambles for multicarrier channelestimation under a maximum likelihood or <b>minimum</b> <b>mean</b> <b>squared</b> <b>error</b> criterion. The derived optimality condition gives insight on how to allocate the powerof the pilots that compose the preamble. While many papers show that equispacedand equipowered allocation is optimal, the generalized condition demonstratesthat there exist many different configurations that offer the same optimalperformance. Furthermore, the condition applies not only to maximum likelihoodbut also to <b>minimum</b> <b>mean</b> <b>squared</b> <b>error</b> channel estimation. An application ofthe generalized condition {{in the presence of}} inactive subcarriers (virtual subcarriersproblem) is shown such that a non equispaced allocation can achieve thesame optimal performance as if an equispaced one could be used. info:eu-repo/semantics/publishe...|$|R
30|$|F {{is chosen}} {{conforming}} to different criteria, either the zero forcing (ZF) criterion, which removes all channel distortions {{at risk of}} noise enhancement, or the <b>minimum</b> <b>mean</b> <b>squared</b> <b>error</b> (MMSE) criterion, which tries to minimize the effects of noise enhancement and channel distortion.|$|R
