96|10000|Public
2500|$|In {{probability}} theory and statistics, a copula is a <b>multivariate</b> <b>probability</b> <b>distribution</b> {{for which the}} marginal probability distribution of each variable is uniform. Copulas are {{used to describe the}} dependence between random variables. Their name comes from the Latin for [...] "link" [...] or [...] "tie", similar but unrelated to grammatical copulas in linguistics. [...] Copulas have been used widely in quantitative finance to model and minimize tail risk and portfolio optimization applications.|$|E
50|$|In statistics, the {{covariance}} matrix of a <b>multivariate</b> <b>probability</b> <b>distribution</b> is always positive semi-definite; {{and it is}} positive definite unless one variable is an exact linear function of the others. Conversely, every positive semi-definite matrix is the {{covariance matrix}} of some multivariate distribution.|$|E
50|$|The multivariate stable {{distribution}} is a <b>multivariate</b> <b>probability</b> <b>distribution</b> {{that is a}} multivariate generalisation of the univariate stable distribution. The multivariate stable distribution defines linear relations between stable distribution marginals. In {{the same way as}} for the univariate case, the {{distribution is}} defined in terms of its characteristic function.|$|E
5000|$|In addition, multivariate {{statistics}} {{is concerned with}} <b>multivariate</b> <b>probability</b> <b>distributions,</b> {{in terms of both}} ...|$|R
2500|$|The {{importance}} of the above is that the reverse of these steps {{can be used to}} generate pseudo-random samples from general classes of <b>multivariate</b> <b>probability</b> <b>distributions.</b> That is, given a procedure to generate a sample [...] from the copula distribution, the required sample can be constructed as ...|$|R
40|$|Modern {{statistical}} applications {{often require}} sampling from large <b>multivariate</b> <b>probability</b> <b>distributions.</b> This is often made difficult {{by the presence}} of the normalizing constant of the distribution, which requires an intractable sum or integral over all possible assignments to the random variable. To work around this, the technique of Markov Chain Monte Carlo allows us to sample from a <b>probability</b> <b>distribution</b> withou...|$|R
50|$|In statistics, {{the multivariate}} {{t-distribution}} (or multivariate Student distribution) is a <b>multivariate</b> <b>probability</b> <b>distribution.</b> It is a generalization to random vectors of the Students t-distribution, {{which is a}} distribution applicable to univariate random variables. While {{the case of a}} random matrix could be treated within this structure, the matrix t-distribution is distinct and makes particular use of the matrix structure.|$|E
5000|$|Generalizations of the Chow-Liu tree are the {{so-called}} t-cherry junction trees. It is {{proved that the}} t-cherry junction trees provide a better {{or at least as}} good approximation for a discrete <b>multivariate</b> <b>probability</b> <b>distribution</b> as the Chow-Liu tree gives.For the third order t-cherry junction tree see , for the kth-order t-cherry junction tree see [...] The second order t-cherry junction tree is in fact the Chow-Liu tree.|$|E
5000|$|In {{probability}} theory and statistics, a copula is a <b>multivariate</b> <b>probability</b> <b>distribution</b> {{for which the}} marginal probability distribution of each variable is uniform. Copulas are {{used to describe the}} dependence between random variables. Their name comes from the Latin for [...] "link" [...] or [...] "tie", similar but unrelated to grammatical copulas in linguistics. Copulas have been used widely in quantitative finance to model and minimize tail risk and portfolio optimization applications.|$|E
40|$|It was {{recently}} shown for arbitrary <b>multivariate</b> <b>probability</b> <b>distributions</b> that angular symmetry is completely characterized by location depth. We use this mathematical result {{to construct a}} statistical test of the null hypothesis that the data were generated from a symmetric distribution, and illustrate the test by several real examples. status: publishe...|$|R
5000|$|Gaussian {{graphical}} {{models are}} <b>multivariate</b> <b>probability</b> <b>distributions</b> encoding {{a network of}} dependencies among variables. Let [...] be a set of [...] variables, such as [...] dihedral angles, and let [...] be {{the value of the}} probability density function at a particular value D. A multivariate Gaussian graphical model defines this probability as follows: ...|$|R
40|$|Often analysts must conduct risk {{analysis}} {{based on a}} small number of observations. This paper describes and illustrates the use of a kernel density estimation procedure to smooth out irregularities in such a sparse data set for simulating univariate and <b>multivariate</b> <b>probability</b> <b>distributions.</b> stochastic simulation, smoothing, multivariate kernel estimator, Parzen, Research Methods/ Statistical Methods, Q 12, C 8,...|$|R
50|$|Graphical {{models can}} still be used when the {{variables}} of choice are continuous. In these cases, the probability distribution is represented as a <b>multivariate</b> <b>probability</b> <b>distribution</b> over continuous variables. Each family of distribution will then impose certain properties on the graphical model. Multivariate Gaussian distribution {{is one of the}} most convenient distributions in this problem. The simple form of the probability, and the direct relation with the corresponding graphical model makes it a popular choice among researchers.|$|E
50|$|In statistics, Gibbs {{sampling}} or a Gibbs sampler is a Markov chain Monte Carlo (MCMC) algorithm {{for obtaining}} {{a sequence of}} observations which are approximated from a specified <b>multivariate</b> <b>probability</b> <b>distribution,</b> when direct sampling is difficult. This sequence {{can be used to}} approximate the joint distribution (e.g., to generate a histogram of the distribution); to approximate the marginal distribution of one of the variables, or some subset of the variables (for example, the unknown parameters or latent variables); or to compute an integral (such as the expected value of one of the variables). Typically, some of the variables correspond to observations whose values are known, and hence {{do not need to be}} sampled.|$|E
40|$|We {{prove that}} the multivariate {{standard}} normal probability distribution function is concave for large argument values. The method of proof allows for the derivation of similar statements for other types of <b>multivariate</b> <b>probability</b> <b>distribution</b> functions too. The result has important application, e. g., in probabilistic constrained stochastic programming problems. RRR 5 - 2001 Page 1...|$|E
40|$|We extend and generalize to {{the multivariate}} set-up our earlier {{investigations}} related to expected remaining life functions and general hazard measures including representations and stability theorems for arbitrary <b>probability</b> <b>distributions</b> {{in terms of}} these concepts. (The univariate case is discussed in detail in Kotz and Shanbhag, Advan. Appl. Probab. 12 (1980), 903 - 921.) expected remaining life function hazard measure reliability representation and stability theorems identifiability <b>multivariate</b> <b>probability</b> <b>distributions...</b>|$|R
40|$|Reconstructability {{analysis}} (RA) decomposes wholes, namely data in {{the form}} either of settheoretic relations or <b>multivariate</b> <b>probability</b> <b>distributions,</b> into parts, namely relations or distributions involving subsets of variables. Data is modeled and compressed by variablebased decomposition, by more general state-based decomposition, or {{by the use of}} latent variables. Models, which specify the interdependencies among the variables, are selected to minimize error and complexity...|$|R
40|$|When {{studying}} entropy {{functions of}} <b>multivariate</b> <b>probability</b> <b>distributions,</b> polymatroids and matroids emerge. Entropy functions of pure multiparty quantum states {{give rise to}} analogous notions, called here polyquantoids and quantoids. Polymatroids and polyquantoids are related via linear mappings and duality. Quantum secret sharing schemes that are ideal are described by selfdual matroids. Expansions of integer polyquantoids to quantoids are studied and linked to that of polymatroids...|$|R
40|$|This {{paper will}} focus on a method to {{describe}} the long-term statistics of water levels and wave heights for a measuring location on relatively deep water. 2. Problem outline The probability of failure of any structure {{is a function of}} the design parameters and the <b>multivariate</b> <b>probability</b> <b>distribution</b> of load and strength variables by: () () (...|$|E
40|$|A {{technique}} {{is presented to}} combine data points, each available with point-dependent uncertainty, when only a subset of these points come from sources, where is unknown. We detect the significant modes of the underlying <b>multivariate</b> <b>probability</b> <b>distribution</b> using a generalization of the nonparametric mean shift procedure. The number of detected modes automatically defines, while the appurtenance of a point to the basin of attraction of a mode provides the fusion rule...|$|E
40|$|A {{graphical}} model {{is nothing but}} an illustration of an equation. This equation determins how a functional over multiple variables factorizes. We are mostly interested in functionals that represent a <b>multivariate</b> <b>probability</b> <b>distribution</b> â€“ but {{graphical model}}s and the related methods could be applied on any functional. Here are some basic examples for equations concerning the factorization of a functional P over variables X 1 : 4 and their graphical notation...|$|E
40|$|We give a {{probabilistic}} {{expression of}} the $ alpha $-determinant by using the Wishart distribution on the cone of positive definite matrices. As a corollary, we give a partial answer to a positivity problem for the $ alpha $-determinant which {{is equivalent to the}} existence problem of certain <b>multivariate</b> <b>probability</b> <b>distributions.</b> We also give a concrete example to support our conjecture for the positivity...|$|R
50|$|The multivariate {{statistics}} in special <b>multivariate</b> discrete <b>probability</b> <b>distributions.</b> Some procedures {{used in this}} context {{can be used in}} dealing with contingency tables.|$|R
5000|$|In {{probability}} and statistics, the Dirichlet distribution (after Peter Gustav Lejeune Dirichlet), often denoted , is {{a family}} of continuous <b>multivariate</b> <b>probability</b> <b>distributions</b> parameterized by a vector [...] of positive reals. It is a multivariate generalization of the beta distribution. [...] Dirichlet distributions are very often used as prior distributions in Bayesian statistics, {{and in fact the}} Dirichlet distribution is the conjugate prior of the categorical distribution and multinomial distribution.|$|R
40|$|AbstractRegular {{variation}} of {{the tail of a}} <b>multivariate</b> <b>probability</b> <b>distribution</b> is implied by regular {{variation of}} the density f provided f satisfies a regularity condition. We give a uniformity condition which controls {{variation of the}} function f across rays. Our condition is somewhat more flexible than the usual regularity condition of monotonicity. Some examples are given. As a by-product we get results on multidimensional regular variation of some independent interest...|$|E
3000|$|... {{with the}} {{alternative}} local approach which {{makes use of}} derivatives and which was introduced in Definition 1. In this sense, we continue to follow the general method of analyzing the non-Euclidean geometry underlying a <b>multivariate</b> <b>probability</b> <b>distribution</b> which was developed in (Richter [2009], [2013]). The following theorem says that the star-generalized surface measure coincides with the integral surface measure. For a comparison of these surface measures, it is sufficient to consider them for sets [...]...|$|E
40|$|A Taylor series {{approximation}} to multivariate integrals {{taken with}} respect to a <b>multivariate</b> <b>probability</b> <b>distribution</b> is proposed and applied to the computation of multivariate normal probabilities and conditional expectations. The approximation does not require that the multivariate distribution have a structured covariance matrix and, in its simplest form, can be written as the product of univariate integrals. The approximation is compared to that of Mendell and Elston (1974) for computing bivariate normal probabilities. Multivariate normal probabilities Taylor series...|$|E
5000|$|The {{importance}} of the above is that the reverse of these steps {{can be used to}} generate pseudo-random samples from general classes of <b>multivariate</b> <b>probability</b> <b>distributions.</b> That is, given a procedure to generate a sample [...] from the copula distribution, the required sample can be constructed asThe inverses [...] are unproblematic as the [...] were assumed to be continuous. The above formula for the copula function can be rewritten to correspond to this as: ...|$|R
40|$|A Bayesian {{network is}} a {{graph-based}} model of joint <b>multivariate</b> <b>probability</b> <b>distributions</b> that captures properties of conditional independence between variables. Such models are {{an effective way}} to characterize probabilistic and causal relations among variables and they provide a clear methodology for learning from observations. In this paper we apply structural learning algorithm to explore the conditional independency relations among the eBusiness Readiness Indicators and we discuss the obtained results. JRC. G. 9 -Econometrics and statistical support to antifrau...|$|R
40|$|Probabilistic {{graphical}} models {{bring together}} graph theory and probability theory {{in a powerful}} formalism for multivariate statistical modelling. Since many machine learning problems involve the modelling of <b>multivariate</b> <b>probability</b> <b>distributions,</b> graphical mod- els {{can be a good}} fit to these problems. In this thesis, we show that applying graphical models in machine learning problems can have several advantages: First, it can better capture the nature of the problem. Second, it gives us great flexibility in modelling. Finally, it provides us with...|$|R
40|$|Regular {{variation}} of {{the tail of a}} <b>multivariate</b> <b>probability</b> <b>distribution</b> is implied by regular {{variation of}} the density f provided f satisfies a regularity condition. We give a uniformity condition which controls {{variation of the}} function f across rays. Our condition is somewhat more flexible than the usual regularity condition of monotonicity. Some examples are given. As a by-product we get results on multidimensional regular variation of some independent interest. multidimensional regular variation domains of attraction multivariate extremes...|$|E
40|$|Let F and G be <b>multivariate</b> <b>probability</b> <b>distribution</b> functions, {{each with}} equal one {{dimensional}} marginals, such that {{there exists a}} sequence of constants an > 0, n [set membership, variant], with [formula] for all continuity points (x 1, [...] ., xd) of G. The distribution function G {{is characterized by the}} extreme-value index (determining the marginals) and the so-called angular measure (determining the dependence structure). In this paper, a non-parametric estimator of G, based on a random sample from F, is proposed. Consistency as well as asymptotic normality are proved under certain regularity conditions. ...|$|E
40|$|One way {{the social}} {{scientists}} explain phenomena is by building structural models. These models are explanatory {{insofar as they}} manage to perform a recursive decomposition on an initial <b>multivariate</b> <b>probability</b> <b>distribution,</b> which {{can be interpreted as}} a mechanism. The social scientists should include the variables in the model {{on the basis of their}} function in the mechanism. This paper examines the notion of `function' within structural modelling. We argue that `functions' ought to be understood as the theoretical underpinnings of the causes, namely as the role that causes play in the functioning of the mechanism...|$|E
50|$|In {{probability}} theory and statistics, the normal-inverse-gamma distribution (or Gaussian-inverse-gamma distribution) is a four-parameter family of <b>multivariate</b> continuous <b>probability</b> <b>distributions.</b> It is the conjugate prior {{of a normal}} distribution with unknown mean and variance.|$|R
40|$|We {{investigate}} the time evolution of stochastic non-Markov processes as they {{occur in the}} coarse-grained description of open and closed systems. We show that semigroups of propagators exist for all <b>multivariate</b> <b>probability</b> <b>distributions,</b> the generators of which yield a set of time-convolutionless master equations. We discuss the calculation of averages and time-correlation functions. Further, linear response theory is developed for such a system. We find that the response function cannot be expressed as an ordinary timecorrelation function. Some aspects of the theory are illustrated for the two-state process and the Gauss process...|$|R
40|$|Directed acyclic graphs (DAGs) are {{a popular}} {{framework}} to express <b>multivariate</b> <b>probability</b> <b>distributions.</b> Acyclic directed mixed graphs (ADMGs) are generalizations of DAGs that can succinctly capture much richer sets of conditional independencies, and are especially useful in modeling {{the effects of}} latent variables implicitly. Unfortunately there are currently no good parameterizations of general ADMGs. In this paper, we apply recent work on cumulative distribution networks and copulas to propose one one general construction for ADMG models. We consider a simple parameter estimation approach, and report some encouraging experimental results. Comment: 11 pages, 4 figure...|$|R
