4|24|Public
500|$|Scanline VFX {{completed}} the reveal {{shots of the}} Helicarrier, from the moment Black Widow and Captain America arrive on the carrier deck {{to the point where}} it lifts off. Evil Eye Pictures composited digital backgrounds into shots filmed against a greenscreen for scenes taking place inside the Helicarrier. Colin Strause of Hydraulx said, [...] "We did the opening ten minutes of the movie, other than the opening set-up in space" [...] including Loki's arrival on Earth and subsequent escape from the S.H.I.E.L.D. base. Luma Pictures worked on shots featuring the Helicarrier's bridge and incorporated the graphic monitor displays that were developed by Cantina Creative. Fuel VFX completed shots taking place in and around Tony Stark's penthouse at Stark Tower. Digital Domain created the asteroid environment, where Loki encounters The Other. Method Design in Los Angeles created the film's closing credits. Steve Viola, creative director at Method Design, said, [...] "This piece was a two-minute, self-contained main on end sequence created entirely in CG. For each of the shots in the sequence, we designed, modeled, textured, and lit all of the environments and many of the foreground objects. We received assets from Marvel to include in the piece, then heavily re-modeled and re-surfaced them to create a post-battle <b>macro</b> <b>sequence.</b> We also designed a custom typeface for the Main Title of The Avengers as well as 30 credits set in-scene." ...|$|E
40|$|Course {{description}} The {{object of}} {{this course is}} to introduce students {{to a variety of}} tools used in advanced dynamic macroeconomic models. The goal is to provide examples of how some specific problems are addressed and how the tools {{can be used in a}} variety of contexts. In the first part (taught by Davide Debortoli), we introduce techniques – both from a theoretical and a computational viewpoint – to analyze policy problems in dynamic models, and discuss some applications to optimal fiscal and monetary policy problems. By way of contrast with the models studied in the first-year <b>macro</b> <b>sequence</b> we study the implications of “inefficiences ” like market incompleteness, lack of commitment and default risk. In the second part of the course (taught by Giacomo Rondina) three independent topics will be covered. First, we will study “global games”, a framework that proves to be particularly effective in modeling coordination problems with incomplete information in macroeconomics (such as currency attacks, bank runs and financial crises more in general). Second, we will look into the fast developing literature on the “macroeconomics of asset shortage”, the view that many of the financial tensions at the global level can be explained by an endogenously varying supply of assets in which to store wealth, which includes non-fundamental valuations (i. e. “bubbles”) ...|$|E
40|$|Welcome to 14. 451, the {{introductory}} {{course of the}} <b>macro</b> <b>sequence.</b> The aim of this course is to familiarize you with the mechanics of growth models, and we anticipate {{that several of the}} models will provide knowledge spillovers into other courses you take here. It is difficult to say that you have been properly educated in growth theory without having come across the “Lucas quote ” (although this statement might be a little dated). So let us begin with it. Is there some action a government of India could take that would lead the Indian economy to grow like Indonesia’s or Egypt’s? If so, what, exactly? If not, {{what is it about the}} “nature of India ” that makes it so? The consequences for human welfare involved in questions like these are simply staggering: once one starts to think about them, it is hard to think about anything else. Lucas [JME 1988] This is the fundamental question we are grappling with; but in order to shed light upon this question we need to try to understand the sources of sustained economic growth. That is the object of the course. The purpose of this handout is to explore in further detail some of the empirical aspects of growth covered in the first lecture. 1. Historical Comparisons of Growth Performance The lecture notes highlighted the dispersion in incomes per capita and post-WWII growth rates. To provide further illustration, in Figure 1 below plots the real GNP per capita of several countries in 1992 against the historical performance of the US economy, using data from Maddison (1995). In 1992 Argentina’s income per capita was comparable to the US income per capita around World War II, and Pakistan’s income per capita in 1992 was below the US level of 1870. The differences are the product of many years of sustained growth...|$|E
5000|$|Special command mode window allows {{entering}} any <b>macro</b> command <b>sequence</b> directly, and {{doubles as}} on-line calculator.|$|R
5000|$|Early {{versions}} of the remote software required a web browser; newer versions are Java-based. The software requires constant Internet connectivity while programming the remote, as remote control codes are downloaded from Logitech. This method allows updates to the product database, remote codes, and <b>macro</b> <b>sequences</b> to be easily distributed. This also allows Logitech to survey their market {{in order to determine}} products for investigation and research. Harmony control software is available for Microsoft Windows and Mac OS X. A group of developers was working on Harmony Remote software for the Linux operating system; [...] the latest available release was dated August 2010.|$|R
40|$|The {{therapeutic}} {{perspectives of}} therapeutic nucleic acids and intracellularly active proteins rely on effective and safe intracellular delivery. Natural biological evolution of viruses or toxins has demonstrated how defined sequence domains can be optimized. A ?chemical evolution? {{process can be}} applied for such a carrier design, with subdomains for the various delivery steps, including improved cargo packaging, targeted cell uptake, and intracellular release to be presented in optimized sequence. Building blocks have to be identified which serve as microdomains useful in a delivery step; these units have to be assembled into defined <b>macro</b> <b>sequences</b> and topologies and be evaluated in assays; the obtained structure-activity relations can be utilized for {{the next round of}} carrier design. In our recent research, automated solid phase assisted peptide synthesis was adopted for the design of > 800 precise oligomers of different topologies (linear, branched, combs). Artificial oligoamino acids were combined with natural amino acids, fatty acids, precise PEG chains and other compounds. Branching points were served by lysines, endosomal-buffering was enhanced by histidines; terminal cysteines served for nanoparticle stabilization by bioreversible disulfide bond formation. Optionally, hydrophobic domains were included for stabilization. Targeting peptides, amphipathic membrane-destabilizing peptides also were incorporated. Such carriers can be well-tolerated and potent in intracellular delivery of pDNA, siRNA, proteins or small drugs...|$|R
5000|$|Macro Editor:The Macro Editor is {{actually}} a subcomponent of the Personality Editor, but it warrants its own description. The user can create their own <b>macros</b> which are <b>sequences</b> of commands that the RS Media can execute. Macros can also include conditional routines {{that are based on}} environmental or user feedback.|$|R
50|$|Keyboard macros {{and mouse}} <b>macros</b> allow short <b>sequences</b> of keystrokes and mouse actions to {{transform}} into other, usually more time-consuming, sequences of keystrokes and mouse actions. In this way, frequently used or repetitive sequences of keystrokes and mouse movements can be automated. Separate programs for creating these macros are called macro recorders.|$|R
25|$|Many {{assemblers}} support predefined macros, {{and others}} support programmer-defined (and repeatedly re-definable) <b>macros</b> involving <b>sequences</b> of text lines in which variables and constants are embedded. This sequence of text lines may include opcodes or directives. Once a macro {{has been defined}} its name {{may be used in}} place of a mnemonic. When the assembler processes such a statement, it replaces the statement with the text lines associated with that macro, then processes them as if they existed in the source code file (including, in some assemblers, expansion of any macros existing in the replacement text). Macros in this sense date to IBM autocoders of the 1950s.|$|R
40|$|The simulation-based {{training}} {{systems that}} are available or under development today for incident management are typically focused on <b>macro</b> level <b>sequence</b> of events. A few systems targeted at individual responders are under development using a gaming environment. Separate uses of such systems provide disparate experiences to decision makers and individual responders. There {{is a need to}} provide common training experiences to these groups for better effectiveness. This paper presents a novel approach integrating gaming and simulation systems for training of decision makers and responders on the same scenarios preparing them to work together as a team. An integrated systems architecture is proposed for this purpose. Major modules in gaming and simulation subsystems are define...|$|R
40|$|Geo-referenced data {{which are}} often communicated via maps are {{inaccessible}} to the visually impaired population. We summarise existing approaches to improving accessibility of geo-referenced data and present the Atlas. txt project which aims to produce textual summaries of such data which can be read out via a screenreader. We outline issues involved in generating descriptions of geo-referenced data and present initial work on content determination based on knowledge acquisition from both parallel corpus analysis and input from visually impaired people. In our corpus analysis we build an ontology containing abstract representations of expert-written sentences which we associate with <b>macros</b> containing <b>sequences</b> of data analysis methods. This helps us to identify which data analysis methods need {{to be applied to}} generate text from data. ...|$|R
40|$|In this paper, {{we apply}} {{the idea of}} testing games by {{learning}} interactions with them that cause unwanted behavior of the game to test the competition entries {{for some of the}} scenarios of the 2010 StarCraft AI competition. By extending the previously published macro action concept to include <b>macro</b> action <b>sequences</b> for individual game units, by adjusting the concept to the realtime requirements of StarCraft, and by using macros involving specific abilities of game units, our testing system was able to find either weaknesses or system crashes for all of the competition entries of the chosen scenarios. Additionally, by requiring a minimal margin with respect to surviving units, we were able to clearly identify the weaknesses of the tested AIs...|$|R
40|$|One of {{the most}} common {{mechanisms}} used for speeding up problem solvers is macrolearning. <b>Macros</b> are <b>sequences</b> of basic operators acquired during problem solving. Macros are used by the problem solver as if they were basic operators. The major problem that macro-learning presents is the vast number of macros that are available for acquisition. Macros increase the branching factor of the search space and can severely degrade problemsolving efficiency. To make macro learning useful, a program must be selective in acquiring and utilizing macros. This paper describes a general method for selective acquisition of macros. Solvable training problems are generated in increasing order of difficulty. The only macros acquired are those that take the problem solver out of a local minimum to a better state. The utility of the method is demonstrated in several domains, including the domain of N Θ N sliding-tile puzzles. After learning on small puzzles, the system is able to efficiently solve puz [...] ...|$|R
50|$|Opening a {{collection}} {{results in a}} project window with two main parts driven entirely by {{a unique set of}} GUI objects. On the left are a number of icon wells, representing new objects that could be created, and a scrollable window on the right (known as the Collection Window) displays the objects that the user has created. Icons are dragged from the wells on the left into the window to create new objects in the collection. These objects include relations (tables) to hold data, <b>sequences</b> (<b>macros)</b> to automate tasks, and users to create custom menus for the end-user. This is one of Helix's most unusual and powerful features.|$|R
40|$|Complex {{systems are}} an {{important}} topic in science education today, but they are usually difficult for secondary-level students to learn. Although graphic simulations have many advantages in teaching complex systems, scaffolding is a critical factor for effective learning. This dissertation study was conducted around two complementary research questions on scaffolding: (1) How can we chunk and sequence learning activities in teaching complex systems? (2) How can we help students make connections among system levels across learning activities (level bridging) ? With a sample of 123 seventh-graders, this study employed a 3 x 2 experimental design that factored sequencing methods (independent variable 1; three levels) with level-bridging scaffolding (independent variable 2; two levels) and compared the effectiveness of each combination. The study measured two dependent variables: (1) knowledge integration (i. e., integrating and connecting content-specific normative concepts and providing coherent scientific explanations); (2) understanding of the deep causal structure (i. e., being able to grasp and transfer the causal knowledge of a complex system). The study used a computer-based simulation environment as the research platform to teach the ideal gas law as a system. The ideal gas law is an emergent chemical system that has three levels: (1) experiential macro level (EM) (e. g., an aerosol can explodes when it is thrown into the fire); (2) abstract macro level (AM) (i. e., the relationships among temperature, pressure and volume); (3) micro level (Mi) (i. e., molecular activity). The sequencing methods of these levels were manipulated by changing {{the order in which}} they were delivered with three possibilities: (1) EM-AM-Mi; (2) Mi-AM-EM; (3) AM-Mi-EM. The level-bridging scaffolding variable was manipulated on two aspects: (1) inserting inter-level questions among learning activities; (2) two simulations dynamically linked in the final learning activity. Addressing the first research question, the Experiential macro-Abstract macro-Micro (EM-AM-Mi) sequencing method, following the "concrete to abstract" principle, produced better knowledge integration while the Micro-Abstract macro-Experiential <b>macro</b> (Mi-AM-EM) <b>sequencing</b> method, congruent with the causal direction of the emergent system, produced better understanding of the deep causal structure only when level-bridging scaffolding was provided. The Abstract macro-Micro-Experiential <b>macro</b> (AM-Mi-EM) <b>sequencing</b> method produced worse performance in general, because it did not follow the "concrete to abstract" principle, nor did it align with the causal structure of the emergent system. As to the second research question, the results showed that level-bridging scaffolding was important for both knowledge integration and understanding of the causal structure in learning the ideal gas law system...|$|R
40|$|A {{combined}} University / Industry {{team has}} developed a prototype system for handling protein crystals aboard the space station. The system used a miniature direct drive robot, CCD television cameras, and a client-server computing system using internet protocols to support the capture of protein crystals from aqueous growth solutions. The system was demonstrated between Huntsville AL. and Seattle WA. An operator in Huntsville controlled the mini robot by invoking pre-defined relative and absolute macros. The movement <b>macros</b> (a predefined <b>sequence</b> of multi-device movement commands) were developed to support precision motion between task locations in the glovebox. The operator invoked the macros by clicking icons in the remote control interface. The system is a promising start {{for the development of}} a space-station based remote protein crystal analysis facility...|$|R
40|$|In this work, {{we present}} an {{algorithm}} for simultaneous template generation and matching. The algorithm profiles the graph and iteratively contracts edges {{to create the}} templates. The algorithm is general and {{can be applied to}} any type of graph, including directed graphs and hypergraphs. We discuss how to target the algorithm towards the novel problem of instruction generation and selection for a hybrid (re) configurable systems. In particular, we target the Strategically Programmable System, which embeds complex computational units like ALUs, IP blocks, etc. into a configurable fabric. We argue that an essential compilation step for these systems is instruction generation, as it is needed to specify the functionality of the embedded computational units. Additionally, instruction generation can be used to create soft <b>macros</b> – tightly <b>sequenced</b> pre-specified operations placed in the configurable fabric. ...|$|R
40|$|The {{purpose of}} this classic {{grounded}} theory {{study was to examine}} the process of transition out of the National Football League (NFL) for former NFL players, and to develop a grounded theory around this substantive area. Twelve, diverse former NFL players were interviewed and asked to submit artifacts that represented or symbolized their transition experience. Based on interviews, artifacts, and relevant literature, a three-model theory was generated around the phenomenon of transitioning out of the NFL. Model 1 outlines the Former Player Life Timeline, Model 2 depicts the Post-NFL <b>Macro</b> Transition <b>Sequence,</b> and Model 3 displays the Process of Purpose Post-NFL. The main concern that participants sought to address emerged as rediscovering and redefining purpose in life post-NFL career. The supporting categories emerged as (1) vision for success in life; (2) intentional engagement in the process; (3) flesh out self-identity; (4) humble yourself; and (5) replicate the blueprint for success. Five properties for each the latter three categories emerged, each supported by relevant literature in neuroscience, sport psychology, and positive psychology. The resulting theory demonstrated a 95 - 100 percent reliability measure through professional triangulation, and was validated by fit, relevance, workability, and modifiability through participant and professional reviews. This theory offers a conceptualization of the psychoemotional experience of the transition process, while also offering a framework/game plan to assist individuals in preparing for and navigating life beyond the game...|$|R
40|$|Due to its {{importance}} in security, syntax analysis has found usage in many high-level programming languages. The Lisp language {{has its share}} of operations for evaluating regular expressions, but native parsing of Lisp code in this way is unsupported. Matching on lists requires a significantly more complicated model, with a different programmatic approach than that of string matching. This work presents a new automata-based approach centered on a set of functions and <b>macros</b> for identifying <b>sequences</b> of Lisp S-expressions using finite tree automata. The objective is to test that a given list is an element of a given tree language. We use a macro that takes a grammar and generates a function that reads off the leaves of a tree and tries to parse them as a string in a context-free language. The experimental results indicate that this approach is a viable tool for parsing Lisp lists and expressions in the abstract interpretation framewor...|$|R
40|$|The Web is {{far less}} usable and {{accessible}} for people with vision impairments {{than it is for}} sighted people. Web automation, a process of automating browsing actions on behalf of the user, has the potential to bridge the divide between the ways sighted and people with vision impairment access the Web; specifically, it can enable the latter to breeze through web browsing tasks that beforehand were slow, hard, or even impossible to accomplish. Typical web automation requires that the user record a <b>macro,</b> a <b>sequence</b> of browsing steps, so that these steps can be automated in the future by replaying the macro. However, for people with vision impairment, automation with macros is not usable. In this paper, we propose a novel model-based approach that facilitates web automation without having to either record or replay macros. Using the past browsing history and the current web page as the browsing context, the proposed model can predict the most probable browsing actions that the user can do. The model construction is “unsupervised”. More importantly, the model is continuously and incrementally updated as history evolves, thereby, ensuring the predictions are not “outdated”. We also describe a novel interface that lets the user focus on the objects associated with the most probable predicted browsing steps (e. g., clicking links and filling out forms), and facilitates automatic execution of the selected steps. A study with 19 blind participants showed that the proposed approach dramatically reduced the interaction time needed to accomplish typical browsing tasks, and the user interface was perceived to be much more usable than the standard screen-reading interfaces...|$|R
5000|$|WordPerfect for DOS {{stood out}} for its <b>macros,</b> in which <b>sequences</b> of keystrokes, {{including}} function codes, were recorded as the user typed them. These macros {{could then be}} assigned to any key desired. This enabled any sequence of keystrokes to be recorded, saved, and recalled. Macros could examine system data, make decisions, be chained together, and operate recursively until a defined [...] "stop" [...] condition occurred. This capability provided a powerful way to rearrange data and formatting codes within a document where the same sequence of actions needed to be performed repetitively, e.g., for tabular data. But since keystrokes were recorded, changes in the function of certain keys as the program evolved would mean that macros from one DOS version of WordPerfect would not necessarily run correctly on another version. Editing of macros was difficult until {{the introduction of a}} macro editor in Shell, in which a separate file for each WordPerfect product with macros enabled the screen display of the function codes used in the macros for that product.|$|R
40|$|The {{purpose of}} this {{dissertation}} was to analyse equivalent therapist's strategyrelated operations and patient's strategy-related change processes as they unfold and relate along the psychotherapeutic process as practiced in real world settings. Ideally, in order to optimize therapeutic responsiveness and effectiveness, therapist´s operations and patient´s processing abilities should proceed in tandem. Processing capacity was conceptualized as the productive use, by the patient, of strategy-related change processes or general psychological processes underlying skilful means to resolve life challenges. There were two research phases, both set {{within the framework of}} Paradigmatic Complementarity. The cross-sectional study used self-report data from therapists in real world practice, while the longitudinal study used observed data from 129 videotaped sessions of four patients of a single therapist. The findings from both studies suggest it is relevant to consider the sequencing of general strategies as a phase-by-phase map of the therapeutic process. First, strategyrelated therapist's operations and patient's change processes can be operationalized in a equivalent format and measured with acceptable reliability, for psychotherapy as practiced in the real-world. Second, the time patients need to show high levels of strategy-related change processes suggests general processing capacities take months to build up, rendering it less a moment-by-moment process. Third, strategy-related patient change seems to unfold sequentially according to a sequence of 3, 5 or 7 dimensions, thus providing sequential maps of intermediate outcomes before hitting termination. Fourth, these intermediate outcomes, predict changes in final outcome, making unique contributions over and above the therapeutic alliance, rendering them good enough vi candidates to stand as mechanisms of change common to any approach. Fifth and finally, decision-making by the therapist can be responsive to this <b>macro</b> developmental <b>sequence</b> influencing its unfolding and being influenced by it. Limitations and implications for real-world essentially integrative psychotherapy theory, research, practice and training are then discussed. The {{purpose of this}} dissertation was to analyse equivalent therapist's strategyrelated operations and patient's strategy-related change processes as they unfold and relate along the psychotherapeutic process as practiced in real world settings. Ideally, in order to optimize therapeutic responsiveness and effectiveness, therapist´s operations and patient´s processing abilities should proceed in tandem. Processing capacity was conceptualized as the productive use, by the patient, of strategy-related change processes or general psychological processes underlying skilful means to resolve life challenges. There were two research phases, both set within the framework of Paradigmatic Complementarity. The cross-sectional study used self-report data from therapists in real world practice, while the longitudinal study used observed data from 129 videotaped sessions of four patients of a single therapist. The findings from both studies suggest it is relevant to consider the sequencing of general strategies as a phase-by-phase map of the therapeutic process. First, strategyrelated therapist's operations and patient's change processes can be operationalized in a equivalent format and measured with acceptable reliability, for psychotherapy as practiced in the real-world. Second, the time patients need to show high levels of strategy-related change processes suggests general processing capacities take months to build up, rendering it less a moment-by-moment process. Third, strategy-related patient change seems to unfold sequentially according to a sequence of 3, 5 or 7 dimensions, thus providing sequential maps of intermediate outcomes before hitting termination. Fourth, these intermediate outcomes, predict changes in final outcome, making unique contributions over and above the therapeutic alliance, rendering them good enough candidates to stand as mechanisms of change common to any approach. Fifth and finally, decision-making by the therapist can be responsive to this <b>macro</b> developmental <b>sequence</b> influencing its unfolding and being influenced by it. Limitations and implications for real-world essentially integrative psychotherapy theory, research, practice and training are then discussed. Doctoral Grant (Ref. SFRH/BD/ 25135 / 2005) of the Science and Technology Foundation (FCT), Portugal, and the Program POCI 2010, which is funded by the Portuguese Ministry of Science, Technology and Higher Education, and the European Social Fund (Community Support Framework III), sponsored the present work...|$|R
40|$|Mission-critical {{embedded}} software performs the core processing logic for pervasive systems that affect people and enterprises everyday, ranging from aerospace systems to financial markets to automotive systems. In order to function properly, these {{embedded software}} systems rely on and are highly interdependent with other {{hardware and software}} systems. This research identifies design principles for large-scale mission-critical embedded software and investigates their application in development strategies, architectures, and techniques. We have examined actual embedded software systems from two different problem domains: advanced robotic spacecraft and financial market systems. Both system types embody solutions that respond to detailed specifications defined and modeled with heavy user involvement. Both system types possess mission-critical logic represented using state machines and structured scenario-based techniques. They both use a layered architecture approach with a foundation that provides infrastructure services, a layer with a simple set of foreground and background tasks, a layer with deterministic synchronous processing steps, and a layer with event-driven monitoring, commanding, and sequencing capabilities. The architectural approaches support domain-specific command <b>sequencing</b> <b>macro</b> languages that define table-driven executable specifications and enable developers to work at higher abstraction levels throughout th...|$|R
40|$|Assigning a {{pathogenic}} role to mitochondrial DNA (mtDNA) variants and unveiling {{the potential}} {{involvement of the}} mitochondrial genome in diseases are challenging tasks in human medicine. Assuming that rare variants {{are more likely to}} be damaging, we designed a phylogeny-based prioritization workflow to obtain a reliable pool of candidate variants for further investigations. The prioritization workflow relies on an exhaustive functional annotation through the mtDNA extraction pipeline MToolBox and includes <b>Macro</b> Haplogroup Consensus <b>Sequences</b> to filter out fixed evolutionary variants and report rare or private variants, the nucleotide variability as reported in HmtDB and the disease score based on several predictors of pathogenicity for non-synonymous variants. Cutoffs for both the disease score as well as for the nucleotide variability index were established with the aim to discriminate sequence variants contributing to defective phenotypes. The workflow was validated on mitochondrial sequences from Leber's Hereditary Optic Neuropathy affected individuals, successfully identifying 23 variants including the majority of the known causative ones. The application of the prioritization workflow to cancer datasets allowed to trim down the number of candidate for subsequent functional analyses, unveiling among these a high percentage of somatic variants. Prioritization criteria were implemented in both standalone ([URL]) and web version ([URL]) of MToolBox...|$|R
40|$|Tourism {{is one of}} {{the most}} rapidly {{developing}} industries in the world. The study of spatio-temporal movement models of tourists are undertaken in variety of disciplines such as tourism, geography, mathematics, economics and artificial intelligence. Knowledge from these different fields has been difficult to integrate because tourist movement research has been conducted at different spatial and temporal scales. This thesis establishes a methodology for modelling the spatial-temporal movement of tourists and defines the spatial-temporal movement of tourists at both the macro and micro level. At the <b>macro</b> level, the <b>sequence</b> of tourist movements is modelled and the trend for tourist movements is predicted based on Markov Chain theory (MC). Log-linear models are then adopted to test the significance of the movement patterns of tourists. Tourism market segmentation based on the significant movement patterns of tourists is implemented using the EM (Expectation-Maximisation) algorithm. At the micro level, this thesis investigates the wayfinding decision-making processes of tourists. Four wayfinding models are developed and the relationships between the roles of landmarks and wayfinding decision-making are also discussed for each type of the wayfinding processes. The transition of a tourist movement between the macro and micro levels was examined based on the spatio-temporal zooming theory. A case study of Phillip Island, Victoria, Australia is undertaken to implement and evaluate the tourist movement models established in this thesis. Two surveys were conducted on Phillip Island to collect the macro and micro level movement data of tourists. As results show particular groups of tourists travelling with the same movement patterns have unique characteristics such as age and travel behaviours such as mode of transport. Effective tour packages can be designed based on significant movement patterns and the corresponding target markets. Tourists with various age groups, residency, gender and different levels of familiarity with physical environment have different wayfinding behaviours. The results of this study have been applied to tourism management on Phillip Island and the novel methods developed in this thesis have proved to be useful in improving park facilities and services provided to tourists, in designing tour packages for tourism market promotion and in understanding tourist wayfinding behaviours...|$|R
40|$|A C++ (or C) {{compiler}} {{begins by}} invoking the preprocessor, {{a program that}} uses special statements, known as directives or control statements, that cause special compiler actions such as: • file inclusion, in which the file being preprocessed incorporates the contents of another file, exactly as if the included file’s text were actually part of the including file; • macro substitution, in which one sequence of text is replaced by another; • conditional compilation, in which parts of the source file’s code can be eliminated at compile time under certain circumstances. All preprocessor directives begin with the # symbol (known as pound or hash), which must occur in the leftmost column of the line. A preprocessor directive that takes up more than one line needs a continuation symbol, (backslash), as the very last character of every line except the last. File Inclusion To include a file inside the file that you are currently compiling, use the #include directive, which takes either of two forms: 1. #include / / Note: filename in pointy brackets which includes the given file {{from one of the}} standard directories (e. g., the directory /usr/include/cxx on ecnalpha); 2. #include "filename " / / Note: filename in double quotes which includes the given file from the current working directory. The #include directive is replaced by the entire contents of the requested file. File inclusion can be nested; that is, if you have a file named myprogram. h that includes the standard input/output stream file named iostream. h, and you have a file named myprogram. cpp that includes the file myprogram. h, then when you compile the file myprogram. cpp, the files myprogram. h and iostream. h will both get included, in the appropriate places. Macro Substitution A <b>macro</b> is a <b>sequence</b> of text that is defined as another (perhaps empty) sequence of text. After a macro has been defined, whenever the preprocessor encounters that macro in the text of the source file being preprocessed, it replaces the macro with the substitution text. To define a macro, you use the #define preprocessor directive, like so: #define MACRO_NAME text_to_be_substituted Once MACRO NAME has been defined, then {{for the rest of the}} source file being preprocessed, the preprocessor will replace any occurrence of MACRO NAME with text to be substituted. For example, if your source file has...|$|R

