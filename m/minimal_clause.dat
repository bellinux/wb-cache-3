12|14|Public
5000|$|Since the pronouns appear {{within the}} same <b>minimal</b> <b>clause</b> {{containing}} their antecedents in these cases, one cannot argue that the relevant binding domain is the clause. The most one can say based on such data is that the domain is [...] "clause-like".|$|E
5000|$|Note that Principles A and B {{refer to}} [...] "governing categories"—domains which limit {{the scope of}} binding. The {{definition}} of a governing category laid out in Lectures on Government and Binding is complex, {{but in most cases}} the governing category is essentially the <b>minimal</b> <b>clause</b> or complex NP.|$|E
40|$|This paper {{presents}} a formal mechanism to properly constrain {{the scope of}} negation and of certain quantificational determiners to their <b>minimal</b> <b>clause</b> in continuation semantics framework introduced in Barker and Shan (2008) and which was subsequently extended from sentential level to discourse level in Dinu (2011). In these works, type shifting is employed to account for side effects such as pronominal anaphora binding or quantifier scope. However, allowing arbitrary type shifting will result in overgenerating interpretations impossible in natural language. To filter out some of these impossible interpretations, once the negation or the quantifiers reach their maximal scope limits (that is their <b>minimal</b> <b>clause),</b> one should force their scope closing by applying a standard type shifter Lower. But the actual mechanism that forces the scope closing was left underspecified in previous work on continuation semantics. We propose here such a mechanism, designed {{to ensure that no}} lexical entries having the scope bounded to their <b>minimal</b> <b>clause</b> (such as not, no, every, each, any, etc) will ever take scope outside. ...|$|E
5000|$|Exclamatives and imitatives always {{appear as}} <b>minimal</b> <b>clauses.</b> The most {{predominant}} exclamatives are hõn [...] "yes", ʔahâ [...] "no", and dâ [...] "now; ready." ...|$|R
5000|$|Quantificatives include numerals {{and others}} like ho'tu [...] "all, everything," [...] na'mu [...] "many, much," [...] ka'šku [...] "a few, a little bit," [...] ka'škuto'hku [...] "several, quite a few," [...] and ʔa'mari [...] "enough." [...] They {{can be used as}} <b>minimal</b> <b>clauses,</b> {{substitutes}} for nouns, modifiers of nouns, and modifiers of active verbs.|$|R
40|$|We {{study the}} {{complexity}} of data disjunctions in disjunctive deductive databases (DDDBs), i. e., <b>minimal</b> <b>clauses</b> R(c 1) ΔΔΔ R(c k), k 2, derived from the database in which all atoms involve the same predicate R. We consider deciding existence and uniqueness of a data disjunction, as well as actually computing one, both for propositional (data) and nonground (program) complexity of the database. Our results extend and complement previous results on {{the complexity of}} disjunctive databases, and provide tools {{for the analysis of}} the complexity of function computation using upgrading techniques, which we develop for this purpose...|$|R
40|$|Introduction In recent years, {{there has}} been renewed and {{sustained}} interest for {{the conditions under which}} clitics (second-position clitics or V-oriented clitics of the Romance type) are banned from clause initial position. For Romance languages, and for Old French in particular, the restriction against initial position of the clitic is known as the Tobler-Mussafia law: clitics appear postverbally only when being preverbal would place them in clause initial position. In French, this initial ban from clause-initial position has undergone a gradual erosion which can be described in at least five stages: Stage 1. Clitics are excluded from the initial position of the <b>minimal</b> <b>clause</b> in all types of clauses. (Strict Tobler-Mussafia stage) Stage 2. Clitics are allowed in preverbal position when the <b>minimal</b> <b>clause</b> is introduced by a conjunction of coordination like et. Stage 3. Clitics are allowed in absolute initial position in all clauses except volitives (impera...|$|E
40|$|Algorithms are {{described}} for constructing semantic structures from encyclopedic texts {{and other types}} of texts. First, sentences are parsed using a statistical parser. Then, for every main verb on the parse tree, a <b>minimal</b> <b>clause</b> structure is built. The initial clauses are refined and null elements on the parse tree are filled, by using verb subcategorization and verb semantics. Then, the semantic roles, adjuncts and verb meaning in each clause are computed. The algorithms have been tested on the fourth release of Ontonotes and Wikipedia. ...|$|E
40|$|This article {{argues that}} the complex {{reflexive}} in Norwegian has a wider distribution than is usually assumed in the literature (for example Hellan 1988). Both simple and complex reflexives {{are used in the}} local domain, which must be defined as the <b>minimal</b> <b>clause.</b> The simple reflexive is used when the physical aspect of the referent of the binder is in focus. It is seen as an inalienable denoting the body of the referent of the binder. Its distribution follows an independently established binding principle for inalienables, while the complex reflexive is an elsewhere form...|$|E
40|$|The {{inference}} rule semi-resolution is introduced and {{shown to be}} complete for ground CNF formulas. It can be employed in a connection-graph setting, and preservation of the spanning property is shown to hold when activated links are deleted. Potential advantages from the employment of semi-resolution as the inference engine in prime implicate computations are briefly discussed. Keywords: prime implicates, resolution, path dissolution 1 Introduction Consequences expressed as <b>minimal</b> <b>clauses</b> that are implied by a formula are its prime implicates; minimal conjunctions that imply a formula are its prime implicants. Implicates are useful in certain approaches to non-monotonic reasoning [6, 10, 13], where all consequences of a formula set (for example, the support set for a proposed commonsense conclusion) are required. The implicants are useful in situations where satisfying models are desired, as in error analysis during hardware verification. Many algorithms have been proposed to compute [...] ...|$|R
50|$|The verb catenae are in blue. The modal {{auxiliary}} in both trees is {{the root}} of the entire sentence. The verb that is immediately subordinate to the modal is always an infinitive. The fact that modal auxiliaries in English are necessarily finite means that within the <b>minimal</b> finite <b>clause</b> that contains them, they can never be subordinate to another verb, e.g.|$|R
40|$|Relative clauses in Twi, a Niger-Congo {{language}} spoken in Ghana, have received little {{attention in the}} literature. I examine a corpus of naturally-occurring relative clauses, collected from a native speaker living in Houston, TX, to describe and analyze the tone, morphosyntax, and discourse characteristics of Twi relative clauses. This research also contributes to understanding of the cross-linguistic accessibility of noun phrases {{to the process of}} relativization. Based on spectrographic comparison within a set of <b>minimal</b> <b>clauses,</b> I determine that the phonemic form of the relativizer is āà. I examine the “optional” use of the relative clause enclitic no using a framework similar to Fox and Thompson’s (1990, 2007) studies on English relative clauses, concluding that the enclitic is only used in about half of cases and that the conditioning environment depends on a number of discourse factors including the topic-worthiness of the relativized argument and the distance between the head noun {{and the end of the}} relative clause. Finally, I examine noun phrase accessibility in Twi according to Keenan and Comrie (1977), finding that Twi relative clauses contradict Keenan and Comrie’s Accessibility Hierarchy Constraints in two respects: Twi resumptive pronouns are obligatory in the relativization of subjects, and the use of the resumptive pronoun strategy in Twi relativization covers a discontinuous portion of the Accessibility Hierarchy...|$|R
40|$|This paper {{investigates the}} {{complexity}} of a general inference problem: given a propositional formula in conjunctive normal form, find all prime implications of the formula. Here, a prime implication means a <b>minimal</b> <b>clause</b> whose validity is implied by {{the validity of the}} formula. We show that, under some reasonable assumptions, this problem can be solved in time polynomially bounded {{in the size of the}} input and in the number of prime implications. In the case of Horn formulae, the result specializes to yield an algorithm whose complexity grows only linearly with the number of prime implications. The result also applies to a class of formulae generalizing both Horn and quadratic formulae. Peer reviewe...|$|E
40|$|AbstractA new methodology/data structure, {{the clause}} tree, is {{developed}} for automated reasoning based on resolution in first order logic. A clause tree T {{on a set}} S of clauses is a 4 -tuple 〈N, E, L, M〉, where N a set of nodes, divided into clause nodes and atom nodes, E {{is a set of}} edges, each of which joins a clause node to an atom node, L is a labeling of N ∪ E which assigns to each clause node a clause of S, to each atom node an instance of an atom of some clause of S, and to each edge either + or −. The edge joining a clause node to an atom node is labeled by the sign of the corresponding literal in the clause. A resolution is represented by unifying two atom nodes of different clause trees which represent complementary literals. The merge of two identical literals is represented by placing the path joining the two corresponding atom nodes into the set M of chosen merge paths. The tail of the merge path becomes a closed leaf, while the head remains an open leaf which can be resolved on. The clause cl(T) that T represents is the set of literals corresponding to the labels of the open leaves modified by the signs of the incident edges. The fundamental purpose of a clause tree T is to show that cl(T) can be derived from S using resolution. Loveland's model elimination ME, the selected literal procedure SL, and Shostak's graph construction procedure GC are explained in a unified manner using clause trees. The condition required for choosing a merge path whose head is not a leaf is given. This allows a clause tree to be built in one way (the build ordering) but justified as a proof in another (the proof ordering). The ordered clause set restriction and the foothold score restriction are explained using the operation on clause trees of merge path reversal. A new procedure called ALPOC, which combines ideas from ME, GC and Spencer's ordered clause set restriction (OC), to form a new procedure tighter than any of the top down procedures above, is developed and shown to be sound and complete. Another operation on clause trees called surgery is defined, and used to define a <b>minimal</b> <b>clause</b> tree. Any non-minimal clause tree can be reduced to a <b>minimal</b> <b>clause</b> tree using surgery, thereby showing that non-minimal clause trees are redundant. A sound procedure MinALPOC that produces only <b>minimal</b> <b>clause</b> trees is given. Mergeless clause trees are shown to be equivalent to each of input resolution, unit resolution and relative Horn sets, thereby giving short proofs of some known results. Many other new proof procedures using clause trees are discussed briefly, leaving many open questions...|$|E
40|$|In past studies {{several authors}} have {{proposed}} {{that there are two}} varieties of the Japanese anaphor zibun; the criteria of classification, however, vary considerably. I propose that three, rather than two, distinct uses of zibun must be postulated in order to obtain a consistent account of its behavior with regard to various syntactic/semantic factors, such as the subject orientation, the awareness requirement, empathy constraints, etc. : it can act as (i) a reflexive anaphor, (ii) a perspective pronoun, or (iii) a logophoric pronoun. Zibun as a reflexive anaphor obeys locality and is bound to its co-argument subject. The perspectival use of zibun represents the empathy-locus of a certain domain (i. e. the <b>minimal</b> <b>clause</b> or NP that contains it) and is bound to a “higher ” subject. Logophoric zibun picks out the agent of reported speech or thought, parallel to logophoric pronouns in some African languages; it also induces a de se interpretation, like long-distance anaphors in Italian, Icelandic etc. 1...|$|E
40|$|A binary matrix has the Consecutive Ones Property (C 1 P) if its columns can {{be ordered}} {{in such a way}} that all 1 ’s on each row are consecutive. A Minimal Conflicting Set is a set of rows that does not have the C 1 P, but every proper subset has the C 1 P. Such submatrices have been {{considered}} in comparative genomics applications, but very little is known about their combinatorial structure and efficient algorithms to compute them. We first describe an algorithm that detects rows that belong to Minimal Conflicting Sets. This algorithm has a polynomial time complexity when the number of 1 s in each row of the considered matrix is bounded by a constant. Next, we show that the problem of computing all Minimal Conflicting Sets can be reduced to the joint generation of all <b>minimal</b> true <b>clause</b> and maximal false clauses for some monotone boolean function. We use these methods in preliminary experiments on simulated data related to ancestral genome reconstruction...|$|R
40|$|The best-known {{representations}} of boolean functions f are those as disjunctions of terms (DNFs) and as conjunctions of clauses (CNFs). It is convenient {{to define the}} DNF size of f as the minimal number of terms in a DNF representing f and the CNF size as the <b>minimal</b> number of <b>clauses</b> in a CNF representing f. This leads to the problem to estimate the largest gap between CNF size and DNF size. More precisely, what is the largest possible DNF size of a function f with polynomial CNF size? We show the answer to be 2 n−Θ(n / log n) ...|$|R
40|$|AbstractDempster-Shafer (DS) {{theory is}} {{formulated}} {{in terms of}} propositional logic, using the implicit notion of provability underlying DS theory. Dempster-Shafer theory can be modeled in terms of propositional logic by the tuple (Σ, ϱ), where Σ {{is a set of}} propositional clauses and ϱ is an assignment of mass to each clause Σi ϵ Σ. It is shown that the disjunction of <b>minimal</b> support <b>clauses</b> for a clause Σi with respect to a set Σ of propositional clauses, ξ(Σi, Σ), when represented in terms of symbols for the ϱi 's, corresponds to a symbolic representation of the Dempster-Shafer belief function for δi. The combination of Belief functions using Dempster's rule of combination corresponds to a combination of the corresponding support clauses. The disjointness of the Boolean formulas representing DS Belief functions is shown to be necessary. Methods of computing disjoint formulas using network reliability techniques are discussed. In addition, the computational complexity of deriving DS Belief functions, including that of the logic-based methods which are the focus of this paper, is explored. Because of intractability even for moderately sized problem instances, efficient approximation methods are proposed for such computations. Finally, implementations of DS theory based on domain restrictions of DS theory, hypertree embeddings, and the ATMS, are examined...|$|R
40|$|University of Minnesota Ph. D. dissertation. July 2011. Major: Linguistics. Advisor: Dr. Hooi Ling Soh. 1 {{computer}} file (PDF); viii, 197 pages, appendices viii, 197 pages, appendices A-E. This dissertation investigates {{the distribution of}} Iron Range English (IRE) reflexives, using judgments collected in a Magnitude Estimation task (Bard et al 1996), and presents a phase-based analysis for their distribution. IRE reflexives (e. g., himself) can corefer with nominal expressions outside their <b>minimal</b> <b>clause</b> in subject or object position. Coreference with an expression outside the <b>minimal</b> <b>clause</b> is not acceptable in two environments: (i) {{if there is an}} intervening subject that does not match the reflexive for person (c. f., Blocking Effects in Mandarin) or (ii) if the reflexive is in an island. The distribution of IRE reflexives is unexpected because generally only monomorphemic reflexives behave this way (Pica 1987). Complex reflexives that behave this way, such as Malay diri-nya `himself/herself' (Cole & Hermon 2003) and Turkish kendi-sin `himself/herself' (Kornfilt 2001), are shown to have pronominal qualities. IRE reflexives do not have pronominal qualities since they exhibit Blocking Effects and island effects. Therefore, they are true long-distance reflexives. Blocking and island effects provide evidence that the reflexive undergoes raising to [Spec, CP], as is suggested for long-distance reflexives in other languages (e. g., Katada 1991). From the [Spec, CP] position, the reflexive is able to corefer with a nominal expression in a higher clause, in accordance with the Phase Impenetrability Condition (Chomsky 2001). Two processes are needed to account for the distribution of IRE long-distance reflexives (c. f., Cole & Wang 1996) since the set of expressions that are potential antecedents and the set of expressions that trigger Blocking Effects are not the same: a reflexive can corefer with a subject or an object, but only subjects trigger Blocking Effects. I posit that reflexives have a [VAR] feature that must be valued by a c-commanding nominal expression within the same phase via Agree, extending Hicks' (2009) analysis of English anaphors. Agree accounts for coreference and offers an inherent c-command relationship between the antecedent and reflexive. I account for Blocking Effects by considerably modifying Hasegawa's (2005) analysis for English anaphors. I suggest that a [+multi] feature on T licenses the reflexive and requires that the reflexive and the subject Agree for person...|$|E
40|$|The SAT {{problem is}} the problem of finding a model for a formula in conjunctive normal form. We {{developed}} two algorithms based on hyper-unit propagation, which solve the SAT problem. Hyper-unit propagation is unit propagation simultaneously by literals, as unit clauses, of an assignment. The first method, called Unicorn-SAT algorithm, solves the resolution-free SAT problem in linear time. A formula is resolution-free if and only if there are no two clauses, which differ only in one variable. For such a restricted formula we can find a model in linear time by hyper-unit propagation. We obtain a submodel, i. e., a part of the model, by negation of a resolution-mate of a <b>minimal</b> <b>clause,</b> which is a clause with the smallest number of literals in the formula. We obtain a resolution-mate of a clause by negating one literal in it. By hyper-unit propagation by a sub-model we obtain a formula, which has fewer variables and clauses and remains resolution-free. Therefore, we can obtain a model by joining the sub-models while we perform hyper-unit propagation by a sub-model recursively until the formula becomes empty. The second method, called General-Unicorn-SAT algorithm, solves the genera...|$|E
40|$|This {{dissertation}} {{presents a}} semantic analysis of long-distance reflexives (LDRs), reflexive pronouns with antecedents {{outside of their}} <b>minimal</b> <b>clause.</b> The study is based on Latin data, but in also includes cross-linguistic considerations. The analyses are framed in Partial Compositional Discourse Representation Theory (Haug 2013). Latin LDRs are frequent in indirect discourse in Latin {{as well as in}} other languages, and they refer to the author of the indirect discourse. Given this pattern, an analysis based on the modal semantics of indirect discourse easily comes to mind. However, LDRs are also attested in non-reported environments, which are often treated in terms of perspective shift. To capture the different uses, I argue that LDRs are anaphors with a presuppositional restriction to shifted perspective holders. Perspective shift is analyzed using events and thematic roles. This approach to perspective shift correctly captures the antecedents of LDRs in indirect discourse, but it can also account for other uses of LDRs. When indirect discourse containing an LDR is embedded within indirect discourse, the LDR becomes ambiguous. By modeling LDR binding as anaphora, this ambiguity is immediately captured, without having to resort to covert structural differences. Latin LDRs are widely attested in so-called unembedded indirect discourse (UID; Bary and Maier 2014), multi-sentence stretches of indirect discourse. In such cases, the LDR is often several sentences away from its antecedent. I show how UID can be analyzed in terms of event anaphora. When paired with the event-semantics of perspective shift, it is possible to capture the discourse antecedents of LDRs in UID. Finally, I discuss the antecedents of Latin LDRs in indirect discourse conveyed by a messenger, which have previously been seen as problematic. I present a new corpus study of such cases and show how they can be explained...|$|E
40|$|The {{concepts}} of w 1 -consistency, w 2 -consistency and n-consistency are defined in [1] and [2]. Now, w 1 -inconsistency {{is equivalent to}} the presence of the negation of a theorem; w 2 -consistency, to the presence of the argument of a negation theorem, and, finally, n-consistency is consistency understood in the customary way, i. e., as the absence of any contradiction. In [1], the basic constructive logic adequate (bcla) to w 1 -consistency in the ternary relational semantics without a set of designated points (trs 0) is defined. Negation is introduced according to the clause (ctrs 0) a ¬A iff (Rabc & c ∈ S) ⇒ b 2 A which is an adaptation to the trs 0 of the <b>minimal</b> intuitionistic <b>clause</b> (cJm) a ¬A iff (Rab & b ∈ S) ⇒ b 2 A in standard binary relational semantics. The aim {{of this paper is to}} define the bcla to w 2 -consistency in the trs 0. Pos-sibilities for introducing negation with a propositional falsity constant instead of the ctrs 0 will be discussed. It will be shown that unlike the bcla to n-consistency, the bcla to w 2 -consistency cannot be defined according to the ctrs 0 in the trs 0, and that it is, however, definable if negation is introduced by means of a falsity constant. It will be proved that all logics referred to above are paraconsistent in a strong sense of the term...|$|R
40|$|Extended Abstract) The {{minimization}} {{problem for}} Horn formulas {{is to find}} a Horn formula equivalent to a given formula, using a <b>minimal</b> number of <b>clauses.</b> This problem is important for applications such as the automated development of large knowledge bases. A 2 log 1 −ǫ (n) -inapproximability result is proven, which is the first inapproximability result for the problem. We also consider several other versions of Horn minimization. An nlog log n/(log n) 1 / 4 -approximation algorithm is given for definite Horn formulas for the Steiner-minimization version, where one can introduce new variables in a restricted manner. This is the first algorithm with a o(n) performance guarantee. The algorithm is based on a new result in algorithmic extremal graph theory, on partitioning bipartite graphs into complete bipartite graphs, which may be of independent interest. Inapproximability results and approximation algorithms are also given for restricted versions where only clauses present in the original formula may be used. 1...|$|R
40|$|Tertius is an Inductive Logic Programming {{system that}} {{performs}} confirmatory induction, i. e., it {{looks for the}} n clauses that have the highest value of a confirmation evaluation function. In this setting, background knowledge is very useful because it can improve {{the reliability of the}} evaluation function, assigning <b>minimal</b> confirmation to <b>clauses</b> that are implied by the background knowledge and increasing the confirmation of the remaining clauses. We propose the algorithms Background 1 and Background 2 that look for clauses in the background that imply the clause under evaluation by Tertius. Both are based on a simplified implication test that is correct with respect to θ-subsumption but not complete. The implication test is not complete because we want to keep the run time inside acceptable bounds. We compare Background 1 with Background 2 on two datasets. The results show that Background 2 is more efficient than Background 1. Moreover, we also present the algorithm Preprocess that infers new clauses from the background knowledge in order to exploit it as much as possible. The algorithm modifies the consequence finding algorithm proposed by Inoue by reducing its execution time while giving up completeness...|$|R
40|$|In {{this thesis}} we study Boolean {{functions}} {{from three different}} perspectives. First, we study the complex- ity of Boolean minimization for several classes of formulas with polynomially solvable SAT, and formulate sufficient conditions for a class which cause the minimization problem to drop at least one level in the polyno- mial hierarchy. Second, we study a class of matched CNFs for which SAT is trivial but minimization remains Σp 2 complete. We prove that every matched CNF {{has at least one}} equivalent prime and irredundant CNF that is also matched. We use this fact to prove the main result of this part, namely that for every matched CNF all <b>clause</b> <b>minimal</b> equivalent CNFs are also matched. Third, we look at propagation completeness - the property of a CNF that says that for every partial assignment all entailed literals can be discovered by unit propagation. We can extend every CNF to be propagation complete by adding empowering impli- cates to it. The main result of this section is a the proof of coNP completeness of the recognition problem for propagation complete CNFs. We also show that there exist CNFs to which an exponential number of empowering implicates have to be added to make them propagation complete...|$|R
40|$|Abstract. Maxsat is an {{optimization}} {{version of}} Satisfiability aimed at finding a truth assignment that maximizes {{the satisfaction of}} the theory. The technique of solving a sequence of SAT decision problems has been quite successful for solving larger, more industrially focused Maxsat instances, particularly when {{only a small number}} of clauses need to be falsified. The SAT decision problems, however, become more and more complicated as the <b>minimal</b> number of <b>clauses</b> that must be falsified increases. This can significantly degrade the performance of the approach. This technique also has more difficulty with the important generalization where each clause is given a weight: the weights generate SAT decision problems that are harder for SAT solvers to solve. In this paper we introduce a new Maxsat algorithm that avoids these problems. Our algorithm also solves a sequence of SAT instances. However, these SAT instances are always simplifications of the initial Maxsat formula, and thus are relatively easy for modern SAT solvers. This is accomplished by moving all of the arithmetic reasoning into a separate hitting set problem which can then be solved with techniques better suited to numeric reasoning, e. g., techniques from mathematical programming. As a result the performance of our algorithm is unaffected by the addition of clause weights. Our algorithm can, however, require solving more SAT instances than previous approaches. Nevertheless, the approach is simpler than previous methods and displays superior performance on some benchmarks. ...|$|R

