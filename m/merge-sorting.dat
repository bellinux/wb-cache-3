1|31|Public
30|$|To {{bypass the}} large memory requirement, slider [13] proposes a {{sequence}} alignment by <b>merge-sorting</b> the reference genome subsequences and read sequences. Recently, string matching algorithms {{based on the}} Burrow-Wheeler Transformation (BWT) [14], which is a string compression technique, has drawn the attention of many research groups. Techniques like Bowtie [15], BWA [16] and BWA-SW [17] {{which are based on}} BWT [14], have also become very popular due to their vastly improved memory efficiency and their support for flexible seed lengths. These BWT-based sequence alignment tools provide fast mapping of short reads of DNA sequences against a reference genome sequence with small memory footprint using a data structure like FM-Index [18] built atop the BWT. These studies use sorting algorithms for matching. Therefore, they are highly accurate with accuracies as high as 99.9 %.|$|E
40|$|Sorting {{algorithms}} {{based on}} successive merging of ordered subsequences are widely used, {{due to their}} efficiency and to their intrinsically parallelizable structure. Among them, the <b>merge-sort</b> algorithm emerges indisputably as the most prominent method. In this paper we present a variant of <b>merge-sort</b> that proceeds through arbitrary merges between pairs of quasi-ordered Subsequences, no matter which their size may be. We provide a detailed analysis [...] showing that a set of n elements can be sorted by performing at most n[logn] key comparisons. Our method has the same optimal asymptotic time and space complexity as compared to previous known unbalanced <b>merge-sort</b> algorithms, but experimental results show that it behaves significantly better in practice. (c) 2005 Elsevier Inc. All rights reserved...|$|R
40|$|This paper proposes an {{external}} sorting algorithm for large data {{as an alternative}} to the widely used <b>merge-sort</b> algorithm. The algorithm we present is an application of the widely known quick-sort algorithm to large sequences of data stored externally on a disk device. The problem with the <b>merge-sort</b> algorithm is not its time complexity but the large amount of time it requires to output its first results. This is a serious problem in the context of pipelined processing, since the operations consuming its result will have to wait all that time before they can start their processing, thus limiting the degree of vertical parallelism achievable by pipelined processing Using quick-sort instead of <b>merge-sort</b> for external sorting in pipelined data processing systems results in an optimization in the order of $log N$ (where N is the size of the data sequence to be sorted) for the entire query pipeline where the sorting operation is involved...|$|R
40|$|The {{issue of}} {{duplicate}} elimination for large data files {{in which many}} occurrences of the same record may appear is addressed. A comprehensive cost analysis of the duplicate elimination operation is presented. This analysis {{is based on a}} combinatorial model developed for estimating the size of intermediate runs produced by a modified <b>merge-sort</b> procedure. The performance of this modified <b>merge-sort</b> procedure is demonstrated to be significantly superior to the standard duplicate elimination technique of sorting followed by a sequential pass to locate duplicate records. The results {{can also be used to}} provide critical input to a query optimizer in a relational database system...|$|R
40|$|Abstract. We present two merging {{algorithms}} on a single-channel single-hop {{radio network}} without collision detection. The simpler of these algorithms merges two sorted sequences of length n in time 4 n with energetic cost for each station lg n. The energetic cost of broadcasting is constant. This yields the <b>merge-sort</b> for n elements in time 2 n lg n, where the energetic cost for each station i...|$|R
40|$|Abstract. In {{the context}} of {{constructive}} synthesis we present a general method for synthesis of seven versions of sorting algorithms and the synthesis of some necessary auxiliary functions. We synthesize also new algorithms like: Special <b>Merge-Sort</b> and Special Quick-Sort. The method that we implemented in the Theorema system and the case studies {{presented in this paper}} complement the work from [8]. The synthesis process is paralleled with the exploration of the appropriate theory of lists. ...|$|R
40|$|We {{consider}} {{a class of}} comparator networks obtained from the omega permutation network by replacing each switch with a comparator exchanger of arbitrary direction. These networks are all isomorphic to each other, have merging capabilities, {{and can be used}} as building blocks of sorting networks in ways different from the standard <b>merge-sort</b> scheme. It is shown that the bitonic merger and the balanced merger are members of the class. These two networks were not previously known to be isomorphic...|$|R
40|$|First {{we present}} a new variant of <b>Merge-sort,</b> which needs only 1. 25 n space, because it uses space again, which becomes {{available}} within the current stage. It does not need more comparisons than classical <b>Merge-sort.</b> The main result is an easy to implement method of iterating the procedure in-place starting to sort 4 / 5 of the elements. Hereby we can keep the additional transport costs linear and only very few comparisons get lost, so that n log n − 0. 8 n comparisons are needed. We show that we can improve the number of comparisons if we sort blocks of constant length with Merge-Insertion, before starting the algorithm. Another improvement is to start the iteration with a better version, which needs only (1 +ε) n space and again additional O(n) transports. The result is, that we can improve this theoretically up to n log n − 1. 3289 n comparisons in the worst case. This {{is close to the}} theoretical lower bound of n log n − 1. 443 n. The total number of transports in all these versions can be reduced to ε n log n+O(1) for any ε> 0. ...|$|R
40|$|In this paper, we {{identify}} {{a class of}} Prolog programs inferable from positive data. Our approach is based on moding information and linear predicate inequalities between input terms and output terms. Our results generalize the results of Arimura and Shinohara (1994). Standard programs for reverse, quick-sort, <b>merge-sort</b> are {{a few examples of}} programs that can be handled by our results but not by the earlier results of Arimura and Shinohara (1994). The generality of our results follows from the fact that we treat logical variables as transmitters for broadcasting communication, whereas Arimura and Shinohara (1994) treat them as point-to-point communication channels...|$|R
40|$|International audienceThis paper {{presents}} a cache tracker, a hardware component {{to track the}} cache state of hundreds of caches serving processors modeled using threads on a single MIPS 64 processor. This host-multithreading approach allows a single, low-cost FPGA to model large systems to allow quick and broad architectural exploration with reasonable simulation performance. The cache tracker stores all state in DRAM to allow maximum scalability in both number of processors and in cache sizes. We describe our approach of scalability versus simulation performance, our implementation in Bluespec SystemVerilog, and give a sample study of a parallel <b>merge-sort</b> over various processor numbers, cache sizes and arrangements...|$|R
40|$|The {{importance}} of a high performance sorting algorithm with low time complexity cannot be over stated. Several benchmark algorithms viz. Bubble Sort, Insertion Sort, Quick Sort, and Merge Sort, etc. have tried to achieve these goals, but with limited success in some scenarios. Newer algorithms like Shell Sort, Bucket Sort, Counting Sort, etc. have their own limitations in terms of category/nature of elements which they can process. The present paper {{is an attempt to}} enhance performance of the standard <b>Merge-Sort</b> algorithm by eliminating the partitioning complexity component, thereby resulting in smaller computation times. Both subjective and numerical comparisons are drawn with existing algorithms in terms of time complexity and data sizes, which show the superiority of the proposed algorithm...|$|R
40|$|Abstract—This paper {{presents}} a cache tracker, a hardware component {{to track the}} cache state of hundreds of caches serving processors modeled using threads on a single MIPS 64 processor. This host-multithreading approach allows a single, low-cost FPGA to model large systems to allow quick and broad architectural exploration with reasonable simulation performance. The cache tracker stores all state in DRAM to allow maximum scalability in both number of processors and in cache sizes. We describe our approach of scalability versus simulation performance, our implementation in Bluespec SystemVerilog, and give a sample study of a parallel <b>merge-sort</b> over various processor numbers, cache sizes and arrangements. INTRODUCTION: FPGA SIMULATION AND OUR APPROACH Our system follows others to adopt an FPGA simulation approach to multi-processor architecture exploration but differ...|$|R
40|$|A {{transformational}} {{approach for}} proving termination of parallel logic {{programs such as}} GHC programs is proposed. A transformation from GHC programs to term rewriting systems is developed; it exploits the fact that unications in GHC-resolution correspond to matchings. The termination of a GHC program for a class of queries is implied by the termination of the resulting rewrite system. This approach facilitates the applicability {{of a wide range}} of termination techniques developed for rewrite systems in proving termination of GHC programs. The method consists of three steps: (a) deriving moding information from a given GHC program, (b) transforming the GHC program into a term rewriting system using the moding information, and nally (c) proving termination of the resulting rewrite system. Using this method, the termination of many benchmark GHC programs such as quick-sort, <b>merge-sort,</b> merge, split, fair-split and append, etc., can be proved. 1 Introduction The success of logic programmin [...] ...|$|R
40|$|Proving {{correctness}} {{of programs}} is a desirable, {{but not yet}} practically solved, problem. Conventional verification techniques do not scale to larger "real-life" programs. In order to overcome problems with the semantics of conventional languages, researchers define subsets of these languages for verification. We take this approach one step further; rather than defining a subset, we apply a new theory based on relaxed single-threading to achieve completely static semantics (i. e. referential transparency) of programs. Our programs with static semantics have complete control over aliasing, the correct order dependencies are enforced, and uninitialized variables are prevented. Due to the static semantics of these programs, first-order logic can be used directly to verify them. Using <b>Merge-Sort</b> as an example, we demonstrate that first-order logic proofs of programs with static semantics are fully composable and thus scale freely to larger programs. We also report on our work towards a fully [...] ...|$|R
40|$|Abstract—Preference-based {{learning}} to rank (LTR) {{is a model}} that learns the underlying pairwise preference with soft binary classification, and then ranks test instances based on pairwise preference predictions. The model {{can be viewed as}} an alternative to the popular score-based LTR model, which learns a scoring function and ranks test instances based on their scores directly. Many existing works on preference-based LTR address the step of ranking test instances as the problem of weighted minimum feed-back arcset on tournament graph. The problem is somehow NP-hard to solve and existing algorithms cannot efficiently produce a decent solution. We propose a practical algorithm to speed up the ranking step while maintaining ranking accuracy. The algorithm employs a divide-and-conquer strategy that mimics <b>merge-sort,</b> and its time complexity is relatively low when compared to other preference-based LTR algorithms. Empirical results demonstrate that the accuracy of the proposed algorithm is competitive to state-of-the-art score-based LTR algorithms. I...|$|R
40|$|Tyt. z nagłówka. Pozostali autorzy artykułu: Marcin Pietroń, Maciej Wielgosz, Kaziemierz Wiatr. Bibliogr. s. 690. Sorting is {{a common}} problem in {{computer}} science. There {{are a lot of}} well-known sorting algorithms created for sequential execution on a single processor. Recently, many-core and multi-core platforms have enabled the creation of wide parallel algorithms. We have standard processors that consist of multiple cores and hardware accelerators, like the GPU. Graphic cards, with their parallel architecture, provide new opportunities to speed up many algorithms. In this paper, we describe the results from the implementation of a few different parallel sorting algorithms on GPU cards and multi-core processors. Then, a hybrid algorithm will be presented, consisting of parts executed on both platforms (a standard CPU and GPU). In recent literature about the implementation of sorting algorithms in the GPU, a fair comparison between many core and multi-core platforms is lacking. In most cases, these describe the resulting time of sorting algorithm executions on the GPU platform and a single CPU core. Dostępny również w formie drukowanej. KEYWORDS: parallel algorithms, GPU, OpenMP, CUDA, sorting networks, <b>merge-sort...</b>|$|R
40|$|This paper {{presents}} an algorithm for fast sorting of large lists using modern GPUs. The method achieves high speed by efficiently utilizing the parallelism of the GPU {{throughout the whole}} algorithm. Initially, GPU-based bucketsort or quicksort splits the list into enough sublists then to be sorted in parallel using <b>merge-sort.</b> The algorithm is of complexity n log n, and for lists of 8 M elements and using a single Geforce 8800 GTS- 512, it is 2. 5 {{times as fast as}} the bitonic sort algorithms, with standard complexity of n(log n) 2, which for long was considered to be the fastest for GPU sorting. It is 6 times faster than single CPU quicksort, and 10 % faster than the recent GPU-based radix sort. Finally, the algorithm is further parallelized to utilize two graphics cards, resulting in yet another 1. 8 times speedup. Key words: parallelism, sorting, GPU-algorithms Sorting is a general problem in computer science. Mergesort [11] is a wellknown sorting algorithm of complexity O(n logn), and it can easily be implemente...|$|R
40|$|Sorting {{hierarchical}} data in external {{memory is}} needed {{in a wide variety}} of applications including archiving scientific data and dealing with large XML datasets. The topic of sorting hierarchical data has received little attention form the research community so far. In this paper, we focus on sorting arbitrary hierarchical datasets that exceed the size of physical memory. We propose HErMeS, an algorithm that generalizes the most widely-used techniques for sorting flat data in external memory, namely multiway <b>merge-sort</b> and replacement selection. HErMeS efficiently takes into consideration the hierarchical nature of the data in order to minimize the number of disk accesses and optimize the usage of available memory. The algorithm’s theoretical bounds with respect to the structure of the hierarchical dataset are extracted, while an experimental study demonstrates that our implementation of the algorithm meets its theoretical expectations. Using several workloads, we compare our algorithm to existing approaches and show that it outperforms them by a significant factor. These results, we believe, prove our technique to be a viable and scalable solution. 1...|$|R
40|$|International audienceWe {{construct}} quasipolynomial-size proofs of the propositional pigeonhole {{principle in}} the deep inference system KS, addressing an open problem raised in previous works and matching the best known upper bound for the more general class of monotone proofs. We make significant use of monotone formulae computing boolean threshold functions, an idea previously considered in works of Atserias et al. The main construction, monotone proofs witnessing the symmetry of such functions, involves an implementation of <b>merge-sort</b> {{in the design of}} proofs in order to tame the structural behaviour of atoms, and so the complexity of normalization. Proof transformations from previous work on atomic flows are then employed to yield appropriate KS proofs. As further results we show that our constructions can be applied to provide quasipolynomial-size KS proofs of the parity principle and the generalized pigeonhole principle. These bounds are inherited for the class of monotone proofs, and we are further able to construct n^O(log log n) -size monotone proofs of the weak pigeonhole principle with (1 + ε) n pigeons and n holes for ε = 1 / polylog n, thereby also improving the best known bounds for monotone proofs...|$|R
40|$|July, 2010 We {{present some}} case studies in {{constructive}} synthesis of sorting algorithms. In order to synthesize some algorithms on tuples (like e. g. insertion-sort, <b>merge-sort)</b> we use an approach based on proving. Namely, we {{start from the}} specification of the problem (input and output condition) and we construct an inductive proof {{of the fact that}} for each input there exists a solution which satisfies the output condition. The problem will be reduced into smaller and smaller problems, the method will be applied like in a ”cascade” and finally the problem is so simple that the corresponding algorithm (function) already exists in the knowledge. The algorithm can be then extracted immediately from the proof. These experiments are paralleled with the exploration of the appropriate theory of tuples. The purpose of these experiments is multi-fold: to construct the appropriate knowledge base necessary for this type of proofs, to find the natural deduction inference rules and the necessary strategies for their application, and finally to implement the corresponding provers in the frame of the Theorema system. The novel specific feature of our approach is applying this method like in a ”cascade...|$|R
40|$|We {{construct}} quasipolynomial-size proofs of the propositional pigeonhole {{principle in}} the deep inference system KS, addressing an open problem raised in previous works and matching the best known upper bound for the more general class of monotone proofs. We make significant use of monotone formulae computing boolean threshold functions, an idea previously considered in works of Atserias et al. The main construction, monotone proofs witnessing the symmetry of such functions, involves an implementation of <b>merge-sort</b> {{in the design of}} proofs in order to tame the structural behaviour of atoms, and so the complexity of normalization. Proof transformations from previous work on atomic flows are then employed to yield appropriate KS proofs. As further results we show that our constructions can be applied to provide quasipolynomial-size KS proofs of the parity principle and the generalized pigeonhole principle. These bounds are inherited for the class of monotone proofs, and we are further able to construct n-size monotone proofs of the weak pigeonhole principle with (1 + ε) n pigeons and n holes for ε = 1 = logk n, thereby also improving the best known bounds for monotone proofs...|$|R
40|$|Recently, we {{proposed}} a systematic method for top-down synthesis and verification of lemmata and algorithms called ”lazy thinking method ” {{as a part of}} systematic mathematical theory exploration (mathematical knowledge management). The lazy thinking method is characterized: • by using a library of theorem and algorithm schemes • and by using the information contained in failing attempts to prove the schematic theorem or the correctness theorem for the algorithm scheme for inventing lemmata or requirements for subalgorithms, respectively. In this paper, we give a couple of examples for algorithm synthesis using the lazy thinking paradigm. These examples illustrate how the synthesized algorithm depends on the algorithm scheme used. Also, we give details about the implementation of the lazy thinking algorithm synthesis method in the frame of the Theorema system. In this implementation, the synthesis of the example algorithms can be carried out completely automatically, i. e. without any user interaction. Key words: algorithm invention, algorithm verification, program synthesis, algorithm correctness, re-usable algorithms, algorithm schemes, learning from failure, conjecture generation, lazy thinking, requirement engineering, didactics of programming, mathematical knowledge retrieval, mathematical knowledge management, sorting, merging, <b>merge-sort,</b> Theorema. ...|$|R
40|$|We {{present a}} case study in proof based {{constructive}} synthesis of sorting algorithms. Using a knowledge base containing the necessary properties of tuples, we start from the specification of the problem (input and output conditions) and we construct an inductive proof of the fact that for each input there exists a sorted tuple. During the proof our problem reduces into simpler and simpler problems, we apply the same method (“cascading”) for the new problems and finally the problem is so simple that one can find the necessary functions in the knowledge base. The algorithm can be then extracted immediately from the proof. We focus here on the synthesis of the <b>merge-sort</b> algorithm. This case study is paralleled with the exploration of the appropriate theory of tuples. The purpose of the case study is multifold: to construct the appropriate knowledge base necessary for this type of proofs, to find the natural deduction inference rules and the necessary strategies for their application, and finally to implement the corresponding prover in the frame of the Theorema system. The novel specific feature of our approach is applying this method like in a “cascade ” and the use (as much as possible) of first order predicate logic because this increases the feasibility of proving...|$|R
40|$|Emerging memory {{technologies}} {{have a significant}} gap between the cost, both in time and in energy, of writing to memory versus reading from memory. In this paper we present models and algorithms that account for this difference, {{with a focus on}} write-efficient sorting algorithms. First, we consider the PRAM model with asymmetric write cost, and show that sorting can be performed in O(n) writes, O(n logn) reads, and logarithmic depth (parallel time). Next, we consider a variant of the External Memory (EM) model that charges k> 1 for writing a block of size B to the secondary memory, and present variants of three EM sorting algorithms (multi-way <b>merge-sort,</b> sample sort, and heapsort using buffer trees) that asymptotically reduce the number of writes over the original algorithms, and per-form roughly k block reads for every block write. Finally, we define a variant of the Ideal-Cache model with asymmetric write costs, and present write-efficient, cache-oblivious parallel algorithms for sorting, FFTs, and matrix multiplication. Adapting prior bounds for work-stealing and parallel-depth-first schedulers to the asymmetric setting, these yield parallel cache complexity bounds for machines with private caches or with a shared cache, respectively. Categories and Subject Descriptors F. 2. 2 [Analysis of Algorithms and Problem Complexity]: Non-numerical Algorithms and Problems—Sorting and searchin...|$|R
40|$|We {{study the}} problem of {{developing}} efficient approaches for proving worst-case bounds of non-deterministic recursive programs. Ranking functions are sound and complete for proving termination and worst-case bounds of nonrecursive programs. First, we apply ranking functions to recursion, resulting in measure functions. We show that measure functions provide a sound and complete approach to prove worst-case bounds of non-deterministic recursive programs. Our second contribution is the synthesis of measure functions in nonpolynomial forms. We show that non-polynomial measure functions with logarithm and exponentiation can be synthesized through abstraction of logarithmic or exponentiation terms, Farkas' Lemma, and Handelman's Theorem using linear programming. While previous methods obtain worst-case polynomial bounds, our approach can synthesize bounds of the form O(n n) as well as O(n^r) where r is not an integer. We present experimental results to demonstrate that our approach can obtain efficiently worst-case bounds of classical recursive algorithms such as (i) <b>Merge-Sort,</b> the divide-and-conquer algorithm for the Closest-Pair problem, where we obtain O(n n) worst-case bound, and (ii) Karatsuba's algorithm for polynomial multiplication and Strassen's algorithm for matrix multiplication, where we obtain O(n^r) bound such that r is not an integer {{and close to the}} best-known bounds for the respective algorithms. Comment: 54 Pages, Full Version to CAV 201...|$|R
40|$|Abstract — This paper {{presents}} an algorithm for fast sorting of large lists using modern GPUs. The method achieves high speed by efficiently utilizing the parallelism of the GPU {{throughout the whole}} algorithm. Initially, a parallel bucketsort splits the list into enough sublists then to be sorted in parallel using <b>merge-sort.</b> The parallel bucketsort, implemented in NVIDIA’s CUDA, utilizes the synchronization mechanisms, such as atomic increment, that is available on modern GPUs. The mergesort requires scattered writing, which is exposed by CUDA and ATI’s Data Parallel Virtual Machine[1]. For lists with more than 512 k elements, the algorithm performs better than the bitonic sort algorithms, which have been {{considered to be the}} fastest for GPU sorting, and is more than twice as fast for 8 M elements. It is 6 - 14 times faster than single CPU quicksort for 1 - 8 M elements respectively. In addition, the new GPU-algorithm sorts on n log n time as opposed to the standard n(log n) 2 for bitonic sort. Recently, it was shown how to implement GPU-based radix-sort, of complexity n log n, to outperform bitonic sort. That algorithm is, however, still up to ∼ 40 % slower for 8 M elements than the hybrid algorithm presented in this paper. GPU-sorting is memory bound and a key to the high performance is that the mergesort works on groups of four-float values to lower the number of memory fetches. Finally, we demonstrate the performance on sorting vertex distances for two large 3 D-models; a key in for instance achieving correct transparency. I...|$|R
40|$|Email: uffe at {{chalmers}} dot se Abstract — This paper {{presents an}} algorithm for fast sorting of large lists using modern GPUs. The method achieves high speed by efficiently utilizing the parallelism of the GPU {{throughout the whole}} algorithm. Initially, a parallel bucketsort splits the list into enough sublists then to be sorted in parallel using <b>merge-sort.</b> The parallel bucketsort, implemented in NVIDIA’s CUDA, utilizes the synchronization mechanisms, such as atomic increment, that is available on modern GPUs. The mergesort requires scattered writing, which is exposed by CUDA and ATI’s Data Parallel Virtual Machine[1]. For lists with more than 512 k elements, the algorithm performs better than the bitonic sort algorithms, which have been {{considered to be the}} fastest for GPU sorting, and is more than twice as fast for 8 M elements. It is 6 - 14 times faster than single CPU quicksort for 1 - 8 M elements respectively. In addition, the new GPU-algorithm sorts on n log n time as opposed to the standard n(log n) 2 for bitonic sort. Recently, it was shown how to implement GPU-based radix-sort, of complexity n log n, to outperform bitonic sort. That algorithm is, however, still up to ∼ 40 % slower for 8 M elements than the hybrid algorithm presented in this paper. GPU-sorting is memory bound and a key to the high performance is that the mergesort works on groups of four-float values to lower the number of memory fetches. Finally, we demonstrate the performance on sorting vertex distances for two large 3 D-models; a key in for instance achieving correct transparency. I...|$|R
40|$|Constraint Handling Rules (CHR), {{closely related}} to Logic Programming (LP), is a {{declarative}} programming language. Over the years, the language proved successful for implementing many kinds of problems efficiently. Mainly this, but also its simple syntax and semantics, accounts for its success and impact on the research community. To further {{encourage the use of}} CHR, we need to further improve its efficiency of execution. To this end, termination analysis of CHR can be seen as one of the main sources of input. Furthermore, in the context of program debugging, termination analysis of CHR is an important asset. Due to the many language specifics, it is often hard for programmers to point out unwanted loops in their CHR programs. It is therefore essential to have a good understanding of the termination problem in CHR. Until recently, however, there was only an informal discussion on termination of the subset of CHR that only considers simplification rules. The contributions of this thesis are therefore twofold. First, we provide for a theoretical framework for termination analysis of the full CHR language. Secondly, based on this theoretical framework, we derive an approach for automated termination analysis of CHR. This approach extends the approaches in LP to integer polynomial interpretations and can be modularised. Furthermore, the approach is practical, as we demonstrate with T*CoP, a Termination analyser for CHR on top of Prolog. Abstract........................... v Contents.......................... ix List of Figures...................... xv List of Tables...................... xvii 1 Introduction....................... 1 1. 1 Motivating termination analysis............. 1 1. 2 Motivating termination analysis for CHR......... 2 1. 3 Defining the termination problem for CHR........ 3 1. 3. 1 Transformational vs. direct approaches........ 5 1. 3. 2 Host language..................... 5 1. 3. 3 Termination of simplification............. 6 1. 3. 4 The effect of matching................ 7 1. 3. 5 The effect of guarded rules.............. 8 1. 3. 6 Termination of propagation.............. 9 1. 3. 7 Availability of different control structures.... 10 1. 4 Overview....................... 11 I Theory......................... 13 2 Termination of abstract CHR............... 14 2. 1 CHR syntax...................... 15 2. 2 The abstract CHR semantics.............. 18 2. 3 Termination of abstract CHR.............. 22 2. 3. 1 Termination of abstract CHR............. 22 2. 3. 2 A termination proof by a well-founded order..... 23 2. 3. 3 The intended use.................. 27 2. 3. 4 Termination of non-ground CHR programs....... 29 2. 3. 5 The success set of a CHR program.......... 30 2. 3. 6 The Ranking Condition for abstract CHR....... 34 2. 3. 7 Correctness of the RC for abstract CHR....... 37 2. 4 Termination of typical abstract CHR programs..... 40 2. 4. 1 Greatest common divisor............... 40 2. 4. 2 <b>Merge-sort.....................</b> 41 3 Termination of CHR with propagation........... 44 3. 1 The theoretical CHR semantics............. 45 3. 2 Termination of CHR with propagation.......... 50 3. 2. 1 The intended use and success set of a CHR program.. 51 3. 2. 2 Termination of CHR with propagation......... 52 3. 2. 3 The RC for CHR with propagation........... 52 3. 2. 4 Correctness of the RC for CHR with propagation... 56 3. 3 Termination of typical CHR programs.......... 58 3. 3. 1 <b>Merge-sort.....................</b> 59 3. 3. 2 Primes....................... 59 3. 3. 3 Fibonacci...................... 60 3. 3. 4 Problem classes................... 61 4 Termination of CHR................... 62 4. 1 Problem description.................. 62 4. 1. 1 A new CHR state representation........... 65 4. 2 Termination of propagation.............. 67 4. 2. 1 The RC for CHR on propagation rules......... 68 4. 2. 2 Correctness of the RC for CHR on propagation rules. 68 4. 3 Termination of CHR.................. 74 4. 3. 1 A new CHR state representation........... 74 4. 3. 2 The RC for CHR................... 77 4. 3. 3 Correctness of the RC for CHR............ 82 4. 4 Termination of typical CHR programs.......... 87 4. 4. 1 <b>Merge-sort.....................</b> 87 4. 4. 2 Primes....................... 88 4. 4. 3 Fibonacci...................... 88 4. 4. 4 Second problem from Section 3. 3. 4.......... 89 5 Theory: Conclusions................... 90 5. 1 A comparison of RCs.................. 91 5. 1. 1 RC for abstract CHR vs. RC for CHR with propagation. 92 5. 1. 2 RC for abstract CHR vs. RC for CHR......... 92 5. 1. 3 RC for CHR with propagation vs. RC for CHR..... 93 5. 2 Limitations of the RC for CHR............. 94 5. 2. 1 The success set of CHR predicates.......... 94 5. 2. 2 Extensions different from multiset extensions.... 95 5. 2. 3 Single-headed propagation rules........... 97 5. 2. 4 Refined control................... 98 II Practice........................ 100 6 Automating termination analysis for CHR......... 102 6. 1 A verifiable RC for CHR................ 104 6. 1. 1 Polynomial interpretations............. 106 6. 1. 2 The RC for CHR with polynomial interpretations... 110 6. 2 Automating the termination proof........... 125 6. 2. 1 Conditions for inferring the success set relations. 128 6. 2. 2 Conditions for inferring the call set relations... 129 6. 2. 3 Conditions for verifying rigidity.......... 134 6. 2. 4 Conditions for verifying N-closedness........ 134 6. 2. 5 Conditions for verifying the decrease conditions.. 135 6. 3 Towards constraints on symbolic coefficients..... 141 6. 4 Solving Diophantine constraints............ 143 6. 5 Termination of CHR for specific queries........ 144 7 Modularised termination proofs for CHR......... 146 7. 1 Problem description.................. 147 7. 2 The CHR dependency graph and CHR net......... 148 7. 3 Self-sustainable SCCs of a CHR program........ 151 7. 3. 1 Self-sustainable SCCs................ 152 7. 3. 2 Non-self-sustainable SCCs.............. 157 8 T*CoP: Termination of CHR on top of Prolog....... 160 8. 1 Implementation.................... 161 8. 1. 1 The non-self-sustainability test.......... 162 8. 1. 2 Verifying the RC for CHR.............. 162 8. 1. 3 The Diophantine constraint solver.......... 163 8. 2 Evaluation...................... 163 8. 2. 1 Interpreting the results.............. 164 8. 2. 2 Evaluating the results............... 166 9 Practice: Conclusions.................. 170 9. 1 Limitations of the RC for CHR with P......... 171 9. 1. 1 Limitations inherited from the RC for CHR...... 171 9. 1. 2 Interpreting functor symbols in Z......... 171 9. 1. 3 Interargument relations with multiple conjuncts... 172 9. 1. 4 Limitations not specific to CHR........... 173 9. 2 Self-sustainability of propagation.......... 174 10 Conclusions...................... 175 10. 1 A formal framework for CHR-termination........ 175 10. 2 Verifiable conditions for CHR-termination...... 176 Bibliography........................ 179 Biography......................... 191 List of Publications.................... 193 nrpages: 228 status: publishe...|$|R
40|$|This book {{presents}} computing {{technologies for}} university {{students enrolled in}} advanced programming classes such as "Algorithms and programming", and software development professionals. It will give the reader an informative, challenging ad entertaining introduction to use C language to solve complex problems. Those problems will include advance algorithms and complex data structures. The book will concentrate on complete working programs, somehow presenting and contrasting several possible solutions. This work assumes a general-purpose knowledge of the C language {{such as the one}} usually learned during basic programming courses delivered at {{the first year of the}} curricula in computer engineering and computer science. The book main highlights are the following: - Extended coverage of pointers, dynamic arrays and matrices, linked-lists, and other basic data structures. - Abstract data types (ADTs). - Recursions and recursive algorithms. Each topic is covered by a rich collection of partial and complete examples, and about 100 fully implemented and debugged programs. The focus is on good software engineering, and on program clarity, such that the reader is guided to learn properties of algorithms and data structures as fast as possible. The content of each chapter is the following: - Chapter 1 presents a revising set of exercise to recall basic C construct and problem solving strategies. This is essentially a very brief summary of Volume I by the same author. Code and programming style follow the same book. - Chapter 2 covers dynamic memory allocation. Pointers, operators on pointers, dynamic arrays, and dynamic matrices are covered into details. - Chapter 3 presents dynamically allocated lists. Simple, ordered, bi-linked, circular, and list-of-lists are described with several figures and code segments. - Chapter 4 describes basic concepts for recursion and it presents simple recursive programs. - Chapter 5 includes standard recursive problems such as <b>merge-sort,</b> quick-sort, the eight queen problems, etc. It also describes combinatorics problems, such as: The multiplication principle, simple arrangements, arrangements with repetitions, simple permutation, permutation with repetitions, simple combination, combinations with repetitions, and the power-set. - Chapter 6 describe how to apply recursion to some hard-to-solve problems. - Chapter 7 describes all required concepts to write programs "in the large", i. e., multi-file programs, with personalized header files. It also illustrates the main concepts of Abstract Data Type (ADTs) and their use in C programming. - Chapter 8 illustrates several ADT-based problems, such as the stack (based on an array or a dynamic list), queues, etc. The book is also covered by online material, as all source codes are available on the editor web page. We would sincerely appreciate any comments, criticisms, corrections and suggestions for improving the text. Please address all correspondence to: stefano. quer@polito. it or visits the following web page: [URL]...|$|R
40|$|The {{focus of}} this thesis is on systems that employ both flash and {{magnetic}} disks as storage media. Considering the widely disparate I/O costs of flash disks currently on the market, our approach is a cost-aware one: we explore techniques that exploit the I/O costs of the underlying storage devices to improve I/O performance. We also study the asymmetric I/O properties of magnetic and flash disks and propose algorithms that {{take advantage of this}} asymmetry. Our work is geared towards database systems; however, most of the ideas presented in this thesis can be generalised to any data-intensive application. For the case of low-end, inexpensive flash devices with large capacities, we propose using them at the same level of the memory hierarchy as magnetic disks. In such setups, we study the problem of data placement, that is, on which type of storage medium each data page should be stored. We present a family of online algorithms {{that can be used to}} dynamically decide the optimal placement of each page. Our algorithms adapt to changing workloads for maximum I/O efficiency. We found that substantial performance benefits can be gained with such a design, especially for queries touching large sets of pages with read-intensive workloads. Moving one level higher in the storage hierarchy, we study the problem of buffer allocation in databases that store data across multiple storage devices. We present our novel approach to per-device memory allocation, under which both the I/O costs of the storage devices and the cache behaviour of the data stored on each medium determine the size of the main memory buffers that will be allocated to each device. Towards informed decisions, we found that the ability to predict the cache behaviour of devices under various cache sizes is of paramount importance. In light of this, we study the problem of efficiently tracking the hit ratio curve for each device and introduce a lowoverhead technique that provides high accuracy. The price and performance characteristics of high-end flash disks make them perfectly suitable for use as caches between the main memory and the magnetic disk(s) of a storage system. In this context, we primarily focus on the problem of deciding which data should be placed in the flash cache of a system: how the data flows from one level of the memory hierarchy to the others is crucial for the performance of such a system. Considering such decisions, we found that the I/O costs of the flash cache play a major role. We also study several implementation issues such as the optimal size of flash pages and the properties of the page directory of a flash cache. Finally, we explore sorting in external memory using external <b>merge-sort,</b> as the latter employs access patterns that can take full advantage of the I/O characteristics of flash memory. We study the problem of sorting hierarchical data, as such is necessary for a wide variety of applications including archiving scientific data and dealing with large XML datasets. The proposed algorithm efficiently exploits the hierarchical structure in order to minimize the number of disk accesses and optimise the utilization of available memory. Our proposals are not specific to sorting over flash memory: the presented techniques are highly efficient over magnetic disks as well. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R

