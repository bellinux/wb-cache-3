11|31|Public
40|$|The {{form of a}} {{building}} determines its envelope-to-volume ratio (E/V), which is an early indicator of the construction cost of its facades & roof. This study examines E/V ratios of several rectangular blocks at various proportions, leading to conclusions on the <b>minimum</b> <b>envelope</b> size relative to total floor area of {{a building}}. E/V ratio is also recognized as a simple index of a building's contact with the surrounding climatic conditions that affect its running cost. To explore solar exposure, the research examines incident radiation on the same configurations in Athens, London, and Riyadh, correlating building form & size to insolation levels...|$|E
40|$|Abstract. Gun pipe {{linearity}} check and measure belongs to difficult deep-hole space check and measure. After the deviation data is obtained using designed {{linearity check}} and measure instrument for gun pipe, it need carry out analysis and operation {{to get the}} error. The essence of using <b>minimum</b> <b>envelope</b> region method to resolve and evaluate gun pipe linearity error is one optimizing problem about envelopment column surface, wherefore, it built error solution model of space linearity based on genetic algorithm, and it developed one solution and analysis software system using VB program. Application shows that the algorithm {{can be used to}} resolve linearity error quickly and conveniently, and the precision is good...|$|E
40|$|Motivated {{by recent}} {{development}} in high speed networks, {{in this paper}} we study two types of stability problems: (i) conditions for queueing networks that render bounded queue lengths and bounded delay for customers, and (ii) conditions for queueing networks in which the queue length distribution of a queue has an exponential tail with rate `. To answer these two types of stability problems, we introduce two new notions of traffic characterization: <b>minimum</b> <b>envelope</b> rate (MER) and <b>minimum</b> <b>envelope</b> rate with respect to `. Based on these two new notions of traffic characterization, we develop a set of rules for network operations such as superposition, input-output relation of a single queue, and routing. Specifically, we show that (i) the MER of a superposition process is {{less than or equal to}} the sum of the MER of each process, (ii) a queue is stable in the sense of bounded queue length if the MER of the input traffic is smaller than the capacity, (iii) the MER of a departure process from a stable queue is less than or equal to that of the input process (iv) the MER of a routed process from a departure process is less than or equal to the MER of the departure process multiplied by the MER of the routing process. Similar results hold for MER with respect to ` under a further assumption of independence. These rules provide a natural way to analyze feedforward networks with multiple classes of customers. For single class networks with nonfeedforward routing, we provide a new method to show that similar stability results hold for such networks under the FCFS policy. Moreover, when restricting to the family of two-state Markov modulated arrival processes, the notion of MER with respect to ` is shown to b...|$|E
3000|$|Using these {{envelopes}} we {{calculate the}} local mean, m(t) as: m([...] t [...]) = ([...] e_min([...] t [...]) + e_max([...] t [...]))/ 2 here emin and emax denotes the <b>minimum</b> and maximum <b>envelope</b> [...]...|$|R
40|$|A {{new method}} {{to find the}} <b>minimum</b> {{circular}} <b>envelope</b> of a radiating system from one view, one frequency field data is presented. Without solving the full inverse problem, the approach estimates {{the center and the}} radius of the smallest ball containing the sources/scatterers by minimizing a monomodal objective functional by means of a gradient-based optimization technique. The results of the numerical analysis asses the reliability of the proposed technique...|$|R
40|$|A {{simple method}} has been {{developed}} to determine the 13; optimum flap deflection schedule and <b>minimum</b> drag <b>envelope</b> 13; for given wing and flap geometries. The method is mainly 13; based on linear theory and the optimum quot;solutionquot; is. 13; determined by minirnising the lift dependent drag of the wing. The method {{can also be used}} in an inverse senso for the design of the gepmetry of plain flaps at leading and 13; trailing edges...|$|R
40|$|Shoulder and hip joints {{for hard}} space suits are {{disclosed}} which are comprised of three serially connected truncated spherical sections, {{the ends of}} which converge. Ball bearings between the sections permit relative rotation. The proximal {{end of the first}} section is connected to the torso covering by a ball bearing and the distal end of the outermost section is connected to the elbow or thigh covering by a ball bearing. The sections are equi-angular and this alleviates lockup, the condition where the distal end of the joint leaves the plane in which the user is attempting to flex. The axes of rotation of the bearings and the bearing mid planes are arranged to intersect in a particular manner that provides the joint with a <b>minimum</b> <b>envelope.</b> In one embodiment, the races of the bearing between the innermost section and the second section is partially within the inner race of the bearing between the torso and the innermost spherical section further to reduce bulk...|$|E
40|$|A compact {{empirical}} mode decomposition (CEMD) {{is presented}} to reduce mode mixing, end effect, and detrend uncertainty in analysis of time series (with N data points). This new approach consists of two parts: (a) highest-frequency sampling (HFS) to generate pseudo extrema for effective identification of upper and lower envelopes, and (b) a set of 2 N algebraic equations for determining the maximum (<b>minimum)</b> <b>envelope</b> at each decomposition step. Among the 2 N algebraic equations, 2 (N − 2) equations are derived {{on the base of}} the compact difference concepts using the Hermitan polynomials with the values and first derivatives at the (N − 2) non-end points. At each end point, zero third derivative and determination of the first derivative from several (odd number) nearest original and pseudo extrema provide two extra algebraic equations for the value and first derivative at that end point. With this well-posed mathematical system, one can reduce the mode mixing, end effect, and detrend uncertainty drastically, and separate scales naturally without any a priori subjective criterion selection...|$|E
40|$|Many {{algorithms}} for spatial {{color correction}} of digital im- ages {{have been proposed}} in the past. Some of the most recently developed algorithms use stochastic sampling of the image in or- der to obtain maximum and <b>minimum</b> <b>envelope</b> functions. The envelopes are in turn used to guide the color adjustment of the en- tire image. In this paper, we propose to use a variational method instead of the stochastic sampling to compute the envelopes. A numerical scheme for solving the variational equations is out- lined, and we conclude that the variational approach is computa- tionally more efficient than using stochastic sampling. A percep-tual experiment with 20 observers and 13 images is carried out in order to evaluate {{the quality of the}} resulting images with the two approaches. There is {{no significant difference between the}} variational approach and the stochastic sampling when it comes to overall image quality as judged by the observers. However, the observed level of noise in the images is significantly reduced by the variational approach...|$|E
40|$|An {{analysis}} {{tool for}} calculating <b>minimum</b> pressure <b>envelopes</b> was developed using XFOIL. This thesis presents MATLAB® executables that interface with {{a modified version}} of XFOIL for determining the minimum pressure of a foil operating in an inviscid fluid. The code creates <b>minimum</b> pressure <b>envelopes,</b> similar to those published by Brockett (1965). XFOIL, developed by Mark Drela in 1986, is a design system for Low Reynolds Number Airfoils that combines the speed and accuracy of high-order panel methods with fully-coupled viscous/inviscid interaction. XFOIL was altered such that it reads in command line arguments that provide operating instructions, rather than operator interaction via menu options. In addition, all screen output and plotting functions were removed. These modifications removed XFOIL's user interface, and created a "black box" version of XFOIL that would perform the desired calculations and write the output to a file. These modifications allow rapid execution and interface by an external program, such as MATLAB®. In addition, XFOIL's algorithms provide a significant improvement in the accuracy of minimum pressure prediction over the method published by Brockett. Development of the modified XFOIL and MATLAB® interface contained in this thesis is intended for future interface with Open-source Propeller Design and Analysis Program (OpenProp). OpenProp is an open source MATLAB®-based suite of propeller design tools. Currently, OpenProp performs parametric analysis and single propeller design, but does not perform cavitation analysis. <b>Minimum</b> pressure <b>envelopes</b> provide the propeller designer information about operating conditions encountered by propellers. (cont.) The code developed in this thesis allows the designer to rapidly assess cavitation conditions while in the design phase, and make modifications to propeller blade design in order to optimize cavitation performance. A methodology for design is discussed outlining future integration with OpenProp. by Christopher J. Peterson. Thesis (S. M.) [...] Massachusetts Institute of Technology, Dept. of Mechanical Engineering, 2008. Includes bibliographical references (leaf 42) ...|$|R
40|$|CIVINSAn {{analysis}} {{tool for}} calculating <b>minimum</b> pressure <b>envelopes</b> was developed using XFOIL. This thesis presents MATLAB xecutables that interface with {{a modified version}} of XFOIL for determining the minimum pressure of a foil operating in an inviscid fluid. The code creates <b>minimum</b> pressure <b>envelopes,</b> similar to those published by Brockett (1965). XFOIL, developed by Mark Drela in 1986, is a design system for Low Reynolds Number Airfoils that combines the speed and accuracy of high-order panel methods with fully-coupled viscous/inviscid interaction. XFOIL was altered such that it reads in command line arguments that provide operating instructions, rather than operator interaction via menu options. In addition, all screen output and plotting functions were removed. These modifications removed XFOIL's user interface, and created a "black box" version of XFOIL that would perform the desired calculations and write the output to a file. These modifications allow rapid execution and interface by an external program, such as MATLABÂ®. In addition, XFOIL's algorithms provide a significant improvement in the accuracy of minimum pressure prediction over the method published by Brockett. Contract number: N 62271 - 97 -G- 0026. CIVINSUS Navy (USN) author (civilian) ...|$|R
40|$|Published October 6, 2016, {{as part of}} the Proceedings of the 2015 Mathematics and Statistics in Industry Study Group. We {{consider}} the pressure pulse or surge in a pipeline due to an unplanned sudden shutdown of a pump system in the pipeline network. This is known as water hammer. Our primary focus is the negative pressure pulse that travels downstream from the pump(s), is reflected with a sign reversal from the end, and travels back to the pump(s). As part of the preliminary design of a pipeline it is necessary to determine the <b>minimum</b> head <b>envelope</b> associated with such an event, which is used to determine where surge protection will be needed in the pipeline. Of particular interest is whether the initial head drop at the pump(s) due to a sudden drop of the flow speed to zero, as given by the Joukowski formula, gives a sufficiently accurate prediction of the minimum head at the pump(s). This minimum head is used to construct the <b>minimum</b> head <b>envelope</b> for the downstream pipeline. An examination of the relevant literature along with solution of the water hammer equations shows that, assuming the flow speed falls instantaneously to zero at the pump(s), the total drop in head at the pump(s) is given by the sum of the initial Joukowski head change and the friction loss under normal operating conditions. While the friction loss may not be significant in short pipelines, in long pipelines it cannot be neglected. Y. M., Stokes, A. Miller, G. Hockin...|$|R
40|$|Are we alone?' {{is one of}} {{the primary}} {{questions}} of astrobiology, and whose answer defines our significance in the universe. Unfortunately, this quest is hindered by {{the fact that we have}} only one confirmed example of life, that of earth. While this is enormously helpful in helping to define the <b>minimum</b> <b>envelope</b> for life, it strains credulity to imagine that life, if it arose multiple times, has not taken other routes. To help fill this gap, our lab has begun using synthetic biology - the design and construction of new biological parts and systems and the redesign of existing ones for useful purposes - as an enabling technology. One theme, the "Hell Cell" project, focuses on creating artificial extremophiles in order to push the limits for Earth life, and to understand how difficult it is for life to evolve into extreme niches. In another project, we are re-evolving biotic functions using only the most thermodynamically stable amino acids in order to understand potential capabilities of an early organism with a limited repertoire of amino acids...|$|E
40|$|AM Canum Venaticorum (AM CVn) {{binaries}} {{consist of}} a degenerate helium donor and a helium, C/O, or O/Ne WD accretor, with accretion rates of ˙ M = 10 − 13 − 10 − 5 M ⊙ yr − 1. For accretion rates < 10 − 6 M ⊙ yr − 1, the accreted helium ignites unstably, resulting in a helium flash. As the donor mass and ˙M decrease, the ignition mass increases and eventually becomes larger than the donor mass, yielding a “last-flash ” ignition mass of � 0. 1 M⊙. Bildsten et al. (2007) predicted that the largest outbursts of these systems will lead to dynamical burning and thermonuclear supernovae. In this paper, we study {{the evolution of the}} He-burning shells in more detail. We calculate maximum achievable temperatures as well as the <b>minimum</b> <b>envelope</b> masses that achieve dynamical burning conditions, finding that AM CVn systems with accretors � 0. 8 M ⊙ will undergo dynamical burning. Triple-α reactions during the hydrostatic evolution set a lower limit to the 12 C mass fraction of 0. 001 − 0. 05 when dynamical burning occurs, but core dredge-up may yield 12 C, 16 O, and/or 20 Ne mass fractions of ∼ 0. 1. Accreted 14 N will likely remain 14 N during the accretion and convective phases, but regardless of 14 N’s fate, the neutron-to-proton ratio at the beginning of convection is fixed until the onset of dynamical burning. During explosive burning, the 14 N will undergo 14 N(α, γ) 18 F(α, p) 21 Ne, liberating a proton for the subsequent 12 C(p, γ) 13 N(α, p) 16 O reaction, which bypasses the relatively slow α-capture onto 12 C. Future hydrodynamic simulations must include these isotopes, as the additional reactions will reduce the Zel’dovich-von Neumann-Döring (ZND) length, making the propagation of the detonation wave more likely. Subject headings: binaries: close — novae, cataclysmic variables — supernovae: general — white dwarfs 1...|$|E
40|$|The {{depth of}} {{closure of the}} beach profile, from now on termed as DoC, is a key {{parameter}} to perform effective evaluations of beach nourishments or coastal defence works. It is defined for a given time interval, as the closest depth to the shore at {{which there is no}} significant change in seabed elevation and no significant net sediment transport between the nearshore and offshore. To obtain this point it is necessary to compare profile surveys at a given period of time, and evaluate them to find the point in the profile where the depth variation is equal to, or less than, a pre-selected criteria. In order to manage all this information, a software application has been developed. On providing the input of the beach profiles, this tool offers the possibility of selecting the dates of the desired period of study, graph the profiles and then obtain, for each XY coordinate, all the required parameters, such as offshore distance, maximum, average and minimum depth, standard deviation and area difference between profiles. By evaluating each point along the profile, the DoC can be obtained at that point that meets the criteria. Moreover, this tool allows to graph not only the initial and final profile of the period, but all the beach profiles recorded, creating its maximum and <b>minimum</b> <b>envelope.</b> In addition, if the user introduces the parameters related to the equilibrium beach profile, this tool also corrects the area difference, taking into account the morphological changes (erosion– accretion) that may have occurred during the period studied. In conclusion, this tool has a friendly interface for obtaining the DoC with accuracy by interactive selection of the period of study. It also stores all the information and exports it to different formats. This research has been partially funded by Universidad de Alicante through the project ‘Estudio sobre el perfil de equilibrio y la profundidad de cierre en playas de arena’ (YGRE 15 - 02) ...|$|E
40|$|We {{give the}} first polynomial-time {{algorithm}} {{for the problem}} of finding a minimumperimeter k-gon that encloses a given n-gon. Our algorithm {{is based on a}} simple structural result, that an optimal k-gon has at least one “flush ” edge with the ngon. This allows one to reduce the problem to computing the shortest k-link path in a simple polygon. As a by-product we observe that the minimum-perimeter “envelope” — a convex polygon with a specified sequence of interior angles — can also be found in polynomial time. Finally, we introduce the problem of finding optimal convex polygons restricted to lie in the region between two nested convex polygons. We give polynomial-time algorithms for the problems of finding the <b>minimum</b> restricted <b>envelopes...</b>|$|R
40|$|Evidence of beam loading due to {{distributed}} injection in Plasma Wakefield Accelerator experiments {{carried out}} at the FACET facility at SLAC during the year 2012 is presented. The source of the injected charge is tunnel ionization of Rb + inside the wake, which occurs {{along the length of}} the interaction at each <b>minima</b> of <b>envelope</b> betatron oscillation. Rb was used specifically to mitigate the problem of head erosion, which limited the energy gain in earlier experiments using Li that were {{carried out at}} FFTB in SLAC. In the present experiment however, electrons produced via secondary ionization of Rb were injected in the wake and led to a severe depletion of the accelerating wake, i. e. beam loading, which is observed as a reduction of mean, i. e. measured, transformer ratio. This “dark current ” limitation on the maximum achievable accelerating gradient is also pertinent to other heavier ions that are potential candidates for high-gradient PWFA...|$|R
40|$|The {{suite of}} a wavelet {{is defined as}} being all wavelets that share a common {{amplitude}} spectrum and total energy but differ in phase spectra. Within a suite there are also classes of wavelets. A wavelet class has a common amplitude envelope and energy distribution. As such, it includes all wavelets that differ by only a constant-angle phase shift. Of all wavelets within suite, the zero-phase wavelet has the <b>minimum</b> energy <b>envelope</b> width; its energy is confined to minimum time dispersion. Therefore, the zero-phase wavelet has maximum resolving power within the suite. Because a zero-phase wavelet shares its amplitude envelope with a class of wavelets that differ by only a constant phase shift, all wavelets of the class also have maximum resolving power within the suite. The most familiar {{of these is the}} quadrature-phase wavelet (90 degree phase shift). Use of the complex trace results in an evaluation of the total energy, both potential and kinetic, of the wavelet signal. Assuming the wave [...] ...|$|R
40|$|A {{distributed}} queueing Medium Access Control (MAC) protocol {{is used in}} Distributed Queue Dual Bus (DQDB) networks. A {{modified version}} of the MAC protocol was proposed by R. R. Pillai and U. Mukherji in an attempt to overcome some of the shortcomings of the DQDB MAC protocol. They analyzed the performance of the system for Bernoulli arrivals and for large propagation delays between the nodes. We extend the performance analysis of the modified MAC protocol for a DQDB type of Network. The parameter of interest to us is the bus access delay. This has two components, viz., the request bus access delay and the data bu 6 access delay. We use the model at the request point at node and present methods to evaluate the delay experienced in such a model. The model is an n-priority. /D/l queue with D vacations (non-preemptive priority) where n is the number of nodes sending requests on the request bus for transmission on the data bus. The methods presented help to evaluate the request bus access delay when the arrivals at each node are Markovian Arrival Processes (MAPs). The algorithms for evaluating the mean request bus access delay are based on matrix geometric techniques. Thus, one can use the algorithms developed in the literature to solve for the finite buffers case too. This model, for the request bus access delay, holds irrespective of the propagation delay between the nodes. We also evaluate the inter-departure time of class 1 customers and virtual customers in a 2 -priority M/G/l system with G vacations (non-preemptive priority). In the case of Poisson arrivals at all the nodes, we would have a 2 -priority M/D/l system with D vacations (non-preemptive priority). We thus evaluate the inter-arrival time of the free slots on the data bus as seen by Node 2. Note that this is independent of the number of active nodes in the network We then develop methods to evaluate the mean data bus access delay experienced by the customers at Node 2 in a three-node network with 2 nodes communicating with the third when the propagation delay between the nodes is large. We consider the case of finite Local Queue buffers at the two nodes. Using this assumption we arrive at process of arrivals to the Combined Queue and the process of free slots on the data bus to be Markov Modulated Bernoulli processes. The model at the combined queue at Node 2 then has a Quasi Birth-Death evolution. Thus, this system is solved by using the Ramaswami-Latouche algorithm. The stationary probabilities are then used to evaluate the mean data bus access delay experienced at Node 2. The finite buffer case of this system can be solved by G. Wi Stewart's algorithm. The method in modelling the system and the results are presented in detail for Poisson arrivals. The extension of this to more complex processes is also explained. We encounter in the analysis an explosion of the state-space of the system. We try to counter this by considering approximations to the process of free slots on the data bus. The approximations considered are {{on the basis of what}} are known as Idealized Aggregates. The performance of the approximation is also detailed. It works very well under low and moderate load but underestimates the mean delay under heavy load. Thereafter, we discuss the performance of the system with reference to the mean of the access delay and the standard deviation of the access delay under varying traffic at the two nodes. For this part we use simulation results to discuss the performance. The comparison between the performance measures at both the nodes is also done. Then we develop methods/techniques to understand the performance of the system when we have finite propagation delays between the nodes. We concentrate on the 3 -node problem and calculate performance bounds based on linear programs. This is illustrated in detail for Bernoulli arrivals for the case of 1 slot propagation delay between the nodes as well as for the case of 2 slots propagation delay. The performance of the bounds obtained is also detailed. The presence of an idling system at the combined queue of Node 2 makes the bounds somewhat loose. Finally, we discuss the performance of the system with reference to the mean access delay and the standard deviation of the access delay under varying load on the system. Again, we rely on simulation studies. Finally, we study the performance of the system as a multiplexer. For this, we re­strict the traffic to Markov Modulated Processes (or those which would satisfy the Gartner-Ellis Theorem requirements). The traffic is characterized by what are known as Envelope Processes - Lower and Upper. The class of processes which satisfy the conditions of the Gartner-Ellis theorem come under the category where both the Envelope Processes exist and the <b>Minimum</b> <b>Envelope</b> Rate and the Maximum Lower Envelope Rate are the same. We use the system evolution equations at the combined queue at any node to develop re­lations between the various input and output processes. First, this is done for a. system of this kind, in isolation. Then, we consider this system as a part of the modified protocol and present relations, among the various input and output processes, which are specific to the modified protocol. The possible use of all of the above to do Admission Control at the entry point to the Asynchronous Transfer Mode (ATM) network is also presented...|$|E
40|$|The {{feasibility}} of propagating annular IREB in periodic permanent magnetic (PPM) field was studied. The magnetic field profile of alternatively arranged permanent magnet rings was calculated first using the finite element magnetic method (FEMM). The forces acting on IREB’s electron in such magnetic field {{were analyzed by}} the use of fluid model and the radial force equation in modified Mathieu function form was drawn then. At last a 2. 5 -D particle in-cell (PIC) simulation code was used to investigate the physical process of the IREB’s propagation. The results showed that the transverse and axial momentum of beam electrons were modulated and the amplitude of beam envelope fluctuation was relative to the beam intensity and the beam electron’s initial incident angle. The condition of the <b>minimum</b> beam <b>envelope</b> scallop was obtained and conclusion was made that several kilo-amperes intense annular electron beam could propagate stably in a guiding periodic magnet with commercial permanent magnets...|$|R
40|$|AbstractA novel invariant-based {{approach}} to describe elastic properties and failure of composite plies and laminates is proposed. The approach {{is based on}} the trace of the plane stress stiffness matrix as a material property, which can be used {{to reduce the number of}} tests and simplify the design of laminates. Omni strain failure envelopes are proposed as the <b>minimum</b> inner failure <b>envelope</b> in strain space, which defines the failure of a given composite material for all ply orientations. The proposed approach is demonstrated using various carbon/epoxy composites and offers radically new scaling to improve design and manufacturing...|$|R
40|$|AbstractA novel invariant-based {{approach}} to describe elastic properties and failure of composite plies and laminates has been recently {{proposed in the}} literature. An omni strain failure envelope {{has been defined as}} the <b>minimum</b> inner failure <b>envelope</b> in strain space, which describes the failure of a given composite material for all ply orientations. In this work, a unit circle is proposed as a strain normalized failure envelope for any carbon fiber reinforced polymer laminate. Based on this unit circle, a failure envelope can be generated from the longitudinal tensile and compressive strains-to-failure of a unidirectional ply. The calculated failure envelope was found in good agreement with experimental data published in the well-known World Wide Failure Exercise...|$|R
40|$|Cost-benefit {{analysis}} fit outdoor PM 2. 5 in Toronto. By combining {{a combined}} mass balanced model, a time-weighted activity exposure model, epidemiological based concentration-response, and monetary valuation method, save US$ 2. 3 billion/year in health care. Citywide adoption of R 2000 standard from current housing ave re Since then, many {{policies have been}} implemented to mitigate hu-man exposures of ambient PM 2. 5 to reduce the citywide health impact. These policies focused on outdoor related strategies such as reducing emission strengths of stacks and using “cleaner” 2. 5 es and regulations ndoor exposure to nal Building Code new buildings and 4]. The NBC is only provincial/territo-vince of Ontario, d by NBC and the Ontario Building Code (OBC) [4, 5]. These codes establish the limiting design factors such as minimum ventilation rates per person, <b>minimum</b> building <b>envelope</b> insulation values and guid-ance on use of filters for safety and fire protection purposes. Resi-dential buildings adopting the building codes typically install Heat Recovery Ventilators (HRVs). Future revisions of NBC include possible reduction of PM 2. 5 using air cleaning devices in the HVAC system if the outdoor air pollution levels are above ambient threshold levels...|$|R
40|$|We {{report on}} the status of Z-Spec, {{including}} preliminary results of our first astronomical measurements. Z-Spec is a cryogenic, broadband, millimeter-wave grating spectrometer designed for molecular line surveys of galaxies, including carbon monoxide redshift measurements of high-redshift submillimeter sources. With an instantaneous bandwidth of 185 - 305 GHz, Z-Spec covers the entire 1 mm atmospheric transmission window with a resolving power of 200 - 400. The spectrometer employs the Waveguide Far-Infrared Spectrometer (WaFIRS) architecture, in which the light propagation is confined within a parallel-plate waveguide, resulting in a <b>minimum</b> mechanical <b>envelope.</b> Its array of 160 silicon-nitride micromesh bolometers is cooled to below 100 mK for background-limited performance. With its sensitivity, broad bandwidth, and compactness, Z-Spec serves as a prototype for a future far-IR spectrometer aboard a cold telescope in space. Z-Spec successfully demonstrated functionality with a partial array of detectors and warm electronics during a week-long engineering run at the Caltech Submillimeter Observatory in June, 2005. We describe the instrument performance evaluated at the telescope and in subsequent laboratory tests and compare these results with design specifications. Following several modifications we returned to the telescope in April, 2006. We present a preliminary astronomical spectrum and discuss our plans to improve sensitivity and throughput to achieve our ultimate science goals...|$|R
40|$|In {{this paper}} we show how {{to convert the}} problem of {{estimating}} delay, slope and curvature of a parabolic event into a frequency estimation problem. Two dimensional data (time and offset) is converted into samples on a two-dimensional manifold embedded in a three-dimensional spaced. To conduct frequency estimation on this manifold we design general domain Hankel matrices and make use of a fixed point algorithm {{that is designed to}} find <b>minima</b> of convex <b>envelopes</b> of functionals using a combination of a rank penalty and a misfit function, under the constraint of a certain matrix structure. We illustrate that the proposed method can successfully detect the parameters of the parabolic events also in the case of unequally spaced spatial sampling and in the presence of rather high levels of noise...|$|R
40|$|Reliable {{control of}} the {{deposition}} process of optical films and coatings frequently requires monitoring of the refractive index profile throughout the layer. In the present work a simple in situ approach is proposed which uses a WKBJ matrix representation of the optical transfer function of a single thin film on a substrate. Mathematical expressions are developed which represent the <b>minima</b> and maxima <b>envelopes</b> of the curves transmittance-vs-time and reflectance-vs-time. The refractive index and extinction coefficient depth profiles of different films are calculated from simulated spectra {{as well as from}} experimental data obtained during PECVD of silicon-compound films. Variation of the deposition rate with time is also evaluated from the position of the spectra extrema as a function of time. The physical and mathematical limitations of the method are discussed. Comment: 10 pages, 11 figures, REVTeX, to be published in Applied Optic...|$|R
40|$|A new {{approach}} to find the number and locations of electromagnetic scatterers (sources) from one view, one frequency field data is presented. The case of scatterers (sources) with dimensions comparable to the wavelength is considered. First, in order to devise an effective technique, a new relevant property of the electromagnetic field, the point source spectral content, is introduced and its relationship with both the sliding windowed Fourier transform {{of the field and}} the local bandwidth function is discussed. To enlighten its usefulness it is also shown that a footprint of the scattering system encoding information on its geometry can be easily extracted from the scattered field by exploiting this new property. On this basis, the new localization technique is introduced. In order to restrict the search region, the <b>minimum</b> circular <b>envelope</b> enclosing the scatterers is found by purposely introducing a new technique exploiting the effective bandwidth of the radiated field. Then, the number and locations of scatterers is retrieved by using the field local quantitative feature previously introduced, without the complexities of the full inverse problem as it is usually done by the traditional approaches. In this way, not only a simplified technique is obtained but also the ill-posedness of the problem and the noise effects are significantly mitigated. The effectiveness of the proposed technique and its overall performance with respect to a singular value decomposition based approach are proved by means of a numerical analysis...|$|R
40|$|International audienceThis paper {{presents}} an experimental method {{for determining the}} local strain distribution in the plies of a thermoplastic 5 -harness satin weave composite under uni-axial static tensile load. In contrast to unidirectional composites, the yarn interlacing pattern in textile composites causes heterogeneous strain fields with large strain gradients around the yarn crimp regions. In addition, depending on the local constraints that are imposed by the surrounding plies, the deformation behaviour of the laminate inner layers may vary {{from that of the}} surface layers, which are relatively more free to deform, compared to the inner layers. In order to validate the above hypothesis, the local strains on the composite surface were measured using digital image correlation technique (LIMESS). Internal strains in the composite laminate were measured using embedded Fibre Optic Sensors (FOS). Based on the DIC results, the strain profiles at various locations on the composite surface were estimated. Using the FOS results, the maximum and minimum strain values in the laminate inner layers were evaluated. Comparison of the local strain values at different laminate positions provides an estimate of the influence of the adjacent layers on the local longitudinal strain behaviour of a satin weave composite. Part II of this paper elucidates the local strain variation computed using the meso-FE simulations. In addition to the comparison of numerical and experimental strain profiles, Part II presents the maximum and <b>minimum</b> strain <b>envelopes</b> for the carbon-PPS (PolyPhenelyne Sulphide) thermoplastic 5 -harness satin weave composite...|$|R
40|$|Accurate {{estimation}} of the instantaneous frequency of speech resonances is a hard problem mainly due to phase discontinuities in the speech signal associated with excitation instants. We review a variety of approaches for enhanced frequency and bandwidth estimation in the time-domain and propose a new cognitively motivated approach using filterbank arrays. We show that by filtering speech resonances using filters of different center frequency, bandwidth and shape, the ambiguity in instantaneous frequency estimation associated with amplitude <b>envelope</b> <b>minima</b> and phase discontinuities can be significantly reduced. The novel estimators are shown to perform well on synthetic speech signals with frequency and bandwidth micro-modulations (i. e., modulations within a pitch period), {{as well as on}} real speech signals. Filterbank arrays, when applied to frequency and bandwidth modulation index estimation, are shown to reduce the estimation error variance by 85 % and 70 % respectively. Index Terms: speech analysis, time-frequency analysis, filterbank arrays, instantaneous frequency, micro-modulation...|$|R
40|$|The {{search for}} extrasolar planets has already {{detected}} rocky planets and several planetary candidates with minimum masses {{that are consistent}} with rocky planets in the habitable zone of their host stars. A low-resolution spectrum {{in the form of a}} color-color diagram of an exoplanet is likely to be one of the first post-detection quantities to be measured for the case of direct detection. In this paper, we explore potentially detectable surface features on rocky exoplanets and their connection to, and importance as, a habitat for extremophiles, as known on Earth. Extremophiles provide us with the <b>minimum</b> known <b>envelope</b> of environmental limits for life on our planet. The color of a planet reveals information on its properties, especially for surface features of rocky planets with clear atmospheres. We use filter photometry in the visible as a first step in the characterization of rocky exoplanets to prioritize targets for follow-up spectroscopy. Many surface environments on Earth have characteristic albedos and occupy a different color space in the visible waveband (0. 4 – 0. 9 lm) that can be distinguished remotely. These detectable surface features can be linked to the extreme niches that support extremophiles on Earth and provide a link between geomicrobiology and observational astronomy. This paper explores how filter photometry can serve as a first step in character-izing Earth-like exoplanets for an aerobic as well as an anaerobic atmosphere, thereby prioritizing targets to search for atmospheric biosignatures. Key Words: Color-color—Habitability—Extrasolar terrestrial planet— Extreme environments—Extremophiles—Reflectivity. Astrobiology 13, 47 – 56...|$|R
40|$|When {{the product}} of a {{vertical}} square-wave grating (contrast envelope) and a horizontal sinusoidal grating (carrier) are viewed binocularly with different disparity cues they can be perceived transparently at different depths. We found, however, that the transparency was asymmetric; it only occurred when the envelope was perceived to be the overlaying surface. When the same two signals were added, the percept of transparency was symmetrical; either signal could be seen in front of or behind the other at different depths. Differences between these multiplicative and additive signal combinations were examined in two experiments. In one, we measured disparity thresholds for transparency {{as a function of the}} spatial frequency of the envelope. In the other, we measured disparity discrimination thresholds. In both experiments the thresholds for the multiplicative condition, unlike the additive condition, showed distinct <b>minima</b> at low <b>envelope</b> frequencies. The different sensitivity curves found for multiplicative and additive signal combinations suggest that different processes mediated the disparity signal. The data are consistent with a two-channel model of binocular matching, with multiple depth cues represented at single retinal locations...|$|R
40|$|In high-risk pregnancies, the {{transport}} of oxygen and nutrients from maternal to fetal blood via the placenta is often impaired. To assess the risk, pulsed Doppler ultrasound (US) is {{used to evaluate the}} flow velocity waveform in the umbilical artery with the pulsatility index (PI), which is derived from the velocity envelope of the Doppler power spectrum. However, simply listening to the Doppler signal can indicate to an experienced sonographer that the type of the blood flow is worse than the PI suggests. This is however dependent on the operator's experience and {{it may be difficult to}} estimate what influences the subjective judgement. Motivated by the description of the Doppler sounds by an experienced operator (AT) as having a "timbre", this study describes an analysis of Doppler sounds in search for an index or method with capacity to better evaluate the blood flow in the umbilical artery in high-risk pregnancies. A test was designed, where synthetically produced Doppler sounds with various spectral contents were played together with a variable sinusoidal sound signal. The task for the five test persons was to match the frequency of the sinusoidal signal to the Doppler sounds. The tests indicated that the human ear is most sensitive to the lower frequencies of Doppler sounds. An analysis of prerecorded sounds showed a difference in the lower frequencies of a sound considered to emanate from the umbilical blood flow of healthy fetuses with normally functioning placenta as compared to a pathological one. This might explain the difference between the sounds experienced by an operator. As a suggestion to extract more information than the maximum <b>envelope,</b> the <b>minimum</b> frequency <b>envelope</b> was extracted from pre-recorded clinical sounds. Based on the pilot tests presented here, this shows to be a promising strategy...|$|R
40|$|Drastic {{changes in}} {{aircraft}} operational requirements {{and the emergence}} of new enabling technologies often occur symbiotically with advances in technology inducing new requirements and vice versa. These changes sometimes lead to the design of vehicle concepts for which no prior art exists. They lead to revolutionary concepts. In such cases the basic form of the vehicle geometry can no longer be determined through an ex ante survey of prior art as depicted by aircraft concepts in the historical domain. Ideally, baseline geometries for revolutionary concepts would be the result of exhaustive configuration space exploration and optimization. Numerous component layouts and their implications for the minimum external dimensions of the resultant vehicle would be evaluated. The dimensions of the <b>minimum</b> enclosing <b>envelope</b> for the best component layout(s) (as per the design need) would then be used as a basis for the selection of a baseline geometry. Unfortunately layout design spaces are inherently large and the key contributing analysis i. e. collision detection, can be very expensive as well. Even when an appropriate baseline geometry has been identified, another hurdle i. e. vehicle scaling has to be overcome. Through the design of a notional Cessna C- 172 R powered by a liquid hydrogen Proton Exchange Membrane (PEM) fuel cell, it has been demonstrated that the various forms of vehicle scaling i. e. photographic and historical-data-based scaling can result in highly sub-optimal results even for very small O(10 - 3) scale factors. There is therefore a need for higher fidelity vehicle scaling laws especially since emergent technologies tend to be volumetrically and/or gravimetrically constrained when compared to incumbents. The Configuration-space Exploration and Scaling Methodology (CESM) is postulated herein as a solution to the above-mentioned challenges. This bottom-up methodology entails the representation of component or sub-system geometries as matrices of points in 3 D space. These typically large matrices are reduced using minimal convex sets or convex hulls. This reduction leads to significant gains in collision detection speed at minimal approximation expense. (The Gilbert-Johnson-Keerthi algorithm is used for collision detection purposes in this methodology.) Once the components are laid out, their collective convex hull (from here on out referred to as the super-hull) is used to approximate the inner mold line of the <b>minimum</b> enclosing <b>envelope</b> of the vehicle concept. A sectional slicing algorithm is used to extract the sectional dimensions of this envelope. An offset is added to these dimensions in order {{to come up with the}} sectional fuselage dimensions. Once the lift and control surfaces are added, vehicle level objective functions can be evaluated and compared to other designs. For each design, changes in the super-hull dimensions in response to perturbations in requirements can be tracked and regressed to create custom geometric scaling laws. The regressions are based on dimensionally consistent parameter groups in order to come up with dimensionally consistent and thus physically meaningful laws. CESM enables the designer to maintain design freedom by portably carrying multiple designs deeper into the design process. Also since CESM is a bottom-up approach, all proposed baseline concepts are implicitly volumetrically feasible. Furthermore the scaling laws developed from custom data for each concept are subject to less design noise than say, regression based approaches. Through these laws, key physics-based characteristics of vehicle subsystems such as energy density can be mapped onto key system level metrics such as fuselage volume or take-off gross weight. These laws can then substitute some historical-data based analyses thereby improving the fidelity of the analyses and reducing design time. Ph. D. Committee Chair: Dr. Dimitri Mavris; Committee Member: Dean Ward; Committee Member: Dr. Daniel Schrage; Committee Member: Dr. Danielle Soban; Committee Member: Dr. Sriram Rallabhandi; Committee Member: Mathias Emenet...|$|R
40|$|Network Calculus theory aims at {{evaluating}} worst-case {{performances in}} communication networks. It provides methods to analyze models where the traffic and the services are constrained by some <b>minimum</b> and/or maximum <b>envelopes</b> (arrival/service curves). While new applications come forward, a challenging and inescapable issue remains open: achieving tight analyzes of networks with aggregate multiplexing. The theory offers efficient methods to bound maximum end-to-end delays or local backlogs. However {{as shown in}} a recent breakthrough paper [Schmitt-Infocom 08], those bounds can be arbitrarily far from the exact worst-case values, even in seemingly simple feed-forward networks (two flows and two servers), under blind multiplexing (i. e. no information about the scheduling policies, except FIFO per flow). For now, only a network with three flows and three servers, {{as well as a}} tandem network called sink tree, have been analyzed tightly. We describe the first algorithm which computes the maximum end-to-end delay for a given flow, as well as the maximum backlog at a server, for any feed-forward network under blind multiplexing, with concave arrival curves and convex service curves. Its computational complexity may look expensive (possibly super-exponential), but we show that the problem is intrinsically difficult (NP-hard). Fortunately we show that in some cases, like tandem networks with cross-traffic interfering along intervals of servers, the complexity becomes polynomial. We also compare ourselves to the previous approaches and discuss the problems left open...|$|R
40|$|Context. The {{seismology}} of early-type {{stars is}} limited by our incomplete understanding of gravito-inertial modes. Aims. We develop a short-wavelength asymptotic analysis for gravito-inertial modes in rotating stars. Methods. The Wentzel-Kramers-Brillouin approximation {{was applied to the}} equations governing adiabatic small perturbations about a model of a uniformly rotating barotropic star. Results. A general eikonal equation, including the effect of the centrifugal deformation, is derived. The dynamics of axisymmetric gravito-inertial rays is solved numerically for polytropic stellar models of increasing rotation and analysed by describing the structure of the phase space. Three different types of phase-space structures are distinguished. The first type results from the continuous evolution of structures of the non-rotating integrable phase space. It is predominant in the low-frequency part of the phase space. The second type of structures are island chains associated with stable periodic rays. The third type of structures are large chaotic regions that can be related to the <b>envelope</b> <b>minimum</b> of the Brunt-Väisälä frequency. Conclusions. Gravito-inertial modes are expected to follow this classification, in which the frequency spectrum is a superposition of sub-spectra associated with these different types of phase-space structures. The detailed confrontation between the predictions of this ray-based asymptotic theory and numerically computed modes will be presented in a companion paper. Comment: 24 pages, 12 figures, accepted in A&...|$|R
40|$|International audienceNetwork Calculus theory aims at {{evaluating}} worst-case {{performances in}} communication networks. It provides methods to analyze models where the traffic and the services are constrained by some <b>minimum</b> and/or maximum <b>envelopes</b> (arrival/service curves). While new applications come forward, a challenging and inescapable issue remains open: achieving tight analyzes of networks with aggregate multiplexing. The theory offers efficient methods to bound maximum end-to-end delays or local backlogs. However {{as shown in}} a recent breakthrough paper [28], those bounds can be arbitrarily far from the exact worst-case values, even in seemingly simple feed-forward networks (two flows and two servers), under blind multiplexing (i. e. no information about the scheduling policies, except FIFO per flow). For now, only a network with three flows and three servers, {{as well as a}} tandem network called sink tree, have been analyzed tightly. We describe the first algorithm which computes the maximum end-to-end delay for a given flow, as well as the maximum backlog at a server, for any feed-forward network under blind multiplexing, with piecewise affine concave arrival curves and piecewise affine convex service curves. Its computational complexity may look expensive (possibly super-exponential), but we show that the problem is intrinsically difficult (NP-hard). Fortunately we show that in some cases, like tandem networks with cross-traffic interfering along intervals of servers, the complexity becomes polynomial. We also compare ourselves to the previous approaches and discuss the problems left open...|$|R
