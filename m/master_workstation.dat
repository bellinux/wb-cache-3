1|4|Public
40|$|Multiple {{loudspeakers}} can be configured {{to create}} a three-dimensional virtual sound space that allows a sound to be placed at any point within the virtual sound space. The accurate placement {{of the position of}} a sound {{plays an important role in}} virtual-reality applications and allows composers to realise compositions that exploit sonic spatialization. Software that represents and controls sound placement within a virtual sound space typically uses expensive commercial or dedicated audio hardware to mix and route audio signals. This paper discusses the perception of sound spaces, the acoustical properties of spatial sound, and describes a Java-based virtual sound environment. A teaching laboratory equipped with multimedia workstations provides the required hardware. A single <b>master</b> <b>workstation</b> controls the mixing and routing of audio by controlling the loudspeakers on multiple slave workstations. The master machine controls the slave loudspeakers using the networking capabilities of Java. Synchronization between the workstations is achieved using MIDI Time Code...|$|E
40|$|Minimally {{invasive}} surgical techniques, {{in particular}} laparoscopy, have many advantages over traditional surgery, most notably more rapid patient recovery. Limitations of current instrumentation, however, can make laparoscopic surgery awkward for the surgeon. A teleoperative surgical workstation {{would improve the}} surgeon's dexterity and control during laparoscopy. This report first presents a brief background on laparoscopy, describes a proposed telesurgical workstation, and discusses design goals for the human interface, or dextrous <b>master,</b> for this <b>workstation.</b> It then describes a prototype glove-like master and presents preliminary performance and calibration data obtained with this device. 1 Introduction Minimally invasive surgical techniques are revolutionizing surgery. These techniques, which involve inserting surgical instruments into the body cavity without making large incisions, have several advantages over more traditional surgical methods, including more rapid patient rec [...] ...|$|R
40|$|Abstract- Due to the {{increasing}} complexity of scientific models, large-scale simulation tools often require a critical amount of computational power to produce results in {{a reasonable amount of}} time. For example, multi-system wireless network simulations involve complex algorithms of traffic balancing and communication control on large geographical areas. Moreover many of these intensive applications are designed for single sequential machines and large sums of money are spent on purchasing powerful servers that can give results in a satisfactory amount of time. The aim {{of this paper is to}} introduce a general-purpose tool, dubbed Transparent Remote Execution (TREx), which avoids resorting to expensive servers by providing a cost effective, high performance, distributed solution. TREx is a daemon that dynamically exploits idle operational in-use workstations. Based on elaborate rules of computational resource management, this daemon permits a <b>master</b> to scan <b>workstations</b> within a predefined subnetwork and share the workload among the least occupied processing elements. It also provides a clear framework for parallelization that applications can exploit. By providing a simple way of federating computational resources, such a framework could drastically reduce hardware investments. 1...|$|R
40|$|Due to the {{character}} of the original source materials and the nature of batch digitization, quality control issues may be present in this document. Please report any quality issues you encounter to digital@library. tamu. edu, referencing the URI of the item. Includes bibliographical references. The computational requirements of science and engineering demand computational resources orders of magnitude of the current day sequential machines. Most of the research effort has been concentrated upon the creation of parallel algorithms and parallel axchitectures, but not enough research has been done on the software environment and parallel program portability. Unfortunately, no architecture can map all programs optimally and most of the programs perform poorly. Heterogeneous network computing offers a viable and cost-efficient alternative to actually constructing a heterogeneous element machine. We attempt to extract machine level parallelism in a given computational problem by selecting parts of a program that could be executed in parallel, and then selecting a machine that will be the most appropriate to execute that part of the program. In effect we could have n machines executing the same program but simultaneously working on a particular part that maps the best on that machine. Alternatively, different programs could execute on the three machines assuming there are minimal conflicts, that is, the jobs are well balanced. In this project we have developed a network based heterogeneous program execution environment. We adopted a master-slave approach to free the user from the rigors of network programming. At the heart of the system is a the <b>master,</b> a <b>workstation,</b> which controls the scheduling and execution of jobs on the different slaves. We selected three different machines each of them a supercomputer in its own different machines each of them a supercomputer in its own right, each of them belonging to a different classification of parallel machines. The machines we selected were, a hypercube(MIMD), an array processor(SIMD), and a vector processor(MISD). An existing physical problem was implemented to benchmark this approach...|$|R
40|$|Progress {{has been}} made {{in each of the three}} project areas during this quarter. Each quarter we are {{highlighting}} one project area. This quarter, Task 2 is highlighted with expanded details. Significant progress {{has been made}} this quarter in testing the functionalities of the foam-durability apparatus for assessment of foam properties at reservoir conditions. Another surfactant, Alipal{reg_sign} CD- 128 at a concentration of 1000 ppm, was used for core flooding experiments. The foam mobility data showed a significant reduction of CO{sub 2 } mobility and a favorable mobility dependence on rock permeability. Two slim tube test series and continuous phase equilibrium were done to examine the effects of pressure, temperature, and oil composition on oil displacement efficiency. A new series of core foam tests were completed to study the effects of flow rate, CO{sub 2 } fraction (foam) quality, and rock permeability on foam-flow behavior. We are in the process of moving the foam reservoir simulator <b>MASTER</b> from a <b>workstation</b> to a Pentium PC environment and test MASTER on a 166 MHz Pentium PC. IFT of CO{sub 2 }/crude oil has been measured using our pendant drop measurement system at 138 {degrees}F and pressures from 850 psig to 2200 psig. The CO{sub 2 } gravity drainage experiment that is in progress using a 50 md Berea core at 138 {degrees}F and pressures from 1700 to 2000 psig has reached 48 % oil recovery and is continuing to increase. The mathematical model developed previously matches the experimental response accurately...|$|R

