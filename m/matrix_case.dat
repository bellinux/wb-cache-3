228|1262|Public
25|$|But, in the <b>matrix</b> <b>case,</b> (M* M)½ is {{a normal}} matrix, so ||M* M||½ is the largest {{eigenvalue}} of (M* M)½, i.e. the largest singular value of M.|$|E
2500|$|... where [...] is an eigenstate of [...] and [...] {{represents}} the eigenvalue. [...] is an observable self adjoint operator, the infinite-dimensional analog of Hermitian matrices. As in the <b>matrix</b> <b>case,</b> {{in the equation}} above [...] is understood to be the vector obtained by application of the transformation [...] to [...]|$|E
2500|$|When [...] is not {{a matrix}} Lie group, [...] is still given by its power series , while the other two occurrences of [...] in the formula, which now are the {{exponential}} map in Lie theory, refer to the time-one flow of the left invariant vector field , i.e. element of the Lie algebra as defined in the general case, on the Lie group [...] viewed as an analytic manifold. This still amounts to exactly the same formula as in the <b>matrix</b> <b>case.</b>|$|E
40|$|Phillips (J. Multivariate Anal. 16 (1985) 157) generalizes Cramer's (Mathematical Methods of Statistics, Princeton University Press, Princeton, NJ, 1946) {{inversion}} {{formula for}} {{the distribution of}} a quotient of two scalar random variables to the <b>matrix</b> quotient <b>case.</b> However, he gives the result for the asymmetric <b>matrix</b> quotient <b>case.</b> This note extends Phillips' (1985) result to the symmetric <b>matrix</b> quotient <b>case.</b> <b>Matrix</b> variate Positive definite Density Transformation Moment generating function Inversion formula...|$|R
40|$|AbstractPhillips (J. Multivariate Anal. 16 (1985) 157) generalizes Cramer's (Mathematical Methods of Statistics, Princeton University Press, Princeton, NJ, 1946) {{inversion}} {{formula for}} {{the distribution of}} a quotient of two scalar random variables to the <b>matrix</b> quotient <b>case.</b> However, he gives the result for the asymmetric <b>matrix</b> quotient <b>case.</b> This note extends Phillips’ (1985) result to the symmetric <b>matrix</b> quotient <b>case...</b>|$|R
3000|$|Regarding the notation, boldface letters {{refer to}} vectors (lower <b>case)</b> or <b>matrices</b> (upper <b>case).</b> Notation [...]...|$|R
50|$|This article first {{summarizes}} the corresponding {{results from the}} <b>matrix</b> <b>case</b> before discussing the spectral properties of compact operators. The reader will see that most statements transfer verbatim from the <b>matrix</b> <b>case.</b>|$|E
5000|$|... where {{multiplicity}} {{is taken}} into account as in the <b>matrix</b> <b>case.</b> When H is infinite-dimensional, the above sequence of eigenvalues is necessarily infinite. We now apply the same reasoning as in the <b>matrix</b> <b>case.</b> Letting Sk ⊂ H be a k dimensional subspace, we can obtain the following theorem.|$|E
5000|$|... v) As in the <b>matrix</b> <b>case,</b> this is {{a direct}} {{application}} of the holomorphic functional calculus.|$|E
40|$|In {{this paper}} two new {{numerical}} algorithms {{based on the}} Fast Fourier Transform techniques (FFT) are used to solve the structured robustness analysis problem {{in the case of}} one parameter entering polynomially. Both scalar and <b>matrix</b> <b>cases</b> are considered. The employed algorithms are namely numerical routines for the computation of one- and two-dimensional polynomial matrix determinants, based on the one- and two-dimensional FFT's respectively...|$|R
3000|$|Notation: As usual, bold letters denote vectors and <b>matrices,</b> upper <b>case</b> denotes random variables, {{and lower}} case denotes realizations. Σ [...]...|$|R
3000|$|The {{notation}} of {{this article}} is as follows: all boldface letters indicate vectors (lower <b>case)</b> or <b>matrices</b> (upper <b>case).</b> A [...]...|$|R
50|$|In 1887, Tolbert Lanston {{invented the}} Monotype {{mechanical}} typesetting machine. This was a type casting system that produced individual characters, {{in which a}} <b>matrix</b> <b>case</b> is used for holding all the font's matrices. In a manner somewhat reminiscent to hand type casting, every time a character is to be cast, the selection mechanism would position the <b>matrix</b> <b>case</b> so that the correct matrix is over the mould, hot metal would be injected, the sort removed and the process repeated until the job is finished.|$|E
50|$|But, in the <b>matrix</b> <b>case,</b> (M* M)½ is {{a normal}} matrix, so ||M* M||½ is the largest {{eigenvalue}} of (M* M)½, i.e. the largest singular value of M.|$|E
5000|$|From the {{transposed}} <b>matrix</b> <b>case</b> of the Gershgorin circle theorem {{it follows}} that all eigenvalues of A, that is, all roots of &fnof;(X), are contained in the union of the disks [...] with a radius [...]|$|E
3000|$|Throughout the paper, boldface {{characters}} {{are used for}} <b>matrices</b> (upper <b>case)</b> and vectors (lower case). Superscript (·)∗ denotes conjugate transposition. I [...]...|$|R
40|$|AbstractA {{procedure}} is proposed for numerical inversion of Laplace transforms x(s) implicitly defined from the functional equation x(s) = g(a(s) − Cx(s)) where a(·) and g(·) are known functions, C is a known constant. This equation is encountered in queueing, inventory, and insurance problems. The procedure constructs coefficients of the Laguerre {{series for the}} original in the Laguerre series inversion method and a sequence of approximants in the Post-Widder inversion method. Scalar and <b>matrix</b> <b>cases</b> are treated in the same fashion. The numerical results are compared with those attainable with the Fourier series method...|$|R
40|$|In {{this paper}} we develop and apply methods for the {{spectral}} analysis of non-self-adjoint tridiagonal infinite and finite random matrices, {{and for the}} spectral analysis of analogous deterministic matrices which are pseudo-ergodic {{in the sense of}} E. B. Davies (Commun. Math. Phys. 216 (2001), 687 - 704). As a major application to illustrate our methods we focus on the "hopping sign model" introduced by J. Feinberg and A. Zee (Phys. Rev. E 59 (1999), 6433 - 6443), in which the main objects of study are random tridiagonal matrices which have zeros on the main diagonal and random ± 1 's as the other entries. We explore the relationship between spectral sets in the finite and infinite <b>matrix</b> <b>cases,</b> and between the semi-infinite and bi-infinite <b>matrix</b> <b>cases,</b> for example showing that the numerical range and p-norm -pseudospectra (> 0, p∈ [1,∞]) of the random finite matrices converge almost surely to their infinite matrix counterparts, and that the finite matrix spectra are contained in the infinite matrix spectrum Σ. We also propose a sequence of inclusion sets for Σ which we show is convergent to Σ, with the nth element of the sequence computable by calculating smallest singular values of (large numbers of) n× n matrices. We propose similar convergent approximations for the 2 -norm -pseudospectra of the infinite random matrices, these approximations sandwiching the infinite matrix pseudospectra from above and below...|$|R
50|$|A {{generalization}} to the <b>matrix</b> <b>case</b> (matrices with {{polynomial function}} entries {{that are always}} positive semidefinite can be expressed as sum of squares of symmetric matrices with rational function entries) was given by Gondard, Ribenboim and Procesi, Schacher, with an elementary proof given by Hillar and Nie.|$|E
5000|$|As in the <b>matrix</b> <b>case,</b> {{the above}} {{spectral}} properties {{lead to a}} decomposition of X into invariant subspaces of a compact operator C. Let λ ≠ 0 be an eigenvalue of C; so λ is an isolated point of σ(C). Using the holomorphic functional calculus, define the Riesz projection E(λ) by ...|$|E
5000|$|... where [...] is an eigenstate of [...] and [...] {{represents}} the eigenvalue. [...] is an observable self adjoint operator, the infinite-dimensional analog of Hermitian matrices. As in the <b>matrix</b> <b>case,</b> {{in the equation}} above [...] is understood to be the vector obtained by application of the transformation [...] to [...]|$|E
3000|$|Notations {{adopted in}} this paper are mostly standard. Uppercase and bold letters denote <b>matrices,</b> lower <b>case</b> and bold letters denote vectors, and lower case and non-bold letters denote scalars. Superscript (·) [...]...|$|R
40|$|Over {{the past}} decade, the {{consensus}} of multiagent systems has received an increasing attention in various fields, such as mathematics, physics, biology, and engineering sciences. There arc numerous results reported on the comsensus of multiagent systems. Now {{it is necessary to}} review the recent advances in consensus of multiagent systems. This paper firstly reviews the main mathematical models of multiagcnt systems, including Ooids model, Vicsek model, and Couzin-Levin model. Moreover, this paper reviews the main advances in {{the consensus of}} multiagent systems, including the linear local updating rules, nonlinear local updating rules, and leader and asymmetric <b>matrix</b> <b>cases...</b>|$|R
40|$|Matrix valued inner {{functions}} on the bidisk have {{a number}} of natural subspaces of the Hardy space on the torus associated to them. We study their relationship to Agler decompositions, regularity up to the boundary, and restriction maps into one variable spaces. We give a complete description of the important spaces associated to matrix rational inner functions. The dimension of some of these spaces can be computed in a straightforward way, and this ends up having an application to the study of three variable rational inner functions. Examples are included to highlight the differences between the scalar and <b>matrix</b> <b>cases.</b> Comment: 40 page...|$|R
50|$|The {{concept of}} normal {{matrices}} {{can be extended}} to normal operators on infinite dimensional Hilbert spaces and to normal elements in C*-algebras. As in the <b>matrix</b> <b>case,</b> normality means commutativity is preserved, to the extent possible, in the noncommutative setting. This makes normal operators, and normal elements of C*-algebras, more amenable to analysis.|$|E
50|$|In the end, while logged {{into the}} <b>matrix,</b> <b>Case</b> catches {{a glimpse of}} himself, his dead girlfriend Linda Lee, and Neuromancer. The {{implication}} of the sighting is that Neuromancer created a copy of Case's consciousness when it previously tried to trap him. The copy of Case's consciousness now exists with that of Linda's, in the matrix, where they are together forever.|$|E
50|$|The {{row of the}} matrix-case {{on which}} the matrix for a {{character}} is contained also indicated the width of that character. This was one major reason why reconfiguring the keyboard needed to be easy, since the arrangement of characters in the <b>matrix</b> <b>case</b> would varied with the typeface. A part called the wedge indicated the width corresponding to each row of the matrix-case.|$|E
40|$|Engineers {{continue}} {{to turn to}} Engineering Design to learn the tools and techniques of formal design that will be useful in framing the design problems. Insights and tips on team dynamics are provided because design and research is increasingly done in teams. Readers are also introduced to conceptual design tools like objectives trees, morphological charts, and requirement <b>matrices.</b> <b>Case</b> studies are included that show the relevance of these tools to practical settings. The third edition offers {{a view of the}} design tools that even the greenest of engineers will have in their toolbox in the coming years. [URL]...|$|R
40|$|In this note, we give a {{generalization}} of some {{extension of the}} Cayley-Hamilton theorem {{in the case of}} a pair of n × n commuting <b>matrices</b> to the <b>case</b> of a pair of n×n non-commuting matrices. The classical Cayley-Hamilton theorem and its extension in the case of pairs of commuting <b>matrices,</b> are special <b>cases</b> of the proposed generalization...|$|R
5000|$|Elementary <b>matrix,</b> {{a special}} <b>case</b> of a Frobenius matrix {{with only one}} off-diagonal nonzero ...|$|R
5000|$|In the <b>matrix</b> <b>case,</b> {{eigenvalues}} [...] may {{be found}} by setting the determinant of the matrix equal to zero, i.e. finding where the matrix has no inverse. Finite matrices have only {{a finite number of}} eigenvalues/eigenvectors, whereas linear operators can have a countably infinite number of eigenvalues/eigenfunctions (in confined regions) or uncountably infinite (continuous) spectra of solutions, as in unbounded regions.|$|E
5000|$|However, these {{solutions}} are neither the most concise ones (e.g. still remains {{the need to}} notationally differentiate overdetermined systems) nor the most computationally efficient. The latter point {{is easy to understand}} when considering again the scalar equivalent , for which the solution [...] would require two operations instead of the more efficient [...]The problem is that generally matrix multiplications are not commutative as the extension of the scalar solution to the <b>matrix</b> <b>case</b> would require: ...|$|E
5000|$|When [...] is not {{a matrix}} Lie group, [...] is still given by its power series , while the other two occurrences of [...] in the formula, which now are the {{exponential}} map in Lie theory, refer to the time-one flow of the left invariant vector field , i.e. element of the Lie algebra as defined in the general case, on the Lie group [...] viewed as an analytic manifold. This still amounts to exactly the same formula as in the <b>matrix</b> <b>case.</b>|$|E
3000|$|We use boldface {{notation}} {{to denote}} random processes, normal face for deterministic parameters, upper <b>case</b> letters for <b>matrices,</b> lower <b>case</b> letters for vectors and scalars, and subscripts for time indices. The observed signal is denoted as [...]...|$|R
50|$|The eigenspinors are eigenvectors of the Pauli <b>matrices</b> in the <b>case</b> of {{a single}} spin 1/2 particle.|$|R
5000|$|For example, {{balanced}} matrices arise as {{the coefficient}} <b>matrix</b> in special <b>cases</b> {{of the set}} partitioning problem.|$|R
