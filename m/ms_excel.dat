1178|18|Public
25|$|An AnyLogic {{model can}} be {{exported}} as a Java application, that can be run separately, or integrated with other software. As an option, an exported AnyLogic {{model can be}} built into other pieces of software and work as an additional module to ERP, MRP, and TMS systems. Another typical use is integration of an AnyLogic model with TXT, <b>MS</b> <b>Excel,</b> or MS Access files and databases (MS SQL, My SQL, Oracle, etc.). Also, Anylogic models include their own databases based on HSQLDB.|$|E
2500|$|... (<b>MS</b> <b>Excel</b> format), Philippine Overseas Employment Administration, 2005, on OFWs: ...|$|E
2500|$|For {{negative}} values of x, the terms integral part or integer part of x are sometimes instead {{taken to be}} {{the value of the}} ceiling function, i.e., the value of x rounded to an integer towards 0. The language APL uses ⌊x; other computer languages commonly use notations like entier(x) (ALGOL), INT(x) (BASIC, <b>MS</b> <b>Excel),</b> or floor(x)(C, C++, R, and Python). In mathematics, it can also be written with boldface or double brackets [...]|$|E
5000|$|Verschuuren, Gerard <b>M.</b> (2013). <b>Excel</b> 2013 for Scientists and Engineers. Holy Macro! Books, Uniontown, OH ...|$|R
5000|$|Verschuuren, Gerard <b>M.</b> (2017). 100 <b>Excel</b> VBA Simulations. Createspace, Charlestown, SC ...|$|R
5000|$|Verschuuren, Gerard <b>M.</b> (2016). 100 <b>Excel</b> Simulations. Holy Macro! Books, Merrit Island, FL ...|$|R
50|$|Erich Neuwirth {{developed}} the statconn Server and RExcel. with Thomas Baier. The statconn server combines R and Scilab with <b>MS</b> <b>Excel</b> or Open Office. RExcel is an add-in for Microsoft Excel, {{which allows the}} use of R within <b>MS</b> <b>Excel.</b>|$|E
5000|$|Exports fields into {{text files}} (CSV and tab delimited text) and <b>MS</b> <b>Excel.</b>|$|E
50|$|May 2009 {{publication}} of an accounting spreadsheet solution, called 'Simple Accounting', based on <b>MS</b> <b>Excel.</b>|$|E
5000|$|In {{spite of}} his {{relatively}} small stature, 1.73 <b>m,</b> he <b>excelled</b> in the air, due to his heading accuracy and elevation. Renowned for his bending shots, {{he was also an}} accurate free-kick taker, and penalty taker, although he often refrained from taking penalties, stating that he believed it to be a cowardly way to score.|$|R
40|$|Copyright © 2014 ISSR Journals. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. ABSTRACT: This is about relating the thermal induction time range from 0 - 40 minutes for Lard of definite composition with {{the change in the}} thermal oxidation by the models developed by using <b>M</b> S <b>Excel</b> and Statistical Software, Design Expert Software 8. 0 with there R 2. And through analysis of prepared model data with their plotted graph...|$|R
50|$|Campbell {{attended}} Barton County Community College in Great Bend, Kansas, {{where she}} set several records and won many titles, including four national junior college {{titles in the}} 60, 100 and 200 metres both indoors and outdoors. She holds the current record for Barton County CC in the outdoor 100 m and 200 <b>m.</b> Campbell also <b>excelled</b> academically, earning an associate degree from Barton County in 2002 with a 3.8 grade average. She later attended the University of Arkansas, where she stood out as a sprint star in a programme dominated by long-distance runners.|$|R
5000|$|MS Word and <b>MS</b> <b>Excel</b> - Their macro {{languages}} {{used to be}} localized in non-English languages.|$|E
5000|$|Retrieved {{data can}} be {{exported}} to the <b>MS</b> <b>EXCEL</b> program or to a simple text file for further use.|$|E
5000|$|Content Files: HTML (and all {{derivatives}} PHP, ASP, JSP), XHTML, XML (including derivative ASP.NET, ASP, JSP and XSL), <b>MS</b> <b>Excel,</b> DITA 1.0 ...|$|E
40|$|AbstractCalculating {{thermal load}} of {{building}} {{is important to}} find exact A/C equipment and air handling unit, to achieve comfort operation and good air distribution in the air-conditioned zone. It should be {{taking into account the}} highest temperature in summer and lower of winters that occurs in the region. Building's location, construction's materials other interior's load must be considered for estimation of accurate thermal load. All equations needed for heat transfer through the building and for the interior's load are used to get the thermal load. Then, all equations can be inserted in a personal program like <b>M.</b> S. <b>Excel,</b> to get the results. An alternative way is to use available software, like HAP 4. 2 and Elit “RHVAC”. In this paper hand calculation is used to estimate the thermal load for existing building and the results were compared by the outcomes from HAP 4. 2 program. It is shown that there is little difference between the two results due to defining the thermal resistance for the used materials of the wall, roof, and windows. Also the sources of constants used for the equations are not the same...|$|R
40|$|Quantitative structure-property {{relationship}} (QSPR) {{is performed}} {{as a means}} to predict octane number of hydrocarbons via correlating properties to parameters calculated from molecular structure; such parameters are molecular mass M, hydration energy EH, boiling point BP, octanol/water distribution coefficient logP, molar refractivity MR, critical pressure CP, critical volume CV, and critical temperature CT. Principal component analysis (PCA) and multiple linear regression technique (MLR) were performed {{to examine the relationship between}} multiple variables of the above parameters and the octane number of hydrocarbons. The results of PCA explain the interrelationships between octane number and different variables. Correlation coefficients were calculated using <b>M.</b> S. <b>Excel</b> to examine the relationship between multiple variables of the above parameters and the octane number of hydrocarbons. The data set was split into training of 40 hydrocarbons and validation set of 25 hydrocarbons. The linear relationship between the selected descriptors and the octane number has coefficient of determination (R 2 = 0. 932), statistical significance (F= 53. 21), and standard errors (s = 7. 7). The obtained QSPR model was applied on the validation set of octane number for hydrocarbons giving RCV 2 = 0. 942 and s= 6. 328...|$|R
40|$|Robin Baker {{was born}} to parents Doug and Patsy Baker in Phoenix, AZ, where {{he spent most of}} his youth. He {{graduated}} from Grand Canyon University in 1980 with honors in history and political science. His love of history led him to puruse a masters in history from Hardin-Simmons University and a doctorate in history from Texas A 2 ̆ 6 <b>M.</b> He <b>excelled</b> in academics at all of these institutions, getting high honors. In 1999 Robin Baker came to George Fox University as the Provost. During his eight years as provost he oversaw the addition of 13 new undergraduate and 9 graduate programs. He is passionate about the globalization of education and helped institute the Act Six scholarship at George Fox. In addition to his work as provost Robin Baker has taught classes in history at the University. His research primarily focuses on American Civil War and Reconstruction, 19 th-century American political/ quantitative history, and the history of the southern United States. He also takes a great interest in the works of C. S. Lewis and J. R. R. Tolkien. Robin Baker was sworn in as the twelfth President of George Fox University on March 6, 2008. [URL]...|$|R
5000|$|Basic Computer Operations, [...] MS Word, <b>MS</b> <b>Excel,</b> [...] MS Powerpoint, [...] Adobe Photoshop, [...] AutoCAD 2D & AutoCAD 3D ( [...] 20 hours each) ...|$|E
5000|$|OLE: It can {{manipulate}} <b>MS</b> <b>Excel</b> and Word OLE document formats. It enables {{the possibility to}} create, read and modify documents. It {{is based on the}} POI library.|$|E
50|$|The {{generated}} reports can be {{exported to}} MS Word, <b>MS</b> <b>Excel,</b> PDF or printed. Complex SQL statements {{can be used}} (sub-select, joins and even stored procedures). ScriptCase allows users to write PHP to handle exceptions and create more complex validation).|$|E
40|$|This issue contains: Alumni News: Vote for our Executive Board Members, CVM Grad is America's Favorite Veterinarian Finalist Lashonn McNair), Alumni Association Funds Conservation Community Service, Reunion Highlights, AAEB Report Update, Grad Honored with National Teaching Award (Etienne Cote), A Book on Better Horse Training (Allen <b>M.</b> Schoen), Alumna <b>Excels</b> in Exotics and Outreach (Michelle Pesce), Connect with Colleagues on LinkedIn; Development News: Alumnus Gives to Scholarship Fund, Clinic Memorial Giving; College News: Dean Michael I. Kotlikoff Named Provost of Cornell University, Interim Dean Interim CUHA Director Appointed, One Health for One Planet Many Species Presentation, CVM and JKF Airport ARK, Dr. Thomas Divers Named by ACVIM; Research: Biobanking the Big Cats, Cornell's Collaborative Lymphoma Research, Cornell Develops Test for Canine Flu; Student News: Benefits of Case-Based Learning, CVM Students Wins Prestigious Cook Award; Events...|$|R
40|$|Introduction: Childhood cancers {{are very}} rare {{diseases}} and accounting for about {{one percent of}} all cancers, also {{it is one of}} the main causes of death among children. The aim of this paper was to ascertain of childhood cancers epidemiology in Fars province. Materials and Methods: In this epidemiological study that Fars province cancer registry was used, frequency distribution of childhood cancers in less than 19 year old in 2001 up to 2008 was evaluated and incidence rates were calculated per 1000, 000 people a year. Data were analyzed by running SPSS software, version 16 (SPSS Inc., Chicago, IL) and <b>M.</b> S. <b>Excel</b> version 2007. Results: Out of 1610 registered new cancer cases that were resident of Fars province, blood cell cancers were most common type of cancers. 57 % of cases were male and 15 - 18 year old age group in comparison to other groups has been the most frequent (30. 7 %). The mean age of cases at diagnose time was 10. 3 years old. Over in eight years period, lowest and highest age standardized incidence rate was 64 (year 2001) and 235 (year 2006) cases per every one million person, respectively. Conclusion: Based on this paper’s results, and despite of our prospect, childhood cancer incidence rate in less than 19 year age in Fars province was similar to developed countries...|$|R
40|$|Efst á síðunni er hægt að nálgast greinina í heild sinni með því að smella á hlekkinnÁ Íslandi hafa hjúkrunarfræðingar ekki leyfi til að gefa lyf án ávísunar frá lækni en lyfjagjöf hjúkrunarfræðinga án milligöngu læknis þekkist þó í bráðaþjónustu hérlendis. Tilgangur rannsóknarinnar var að varpa ljósi á umfang og eðli stakra lyfjagjafa hjúkrunarfræðinga án skriflegra fyrirmæla lækna á Landspítala. Um lýsandi afturvirka megindlega rannsókn var að ræða. Í þýðinu voru allar ávísaðar og skráðar lyfjagjafir á Landspítala í rafræna lyfjaumsýslukerfinu Therapy sem voru 1. 586. 684 árið 2010 og 1. 633. 643 árið 2011. Í úrtakinu voru stakar lyfjagjafir hjúkrunarfræðinga á Landspítala án skriflegra fyrirmæla lækna, sem voru skráðar í rafræna lyfjaumsýslukerfið Therapy undir heitinu umbeðið af hjúkrun árin 2010 og 2011, samtals 24 mánaða tímabil. Gögn voru fengin frá heilbrigðis- og upplýsingatæknisviði Landspítala að fengnum viðeigandi leyfum. Við úrvinnslu voru gögnin lesin inn í Microsoft <b>Excel</b> <b>með</b> Power Pivot viðbót. Árið 2010 voru stakar lyfjagjafir 4...|$|R
50|$|Truncated graphs {{are useful}} in {{illustrating}} small differences. Graphs may also be truncated to save space. Commercial software such as <b>MS</b> <b>Excel</b> will tend to truncate graphs by default if the values are all within a narrow range, as in this example.|$|E
5000|$|Sample {{code for}} reading/writing {{workbook}} attributes, setting password, and saving <b>MS</b> <b>Excel</b> 2003 format, {{might look like}} as follows:import com.jniwrapper.win32.jexcel.Application;import com.jniwrapper.win32.jexcel.FileFormat;import com.jniwrapper.win32.jexcel.GenericWorkbook;import com.jniwrapper.win32.jexcel.Workbook;import java.io.File;/** * This sample shows how to read/modify workbook attributes, how to save workbook in Excel 2003 format, * and how to reopen workbook. * * The sample works with <b>MS</b> <b>Excel</b> in non-embedded mode. */public class WorkbookSample{ public static void main(String args) throws Exception { //Start <b>MS</b> <b>Excel</b> application, crate workbook and make it visible. // Application starts invisible and without any workbooks Application application = new Application (...) Workbook workbook = application.createWorkbook("Custom title"); printWorkbookAttributes(workbook); modifyWorkbookAttributes(workbook); File newFile = new File("Workbook.xls"); //Save workbook in Excel 2003, to save in Excel 2007 format use FileFormat.OPENXMLWORKBOOK // format specificator and *.xlsx extension workbook.saveAs(newFile, FileFormat.WORKBOOKNORMAL, true); File workbookCopy = new File("WorkbookCopy.xls"); workbook.saveCopyAs(workbookCopy); //Close workbook saving changes workbook.close(true); //Reopening the workbook workbook = application.openWorkbook(newFile, true, [...] "xxx001"); printWorkbookAttributes(workbook); //Perform cleanup after yourself and close the <b>MS</b> <b>Excel</b> application forcing it to quit application.close(true); } /** * Prints workbook attributes to console * @param workbook - workbook to print information about */ public static void printWorkbookAttributes(GenericWorkbook workbook) { String fileName = workbook.getFile (...) [...]getAbsolutePath (...) String name = workbook.getWorkbookName (...) String title = workbook.getTitle (...) String author = workbook.getAuthor (...) System.out.println("\nInformation"); System.out.println("File path: [...] " [...] + fileName); System.out.println("Name: [...] " [...] + name); System.out.println("Title: [...] " [...] + title); System.out.println("Author: [...] " [...] + author); if (workbook.hasPassword (...) [...] ) { System.out.println("The workbook is protected with a password"); } else { System.out.println("The workbook is not protected with a password"); } if (workbook.isReadOnly (...) [...] ) { System.out.println("Read only mode"); } } /** * Modify workbook title, author and set password * @param workbook - workbook to modify attributes */ public static void modifyWorkbookAttributes(GenericWorkbook workbook) { workbook.setTitle("X-files"); workbook.setPassword("xxx001"); workbook.setAuthor("Agent Smith"); }} ...|$|E
50|$|Event {{studies can}} be {{implemented}} with various different tools. Single event studies can easily be implemented with <b>MS</b> <b>Excel,</b> event studies covering multiple events need to be built using statistical software packages (e.g., STATA, Matlab). Besides of these multi-use tools, there are solutions tailored to conducting event study analyses (e.g., Eventus, Event Study Metrics, EventStudyTools).|$|E
40|$|Social {{media is}} the online tools that people use to share content, profiles, opinions, insights, experiences, {{perspectives}} and media itself, thus facilitating conversations and interaction online between groups of people. The role of Social {{media in the}} present times is an interesting and captivating to contemplate given {{the manner in which}} it has changed the way we live, interact but most importantly influenced organizational culture. Project management has not been left behind, being a relatively new area of research albeit growing immensely. The Kenya’s vision 2030 will be achieved through successful implementation of projects. And this success will depend on how well these projects will be implemented to enhance success rate. The general objective was to establish how social media could be utilized to facilitate project implementation. Specifically, it assessed the role of social media as a platform for collaborative intelligence and gave recommendations on incorporating social media in the implementation of all projects under vision 2030. The target population were project managers and team leaders involved in implementation of projects earmarked under vision 2030. A descriptive study design was used to collect quantitative and qualitative data. Data collected through the questionnaire was analyzed through the use of Statistical Package for Social Sciences version 20 and <b>Ms</b> 2007 <b>excel.</b> This study revealed that the vision 2030 project official are embracing social media for work related purposes. Organizations running projects can use the power of social media to foster the culture of learning from each other. If project officials provide a platform and a framework around social computing tools for their people to meaningfully interact and collectively learn, this will become an extremely useful exercise for stakeholders and will contribute immensely to successful project implementation...|$|R
40|$|Purpose. This {{research}} study on automobile collision in Virginia amongst fifteen to nineteen (15 - 19) year olds {{looked into the}} trend analysis over a five (5) year period of 2000 to 2004. Trend analysis is usually done for aggregates of all injurieseither intentional or unintentional injuries, or both. The primary objective of this {{research study}} was to examine the trend in hospitalization rates and mortality rates for males and females independently. It further looked into the trend, if any, in hospitalization rates, mortality rates, and case-fatality rates, for both males and females combined. The different Tables illustrate the extent and the impact of automobile collision in terms of demographics and characteristics of hospitalizations, types of hospitalizations, hospitalization rates, mortality rates and case-fatality rates among this age group. Methods: An investigation was carried out in a case control manner of 2353 cases using data from the Virginia Department of Health-Division of Injury Prevention 2 ̆ 6 Violence on automobile collision amongst 15 - 19 year olds, from 2000 to 2004, a (5) five year period. Hospitalization data were obtained from Virginia Health Information, coded in line with International Classification of Diseases, 9 th revision (ICD- 9); external cause of injury (E) -codes. Mortalityldeath rates and case fatality rates were calculated using U. S. Census Bureau, Census 2000 for Virginia 2 ̆ 7 s population data. Frequency distribution analysis was done with SPSS 14. 0, data entry using <b>M.</b> S. <b>Excel,</b> while rate ratio and confidence intervals for hospitalization rates, mortality rates were calculated. Linear trend was analyzed for hospitalization rates, mortality rates and case-fatality rates, using Chi square statistics test for significance. Geographical Information System (GIs) methods were used to display counties in Virginia. Results: Out of 2353 cases of automobile collision in Virginia, amongst 15 - 19 year olds, from 2000 to 2004, the demographic did not changed much. Males were fairly distributed over the five year period, while automobile collision characteristics showed that 2142 cases (91...|$|R
40|$|This study, {{describes}} {{the operation of}} Roseires and Sennar dams {{during the dry season}} when the demand is greater than the available water at El Deim station and tries to find out the relationship between water levels, amount of evaporation losses, predicted inflow discharges at El Deim station during the recession period and reservoirs contents of Roseires & Sennar dams. Artificial neural network has been used to establish three models: Model - 1 : To predict inflow discharges at El Deim station. Model - 2 : To predict reservoir water level knowing reservoir content and year of application. Model - 3 : To find out the numeric relationship between reservoir water level & Evaporation losses. Artificial Neural network using NeuroShell 2 software, which provides a quick and flexible means of creating models for prediction, has been shown to perform well in comparison with conventional methods. Different scenarios of different network structures have been tried, the best architecture for the three models is the three connected layer with specified hidden neurons, but the hidden layer in the first two models consist of two slabs. The results obtained from model- 1, model - 2 and model - 3 are used in a computer program to establish the emptying program of Roseires & Sennar reservoirs using <b>M.</b> S <b>Excel</b> software. By assuming different areas of Wheat crop - its planting season start on 2 nd period of October - the emptying of the two reservoirs has been run taking into considerations the following aspects: i. At the beginning of the empting program, the two reservoirs of Sennar & Roseires dams are at their predetermined maximum water levels. ii. Irrigation demands include projects upstream Sennar dam, Gezira & Managil schemes and minimum downstream requirements. iii. At the end of emptying program, the water level of the two reservoirs should not drop below the minimum water level of both reservoirs respectively. Different abstraction ratios for Sennar & Roseires have been tried to satisfy the above mentioned considerations. The best ratio found is 1 (Sennar) : 4. 4 (Roseires) ...|$|R
50|$|Since 2004/2005 the LSG {{has used}} a system utilising PHP and MySQL {{in order for}} {{reservation}} of the ICT rooms via the internet. This system replaces the previous one of <b>MS</b> <b>Excel</b> tables, and was developed by students of the LSG themselves, {{on the basis of}} the GPL. This system is also used by the Käthe-Kollwitz-Gymnasium.|$|E
50|$|Because {{professionals}} in these industries manage {{large amounts of}} data in spreadsheets and because one change to a value or formula could affect {{a substantial amount of}} data, {{professionals in}} the banking, finance, and accounting industries find document comparison (such as comparison of two versions of a <b>MS</b> <b>Excel</b> spreadsheet) to be extremely useful in assuring accuracy in document change management.|$|E
5000|$|Also {{note that}} if a {{character}} string in the SYLK file is to contain a semicolon ( [...] ;) then it should be prefixed with another semicolon so the string would appear as e.g.; [...] "WIDGET#04;;AXC1254". <b>MS</b> <b>Excel</b> will strip the first semicolon on import and the data element will appear as [...] "WIDGET#04;AXC1254". It appears that the semicolon acts as an escape character of sorts.|$|E
40|$|Macrophages are 2 ̆ 2 first {{responders}} 2 ̆ 2, {{innate immune system}} cells which quickly arrive to a site of infection and injury, consuming cell debris and foreign matter and recruiting other immune system cells to the area. While historically they have been thought to react uniformly to all challenges, the discovery of toll-like receptors has shown that macrophages actually {{work closely with the}} adaptive immune system in fine-tuning the immune response. Furthermore, it has recently been discovered that macrophages can become polarized to one of two subtypes-M 1 or M 2. M 1 macrophages are efficient producers of reactive oxygen species, nitrogen intermediates, and inflammatory cytokines. They are especially effective at mediating resistance against intracellular parasites and tumors. Arginine metabolism in M 1 macrophages is characterized by high levels of inducible nitric oxide synthetase (iNos), and this is used as a marker for polarization of macrophages to the M 1 phenotype. M 2 macrophages, by contrast, produce anti-inflammatory molecules, have high levels of scavenger, mannose, and galactose-type receptors, and arginine metabolism is shifted to production of ornithine and polyamines via arginase. Arginase, encoded by the ARG 1 gene, is considered {{to be one of the}} hallmarks of the M 2 phenotype, and is one of the most specific markers used to determine polarization to that phenotype. Polarization to one phenotype or another is not permanent, and macrophages can be polarized directly from one state to the other directly by addition of appropriate cytokines (IFNy, LPS, TNFa for M 1, IL- 4, IL- 13, IL- 10, TGFb for M 2). The state of macrophage polarization can be determined by examining a population of macrophages for tell-tale products of one state or another (ROS, RNS, TNFa, IL- 1, IL- 6, IL- 12, or IL- 23 for M 1, IL- 10, TGFb, PDGF, VEGF, EGF, and arginase for M 2). Determining macrophage polarization has implications in health outcomes- <b>M</b> 1 macrophages <b>excel</b> at fighting parasites and fighting tumorous growth, while M 2 macrophages assist in wound healing and nerve re-growth. The problem is that the methods of detecting macrophage polarization-flow cytometry, western blots, or ELISA tests, are not real time and kill the cells involved. This paper describes the theory and methods behind creating a plasmid which combines the promoter for genes whose transcription indicates the M 1 or M 2 phenotype with a GFP or RFP, when transfected into a colony of RAW macrophages, will enable realtime, quantitative visualization of the production of inducible nitric oxide synthetase or arginase, markers for the M 1 and M 2 phenotype. A clearer understanding of macrophage polarity in the course of illness, wounding, and cancer might lead to diagnostic or therapeutic discoveries in those areas...|$|R
40|$| washing {{technique}} i. e., Batch leach {{tests and}} column leach tests were also explained. The laboratory assessment of immobilization efficiency through leaching test was explained briefly. The analytical and numerical solutions {{used for this}} study were discussed in detail. This chapter also includes a method of prediction of breakthrough curves from the incomplete column test data. The contaminant transport parameters of metal ion Copper in two locally available soils i. e., Black cotton soil and Red soil were determined by various techniques i. e., Analytical (using MATLAB v 7 software), semi-analytical (using POLLUTE v 7 software), Explicit Finite Difference Method with two software tools (MATLAB v 7 and <b>M.</b> S. <b>EXCEL</b> 2010), Implicit Finite Difference method with three schemes (BTCS, UPWIND & CRANK NICOLSON) using two software tools (MATLAB v 7 and <b>M.</b> S. <b>EXCEL</b> 2010). Modifications were done in the spreadsheet solution of non-reactive solute available from the literature to incorporate the retardation factor as the solutes used in this study are reactive in nature. These results are presented in Chapter 3. The contaminant transport parameters determined for different test conditions (constant and variable source concentrations) and for different densities of soil are reported in this chapter. Determination of transport rates corresponding to maximum dry density using trend lines and preparation of design charts to estimate the thickness of the liner are also discussed in this chapter. The contaminant transport parameters were also determined for metal ion Zinc in the same soils with the same techniques as that of Copper and the migration rates were compared for both the ions. These models and comparative results are presented Chapter 4. It was observed that with increase in density, the dispersion coefficient decreases and Distribution coefficient increases. It was also found that the dispersion coefficient of Black Cotton Soil was lower than that of Red Soil whereas the distribution coefficient of Black Cotton soil is much higher than that of Red Soil. Further, it was observed that the dispersion coefficient of Copper was less than that of Zinc whereas the distribution coefficient of Copper was higher than Zinc. The design of liner thickness, based on transport rates of Zinc is briefly discussed in this chapter. A case study has been explained for the remediation of Zinc contaminated sandy soil using soil washing technique. The undisturbed soil samples collected from four locations of waste disposal site of Hindustan Zinc Limited located near Udaipur in Rajasthan State of Western India were assessed to find the suitable leaching solution and number of pore volumes for the effective removal of Zinc from this soil. The chelates/ solvents used for this soil were 0. 1 N HCl, 0. 1 N EDTA, 0. 1 N HCl+ 0. 1 N EDTA and 0. 1 N FeCl 3. The contaminant transport parameters were also determined from the column leach tests based on the Leaching Mass Ratio approach and the results are presented in Chapter 5. From the experimental study it was observed that 0. 1 N FeCl 3 and 0. 1 N HCl+ 0. 1 N EDTA are the most suitable leaching solutions to treat this soil. The Chapter 6 contains the sludge analysis of an industrial ETP sludge, column leach test results of this sludge with different leaching solutions, removal efficiencies of different solutions used and the transport rates of different contaminants. The leaching solutions used for this sludge were distilled water, 0. 1 N HCl, 0. 1 N EDTA, 0. 1 N HCl+ 0. 1 N EDTA and 0. 1 N FeCl 3. It was observed that 0. 1 N FeCl 3 and 0. 1 N HCl+ 0. 1 N EDTA are the most suitable leaching solutions to treat this sludge. Other solutions have also removed the contaminants by more than 50 %, but the number of pore volumes required to leach out the contaminants was high. The order of removal efficiencies of different solutions is presented below: 0. 1 N FeCl 3 > 0. 1 N HCl + 0. 1 N EDTA > 0. 1 N EDTA > 0. 1 N HCl > distilled water. The transport rates of different contaminants (Cu, Zn, Cd, Fe, Ni, Pb and Cr) were determined using analytical solution and are presented in this chapter. These transport rates are useful to estimate the quantity of leaching solution required in the field to remediate the sludge using soil washing technique. A contaminated soil collected from an open dump site within Bangalore city and ETP Sludge were analyzed to know the efficiency of immobilization/ solidification technique of remediation using three chemical agents lime, NaOH and cement. The soil samples were mixed with different proportions of these chemicals to adjust the pH of the mixtures to 7. 0, 8. 5 and 10. 0. Leaching tests were conducted on the modified soils to know the long term efficiency of these chemical agents to immobilize the contaminants and these results are discussed in Chapter 7. The results showed that highest immobilization efficiencies can be achieved with lime for this contaminated soil and cement is the most suitable chemical agent to treat this sludge. The immobilization efficiencies of different stabilizing agents for various metals were studied and the results analyzed. The Chapter 8 includes the major observations and conclusions of the present research work which will be useful for Geotechnical and Geo-environmental engineers to estimate the transport rates of contaminants, to design the soil liners, to assess the efficiency of soil washing technique to remediate the contaminated soil, to estimate the quantity of leaching solution required in the field for soil washing and to find the suitable chemical agent for remediating the contaminated soil by immobilization technique...|$|R
40|$|Corresponding peer-reviewed {{publication}} This dataset corresponds {{to all the}} RAPID input and output files {{that were used in}} the study reported in: 	David, Cédric H., David R. Maidment, Guo-Yue Niu, Zong-Liang Yang, Florence Habets and Victor Eijkhout (2011), River Network Routing on the NHDPlus Dataset, Journal of Hydrometeorology, 12 (5), 913 - 934. DOI: 10. 1175 / 2011 JHM 1345. 1. When making use of any of the files in this dataset, please cite both the aforementioned article and the dataset herein. Time format The times reported in this description all follow the ISO 8601 format. For example 2000 - 01 - 01 T 16 : 00 - 06 : 00 represents 4 : 00 PM (16 : 00) on Jan 1 st 2000 (2000 - 01 - 01), Central Standard Time (- 06 : 00). Additionally, when time ranges with inner time steps are reported, the first time corresponds {{to the beginning of the}} first time step, and the second time corresponds to the end of the last time step. For example, the 3 -hourly time range from 2000 - 01 - 01 T 03 : 00 + 00 : 00 to 2000 - 01 - 01 T 09 : 00 + 00 : 00 contains two 3 -hourly time steps. The first one starts at 3 : 00 AM and finishes at 6 : 00 AM on Jan 1 st 2000, Universal Time; the second one starts at 6 : 00 AM and finishes at 9 : 00 AM on Jan 1 st 2000, Universal Time. Data sources The following sources were used to produce files in this dataset: 	The National Hydrography Dataset Plus (NHDPlus) Version 1, obtained from [URL] 	The National Water Information System (NWIS), obtained from [URL] 	Outputs from a simulation using the community Noah land surface model with multiparameterization options (Noah-MP, Niu et al. 2011, [URL] The simulation was run by Guo-Yue Niu, and produced 3 -hourly time steps from 2004 - 01 - 01 T 00 : 00 + 00 : 00 to 2008 - 01 - 01 T 00 : 00 + 00 : 00. Further details on the inputs and options used for this simulation are provided in David et al. (2011). Software The following software were used to produce files in this dataset: 	The Routing Application for Parallel computation of Discharge (RAPID, David et al. 2011, [URL] Version 1. 0. 0. Further details on the inputs and options used for this series of simulations are provided below and in David et al. (2011). 	ESRI ArcGIS ([URL] 	Microsoft Excel ([URL] 	CUAHSI HydroGET ([URL] 	The GNU Compiler Collection ([URL] and the Intel compilers ([URL] Study domain The files in this dataset correspond to two study domains: 	The combination of the San Antonio and Guadalupe River Basins, TX. RAPID can only use the river reaches of NHDPlus that have a known flow direction and focus is made on these reaches here (a total of 5, 175). The temporal range corresponding to this domain is from 2004 - 01 - 01 T 00 : 00 - 06 : 00 to 2007 - 12 - 31 T 00 : 00 - 06 : 00. 	The Upper Mississippi River Basin. RAPID can only use the river reaches of NHDPlus that have a known flow direction and focus is made on these reaches here (a total of 182, 240). The temporal range corresponding to this domain spans 100 fictitious days. Description of files for the San Antonio and Guadalupe River Basins All files below were prepared by Cédric H. David, using the data sources and software mentioned above. 	rapid_connect_San_Guad. csv. This CSV file contains the river network connectivity information and is based on the unique IDs of NHDPlus reaches (the COMIDs). For each river reach, this file specifies: the COMID of the reach, the COMID of the unique downstream reach, the number of upstream reaches with a maximum of four reaches, and the COMIDs of all upstream reaches. A value of zero is used in place of NoData. The river reaches are sorted in increasing value of COMID. The values were computed using a combination of the following NHDPlus fields: COMID, DIVERGENCE, FROMNODE and TONODE. This file was prepared using ArcGIS and <b>Excel.</b> 	<b>m</b> 3 _riv_San_Guad_ 2004 _ 2007 _cst. nc. This netCDF file contains the 3 -hourly accumulated inflows of water (in cubic meters) from surface and subsurface runoff into the upstream point of each river reach. The river reaches have the same COMIDs and are sorted similarly to rapid_connect_San_Guad. csv. The time range for this file is from 2004 - 01 - 01 T 00 : 00 - 06 : 00 to 2007 / 12 / 31 T 18 : 00 - 06 : 00. The values were computed by superimposing a 900 -m gridded map of NHDPlus catchments to the outputs of Noah-MP. This file was prepared using ArcGIS and a Fortran program. 	kfac_San_Guad_ 1 km_hour. csv. This CSV file contains a first guess of Muskingum k values (in seconds) for all river reaches. The river reaches have the same COMIDs and are sorted similarly to rapid_connect_San_Guad. csv. The values were computed based on the following NHDPlus fields: COMID, LENGTHKM, Equation (13) in David et al. (2011), and using a wave celerity of 1 km/h. This file was prepared using a Fortran program. 	kfac_San_Guad_celerity. csv. This CSV file contains a first guess of Muskingum k values (in seconds) for all river reaches. The river reaches have the same COMIDs and are sorted similarly to rapid_connect_San_Guad. csv. The values were computed based on the following NHDPlus fields: COMID, LENGTHKM, Equation (13) in David et al. (2011), and using the wave celerity numbers of Table 2 in David et al. (2011). This file was prepared using a Fortran program. 	k_San_Guad_ 2004 _ 1. csv. This CSV file contains Muskingum k values (in seconds) for all river reaches. The river reaches have the same COMIDs and are sorted similarly to rapid_connect_San_Guad. csv. The values were computed based on the following NHDPlus fields: COMID, LENGTHKM, and using Equation (17) in David et al. (2011). This file was prepared using a Fortran program. 	k_San_Guad_ 2004 _ 2. csv. This CSV file contains Muskingum k values (in seconds) for all river reaches. The river reaches have the same COMIDs and are sorted similarly to rapid_connect_San_Guad. csv. The values were computed based on the following NHDPlus fields: COMID, LENGTHKM, and using Equation (18) in David et al. (2011). This file was prepared using a Fortran program. 	k_San_Guad_ 2004 _ 3. csv. This CSV file contains Muskingum k values (in seconds) for all river reaches. The river reaches have the same COMIDs and are sorted similarly to rapid_connect_San_Guad. csv. The values were computed based on the following NHDPlus fields: COMID, LENGTHKM, and using Equation (19) in David et al. (2011). This file was prepared using a Fortran program. 	k_San_Guad_ 2004 _ 4. csv. This CSV file contains Muskingum k values (in seconds) for all river reaches. The river reaches have the same COMIDs and are sorted similarly to rapid_connect_San_Guad. csv. The values were computed based on the following NHDPlus fields: COMID, LENGTHKM, and using Equation (21) in David et al. (2011). This file was prepared using a Fortran program. 	x_San_Guad_ 2004 _ 1. csv. This CSV file contains Muskingum x values (dimensionless) for all river reaches. The river reaches have the same COMIDs and are sorted similarly to rapid_connect_San_Guad. csv. The values were computed based on Equation (17) in David et al. (2011). This file was prepared using a Fortran program. 	x_San_Guad_ 2004 _ 2. csv. This CSV file contains Muskingum x values (dimensionless) for all river reaches. The river reaches have the same COMIDs and are sorted similarly to rapid_connect_San_Guad. csv. The values were computed based on Equation (18) in David et al. (2011). This file was prepared using a Fortran program. 	x_San_Guad_ 2004 _ 3. csv. This CSV file contains Muskingum x values (dimensionless) for all river reaches. The river reaches have the same COMIDs and are sorted similarly to rapid_connect_San_Guad. csv. The values were computed based on Equation (19) in David et al. (2011). This file was prepared using a Fortran program. 	x_San_Guad_ 2004 _ 4. csv. This CSV file contains Muskingum x values (dimensionless) for all river reaches. The river reaches have the same COMIDs and are sorted similarly to rapid_connect_San_Guad. csv. The values were computed based on Equation (21) in David et al. (2011). This file was prepared using a Fortran program. 	basin_id_San_Guad_hydroseq. csv. This CSV file contains the list of unique IDs of NHDPlus river reaches (COMID) in the San Antonio and Guadalupe River Basins. The river reaches are sorted from upstream to downstream. The values were computed using the following NHDPlus fields: COMID and HYDROSEQ. This file was prepared using Excel. 	Qout_San_Guad_ 1460 days_p 1 _dtR= 900 s. nc. This netCDF file contains the 3 -hourly averaged outputs (in cubic meters per second) from RAPID corresponding to the downstream point of each reach. The river reaches have the same COMIDs and are sorted similarly to basin_id_San_Guad_hydroseq. csv. The time range for this file is from 2004 - 01 - 01 T 00 : 00 - 06 : 00 to 2007 - 12 - 31 - 00 : 00 - 06 : 00. The values were computed using the Muskingum method with parameters of Equation (17) in David et al. (2011). This file was prepared using RAPID v 1. 0. 0 running with the preonly ILU solver on one core. 	Qout_San_Guad_ 1460 days_p 2 _dtR= 900 s. nc. This netCDF file contains the 3 -hourly averaged outputs (in cubic meters per second) from RAPID corresponding to the downstream point of each reach. The river reaches have the same COMIDs and are sorted similarly to basin_id_San_Guad_hydroseq. csv. The time range for this file is from 2004 - 01 - 01 T 00 : 00 - 06 : 00 to 2007 - 12 - 31 - 00 : 00 - 06 : 00. The values were computed using the Muskingum method with parameters of Equation (18) in David et al. (2011). This file was prepared using RAPID v 1. 0. 0 running with the preonly ILU solver on one core. 	Qout_San_Guad_ 1460 days_p 3 _dtR= 900 s. nc. This netCDF file contains the 3 -hourly averaged outputs (in cubic meters per second) from RAPID corresponding to the downstream point of each reach. The river reaches have the same COMIDs and are sorted similarly to basin_id_San_Guad_hydroseq. csv. The time range for this file|$|R
