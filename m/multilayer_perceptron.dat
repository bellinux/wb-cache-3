2810|782|Public
25|$|The <b>multilayer</b> <b>perceptron</b> is a {{universal}} function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.|$|E
25|$|Polikar, R., Udpa, L., Udpa, S., and Honavar, V. (2000). Learn++: An Incremental Learning Algorithm for <b>Multilayer</b> <b>Perceptron</b> Networks. In: Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2000. Istanbul, Turkey.|$|E
25|$|The group {{method of}} data {{handling}} (GMDH) features fully automatic structural and parametric model optimization. The node activation functions are Kolmogorov-Gabor polynomials that permit additions and multiplications. It used a deep feedforward <b>multilayer</b> <b>perceptron</b> with eight layers. It is a supervised learning network that grows layer by layer, where each layer is trained by regression analysis. Useless items are detected using a validation set, and pruned through regularization. The size {{and depth of}} the resulting network depends on the task.|$|E
40|$|A {{denoising}} unit {{based on}} wavelet multiresolution analysis is added {{ahead of the}} <b>multilayered</b> <b>perceptron.</b> The cost function used in neural network learning is also applied as the denoising criterion and hence denoising itself is treated {{as a part of}} the integrated model. By introducing continuously derivable generalized soft threshold-ing function and infinite thresholds, a gradient based learning algorithm for simul-taneous setting of all free parameters of the model is derived. The proposed model outmatches the classical <b>multilayered</b> <b>perceptron</b> and the <b>multilayered</b> <b>perceptron</b> with statistical denoising in noisy time series prediction problems. Key words: <b>Multilayered</b> <b>perceptron,</b> Wavelet multiresolution analysis, Denoising, Gradient descent threshold adaptation, Time series predictio...|$|R
40|$|Artificial neural networks, {{especially}} <b>multilayer</b> <b>perceptrons,</b> {{have been}} recognised {{as being a}} powerful technique for forecasting nonlinear time series; however, cascade-correlation architecture is a strong competitor in this task due to it incorporating several advantages related to the statistical identification of <b>multilayer</b> <b>perceptrons.</b> This paper compares the accuracy of a cascade-co- rrelation neural network to the linear approach, <b>multilayer</b> <b>perceptrons</b> and dynamic architecture for artificial neural networks (DAN 2) {{to determine whether the}} cascade-correlation network was able to forecast the time series being studied with more accu- racy. It was concluded that cascade-correlation was able to forecast time series with more accuracy than other approaches...|$|R
50|$|Backpropagation {{of errors}} in <b>multilayer</b> <b>perceptrons,</b> a {{technique}} used in machine learning, {{is a special}} case of reverse mode AD.|$|R
25|$|Igor Aizenberg {{and colleagues}} {{introduced}} it to Artificial Neural Networks in 2000. The first functional Deep Learning networks were published by Alexey Grigorevich Ivakhnenko and V. G. Lapa in 1965. These networks are trained one layer at a time. Ivakhnenko's 1971 paper describes {{the learning of}} a deep feedforward <b>multilayer</b> <b>perceptron</b> with eight layers, already much deeper than many later networks. In 2006, a publication by Geoffrey Hinton and Ruslan Salakhutdinov introduced another way of pre-training many-layered feedforward neural networks (FNNs) one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then using supervised backpropagation for fine-tuning. Similar to shallow artificial neural networks, deep neural networks can model complex non-linear relationships. Over the last few years, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.|$|E
5000|$|PNNs {{can be more}} {{accurate}} than <b>multilayer</b> <b>perceptron</b> networks.|$|E
5000|$|PNN are {{slower than}} <b>multilayer</b> <b>perceptron</b> {{networks}} at classifying new cases.|$|E
5000|$|<b>Multilayer</b> <b>perceptrons</b> are {{sometimes}} colloquially {{referred to as}} [...] "vanilla" [...] neural networks, especially {{when they have a}} single hidden layer.|$|R
40|$|We {{present a}} {{framework}} to apply Volterra series to analyze multi-layered perceptrons trained {{to estimate the}} posterior probabilities of phonemes in automatic speech recognition. The identified Volterra kernels reveal the spectro-temporal patterns that are learned by the trained system for each phoneme. To demonstrate the applicability of Volterra series, we analyze a <b>multilayered</b> <b>perceptron</b> trained us-ing Mel filter bank energy features and analyze its first order Volterra kernels. Index Terms — Volterra series, <b>multilayered</b> <b>perceptrons,</b> speech recognitio...|$|R
40|$|The {{importance}} of path planning is very {{significant in the}} field of robotics. This paper presents the application of <b>multilayer</b> <b>perceptrons</b> to the robot path planning problem, and in particular to the task of maze navigation. Previous published results implied that the training of feedforward multilayered networks failed, because of the non- smoothness of data. Here the path planning problem is reconsidered, and it is shown that <b>multilayer</b> <b>perceptrons</b> are able to learn the task successfully...|$|R
50|$|There {{are several}} {{advantages}} and disadvantages using PNN instead of <b>multilayer</b> <b>perceptron.</b>|$|E
50|$|In {{supervised}} feature learning, {{features are}} learned {{in part with}} labeled input data. Examples include supervised neural networks, <b>multilayer</b> <b>perceptron,</b> and (supervised) dictionary learning.|$|E
50|$|The {{most widely}} used {{learning}} algorithms are Support Vector Machines, linear regression, logistic regression, naive Bayes, linear discriminant analysis, decision trees, k-nearest neighbor algorithm, and Neural Networks (<b>Multilayer</b> <b>perceptron).</b>|$|E
40|$|Neural {{models for}} calculating the {{resonant}} frequency of electrically thin and thick circular microstrip antennas, {{based on the}} <b>multilayered</b> <b>perceptrons</b> and the radial basis function networks, are presented. Five learning algorithms, delta-bar-delta, extended delta-bar-delta, quick-propagation, directed random search and genetic algorithms, are used to train the <b>multilayered</b> <b>perceptrons.</b> The radial basis function network is trained according to its learning strategy. The resonant frequency results obtained by using neural models are in very good agreement with the experimental results available in the literature. I...|$|R
40|$|This paper {{compares the}} <b>Multilayer</b> <b>Perceptrons</b> network (trained by the backpropagation) and the Radial Basis Function {{networks}} in the task of speaker identification. The experiments were carried out on 200 utterances (10 digits) of 10 speakers. LPC-derived cepstrum coefficients were used as the speaker specific features. The {{results showed that the}} <b>Multilayer</b> <b>Perceptrons</b> networks were superior in memory usage and classification time. However, they suffered from long training time and the error rate was slightly higher than that of Radial Basis Function networks...|$|R
40|$|This paper proposes {{new methods}} of {{generating}} input locations actively in gathering training data, aiming at solving problems special to <b>multilayer</b> <b>perceptrons.</b> One {{of the problems is}} that the optimum input locations which are calculated deterministically sometimes result in badlydistributed data and cause local minima in back-propagation training. Two probabilistic active learning methods, which utilize the statistical variance of locations, are proposed to solve this problem. One is parametric active learning and the other is multi-point-search active learning. Another serious problem in applying active learning to <b>multilayer</b> <b>perceptrons</b> is the singularity of a Fisher information matrix, whose regularity is assumed in many methods including the proposed ones. A technique of pruning redundant hidden units is proposed to keep the regularity of a Fisher information matrix, which makes active learning applicable to <b>multilayer</b> <b>perceptrons.</b> The effectiveness of the proposed methods is demo [...] ...|$|R
5000|$|Polikar, R., Udpa, L., Udpa, S., and Honavar, V. (2000). Learn++: An Incremental Learning Algorithm for <b>Multilayer</b> <b>Perceptron</b> Networks. In: Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2000. Istanbul, Turkey.|$|E
50|$|The <b>multilayer</b> <b>perceptron</b> is a {{universal}} function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.|$|E
50|$|The {{transfer}} {{function of a}} neuron is chosen to {{have a number of}} properties which either enhance or simplify the network containing the neuron. Crucially, for instance, any <b>multilayer</b> <b>perceptron</b> using a linear {{transfer function}} has an equivalent single-layer network; a non-linear function is therefore necessary to gain the advantages of a multi-layer network.|$|E
40|$|Proper {{initialization}} {{is one of}} {{the most}} important prerequisites for fast convergence of feed-forward neural networks like high order and <b>multilayer</b> <b>perceptrons.</b> This publication aims at determining the optimal value of the initial weight variance (or range), which is the principal parameter of random weight initialization methods for both types of neural networks. An overview of random weight initialization methods for <b>multilayer</b> <b>perceptrons</b> is presented. These methods are extensively tested using eight real-world benchmark data sets and a broad range of initial weight variances by means of more than 30; 000 simulations, in the aim to find the best weight initialization method for <b>multilayer</b> <b>perceptrons.</b> For high order networks, a large number of experiments (more than 200; 000 simulations) was performed, using three weight distributions, three activation functions, several network orders, and the same eight data sets. The results of these experiments are compared to weight initializa [...] ...|$|R
40|$|When {{reinforcement}} learning {{is applied to}} large state spaces, such as those occurring in playing board games, {{the use of a}} good function approximator to learn to approximate the value function is very important. In previous research, multi-layer perceptrons have often been quite successfully used as function approximator for learning to play particular games with temporal difference learning. With the recent developments in deep learning, it is important to study if using multiple hidden layers or particular network structures can help to improve learning the value function. In this paper, we compare five different structures of <b>multilayer</b> <b>perceptrons</b> for learning to play the game Tic-Tac-Toe 3 D, both when training through self-play and when training against the same fixed opponent they are tested against. We compare three fully connected <b>multilayer</b> <b>perceptrons</b> with a different number of hidden layers and/or hidden units, as well as two structured ones. These structured <b>multilayer</b> <b>perceptrons</b> have a first hidden layer that is only sparsely connected to the input layer, and has units that correspond to the rows in Tic-Tac-Toe 3 D. This allows them to more easily learn the contribution of specific patterns on the corresponding rows. One of the two structured <b>multilayer</b> <b>perceptrons</b> has a second hidden layer that is fully connected to the first one, which allows the neural network to learn to non-linearly integrate the information in these detected patterns. The results on Tic-Tac-Toe 3 D show that the deep structured neural network with integrated pattern detectors has the strongest performance out of the compared <b>multilayer</b> <b>perceptrons</b> against a fixed opponent, both through self-training and through training against this fixed opponent...|$|R
40|$|A new {{analysis}} {{strategy was}} used to classify the carcinogenicity of aromatic amines. The physical-chemical parameters {{are closely related to}} the carcinogenicity of compounds. Quantitative structure activity relationship (QSAR) is a method of predicting the carcinogenicity of aromatic amine, which can reveal the relationship between carcinogenicity and physical-chemical parameters. This study accessed gene expression programming by APS software, the <b>multilayer</b> <b>perceptrons</b> by Weka software to predict the carcinogenicity of aromatic amines, respectively. All these methods relied on molecular descriptors calculated by CODESSA software and eight molecular descriptors were selected to build function equations. As a remarkable result, the accuracy of gene expression programming in training and test sets are 0. 92 and 0. 82, the accuracy of <b>multilayer</b> <b>perceptrons</b> in training and test sets are 0. 84 and 0. 74 respectively. The precision of the gene expression programming is obviously superior to <b>multilayer</b> <b>perceptrons</b> both in training set and test set. The QSAR application in the identification of carcinogenic compounds is a high efficiency method...|$|R
50|$|In {{the context}} of neural networks, a {{perceptron}} is an artificial neuron using the Heaviside step function as the activation function. The perceptron algorithm is also termed the single-layer perceptron, to distinguish it from a <b>multilayer</b> <b>perceptron,</b> which is a misnomer for a more complicated neural network. As a linear classifier, the single-layer perceptron is the simplest feedforward neural network.|$|E
50|$|Neuroph's core classes {{correspond}} to basic neural network concepts like artificial neuron, neuron layer, neuron connections, weight, transfer function, input function, learning rule etc. Neuroph supports common neural network architectures such as <b>Multilayer</b> <b>perceptron</b> with Backpropagation, Kohonen and Hopfield networks. All these classes {{can be extended}} and customized to create custom neural networks and learning rules. Neuroph has built-in support for image recognition.|$|E
5000|$|The {{output from}} the Kohonen layer, {{which is the}} winning neuron, feeds into a hidden layer and finally into an output layer. In other words, the Kohonen SOM is the front-end, while the hidden and output layer of a <b>multilayer</b> <b>perceptron</b> is the {{back-end}} of thehybrid Kohonen SOM. The hybrid Kohonen SOM was first applied to machine vision systems for image classification and recognition.|$|E
50|$|Some {{examples}} of neural network training techniques are backpropagation, quick propagation, conjugate gradient descent, projection operator, Delta-Bar-Delta etc. Some unsupervised network architectures are <b>multilayer</b> <b>perceptrons,</b> Kohonen networks, Hopfield networks, etc.|$|R
40|$|The {{ice cover}} on lakes {{is one of}} the most {{influential}} factors in the lakes’ winter aquatic ecosystem. The paper presents a method for predicting ice coverage of lakes by means of <b>multilayer</b> <b>perceptrons.</b> This approach is based on historical data on the ice coverage of lakes taking Lake Onega as an example. The daily time series of ice coverage of Lake Onega for 2004 – 2017 was collected by means of satellite data analysis of snow and ice cover of the Northern Hemisphere. Input signals parameters for the <b>multilayer</b> <b>perceptrons</b> aimed at predicting ice coverage of lakes are based on the correlation analysis of this time series. The results of training of <b>multilayer</b> <b>perceptrons</b> showed that perceptrons with architectures of 3 - 2 - 1 within the Freeze-up phase (arithmetic mean of the mean square errors for training epoch MSE¯= 0. 0155 MSE = 0. 0155) and 3 - 6 - 1 within the Break-up phase (MSE¯= 0. 0105 MSE = 0. 0105) have the least mean-squared error for the last training epoch. Tests within the holdout samples prove that <b>multilayer</b> <b>perceptrons</b> give more adequate and reliable prediction of the ice coverage of Lake Onega (mean-squared prediction error MSPE = 0. 0076) comparing with statistical methods such as linear regression, moving average and autoregressive analyses of the first and second order...|$|R
40|$|We {{investigate}} {{the use of}} the log-likelihood of the features obtained from a generative Gaussian mixture model, and the posterior probability of phonemes from a discriminative <b>multilayered</b> <b>perceptron</b> in multi-stream combination for recognition of phonemes. Multi-stream combination techniques, namely early integration and late integration are used to combine the evidence from these models. By using multi-stream combination, we obtain a phoneme recognition accuracy of 74 % on the standard TIMIT database, an absolute improvement of 2. 5 % over the single best stream. Index Terms: Phoneme recognition, Gaussian mixture model, <b>multilayered</b> <b>perceptron,</b> phoneme posterior probability, early integration, late integration. 1...|$|R
50|$|A <b>multilayer</b> <b>perceptron</b> (MLP) is a {{class of}} {{feedforward}} artificial neural network. An MLP consists {{of at least three}} layers of nodes. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable.|$|E
50|$|If a <b>multilayer</b> <b>perceptron</b> has {{a linear}} {{activation}} function in all neurons, that is, a linear function that maps the weighted inputs to {{the output of}} each neuron, then linear algebra shows that any number of layers {{can be reduced to}} a two-layer input-output model. In MLPs some neurons use a nonlinear activation function that was developed to model the frequency of action potentials, or firing, of biological neurons.|$|E
5000|$|Architecturally, the {{simplest}} {{form of an}} autoencoder is a feedforward, non-recurrent neural network {{very similar to the}} <b>multilayer</b> <b>perceptron</b> (MLP) - having an input layer, an output layer and one or more hidden layers connecting them -, but with the output layer having the same number of nodes as the input layer, and with the purpose of reconstructing its own inputs (instead of predicting the target value [...] given inputs [...] ). Therefore, autoencoders are unsupervised learning models.|$|E
40|$|Abstract—An {{algorithmic}} {{procedure is}} {{developed for the}} random expansion of a given training set to combat overfitting and improve the generalization ability of backpropagation trained <b>multilayer</b> <b>perceptrons</b> (MLPs). The training set is-means clustered and locally most entropic colored Gaussian joint input–output probability density function (pdf) estimates are formed per cluster. The number of clusters is chosen such that the resulting overall colored Gaussian mixture exhibits minimum differential entropy upon global cross-validated shaping. Numerical studies on real data and synthetic data examples drawn from the literature illustrate and support these theoretical developments. Index Terms—Backpropagation, clustering methods, entropy, Gaussian distributions, <b>multilayer</b> <b>perceptrons</b> (MLPs), stochasti...|$|R
40|$|All-optical <b>multilayer</b> <b>perceptrons</b> {{differ in}} various ways from the ideal neural network model. Examples are the use of non-ideal {{activation}} functions which are truncated, asymmetric, and have a non-standard gain, restriction of the network parameters to non-negative values, and the limited accuracy of the weights. In this paper, a backpropagation-based learning rule is presented that compensates for these non-idealities and enables the implementation of all-optical <b>multilayer</b> <b>perceptrons</b> where learning occurs under control of a computer. The good performance of this learning rule, even when using {{a small number of}} weight levels, is illustrated by a series of computer simulations incorporating the non-idealities...|$|R
50|$|<b>Multilayer</b> <b>perceptrons</b> {{provide a}} natural {{extension}} to the multi-class problem. Instead of just having one neuron in the output layer, with binary output, {{one could have}} N binary neurons leading to multi-class classification.|$|R
