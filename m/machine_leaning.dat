43|9|Public
50|$|One of {{the main}} fields of data {{analysis}} today is machine learning. Some examples of machine learning in TDA {{can be found in}} Adcock et al. A conference is dedicated to the link between TDA and machine learning. In order to apply tools from <b>machine</b> <b>leaning,</b> the information obtained from TDA should be represented in vector form. An ongoing and promising attempt is the persistence landscape discussed above. Another attempt uses the concept of persistence images. However, one problem of this method is the loss of stability, since the hard stability theorem depends on the barcode representation.|$|E
40|$|Abstract. Data {{mining and}} <b>machine</b> <b>leaning</b> {{communities}} were surprised when Keogh et al. (2003) {{pointed out that}} the k-means cluster centers in subsequence time-series clustering become sinusoidal pseudopatterns for almost all kinds of input time-series data. Understanding this mechanism is an important open problem in data mining. Our new theoretical approach (based on spectral clustering and translational symmetry) explains why the cluster centers of k-means naturally tend to form sinusoidal patterns. ...|$|E
40|$|This paper {{presents}} various {{techniques used}} {{in the area of}} Word Sense Disambiguation (WSD). There are a number of techniques such as: Knowledge based approaches, which use the knowledge encoded in Lexical resources; Supervised <b>Machine</b> <b>Leaning</b> methods in which the classifier is made to learn from previously semantically annotated corpus; Unsupervised approaches that form cluster occurrences of words. Then there are also semi supervised approaches which use semi annotated corpus as reference data along with unlabeled data...|$|E
40|$|Support Vector Machines (SVM) {{is one of}} {{the most}} widely used {{technique}} in <b>machines</b> <b>leaning.</b> After the SVM algorithms process the data and produce some classification, it is desirable to learn how well this classification fits the data. There exist several measures of fit, among them the most widely used is kernel target alignment. These measures, however, assume that the data are known exactly. In reality, whether the data points come from measurements or from expert estimates, they are only known with uncertainty. As a result, even if we know that the classification perfectly fits the nominal data, this same classification can be a bad fit for the actual values (which are somewhat different from the nominal ones). In this paper, we show how to take this uncertainty into account when estimating the quality of the resulting classification...|$|R
40|$|Summary. Support Vector Machines (SVM) {{is one of}} {{the most}} widely used {{technique}} in <b>machines</b> <b>leaning.</b> After the SVM algorithms process the data and produce some classification, it is desirable to learn how well this classification fits the data. There exist several measures of fit, among them the most widely used is kernel target alignment. These measures, however, assume that the data are known exactly. In reality, whether the data points come from measurements or from expert estimates, they are only known with uncertainty. As a result, even if we know that the classification perfectly fits the nominal data, this same classification can be a bad fit for the actual values (which are somewhat different from the nominal ones). In this paper, we show how to take this uncertainty into account when estimating the quality of the resulting classification. 1 Formulation of the Problem Machine learning: main problem. In many practical situations, we have examples of several types of objects, and we would like to use these examples to teach the computers to distinguish between objects of different types. Each object can be characterized by the corresponding values of several relevant quantities. If we denote the number of these quantities by d, then we can say that each object i can be represented by a d-dimensional vector, [...] ., x(i) k, [...] ., x(i...|$|R
40|$|In this paper, the {{ensemble}} of support vector machines {{is applied to}} text-independent speaker recognition, and the bagging-like model and boosting-like model are proposed by adopted {{the ensemble}} idea. The purposes of adopting this idea are {{to deal with the}} large scale speech data and improve the performance of speaker recognition. The distance-based and probability-based scoring methods are used to score the new utterance. Compared with the conventional vector-based speaker models (Vector Quantization and Gaussian Mixture Model), our method is hyperplan-based. The experiments have been run on the YOHO database, and the results show that our models can get attractive performances. Key words: Speaker recognition, support vector <b>machine,</b> ensemble <b>leaning,</b> mixture of expert...|$|R
30|$|The {{final step}} is classification. Its {{task is to}} {{determine}} the category to which the detected target belongs. Here, we employ classifiers that are based on supervised <b>machine</b> <b>leaning,</b> where a training set is applied for each classifier. Each classifier operates on the training parameters, which are computed off-line, and the feature vectors, which are derived from the input data. It outputs an estimate of the class to which the detected target belongs. In this work, we consider three possible output classes: (a) person, (b) vehicle, and (c) noise (neither a person nor a vehicle).|$|E
40|$|Data mining is {{the process}} of {{analysing}} data from different viewpoints and summarizing it into useful information. Data mining tool allows users to analyse data from different dimensions or angles, categorize it, and précis the relations recognized. Clustering is the important aspect of data mining. It {{is the process}} of grouping of data, where the grouping is recognized by finding similarities between data based on their features. Weka is a data mining tool. It provides the facility to classify and cluster the data through <b>machine</b> <b>leaning</b> algorithms. This paper compares various clustering algorithm...|$|E
30|$|<b>Machine</b> <b>leaning</b> is a {{field of}} {{research}} that formally focuses on the theory, performance, and properties of learning systems and algorithms. It is a highly interdisciplinary field building upon ideas from {{many different kinds of}} fields such as artificial intelligence, optimization theory, information theory, statistics, cognitive science, optimal control, and many other disciplines of science, engineering, and mathematics [15 – 18]. Because of its implementation {{in a wide range of}} applications, machine learning has covered almost every scientific domain, which has brought great impact on the science and society [19]. It has been used on a variety of problems, including recommendation engines, recognition systems, informatics and data mining, and autonomous control systems [20].|$|E
40|$|Abstract:- Single Class Classification (SCC) is {{the problem}} to {{distinguish}} one class of data (called positive class) from the rest data of multiple classes (negative class). SCC problems are common in real world where positive and unlabeled data are available but negative data is expensive or very hard to acquire. In this paper, extreme <b>leaning</b> <b>machine</b> (ELM), a recently developed machine learning algorithm, is fused with mapping convergence algorithm {{that is based on}} the support vector machine (SVM). The proposed method achieves both high accuracy in classification, very fast learning and high speed in operation...|$|R
40|$|In this study, we {{introduce}} a statistic for testing neglected nonlinearity using the extreme <b>leaning</b> <b>machines</b> introduced by Huang, Zhu, and Siew (2006, Neurocomputing) {{and call it}} ELMNN test. The ELMNN test is very convenient and can be widely applied because it is obtained as a by-product of estimating a linear model. For the proposed test statistic, we provide a set of regularity conditions under which it asymptotically follows a chi-squared distribution under the null and is consistent under the alternative. We conduct Monte Carlo experiments and examine how it behaves when the sample size is finite. Our experiment shows that the test exhibits the properties desired by the theory of this paper...|$|R
40|$|In this study, we {{introduce}} {{statistics for}} testing neglected nonlinearity using the extreme <b>leaning</b> <b>machines</b> introduced by Huang, Zhu, and Siew (2006, Neurocomputing) and call them ELMNN tests. The ELMNN tests are very convenient {{and can be}} widely applied because they are obtained as by-products of estimating linear models, and they can serve as quick diagnostic test statistics complementing the computational burdens of other tests. For the proposed test statistics, we provide a set of regularity conditions under which they asymptotically follow a chi-squared distribution under the null and are consistent under the alternative. We conduct Monte Carlo experiments and examine how they behave when the sample size is finite. Our experiment shows that the tests exhibit the properties desired by the theory of this paper...|$|R
40|$|In this paper, {{we propose}} a new {{approach}} to sensor localization problems, based on recent developments in <b>machine</b> <b>leaning.</b> The main idea behind it is to consider a matrix regression method between the ranging matrix and the matrix of inner products between positions of sensors, in order to complete the latter. Once we have learnt this regression from information between sensors of known positions (beacons), we apply it to sensors of unknown positions. Retrieving the estimated positions of the latter can be done by solving a linear system. We propose a distributed algorithm, where each sensor positions itself with information available from its nearby beacons. The proposed method is validated by experimentations. 1...|$|E
40|$|Musical genre {{classification}} task {{falls into}} two major stages: feature extraction and classification. The latter implies {{a choice of}} a variety of <b>machine</b> <b>leaning</b> methods, as support vector machines, neural networks, etc. However, the former stage provides much more creativity in development of musical genre classification system and it plays crucial part in performance of the system as a whole. In this paper we present initial study of waveletbased feature extraction in the task of musical genre classification. A new type of feature vector, based on continuous wavelet transform of input audio data is proposed. The method of feature extraction was tested using support vector machine as a classifier. The results of our experimental study are shown. 1...|$|E
40|$|International audienceThere are {{two major}} stages in musical genre classification: feature {{extraction}} and classification. While the second stage implies a choice {{of a variety of}} <b>machine</b> <b>leaning</b> methods (SVMs, neural networks, etc), the first stage plays crucial part in perfomance and accuracy of the classification system, providing much more creativity in development of different feature extraction methods. In this paper we present initial study of feature extraction based on wavelets and pseudo-wavelets in the area of musical genre classification. A new type of feature vector, based on continuous wavelet and wavelet-like transform of input audio data is proposed. Support vector machine was used as a classifier for testing the feature extraction procedure. The results of our experimental study are shown...|$|E
30|$|The {{research}} in the learning theory provides a rich set of knowledge in learning the complex relationships and patterns in the datasets. Vapnik et al. show {{that the proportion of}} the training dataset size to the complexity of the regression model determines whether to use the empirical or the structural risk minimizations [25]. In the auto-scaling domain, the Predictor component corresponds to the learning <b>machine</b> of the <b>leaning</b> process. Therefore, to improve the accuracy of the Predictor component, the risk minimization principle should be determined based on the complexity of the prediction techniques (i.e., the VC-dimension) and the training dataset size. The workload pattern complexity is the main driving factor of the Predictor component’s VC-dimension. Four sub-hypotheses are introduced in order to experiment the risk minimization principles vis-à-vis the different workload patterns.|$|R
40|$|This thesis {{presents}} {{a collection of}} event-by-event models that simulate fundamental optical experiments. The simulation approach is completely based on the experimental facts. Each component in the model corresponds to one kind of optical device, such as a beam splitter, a wave plate, a detector and so on. Networks of such components build computational experiments which are one-to-one copies of real experiments. As all components share the same mechanism (<b>leaning</b> <b>machine)</b> {{as in the previous}} work, our event-by-event simulation models are systematic and consistent with each other. As the model provides a description of interference and other wave phenomena on the level of individual event, it goes beyond the description of quantum theory. All the results presented in this thesis demonstrate {{that it is possible to}} simulate quantum phenomena by classical, non-Hamiltonian, local, causal and dynamical models. ...|$|R
5000|$|These {{safety issues}} with three-wheel ATVs caused all ATV {{manufacturers}} to upgrade to four-wheel {{models in the}} late 1980s, and three-wheel models ended production in 1988, due to consent decrees between the major manufacturers and the Consumer Product Safety Commission—the result of legal battles over safety issues among consumer groups, the manufacturers and CPSC. The lighter weight of the three-wheel models made them popular with expert riders. Cornering is more challenging than with a four-wheeled <b>machine</b> because <b>leaning</b> into the turn is even more important, to counterbalance the weight and keep the machine stable. Operators may roll over if caution isn't used at high speeds. The front end of three-wheelers obviously has a single wheel, making it lighter, and flipping backwards is a potential hazard, especially when climbing hills. Rollovers may also occur when traveling down a very steep incline. However, in most terrain with the proper riding technique and safety gear, accidents are not an issue. Three-wheelers take more time to learn to ride properly than other machines and require a different style that others as well; such as leaning {{to the inside of}} the turn and steering with the throttle. The key is to break traction with the rear tires and [...] "power slide". The consent decrees expired in 1997, allowing manufacturers to, once again, make and market three-wheel models, though there are none marketed today. Recently, the CPSC has succeeded in finally banning importation of three-wheeled ATV's with attachments to bill HR4040. Many believe this is in response to Chinese manufacturers trying to import three-wheeled ATV's. The Japanese manufacturers were also behind this legislation, as they have been held responsible for years to provide ATV Safety training and to apply special labels and safety equipment to their ATVs while Chinese manufacturers did not. Three-wheelers can still be continued to be built and sold by American manufacturers if any chose to build them.|$|R
40|$|Research sites All {{remote sensing}} data sets were {{collected}} at two pilot research sites, Antarctic Specially Protected Area 135 (ASPA) and Robinson Ridge (Robbos), that host significant populations of Antarctic moss species, particularly: Schistidium antarctici (Cardot) L. I. Savicz and Smirnova, Bryum pseudotriquetrum (Hedw.) Gaertn., Meyer and Scherb., and Ceratodon purpureus (Hedw.) Brid. Verification of remote sensing products was performed {{with data from}} a long-term monitoring project of Windmill Islands 2 ̆ 7 plant communities using observations of 13 permanent quadrats, which were established at ASPA and Robbos in 2003 (Wasley et al., 2012). Laboratory spectral and biochemical measurements for training of predictive <b>machine</b> <b>leaning</b> algorithms were performed on moss samples collected {{in the vicinity of}} the Casey polar station in 2013 and previously in 1999 (Lovelock and Robinson, 2002) ...|$|E
40|$|Linearized {{alternating}} direction {{method of}} multipliers (ADMM) {{as an extension}} of ADMM has been widely used to solve linearly constrained problems in signal processing, <b>machine</b> <b>leaning,</b> communications, and many other fields. Despite its broad applications in nonconvex optimization, for a great number of nonconvex and nonsmooth objective functions, its theoretical convergence guarantee is still an open problem. In this paper, we propose a two-block linearized ADMM and a multi-block parallel linearized ADMM for problems with nonconvex and nonsmooth objectives. Mathematically, we present that the algorithms can converge for a broader class of objective functions under less strict assumptions compared with previous works. Furthermore, our proposed algorithm can update coupled variables in parallel and work for less restrictive nonconvex problems, where the traditional ADMM may have difficulties in solving subproblems. Comment: 29 pages, 2 tables, 2 figure...|$|E
40|$|This paper {{provides}} asymptotic {{theory for}} Inverse Probability Weighing (IPW) and Locally Robust Estimator (LRE) of Best Linear Predictor where the response missing at random (MAR), but not completely at random (MCAR). We relax previous assumptions {{in the literature}} about the first-step nonparametric components, requiring only their mean square convergence. This relaxation allows to use a wider class of <b>machine</b> <b>leaning</b> methods for the first-step, such as lasso. For a generic first-step, IPW incurs a first-order bias unless the model it approximates is truly linear in the predictors. In contrast, LRE remains first-order unbiased provided one can estimate the conditional expectation of the response with sufficient accuracy. An additional novelty is allowing the dimension of Best Linear Predictor to grow with sample size. These relaxations are important for estimation of best linear predictor of teacher-specific and hospital-specific effects with large number of individuals...|$|E
40|$|Abstract. Supervised {{learning}} from multiple annotators is {{an increasingly important}} problem in <b>machine</b> <b>leaning</b> and data mining. This paper develops a probabilistic approach to this problem when annotators are not only unreliable, but also have varying performance depending on the data. The proposed approach uses a Gaussian mixture model (GMM) and Bayesian information criterion (BIC) to find the fittest model to approximate {{the distribution of the}} instances. Then the maximum a posterior (MAP) estimation of the hidden true labels and the maximum-likelihood (ML) estimation of quality of multiple annotators are provided alternately. Experiments on emotional speech classification and CASP 9 protein disorder prediction tasks show performance improvement of the proposed approach as compared to the majority voting baseline and a previous data-independent approach. Moreover, the approach also provides more accurate estimates of individual annotators performance for each Gaussian component, thus paving the way for understanding the behaviors of each annotator...|$|E
40|$|In this paper, {{we present}} a named entity {{recognition}} model for Korean Language. Named entity recognition is an essential and important process of Question Answering and Information Extraction system. This paper proposes a HMM based named entity recognition using compound word construction principles. In Korean, above 60 % of NE (Named-Entity) is a compound word. This compound word may be consisted of proper noun, common noun, or bound noun, etc. There is an intercontextual relationship among nouns which consists NE. NE and surrounding words of NE have a contextual relationship. For considering these relationships, we classified nouns into 4 word classes (Independent Entity, Constituent Entity, Adjacent Entity, Not an Entity). With this classification, our system gets contextual and lexical information by stochastic based <b>machine</b> <b>leaning</b> method from a NE labeled training data. Experimental result shows that this approach is better approach than rulebased in the Korean named-entity recognition...|$|E
40|$|Early {{indication}} of bancruptcy {{is important for}} a company. If companies aware of  potency of their bancruptcy, they can take a preventive action to anticipate the bancruptcy. In order to detect the potency of a bancruptcy, a company can utilize a a model of bancruptcy prediction. The prediction model can be built using a machine learning methods. However, the choice of machine learning methods should be performed carefully. Because the suitability of a model depends on the problem specifically. Therefore, {{in this paper we}} perform a comparative study of several <b>machine</b> <b>leaning</b> methods for bancruptcy prediction. According to the comparative study, the performance of several models that based on machine learning methods (k-NN, fuzzy k-NN, SVM, Bagging Nearest Neighbour SVM, Multilayer Perceptron(MLP), Hybrid of MLP + Multiple Linear Regression), it can be showed that fuzzy k-NN method achieve the best performance with accuracy 77. 5 %</em...|$|E
40|$|Abstract — Generally, {{data mining}} (sometimes called data or {{knowledge}} discovery) {{is the process}} of analyzing data from different perspectives and summarizing it into useful information. Data mining software is {{one of a number of}} analytical tools for analyzing data. It allows users to analyze data from many different dimensions or angles, categorize it, and summarize the relationships identified. Weka is a data mining tools. It is contain the many <b>machine</b> <b>leaning</b> algorithms. It is provide the facility to classify our data through various algorithms. In this paper we are studying the various clustering algorithms. Cluster analysis or clustering is the task of assigning a set of objects into groups (called clusters) so that the objects in the same cluster are more similar (in some sense or another) to each other than to those in other clusters. Our main aim to show the comparison of the different- different clustering algorithms of weka and find out which algorithm will be most suitable for the users...|$|E
40|$|Most <b>machine</b> <b>leaning</b> methods {{assume that}} the {{original}} representation space is adequate, that is, the initial attributes or ternas ave sufficiently relevant Io the problem at hand. To cope with learning problems in which this assumption does not bold, the idea of constructive induction has been ioposocl. A constructive induction system conducts two searches, f'st fo an improved rpresemation space, and the second fo the best" hypothesis in this space. Research o constructive induction is feviewod aad a method for hypofi$i$-drien corrucli induction (AQ-HCI) is presentezL method seasrobes fo an adequate representation space by analyzing tl hypotheses generalad in each step of a. iterative duble-searc. h leftming process. In an xperimemtal study, the method outperformed all learning methods that were tested. Also, it achieved t! top peformaac in solving the so-called Monks' problems that benchmarie in the First international competition of letrning programs. The conlu$io outlines sveral open proble. m in this area...|$|E
40|$|Natural {{language}} recognization is {{a popular}} topic of research as it covers many areas such as computer science, artificial intelligence, theory of computation, and <b>machine</b> <b>leaning</b> etc. Many of the techniques are used for natural language recognization by the researchers, parsing is one of them. The aim to propose {{this paper is to}} implement nondeterministic pushdown automata (NPDA) for the English Language (ELR-NPDA) that can modernize Context Free Grammar (CFG) for English language and then refurbish into Nondeterministic Pushdown Automata (NPDA). This converting procedure can uncomplicatedly parse legitimate English language sentences. Parsing can be organized by Nondeterministic Pushdown Automata (NPDA) that used push down stack and input tape for recognizing English language sentences. To formulate this NPDA convertor we have to exchange Context Free Grammar into Chomsky Normal Form (CNF). The move toward this is more appropriate because it uses nondeterministic approach of PDA that can improve language recognizing capabilities as compare to other parsing approach...|$|E
40|$|Constrained pattern mining {{extracts}} patterns {{based on}} their individ-ual merit. Usually this results in far more patterns than a human expert or a <b>machine</b> <b>leaning</b> technique could make use of. Often different pat-terns or combinations of patterns cover a similar subset of the examples, thus being redundant and not carrying any new information. To remove the redundant information contained in such pattern sets, we propose two general heuristic algorithms- Bouncer and Picker- for selecting a small subset of patterns. We identify several selection techniques for use in this general algorithm and evaluate those on several data sets. The results show that both techniques succeed in severely {{reducing the number of}} patterns, {{while at the same time}} apparently retaining much of the orig-inal information. Additionally, the experiments show that reducing the pattern set indeed improves the quality of classification results. Both re-sults show that the developed solutions are very well suited for the goals we aim at. ...|$|E
40|$|Indoor {{localization}} {{has gained}} considerable attention {{over the past}} decade because of the emergence of numerous location-aware services. Research works have been proposed on solving this problem by using wireless networks. Nevertheless, there is still much room for improvement in the quality of the proposed classification models. In the last years, the emergence of Visible Light Communication (VLC) brings a brand new approach to high quality indoor positioning. Among its advantages, this new technology is immune to electromagnetic interference and has the advantage of having a smaller variance of received signal power compared to RF based technologies. In this paper, a performance analysis of seventeen <b>machine</b> <b>leaning</b> classifiers for indoor localization in VLC networks is carried out. The analysis is accomplished in terms of accuracy, average distance error, computational cost, training size, precision and recall measurements. Results show that most of classifiers harvest an accuracy above 90 %. The best tested classifier yielded a 99. 0 % accuracy, with an average error distance of 0. 3 centimetres. </p...|$|E
40|$|Digital {{marketing}} {{has brought}} in enormous capture of consumer data. In quantitative marketing, researchers have adopted structural models to explain the (dynamic) decision process and incentives of consumers. However, due to computational burdens, structural models have rarely been applied to big data. Machine learning algorithms that perform well on big data, however, hardly have any theoretical support. Equipped with both economics perspective and <b>machine</b> <b>leaning</b> techniques, in this dissertation, I aim to combine rigorous economic theory with machine learning methods and apply them to data on a large scale. First of all, I am deeply rooted in economic theory in understanding the behaviors of consumers, firms and the entire market. In my first essay “An Empirical Analysis of Consumer Purchase Behavior of Base Products and Add-ons Given Compatibility Constraints”, I model consumers as forward-looking utility maximizers who consider the product bundle of base products and add-ons jointly. I derive the add-on-to-base effect from {{the solution of the}} dynamic programming problem and then quantified it by model estimates. The underlying theory of switching cost caused by incompatibility constraints helps m...|$|E
40|$|Visualization {{of complex}} data, {{such as a}} file system or file, allows a {{forensic}} analyst or reverse engineer to rapidly locate areas of interest amidst a large quantity of data. While visualization provides a promising form of analysis, {{is the subject of}} much skepticism, as human interaction is required in order for this method to be successful. As a result of this, visualization methods face two major obstacles: tediousness and time. As our contribution, we propose a unique method of graphing visual information into a measurable format suitable for use with machine learning algorithms. This method will still utilize the visual layout of the data but streamline this form into one that can be rapidly processed by a machine. In this work we examine existing methods of file fragment analysis, determine how to apply visualization to this analysis, and transform this visual data into a measurable format for <b>machine</b> <b>leaning</b> algorithms using our tool called VMIFF (Visualization Metrics for the Identification of File Fragments). In its breadth, this work aims to demonstrate that such transformations will still yield meaningful results...|$|E
40|$|This book {{introduces}} {{readers to}} a variety of tools for automatic analog integrated circuit (IC) sizing and optimization. The authors provide a historical perspective on the early methods proposed to tackle automatic analog circuit sizing, with emphasis on the methodologies to size and optimize the circuit, and on the methodologies to estimate the circuit’s performance. The discussion also includes robust circuit design and optimization and the most recent advances in layout-aware analog sizing approaches. The authors describe a methodology for an automatic flow for analog IC design, including details of the inputs and interfaces, multi-objective optimization techniques, and the enhancements made in the base implementation by using <b>machine</b> <b>leaning</b> techniques. The Gradient model is discussed in detail, along with the methods to include layout effects in the circuit sizing. The concepts and algorithms of all the modules are thoroughly described, enabling readers to reproduce the methodologies, improve the quality of their designs, or use them as starting point for a new tool. An extensive set of application examples is included to demonstrate the capabilities and features of the methodologies described...|$|E
40|$|Supervised {{classification}} is {{the most}} studied task in Machine Learning. Among the many algorithms used in such task, Decision Tree algorithms are a popular choice, since they are robust and efficient to construct. Moreover, they {{have the advantage of}} producing comprehensible models and satisfactory accuracy levels in several application domains. Like most of the <b>Machine</b> <b>Leaning</b> methods, these algorithms have some hyper-parameters whose values directly affect the performance of the induced models. Due to the high number of possibilities for these hyper-parameter values, several studies use optimization techniques to find a good set of solutions in order to produce classifiers with good predictive performance. This study investigates how sensitive decision trees are to a hyper-parameter optimization process. Four different tuning techniques were explored to adjust J 48 Decision Tree algorithm hyper-parameters. In total, experiments using 102 heterogeneous datasets analyzed the tuning effect on the induced models. The experimental results show that even presenting a low average improvement over all datasets, in most of the cases the improvement is statistically significant...|$|E
40|$|This paper empirically {{shows that}} the effect of {{applying}} selected feature subsets on machine learning techniques significantly improves the accuracy for solar power prediction. Experiments are performed using five well-known wrapper feature selection methods to obtain the solar power prediction accuracy of machine learning techniques with selected feature subsets. For all the experiments, the machine learning techniques, namely, least median square (LMS), multilayer perceptron (MLP), and support vector machine (SVM), are used. Afterwards, these results are compared with the solar power prediction accuracy of those same <b>machine</b> <b>leaning</b> techniques (i. e., LMS, MLP, and SVM) but without applying feature selection methods (WAFS). Experiments are carried out using reliable and real life historical meteorological data. The comparison between the results clearly shows that LMS, MLP, and SVM provide better prediction accuracy (i. e., reduced MAE and MASE) with selected feature subsets than without selected feature subsets. Experimental results of this paper facilitate to make a concrete verdict that providing more attention and effort towards the feature subset selection aspect (e. g., selected feature subsets on prediction accuracy which is investigated in this paper) can significantly contribute to improve the accuracy of solar power prediction...|$|E
40|$|The paper {{presents}} a structure based on samplings and <b>machine</b> <b>leaning</b> techniques {{for the detection}} of multicategory EEG signals where random sampling (RS) and optimal allocation sampling (OS) are explored. In the proposed framework, before using the RS and OS scheme, the entire EEG signals of each class are partitioned into several groups based on a particular time period. The RS and OS schemes are used in order to have representative observations from each group of each category of EEG data. Then all of the selected samples by the RS from the groups of each category are combined in a one set named RS set. In the similar way, for the OS scheme, an OS set is obtained. Then eleven statistical features are extracted from the RS and OS set, separately. Finally this study employs three well-known classifiers: k-nearest neighbor (k-NN), multinomial logistic regression with a ridge estimator (MLR), and support vector machine (SVM) to evaluate the performance for the RS and OS feature set. The experimental outcomes demonstrate that the RS scheme well represents the EEG signals and the k-NN with the RS is the optimum choice for detection of multicategory EEG signals...|$|E
40|$|Current {{development}} of algorithms in computer-aided diagnosis (CAD) scheme is growing rapidly {{to assist the}} radiologist in medical image interpretation. Texture analysis of computed tomography (CT) scans is one of important preliminary stage in the computerized detection system and classification for lung cancer. Among different types of images features analysis, Haralick texture with variety of statistical measures has been used widely in image texture description. The extraction of texture feature values is essential {{to be used by}} a CAD especially in classification of the normal and abnormal tissue on the cross sectional CT images. This paper aims to compare experimental results using texture extraction and different <b>machine</b> <b>leaning</b> methods in the classification normal and abnormal tissues through lung CT images. The machine learning methods involve in this assessment are Artificial Immune Recognition System (AIRS), Naive Bayes, Decision Tree (J 48) and Backpropagation Neural Network. AIRS is found to provide high accuracy (99. 2 %) and sensitivity (98. 0 %) in the assessment. For experiments and testing purpose, publicly available datasets in the Reference Image Database to Evaluate Therapy Response (RIDER) are used as study cases. © (2013) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only...|$|E
