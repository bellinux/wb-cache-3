0|472|Public
40|$|The <b>multiple</b> <b>inference</b> {{character}} of several {{tests in the}} same application is usually taken into consideration by requiring that the tests have a multiple level of significance. Also, a prediction problem in an application with several possible predictor variables requires that the <b>multiple</b> <b>inference</b> {{character of}} the problem be considered. This is not being done in the methods commonly used to choose predictor variables. Here, we discuss both the test and prediction methods in two-level factorial designs and suggest a principle for choosing variables {{which is based on}} <b>multiple</b> <b>inference</b> thinking. By an example use demonstrated that the principle proposed leads to the use of fewer prediction variables than does the Akaike method. prediction, <b>multiple</b> <b>inference,</b> factorial design, Akaike's method,...|$|R
40|$|The {{influence}} of working memory on facets of comprehension such as inferencing {{has been widely}} investigated. Working memory capacity for language (WMCL), a common measure of the central executive component of working memory, {{has been linked to}} inferencing and particularly to the maintenance or narrowing of <b>inferences</b> when <b>multiple</b> interpretations are generated. Conflicting conclusions are evident, however. Some work suggests that poor WMCL leads to overly narrowing inferences, attributing this performance to a lack of cognitive fuel to maintain simultaneous interpretations. Other work concludes that poor WMCL yields a difficulty in narrowing inferences to reach a single conclusion. And some data from the realm of syntax suggest that maintaining multiple options is a strength, available only to individuals with good WMCL. To further explore this issue, this experiment investigated inference narrowing in neurotypical adults with High vs. Low WMCL. In a thinking-outloud task, participants responded with whatever came to mind after hearing each sentence of 8 -to- 9 sentence narratives. Each response was coded as to whether it reflected a single <b>inference,</b> <b>multiple</b> <b>inferences,</b> or maintained inference. This study also expanded the conceptual scope of prior investigations by assessing inference maintenance in relation to a newer component of the working memory system, the episodic buffer. The episodic buffer participates in combining and encoding the processes and outputs of other components of working memory and long-term memory. Its functioning was measured in terms of relative verbatim recall of a separate set of coherent versus scrambled stories. 	One main result of this study was that the WMCL groups did not differ in the maintenance or narrowing of <b>multiple</b> <b>inferences.</b> Responses with <b>multiple</b> <b>inferences</b> declined for the Low WMCL group as the narratives progressed, but this result was not significant. Another major result was that the episodic buffer measure significantly predicted inference narrowing. Secondary analyses assessed potential differences between WMCL groups in inference revision and total inferences, neither of which were significant. Discussion centers around the limitations of relying solely on the central executive concept to predict inferencing outcomes, in light of its emphasis on the flexible allocation of cognitive resources, and suggests that a measure of episodic buffer functioning may be a better predictor, at least in some cases. ...|$|R
50|$|Benjamini's {{scientific}} work combines theoretical research in statistical methodology with applied research that involves complex problems with massive data. The methodological work is on selective and simultaneous <b>inference</b> (<b>multiple</b> comparisons), {{as well as}} on general methods for data analysis, data mining, and data visualization.|$|R
40|$|The {{view that}} the returns to public {{educational}} investments are highest for early childhood interventions stems primarily from several influential randomized trials- Abecedarian, Perry, and the Early Training Project- that point to super-normal returns to preschool interventions. This paper implements a unified statistical framework to present a de novo analysis of these experiments, focusing on two core issues that have received little attention in previous analyses: treatment effect heterogeneity by gender and over-rejection of the null hypothesis due to <b>multiple</b> <b>inference.</b> The primary finding of this reanalysis is that girls garnered substantial shortand long-term benefits from the interventions. However, {{there were no significant}} long-term benefits for boys. These conclusions would not be apparent when using ”naive ” estimators that do not adjust for <b>multiple</b> <b>inference...</b>|$|R
40|$|IDP 3 is a knowledge-base system, {{offering}} a rich, {{declarative knowledge representation}} language, a range of inferences and built-in interaction with a procedural language. In this paper, we give {{an overview of the}} system and show how <b>multiple</b> <b>inferences</b> are combined to obtain state-of-the-art model generation. status: publishe...|$|R
40|$|Abstract – Sensor {{networks}} {{collect data}} upon which <b>multiple</b> <b>inferences</b> (estimations and decisions) will be based. The optimizing compression {{with respect to}} one inference may lead to suboptimal compression {{with respect to the}} others. Furthermore, not all of these inferences have the same level of importance to the end users of the network’s data. To address this, a framework is developed that uses specific distortion measures {{to assess the impact of}} compression on the multiple inferences: Fisher information is used to assess the impact on estimation accuracy while Chernoff and Kullback-Liebler distances are used to assess the impact on decision accuracy. This framework is applied to two examples: (i) multiple estimations and (ii) single estimation and a binary decision. Simulation results are provided that show that there is indeed a trade-off between these <b>multiple</b> <b>inferences</b> and that with proper a priori information the proposed data compression framework can enact trade-offs between them...|$|R
40|$|A {{post hoc}} {{analysis}} of a randomized trial comparing progression of chronic kidney disease among blacks and non-blacks provides an opportunity to explore statistical <b>inference.</b> <b>Multiple</b> comparisons without penalizing the P-value can lead to false positive results; this is illustrated using simulation. Tests of statistical interaction are then applied and interpreted to understand effect modification (or lack thereof) {{in the context of}} this trial...|$|R
40|$|One of {{the most}} {{difficult}} parts of the natural language understanding process is forming a semantic interpretation of the text. A reader must often make <b>multiple</b> <b>inferences</b> to understand the motives of actors and to causally connect actions that are unrelated on the basis of surface semantics alone. The inference process is {{complicated by the fact that}} text is of-ten ambiguous both lexically and pragmatically, and that new context often forces a reinterpretation of the input’s mean...|$|R
5000|$|Carlo Emilio Bonferroni did {{not take}} part in inventing the method {{described}} here. Holm originally called the method the [...] "sequentially rejective Bonferroni test", and it became known as Holm-Bonferroni only after some time. Holm's motives for naming his method after Bonferroni are explained in the original paper:"The use of the Boole inequality within <b>multiple</b> <b>inference</b> theory is usually called the Bonferroni technique, {{and for this reason}} we will call our test the sequentially rejective Bonferroni test." ...|$|R
40|$|This thesis {{builds a}} knowledge-based, {{computer-aided}} decision making shell, written in LISP, {{for assistance in}} generic engineering system design problems. The theoretical framework presented in the thesis places system design processes in the environment of multifaceted modelling methodology and artificial intelligence techniques. A new reliable and efficient knowledge representation scheme [...] FRASES is introduced into the knowledge base design. The scheme combines system entity structure and frame and production rule system, and allows us to easily acquire, represent, and infer knowledge and information about the system being designed. In {{the design of the}} <b>inference</b> engine, <b>multiple</b> <b>inference</b> algorithms are supported in the shell. They infer a set of desired system configurations with respect to the designer's objectives and requirements. In comparison, top-down reasoning with depth-first offers the most efficient reasoning algorithm when using the FRASES knowledge representation scheme...|$|R
40|$|In {{the past}} two decades, {{functional}} Magnetic Resonance Imaging {{has been used to}} relate neuronal network activity to cognitive processing and behaviour. Recently this approach has been augmented by algorithms that allow us to infer causal links between component populations of neuronal networks. <b>Multiple</b> <b>inference</b> procedures have been proposed to approach this research question but so far, each method has limitations when it comes to establishing whole-brain connectivity patterns. In this paper, we discuss the ways to infer causality in fMRI research. We also formulate recommendations for the future directions in this area...|$|R
40|$|This paper {{deals with}} <b>inference</b> using <b>multiple</b> patterns. <b>Inference</b> among vague information, e. g. patterns, has caused "explosion of vagueness". This paper proposes a new {{mechanism}} {{to suppress the}} increase of vagueness. This paper also proposes a modification method for pattern space based on a new concept of vagueness in patterns. Simulations are done to demonstrate the potentiality of the proposed inference method and the pattern space modification method...|$|R
40|$|Class size proponents draw {{heavily on}} the results from Project STAR to support their initiatives. Adding to the {{political}} appeal of these initiative are reports that minority and economic disadvantaged students received the largest benefits. We extend this research in two dimensions. First we con-duct a more detailed examination of the heterogeneous impacts of class size reductions on measures of cognitive and non-cognitive achievement using both conditional and unconditional quantile regression strategies. Second to address correlated outcomes from the same treatment(s) we account for over-rejection of the null hypotheses by using <b>multiple</b> <b>inference</b> procedures. We present evidence of substantial heterogeneity in the impacts of class size reductions on measures of cognitive achievement. Our evidence indicates that higher abil-ity students gain the most from class size reductions while many low ability students do not benefit from these reductions. Further, the <b>multiple</b> <b>inference</b> procedures render the few significantly differential impacts of smaller classes by race and free lunch status when the outcomes were assumed independent to appear at a frequency that one could reasonably expect due to chance. *We are grateful to Alan Krueger for providing {{a subset of the}} data used in the study. We wish to thank Caroline Hoxby and Richard Murnane for suggestions which helped improve this paper. Lehrer wishes to thank SSHRC for research support. We are responsible for all errors. 1...|$|R
40|$|The {{view that}} the returns to {{educational}} investments are highest for early childhood interventions is widely held and stems primarily from several influential randomized trials—Abecedarian, Perry, and the Early Training Project—that point to super-normal returns to early interventions. This article presents a de novo analysis of these experiments, focusing on two core issues that have received limited attention in previous analyses: treatment effect heterogeneity by gender and overrejection of the null hypothesis due to <b>multiple</b> <b>inference.</b> To address the latter issue, a statistical framework that combines summary index tests with familywise error rate and false discovery rate corrections is implemented. The first technique reduces the number of tests conducted; the latter two techniques adjust the p values for <b>multiple</b> <b>inference.</b> The primary finding of the reanalysis is that girls garnered substantial short- and long-term benefits from the interventions, {{but there were no}} significant long-term benefits for boys. These conclusions, which have appeared ambiguous when using “naive ” estimators that fail to adjust for multiple testing, contribute to a growing literature on the emerging female–male academic achievement gap. They also demonstrate that in complex studies where multiple questions are asked of the same data set, it can be important to declare the family of tests under consideration and to either consolidate measures or report adjusted and unadjusted p values...|$|R
30|$|It is {{important}} to know the number of imputations needed for a good statistical <b>inference.</b> <b>Multiple</b> imputation theorists suggest that small values of M, on the order of three to five imputations, yield excellent results (Rubin, 1987; Schafer & Olsen, 1998). Schafer (1999 a) suggests that no more than 10 imputations are usually required. Graham, Olchowski, and Gilreath (2007) recommend that researchers using multiple imputation should perform many more imputations than previously considered sufficient. They reached this conclusion after using a Monte Carlo simulation to test multiple-imputation models across several scenarios in which the fraction of missing informationh for the parameter being estimated and M were varied.|$|R
40|$|Association rule {{discovery}} in large data sets {{is vulnerable to}} producing excessive false positives, due to the <b>multiple</b> <b>inference</b> effect. Analytical results presented here indicate that Bonferonni-based solutions to this problem may have inherent limitations. Thus the paper proposes {{a new approach to}} this problem, based on simultaneous confidence intervals, computed via a novel use of the statistical bootstrap tool. The proposal here differs markedly from previous bootstrap/resampling approaches, not only in function but also in basic goal, which is to enable much more active participation by domain experts. The new method is computationally intensive, but another analytical result presented here has implications for reducing the amount of computation. ...|$|R
40|$|Urrusti-Frenk, Felix Vardy, Noam Yuchtman, and {{to various}} seminar and conference {{participants}} for helpful suggestions. We also thank Michael Anderson for generously {{providing us the}} code used in the <b>multiple</b> <b>inference</b> adjustments and the IT group at Berkeley-Haas for constructing the data-capturing application. Katherine Nguyen also provided invaluable help. The views expressed herein {{are those of the}} authors and do not necessarily reflect the views of the National Bureau of Economic Research. NBER working papers are circulated for discussion and comment purposes. They have not been peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies official NBER publications...|$|R
40|$|The task of gene {{regulatory}} network reconstruction from high-throughput data {{is receiving}} increasing attention in recent years. As a consequence, many inference methods for solving this task {{have been proposed}} in the literature. It has been recently observed, however, that no single inference method performs optimally across all datasets. It {{has also been shown}} that the integration of predictions from <b>multiple</b> <b>inference</b> methods is more robust and shows high performance across diverse datasets. Inspired by this research, in this paper, we propose a machine learning solution which learns to combine predictions from <b>multiple</b> <b>inference</b> methods. While this approach adds additional complexity to the inference pro-cess, we expect it would also carry substantial benefits. These would come from the auto-matic adaptation to patterns on the outputs of individual inference methods, so {{that it is possible to}} identify regulatory interactions more reliably when these patterns occur. This article demonstrates the benefits (in terms of accuracy of the reconstructed networks) of the proposed method, which exploits an iterative, semi-supervised ensemble-based algorithm. The algorithm learns to combine the interactions predicted by many different inference methods in the multi-view learning setting. The empirical evaluation of the proposed algo-rithm on a prokaryotic model organism (E. coli) and on a eukaryotic model organism (S. cer-evisiae) clearly shows improved performance over the state of the art methods. The results indicate that gene regulatory network reconstruction for the real datasets is more difficult for S. cerevisiae than for E. coli. The software, all the datasets used in the experiments and all the results are available for download at the following link...|$|R
40|$|GR- 2 is {{a hybrid}} {{knowledge-based}} system {{consisting of a}} Multilayer Perceptron and a rule based system for hybrid knowledge representations and reasoning. Knowledge embedded in the trained Multilayer Perceptron (MLP) is extracted {{in the form of}} general (production) rules [...] a natural format of abstract knowledge representation. The rule extraction method integrates Black-box and Open-box techniques on the MLP, obtaining feature salient and statistical properties of the training pattern set. The extracted general rules are quantified and selected in a rule validation process. <b>Multiple</b> <b>inference</b> facilities such as categorical reasoning, probabilistic reasoning and exceptional reasoning are performed in GR- 2. Experiments have shown that GR 2 is a reliable and general model for Knowledge Engineering...|$|R
40|$|This paper {{deals with}} the problem of <b>multiple</b> <b>inference</b> in {{psychiatric}} research, an issue which arises whenever a researcher has to make more than one statistical inference in a single research study. It frequently arises in psychiatric research because of multivariate study designs, with subjects being measured on more than one dependent variable with the intention of studying differences between groups in mean scores. The disadvantages of the commonly adopted strategy of using multiple univariate tests (e. g. multiple t-tests) are outlined. Two broad strategies — Bonferroni-adjusted univariate tests and multivariate statistical analysis — are introduced. Their advantages and disadvantages are discussed in terms of their usefulness in confirmatory and exploratory research in psychiatry...|$|R
40|$|A shared {{concern of}} {{knowledge}} based systems (KBS) development and traditional information system {{development is the}} choice of modeling construct most natural to the information (knowledge) structures being modeled, and to human practitioners in the modeled application area. Research in cognitive psychology has shown that human reasoning about non-trivial real world problems is a complex activity that frequently incorporates multiple reasoning strategies, concerning multiple domains of knowledge which {{may be perceived as}} having different structures. Thus the interest in hybrid knowledge representations (KR’s) and hybrid reasoning strategies which are potentially capable of capturing a greater portion of the richness of reality than single paradigm KR’s. This paper proposes the use of the “smart object paradigm ” for the design of complex knowledge based systems. The paradigm provides mechanisms for both hybrid knowledge representation and <b>multiple</b> <b>inference</b> strategies. Central to the paradigm is the concept of “smart objects. ” A smart object is an engineered artifact that combines a high level object structure with a rule based lower level language. The concept of smart objects was developed using criteria that evolved in the course of development of a large, complex knowledge based system. We present these criteria as desirable characteristics for KR’s in general, and evaluate traditional KR’s using them. The criteria are: provision for reusability of knowledge, the ability to partition and encapsulate the knowledge space, support for <b>multiple</b> <b>inference</b> strategies, and explicit meta-control of both the reasoning strategy, and the structure and control flow of the implemented system. Smart objects are shown to possess these characteristics and an overview of a prototype KBS implemented using the smart object paradigm makes their benefits concrete. 1...|$|R
40|$|Association rule {{discovery}} in large data sets {{is vulnerable to}} producing excessive false positives, due to the <b>multiple</b> <b>inference</b> effect. This paper first sets this issue in precise mathematical terms and presents some analytical results. These show that a common concern regarding effects of filtering is not as problematic as had been previously thought. The analytical results also shed new light on a recently proposed method {{for dealing with the}} problem. The paper then proposes a new method based on simultaneous confidence intervals, computed via a novel use of the statistical bootstrap tool. The proposal here differs markedly from previous bootstrap/resampling approaches, not only in function but also in basic goal, which is to enable much more active participation by domain experts. ...|$|R
40|$|Abstract Reconstructing gene {{regulatory}} networks from high-throughput data is {{a long-standing}} challenge. Through the Dialogue on Reverse Engineering Assessment and Methods (DREAM) project, {{we performed a}} comprehensive blind assessment of over 30 network inference methods on Escherichia coli, Staphylococcus aureus, Saccharomyces cerevisiae and in silico microarray data. We characterize the performance, data requirements and inherent biases of different inference approaches, and we provide guidelines for algorithm application and development. We observed that no single inference method performs optimally across all data sets. In contrast, integration of predictions from <b>multiple</b> <b>inference</b> methods shows robust and high performance across diverse data sets. We thereby constructed high-confidence networks for E. coli and S. aureus, each comprising 1, 700 transcriptional interactions at a precision of 50...|$|R
40|$|After we have {{specified}} why in {{this paper}} we will not make a distinction between the concepts of intertextuality and interdiscursivity, the author proposes to analyse this question in a socio-communicational discourse model within which first of all he will specify the basic parameters. Firstly, it is shown by an example that the interdiscursivity depends on an inference mechanism, and that the interpretation of a text demands <b>multiple</b> <b>inferences.</b> Secondly, this interdiscursivity mechanism is described through a triangular interconnectedness of "I – third person – You" around shared knowledge. This leads the author to the definition of "socio discursive imaginary". Finally, this interdiscursivity mechanism that rests on the socio-discursive imaginary is demonstrated on the basis of several examples taken from media discourse...|$|R
40|$|This paper {{provides}} a connectionist {{account of the}} processes underlying the <b>multiple</b> <b>inference</b> model of person impression formation proposed by Reeder, Kumar, Hesson-McInnis and Trafimow (2002). First, in a replication and extension {{of one of their}} main studies, I found evidence for discounting of trait inferences when facilitating situational forces were present consistent with earlier causality-based theories, {{while at the same time}} I replicated the lack of discounting in moral inferences as documented and predicted by Reeder et al. (2002). Second, to provide an account of how these different and sometimes contradictory inferences are formed and integrated in a coherent person impression, I present a recurrent network model that automatically integrates these inferences, resulting in a pattern that closely reproduces the observed data. 1...|$|R
40|$|Theories of analogical {{reasoning}} {{have assumed that}} a 1 -to- 1 constraint discourages reasoners from mapping a single element in 1 analog to multiple elements in another. Empirical evidence suggests that reasoners sometimes appear to violate the 1 -to- 1 constraint when asked to generate mappings, yet virtually never violate it when asked to generate analogical inferences. However, {{few studies have examined}} analogical inferences based on nonisomorphic analogs, and their conclusions are suspect due to methodological problems. We sought to elicit mixed inferences that could result from combining information from 2 possible mappings. Participants generated 2 -to- 1 correspondences when asked for explicit mappings, but did not produce mixed <b>inferences.</b> <b>Multiple</b> correspondences appear to arise from multiple isomorphic mappings, rather than from a single homomorphic mapping...|$|R
40|$|This paper {{describes}} {{efforts to}} provide access to the free text in biomedical databases. The focus of the effort is the development of SPECIALIST, an experimental natural language processing system for the biomedical domain. The system includes a broad coverage parser supported by a large lexicon, modules that provide access to the extensive Unified Medical Language System (UMLS) Knowledge Sources, and a retrieval module that permits experiments in information retrieval. The UMLS Metathesaurus and Semantic Network provide a rich source of biomedical concepts and their interrelationships. Investigations have been conducted to determine the type of information required to effect a map between the language of queries and the language of relevant documents. Mappings are never straightforward and often involve <b>multiple</b> <b>inferences...</b>|$|R
50|$|Statistical methodologists {{have argued}} that QCA's strong {{assumptions}} render its findings both fragile and prone to type I error. Simon Hug argues that deterministic hypotheses and error-free measures are exceedingly rare in social science and uses Monte Carlo simulations to demonstrate the fragility of QCA results if either assumption is violated. Chris Krogslund, Donghyun Danny Choi, and Mathias Poertner further demonstrate that QCA results are highly sensitive to minor parametric and model-susceptibility changes and are vulnerable to type I error. Bear F. Braumoeller further explores {{the vulnerability of the}} QCA family of techniques to both type I error and <b>multiple</b> <b>inference.</b> Braumoeller also offers a formal test of the null hypothesis and demonstrates that even very convincing QCA findings {{may be the result of}} chance.|$|R
40|$|We provide novel {{methods for}} {{inference}} on quantile treatment effects in both uncon-ditional and conditional (nonparametric) settings. These methods achieve high-order accuracy {{by using the}} probability integral transform and a Dirichlet (rather than Gaus-sian) reference distribution. We propose related methods for joint <b>inference</b> on <b>multiple</b> quantiles and <b>inference</b> on linear combinations of quantiles, again in both unconditional and conditional settings. Optimal bandwidth and coverage probability rates are derived for all methods, and code is provided...|$|R
40|$|We suggest solving fuzzy {{mathematical}} programming problems via {{the use of}} multiple fuzzy reasoning techniques. We show that our approach gives Buckley's solution [1] to possibilistic mathematical programs when the inequality relations are understood in possibilistic sense. Keywords: Compositional rule of <b>inference,</b> <b>multiple</b> fuzzy reasoning, fuzzy {{mathematical programming}}, possibilistic mathematical programming, fuzzy implication 1 Preliminaries Yager [4] introduced an approach to making inferences in a knowledge-based system which uses mathematical programming techniques. In this paper we apply multiple fuzzy reasoning (MFR) techniques to solve mathematical programming problems with fuzzy parameters. We show that our approach yields Buckley's solution [1] to possibilistic mathematical programs, when the inequality relations are understood in possibilistic sense. We shall use the following inference rules: The compositional rule of inference Antecedent 1 : x and y have property W Fact: [...] ...|$|R
40|$|We define an {{intelligent}} textbook of medicine to be {{a computer system}} that: (1) provides for storage and selective retrieval of synthesized clinical knowledge for reference purposes; and (2) supports the application by computer of its knowledge to patient information to assist physicians with decision making. This paper describes an experimental system called KMS (a Knowledge Management System) for creating and using intelligent medical textbooks. KMS is domain-independent, supports <b>multiple</b> <b>inference</b> methods and representation languages, and is designed for direct use by physicians during the knowledge acquisition process. It is presented here {{in the context of}} the development of an Intelligent Textbook of Neurology. We suggest that KMS has the potential to overcome some of the problems that have inhibited the use of knowledge-based systems by physicians in the past...|$|R
40|$|Traditional Machine Learning {{approaches}} {{are based on}} single inference mechanisms. A step forward concerned the integration of <b>multiple</b> <b>inference</b> strategies within a first-order logic learning framework, {{taking advantage of the}} benefits that each approach can bring. Specifically, abduction is exploited to complete the incoming information in order to handle cases of missing knowledge, and abstraction is exploited to eliminate superfluous details that can affect the performance of a learning system. However, these methods require some background information to exploit the specific inference strategy, that must be provided by a domain expert. This work proposes algorithms to automatically discover such an information {{in order to make the}} learning task completely autonomous. The proposed methods have been tested on the system INTHELEX, and their effectiveness has been proven by experiments in a real-world domain...|$|R
40|$|Reconstructing gene {{regulatory}} networks from high-throughput data is {{a long-standing}} challenge. Through the Dialogue on Reverse Engineering Assessment and Methods (DREAM) project, {{we performed a}} comprehensive blind assessment of over 30 network inference methods on Escherichia coli, Staphylococcus aureus, Saccharomyces cerevisiae and in silico microarray data. We characterize the performance, data requirements and inherent biases of different inference approaches, and we provide guidelines for algorithm application and development. We observed that no single inference method performs optimally across all data sets. In contrast, integration of predictions from <b>multiple</b> <b>inference</b> methods shows robust and high performance across diverse data sets. We thereby constructed high-confidence networks for E. coli and S. aureus, each comprising ~ 1, 700 transcriptional interactions at a precision of ~ 50 %. We experimentally tested 53 previousl...|$|R
40|$|Monostrategy {{classification}} {{systems are}} very limited {{in the type}} of knowledge they can use for decision making. On the other hand, potentially better results are achievable using multistrategy systems that integrate two or more types of knowledge representation and/or <b>multiple</b> <b>inference</b> underlying a decision process. In this paper a decision tree and a neural network technique for competitive integration of heterogeneous local experts are proposed. The local experts are either symbolic rule-based classifiers or neural network based monostrategy learning systems. The integration is simple, as it involves no modification of existing symbolic components. The proposed competitive integration systems are tested versus a previously used cooperative neural network based approach. The experimental results on a small financial advising problem indicate significant performance improvements when using the neural network based competitive integration approach as compared to the results obtained f [...] ...|$|R
40|$|Medical {{diagnosis}} is an outcome got from {{a complex process}} that implies <b>multiple</b> <b>inferences</b> from sets of clinical information. A large body of {{research has been conducted}} to develop tools in order to aid analyzing the amount of clinical information. In this paper, a system- called CaseServer- is presented. It supports radiology teaching and research practices. The initial intention was to create an electronic and MIRC-compliant infrastructure to provide integrated and context-based access to all relevant patient data at the time of image interpretation. A successful implementation has integrated information collected from various sources. Furthermore, the application of role-based coupled with context-based principles to acquisition, organization, control, dissemination and use of clinical cases in teaching and research has brought out a multipurpose infrastructure which promotes and makes feasible the usage of clinical cases within academic and research communities...|$|R
40|$|An {{architecture}} for object {{modeling and}} recognition for an autonomous land vehicle is presented. Examples of objects of interest include terrain features, fields, roads, horizon features, trees, etc. The architecture is organized around {{a set of}} data bases for generic object models and perceptual structures, temporary memory for the instantiation of object and relational hypotheses, and a long term memory for storing stable hypotheses that are affixed to the terrain representation. <b>Multiple</b> <b>inference</b> processes operate over these databases. Researchers describe these particular components: the perceptual structure database, the grouping processes that operate over this, schemas, and the long term terrain database. A processing example that matches predictions from the long term terrain model to imagery, extracts significant perceptual structures for consideration as potential landmarks, and extracts a relational structure to update the long term terrain database is given...|$|R
