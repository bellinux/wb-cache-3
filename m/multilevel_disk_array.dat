0|660|Public
50|$|For example, one of {{the checks}} in the risk {{assessment}} report lists volumes whose mirrors are composed of logical units from the same <b>disk</b> <b>array.</b> Logical volumes that are mirrored have higher availability when mirrored across separate <b>disk</b> <b>array</b> controllers. Mirroring logical volumes across separate <b>disk</b> <b>array</b> controllers allows the logical volumes to continue to operate should one <b>disk</b> <b>array</b> fail.|$|R
40|$|Abstract—The {{performance}} {{modeling and}} analysis of <b>disk</b> <b>arrays</b> is challenging due {{to the presence of}} multiple <b>disks,</b> large <b>array</b> caches, and sophisticated array controllers. Moreover, storage manufacturers may not reveal the internal algorithms implemented in their devices, so real <b>disk</b> <b>arrays</b> are effectively black-boxes. We use standard performance techniques to develop an integrated performance model that incorporates some of the complexities of real <b>disk</b> <b>arrays.</b> We show how measurement data and baseline performance models can be used to extract information about the various features implemented in a <b>disk</b> <b>array.</b> In this process, we identify areas for future research in the performance analysis of real <b>disk</b> <b>arrays.</b> Index Terms—RAID, analytical performance model, array cache, parallel I/O, enterprise storage systems, I/O performance evaluation, <b>disk</b> <b>array...</b>|$|R
40|$|The {{performance}} {{modeling and}} analysis of <b>disk</b> <b>arrays</b> is challenging due {{to the presence of}} multiple <b>disks,</b> large <b>array</b> caches, and sophisticated array controllers. Moreover, storage manufacturers may not reveal the internal algorithms implemented in their devices, so real <b>disk</b> <b>arrays</b> are effectively black-boxes. We use standard performance techniques to develop an integrated performance model that incorporates some of the complexities of real <b>disk</b> <b>arrays.</b> We show how measurement data and baseline performance models can be used to extract information about the various features implemented in a <b>disk</b> <b>array.</b> In this process, we identify areas for future research in the performance analysis of real <b>disk</b> <b>arrays...</b>|$|R
50|$|Since {{the crucial}} {{function}} of a file server is storage, technology has been developed to operate multiple disk drives together as a team, forming a <b>disk</b> <b>array.</b> A <b>disk</b> <b>array</b> typically has cache (temporary memory storage that is faster than the magnetic disks), as well as advanced functions like RAID and storage virtualization. Typically <b>disk</b> <b>arrays</b> increase level of availability by using redundant components other than RAID, such as power supplies. <b>Disk</b> <b>arrays</b> may be consolidated or virtualized in a SAN.|$|R
40|$|In 1989, the RAID {{group at}} U. C. Berkeley built a {{prototype}} <b>disk</b> <b>array</b> called RAID-I. The bandwidth achieved by RAID-I was severely {{limited by the}} memory system bandwidth limitations of the <b>disk</b> <b>array's</b> host workstation. As a result, most of the bandwidth available from the disks could not be delivered to clients of the <b>disk</b> <b>array</b> #le server. We designed our second prototype, RAID-II, to deliver {{as much of the}} <b>disk</b> <b>array</b> bandwidth as possible to #le server clients. A custom-built circuit-board <b>disk</b> <b>array</b> controller, called the XBUS board, connects the disks and the high-speed network directly, allowing data for large requests to bypass the server workstation. A single workstation may control several XBUS boards for increased bandwidth...|$|R
40|$|Declustered data {{organizations}} in <b>disk</b> <b>arrays</b> (RAIDs) achieve less-intrusive {{reconstruction of data}} after a disk failure. We present PDDL, a new data layout for declustered <b>disk</b> <b>arrays.</b> PDDL layouts exist for a large variety of <b>disk</b> <b>array</b> configurations with a distributed spare disk. PDDL declustered <b>disk</b> <b>arrays</b> have excellent run-time performance under light and heavy workloads. PDDL maximizes access parallelism in the most critical circumstances, namely during reconstruction of data on the spare disk. PDDL occurs minimum address translation overhead compared to all other proposed declustering layouts. ...|$|R
40|$|Thesis {{focuses on}} <b>disk</b> <b>arrays,</b> where {{the goal is}} to design test {{scenarios}} to measure performance of <b>disk</b> <b>array</b> and use predictive analytics tools to train a model that will predict the selected performance parameter on a measured set of data. The implemented web application demonstrates the functionality of the trained model and shows estimate of the <b>disk</b> <b>array</b> performance...|$|R
50|$|Of course, {{the overall}} {{performance}} of a system is not only relevant {{to the performance of}} host and network, but also influenced by the performance of the disk constituting file system. So, BWFS file system can be structured by the LUN from multiple <b>disk</b> <b>arrays.</b> It equals to another layer of RAID structured between multiple <b>disk</b> <b>arrays,</b> which maximizes the performance of <b>disk</b> <b>arrays.</b>|$|R
40|$|As <b>disk</b> <b>arrays</b> {{become widely}} used, tools for {{understanding}} and analyzing their performance become increasingly important. In particular, performance models can be invaluable in both configuring and designing <b>disk</b> <b>arrays.</b> Accurate analytic performance models are desirable over {{other types of}} models {{because they can be}} quickly evaluated, are applicable under a wide range of system and workload parameters, and can be manipulated by a range of mathematical techniques. Unfortunately, analytical performance models of <b>disk</b> <b>arrays</b> are difficult to formulate due to the presence of queuing and fork-join synchronization; a <b>disk</b> <b>array</b> request is broken up into independent disk requests which must all complete to satisfy the original request. We develop, validate, and apply an analytic performance model for <b>disk</b> <b>arrays.</b> We derive simple equations for approximating their utilization, response time, and throughput. We then validate the analytic model via simulation and investigate the accuracy of each approximation used in deriving the analytical model. Finally, we apply the analytical model to derive an equation for the optimal unit of data striping in <b>disk</b> <b>arrays...</b>|$|R
50|$|In {{a modern}} {{enterprise}} architecture <b>disk</b> <b>array</b> controllers (sometimes also called storage processors, or SPs) {{are parts of}} physically independent enclosures, such as <b>disk</b> <b>arrays</b> placed in a storage area network (SAN) or network-attached storage (NAS) servers.|$|R
40|$|A <b>disk</b> <b>array</b> is a {{collection}} of physically small magnetic disks that is packaged as a single unit but operates in parallel. <b>Disk</b> <b>arrays</b> capitalize on the availability of small-diameter disks from a price-competitive market to provide the cost, volume, and capacity of current disk systems but many times their performance. Unfortunately, relative to current disk systems, the larger number of components in <b>disk</b> <b>arrays</b> leads to higher rates of failure. To tolerate failures, redundant <b>disk</b> <b>arrays</b> devote a fraction of their capacity to an encoding of their information. This redundant information enables the contents of a failed disk to be recovered from the contents of non-failed disks. The simplest and least expensive encoding for this redundancy, known as N+ 1 parity is highlighted. In addition to compensating for the higher failure rates of <b>disk</b> <b>arrays,</b> redundancy allows highly reliable secondary storage systems to be built much more cost-effectively than is now achieved in conventional duplicated <b>disks.</b> <b>Disk</b> <b>arrays</b> that combine redundancy with the parallelism of many small-diameter disks are often called Redundant <b>Arrays</b> of Inexpensive <b>Disks</b> (RAID). This combination promises improvements to both the performance and the reliability of secondary storage. For example, IBM's premier disk product, the IBM 3390, is compared to a redundant <b>disk</b> <b>array</b> constructed of 84 IBM 0661 3 1 / 2 -inch disks. The redundant <b>disk</b> <b>array</b> has comparable or superior values for each of the metrics given and appears likely to cost less. In the first section of this tutorial, I explain how <b>disk</b> <b>arrays</b> exploit the emergence of high performance, small magnetic disks to provide cost-effective disk parallelism that combats the access and transfer gap problems. The flexibility of disk-array configurations benefits manufacturer and consumer alike. In contrast, I describe in this tutorial's second half how parallelism, achieved through increasing numbers of components, causes overall failure rates to rise. Redundant <b>disk</b> <b>arrays</b> overcome this threat to data reliability by ensuring that data remains available during and after component failures...|$|R
40|$|All server storage environments {{depend on}} <b>disk</b> <b>arrays</b> {{to satisfy their}} capacity, reliability, and {{availability}} requirements. In order to manage these storage systems efficiently, {{it is necessary to}} understand the behavior of <b>disk</b> <b>arrays</b> and predict their performance. We develop an analytical model that estimates mean performance measures of <b>disk</b> <b>arrays</b> under a synchronous I/O workload. Synchronous I/O requests are generated by jobs that each block while their request is serviced. Upon I/O service completion, a job may use other computer resources before issuing another I/O request. Our <b>disk</b> <b>array</b> model considers the effect of workload sequentiality, read-ahead caching, write-back caching, and other complex optimizations incorporated into most <b>disk</b> <b>arrays.</b> The model is validated against a mid-range disk-array for a variety of synthetic I/O workloads. The model is computationally simple and scales easily as the number of jobs issuing requests increases, making it potentially useful to performance engineers...|$|R
40|$|In this paper, {{we examine}} how <b>disk</b> <b>arrays</b> and shared memory multiprocessors {{lead to an}} {{effective}} method for constructing database machines for general-purpose complex query processing. We show that <b>disk</b> <b>arrays</b> can lead to cost-effective storage systems if they are configured from suitably small formfactor disk drives. We introduce the storage system metric data temperature {{as a way to}} evaluate how well a disk configuration can sustain its workload, and we show that <b>disk</b> <b>arrays</b> can sustain the same data temperature as a more expensive mirrored-disk configuration. We use the metric to evaluate the performance of <b>disk</b> <b>arrays</b> in XPRS, an operational shared-memory multiprocessor database system being developed at the University of California, Berkeley...|$|R
40|$|In this paper, {{we propose}} a highly {{reliable}} <b>disk</b> <b>array</b> architecture called Dual-Crosshatch <b>Disk</b> <b>Array</b> (DCDA). We {{have proposed a}} low overhead, triple-erasure correcting parity organization called interleaved 2 d-parity scheme. This parity scheme is {{used as the basis}} for the design of the DCDA architecture. DCDA uses a hybrid approach of RAID- 4 and RAID 5 with one dedicated parity group and another parity group with block-interleaved data and parity. This architecture has very high reliability with low overheads, good degradedmode performance, and acceptable normal-mode performance. The results obtained from simulations indicate that DCDA is about 10 4 times more reliable than the best existing <b>disk</b> <b>array</b> architecture for the parameters considered in this paper. The average response time and throughput of DCDA is better than that of the RAID- 5 organization. Index terms: <b>Disk</b> <b>Arrays,</b> Dual Crosshatch <b>Disk</b> <b>Array,</b> Interleaved 2 d-Parity, RAID, Reliability. 1 A preliminary version of this p [...] ...|$|R
50|$|Typically a <b>disk</b> <b>array</b> {{provides}} increased availability, resiliency, and maintainability {{by using}} existing components (controllers, power supplies, fans, etc.), often {{up to the}} point where all single points of failure (SPOFs) are eliminated from the design. Additionally, <b>disk</b> <b>array</b> components are often hot-swappable.|$|R
40|$|A Video-on-Demand (VOD) server {{needs to}} store {{hundreds}} of movie titles {{and to support}} thousands of concurrent accesses. We define access profile {{as the number of}} concurrent accesses to each movie title that should be supported by a VOD server. A current highend magnetic <b>disk</b> <b>array</b> (<b>disk)</b> can only support tens of MPEG- 2 concurrent accesses, and it is necessary to replicate and/or stripe the hot movie files over multiple <b>disk</b> <b>arrays.</b> How to replicate, stripe, and place the movie files over a minimum number of magnetic <b>disk</b> <b>arrays</b> such that a given access profile can be supported is an important problem. In this paper, we formulate and solve this problem. The result of this study {{can be applied to the}} design of the storage subsystem of a VOD server to economically minimize the cost or to maximize the utilization of <b>disk</b> <b>arrays.</b> 1. Introduction A central design issue of providing VOD services is how to organize and store hundreds of movie files over multiple <b>disks</b> (<b>disk</b> <b>arrays)</b> such that [...] ...|$|R
5000|$|Clariion (styled CLARiiON) is a {{discontinued}} [...] SAN <b>disk</b> <b>array</b> manufactured {{and sold}} by EMC Corporation, it occupied the entry-level and mid-range of EMC's SAN <b>disk</b> <b>array</b> products. In 2011, EMC introduced the EMC VNX Series, designed to replace both the Clariion and Celerra products.|$|R
50|$|Protocol analyzers {{can also}} be hardware-based, either in probe format or, as is {{increasingly}} common, combined with a <b>disk</b> <b>array.</b> These devices record packets (or {{a slice of the}} packet) to a <b>disk</b> <b>array.</b> This allows historical forensic analysis of packets without users having to recreate any fault.|$|R
40|$|Provisioning {{storage in}} <b>disk</b> <b>arrays</b> is a {{difficult}} problem because many applications with different workload characteristics and priorities share resources provided by the array. Currently, storage in <b>disk</b> <b>arrays</b> is statically partitioned, leading to difficult choices between over-provisioning to meet peak demands and resource sharing to meet efficiency targets. In this paper, we present Maestro, a feedback controller that can manage resources on large <b>disk</b> <b>arrays</b> to provide performance differentiation among multiple applications. Maestro monitors the performance of each application and dynamically allocates the array resources so that diverse performance requirements can be met without static partitioning. It supports multiple performance metrics (e. g., latency and throughput) and application priorities so that important applications receive better performance in case of resource contention. By ensuring that high-priority applications sharing storage with other applications obtain the performance levels they require, Maestro {{makes it possible to}} use storage resources efficiently. We evaluate Maestro using both synthetic and real-world workloads on a large, commercial <b>disk</b> <b>array.</b> Our experiments indicate that Maestro can reliably adjust the allocation of <b>disk</b> <b>array</b> resources to achieve application performance targets. 1...|$|R
5000|$|... #Caption: SPARCserver 1000 and SPARC Storage <b>Array</b> <b>disk</b> <b>array</b> ...|$|R
50|$|The AX {{series is}} {{considered}} the entry-level <b>disk</b> <b>array.</b>|$|R
50|$|The Hitachi Universal Storage Platform {{was first}} {{introduced}} in 2004. An entry level enterprise and high-end midrange model, the Network Storage Controller was introduced in 2005. The Universal Storage Platform {{was one of the}} first <b>disk</b> <b>arrays</b> to virtualize other <b>disk</b> <b>arrays</b> in the appliance instead of in the network.|$|R
50|$|A <b>disk</b> <b>array</b> {{controller}} provides front-end interfaces and back-end interfaces.|$|R
40|$|The {{excellent}} reliability {{provided by}} RAID Level 5 data organization {{has been seen}} to be insufficient for future mass storage systems. We analyze the multi-dimensional <b>disk</b> <b>array</b> {{in search of the}} necessary improved reliability. The paper begins by introducing multi-dimensional <b>disk</b> <b>array</b> data organization schemes based on maximum distance separable error correcting codes and incorporating both strings and spares. Several figures of merit are calculated using a standard Markov failure and repair model for these organizations. Based on our results, the multi-dimensional <b>disk</b> <b>array</b> organization is an excellent approach to providing improved reliability. 1 Introduction <b>Disk</b> <b>array</b> storage systems, especially those with redundant <b>arrays</b> of independent <b>disks</b> (RAID) Level 5 data organization [6], provide excellent cost, run-time performance as well as reliability and will meet the needs of computing systems for the immediate future. Computing systems, especially those with massive storage requ [...] ...|$|R
40|$|In 1989, the RAID (Redundant <b>Arrays</b> of Inexpensive <b>Disks)</b> {{group at}} U. C. Berkeley built a {{prototype}} <b>disk</b> <b>array</b> called RAID-I. The bandwidth delivered to clients by RAID-I was severely {{limited by the}} memory system bandwidth of the <b>disk</b> <b>array's</b> host workstation. We designed our second prototype, RAID-II, to deliver more of the <b>disk</b> <b>array</b> bandwidth to file server clients. A custom-built crossbar memory system called the XBUS board connects the disks directly to the high-speed network, allowing data for large requests to bypass the server workstation. RAID-II runs Log-Structured File System (LFS) software to optimize performance for bandwidth-intensive applications. Th...|$|R
50|$|Servers {{commonly}} have a backplane {{to attach}} hot swappable hard drives; backplane pins pass directly into hard drive sockets without cables. They may have single connector to connect one <b>disk</b> <b>array</b> controller or multiple connectors {{that can be}} connected {{to one or more}} controllers in arbitrary way. Backplanes are commonly found in <b>disk</b> enclosures, <b>disk</b> <b>arrays,</b> and servers.|$|R
40|$|A {{variety of}} performance-enhancing techniques, such as striping, mirroring, and {{rotational}} data replication, {{exist in the}} <b>disk</b> <b>array</b> literature. Given a fixed budget of disks, one must intelligently choose what combination of these techniques to employ. In this paper, we present a way of designing <b>disk</b> <b>arrays</b> that can flexibly and systematically reduce seek and rotational delay in a balanced manner. We give analytical models that can guide an array designer towards optimal configurations by considering both disk and workload characteristics. We have implemented a prototype <b>disk</b> <b>array</b> that incorporates the configuration models. In the process, we have also developed a robust disk head position prediction mechanism without any hardware support. The resulting prototype demonstrates the e#ectiveness of the configuration models. 1 Introduction In this paper, {{we set out to}} answer a simple question: how do we systematically increase the performance of a <b>disk</b> <b>array</b> by adding more disks? Thi [...] ...|$|R
40|$|Abstract—Disk {{scrubbing}} periodically {{scans the}} contents of a <b>disk</b> <b>array</b> to detect the presence of irrecoverable read errors and reconstitute {{the contents of}} the lost blocks using the builtin redundancy of the <b>disk</b> <b>array.</b> We address the issue of scheduling scrubbing runs in <b>disk</b> <b>arrays</b> that can tolerate two disk failures without incurring a data loss, and propose to start an urgent scrubbing run of the whole <b>array</b> whenever a <b>disk</b> failure is detected. Used alone or in combination with periodic scrubbing runs, these expedited runs can improve the mean time to data loss of <b>disk</b> <b>arrays</b> over a wide range of disk repair times. As a result, our technique eliminates the need for frequent scrubbing runs and the need to maintain spare disks and personnel on site to replace failed disks within a twentyfour hour interval. Keywords-irrecoverable read errors; RAID arrays; disk scrubbing. I...|$|R
40|$|Enterprise storage systems {{depend on}} <b>disk</b> <b>arrays</b> for their {{capacity}} and availability needs. To design and maintain storage systems that efficiently satisfy evolving requirements, {{it is critical}} to be able to evaluate configuration alternatives without having to physically implement them. In this paper, we describe an analytical model to predict <b>disk</b> <b>array</b> throughput, based on a hierarchical decomposition of the internal array architecture. We validate the model against a state-of-the-art <b>disk</b> <b>array</b> for a variety of synthetic workloads and array configurations. To our knowledge, no previously published analytical model has either incorporated the combined effects of the complex optimizations present in modern <b>disk</b> <b>arrays,</b> or been validated against a real, commercial array. Our results are quite encouraging for an analytical model: predictions are accurate in most cases within 32 % of the observed array performance (15 % on the average) for our set of experiments. ...|$|R
40|$|<b>Disk</b> <b>arrays</b> {{have been}} {{designed}} with two competing goals in mind, the ability to reconstruct erased disks (reliability), and {{the speed with which}} information can be read, written, and reconstructed (performance). The substantial loss in performance of write operations as reliability requirements increase has resulted in an emphasis on performance at the expense of reliability. This has proved acceptable given the relatively small numbers of disks in current <b>disk</b> <b>arrays.</b> We develop a method for improving the performance of write operations in <b>disk</b> <b>arrays</b> capable of correcting any double erasure, by ordering the columns of the erasure code to minimize the amount of parity information that requires updating. For large <b>disk</b> <b>arrays,</b> this affords a method to support the reliability needed without the generally accepted loss of performance. Regular or Revue? Regular. Ordering Disks for Double Erasure Codes Myra B. Cohen and Charles J. Colbourn Computer Science, University of [...] ...|$|R
5000|$|<b>Disk</b> <b>array</b> {{controller}}, {{also known}} as RAID controller, a type of storage controller ...|$|R
5000|$|... 9337 <b>Disk</b> <b>Array</b> Subsystem {{used the}} IBM 0662 (Spitfire) or 0663 (Corsair) HDDs.|$|R
5000|$|... 2002 - StorageTek {{introduces}} BladeStore, a <b>disk</b> <b>array</b> {{based on}} ATA disk technology.|$|R
40|$|Abstract- We {{have studied}} the {{magnetization}} dynamics of <b>disk</b> <b>array</b> La 0. 7 Sr 0. 3 MnO 3 ferromagnetic by means of timeresolved micromagnetic simulation. The coercivity field (Hc) is assumed depend on the interdisk distance. Meanwhile, the distribution of C-states of <b>disk</b> <b>array,</b> in the along of the same diagonal, exhibits an opposite state which is called positive C-states and negative C-states. Along of the same diagonal, these two states make the configurations of S-states. Instead, the demagnetization energy dominates in the magnetization energy systems. Index Term [...] <b>Disk</b> <b>array,</b> LSMO (La 0. 7 Sr 0. 3 MnO 3) ferromagnetic, coercivity field (H c), demagnetization energy, dynamics micromagnetic. I...|$|R
50|$|The Digital {{video is}} stored on a {{storage area network}} (SAN) hard <b>disk</b> <b>array.</b>|$|R
50|$|Thinking Machines also {{introduced}} an early commercial RAID2 <b>disk</b> <b>array,</b> the DataVault, circa 1988.|$|R
