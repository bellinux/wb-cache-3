18|8|Public
5000|$|The <b>meta-problem</b> here is {{that the}} {{configuration}} wizard does all the approved rituals (GUI with standardized clicky buttons, help popping up in a browser, etc. etc.) but doesn't have the central attribute these are supposed to achieve: discoverability. That is, the quality that every point in the interface has prompts and actions attached to it from which you can learn what to do next. Does your project have this quality? ...|$|E
40|$|I {{argue that}} we must {{distinguish}} between: (0) the Three-Doors-Problem Problem [sic], which is {{to make sense of}} some real world question of a real person. (1) a large number of solutions to this <b>meta-problem,</b> i. e., many specific Three-Doors-Problem problems, which are competing mathematizations of the <b>meta-problem</b> (0). Each of the solutions at level (1) can well have a number of different solutions: nice ones and ugly ones; correct ones and incorrect ones. I discuss three level (1) solutions, i. e., three different Monty Hall problems; and try to give three short correct and attractive solutions. These are: an unconditional probability question; a conditional probability question; and a game-theory question. The meta-message of the article is that applied statisticians should beware of solution-driven science. Comment: Submitted to Springer Lexicon of Statistics. Version 2 : some minor improvement...|$|E
40|$|In {{this paper}} we explore the use of Probabilistic Latent Semantic Analysis (PLSA) as a method for {{quantifying}} semantic differences between land cover classes. The results are promising, revealing ‘hidden’ or not easily discernible data concepts. PLSA provides a ‘bottom up’ approach to interoperability problems for users {{in the face of}} ‘top down’ solutions provided by formal ontologies. We note the potential for a <b>meta-problem</b> of how to interpret the concepts and the need for further research to reconcile the top-down and bottom-up approaches...|$|E
40|$|Sustainability is a {{normative}} topic framed by disciplinary perspectives. This can be problematic as {{the tools that}} are used and applied to <b>meta-problems</b> and ‘grand challenges’ associated with societal (un) sustainability, and which may result in proposed ‘sustainable solutions’, are framed {{through the lens of}} the ‘object world’ disciplinarian. Traditional engineering education and practice has tended to frame problems in narrow techno-economic terms, often neglecting broader social, environmental, ethical and political issues; or what might be termed the social complexities of problems (Bucciarelli, 2008; Mulder et al., 2012). This reductionist approach has sought to close down risk and uncertainty through deterministic modelling and design, resulting in frameworks/models which provide an air of misplaced confidence but which are incapable of accounting for (or recognising) unknowability, and can thus lead to behaviour which ironically, results in increased fragility, rather than promoting increased robustness or resilience. Researchers in the social sciences and humanities are inherently more comfortable and adept with dealing with complexity, uncertainty and unknowability. This paper is posited in this context, whereby chemical engineering and sociology students taking respective disciplinary sustainability/environmental modules were brought together to work on a common assignment dealing with some aspect of sustainability. This paper reflects on this collaborative exercise, including the experiences of the students themselves, alongside some challenges and successes. It concludes that transdisciplinary approaches to learning are not just desirable in addressing wicked and <b>meta-problems</b> when addressing challenges of (un) sustainability, but represent a sine qua non for building the social capacity in confronting these issues...|$|R
40|$|Computer network {{technologies}} and strategic management methodologies have undoubtedly {{had a major}} impact {{in the way people}} communicate, exchange information and participate in attractive e-commerce and e-business opportunities. The small and medium sized businesses, by pursuing their ambitious plans to grow into major e-commerce players and in their continuous effort to take advantage of these new rapidly expanded {{technologies and}} methodologies, have to consider carefully certain related critical factors such as presentation time of new products, physical location of the enterprise, effective channels of distribution, niche marketing et al. Certain key issues concerning strategic management methodologies of e-businesses in the case of small and medium sized publishing firms are examined. The concepts of <b>meta-problems,</b> meta-computing and meta-strategies are considered and their application and solution to e-business is briefly discussed. 1...|$|R
40|$|Economic and {{political}} demands are driving computational investigation of {{systems and processes}} like never before. It is foreseen that questions of safety, optimality, risk, robustness, likelihood, credibility, etc. will increasingly be posed to computational modelers. This will require the development and routine use of computing infrastructure that incorporates computational physics models {{within the framework of}} larger meta-analyses involving aspects of optimization, nondeterministic analysis, and probabilistic risk assessment. This paper describes elements of an ongoing case study involving the computational solution of several <b>meta-problems</b> in optimization, nondeterministic analysis, and optimization under uncertainty pertaining to the surety of a generic weapon safing device. The goal of the analyses is to determine the worst-case heating configuration in a fire that most severely threatens the integrity of the device. A large, 3 -D, nonlinear, finite element thermal model is used to determine the transient thermal response of the device in this coupled conduction/radiation problem. Implications of some of the numerical aspects of the thermal model on the selection of suitable and efficient optimization and nondeterministic analysis algorithms are discussed...|$|R
40|$|Abstract—We {{classify}} {{completely the}} complexity of evaluating positive equality-free sentences of first-order logic over a fixed, finite structure D. This problem {{may be seen as}} a natural generalisation of the quantified constraint satisfaction problem QCSP(D). We obtain a tetrachotomy for arbitrary finite struc-tures: each problem is either in L, is NP-complete, is co-NP-complete or is Pspace-complete. Moreover, its complexity is characterised algebraically in terms of {{the presence or absence of}} specific surjective hyper-endomorphisms; and, logically, in terms of relativisation properties with respect to positive equality-free sentences. We prove that the <b>meta-problem,</b> to establish for a specific D into which of the four classes the related problem lies, is NP-hard...|$|E
40|$|Abstract. Constraint Satisfaction Problems (CSP) {{constitute}} a convenient way to capture many combinatorial problems. The general CSP {{is known to}} be NP-complete, but its complexity depends on a parameter, usually a set of relations, upon which they are constructed. Following the parameter, there exist tractable and intractable instances of CSPs. In this paper we show a dichotomy theorem for every finite domain of CSP including also disjunctions. This dichotomy condition is based on a simple condition, allowing us to classify monotone CSPs as tractable or NP-complete. We also prove that the <b>meta-problem,</b> verifying the tractabil-ity condition for monotone constraint satisfaction problems, is fixed-parameter tractable. Moreover, we present a polynomial-time algorithm to answer this ques-tion for monotone CSPs over ternary domains. ...|$|E
40|$|Radiation therapy uses {{ionizing}} radiation to treat cancerous tumors. This paper reports our {{experiences with the}} parallelization of a real-world 3 -D radiation therapy treatment planning (RTTP) system {{on a wide range}} of platforms, including SMP servers, Cray J 916 vector machines, and clusters of SMPs. The RTTP system is a <b>meta-problem,</b> comprising two major loosely-coupled components: dose calculation and genetic optimization. To accelerate the planning process, we integrated the two components and parallelized the system {{on a wide range of}} platforms using vendors' native optimization tools and Stanford SUIF parallelizing compiler. For comparison, we also manually parallelized the system using multithreading, messagepassing, and distributed shared memory tools. The experimental results showed that none of the automatic parallelization tools were capable of handling the real-world application, although plenty of parallelism exists in the codes. The difficulty is due to RTTP's dynamic and [...] ...|$|E
40|$|Recent {{directions}} in engineering {{for sustainable development}} (EESD) (and in ESD more generally) have pointed towards an increasing realisation {{that in order to}} adequately begin to address respective <b>meta-problems</b> associated with global (un) sustainability, ‘object world’ disciplinary perspectives alone are insufficient. Instead, the required depth of knowledge that expert disciplinary knowledge can provide must be both complimented and built upon by other disciplinary as well as experiential knowledge. Integral and transdisciplinary approaches to learning can {{play a central role in}} helping achieve this. When such approaches are applied, they facilitate the possibility of new and emergent knowledge and insights which can transcend disciplinary bounds, with the potential to reach places where no single disciplinary approach can; a classic case of ‘whole greater than the sum of parts’. This however requires a degree of disciplinary humility and openness to other approaches and disciplinary norms, as well as a degree of trust, patience and time. Nevertheless, in the context of seeking authentic sustainability, it is necessary. The classical engineering degree structure is not amenable to this approach. Engineering has traditionally seen itself as a ‘problem solving profession only insofar as ‘problems’, including complex socio-technological ones (with ecological and economic import) can be neatly reduced to well-defined closed system decontextualized ‘puzzles’ which can then be algorithmically optimised. This is deeply problematic as it cannot map reality; specifically, complex contemporary 21 st century reality, instead resulting in emergent ‘unintended consequences’. A key intervention point therefore in the development of a fit-for-purpose cohort of engineering graduates capable of addressing emergent twenty first century <b>meta-problems</b> is through their formative education. Here integral and transdisciplinary approaches to sustainability education/ESD offer a useful approach. But this requires not just the inclusion of ‘sustainable development material’, but a perpendicular reconceptualization of pedagogical approaches. This approach coheres with contemporary pedagogical best practice as it privileges relational and constructivist approaches to learning over the traditional atomistic approach, incorporating as it does, peer to peer and personal reflective learning opportunities. This paper reflects on the experiences of a programme where undergraduate chemical engineering students undertaking a sustainability module collaborate with students on an analogous sociology module. It describes how this transdisciplinary collaboration takes an integral approach to sustainability learning, incorporating both subjective and objective perspectives as well as inter-subjective and inter-objective. The work reflects on how this initiative worked by drawing on student feedback and the authors’ experiences...|$|R
40|$|Fragmentation and {{the lack}} of {{appropriately}} coordinated government services are widely considered to be costly problems impeding effective and efficient government service provision. Moreover, there is a growing realization that many modern social issues have developed into <b>meta-problems</b> that cannot be resolved by the traditional single agency approach. Coordination of services through more cooperative and collaborative networks of relationships between government agencies has become a preferred strategy for many public administrators. In this way, actors from a range of sectors form and reform into action networks to respond to existing and emergent issues. Managing these networks in order to achieve appropriate policy outcomes is an important aspect of modern day governance and strategy development. This issue is particularly important for the central agencies of state since they have a responsibility for ensuring consistent and cohesive government policy and service delivery. This paper gives an account of a public sector initiative aimed at enhancing service provision through the formation and management of inter-departmental networks of coordinative and cooperative action. It concludes that although networks are a useful mechanism of social coordination, their inherent benefits may be jeopardized when network management issues make them vulnerable to pressures from the centre...|$|R
40|$|Language {{interoperability}} {{is not a}} {{new research}} area for the programming languages community. In the early 2000 s, considerable effort went into understanding how to efficiently marshal data between two interoperating languages. The “last word ” in this area was the birth of the. NET framework which solved the problem by unifying the data model that interoperating languages utilize. Since then, both the. NET framework and the JVM have evolved to handle increasingly larger classes of programming languages, most recently dynamic languages with. NET’s DLR and Java’s Da Vinci machine project. One of the biggest <b>meta-problems</b> that programming language researchers face when developing new languages with fancy type systems is that of relevance. More often than not, the advanced types of these languages make it difficult to understand how to extend these works into more mainstream languages {{so that they can be}} enjoyed by the masses. Because of this, it is difficult for a language designer to make the critical argument that their creation is relevant and useful. Language interoperability makes this situation much more palatable. Imagine a world where your fancy linearly-typed or dependently-typed language could interoperate with the mainstream programming language you use for day-to-day work. As an end-user, this is great because the barrier to adopting the fancy language has been greatly lowered. But as a researcher, now we can leverage a mainstream programming language’...|$|R
40|$|Many {{different}} {{mobility problems}} can be distinguished, but a <b>meta-problem</b> seems to consist in the self-propagating dynamics of the mobility system. Due to these dynamics, accomodative optimization strategies typically fail. Traffic management {{is an example of}} systemic feedbacks turning optimization into ‘pseudo-solutions’. It illustrates the background of the current quest for integrated mobility policy. Integration proves to be difficult, however, typically running counter to societal differentiation and fragmentation. Reasoning from the desire for integration, fragmentation may be the unfortunate exception; a barrier. But is it? If fragmentation is taken as the rule, as Luhmann suggests, the promotion of sustainable mobility cannot rely on integration attempts only. Transition management is an effort to deal with the limits to integration from an evolutionary viewpoint. Instead of designing a ‘supersystem’, the long-term perspective of transition management may help manoeuver between integration and differentiation. ‘Critical Systems Thinking’ makes operative such effort, refining analysis and guiding solution strategiesinfo:eu-repo/semantics/publishe...|$|E
40|$|Dynamic {{ensemble}} selection systems work by {{estimating the}} level of competence of each classifier {{from a pool of}} classifiers. Only the most competent ones are selected to classify each specific test sample. The classifiers’ competences are usually estimated over the neighborhood of the test sample, according to a given criterion, such as the local accuracy estimates or the confidence of the base classifier, computed over the neighborhood of the test sample. However, using only one selection criterion can lead to poor estimation of the classifier’s competence. Consequently, the system end up not selecting the most appropriate classifier for the classification of the given test sample. In this thesis, dynamic ensemble selection is formalized as a <b>meta-problem.</b> From a metalearning perspective, the dynamic ensemble selection problem is considered as another classification problem, called the <b>meta-problem.</b> The meta-features of the <b>meta-problem</b> are the different criteria used to measure {{the level of}} competence of the base classifier. Each set captures a different property of the behavior of the base classifier, and {{can be seen as a}} different criterion for estimating the competence level of a base classifier; such criteria include, the classification performance in a local region of the feature space and the classifier confidence for the classification of the input sample. The meta-classifier is trained, based on the defined set of meta-features, to predict the competence level of a given base classifier for the classification of a new test sample. Thus, several criteria can be used in conjunction for a better estimation of the classifiers’ competences. In Chapter 2, a novel dynamic ensemble selection framework using meta-learning is proposed, called META-DES. Five distinct sets of meta-features, each corresponding to a different criterion for measuring the level of competence of a classifier for the classification of input samples are introduced for this specific <b>meta-problem.</b> The meta-features are extracted from the training data and used to train a meta-classifier to predict whether or not a base classifier is competent enough to classify an input instance. During the generalization phase, the meta-features are extracted from the query instance and passed down as input to the meta-classifier. The metaclassifier estimates whether a base classifier is competent enough to be added to the ensemble. Experiments are conducted over several small sample size classification problems, i. e., problems with a high degree of uncertainty due to a lack of training data. Experimental results show the proposed meta-learning framework greatly improves classification accuracy when compared against current state-of-the-art dynamic selection techniques. In Chapter 3, a step-by-step analysis of each phase of the META-DES framework is conducted. We show how each set of meta-features is extracted as well as their impact on the estimation of the competence level of the base classifier. Moreover, an analysis of the impact of several factors on the system performance is carried out; these factors include, the number of classifiers in the pool, the use of different linear base classifiers, as well as the size of the validation data. Experimental results demonstrate that using the dynamic selection of linear classifiers through the META-DES framework, it is possible to solve complex non-linear classification problems using only a few linear classifiers. In Chapter 4, a novel version of the META-DES framework based on the formal definition of the Oracle, called META-DES. Oracle is proposed. The Oracle is an abstract method that represents an ideal classifier selection scheme. A meta-feature selection scheme using an overfitting cautious BPSO is proposed for improving the performance of the meta-classifier. The difference between the outputs obtained by the meta-classifier and those presented by the Oracle is minimized. Thus, the meta-classifier is expected to provide results that are similar to those of the Oracle. Experiments carried out using 30 classification problems demonstrate that the optimization procedure based on the Oracle definition leads to a significant improvement in classification accuracy when compared to previous versions of the META-DES framework. Finally, in Chapter 5, two techniques are investigated in order to improve the generalization performance of the META-DES framework as well as any other dynamic selection technique. First, a prototype selection technique is applied over the validation data to reduce the amount of overlap between the classes, producing smoother decision boundaries. During generalization, a local adaptive K-Nearest Neighbor algorithm is employed for a better definition of the neighborhood of the test sample. Thus, DES techniques can better estimate the classifiers’ competences. Experiments were conducted using 10 state-of-the-art DES techniques over 30 classification problems. The results demonstrate that the use of prototype selection in editing the validation data and the local adaptive distance significantly improve the classification accuracy of dynamic selection techniques...|$|E
40|$|As {{the number}} of {{autonomous}} decision-making entities in the electricity grid increases, {{it is necessary to}} develop (1) new decision-making capabilities embedded within the grid's control and management, and (2) new grid architecture models ensuring that both individual and system objectives are met. This work develops (1) new decision-making mechanisms enabling residential energy users and electricity providers to interact through the use of dynamic price signals, and (2) policy recommendations to facilitate the emergence of shared architecture models describing the future state of the electricity grid. In the first part, two optimization models that capture the emerging flexible consumption, storage, and generation capabilities of residential end-users are formulated. An economic dispatch model that explicitly accounts for end-users' internal dynamics is proposed. A non-iterative pricing algorithm using convex and inverse linear programming is developed to induce autonomous residential end-users to behave cooperatively and minimize the provider's generation costs. In the second part, several factors that make the development of grid architecture models necessary from a public policy standpoint are identified and discussed. The grid architecture problem is rigorously framed as both a market failure legitimizing government intervention, and a <b>meta-problem</b> requiring the development of non-conventional methods of solution. A policy approach drawing on the theoretical concepts of broker, boundary object and boundary organization is proposed. Ph. D...|$|E
40|$|Ulrick Beck's {{notion of}} sub-politics, in {{describing}} civil society's trans-national {{responses to the}} challenges of late industrial capitalism, embodies implicitly the power of civil society and the third sector to create desired social changes within a widening arc of risk horizons. Subsequent to Beck's 1999 writings, we have seen the emergence of the World Social Forum Process, which has become a platform for the global justice / alter-globalisation movement, and has worked as a catalyst in bringing together civil society / the community sector into new meta-networks, in order to address such <b>meta-problems.</b> A primary question explored in this paper is how communities address the large scale global challenges of neo-liberalism turn neo-conservatism, through a globalisation from below. This paper addresses this question through an examination of the World Social Forum Process as complex agent of social change, arguing that the World Social Forum is a platform for social innovation, which can be seen through a 'layered complexity' perspective. Causal Layered Analysis, and complex adaptive socio-ecological systems perspectives, offer layered frameworks {{that can be used to}} understand the World Social Forum Process as platform for social innovations. From this view Social Forums can be seen to be platforms for fast moving resistance, to deeper policy, law and institutional innovations, and on to even deeper worldview shifts, epistemological reconstructions (the epistemology of the Global South), and a culture of 'horizontalism', and through to the emergence of deep narratives for a Global Commons, people’s power and building a planetary society, paralleled by new myths and metaphors. Using this approach, Beck's notion of sub-politics is expanded, as the construction of a cosmopolitan world order takes on a multi-causal and multi-temporal dynamic...|$|R
40|$|This paper {{explores the}} way in which we define and deal with social {{problems}} such as crime and proposes a new way of thinking about them. Criminality, poverty, illiteracy, addiction and child abuse are some of society's most acute and intractable problems. Despite countless attempted remedies, these complex social problems have continued to grow around the world. Although we have developed systems to address these problems, their operation routinely increases problem severity and scope. They are, in effect, perfectly designed to grow the very pathologies which they were designed to eliminate. To confront these paradoxical outcomes, I took a trans-disciplinary approach to develop a new systemic view for designing systems to cope with the emergent <b>meta-problems.</b> Anchored in second-order cybernetics, and ethnography, this research re-contextualized the problem within a self-reproductive economy of interaction and meaning-making, drawing its boundaries on the basis of its systemic operations and conditions of connectivity across intersecting roles related to the problem-solver, the problem host and the identified problem itself. The result is a model of pathogenesis as nested interactions appearing iteratively from individual to societal levels, revealing a self-referential, recursive and paradoxical structure. Within the multitude of self-referential systems, both biological and social, this research provides a new framework which exposes those factors that initiate, reinforce, escalate and perpetuate unintended evolutionary consequences and identifies specific alterations required to systemically produce beneficial results. An ethnographic case study from the criminal justice system serves as the starting point for this research which provides the basis for an innovative systems methodology relevant to understanding the human condition, and a model for effective, sustainable decision-making processes...|$|R
40|$|This {{dissertation}} tackles {{several questions}} in extremal graph {{theory and the}} theory of random graphs. It consists of three more or less independent parts that all fit into one bigger picture – the <b>meta-problem</b> of describing the structure and properties of large random and pseudo-random graphs. Given a positive constant c, we call an n-vertex graph G c-Ramsey if G does not contain a clique or an independent set of size greater than c log n. Since all of the known examples of Ramsey graphs come from various constructions employing randomness, several researchers have conjectured that all Ramsey graphs possess certain pseudo-random properties. We study one such question – a conjecture of Erdős, Faudree and Sós regarding the orders and sizes of induced subgraphs of Ramsey graphs. Although we do not fully resolve this conjecture, the main theorem {{in the first part}} of this dissertation, joint work with Noga Alon, József Balogh, and Alexandr Kostochka, significantly improves the previous state-of-the-art result of Alon and Kostochka. For a positive integer n and a real number p ∈ [0, 1], one defines the Erdős-Rényi random graph G(n, p) to be the probability distribution on the set of all graphs on the vertex set { 1, [...] ., n} such that the probability that a particular pair {i, j} of vertices is an edge in G(n, p) is p, independentl...|$|E
40|$|International audienceSuppose F is {{a finite}} family of graphs. We {{consider}} the following <b>meta-problem,</b> called F-Immersion Deletion: given a graph G and an integer k, decide whether the deletion of at most k edges of G {{can result in a}} graph that does not contain any graph from F as an immersion. This problem is a close relative of the F-Minor Deletion problem studied by Fomin et al. [FOCS 2012], where one deletes vertices in order to remove all minor models of graphs from F. We prove that whenever all graphs from F are connected and at least one graph of F is planar and subcubic, then the F-Immersion Deletion problem admits: a constant-factor approximation algorithm running in time O(m 3 · n 3 · log m); a linear kernel that can be computed in time O(m 4 · n 3 · log m); and a O(2 O(k) + m 4 · n 3 · log m) -time fixed-parameter algorithm, where n, m count the vertices and edges of the input graph. Our findings mirror those of Fomin et al. [FOCS 2012], who obtained similar results for F-Minor Deletion, under the assumption that at least one graph from F is planar. An important difference is that we are able to obtain a linear kernel for F-Immersion Deletion, while the exponent of the kernel of Fomin et al. depends heavily on the family F. In fact, this dependence is unavoidable under plausible complexity assumptions, as proven by Giannopoulou et al. [ICALP 2015]. This reveals that the kernelization complexity of F-Immersion Deletion is quite different than that of F-Minor Deletion...|$|E
40|$|This {{dissertation}} {{considers a}} particular aspect of sequential decision making under uncertainty in which, at each stage, a decision-making agent operating in an uncertain world takes {{an action that}} elicits a reinforcement signal and causes {{the state of the}} world to change. Optimal learning is a pattern of behavior that yields the highest expected total reward over the entire duration of an agent 2 ̆ 7 s interaction with its uncertain world. The problem of determining an optimal learning strategy is a sort of <b>meta-problem,</b> with optimality defined with respect to a distribution of environments that the agent is likely to encounter. Given this prior uncertainty over possible environments, the optimal-learning agent must collect and use information in an intelligent way, balancing greedy exploitation of certainty-equivalent world models with exploratory actions aimed at discerning the true state of nature. ^ My approach to approximating optimal learning strategies retains the full model of the sequential decision process that, in incorporating a Bayesian model for evolving uncertainty about unknown process parameters, {{takes the form of a}} Markov decision process defined over a set of “hyperstates” whose cardinality grows exponentially with the planning horizon. ^ I develop computational procedures that retain the full Bayesian formulation, but sidestep intractability by utilizing techniques from reinforcement learning theory (specifically, Monte-Carlo simulation and the adoption of parameterized function approximators). By pursuing an approach that is grounded in a complete Bayesian world model, I develop algorithms that produce policies that exhibit performance gains over simple heuristics. Moreover, in contrast to many heuristics, the justification or legitimacy of the policies follows directly from the fact that they are clearly motivated by a complete characterization of the underlying decision problem to be solved. ^ This dissertation 2 ̆ 7 s contributions include a reinforcement learning algorithm for estimating Gittins indices for multi-armed bandit problems, a Monte-Carlo gradient-based algorithm for approximating solutions to general problems of optimal learning, a gradient-based scheme for improving optimal learning policies instantiated as finite-state stochastic automata, and an investigation of diffusion processes as analytical models for evolving uncertainty. ...|$|E
40|$|Given a {{symmetric}} D Ã D matrix M over { 0, 1, â}, a list M -{{partition of}} a graph G is a partition of the vertices of G into D parts which {{are associated with}} the rows of M. The part of each vertex is chosen from a given list {{in such a way that}} no edge of G is mapped to a 0 in M and no non-edge of G is mapped to a 1 in M. Many important graph-theoretic structures can be represented as list M -partitions including graph colourings, split graphs and homogeneous sets and pairs, which arise in the proofs of the weak and strong perfect graph conjectures. Thus, there has been quite a bit of work on determining for which matrices M computations involving list M -partitions are tractable. This paper focuses on the problem of counting list M -partitions, given a graph G and given a list for each vertex of G. We identify a certain set of âtractableâ matrices M. We give an algorithm that counts list M partitions in polynomial time for every (fixed) matrix M in this set. The algorithm relies on data structures such as sparse-dense partitions and subcube decompositions to reduce each problem instance to a sequence of problem instances in which the lists have a certain useful structure that restricts access to portions of M in which the interactions of 0 s and 1 s is controlled. We show how to solve the resulting restricted instances by converting them into particular counting constraint satisfaction problems (#CSPs) which we show how to solve using a constraint satisfaction technique known as âarc-consistencyâ. For every matrix M for which our algorithm fails, we show that the problem of counting list M -partitions is #P-complete. Furthermore, we give an explicit characterisation of the dichotomy theorem â counting list M partitions is tractable (in FP) if the matrix M has a structure called a derectangularising sequence. If M has no derectangularising sequence, we show that counting list M-partitions is #P-hard. We show that the <b>meta-problem</b> of determining whether a given matrix has a derectangularising sequence is NP-complete. Finally, we show that list M partitions can be used to encode cardinality restrictions in M partitions problems and we use this to give a polynomial-time algorithm for counting homogeneous pairs in graphs...|$|E
40|$|This {{dissertation}} {{reports on}} interdisciplinary research concerning interaction {{in the fields}} of computing, Artificial Intelligence (AI), and computer music. It claims that computer programs prescribed in advance of the computing often cannot reasonably facilitate interactive music and AI, whereas interactive computing is more suitable. Interactive computing is an alternative approach, emplacing emphasis on real-time emergent 'interaction with the environment. A new agent model developed during this research extends the current conception of interactive computing but is ill-suited to simulation on a modem computer. In this respect the model is theoretical, employing an interactive transducing strategy which points toward prospective generative self-modifiable future computers. The model facilitates general environmental problem solving and potential solutions to the Symbol Grounding Problem and Frame Problem, and enables the extrapolation of a <b>meta-problem,</b> original to this dissertation, termed the interaction problem. These features support an extension of computationalism (the thesis that cognition is computable) with a proposal that cognition is interactively computable (interactive computationalism), offering a potential solution to the interaction problem. In consideration of the relevance of interactive computing in computer music, the new agent model is applied to the topic of virtual societies of interactive music agents, which includes a comparison with an existing computer music model. This aids an analysis and a presentation of exploratory ideas in interactive music, which is portrayed as comprising of real-time, parallel, unpredictable, and modifiable processes, and not a prescribed or static act. These ideas aim to assist the ongoing musical progression and growing sophistication of interactive music systems, which in the future might be restricted to prescriptive computing strategies. The relationship between the prescription and emergence of processes is a key issue for discussion in the respective topics that this dissertation reports on, and provides a point of debate around computer music composition and the predictability of agent computing in interaction with the environment. The subject of predictability results in 1 lI a number of core ideas in this dissertation, enabling a new approach to the Frame Problem and Symbol Grounding Problem in AI and exploratory discussions of interactive computer music. Broadly, the examination provides an interactive framework incorporating interaction as computation in computer science, interaction as a thesis in AI, and interaction as a computational compositional conception in computer music. The dissertation closes in consideration of the challenges in a future implementation of the new agent model. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E
40|$|This {{dissertation}} tackles {{several questions}} in extremal graph {{theory and the}} theory of random graphs. It consists of three more or less independent parts that all fit into one bigger picture [...] the <b>meta-problem</b> of describing the structure and properties of large random and pseudo-random graphs. Given a positive constant c, we call an n-vertex graph G c-Ramsey if G does not contain a clique or an independent set of size greater than c*log(n). Since all of the known examples of Ramsey graphs come from various constructions employing randomness, several researchers have conjectured that all Ramsey graphs possess certain pseudo-random properties. We study one such question [...] a conjecture of Erdos, Faudree, and Sos regarding the orders and sizes of induced subgraphs of Ramsey graphs. Although we do not fully resolve this conjecture, the main theorem {{in the first part}} of this dissertation, joint work with Noga Alon, Jozsef Balogh, and Alexandr Kostochka, significantly improves the previous state-of-the-art result of Alon and Kostochka. For a positive integer n and a real number p in [0, 1], one defines the Erdos-Renyi random graph G(n,p) to be the probability distribution on the set of all graphs on the vertex set { 1, [...] .,n} such that the probability that a particular pair {i,j} of vertices is an edge in G(n,p) is p, independently of all other pairs. In the second part of this dissertation, we study the behavior of the random graph G(n,p) with respect to the property of containing large trees with bounded maximum degree. Our first main theorem, joint work with Jozsef Balogh, Bela Csaba, and Martin Pei, gives a sufficient condition on p to imply that with probability tending to 1 as n tends to infinity, G(n,p) contains all almost spanning trees with bounded maximum degree, improving a previous result of Alon, Krivelevich, and Sudakov. In the second main theorem of this part, joint work with Jozsef Balogh and Bela Csaba, we show that G(n,p) almost surely contains all almost spanning trees with bounded maximum degree even after an adversary removes asymptotically half of the edges in G(n,p). Given an arbitrary graph H, we say that a graph G is H-free if G does not contain H as a subgraph. Edros, Frankl, and Rodl generalized a famous theorem of Erdos and Stone by proving that for every non-bipartite H, the number of labeled H-free graphs on a fixed n-vertex set, f_n(H), satisfies log_ 2 f_n(H) < (1 +o(1)) ex(n,H). The case when H is bipartite has proved to be much harder. For all such H, apart from the cycles of length 4 and 6, it is not even known whether log_ 2 f_n(H) < C*ex(n,H). The main result of the last part of this thesis, joint work with Jozsef Balogh, proves such a bound for `almost all' complete bipartite graphs. This result and the methods used to prove it have many interesting applications, some of which we study in the last chapter...|$|E
40|$|A {{homomorphism}} from a graph G to a graph H is {{a function}} from V (G) to V (H) that preserves edges. Many combinatorial structures that arise in mathematics and in computer science can be represented naturally as graph homomorphisms and as weighted sums of graph homomorphisms. In this thesis we study the complexity of various problems related to graph homomorphisms. We first study the problem # k HomsTo H of counting the homomorphisms from an input graph to a fixed undirected graph H modulo an integer k. A characteristic feature of counting modulo k is that the objects to be counted can be grouped in sets of size k and, thus, cancel out. These cancellations make wider classes of instances tractable than exact (non-modular) counting and, furthermore, {{the value of the}} modulus can affect the tractability of a problem. Modular counting provides a rich setting for studying the structure of homomorphism problems. In this case, the structure of the graph H has a large influence on the complexity of the problem. We show the following results. â¢ When p is a prime and H is an asymmetric three, then # p HomsTo H is either polynomial-time computable or # p P-complete. â¢ When H is a graph that has no cycles that share edges, then # 2 HomsTo H is either polynomial-time computable or # 2 P-complete. â¢ When H is a graph that contains no 4 -cycles, then # 2 HomsTo H is either polynomial-time computable or # 2 P-complete. These types of results in computational complexity are known as dichotomy theorems. Dichotomy theorems give a complete characterisation of the complexity of a problem, depending on the values of its parameter, by showing that a problem is either tractable or hard, and that there are no values of the parameter for which the problem has intermediate complexity. Our results on # 2 HomsTo H partially confirm a conjecture of Faben and Jerrum that was previously known to hold for trees. We also study a counting problem related to matrix partitions of a graph, a generalisation of graph homomorphisms. Given a symmetric D Ã D matrix M over { 0, 1, *}, an M -partition of a graph G is a partition of the vertices of G into D parts which are associated with the rows of M. The vertices of G are mapped {{in such a way that}} no edge of G is mapped to a 0 in M and no non-edge of G is mapped to a 1 in M. In a list M -partition of a graph G, for every vertex &# 119907; of G we are also given a list of allowable parts. A list M -partition of a graph G is an M -partition of G that respects the given lists. There has been quite a bit of work on determining for which matrices M computations involving list M -partitions are tractable. We focus on the problem of counting list M -partitions, given a graph G and given a list for each vertex of G. We identify a certain set of "tractable" matrices M. We give an algorithm that counts list M -partitions in polynomial time for every (fixed) matrix M in this set. Furthermore, we give an explicit characterisation of the dichotomy theorem â counting list M -partitions is tractable (in FP) if the matrix M has a structure called a derectangularising sequence. If M has no derectangularising sequence, we show that counting list M -partitions is #&# 120239;-hard. Finally, we show that the <b>meta-problem</b> of determining whether a given matrix has a derectangularising sequence is &# 120237;&# 120239;-complete. Finally we study the Moran process on directed graphs. The Moran process, as studied by Lieberman, Hauert and Nowak, is a stochastic process modelling the spread of genetic mutations in populations. The process has an underlying graph in which vertices correspond to individuals. Initially, one individual (chosen uniformly at random) possesses a mutation, with fitness r > 1. All other individuals have fitness 1. At each step of the discrete-time process, an individual is chosen with probability proportional to its fitness, and its state (mutant or non-mutant) is passed on to an out-neighbour which is chosen uniformly at random. If the underlying graph is strongly connected then the process will eventually reach fixation, in which all individuals are mutants, or extinction, in which no individuals are mutants. An infinite family of directed graphs is said to be strongly amplifying if, for every r > 1, the extinction probability tends to 0 as the number of vertices increases. Strong amplification is a rather surprising property â it means that in such graphs, the fixation probability of a uniformly-placed initial mutant tends to 1 even though the initial mutant only has a fixed selective advantage of r > 1 (independent of n). Strong amplifiers have received quite a bit of attention, and Lieberman et al. proposed two potentially strongly-amplifying families âone of them called superstars. Heuristic arguments have been published, arguing that there are infinite families of superstars that are strongly amplifying. In this thesis we explore the amplification limits of superstars by proving a rigorous upper bound for the fixation probability of the Moran process on superstars. </p...|$|E

