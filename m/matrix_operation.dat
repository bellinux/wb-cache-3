152|1306|Public
2500|$|Expanding the <b>matrix</b> <b>operation,</b> and {{simplifying}} terms {{using the}} symmetry {{of the stress}} tensor, gives ...|$|E
2500|$|The dual {{quaternion}} product ÂĈ = (A, B)(C, D) = (AC, AD+BC) can be formulated as a <b>matrix</b> <b>operation</b> as follows. [...] Assemble {{the components}} of Ĉ into the eight dimensional array Ĉ = (C1, C2, C3, c0, D1, D2, D3, d0), then ÂĈ is given by the 8x8 matrix product ...|$|E
2500|$|When {{two or more}} two-port {{networks}} are connected, the two-port parameters of the combined network can be found by performing matrix algebra on the matrices of parameters for the component two-ports. [...] The <b>matrix</b> <b>operation</b> can be made particularly simple with an appropriate choice of two-port parameters to match the form of connection of the two-ports. [...] For instance, the z-parameters are best for series connected ports.|$|E
50|$|Some dialects of BASIC {{supported}} <b>matrices</b> and <b>matrix</b> <b>operations,</b> {{useful for}} the solution of sets of simultaneous linear algebraic equations. These dialects would directly support <b>matrix</b> <b>operations</b> such as assignment, addition, multiplication (of compatible matrix types), and evaluation of a determinant. Many microcomputer BASICs did not support this data type; <b>matrix</b> <b>operations</b> were still possible, but had to be programmed explicitly on array elements.|$|R
50|$|Elemental sparse <b>matrix</b> <b>operations</b> (scaling, add, etc).|$|R
25|$|This {{calculation}} {{is easily}} managed using <b>matrix</b> <b>operations.</b>|$|R
2500|$|The most {{important}} projects are developing new programming languages which work more efficiently for claytronics. The {{goal of a}} claytronics matrix is to dynamically form three-dimensional shapes. However, the vast number of catoms in this distributed network increases complexity of micro-management of each individual catom. So, each catom must perceive accurate position information and command of cooperation with its neighbors. In this environment, software language for the <b>matrix</b> <b>operation</b> must convey concise statements of high-level commands {{in order to be}} universally distributed. [...] Languages to program a matrix require a more abbreviated syntax and style of command than normal programming languages such as C++ and Java.|$|E
5000|$|Toeplitz matrix (convolutions can be {{considered}} a Toeplitz <b>matrix</b> <b>operation</b> where each row is a shifted copy of the convolution kernel) ...|$|E
50|$|Expanding the <b>matrix</b> <b>operation,</b> and {{simplifying}} terms {{using the}} symmetry {{of the stress}} tensor, givesThe Mohr circle for stress is a graphical representation of this transformation of stresses.|$|E
5000|$|Perform vector and <b>matrix</b> <b>operations,</b> {{including}} eigenvalues and eigenvectors ...|$|R
30|$|Another {{flexibility}} {{requirement is}} related to antenna dimension. To cope with diverse configurations which are imposed by the emerging communication standards, different MIMO schemes are supported. In order to maintain efficiency and to meet the requested flexibility requirement, the hardware implementation considers the lowest complex configuration (2 × 2) and applies a hardware resource sharing technique to support the other high-order configurations. To manage variable size complex <b>matrix</b> <b>operations</b> {{that are involved in}} the MMSE equalization algorithm, complex <b>matrix</b> <b>operations</b> are decomposed into basic real arithmetic operations. The required operations to perform coefficient computations and symbol estimation can be categorized into complex number <b>operations</b> and complex <b>matrix</b> <b>operations.</b>|$|R
40|$|Abstract—This letter derives fast {{decomposition}} for the quadrature mirror filterbanks (QMFs) of the {{low power}} spectral band replication (SBR) tools in the MPEG high efficiency advanced audio coding (HE AAC) decoder. In contrast with the standard method where computation-intensive <b>matrix</b> <b>operations</b> are employed in the QMF, the proposed method decomposes the <b>matrix</b> <b>operations</b> into conventional discrete cosine transform of type II and III (DCT-II and DCT-III) and simple permutations for easy implementation. The computational complexity can be also reduced effectively by using fast algorithms for DCT. Index Terms—Audio coding, fast algorithm. II. REVIEW The <b>matrix</b> <b>operations</b> (scale factors are neglected) in the QMF are defined as follows...|$|R
50|$|Diagonal {{matrices}} {{occur in}} many areas of linear algebra. Because of the simple description of the <b>matrix</b> <b>operation</b> and eigenvalues/eigenvectors given above, it is typically desirable to represent a given matrix or linear map by a diagonal matrix.|$|E
50|$|A linear {{transformation}} of a vector space, L:Rn→ Rn, has the property that {{the transformation of}} a vector, V=av+bw, {{is the sum of}} the transformations of its components, that is,Each {{linear transformation}} L can be formulated as a <b>matrix</b> <b>operation,</b> which means L:v→Lv, where L is an nxn matrix.|$|E
5000|$|The dual {{quaternion}} product ÂĈ = (A, B)(C, D) = (AC, AD+BC) can be formulated as a <b>matrix</b> <b>operation</b> as follows. Assemble {{the components}} of Ĉ into the eight dimensional array Ĉ = (C1, C2, C3, c0, D1, D2, D3, d0), then ÂĈ is given by the 8x8 matrix product ...|$|E
50|$|<b>Matrix</b> <b>Operations,</b> Schaum's Outline Series, McGraw-Hill Book Company, New York, 1989 (still in print).|$|R
5000|$|Magma {{contains}} asymptotically fast algorithms for all fundamental dense <b>matrix</b> <b>operations,</b> such as Strassen multiplication.|$|R
3000|$|... [...]. Note though, {{that the}} <b>matrix</b> <b>operations</b> are not {{definitely}} {{the most efficient}} way of implementation.|$|R
50|$|Convolution is {{the process}} of adding each element of the image to its local neighbors, {{weighted}} by the kernel. This is related to a form of mathematical convolution. It {{should be noted that the}} <b>matrix</b> <b>operation</b> being performed - convolution - is not traditional matrix multiplication, despite being similarly denoted by *.|$|E
50|$|The Kronecker {{product is}} named after Leopold Kronecker, {{even though there is}} little {{evidence}} that he was the first to define and use it. Indeed, in the past the Kronecker product was sometimes called the Zehfuss matrix, after Johann Georg Zehfuss who in 1858 described the <b>matrix</b> <b>operation</b> we now know as the Kronecker product.|$|E
50|$|When {{two or more}} two-port {{networks}} are connected, the two-port parameters of the combined network can be found by performing matrix algebra on the matrices of parameters for the component two-ports. The <b>matrix</b> <b>operation</b> can be made particularly simple with an appropriate choice of two-port parameters to match the form of connection of the two-ports. For instance, the z-parameters are best for series connected ports.|$|E
5000|$|<b>Matrix</b> <b>operations</b> (including a <b>matrix</b> editor, dot product, {{cross product}} and solver for {{simultaneous}} linear equations) ...|$|R
50|$|LINPACK {{makes use}} of the BLAS (Basic Linear Algebra Subprograms) {{libraries}} for performing basic vector and <b>matrix</b> <b>operations.</b>|$|R
50|$|Using {{the matrix}} {{form of the}} dual {{quaternion}} product this becomes,This calculation is easily managed using <b>matrix</b> <b>operations.</b>|$|R
5000|$|In dynamic analysis, static {{reduction}} {{refers to}} {{reducing the number}} of degrees of freedom. Static reduction can also be used in FEA analysis to refer to simplification of a linear algebraic problem. Since a static reduction requires several inversion steps it is an expensive <b>matrix</b> <b>operation</b> and is prone to some error in the solution. Consider the following system of linear equations in an FEA problem: ...|$|E
5000|$|High-dimensional spaces {{frequently}} {{occur in}} mathematics and the sciences, in example N-dimensional feature space, which presents input signals of neural network or collection of N-dimensional parameters for multidimensional data analysis. Rotation is one of rigid transformations in geometrical space, that preserves length of vectors and can be presented using <b>matrix</b> <b>operation</b> like Y = M.X , where X and Y are input and output vector respectively, and M - rotation matrix.|$|E
50|$|A {{powerful}} {{feature of}} ARPACK {{is its ability}} to use any matrix storage format. This is possible because it doesn't operate on the matrices directly, but instead when a <b>matrix</b> <b>operation</b> is required it returns control to the calling program with a flag indicating what operation is required. The calling program must then perform the operation and call the ARPACK routine again to continue. The operations are typically matrix-vector products, and solving linear systems.|$|E
25|$|Those {{that support}} {{efficient}} access and <b>matrix</b> <b>operations,</b> such as CSR (Compressed Sparse Row) or CSC (Compressed Sparse Column).|$|R
3000|$|... {{provided}} that the involved operations make sense. In the same spirit, interval <b>matrix</b> <b>operations</b> are defined as follows: [...]...|$|R
50|$|In this equation, {{capital letters}} are 2D matrices, and {{lowercase}} letters are scalars. All <b>matrix</b> <b>operations</b> are performed element-by-element.|$|R
50|$|A linear {{transformation}} is a rigid transformation if it satisfies the condition,that isNow use {{the fact that}} the scalar product of two vectors v.w can be written as the <b>matrix</b> <b>operation</b> vTw, where the T denotes the matrix transpose, we haveThus, the {{linear transformation}} L is rigid if its matrix satisfies the conditionwhere I is the identity matrix. Matrices that satisfy this condition are called orthogonal matrices. This condition actually requires the columns of these matrices to be orthogonal unit vectors.|$|E
50|$|RBF {{networks}} {{have the advantage}} of avoiding local minima {{in the same way as}} multi-layer perceptrons. This is because the only parameters that are adjusted in the learning process are the linear mapping from hidden layer to output layer. Linearity ensures that the error surface is quadratic and therefore has a single easily found minimum. In regression problems this can be found in one <b>matrix</b> <b>operation.</b> In classification problems the fixed non-linearity introduced by the sigmoid output function is most efficiently dealt with using iteratively re-weighted least squares.|$|E
50|$|The transfer-matrix {{method is}} based on the fact that, {{according}} to Maxwell's equations, there are simple continuity conditions for the electric field across boundaries from one medium to the next. If the field is known {{at the beginning of a}} layer, the field at the end of the layer can be derived from a simple <b>matrix</b> <b>operation.</b> A stack of layers can then be represented as a system matrix, which is the product of the individual layer matrices. The final step of the method involves converting the system matrix back into reflection and transmission coefficients.|$|E
5000|$|Those {{that support}} {{efficient}} access and <b>matrix</b> <b>operations,</b> such as CSR (Compressed Sparse Row) or CSC (Compressed Sparse Column).|$|R
5000|$|Some array {{programming}} languages include vectorized {{expressions and}} <b>matrix</b> <b>operations</b> as non-ASCII formulas, mixed with conventional control structures. Examples are: ...|$|R
40|$|This thesis {{considers}} {{the application of}} desktop computer video card as a processor to solve two algorithms in medical imaging and sparse <b>matrix</b> <b>operations.</b> The GPU (Graphic Processing Unit) hardware structure in the video card is designed and dedicated to 3 D graphic rendering that include <b>matrix</b> and vector <b>operation.</b> To reconstruct the Magnetic Resonance Images, we apply IFFT that is a fast algorithm for Fourier transforms and has a parallel structure {{that can be used}} in GPU processor. Another experiment for GPU application is sparse <b>matrix</b> <b>operations.</b> Two case studies to work with sparse <b>matrix</b> <b>operations</b> are 662 _bus and 494 _bus admittance matrices. We apply these two matrices to obtain lines current. We Implement the algorithms on GPU GeForce GTX 295 in CUDA platform at Visual C++ Host compiler, the results show 7 X speedup when the same kernels running on CPU Phentom™ II X 4 2. 6 GHz...|$|R
