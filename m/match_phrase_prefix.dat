0|154|Public
500|$|Users can group posts {{together}} by topic or type {{by use of}} hashtags – words or <b>phrases</b> <b>prefixed</b> with a [...] "#" [...] sign. Similarly, the [...] "@" [...] sign followed by a username is used for mentioning or replying to other users.|$|R
5000|$|Users can group posts {{together}} by topic or type {{by use of}} hashtags - words or <b>phrases</b> <b>prefixed</b> with a [...] sign. Similarly, the [...] sign followed by a username is used for mentioning or replying to other users.To repost a message from another Twitter user and share it with one's own followers, a user can click the retweet button within the Tweet.|$|R
25|$|He mistook {{an editor}} and re-publisher for the {{original}} author. Sutton cites Wilkin (1852) which is an edited collected works, within which the <b>matching</b> <b>phrase</b> occurs, in Browne (1658).|$|R
40|$|We {{present a}} system that enables {{flexible}} and efficient <b>phrase</b> <b>matching</b> in XML documents. Since XML allows structured and unstructured information to be interleaved, <b>phrase</b> <b>matching</b> in XML raises new challenges. Our system, named PIX, permits <b>phrase</b> <b>matching</b> in XML documents that contain "mixed content". A key feature of PIX is that users can specify which element and content to ignore when <b>matching</b> a <b>phrase.</b> PIX uses inverted indices and an efficient evaluation algorithm to compute the set of matches and returns answers where phrases, ignored tags and content are highlighted. In addition, query answers are sorted using a ranking function. PIX is implemented {{as an extension of}} GALAX, a full-fledged XQuery engine. The functionality of PIX is fully integrated into XQuery and permits a natural combination of XPath-based structure <b>matching</b> with <b>phrase</b> <b>matching...</b>|$|R
5000|$|In {{order to}} {{integrate}} geotags {{in social media}} and enhance text readability or oral use, the concept of 'meetag' or tag-to-meet has been proposed. Differing from hashtag construction, meetag includes the geolocation information after an underscore. A meetag is therefore a word or an unspaced <b>phrase</b> <b>prefixed</b> with an underscore ("_"). Words in messages on microblogging and social networking services may be tagged by putting [...] "_" [...] before them, either as they appear in a sentence, (e.g. [...] "There is a concert going _montreuxjazzfestival", [...] "the world wide web was invented _cern _geneve", ...) or appended to it.|$|R
40|$|Most {{people think}} that {{dictionaries}} contain words and that words contain letters. To a certain extent this is true. However, dictionaries contain more than just words [...] they contain abbreviations, symbols, <b>prefixes,</b> suffixes and <b>phrases</b> as well. For example, NATIONAL SCHOOL-BUS CHROME is in Webster 2 ̆ 7 s Third. So, too, is the prefix CONTRA-. To avoid continually distinguishing between words, <b>phrases,</b> <b>prefixes</b> and so on, {{all of which are}} in the dictionary, we lump them all together and refer to them as entries. We could even {{go so far as to}} call these entries lexemes [...] that is, meaningful forms that belong to the vocabulary of English...|$|R
40|$|We {{present a}} {{hierarchical}} phrase-based {{statistical machine translation}} in which a target sentence is efficiently generated in left-to-right order. The model is a class of synchronous-CFG with a Greibach Normal Form-like structure for the projected production rule: The paired target-side of a production rule takes a <b>phrase</b> <b>prefixed</b> form. The decoder for the targetnormalized form {{is based on an}} Earlystyle top down parser on the source side. The target-normalized form coupled with our top down parser implies a left-toright generation of translations which enables us a straightforward integration with ngram language models. Our model was experimented on a Japanese-to-English newswire translation task, and showed statistically significant performance improvements against a phrase-based translation system. ...|$|R
50|$|Graph Search {{operated}} {{by use of}} a search algorithm similar to traditional search engines such as Google. However, the search feature is distinguished as a semantic search engine, searching based on intended meaning. Rather than returning results based on matching keywords, the search engine is designed to <b>match</b> <b>phrases,</b> as well as objects on the site.|$|R
5000|$|For example, {{a search}} {{could be used}} to find [...] "red brick house", and <b>match</b> <b>phrases</b> such as [...] "red house of brick" [...] or [...] "house made of red brick". By {{limiting}} the proximity, these <b>phrases</b> can be <b>matched</b> while avoiding documents where the words are scattered or spread across a page or in unrelated articles in an anthology.|$|R
40|$|Document {{clustering}} techniques mostly rely on single term {{analysis of}} the document data set, such as the Vector Space Model. To better capture the structure of documents, the underlying data model {{should be able to}} represent the phrases in the document as well as single terms. We present a novel data model, the Document Index Graph, which indexes web documents based on phrases, rather than single terms only. The semi-structured web documents help in identifying potential <b>phrases</b> that when <b>matched</b> with other documents indicate strong similarity between the documents. The Document Index Graph captures this information, and finding significant <b>matching</b> <b>phrases</b> between documents becomes easy and efficient with such model. The similarity between documents is based on both single term weights and <b>matching</b> <b>phrases</b> weights. The combined similarities are used with standard document clustering techniques to test their effect on the clustering quality. Experimental results show that our phrase-based similarity, combined with single-term similarity measures, enhances web document clustering quality significantly. 1...|$|R
5000|$|Phonosemantic <b>matching,</b> finding <b>phrases</b> which combine {{both the}} meaning and sound of the neologism. Examples: ...|$|R
5000|$|Phrasing {{and form}} (movement and parts are {{structured}} to <b>match</b> the <b>phrasing</b> of the music) ...|$|R
50|$|Negative {{keywords}} {{are often}} necessary for paid search campaigns that contain keywords on either broad <b>match</b> or <b>phrase</b> <b>match.</b> These match types will display your ad for additional search queries (i.e. search queries {{other than the}} actual keyword that {{was added to the}} account). Thus, removing irrelevant terms often becomes necessary.|$|R
40|$|This paper {{introduces}} a novel journal splitting algorithm. It takes {{full advantage of}} various kinds of information such as text match, layout and page numbers. The core procedure is a highly efficient text-mining algorithm, which detects the <b>matched</b> <b>phrases</b> between the content pages and the title pages of individual articles. Experiments show that this algorithm is robust and able to split {{a wide range of}} journals, magazines and books. 1...|$|R
50|$|Other possessed nouns {{require a}} {{following}} noun phrase, occurring only in an NP-internal possessive <b>phrase.</b> A following <b>prefixed</b> noun {{must be in}} the EA.|$|R
40|$|This paper {{presents}} a partial matching strategy for phrase-based {{statistical machine translation}} (PBSMT). Source phrases which do {{not appear in the}} training corpus can be translated by word substitution according to partially <b>matched</b> <b>phrases.</b> The advantage of this method is that it can alleviate the data sparseness problem if the amount of bilingual corpus is limited. We incorporate our approach into the state-of-the-art PBSMT system Moses and achieve statistically significant improvements on both small and large corpora. ...|$|R
50|$|Putting hyperlinks where {{visitors}} {{will not see}} them to increase link popularity. Highlighted link text can help rank a webpage higher for <b>matching</b> that <b>phrase.</b>|$|R
40|$|Abstract. Document {{clustering}} techniques mostly rely on single term {{analysis of}} text, {{such as the}} vector space model. To better capture the structure of documents, the underlying data model {{should be able to}} represent the phrases in the document as well as single terms. We present a novel data model, the Document Index Graph, which indexes Web documents based on phrases rather than on single terms only. The semistructured Web documents help in identifying potential <b>phrases</b> that when <b>matched</b> with other documents indicate strong similarity between the docu-ments. The Document Index Graph captures this information, and finding significant <b>matching</b> <b>phrases</b> between documents becomes easy and efficient with such model. The model is flexi-ble in that it could revert to a compact representation of the vector space model if we choose not to index phrases. However, using phrase indexing yields more accurate document similar-ity calculations. The similarity between documents is based on both single term weights and <b>matching</b> <b>phrase</b> weights. The combined similarities are used with standard document cluster-ing techniques to test their effect on the clustering quality. Experimental results show that our phrase-based similarity, combined with single-term similarity measures, gives a more accurate measure of document similarity and thus significantly enhances Web document clustering qual-ity...|$|R
5000|$|The Flute Concerto has a {{duration}} of roughly 15 minutes and is composed in one continuous movement. Tower briefly described the {{piece in the}} score program notes, writing, [...] "The 15-minute work starts with the low register of the flute alone before the orchestra comes. As the flute gets more active, the chamber-size orchestra provides competitive tension which is <b>matched</b> <b>phrase</b> by phrase as the piece heads relentlessly towards to a finale where the [...] "music blows wide open" [...] (Wincenc) in a virtuosic display of flute scales and arpeggios." ...|$|R
40|$|This paper {{presents}} a noun phrase driven two-level {{statistical machine translation}} system. Noun phrases (NPs) are used as the unit of decomposition to build a two level hierarchy of phrases. English noun phrases are identified using a parser. The corresponding translations are induced using a statistical word alignment model. Identified noun phrase pairs in the training corpus are replaced with a tag to produce a NP tagged corpus. This corpus is then used to extract phrase translation pairs. Both NP translations and NP-tagged phrases are used in a two-level translation decoder: NP translations tag NPs in the first level, where NP-tagged <b>phrases</b> <b>match</b> across NPs to produce translations in the second level. The two-level system shows significant improvements over a baseline SMT system. It also produces longer <b>matching</b> <b>phrases</b> due to the generalization introduced by tagging NPs. 1...|$|R
5000|$|This {{occurs when}} a {{morpheme}} is preceded by another morpheme within the same <b>phrase</b> (e.g. a <b>prefix</b> or an adjunct), unless the preceding morpheme ends itself in a fricative or trill, or in a nasal or [...]|$|R
40|$|<b>Phrase</b> <b>matching</b> is {{a common}} IR {{technique}} to search text and identify relevant documents in a document collection. <b>Phrase</b> <b>matching</b> in XML presents new challenges as text may be interleaved with arbitrary markup, thwarting search techniques that require strict contiguity or close proximity of keywords. We present a technique for <b>phrase</b> <b>matching</b> in XML that permits dynamic specification of both the <b>phrase</b> to be <b>matched</b> and the markup to be ignored. We develop an effective algorithm for our technique that utilizes inverted indices on phrase words and XML tags. We describe experimental results comparing our algorithm to an indexed-nested loop algorithm that illustrate our algorithm's efficiency...|$|R
40|$|In natural {{language}} question answering (QA) systems, questions often contain terms and phrases that are critically important for retrieving or finding answers from documents. We present a learnable {{system that can}} extract and rank these terms and <b>phrases</b> (dubbed mandatory <b>matching</b> <b>phrases</b> or MMPs), and demonstrate their utility in a QA system on Internet discussion forum data sets. The system relies on deep syntactic and semantic analysis of questions only and is independent of relevant documents. Our proposed model can predict MMPs with high accuracy. When used in a QA system features derived from the MMP model improve performance significantly over a state-of-the-art baseline. The final QA system was the best performing system in the DARPA BOLT-IR evaluation. ...|$|R
40|$|The {{fundamental}} {{function of}} an information retrieval {{system is to}} retrieve texts or documents from a database {{in response to a}} user’s request for information, such that the content of the retreived documents will be relevant to the user’s original information need. This is accomplished through matching the user’s information request against the texts in the database in order to estimate which texts are relevant. In this thesis I propose a method for using current natural language processing techniques {{for the construction of a}} text representation to be used in an information retrieval system. In order to support this proposal I have designed a matching algorithm specifically for performing the retrieval task of matching user queries against texts in a database, using the proposed text representation. Having designed this text representation and matching algorithm, I then constructed an experiment to investigate the effectiveness of the algorithm at <b>matching</b> <b>phrases.</b> This experiment involved the use of standard statistical methods to compare the <b>phrase</b> <b>matching</b> capabilities of the proposed matching algorithm to a sample of information retrieval users performing the same task. The results of this evaluation experiment allow me to comment first of all on the effectiveness of the <b>phrase</b> <b>matching</b> algorihtm that I have designed and more generally, on the usefulness of incorporating natural language processing techniques into information retrieval systems...|$|R
40|$|Twitter, one of {{the biggest}} and most popular microblogging Websites, has evolved into a {{powerful}} communication platform which allows millions of active users to generate huge volume of microposts and queries on a daily basis. To accommodate effective categorization and easy search, users are allowed to make use of hashtags, keywords or <b>phrases</b> <b>prefixed</b> by hash character, to categorize and summarize their posts. However, valid hashtags are not restricted and thus are created in a free and heterogeneous style, increasing difficulty of the task of tweet categorization. In this paper, we propose a low-rank weighted matrix factorization based method to recommend hashtags to the users solely based on their hashtag usage history and independent from their tweets' contents. We confirm using two-sample t-test that users are more likely to adopt new hashtags similar to the ones they have previously adopted. In particular, we formulate the problem of hashtag recommendation into an optimization problem and incorporate hashtag correlation weight matrix into it to account for the similarity between different hashtags. We finally leverage widely used matrix factorization from recommender systems to solve the optimization problem by capturing the latent factors of users and hashtags. Empirical experiments demonstrate that our method is capable to properly recommend hashtags...|$|R
40|$|UIC) {{participate}} in the robust track, which is a traditional ad hoc retrieval task. The emphasis is based on average effectiveness {{as well as individual}} topic effectiveness. Noun phrases in the query are identified and classified into 4 types: proper names, dictionary phrases, simple phrases and complex phrases. A document has a phrase if all content words in a phrase are within a window of a certain size. The window sizes for different types of phrases are different. We consider phrases to be more important than individual terms. As a consequence, documents in response to a query are ranked with <b>matching</b> <b>phrases</b> given a higher priority. WordNet is used to disambiguate word senses and bring in useful synonyms and hyponyms once the correct senses of the words in a query have been identified. The usual pseudo-feedback process is modified so that the documents are also ranked according to phrase and word similarities with <b>phrase</b> <b>matching</b> having a higher priority. Five runs which use either title or title and description have been submitted. 1...|$|R
40|$|Online resources, such as Wiktionary, {{provide an}} {{accurate}} but incomplete source of idiomatic phrases. In this paper, we study {{the problem of}} automatically identifying idiomatic dictionary entries with such resources. We train an idiom classifier on a newly gathered corpus of over 60, 000 Wiktionary multi-word definitions, incorporating features that model whether phrase meanings are constructed compositionally. Experiments demonstrate that the learned classifier can provide high quality idiom labels, more than doubling the number of idiomatic entries from 7, 764 to 18, 155 at precision levels of over 65 %. These gains also translate to idiom detection in sentences, by simply using known word sense disambiguation algorithms to <b>match</b> <b>phrases</b> to their definitions. In a set of Wiktionary definition example sentences, the more complete set of idioms boosts detection recall by over 28 percentage points. ...|$|R
40|$|The {{problem of}} {{manually}} modifying the lexicon appears with any {{natural language processing}} program. Ideally, a program {{should be able to}} acquire new lexieal entries from context, the way people learn. We address the problem of acquiring entire phrases, specifically Jigurative phr~es, through augmenting a phr~al lezico~ Facilitating such a self-extending lexicon involves (a) disambiguation-selection of the intended phrase from a set of <b>matching</b> <b>phrases,</b> (b) robust parsin~-comprehension of partially-matching phrases, and (c) error analysis [...] -use of errors in forming hy-potheses about new phrases. We have designed and im-plemented a program called RINA which uses demons to implement functional-qrammar principles. RINA receives new figurative phrases in context and through the appli-cation of a sequence of failure-driven rules, creates and refines both the patterns and the concepts which hold syntactic and semantic information about phrases...|$|R
40|$|In a {{joint effort}} of the Peshitta Institute Leiden and the Werkgroep Informatica of the Vrije Universiteit Amsterdam an {{electronic}} database of Syriac texts is being developed. Percy van Keulen and I have been assigned the Books of Kings, which we have analyzed from morpheme level up through clause-level parsing. Using the Hebrew material already available in the Werkgroep Informatica database, a synopsis of the Masoretic text and the Peshitta has been made at clause level. On {{the basis of the}} synop-sis, clause constituents have been matched, providing a basis for <b>matching</b> <b>phrases</b> within clauses, and for <b>matching</b> words within <b>phrases.</b> One of the products is an elec¬tronic translation concordance with lists of translation correspondences occur¬ring within Kings, which was introduced at the 2005 ISLP meeting in Philadelphia. The lexical items occurring at corresponding points in the two texts need not necessarily be lexicon-based semantic translations of one another, but they are what do occur at that point in the two texts. In this manner, both similarities and differences are brought to light. The occurrences of the two cognate verbs sym and swm within Kings are illustrative of the factors at work during the process of translation...|$|R
40|$|Verbal {{command and}} control systems are fairly common; almost all {{off-the-shelf}} speech recognition packages come {{with a way to}} perform various tasks through a voice command. Unfortunately, these systems require that the user utter the commands precisely in the format that it is expecting. These systems have a small number of grammar rules defined that are used to match against incoming utterances. Here, we present a method of using these same grammar rules to expand the capabilities of {{command and control}} engines to include semantically similar utterances. Latent Semantic Analysis (LSA) is used to connect specific grammar rules with the meanings underlying <b>matching</b> <b>phrases</b> resulting in utterances being matched to grammar rules even though the exact <b>phrase</b> did not <b>match</b> any specific rule. Experiments are described that {{determine the extent to which}} this method can be used and how accurate it is...|$|R
40|$|Generally, speech {{recognition}} engines can employ two different grammar methods, rule and dictation, to recognize an utterance. The {{purpose of these}} grammars is to constrain the search space {{in a way that}} anticipates the speaker's utterance. The research described in this paper attempts to maintain the accuracy of a rule grammar without limiting the speaker to rigorous phraseology. Latent Semantic Analysis (LSA) is used to connect specific grammar rules with the meanings underlying <b>matching</b> <b>phrases</b> resulting in utterances being matched to knowledge base elements even though the exact <b>phrase</b> did not <b>match</b> any grammar rule. A separate knowledge base is used to dynamically add or remove grammar rules in the {{speech recognition}} engine as the conversation context changes. Finally, a learning technique is used to create new regular expressions based on utterances that matched semantically through LSA...|$|R
2500|$|The phrase [...] "No surrender" [...] is {{occasionally}} sung in {{the bridge}} before [...] "Send her victorious" [...] by England football fans at <b>matches.</b> The <b>phrase</b> [...] "no surrender" [...] is {{also associated with}} Combat 18, a white supremacist group. The phrase is also associated with Ulster loyalism and can sometimes be heard {{at the same point}} before Northern Ireland football matches.|$|R
50|$|Sparse binary {{polynomial}} hashing (SBPH) is a {{generalization of}} Bayesian spam filtering that can <b>match</b> mutating <b>phrases</b> {{as well as}} single words. SBPH {{is a way of}} generating a large number of features from an incoming text automatically, and then using statistics to determine the weights for each of those features in terms of their predictive values for spam/nonspam evaluation.|$|R
5000|$|The phrase [...] "No surrender" [...] is {{occasionally}} sung in {{the bridge}} before [...] "Send her victorious" [...] by England football fans at <b>matches.</b> The <b>phrase</b> [...] "no surrender" [...] is {{also associated with}} Combat 18, a white supremacist group. The phrase is also associated with Ulster loyalism and can sometimes be heard {{at the same point}} before Northern Ireland football matches.|$|R
5000|$|Several {{arrangements}} of the tune {{are often used}} for the ballroom Paso Doble dance (to the point that, among ballroom dancers, it is known as [...] "the paso doble song" [...] as it is very commonly played in competition due to the common custom for the choreography to <b>match</b> the <b>phrasing</b> and accents of the music for the full effect of the dance).|$|R
40|$|Noun phrases in {{queries are}} {{identified}} and classified into four types: proper names, dictionary phrases, simple phrases and complex phrases. A document has a phrase if all content {{words in the}} phrase are within a window of a certain size. The window sizes for different types of phrases are different and are determined using a decision tree. Phrases {{are more important than}} individual terms. Consequently, documents in response to a query are ranked with <b>matching</b> <b>phrases</b> given a higher priority. We utilize WordNet to disambiguate word senses of query terms. Whenever the sense of a query term is determined, its synonyms, hyponyms, words from its definition and its compound words are considered for possible additions to the query. Experimental results show that our approach yields between 23 % and 31 % improvements over the best-known results on the TREC 9, 10 and 12 collections for short (title only) queries, without using Web data...|$|R
