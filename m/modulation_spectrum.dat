191|353|Public
50|$|PAL/SECAM {{analogue}} SDTV broadcasts use 6-, 7- (VHF), or 8 MHz (UHF). The 819-line (system E) used 14 MHz wide VHF channels. For HD-MAC, {{the transmission}} medium must guarantee a baseband bandwidth {{of at least}} 11.14 MHz. This translates to a 12 MHz channel spacing in cable networks. The specification allows for 8 MHz channels, {{but in this case}} assistance data can no longer be correctly decoded, and it is only possible to extract a standard definition signal, using a D2-MAC receiver. For satellite broadcasting, due to FM <b>modulation</b> <b>spectrum</b> expansion, an entire satellite transponder would be used, resulting in 27 to 36 MHz of bandwidth. The situation is pretty much the same in analogue standard definition : a given transponder can only support one analogue channel. So from this point of view, going to HD does not represent an inconvenience.|$|E
40|$|The {{amplitude}} and phase {{components of}} the <b>modulation</b> <b>spectrum</b> were dissociated in order to ascertain the importance of cross-spectral, envelope-modulation phase information for understanding spoken language. The dissociation was effected via local time reversals of the speech waveform (i. e., flipping the signal on its horizontal axis) at intervals ranging between 0 and 180 ms. Intelligibility declines progressively as {{the length of the}} time-reversed segment increases, down to an asymptotic trough in performance at 100 ms (4 % of the words correct). Intelligibility does not correlate highly with the amplitude component of the <b>modulation</b> <b>spectrum,</b> but does coincide closely with the contour of the complex <b>modulation</b> <b>spectrum,</b> a representation that integrates the cross-spectral modulation phase and the conventional (amplitude-based) <b>modulation</b> <b>spectrum</b> into a unified representation. The results imply that intelligibility is based on both the phase and amplitude {{components of the}} <b>modulation</b> <b>spectrum.</b> 1...|$|E
40|$|The <b>modulation</b> <b>spectrum</b> is a {{promising}} method to incorporate dynamic information in pattern classification. It contains important cues about the nonstationary {{content of a}} signal and yields complementary improvements when it is combined with conventional features derived from short-term analysis. Many prior <b>modulation</b> <b>spectrum</b> approaches are based on uniform modulation frequency decomposition. The drawbacks of these approaches are high dimensionality {{and a lack of}} a connection to human perception of modulation. This paper presents multi-scale modulation frequency decomposition and shows an improvement over standard <b>modulation</b> <b>spectrum</b> in a digital communication signal classification task. Features derived from this representation provide lower classification error rates than those from a constant-bandwidth <b>modulation</b> <b>spectrum</b> whether used alone or in combination with short-term features. 1...|$|E
40|$|The {{relative}} frequency stabilities and the <b>modulation</b> <b>spectra</b> of differently stabilized lasers are compared. The measurements {{were made by}} a Michelson interferometer with an internal optical de­lay line. For 6328 Å He - Ne-lasers stabilized {{by means of a}} Zeeman-absorption-cell and by the Lamb-dip-method frequency-standard-deviations of 1. 3 × 105 and 9. 0 × 105 Hz for observation times between 1 s and 20 s were measured. The frequency <b>modulation</b> <b>spectra</b> are composed of a low frequency noise part and many discrete frequencies, which can be related to the stabilization mechanism...|$|R
40|$|Disordered {{voices are}} {{frequently}} assessed by speech pathologists using perceptual evaluations. This {{might lead to}} problems caused by the subjective nature {{of the process and}} due to the influence of external factors which compromise the quality of the assessment. In order to increase the reliability of the evaluations, the design of automatic evaluation systems is desirable. With that in mind, this paper presents an automatic system which assesses the Grade and Roughness level of the speech according to the GRBAS perceptual scale. Two parameterization methods are used: one based on the classic Mel-Frequency Cepstral Coefficients, which has already been used successfully in previous works, and other derived from <b>modulation</b> <b>spectra.</b> For the latter, a new group of parameters has been proposed, named <b>Modulation</b> <b>Spectra</b> Morphological Parameters: MSC, DRB, LMR, MSH, MSW, CIL, PALA, and RALA. In methodology, PCA and LDA are employed to reduce the dimensionality of feature space, and GMM classifiers to evaluate the ability of the proposed features on distinguishing the different levels. Efficiencies of 81. 6 % and 84. 7 % are obtained for Grade and Roughness, respectively, using <b>modulation</b> <b>spectra</b> parameters, while MFCCs performed 80. 5 % and 77. 7 %. The obtained results suggest the usefulness of the proposed <b>Modulation</b> <b>Spectra</b> Morphological Parameters for automatic evaluation of Grade and Roughness in the speech...|$|R
40|$|In this paper, we employ {{normalized}} modulation spectral anal-ysis for voice pathology detection. Such normalization is im-portant {{when there}} is a mismatch between training and testing conditions, or in other words, employing the detection system in real (testing) conditions. <b>Modulation</b> <b>spectra</b> usually produce a high-dimensionality space. For classification purposes, the size of the original space is reduced using Higher Order Singular Value Decomposition (SVD). Further, we select most relevant features based on the mutual information between subjective voice quality and computed features, which leads to an adap-tive to the classification task <b>modulation</b> <b>spectra</b> representation. For voice pathology detection, the adaptive <b>modulation</b> <b>spectra</b> is combined with an SVM classifier. To simulate the real test-ing conditions; one for training and the other for testing. We address the difference of signal characteristics between training and testing data through subband normalization of modulation spectral features. Simulations show that feature normalization enables the cross-database detection of pathological voices even when training and test data are different. Index Terms: pathologic voice detection, modulation spec-trum, feature normalization, mutual information, SVD...|$|R
40|$|The {{derivation}} of the reflectivity <b>modulation</b> <b>spectrum</b> of the {{sea surface}} for near-nadir-viewing microwave radars using geometrical optics is described. The equations required for the derivation are presented. The derived reflectivity <b>modulation</b> <b>spectrum</b> provides data on the physical basis of the radar ocean-wave spectrometer measurements of ocean-wave directional spectra...|$|E
30|$|The {{authors in}} [15] also {{proposed}} {{to use the}} <b>modulation</b> <b>spectrum</b> of spectral features to process over-smoothness of spectral features in both time and frequency domains. However, over-smoothness in spectral features mostly occurs in the time domain [4]. In addition, using the <b>modulation</b> <b>spectrum</b> increases the reconstruction errors in synthesized speech.|$|E
30|$|In {{this paper}} {{we use the}} <b>modulation</b> <b>spectrum</b> {{directly}} to exploit the continuity constraints imposed by the speech production system. Since the <b>modulation</b> <b>spectrum</b> captures information about {{the continuity of the}} speech signal in the low-frequency bands, {{there is no need for}} a representation that stacks a large number of subsequent time frames. Therefore, our exemplar dictionary can be created by selecting individual frames of the <b>modulation</b> <b>spectrum</b> in a database of labelled speech. As in [20],[24], we will convert the weights assigned to the exemplars when coding unknown speech signals into estimates of the probability that a frame in the unknown signal corresponds to one of the states.|$|E
40|$|The interactionof thereduced[2 Fe- 2 S]clusterof isolatedRieske {{fragment}} {{from the}} bc 1 complex of Rhodobacter sphaeroides with nitrogens (14 N and 15 N) {{from the local}} protein environment has been studied byX- andS-bandpulsedEPR spectroscopy. The two-dimensional electron spin echo envelope <b>modulation</b> <b>spectra</b> of uniformly 15 N-labeled protein show twowell resolved cross-peaks with weak couplings of 0. 3 – 0. 4 and 1. 1 MHz in addition to cou-plings {{in the range of}} 6 – 8 MHz from two coordinatingN of histi-dine ligands. The quadrupole coupling constants for weakly cou-pled nitrogens determined from S-band electron spin echo envelope <b>modulation</b> <b>spectra</b> identify them as N of histidine ligands andpeptide nitrogen (Np), respectively. Analysis of the line intensities inorientation-selectedS-bandspectra indicated thatNp is the backbone N-atom of Leu- 132 residue. The hyperfine cou-plings fromN andNpdemonstrate the predominantly isotropi...|$|R
40|$|Disordered {{voices are}} {{frequently}} assessed by speech pathologists using acoustic perceptual evaluations. This {{might lead to}} problems due to the subjective nature {{of the process and}} due to the in uence of external factors which compromise the quality of the assessment. In order to increase the reliability of the evaluations the design of new indicator parameters obtained from voice signal processing is desirable. With that in mind, this paper presents an automatic evaluation system which emulates perceptual assessments of the roughness level in human voice. Two parameterization methods are used: complexity, which has already been used successfully in previous works, and <b>modulation</b> <b>spectra.</b> For the latter, a new group of parameters has been proposed as Low Modulation Ratio (LMR), Contrast (MSW) and Homogeneity (MSH). The tested methodology also employs PCA and LDA to reduce the dimensionality of the feature space, and GMM classiffers for evaluating the ability of the proposed features on distinguishing the different roughness levels. An effciency of 82 % and a Cohen's Kappa Index of 0 : 73 is obtained using the <b>modulation</b> <b>spectra</b> parameters, while the complexity parameters performed 73 % and 0 : 58 respectively. The obtained results indicate the usefulness of the proposed <b>modulation</b> <b>spectra</b> features for the automatic evaluation of voice roughness which can derive in new parameters to be useful for clinicians...|$|R
40|$|For most buildings, {{virtually}} all mechanical subsystems such as fans, generators, and motors generate acoustic energy. This acoustic energy can weakly penetrate walls and pass through hallways and conduits. The propagation paths are complex and, given the typically low energy of received acoustic signals, present {{challenges for the}} detection and classification of the subsystems. While conventional approaches would {{not be expected to}} work under these difficult conditions, a key observation can be made: these types of subsystems produce line spectra which consist of harmonics of a fundamental frequency. Acoustic propagation effects then strongly affect the relative energy of these harmonics. <b>Modulation</b> <b>spectra,</b> which make use of the frequency spacing instead of the relative energy of the harmonics, are especially insensitive to these frequency-dependent acoustic attenuation affects. When combined with temporal averaging and 3 -dimensional spatial (over an array of acoustic sensors) processing, enhanced <b>modulation</b> <b>spectra</b> offer a new approach to the detection and classification of building subsystems which produce sound. (EDICS: DSP-TFSR...|$|R
30|$|Table 2 {{shows the}} results {{obtained}} with SC systems operating on <b>modulation</b> <b>spectrum</b> and Mel-spectrum features and the MLP-based systems trained with multi-condition data. It {{can be seen that}} adding Δ and Δ Δ features to the ‘static’ <b>modulation</b> <b>spectrum</b> features increases performance somewhat, but by no means to the extent that adding Δ and Δ Δ features improves performance with Mel-spectrum or PLP features [12],[13].|$|E
40|$|Abstract. This paper {{proposes a}} tempo feature {{extraction}} method. The tempo information is modeled by the narrow-band, low-pass temporal modulation component, which is decomposed into a <b>modulation</b> <b>spectrum</b> via joint frequency analysis. In implementation, the <b>modulation</b> <b>spectrum</b> is directly estimated from the modified discrete cosine transform coefficients, which are output of partial MP 3 (MPEG 1 Layer 3) decoder. Then the log-scale modulation frequency coefficients are {{extracted from the}} amplitude of <b>modulation</b> <b>spectrum.</b> The tempo feature is employed in automatic music emotion classification. The accuracy is improved with several hybrid classification methods based on posterior fusion. The experimental results confirm {{the effectiveness of the}} presented tempo feature and the hybrid classification approach. ...|$|E
40|$|Two {{approaches}} for <b>modulation</b> <b>spectrum</b> equalization are proposed for robust feature extraction in speech recognition. In {{both cases the}} temporal trajectories of the feature parameters are first transformed into the <b>modulation</b> <b>spectrum.</b> In the spectral histogram equalization (SHE) approach, we equalize the histogram of the <b>modulation</b> <b>spectrum</b> for each utterance to a reference histogram obtained from clean training data. In the magnitude ratio equalization (MRE) approach, we equalize the magnitude ratio of lower to higher frequency components on the <b>modulation</b> <b>spectrum</b> to a reference value also obtained from clean training data. Preliminary experimental results performed on the AURORA 2 testing environment indicate that significant performance improvements are achievable with these approaches, when integrated with cepstral mean and variance normalization (CMVN), for all testing sets A, B, and C, all types of noise, for all SNR values. We also show that the approach of magnitude ratio equalization (MRE) offers additional performance improvements when integrated with other more advanced feature normalization approaches such as histogram equalization (HEQ) and higher-order cepstral moment normalization (HOCMN) ...|$|E
40|$|The {{technique}} of Stark modulation spectroscopy for unraveling and assigning rotationally resolved dense molecular spectra has been employed using a tunable diode laser (TDL) source. Doppler-limited absorption and Stark <b>modulation</b> <b>spectra</b> of the HNO 3 7. 5 -micron band near the 1326 /cm band origin {{are presented with}} preliminary values of the excited-state rovibrational constants derived from both TDL and Bomem Fourier transform IR spectra...|$|R
40|$|Subband <b>modulation</b> <b>spectra</b> (SB-MS) {{have proven}} to be useful for many {{applications}} in audio signal analysis and audio signal processing. This paper presents a novel blind watermarking scheme for embedding and detecting a watermark in the subband modulation spectral domain representation of an audio signal. Some results of audio quality, data rate and robustness are presented. The scheme is shown to be robust to a number of attacks while providing excellent subjective audio quality...|$|R
40|$|In {{this paper}} our {{aim is to}} {{investigate}} {{the properties of the}} modulation domain and more specifically, to evaluate the relative contributions of the modulation magnitude and phase spectra towards speech intelligibility. For this purpose, we extend the traditional (acoustic domain) analysis-modification- synthesis framework to include modulation domain processing. We use this framework to construct stimuli that retain only selected spectral components, for the purpose of objective and subjective intelligibility tests. We conduct three experiments. In the first, we investigate the relative contributions to intelligibility of the modulation magnitude, modulation phase, and acoustic phase spectra. In the second experiment, the effect of modulation frame duration on intelligibility for processing of the <b>modulation</b> magnitude <b>spectrum</b> is investigated. In the third experiment, the effect of modulation frame duration on intelligibility for processing of the <b>modulation</b> phase <b>spectrum</b> is investigated. Results of these experiments show that both the modulation magnitude and phase spectra are important for speech intelligibility, and that significant improvement is gained by the inclusion of acoustic phase information. They also show that smaller modulation frame durations improve intelligibility when processing the <b>modulation</b> magnitude <b>spectrum,</b> while longer frame durations improve intelligibility when processing the <b>modulation</b> phase <b>spectrum...</b>|$|R
40|$|Many {{methods of}} speech dereverberation have been {{proposed}} to reduce the effects of reverberation. The IMTF (inverse MTF) -based filtering on the power envelope {{does not need to}} measure the room impulse response (RIR) 、 but the RIR has to be precisely measured before the dereverberation in typical methods. However、 improvement of the restoration accuracy of the restored power envelope is saturated as the reverberation time increases. This is a remaining problem. This paper proposes IMTF-based filtering on the <b>modulation</b> <b>spectrum</b> to resolve the problem. The proposed method estimates the reverberation time on the <b>modulation</b> <b>spectrum</b> and then dereverberates the <b>modulation</b> <b>spectrum</b> of reverberant signal using the IMTF. Three simulations were carried out to evaluate the proposed method. The results showed that the proposed method could adequately restorethe power envelope of a reverberant signal in comparison with the previous method...|$|E
30|$|The {{experiments}} with the MLP classifiers for obtaining state posterior probabilities from the <b>modulation</b> <b>spectrum</b> features {{confirm that the}} <b>modulation</b> <b>spectrum</b> features capture {{most of the information}} that is relevant for speech decoding. Still, the WERs obtained with the MLPs were always inferior to the results obtained with stacks of nine conventional PLP features that include Δ and Δ Δ features, especially in the cleanest SNR conditions. Although the <b>modulation</b> <b>spectrum</b> features are performing quite well in noisy conditions, in cleaner conditions their performance is worse than the classical PLP features. Adding Δ s and Δ Δ s, computed as linear regressions over 90 ms windows, to the <b>modulation</b> <b>spectrum</b> features does not improve performance nearly as much as adding speed and acceleration to MFCC or PLP features. This suggests that our <b>modulation</b> <b>spectrum</b> features are suboptimal with respect to describing the medium-term dynamics of the speech signal. The time windows associated with the modulation frequency filters with the lowest centre frequencies is larger than 500 ms. As a consequence, time derivatives computed over a window of 90 ms for these slowly varying filter outputs is not likely to carry much additional information. We suspect that the features in the lowest modulation bands play too heavy a role. If we want to optimally exploit the redundancy in the different modulation frequency channels when part of them gets obscured by noise, information about relevant speech events (such as word or syllable onsets and offsets) should ideally be represented equally well by their temporal dynamics in all channels.|$|E
30|$|From row 4 (Sys 4) in Table 1, {{it can be}} {{seen that}} fusing the state {{likelihood}} estimates from the nine individual modulation filters with the state likelihoods from the full <b>modulation</b> <b>spectrum</b> deteriorates the recognition accuracy for all but two SNRs. From Table 3 it appears that the Genetic Algorithm returns very small weights for all nine modulation bands. This strongly suggests that the individual modulation bands are not able to highlight specific information that is less easily seen in the complete <b>modulation</b> <b>spectrum.</b>|$|E
40|$|Speech is an {{acoustic}} signal with inherent amplitude modulations in the 1 - 9 Hz range. Recent models of speech perception propose that this rhythmic nature of speech {{is central to}} speech recognition. Moreover, rhythmic amplitude modulations {{have been shown to}} have beneficial effects on language processing and the subjective impression listeners have of the speaker. This study investigated the role of amplitude modulations in the political arena by comparing the speech produced by Hillary Clinton and Donald Trump in the three presidential debates of 2016. Inspection of the <b>modulation</b> <b>spectra,</b> revealing the spectral content of the two speakers’ amplitude envelopes after matching for overall intensity, showed considerably greater power in Clinton’s <b>modulation</b> <b>spectra</b> (compared to Trump’s) across the three debates, particularly in the 1 - 9 Hz range. The findings suggest that Clinton’s speech had a more pronounced temporal envelope with rhythmic amplitude modulations below 9 Hz, with a preference for modulations around 3 Hz. This may be taken as evidence for a more structured temporal organization of syllables in Clinton’s speech, potentially due to more frequent use of preplanned utterances. Outcomes are interpreted in light of the potential beneficial effects of a rhythmic temporal envelope on intelligibility and speaker perception...|$|R
30|$|The {{success of}} the SC {{approach}} in [20],[24] for noise-robust speech recognition is attributed {{to the fact that}} the speech exemplars are characterized by peaks in the spectral energy that exhibit substantial continuity over time; the human articulatory system can only produce signals that contain few clear discontinuities (such as the release of stop consonants), while many noise types lack such continuity. Therefore, it is reasonable to expect that the <b>modulation</b> <b>spectra</b> of speech and noise are rather different, even if the short-time spectra may be very similar.|$|R
40|$|We present {{infrared}} transmittance {{and reflection}} <b>modulation</b> <b>spectra</b> {{for changes in}} the reverse bias voltage across a variety of amorphous silicon (a-Si:H) based pin and min solar cells and diodes. The spectra originate with the change in charge state of levels near the two intrinsic-layer interfaces. The spectra vary significantly for differing interfaces, and we therefore propose their application to ex situ monitoring of the interfaces in solar cell manufacturing. The measurements also support the model that phosphorus doping occurs through dopant complex formation at the concentrations commonly used for solar cell fabrication...|$|R
40|$|In {{this paper}} we propose a deep neural network to model the {{conditional}} probability of the spectral differences between nat-ural and synthetic speech. This allows us to reconstruct the spectral fine structures in speech generated by HMMs. We com-pared the new stochastic data-driven postfilter with global vari-ance based parameter generation and <b>modulation</b> <b>spectrum</b> en-hancement. Our results confirm that the proposed method sig-nificantly improves the segmental quality of synthetic speech compared to the conventional methods. Index Terms: HMM, speech synthesis, DNN, <b>modulation</b> <b>spectrum,</b> postfilter, segmental qualit...|$|E
30|$|In {{this paper}} we {{introduced}} a basic {{implementation of a}} noise-robust ASR system that uses the <b>modulation</b> <b>spectrum,</b> instead of the short-time spectrum to represent noisy speech signals, and sparse classification to derive state probability estimates from time samples of the <b>modulation</b> <b>spectrum.</b> Our approach differs from previous attempts to deploy sparse classification for noise-robust ASR. The first difference {{is the use of}} the <b>modulation</b> <b>spectrum</b> and the second is that the exemplars in our system are constituted by individual frames, rather than by (long) sequences of adjacent frames in [20],[24], which needed such sequences to effectively cover essential information about continuity over time that comes for free in the <b>modulation</b> <b>spectrum,</b> where individual frames capture information about the dynamic changes in the short-time spectrum. Our unadorned implementation yielded recognition accuracies that are slightly below the best results in [20],[24], but especially the fact that our system yielded higher accuracies in the − 5 -dB SNR condition than their systems with exemplars with a length of 50 ms corroborates our belief that we are on a promising track towards a novel approach to noise-robust ASR. Although all results are based on a combination of feature extraction and posterior state probability estimation, we will discuss the features and the estimators separately - to the extent possible.|$|E
40|$|The {{statistical}} {{properties of}} a radiation sources are commonly characterized by second-order-correlation or Mandel parameter. Our research {{found that the}} single photons <b>modulation</b> <b>spectrum</b> provides us another optional way which is {{more sensitive to the}} high frequency information contained in the photon sequence. In this paper, we present direct laser communication by using a multi-channel frequency coding scheme based on the single photons <b>modulation</b> <b>spectrum</b> in which the multi-frequency modulation makes the transmission capacity efficiently enhanced. The modulation frequencies could be operated in a wide band without frequency aliasing due to the inherent randomness of photons arrival time of weak coherent light. The error rate less than 10 - 5 has been achieved experimentally when the mean signal photon count is 80 kcps. The modulated coherent light field shows nonlinear effects of single photons <b>modulation</b> <b>spectrum.</b> The studies of statistical properties of the single photons <b>modulation</b> <b>spectrum,</b> including the dependence of mean noise photon count, integration time, channel spacing and the number of frequency component, helped us to optimize the error rate and transmission capacity. Comment: There are 11 pages, 9 figures in the paper. In this paper, we proposed a new MCFC coding scheme, which is specially designed for laser communication when the received signal is at the single-photon leve...|$|E
40|$|Acoustic {{parameters}} are frequently {{used to assess}} the presence of pathologies in human voice. Many of them have demonstrated to be useful but in some cases its results could be optimized by selecting appropriate working margins. In this study two indices, CIL and RALA, obtained from <b>Modulation</b> <b>Spectra</b> are described and tuned using different frame lengths and frequency ranges to maximize AUC in normal to pathological voice detection. After the tuning process, AUC reaches 0. 96 and 0. 95 values for CIL and RALA respectively representing an improvement of 16...|$|R
50|$|The {{complete}} {{collection of}} M possible symbols over a particular channel {{is called a}} M-ary modulation scheme. Most modulation schemes transmit some integer number of bits per symbol b, requiring the complete collection to contain M = 2^b different symbols. Most popular modulation schemes can be described by showing each point on a constellation diagram, although a few modulation schemes (such as MFSK, DTMF, pulse-position <b>modulation,</b> spread <b>spectrum</b> <b>modulation)</b> require a different description.|$|R
40|$|We {{tackle the}} task of localizing speech signals on the {{horizontal}} plane using monaural cues. We show that monaural cues as incorporated in speech are efficiently captured by amplitude <b>modulation</b> <b>spectra</b> patterns. We demonstrate that by using these patterns, a linear Support Vector Machine can use directionality related information to learn to discriminate and classify sound location at high resolution. We propose a straightforward and robust way of integrating information from two ears: treating each ear as an independent processor and integrate the information at the decision level by doing that ambiguity is {{to a large extent}} resolved...|$|R
30|$|We use the {{well-known}} AURORA- 2 task [22] as the platform for developing our <b>modulation</b> <b>spectrum</b> approach to noise-robust ASR. We {{will use the}} ‘standard’ back end for this task, i.e. a Viterbi decoder that finds the best path in a lattice spanned by the 179 states that result from representing 11 digit words by 16 states each, plus 3 states for representing non-speech. We expect {{that the effect of}} the additive noise is limited to a subset of the 135 output channels of the <b>modulation</b> <b>spectrum</b> analyser.|$|E
30|$|The {{two systems}} that used <b>modulation</b> <b>spectrum</b> {{features}} perform much worse on clean speech than the MLP-based system that used nine adjacent 10 -ms PLP +Δ+Δ Δ features [37]. This {{suggests that the}} <b>modulation</b> <b>spectrum</b> features fail to capture part of the dynamic information that {{is represented by the}} speed and acceleration features derived from PLPs. Interestingly, that information is not restored by adding the regression coefficients obtained with stacks of modulation frequency features. In the noisier conditions, the networks trained with modulation frequency features derived from the multi-condition training data approximate the performance of the stacks of nine extended PLP features.|$|E
30|$|Our {{approach}} combines two novelties, viz. {{the features}} {{and the state}} posterior probability estimation. To {{make it possible to}} disentangle the contributions and implications of the two novelties, we will also conduct experiments in which we use conventional multi-layered perceptrons (MLPs) to derive state posterior probability estimates from the outputs of the <b>modulation</b> <b>spectrum</b> analyser. In section 2, we will compare the sparse classification approach with the results obtained with the MLP for estimating state posterior probabilities. This will allow us to assess the advantages of the <b>modulation</b> <b>spectrum</b> analyser, as well as the contribution of the sparse classification approach.|$|E
40|$|We {{measured}} the self-phase <b>modulation</b> <b>spectra</b> at {{the output of}} deep-etched AlGaAs waveguides. The light source was an OPO laser system providing 250 fs long pulses, at wavelengths around 1520 nm, with a peak power as high as 1 kW. FROG measurements of the OPO pulses reveal their asymmetric shape. The multi-peak spectra, observed at the waveguide output, show a dramatic asymmetry and a power shift towards short wavelengths. The interplay between the Kerr non-linearity, dispersion and self-steepening amplifies the initial slight asymmetry of the injected spectrum. Experimental results compare favourably with the numerical solution of the generalized non-linear Schrödinger equation...|$|R
40|$|Speakers {{adjust their}} voice when talking in noise (known as Lombard speech), facilitating speech comprehension. Recent neurobiological models of speech {{perception}} emphasize {{the role of}} amplitude modulations in speech-in-noise comprehension, helping neural oscillators to ‘track’ the attended speech. This study tested whether talkers produce more pronounced amplitude modulations in noise. Across four different corpora, <b>modulation</b> <b>spectra</b> showed greater power in amplitude modulations below 4 Hz in Lombard speech compared to matching plain speech. This suggests that noise-induced speech contains more pronounced amplitude modulations, potentially helping the listening brain to entrain to the attended talker, aiding comprehension...|$|R
40|$|Expressions {{are given}} to analyze <b>modulation</b> <b>spectra</b> taken at non‐normal incidence. These {{expressions}} are {{used to determine the}} optimum angle of incidence to maximize the signal‐to‐noise ratio. Significant improvements are shown to be obtained in the vacuum‐uv spectral region by making measurements at relatively large angles of incidence. We apply these expressions to evaluate the field‐induced change in the dielectric function for the 20. 5 – 21. 0 ‐eV core‐level doublet in GaP from Schottky‐barrier electroreflectance data. The line shape obtained is consistent with that of a field‐modulated M 0 critical point modified by a Coulomb attraction between the core hole and the excited electron...|$|R
