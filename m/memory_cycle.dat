108|243|Public
25|$|The AGC {{transferred}} data to {{and from}} memory through the G register in a process called the <b>memory</b> <b>cycle.</b> The <b>memory</b> <b>cycle</b> took 12 timing pulses (11.72μs). The cycle began at timing pulse 1 (TP1) when the AGC loaded the memory address to be fetched into the S register. The memory hardware retrieved the data word from memory at the address specified by the S register. Words from erasable memory were deposited into the G register by timing pulse 6 (TP6); words from fixed memory were available by timing pulse 7. The retrieved memory word was then available in the G register for AGC access during timing pulses 7 through 10. After timing pulse 10, the data in the G register was written back to memory.|$|E
25|$|The AGC <b>memory</b> <b>cycle</b> {{occurred}} continuously during AGC operation. Instructions needing memory data had {{to access}} it during timing pulses 7-10. If the AGC changed the memory {{word in the}} G register, the changed word was written back to memory after timing pulse 10. In this way, data words cycled continuously from memory to the G register and then back again to memory.|$|E
25|$|This {{standard}} supports standard memory cycles with {{lengths of}} 1 byte to 4 kilobytes of data, short memory cycles with lengths of 1, 2, or 4 bytes that have much less overhead compared to standard memory cycles, and I/O cycles with lengths of 1, 2, or 4 bytes of data which are low overhead as well. This significantly reduces overhead {{compared to the}} LPC bus, where all cycles except for the 128-byte firmware hub read cycle spends more than one-half {{of all of the}} bus's throughput and time in overhead. The standard <b>memory</b> <b>cycle</b> allows a length of anywhere from 1 byte to 4 kilobytes in order to allow its larger overhead to be amortised over a large transaction. eSPI slaves are allowed to initiate bus master versions of all of the memory cycles. Bus master I/O cycles, which were introduced by the LPC bus specification, and ISA-style DMA including the 32-bit variant introduced by the LPC bus specification, are not present in eSPI. Therefore, bus master memory cycles are the only allowed DMA in this standard.|$|E
50|$|Instructions took 8 <b>memory</b> <b>cycles</b> (160 μs) {{to fetch}} and a {{variable}} number of <b>memory</b> <b>cycles</b> to execute. Indirect addressing added 4 <b>memory</b> <b>cycles</b> (80 μs) for {{each level of}} indirection.|$|R
50|$|The fetch/execute {{mechanism}} {{was completely}} redesigned, optimizing {{the timing and}} allowing partial fetches when the P or Q fields were not needed. Instructions took either 1, 4, or 6 <b>Memory</b> <b>cycles</b> (10 µs, 40 µs, or 60 µs) to fetch and a variable number of <b>memory</b> <b>cycles</b> to execute. Indirect addressing added 3 <b>memory</b> <b>cycles</b> (30 µs) for each level of indirection. Indexed addressing added 5 <b>memory</b> <b>cycles</b> (50 µs) for each level of indexing. Indirect and indexed addressing could be combined at any level of indirection or indexing.|$|R
50|$|To {{obtain the}} operand's address an {{indirect}} indexed instruction required three <b>memory</b> <b>cycles</b> (the index register being in memory) while the direct access instruction required only one.|$|R
5000|$|Magnetic core memory: 32 KB {{capacity}} (<b>memory</b> <b>cycle</b> time of 2 microseconds); ...|$|E
50|$|The {{access time}} plus {{the time to}} rewrite is the <b>memory</b> <b>cycle</b> time.|$|E
5000|$|... memory unit: 4096 12-bit {{words of}} core memory (access time 2.5 microseconds, <b>memory</b> <b>cycle</b> time 16 microseconds) ...|$|E
50|$|Super I/O {{devices and}} audio devices {{are allowed to}} accept I/O cycles, accept ISA-style {{third-party}} DMA cycles, and generate bus master <b>cycles.</b> Generic-application <b>memory</b> devices like nonvolatile BIOS memory and LPC flash devices are allowed to accept <b>memory</b> <b>cycles.</b> Firmware hubs are allowed to accept firmware <b>memory</b> <b>cycles.</b> Embedded controllers are allowed to accept I/O cycles and generate bus master cycles. Some ISA cycles that were deemed not useful to these classes were removed. They include host-initiated two-byte <b>memory</b> <b>cycles</b> and host-initiated two-byte I/O cycles. These removed transfer types could be initiated by the host on ISA buses but not on LPC buses. The host would have to simulate two-byte cycles by splitting them up into two one-byte cycles. The ISA bus has a similar concept because the original 8-bit ISA bus required 16-bit cycles to be split up. Therefore, the 16-bit ISA bus automatically split 16-bit cycles into 8-bit cycles {{for the benefit of}} 8-bit ISA peripherals unless the ISA device being targeted by a 16-bit <b>memory</b> or I/O <b>cycle</b> asserted a signal that told the bus that it could accept the requested 16-bit transfer without assistance from an ISA cycle splitter. ISA-style bus mastering has been replaced in the LPC bus with a bus mastering protocol that does not rely on the ISA-style DMA controllers at all. This was done in order to remove ISA's limit on what type of bus master cycles a device is allowed to initiate on which DMA channel. The ISA-style bus cycles that were inherited by LPC from ISA are one-byte host-initiated I/O bus <b>cycles,</b> one-byte host-initiated <b>memory</b> <b>cycles,</b> and one- or two-byte host-initiated ISA-style DMA cycles.|$|R
40|$|Hardware-assisted garbage {{collection}} {{makes use of}} dedicated circuits located within a special expansion memory module to enhance the response time and throughput of {{garbage collection}} operations. This paper provides detailed descriptions of the <b>memory</b> <b>cycles</b> required to implement each of the primitive garbage collection operations provided by the hardware-assisted garbage collection module...|$|R
40|$|This paper {{discusses}} the fine-grain and thread-specific control of CPU clock speed to save power while considering real-time requirements. Because the expected power savings base on very subtle effects (saving <b>memory</b> <b>cycles,</b> battery discharge analysis, speed-voltage correlation) a trace-driven simulation is unfeasible. Therefore we describe our approach in a way, that a true evaluation with real hardware can follow...|$|R
5000|$|The entire {{core memory}} {{was in the}} IBM 1625 memory unit. <b>Memory</b> <b>cycle</b> time was halved {{compared}} to the Model I's (internal or 1623 memory unit), to 10 µs (i.e., the cycle speed was raised to 100 kHz) by using faster cores. A Memory Address Register Storage (MARS) core memory read, clear, or write operation took 1.5 µs and each write operation was automatically (but not necessarily immediately) preceded by a read or clear operation of the same [...] "register(s)" [...] during the 10 µs <b>memory</b> <b>cycle.</b>|$|E
50|$|A memory arbiter is {{a device}} {{used in a}} shared memory system to decide, for each <b>memory</b> <b>cycle,</b> which CPU {{will be allowed to}} access that shared memory.|$|E
50|$|Memory {{capacity}} for the 70/55 ranged from 65,536 bytes of core memory to 524,288 bytes. The <b>memory</b> <b>cycle</b> time was 0.84 microseconds to access four bytes of information.|$|E
50|$|The {{idea was}} {{originally}} formulated to use CPU <b>memory</b> <b>cycles.</b> It was quickly {{concluded that it}} was better to use memory latency - the time it takes for the computer’s processor to get information from its memory chip - than CPU power. This way, spammers could not avoid the problem just by having a higher end processor.|$|R
50|$|Amiga {{systems can}} also be {{expanded}} with Fast RAM, which is only accessible to the CPU. This improves execution speed, as CPU cycles are never blocked even when the custom chipset is simultaneously accessing Chip RAM. Adding Fast RAM to systems with 32-bit CPUs roughly doubles the instruction speed, as the more advanced 68020, '030, and '040 CPUs can utilize more <b>memory</b> <b>cycles</b> than the earlier 68000.|$|R
50|$|Most systems use {{multiple}} buffering {{and some}} means of synchronization of display and video <b>memory</b> refresh <b>cycles.</b>|$|R
50|$|Core <b>memory</b> <b>cycle</b> {{times were}} 20 microseconds for the Model I, 10 microseconds for the Model II (about a {{thousand}} times slower than typical computer main memory in 2006).|$|E
5000|$|When [...] is {{asserted}} {{during a}} valid <b>memory</b> <b>cycle,</b> that is, when the processor has asserted the [...] and/or [...] status outputs, the following {{sequence of events}} will occur: ...|$|E
50|$|Memory {{capacities}} for the 70/25 {{ranged from}} a minimum of 16,384 bytes {{to a maximum of}} 65,536 bytes. The <b>memory</b> <b>cycle</b> time was 1.5 microseconds to access one 8 bit byte.|$|E
2500|$|The term [...] "reduced" [...] in {{that phrase}} was {{intended}} to describe {{the fact that the}} amount of work any single instruction accomplishes is reduced—at most a single data memory cycle—compared to the [...] "complex instructions" [...] of CISC CPUs that may require dozens of data <b>memory</b> <b>cycles</b> in order to execute a single instruction. In particular, RISC processors typically have separate instructions for I/O and data processing.|$|R
50|$|Huffman {{encodings}} {{are fairly}} CPU intensive, {{in comparison to}} other board representations which seek to minimize required processor and <b>memory</b> <b>cycles.</b> However, {{the small size of}} the final representation makes this approach well suited to storage of long-term knowledge, for instance in storing positions in an opening book, where minimizing the board representation's size is more important than minimizing CPU cycles. Huffman encoded boards are also sometimes used in transposition tables for shallow entries.|$|R
5000|$|A 16x16 {{crossbar}} switch - used {{to connect the}} 16 CMs {{on one side and}} 16 banks of shared memory on the other. If all 16 processors were accessing different banks of memory, the memory accesses would all be concurrent. If two or more processors were trying to access the same bank of memory, one of them would be granted access on one cycle and the remainder would be negotiated on subsequent <b>memory</b> <b>cycles.</b>|$|R
50|$|Memory {{capacity}} for the 70/45 {{ranged from a}} minimum of 16,384 bytes to 262,144 bytes. The <b>memory</b> <b>cycle</b> time was 1.44 microseconds to access two bytes (one half word) of information.|$|E
50|$|Two memory {{configurations}} for the 70/15 were available: either 4,096 bytes or 8,192 bytes of core memory. The <b>memory</b> <b>cycle</b> {{time for}} a 70/15 was 2 microseconds per byte of information.|$|E
50|$|In the 160 and 160-A, the <b>memory</b> <b>cycle</b> {{time was}} 6.4 microseconds. An add took two cycles. The average {{instruction}} took 15 microseconds, for a processing rate of 67,000 instructions per second.|$|E
5000|$|V krajině vzpomínání (Landscape of <b>Memories),</b> Song <b>Cycle</b> {{for high}} voice and piano (or string orchestra) (1977, 1986); words by František Branislav ...|$|R
5000|$|The Penny Black Project is a Microsoft Research {{project that}} tries to find {{effective}} and practical ways of fighting spam. Because identifying spams consumes a recipient's time, {{the idea is to}} make the sender of emails [...] "pay" [...] a certain amount for sending them. The currency or the mode of payment could be CPU cycles, Turing tests or <b>memory</b> <b>cycles.</b> Such a payment would limit spammers' ability to send out large quantities of emails quickly.|$|R
5000|$|The Commodore Amiga, {{released}} the following year, {{also has a}} full-featured blitter. The first US patent filing {{to use the term}} blitter was [...] "Personal computer apparatus for block transfer of bit-mapped image data," [...] assigned to Commodore-Amiga, Inc. Compared to the MC68000 processor, the blitter needs no <b>memory</b> <b>cycles</b> for fetching instructions, no silicon for decoding, and contains a barrel shifter to assist shifting pixel-accurate graphics in bitplanes. It also performs a [...] "4 operand" [...] boolean operation (typically destination:=op(destination, source, mask)) ...|$|R
50|$|Processor {{performance}} and frequency has grown rapidly {{over the past}} three decades. Post mid-1980's it has grown by 52% annually largely driven by organisational and architectural ideas. 64-bit Intel Xeon processor (2004) can clock at 3.6 GHz thus having a cycle time of 0.27 ns. <b>Memory</b> <b>cycle</b> time however has not grown at this fast rate. This has resulted in processor cycle times being currently much faster than <b>memory</b> <b>cycle</b> times, and the trend has been for this gap to increase over time. The problem of increasing memory latency, relative to processor speed, has been dealt with by adding high speed cache memory.|$|E
5000|$|The 550 and 560 {{supported}} 16 K to 256 K 32-bit words (64 KB to 1 MB) [...] Main <b>memory</b> <b>cycle</b> {{time was}} 645 ns. Virtual memory and memory protection were standard features.|$|E
50|$|The 128-word {{thin-film}} memory general register stack (16 each arithmetic, index, and repeat {{with a few}} in common) had a 300-nanosecond access time with a complete cycle time of 600 nanoseconds. Six cycles of {{thin-film memory}} per core <b>memory</b> <b>cycle</b> and fast adder circuitry permitted memory address indexing within the current instruction core <b>memory</b> <b>cycle</b> and also modification of the index value (the signed upper 18 bits {{were added to the}} lower 18 bits) in the specified index register (16 were available). The 16 input/output (I/O) channels also used thin-film memory locations for direct to memory I/O memory location registers. Programs could not be executed from unused thin-film memory locations.|$|E
5000|$|Note: EXEC 8 Idle Loop - the [...] "Idle Loop" [...] {{was entered}} when a CPU had no {{available}} task to execute (typically when {{waiting for an}} I/O operation to complete). A simplified description is that the CPU executed a block transfer (op code 022) of the ICR stack (the first 0200 memory addresses) {{back to the same}} addresses. Since the ICR stack was contained in the CPU, this minimized use of core <b>memory</b> <b>cycles,</b> freeing them up for active CPUs.|$|R
40|$|Poly(epsilon-caprolactone) -based {{non-woven}} fibrous mats showing excellent one-way {{shape memory}} properties were obtained through a straightforward approach by combining electrospinning process and sol-gel reaction. A solution of partially crosslinked a, alpha,omega-triethoxysilane-terminated poly(3 -caprolactone) {{was used to}} obtain bead-free fibers through electrospinning. Non-woven mats with different crosslinking degrees have been prepared {{and the effect of}} the different crosslinking extent and of the microfibrous structure were correlated to the mechanical and shape memory properties of the material. The evolution of fiber architecture within the non-woven mat following deformation and shape <b>memory</b> <b>cycles</b> was also investigated...|$|R
5000|$|A simplified, {{inexpensive}} form of DMA called [...] "three-cycle data break" [...] was supported; this {{required the}} assistance of the processor. The [...] "data break" [...] method moved some of common logic needed to implement DMA I/O from each I/O device into one common copy of the logic within the processor. [...] "Data break" [...] placed the processor in charge of maintaining the DMA address and word count registers. In three successive <b>memory</b> <b>cycles,</b> the processor would update the word count, update the transfer address, and store or retrieve the actual I/O data word.|$|R
