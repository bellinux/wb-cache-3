3|27|Public
40|$|The {{increasing}} demand in automatic code transformation tools – which can preserve the layout, and {{can handle the}} whole <b>macro</b> <b>syntax</b> – led us to develop our scanner and parser tool. ParsErl is a generic syntactic analyser for Erlang. The scanner and the parser are generated from an XML definition of the grammar. The result of the scanning process is a graph, which can be optimised or balanced for applications. The tool can preserve the original layout of the source code, including the original macro definitions. Our preprocessor creates connection between the original source code’s tokens and syntax tree’s nodes. We can provide the substituted and parsed code for the applications and we can generate the original source code back, when it is needed. ...|$|E
40|$|ABSTRACT This study {{aimed to}} {{examine the effect of}} two contextual variables: task {{uncertainty}} and decentralization on managerial performance by management accounting system (MAS) as the intervening variable. This study also examined the effect of contextual variables on management accounting system (MAS), the effect of MAS on managerial performance, either directly or moderated by those two contextual variables. Respondents of this study were functional managers of large scale manufacture companies in Central Java Province. There were 60 respondents for the study samples, who were used for the subject of data analysis using Sobel test and SPSS <b>Macro</b> <b>Syntax.</b> Results of the study indicated as the followings: there is positive effect of decentralization to management accounting system, there is negative effect of task uncertainty to management accounting system, there is positive effect of management accounting system to managerial performance. From these relationships it was evidenced that MAS was able to mediate the effect of decentralization to managerial performance, but MAS wasn’t able to mediate the effect of task uncertainty to managerial performance. The effect of MAS and managerial performance could not be moderated by the two contextual variables. This study only examined manufacture companies. Therefore, the result could not be generalized for other setting, such as for service or trade companies. Keywords : task uncertainty, decentralization, management accounting system (MAS), managerial performance...|$|E
40|$|The {{dissertation}} {{consists of}} three essays on capital structure. The first essay is entitled “Why should PLS-SEM be used rather than Regression? Evidence from the Capital Structure perspective”. This study examines capital structure determinants using a simultaneous causal model with interaction effects between manifest and latent variables. Partial Least Squares is an approach to Structural Equation Models that allows researchers to analyse the relationships simultaneously. It is interesting to compare and contrast this approach in analysing mediation relationships with the regression analysis popularized by Baron and Kenny (1986). In addition to statistical data, logical arguments are presented supported by two case studies from PLS-SEM and regression models. I find that the choice between regression and PLS-SEM matters even with the simplest scenarios per item for constructs. This study’s originality is the provision of new comparative analyses of PLS-SEM versus regression analysis {{in the context of}} capital structure determinants. The “indirect” and “mediate” <b>macro</b> <b>syntax</b> normal theory of the Sobel test, and the bootstrapping techniques proposed by Preacher and Hayes (2004, 2008) are compared with PLS-SEM. I find that the PLS-SEM analysis provides less contradictory results than regression analysis in terms of detecting mediation effects. The second essay is entitled “Determinants of Capital Structure and Firm Financial Performance - A PLS-SEM Approach: Evidence from Malaysia and Indonesia”. This essay investigates the impact of capital structure determinants on firm financial performance together with the mediation effect of firm leverage in Malaysia and Indonesia over the period 1990 to 2010. My results show that some of the capital structure determinants directly affect firm financial performance. I also find that only the Malaysian sample has a positive significant correlation between firm leverage and firm financial performance. Malaysian firms use external financing instead of internal financing to heighten the performance. My results also show that firm leverage plays a mediating role in Malaysia but not for the Indonesian sample. Asset structure, growth opportunities, liquidity, non-debt tax shield and interest rate are the attributes that were indirectly influenced by firm leverage on firm financial performance. Further analysis for multi-group analysis (MGA) in PLS was also tested for equality of the parameter estimates. I find that some attribute coefficients in the determinants of capital structure and firm financial performance are significantly different between Malaysia and Indonesia. The third and final essay is entitled “Interrelation between Countries, Economic Sectors, Capital Structure and Performance: The Malaysian and Indonesian evidence”. Here I study the comprehensive, simultaneous interrelationships between countries, economic sectors (i. e., primary, secondary and tertiary sector), capital structure and performance, especially involving mediation and the equality of coefficient effects in different sectors. I find a direct relationship between the determinants of capital structure and firm financial performance within different economic sectors. I find a significant relationship between firm leverage and firm financial performance in the secondary and tertiary sectors but not for the primary sector. I find that the secondary sector tends to use internal financing while the tertiary sector tends to use external financing to enhance firm financial performance. My results also reveal that the effect of firm leverage on firm financial performance tends to be mediated by firm- and country-specific attributes, {{as well as by the}} sector in which they operate. A closer examination of the data showed that in the economic sectors, I find robust results that there are not just positive direct and indirect effects, but also negative direct and indirect effects. Using PLS multi-group analysis (MGA), my findings show that I reject the null hypothesis that the all path coefficient estimates for the sector comparisons are equal...|$|E
50|$|Fully {{supports}} Microsoft <b>macro</b> assembler 6 <b>syntax,</b> all MASM 8 instructions sets.|$|R
5000|$|Julia (has Lisp-like <b>macros,</b> but ALGOL-like <b>syntax)</b> (also under Python, Ruby, ALGOL) ...|$|R
50|$|Version 2 {{introduced}} <b>macros</b> with <b>syntax</b> and commands {{similar in}} complexity to an advanced BASIC interpreter, {{as well as}} string variable expressions. Later versions supported multiple worksheets and were written in C. The charting/graphing routines were written in Forth by Jeremy Sagan (son of Carl Sagan) and the printing routines by Paul Funk (founder of Funk Software).|$|R
5000|$|Languages such as C and {{assembly}} language have rudimentary macro systems, implemented as preprocessors to the compiler or assembler. C preprocessor macros work by simple textual search-and-replace at the token, {{rather than the}} character level.A classic use of macros is in the computer typesetting system TeX and its derivatives, {{where most of the}} functionality is based on macros.MacroML is an experimental system that seeks to reconcile static typing and macro systems. Nemerle has typed <b>syntax</b> <b>macros,</b> and one productive way to think of these <b>syntax</b> <b>macros</b> is as a multi-stage computation.Other examples: ...|$|R
40|$|Lisp {{has shown}} that a {{programmable}} <b>syntax</b> <b>macro</b> system acts {{as an adjunct to}} the compiler that gives the programmer important and powerful abstraction facilities not provided by the language. Unlike simple token substitution macros, such as are provided by CPP #the C preprocessor#, <b>syntax</b> <b>macros</b> operate on Abstract Syntax Trees #ASTs#. Programmable <b>syntax</b> <b>macro</b> systems have not yet been developed for syntactically rich languages such as C because rich concrete syntax requires the manual construction of syntactically valid program fragments, which is a tedious, di#cult, and error prone process. Also, using two languages, one for writing the program, and one for writing macros, is another source of complexity. This research solves these problems byhaving the macro language be a minimal extension of the programming language, byintroducing explicit code template operators into the macro language, and by using a type system to guarantee, at macro de#nition time, that all macros and macro functions only produce syntactically valid program fragments. The code template operators make the language context sensitive, which requires changes to the parser. The parser must perform type analysis in order to parse macro de#nitions, or to parse user code that invokes macros. ...|$|R
50|$|UltraEdit is a {{commercial}} text editor for Microsoft Windows, Linux and OS X created in 1994 by {{the founder of}} IDM Computer Solutions Inc., Ian D. Mead. The editor contains tools for programmers, including <b>macros,</b> configurable <b>syntax</b> highlighting, code folding, file type conversions, project management, regular expressions for search-and-replace, a column-edit mode, remote editing of files via FTP, interfaces for APIs or command lines of choice, and more. Files can be browsed and edited in tabs, and it also supports Unicode and hex editing mode.|$|R
5000|$|Multi-Edit is a {{commercial}} text editor for Microsoft Windows {{created in the}} 1980s by Todd Johnson. Multi Edit Software obtained ownership rights for the product in October 2002. [...] Multi-Edit contains tools for programmers, including <b>macros,</b> configurable <b>syntax</b> highlighting, code folding, file type conversions, project management, regular expressions, three block highlight modes including column, stream and line modes, remote editing of files via FTP and interfaces for APIs or command lines of choice. The editor uses a tabbed document interface and sessions can be saved.|$|R
40|$|Purpose – Missing {{data are}} a {{recurring}} problem {{that can cause}} bias or lead to inefficient analyses. The objective {{of this paper is}} a direct comparison between the two statistical software features R and SPSS, in order {{to take full advantage of}} the existing automated methods for data editing process and imputation in business surveys (with a proper design of consistency rules) as a partial alternative to the manual editing of data. Approach – The comparison of different methods on editing surveys data, in R with the ‘editrules’ and ‘survey’ packages because inside those, exist commonly used transformations in ofﬁcial statistics, as visualization of missing values pattern using ‘Amelia’ and ‘VIM’ packages, imputation approaches for longitudinal data using ‘VIMGUI’ and a comparison of another statistical software performance on the same features, such as SPSS. Findings – Data on business statistics received by NIS’s (National Institute of Statistics) are not ready to be used for direct analysis due to in-record inconsistencies, errors and missing values from the collected data sets. The appropriate automatic methods from R packages, offers the ability to set the erroneous fields in edit-violating records, to verify the results after the imputation of missing values providing for users a flexible, less time consuming approach and easy to perform automation in R than in SPSS <b>Macros</b> <b>syntax</b> situations, when <b>macros</b> are very handy...|$|R
5000|$|... vile was {{initially}} derived {{from an early}} version of Microemacs in an attempt to bring the Emacs multi-window/multi-buffer editing paradigm to vi users, and was first published on Usenet's alt.sources in 1991. It provides infinite undo, UTF-8 compatibility, multi-window/multi-buffer operation, a <b>macro</b> expansion language, <b>syntax</b> highlighting, file read and write hooks, and more.|$|R
50|$|Pure, {{successor}} to the equational language Q, is a dynamically typed, functional programming language based on term rewriting. It has facilities for user-defined operator <b>syntax,</b> <b>macros,</b> arbitrary-precision arithmetic (multiple-precision numbers), and compiling to native code through the LLVM. Pure is free and open-source software distributed (mostly) under the GNU Lesser General Public License version 3 or later.|$|R
40|$|From now on, a {{main goal}} in {{designing}} a language {{should be to}} plan for growth. " Guy Steele: Growing a Language, OOPSLA' 98 invited talk. We present our experiences with a <b>syntax</b> <b>macro</b> language which we claim forms a general abstraction mechanism for growing (domain-specific) extensions of programming languages. Our <b>syntax</b> <b>macro</b> language is designed to guarantee type safety and termination. A concept of metamorphisms allows the arguments of a macro to be inductively defined in a meta level grammar and morphed into the host language. We also show how the metamorphisms {{can be made to}} operate simultaneously on multiple parse trees at once. The result is a highly flexible mechanism for growing new language constructs without resorting to compile-time programming. In fact, whole new languages can be defined at surprisingly low cost. This work is fully implemented as part of the system for defining interactive Web services, but could find use in many other languages. 1...|$|R
40|$|Abstract. Having {{extensible}} languages {{is appealing}} but {{raises the question}} of how to construct extensible compilers and how to compose compilers out of a collection of pre-compiled components. Being able to deal with attribute grammar fragments as first-class values makes it possible to describe semantics in a compositional way; this leads naturally to a plug-in architecture, in which a core compiler can be constructed as a (collection of) pre-compiled component(s), and to which extra components can safely be added at will as need arises. We present an Emddded Domain Specific language (EDSL), in the form of a Haskell combinator library, which makes the above possible in a typeful way; both the check for the well-definedness of the constructed attribute grammar and the well-definedness of attribute computations is taken care of by the Haskell type checker. With our combinators it is easy to describe semantics in terms of already existing semantics, just as <b>syntax</b> <b>macros</b> extend language <b>syntax.</b> We also show how existing semantics can be redefined, thus adapting some aspects from the behavior defined by the macros. ...|$|R
40|$|In his {{personal}} diary Tempo morto e outros tempos: trechos de um diário de adolescência e primeira mocidade (1975) the Brazilian writer Gilberto Freyre (1900 - 1980) comments social, political, cultural, religious and intellectual {{aspects of his}} country. This article will analyze the task complexity and the challenges faced {{during the process of}} translation like critical and theoretical act involving not simply the linguistics aspects. At the end, the emphasis will be put on the analysis of two <b>macro</b> aspects: <b>syntax</b> and punctuation, lexicon and note, where the translator appears with strategies and comment. The first one, as representative of Freyre?s style and the second one as representative to overcome the difficulties associated with the transposition of themes and words so distant from the Italian context...|$|R
40|$|The {{purpose of}} this {{research}} is to test the effect of job satisfaction on Organizational Citizenship Behavior (OCB) employees Hospital Pantai Indah Kapuk (RSPIK), Jakarta mediated by organizational commitment. Data was collected using a convenience sampling. There are 70 nurses, health workers and administration used as a sample. Data analysis methods used in this research is the analysis of the path by using analytical tools of Preacher-Hayes the Simple Mediation Model. The results of data processing using SPSS 22 for windows with <b>macros</b> and <b>syntax.</b> This research findings RSPIK employees have job satisfaction, organizational commitment and OCB are relatively high, where organizational commitment can be intervening variables on job satisfaction influence on organizational citizenship behavior (OCB) at Hospital Pantai Indah Kapuk in Jakarta...|$|R
40|$|This paper aims to {{demonstrate}} {{that it is possible}} for a language with a rich, conventional syntax to provide Lisp-style macro power and simplicity. We describe a <b>macro</b> system and <b>syntax</b> manipulation toolkit designed for the Dylan programming language that meets, and in some areas exceeds, this standard. The debt to Lisp is great, however, since although Dylan has a conventional algebraic syntax, the approach taken to describe and represent that syntax is distinctly Lisp-like in philosophy. ...|$|R
40|$|Domain {{specific}} languages {{embedded in}} C++ (EDSLs) often use {{the techniques of}} template metaprogramming and ex-pression templates. However, these techniques can require verbose code and introduce maintenance and debugging challenges. This paper presents a tool written in Racket for generating C++ programs, paying {{particular attention to the}} challenges of metaprogramming. The code generator uses Racket’s <b>macros</b> to provide <b>syntax</b> for defining C++ meta-functions that is more concise and offers more opportunity for error checking than that of native C++. 1...|$|R
40|$|Many {{research}} {{questions in the}} field of special education are best tackled using single-subject studies. These approaches have a long tradition within this discipline. However, {{the question of how to}} best analyze data from those designs remains controversial. Visual inspections and effect size calculations are often problematic. Using common parametric tests to inferentially analyze the data is often unsuitable due to severe violations of their assumptions. Randomization tests could present a useful alternative in this regard, but they are considered to be extremely intricate and unmanageable. This explains why these methods have not yet played a role in analyzing data from single-subject designs in practical research within the scope of special education. More recently it has been made possible to compute respective calculations rather economically by using efficient <b>macros</b> or <b>syntaxes</b> for certain familiar statistical packages (Microsoft Excel and IBM SPSS Statistics). It has to be noted that randomization tests render their own methodological problems. In addition, they are only suitable for certain {{research questions}}. But taken as a whole, randomization tests are certainly very promising concepts that can substantially contribute to improve the validity of many single-subject studies...|$|R
50|$|The {{process of}} {{expanding}} a procedure in-line {{should not be}} regarded as a variant of textual replacement (as in <b>macro</b> expansions) because <b>syntax</b> errors may arise as when parameters are modified and the particular invocation uses constants as parameters. Because {{it is important to be}} sure that any constants supplied as parameters will not have their value changed (constants can be held in memory just as variables are) lest subsequent usages of that constant (made via reference to its memory location) go awry, a common technique is for the compiler to generate code copying the constant's value into a temporary variable whose address is passed to the procedure, and if its value is modified, no matter; it is never copied back to the location of the constant.|$|R
5000|$|StructuralEquality // Implement IEquatableSample [...]Net {{interface}} using by element comparison equality.class Sample{ Memoize // remember first evaluation result [...] public static SomeLongEvaluations (...) : int [...] { MathLib.CalculateNthPrime(10000000) } DependencyProperty // WPF dependency property public DependencyPropertySample { get; set; } [...] public static Main (...) : void {/* <b>syntax</b> <b>macro</b> [...] "json" [...] generates code: JObject.Object(JValue.Number(SomeLongEvaluations (...) [...] )), ("b", JValue.Number(SomeLongEvaluations (...) + 1)))*/ def jObject = json { a: SomeLongEvaluations (...) b: (SomeLongEvaluations (...) + 1)} // object initialization macro [...] "<-" [...] is {{development of}} C# curly brackets object initialization def k = Diagnostics.Process (...) <- [...] { StartInfo <- // can init inner objects properties without ctor call { FileName = [...] "calc.exe"; UseShellExecute = true; } [...] Exited += (...) => WriteLine("Calc done"); // events and delegates } ReadLine (...) }} ...|$|R
40|$|The {{phenomenon}} of monotone likelihood is {{observed in the}} fitting process of a Cox model if the likelihood converges while at least one parameter estimate diverges to ± infinity. Monotone likelihood primarily occurs in small samples with several unbalanced and highly predictive covariates, and with {{a high percentage of}} censoring. A procedure by Firth (1993) originally developed to reduce the bias of maximum likelihood estimates is shown to provide an ideal solution to monotone likelihood (cf. Heinze & Schemper, 2001). It produces finite parameter estimates by means of penalized maximum likelihood estimation. Corresponding Wald tests and confidence intervals are available but it is shown that penalized likelihood ratio tests and profile penalized likelihood confidence intervals are often preferable. The SAS macro FC (Heinze & Ploner, 2002) implements Firth’s penalization to Cox regression and has been available since 2000. This macro was restricted to time-invariant covariates and effects. A new SAS macro program FC 06 was written to enhance the functionality of its predecessor FC by providing options that allow to fit models including time-varying covariates and time-dependent effects. It is even possible to use the macro for conditional logistic regression analysis of 1 : m matched case-control studies. The present report contains the complete User’s Guide to this <b>macro</b> including <b>syntax,</b> computational methods and examples...|$|R
40|$|We {{introduce}} {{a class of}} program editors that present a program using a rich set of transformations; we call these kinds of editors composable presentation editors. Proper use {{of these kinds of}} editors appears to lead to more expressive programs—programs whose structure are aligned with the problem they are trying to solve. By default, the composable presentation editor presents program elements textually as concrete syntax and enables typical editor commands on the program. Metadata on program elements control how the transformations are applied. Customized metadata can re-order, pictorialize, collapse, duplicate, or expand the displayed form of program elements and can additionally alter the available editor commands. We have developed a set of presentation techniques to be used by presentation designers (i. e., the programmers who design how a program is presented in the editor). These techniques relate to well-understood programming language design, editor design, and programming best-practices techniques including scoping, higher order functions, refactoring, prettyprinting, naming conventions, syntax highlighting, and text hovers. We introduce two implementations of composable presentation editors and a number of examples showing how programs can be made more expressive when presentation techniques are properly used. The first implementation is the ETMOP, an open editor, where a metaobject protocol is provided that allows language and editor designers to customize the way program elements are displayed. These customizations are called presentation extensions and the corresponding presentation extension protocol acts in a way similar to the way that <b>syntax</b> <b>macros</b> extend the <b>syntax</b> of a language. The second implementation is Embedded CAL, a closed editor that uses these presentation techniques to embed one language (CAL) inside a host language (Java) through the use of presentation techniques, without changing the syntax or compiler of either language...|$|R
40|$|We {{describe}} the programming language FOBS-X (Extensible FOBS). FOBS-X is interpreted, and is {{intended as a}} universal scripting language. One {{of the more interesting}} features of FOBS-X is its ability to be extended, allowing it to be adopted to new scripting environments. FOBS-x is structured as a core language that is parsed by the interpreter, and an extended language that is translated to the core by <b>macro</b> expansion. The <b>syntax</b> of the language can easily be modified by writing new macros. The library for FOBS-X is reconfigurable, allowing the semantics of the language to be modified, and adapted to facilitate the interaction with interfaces to new scripting environments. This paper focuses on the tools used for the semantic extension of the language. A tool called FEDELE has been developed, allowing the user to add library modules to the FOBS-X library. In this way the semantics of the language can be enhanced, and the language can be adapted...|$|R
40|$|Over {{the past}} two decades, Scheme macros have evolved into a pow-erful API for the {{compiler}} front-end. Like Lisp macros, their predecessors, Scheme macros expand source programs into a small core language; unlike Lisp systems, Scheme macro expanders preserve lexical scoping, and advanced Scheme macro systems handle other important properties such as source location. Using such macros, Scheme programmers now routinely develop the ultimate abstraction: embedded domain-specific programming languages. Unfortunately, a typical Scheme programming environment provides little support for macro development. The tools for understanding macro expansion are poor, which {{makes it difficult for}} experienced programmers to debug their macros and for novices to study the behavior of macros. At the same time, the language for specifying macros is limited in expressive power, and it fails to validate syntactic correctness of macro uses. This dissertation presents tools for macro development that specifically address these two needs. The first is a stepping debugger specialized to the pragmatics of hygienic macros. The second is a system for writing <b>macros</b> and specifying <b>syntax</b> that automatically validates macro uses and reports syntax errors...|$|R
40|$|Lisp and Scheme have {{demonstrated}} the power of macros to enable programmers to evolve and craft languages. In languages with more complex <b>syntax,</b> <b>macros</b> have had less success. In part, this has been due to the difficulty in building expressive hygienic macro systems for such languages. JavaScript in particular presents unique challenges for macro systems due to ambiguities in the lexing stage that force the JavaScript lexer and parser to be intertwined. In this paper we present a novel solution to the lexing ambiguity of JavaScript that enables us to cleanly separate the JavaScript lexer and parser by recording enough history during lexing to resolve ambiguities. We give an algorithm for this solution along with a proof that it does in fact correctly resolve ambiguities in the language. Though the algorithm and proof we present is specific to JavaScript, the general technique {{can be applied to}} other languages with ambiguous grammars. With lexer and parser separated, we then implement an expressive hygienic macro system for JavaScript called sweet. js...|$|R
40|$|We {{present the}} results of the project, which aims to design and {{implement}} a high-level domain-specific language for programming interactive Web services. A fundamental aspect of the development of the World Wide Web during the last decade is the gradual change from static to dynamic generation of Web pages. Generating Web pages dynamically in dialog with the client has the advantage of providing up-to-date and tailor-made information. The development of systems for constructing such dynamic Web services has emerged as a whole new research area. The language is designed by analyzing its application domain and identifying fundamental aspects of Web services inspired by problems and solutions in existing Web service development languages. The core of the design consists of a session-centered service model together with a flexible template-based mechanism for dynamic Web page construction. Using specialized program analyses, certain Web-specific properties are verified at compile time, for instance that only valid HTML 4. 01 is ever shown to the clients. In addition, the design provides high-level solutions to form field validation, caching of dynamic pages, and temporal-logic based concurrency control, and it proposes <b>syntax</b> <b>macros</b> for making highly domain-specific languages. The language is implemented via widely available Web technologies, such as Apache on the server-side and JavaScript and Java Applets on the client-side. We conclude with experience and evaluation of the project...|$|R
40|$|The term “extensible language” is {{especially}} used when a language allows {{the extension of}} its own concrete syntax and {{the definition of the}} semantics of new constructs. Most popular tools designed for automatic generation of syntactic analysers do not offer any adequate resources for the specification of extensible languages. When used in the implementation of features like <b>syntax</b> <b>macro</b> definitions, these tools usually impose severe restrictions. For example, it may be required that macro definitions and their use reside indifferent files; or it may be impossible to perform the syntax analysis in one single pass. We claim that {{one of the main reasons}} for these limitations is the lack of appropriate formal models for the definition of the syntax of extensible languages. This paper presents the design and formal definition of Adaptable Parsing Expression Grammars, an extension to the Parsing Expression Grammar (PEG) model that allows the manipulation of its own production rules during the analysis of an input string. The proposed model compares favourably with similar approaches for the definition of the syntax of extensible languages. An implementation of the model is also presented, simulating the behavior of packrat parsers. Among the challenges for this implementation is the use of attributes and on the fly modifications on the production rules at parse time, features not present in standard PEG. This approach has been used on the definition of a real extensible language, and initial performance tests suggest that the model may work well in practice...|$|R
40|$|The present paper {{introduces}} {{the concept of}} adaptive automata as an alternative formal tool for describing context-dependent languages. This formal framework {{has the advantage of}} allowing easy mapping of a language description into an efficient parser for that language. Such a good performance is due to the potential hierarchical structure adaptive automata may exhibit, allowing natural construction of acceptors no more complex than strictly needed by each particular language. Efficiency is also due to the way adaptive automata operate, by changing according to its input, including and discarding transitions as needed to parse the particular input text - adaptive automata start from an initial self-modifying version, and evolve through a path of intermediate configurations until a final configuration is reached, when the source text is exhausted. The evolution from an automatoh's configuration to the next one may be designed to occur strictly when a construct is found which is not recognized by the current configuration of the automaton. So, one may view the acceptance of a particular sentence as a sequence of recognitions of its substrings, each operated by the corresponding configuration of the adaptive automaton. That offers a practical way for efficiently accepting context-dependent languages in a purely syntactical way, allowing full treatment for syntactical aspects of the language such as dynamic syntax and the socalled static semantics. Then, the use of adaptive automata brings the possibility of handling in a purelly syntactical way several autentically syntactical concepts, such as predefined words, symbol-tables, scoping, type-checking, argument-to-parameter matching, macro definitions and expansions, <b>syntax</b> <b>macros</b> for defining new languag [...] ...|$|R

