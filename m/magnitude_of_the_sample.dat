13|10000|Public
5000|$|The first solution, {{proposed}} by Malmquist in his 1922 work, was {{to correct the}} calculated average absolute <b>magnitude</b> (...) <b>of</b> <b>the</b> <b>sample</b> back to the true average absolute magnitude (M0). The correction would be ...|$|E
40|$|This study aims to {{determine}} the effect on the brand image in the consumer 2 ̆ 7 s decision to buy furniture in Raffa Meubel Sand Pengaraian. The study population is the entire consumer Raffa Furniture totaling 213 people and {{to determine}} the <b>magnitude</b> <b>of</b> <b>the</b> <b>sample</b> using the formula Slovin the number sebayak 68 respondents and the sampling technique is random sampling. Analysis of the data using a questionnaire with Likert scale measurement and testing using simple linear regression. Based on the test results show the value of R 2 = 0. 401 indicates that the influence of brand image on purchase decisions in at 40. 10...|$|E
40|$|Recently, the {{aggregate}} association index (or AAI) was proposed {{to quantify the}} strength of the association between two dichotomous variables given only the marginal, or aggregate, data from a 2 x 2 contingency table. One feature of this index is that it is susceptible to changes in the sample size; as the sample size increases, so too does the AAI even when the relative distribution of {{the aggregate}} data remains unchanged. Therefore {{the true nature of the}} association between the variables is at great risk of being masked by the <b>magnitude</b> <b>of</b> <b>the</b> <b>sample</b> size. This paper proposes two adjustments to the AAI that overcome this problem. We consider a simple example using Fisher's twin criminal data to demonstrate the application of the AAI and its adjustments...|$|E
3000|$|... and its {{associated}} signal <b>sample</b> enters <b>the</b> new active set. This choice ensures that no other <b>sample</b> exceeds <b>the</b> <b>magnitude</b> <b>of</b> <b>the</b> <b>samples</b> in <b>the</b> current active set {{because we have the}} smallest possible reduction in the peak magnitude.|$|R
40|$|Thesis (M. Ed.) [...] University of Melbourne, 1975 An {{empirical}} sampling {{approach was}} used to assess <b>the</b> <b>magnitude</b> <b>of</b> <b>the</b> <b>sampling</b> errors <b>of</b> statistics which describe a recursive causal model based on survey data gathered with four complex sample designs which are commonly used in educational research. <b>The</b> influence <b>of</b> <b>the</b> complex <b>sample</b> designs on sampling errors was shown to be capable of seriously distorting statistical confidence intervals. The jackknife and the balanced half-sample error estimation techniques were applied to this recursive causal model in order to evaluate. their capacity for estimating sampling errors from single samples of data. Restricted Access: Staff and Students <b>of</b> <b>the</b> University Onl...|$|R
40|$|Data {{precision}} for {{rainfall intensity}} and runoff rate using tipping bucket technology {{is controlled by}} the bucket volume, V, the catchment area, A, and <b>the</b> <b>sampling</b> interval, t. <b>The</b> maximum absolute error is less than V(A. t) - 1 and the standard error is given by V(v _ 6 A. t) - 1, both of which are independent <b>of</b> <b>the</b> <b>magnitude</b> <b>of</b> rainfall intensity and runoff rate being measured. Therefore, the relative error can be quite large during events with low rainfall intensity or runoff rate. <b>The</b> <b>magnitude</b> <b>of</b> <b>the</b> <b>sampling</b> error given above can be used to assist in experiment and equipment design and in data analysis. Griffith Sciences, Griffith School of EngineeringFull Tex...|$|R
40|$|We {{have used}} {{scanning}} tunneling microscopy to identify individual phosphorus dopant atoms near the clean silicon (100) -(2 x 1) reconstructed surface. The charge-induced band bending signature associated with the dopants shows up as an enhancement in both filled and empty states and {{is consistent with the}} appearance of n-type dopants on compound semiconductor surfaces and passivated Si(100) -(2 x 1). We observe dopants at different depths and see a strong dependence of the signature on the <b>magnitude</b> <b>of</b> <b>the</b> <b>sample</b> voltage. Our results suggest that, on this clean surface, the antibonding surface state band acts {{as an extension of the}} bulk conduction band into the gap. The positively charged dimer vacancies that have been observed previously appear as depressions in the filled states, as opposed to enhancements, because they disrupt these surface bands. Comment: 4 pages, 3 figures. TeX for OSX from Wierde...|$|E
40|$|This article {{describes}} {{a method for}} the quantitative determination of the toxic military agent bis(2 -chloroethyl) -sulfide (or sulfur mustard) in air or other similar gases at parts-per-trillion levels. The method entails the adsorptive trapping of mustard vapor {{on a bed of}} Tenax-GC, followed by the transfer of trapped mustard to a smaller sorbent bed and the thermal desorption of the mustard into a gas chromatograph equipped with a flame photometric detector. Interference from an oxidizing gas (probably NO 2) in the air is circumvented by sampling through a filter impregnated with triethanolamine (TEA) which selectively attenuates the NO 2 while transmitting the mustard. The method is found to possess adequate accuracy and precision for most purposes, and the detection limit is observed to depend on the <b>magnitude</b> <b>of</b> <b>the</b> <b>sample</b> or sorbent background response rather than on instrument noise, adsorptive sampling capacity, or other fundamental limitations of the hardware...|$|E
40|$|It is {{well known}} that for a fixed number of {{independent}} identically distributed summands with light tail, large values of the sample mean are obtained only when all the summands take large values. This paper explores this property as the number of summands tends to infinity. It provides the order of <b>magnitude</b> <b>of</b> <b>the</b> <b>sample</b> mean for which all summands are in some interval containing this value and it also explores the width of this interval with respect to the distribution of the summands in their upper tail. These results are proved for summands with log-concave or nearly log concave densities. Making use of some extension of the Erdös-Rényi law of large numbers it explores the forming of aggregates in a sequence of i. i. d. random variables. As a by product the connection is established between large exceedances of the local slope of a random walk on growing bins and the theory of extreme order statistics...|$|E
40|$|Applying {{nonlinear}} optimization strategies {{directly to}} complex multidisciplinary {{systems can be}} prohibitive when <b>the</b> complexity <b>of</b> <b>the</b> simulation codes is large. Increasingly, response surface approximations(RSAs), and specifically quadratic approximations, are being integrated with nonlinear optimizers {{in order to reduce}} the CPU time required for <b>the</b> optimization <b>of</b> complex multidisciplinary systems. RSAs provide a computationally inexpensive lower fidelity representation <b>of</b> <b>the</b> system performance. <b>The</b> curse <b>of</b> dimensionality is a major drawback in <b>the</b> implementation <b>of</b> these approximations as <b>the</b> amount <b>of</b> required data grows quadratically with <b>the</b> number <b>of</b> design variables. In this paper a novel technique to reduce <b>the</b> <b>magnitude</b> <b>of</b> <b>the</b> <b>sampling</b> to O(n) is presented. The technique uses prior information to approximate <b>the</b> eigenvectors <b>of</b> <b>the</b> Hessian matrix and only requires the eigenvalue...|$|R
40|$|<b>The</b> theory <b>of</b> {{trigonometric}} polynominals {{has attracted}} new attention with <b>the</b> application <b>of</b> polynominals to multicarrier communication systems (OFDM, DMT, MC-CDMA). It is also well-known that polynominals {{have been successfully}} applied to system identification. Following the classical theory these signals can be described by sampling sets such {{that there is a}} unique relation between <b>the</b> <b>samples</b> and <b>the</b> polynominals. However, it is a problem in many applications that <b>the</b> peak <b>magnitude</b> <b>of</b> <b>the</b> <b>samples</b> does not indicate <b>the</b> peak <b>magnitude</b> <b>of</b> <b>the</b> polynominal. For example amplifier clipping in multicarrier systems leading to performance degradation is caused by the continuous signals whereas the signal is digitally generated and processed. This paper addresses this problem and introduces <b>the</b> method <b>of</b> oversampling in order to relate <b>the</b> peak <b>magnitudes</b> <b>of</b> <b>the</b> polynominal and <b>the</b> <b>samples.</b> In particular, a practical formula is derived in order to trade off signal processing effort for performance. This formula improves a result given by [1, 2]...|$|R
40|$|Subjects saw {{samples from}} each of two populations of numbers and made {{intuitive}} inferences about which population had the larger variance. Then they either estimated <b>the</b> ratios <b>of</b> <b>the</b> variances or stated their confidence (subjective probability) in their inferences. The ratios were used to infer <b>the</b> subjective <b>magnitudes</b> <b>of</b> <b>the</b> <b>sample</b> variances; they were systematically inaccurate because of a tendency to underweight deviant sample data and because the subjects regard variance among large numbers as less variable than variance among small numbers. Then, confidence in the inferences about the population variances was compared to the probabilities that would have resulted if <b>the</b> ratios <b>of</b> <b>sample</b> variances had actually been the ratios that the subjects reported. Confidence was systematically related to these probabilities {{but it was always}} lower. The results are discussed in terms <b>of</b> <b>the</b> conservatism findings reported in other investigations of intuitive statistics. A Bayesian F-test is appended...|$|R
40|$|Traditional {{tests for}} {{conditional}} heteroscedasticity {{are based on}} testing for significant autocorrelations of squared or absolute observations. In the context of high frequency time series of financial returns, these autocorrelations are often positive and very persistent, although their magnitude is usually very small. Moreover, the sample autocorrelations are severely biased towards zero, especially if the volatility is highly persistent. Consequently, {{the power of the}} traditional tests is often very low. In this paper, we propose a new test that takes into account not only the <b>magnitude</b> <b>of</b> <b>the</b> <b>sample</b> autocorrelations but also possible patterns among them. This additional information makes the test more powerful in situations of empirical interest. The asymptotic distribution of the new statistic is derived and its finite sample properties are analyzed by means of Monte Carlo experiments. The performance of the new test is compared with various alternative tests. Finally, we illustrate the results analyzing several time series of financial returns. Publicad...|$|E
40|$|Abstract: Traditional {{tests for}} {{conditional}} heteroscedasticity {{are based on}} testing for signicant autocorrelations of squared or absolute observations. In the context of high frequency time series of nancial returns, these autocorrelations are often positive and very persistent, although their magnitude is usually very small. More-over, the sample autocorrelations are severely biased towards zero, especially if the volatility is highly persistent. Consequently, {{the power of the}} traditional tests is often very low. In this paper, we propose a new test that takes into account not only the <b>magnitude</b> <b>of</b> <b>the</b> <b>sample</b> autocorrelations but also possible patterns among them. This additional information makes the test more powerful in situations of empirical interest. The asymptotic distribution of the new statistic is derived and its nite sample properties are analyzed by means of Monte Carlo experiments. The performance of the new test is compared with various alternative tests. Finally, we illustrate the results analyzing several time series of nancial returns. Key words and phrases: Autocorrelations of non-linear transformations, GARCH, long-memory, McLeod-Li statistic, stochastic volatility...|$|E
40|$|The optical {{morphological}} and {{photometric properties}} of 79 low redshift radio galaxies are discussed. It is {{found that most}} radio galaxies are luminous bulge dominated systems similar to normal non radio giant ellipticals. The average absolute <b>magnitude</b> <b>of</b> <b>the</b> <b>sample</b> is = - 23. 98, with a clear trend for FR I sources to be # 0. 5 mag brighter than FR II galaxies. In about 40 % of the objects we find an excess {{of light in the}} nucleus attributable to the presence of a nuclear point source. This contributes on average for # 1 - 2 % of the total flux from the host galaxy. Radio galaxies follow the same e - R e relationship of normal (non-active) elliptical galaxies. The distribution of ellipticity, the amount of twisting and shape of isophotes do not di#er significantly from other ellipticals. These results support a scenario where radio emission is little related with the overall properties of the host galaxy. 1 Introduction Comparing radio-loud and radio-quiet elliptical [...] ...|$|E
40|$|A {{representation}} and interpretation <b>of</b> <b>the</b> area under a {{receiver operating characteristic}} (ROC) curve obtained by the “rating ” method, or by mathematical predictions based on patient characteristics, is presented. It is shown that in such a setting the area represents {{the probability that a}} randomly chosen diseased subject is (correctly) rated or ranked with greater suspicion than a randomly chosen nondiseased subject. Moreover, this probability of a correct ranking is the same quantity that is estimated by the already wellstudied nonparametric Wilcoxon statistic. These two relationships are exploited to (a) provide rapid closed-form expressions for <b>the</b> approximate <b>magnitude</b> <b>of</b> <b>the</b> <b>sampling</b> variability, i. e., standard error that one uses to accompany the area under a smoothed ROC curve, (b) guide in determining <b>the</b> size <b>of</b> <b>the</b> <b>sample</b> required to provide a sufficiently reliable estimate of this area, and (c) determine how large sample sizes should be to ensure that one can statistically detect differences in <b>the</b> accuracy <b>of</b> diagnostic techniques...|$|R
30|$|We {{developed}} a PF tracker adapted to our NIR facial points tracking problem based on Harris corner detection. Some normalization functions are applied on <b>the</b> <b>samples</b> scoring and on geometric and appearance features. They {{are used to}} combine different measure values to normalize their <b>magnitudes.</b> Normalization <b>of</b> <b>the</b> <b>sample</b> scores might be done by any function, but analytical normalization functions allow tuning <b>of</b> <b>the</b> score values.|$|R
40|$|Maloney and Wandell (1984) {{describe}} a model <b>of</b> <b>the</b> response <b>of</b> a single visual channel to weak test lights. The initial channel response is a linearly filtered version <b>of</b> <b>the</b> stimulus. The filter output is randomly sampled over time. Each time a sample occurs {{there is some}} probability increasing with <b>the</b> <b>magnitude</b> <b>of</b> <b>the</b> <b>sampled</b> response - that a discrete detection event is generated. Maloney and Wandell derive <b>the</b> statistics <b>of</b> <b>the</b> detection events. In this paper a test is conducted <b>of</b> <b>the</b> hypothesis that the reaction time responses to <b>the</b> presence <b>of</b> a weak test light are initiated at the first detection event. This {{makes it possible to}} extend <b>the</b> application <b>of</b> <b>the</b> model to lights that are slightly above threshold, but still within the linear operating range <b>of</b> <b>the</b> visual system. A parameter-free prediction <b>of</b> <b>the</b> model proposed by Maloney and Wandell for lights detected by this statistic is tested. The data are in agreement with the prediction...|$|R
40|$|The KPNO International Spectroscopic Survey (KISS) is an objective-prism {{survey for}} extragalactic emission-line objects. It {{combines}} {{many of the}} features of previous slitless spectroscopic surveys with the advantages of modern CCD detectors, and is the first purely digital objective-prism survey for emissionline galaxies. Here we present the first list of emission-line galaxy candidates selected from our blue spectral data, which cover the wavelength range 4800 – 5500 ˚A. In most cases, the detected emission line is [O III]λ 5007. The current survey list covers a one-degree-wide strip located at δ = 29 ◦ 30 ′ (B 1950. 0) and spanning the right ascension range 8 h 30 m to 17 h 0 m. An area of 116. 6 deg 2 is covered. A total of 223 candidate emission-line objects have been selected for inclusion in the survey list (1. 91 deg − 2). We tabulate accurate coordinates and photometry for each source, as well as estimates of the redshift, emission-line flux, and equivalent width based on measurements of the digital objective-prism spectra. The median apparent <b>magnitude</b> <b>of</b> <b>the</b> <b>sample</b> is B = 18. 2, and galaxies with redshifts approaching...|$|E
40|$|The {{purpose of}} this study was to examine the {{relationship}} between social support and health outcome variables, and the effect size of social support on health outcomes. Meta-analysis was used to synthesize the primary studies identified initially from a computer search of the literature in Taiwan. Through preliminary screening related to the inclusion criteria, 165 dissertations and theses and 43 journal articles were included in this study. Finally, 182 primary studies, including 145 dissertations and theses and 37 journal articles, were retained after eliminating outliers of each outcome variable to achieve homogeneity. Based on Smith's four modes of health, 16 health outcome variables were used. Health status, physical symptoms and responses, psychologic symptoms and responses, and depression were categorized as clinical variables. Role function and behaviors and role burden were categorized as role-function variables. Physical adjustment, psychosocial adjustment, adjustment of life, coping behavior, and stress were categorized as adaptive variables. Health belief, health promotion behavior, quality of life, well-being, and self-actualization were categorized as eudemonistic variables. Other than physical adjustment, social support could significantly predict all health outcomes (p < 0. 0001). The results provided information not only on the <b>magnitude</b> <b>of</b> <b>the</b> <b>sample</b> size required to achieve statistical significance between social support and each outcome variable as a measure of health in future studies, but also on strategies to guide further intervention programs and to evaluate their effectiveness...|$|E
40|$|New {{estimates}} of the distances of 36 nearby galaxies are presented based on accurate distances of galactic Cepheids obtained by Gieren, Fouque and Gomez (1998) from the geometrical Barnes-Evans method. The concept of 'sosie' is applied to extend the distance determination to extragalactic Cepheids without assuming the linearity of the PL relation. Doing so, the distance moduli are obtained in a straightforward way. The correction for extinction is made using two photometric bands (V and I) according to the principles introduced by Freedman and Madore (1990). Finally, the statistical bias due to the incompleteness of the sample is corrected according to the precepts introduced by Teerikorpi (1987) without introducing any free parameters (except the distance modulus itself in an iterative scheme). The final distance moduli depend on the adopted extinction ratio {R_V}/{R_I} and on the limiting apparent <b>magnitude</b> <b>of</b> <b>the</b> <b>sample.</b> A comparison with the distance moduli recently published by the Hubble Space Telescope Key Project (HSTKP) team reveals a fair agreement when the same ratio {R_V}/{R_I} is used but shows a small discrepancy at large distance. In order to bypass the uncertainty due to the metallicity effect it is suggested to consider only galaxies having nearly the same metallicity as the calibrating Cepheids (i. e. Solar metallicity). The internal uncertainty of the distances is about 0. 1 magnitude but the total uncertainty may reach 0. 3 magnitude. Comment: 12 pages, 4 figures, access to a database of extragalactic Cepheids. Astronomy & Astrophysics (in press) 200...|$|E
5000|$|... (1) <b>the</b> {{total number}} <b>of</b> organisms studied be > 15(2) <b>the</b> minimum number <b>of</b> groups of organisms studied be > 5 (3) <b>the</b> density <b>of</b> <b>the</b> organisms should vary {{by at least}} 2 orders <b>of</b> <b>magnitude</b> within <b>the</b> <b>sample</b> ...|$|R
40|$|In this paper, we {{show that}} <b>the</b> order <b>of</b> <b>magnitude</b> <b>of</b> <b>the</b> finite <b>sample</b> bias <b>of</b> <b>the</b> {{estimator}} <b>of</b> Bun and Kiviet (2006) reduces from O(T/N) to O(1 /N) if the original level model is transformed by the upper triangular Cholesky factorization <b>of</b> <b>the</b> inverse <b>of</b> <b>the</b> pseudo variance matrix of error component ui wherein true values <b>of</b> <b>the</b> variances <b>of</b> individual effects and disturbances may not be used. Some variants <b>of</b> <b>the</b> system GMM estimator {{that are associated with}} the Cholesky-transformed model are also discussed. ...|$|R
40|$|International audienceA {{stochastic}} model <b>of</b> <b>the</b> microstructure <b>of</b> rainfall {{is used to}} derive explicit expressions for <b>the</b> <b>magnitude</b> <b>of</b> <b>the</b> <b>sampling</b> fluctuations in rainfall properties estimated from raindrop size measurements in stationary rainfall. The model is a marked point process, in which the points represent the drop centers, assumed to be uniformly distributed in space. This assumption, which is supported both by theoretical and by empirical evidence, implies that during periods <b>of</b> stationary rainfall <b>the</b> number <b>of</b> drops in a sample volume follows a Poisson distribution. The marks represent the drop sizes, assumed to be distributed independent of their positions according to some general drop size distribution. Within this framework, it is shown analytically how <b>the</b> <b>sampling</b> distribution <b>of</b> <b>the</b> estimator <b>of</b> any bulk rainfall variable (such as liquid water content, rain rate, or radar reflectivity) in stationary rainfall converges from a strongly skewed distribution to a (symmetrical) Gaussian distribution with increasing <b>sample</b> size. <b>The</b> relevant parameter controlling this evolution is <b>the</b> average number <b>of</b> drops in <b>the</b> <b>sample</b> ns. For a given <b>sample</b> size, <b>the</b> skewness <b>of</b> <b>the</b> <b>sampling</b> distribution {{is found to be}} more pronounced for higher order moments <b>of</b> <b>the</b> drop size distribution. For instance, <b>the</b> <b>sampling</b> distribution <b>of</b> <b>the</b> normalized mean diameter becomes nearly Gaussian for ns > 10, while <b>the</b> <b>sampling</b> distribution <b>of</b> <b>the</b> normalized rain rate remains skewed for ns not, vert, similar 500. Additionally, it is shown analytically that, as a result <b>of</b> <b>the</b> mentioned skewness, the median Q 50 as an estimator of a bulk rainfall variable always underestimates its population value Qp in stationary rainfall. <b>The</b> ratio <b>of</b> <b>the</b> former to the latter is found to be View the MathML source, where b is a constant depending on the drop size distribution. For bulk rainfall variables this constant is positive and therefore the median always underestimates the population value. This provides a theoretical confirmation and explanation of previously published simulation results. Finally, relationships between <b>the</b> expected number <b>of</b> raindrops in <b>the</b> <b>sample</b> ns and <b>the</b> rain rate are established for different parametric forms <b>of</b> <b>the</b> raindrop size distribution. These relationships are first compared to experimental results and then used to provide examples of sampling distributions of bulk rainfall variables (in this case rain rate) for different values <b>of</b> <b>the</b> average rain rate and different integration times <b>of</b> <b>the</b> disdrometric device involved (in this case a Joss–Waldvogel disdrometer). <b>The</b> practical relevance <b>of</b> these results is (1) that they provide exact solutions to <b>the</b> <b>sampling</b> problem during (relatively rare) periods of stationary rainfall (e. g., drizzle), and (2) that they provide a lower bound to <b>the</b> <b>magnitude</b> <b>of</b> <b>the</b> <b>sampling</b> problem in <b>the</b> general situation where sampling fluctuations and natural variability co-exist...|$|R
40|$|We {{analyze the}} K band luminosities {{of a sample}} of galactic long-period {{variables}} using parallaxes measured by the Hipparcos mission. The parallaxes are in most cases re-computed from the Hipparcos Intermediate Astrometric Data using improved astrometric fits and chromaticity corrections. The K band magnitudes are taken from the literature and from measurements by COBE, and are corrected for interstellar and circumstellar extinction. The sample contains stars of several spectral types: M, S and C, and of several variability classes: Mira, semiregular SRa, and SRb. We find that the distribution of stars in the period-luminosity plane is independent of circumstellar chemistry, but that the different variability types have different P-L distributions. Both the Mira variables and the SRb variables have reasonably well-defined period-luminosity relationships, but with very different slopes. The SRa variables are distributed between the two classes, suggesting {{that they are a}} mixture of Miras and SRb, rather than a separate class of stars. New period-luminosity relationships are derived based on our revised Hipparcos parallaxes. The Miras show a similar period-luminosity relationship to that found for Large Magellanic Cloud Miras by Feast et al. (1989). The maximum absolute K <b>magnitude</b> <b>of</b> <b>the</b> <b>sample</b> is about - 8. 2 for both Miras and semi-regular stars, only a little fainter than the expected AGB limit. We show that the stars with the longest periods (P> 400 d) have high mass loss rates and are almost all Mira variables. Comment: Comments welcome. Submitted to A&A 11 pages, 7 figs, 3 table...|$|E
40|$|A {{stochastic}} model <b>of</b> <b>the</b> microstructure <b>of</b> rainfall {{is used to}} derive explicit expressions for <b>the</b> <b>magnitude</b> <b>of</b> <b>the</b> <b>sampling</b> fluctuations in rainfall properties estimated from raindrop size measurements in stationary rainfall. The model is a marked point process, in which the points represent the drop centers, assumed to be uniformly distributed in space. This assumption, which is supported both by theoretical and by empirical evidence, implies that during periods <b>of</b> stationary rainfall <b>the</b> number <b>of</b> drops in a sample volume follows a Poisson distribution. The marks represent the drop sizes, assumed to be distributed independent of their positions according to some general drop size distribution. Within this framework, it is shown analytically how <b>the</b> <b>sampling</b> distribution <b>of</b> <b>the</b> estimator <b>of</b> any bulk rainfall variable (such as liquid water content, rain rate, or radar reflectivity) in stationary rainfall converges from a strongly skewed distribution to a (symmetrical) Gaussian distribution with increasing <b>sample</b> size. <b>The</b> relevant parameter controlling this evolution is <b>the</b> average number <b>of</b> drops in <b>the</b> <b>sample</b> n(s). For a given <b>sample</b> size, <b>the</b> skewness <b>of</b> <b>the</b> <b>sampling</b> distribution {{is found to be}} more pronounced for higher order moments <b>of</b> <b>the</b> drop size distribution. For instance, <b>the</b> <b>sampling</b> distribution <b>of</b> <b>the</b> normalized mean diameter becomes nearly Gaussian for n(s) > 10, while <b>the</b> <b>sampling</b> distribution <b>of</b> <b>the</b> normalized rain rate remains skewed for n(s) similar to 500. Additionally, it is shown analytically that, as a result <b>of</b> <b>the</b> mentioned skewness, the median Q(50) as an estimator of a bulk rainfall variable always underestimates its population value Q(p) in stationary rainfall. <b>The</b> ratio <b>of</b> <b>the</b> former to the latter is found to be Q(50) /Q(p) = 1 - b/n(s) + 0 (n(s) (- 2)), where b is a constant depending on the drop size distribution. For bulk rainfall variables this constant is positive and therefore the median always underestimates the population value. This provides a theoretical confirmation and explanation of previously published simulation results. Finally, relationships between <b>the</b> expected number <b>of</b> raindrops in <b>the</b> <b>sample</b> n(s) and <b>the</b> rain rate are established for different parametric forms <b>of</b> <b>the</b> raindrop size distribution. These relationships are first compared to experimental results and then used to provide examples of sampling distributions of bulk rainfall variables (in this case rain rate) for different values <b>of</b> <b>the</b> average rain rate and different integration times <b>of</b> <b>the</b> disdrometric device involved (in this case a Joss-Waldvogel disdrometer). <b>The</b> practical relevance <b>of</b> these results is (1) that they provide exact solutions to <b>the</b> <b>sampling</b> problem during (relatively rare) periods of stationary rainfall (e. g., drizzle), and (2) that they provide a lower bound to <b>the</b> <b>magnitude</b> <b>of</b> <b>the</b> <b>sampling</b> problem in <b>the</b> general situation where sampling fluctuations and natural variability co-exist. (c) 2005 Elsevier B. V. All rights reserved...|$|R
40|$|Photometry {{has been}} {{obtained}} for <b>magnitude</b> limited <b>samples</b> <b>of</b> galaxies in <b>the</b> two rich clusters Abell 665 (37 galaxies) and Abell 2218 (61 galaxies). Both clusters have a redshift <b>of</b> 0. 18. <b>The</b> limiting <b>magnitude</b> <b>of</b> <b>the</b> <b>samples</b> is 19 m in the I-band. Spectroscopy has been obtained for seven galaxies in A 665 and nine galaxies in A 2218, {{all of which}} also have available photometry. Spectroscopy has been obtained for two additional galaxies in A 2218, {{one of which is}} a background galaxy. Effective radii re and mean surface brightnesses e were derived from the photometry. The typical uncertainties are ± 0. 078 in log re and ± 0. 12 in log e. The combination log re+ 0. 82 loge that enters the Fundamental Plane (FP) has an uncertainty of only ± 0. 018. The spectroscopy was used for measurements <b>of</b> <b>the</b> velocity dispersions, σ. The typical uncertainty is ± 0. 023 on log σ. The data are used to establish the FP, log re = α log σ + β log e + γ, for the clusters. The FP for these two clusters adds important knowledge about <b>the</b> properties <b>of</b> E and S 0 galaxies in the relatively unexplored redshift interval 0. 05 to 0. 3. We hav...|$|R
5000|$|This group {{action of}} diffeomorphisms on ODF reorients the ODF and {{reflects}} changes in both <b>the</b> <b>magnitude</b> <b>of</b> [...] and <b>the</b> <b>sampling</b> directions <b>of</b> [...] due to affine transformation. t guarantees that <b>the</b> volume fraction <b>of</b> fibers oriented toward {{a small patch}} must remain the same after the patch is transformed.|$|R
40|$|<b>The</b> {{development}} <b>of</b> <b>the</b> theory <b>of</b> {{estimation of}} gametic disequilibrium for multiallelic systems is particularly necessary, since {{a large number}} <b>of</b> <b>the</b> genetic markers available at present are highly polymorphic multiallelic systems. The D' coefficient is one <b>of</b> <b>the</b> most commonly used measures <b>of</b> <b>the</b> extent <b>of</b> overall disequilibrium between all possible pairs of alleles at two multiallelic loci. Nevertheless, <b>the</b> <b>sampling</b> properties <b>of</b> this measure of overall disequilibrium, are to date, unknown. In this work, we have derived explicit expressions by large-sample theory to compute <b>the</b> approximate <b>sampling</b> variance <b>of</b> D^' between pairs of multiallelic loci, when samples of haplotypes are taken from populations. Formulae for calculating <b>the</b> asymptotic <b>sampling</b> variance were checked by Monte Carlo simulation. In addition, <b>the</b> <b>magnitude</b> <b>of</b> <b>the</b> <b>sampling</b> variance <b>of</b> D^' was investigated under different scenarios of disequilibrium between multiallelic loci. Extensive simulations were also carried out for describing <b>the</b> <b>sampling</b> distribution <b>of</b> D^', conditioned on <b>the</b> <b>sample</b> size, number <b>of</b> alleles and their frequencies, and disequilibrium components. It was found that <b>the</b> <b>sampling</b> distribution <b>of</b> D^' generally approaches well the theoretical normal distribution for experimental sample sizes, particularly when loci have many alleles. Disequilibrium data between microsatellite loci of human chromosome 11 p are used for illustration. These investigations increase substantially our knowledge about this widely used measure of overall disequilibrium, which is relevant to evaluate disequilibrium between multiallelic loci in populations...|$|R
40|$|The paper {{presents}} a theoretical analysis <b>of</b> <b>the</b> method that enables optimisation <b>of</b> <b>the</b> transient response <b>of</b> a digital filter or of an arbitrary discrete system, by pre-setting <b>the</b> initial conditions <b>of</b> <b>the</b> inner state description. For a particular {{design of a}} filter, {{it is enough to}} once evaluate coefficients, multiply them by <b>the</b> <b>magnitude</b> <b>of</b> <b>the</b> first <b>sample</b> <b>of</b> <b>the</b> signal and do filtration by using these initial conditions. The method was verified using the simulated, and NMR signals to maintain spectrum baselines correction, and will also be used for <b>the</b> study <b>of</b> an optimum filtration <b>of</b> <b>the</b> NMR signal with a variable instantaneous frequency...|$|R
40|$|In studies <b>of</b> <b>the</b> effects <b>of</b> {{different}} training programmes, one muscle [...] {{most commonly}} the vastus lateralis [...] {{is used for}} the experiment while the contralateral muscle serves as a control, {{at the same time as}} muscle biopsies are taken from both sides. In order to increase <b>the</b> reliability <b>of</b> such studies, <b>the</b> sources and <b>the</b> <b>magnitude</b> <b>of</b> <b>the</b> <b>sampling</b> errors in <b>the</b> biopsy techniques need to be assessed in detail. In this study, cross-sections of whole right and left vastus lateralis muscle from six young sedentary right-handed men were prepared, and the total number and size <b>of</b> fibres and <b>the</b> proportion <b>of</b> <b>the</b> different fibre types were calculated. A significant difference (P less than 0. 05 -P less than 0. 001) between the right and the left muscle was found for at least one <b>of</b> <b>the</b> three variables in each <b>of</b> <b>the</b> six men, but there was no systematic difference and, therefore, no significant right-left difference for the whole group. The maximum difference between the right and the left side for the mean fibre size was 25 % and for the fibre type proportion 5 %; these differences are much smaller than the known variation within individual muscles. In conclusion, any study involving biopsies from both the right and the left vastus lateralis may use either muscle for the experiment while the contralateral muscle serves as a control without leading to systematic <b>sampling</b> error, whereas <b>the</b> errors involved in taking small samples from each muscle are much more important to control and to reduce...|$|R
50|$|Simple back-of-the-envelope test takes <b>the</b> <b>sample</b> maximum {{and minimum}} and computes their z-score, or more {{properly}} t-statistic(number of sample standard deviations that a sample is {{above or below}} <b>the</b> <b>sample</b> mean), and compares it to the 68-95-99.7 rule:if one has a 3σ event (properly, a 3s event) and substantially fewer than 300 samples, or a 4s event and substantially fewer than 15,000 samples, then a normal distribution will understate <b>the</b> maximum <b>magnitude</b> <b>of</b> deviations in <b>the</b> <b>sample</b> data.|$|R
40|$|<b>The</b> {{distribution}} <b>of</b> a contaminant in a bulk mass greatly influences <b>the</b> effectiveness <b>of</b> <b>sampling</b> procedures. Most {{currently used}} sampling guidelines for {{genetically modified organisms}} (GMO) testing have stringent distribution requirements. Specifically, all these protocols are based upon the assumption that GM material, if present, is randomly distributed. Yet, assuming randomness is risky because <b>the</b> applied <b>sampling</b> plan may not provide an accurate estimate <b>of</b> <b>the</b> content <b>of</b> <b>the</b> contaminant. Despite the general consensus that an evaluation <b>of</b> <b>the</b> possibility <b>of</b> non-random distribution of GM materials within lots is a pre-requisite for <b>the</b> definition <b>of</b> effective <b>sampling</b> protocols, no experimental data on <b>the</b> distribution <b>of</b> GMOs in kernel lots are available anywhere in the world. Additionally, to date there is no distribution-free statistical model to estimate sampling errors, thus making <b>the</b> theoretical definition <b>of</b> optimal sampling technique impossible when randomness cannot be assumed. Here we presents preliminary results <b>of</b> <b>the</b> KeLDA project, {{which is the first}} study investigating <b>the</b> distribution <b>of</b> GM materials in large soybean lots imported within the European Union. <b>The</b> extensive variability <b>of</b> GM content, observable both within and among soybean lots, indicates that heterogeneity issues cannot be overlooked when defining sampling strategies for large lots of particulate materials and that randomness cannot be assumed a priori. A new statistical model free from any distribution assumptions, developed in order to estimate <b>the</b> <b>sampling</b> error associated with different sampling protocols together with <b>the</b> risk <b>of</b> false-negative results, is also presented. The new model, which is applicable to any consignment of particulate material with respect to any kind of contamination, presents two novelties: first, freedom from any distribution assumption; second, <b>the</b> possibility <b>of</b> estimating <b>the</b> <b>magnitude</b> <b>of</b> <b>the</b> <b>sampling</b> error associated with different sampling protocols as a function of specific distributional properties. JRC. I. 6 -Biotechnology and GMO...|$|R
40|$|The United States Mine Safety and Health Administration, has {{proposed}} an exposure standard for diesel particulate matter for underground metal and nonmetal mining operations. These limits are {{in terms of}} total carbon as determined by <b>the</b> National Institute <b>of</b> Occupational Safety and Health (NIOSH) Method 5040. Method 5040 is a thermal optical procedure for determining elemental and organic carbon content of an airborne sample. Elemental and organic carbon content <b>of</b> <b>the</b> <b>sample</b> are added to obtain the total carbon concentration. Mining industry comments and concerns on the proposed regulation included: <b>the</b> applicability <b>of</b> Method 5040 to diesel particulate measurements, <b>the</b> <b>sample</b> collection system and the impact various interferences may have on diesel particulate measurements. Concerns on Method 5040 included accuracy, calibration <b>of</b> <b>the</b> instrumentation and calculation <b>of</b> <b>the</b> total carbon concentration. Sample collection issues addressed sample collection method (total dust (open face), respirable dust or submicron particulate). Potential interferences included: sample blanks, carbonaceous minerals (including carbonates), graphitic minerals, oil mist and cigarette smoke. In order to address these concerns, MSHA reviewed the analytical method, and conducted studies to verify <b>the</b> <b>magnitude</b> <b>of</b> <b>the</b> interferences. <b>The</b> <b>sampling</b> and analytical procedures and <b>the</b> results <b>of</b> <b>the</b> supporting tests are described in this paper...|$|R
40|$|<b>The</b> purpose <b>of</b> {{this paper}} is to reduce the {{computational}} complexity per step from O(n^ 2) to O(n) for optimization based on quadratic surrogates, where n is <b>the</b> number <b>of</b> design variables. Applying nonlinear optimization strategies directly to complex multidisciplinary systems can be prohibitively expensive when <b>the</b> complexity <b>of</b> <b>the</b> simulation codes is large. Increasingly, response surface approximations, and specifically quadratic approximations, are being integrated with nonlinear optimizers {{in order to reduce the}} CPU time required for <b>the</b> optimization <b>of</b> complex multidisciplinary systems. For evaluation by the optimizer, response surface approximations provide a computationally inexpensive lower fidelity representation <b>of</b> <b>the</b> system performance. <b>The</b> curse <b>of</b> dimensionality is a major drawback in <b>the</b> implementation <b>of</b> these approximations as <b>the</b> amount <b>of</b> required data grows quadratically with <b>the</b> number n <b>of</b> design variables in the problem. In this paper a novel technique to reduce <b>the</b> <b>magnitude</b> <b>of</b> <b>the</b> <b>sampling</b> from O(n^ 2) to O(n) is presented. The technique uses prior information to approximate <b>the</b> eigenvectors <b>of</b> <b>the</b> Hessian matrix <b>of</b> <b>the</b> response surface approximation and only requires the eigenvalues to be computed by response surface techniques. The technique is implemented in a sequential approximate optimization algorithm and applied to engineering problems of variable size and characteristics. Results demonstrate that a reduction in the data required per step from O(n^ 2) to O(n) points can be accomplished without significantly compromising <b>the</b> performance <b>of</b> <b>the</b> optimization algorithm. A reduction in <b>the</b> time (number <b>of</b> system analyses) required per step from O(n^ 2) to O(n) is significant, even more so as n increases. The novelty lies in how only O(n) system analyses can be used to approximate a Hessian matrix whose estimation normally requires O(n^ 2) system analyses...|$|R
40|$|We present {{photometric}} redshifts for the COSMOS survey {{derived from}} a new code, optimized to yield accurate and reliable redshifts and spectral types of galaxies down to faint magnitudes and redshifts out to z 1. 2. The technique uses χ^ 2 template fitting, combined with luminosity function priors and with the option to estimate the internal extinction [or E(B - V) ]. The median most probable redshift, best-fit spectral type and reddening, absolute magnitude, and stellar mass are derived {{in addition to the}} full redshift probability distributions. Using simulations with sampling and noise similar to those in COSMOS, the accuracy and reliability is estimated for the photometric redshifts as a function <b>of</b> <b>the</b> <b>magnitude</b> limits <b>of</b> <b>the</b> <b>sample,</b> S/N ratios, and <b>the</b> number <b>of</b> bands used. We find from the simulations that <b>the</b> ratio <b>of</b> derived 95...|$|R
