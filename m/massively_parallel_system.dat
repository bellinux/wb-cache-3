84|10000|Public
25|$|Systems with {{a massive}} number of {{processors}} generally take one of two paths: in one approach, e.g., in grid computing the processing power {{of a large number}} of computers in distributed, diverse administrative domains, is opportunistically used whenever a computer is available. In another approach, a large number of processors are used in close proximity to each other, e.g., in a computer cluster. In such a centralized <b>massively</b> <b>parallel</b> <b>system</b> the speed and flexibility of the interconnect becomes very important, and modern supercomputers have used various approaches ranging from enhanced Infiniband systems to three-dimensional torus interconnects.|$|E
5000|$|Efficiently {{broadcasting}} data tokens in a <b>massively</b> <b>parallel</b> <b>system.</b>|$|E
5000|$|Efficiently {{dispatching}} instruction tokens in a <b>massively</b> <b>parallel</b> <b>system.</b>|$|E
30|$|Group-level data {{aggregation}} in <b>parallel</b> <b>systems</b> {{is useful for}} efficient summarization, graph traversal and matrix operations, therefore it is of great importance in programming models for data analysis on <b>massively</b> <b>parallel</b> <b>systems.</b>|$|R
40|$|Abstract: In {{this paper}} {{we present a}} routing scheme which is {{extremely}} suited for use in <b>massively</b> <b>parallel</b> <b>systems.</b> The routing algorithm is fault-tolerant so that network failures will not stop the system. For reasons of scalability, the routing information is extremely compact, also when the network is injured. The wormhole routing technique guarantees very low routing latency. Moreover the routing is guaranteed to be deadlock-free {{without the need for}} dedicated buffering. Key words: compact routing, wormhole rousting, deadlock-free routing, fault tolerance, <b>massively</b> <b>parallel</b> <b>systems</b> Nowadays challenge in supercomputing Is to reach the teraflop goal to deal with the grand challenge applications. Despite the ever increasing performance of the processors, <b>massively</b> <b>parallel</b> <b>systems</b> must still be used. Thousands of processors must work in parallel to obtain this enormous computational power. Not only more processors should be used, the performance of the communication network should also scale. Small delays ar...|$|R
40|$|<b>Massively</b> <b>parallel</b> {{programming}} is an increasingly growing field {{with the recent}} introduction of general purpose GPU computing. Modern graphics processors from NVidia and AMD have <b>massively</b> <b>parallel</b> architectures {{that can be used}} for 3 D ren-dering, financial analysis, physics simulations, and biomedical analysis. These mas-sively <b>parallel</b> <b>systems</b> are exposed to programmers through interfaces such as NVidias CUDA, openCL, and Microsofts C++ AMP. These frameworks expose functionality using primarily either C or C++. In order to use these <b>massively</b> <b>parallel</b> frameworks, the programs being implemented must be run on machines equipped with <b>massively</b> <b>parallel</b> hardware. These requirements limit the flexibility of new <b>massively</b> <b>parallel</b> <b>systems.</b> This paper explores the possibility that <b>massively</b> <b>parallel</b> <b>systems</b> can be exposed through web services in order to facilitate using these architectures from remote systems written in other languages. ii Content...|$|R
50|$|The Edinburgh Concurrent Supercomputer (ECS) {{was a large}} Meiko Computing Surface supercomputer. This transputer-based, <b>massively</b> <b>parallel</b> <b>system</b> was {{installed}} at the University of Edinburgh during the late 1980s and early 1990s.|$|E
50|$|While in a {{traditional}} multi-user computer system job scheduling is in effect a tasking problem for processing and peripheral resources, in a <b>massively</b> <b>parallel</b> <b>system,</b> the job management system needs to manage the allocation of both computational and communication resources, as well as gracefully dealing with inevitable hardware failures when {{tens of thousands of}} processors are present.|$|E
50|$|While in {{traditional}} multi-user computer systems and early supercomputers, job scheduling {{was in effect}} a task scheduling problem for processing and peripheral resources, in a <b>massively</b> <b>parallel</b> <b>system,</b> the job management system needs to manage the allocation of both computational and communication resources. It is essential to tune task scheduling, and the operating system, in different configurations of a supercomputer. A typical parallel job scheduler has a master scheduler which instructs some number of slave schedulers to launch, monitor, and control parallel jobs, and periodically receives reports from them {{about the status of}} job progress.|$|E
40|$|Increasingly <b>parallel</b> <b>systems</b> {{promise a}} remedy for the cur-rent {{stagnation}} of single-core performance. However, the battle {{to find the most}} appropriate architecture for the re-sulting <b>massively</b> <b>parallel</b> <b>systems</b> is still ongoing. Currently, there are two active contenders: <b>Massively</b> <b>Parallel</b> Sin-gle Instruction Multiple Threads (SIMT) systems such a...|$|R
40|$|Developers of {{application}} codes for <b>massively</b> <b>parallel</b> computer <b>systems</b> face daunting performance tuning and optimization problems {{that must be}} solved if <b>massively</b> <b>parallel</b> <b>systems</b> are to fulfill their promise. Recording and analyzing the dynamics {{of application}} program, system software, and hardware interactions {{is the key to}} understanding and the prerequisite to performance tuning, but this instrumentation and analysis must not unduly perturb program execution. Pablo is a performance analysis environment designed to provide unobtrusive performance data capture, analysis, and presentation across a wide variety of scalable <b>parallel</b> <b>systems.</b> Current efforts include dynamic statistical clustering to reduce the volume of data that must be captured and complete performance data immersion via head-mounted displays. 1 Introduction As computational science becomes an equal partner to theory and experiment, there is growing consensus that <b>massively</b> <b>parallel</b> <b>systems</b> are the only technically an [...] ...|$|R
30|$|Data-driven local {{communication}} that {{is useful to}} limit the data exchange overhead in <b>massively</b> <b>parallel</b> <b>systems</b> composed of many cores; in this case data availability among neighbor nodes dictates the operations taken by those nodes.|$|R
50|$|Systems with {{a massive}} number of {{processors}} generally take one of two paths: in one approach, e.g., in grid computing the processing power {{of a large number}} of computers in distributed, diverse administrative domains, is opportunistically used whenever a computer is available. In another approach, a large number of processors are used in close proximity to each other, e.g., in a computer cluster. In such a centralized <b>massively</b> <b>parallel</b> <b>system</b> the speed and flexibility of the interconnect becomes very important, and modern supercomputers have used various approaches ranging from enhanced Infiniband systems to three-dimensional torus interconnects.|$|E
50|$|MicroWorlds JR {{was first}} {{launched}} in 2004 by LCSI, and {{as in other}} versions of Logo such as MicroWorlds (which allows for multi-tasking, or parallel processing; several processes can be launched independently) or StarLogo (an even more <b>massively</b> <b>parallel</b> <b>system),</b> the object on the screen begins as a turtle and can be controlled with basic commands to make it move. Turtle graphics automatically replace the command names and are selected when children click on the controls. The turtle object can be given a variety of shapes that act as a costume for the turtle, and therefore lends itself {{to a variety of}} animations and creative stories and projects for younger students.|$|E
5000|$|Systems with {{a massive}} number of {{processors}} generally take one of two paths. In the grid computing approach, the processing power of many computers, organised as distributed, diverse administrative domains, is opportunistically used whenever a computer is available. In another approach, {{a large number of}} processors are used in proximity to each other, e.g. in a computer cluster. In such a centralized <b>massively</b> <b>parallel</b> <b>system</b> the speed and flexibility of the [...] becomes very important and modern supercomputers have used various approaches ranging from enhanced Infiniband systems to three-dimensional torus interconnects. The use of multi-core processors combined with centralization is an emerging direction, e.g. as in the Cyclops64 system.|$|E
30|$|Reproducibility requirements. Big data {{analysis}} running on <b>massively</b> <b>parallel</b> <b>systems</b> demands for reproducibility. New {{data analysis}} programming frameworks must collect and generate metadata and provenance information about algorithm characteristics, software configuration and execution environment for supporting application reproducibility on large-scale computing platforms.|$|R
50|$|Although the RM1000 was {{eventually}} discontinued and not replaced by Siemens, customers who had large installations {{such as a}} large UK telecommunications company {{took a long time}} to find suitable replacements for these <b>massively</b> <b>parallel</b> <b>systems</b> due to their massive I/O and computing capabilities.|$|R
40|$|Scalability is an {{important}} issue in the design of interconnection networks for <b>massively</b> <b>parallel</b> <b>systems.</b> In this paper a scalable class of interconnection network of Hex-Cell for <b>massively</b> <b>parallel</b> <b>systems</b> is introduced. It is called Multilayer Hex-Cell (MLH). A node addressing scheme and routing algorithm are also presented and discussed. An interesting feature of the proposed MLH is that it maintains a constant network degree regardless of the increase in the network size degree which facilitates modularity in building blocks of scalable systems. The new addressing node scheme makes the proposed routing algorithm simple and efficient in terms of that it needs a minimum number of calculations to reach the destination node. Moreover, the diameter of the proposed MLH is less than Hex-Cell network...|$|R
50|$|SciEngines GmbH is {{a privately}} owned company founded 2007 as a {{spin-off}} of the COPACOBANA project by the Universities of Bochum and Kiel, both in Germany. The project intended {{to create a}} platform for an affordable Custom hardware attack. COPACOBANA is a massively-parallel reconfigurable computer. It can be utilized to perform a so-called Brute force attack to recover DES encrypted data. It consists of 120 commercially available, reconfigurable integrated circuits (FPGAs). These Xilinx Spartan3-1000 run in parallel, and create a <b>massively</b> <b>parallel</b> <b>system.</b> Since 2007, SciEngines GmbH has enhanced and developed successors of COPACOBANA. Furthermore the COPACOBANA has become a well known reference platform for cryptanalysis and custom hardware based attacks to symmetric, asymmetric cyphers and stream ciphers. 2008 attacks against A5/1 stream cipher an encryption system been used to encrypt voice streams in GSM have been published as the first known real world attack utilizing off-the-shelf custom hardware.|$|E
40|$|This paper {{presents}} the aims {{and the status}} of our research on basic system software for massively parallel systems. Since novel functionsrequire much computation power, massively parallel systems will be used as their vehicles. The number of processors incorporated in a <b>massively</b> <b>parallel</b> <b>system</b> is being increased from hundreds (current) to ten thousands (RWC-l). We can expect tha...|$|E
40|$|Deep Blue is {{the chess}} machine that {{defeated}} then- reigning World Chess Champion Garry Kasparov in a six-game match in 1997. Factors {{that contributed to}} this success： a single-chip chess search engine a <b>massively</b> <b>parallel</b> <b>system</b> with multiple levels of parallelism {{a strong emphasis on}} search extensions a complex evaluation function effective use of a Grandmaster game database...|$|E
40|$|<b>Massively</b> <b>parallel</b> {{programming}} is an increasingly growing field {{with the recent}} introduction of general purpose GPU computing. Modern graphics processors from NVIDIA and AMD have <b>massively</b> <b>parallel</b> architectures {{that can be used}} for such applications as 3 D rendering, financial analysis, physics simulations, and biomedical analysis. These <b>massively</b> <b>parallel</b> <b>systems</b> are exposed to programmers through in- terfaces such as NVIDIAs CUDA, OpenCL, and Microsofts C++ AMP. These frame- works expose functionality using primarily either C or C++. In order to use these <b>massively</b> <b>parallel</b> frameworks, programs being implemented must be run on machines equipped with <b>massively</b> <b>parallel</b> hardware. These requirements limit the flexibility of new <b>massively</b> <b>parallel</b> <b>systems.</b> This paper explores the possibility that <b>massively</b> <b>parallel</b> <b>systems</b> can be exposed through web services in order to facilitate using these architectures from remote systems written in other languages. To explore this possi- bility, an architecture is put forth with requirements and high level design for building a web service that can overcome limitations of existing tools and frameworks. The CUDA Web API is built using Python, PyCUDA, NumPy, JSON, and Django to meet the requirements set forth. Additionaly, a client application, CUDA Cloud, is built and serves as an example web service client. The CUDA Web API’s performance and its functionality is validated using a common matrix multiplication algorithm implemented using different languages and tools. Performance tests show runtime improvements for larger datasets using the CUDA Web API for remote CUDA kernel execution over serial implementations. This paper concludes that existing limitations associated with GPGPU usage can be overcome with the specified architecture...|$|R
40|$|A {{conference was}} held on {{parallel}} computational fluid dynamics and produced related papers. Topics discussed in these papers include: parallel implicit and explicit solvers for compressible flow, parallel computational techniques for Euler and Navier-Stokes equations, grid generation techniques for parallel computers, and aerodynamic simulation om <b>massively</b> <b>parallel</b> <b>systems...</b>|$|R
40|$|The {{feasibility}} {{and constraints}} of workstation clusters for parallel processing are investigated. Measurements of latency and bandwidth are presented {{to fix the}} position of clusters in comparison to <b>massively</b> <b>parallel</b> <b>systems.</b> So it becomes possible to identify the kind of applications {{that seem to be}} suited for running on a cluster...|$|R
40|$|This {{paper is}} the final report of an {{undergraduate}} honors thesis project advised by Prof. Dennis Hejhal of the School of Mathematics, University of Minnesota. The main purpose of this project {{is to examine the}} analytic properties of certain "quantum-mechanical particles" in Lobachevsky space. The results were obtained on vectorized CRAY serial supercomputers, a CRAY 64 -CPU T 3 D <b>massively</b> <b>parallel</b> <b>system,</b> and a 1 K-CPU <b>massively</b> <b>parallel</b> <b>system</b> CM- 5 located in University of Minnesota. Using complex arithmetic, wehave successfully determined numerous Fourier coefficients for certain types of holomorphic modular forms, including the Ramanujan #-function. Our experiments involve both arithmetic and non-arithmetic groups. The treatment of the latter is new. Analyzing the output data enables us to experimentally justify a number of properties. Finally,a verification of a Central Limit Theorem for automorphic functions on Hecke Groups was attempted, and very promising results have been obtained...|$|E
40|$|While {{libraries}} such as Nvidia’s CUDA {{can greatly}} optimize the graphical application, it’s pipeline structure causes inaccuracies {{to occur in}} lighting physics. As such, an efficient, accurate graphical application is required Ray-tracing can render lightings very accu-rately, but falls short in efficiency With a <b>massively</b> <b>parallel</b> <b>system</b> such as BlueGene/L (BG/L), however, ray-tracing can be rendered at a much faster speed...|$|E
40|$|In {{order to}} {{facilitate}} an application-oriented assessment of high-performance massively parallel computing systems, a catalog of about 350 classifying characteristics concerning the architecture and software environment of such systems has been compiled. The data required for the catalog allow a rather complete and homogeneous description of a <b>massively</b> <b>parallel</b> <b>system</b> to be established. This article contains {{an overview of the}} catalog and the hardware model on which it is based...|$|E
40|$|The term {{cellular}} {{computer can}} {{be associated with}} a number of computational engines that range from chip-level architectures to <b>massively</b> <b>parallel</b> <b>systems</b> of the size of Connection Machine, and whose common characteristics include the relative large number of simple processing units with limited local memory, as well as, the simple (mostly synchronous/broadcasting...|$|R
40|$|Active {{messages}} {{provide an}} important new communication primitive for building message passing systems. CMMD, the message passing {{system of the}} CM- 5, uses active messages as a basic substrate for constructing multiple, low overhead, communication paradigms. Examples are also given which show how developers may incorporate active messages in application-specific ways. KEYWORDS: Message passing, active messages, <b>massively</b> <b>parallel</b> <b>systems</b> 1. Introduction The current generation of distributed-memory <b>massively</b> <b>parallel</b> <b>systems</b> stand {{in marked contrast to}} systems of only a few years ago. Hardware advancements have brought improved floating point performance and communication rates. Node-to-node network latencies are now in the microsecond range and bandwidths range from the tens to hundreds of megabytes per second. Message passing libraries and compilers for parallel languages alike benefit from improved communication performance. However, as network latencies decrease and bandwidths inc [...] ...|$|R
40|$|Recording and {{analyzing}} {{the dynamics of}} application program, system software, and hardware interactions are the keys to understanding and tuning the performance of <b>massively</b> <b>parallel</b> <b>systems.</b> Because <b>massively</b> <b>parallel</b> <b>systems</b> contain {{hundreds or thousands of}} processors, each potentially with many dynamic performance metrics, the performance data occupy a sparsely populated, high-dimensional space. These dynamic performance metrics for each processor define a group of evolving, n-dimensional points. Understanding the dynamic "shape" of these metric paths is only possible if one can examine multiple projections. We have implemented an immersive virtual world, called Avatar, that shows all the possible three-dimensional projections of a sparsely populated, n-dimensional metric space. The presentation metaphor is a three-dimensional generalization of a two-dimensional scatterplot matrix. Users can walk inside a single scatterplot cube, fly about the cube of scatterplot cubes, control sele [...] ...|$|R
40|$|A central {{problem in}} massively {{parallel}} computing is ef-ficiently routing data between processors. This problem {{is complicated by}} two considerations. First, in any <b>massively</b> <b>parallel</b> <b>system,</b> some processors are bound to fail, disrupt-ing message routing. Second, one must avoid deadlock con-figurations in which messages permanently block one an-other. We present an efficient, oblivious, and deadlock-free routing algorithm for the hypercube. The algorithm toler-ates {{a large number of}} faults in a worst-case configuration. 1...|$|E
40|$|This paper {{presents}} the first {{prototype of the}} PAPRICA <b>massively</b> <b>parallel</b> <b>system</b> which is integrated on the MOB-LAB experimental land vehicle for real-time vision-based road markings detection. Its main bottlenecks are highlighted and its evolution toward a linear array is discussed. This system has been enhanced with a simple but powerful interprocessor communication network for the exchange of information among processors not directly connected, which allows an extremely efficient implementation of the road markings detection algorithm {{as well as other}} morphological applications. ...|$|E
40|$|AbstractDeep Blue is {{the chess}} machine that {{defeated}} then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There {{were a number}} of factors that contributed to this success, including: •a single-chip chess search engine,•a <b>massively</b> <b>parallel</b> <b>system</b> with multiple levels of parallelism,•a strong emphasis on search extensions,•a complex evaluation function, and•effective use of a Grandmaster game database. This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue...|$|E
25|$|Approaches to {{supercomputer}} architecture {{have taken}} dramatic turns since the earliest systems {{were introduced in}} the 1960s. Early supercomputer architectures pioneered by Seymour Cray relied on compact innovative designs and local parallelism to achieve superior computational peak performance. However, in time the demand for increased computational power ushered {{in the age of}} <b>massively</b> <b>parallel</b> <b>systems.</b>|$|R
40|$|In {{this paper}} we {{introduce}} a methodology for the anal-ysis of the paging activity of parallel programs running on <b>massively</b> <b>parallel</b> <b>systems.</b> The methodology includes par-allel program monitoring and {{the analysis of}} the collected data. We study the correlation of the paging activities of in-dividual node programs in the SPMD execution mode and its effect on scheduling. ...|$|R
40|$|In {{this paper}} we {{introduce}} a methodology {{for the analysis}} of the paging activity of parallel programs running on <b>massively</b> <b>parallel</b> <b>systems.</b> The methodology includes parallel program monitoring and the analysis of the collected data. We study the correlation of the paging activities of individual node programs in the SPMD execution mode and its effect on scheduling. 1 Introduction <b>Massively</b> <b>parallel</b> <b>systems</b> (MPPs) are viewed today as expensive scientific and engineering instruments. Their primary use is in the area of numeric simulation of complex physical phenomena. The performance/usability tradeoffs of such systems are heavily tilted in favor of performance. Most distributed memory MIMD (DMIMD) systems have rather primitive operating systems with restricted functionality and rudimentary management of system resources. Only recently MPPs which run commodity operating systems in all Processing Elements (PEs) have been announced, e. g., IBM's, SP 1 and SP 2 (running AIX), and Intel Parag [...] ...|$|R
