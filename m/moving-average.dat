568|5|Public
25|$|Note {{that the}} ARMA {{model is a}} {{univariate}} model. Extensions for the multivariate case are the vector autoregression (VAR) and Vector Autoregression <b>Moving-Average</b> (VARMA).|$|E
2500|$|The {{notation}} ARMA(p, q) {{refers to}} the model with p autoregressive terms and q <b>moving-average</b> terms. This model contains the AR(p) and MA(q) models, ...|$|E
50|$|In {{time series}} analysis, the <b>moving-average</b> (MA) {{model is a}} common {{approach}} for modeling univariate time series. The <b>moving-average</b> model specifies that the output variable depends linearly on the current and various past values of a stochastic (imperfectly predictable) term.|$|E
40|$|Abstract- The {{kinematic}} filter is {{a common}} tool in control and signal processing applications dealing with position, velocity and other kinematical variables. Usually the filter gain is given a fixed value determined due to dynamic and measurement conditions. Most studies provide analytical solutions for optimal gains in particular scenarios. In practice, {{due to a lack}} of information (or under time-varying conditions) these recipes are mostly inapplicable and the kinematic filter requires appropriate adaptation tools instead. In its simplest form, the problem may be formulated as the gain adaptation under the tracking index uncertainty. We suggest a simple adaptive-gain kinematic filter based on minimization of the innovation variance which is known to give the optimal Kalman gain. The study deals with commonly used kinematic models of order 2 - 4. As shown, for any order of the kinematic filter its transfer function matches the <b>moving-averaging</b> (MA) model parameterized by the filter gain. In this view, the adaptive kinematic filter may be implemented in a variety of forms either based on the MA identification or by a direct gain adaptation. Optimal closed-form solutions may be incorporated into the adaptive filter as constraints. With the optimally constrained gain-vector components, the multiple-parameter adaptive filter is translated into a beneficial single-parameter version. The simulation study demonstrates behavior of suggested filters in a wide range of conditions. Index Terms- kinematic/tracking filter, adaptive gain, time...|$|R
30|$|As {{the demands}} for more {{accurate}} VADs in noisy conditions increase, a lot of efforts {{have been made to}} enhance the performance of VAD [2 – 14]. One successful approach is the statistical model-based VAD (SMVAD) proposed by Sohn et al. [2]. It utilizes the complex Gaussian probability density function (PDF). More recently, various efforts have been made to optimize SMVAD by modifying the decision rule originally derived from the likelihood ratio test (LRT). To decrease detection errors at speech offset regions, Sohn et al. [3] proposed an effective hang-over scheme based on the hidden Markov model (HMM), and Cho and Kondoz [4] proposed smoothed likelihood ratios (SLRs) in the decision rule. Other approaches have involved various statistical models for noise and noisy speech [5], and discriminative weight training (DWT) scheme [6]. The DWT scheme is a good approach in the name of optimizing frequency weights, but it does not yet consider temporal variations of input signal statistics because the optimized weights can be calculated only once through the whole training data. Also, the DWT scheme is not very practical since the weights need to be optimized differently in various noise conditions according to noise types and signal-to-noise ratio levels. Another technique is the <b>moving-averaged</b> decision rule over a certain number of neighboring frames applied for performance improvement of SMVAD [7]. However, to our knowledge, it seems that there has been no study about the reliability of likelihood ratios (LR).|$|R
40|$|A new {{methodology}} {{is proposed}} for scaling Doppler lidar observations of wind gusts {{to make them}} comparable with those observed at a meteorological mast. Doppler lidars can then be used to measure wind gusts in regions and heights where traditional meteorological mast measurements are not available. This novel method also provides estimates for wind gusts at arbitrary gust durations, including those shorter than the temporal resolution of the Doppler lidar measurements. The input parameters for the scaling method are the measured wind-gust speed {{as well as the}} mean and standard deviation of the horizontal wind speed. The method was tested using WindCube V 2 Doppler lidar measurements taken next to a 100 m high meteorological mast. It is shown that the method can provide realistic Doppler lidar estimates of the gust factor, i. e. the ratio of the wind-gust speed to the mean wind speed. The method reduced the bias in the Doppler lidar gust factors from 0. 07 to 0. 03 and can be improved further to reduce the bias by using a realistic estimate of turbulence. Wind gust measurements are often prone to outliers in the time series, because they represent the maximum of a (<b>moving-averaged)</b> horizontal wind speed. To assure the data quality in this study, we applied a filtering technique based on spike detection to remove possible outliers in the Doppler lidar data. We found that the spike detection-removal method clearly improved the wind-gust measurements, both with and without the scaling method. Spike detection also outperformed the traditional Doppler lidar quality assurance method based on carrier-to-noise ratio, by removing additional unrealistic outliers present in the time serie...|$|R
5000|$|<b>Moving-Average</b> (Unit) Cost is {{a method}} of calculating Ending Inventory cost.|$|E
50|$|The <b>moving-average</b> model {{should not}} be {{confused}} with the moving average, a distinct concept despite some similarities.|$|E
50|$|The linear constant-coefficient {{difference}} (LCCD) {{equation is}} a representation for a linear {{system based on}} theautoregressive <b>moving-average</b> equation.|$|E
40|$|We {{developed}} generalized Levy {{walks with}} varying velocity (shorter called the Weierstrass walks model) by {{which one can}} describe both stationary and non-stationary stochastic time series. Essentially, our approach {{is based on the}} continuous-time random walk formalism where the waiting-time distribution or memory kernel and sojourn probability density play a fundamental role. We considered a non-Brownian random walk where the walker moves, in general, with a velocity that assumes a dierent constant value between the successive turning points, i. e. the velocity is a piecewise constant function. So far models were developed that took into account only one chosen value of this velocity and therefore were unable to consider more realistic stochastic time series. Moreover, our model is a kind of Levy walks where we assume a hierarchical, self-similar in the stochastic sense, spatio-temporal representation of waiting-time distribution and sojourn probability density. The Weierstrass walks model makes possible to analize both the structure of the Hurst exponent and the power-law behavior of kurtosis. This structure results from the hierarchical, spatio-temporal coupling between the walker displacement and the corresponding time of the walks. The analysis makes use of both the fractional diusion and the super Burnett coecients. We constructed the diusion phase diagram which distinguishes regions occupied by classes of dierent universality. We study only such classes which are characteristic for stationary situations. We proved that even after performing the <b>moving-averaging</b> over the stochastic time series (being a kind of thermalization) which makes results stationary {{in the sense that they}} are already independent of the beginning moment of the random walk, it is still pos [...] ...|$|R
40|$|During {{the period}} from late June to mid-December 2002, two continuous, {{real-time}} radon detectors were being operated in Northampton in the English East Midlands region, in order to compare hourly readings of radon with short and long-term integrated testing methods (e. g. track-etch and charcoal) for a DEFRA project. Long-term cycles in the atmospheric indoor radon data were noted and the project team then looked for causes. We {{were able to take}} account of variability due to the affects of weather, human activity (e. g. occupancy – which produces short-term changes) and building works etc. The two real-time detectors were in basements (in terraced houses) 2. 25 km apart which meant {{that it was possible to}} filter out localized and short-term radon changes (radon in the ground is highly variable). Two time-series of hourly radon readings were obtained, one from each detector. The time-series were examined for simultaneous anomalies, as evidence of big disturbances occurring at big distances by cross-correlating over periods of 1 - 30 days duration, rolled forward through the time-series at 1 -hour intervals. Both ‘as-logged’ and 3 - 7 -hour <b>moving-averaged</b> data were investigated. Simultaneous similar anomalies occurred at two points in the time series, with some evidence of a third period of simultaneous anomaly. One of the two corresponded to an English Channel earthquake which occurred at 23 : 41 on the 26 th August 2002, with a magnitude of 3 and 250 km from the measurement location. The other corresponded to the Dudley event which occurred at 23 : 53 on the 22 nd September 2002, 90 km west of Northampton and which was a magnitude 5. 0. The third, less clear anomaly corresponded to the larger initial events within the Manchester earthquake swarm – a series of earthquakes during the period 19 th – 21 st October 2002 ranging from magnitude 2. 5 to 4. 3. Note that the Manchester events took place 160 km north-west of the measurement centre. Despite these distances from the place of measurement, the impact of these events was noted in the form of short radon pulses. The Dudley and English Channel earthquake events are characterised by 6 - 9 hour in-phase spikes in the radon time-series which both precede and follow the earthquake event itself. We have identified radon anomalies temporally associated with several UK earthquakes. If such observed precursors are generally observable, then simultaneous realtime monitoring of radon levels – for short-term simultaneous anomalies – at several locations in earthquake areas might provide the core of an earthquake prediction metho...|$|R
50|$|Contrary to the <b>moving-average</b> model, the {{autoregressive model}} {{is not always}} {{stationary}} as it may contain a unit root.|$|E
5000|$|<b>Moving-average</b> model (MA) estimation, which {{assumes that}} the nth sample is {{correlated}} with noise terms in the previous p samples.|$|E
5000|$|The Fig. (a) on {{the right}} shows the block diagram of a 2nd-order <b>moving-average</b> filter {{discussed}} below. The transfer function is: ...|$|E
50|$|Note {{that the}} ARMA {{model is a}} {{univariate}} model. Extensions for the multivariate case are the Vector Autoregression (VAR) and Vector Autoregression <b>Moving-Average</b> (VARMA).|$|E
50|$|Hendry, D.F. and P.K. Trivedi (1972). “Maximum {{likelihood}} {{estimation of}} difference equations with <b>moving-average</b> errors: A simulation study.” Review of Economic Studies, 32, 117-145.|$|E
5000|$|The {{notation}} ARMA(p, q) {{refers to}} the model with p autoregressive terms and q <b>moving-average</b> terms. This model contains the AR(p) and MA(q) models, ...|$|E
5000|$|... 1.Using <b>moving-average</b> {{smoothing}} method {{to estimate the}} trend-cycle for all periods. In the monthly data, use 12-month centered moving average is appropriate {{to be applied to}} estimate the trend-cycle component.|$|E
50|$|Together {{with the}} autoregressive (AR) model, the <b>moving-average</b> {{model is a}} special case and key {{component}} of the more general ARMA and ARIMA models of time series, which have a more complicated stochastic structure.|$|E
50|$|The period {{selected}} {{depends on}} the type of movement of interest, such as short, intermediate, or long-term. In financial terms <b>moving-average</b> levels can be interpreted as support in a falling market, or resistance in a rising market.|$|E
50|$|The {{particular}} case where simple equally weighted moving-averages are used {{is sometimes called}} a simple <b>moving-average</b> (SMA) crossover. Such a crossover {{can be used to}} signal a change in trend and can be used to trigger a trade in a Black Box trading system.|$|E
5000|$|A CIC filter is an {{efficient}} {{implementation of a}} <b>moving-average</b> filter. To see this, consider how a moving average filter can be implemented recursively by adding the newest sample [...] to the previous result [...] and subtracting the oldest sample. Omitting the division by , we have: ...|$|E
5000|$|Technical: change/consistency, 50-day <b>moving-average</b> {{crossover}}. A crossover is a {{point on}} a stock chart when a security and an indicator intersect. Crossovers are used by technical analysts to aid in forecasting future movements in a stock’s price. In most technical analysis models, a crossover is a signal to either buy or sell ...|$|E
50|$|Thus, a <b>moving-average</b> {{model is}} conceptually a linear {{regression}} {{of the current}} value of the series against current and previous (unobserved) white noise error terms or random shocks. The random shocks at each point {{are assumed to be}} mutually independent and to come from the same distribution, typically a normal distribution, with location at zero and constant scale.|$|E
5000|$|Technical trading {{strategies}} {{were found to}} be effective in the Chinese marketplace by a recent study that states, [...] "Finally, we find significant positive returns on buy trades generated by the contrarian version of the <b>moving-average</b> crossover rule, the channel breakout rule, and the Bollinger band trading rule, after accounting for transaction costs of 0.50 percent." ...|$|E
50|$|Together {{with the}} <b>moving-average</b> (MA) model, it {{is a special}} case and key {{component}} of the more general ARMA and ARIMA models of time series, which have a more complicated stochastic structure; {{it is also a}} special case of the vector autoregressive model (VAR), which consists of a system of more than one interlocking stochastic difference equation in more than one evolving random variable.|$|E
5000|$|The {{number of}} stocks {{in a stock}} market {{determine}} the dynamic range of the MSI. For the NZSX (one of the smallest exchanges in the English-speaking world) the MSI would probably range between (−50 ... +50), the 19 and 39 constants (used for the US exchanges) {{would have to be}} revised. For the NZSX a MSI <b>moving-average</b> mechanism might be needed to smooth out the perturbations of such a small number of traded stocks.|$|E
5000|$|NPML {{detection}} {{was first}} described in 1996 [...] and eventually found wide application in HDD read channel design. The “noise predictive” concept was later extended to handle autoregressive (AR) noise processes and autoregressive <b>moving-average</b> (ARMA) stationary noise processes [...] The concept {{was extended to}} include a variety of non-stationary noise sources, such as head, transition jitter and media noise; it was applied to various post-processing schemes. Noise prediction became {{an integral part of}} the metric computation in a wide variety of iterative detection/decoding schemes.|$|E
50|$|Non-seasonal ARIMA {{models are}} {{generally}} denoted ARIMA(p,d,q) where parameters p, d, and q are non-negative integers, p is the order (number of time lags) of the autoregressive model, d {{is the degree}} of differencing (the number of times the data have had past values subtracted), and q is {{the order of the}} <b>moving-average</b> model. Seasonal ARIMA models are usually denoted ARIMA(p,d,q)(P,D,Q)m, where m refers to the number of periods in each season, and the uppercase P,D,Q refer to the autoregressive, differencing, and moving average terms for the seasonal part of the ARIMA model.|$|E
5000|$|Similar {{results were}} found in another study, which {{concluded}} that Bollinger Band trading strategies may be effective in the Chinese marketplace, stating: [...] "we find significant positive returns on buy trades generated by the contrarian version of the <b>moving-average</b> crossover rule, the channel breakout rule, and the Bollinger Band trading rule, after accounting for transaction costs of 0.50 percent." [...] (By [...] "the contrarian version", they mean buying when the conventional rule mandates selling, and vice versa.) A recent study examined the application of Bollinger Band trading strategies combined with the ADX for Equity Market indices with similar results.|$|E
50|$|Time series models {{estimate}} difference equations containing stochastic components. Two {{commonly used}} forms {{of these models}} are autoregressive models (AR) and <b>moving-average</b> (MA) models. The Box-Jenkins methodology (1976) developed by George Box and G.M. Jenkins combines the AR and MA models to produce the ARMA (autoregressive moving average) model, which {{is the cornerstone of}} stationary time series analysis. ARIMA (autoregressive integrated moving average models), on the other hand, are used to describe non-stationary time series. Box and Jenkins suggest differencing a non-stationary time series to obtain a stationary series to which an ARMA model can be applied. Non-stationary time series have a pronounced trend and do not have a constant long-run mean or variance.|$|E
5000|$|Fig. (c) on {{the right}} shows the {{magnitude}} and phase components of [...] But plots like these can also be generated by doing a discrete Fourier transform (DFT) of the impulse response. [...] And because of symmetry, filter design or viewing software often displays only the π region. The magnitude plot indicates that the <b>moving-average</b> filter passes low frequencies with a gain near 1 and attenuates high frequencies, and is thus a crude low-pass filter. The phase plot is linear except for discontinuities at the two frequencies where the magnitude goes to zero. The size of the discontinuities is π, representing a sign reversal. They do not affect the property of linear phase. That fact is illustrated in Fig. (d).|$|E
5000|$|To Lexis, a {{time series}} was [...] "stable" [...] if the {{underlying}} probability {{giving rise to}} the observed rates remained constant {{from year to year}} (or, more generally, from one measurement period to the next). Using modern terminology, such a time series would be called a zero-order <b>moving-average</b> series (also known as a white noise process). Lexis was aware that many series were not stable. For non-stable series, he imagined that the underlying probabilities varied over time, being affected by what he called [...] "physical" [...] forces (as opposed to the random [...] "non-essential" [...] forces that would cause an observed rate to be different than the underlying probability). In his 1879 paper [...] "On the Theory of the Stability of Statistical Series", Lexis set himself the task of devising a method for distinguishing between stable and non-stable time series.|$|E
50|$|In the {{statistics}} of time series, {{and in particular}} the analysis of financial time series for stock trading purposes, a <b>moving-average</b> crossover occurs when, on plotting two moving averages each based on different degrees of smoothing, the traces of these moving averages cross. It does not predict future direction but shows trends. This indicator uses two (or more) moving averages, a slower moving average and a faster moving average. The faster moving average is a short term moving average. For end-of-day stock markets, for example, it may be 5, 10 or 25 day period while the slower moving average is medium or long term moving average (e.g. 50-, 100- or 200-day period). A short term moving average is faster because it only considers prices over short period of time and is thus more reactive to daily price changes. On the other hand, a long term moving average is deemed slower as it encapsulates prices over a longer period and is more lethargic. However, it tends to smooth out price noises which are often reflected in short term moving averages.|$|E
50|$|You’ve {{discovered}} how changes in direction {{are the way}} the KST triggers signals, but also that <b>moving-average</b> crossovers offer less timely, but more reliable signals. The average to use is a simple 10-day moving average. It is possible to anticipate a moving average crossover if the KST has already turned and the price violates a trendline. The KST started to reverse to the downside before the up trendline was violated. Since either a reversal or a trading range follow a valid trendline violation, it’s evident that upside momentum has temporarily dissipated, causing the KST to cross below its moving average. Traditionally, the MACD gives buy and sell signals when it crosses above and below its exponential moving average, known as the “signal line”. This approach isn’t perfect; the ellipses on the chart highlight all the whipsaws. As said earlier, the KST can also give false or misleading signals, {{as you can see}} from the April 2005 buy signal. It comes close to a couple of whipsaws, but by and large, it’s more accurate, even though the MACD often turns faster than the KST.|$|E
5000|$|The <b>moving-average</b> {{model is}} {{essentially}} a finite impulse response filter applied to white noise, with some additional interpretation placed on it. The role of the random shocks in the MA model differs from {{their role in the}} autoregressive (AR) model in two ways. First, they are propagated to future values of the time series directly: for example, [...] appears directly {{on the right side of}} the equation for [...] In contrast, in an AR model [...] does not appear on the right side of the [...] equation, but it does appear on the right side of the [...] equation, and [...] appears on the right side of the [...] equation, giving only an indirect effect of [...] on [...] Second, in the MA model a shock affects [...] values only for the current period and q periods into the future; in contrast, in the AR model a shock affects [...] values infinitely far into the future, because [...] affects , which affects , which affects , and so on forever (see Vector autoregression#Impulse response).|$|E
