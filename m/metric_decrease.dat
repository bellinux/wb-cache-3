0|38|Public
40|$|I {{consider}} a semiclassical {{expansion of the}} scalar field in the warm inflation scenario. I study the evolution for the fluctuations of the metric around the Friedmann-Robertson-Walker one. The formalism predicts that, in the power-law expansion universe, the fluctuations of the <b>metric</b> <b>decreases</b> with time. Comment: Accepted for publication in Il Nuovo Cimento B, 5 pages, no figure...|$|R
3000|$|..., the {{coherence}} between experiments and model is further improved, {{as can be}} shown by the probability of null delay or, for instance, by {{the sum of the}} least squares, a common comparison <b>metric,</b> which <b>decreases</b> from 0.0224 to 0.0092.|$|R
40|$|Recently Gao et al. {{proposed}} a wavelet-based image quality metric, based on which, a quality constrained image subband quantization scheme is derived for lossy compression. But this scheme needs entropy coding {{to find the}} compression ratio decrement for tuning the final quantization step of each subband, which is time consuming and wasteful. In addition, computing the initial quantization steps also relies too much upon parameter tuning. This paper is a follow-up study, and proposes a simple binary search to determine the quantization step, according to the observation that, the quality <b>metric</b> <b>decreases</b> monotonically as the quantization steps increase. Experiments show that, the proposed quantization method outperforms its old counterpart in terms of computing expense, image quality, compression ratio and mathematical elegance...|$|R
40|$|PURPOSE: To {{investigate}} a fluence-based trajectory optimization technique for non-coplanar VMAT for brain cancer. METHODS: Single-arc non-coplanar VMAT trajectories were determined using a heuristic technique for five patients. Organ at risk (OAR) volume intersected during raytracing was minimized for two cases: absolute volume and {{the sum of}} relative volumes weighted by OAR importance. These trajectories and coplanar VMAT formed starting points for the fluence-based optimization method. Iterative least squares optimization was performed on control points 24 ° apart in gantry rotation. Optimization minimized the root-mean-square (RMS) deviation of PTV dose from the prescription (relative importance 100), maximum dose to the brainstem (10), optic chiasm (5), globes (5) and optic nerves (5), plus mean dose to the lenses (5), hippocampi (3), temporal lobes (2), cochleae (1) and brain excluding other regions of interest (1). Control point couch rotations were varied in steps of up to 10 ° and accepted if the cost function improved. Final treatment plans were optimized with the same objectives in an in-house planning system and evaluated using a composite metric - the sum of optimization metrics weighted by importance. RESULTS: The composite <b>metric</b> <b>decreased</b> with fluence-based optimization in 14 of the 15 plans. In the remaining case its overall value, and the PTV and OAR components, were unchanged but the balance of OAR sparing differed. PTV RMS deviation was improved in 13 cases and unchanged in two. The OAR component was reduced in 13 plans. In one case the OAR component increased but the composite <b>metric</b> <b>decreased</b> - a 4 Gy increase in OAR metrics was balanced by a reduction in PTV RMS deviation from 2. 8 % to 2. 6 %. CONCLUSION: Fluence-based trajectory optimization improved plan quality {{as defined by the}} composite metric. While dose differences were case specific, fluence-based optimization improved both PTV and OAR dosimetry in 80 % of cases...|$|R
30|$|As {{suggested}} in [13], {{the performance of}} optical flow estimation algorithms can be evaluated using gradient-normalized RMSE <b>metric.</b> Such measure <b>decreases</b> the over-penalization of errors caused by fine textures.|$|R
40|$|Measured {{materials}} {{are used in}} computer graphics to enhance the realism of synthetic images. They are often approximated with analytical models to improve storage efficiency and allow for importance sampling. However, the error metrics used in the optimization procedure {{do not have a}} perceptual basis and the obtained results do not always correspond to the best visual match. In this paper we present a first steps towards creating a perceptually-based metric for BRDF modeling. First, a set of measured materials were approximated with different error metrics and analytical BRDF models. Next, a psychophysical study was performed to compare the visual fidelity obtained using different error metrics and models. The results of this study show that the cube root metric leads to a better perceptual approximation than other RMS based metrics, independently of the analytical BRDF model used. More benefit of using the cube root metric compared to the RMS based metrics is obtained for sharp specular lobes, and as the specular lobe broadens the benefit of using the cube root <b>metric</b> <b>decreases.</b> The use of the cube root error metric will improve the visual fidelity of renderings made using BRDF approximations and expand the usage of measured materials in computer graphics...|$|R
40|$|AbstractMachining {{is one of}} {{the most}} widely used {{subtractive}} processes, and many studies have reported methods to improve the quality, productivity, and effectiveness of the process. Moreover, in recent years, energy efficiency has become an increasingly important consideration, largely as a result of new legislation and standardization in response to environmental concerns. However, the analysis of the energy efficiency of machine tools is not straightforward because of the complexity of the components and process. This study compared the power efficiency of various machining processes at different scales. Here, power efficiency is defined as the ratio of the process power to the total power consumption, and it was calculated using experimental results from conventional milling, micro-scale drilling, and brushing. The calculated power efficiency is compared for the processes reported here, as well as with selected published data. We found that the power efficiency varied regardless of machining scales or specific energy consumed, and also can vary widely in terms of the peripheral devices used. Moreover, for the case of laser-assistant machining, the present power efficiency <b>metric</b> <b>decreased</b> as the cutting load decreased. Therefore, it is need to consider the effect of the surrounding environment and effectiveness of machining, and suggestions were shown for the concept of novel power efficiency...|$|R
30|$|The {{severity}} metric (Equation 1) and {{the continuous}} approximation (Equation 4) provide additional methods for quantifying severity. Severity {{is most often}} quantified by classifying into four, or sometimes six, levels of severity. The severity of entire burned areas has previously been quantified by averages (i. e., Roberts et al. 2008) or proportions of high severity area (Lutz et al. 2009 b), but SM represents a continuum of severities and distributions. No matter what characteristic severity distribution exists for a particular management unit, questions related to a fire year (i.e., “Was this year an active fire year?”) can be examined by comparing the SM for the year in question to the historical average (Figure 3), along with the comparisons of number of fires and annual area burned. The Weibull shape parameters (Table 2), {{as well as the}} SM, maintain the continuous distribution of dNBR, and they may also reveal relationships between fire severity and abiotic predictors such as climate conditions, elevation, or vegetation type that are not evident in analyses of classified fire severity. When examining questions related to an area burned, either a single fire or a fire year, (i.e., “Did this fire burn more severely than normal for the area?”), a better comparison is with the severity mean calculated from all pixels over the period of record. An areal <b>metric</b> <b>decreases</b> the influence of those years where there was little burned area.|$|R
30|$|Figure  9 {{demonstrates}} a continuous decrease of the tag discrimination metric, meaning that, overtime, the tags’ capacity to differentiate each LO {{in the system}} from the rest, tends to reduce. This finding can be explained since the tag growth metric keeps increasing at a high rate and the tag reuse <b>metric</b> is <b>decreasing.</b> As the Fig.  9 depicts, {{the value of the}} tag discrimination metric for the OSR Repository has been stabilized to 3, 65 LOs/tag. This value is lower than the reported by Farooq et al. (2007) value in CiteULike (4, 47 LOs /tags) and the reported by Makani & Spiteri (2010) value (4, 11 LOs/tags) in Calibrate Portal.|$|R
30|$|While {{there was}} a similar {{increasing}} trend with increasing Corexit exposure in both percent symbiont loss and zooxanthellae density normalized to protein, only percent symbiont loss showed a significant change. The symbiont loss calculation presented {{in this study is}} more sensitive than the traditional method of normalizing to protein concentration. Corexit exposure may have affected the protein concentration as well within the corals, but this effect is relatively unstudied (Kendall Jr et al. 1983). The increasing trend observed with the protein-normalized zooxanthellae data may be explained by this claim. As bleaching severity increases, the value from the zooxanthellae per mg/ml protein <b>metric</b> should <b>decrease</b> as the numerator value decreases. However, if Corexit somehow negatively affected the protein concentration, the denominator value would decrease as well, possibly resulting in an increasing relationship with Corexit exposure.|$|R
40|$|OBJETIVO: O objetivo deste trabalho é o estabelecimento de uma versão abreviada da Escala de Ritmo Social, com vistas à aplicação em pesquisa. MÉTODOS: Tomando como padrão-ouro a Escala de Ritmo Social de 17 itens, estabelecidas três versões breves a partir de três critérios diferentes. Comparados escores de regularidade e quantidade de atividades desenvolvidas em um período de uma semana de 167 sujeitos saudáveis, 25 portadores de epilepsia mioclônica juvenil e 16 portadores de transtorno depressivo. RESULTADOS: A versão breve de seis itens mostrou melhor concordância com relação ao padrão-ouro k = 0, 51; p OBJECTIVE: The {{objective}} of this work was to establish a brief version of the Social Rhythm Metric, aimed at applying it in research. METHODS: Taking the 17 -item Social Rhythm Metric as the gold standard, three brief versions were created based on three different criteria. compared the scores of the regularity and quantity of activities carried out in a week for 167 healthy subjects and 25 individuals with juvenile myoclonic epilepy and 16 with depressive disturbance. RESULTS: The brief version of 6 items showed better concordance {{in relation to the}} gold standard; k = 0. 51, p < 0. 001. Bivariate analysis demonstrated a significant correlation between the brief version 6 and the gold standard (r = 0. 87; p < 0. 001). There was a correlation with age in the brief 6 version (r = 0. 2; p < 0. 001), even more significant than in the gold standard (r = 0. 2; P < 0. 01). ANOVA showed higher scores for regularity in the healthy using both scales. However, in relation to the quantity of activities, the healthy group resembled the epilepsy group, and the individuals with depression showed lower means. CONCLUSION: The simplification of the Social Rhythm <b>Metric</b> <b>decreased</b> the percenta-ge of items not filled in and the cost of printed matter and facilitate the standardization. The process involved a careful analysis of suitability of the instrument for the target culture...|$|R
30|$|The first {{cluster of}} {{research}} is mainly based on qualitative, questionnaire-based assessment. The authors derive questionnaires from lean principles and the assessment focuses {{on the extent of}} the lean-compliance of the applied manufacturing system. Soriano-Meier and Forester [31] offer an approach to also compare the degree of lean implementation among companies as they calculate an overall score. In contrast, Karlsson and Ahlström [30] assess lean implementation by analysing the determinants of lean production, e.g. reduction in waste by decreasing lot sizes. They apply metrics to reflect the implementation and point out an overall direction for each <b>metric</b> (increase and <b>decrease).</b>|$|R
40|$|The Euclidean (imaginary time) Schwarzschild {{solution}} {{of general relativity}} is known to possess a spin- 2 <b>metric</b> perturbation which <b>decreases</b> its Euclidean action. This "negative mode" contributes an imaginary part to the effective action, and renders hot flat space unstable against the nucleation of black holes. In this paper, we enclose the black hole in a spherical "box" by imposing boundary conditions on the perturbations. Two conditions, which correspond to a fixed temperature (isothermal wall) and fixed energy (reflecting wall) are examined. The isothermal boundary condition eliminates the negative mode if the box is small enough, and stabilizes hot flat space...|$|R
40|$|Shapiro's {{lethargy}} theorem {{states that}} if A_n is any non-trivial linear approximation scheme on a Banach space X, then the sequences of errors of best approximation E(x,A_n) = _a ∈ A_n ||x - a_n||_X decay almost arbitrarily slowly. Recently, Almira and Oikhberg investigated {{this kind of}} result for general approximation schemes in the quasi-Banach setting. In this paper, we consider the same question for F-spaces with non <b>decreasing</b> <b>metric</b> d. We also provide applications to the rate of decay of s-numbers, entropy numbers, and slow convergence of sequences of operators. Comment: 22 pages, submitted to a Journa...|$|R
40|$|Abstract. Let X be {{a smooth}} {{projective}} Berkovich space over a complete discrete val-uation field K of residue characteristic zero, endowed with an ample line bundle L. We introduce a general notion of (possibly singular) semipositive (or plurisubharmonic) metrics on L, and prove the analogue {{of the following}} two basic results in the complex case: the set of semipositive metrics is compact modulo constants, and each semipositive <b>metric</b> is a <b>decreasing</b> limit of smooth semipositive ones. In particular, for continuous metrics our definition agrees with the one by S. -W. Zhang. The proofs use multiplier ideals {{and the construction of}} suitable models of X over the valuation ring of K. Content...|$|R
40|$|Two new {{notions of}} {{reduction}} for {{terms of the}} λ-calculus are introduced {{and the question of}} whether a λ-term is beta-strongly normalizing is reduced {{to the question of whether}} a λ-term is merely normalizing under one of the new notions of reduction. This leads to a new way to prove beta-strong normalization for typed λ-calculi. Instead of the usual semantic proof style based on Girard's "candidats de réductibilité'', termination can be proved using a <b>decreasing</b> <b>metric</b> over a well-founded ordering in a style more common in the field of term rewriting. This new proof method is applied to the simply-typed λ-calculus and the system of intersection types. NSF (CCR 9113196...|$|R
40|$|Two {{notions of}} {{reduction}} for {{terms of the}} λ-calculus are introduced {{and the question of}} whether a λ-term is β-strongly normalizing is reduced {{to the question of whether}} a λ-term is merely normalizing under one of the notions of reduction. This gives a method to prove strong β-normalization for typed λ-calculi. Instead of the usual semantic proof style based on Tait's realizability or Girard's "candidats de réductibilité", termination can be proved using a <b>decreasing</b> <b>metric</b> over a well-founded ordering. This proof method is applied to the simply-typed λ-calculus and the system of intersection types, giving the first non-semantic proof for a polymorphic extension of the λ-calculus...|$|R
50|$|News {{about the}} coal mine in Malangas has been rumored that PNOC-EC may sell mine to San Miguel Corp.According to the news, the Philippine National Oil Company - Exploration Corp. (PNOC-EC) is {{in talks with}} diversified {{conglomerate}} San Miguel Corp. to sell a coal mine in Mindanao and this is probably in Zamboanga Peninsula where Malangas is located. For 2009, total coal production in Malangas amounted to 91,440 <b>metric</b> tons, a <b>decrease</b> from the 2008 output of 110, 549 metric tons due to major repair and rehabilitation “conducted immediately upon takeover of the mines.” Today, Mining operators are still operating in the coal reservation of the town.|$|R
40|$|In Multiple-Input Multiple-Output (MIMO) systems, Sphere Decoding (SD) {{can achieve}} {{performance}} equivalent to full search Maximum Likelihood (ML) decoding, with reduced complexity. Several researchers reported techniques that reduce {{the complexity of}} SD further. In this paper, a new technique is introduced which decreases the computational complexity of SD substantially, without sacrificing performance. The reduction is accomplished by deconstructing the decoding <b>metric</b> to <b>decrease</b> the number of computations and exploiting {{the structure of a}} lattice representation. Furthermore, an application of SD, employing a proposed smart implementation with very low computational complexity is introduced. This application calculates the soft bit metrics of a bit-interleaved convolutional-coded MIMO system in an efficient manner. Based on the reduced complexity SD, the proposed smart implementation employs the initial radius acquired by Zero-Forcing Decision Feedback Equalization (ZF-DFE) which ensures no empty spheres. Other than that, a technique of a particular data structure is also incorporated to efficiently reduce the number of executions carried out by SD. Simulation results show that these approaches achieve substantial gains in terms of the computational complexity for both uncoded and coded MIMO systems. Comment: accepted to Journal. arXiv admin note: substantial text overlap with arXiv: 1009. 351...|$|R
40|$|We {{consider}} {{a class of}} partial mass problems in which {{a fraction of the}} mass of a probability measure is allowed to be changed (trimmed) to maximize fit to a given pattern. This includes the problem of optimal partial transportation of mass, where a part of the mass need not be transported, and also trimming procedures which are often used in statistical data analysis to discard outliers in a sample (the data with lowest agreement to a certain pattern). This results in a modified, trimmed version of the original probability which is closer to the pattern. We focus on the case of the empirical measure and analyze to what extent its optimally trimmed version is closer to the true random generator in terms of rates of convergence. We deal with probabilities on ℝk and measure agreement through probability metrics. Our choices include transportation cost metrics, associated to optimal partial transportation, and the Kolmogorov distance. We show that partial transportation (as opposed to classical, complete transportation) results in a sharp decrease of costs only in low dimension. In contrast, for the Kolmogorov <b>metric</b> this <b>decrease</b> is seen in any dimension. © 2011 Springer-Verlag...|$|R
40|$|Let X be {{a smooth}} {{projective}} Berkovich space over a complete discrete valuation field K of residue characteristic zero, endowed with an ample line bundle L. We introduce a general notion of (possibly singular) semipositive (or plurisubharmonic) metrics on L, and prove the analogue {{of the following}} two basic results in the complex case: the set of semipositive metrics is compact modulo constants, and each semipositive <b>metric</b> is a <b>decreasing</b> limit of smooth semipositive ones. In particular, for continuous metrics our definition agrees with the one by S. -W. Zhang. The proofs use multiplier ideals {{and the construction of}} suitable models of X over the valuation ring of K, using toroidal techniques. Comment: 49 pages, 1 figure. Accepted in the Journal of Algebraic Geometr...|$|R
40|$|We {{compared}} {{the ability of}} several classification and regression algorithms to predict forest stand structure metrics and standard surface fuel models. Our study area spans a dense, topographically complex Sierra Nevada mixed-conifer forest. We used clustering, regression trees, and support vector machine algorithms to analyze high density (average 9 pulses/m 2), discrete return, small-footprint lidar data, along with multispectral imagery. Stand structure <b>metric</b> predictions generally <b>decreased</b> with increased canopy penetration. For example, {{from the top of}} canopy, we predicted canopy height (r 2 � 0. 87), canopy cover (r 2 � 0. 83), basal area (r 2 � 0. 82), shrub cover (r 2 � 0. 62), shrub height (r 2 � 0. 59), combined fuel loads (r 2 � 0. 48), and fuel bed depth (r 2 � 0. 35). While the general fuel types were predicted accurately, specifi...|$|R
40|$|We {{examined}} polymorphism in {{an urban}} population of the ground beetle Pterostichus melanarius (Illiger, 1798) in forest plantations {{on the outskirts of}} Dnipropetrovsk (Ukraine). We took measurements for 130 males and 95 females according to 14 metric, 10 non-metric parameters and 10 indices. According to 13 out of the 14 metric parameters (except for the length of the flight wings) P. melanarius showed a significant sexual dimorphism. The female specimens showed a normal distribution in body length and the males showed a significant positive excess. The 10 most significant body proportions of the ground beetles showed much lower sexual differences (0. 26 ± 0. 86 %) compared to linear measurements (6. 56 ± 0. 96 %). Females and males are practically isomorphic: when one <b>metric</b> parameter <b>decreases,</b> another decreases proportionally. Statistically significant differences between males and females were registered only for the ratio of elytra to length of prothorax, width of elytra to maximum width of prothorax and length of elytra to their width. A highly significant excess was registered in 6 out of 10 assessments of body proportions for females and 5 out of 10 for males. The coefficient of variation is minimal for body proportions and is maximal for metric parameters, which proves the existence of isomorphic differences between P. melanarius males and females. Out of 10 non-metric parameters a statistically significant sexual dimorphism (P= 0. 01) was registered only for the shape of the front edge of the labrum (males mostl...|$|R
40|$|A {{survey on}} the recent work of Danciger, Guéritaud and Kassel on Margulis space-times and {{complete}} anti-de Sitter space-times. Margulis space-times are quotients of the 3 -dimensional Minkowski space by (non-abelian) free groups acting propertly discontinuously. Goldman, Labourie and Margulis have shown that they are determined by a convex co-compact hyperbolic surface S along with a first-order deformation of the <b>metric</b> which uniformly <b>decreases</b> the lengths of closed geodesics. Danciger, Guéritaud and Kassel show that those space-times are principal R-bundles over S with time-like geodesics as fibers, that they are homeomorphic to {{the interior of a}} handlebody, and that they admit a fundamental domain bounded by crooked planes. To obtain those results they show that those Margulis space-times are "infinitesimal" versions of 3 -dimensional anti-de Sitter manifolds, and are lead to introduce a new parameterization of the space of deformations of a hyperbolic surface that increase the lengths of all closed geodesics. Comment: Bourbaki seminar no 1103. In frenc...|$|R
30|$|The stack {{algorithm}} (SA), {{which was}} originally proposed for decoding convolutional and tree codes [20], is a best-first tree-search technique {{that can be}} used to approximate the ML solution to sequential detection problems [21]. When the SA is applied to sequential detection, the tree represents the space spanned by a sequence of transmitted bits. Each path in the tree represents a possible realization of the transmitted sequence. A metric is computed for each path; the path metric represents the likelihood that the corresponding bit sequence was transmitted, conditioned on the observations. At each time step, the SA extends the path with the highest likelihood (metric) and stores a set of possible paths and their associated metrics in a stack (or list) in order of <b>decreasing</b> <b>metric</b> value. The SA terminates when the top path in the stack reaches a leaf of the tree, or equivalently when the top path represents a full block of transmitted bits.|$|R
40|$|Two new {{notions of}} {{reduction}} for {{terms of the}} -calculus are introduced {{and the question of}} whether a -term is fi-strongly normalizing is reduced {{to the question of whether}} a -term is merely normalizing under one of the new notions of reduction. This leads to a new way to prove fi-strong normalization for typed -calculi. Instead of the usual semantic proof style based on Girard's "candidats de r'eductibilit'e", termination can be proved using a <b>decreasing</b> <b>metric</b> over a well-founded ordering in a style more common in the field of term rewriting. This new proof method is applied to the simply-typed -calculus and the system of intersection types. This work is partly supported by NSF grant CCR [...] 9113196. 1 Introduction 1. 1 Background and Motivation. The problem of strong normalization of fi-reduction (fi-SN) has been considered for various typed -calculi for over 25 years. Tait's proof that all -terms typable in the simply-typed -calculus (actually, Godel's system T) are fi-SN can b [...] ...|$|R
3000|$|In this paper, a no-reference metric for {{assessing}} the quality of MPEG-based video transmissions over IP-based networks is presented. The proposed approach {{is based on the}} analysis of the inter-frame correlation measured at the receiver side. Several tests have been performed for tuning and evaluating the performances of the proposed metric. The scores collected by this tool in evaluating impaired videos have been compared with the ones gathered with the full reference VQMNTIA metrics and with the MOS collected by means of a subjective experiment. The overall analysis demonstrates the effectiveness of the VQMNR. Current investigation is devoted to solve the problems arising when using evaluation methods that are not based on reference signals. In particular, for the temporal realignment algorithm that is needed for the FR metrics in order to correctly estimate the NR parameters, we plan to test a novel re-synchronization procedure. Recently, the NTIA group announced the release of a new version of VQM metrics especially tuned for variable packet loss rate. Even if the problem of realignment is still to be solved, the use of such a metric could probably be used for a more effective parameters tuning. As a general remark, the influence of the adopted key-frame detection algorithm should be investigated. In fact, if a fake key-frame is selected due to estimation errors, the quality <b>metric</b> immediately <b>decreases.</b> Another issue is related to the amount of motion characterizing the sequences. We noticed a difference in the scores when slow or almost null motion rate is present. The choice of the parameter λ [...]...|$|R
40|$|SNR {{values were}} {{observed}} to decrease linearly when plotted against a path-length <b>metric.</b> Moreover, it <b>decreased</b> more rap-idly for a 70 degree flip angle {{than for a}} 90 degree flip angle. Further investigation shows that the variance of motion-induced signal changes can be modeled {{as the product of}} the variance of the through-slice displacement and a scale factor and that this scale factor can be approximated analytically. A model for SNR was used to estimate the motion sensitivity at flip angles of 70 and 90 degrees as well as the variance of the pulsatile motion. The fitted values are consistent with analytic results that suggest a minimum in the sensitivity to motion at approximately 70 degees and a relatively constant sensitivity at lower flip angles. Recent work by Bodurka et al. [1] shows that lower flip angles do not degrade temporal SNR. This work suggests that low flip angles do not reduce sensitivity to motion, but that motion sensitivity does increase rapidly for flip angles greater than the 70 degrees...|$|R
40|$|Being able to finely {{characterize}} {{the spinal cord}} (SC) microstructure and its alterations is a key point when investigating neural damage mechanisms encountered in different central nervous system (CNS) pathologies, such as multiple sclerosis, amyotrophic lateral sclerosis or myelopathy. Based on novel methods, including inhomogeneous magnetization transfer (ihMT) and dedicated SC probabilistic atlas post-processing, the present study focuses on the in vivo characterization of the healthy SC tissue in terms of regional microstructure differences between (i) upper and lower cervical vertebral levels and (ii) sensory and motor tracts, as well as differences attributed to normal aging. Forty-eight healthy volunteers aged from 20 to 70 years old {{were included in the}} study and scanned at 3 ?T using axial high-resolution T 2 *-w imaging, diffusion tensor imaging (DTI) and ihMT, at two vertebral levels (C 2 and C 5). A processing pipeline with minimal user intervention, SC segmentation and spatial normalization into a reference space was implemented in order to assess quantitative morphological and structural parameters (cross-sectional areas, scalar DTI and MT/ihMT metrics) in specific white and gray matter regions of interest. The multi-parametric MRI metrics collected allowed upper and lower cervical levels to be distinguished, with higher ihMT ratio (ihMTR), higher axial diffusivity (??) and lower radial diffusivity (??) at C 2 compared with C 5. Significant differences were also observed between white matter fascicles, with higher ihMTR and lower ?? in motor tracts compared with posterior sensory tracts. Finally, aging was found to be associated with significant <b>metric</b> alterations (<b>decreased</b> ihMTR and ??). The methodology proposed here, which can be easily transferred to the clinic, provides new insights for SC characterization. It bears great potential to study focal and diffuse SC damage in neurodegenerative and demyelinating diseases. Copyright © 2016 John Wiley & Sons, Ltd...|$|R
40|$|Viruses are {{the most}} {{abundant}} biological entity on Earth and outnumber their hosts ten-to-one. Ocean viruses (phages) impact bacterial-driven global biogeochemical cycles through lysis, manipulating host metabolism, and horizontal gene transfer. However, knowledge of virus-host interactions and viral roles in ecosystems remains limited due to few cultured marine phage genomes and non-quantitative culture-independent metagenomes. Here, I develop and apply novel and well-tested bioinformatic techniques to explore Pacific Ocean viral communities using quantitative datasets derived from rigorously-tested preparation methods. To evaluate concentration and purification methods, I examined triplicate metagenomes from a single ocean sample using four protocols. Concentration protocols showed statistical differences in taxonomy whereas purification protocols did not. Specifically, TFF-concentrated metagenomes contained trace bacterial contamination and had fewer abundant taxa as compared to FeCl₃-precipitated metagenomes. K-mer analysis using the complete dataset revealed polymerase choice defined access to "rare" sequences. To explore unknown viral sequences, I organized known and unknown sequence space into 27 K high-confidence protein clusters (PCs) from 32 diverse Pacific Ocean Virus (POV) metagenomes, which doubled available PCs and included the first pelagic deep-sea viral metagenomes. Using PCs as a whole-viral-community diversity <b>metric</b> revealed <b>decreases</b> from coastal to open ocean, winter to summer, and deep to surface, that correlate with data from microbial genetic diversity markers (no parallel viral markers exist). Biologically, POV metagenomes showed that viruses likely reprogram central metabolic pathways in microbial communities far beyond the "photosynthesis viruses" paradigm. Gene distribution patterns from 35 viral gene families (31 new) revealed niche-specific (photic vs aphotic zone) altered pathway carbon flux presumably optimized to best locally generate energy and drive viral replication. Further, these PCs define the first "core" (180 genes) and "flexible" (423 K genes total) viral community genome. Functionally, core genes again suggest niche-differentation with extensive Fe-S cluster-related genes for electron transport and metabolic enzyme catalysis in photic samples, and manipulation of host pressure-sensitive genes in aphotic samples. Taxonomically, these data deconstruct the culture-based paradigm that tailed viruses dominate in the wild - instead they appear ubiquitous, but not abundant...|$|R
40|$|The {{mechanisms}} {{selecting a}} single odorant receptor (OR) gene for expression in each olfactory sensory neuron (OSN) establish an OR expression pattern critical for odor discrimination. These mechanisms are largely unknown, but putative OR promoters contain homeodomain-like sites, implicating homeobox transcription {{factors such as}} Emx 2. At embryonic day 18. 5, expression of 49 – 76 % of ORs was decreased in mice lacking Emx 2, depending on the <b>metric</b> used. The <b>decreases</b> were due to fewer OSNs expressing each OR. Affected ORs showed changes that were disproportionately greater than the 42 % reduction in mature neurons and similar decreases in unrelated olfactory neuron-enriched messenger RNAs in Emx 2 / mice. Both Class I and Class II ORs decreased, as did ORs expressed in both the dorsal and ventral regions of the epithelium. Conversely, 7 % of Class II ORs tested were expressed more frequently, suggesting that some ORs are independent of Emx 2. Emx 2 helps stimulate transcription for many OR genes, which we hypothesize is through direct action at OR promoters, but Emx 2 appears to have no significant role in regulating other aspects of OR gene expression, including the zonal patterns, OR gene cluster selection mechanisms, and singularity of OR gene choice. Key words: gene choice, olfaction, olfactory receptor, smell, transcriptio...|$|R
40|$|The {{sensitivity}} of forecasts to observations is evaluated using an ensemble approach with data {{drawn from a}} pseudo-operational ensemble Kalman filter. For Gaussian statistics and a forecast metric defined as a scalar function of the forecast variables, the effect of observations on the forecast metric is quantified by changes in the metric mean and variance. For a single observation, expressions for these changes involve a product of scalar quantities, which can be rapidly evaluated for large numbers of observations. This tech-nique is applied to determining climatological forecast sensitivity and predicting the impact of observations on sea level pressure and precipitation forecast metrics. The climatological 24 -h forecast {{sensitivity of}} the average pressure over western Washington State shows a region of maximum sensitivity {{to the west of}} the region, which tilts gently westward with height. The accuracy of ensemble sensitivity predictions is tested by withholding a single buoy pressure observation from this region and comparing this perturbed forecast with the control case where the buoy is assimilated. For 30 cases, there is excellent agreement between these forecast differences and the ensemble predictions, as measured by the forecast <b>metric.</b> This agreement <b>decreases</b> for increasing numbers of observations. Nevertheless, by using statistical confidence tests to address sampling error, the impact of thousands of observations on forecast-metric variance is shown to be well estimated by a subset of the O(100) most significant observations...|$|R
40|$|Since the {{introductions}} of the Habitat Directive and the European Water Framework Directive, water authorities are now obliged to monitor changes in conservation value/ecological quality on larger spatial scales (opposed to site scale), {{as well as}} to indicate the level of confidence and precision of the results provided by the monitoring programs in their river basin management plans (European Commission, 2000). To meet these requirements, analyses of the statistical power of the monitoring programs should be implemented. Currently, the statistical properties associated with aquatic monitoring programs are often unknown. We collected macroinvertebrate samples from 25 meso-eutrophic drainage ditches in the Netherlands and selected 7 taxonomic richness metrics for the evaluation of spatial and temporal variability. Simulations were performed to investigate the effects of changes in (1) the total number of species included in a taxonomic richness metric and (2) the relative number of rare species included in a taxonomic richness metric. Of the 7 metrics evaluated, the number of common species required the smallest number of monitoring sites, followed by the number of Gastropoda species, and the number of species. Also, results showed that <b>metric</b> variability will <b>decrease</b> when the proportion of rare species included in a taxonomic richness metric is reduced or the total number of species included is increased. Irrespective of the metric applied a large effort will be required to detect change within drainage ditches in the Wieden, due to high spatial variability. Therefore, we need to explore the possibilities of applying alternative more cost-effective methods for sampling and sample processing in biomonitoring program...|$|R
40|$|Hydrologic metrics {{have been}} used {{extensively}} in ecology and hydrology to summarize the characteristics of riverine flow regimes at various temporal scales {{but there has been}} limited evaluation of the sources and magnitude of uncertainty involved in their computation. Variation in bias, precision and overall accuracy of these metrics influences the ability to correctly describe flow regimes, detect meaningful differences in hydrologic characteristics through time and space, and define flow-ecological response relationships. Here, we examine the effects of two primary factors-discharge record length and time period of record-on uncertainty in the estimation of 120 separate hydrologic metrics commonly used by researchers to describe ecologically relevant components of the hydrologic regime. <b>Metric</b> bias rapidly <b>decreased</b> and precision and overall accuracy markedly increased with increasing record length, but tended to stabilize > 15 years and did not change substantially > 30 years. We found a strong positive relationship between the degree of overlap of discharge record and similarity in hydrologic metrics when based on 15 - and 30 -year discharge periods calculated within a 36 -year temporal window (1965 - 2000), although hydrologic metrics calculated for a given stream gauge tended to vary only within a restricted range through time. Our study provides critical guidance for selecting an appropriate record length and temporal period of record given a degree of metric bias and precision deemed acceptable by a researcher. We conclude that: (1) estimation of hydrologic metrics based on at least 15 years of discharge record is suitable for use in hydrologic analyses that aim to detect important spatial variation in hydrologic characteristics; (2) metric estimation should be based on overlapping discharge records contained within a discrete temporal window (ideally > 50 % overlap among records); and (3) metric uncertainty varies greatly and should be accounted for in future analyses. Griffith Sciences, Griffith School of EnvironmentNo Full Tex...|$|R
40|$|Analyses {{have been}} {{undertaken}} of the {{spatial and temporal}} trends and drivers of the distributions of ground-level O 3 concentrations associated with potential impacts on human health and vegetation using measurements at the two UK European Monitoring and Evaluation Program (EMEP) supersites of Harwell and Auchencorth. These two sites provide representation of rural O 3 over the wider geographic areas of south-east England and northern UK respectively. The O 3 exposures associated with health and vegetation impacts were quantified respectively by the SOMO 10 and SOMO 35 metrics and by the flux-based POD Y metrics for wheat, potato, beech and Scots pine. Statistical analyses of measured O 3 and NO x concentrations were supplemented by analyses of meteorological data and NO x emissions along air-mass back trajectories. The findings highlight the differing responses of impact metrics to the decreasing contribution of regional O 3 episodes in determining O 3 concentrations at Harwell between 1990 and 2013, associated with European NO x emission reductions. An improvement in human health-relevant O 3 exposure observed when calculated by SOMO 35, which decreased significantly, was not observed when quantified by SOMO 10. The decrease in SOMO 35 is driven by decreases in regionally produced O 3 which makes a larger contribution to SOMO 35 than to SOMO 10. For the O 3 vegetation impacts at Harwell, no significant trend was observed for the POD Y metrics of the four species, {{in contrast to the}} decreasing trend in vegetation-relevant O 3 exposure perceived when calculated using the crop AOT 40 <b>metric.</b> The <b>decreases</b> in regional O 3 production have not decreased POD Y as climatic and plant conditions reduced stomatal conductance and uptake of O 3 during regional O 3 production. Ozone concentrations at Auchencorth (2007 – 2013) were more influenced by hemispheric background concentrations than at Harwell. For health-related O 3 exposures this resulted in lower SOMO 35 but similar SOMO 10 compared with Harwell; for vegetation POD Y values, this resulted in greater impacts at Auchencorth for vegetation types with lower exceedance ("Y") thresholds and longer growing seasons (i. e. beech and Scots pine). Additionally, during periods influenced by regional O 3 production, a greater prevalence of plant conditions which enhance O 3 uptake (such as higher soil water potential) at Auchencorth compared to Harwell resulted in exacerbation of vegetation impacts at Auchencorth, despite being further from O 3 precursor emission sources. These analyses indicate that quantifications of future improvement in health-relevant O 3 exposure achievable from pan-European O 3 mitigation strategies are highly dependent on the choice of O 3 concentration cut-off threshold, and reduction in potential health impact associated with more modest O 3 concentrations requires reductions in O 3 precursors on a larger (hemispheric) spatial scale. Additionally, while further reduction in regional O 3 is more likely to decrease O 3 vegetation impacts within the spatial domain of Auchencorth compared to Harwell, larger reductions in vegetation impact could be achieved across the UK from reduction of hemispheric background O 3 concentrations...|$|R
40|$|In the mid- 1800 s, {{monitoring}} {{networks were}} established to investigate atmospheric composition impacts, {{and the conditions}} giving rise to them. The development of these networks, in terms of coordination and standardisation between contributing sites, has resulted in large advances in knowledge {{of the nature of}} atmospheric composition. Currently thousands of sites collect high quality atmospheric composition measurements globally. This thesis contends that in order to maximise the information derived from these measurements, a further advancement in standardisation is required to encompass the interpretation of monitoring network data. Currently there are limited examples of a common interpretation of data applied across all sites in a monitoring network, especially in relation to specific atmospheric composition impacts. In this thesis, a ‘chemical climatology’ framework is outlined which provides a common basis for targeting analysis towards identifying the linkage between a specific atmospheric composition impact and its causal drivers. Case studies apply the chemical climatology framework to demonstrate its utility in deriving scientific and policy relevant conclusions using measurement data from the UK monitoring supersites located at Harwell and Auchencorth. Prior to this, the representativeness of each site is quantified through the application of cluster analysis to ozone data at 100 rural European sites to identify groupings of sites with similar ozone variation. Harwell was representative of rural locations within 120 km of London, while Auchencorth was representative of a larger, transboundary spatial domain including the remainder of the rural UK. The first case study links the impact of ozone on human health (quantified by SOMO 10 and SOMO 35 metrics) and vegetation (flux-based PODY) to meteorological and emissions drivers. Between 1990 and 2013 at Harwell, there was a significant decrease in the contribution of European ozone to determining the impacts. Improvement in the human health impact was heavily dependent on the choice of <b>metric</b> (SOMO 35 <b>decreased,</b> no change in SOMO 10), and the vegetation impacts had not improved as high ozone episodes frequently coincided with plant conditions which reduced ozone uptake. These chemical climates emphasise the need for ozone mitigation on larger (hemispheric) scales than currently implemented. Secondly, the impact of 27 measured VOCs on the extent of the regional ozone increment is assessed. The photochemical loss of VOCs is then linked to reported gridded VOC emissions using air mass back trajectory analysis. Ethene and m+p-xylene had the largest diurnal photochemical loss during maximum monthly regional ozone increment, but the key conclusion was the limitation introduced through the reporting of gridded VOC emissions in heavily aggregated source sectors. Finally, the conditions producing the long term health impact of particulate matter (quantified by annual average PM 10 and PM 2. 5 concentrations) at each site are derived through integration of measurements of PM 10 and PM 2. 5 with measurements of PM constituents. It is shown that the frequent, moderate PM 10 and PM 2. 5 concentrations made a larger contribution to annual average values compared to the relatively infrequent high, episodic concentrations. The contribution of PM constituents and the contribution of local vs regional emissions to the range of PM concentrations is investigated. It was concluded that similar reductions in the contribution of secondary inorganic aerosol to the moderate PM 10 and PM 2. 5 concentrations could be achieved from both the reduction of frequently traversed, smaller emissions sources, and less frequently traversed, larger emissions sources. The final chapter demonstrates the benefits from the extension of this framework to an entire monitoring network. It is envisioned that for each atmospheric composition impact, a standard set of statistics would be calculated which quantify the ‘impact’, ‘state’ and ‘drivers’ of that chemical climate. Calculation of ozone human health chemical climates across 100 European monitoring sites demonstrate this concept. This standardised interpretation of monitoring network data not only allows consistent comparison of an impact, but the common basis for determining how the impact is derived allows for the consideration of novel mitigation strategies and their spatial applicability...|$|R
