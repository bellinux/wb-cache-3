1331|1470|Public
5000|$|Temporal: It {{uses the}} block <b>motion</b> <b>vector</b> from List 1 frame, {{located at the}} same {{position}} to deduce the <b>motion</b> <b>vector.</b> List 1 block uses a List 0 block as reference.|$|E
5000|$|Adaptive rood pattern search (ARPS) [...] {{algorithm}} {{makes use}} {{of the fact that}} the general motion in a frame is usually coherent, i.e. if the macro blocks around the current macro block moved in a particular direction then there is a high probability that the current macro block will also have a similar <b>motion</b> <b>vector.</b> This algorithm uses the <b>motion</b> <b>vector</b> of the macro block to its immediate left to predict its own <b>motion</b> <b>vector.</b>|$|E
50|$|Adaptive <b>motion</b> <b>vector</b> resolution.|$|E
30|$|If the {{selected}} <b>motion</b> <b>vectors</b> remain unchanged for neighboring blocks (which is valid when transcoding with mode and MV reuse), the predicted <b>motion</b> <b>vectors</b> and consequently {{the rate of}} the candidate <b>motion</b> <b>vectors</b> (RMC) remain unaffected for the current block.|$|R
3000|$|The {{first feature}} {{was chosen to}} capture the {{stability}} of the head. All perpendiculars to the rotational <b>motion</b> <b>vectors</b> will intersect the centre of the rotating head. Nonrotational <b>motion</b> <b>vectors</b> should rarely cross the centre of rotation. Plotting the perpendicular lines to the <b>motion</b> <b>vectors</b> in an accumulator array allows an approximate centre of rotation to be found. This is similar to the straight line analysis of Wong et al. for fast rotation centre identification [35]. Using only <b>motion</b> <b>vectors</b> within [...]...|$|R
3000|$|When {{observing a}} {{rotational}} motion field, {{it can be}} seen that in a row of <b>motion</b> <b>vectors,</b> no two <b>motion</b> <b>vectors</b> have the same [...]...|$|R
5000|$|... 1. Binarize {{the value}} MVDx, the <b>motion</b> <b>vector</b> {{difference}} in the x direction.|$|E
5000|$|May contain both {{image data}} and <b>motion</b> <b>vector</b> {{displacements}} and combinations of the two.|$|E
5000|$|Set {{step size}} S = max (|X|,|Y|), where (X,Y) is the {{coordinate}} of predicted <b>motion</b> <b>vector</b> ...|$|E
40|$|In this work, we have {{explored}} {{the prospect of}} segmenting crowd flow in H. 264 compressed videos by merely using <b>motion</b> <b>vectors.</b> The <b>motion</b> <b>vectors</b> are extracted by partially decoding the corresponding video sequence in the H. 264 compressed domain. The region of interest ie., crowd flow region is extracted and the <b>motion</b> <b>vectors</b> that spans the region of interest is preprocessed and a collective representation of the <b>motion</b> <b>vectors</b> for the entire video is obtained. The obtained <b>motion</b> <b>vectors</b> for the corresponding video is then clustered by using EM algorithm. Finally, the clusters which converges to a single flow are merged together based on the bhattacharya distance measure between the histogram of the of {{the orientation of the}} <b>motion</b> <b>vectors</b> at the boundaries of the clusters. We had implemented our proposed approach on the complex crowd flow dataset provided by 1] and compared our results by using Jaccard measure. Since we are performing crowd flow segmentation in the compressed domain using only <b>motion</b> <b>vectors,</b> our proposed approach performs much faster compared to other pixel domain counterparts still retaining better accuracy...|$|R
40|$|Abstract—This paper {{presents}} a robust algorithm for video sequences stabilization. Motion estimation is achieved using block <b>motion</b> <b>vectors.</b> In {{this way the}} same motion estimator of mpeg encoder can be used. The simple use of block <b>motion</b> <b>vectors</b> can give unreliable global <b>motion</b> <b>vectors</b> and so elaborations are done to make the algorithm robust...|$|R
3000|$|... {{estimates}} {{the reliability of}} the <b>motion</b> <b>vectors</b> at each point by measuring the similarity of the backward and forward <b>motion</b> <b>vectors</b> and the difference in pixel intensities at the estimated displacements.|$|R
5000|$|If {{the minimum}} cost {{function}} occurs at origin, stop {{the search and}} set <b>motion</b> <b>vector</b> to (0,0) ...|$|E
5000|$|The {{one that}} gives lowest weight {{is the closest}} match, set the <b>motion</b> <b>vector</b> to that {{location}} ...|$|E
50|$|Concisely, the target's {{movement}} was a vector, {{and if that}} didn't change, the generated range, bearing, and elevation were accurate for up to 30 seconds. Once the target's <b>motion</b> <b>vector</b> became stable, the computer operators told the gun director officer ("Solution Plot!"), who usually gave the command to commence firing. Unfortunately, this process of inferring the target <b>motion</b> <b>vector</b> required a few seconds, typically, which might take too long.|$|E
30|$|For B-frames, {{forward or}} {{backward}} reference frames {{are used for}} predicting the blocks. To ensure that the estimated <b>motion</b> <b>vectors</b> represent the displacement of objects over consecutive frames, the <b>motion</b> <b>vectors</b> of each block should refer to the same region 2 in forward or backward reference frames. If {{that is not the}} case, the block's motion is estimated as the median of its neighbouring-block <b>motion</b> <b>vectors.</b>|$|R
40|$|In this work, {{the motion}} {{parameters}} of the bi-directionally predicted pictures (B-pictures) of MPEG- 1, 2 are exploited for concealment of large portions of corrupted anchor pictures that might arise due to channel errors or packet losses. To further {{enhance the quality of}} the concealed anchor pictures, we propose two methods of constraining the <b>motion</b> <b>vectors</b> of the B-pictures that strengthen the tie between them and those of the anchor pictures in the same picture sub-group. In one method, the macroblock decisions on the last B-picture in each sub-group is constrained to be bi-directional if those of the other B-pictures are not, such that the derived <b>motion</b> <b>vectors</b> for the concealment of the anchor picture are always composed from the forward and backward <b>motion</b> <b>vectors</b> of the bi-directional motions. Second, the bi-directional <b>motion</b> <b>vectors</b> of the B-pictures in each sub-group is constrained such that the vectorial sum of their forward and backward <b>motion</b> <b>vectors</b> results in accurate motion prediction of the anchor picture. The experimental results show that while the composed <b>motion</b> <b>vectors</b> improve the quality of concealment over the conventional methods by more than 3 - 4 dB, another 2 dB improvement can be achieved by constraining the generation of the bi-directional <b>motion</b> <b>vectors.</b> * Author for correspondence...|$|R
40|$|We {{propose a}} new {{approach}} to progressively compress time-dependent geometry. Our approach exploits correlations in <b>motion</b> <b>vectors</b> to achieve better compression. We use unsupervised learning techniques to detect good clusters of <b>motion</b> <b>vectors.</b> For each detected cluster, we build a hierarchy of <b>motion</b> <b>vectors</b> using pairwise agglomerative clustering, and succinctly encode the hierarchy using entropy encod-ing. We demonstrate our approach on a client-server system that we have built for downloading time-dependent geometry...|$|R
50|$|Rood pattern search {{directly}} {{puts the}} search {{in an area}} where there is a high probability of finding a good matching block. The main advantage of ARPS over DS is if the predicted <b>motion</b> <b>vector</b> is (0, 0), it does not waste computational time in doing LDSP, but it directly starts using SDSP. Furthermore, if the predicted <b>motion</b> <b>vector</b> is far away from the center, then again ARPS saves on computations by directly jumping to that vicinity and using SDSP, whereas DS takes its time doing LDSP.|$|E
5000|$|May contain {{image data}} and/or <b>motion</b> <b>vector</b> displacements. Older {{standards}} allow {{only a single}} global motion compensation vector for the entire frame or a single motion compensation vector per macroblock.|$|E
50|$|An inter {{coded frame}} {{is divided into}} blocks known as macroblocks. After that, instead of {{directly}} encoding the raw pixel values for each block, the encoder will {{try to find a}} block similar to the one it is encoding on a previously encoded frame, referred to as a reference frame. This process is done by a block matching algorithm. If the encoder succeeds on its search, the block could be encoded by a vector, known as <b>motion</b> <b>vector,</b> which points to the position of the matching block at the reference frame. The process of <b>motion</b> <b>vector</b> determination is called motion estimation.|$|E
30|$|The {{system can}} {{normalize}} the <b>motion</b> <b>vectors</b> more accurately when the blocks {{are set to}} be small. However, the suitable block size depends on {{the resolution of the}} video, the distance from the camera to people, and the number of <b>motion</b> <b>vectors</b> used for training. If the resolution was low, the camera was near people, and the training sample vectors were poor, block size should be large, because the average velocity might be influenced by unusual <b>motion</b> <b>vectors.</b>|$|R
30|$|In [9], {{a method}} was {{presented}} for detecting the human’s reciprocating motion in pornographic videos. The approach extracted <b>motion</b> <b>vectors</b> from the MPEG video stream. The <b>motion</b> <b>vectors</b> were smoothed by vector median and mean filters to remove outliers and small <b>motion</b> <b>vectors.</b> Objectionable videos were then extracted by motion-based features. The method used only motion information for classification. Therefore, the algorithm could not recognize objectionable videos with global motions or videos with no considerable motion.|$|R
40|$|This paper {{describes}} novel motion mapping algorithms {{aimed for}} low-complexity MPEG- 2 to AVC transcoding. The proposed algorithms efficiently map incoming MPEG- 2 <b>motion</b> <b>vectors</b> to outgoing AVC <b>motion</b> <b>vectors</b> {{regardless of the}} block sizes that the <b>motion</b> <b>vectors</b> correspond to. Extensive simulation results show that our proposed transcoder incorporating the proposed algorithms achieves very good rate-distortion performance with low complexity. Compared with the cascaded decoder-encoder solution, the proposed approach could achieve similar coding efficiency while significantly reduce the complexity...|$|R
50|$|While MPEG-2 {{allowed a}} ½ pixel resolution, Inter frame allows up to ¼ pixel resolution. That {{means that it}} is {{possible}} to search a block in the frame to be coded in other reference frames, or we can interpolate nonexistent pixels to find blocks that are even better suited to the current block. If <b>motion</b> <b>vector</b> is an integer number of units of samples, that means it is possible to find in reference pictures the compensated block in motion. If <b>motion</b> <b>vector</b> is not an integer, the prediction will be obtained from interpolated pixels by an interpolator filter to horizontal and vertical directions.|$|E
50|$|P-frames {{have one}} <b>motion</b> <b>vector</b> per macroblock, {{relative}} to the previous anchor frame. B-frames, however, can use two motion vectors; one from the previous anchor frame, and one from the future anchor frame.|$|E
5000|$|For video compression, key {{frames are}} divided into macroblocks. The motion model is a {{disruption}} of a key frame, where each macroblock is translated by a <b>motion</b> <b>vector</b> given by the motion parameters.|$|E
30|$|Jansohn et al. [10] {{utilized}} {{the fusion of}} <b>motion</b> <b>vectors</b> and spatial features for detecting pornographic video contents. Bag-of-Visual-Words based on the histograms of local patches were used as spatial features. The motion analysis was based on MPEG- 4 <b>motion</b> <b>vectors</b> extracted by the XViD codec.|$|R
5000|$|The {{ability to}} use {{multiple}} <b>motion</b> <b>vectors</b> per macroblock (one or two per partition) with a maximum of 32 {{in the case of}} a B macroblock constructed of 16 4×4 partitions. The <b>motion</b> <b>vectors</b> for each 8×8 or larger partition region can point to different reference pictures.|$|R
50|$|The DC {{coefficients}} and <b>motion</b> <b>vectors</b> are DPCM-encoded.|$|R
5000|$|AVIATION...A few severe {{thunderstorms}} with hail {{surface and}} aloft to 3 inches. Extreme turbulence and surface wind gusts to 60 knots. A few cumulonimbi with maximum tops to 500. Mean storm <b>motion</b> <b>vector</b> 23035.|$|E
5000|$|Since {{interframe}} {{motion is}} often predictable owing to Newton's laws of {{motion in the}} real world, the <b>motion</b> <b>vector</b> can then be used to calculate where the block will probably be in the next field.|$|E
50|$|The {{process of}} {{determining}} the target's <b>motion</b> <b>vector</b> was done primarily with an accurate constant-speed motor, disk-ball-roller integrators, nonlinear cams, mechanical resolvers, and differentials. Four special coordinate converters, each with a mechanism in part {{like that of a}} traditional computer mouse, converted the received corrections into target <b>motion</b> <b>vector</b> values. The Mk. 1 computer attempted to do the coordinate conversion (in part) with a rectangular-to polar converter, but that didn't work as well as desired (sometimes trying to make target speed negative!). Part of the design changes that defined the Mk. 1A were a re-thinking of how to best use these special coordinate converters; the coordinate converter ("vector solver") was eliminated.|$|E
30|$|After {{refining}} the <b>motion</b> <b>vectors</b> in different stages, the absolute horizontal {{values of the}} refined <b>motion</b> <b>vectors</b> are used to approximate initial depth values. To enhance the visual depth perception, we propose to increase the contrast among the initial approximated depth values by using a nonlinear scaling model.|$|R
30|$|We {{compared}} the tracking accuracy {{of our system}} with a baseline method that was created based on the KLT tracker. The KLT tracker is an algorithm that selects and keeps track of feature points that are optimal for tracking. It is widely used in visual feature tracking and the method {{can be used with}} the OpenCV video library [34]. The KLT tracker detects <b>motion</b> <b>vectors</b> around moving objects; <b>motion</b> <b>vectors</b> that have the same length and direction tend to be detected around one person. Thus, the baseline method uses <b>motion</b> <b>vectors</b> to cluster human regions.|$|R
40|$|To {{convert a}} {{compressed}} video sequence to a lower-resolution compressed video, one typically needs to decompress the original sequence, down-sample each frame, and recompress it. It involves motion estimation in the reduced sequence, which is computational intensive. In this paper, a novel fast motion estimation {{algorithm is proposed}} to predict the <b>motion</b> <b>vectors</b> of the reduced-resolution video without performing any search. The <b>motion</b> <b>vectors</b> are predicted by applying spatial-variant filters to the <b>motion</b> <b>vectors</b> of the original compressed high-resolution video. In our simulations, our method outperforms other existing fast predictive algorithms which involves no searching...|$|R
