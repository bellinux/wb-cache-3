51|226|Public
40|$|Objectives: To {{assess and}} {{describe}} the experiences of those caring for hepatitis C patients in Karachi, Pakistan. Methods: Using a qualitative approach, the study adopted a descriptive exploratory design for which 8 caregirers were selected through snowball sampling technique {{from different parts of}} Karachi. Data was collected between May and July 2010 through semi-structured interviews from the caregivers. The interviews were recorded on tape and were transcribed verbatim. The data was <b>manually</b> <b>analysed</b> for extracting themes and categories. Results: The analyses of data led to one theme -...|$|E
40|$|Sentence types typical to Swedish {{clinical}} text were extracted {{by comparing}} sentence part-of-speech tag sequences in clinical and in standard Swedish text. Parsings by a syntactic dependency parser, trained on standard Swedish, were <b>manually</b> <b>analysed</b> for the 33 sentence types most typical to clinical text. This analysis {{resulted in the}} identification of eight error types, and for two of these error types, preprocessing rules were constructed to improve the performance of the parser. For {{all but one of the}} ten sentence types affected by these two rules, the parsing was improved by pre-processing. ...|$|E
40|$|This report {{describes}} {{the transformation of}} feature films into hypervideo by representing their story structures using plot unties. Plot units represent cause-effect relationships between characters’ affect states and the events in a story. We use plot units to structure hypervideo links between intervals of video data. We have <b>manually</b> <b>analysed</b> two full-length feature films in terms of plot units. A system was developed to store and edit data about plot units and to navigate films by following hypervideo links based {{on the structure of}} the plot units. The effect of plot units was evaluated by having users complete question-answering tasks using the system. Results suggest that when using the links subjects gave better answers to questions about the film. In questionnaire feedback, users supported using this kind of hypervideo for watching and rewatching films on future domestic video players. </p...|$|E
5000|$|Schemas [...] are {{templates}} which explicitly {{specify the}} content of a generated text (as well as Document structuring information). Typically they are constructed by <b>manually</b> <b>analysing</b> a corpus of human-written texts in the target genre, and extracting a content template from these texts. Schemas work well in practice in domains where content is somewhat standardised, but work less well in domains where content is more fluid (such as the medical example above).|$|R
5000|$|Schemas [...] are {{templates}} which explicitly specify sentence ordering and grouping for {{a document}} (as well as Content determination information). Typically they are constructed by <b>manually</b> <b>analysing</b> a corpus of human-written texts {{in the target}} genre, and extracting a document template from these texts. Schemas work well in practice for texts which are short (5 sentences ot less) and/or have a standardised structure, but have problems in generating texts which are longer {{and do not have}} a fixed structure.|$|R
40|$|Altmetrics measure {{scientific}} impact {{outside of}} traditional scientific literature. We identify mentions {{of scientific research}} or entities like researchers, academic or research organizations in a corpus containing blogs, articles, news items etc. We first <b>manually</b> <b>analyse</b> the corpus for patterns of such informal mentions and then apply text mining techniques by developing extraction rules for mining informal mentions. We apply them to our development corpus and present our results. This work takes us closer to developing concrete altmetrics for determining research impact on news and public discourse ultimately leading to measuring impact of scientific research on government policies. This publication has emanated from research conducted with the financial support of Science Foundation Ireland (SFI) under Grant Number SFI/ 12 /RC/ 2289 and by targeted project funding from Elsevier...|$|R
40|$|We {{present an}} Embodied Conversational Agent(ECA) that {{incorporates}} a context-sensitive mechanism for handling user barge-in. The affective ECA engages the user in social conversation, and is fully implemented. We will use actual examples of system behaviour to illustrate. The ECA {{is designed to}} recognise and be empathetic to the emotional state of the user. It is able to detect, react quickly to, and then follow up with considered responses to different kinds of user interruptions. The design of the rules which enable the ECA to respond intelligently to different types of interruptions was informed by <b>manually</b> <b>analysed</b> real data from human–human dialogue. The rules represent recoveries from interruptions as two-part structures: an address followed by a resumption. The system is robust enough to man- age long, multi-utterance turns by both user and system, which creates good opportunities for the user to interrupt while the ECA is speaking...|$|E
40|$|International audienceThis study {{addresses}} the methodological problem of result falsification in Cognitive Semantics, {{specifically in the}} descriptive analysis of semasiological variation, or “polysemy. ” It argues that <b>manually</b> <b>analysed</b> corpus data {{can be used to}} describe models of semantic structure. The method proposed is quantified, permitting repeat analysis. The operationalisation of a semasiological structure employed in the study takes the principle of semantic features and applies them to a contextual analysis of usage-events, associated with the lexeme under scrutiny. The feature analysis, repeated on a large collection of occurrences, results in a set of metadata that constitutes the usage-profile of the lexeme. Multivariate statistics are then employed to identify patterns in those metadata. The case study examines 500 occurrences of the English lexeme annoy. Three basic senses are identified as well as a more complex array of semantic variations linked to morpho-syntactic context of usage...|$|E
40|$|We {{present a}} study on the text simplifica-tion {{operations}} undertaken collaboratively by Simple English Wikipedia contribu-tors. The aim is to understand whether a complex-simple parallel corpus involv-ing this version of Wikipedia is appropri-ate as data source to induce simplifica-tion rules, and whether we can automat-ically categorise the different operations performed by humans. A subset of the cor-pus was first <b>manually</b> <b>analysed</b> to iden-tify its transformation operations. We then built machine learning models to attempt to automatically classify segments based on such transformations. This classifica-tion could be used, e. g., to filter out po-tentially noisy transformations. Our re-sults show that the most common transfor-mation operations performed by humans are paraphrasing (39. 80 %) and drop of in-formation (26. 76 %), which {{are some of the most}} difficult operations to generalise from data. They are also the most diffi-cult operations to identify automatically, with the lowest overall classifier accuracy among all operations (73 % and 59 %, re-spectively). ...|$|E
40|$|This paper {{outlines}} {{a number}} of key lessons learned from an investigation into the techniques malicious executable software can employ to hinder digital forensic examination. Malware signature detection has been recognised by researchers to be far less than ideal. Thus, the forensic analyst may be required to <b>manually</b> <b>analyse</b> suspicious files. However, in order to hinder the forensic analyst, hide its true intent and to avoid detection, modern malware can be wrapped with packers or protectors, and layered with a plethora of antianalysis techniques. This necessitates the forensic analyst to develop static and dynamic analysis skills tailored to navigate a hostile environment. To this end, the analyst must understand the anti-analysis techniques that can be employed and how to mitigate them, the limitations of existing tools and how to extend them, and how to employ an appropriate analysis methodology to uncover the intent of the malware...|$|R
40|$|The {{amount of}} {{information}} available in organisation internal information systems or in international networks such as Internet is growing at an important rate. As a consequence, more and more sophisticated information management systems are needed. Among the functionalities of those systems, information retrieval {{is one of the}} key elements. It allows one to find the relevant information according to its needs. Because of the amount of retrieved information, it is less and less possible to <b>manually</b> <b>analyse</b> it. That is why new systems functionalities needs have recently appeared. The objective is to automate as much as possible the transformation of the retrieved information (raw information) into knowledge or generalised patterns (hidden information). That new computed information could then be used directly by the users (manufacturers, searchers, [...] .) to have an overview of the area they are interested on and to take the relevant decisions. One of the main applications is monitoring of [...] ...|$|R
40|$|Most of {{the data}} {{produced}} in software projects is of textual nature: source code, specifications, or documentations. The advances in quantitative analysis methods drove a lot of data analytics in software engineering. This has overshadowed to some degree the importance of texts and their qualitative analysis. Such analysis has, however, merits for researchers and practitioners as well. In this chapter, we describe the basics of analysing text in software projects. We first describe how to <b>manually</b> <b>analyse</b> and code textual data. Next, we give an overview of mixed methods to automatic text analysis including N-Grams and clone detection {{as well as more}} sophisticated natural language processing identifying syntax and contexts of words. Those methods and tools are of critical importance to aid in the challenges in today's huge amounts of textual data. We illustrate the introduced methods via a running example and conclude by presenting two industrial studies. Comment: in The Art and Science of Analyzing Software Data, 201...|$|R
40|$|Most of the {{research}} on the extraction of idiomatic multiword expressions (MWEs) focused on the acquisition of MWE types. In the present work we investigate whether a text instance of a potentially idiomatic MWE is actually used idiomatically in a given context or not. Inspired by the dataset provided by (Cook et al., 2008), we <b>manually</b> <b>analysed</b> 9, 700 instances of potentially idiomatic prepositionnoun-verb triples (a frequent pattern among German MWEs) to identify, on token level, idiomatic vs. literal uses. In our dataset, all sentences are provided along with their morpho-syntactic properties. We describe our data extraction and annotation steps, and we discuss quantitative results from both EUROPARL and a German newspaper corpus. We discuss the relationship between idiomaticity and morpho-syntactic fixedness, and we address issues of ambiguity between literal and idiomatic use of MWEs. Our data show that EUROPARL is particularly well suited for MWE extraction, as most MWEs in this corpus are indeed used only idiomatically. 1...|$|E
40|$|Integrated {{video camera}} {{systems have been}} {{installed}} on fishing boats to trial for the fishery monitoring in some countries. Currently, substantial amount of video footage is <b>manually</b> <b>analysed</b> off the boats after each trip. Automatic processing of the videos is important for saving time and manpower. In this paper, an intelligent tuna recognition method is proposed. The method includes four steps. Firstly, the video is pre-processed by suppressing fast moving objects such as human. Secondly, the colour and texture features are extracted to describe tuna, deck and other objects. Thirdly, support vector machine and statistic shape model are employed to identity and recognise tuna. Finally, a prior-knowledge based post-processing method is used to refine the recognition result. The experiment has showed that the proposed method is accurate and robust in tuna recognition. The method {{can also be used}} for other fish recognition applications, benefitting fisheries monitoring by providing efficient and automatic fish recognition...|$|E
40|$|Despite their {{importance}} for educational practice, reflective writings are still <b>manually</b> <b>analysed</b> and assessed, posing a constraint {{on the use}} of this educational technique. Recently, research started to investigate automated approaches for analysing reflective writing. Foundational to many automated approaches is the knowledge of words that are important for the genre. This research presents keywords that are specific to several categories of a reflective writing model. These keywords have been derived from eight datasets, which contain several thousand instances using the log-likelihood method. Both performance measures, the accuracy and the Cohen's κ, for these keywords were estimated with ten-fold cross validation. The results reached an accuracy of 0. 78 on average for all eight categories and a fair to good inter-rater reliability for most categories even though it did not make use of any sophisticated rule-based mechanisms or machine learning approaches. This research contributes to the development of automated reflective writing analytics that are based on data-driven empirical foundations...|$|E
40|$|The study {{attempted}} {{to determine whether}} the failure of Down Syndrome (DS) individuals to show the modality effect (the tendency to show better short-term memory for brief sequences of auditory rather than visual information) is due to the verbal-expressive demands of oral responding in memory tasks. DS, nonretarded (NR) and MR (non-DS mentally retarded) Ss (N= 33) listened to or looked at increasingly-long sequences of single digits and {{attempted to}} recall them either orally or <b>manually.</b> <b>Analyses</b> suggested that (1) manual responding failed to enhance auditory recall in either DS or any other Ss; (2) difficulity in recalling auditory stimuli was greatest for DS mentally retarded Ss; and (3) DS auditory difficulty was most apparent in the recall of order information. Findings suggested that the adverse consequences of mental retardation were most apparent in the recall of order information, and that this recall difficulty was greater for DS Ss. (Author /CL Reproductions supplied by EDRS are the best that can be made from the original document. rr...|$|R
40|$|Abstract—This paper {{presents}} a dynamic reverse engineering approach and a tool, ReGUI, developed {{to reduce the}} effort of obtaining models of the structure and behaviour of a software applications Graphical User Interface (GUI). It describes, in more detail, {{the architecture of the}} REGUI tool, the process followed to extract information and the different types of models produced to represent such information. Each model describes different characteristics of the GUI. Besides graphical representations, which allow checking visually properties of the GUI, the tool also generates a textual model in Spec # {{to be used in the}} context of model based GUI testing and a Symbolic Model Verification model, which enables the verification of several properties expressed in computation tree logic. The models produced must be completed and validated in order to ensure that they faithfully describe the intended behaviour. This validation process may be performed by <b>manually</b> <b>analysing</b> the graphical models produced or automatically by proving properties, such as reachability, through model checking. A feasibility study is described to illustrate the overall approach, the tool and the results obtained...|$|R
40|$|Among {{the vast}} numbers of images on the web are many duplicates and near-duplicates, that is, {{variants}} derived from the same original image. Such near-duplicates appear in many web image searches and may represent infringements of copyright or indicate the presence of redundancy. While methods for identifying near-duplicates have been investigated, {{there has been no}} analysis of the kinds of alterations that are common on the web or evaluation of whether real cases of near-duplication can in fact be identified. In this paper we use popular queries and a commercial image search service to collect images that we then <b>manually</b> <b>analyse</b> for instances of near-duplication. We show that such duplication is indeed significant, but that not all kinds of image alteration explored in previous literature are evident in web data. Removal of near-duplicates from a collection is impractical, but we propose that they be removed from sets of answers. We evaluate our technique for automatic identification of near duplicates during query evaluation and show that it has promise as an effective mechanism for management of near-duplication in practice...|$|R
40|$|Although {{research}} is increasingly interested in session-based retrieval, comparably little work {{has focused on}} how best to divide web histories into sessions. Most automated attempts to divide web histories into sessions have focused on dividing web logs using simplistic rules, including user identifiers and specific time gaps. This research, however, is focused on understanding the full range of factors that affect the division of sessions, so that we can begin to go beyond current naive techniques like fixed time periods of inactivity. To investigate these factors, 10, 000 log items were <b>manually</b> <b>analysed</b> by their owners into 847 naturally occurring web sessions. During interviews, participants reviewed their own web histories to identify these sessions, and described the causes of divisions between sessions. This paper contributes a taxonomy of six factors {{that can be used to}} better model the divisions between sessions, along with initial insights into how the divided sessions manifested in web logs. The factors in our taxonomy provide focus for future work, including our own, for finding practical ways to more intelligently divide and identify sessions for improved session-based retrieval...|$|E
40|$|Continuous patient {{monitoring}} systems acquire {{enormous amounts of}} data that is either <b>manually</b> <b>analysed</b> by doctors or automatically processed using intelligent algorithms. Sections of data acquired over {{long period of time}} can be corrupted with artefacts due to patient movement, sensor placement and interference from other sources. Because of the large volume of data these artefacts need to be automatically identified so that the analysis systems and doctors are aware of them while making medical diagnosis. This paper explores three important factors that must be considered and quantified for the design and evaluation of automatic artefact identification algorithms: signal quality, interpretation quality and computational complexity. The first two are useful to determine the effectiveness of an algorithm while the third is particularly vital in mHealth systems where computational resources are heavily constrained. A series of artefact identification and filtering algorithms are then presented focusing on the electrocardiography data. These algorithms are quantified using the three metrics to demonstrate how different algorithms can be evaluated and compared to select the best ones for a given wireless sensor network...|$|E
40|$|The urge {{to develop}} Malaysia through {{the growth of}} {{technology}} entrepreneurship has necessitated the government in establishing some technology financing agencies aimed at providing full assistance to technology entrepreneurs. This {{is because they are}} perceived to lack in certain support in technical expertise, training, disseminating information and, above all, in financing. The establishment of venture capital firms by the Malaysian Government is with the intention to encourage investments in high growth firms because they find it difficult to raise adequate financing at the early stage for growth due to their perceived high risk and opportunity uncertainty nature. This study elaborates on the assessment criteria of venture capital firms in Malaysia. Multiple case study approach is utilised to collect data for this study. Data is collated, transcribed and <b>manually</b> <b>analysed.</b> This study reported the factors considered by Malaysian venture capital firms when decisions to finance high growth companies are to be taken. The findings from this study will be useful to decision makers in public and private sector, professionals, researchers in Malaysia and other countries...|$|E
40|$|The {{traditional}} approach to bioinformatics analyses relies on independent task-specific services and applications, using different {{input and output}} formats, often idiosyncratic, and frequently not designed to inter-operate. In general, such analyses were performed by experts who manually verified the results obtained at each step in the process. Today, the amount of bioinformatics information continuously being produced means that handling the various applications used to study this information presents a major data management and analysis challenge to researchers. It is now impossible to <b>manually</b> <b>analyse</b> all this information and new approaches are needed {{that are capable of}} processing the large-scale heterogeneous data in order to extract the pertinent information. We review the recent use of integrated expert systems aimed at providing more efficient knowledge extraction for bioinformatics research. A general methodology for building knowledge-based expert systems is described, focusing on the unstructured information management architecture, UIMA, which provides facilities for both data and process management. A case study involving a multiple alignment expert system prototype called AlexSys is also presented...|$|R
40|$|Abstract. A few {{attempts}} to create taxonomies in evolutionary computation have been made. These either group algorithms or group {{problems on the}} basis of their similarities. Similarity is typically evaluated by <b>manually</b> <b>analysing</b> algorithms/problems to identify key characteristics that are then used as a basis to form the groups of a taxonomy. This task is not only very tedious but it is also rather subjective. As a consequence the resulting taxonomies lack universality and are sometimes even questionable. In this paper we present a new and powerful approach to the construction of taxonomies and we apply it to Genetic Programming (GP). Only one manually constructed taxonomy of problems has been proposed in GP before, while no GP algorithm taxonomy has ever been suggested. Our approach is entirely automated and objective. We apply it to the problem of grouping GP systems with their associated parameter settings. We do this {{on the basis of}} performance signatures which represent the behaviour of each system across a class of problems. These signatures are obtained thorough a process which involves the instantiation of models of GP’s performance. We test the method on a large class of Boolean induction problems. ...|$|R
40|$|Brain {{hemorrhage}} detection is clinically {{crucial for}} the patients having head trauma and neurological disturbances. Early finding and accurate diagnosis of the brain abnormalities {{is one of the}} key contributions for the execution of the successful therapy and proper treatment. Multi-slice Computed Tomograph (CT) scans are widely employed in today’s examination of head traumas due to its effectiveness to disclose some abnormalities such as brain hemorrhages and so on. However, radiologists have to <b>manually</b> <b>analyse</b> the CT slices for the presence of brain hemorrhages. Due to the large volume of CT scan examinations, it is important to develop a computerised system that can assist the radiologists to automatically detect the presence of the brain abnormalities as well as automatically retrieve the images. This thesis presents an automated annotation and classification of the CT brain images. The main objective is to propose a new methodology to annotate and classify the different types of brain hemorrhages which are intra-axial, subdural and extradural hemorrhages. Besides, this thesis also aims to evaluate and investigate the effectiveness and suitability of different segmentation and classification techniques as well as introduce the new features for the classification...|$|R
40|$|International audienceWith the {{continuous}} expansion of single cell biology, {{the observation of}} the behaviour of individual cells over extended durations and with high accuracy has become a problem of central importance. Surprisingly, even for yeast cells that have relatively regular shapes, no solution has been proposed that reaches the high quality required for long-term experiments for segmentation and tracking (S&T) based on brightfield images. Here, we present CellStar, a tool chain designed to achieve good performance in long-term experiments. The key features are {{the use of a}} new variant of parametrized active rays for seg-mentation, a neighbourhood-preserving criterion for tracking, and the use of an iterative approach that incrementally improves S&T quality. A graphical user interface enables manual corrections of S&T errors and their use for the automated correction of other, related errors and for parameter learning. We created a benchmark dataset with <b>manually</b> <b>analysed</b> images and compared CellStar with six other tools, showing its high performance, notably in long-term tracking. As a community effort, we set up a website, the Yeast Image Toolkit, with the benchmark and the Evaluation Platform to gather this and additional information provided by others...|$|E
40|$|Purpose To {{describe}} {{barriers and}} facilitators of cervical screening practices among African immigrant {{women living in}} Brisbane, Australia. Method: Nineteen African immigrant women (10 refugee and 9 non-refugee) were recruited using convenience sampling. The {{interviews were conducted with}} a semi-structured and open-ended questionnaire guide. All the interviews were audio recorded and transcribed verbatim. The data was <b>manually</b> <b>analysed</b> using interpretative thematic analysis. Thematic categories were identified and organised into coherent broader areas. Results Lack of knowledge about cervical cancer and Pap smear, the absence of warning signs, embarrassment, fear, concern about the gender of the service provider, lack of privacy, cultural and religious beliefs, and healthcare system factors were identified as barriers to screening. The results did not show any major differences between refugee and non-refugee women. Recommendation of the test by health professionals, provision of standardised information on the test, and preferences for female service providers were identified as facilitators of cervical screening. Conclusion There is a need to provide culturally appropriate approaches to cervical screening practices and to enhance cultural competence among health professionals to apply service delivery models that honour group cultures...|$|E
40|$|International audienceThis {{study has}} two aims: {{to show the}} methodological {{possibility}} of doing purely subjective semantic research quantitatively and to demonstrate theoretically that discreet senses and discreet linguistic forms do not exist. On the methodological front, it argues that, with due caution and statistical modelling, subjective semantic characteristics, such as affect and cause, can be successfully employed in corpusdriven research. The theoretical implications show that we cannot treat lexical senses as discreet categories and that the semasiological - onomasiological and polysemy - synonymy distinctions are not tenable and must be replaced with a more multidimensional and variable conception of semantic structure. The case study examines a sample of 650 occurrences of the lexeme bother in British and American English. The occurrences are <b>manually</b> <b>analysed</b> {{for a range of}} formal and semantic features. The exploratory multivariate technique Correspondence Analysis is used to indentify three basic senses relative to formal variation and subjective usage-features. Two of these sense clusters are then verified using Logistic Regression Analysis. The analysis demonstrates a statistically significant difference between the two senses and indentifies which of the semantic features are most important in distinguishing the uses. The statistical model is powerful and its predictive strength serves as further verification of the accuracy of the semantic analysis...|$|E
40|$|The photo-sharing website Flickr {{has become}} a {{valuable}} informal information source in disciplines such as geography and ecology. Some ecologists, for instance, have been <b>manually</b> <b>analysing</b> Flickr to obtain information that is more up-to-date than what is found in traditional sources. While several previous works have shown the potential of Flickr tags for characterizing places, it remains unclear to what extent such tags {{can be used to}} derive scientifically useful information for ecologists in an automated way. To obtain a clearer picture about the kinds of environmental features that can be modelled using Flickr tags, we consider the problem of predicting scenicness, species distribution, land cover, and several climate related features. Our focus is on comparing the predictive power of Flickr tags with that of structured data from more traditional sources. We find that, broadly speaking, Flickr tags perform comparably to the considered structured data sources, being sometimes better and sometimes worse. Most importantly, we find that combining Flickr tags with structured data sources consistently, and sometimes substantially, improves the results. This suggests that Flickr indeed provides information that is complementary to traditional sources...|$|R
40|$|Sistem Maklumat Keluar Masuk Penghuni Asrama SMK Kuala Krau Menggunakan MyKad dan Aplikasi Mobile (SMAKK) is {{system which}} {{developed}} in order to / replace current system at Sekolah Menengah Kebangsaan (SMK) Kuala Krau hostel. Currently, manual registration is use in SMK Kuala Krau hostel for students movement, where students have to record their movement details on the log book. The problem in the current system are, the possibility of losing students record data due to losing the log book or misplacement of it. In this manual system, warden have problem to keep track the student movement because warden have to <b>manually</b> <b>analyse</b> each record one by one. Therefore, SMAKK is develop to overcome the problem. This system is able to record student information by using Malaysia Government Multipurpose Card (MyKad). This system able to capture date and time when student register in/out. In/out information are recorded in a database, which provides data manipulation and report generating. The benefits of this system are to provide organized view of student records and reducing the time spent on managing student in/out information and can be access using PDA which can help warden to monitoring in/out activities easily...|$|R
50|$|The current {{practice}} is to <b>manually</b> transform and <b>analyse</b> data from various investigative sites and IT systems. This is mostly an offline exercise done {{towards the end}} of a clinical trial. The current FDA rejection rate of New Drug Applications (NDA) due to data related issues stand at 50%, requiring rework and re-submissions leading to further delays and cost over runs.|$|R
40|$|Patients with obstructive {{sleep apnea}} (OSA) {{syndrome}} experience repeated periods of apnea and arousal during sleep. A condition which in short term leads to excessive daytime sleepiness {{and in the long}} term may have clinical consequences such as stroke and cardiovascular abnormalities. Although complex equipment can be used to screen for sleep apnea, the screening tests are often expensive, inconvenient for the patient, and time-consuming to be <b>manually</b> <b>analysed.</b> This research investigates methods for automating sleep apnea screening using low-cost off-body cameras. Polysomnography video recordings of twenty one patients, 11 with OSA, and 10 'normals' who were referred for suspected OSA, were analysed with the objective to differentiate the two groups. The proposed technique is based on motion estimation in videos using two successive video frames. The complexities of motion signals from the video data were analysed by calculating sample entropy over multiple time scales. The sample entropy values providing the best separation between the OSA and non-OSA groups were chosen using the Bhattacharyya distance and were then used as the input to a support vector machine classifier. The classification results both on the training and validation data indicate that patients with OSA can be differentiated from patients without OSA with 90 % accuracy. © 2012 IEEE...|$|E
40|$|BACKGROUND: To {{report the}} {{experience}} of health workers who had played key roles {{in the early stages}} of implementing the prevention of mother-to-child HIV transmission services (PMTCT) in Uganda. METHODS: Interviews were conducted with 15 key informants including counsellors, obstetricians and PMTCT coordinators at the five PMTCT test sites in Uganda to investigate the benefits, challenges and sustainability of the PMTCT programme. Audio-taped interviews were held with each informant between January and June 2003. These were transcribed verbatim and <b>manually</b> <b>analysed</b> using the framework approach. RESULTS: The perceived benefits reported by informants were improvement of general obstetric care, provision of antiretroviral prophylaxis for HIV-positive mothers, staff training and community awareness. The main challenges lay in the reluctance of women to be tested for HIV, incomplete follow-up of participants, non-disclosure of HIV status and difficulties with infant feeding for HIV-positive mothers. Key informants thought that the programme's sustainability depended on maintaining staff morale and numbers, on improving services and providing more resources, particularly antiretroviral therapy for the HIV-positive women and their families. CONCLUSION: Uganda's experience in piloting the PMTCT programme reflected the many challenges faced by health workers. Potentially resource-sparing strategies such as the 'opt-out' approach to HIV testing required further evaluation...|$|E
40|$|Background: Bangladesh {{is facing}} serious {{shortage}} of trained health professionals. In the pluralistic healthcare system of Bangladesh, formal {{health care providers}} constitute only 5 % of the total workforce; the rest are informal health care providers. Information Communication Technologies (ICTs) are increasingly seen as {{a powerful tool for}} linking the community with formal healthcare providers. Our study assesses an intervention that linked village doctors (a cadre of informal health care providers practising modern medicine) to formal doctors through call centres {{from the perspective of the}} village doctors who participated in the intervention. Methods: The study was conducted in Chakaria, a remote rural area in south-eastern Bangladesh during April–May 2013. Twelve village doctors were selected purposively from a pool of 55 village doctors who participated in the mobile health (mHealth) intervention. In depth interviews were conducted to collect data. The data were <b>manually</b> <b>analysed</b> using themes that emerged. Result: The village doctors talked about both business benefits (access to formal doctors, getting support for decision making, and being entitled to call trained doctors) and personal benefits (both financial and non-financial). Some of the major barriers mentioned were technical problems related to accessing the call centre, charging consultation fees, and unfamiliarity with the call centre physicians...|$|E
40|$|Abstract: The mobile {{applications}} industry experiences {{an unprecedented}} high growth, developers {{working in this}} context face a fierce competition in acquiring and retaining users. They have to quickly implement new features and fix bugs, or risks losing their users to the competition. The only {{way for them to}} achieve this goal is by closely monitoring the user feedback they receive in form of reviews. However, successful apps can receive up to several thousands of reviews per day, <b>manually</b> <b>analysing</b> each of them is a time consuming task. To help developers deal with the large amount of available information, we have developed a novel approach, called URR (User Request Referencer), that is able to organise reviews according to predefined fine grained maintenance and evolution tasks (battery, performance, memory, privacy, etc.) and recommend the related source code artifacts. We evaluated our approach through an empirical study involving the reviews and code of 39 mobile applications. Our results show a high precision and recall of URR in organising reviews according to predefined maintenance and evolution tasks and recommending the source code files that need to be modified to handle the issues raised by users in their reviews. Finally, during the evaluation we discovered that using information concerning the organization of mobile software projects improves the source code localization results. This repository provides the replication package with (i) material and working data sets of our study, (ii) complete results of the SURVEY; and (iii) rawdata for replication purposes and to support future studies. A detailed description of the contents is included in README. txt...|$|R
40|$|The {{identification}} of performance {{issues and the}} diagnosis of their root causes are time-consuming and complex tasks, especially in clustered environments. To simplify these tasks, researchers have been developing tools with built-in expertise for practitioners. However, various limitations exist in these tools that prevent their efficient usage in the performance testing of clusters (e. g. the need of <b>manually</b> <b>analysing</b> huge volumes of distributed results). In a previous work, we introduced a policy-based adaptive framework (PHOEBE) that automates the usage of diagnosis tools in the performance testing of clustered systems, {{in order to improve}} a tester's productivity, by decreasing the effort and expertise needed to effectively use such tools. This paper extends that work by broadening the set of policies available in PHOEBE, as well as by performing a comprehensive assessment of PHOEBE in terms of its benefits, costs and generality (with respect to the used diagnosis tool). The performed evaluation involved a set of experiments in assessing the different trade-offs commonly experienced by a tester when using a performance diagnosis tool, as well as the time savings that PHOEBE can bring to the performance testing and analysis processes. Our results have shown that PHOEBE can drastically reduce the effort required by a tester to do performance testing and analysis in a cluster. PHOEBE also exhibited consistent behaviour (i. e. similar time-savings and resource utilisations), when applied to a set of commonly used diagnosis tools, demonstrating its generality. Finally, PHOEBE proved to be capable of simplifying the configuration of a diagnosis tool. This was achieved by addressing the identified trade-offs without the need for manual intervention from the tester...|$|R
40|$|Sentiment Analysis {{is an area}} of Computer Science {{that deals}} with the impact a {{document}} makes on a user. The very field is further sub-divided into Opinion Mining and Emotion Analysis, the latter of which is the basis for the present work. Work on songs is aimed at building affective interactive applications such as music recommendation engines. Using song lyrics, we are interested in both supervised and unsupervised analyses, each of which has its own pros and cons. For an unsupervised analysis (clustering), we use a standard probabilistic topic model called Latent Dirichlet Allocation (LDA). It mines topics from songs, which are nothing but probability distributions over the vocabulary of words. Some of the topics seem sentiment-based, motivating us to continue with this approach. We evaluate our clusters using a gold dataset collected from an apt website and get positive results. This approach would be useful {{in the absence of a}} supervisor dataset. In another part of our work, we argue the inescapable existence of supervision in terms of having to <b>manually</b> <b>analyse</b> the topics returned. Further, we have also used explicit supervision in terms of a training dataset for a classifier to learn sentiment specific classes. This analysis helps reduce dimensionality and improve classification accuracy. We get excellent dimensionality reduction using Support Vector Machines (SVM) for feature selection. For re-classification, we use the Naive Bayes Classifier (NBC) and SVM, both of which perform well. We also use Non-negative Matrix Factorization (NMF) for classification, but observe that the results coincide with those of NBC, with no exceptions. This drives us towards establishing a theoretical equivalence between the two...|$|R
