28|89|Public
50|$|ASSIMPLER Blueprinting - The Business Blueprinting of the {{organization}} is designed based on the ASSIMPLER framework. ASSIMPLER stands for Availability, Scalability, Security, Interoperability, <b>Maintainability,</b> <b>Performance,</b> Low cost of ownership, Extendability and Reliability - applied to business services and processes. The framework helps model the business expectations and challenges to be addressed through the Digital Strategy.|$|E
5000|$|EUC {{applications}} {{should not}} be evolved by accident, but {{there should be a}} defined EUC strategy. Any Application Architecture Strategy / IT Strategy should consider the white spaces in automation (enterprise functionality not automated by [...] ERP / Enterprise Grade Applications). These are the potential areas where EUC can play a major role. Then ASSIMPLER parameters should be applied to these white spaces to develop the EUC strategy. (ASSIMPLER stands for availability, scalability, security, interoperability, <b>maintainability,</b> <b>performance,</b> low cost of ownership, extendibility and reliability.) ...|$|E
40|$|With the {{increasing}} demand for energy over recent decades, the Arctic region has become an interesting area for future exploration and development for {{the oil and gas}} industry. The Arctic region is known to have a harsh climate and a sensitive environment in a remote location. The severe and complex operational conditions in the Arctic can significantly affect the lifetime of a system, the repair processes and the support activities. Hence, {{it is important to consider}} the effect of the operational conditions on the performance of the production facility/systems/equipment and machines, and the related reliability and maintainability characteristics. The aim of this thesis is to study, analyze and suggest a methodology for production performance analysis considering operational conditions. Furthermore, the study focuses on developing and modifying the available statistical approach for prediction of <b>maintainability</b> <b>performance</b> and spare part provision considering the effect of time-dependent and time-independent covariates (influence factors). In this research study, firstly a brief survey of technological and operational challenges in the Arctic region from a maintainability and reliability performance point of view is presented. Then, available statistical approaches for reliability and <b>maintainability</b> <b>performance</b> analysis considering the effect of covariates are reviewed. Thereafter, a methodology is developed and proposed for production performance analysis considering time-dependent and time-independent covariates. The methodology is based on the concept of the proportional hazard model (PHM) and the proportional repair model (PRM), as well as their extensions. A case study from the mining industry is presented to demonstrate how the proposed methodology can be applied. In the second part of this research study, the application of the extension of PHM is developed and discussed in order to predict the <b>maintainability</b> <b>performance</b> considering time-dependent covariates. Furthermore, the existing methods for calculating the number of spare parts on the basis of the reliability characteristics, without the consideration of time-dependent iv ABBAS BARABADI covariates, is modified and improved to enhance their application in the presence of time-dependent covariates. The applications of these methods are demonstrated and discussed using a case study. The result of the study shows that the operational conditions may have a significant effect on the reliability and <b>maintainability</b> <b>performance</b> of a component. This also consequently affects the number of the required spare parts for a given operational condition. The result also shows that considering time-dependent covariates as time-independent covariates may lead to wrong results in the prediction of reliability and <b>maintainability</b> <b>performance</b> as well as the required spare parts. Therefore, before any analysis, the timedependency of covariates must be checked. Thereafter, based on the result of the analysis, the appropriate statistical approach must be selected...|$|E
40|$|Abstract. The major {{contributions}} {{of our work}} include adopting the NFR framework to represent and analyze two software qualities that often conflict with each other: <b>maintainability</b> and <b>performance.</b> We identified and described many heuristics that can be implemented in a system's source code to achieve either quality. We implemented some of the heuristics in two medium-sized software systems and then collected measurements {{to determine the effect}} of the heuristics on <b>maintainability</b> and <b>performance.</b> A general methodology is described for evaluating and selecting the heuristics that will improve a system’s software quality the most. The results of our research were also encoded in XML files, and made available on the World Wide Web for use by software developers. The WWW address is...|$|R
50|$|Not all {{software}} defects {{are caused}} by coding errors. One common source of expensive defects is requirement gaps, e.g., unrecognized requirements which result in errors of omission by the program designer. Requirement gaps can often be non-functional requirements such as testability, scalability, <b>maintainability,</b> usability, <b>performance,</b> and security.|$|R
40|$|Efforts for {{software}} evolution supersede {{any other part}} of the software life cycle. Technological decisions {{have a major impact on}} the maintainability, but are not well reflected by existing code or architecture based metrics. The way the persistency of object structures with relational databases is solved affects the maintainability of the overall system. Besides maintainability other quality attributes of the software are of interest, in particular performance metrics. However, a systematic evaluation of the benefits and drawback of different persistency frameworks is lacking. In this paper we systematically evaluate the <b>maintainability</b> and <b>performance</b> of different technological approaches for this mapping. The paper presents a testbed and an evaluation process with specifically designed metrics to evaluate persistency techniques regarding their <b>maintainability</b> and <b>performance.</b> In the second part we present and discuss the results of the case study. Categories and Subject Descriptor...|$|R
40|$|PhD thesis in Offshore technologyThis {{thesis is}} based on the {{following}} papers, not yet available in UiS Brage due to copyright:PAPER 1 : Barabadi, A. and Markeset, T. (2011). Reliability and <b>maintainability</b> <b>performance</b> under Arctic conditions, International Journal of Systems Assurance Engineering and Management, DOI 10. 1007 /s 13198 - 011 - 0071 - 8. PAPER 2 : Barabadi, A., Barabady, J. and Markeset, T. (2011). A methodology for throughput capacity analysis of a production facility considering environment condition, Reliability Engineering and System Safety, Vol. 96, No. 12, pp. 1637 - 1646. [URL] 3 : Barabadi, A., Barabady, J. and Markeset, T. (2011). Maintainability analysis considering time-dependent and timeindependent covariates, Reliability Engineering and System Safety, Vol. 96, No. 1, pp. 210 - 217. [URL] 4 : Kayrbekova, D., Barabadi, A. and Markeset, T. (2011). Maintenance cost evaluation of a system to be used in Arctic conditions: A case study, Journal of Quality in Maintenance Engineering, Vol. 17, No. 4, pp. 320 - 336. [URL] 5 : Barabadi, A. (2012). Reliability and spare part provision considering operational environment: A case study, To appear in International Journal of Performability Engineering, Vol. 8, No. 4, pp. 417 - 426. With the increasing demand for energy over recent decades, the Arctic region has become an interesting area for future exploration and development for the oil and gas industry. The Arctic region is known to have a harsh climate and a sensitive environment in a remote location. The severe and complex operational conditions in the Arctic can significantly affect the lifetime of a system, the repair processes and the support activities. Hence, {{it is important to consider}} the effect of the operational conditions on the performance of the production facility/systems/equipment and machines, and the related reliability and maintainability characteristics. The aim of this thesis is to study, analyze and suggest a methodology for production performance analysis considering operational conditions. Furthermore, the study focuses on developing and modifying the available statistical approach for prediction of <b>maintainability</b> <b>performance</b> and spare part provision considering the effect of time-dependent and time-independent covariates (influence factors). In this research study, firstly a brief survey of technological and operational challenges in the Arctic region from a maintainability and reliability performance point of view is presented. Then, available statistical approaches for reliability and <b>maintainability</b> <b>performance</b> analysis considering the effect of covariates are reviewed. Thereafter, a methodology is developed and proposed for production performance analysis considering time-dependent and time-independent covariates. The methodology {{is based on}} the concept of the proportional hazard model (PHM) and the proportional repair model (PRM), as well as their extensions. A case study from the mining industry is presented to demonstrate how the proposed methodology can be applied. In the second part of this research study, the application of the extension of PHM is developed and discussed in order to predict the <b>maintainability</b> <b>performance</b> considering time-dependent covariates. Furthermore, the existing methods for calculating the number of spare parts on the basis of the reliability characteristics, without the consideration of time-dependent iv ABBAS BARABADI covariates, is modified and improved to enhance their application in the presence of time-dependent covariates. The applications of these methods are demonstrated and discussed using a case study. The result of the study shows that the operational conditions may have a significant effect on the reliability and <b>maintainability</b> <b>performance</b> of a component. This also consequently affects the number of the required spare parts for a given operational condition. The result also shows that considering time-dependent covariates as time-independent covariates may lead to wrong results in the prediction of reliability and <b>maintainability</b> <b>performance</b> as well as the required spare parts. Therefore, before any analysis, the timedependency of covariates must be checked. Thereafter, based on the result of the analysis, the appropriate statistical approach must be selected...|$|E
40|$|This study {{introduces}} a systems-engineering and evaluation methodology {{that focuses on}} the stability of an entire computing infrastructure. More specifically, the conducted research elaborates on the cohesive systems assurance (CSA) methodology, which encapsulates the concepts and methods of product assurance (reliability, availability, and <b>maintainability),</b> <b>performance</b> & scalability, and dependability (security and safety), respectively. The argument made in this study is that systems stability represents the quality of service provided by an entire computing infrastructure, and therefore quantifies the usefulness, trustworthiness, and effectiveness of the environment...|$|E
40|$|The {{authors have}} {{developed}} an object-based linear algebra package, currently with emphasis on sparse Krylov methods, driven primarily by needs of the Los Alamos National Laboratory parallel unstructured-mesh casting simulation tool Telluride. Support {{for a number of}} sparse storage formats, methods, and preconditioners have been implemented, driven primarily by application needs. They describe the object-based Fortran 90 approach, which enhances <b>maintainability,</b> <b>performance,</b> and extensibility, the parallelization approach using a new portable gather/scatter library (PGSLib), current capabilities and future plans, and present preliminary performance results on a variety of platforms...|$|E
40|$|PICASSO is a {{graphical}} {{user interface}} development en-vironment built using the Common Lisp Object System (CLOS). This paper describes how CLOS features in-cluding multiple inheritance, instance methods, multi-methods, and method combinations were used to imple-ment the system. In addition, the benefits and draw-backs of CLOS development are discussed including code quality, <b>maintainability</b> and <b>performance.</b> 1...|$|R
40|$|This article {{presents}} XML-based tools for parser generation and data binding generation. The underlying concept {{is that of}} transformation between formal languages, which {{is a form of}} meta-programming. We discuss the bene ts of such a declarative approach with well-de ned semantics: productivity, <b>maintainability,</b> veri ability, <b>performance</b> and safety...|$|R
40|$|The AFTI F- 111 Mission Adaptive Wing (MAW) {{development}} is described {{together with the}} flight test demonstration program. The program developed a smooth variable camber wing and flight control system capable of adjusting the wing's airfoil shape in flight in response to pilot input and flight condition, {{in order to maximize}} the aerodynamic efficiency in all areas of flight envelope. This paper examines the instrumentation requirements and the systems implemented and presents the results of flight tests. These results demonstrate the practicality, airworthiness, <b>maintainability,</b> and <b>performance</b> benefits of the smooth variable camber wing system...|$|R
40|$|Purpose of the ATS {{program is}} to develop a new {{baseline}} for industrial gas turbine systems for the 21 st century. A recuperated gas turbine cycle was selected; the eventual engine that result will utilize Solar`s Primary Surface Recuperator (PSR) technology. Besides higher thermal efficiency, other goals included lower emission, cost of power, and improved RAMD (reliability, availability, <b>maintainability).</b> <b>Performance</b> data have been obtained for the candidate heat transfer surface, and on a scaled rig. Pretest predictions of air-side and gas-side pressure drop were in very good agreement with tests results; predicted effectiveness also agreed well with experiment. A flattened tube test to determine changes of the PSR heat transfer surface profile after exposure is underway...|$|E
40|$|Software {{engineering}} {{community has}} proposed several methods to evaluate software architectures {{with respect to}} desired quality attributes such as <b>maintainability,</b> <b>performance,</b> and so on. There is, however, little effort on systematically comparing such methods to discover {{similarities and differences between}} existing approaches. In this paper, we compare four well known scenario-based SA evaluation methods using an evaluation framework. The framework considers each method {{from the point of view}} of method context, stakeholders, structure, and reliability. The comparison reveals that most of the studied methods are structurally similar but there are a number of differences among their activities and techniques. Therefore, some methods overlap, which guides us to identify five common activities that can form a generic process model. 1...|$|E
40|$|In this paper, {{we discuss}} {{what we believe}} is the grand {{challenge}} facing the software quality research community: the ability to accurately define, in the very earliest stages of development, the techniques that will be need to achieve the needed levels of the various non-functional attributes: reliability, availability, fault tolerance, testability, <b>maintainability,</b> <b>performance,</b> software safety, software security. We will further explore the associated technical and economic tradeoffs that must be made {{in order to achieve}} quality and to also certify software quality. And we will also take into account the fact that satisfying a particular level of each attribute requires specific cost expenditures, and some of these attributes conflict with each other. 1...|$|E
40|$|Refactoring {{focuses on}} {{improving}} the reusability, <b>maintainability</b> and <b>performance</b> of programs. However, {{the impact of}} refactoring on {{the security of a}} given program has received little attention. In this work, we focus on the design of object-oriented applications and use metrics {{to assess the impact of}} a number of standard refactoring rules on their security by evaluating the metrics before and after refactoring. This assessment tells us which refactoring steps can increase the security level of a given program from the point of view of potential information flow, allowing application designers to improve their system’s security at an early stage...|$|R
40|$|Ultrastable fiber-optic {{communications}} system transmits microwave signals between antenna sites of Deep Space Network (DSN) and central processing station several kilometers away. Permits relocation of critical components from front-end areas of DSN antennas to central location, permitting radio-frequency (RF) antenna arraying, improving DSN flexibility, <b>maintainability,</b> and system <b>performance.</b> Also useful in commercial analog and digital communications...|$|R
40|$|In {{this paper}} we {{describe}} WebTest, an Open Source tool for automated testing of web applications. In particular {{we will show}} how to quickly create tests that shine with excellent <b>maintainability</b> and runtime <b>performance</b> as well as perfect integration in the application development cycle. Categories and Subject Descriptors D. 2. 5 [Software engineering]: Testing and Debugging – Testing tool...|$|R
40|$|Abstract – The idea of {{ensuring}} that non-functional requirements such as <b>maintainability,</b> <b>performance,</b> reusability, reliability, and others are designed and built in to new software {{is not a new}} one. However, software security as a particular non-functional requirement of software systems is all too often addressed late in the software development process. As a result, the security of such software systems is poor and can lead to security compromises (such as theft of service or information), increased costs in maintaining the software, and many indirect costs such as system downtime, loss of productivity, etc. This paper will survey existing research into architecting and engineering security and will present this material along with a case study of Daniel J. Bernstein’s qmail Internet mail server software. Index terms – Software Architecture, Security I...|$|E
40|$|FLASH is a {{publicly}} available high performance application code which {{has evolved into}} a modular, extensible software system from a collection of unconnected legacy codes. FLASH has been successful because its capabilities have been driven by the needs of scientific applications, without compromising <b>maintainability,</b> <b>performance,</b> and usability. In its newest incarnation, FLASH 3 consists of inter-operable modules that can be combined to generate different applications. The FLASH architecture allows arbitrarily many alternative implementations of its components to co-exist and interchange with each other, resulting in greater flexibility. Further, a simple and elegant mechanism exists for customization of code functionality without the need to modify the core implementation of the source. A built-in unit test framework providing verifiability, combined with a rigorous software maintenance process, allow the code to operate simultaneously in th...|$|E
40|$|Component-based {{software}} engineering is currently an emerging technology {{used to develop}} complex embedded systems. These embedded systems need to fulfil requirements regarding quality attributes such as safety, reliability, availability, <b>maintainability,</b> <b>performance,</b> security and temporal correctness. Since quality problems should be identified and tackled early in the development process, there is a rising need to predict and evaluate these properties in the architecture design phase. This paper describes a generic framework for predicting quality properties based on component-based architectures, which is derived from a comprehensive study of recent architecture evaluation methods. This generic framework defines common aspects between the different evaluation methods and enables the improvement of evaluation methods for specific quality properties, by transferring knowledge from one quality domain to the other. Thus, this paper can help to create better evaluation methods in the future. (c) 2006 Elsevier Inc. All rights reserved...|$|E
40|$|This paper {{presents}} {{the principles of}} a development methodology for software design. The methodology {{is based on a}} nonlinear, product-driven approach that integrates quality aspects. The principles are made more concrete in two examples: one for developing educational simulations and one for developing expert systems. It is shown that the flexibility needed for building high quality systems leads to integrated development environments in which methodology, product and tools are closely attuned to each other. This "development process reengineering" can lead to significant improvements in the quality of the product in terms of both <b>maintainability</b> and <b>performance</b> enhancement of the people involved in the development process...|$|R
40|$|Abstract. Data in {{object-oriented}} programming is organized in {{a hierarchy of}} classes. The problem of object-oriented pattern matching is how to explore this hierarchy from the outside. This usually involves classifying objects by their run-time type, accessing their members, or determining some other characteristic {{of a group of}} objects. In this paper we compare six different pattern matching techniques: object-oriented decomposition, visitors, type-tests/type-casts, typecase, case classes, and extractors. The techniques are compared on nine criteria related to conciseness, <b>maintainability</b> and <b>performance.</b> The paper introduces case classes and extractors as two new pattern-matching methods and shows that their combination works well for all of the established criteria. ...|$|R
40|$|Applications that execute on {{parallel}} clusters face scalability concerns due to {{the high}} communication overhead that is usually associated with such environments. Modern network technologies that support Remote Direct Memory Access (RDMA) can offer true zero copy communication and reduce communication overhead by overlapping it with computation. For this approach to be effective the parallel application using the cluster must be structured {{in a way that}} enables communication computation overlapping. Unfortunately, the trade-off between <b>maintainability</b> and <b>performance</b> often leads to a structure that prevents exploiting the potential for communication computation overlapping. This paper describes a sourceto-source optimizing transformation that can be performed by an automatic (or semi-automatic) system in order to restructure MPI codes towards maximizing communication-computation overlapping. ...|$|R
40|$|We have {{developed}} an object-based linear algebra package, currently with emphasis on sparse Krylov methods, driven primarily by needs of the Los Alamos National Laboratory parallel unstructured-mesh casting simulation tool Telluride. Support {{for a number of}} sparse storage formats, methods, and preconditioners have been implemented, driven primarily by application needs. We describe our object-based Fortran 90 approach, which enhances <b>maintainability,</b> <b>performance,</b> and extensibility, our parallelization approach using a new portable gather/scatter library (PGSLib), current capabilities and future plans, and present preliminary performance results on a variety of platforms. 1 Introduction An effort was initiated recently at Los Alamos National Laboratory (LANL) to build a new 3 -D high-resolution tool for simulating casting processes, i. e. the flow of molten material into molds and the subsequent cooling and solidification of the material. The simulation process includes incompressible f [...] ...|$|E
40|$|This thesis reviews if OpenCL is a {{suitable}} and cost effective platform for algorithm development {{in health care}} systems. Aspects such as <b>maintainability,</b> <b>performance,</b> portability and integration with high-level languages (in this case Python) are analyzed. The review is done by implementing {{one part of a}} dose calculation algorithm that is complex enough to provide a realistic case. The vision is that OpenCL can replace multiple platforms for both multi core CPU and GPU computing and removing the need of implementing an optimized version of an algorithm for every platform. To achieve performance-portability, automatic optimization is done using parameter tuning. Both its effects on performance and code structure  are analyzed. The conclusion is that OpenCL coupled with auto tuning is not {{a suitable}} platform due to problems with code structure, language limitations, programming-portability, tool  support and the effort and difficulty in implementing auto tuning...|$|E
40|$|The {{feasibility}} of the propfan {{relative to the}} turbofan is summarized, using the Douglas DC- 9 Super 80 (DS- 8000) as the actual operational base aircraft. The 155 passenger economy class aircraft (31, 775 lb 14, 413 kg payload), cruise Mach at 0. 80 at 31, 000 ft (8, 450 m) initial altitude, and an operational capability in 1985 was considered. Three propfan arrangements, wing mounted, conventional horizontal tail aft mounted, and aft fuselage pylon mounted are selected for comparison with the DC- 9 Super 80 P&WA JT 8 D- 209 turbofan powered aircraft. The configuration feasibility, aerodynamics, propulsion, structural loads, structural dynamics, sonic fatigue, acoustics, weight <b>maintainability,</b> <b>performance,</b> rough order of magnitude economics, and airline coordination are examined. The effects of alternate cruise Mach number, mission stage lengths, and propfan design characteristics are considered. Recommendations for further study, ground testing, and flight testing are included...|$|E
40|$|Abstract {{data types}} and {{object-oriented}} design are active research areas {{in computer science}} and software engineering. Much of the interest is aimed at new software development. Abstract data type packages developed for a discontinued software project were used to improve a real-time data-acquisition system under maintenance. The result saved effort and contributed to a significant improvement in the <b>performance,</b> <b>maintainability,</b> and reliability of the Goldstone Solar System Radar Data Acquisition System...|$|R
40|$|In this paper, {{we present}} TALC [...] a small {{language}} extension for C and C++ suitable for applications that traverse common data {{structures such as}} large meshes or cubes. We make three contributions in this paper. First, we motivate {{the need for a}} new C/C++ extension focused on addressing emerging problem areas in <b>performance</b> and code <b>maintainability.</b> Second, we define the language extension and illustrate how it is employed in C. Third, we show the utility of such an extension by providing comparison code snippets that demonstrate advantages in both software <b>maintainability</b> and <b>performance.</b> Performance benefits of the extension are provided for several experiments resulting in up to 200 % speedups over more conventional methods to achieve the same algorithm...|$|R
40|$|The {{transfer}} of program data between the nodes of a distributed {{system is a}} fundamental operation. It usually requires some form of data serialization. For a functional language such as Scheme it is clearly desirable to also allow the unrestricted {{transfer of}} functions between nodes. With the goal of developing a portable implementation of the Termite system we have designed the Mobit Scheme interpreter which supports unrestricted serialization of Scheme objects, including procedures and continuations. Mobit is derived from an existing Scheme in Scheme fast interpreter. We demonstrate how macros were valuable in transforming the interpreter while preserving its structure and <b>maintainability.</b> Our <b>performance</b> evaluation shows that the run time speed of Mobit is comparable to existing Scheme interpreters. 1...|$|R
40|$|JavaScript {{has become}} one of the most popular program-ming languages, yet it is known for its {{suboptimal}} design. To effectively use JavaScript despite its design flaws, developers try to follow informal code quality rules that help avoid cor-rectness, <b>maintainability,</b> <b>performance,</b> and security prob-lems. Lightweight static analyses, implemented in “lint-like” tools, are widely used to find violations of these rules, but are of limited use because of the language’s dynamic nature. This paper presents DLint, a dynamic analysis approach to check code quality rules in JavaScript. DLint consists of a generic framework and an extensible set of checkers that each addresses a particular rule. We formally describe and implement 28 checkers that address problems missed by state-of-the-art static approaches. Applying the approach in a comprehensive empirical study on over 200 popular web sites shows that static and dynamic checking complement each other. On average per web site, DLint detects 49 problems that are missed statically, including visible bugs on the web sites of IKEA, Hilton, eBay, and CNBC...|$|E
40|$|This thesis {{describes}} {{the elements of}} a formal Reliability and Maintainability Program which should be implemented by the electric power industry in the design of future generating stations, in order to improve their reliability and <b>maintainability</b> <b>performance.</b> The severe financial penalties of generating unit unreliability are discussed to emphasize the value of the proposed Program. Elements such as design specifications, target allocation, reliability and maintainability prediction and analysis, and design review are described, and a detailed example of reliability and maintainability target allocation to a 500 MW unit is included. Also presented are several prerequisites for an effective Reliability and Maintainability Program, such as the definition of supplier responsibility, criteria for supplier and equipment selection, data collection, and training of personnel. A reliability modelling and prediction technique known as derated state modelling is investigated. This technique is applied to the design of a typical boiler feed system, to show how a suitable design may be evolved with the use of reliability and maintainability...|$|E
40|$|In {{the mining}} industry, {{equipment}} are continuously increasing {{in size and}} complexity. At the same time, the demand for available plants and continuous production has never been higher. The performance of equipment depends on the reliability and <b>maintainability</b> <b>performance</b> of the equipment along with the maintenance supportability, operational conditions, and environmental conditions. In order to improve plant availability, fully utilize equipment performance, avoid equipment breakdowns and optimize operation and maintenance (O&M), the concept of reliability, availability and maintainability (RAM) analysis is required. In most industries, the only collected explanatory variables used in RAM analysis have been time to failure (TTF) and time to repair (TTR). For a more precise estimation of the reliability and maintainability characteristics of mining equipment, factors influencing the reliability and maintainability of equipment should be collected and included in the analysis. In this thesis, the concept of RAM analysis is applied for availability improvement in the mining industry as a quantitative case study. Furthermore, a framework for data collection including influence factors has been developed, which highlights important steps in the data collection process. For including the effects of influence factors in RAM analysis, the Proportional Hazard Model (PHM) with the modified Proportional Repair Model (PRM) are discussed. Finally, a qualitative case study is conducted to demonstrate {{the application of the}} framework for data collection for RAM analysis. The result of the RAM analysis have been used to determine optimum preventive maintenance interval in order to improve availability performance. Furthermore, aspects for improvement of reliability performance and <b>maintainability</b> <b>performance</b> have been assessed in order to improve overall system availability. The framework developed for data collection is considered general enough to cover several industries. However, the framework is especially suited for the mining industry {{with the use of the}} PHM and PRM for including influence factors in reliability and maintainability analysis. The work in this thesis, the framework for data collection especially, is considered valuable and necessary as it addresses an area that has received less focus in today's mining industry. Keywords: RAM, mining, O&M optimization, data collection, influence factors, Proportional hazard model, Proportional repair mode...|$|E
40|$|A {{model was}} {{developed}} to assess the maintainability of facade using neural network techniques. Inputs were derived from comprehensive studies of 570 tall buildings (more than 12 stories) through detailed field evaluation and interviews with professionals in the whole building delivery process. Sensitivity analysis showed that the most significant factors associated with facade maintainability include the system selection, detailing, accessibility and material <b>performance.</b> <b>Maintainability,</b> facade, risk, building defect, neural network, sensitivity analysis,...|$|R
5000|$|The {{most popular}} format for {{recording}} business requirements is the Business Requirements Document (BRD). The intent behind the BRD is {{to define what}} results would be wanted from a system, however it might eventually be designed. Hence, BRD documents are complemented with a systems reference document (SRD) that details the technology performance and infrastructure expectations, including any technology requirements pertaining to quality of service, such as <b>performance,</b> <b>maintainability,</b> adaptability, reliability, availability, security, and scalability.|$|R
40|$|International audienceSoftware {{architectures}} {{are often}} designed {{with respect to}} some architecture patterns, like the pipeline and peer-to-peer. These patterns are the guarantee of some quality attributes, like <b>maintainability</b> or <b>performance.</b> These patterns should be dynamically enforced in the running system to beneﬁt from their associated quality characteristics at runtime. In dynamic hosting platforms where machines can enter the network, oﬀering new resources, or fail, making the components they host unavailable, these patterns can be aﬀected. In addition, {{in this kind of}} infrastructures, some resource requirements can also be altered. In this paper we present an approach which aims at dynamically assist deployment process with information about architectural patterns and resource constraints. This ensures that, faced with disconnections or machine failures, the runtime system complies permanently with the original architectural pattern and the initial resource requirements...|$|R
