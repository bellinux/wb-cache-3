0|10000|Public
40|$|Abstract. The paper {{presents}} {{a method for}} automatic detection <b>of</b> “non-trivial” <b>word</b> <b>combinations</b> in the text. It is based on automatic syntactic analysis. The method shows better precision and recall than the baseline method (bigrams). It was tested on a text in Spanish. The method {{can be used for}} enrichment of very large dictionaries <b>of</b> <b>word</b> <b>combinations.</b> ...|$|R
40|$|In {{this paper}} I present a study which {{contributes}} to the description of lexicographic traditions in Sweden and Denmark. More exactly, the study concerns the treatment <b>of</b> <b>word</b> <b>combinations,</b> which are followed by some kind of explanation, in two 19 th century monolingual dictionaries. The works are the Swedish dictionary Ordbok öfver svenska språket (1850 – 55) by A. F. Dalin and the Danish dictionary Dansk Ordbog (…) (1833) by C. Molbech. The focus is, among other things, on the frequencies and types <b>of</b> <b>word</b> <b>combinations</b> that are represented in the data. Furthermore, I consider in which entries the <b>word</b> <b>combinations</b> are found, and {{the position of the}} expressions in the microstrucure of the articles. Moreover, I discuss the form and meaning <b>of</b> the <b>word</b> <b>combinations</b> and, finally, the authors’ use of (signed) examples...|$|R
40|$|Fuzzy {{approach}} {{deals with}} the linguistic properties of elements such as beauty, coldness, hotness etc. Collocations are linguistically motivated. Decision <b>of</b> <b>word</b> <b>combination</b> for being collocation is a linguistic term as merely cooccurrence <b>of</b> <b>word</b> <b>combinations</b> does not signify the presence of collocation. Thus collocation extraction can be made possible by looking its linguistic aspect. In the present paper, an attempt {{has been made to}} make two different fuzzy sets <b>of</b> <b>word</b> <b>combinations</b> to be considered for collocations. Mutual information and t-test have been taken as basis for the construction of fuzzy sets. Two fuzzy set theoretical models have been proposed to identify collocations. It has been shown that fuzzy set theoretical approach works very well for collocation extraction. The working data has been based on a corpus <b>of</b> about one <b>million</b> <b>words</b> contained in different novels constituting project Gutenberg available on www. gutenberg. org...|$|R
40|$|Review {{of a new}} {{dictionary}} <b>of</b> Italian <b>word</b> <b>combinations.</b> A {{full description}} of the dictionary is given from the points of view of headwords, the structure of entries, the types <b>of</b> <b>word</b> <b>combination</b> included, and indications of meaning and usage. The dictionary is then evaulated in terms of (a) {{the usefulness of the}} phrases included, (b) help given in using specific phrases, (c) presentation and accessibility, and (d) accuracy...|$|R
40|$|This paper {{reports on}} work {{carried out in}} the {{framework}} of an ongoing project aimed at building an online, corpus - based lexicographic resource for Italian <b>Word</b> <b>Combinations.</b> Our aim is to compare two of the most commonly used methods for the automatic extraction <b>of</b> <b>word</b> <b>combinations</b> from corpora, with a view to evaluate their performance – and ultimately their efficacy – with respect to the task <b>of</b> acquiring <b>word</b> <b>combinations</b> for inclusion in the lexi cographi c combinatory resource...|$|R
5000|$|Ezhuththathigaaram - Formation <b>of</b> <b>words</b> and <b>combination</b> <b>of</b> <b>words</b> ...|$|R
5000|$|The duo {{borrowed}} the title [...] "Electric Arguments" [...] from the poem [...] "Kansas City to St. Louis" [...] by Allen Ginsberg. In Wired magazine, McCartney stated {{this was because}} [...] "he's {{been looking at the}} beauty <b>of</b> <b>word</b> <b>combinations</b> rather than their meaning." ...|$|R
5000|$|Wasei-eigo ("Japanese-made English", [...] "English words coined in Japan") are Japanese {{language}} expressions {{based on}} English <b>words</b> or parts <b>of</b> <b>words</b> <b>combinations,</b> {{that do not}} exist in standard English language or whose meaning differs from the words they were derived from.Linguistics classifies them as pseudo-loanwords or pseudo-anglicisms.|$|R
40|$|We {{proposed}} a novel kernel for text categorization. This kernel is an inner {{product in the}} feature space generated by all <b>word</b> <b>combinations</b> <b>of</b> specified length. A <b>word</b> <b>combination</b> is a collection <b>of</b> different <b>words</b> co-occurring in the same sentence. The <b>word</b> <b>combination</b> <b>of</b> length k is weighted by the k-th root of {{the product of the}} inverse document frequencies (IDF) <b>of</b> its <b>words.</b> A computationally simple and efficient algorithm was proposed to calculate this kernel. By restricting the <b>words</b> <b>of</b> a <b>word</b> <b>combination</b> to the same sentence and considering multi-word <b>combinations,</b> the <b>word</b> <b>combination</b> features can capture similarity at a more specific level than single words. By discarding word order, the <b>word</b> <b>combination</b> features are more compatible with the flexibility of natural language and the dimensionality this kernel can be reduced significantly compared to the word-sequence kernel. We conducted a series of experiments on the Reuters- 21578 dataset and 20 Newsgroups dataset. This kernel consistently achieves better performance than the classical word kernel and word-sequence kernel on the two datasets. We also assessed the impact <b>of</b> <b>word</b> <b>combination</b> length on performance and compared the computing efficiency of this kernel to those <b>of</b> the <b>word</b> kernel and word-sequence kernel. We {{proposed a}} novel kernel for text categorization. This kernel is an inner product in the feature space generated by all <b>word</b> <b>combinations</b> <b>of</b> specified length. A <b>word</b> <b>combination</b> is a collection <b>of</b> different <b>words</b> co-occurring in the same sentence. The <b>word</b> <b>combination</b> <b>of</b> length k is weighted by the k-th root of the product of the inverse document frequencies (IDF) <b>of</b> its <b>words.</b> A computationally simple and efficient algorithm was proposed to calculate this kernel. By restricting the <b>words</b> <b>of</b> a <b>word</b> <b>combination</b> to the same sentence and considering multi-word <b>combinations,</b> the <b>word</b> <b>combination</b> features can capture similarity at a more specific level than single words. By discarding word order, the <b>word</b> <b>combination</b> features are more compatible with the flexibility of natural language and the dimensionality this kernel can be reduced significantly compared to the word-sequence kernel. We conducted a series of experiments on the Reuters- 21578 dataset and 20 Newsgroups dataset. This kernel consistently achieves better performance than the classical word kernel and word-sequence kernel on the two datasets. We also assessed the impact <b>of</b> <b>word</b> <b>combination</b> length on performance and compared the computing efficiency of this kernel to those <b>of</b> the <b>word</b> kernel and word-sequence kernel...|$|R
40|$|In {{this paper}} we {{proposed}} a novel kernel for text categorization. This kernel is an inner product in the feature space generated by all <b>word</b> <b>combinations</b> <b>of</b> specified length. A <b>word</b> <b>combination</b> is a collection <b>of</b> different <b>words</b> co-occurring in the same sentence. The <b>word</b> <b>combination</b> <b>of</b> length k is weighted by the k-th root of {{the product of the}} inverse document frequencies (IDF) <b>of</b> its <b>words.</b> A computationally simple and efficient algorithm was proposed to calculate this kernel. We conducted experiments on the 20 Newsgroups dataset. This kernel achieves better performance than the classical word kernel and word-sequence kernel. We also assessed the impact <b>of</b> <b>word</b> <b>combination</b> length on performance. © 2012 IEEE. IEEE Beijing Section; Hunan University of Humanities, Science and Technology; Tongji University; Xiamen University; Central South UniversityIn this paper {{we proposed a}} novel kernel for text categorization. This kernel is an inner product in the feature space generated by all <b>word</b> <b>combinations</b> <b>of</b> specified length. A <b>word</b> <b>combination</b> is a collection <b>of</b> different <b>words</b> co-occurring in the same sentence. The <b>word</b> <b>combination</b> <b>of</b> length k is weighted by the k-th root of the product of the inverse document frequencies (IDF) <b>of</b> its <b>words.</b> A computationally simple and efficient algorithm was proposed to calculate this kernel. We conducted experiments on the 20 Newsgroups dataset. This kernel achieves better performance than the classical word kernel and word-sequence kernel. We also assessed the impact <b>of</b> <b>word</b> <b>combination</b> length on performance. © 2012 IEEE...|$|R
40|$|We {{report on}} three {{experiments}} aimed at comparing two popular methods for the automatic extraction <b>of</b> <b>Word</b> <b>Combinations</b> from corpora, {{with a view}} to evaluate: i) their efficacy in acquiring data to be included in a combinatory resource for Italian; ii) the impact of different types of benchmarks on the evaluation itself...|$|R
5000|$|Metody semanticheskogo issledovaniya ogranichennogo podyazyka /Methods of Semantic Investigation of a Restricted Sublanguage/ (414 pp.), Moscow University Press, 1971 (with B. Gorodetsky)* Slovari slovosochetaniy i chastotnye slovari slov ogranichennogo podyazyka /Dictionaries <b>of</b> <b>Word</b> <b>Combinations</b> and Dictionaries <b>of</b> <b>Words</b> with Frequencies <b>of</b> a Restricted Sublanguage/ (538 pp.), Moscow University Press, 1972 (with B. Y. Gorodetsky, A. E. Kibrik, L. S. Logakhina, G. V., Maksimova, and E. S. Prytkov) ...|$|R
40|$|The present {{dissertation}} aims at simulating {{the construction}} of lexicographic layouts for an Italian combinatory dictionary based on real linguistic data, extracted from corpora by using computational methods. This work {{is based on the}} assumption that the intuition of the native speaker, or the lexicographer, who manually extracts and classifies all the relevant data, are not adequate to provide sufficient information on the meaning and use <b>of</b> <b>words.</b> Therefore, a study of the real use of language is required and this is particularly true for dictionaries that collect the combinatory behaviour <b>of</b> <b>words,</b> where the task of the lexicographer is to identify typical <b>combinations</b> where a <b>word</b> occurs. This study is conducted in the framework of the CombiNet project aimed at studying Italian Word Combinationsand and at building an online, corpus-based combinatory lexicographic resource for the Italian language. This work is divided into three chapters. Chapter 1 describes the criteria considered for the classification <b>of</b> <b>word</b> <b>combinations</b> according to the work of Ježek (2011). Chapter 1 also contains a brief comparison between the most important Italian combinatory dictionaries and the BBI Dictionary <b>of</b> <b>Word</b> <b>Combinations</b> in order to describe how <b>word</b> <b>combinations</b> are considered in these lexicographic resources. Chapter 2 describes the main computational methods used for the extraction <b>of</b> <b>word</b> <b>combinations</b> from corpora, taking into account the advantages and disadvantages of the two methods. Chapter 3 mainly focuses on the practical word carried out in the framework of the CombiNet project, with reference to the tools and resources used (EXTra, LexIt and "La Repubblica" corpus). Finally, the data extracted and the lexicographic layout of the lemmas to be included in the combinatory dictionary are commented, namely the words "acqua" (water), "braccio" (arm) and "colpo" (blow, shot, stroke) ...|$|R
40|$|This paper {{reports on}} work, {{carried out in}} the {{framework}} of the CombiNet project, focusing on the automatic extraction <b>of</b> <b>word</b> <b>combinations</b> from large corpora, with a view to represent the full distributional profile of selected lemmas. We describe two extraction methods, based on part-of-speech sequences (P-method) and syntactic patterns (S-method), respectively, evaluating their performance – contrastively, and with reference to external benchmarks – and discussing the relevance of automatic knowledge acquisition for lexicographic purposes. Our results indicate that both approaches provide valuable data and confirm previous claims that P-methods and S-methods are largely complementary, as they tend to retrieve different types <b>of</b> <b>word</b> <b>combinations.</b> In {{the second part of the}} paper, we present SYMPAThy, a data representation format devised to fruitfully merge the two methods by leveraging their respective points of strength. In order to explore SYMPAThy’s potentialities, a preliminary investigation on a small set of Italian idioms, and specifically their degree of fixedness/productivity, is also described...|$|R
40|$|Abstract. The paper {{presents}} {{a method of}} automatic enrichment {{of a very large}} dictionary <b>of</b> <b>word</b> <b>combinations.</b> The method is based on results of automatic syntactic analysis (parsing) of sentences. The dependency formalism is used for representation of syntactic trees that allows for easier treatment of information about syntactic compatibility. Evaluation of the method is presented for the Spanish language based on comparison of the automatically generated results with manually marked <b>word</b> <b>combinations.</b> Key words: collocations, parsing, dependency grammar, Spanish. ...|$|R
40|$|It is {{generally}} accepted now that learners {{of a foreign}} language need to have command <b>of</b> different <b>word</b> <b>combinations,</b> particularly collocations. However, {{in spite of the}} fact that collocations have recently been discussed by many linguists, there is still a lack of understanding <b>of</b> how <b>word</b> <b>combinations,</b> and among them collocations, are learned. It is of great importance that collocations are taught intensively to students who have already acquired the basis of a foreign language they wish to master. Collocations are also of the utmost importance in the study of language for specific purposes. Teachers of a foreign language try to approach this issue in different manners. This paper addresses different ways of making students aware <b>of</b> <b>word</b> <b>combinations</b> and their importance. Moreover, teachers of a foreign language are provided with some ideas for teaching collocations and correcting collocational errors students of a foreign language make when they write or speak. Some mistakes made by Slovene speakers of English are listed. The paper also provides some examples of exercises that may be of help when correcting collocational errors made by Slovene speakers of English. Finally, there is some information about different types of dictionaries...|$|R
40|$|Three {{experiments}} {{were conducted to}} test the psychological relevance <b>of</b> objectively quantified <b>word</b> collocations. The first experiment showed that perceived frequency <b>of</b> <b>word</b> <b>combinations</b> roughly followed the objective count. Anot­her recurrent quality <b>of</b> <b>words,</b> constructional tendency, was supplemented as independent variable in the two following experiments. This variable reflects a words tendency to appear in <b>word</b> <b>combinations</b> and it was found to interact with frequency when subjects rated frequency and comprehensibility. The experiments showed that word collocations, defined at the levels of combinations and constructional tendency <b>of</b> individual <b>words,</b> can be supposed to have psychological counterparts; that linguis­tic recurrence seems to have cognitive representations. digitalisering@umu. s...|$|R
40|$|Middle Welsh can now {{be studied}} exhaustively owing to the {{availability}} <b>of</b> <b>millions</b> <b>of</b> <b>words</b> <b>of</b> text, mostly in machine-readable form. This paper analyses the extensive variation in conjugated prepositions seen in thirteenth-century manuscripts, and uses it {{to shed light on}} the evolution of the language between Old and Modern Welsh, on the varying linguistic preferences of poets and prose writers, and on the emergence of the Welsh dialects. Peer reviewe...|$|R
40|$|Frequency {{effects are}} central in current second {{language}} research and theory {{but they have}} often been restricted to effects of input frequency in naturalistic environments (e. g. Ellis, 2002; Ellis and Ferreira-Junior, 2009). In {{a large majority of}} EFL contexts, however, the amount of input in the foreign language is poor and the influence of the first language may be strong (Krashen, 1981). Research on cross-linguistic influence in SLA has indeed shown that it can impact learning {{in a number of ways}} (Odlin, 1989; Jarvis and Pavlenko, 2008). The main objective of this paper is to provide empirical data to support Gass and Mackey’s (2002) view that transfer deserves a more prominent position in the ongoing debate and investigation of frequency effects. The paper draws on findings from three consecutive corpus-based investigations of EFL learners’ use <b>of</b> <b>word</b> <b>combinations</b> in which I made use of Jarvis’s (2000) methodological framework to investigate transfer effects on learners’ use of lexical bundles, i. e. “recurrent expressions, regardless of their idiomaticity, and regardless of their structural status” (Biber et al., 1999), in the French component of the International Corpus of Learner English. The studies focused on 3 -word lexical bundles that include a lexical verb, lexical bundles that function as text organizers (e. g. on the contrary, let us take the example) and bundles that are prominent in academic writing. Results suggest that transfer effects are detectable in French EFL learners’ selection of a number <b>of</b> English <b>word</b> <b>combinations</b> whose translational equivalents are deeply entrenched in their mental lexicon. L 1 frequency proved to contribute to transferability in a significant way and the different manifestations of L 1 influence displayed in the learners’ idiosyncratic use <b>of</b> <b>word</b> <b>combinations</b> were traced back to various properties <b>of</b> French <b>words,</b> including their preferred collocational use and syntactic structures, their functions and discourse conventions. Comparisons of interlanguages (e. g. the interlanguage of French vs. Spanish learners) and spot-checks in corpora of different first languages (e. g. French vs. Spanish) also provided further evidence that L 1 frequency effects play a crucial role in learners’ use <b>of</b> <b>word</b> <b>combinations</b> in input-poor environments...|$|R
40|$|The Penn Treebank, in its {{eight years}} of {{operation}} (1989 - 1996), produced approximately 7 <b>million</b> <b>words</b> <b>of</b> part-of-speech tagged text, 3 <b>million</b> <b>words</b> <b>of</b> skeletally parsed text, over 2 <b>million</b> <b>words</b> <b>of</b> text parsed for predicateargument structure, and 1. 6 <b>million</b> <b>words</b> <b>of</b> transcribed spoken text annotated for speech disfluencies. This paper describes {{the design of the}} three annotation schemes used by the Treebank: POS tagging, syntactic bracketing, and disfluency annotation and the methodology employed in production. All available [URL]...|$|R
25|$|To {{choose a}} {{value for n}} in an n-gram model, it is {{necessary}} to find the right trade off between the stability of the estimate against its appropriateness. This means that trigram (i.e. triplets <b>of</b> <b>words)</b> is a common choice with large training corpora (<b>millions</b> <b>of</b> <b>words),</b> whereas a bigram is often used with smaller ones.|$|R
50|$|The Wordtheque {{is also a}} {{powerful}} interface with a massive database (<b>millions</b> <b>of</b> <b>words)</b> containing multilingual novels, technical literature and translated texts. Hits are highlighted in context windows that can be expanded up or down. To go to the source web pages (novels, etc.) click on the title.|$|R
5000|$|<b>Millions</b> <b>of</b> <b>words</b> {{have been}} written about these horror camps, many of them by inmates of those {{unbelievable}} places. I’ve tried, without success, to describe it from my own point of view, but the words won’t come. To me Belsen was the ultimate blasphemy. (The Reluctant Jester, Chapter 17.) ...|$|R
40|$|In many {{applications}} of {{natural language processing}} (NLP) {{it is necessary to}} determine the likelihood <b>of</b> a given <b>word</b> <b>combination.</b> For example, a speech recognizer may need to determine which <b>of</b> the two <b>word</b> <b>combinations</b> "eat a peach" and "eat a beach" is more likely. Statistical NLP methods determine the likelihood <b>of</b> a <b>word</b> <b>combination</b> from its frequency in a training corpus. However, the nature of language is such that many <b>word</b> <b>combinations</b> are infrequent and do not occur in any given corpus. In this work we propose a method for estimating the probability of such previously unseen <b>word</b> <b>combinations</b> using available information on "most similar" words. We describe probabilistic word association models based on distributional word similarity, and apply them to two tasks, language modeling and pseudo-word disambiguation. In the language modeling task, a similarity-based model is used to improve probability estimates for unseen bigrams in a back-off language model. The similarity-based [...] ...|$|R
50|$|Also {{included}} in the capsule were copies of Life magazine, a kewpie doll, one dollar in change, a pack of Camel cigarettes, a 15-minute RKO Pathe Pictures newsreel, a Lilly Daché hat, and <b>millions</b> <b>of</b> <b>words</b> <b>of</b> text put on microfilm rolls which included a Sears Roebuck catalog, a dictionary, and an almanac. A variety of seeds {{were placed in the}} time capsule including wheat, corn, oats, tobacco, cotton, flax, rice, soy beans, alfalfa, sugar beets, carrots, and barley.|$|R
40|$|Information {{extraction}} (IE) culls {{information from}} text including relations, our focus here, such as head-of (Sergej-Brin, Google). The Espresso algorithm {{was developed to}} do this (Pantel & Pennacchiotti 2006), and we extend their work here first by using as input not raw text but rather syntactic analyses derived from the text, and second by applying the algorithm to Dutch. This required parsing hundreds <b>of</b> <b>millions</b> <b>of</b> <b>words</b> <b>of</b> text, which was regarded as infeasible only ten years ago...|$|R
5000|$|... tsukeai (付合): May also {{be called}} tsukekata (付け方) or tsukeaji (付け味). Refers to the mixing and {{matching}} <b>of</b> unlikely <b>word</b> <b>combinations</b> to spur imagination or evoke an image. One {{of the interesting}} features of renga.|$|R
50|$|A {{few years}} later they and their two sons moved to the South of France, in Fontarèches, near Uzès where he lived {{for the rest of}} his life, writing {{thousands}} of letters to friends, often with abject apologies for past hurts, and keeping a daily journal that runs to <b>millions</b> <b>of</b> <b>words.</b>|$|R
40|$|This article {{deals with}} noun+verb {{combinations}} in bilingual Basque-Spanish and Spanish-Basque dictionaries. We {{take a look}} at morphosyntactic and semantic features <b>of</b> <b>word</b> <b>combinations</b> in both language directions, and compare them to identify differences and similarities. Our work reveals the high complexity of those constructions and, hence, the need to address them specifically in Natural Language Processing tools, for example in Machine Translation. All of our results are publicly available online, where users can query the combinations we have analysed...|$|R
5000|$|Lees {{was known}} as a fierce {{partisan}} of Chomsky's brand of linguistics, and could be withering in his criticism. A famous example is his response when informed that Nelson Francis had received a grant to produce the Brown Corpus: [...] "That is a complete waste of your time and the government's money. You are a native speaker of English; in ten minutes you can produce more illustrations of any point in English grammar than you will find in many <b>millions</b> <b>of</b> <b>words</b> <b>of</b> random text." ...|$|R
2500|$|Loanwords often {{keep their}} {{original}} spellings: cadeau [...] 'gift' (from French). The Latin letters c, qu, x and y (from Greek υ) are sometimes adapted to k, kw, ks and i. Greek letters φ and ῥ become f and r, not ph or rh, but θ mostly becomes th (except before a consonant, after f or ch {{and at the}} end <b>of</b> <b>words).</b> <b>Combinations</b> -eon-, -ion-, -yon- in loanwords from French are written with a single n (mayonaise) except when a schwa follows (stationnement).|$|R
40|$|We {{describe}} {{tools for}} {{the extraction of}} collocations {{not only in the}} form <b>of</b> <b>word</b> <b>combinations,</b> but also <b>of</b> data about the morphosyntactic properties of collocation candidates. Such data are needed for a detailed lexical description of collocations, and to support both their recognition in text and the generation of collocationally acceptable text. We describe the tool architecture, report on a case study based on noun+verb collocations, and we give a first rough evaluation of the data quality produced. selection: syntactic analysis (pattern matching...|$|R
5000|$|Loanwords often {{keep their}} {{original}} spellings: cadeau [...] 'gift' (from French). The Latin letters c, qu, x and y (from Greek υ) are sometimes adapted to k, kw, ks and i. Greek letters φ and ῥ become f and r, not ph or rh, but θ mostly becomes th (except before a consonant, after f or ch {{and at the}} end <b>of</b> <b>words).</b> <b>Combinations</b> -eon-, -ion-, -yon- in loanwords from French are written with a single n (mayonaise) except when a schwa follows (stationnement).|$|R
40|$|<b>Millions</b> <b>of</b> <b>words</b> {{have been}} written about Ned Kelly, and the latest {{installment}} comes from legal historian Alex Castles. "Ned Kelly’s Last Days: Setting the Record Straight on the Death of an Outlaw" examines in detail the legal process which followed Kelly’s capture at Glenrowan in June 1880 and led to his execution in Melbourne on 11 November that year...|$|R
5000|$|In 1964, Bates {{recalled}} his editorship of Astounding: [...] "Long ago I {{was a party}} to the genesis of a magazine which persisted through thirty years and thirty <b>millions</b> <b>of</b> <b>words.</b> ... Astounding was a living being. I served it in its infancy and childhood, Orlin Tremaine brought it through youth and adolescence, John Campbell guided it through adulthood and maturity." ...|$|R
5000|$|Prior to Harris's {{discovery}} of transformations, grammar as so far developed could not yet treat <b>of</b> individual <b>word</b> <b>combinations,</b> but only <b>of</b> <b>word</b> classes. A sequence or ntuple <b>of</b> <b>word</b> classes (plus invariant morphemes, termed constants) specifies {{a subset of}} sentences that are formally alike. Harris investigated mappings from one such subset to another in the set of sentences. In linear algebra, a mapping that preserves a specified property is called a transformation, {{and that is the}} sense in which Harris introduced the term into linguistics. Harris's transformational analysis refined the word classes found in the 1946 [...] "From Morpheme to Utterance" [...] grammar of expansions. By recursively defining semantically more and more specific subclasses according to the combinatorial privileges <b>of</b> <b>words,</b> one may progressively approximate a grammar <b>of</b> individual <b>word</b> <b>combinations.</b> This relation <b>of</b> progressive refinement was subsequently shown in a more direct and straightforward way in a grammar of substring combinability resulting from string analysis (Harris 1962).|$|R
