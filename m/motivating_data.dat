35|560|Public
30|$|The {{remainder}} {{of this paper is}} organized as follows. Section 2 outlines the notation and model setup. In Section 3 we explore the misclassification effects. In Section 4, we develop inferential procedures to accommodate misclassification effects with the availability of replicated measurements of error-contaminated covariates. Analysis of the <b>motivating</b> <b>data</b> with the proposed method is reported in Section 5, together with simulation studies which demonstrate the performance of our method. The manuscript is concluded with discussion and extensions.|$|E
40|$|We derive a quantile-adjusted {{conditional}} {{maximum likelihood}} (qCML) estimator for the dispersion parameter {{of the negative}} binomial (NB) distribution and compare its performance, in terms of bias, to various other methods. Our estimation scheme outperforms all other methods in very small samples, typical of those from serial analysis of gene expression (SAGE) studies, the <b>motivating</b> <b>data</b> for this study. The impact of dispersion estimation on hypothesis testing is studied. We derive an “exact ” test that outperforms the standard approximate asymptotic tests. 1...|$|E
40|$|International audienceComparing {{groups is}} a {{fundamental}} skill preservice teachers are supposed to gain after attending a statistics course at university level. Preferably these activities are embedded in the well-known PPDAC-Cycle and contain the exploration of real and <b>motivating</b> <b>data.</b> Adequate software such as TinkerPlots may support learners when exploring data and carving out differences between distributions of numerical variables. In this article we want to present first results of a study on statistical reasoning of preservice teachers while doing group comparisons with TinkerPlots...|$|E
50|$|Diversion curves {{are based}} on {{empirical}} observations, and their improvement has resulted from better (more and more pointed) data. Curves are available for many markets. It {{is not difficult to}} obtain data and array results. Expansion of transit has <b>motivated</b> <b>data</b> development by operators and planners. Yacov Zahavi’s UMOT studies, discussed earlier, contain many examples of diversion curves.|$|R
40|$|This {{paper offers}} {{a new way of}} {{representing}} the results of automatic clustering algorithms by employing a Visual Analytics system which maps members of a cluster and their distance to each other onto a twodimensional space. A case study on Urdu complex predicates shows that the system allows for an appropriate investigation of linguistically <b>motivated</b> <b>data.</b> ...|$|R
40|$|We prove that, under {{suitable}} assumptions, operationally <b>motivated</b> <b>data</b> completely {{determine a}} space-time {{in which the}} quantum systems {{can be interpreted as}} evolving. At the same time, the dynamics of the quantum system is also determined. To minimize technical complications, this is done in the example of three-dimensional Minkowski space. Comment: 19 pages, to appear in Communications in Mathematical Physics; minor corrections mad...|$|R
40|$|When {{travelling}} in space, humans {{perceive the}} environment and evaluate it affectively. This chapter illustrates how mobile crowdsourcing and social media data {{can be used to}} study people’s affective responses to different environments. It also showcases how these affective responses can be used to provide a better understanding of human?environment interaction, as well as to enable smart geospatial applications (particularly navigation systems). This chapter also discusses some essential challenges that need further investigations when crowdsourcing people’s affective responses. Some of these challenges are participation <b>motivating,</b> <b>data</b> quality and privacy...|$|E
40|$|Consider a {{realisation}} {{of a point}} {{process which}} {{is formed as a}} superposition of a regular point process, here a Strauss process, and some Poisson noise. The aim of the current work is to decide which of the two processes each point belongs to. We construct an MCMC algorithm which estimates the parameters of the superposition model and obtains posterior probabilities for each point of being a Strauss point. The algorithm is evaluated in a simulation study. Finally, it is applied to our <b>motivating</b> <b>data</b> set containing the locations of air bubbles, some of which are noise, in an Antarctic ice core...|$|E
40|$|This thesis {{presents}} {{a model of}} incremental natural language understanding based on the gram-matical formalism known as Combinatory Categorial Grammar. The model constitutes an integrated system involving a cyclical process of parsing, semantic adjudication and filtering. The <b>motivating</b> <b>data</b> for the model are the well-known observations about garden path effects in human sentence processing, and particularly {{the fact that the}} presence and strength of the garden path effect is influenced by the referential context in which the sentence is uttered, as well as the actual lexical items selected. It is argued that the model successfully explains certain garden path phenomena in English. The model has been implemented in the Java programming language...|$|E
40|$|The mixture {{transition}} distribution (MTD) {{model was}} introduced by Raftery (1985) as a parsimonious model for high-order Markov chains. It is flexible, can {{a wide range of}} patterns, can be physically <b>motivated,</b> <b>data</b> well, and appears to be a discrete-valued analogue for the class of autoregressive series models. However, estimation has presented difficulties because the space is highly nonconvex, being defined by a large number of nonlinear constraints. we propose an computational algorithm for maximum likelihood is on a way of reducing the of also stnlcttlred verSl 011 S of Away apli>liE!d to a sequence...|$|R
40|$|The paper {{develops}} a simple supergame model of collusion {{that focuses on}} the role of fixed (exogenous to game played) system of quantity market shares. Conclusions implied by the model could be used to <b>motivate</b> <b>data</b> - saving markers of collusion based on market price behavior. Following conclusions of the theoretical model we propose marker of collusion based on detecting changes in seasonal parameters of prices in periods of possible collusion. An empirical application of method has been done on well known data of Lysine cartel case. Collusion, repeated games, fixed market shares, seasonality of market price. ...|$|R
40|$|Privacy {{preserving}} querying {{and data}} publishing {{has been studied}} {{in the context of}} statistical databases and statistical disclosure control. Recently, large-scale data collection and integration efforts increased privacy concerns which <b>motivated</b> <b>data</b> mining researchers to investigate privacy implications of data mining and how data mining can be performed without violating privacy. In this paper, we first provide an overview of privacy preserving data mining focusing on distributed data sources, then we compare two technologies used in privacy preserving data mining. The first technology is encryption based, and it is used in earlier approaches. The second technology is secret-sharing which is recently being considered as a more efficient approach...|$|R
40|$|Research and {{classroom}} experience identify topics with which students in introductory statistics struggle such as interpreting box plots, standard deviation or z-scores and the normal curve. One {{reason is that}} many core statistical concepts are subtle and difficult to sort out. Dynamic interactive technology can provide opportunities for learners to begin {{to make sense of}} these concepts by enabling them to generate large amounts of data, explore distributions, examine probability models and investigate the nuances that often seem to obscure reasoning and sense making in statistics. Interactive technology allows learners, using real and <b>motivating</b> <b>data</b> that stem from questions about ways of reasoning in statistics, to move between representations, looking for patterns and generating models related to hypotheses and to informed decision making...|$|E
40|$|Measures {{of daily}} {{cigarette}} consumption, like many self-reported numerical data, exhibit {{a form of}} measurement error termed heaping. This occurs when quantities are reported with varying levels of precision, often {{in the form of}} round numbers. As heaping can introduce substantial bias to estimates, conclusions drawn from data subject to heaping are suspect. Because more precise measurements are seldom available, methods to estimate the true underlying distribution from heaped data depend on unverifiable assumptions about the heaping mechanism. A doubly-coded dataset with both a conventional retrospective recall measurement (timeline followback) and an instantaneous measurement not subject to heaping (ecological momentary assessment), motivates this dissertation and allows us to model the heaping mechanism. ^ We take three approaches to this problem. First, we develop a nonparametric method that involves the estimation of heaping probabilities directly, where possible, and calculating others by smoothing, interpolation and subtraction. Next, we use the <b>motivating</b> <b>data</b> as a calibration data set, allowing us to create a predictive model for imputation. We apply this model to multiply impute precise cigarette counts for data from a randomized, placebo-controlled trial of bupropion where only heaped cigarette counts are available. Finally, we build on findings from the first two approaches to develop a more flexible model which forgoes the restrictive rounding framework of previous models. Rather than assuming subjects will round off when providing self-reported counts, we posit that numbers possess an intrinsic gravity that tends to attract subjects to characteristically round numerals. We outline procedures for parameterizing and estimating such a model and apply it to the <b>motivating</b> <b>data.</b> Our findings suggest that the self-reporting process is more complex than the mechanism assumed in conventional rounding-based models. While we apply these models exclusively to smoking cessation data, they have wide applicability to many types of self-reported count data. ...|$|E
40|$|A {{community}} ecologist {{provided a}} <b>motivating</b> <b>data</b> set involving a certain animal species with two behavior groups, {{along with a}} pairwise genetic distance matrix among individuals. Many community ecologists have analyzed similar data sets with a method known as the Hopkins method, testing for an association between the subject-level covariate (behavior group) and the pairwise distance. This community ecologist {{wanted to know if}} they used the Hopkins method, would their results be meaningful? Their question inspired this thesis work, where a different data set was used for confidentiality reasons. Multiple methods (Hopkins method, ADONIS, ANOSIM, and Distance Regression) were used to analyze the distance matrix for association with a binary covariate of interest. To compare the performance of the Hopkins method with the performance of the remaining, more established methods, a simulation was run. The results of the simulation indicate that ADONIS, ANOSIM, and distance regression would all be preferable to the Hopkins method...|$|E
30|$|The {{next section}} <b>motivates</b> the <b>data</b> {{collection}} {{for this work}} and explains the principle data processing steps. The analysis and results are presented and discussed in Analysis and results section, while the conclusions in Sec. 4 summarizes the main findings.|$|R
40|$|Suvey on researchers' {{attitudes and}} {{practices}} of data sharing {{in the area of}} Environmental sciences. It is based on an online questionnaire submitted to CNR researchers belonging to the 13 Institutes of the Department of Earth and Environment. It is a semi- structured questionnaire of 40 questions that consists of two main parts. the first one aims to gain insight into research practices that may influence data sharing (research context, data acquisition, data managment, data re-use). The second part is focused on capturing perceived barriers and enablers that may <b>motivate</b> <b>data</b> sharing. This part contains also a selection of questions submitted in large-scale international survey, in order to explore commonalitiy and differences in attitudes. The survey survey period was June-September 2012. The response rate reached 48 %...|$|R
40|$|Cloud {{computing}} <b>motivates</b> <b>data</b> {{owners to}} economically outsource {{large amounts of}} data to the cloud. To preserve the privacy and confidentiality of the documents, the documents need to be encrypted prior to being outsourced to the cloud. In this paper, we propose a lightweight construction that facilitates ranked disjunctive keyword (multi-keyword) searchable encryption based on probabilistic trapdoors. The security analysis yieldsthat the probabilistic trapdoors help resist distinguishability attacks. Through the computational complexity analysis we realize that our scheme outperforms similar existing schemes. We explore the use of searchable encryption in the telecom domain by implementing and deploying our proof of concept prototypeonto the British Telecommunication's Public Cloud offering and testing it over a real corpus of audio transcriptions. The extensive experimentation thereafter validates our claim that our scheme is lightweight...|$|R
40|$|False {{discovery}} rate (FDR) procedures provide misleading inference when testing multiple null hypotheses with heterogeneous multinomial data. For example, in the motivating {{study the}} goal is to identify species of bacteria near the roots of wheat plants (rhizobacteria) that are associated with productivity, but standard procedures discover the most abundant species even when the association is weak or negligible, and fail to discover strong associations when species are not abundant. Consequently, a list of abundant species is produced by the multiple testing procedure even though the goal was to provide a list of producitivity-associated species. This paper provides an FDR method based on a mixture of multinomial distributions and shows that it tends to discover more non-negligible effects and fewer negligible effects when the data are heterogeneous across tests. The proposed method and competing methods are applied to the <b>motivating</b> <b>data.</b> The new method identifies more species that are strongly associated with productivity and identifies fewer species that are weakly associated with productivity...|$|E
40|$|This study {{introduces}} a new method of visualizing complex tree structured objects. The usefulness {{of this method}} is illustrated {{in the context of}} detecting unexpected features in a data set of very large trees. The major contribution is a novel two-dimensional graphical representation of each tree, with a covariate coded by color. The <b>motivating</b> <b>data</b> set contains three dimensional representations of brain artery systems of 105 subjects. Due to inaccuracies inherent in the medical imaging techniques, issues with the reconstruction algo- rithms and inconsistencies introduced by manual adjustment, various discrepancies are present in the data. The proposed representation enables quick visual detection of the most common discrepancies. For our driving example, this tool led to the modification of 10 % of the artery trees and deletion of 6. 7 %. The benefits of our cleaning method are demonstrated through a statistical hypothesis test on the effects of aging on vessel structure. The data cleaning resulted in improved significance levels. Comment: 17 pages, 8 figure...|$|E
40|$|We are {{concerned}} with the estimation of the exterior surface and interior summaries of tube-shaped anatomical structures. This interest is motivated by two distinct scientific goals, one dealing with the distribution of HIV microbicide in the colon and the other with measuring degradation in white-matter tracts in the brain. Our problem is posed as the estimation of the support of a distribution in three dimensions from a sample from that distribution, possibly measured with error. We propose a novel tube-fitting algorithm to construct such estimators. Further, we conduct a simulation study to aid in the choice of a key parameter of the algorithm, and we test our algorithm with validation study tailored to the <b>motivating</b> <b>data</b> sets. Finally, we apply the tube-fitting algorithm to a colon image produced by single photon emission computed tomography (SPECT) and to a white-matter tract image produced using diffusion tensor imaging (DTI). Comment: Published in at [URL] the Annals of Applied Statistics ([URL] by the Institute of Mathematical Statistics ([URL]...|$|E
50|$|The head-movement {{analysis}} is <b>motivated</b> by ellipsis <b>data</b> in Celtic and Semitic languages, and by verb-adjacent particles and adverbs in Austronesian languages.|$|R
40|$|Recent {{concerns}} about privacy issues have <b>motivated</b> <b>data</b> mining researchers to develop methods for performing data mining while preserving {{the privacy of}} individuals. One approach to develop privacy preserving data mining algorithms is secure multiparty computation, which allows for privacy preserving data mining algorithms that do not trade accuracy for privacy. However, earlier methods suffer from very high communication and computational costs, making them infeasible to use in any real world scenario. Moreover, these algorithms have strict assumptions on the involved parties, assuming involved parties will not collude with each other. In this paper, the authors propose a new secure multiparty computation based k-means clustering algorithm that is both secure and efficient enough {{to be used in}} a real world scenario. Experiments based on realistic scenarios reveal that this protocol has lower communication costs and significantly lower computational costs...|$|R
40|$|Biological studies {{across all}} omics elds {{generate}} {{vast amounts of}} data. To understand these complex <b>data,</b> biologically <b>motivated</b> <b>data</b> mining techniques are indispensable. Evaluation of the high-throughput mea-surements usually relies on the identication of underlying signals as well as shared or outstanding characteristics. erein, methods have been de-veloped to recover source signals of present datasets, reveal objects which are more similar to each other than to other objects {{as well as to}} detect observations which are in contrast to the background dataset. Biolog-ical problems got individually addressed by using solutions from com-puter science according to their needs. e study of protein-protein in-teractions (interactome) focuses on the identication of clusters, the sub-graphs of graphs: A parameter-free graph clustering algorithm was devel-oped, which was based on the concept of graph compression, in order to nd sets of highly interlinked proteins sharing similar characteristics. ...|$|R
40|$|Object Oriented Data Analysis is a {{new area}} in {{statistics}} that studies populations of general data objects. In this article we consider populations of tree-structured objects as our focus of interest. We develop improved analysis tools for data lying in a binary tree space analogous to classical Principal Component Analysis methods in Euclidean space. Our exten-sions of PCA are analogs of one dimensional subspaces that best fit the data. Previous work {{was based on the}} notion of tree-lines. In this paper, a generalization of the previous tree-line notion is pro-posed: k-tree-lines. Previously proposed tree-lines are k-tree-lines where k = 1. New sub-cases of k-tree-lines studied in this work are the 2 -tree-lines and tree-curves, which explain much more variation per principal component than tree-lines. The optimal principal component tree-lines were computable in linear time. Because 2 -tree-lines and tree-curves are more complex, they are computationally more expensive, but yield im-proved data analysis results. We provide a comparative study of all these methods on a <b>motivating</b> <b>data</b> set consisting of brain vessel structures of 98 subjects. ...|$|E
40|$|In several {{scientific}} applications, {{data are}} generated from {{two or more}} diverse sources (views) {{with the goal of}} predicting an outcome of interest. Often it is the case that the outcome is not associated with any single view. However, the synergy of all measurements from each view may yield a more predictive classifier. For example, consider a drug discovery application in which individual molecules are described partially by several assay screens based on diverse profiles and partially by their chemical structural fingerprints. A common classification problem is to determine whether the molecule is associated with a particular disease. In this paper, a co-training algorithm is developed to utilize data from diverse sources to predict the common class variable. Novel enhancements for variable importance, robustness to a mislabeled class variable, and a technique to handle unbalanced classes are applied to the <b>motivating</b> <b>data</b> set, highlighting that the approach attains strong performance and provides useful diagnostics for data analytic purposes. In addition, comparisons to a framework with data fusion using partial least squares (PLS) are also assessed on real data. An R package for performing the proposed approach is provided as Supporting information. Copyright © 2003 John Wiley & Sons, Ltd...|$|E
40|$|Fluorescence {{resonance}} energy transfer (FRET) microscopy can measure the {{spatial distribution of}} protein interactions inside live cells. Such experiments give rise to complex data sets with many images of single cells, <b>motivating</b> <b>data</b> reduction and abstraction. In particular, determination {{of the value of}} the equilibrium dissociation constant (K d) will provide a quantitative measure of protein–protein interactions, which is essential to reconstructing cellular signaling networks. Here, we investigate the feasibility of using quantitative FRET imaging of live cells to estimate the local value of K d for two interacting labeled molecules. An algorithm is developed to infer the values of K d using the intensity of individual voxels of 3 -D FRET microscopy images. The performance of our algorithm is investigated using synthetic test data, both in the absence and in the presence of endogenous (unlabeled) proteins. The influence of optical blurring caused by the microscope (confocal or wide field) and detection noise on the accuracy of K d inference is studied. We show that deconvolution of images followed by analysis of intensity data at local level can improve the estimate of K d. Finally, the performance of this algorithm using cellular data on the interaction between yellow fluorescent protein-Rac and cyan fluorescent protein-PBD in mammalian cells is shown...|$|E
40|$|This work is {{targeted}} towards pathway restoration and deformation sensing using acceleration data. This problem is solved analyzing two point system movements in a plane. Integration methods are compared, choice of Newton - Kotess formulas for integration is <b>motivated.</b> <b>Data</b> for analysis {{is provided by}} simulation, because real data acquisition requires financial investments. Error dependencies on integration parameters, accelerometer operation parameters are analyzed. The dynamics of error development while observing different pathways is being tracked; a criterion for sensing deformation is defined. This work has been reported at the three conferences (6 th Student‘s Conference, 2006; Mathematics and Mathematical Modeling, 2006; 47 th Conference of Lithuanian Mathematician Association, 2006). In additions, three articles have been published based on this topic [6], [7], [8] and one more article {{is going to be}} presented at the 48 th Conference of Lithuanian Mathematician Association...|$|R
40|$|We {{investigate}} a combinatorial two-player game, {{in which one}} player wants to keep the behavior of an underlying water-bucket system stable whereas the other player wants to cause overflows. This game is <b>motivated</b> by <b>data</b> management applications in wireless sensor networks. We construct optimal strategies and characterize optimal bucket sizes for many instances of this game...|$|R
40|$|Abstract. Imprecision, incompleteness, prior {{knowledge}} or improved learning speed can <b>motivate</b> interval–represented <b>data.</b> Most approaches for SVM learning of interval data use local kernels based on interval distances. We present here a novel approach, suitable for linear SVMs, which allows {{to deal with}} interval data without resorting to interval distances. The experimental results confirms the validity of our proposal. ...|$|R
40|$|Five year post-transplant {{survival}} rate {{is an important}} indicator on quality of care delivered by kidney transplant centers in the United States. To provide a fair assessment of each transplant center, an effect that represents the center-specific care quality, along with patient level risk factors, is often included in the risk adjustment model. In the past, the center effects have been modeled as either fixed effects or Gaussian random effects, with various pros and cons. Our numerical analyses reveal that the distributional assumptions do impact the prediction of center effects especially when the effect is extreme. To {{bridge the gap between}} these two approaches, we propose to model the transplant center effect as a latent random variable with a finite Gaussian mixture distribution. Such latent Gaussian mixture models provide a convenient framework to study the heterogeneity among the transplant centers. To overcome the weak identifiability issues, we propose to estimate the latent Gaussian mixture model using a penalized likelihood approach, and develop sequential locally restricted likelihood ratio tests to determine the number of components in the Gaussian mixture distribution. The fitted mixture model provides a convenient means of controlling the false discovery rate when screening for underperforming or outperforming transplant centers. The performance of the methods is verified by simulations and by the analysis of the <b>motivating</b> <b>data</b> example...|$|E
40|$|Summary. A {{distributed}} lag model (DLagM) is a regression model that includes lagged exposure vari-ables as covariates; its corresponding {{distributed lag}} (DL) function describes {{the relationship between}} the lag and the coefficient of the lagged exposure variable. DLagMs have recently been used in environmental epidemiology for quantifying the cumulative effects of weather and air pollution on mortality and morbid-ity. Standard methods for formulating DLagMs include unconstrained, polynomial, and penalized spline DLagMs. These methods may fail {{to take full advantage of}} prior information about the shape of the DL function for environmental exposures, or for any other exposure with effects that are believed to smoothly approach zero as lag increases, and are therefore at risk of producing suboptimal estimates. In this article, we propose a Bayesian DLagM (BDLagM) that incorporates prior knowledge about the shape of the DL function and also allows the degree of smoothness of the DL function to be estimated from the data. We apply our BDLagM to its <b>motivating</b> <b>data</b> from the National Morbidity, Mortality, and Air Pollution Study to estimate the short-term health effects of particulate matter air pollution on mortality from 1987 to 2000 for Chicago, Illinois. In a simulation study, we compare our Bayesian approach with alternative methods that use unconstrained, polynomial, and penalized spline DLagMs. We also illustrate the connection be-tween BDLagMs and penalized spline DLagMs. Software for fitting BDLagM models and the data used in this article are available online...|$|E
40|$|In multi-locus {{association}} analysis, {{since some}} markers {{may not be}} associated with a trait, it seems attractive to use penalized regression with the capability of automatic variable selection. On the other hand, in spite of a rapidly growing body of literature on penalized regression, most focus on variable selection and outcome prediction, for which penalized methods are generally more effective than their non-penalized counterparts. However, for statistical inference, i. e. hypothesis testing and interval estimation, it is less clear how penalized methods would perform, or even how to best apply them, largely due to lack of studies on this topic. In our <b>motivating</b> <b>data</b> for a cohort of kidney transplant recipients, it is of primary interest to assess whether a group of genetic variants are associated with a binary clinical outcome, acute rejection at 6 months. In this paper, we study some technical issues and alternative implemen-tations of hypothesis testing in Lasso penalized logistic regression, and compare their performance {{with each other and with}} several existing global tests, some of which are specifically designed as variance component tests for high-dimensional data. The most interesting, and perhaps surprising, conclusion of this study is that, for low to moderately high-dimensional data, statistical tests based on Lasso penalized regres-sion are not necessarily more powerful than some existing global tests. In addition, in penalized regression, rather than building a test based on a single selected “best” model, combining multiple tests, each of which is built on a candidate model, might be more promising...|$|E
40|$|The {{performance}} and immersiveness of telepresence and teleaction systems critically {{depend on the}} quality of the communication between the operator and the teleoperator. High packet rate for low delay data exchange in internet-based teleoperation is one of the major challenges in this context. A psychophysically <b>motivated</b> <b>data</b> reduction scheme, earlier presented by the authors, solves this problem by the use of signal amplitude dependent deadband quantization. This scheme successfully limits the amount of transmitted packets, but as soon as the size of the deadband exceeds a certain limit, noticeable and disturbing artifacts are introduced into the haptic signals. In this paper, we present a novel signal reconstruction technique, based on a signal adaptive synthesis filter. It removes discontinuities in the displayed signals and enables the use of strong deadband based data reduction without perceptible disturbance in the telepresence system. Index Terms — perceptual coding, haptics, deadband, synthesis filter, teleaction system, telepresenc...|$|R
5000|$|During the 1930s the {{expansion}} of economically important government weather forecasting services and their increasing need for <b>data</b> <b>motivated</b> many nations to begin regular radiosonde observation programs ...|$|R
40|$|Dedicated to Rudolf Haag on the {{occasion}} of his eightieth birthday We prove that, under suitable assumptions, operationally <b>motivated</b> quantum <b>data</b> completely determine a space–time in which the quantum systems can be interpreted as evolving. At the same time, the dynamics of the quantum system is also determined. To minimize technical complications, this is done in the example of three-dimensional Minkowski space. ...|$|R
