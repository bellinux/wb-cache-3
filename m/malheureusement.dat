39|0|Public
6000|$|... "If Monsieur de Barbérie vas 'live, Monsieur Alderman, {{he should}} say des choses convenables; mais, <b>malheureusement,</b> mon chèr, maître est mort; and, sair, I shall be bold to remercier pour lui, et pour toute sa famille." ...|$|E
6000|$|... "La tombe d'Evrard de Fouilloy, (died 1222,) coulée {{en bronze}} en plein-relief, était supportée dès le principe, par des monstres engagés dans une maçonnerie remplissant le dessous du monument, pour {{indiquer}} que cet évêque avait posé les fondements de la Cathédrale. Un architecte <b>malheureusement</b> inspiré a osé arracher la maçonnerie, pour qu'on ne vit plus la main du prélat fondateur, à la base de l'édifice.|$|E
6000|$|... "Bon soir, Monsieur le Capitaine; c'est un brave Monsieur que celui-la, et de très bonne famille! Il n'a pas de si grandes terres, que Monsieur le Patteroon, pourtant, on dit, qu'il {{doit avoir}} de jolies maisons et assez de rentes publiques! J'aime à servir un si généreux et loyal maître, mais, <b>malheureusement,</b> il est marin! M. de Barbérie n'avait pas trop d'amitié pour les gens de cette {{profession}} là." ...|$|E
6000|$|... "Ce Smiley avait une jument que les gars appelaient le bidet du quart d'heure, mais seulement pour plaisanter, vous comprenez, parce que, bien entendu, elle était plus vite que ca! Et il avait coutume de gagner de l'argent avec cette bête, quoi-qu'elle fût poussive, cornarde, toujours prise d'asthme, de coliques ou de consomption, ou de {{quelque chose}} d'approchant. On lui donnait 2 ou 300 'yards' au départ, puis on la dépassait sans peine; mais jamais à la fin elle ne manquait de s'échauffer, de s'exaspérer et elle arrivait, s'écartant, se défendant, ses jambes grêles en l'air devant les obstacles, quelquefois les évitant et faisant avec cela plus de poussière qu'aucun cheval, plus de bruit surtout avec ses éternumens et reniflemens.---crac! elle arrivait donc toujours première d'une tête, aussi juste qu'on peut le mesurer. Et il avait un petit bouledogue qui, à le voir, ne valait pas un sou; on aurait cru que parier contre lui c'était voler, tant il était ordinaire; mais aussitôt les enjeux faits, il devenait un autre chien. Sa mâchoire inférieure commencait à ressortir comme un {{gaillard}} d'avant, ses dents se découvcraient brillantes commes des fournaises, et un chien pouvait le taquiner, l'exciter, le mordre, le jeter deux ou trois fois par-dessus son épaule, André Jackson, c'était le nom du chien, André Jackson prenait cela tranquillement, comme s'il ne se fût jamais attendu à autre chose, et quand les paris étaient doublés et redoublés contre lui, il vous saisissait l'autre chien juste à l'articulation de la jambe de derrière, et il ne la lâchait plus, non pas qu'il la mâchât, vous concevez, mais il s'y serait tenu pendu jusqu'à ce qu'on jetât l'éponge en l'air, fallût-il attendre un an. Smiley gagnait toujours avec cette bête-là; <b>malheureusement</b> ils ont fini par dresser un chien qui n'avait pas de pattes de derrière, parce qu'on les avait sciées, et quand les choses furent au point qu'il voulait, et qu'il en vint à se jeter sur son morceau favori, le pauvre chien comprit en un instant qu'on s'était moqué de lui, et que l'autre le tenait. Vous n'avez jamais vu personne avoir l'air plus penaud et plus découragé; il ne fit aucun effort pour gagner le combat et fut rudement secoué, de sorte que, regardant Smiley comme pour lui dire:--Mon coeur est brisé, c'est ta faute; pourquoi m'avoir livré à un chien qui n'a pas de pattes de derrière, puisque c'est par là que je les bats?--il s'en alla en clopinant, et se coucha pour mourir. Ah! c'était un bon chien, cet André Jackson, et il se serait fait un nom, s'il avait vécu, car il y avait de l'etoffe en lui, il avait du génie, je la sais, bien que de grandes occasions lui aient manqué; mais il est impossible de supposer qu'un chien capable de se battre comme lui, certaines circonstances étant données, ait manqué de talent. Je me sens triste toutes les fois que je pense à son dernier combat et au dénoûment qu'il a eu. Eh bien! ce Smiley nourrissait des terriers à rats, et des coqs combat, et des chats, et toute sorte de choses, au point qu'il était toujours en mesure de vous tenir tête, et qu'avec sa rage de paris on n'avait plus de repos. Il attrapa un jour une grenouille et l'emporta chez lui, disant qu'il prétendait faire son éducation; vous me croirez si vous voulez, mais pendant trois mois il n'a rien fait que lui apprendre à sauter dans une cour retirée de sa maison. Et je vous réponds qu'il avait reussi. Il lui donnait un petit coup par derrière, et l'instant d'après vous voyiez la grenouille tourner en l'air comme un beignet au-dessus de la poêle, faire une culbute, quelquefois deux, lorsqu'elle était bien partie, et retomber sur ses pattes comme un chat. Il l'avait dressée dans l'art de gober des mouches, er l'y exercait continuellement, si bien qu'une mouche, du plus loin qu'elle apparaissait, était une mouche perdue. Smiley avait coutume de dire que tout ce qui manquait à une grenouille, c'était l'éducation, qu'avec l'éducation elle pouvait faire presque tout, et je le crois. Tenez, je l'ai vu poser Daniel Webster là sur se plancher,--Daniel Webster était le nom de la grenouille,--et lui chanter: Des mouches! Daniel, des mouches!--En un clin d'oeil, Daniel avait bondi et saisi une mouche ici sur le comptoir, puis sauté de nouveau par terre, où il restait vraiment à se gratter la tête avec sa patte de derrière, comme s'il n'avait pas eu la moindre idée de sa superiorité. Jamais vous n'avez grenouille vu de aussi modeste, aussi naturelle, douee comme elle l'était! Et quand il s'agissait de sauter purement et simplement sur terrain plat, elle faisait plus de chemin en un saut qu'aucune bete de son espèce que vous puissiez connaître. Sauter à plat, c'était son fort! Quand il s'agissait de cela, Smiley entassait les enjeux sur elle tant qu'il lui, restait un rouge liard. Il faut le reconnaitre, Smiley était monstrueusement fier de sa grenouille, et il en avait le droit, car des gens qui avaient voyagé, qui avaient tout vu, disaient qu'on lui ferait injure de la comparer à une autre; de facon que Smiley gardait Daniel dans une petite boîte a claire-voie qu'il emportait parfois à la Ville pour quelque pari.|$|E
5000|$|Goa est <b>malheureusement</b> célèbre par son inquisition, également contraire à l'humanité et au commerce. Les moines portugais firent accroire que le peuple adorait le diable, et ce sont eux qui l'ont servi. (Goa is sadly {{famous for}} its inquisition, equally {{contrary}} to humanity and commerce. The Portuguese monks made us believe that the people worshiped the devil, and it is they who have served him.) ...|$|E
5000|$|V. Il semble, d'autre part, résulter de quelques-unes de nos expériences, <b>malheureusement</b> trop peu nombreuses et qu'il importera de répéter à nouveau et de contrôler, que certaines moisissures (Penicillum glaucum), inoculées à un animal en même temps que des {{cultures}} très virulentes de quelques microbes pathogènes (B. coli et B. typhosus d'Eberth), sont capables d'atténuer dans de très notables proportions la virulence de ces cultures bactériennes.Translation: V. It seems, on {{the other}} hand, to follow from some of our experiments — unfortunately too few and which {{it will be important}} to repeat anew and to check — that certain molds (Penicillum glaucum), inoculated into an animal {{at the same time as}} very virulent cultures of some pathogenic microbes (E. coli and typhoid), are capable of reducing to a very considerable degree the virulence of these bacterial cultures. Ernest Duchesne ...|$|E
40|$|Direct {{manipulation}} {{has proven}} to be an excellent method for interacting with geometric objects. Unfortunately, traditional approaches for implementing direct manipulation suffer from a lack of generality, requiring the system designer to hand craft interfaces to different types of objects. In this paper we present differential manipulation, a new paradigm for direct manipulation of geometric objects. By interpreting graphical entities as physical objects, we obtain a uniform interface {{to a wide variety of}} geometric objects, making it simple to add new types of complicated or compound objects. Geometric constraints fit neatly into the paradigm. R esum e La manipulation directe est une excellente methode pour le traitement interactif des objets geometriques. <b>Malheureusement,</b> les approches traditionelles pour l'implementation de la manipulation directe manquent de generalite en necessitant que differentes interfaces soient associees a differents types d'objets. Dans cet article nous pr [...] ...|$|E
40|$|In [FRE 91], Freuder {{has defined}} {{the notion of}} {{interchangeability}} on values in a CSP, whose purpose {{is to reduce the}} size of the problem without losing any solution. He also defined related and weaker notions, as subsitutability. This paper explores this latter notion in several ways: first the partial orders on domain values induced by the substitutability relation are emphasized and used {{to reduce the size of}} the problem without losing its satisfiability (but some solutions may be lost); finally weaker forms of this notion are presented and used to improve classical resolution methods like backtracking and forward-checking. MOTS-CLE S: Problemes de satisfaction de contraintes, pre-ordres partiels. KEY WORDS: Constraint satisfaction problems, partial pre-orders. 1. Introduction Les problemes de satisfaction de contraintes constituent un outil interessant de mode lisation pour beaucoup de problemes d'intelligence artificielle et de problemes d'optimisation. <b>Malheureusement,</b> de te [...] ...|$|E
40|$|Abstract: Controlled {{vocabulary}} {{is known}} to help searchers and Semantic Web ontologies are one of its latest incarnations. Unfortunately, many searchers are unaware of this useful organization of information. An interactive 3 D fly-through search and browsing prototype was developed and is proposed as potential solution to enhance navigation. Résumé: Le vocabulaire contrôlé est reconnu pour aider les chercheurs et les ontologies du web sémantique en sont la dernière incarnation. <b>Malheureusement,</b> nombre de chercheurs ne connaissent pas cet utile outil d'organisation de l'information. Un prototype interactif de recherche et de navigation en 3 D a été développé et est proposé comme solution pour rehausser la navigation. 1. Context and Problem All libraries, many corporations and the semantic Web organize information using forms of controlled vocabulary (CV); however, many searchers are unaware of CV {{in part because of}} inadequate subject browsing interfaces (Bates 2003; Drabenstott 1991). This is unfortunate because the value of controlled subject vocabulary has been demonstrated (Bates 2003; Lancaster 1986), and information professionals explicitly use CV becaus...|$|E
40|$|Educational {{psychology}} {{is in the}} midst of examination and potential change in the province of Newfoundland and Labrador. Unfortunately, there has been a paucity of published empirical research investigating educational psychology in the province. The current study surveyed educational psychologists from the four English districts in Newfoundland and Labrador to investigate areas such as current and preferred work activities, typical assessment and counselling practices, and psychological test access and perceived test usefulness. Results highlighted a range of current and preferred work activities, with psychoeducational assessment as the most time consuming responsibility. Although some findings varied between school districts, participants overall reported preferring increased amounts of time to devote to areas such as prevention, counselling, and research. Implications for training, future research, and practice are discussed. Resume La psychopédagogie est présentement sous examen et des changements pourraient être apportés à cette discipline dans la province de Terre-Neuve-et-Labrador. <b>Malheureusement,</b> il y avait une pénurie de publications en matière de recherche empirique sur la psychologie de l’éducation dans la province. Cette étude a interrogé des psychologues scolaires des quatre districts anglophones de Terre-Neuve-et-Labrado...|$|E
40|$|International audienceOn {{rencontre}} en arabe moderne une structure kāna sa-yafʿalu, équivalent au français « ilallait faire » ou à l’anglais « he {{was going}} to do » et ayant, comme eux, entre autresinterprétations possibles, celle de conditionnel passé (« il aurait fait » / « he wouldhave done »). Une structure analogue existe dans les dialectes. On pourrait doncpenser à une innovation de l’arabe moderne, influencée par les dialectes, ce qu’onpourrait appeler un « dialectalisme classicisé ». <b>Malheureusement,</b> un exemple de kānasa-yafʿalu apparaît dans le Kitāb de Sībawayhi (m. 179 / 795 ?), à propos de la descriptionde law, semblant ainsi confirmer le caractère primordial de l’interprétation commeconditionnel passé. Plutôt que de segmenter la grammaire de l’arabe en grammaire del’arabe classique et grammaire de l’arabe moderne, il serait plus judicieux de construireune grammaire historique de l’arabe écrit. In Modern Standard Arabic one encounters a structure kāna sa-yafʿalu equivalent to theFrench « il allait faire » or to the English « {{he was going}} to do » and having like those,among other possible interpretations, that of the past conditional (« il aurait fait » /« he would have done »). A similar structure exists in the dialects. One might thereforethink of an innovation – influenced by the dialects – of Modern Standard Arabic, whatmight be called a “classicised dialectalism”. Unfortunately, an example of kāna sayafʿalu appears in Sībawayhi’s (d. 179 / 795 ?) Kitāb concerning the description of lawand thus seeming to confirm the primordial character of its interpretation as past conditional. Rather than segmenting Arabic grammar into a grammar of Classical Arabicand one of Modern Standard Arabic, it would be wiser to build a historical grammar ofwritten Arabic...|$|E
40|$|PARMI LES NOMBREUX FACTEURS IMPLIQUES DANS LES CONSEQUENCES PHYSIOPATHOLOGIQUES DU DIABETE NON INSULINODEPENDANT (DNID), DEUX SEMBLENT AVOIR UNE IMPORTANCE PARTICULIERE : 1) UN STRESS OXYDANT, CARACTERISE PAR UNE RUPTURE DE L'EQUILIBRE CELLULAIRE ENTRE OXYDANTS ET ANTIOXYDANTS, 2) DES DYSFONCTIONNEMENTS DU METABOLISME ENERGETIQUE CONDUISANT A L'OBESITE. L'AMELIORATION DU POTENTIEL ANTIOXYDANT D'UNE PART, ET LA STIMULATION DES RECEPTEURS 3 ADRENERGIQUES (3 AR), LOCALISES MAJORITAIREMENT AU NIVEAU DES TISSUS ADIPEUX BLANC ET BRUN, INDUISANT UNE AUGMENTATION DE LA LIPOLYSE ET DE LA THERMOGENESE RESPECTIVEMENT, CONSTITUENT DONC DES CIBLES PHARMACOLOGIQUES INTERESSANTES DANS LE TRAITEMENT DU DNID. POUR CETTE RAISON, NOUS AVONS ETUDIE LES EFFETS DE MOLECULES ORIGINALES, PRESENTANT A LA FOIS UNE ACTIVITE AGONISTE 3 ADRENERGIQUE ET UNE ACTIVITE ANTIOXYDANTE. NOTRE TRAVAIL A TOUT D'ABORD CONSISTE A CARACTERISER LE POUVOIR ANTIOXYDANT DE CES MOLECULES (TEST D'ELLMAN ET DES TBARS). LA PLUS ACTIVE DE CES MOLECULES, M 26, A ETE UTILISEE AFIN D'APPRECIER SON EFFET POTENTIEL PROTECTEUR VIS-A-VIS DE CELLULES FIBROBLASTIQUE (HELA WILD ET TAT) EN CULTURE, SOUMISES A UN STRESS OXYDANT. CETTE ETUDE A <b>MALHEUREUSEMENT</b> MIS EN EVIDENCE UN EFFET CYTOTOXIQUE DE M 26 ALORS QUE LES PRECURSEURS DE LA MOLECULE EN SONT DENUES. PAR AILLEURS, LES EFFETS VASCULAIRES DE M 26 ONT ETE ETUDIES. LES AGONISTES 3 ADRENERGIQUES PRESENTENT EN EFFET UNE CAPACITE VASORELAXANTE, PROPRIETE BENEFIQUE POUR LA PRESERVATION DE L'ARBRE VASCULAIRE CHEZ LE DIABETIQUE. L'EFFET ANTIOXYDANT DE M 26 AINSI QUE SON EFFET AGONISTE 3 ADRENERGIQUE A ETE TESTE SELON DEUX PROTOCOLES SUR DES PREPARATIONS D'ANNEAUX D'AORTE ISOLEE DE RAT. DANS CES CONDITIONS EXPERIMENTALES, NOUS AVONS PU METTRE EN EVIDENCE UN LEGER EFFET AGONISTE 3 ADRENERGIQUE MAIS PAS D'EFFET ANTIOXYDANT VIS-A-VIS DU PEROXYDE D'HYDROGENE ADMINISTRE PAR VOIE EXOGENE. CES TRAVAUX DOIVENT ETRE POURSUIVIS AFIN DE DEFINIR ET DE S'AFFRANCHIR DE LA CYTOTOXICITE DE M 26, CONDITION NECESSAIRE A LA POURSUITE DE SON ETUDE. GRENOBLE 1 -BU Sciences (384212103) / SudocSudocFranceF...|$|E
40|$|International audienceThe Vietoris–Rips {{filtration}} is {{a versatile}} tool in topological data analysis. It is {{a sequence of}} simplicial complexes built on a metric space to add topological structure to an otherwise disconnected set of points. It is widely used because it encodes useful information about the topology of the underlying metric space. This information is of-ten extracted from its so-called persistence diagram. Unfortunately, this filtration is often too large to construct in full. We show how to construct an O(n) -size filtered simplicial complex on an n-point metric space such that its persistence diagram is a good approximation {{to that of the}} Vietoris–Rips filtration. This new filtration can be constructed in O(n log n) time. The constant factors in both the size and the run-ning time depend only on the doubling dimension of the metric space and the desired tightness of the approximation. For the first time, this makes it computationally tractable to approximate the persistence di-agram of the Vietoris–Rips filtration across all scales for large data sets. We describe two different sparse filtrations. The first is a zigzag filtration that removes points as the scale increases. The second is a (non-zigzag) filtration that yields the same persistence diagram. Both methods are based on a hierarchical net-tree and yield the same guar-antees. La filtration de Vietoris-Rips est un outil très versatile en analyse topologique des données. C'est une séquence de complexes simpliciaux construits sur une métrique pour ajouter de la structure topologique à un nuage de points. <b>Malheureusement,</b> cette filtration est souvent trop large pour tenir entièerement en mémoire. Nous montrons comment construire un complexe simplicial filtré de taille O(n) à partir d'un espace métrique fini composé de n points, de manièere à ce que le diagramme de persistance de ce complexe filtré soit une bonne approximation de celui de la filtration de Vietoris-Rips...|$|E
40|$|LES SIMULATIONS LES REQUIERENT LE DEVELOPPEMENT ET L'UTILISATION DE MODELES DE SOUS-MAILLE POUR DECRIRE L'EFFET DES ECHELLES TURBULENTES NON-RESOLUES. AU COURS DE NOTRE ETUDE, NOUS AVONS PROPOSE UNE NOUVELLE FORMULATION FONDEE SUR L'EQUATION DE TRANSPORT DE LA VARIABLE D'AVANCEMENT C. CETTE GRANDEUR, BIEN DEFINIE, PEUT ETRE FACILEMENT EXTRAITE DE SIMULATIONS NUMERIQUES DIRECTES (TESTS A PRIORI) OU MESUREE EXPERIMENTALEMENT (TESTS A PRIORI POUR DEVELOPPER DES MODELES OU A POSTERIORI POUR VALIDER LES SIMULATIONS). L'EQUATION DE TRANSPORT DE LA VARIABLE D'AVANCEMENT EST FILTREE AVEC UN FILTRE LES PLUS LARGE QUE LE MAILLAGE DE CALCUL POUR POUVOIR RESOUDRE NUMERIQUEMENT LA VARIABLE D'AVANCEMENT FILTREE $. CETTE OPERATION INTRODUIT DES GRANDEURS NON FERMEES QU'IL FAUT MODELISER. TOUTES CES GRANDEURS ONT ETE ETUDIEES, A LA FOIS ANALYTIQUEMENT, A PARTIR DE SIMULATIONS NUMERIQUES DIRECTES ET A PARTIR DE MESURES EXPERIMENTALES. CETTE ANALYSE NOUS A CONDUIT A PROPOSER UNE FORMULATION BASEE SUR LA NOTION DE DENSITE DE SURFACE DE FLAMME DE SOUS-MAILLE. UN POINT IMPORTANT CONCERNE LA CAPACITE DU MODELE PROPOSE A PREDIRE LA PROPAGATION D'UNE FLAMME LAMINAIRE PLANE. IL APPARAIT ALORS QUE LE FLUX CONVECTIF NON RESOLU COMPORTE UNE CONTRIBUTION LAMINAIRE QUI NE PEUT ETRE NEGLIGEE. UN TERME DIFFUSIF A EGALEMENT ETE INCORPORE A NOTRE MODELE POUR GERER, DANS LES SIMULATIONS PRATIQUES, L'EPAISSEUR DE LA FLAMME RESOLUE. FINALEMENT, NOTRE MODELE A ETE IMPLANTE DANS LE CODE AVBP. TOUT D'ABORD, LA CAPACITE DU MODELE A DEGENERER CORRECTEMENT VERS LES SITUATIONS LAMINAIRES A ETE VERIFIEE PAR LA SIMULATION D'UNE FLAMME PLANE, MODIMENSIONNELLE, STATIONNAIRE. DES SIMULATIONS BI-DIMENSIONNELLES ONT ENSUITE ETE CONDUITES ET ONT MONTRE UN BON COMPORTEMENT DU MODELE, COMPARATIVEMENT A L'APPROCHE BASEE SUR L'EPAISSISSEMENT ARTIFICIEL DE LA FLAMME. LES DONNEES EXPERIMENTALES QUANTITATIVES DISPONIBLES POUR VALIDER UN TEL MODELE SONT, <b>MALHEUREUSEMENT,</b> ENCORE PEU NOMBREUSES, MAIS LES PREMIERS TESTS S'AVERENT TRES PROMETTEURS. IL FAUT SOULIGNER QUE CES CALCULS LES ONT PERMIS DE METTRE EN EVIDENCE L'APPARITION, DANS CERTAINS CAS, DE TRANSPORT TURBULENT DE TYPE CONTRE-GRADIENT, AU MOINS A L'ECHELLE RESOLUE, MALGRE L'UTILISATION D'UNE MODELISATION DE TYPE GRADIENT DES FLUX NON-RESOLUS. CHATENAY MALABRY-Ecole {{centrale}} (920192301) / SudocSudocFranceF...|$|E
40|$|This is {{a record}} of a conference {{of most of the}} active Canadian workers in the field of Soil Mechanics held in the Council Chamber of the National Research Council of Canada in Ottawa on 28 and 29 April, 1947, {{under the auspices of the}} Associate Committee on Soil and Snow Mechanics. The meeting was held at this time in order to {{coincide}} with the visit to Canada of Mr. L. F. Cooling, head of the Soil Mechanics Section of the British Building Research Station, Garston, near London, England. Unfortunately, Mr. Cooling was unwell on arrival in Canada and was therefore able to attend only one session of the conference. He was represented throughout by his assistant, Mr. Meyerhof, who contributed to the proceedings of the meeting as will be been later. Nous donnons ici le compte-rendu d?un congr 8 s int 9 ressant la majorit 9 des travailleurs canadiens qui sont actifs dans le domaine de la m 9 canique des sols, qui s?est tenu 0 la salle du conseil du Conseil national de recherches du Canada, 0 Ottawa, les 28 et 29 avril 1947, sous les auspices du Comit 9 associ 9 de la m 9 canique des sols et de la neige. L? 9 v 9 nement s?est d 9 roul 9 0 un moment qui co avec la visite au Canada de Monsieur L. F. Cooling, chef de la section de m 9 canique des sols de la British Building Research Station, 0 Garston, pr 8 s de Londres. <b>Malheureusement,</b> M. Cooling ayant 9 t 9 indispos 9 0 son arriv 9 e au Canada, il n?a donc pu n?assister qu? 0 une seule s 9 ance du congr 8 s, et a 9 t 9 repr 9 sent 9 autrement par son assistant, M. Meyerhof, lequel a contribu 9 aux d 9 lib 9 rations de la r 9 union, comme nous le verrons plus loin. Peer reviewed: NoNRC publication: Ye...|$|E
40|$|LES LOGICIELS DE CAO PERMETTENT AUJOURD'HUI DE DECRIRE LES OBJETS GEOMETRIQUES PAR UN ENSEMBLE DE SPECIFICATIONS, APPELEES CONTRAINTES GEOMETRIQUES. UN PROGRAMME, APPELE SOLVEUR, SE CHARGE ENSUITE DE RESOUDRE LES CONTRAINTES ET AINSI CALCULE LA -OU UNE- GEOMETRIE SOLUTION. LE TRAVAIL PRESENTE EXPLORE DEUX MODELISATIONS GEOMETRIQUES NON CARTESIENNES. LA MODELISATION DECLARATIVE EST TOUT D'ABORD DEFINI ET SES AVANTAGES ET INCONVENIENTS SONT IDENTIFIES. L'ACCENT EST PRINCIPALEMENT PORTE SUR LA DIFFICULTE DU CONTROLE DE LA COHERENCE ET DE LA NON-AMBIGUITE DU SYSTEME DE CONTRAINTES. L'OBJECTIF DE LA THESE EST L'ETUDE DE LA VALIDITE DU SYSTEMES DE CONTRAINTES ET POUR CELA LES CONTRAINTES DUES AUX PROPRIETES DE L'ESPACE EUCLIDIEN DOIVENT ETRE EXPLICITEES. UNE ETUDE CRITIQUE DE 4 MODELES DE CONTRAINTES, L'UN ISSU DU PROJET GPS DE ISO, DEUX AUTRES DU PROJET STEP DE L'ISO ET LE DERNIER, DE LA THEORIE SATT DEVELOPPEE AU LISMMA PERMET DE PROPOSER UN MODELE D'INFORMATION ORIGINAL. UNE NOUVELLE CONTRAINTE D'EXISTENCE PERMET D'EXPLICITER QUE L'OBJET A CONSTRUIRE EXISTE ALORS QUE LA CONTRAINTE DE PLONGEMENT PERMET D'EXPLICITER QU'IL APPARTIENT A UN ESPACE DE DIMENSION D. DE PLUS, LE NOUVEAU CONCEPT DE CONTRAINTE DE CHIRALITE EST INTRODUIT POUR DISTINGUER SI DEUX OBJETS SONT DE MEME SENS OU DE SENS OPPOSE ET IL EST AJOUTE LA DISTINCTION ENTRE ELEMENT GEOMETRIQUE COURANT (QUI APPARTIENT A LA SURFACE) ET ELEMENT GEOMETRIQUE DE SITUATION (PAR EXEMPLE LE CENTRE D'UNE SPHERE). UN PREMIER MODELE GEOMETRIQUE NON CARTESIEN TRADUIT TOUTES LES CONTRAINTES GEOMETRIQUES EN CONTRAINTES PORTANT UNIQUEMENT SUR DES DISTANCES ENTRE DES POINTS. IL EST A NOTER QUE LES TRAVAUX DE DE TILLY SONT, POUR LA MAJEURE PARTIE, A L'ORIGINE DES RELATIONS GENEREES ICI. <b>MALHEUREUSEMENT,</b> EN L'ETAT ACTUEL CETTE MODELISATION EST PRATIQUEMENT INUTILISABLE DU FAIT DE LA QUANTITE EXPONENTIELLE DE RELATIONS A GENERER. LE SECOND MODELE GEOMETRIQUE NON CARTESIEN TRADUIT TOUTES LES CONTRAINTES GEOMETRIQUES EN EQUATIONS ET INEQUATIONS A PARTIR DES PROPRIETES DU TENSEUR METRIQUE. EN PARTICULIER, LA CONTRAINTE D'EXISTENCE CORRESPOND AU FAIT QU'UN TENSEUR METRIQUE EST UNE FORME DEFINIE POSITIVE ET LA CONTRAINTE DE PLONGEMENT QUE LE RANG DU TENSEUR METRIQUE EST EGAL A LA DIMENSION DE L'ESPACE QU'IL DEFINIT. EN L'ETAT, CETTE MODELISATION GENERE N 2 / 2 RELATIONS. CHATENAY MALABRY-Ecole {{centrale}} (920192301) / SudocSudocFranceF...|$|E
40|$|Le Discours historique de l’estat du royaume de Borno, écrit par un ancien captif de Tripoli en 1685, est l’une des sources les plus importantes de l’histoire de ce royaume sahélien situé sur la rive ouest du lac Tchad. <b>Malheureusement,</b> son analyse fut délaissée du fait d’incohérences, et l’intérêt de la source fut dévalué. Cependant, une étude du contexte de rédaction du manuscrit ainsi que de la {{construction}} du récit montre la grande richesse de ce texte. Tout en s’inscrivant dans un contexte favorable à la mise à l’écrit de l’expérience de captivité, celui que nous appellerons le « chirurgien français » effectue un réel travail de recherche tant à Tripoli qu’à son retour en France, où il rédige avec l’aide d’une tierce personne ce chapitre dans un ouvrage bien plus volumineux sur l’histoire de Tripoli. Le plus intéressant reste la façon dont le « chirurgien français » a recueilli ses sources : leur recensement permet un premier classement de celles-ci et une première analyse de l’apport du Discours historique de l’estat du royaume de Borno pour l’histoire moderne du sultanat de Borno. The Discours historique de l’estat du royaume de Borno, {{written by}} a former French slave in Tripoli in 1685, {{is one of the}} most important sources for the history of Borno sultanate. Unfortunately, its study has been neglected due to inconsistencies between the manuscript and other sources. The manuscript was then considered as a second hand source. However, a close study of the context where the manuscript was conceived and written shows that we can deeply learn from it. The work of whom we call the « French surgeon », comes within the scope of a context favorable for writing the experience of captivity. It is the consequence of the author’s intense works of research in Tripoli and in France. The « French surgeon » wrote this chapter in a voluminous manuscript on the history of Tripoli with the reviews of a second author. The most interesting part of his work remains the way how the « French surgeon » collected his sources : their census is the occasion to propose their first classification, and a first overview of the contribution of the Discours historique de l’estat du royaume de Borno for the modern history of Borno sultanate...|$|E
40|$|Testing is an {{more and}} more {{important}} activity of the software development process. As testing activity is often tedious, {{the aim of the}} work presented in this thesis is to de ne how to make the engineer free from the most repetitious tasks of the test synthesis activity. Our approach, within the framework of conformance testing, is {{based on the fact that}} there are various levels of abstraction to de ne tests : executable tests relative to a given technology and abstract tests independent of technology. Our work is focused on two research themes. The rst one aims to reduce the e orts required to generate the tests. In that purpose, we de ne a new abstraction level : test schemas, which o ers an additional abstraction to the worked instances and values. The Tobias tool has been developed during the thesis to help to the testing schemas generation, to unfold them in abstract test cases, and then to concretise these test cases. The second contribution of our work aims to restrain the problem of combinatorial explosion of the tests number, relative to the principles of the language of tests schema. So, we propose various techniques, rst to control the number of generated tests and then to optimize the execution time of these tests. We have integrated these various techniques into the Tobias tool and realized two case studies that validate our work. Notre approche, dans le cadre du test de conformité, se base sur le fait qu'il existe différents niveaux d'abstraction pour définir des tests : les tests exécutables pour une cible technologique et les tests abstraits qui sont indépendants de la technologie. Nos travaux portent sur deux points. Le premier point vise à réduire l'effort alloué à la conception des tests. Pour cela nous définissons un nouveau niveau d'abstraction : les schémas de test qui offrent une abstraction supplémentaire sur les instances et valeurs manipulées. L'outil Tobias a été développé au cours de la thèse pour aider à la conception des schémas de test, les déplier en cas de test abstraits, puis concrétiser ces cas de test. <b>Malheureusement,</b> les principes du langage de schéma de tests entraînent un problème d'explosion combinatoire du nombre de tests. Nous proposons donc diverses approches pour, d'une part, mieux contrôler le nombre de tests produits et, d'autre part, optimiser le temps d'exécution des tests. Ces diverses techniques ont pu être intégrées à l'outil Tobias, nous permettant de réaliser deux études de cas afin de valider notre approche...|$|E
40|$|Les enrochements sont des matériaux granulaires constitués de blocs rocheux dont la taille peut atteindre {{plusieurs}} dizaines de centimètres. Les barrages en enrochements, constitués par ces matériaux grossiers, présentent des déformations relativement importantes au cours du temps et peuvent également tasser au moment de leur remplissage ou lors d une crue accidentelle. Ces déformations semblent liées à des ruptures de blocs à l intérieur de l ouvrage, mais ne sont <b>malheureusement</b> pas connues après la construction. Le comportement des matériaux granulaires étant fortement lié à {{la nature}} discrète du milieu, un modèle discret est proposé afin de prendre en compte les particularités des enrochements avec des paramètres locaux au sens physique clair. Ce travail a donc consisté à développer un modèle numérique discret capable de prendre en compte la fissuration progressive et différée des blocs rocheux. La modélisation adoptée est de type "Non Smooth Contact Dynamics"Rockfill {{are made by}} coarse materials, blocks, whose size can reach one meter. Rockfill dams, composed by these blocks, show some relatively important deformations during time, during their filling or during an accidental flood. These deformations seems {{to be related to}} blocks ruptures in the inner structure, but they are unfortunately unknown after the dam construction. The settlements prediction is then particularly important to ensure the structural integrity of the dams all along their life. The granular media behaviour is strongly related to the discrete nature of the medium. A discrete model is proposed to take into account rockfill particularities with local parameters which have a clear physical sense. Ruptures are rockfill major features. The major idea in this work, consisted in developing a numerical discrete model to account for progressive and delayed blocks failure. Each block is considered as an assembly of particles with initial cohesive bonds which can decrease progressively during the loading. A damaging interface model is proposed to describe this progressive phenomenon. The model is composed by two yield surfaces (damage and fracture yield surfaces). A characteristic time is related to the damage evolution. The water influence is introduced by coupling the parameters of the cohesion model, by introducing the buoyancy forces and by decreasing the local friction. The modelling adopted is based on the "Non Smooth Contact Dynamics Method", where grains and particles are rigid. The model is implemented on the discrete element code, LMGC 90. Numerical simulations are realised at contacts, block and rockfill scales. First simulations consisted in modelling an assembly of unbreakable blocks disposed in a rockfill column and progressively filled by water. These simulations verify that blocks fracture is the major phenomenon responsible of the settlements of rockfill dams. Simulations of compression of breakable blocks and oedometric compression of an assembly of these blocks are realised. Numerical simulations of oedometric compression tests composed by rockfill reveal some creep and relaxation responses. These tests reproduce the basic phenomena with only few parameters. AIX-MARSEILLE 1 -BU Sci. St Charles (130552104) / SudocSudocFranceF...|$|E
40|$|Is Banat a {{sensitive}} cross-border region? Examining {{relations at the}} borders in South-East Europe Politically, {{the end of the}} transition period, which started after the breakdown of the Communist regime, was marked by the accession of many Central and South-East European states to the European Union (EU). One of the worries at that time were the “risky” areas situated on the states’ borders of South-East Europe and which were felt as having been “imposed” from outside, such as the Banat. The sensitive feature of borders - or of border regions – is based in particular on a distance criterion, whether physical or imagined, at the border. It can be illustrated here using the example of the Danube-Kris-Mures-Tisza (DKMT) Euroregion which reunites the territory of the historic region of Banat: the increased distance makes the border less tangible. Is the border perceived to be unbridgeable from this territory? The Banat Region represents a space within which it is difficult to envisage any form of interaction across the border, for an hour’s drive is needed to meet each other and there are hardly any shared interests in cooperating. The regional area of Banat is sensitive because of its history, the overlapping of ethno-cultural and/or confessional groups, as well as due to the territorial and administrative divisions resulting from the grouping together of states within the EU and NATO. This text describes a complex situation, from which emanate initiatives which could solve potential risks at the borders through cooperation between the inhabitants of these border areas. The local level leads to more effective cooperation. The fact is that sensitivities connected to history persist: Hungary and Serbia have still not resolved their conflict which resulted from interethnic conflicts in Yugoslavia; Romania and Hungary are sensitive to any position taken by their neighbour on issues that the authorities of the other country consider to be part of their internal affairs. Even if, little by little, the border becomes a shared space, it will never be shared in the same way by each citizen as each has different  capacities for connecting to the border through their activities, through mobility, or through their linguistic and cultural knowledge. On this basis, new sensitivities can come to the surface, but cross-border networks can also be formed. (extrait de l'introduction) Au sens défini par la géographie durant la décennie 1990, l’Europe du Sud-Est et à travers elle ses États, pouvaient être lus, ainsi que l’ensemble de l’Europe centrale, comme une Europe « entre-deux »[1]. Cette expression souligne la relative incertitude du moment quant au devenir géopolitique et socio-économique de cet espace en transition. Elle sous-entend une situation ou position intermédiaire entre deux constructions politiques territoriales, entre deux horizons, deux temps [...] . Ce qui inquiète à l’époque est, entre autres, le devenir des espaces « à risques » que constituent des frontières d’États qui sont lues le plus souvent comme ayant été imposées depuis l’extérieur, principalement à l’issue de la Première Guerre mondiale, et qui sont longtemps restées closes à la mobilité du plus grand nombre des citoyens de ces États, y compris des résidents des espaces frontaliers. La remise en cause de certains tracés frontaliers ou de la légitimité d’autrui à résider au sein de l’espace découpé par les frontières sera <b>malheureusement</b> confirmée et illustrée par les conflits yougoslaves ainsi que par de multiples refontes territoriales, ayant eu cours ailleurs, dans l’Europe postcommuniste. Mais ces horizons semblent aujourd’hui dépassés [...] .   [1]        REY, V., « L’Europe centre orientale, un entre-deux », dans BAILLY, A., FERRAS, R., PUMAIN, D., (dir.), Encyclopédie de géographie, Paris, 1992, pp. 813 - 826. </p...|$|E
40|$|The {{exponential}} increasing of {{the number}} of images requires efficient ways to classify them based on their visual content. The most successful and popular approach is the Bag of visual Word (BoW) representation due to its simplicity and robustness. Unfortunately, this approach fails to capture the spatial image layout, which plays an important roles in modeling image categories. Recently, Lazebnik et al (2006) introduced the Spatial Pyramid Representation (SPR) which successfully incorporated spatial information into the BoW model. The idea of their approach is to split the image into a pyramidal grid and to represent each grid cell as a BoW. Assuming that images belonging to the same class have similar spatial distributions, it is possible to use a pairwise matching as similarity measurement. However, this rigid matching scheme prevents SPR to cope with image variations and transformations. The main objective of this dissertation is to study a more flexible string matching model. Keeping the idea of local BoW histograms, we introduce a new class of edit distance to compare strings of local histograms. Our first contribution is a string based image representation model and a new edit distance (called SMD for String Matching Distance) well suited for strings composed of symbols which are local BoWs. The new distance benefits from an efficient Dynamic Programming algorithm. A corresponding edit kernel including both a weighting and a pyramidal scheme is also derived. The performance is evaluated on classification tasks and compared to the standard method and several related methods. The new method outperforms other methods thanks to its ability to detect and ignore identical successive regions inside images. Our second contribution is to propose an extended version of SMD replacing insertion and deletion operations by merging operations between successive symbols. In this approach, the number of sub regions ie. the grid divisions may vary according to the visual content. We describe two algorithms to compute this merge-based distance. The first one is a greedy version which is efficient but can produce a non optimal edit script. The other one is an optimal version but it requires a 4 th degree polynomial complexity. All the proposed distances are evaluated on several datasets and are shown to outperform comparable existing methods. L'augmentation exponentielle du nombre d'images nécessite des moyens efficaces pour les classer en fonction de leur contenu visuel. Le sac de mot visuel (Bag-Of-visual-Words, BOW), en raison de sa simplicité et de sa robustesse, devient l'approche la plus populaire. <b>Malheureusement,</b> cette approche ne prend pas en compte de l'information spatiale, ce qui joue un rôle important dans les catégories de modélisation d'image. Récemment, Lazebnik ont introduit la représentation pyramidale spatiale (Spatial Pyramid Representation, SPR) qui a incorporé avec succès l'information spatiale dans le modèle BOW. Néanmoins, ce système de correspondance rigide empêche la SPR de gérer les variations et les transformations d'image. L'objectif principal de cette thèse est d'étudier un modèle de chaîne de correspondance plus souple qui prend l'avantage d'histogrammes de BOW locaux et se rapproche de la correspondance de la chaîne. Notre première contribution est basée sur une représentation en chaîne et une nouvelle distance d'édition (String Matching Distance, SMD) bien adapté pour les chaînes de l'histogramme qui peut calculer efficacement par programmation dynamique. Un noyau d'édition correspondant comprenant à la fois d'une pondération et d'un système pyramidal est également dérivée. La seconde contribution est une version étendue de SMD qui remplace les opérations d'insertion et de suppression par les opérations de fusion entre les symboles successifs, ce qui apporte de la souplesse labours et correspond aux images. Toutes les distances proposées sont évaluées sur plusieurs jeux de données tâche de classification et sont comparés avec plusieurs approches concurrente...|$|E
40|$|Cet article fait le point des {{connaissances}} actuelles sur la variabilité génétique du taux d’ovulation (TO) et de la survie embryonnaire (SE). Les 2 caractères présentent une forte étendue de variation entre races (8 à 10 ovules et 20 à 25 points de pourcentage pour TO et SE respectivement). Les effets d’hétérosis direct sont peu importants (2 à 3 % pour TO; 1 à 2 % pour SE). Par contre, la survie embryonnaire présente un effet d’hétérosis maternel important (environ 8 %). L’héritabilité du taux d’ovulation est relativement élevée (0, 30). Celle de la survie embryonnaire est moins bien connue, mais semble comparable ou légèrement supérieure à celle de la prolificité (environ 0, 15). La corrélation génétique entre TO et SE est négative, les liaisons avec la prolificité étant par contre positives. Les quelques estimations disponibles ne permettent <b>malheureusement</b> pas de préciser l’importance de ces relations. Des liaisons avec le système majeur d’histocompatibilité et la présence de translocations réciproques ont par ailleurs été mises en évidence. D’autres sont suspectées avec le groupe sanguin H et le locus de la transferrine. Les possibilités d’utilisation de ces composantes pour l’amélioration génétique de la prolificité sont ensuite discutées. Des perspectives semblent exister, mais l’imprécision de nos connaissances sur la variabilité génétique de TO et SE et les problèmes de modélisation de la survie embryonnaire ne permettent pas, pour l’instant,de conclure. The {{objective of}} this study was to determine the effect of disease on milk production and their relationships with milking characteristics and milking career. This work was carried out at the same time as a feed trial (comparison of feed based on hay or silage with two different amounts of concentrate) which lasted 6 years and concerned 487 lactations of 190 "Pie-Noire" and "Montbéliarde" cows. During the 487 lactations, 595 health problems were observed, which concerned 59 % of these lactations, at an average rate of 2. 1 per lactation. Lameness and mastitis represented 52 and 24 %, respectively, of all infections. 47 % of mastitis occured during the first two months of lactation, whereas the frequency of lameness increased throughout winter to reach a maximum when the animals were turned out to pasture. In the short term (5 weeks), the greatest loss in production was caused by winter mastitis (24 kg) and particularly lameness arising from animals being turned out to pasture (56 kg). Throughout the milk producing period it is repeated bouts of lameness which cause the greatest loss in milk production : 640 kg of milk for cows having had 3 lamenesses or more as opposed to 20 kg for those having had only one lameness and 160 kg for those with one or several mastitis infections. Repeated lameness was 3 times greater in "Pie-Noire" cows than in "Montbéliardes" and 4 times more frequent in cows having received winter feed based on silage than those receiving hay based feed. Four main classes of milk production can be distinguished based on the amount of milk produced, animal health, their reproduction and finally, their culling. The differences in milk production between the 4 groups reached 1 800 kg of milk per lactation. Cows receiving grass silage feed with a small amount of concentrate had a shorter milking career than those on other types of feed (2. 5 as compared to 3. 2 - 3. 5 lactations). Cows with serious health problems (multiple lameness) in their first lactation had a much shorter milking career (1. 1 lactation) than cows that were healthy in their first lactation. On average, milk production and live weight increased, respectively, 622 and 51 kg, between the first and the third lactation. This increase in milk production was greater in cows receiving winter fodder based on hay (high or low amount of concentrate) or based on grass silage with a high amount of concentrate (+ 752 kg) than those being fed on rations of grass silage with a low amount of concentrate (+ 359 kg). Over 3 lactations, the greatest difference in milk production between the groups (hay + high amount of concentrate, silage + low amount of concentrate) reached 2 770 kg thus a difference of 26 %. These findings demonstrate that certain conclusions based on results obtained during winter or from lactation must be reconsidered...|$|E
40|$|Current {{areas of}} research, such as {{ubiquitous}} and cloud computing, consider execution environments {{to be in}} a constant state of change. Dynamic applications [...] where components can be added, removed and substituted during execution [...] allow software to adapt and adjust to changing environments, and to accommodate evolving features. Unfortunately, dynamic applications raise design and development issues that have yet to be fully addressed. In this dissertation we show that dynamism is a crosscutting concern that breaks many of the assumptions that developers are otherwise allowed to make in classic applications. Dynamism deeply impacts software design and development. If not handled correctly, dynamism can silently corrupt the application. Furthermore, writing dynamic applications is complex and error-prone, and given the level of complexity and the impact dynamism has on the development process, software cannot become dynamic without (extensive) modification and dynamism cannot be entirely transparent (although much of it may often be externalized or automated). This work focuses on giving the software architect control over the level, the nature and the granularity of dynamism that is required in dynamic applications. This allows architects and developers to choose where the efforts of programming dynamic components are best spent, avoiding the cost and complexity of making all components dynamic. The idea is to allow architects to determine the balance between the efforts spent and the level of dynamism required for the application's needs. At design-time we perform an impact analysis using the architect's requirements for dynamism. This serves to identify components that can be corrupted by dynamism and to [...] at the architect's disposition [...] render selected components resilient to dynamism. The application becomes a well-defined mix of dynamic areas, where components are expected to change at runtime, and static areas that are protected from dynamism and where programming is simpler and less restrictive. At runtime, our framework ensures the application remains consistent [...] even after unexpected dynamic events [...] by computing and removing potentially corrupt components. The framework attempts to recover quickly from dynamism and to minimize the impact of dynamism on the application. Our work builds on recent Software Engineering and Middleware technologies [...] namely, OSGi, iPOJO and APAM [...] that provide basic mechanisms to handle dynamism, such as dependency injection, late-binding, service availability notifications, deployment, lifecycle and dependency management. Our approach, implemented in the Robusta prototype, extends and complements these technologies by providing design and development-time support, and enforcing application execution consistency in the face of dynamism. Les domaines de recherche actuels, tels que l'informatique ubiquitaire et l'informatique en nuage (cloud computing), considèrent que ces environnements d'exécution sont en changement continue. Les applications dynamiques; où les composants peuvent être ajoutés, supprimés pendant l'exécution, permettent a un logiciel de s'adapter et de s'ajuster à l'évolution des environnements, et de tenir compte de l'évolution du logiciel. <b>Malheureusement,</b> les applications dynamiques soulèvent des questions de conception et de développement qui n'ont pas encore été pleinement explorées. Dans cette thèse, nous montrons que le dynamisme est une préoccupation transversale qui rompt avec un grand nombre d'hypothèses que les développeurs d'applications classiques sont autorisés à prendre. Le dynamisme affecte profondément la conception et développement de logiciels. S'il n'est pas manipulé correctement, le dynamisme peut " silencieusement " corrompre l'application. De plus, l'écriture d'applications dynamiques est complexe et sujette à erreur. Et compte tenu du niveau de complexité et de l'impact du dynamisme sur le processus du développement, le logiciel ne peut pas devenir dynamique sans (de large) modification et le dynamisme ne peut pas être totalement transparent (bien que beaucoup de celui-ci peut souvent être externalisées ou automatisées). Ce travail a pour but d'offrir à l'architecte logiciel le contrôle sur le niveau, la nature et la granularité du dynamisme qui est nécessaire dans les applications dynamiques. Cela permet aux architectes et aux développeurs de choisir les zones de l'application où les efforts de programmation des composants dynamiques seront investis, en évitant le coût et la complexité de rendre tous les composants dynamiques. L'idée est de permettre aux architectes de déterminer l'équilibre entre les efforts à fournir et le niveau de dynamisme requis pour les besoins de l'application...|$|E
40|$|Depuis le début des années 90, le réseau de la santé au Québec est soumis à une vaste {{restructuration}} qui a eu des conséquences négatives sur la qualité de vie au travail (QVT) des infirmières et infirmiers. Les hommes se retrouvent en nombre croissant dans toutes les sphères de la pratique infirmière, mais les études existantes ne font <b>malheureusement</b> pas mention de la qualité de vie au travail de ceux-ci. Alors, il apparaît pertinent de s’attarder au phénomène de la qualité de vie au travail des hommes infirmiers dans la profession infirmière, et ce, plus précisément en CSSS mission CLSC. Le but de cette étude phénoménologique consiste à décrire et à comprendre la signification de la qualité de vie au travail pour des infirmiers œuvrant en CSSS mission CLSC. L’essence du phénomène, les huit thèmes et les 35 sous-thèmes qui se dégagent directement des entrevues énoncent que la signification de la qualité de vie au travail pour des infirmiers œuvrant en centre de santé et des services sociaux (CSSS), mission CLSC et déclarant avoir une qualité de vie positive au travail, signifie « un climat empreint de caring qui favorise l'épanouissement de l'infirmier en CLSC en œuvrant pour le maintien de l'harmonie entre les sphères professionnelle et familiale ». Si certains résultats corroborent ceux d’études antérieures, d’autres apportent des éléments nouveaux favorisant la santé des infirmiers par le biais de la qualité de vie au travail. Enfin, des avenues concrètes visant {{la mise en place}} de programmes d’optimisation de la qualité de vie au travail, sont proposées. In the 1990 s, {{health care}} organizations in Québec underwent sweeping reforms that disrupted the work climate {{and practices of}} nurses (Bourbonnais et al., 2000; Pérodeau et al., 2002). These reforms had a negative impact on nurses’ quality of working life (QWL), leading decision makers and researchers to investigate the QWL phenomenon from several perspectives (Delmas, 1999; 2001; Gascon, 2001; O’Brien-Pallas & Baumann, 1992). Most of the studies of this phenomenon were conducted in hospital settings and were based on paradigms of psychological distress (Bourbonnais et al., 1998, 2000) or burn-out (a pathogenic perspective) (Duquette et al., 1995) rather than a health paradigm (a salutogenic perspective) (Gascon, 2001). A salutogenic perspective represents a positive vision of an approach to health (Antonovsky, 1996; Delmas, 2001, Duquette & Delmas, 2002). The scientific literature (Brooks et al., 1996; Ekstrom, 1999; Evans, 2001) suggests that, in addition to living through the same upheavals as their female colleagues, some male nurses also have negative feelings related to sex discrimination, feelings of isolation, and the conflict between masculine values (strength, aggressiveness) and the feminine values (gentleness, flexibility) of the nursing profession. These feelings can only hamper nurses’ QWL, yet they are not mentioned in studies of male nurses (Boughn, 2001; Ekstrom, 1999; Evans, 1997, 2001). The aim of the study, using Giorgi’s (1985, 1997 a) descriptive phenomenology as a method, is to describe and understand the significance of phenomena through people’s experiences. The findings were derived from semi-structured individual interviews of 60 to 90 minutes with five male nurses who reported a positive quality of life at work. Data analysis consisted of: collecting the data, reading and rereading the results, dividing the data into meaningful units, organizing and stating the raw data {{in the language of the}} discipline and, finally, synthesizing the findings and letting the essence of the phenomena emerge. Watson’s (1988, 2005) human caring philosophy served as a backdrop for the entire process. The analysis of verbatim transcripts revealed eight themes that defined the significance of the quality of working life for male nurses practising in community settings: 1) autonomy in their professional practice; 2) job satisfaction; 3) a healthy workplace setting; 4) relations with the administration characterized by support and respect; 5) caring relationships with other members of the interdisciplinary team; 6) working in partnership with female peers; 7) commitment to clients and their families; and 8) professional work-life balance. The essence of the phenomenon stems directly from the themes that emerged during the interviews; it states that for male nurses working in health and social service centres (CSSSs), as part of CLSCs, working life means “a caring climate that fosters the vitality of male CLSC nurses by trying to maintain a balance between their professional and family lives. ” If some of the findings confirm what has been reported in other studies, others have added new information on how to promote the health of male nurses by targeting quality of working life. Concrete avenues are proposed for implementing quality of working life optimization programs...|$|E
40|$|The early Upper Palaeolithic in Haute-Normandie, {{and more}} {{generally}} in northern France, is poorly known. The industries from Saint-Martin-Osmonville and Epouville are particularly interesting {{because the first}} analyses conducted during the 1970 s and 1980 s led to {{the conclusion of a}} possible Châtelperronian for the first site and a mixture of Aurignacian and Mousterian levels for the second one. The stratigraphic position of the industries could be an issue. In Saint-Martin-Osmonville, stratigraphic studies in the 1980 s set the industry in the Nagelbeek level (22000 BP), usually sterile in this area. Recent data have shown the existence of an important hiatus covering the previous period of time, circa 70000 - 30000 BP. At Epouville, the archaeological level is systematically associated with a thin layer of "grumeleux" [gritty] alluvium which does not always rest on the same underlying deposits. More generally, this layer tops the Mesnil-Esnard stratum, which can be changeable, dilated, divided and sometimes non-existent. The industry, included in the immediately posterior alluvium, might be dated back to 45000 - 40000 BP. The "série 3 " from Saint-Martin-Osmonville consists of 1134 knapped flints. Even though the patinas may vary, the assemblage remains technologically homogeneous. There are few laminar products as opposed to the number of points. The use of a softer hammer has been attested on some pieces. The study of operational schemata shows a conceptual continuity with the production of Levallois flakes and laminar production. The knapping method using unipolar sequences dominates the others. The knapping evolves progressively from a facial to a semi-rotating technique. The management of flaking surface convexities can vary greatly. There are few retouched elements, thus it is difficult to propose cultural interpretations. Unlike A. Michel's conclusions, there are no Upper Palaeolithic type tools, there are no unfinished Châtel-perronian points. This assemblage may present some Châtelperronian features including laminar products, a more rotating knapping technique, the use of a softer hammer [...] . But some elements are missing, partly understood operational schemata (the beginning of core exploitation is missing), rare tools and few specific ones [...] . Nevertheless, it seems that the assemblage belongs to the Middle Palaeolithic. In Epouville, the technological analysis has shown the presence of two different archaeological levels, in which two major assemblages can be found. The most important assemblage, also know as "série grise " [grey series], consists of 1034 artefacts. Its main features are a grey patina as well as variable gloss; it has been interpreted as Mousterian. The second one, also called " série vert-marron " [green-brown series], consists of 655 artefacts. Its main features are a brownish-green patina and an important gloss. This assemblage has been extremely disturbed by frost, and has been interpreted as Aurignacian. This second assemblage was the matter under discussion. The "série vert-marron " is characterized by varied production — on the one hand, we have long, regular flakes (first generation striking), and on the other we have shorter, less regular elements (second generation). These two types of production belong to the same evolutional operational schema (a few cores show successive traces of the two sequences). The first laminar knapping is done according to the semi-rotating unipolar method. The operating table is shaped by either lateral crests or a frontal crest. The use of " neo-crested " blades is frequent. The softer hammer is the sole hammer used for this operational schema. As for second generation elements, hard hammer percussion is obvious. Their knapping corresponds to a more opportunistic management of the core. The toolmaker always uses the same method (unipolar), but in a less sophisticated way and with irregular striking. There are few retouched elements, with no sign of the Aurignacian. The " série vert-marron " seems to belong to the early Upper Palaeolithic {{but it is difficult to}} be precise. The lack of technological and typological references for these times in Northern France makes it difficult for us to propose a precise interpretation of this series. La phase ancienne du Paléolithique supérieur en Haute-Normandie, et d'une manière générale dans l'ensemble du Nord de la France, est méconnue. Les industries de Saint-Martin-Osmonville et Epouville sont particulièrement intéressantes à cet égard car les premières analyses dont elles firent l'objet aboutissaient, pour l'une, à un éventuel Châtelperronien et, pour l'autre, à un mélange de couches aurignacienne et moustérienne. La difficulté d'interprétation de ces séries s'exprime par l'absence de calages stratigraphiques fiables et par la faiblesse numérique des enlèvements retouchés. Seule une analyse technologique suffisamment précise pouvait nous permettre d'affiner les hypothèses d'attribution culturelle. <b>Malheureusement,</b> le manque de referents technologiques et typologiques pour le Paléolithique supérieur ancien de la partie septentrionale de la France restreint considérablement le champ de nos investigations. Guette Caroline. Le Paléolithique supérieur ancien en Haute-Normandie ? Etat de la recherche à travers l'étude technologique de deux sites du pays de Caux : Saint-Martin-Osmonville/la Salle et Epouville/la briqueterie Dupray (Seine-Maritime, France). In: Bulletin de la Société préhistorique française, tome 101, n° 4, 2004. pp. 781 - 795...|$|E
40|$|Due to its {{prominent}} directionality and strength, H-bonds {{are ones}} {{of the most}} widely used non-covalent interactions in supramolecular chemistry. Despite its relative high strength (energy of an H-bond in the gas phase typically ranges between 0 − 5 Kcal mol− 1) in comparison with other non-covalent interactions, association of two molecules by means of a single H-bond leads to complexes displaying low thermodynamical stabilities, thus limiting their exploitation in the non-covalent synthesis of functional materials for real-world applications. Thereby, when stronger interactions are required, the general engineering approach focuses on the covalent synthesis of rigid planar molecular scaffolding in which several H-bonding donating (D) and accepting (A) moieties are arranged into a so-called ‘H-bonding array’. Due to the selective recognition processes and to the tunability of their association strength, multiple H-bonding arrays have become an indispensable molecular module in the tool-box of supramolecular chemists, allowing, through selective self-assembly and/or self-organization processes, the bottom-up preparation of functional materials such as liquid crystals, patterned surfaces and supramolecular polymers. In principle, the stability of H-bonded supramolecular complexes could be modulated in an indefinite number of ways. For example, when stronger interactions (e. g., higher association constant values) are required, the increase of the number of the H-bonding sites represents one of the efficient strategy to reinforce the stability of the ultimate assembly. Nevertheless, a strong Ka value is not always requested. In fact, whilst highly stable complexes are required in the field of supramolecular polymers, whose properties at the molecular level (such as degree of polymerization, Dp, and viscosity) result linearly correlated to the Ka values, these may instead be detrimental for the construction of more sophisticated hierarchized nano-architectures, arising from a delicate interplay between internal (e. g. ii stacking, solvophobic/solvophilic interactions) and external (e. g. time, temperature, concentration, etc.) factors. The aim of this thesis is to design and synthesize novel triple H-bonding arrays (DAD, ADD and DDD) based on five-membered heteroaromatic rings. The proposed use of thiolyl, oxolyl, azolyl, and triazolyl scaffoldings for recognition systems, it is intended as a mean to better achieve the control on the binding properties and selectivity of triple H-bondind recognition arrays, allowing an easy tunability of the binding motifs. With the variation of the substituents and the heteroatom onto the hetero-aromatic rings, it has been intended to create a selection of versatile, structurally similar, host-guest pairs complexes that display different association constants (Ka) in order to better match the requirements of different supramolecular applications. Focusing on the most relevant factors that influence the association constants of hydrogen bonded complexes, {{in the first part of}} Chapter 1 the reader is introduced on how specific H-bonding arrays, featuring wide ranges of Ka values (spanning among eight orders of magnitude) can be designed. Subsequently, the second part is focused on the physical and chemical properties of a large variety of H-bonding assembled molecular modules that upon self-assembly and self-organization processes opened new ways towards novel fascinating applications. Figure 1 Designed H-bonding arrays based on 5 -membered heterocycles. Chapter 2 deals with the description of the synthetic efforts undertaken towards the preparation of the DAD and DDD H-bonding arrays. The first two subsections (2. 1 - 2) describe the rethrosynthetic approaches and the results of the unsuccessful methodological routes (through Buchwald-Hartwig amidation cross-coupling reactions, reduction of azido-derivatives and nucleophilic addition of organo-metallic reagents to isocyanate derivatives as produced through Curtius rearrangement) tackled to introduce amidic and/or ureidic functions at the 2 -position of five-membered heteroaromatic rings. Several DAD H-bonding arrays based on thiolyl scaffolding were successfully synthesized (sections 2. 3). Figure 2 Synthesized thiolyl DAD H-bonding arrays. In section 2. 4 are presented the synthetic step undertaken in the attempt to generate DAD arrays based on oxalyl derivatives. Unfortunately the introduction of electron-donating groups such as amidic or carbamic functions to the ring led to very unstable intermediates, and thus the amido-oxolyl derivatives capable of recognition mediated by triple H bonding were never isolated. Figure 3 Synthesized oxolyl-protected DAD H-bond array. The synthetic strategies towards the synthesis of DDD arrays based on of azolyl scaffolding are described in section 2. 5. Protected azolyl module 182 (see Figure 4) was synthesized in thirteen steps starting from the pyrrole module. Unfortunately, due to the complications encountered in the cleavage of the N-azolyl protecting group, the synthesis of the azolyl DDD H-bonding arrays based could not be finally accomplished. Figure 4 Synthesized azolyl-protected DDD H-bond array. Section 2. 6 presents the synthesis of newly designed self-adapting ADD/DDD H-bonding array based on ureido-triazolyl scaffoldings. Exploiting the prototropic equilibrium of the triazole nucleus the modules synthesized are expected to show an ADD or a DDD arrangement of the binding sites depending on the H-bonding functionalities of the complementary guest used for the complexation. Figure 5 Synthesized triazolyl-based ureido H-bonding arrays. Prototropic self-adapting properties: from a DDD to a ADD H-bonding array. Due to solubility limitations in common organic solvents (e. g., CDCl 3 and CD 2 Cl 2), the molecular recognition ability in solution could not be studied and further modifications of the molecular structural properties are required. Grâce à leur directionnalité proéminente ainsi qu’à leur force, les ponts hydrogène sont l’une des interactions non-covalentes les plus exploitées en chimie supramoléculaire. Malgré leur force relativement élevée en comparaison avec d’autres interactions non-covalentes (l’énergie d’un pont hydrogène en phase gazeuse se situe typiquement entre 0 et 5 Kcal mol- 1), l’association de deux molécules au moyen d’un pont hydrogène simple mène à des complexes présentant des stabilités thermodynamiques faibles, limitant ainsi leur exploitation dans la synthèse non-covalente de matériaux fonctionnels menant à de réelles applications. Dès lors, lorsque des interactions plus fortes sont nécessaires, l’approche d’ingénierie habituelle se concentre sur la synthèse covalente d’échafaudages moléculaires rigides et planaires dans lesquels plusieurs fonctionnalités donneurs (D) et accepteurs (A) de ponts hydrogènes sont arrangées dans ce que l’on appelle un ‘réseau de ponts hydrogène’. Grâce aux procédés de reconnaissances spécifiques et au contrôle de leur force d’association, les réseaux de ponts hydrogène multiples sont devenus des modules moléculaires indispensables dans la boîte à outils des chimistes supramoléculaires, permettant, via des procédés sélectifs d’auto-assemblage et/ou auto-organisation, la préparation bottom-up de matériaux fonctionnels tels que des cristaux liquides, des surfaces à motifs et des polymères supramoléculaires. En principe, la stabilité des complexes supramoléculaires à base de ponts hydrogène peut être modulée d’un nombre infini de manières. Par exemple, lorsque des interactions plus fortes (e. g., des valeurs de constante d’association élevées) sont requises, augmenter le nombre de sites de ponts hydrogène représente une stratégie efficace pour renforcer la stabilité de l’assemblage final. Néanmoins, une valeur élevée de Ka n’est pas toujours nécessaire. En effet, alors que des complexes hautement stabilisés sont requis dans le domaine des polymères supramoléculaires, dont les propriétés à l’échelle moléculaire (tel que le degré de polymérisation Dp, et la viscosité) corrèlent linéairement aux valeurs des Ka, ces dernières peuvent également être néfastes à la construction de nano-architectures hiérarchisées plus sophistiquées, résultant d’une interaction délicate entre des facteurs internes (comme par exemple du π−π stacking, des interactions solvophobiques/solvophiliques) et externes (tels que le temps, la température, la concentration etc.). L’objectif de cette thèse est l’élaboration et la synthèse de nouveaux réseaux de ponts hydrogène (DAD, ADD, DDD) basés sur des cycles hétéroatomiques à cinq chaînons. Nous proposons l’utilisation d’échafaudages thiolyl, oxolyl, azolyl, et triazolyl comme systèmes de reconnaissance, afin de réaliser un meilleur contrôle des propriétés de liaison et de la sélectivité des réseaux de reconnaissance à ponts hydrogène triples, permettant ainsi une transformation aisée des motifs de liaison. La variation des substituants et de l’hétéroatome sur les cycles hétéroaromatiques permettrait de créer une sélection de complexes de paires host-guest versatiles et de structure similaire, présentant ainsi différentes constantes d’association (Ka) pour mieux répondre aux différentes demandes d’applications supramoléculaires. Se concentrant sur les facteurs les plus pertinents qui influencent les constantes d’association des complexes à base de ponts hydrogène, le lecteur est introduit dans la première partie du Chapitre 1 sur la façon dont peuvent être élaborés des réseaux spécifiques à base de ponts hydrogène présentant une large gamme de valeurs de Ka (s’étendant sur un ordre de huit de magnitude). Ensuite, la deuxième partie traite des propriétés physiques et chimiques d’une grande variété d’assemblages par ponts hydrogène de modules moléculaires qui après auto-assemblage et auto-organisation ont ouvert de nouvelles voies à l’élaboration de nouvelles applications fascinantes. Figure 1 Réseaux de ponts hydrogène élaborés sur base d’hétérocycles à 5 membres. 	Le Chapitre 2 présente la description des efforts synthétiques réalisés pour la préparation des réseaux de ponts hydrogène DAD et DDD. Les deux premières sous-sections (2. 1 - 2) décrivent les approches rétrosynthétiques et les résultats des voies méthodologiques non fructueuses (via des réactions de cross-coupling d’amidation de Buchwald-Hartwig, réduction de dérivés azido et addition nucléophile de réactifs organométalliques à des dérivés isocyanate comme produit par le réarrangement de Curtius) entreprises pour introduire les fonctions amidiques et/ou uréidiques en position 2 des cycles hétéroaromatiques à cinq chaînons. 	Plusieurs réseaux de ponts hydrogène DAD à base de thiolyl ont été synthétisés avec succès (section 2. 3). Figure 2 Réseaux de ponts hydrogène DAD synthétisés, protégés par du thiolyl. Dans la section 2. 4 sont présentées les étapes synthétiques réalisées pour générer les réseaux DAD à base de dérivés oxalyl. <b>Malheureusement,</b> l’introduction sur le cycle de groupements électrodonneurs tels que des fonctions amidiques ou carbamiques a mené à des intermédiaires très instables. Dès lors, les dérivés amido-oxolyl capables de reconnaissance via des ponts hydrogène triples n’ont jamais été isolés. Figure 3 Réseau de ponts hydrogène DAD oxolyl-protégé synthétisé. Les stratégies synthétiques menant à la synthèse des réseaux DDD à base d’azolyl sont décrites dans la section 2. 5. Le module protégé par le groupemement azolyl 182 (voir Figure 4) a été synthétisé en treize étapes à partir du module pyrrole. <b>Malheureusement,</b> à cause de complications rencontrées lors du clivage du groupement protecteur N-azolyl, la synthèse des réseaux de ponts hydrogènes DDD protégés par l’azolyl n’a finalement pu être réalisée. Figure 4 Réseau de ponts hydrogène DDD azolyl-protégé synthétisé. La Section 2. 6 présente la synthèse de réseaux de ponts hydrogène auto-adaptant ADD/DDD, nouvellement élaborés à partir d’échafaudages ureido-triazolyl. Grâce à l’équilibre prototropique du noyau triazole, nous nous attendons à ce que les modules synthétisés présentent un arrangement des sites de liaisons ADD ou DDD, selon les fonctionnalités ponts hydrogène des guest complémentaires utilisés pour la complexation. Figure 5 Réseaux de ponts hydrogène ureido synthétisés à base de triazole. Propriétés auto-adaptantes prototropiques: d’un réseau ponts hydrogène DDD à ADD. 	Dû aux limitations de solubilité dans les solvants organiques communs (tels que CDCl 3 et CD 2 Cl 2), la capacité de reconnaissance moléculaire en solution n’a pu être étudiée. De plus amples modifications des propriétés moléculaires structurelles sont dès lors nécessaires. (DOCSC 02) [...] FUNDP, 201...|$|E
40|$|Modern {{real-time}} systems {{tend to be}} mixed-critical, in {{the sense}} that they integrate on the same computational platform applications at different levels of criticality. Integration gives the advantages of reduced cost, weight and power consumption, which can be crucial for modern applications like Unmanned Aerial Vehicles (UAVs). On the other hand, this leads to major complications in system design. Moreover, such systems are subject to certification, and different criticality levels needs to be certified at different level of assurance. Among other aspects, the real-time scheduling of certifiable mixed critical systems has been recognized to be a challenging problem. Traditional techniques require complete isolation between criticality levels or global certification to the highest level of assurance, which leads to resource waste, thus loosing the advantage of integration. This led to a novel wave of research in the real-time community, and many solutions were proposed. Among those, one of the most popular methods used to schedule such systems is Audsley approach. However this method has some limitations, which we discuss in this thesis. These limitations are more pronounced in the case of multiprocessor scheduling. In this case priority-based scheduling looses some important properties. For this reason scheduling algorithms for multiprocessor mixed-critical systems are not as numerous in literature as the single processor ones, and usually are built on restrictive assumptions. This is particularly problematic since industrial real-time systems strive to migrate from single-core to multi-core and many-core platforms. Therefore we motivate and study a different approach that can overcome these problems. A restriction of practical usability of many mixed-critical and multiprocessor scheduling algorithms is assumption that jobs are independent. In reality they often have precedence constraints. In the thesis we show the mixed-critical variant of the problem formulation and extend the system load metrics to the case of precedence-constraint task graphs. We also show that our proposed methodology and scheduling algorithm MCPI can be extended to the case of dependent jobs without major modification and showing similar performance with respect to the independent jobs case. Another topic we treated in this thesis is time-triggered scheduling. This class of schedulers is important because they considerably reduce the uncertainty of job execution intervals thus simplifying the safety-critical system certification. They also simplify any auxiliary timing-based analyses that may be required to validate important extra-functional properties in embedded systems, such as interference on shared buses and caches, peak power dissipation, electromagnetic interference etc [...] The trivial method of obtaining a time-triggered schedule is simulation of the worst-case scenario in event-triggered algorithm. However, when applied directly, this method is not efficient for mixed-critical systems, as instead of one worst-case scenario they have multiple corner-case scenarios. For this reason, it was proposed in the literature to treat all scenarios into just a few tables, one per criticality mode. We call this scheduling approach Single Time Table per Mode (STTM) and propose a contribution in this context. In fact we introduce a method that transforms practically any scheduling algorithm into an STTM one. It works optimally on single core and shows good experimental results for multi-cores. Finally we studied the problem of the practical realization of mixed critical systems. Our effort in this direction is a design flow that we propose for multicore mixed critical systems. In this design flow, as the model of computation we propose a network of deterministic multi-periodic synchronous processes. Our approach is demonstrated using a publicly available toolset, an industrial application use case and a multi-core platform. Les systèmes temps-réels modernes ont tendance à obtenir la criticité mixte, dans le sens où ils intègrent sur une même plateforme de calcul plusieurs applications avec différents niveaux de criticités. D'un côté, cette intégration permet de réduire le coût, le poids et la consommation d'énergie. Ces exigences sont importantes pour des systèmes modernes comme par exemple les drones (UAV). De l'autre, elle conduit à des complications majeures lors de leur conception. Ces systèmes doivent être certifiés en prenant en compte ces différents niveaux de criticités. L'ordonnancement temps réel des systèmes avec différents niveaux de criticités est connu comme étant l’un des plus grand défi dans le domaine. Les techniques traditionnelles nécessitent une isolation complète entre les niveaux de criticité ou bien une certification globale au plus haut niveau. Une telle solution conduit à un gaspillage des ressources, et à la perte de l’avantage de cette intégration. Ce problème a suscité une nouvelle vague de recherche dans la communauté du temps réel, et de nombreuses solutions ont été proposées. Parmi elles, l'une des méthodes la plus utilisée pour ordonnancer de tels systèmes est celle d'Audsley. <b>Malheureusement,</b> elle a un certain nombre de limitations, dont nous parlerons dans cette thèse. Ces limitations sont encore beaucoup plus accentuées dans le cas de l'ordonnancement multiprocesseur. Dans ce cas précis, l'ordonnancement basé sur la priorité perd des propriétés importantes. C’est la raison pour laquelle, les algorithmes d'ordonnancement avec différents niveaux de criticités pour des architectures multiprocesseurs ne sont que très peu étudiés et ceux qu’on trouve dans la littérature sont généralement construits sur des hypothèses restrictives. Cela est particulièrement problématique car les systèmes industriels temps réel cherchent à migrer vers plates-formes multi-cœurs. Dans ce travail nous proposons une approche différente pour résoudre ces problèmes...|$|E
40|$|L'utilisation de l'ozone, aujourd'hui très répandue dans les filières de potabilisation, n'est pas sans effet secondaire. De nombreux sous-produits peuvent se former comme notamment les ions bromates, sous produits finaux d'oxydation des bromures contenus dans les eaux. <b>Malheureusement,</b> le mécanisme de {{production}} de cette espèce est complexe et dépend de nombreux paramètres difficiles à appréhender. Sur une installation pilote de type colonne à bulles fonctionnant à contre-courant, nous avons étudié l'influence de différents paramètres, comme le pH, {{le temps}} de contact, la dose d'ozone et la dose de peroxyde d'hydrogène, sur la formation des bromates et la dégradation des pesticides, représentée par l'atrazine. Les résultats de la littérature ont été confirmés lors de l'emploi unique de l'ozone. La formation des ions bromate est influencée par la présence du peroxyde d'hydrogène. Cet oxydant intervient de manière non négligeable sur la consommation des entités intermédiaires. Le couple HOBr/OBr- peut être oxydé par l'ozone moléculaire et le radical OH° mais peut également être réduit par l'ozone et par le peroxyde sous sa forme acide ou sa base conjuguée. En ce qui concerne la dégradation des pesticides, l'utilisation de peroxyde d'hydrogène couplé à l'ozone favorise l'oxydation de la molécule d'atrazine grâce à la présence plus importante de radicaux hydroxyles. Une pollution accidentelle en pesticides pourra être traitée par l'ajout ponctuel de peroxyde d'hydrogène avec une augmentation de pH, la formation des bromates sera, dans ce cas, faible. La désinfection sera alors assurée par l'étape de chloration. In drinking water treatment plants, ozonation {{is often used}} to disinfect, to remove micropollutants and to improve water taste and odour. Ozonation increases organic matter biodegradability before filtration through granular active carbon and reduces the concentration of haloform precursors that react in the final chlorination step. However, by-products that could be detrimental to human health could be formed. For example, bromates, which are classified as carcinogenic compounds by the I. A. R. C, are produced during the ozonation of bromide-containing water. The mechanism of bromate formation is complex, due to the participation of molecular ozone and radical (hydroxyl and carbonate) reactions. The optimisation of the process should allow for a good disinfection and a reduction in the levels of micropollutants, together with low by-product formation. Using a pilot-scale counter-current bubble column, we have measured the bromate concentration in relation to pesticide removal. Water spiked with bromide and atrazine was stored in a completely stirred-tank (2 m 3) before being pumped {{to the top of the}} column. The inlet gaseous ozone was measured by an analyser using UV detection, the outlet gaseous ozone was monitored by the potassium iodide method, and the dissolved ozone concentration was determined by the indigo trisulfonate method. Bromides and bromates were quantified by ion chromatography with a conductimetric detector, with a sodium carbonate solution as the eluant. Samples for bromate analysis were pretreated by OnGuard-Ag and OnGuard-H cartridges in series before injection. Atrazine degradation was measured by high performance liquid chromatography with a diode array detector, with a CH 3 CN/H 2 O mixture as the eluant. The linearisation of atrazine removal allowed us to calculate the hydroxyl radical concentration in a series of a completely-stirred tank reactors and in a plug-flow reactor. We have studied the influence of several parameters on bromate formation, including pH, bromide concentration and hydrogen peroxide concentration. As bromate production is a function of bromide concentration, we have chosen to calculate the ratio between the real bromate concentration and the theoretical bromate concentration if all bromide were oxidised to bromate. The pH affects bromate formation: an increase in pH in the absence of hydrogen peroxide increases bromate production, but when this oxidant is applied bromate production decreases when the pH increases. If reaction progress is represented as a function of [O 3]*TC, we note that the presence of hydrogen peroxide increases bromate formation because of the increase in hydroxyl radical concentration, which favours radical formation. Nevertheless, if we represent reaction progress as a function of [OH∘]*TC, hydrogen peroxide seems to be an initiator and a scavenger in the mechanism of bromate formation. If we calculate the rates of all the oxidation and reduction reactions for HOBr/OBr- species, the contribution to the reduction of HOBr/OBr- species by peroxide is very important in comparison to the oxidation reactions, which inhibits bromate production. Without the hydrogen peroxide, the contribution of oxidation is equal to that of the reduction reaction, and in this case bromate formation is effective. When, under the same initial operational conditions, we apply hydrogen peroxide with an increase in pH, we observe a decrease in bromate formation with a decrease of the dissolved ozone concentration, which hinders the desired disinfection. The main contribution to atrazine oxidation is from the free-radical reactions, which explains why removal is better when we apply hydrogen peroxide than when we use ozone alone. However, if we want to respect a low bromate level in drinking water, atrazine degradation should not be greater than 90 % for the operational conditions on our pilot-scale. If an accidental high pesticide concentration is observed, an addition of hydrogen peroxide with a concurrent increase of pH, could treat the pollution. In this case, a subsequent chlorination step would then have to be used to assure the disinfection alone...|$|E
40|$|Divers modèles, ayant pour but de prédire le taux d'imbrûlés solides lors de la {{combustion}} du fuel lourd, ont été mis au point dans le passé. Les paramètres entrant en ligne de compte sont le plus souvent les teneurs en résidus lourds hydrocarbonés (asphaltènes précipités au pentane ou à l'heptane et carbone Conradson) et en métaux : c'est le cas des modèles Exxon et Shell développés respectivement en 1979 et 1981. D'autres modèles tiennent compte, {{en plus de}} la composition du fuel, de son mode d'atomisation, de son mode de diffusion dans le foyer et de la cinétique de combustion : on peut citer les travaux du Laboratoire Energie du MIT publiés en 1986. Néanmoins, ces facteurs ne sont pas les seuls à intervenir : l'expérience a montré que l'état de dispersion des asphaltènes peut jouer également un grand rôle, notamment dans le cas d'installations de combustion à injection mécanique, pour lesquelles la dispersion des gouttelettes n'est pas aussi fine que pour des installations munies d'une injection assistée par la vapeur. Cette influence de la dispersion des asphaltènes sur la combustion a été mise en évidence dans le passé par l'utilisation d'additifs dispersants et également par la combustion de fuels lourds constitués par dilution d'asphaltes précipités au pentane avec un gas-oil de cracking catalytique de raffinerie (LCO). Ce sont ces fuels que l'on a considérés dans la présente étude. L'effet de ce facteur dispersion n'a pas été quantifié jusqu'alors, la difficulté étant de définir une grandeur mesurable représentant la répartition des agglomérats d'asphaltènes. Dans cette étude, on a essayé en un premier temps de faire une approche fractale de la répartition des asphaltènes à partir de clichés (préparés par la société Total), cette méthode ayant déjà été utilisée avec succès pour décrire des structures d'aspects comparables. <b>Malheureusement,</b> on s'est heurté à des difficultés relevant du mode d'exploration et de la non adéquation entre les structures asphalténiques et fractales. On a finalement opté pour une détermination visuelle s'appuyant sur les clichés sur lesquels les agglomérats d'asphaltènes sont clairement visualisés tels qu'ils sont dans le fuel. Ce mode d'exploration laborieux a cependant permis de déterminer un modèle construit sur une série de 25 fuels dont 10 ont été brûlés sur une chaudière de 2 MW, et 15 sur un four de 100 kW. Ce modèle fait intervenir les teneurs en carbone Conradson et en métaux, ainsi que le taux de dispersion des asphaltènes. Le perfectionnement des moyens d'exploration aidant, on peut s'attendre à ce que soient disponibles des techniques d'évaluation de la dispersion sur les clichés. Ce paramètre pourra alors être pris en considération pour une meilleure prédiction de résultats de combustion insuffisamment expliqués avec les paramètres classiques. Various models {{aiming to}} predict the amount of unburned particles (solids) during heavy fuel-oil combustion have been developed. The parameters taken into consideration are generally asphaltenes precipitated by normal heptane or pentane and Conradson carbon {{as well as the}} metals content having a known catalytic effect on cenosphere combustion in the combustion chamber. The Exxon and Shell models can be mentioned, which were developed respectively in 1979 and 1981 (Chapter II). Other models also give consideration to the fuel-oil composition, the way it is atomized and diffused in the chamber and the combustion kinetics (research done by the MIT Energy Laboratory published in 1986). However, the above parameters {{are not the only ones}} involved. For some fuel oils, experience has shown that the state of dispersion of asphaltenes may also play an important role particularly for combustion installations with mechanical injection for which the dispersion of fuel-oil droplets is not very great and does not affect the structures built up by asphaltene aggregates. This influence of asphaltene dispersion on combustion was revealed in the past by the use of dispersant additives, and more recently in the combustion of heavy fuel oils made up by the dilution of pentane-precipited asphalts with a light cycle oil (LCO). These fuel oils are considered in this article because a divergence has been found between prediction and the measurement of solid unburned hydrocarbons as the result of a more or less dispersed state of asphaltenes, depending on the conditions of diluted-asphalt preparation with a fixed fuel oil/LCO ratio. The goal has thus been to add on a term representative of this state of dispersion to the terms normally considered (asphaltenes, Conradson carbon, metals). To assess the state of dispersion of asphaltenes in fuel oils, pictures implying a special preparation of sample (taken by Total) were examined. These photos give a fairly representative picture of aggregate distribution in the fuel oil. To assess this dispersion, a fractal approach, which had already been applied successfully to describe structures with comparable aspects, was tried, but we came up against difficulties stemming from the exploration method and from the unmatching of asphaltenic and fractal structures. We finally chose a visual determination based on the photos in which the asphaltene agglomerates are clearly represented as they occur in the fuel oil (set of photos in the article). This laborious exploration method (liable to be replaced by image-scanning software) nevertheless enabled a more complete model to be designed for this type of production. This model was based on a serie of 25 fuel oils, ten of which were burned in a 2 MW boiler and 15 in a 100 IkW furnace. The characteristics of these fuel oils are given in Table I. The designations tau s and tau v represent the rates of surface and volume dispersion of the fuel oils expressed respectively in agg/µm² and agg/µm 3 (agg = agglomerates). Table II has to do with the nature of the fuel oils used (origin of the crude and method of preparation). The prospection methodology in creating the overall model is diagramed in a flowchart. The two series of fuel oils were burned respectively in a 2 MW boiler and a 100 kW furnace. The aim was to reduce them to a single serie of 25 measurement so that the model would have greater significance. The particles emission between the two installations were appreciably different, and so they had to be made comparable by a passage equationbetween the actual values obtained for the 2 MW boiler and the fictive values predicted by the intermediate model used (model of the 100 kW furnace). With the help of the passage equation, we were thus able to build a model on all the data. The model concerning the 100 kW furnace was taken as the intermediate model, and thus the values predicted by the overall model are directly applicable to the 100 kW furnace. However, the reverse passage equationhad to be applied to the fictive predicted values for the 2 MW boiler to obtain the particle emission corresponding to this latter installation...|$|E
40|$|This {{document}} {{constitutes a}} synthesis {{in preparation for}} my habilitation degree in computer science. I am now researcher at INRIA Rennes since September 2001. From September 2001 to January 2004, I was a researcher in the Vista project headed by Patrick Bouthemy, {{and moved to the}} Visages project headed by Christian Barillot. In January 2010, I moved to the Serpico team headed by Charles Kervrann that focuses on ”imaging and modeling intracellular dy- namics of molecular architectures”. This document presents part of my work among the Visages team. Actually, this habilitation thesis will focus on image processing aspects of intraoperative ultrasound in neurosurgery. My work on non-rigid registration will not be described here. The work on non-rigid registration began during my PhD thesis, where the three main contributions were the design of a 3 D non-rigid registration method based on optical flow [72], the incorporation of local constraints [74] and the retrospec- tive evaluation of inter-subject registration [71]. I continued working on image registration, with Anne Cuzol and Etienne M ́emin using fluid motion descrip- tion [44], with Nicolas Courty on GPU accelerated registration [42] and on evaluation of non-rigid registration techniques: with Mallar Chakravarty and co-authors [29] for deep-brain stimulation planning; with Arno Klein and co- authors [92] concerning inter-subject brain registration. In the last decade, it has become increasingly common to use image-guided navigating systems to assist surgical procedures [51]. The reported benefits are improved accuracy, reduced intervention time, improved quality of life, reduced morbidity (and perhaps mortality), reduced intensive care and reduced hospital costs. Image-guided systems can help the surgeon plan the operation and provide accurate information about the anatomy during the intervention. Image-guided systems are also useful for minimally invasive surgery, since the intraoperative images can be used interactively as a guide. Current surgical procedures rely on complex preoperative planning, includ- ing various multimodal examinations: anatomical, vascular, functional explo- rations for brain surgery. Once all information has been merged, it can be used for navigation in the operating theatre (OR) using image-guided surgery systems. Image-guided surgery involves the rigid registration of the patient's body with the preoperative data. With an optical tracking system, and Light Emitting Diodes (LED), it is possible to track the patient's body, the micro- scope and the surgical instruments in real time. The preoperative data can then be merged with the surgical field of view displayed in the microscope. This fusion is called “augmented reality”. Unfortunately, the assumption of a rigid registration between the patient's body and the preoperative images only holds {{at the beginning of the}} procedure. This is because soft tissues tend to deform during the intervention. This is a common problem in many image-guided interventions, the particular case of neurosurgical procedures can be considered as a representative case. When dealing with neurosurgery, his phenomenon is called “brain shift”. Although the impact and the magnitude of soft tissue motion have been studied over the last few years, this phenomenon is still poorly understood. Soft tissue deformation can be explained by physiological (steroids, diuretic medication, mechanical ventilation) and mechanical factors (CSF leakage, pa- tient positioning, tumor nature and location, craniotomy size, gravity [117], etc). The magnitude of brain shift shows striking differences at each stage of surgery. Brain shift must be considered as a spatio-temporal phenomenon, and should be estimated continuously, or at least at key moments, to update the preoperative planning. To do so, one possibility is to deform the anatomical and functional images according to the estimated deformation. Ce document constitue une synth`ese de travaux de recherche en vue de l'obten- tion du diplˆome d'habilitation `a diriger les recherches. A la suite ce cette in- troduction r ́edig ́ee en franc ̧ais, le reste de ce document sera en anglais. Je suis actuellement charg ́e de recherches INRIA au centre de Rennes Bretagne Atlantique. J'ai rejoint en Septembre 2001 l' ́equipe Vista dirig ́ee par Patrick Bouthemy, puis l' ́equipe Visages dirig ́ee par Christian Barillot en Janvier 2004. Depuis Janvier 2010, je travaille dans l' ́equipe-projet Serpico dirig ́ee par Charles Kervrann dont l'objet est l'imagerie et la mod ́elisation de la dynamique intra- cellulaire. Parmi mes activit ́es pass ́ees, ce document va se concentrer uniquement sur les activit ́es portant sur la neurochirurgie guid ́ee par l'image. En parti- culier, les travaux effectu ́es sur le recalage non-rigide ne seront pas pr ́esent ́es ici. Concernant le recalage, ces travaux ont commenc ́e pendant ma th`ese avec le d ́eveloppement d'une m ́ethode de recalage 3 D bas ́e sur le flot optique [72], l'incorporation de contraintes locales dans ce processus de recalage [74] et la validation de m ́ethodes de recalage inter-sujets [71]. J'ai poursuivi ces travaux apr`es mon recrutement avec Anne Cuzol et Etienne M ́emin sur la mod ́elisation fluide du recalage [44], avec Nicolas Courty sur l'acc ́el ́eration temps-r ́eel de m ́ethode de recalage [42], et sur l' ́evaluation des m ́ethodes de recalage dans deux contextes : celui de l'implantation d' ́electrodes profondes [29] et le re- calage inter-sujets [92]. L'utilisation de syst`emes dits de neuronavigation est maintenant courante dans les services de neurochirurgie. Les b ́en ́efices, attendus ou report ́es dans la litt ́erature, sont une r ́eduction de la mortalit ́e et de la morbidit ́e, une am ́elio- ration de la pr ́ecision, une r ́eduction de la dur ́ee d'intervention, des couˆts d'hospitalisation. Tous ces b ́en ́efices ne sont pas `a l'heure actuelle d ́emontr ́es `a ma connaissance, mais cette question d ́epasse largement le cadre de ce doc- ument. Ces syst`emes de neuronavigation permettent l'utilisation du planning chirurgical pendant l'intervention, dans la mesure ou` le patient est mis en cor- respondance g ́eom ́etrique avec les images pr ́eop ́eratoires `a partir desquelles est pr ́epar ́ee l'intervention. Ces informations multimodales sont maintenant couramment utilis ́ees, com- prenant des informations anatomiques, vasculaires, fonctionnelles. La fusion de ces informations permet de pr ́eparer le geste chirurgical : ou` est la cible, quelle est la voie d'abord, quelles zones ́eviter. Ces informations peuvent main- tenant ˆetre utilis ́ees en salle d'op ́eration et visualis ́ees dans les oculaires du mi- croscope chirurgical grˆace au syst`eme de neuronavigation. <b>Malheureusement,</b> cela suppose qu'il existe une transformation rigide entre le patient et les im- ages pr ́eop ́eratoires. Alors que cela peut ˆetre consid ́er ́e comme exact avant l'intervention, cette hypoth`ese tombe rapidement sous l'effet de la d ́eformation des tissus mous. Ces d ́eformations, qui doivent ˆetre consid ́er ́ees comme un ph ́enom`ene spatio-temporel, interviennent sous l'effet de plusieurs facteurs, dont la gravit ́e, la perte de liquide c ́ephalo-rachidien, l'administration de pro- duits anesth ́esiants ou diur ́etiques, etc. Ces d ́eformations sont tr`es difficiles `a mod ́eliser et pr ́edire. De plus, il s'agit d'un ph ́enom`ene spatio-temporel, dont l'amplitude peut varier consid ́era- blement en fonction de plusieurs facteurs. Pour corriger ces d ́eformations, l'imagerie intra-op ́eratoire apparait comme la seule piste possible...|$|E
40|$|The {{world is}} {{day by day}} more computerized. There {{is more and more}} {{software}} running everywhere, from personal computers to data servers, and inside most of the new popularized inventions such as connected watches or intelligent washing machines. All of those technologies use software applications to perform the services they are designed for. Unfortunately, the number of software errors grows with the number of software applications. In isolation, software errors are often annoyances, perhaps costing one person a few hours of work when their accounting application crashes. Multiply this loss across millions of people and consider that even scientific progress is delayed or derailed by software error: in aggregate, these errors are now costly to society as a whole. There exists two techniques to deal with those errors. Bug fixing consists in repairing errors. Resilience consists in giving an application the capability to remain functional despite the errors. This thesis focuses on bug fixing and resilience in the context of exceptions. Exceptions are programming language constructs for handling errors. They are implemented in most mainstream programming languages and widely used in practice. We specifically target two problems:Problem # 1 : There is a lack of debug information for the bugs related to exceptions. This hinders the bug fixing process. To make bug fixing of exceptions easier, we will propose techniques to enrich the debug information. Those techniques are fully automated and provide information about the cause and the handling possibilities of exceptions. Problem # 2 : There are unexpected exceptions at runtime {{for which there is no}} error-handling code. In other words, the resilience mechanisms against exceptions in the currently existing (and running) applications is insufficient. We propose resilience capabilities which correctly handle exceptions that were never foreseen at specification time neither encountered during development or testing. In this thesis, we aim at finding solutions to those problems. We present four contributions to address the two presented problems. In Contribution # 1, we lies the foundation to address both problems. To improve the available information about exceptions, we present a characterization of the exceptions (expected or not, anticipated or not), and of their corresponding resilience mechanisms. We provide definitions about what is a bug when facing exceptions and what are the already-in-place corresponding resilience mechanisms. We formalize two formal resilience properties: source-independence and pure-resilience as well as an algorithm to verify them. We also present a code transformation that uses this knowledge to enhance the resilience of the application. Contribution # 2 aims at addressing the limitations of Contribution # 1. The limitations is that there are undecidable cases, for which we lack information to characterize them in the conceptual framework of Contribution # 1. We focus on the approaches that use the test suite as their main source of information as in the case of Contribution # 1. In this contribution, we propose a technique to split test cases into small fragments in order to increase the efficiency of dynamic program analysis. Applied to Contribution # 1, this solution improves the knowledge acquired by providing more information on more cases. Applied to other dynamic analysis techniques which also use test suites, we show that it improve the quality of the results. For example, one case study presented is the use of this technique on Nopol, an automatic repair tool. Contribution # 1 and # 2 are generic, they target any kind of exceptions. In order to further contribute to bug fixing and resilience, we need to focus on specific types of exceptions. This focus enables us to exploit the knowledge we have about them and further improve bug fixing and resilience. Hence, in the rest of this thesis, we focus on a more specific kind of exception: the null pointer dereference exceptions (NullPointerException in Java). Contribution # 3 focuses on Problem # 1, it presents an approach to make bug fixing easier by providing information about the origin of null pointer dereferences. We present an approach to automatically provide information about the origin of the null pointer dereferences which happen in production mode (i. e. those for which no efficient resilience mechanisms already exists). The information provided by our approach is evaluated w. r. t. its ability to help the bug fixing process. This contribution is evaluated other 14 real-world null dereference bugs from large-scale open-source projects. Contribution # 4 addresses Problem # 2, we present a way to tolerate the same kind of errors as Contribution # 3 : null pointer dereference. We first use dynamic analysis to detect harmful null dereferences, skipping the non-problematic ones. Then we propose a set of strategies able to tolerate this error. We define code transformations to 1) detect harmful null dereferences at runtime; 2) allow a runtime ehavior modification to execute strategies; 3) assess the correspondance between the modified behavior and the specifications. This contribution is evaluated other 11 real-world null dereference bugs from large-scale open-source projects. To sum up, this thesis studies the exceptions, their behaviors, the information one can gathered from them, the problems they may cause and the applicable solutions to those problems. Le monde est de plus en plus informatisé. Il y a de plus en plus de logiciels en cours d'exécution partout, depuis les ordinateurs personnels aux serveurs de données, et à l'intérieur de la plupart des nouvelles inventions connectées telles que les montres ou les machines à laver intelligentes. Toutes ces technologies utilisent des applications logicielles pour effectuer les taches pour lesquelles elles sont conçus. <b>Malheureusement,</b> le nombre d'erreurs de logiciels croît avec le nombre d'applications logicielles. Localement, une erreur logicielle est embêtante, elle peut coûter quelques heures de travail à une personne lorsque l'application crashe. Multipliez cette perte sur des millions de personnes et considérez que même les avancées scientifiques sont retardées ou empêchées par des erreurs de ce type: dans l'ensemble, ces erreurs sont coûteuses pour la société dans son ensemble. Il existe deux techniques pour faire face à ces erreurs. La réparation logicielle consiste à réparer ces erreurs manuellement. La résilience consiste à donner à une application la capacité de rester fonctionnelle malgré les erreurs. Cette thèse porte sur la correction des bugs et la résilience dans le contexte des exceptions. L'Exception est un mécanisme de gestion d'erreurs. Il est intégré dans la plupart des langages de programmation et largement utilisé dans la pratique. Dans cette thèse, nous ciblons spécifiquement deux problèmes:Problème n° 1 : Il ya un manque d'informations de débogage pour les bugs liés à des exceptions. Cela entrave le processus de correction de bogues. Pour rendre la correction des bugs liées aux exceptions plus facile, nous allons proposer des techniques pour enrichir les informations de débogage. Ces techniques sont entièrement automatisées et fournissent des informations sur la cause et les possibilités de gestion des exceptions. Problème n ° 2 : Il ya des exceptions inattendues lors de l'exécution pour lesquelles il n'y a pas de code pour gérer l'erreur. En d'autres termes, les mécanismes de résilience actuels contre les exceptions ne sont pas suffisamment efficaces. Nous proposons de nouvelles capacités de résilience qui gérent correctement les exceptions qui n'ont jamais été rencontrées avant. Dans cette thèse, nous nous efforçons de trouver des solutions à ces problèmes. Nous présentons quatre contributions pour résoudre les deux problèmes présentés. En résumé, cette thèse étudie les exceptions, leurs comportements, l'information qu'on peut recueillir auprès d'elles, les problèmes qu'elles peuvent causer et les solutions applicables à ces problèmes...|$|E
40|$|National audienceHow to make ecology into a {{political}} force without limiting it to managerial, political and technocratic practices? Which common world does it offer us? We here present ongoing research exploring {{the development of a}} general ecology within a cosmopolitical and aesthetic perspective. The unfinished aspect of this research explains the hypothetical and suggestive form of this text. First of all, a few introductory items to serve as parts of a framework. The notion of cosmopolitics was developed by Kant in his Perpetual Peace: A Philosophical Sketch (Kant 1795, 1970) in order to guarantee world peace in a sustainable manner. The cosmopolitical project relies on three hypotheses: the peaceful character of republics, the socializing virtue of international trade, and the regulating function of the international public sphere, in order to propose a universal Republic, real federation of free and peaceful states, and an alliance of peoples guaranteed by the emergence of international law between states, and cosmopolitical law, preserving foreigners' rights against these states (rights of visit and hospitality). This anticipation of democratic politics to be used by the peoples of the world is currently witnessing renewed interest. Most contemporary western reflections bear on the conditions to be met in a cosmopolitical democracy (Held, 1996, Archibugi Held 1999) : a preliminary commitment to democracy from each citizen of the world, and the possibility of a worldwide political order that goes beyond the globalization of financial and economic markets {{in order to create a}} pacified common world with the other cultures and civilizations. Some researchers (including Beck 1986, Stengers 1996 - 97, Latour 2004) go even further in their requirement for a cosmopolitical integration of what is alien: according to them, cosmopolitics imply the composition of a common world with non-humans: natural entities, at-risk entities, scientific artefacts, technical devices, symbolic worlds, etc [...] . This is about “making sciences and technologies come into democracy,” and about re-defining the human by adding to it its missing part: things (Latour 1993). Our article partakes of this perspective. Our idea of cosmopolitics starts with the concept as it has been reworded by Isabelle Stengers and with the debates of the international Cerisy colloquium on the emergence of cosmopolitics, which was held from September 20 th to 27 th 2003. Cosmopolitics, politics of the cosmos or politics of worlds, are experimental politics, aiming at creating a common world without cutting our ties to our worlds. The notion of world stresses a competence common to all human subjects:we create worlds via a creative process that defines us as subjectivities:a subject in his world, which surrounds him and goes beyond him. These worlds of the subject could represent “the places of the common world”. True to the ancient concept of a cosmos as linking microcosm and macrocosm, a politics of the cosmos will try to organize our own worlds, “microworlds” which are conflicting with each other and which are apparently incompatible, with the aim of a common world. However, can we suggest a cosmos for a common world there where the universe of modern physics is axiologically neutral? Indeed, as Augustin Berque reminds us, the Greek word kosmos has three meanings: “order”, “world” and “costume,” but can the composition of things making up the world, its organization, still abide by ethical and aesthetic principles? The word cosmopolitics is an oxymoron uniting the two opposed modalities that compose the collective: the polis and the cosmos. Bringing these two terms into tension with each other brings to mind an arrangement where cosmos and politics will make reference to each other. Cosmos is an inhabitable common world, “the habitat in a well organized house of the world” (Sloterdijk 2003). It implies a partition between our own space, that of our particular worlds, and an alien or foreign outside. Referring to habitability enables us to put to the test the two complementary modes of putting together the shared world that are privileged by modernity, political justification and scientific proof, which are going to homogenize to an extreme the components of the future common world, whether the universe or the city, and neglect the necessary partition to such an extent that the “macroworld” might become uninhabitable; in other words, the cosmos might be destroyed. This decosmization (Berque 2000) can be explained by the fact that science and politics come too fast to an agreement when they disqualify “aesthetics in a broad sense, i. e. the aisthêsis, the ability to feel, to perceive with your senses” (Berque 2000) or, even more so, when they reprove the ability to imagine and to envisage a common world as an aim. These faculties combine apprehension through the senses and aesthetic judgement. Conversely, the reference to politics aims, with democracy as the target, to go beyond the issue of singular worlds. The aesthetic perspective creates a category of experience that is different from the scientific one (Dewey 1980, Reid, Taylor 2003). Its ambition is that of creating a habitable common world based on each and every one's capacity to collectively imagine it and shape it; it is therefore a vitalistic and creative perspective, and sometimes an exhilarating one; it builds upon the affects and intends to be more rigorous towards the totality of living things. It could counteract the hegemony of the scientific and the political world-building modes by specifying the creative openings and the ethical requirements that the aesthetic composition of the common world make possible: respiration, gestation, respect, and density. To present our research process, we will start with the world of subjects as analyzed today by environmental and politistic research paths, then we will examine how some of these paths lead to theorizing a common world based on the creative openings suggested by artists. Les artistes contemporains qui contribuent à l'espace public, mettent-ils en œuvre une esthétique qui puisse fournir des pistes pour la composition d'un cosmos, d'un monde commun plus qu'humain, l'élaboration d'une cosmopolitique ? La perspective esthétique peut-elle faire contrepoids à l'hégémonie scientifique et technique dans le registre politique ? Comment les cosmopolitiques peuvent-elles intégrer une perspective esthétique ? Ces interrogations nous incitent à formuler des éléments théoriques pour la place de l'art dans l'espace public et une esthétique écologique. Nous utilisons la question esthétique comme une modalité riche de passage entre microcosmes et espace public, et les cosmopolitiques comme une manière de penser l'articulation des mondes et une politique. Quelles sont les règles, dès lors, qui permettent de penser cet espace public esthétique ? Notre recherche, aux aspects théoriques aussi bien qu'empiriques, propose d'alimenter la réflexion sur les cosmopolitiques en partant des pratiques artistiques contemporaines dans l'espace public. Notre hypothèse est la suivante : nous pensons que, par fragments, les artistes composent des mondes et réalisent des jonctions entre mondes privés et monde commun. Nous pensons que les modalités du travail artistique fournissent des pistes, sur le plan du jugement esthétique, pour l'élaboration d'une cosmopolitique. C'est à l'intérieur de ce champ d'interrogation et conscients des difficultés mais aussi de l'importance de l'entreprise, que nous avons entrepris de réaliser, dans le cadre d'une réflexion sur la place des artistes et du jugement esthétique dans le registre du politique, des entretiens avec des artistes contemporains intervenant selon des modalités diverses dans l'espace public. Ce travail, dont nous rendons compte ici partiellement, se poursuit aujourd'hui avec des rencontres pour évaluer plus généralement la place de l'esthétique et de la créativité dans le champ de l'écologie. Nous présenterons d'abord notre problématique générale qui associe cosmopolitique et esthétique (chapitre I), puis nous exposerons notre parcours de recherche. Nous explorerons dans un premier temps la diversité des liens (rationnels mais aussi sensibles et esthétiques) qu'entretiennent les sujets avec leurs mondes singuliers (chapitre II). Selon nous, cette part-monde des sujets si souvent amnésiée constitue pourtant la chair de notre vivre ensemble, la matière vivante de notre monde commun. <b>Malheureusement</b> la prise en compte du sujet et de sa part mondaine par les mécanismes politiques qui composent ce monde commun se heurte à deux obstacles : la clôture territoriale des mondes singuliers et leur éviction par le recours à l'intérêt général et l'expertise scientifique (chapitre III). La perspective esthétique fournit quelques pistes pour résoudre ce problème : proposer les mondes artistes comme modèle d'ouverture et de créativité pour enrichir les mondes singuliers (chapitre IV) introduire le jugement esthétique comme une autre manière d'envisager la composition d'un espace public plus ouvert aux singularités (chapitre V) ...|$|E
40|$|Between 1999 and 2001, three {{measurements}} changed Cosmology forever: {{the discovery}} of Cosmic Acceleration (Riess et al. 1998, Perlmutter et al. 1999) indicated that {{the density of the}} Universe is dominated by some kind of repulsive energy of unknown nature (namely Dark Energy). The measurement of the first acoustic peak in the CMB temperature anisotropy spectrum (de Bernardis et al. 2000) combined with the precise determination of H 0 (Freedman et al. 2001) gave strong constraints on the flatness of space-time. These measurements contributed to solve the persisting disagreements between the observations that were favouring a low density of matter, and theoretical motivations for a higher-density (critical) Universe. It favoured the emergence of the Standard Model of Cosmology (CDM) that describes nearly all of today's observations with only a handful of free parameters (Planck Collaboration et al. 2013 b). Cosmology has now entered an era of precision measurements, and the goal of observations is now to hunt for "tensions" within the cosmological model. The case of Supernova cosmology is very characteristic of this situation. The measurement of luminosity distances to SNe-Ia as a function of their redshift allowed one to discover (with less than 100 supernovae) the acceleration of cosmic expansion. Today, SNe-Ia are still the most sensitive probe to w, the Dark Energy equation of state parameter, and growing number of SNe-Ia are being detected and studied by several Collaborations all over the world, in order to pin down the value of w, and to start ruling out Dark Energy models. The precision on w is now as low as 7 % (Conley et al. 2011, Sullivan et al. 2011) with nearly 1000 SNe-Ia in the Hubble diagram. Unfortunately, the measurement is now dominated by systematic uncertainties, the dominant source of systematics being the photometric calibration of the imagers used to measure the SNe-Ia fluxes. This work is about photometric calibration. This is a rather esoteric subject, which is seldom chosen by PhD students. But the thing is that, to improve on the current results, astronomers {{have no choice but to}} revisit the ancient calibration schemes. Since 2005, most Dark Energy Collaborations (with the invaluable help of the HST calibration program) have launched ambitious calibration efforts, redefined primary standards and metrology between those standards and their science images and push down their error budget well below 1 % (e. g. Betoule et al. 2012). One suspect however, that these techniques, which rely on observations of stellar calibrators, will not allow one to reach the calibration requirements of future surveys. For this reason, several groups in the world are working on experimental laboratory sources, that would allow one to inject very well characterised light into the telescope optics and derive, from these measurements, the telescope throughput as a function of wavelength. Since 2007, LPNHE cosmology group has been involved in the construction of a spectro-photometric calibration system for the last generation of wide field imagers (Barrelet and Juramy 2008). In particular, the team has designed and built two devices: SnDICE (Supernovae Direct Illumination Calibration Experiment) and SkyDICE (SkyMapper Direct Illumination Calibration Experiment), the first installed in the enclosure of the Canada France Hawaii Telescope (CFHT) on top of Mauna Kea, and the other in the dome of SkyMapper (Siding Springs Observatory, NSW, Australia). I started my PhD a few months after the project was funded. I was involved in nearly all stages of the project, in particular the integration and the calibration of the device on our test bench, as well as the installation and commissioning at Siding Springs. I then spent my third year analysing the commissioning data. We have shown that it is possible to build a LED based light source that samples evenly the full visible wavelength range. The stability of the source is remarkable, ranging from a few 10 − 4 for a few of the LEDs, to 10 − 3 for the less stable channels. I have detailed the spectrophotometric characterisation of the device on our test bench at LPNHE. More importantly, I have shown that it is possible to build a smooth spectrophotometric model of each LED, that can predict the LED spectrum at any temperature (in a temperature range representative of what is measured in the telescope enclosure). Each of these models comes with an uncertainty budget that accounts for (1) -the finite number of spectroscopic and photometric measurements and (2) -the test bench uncertainties. Finally, I have described a method to calibrate the effective passbands of the imager, and monitor their fronts from series of calibration frames taken with SkyDICE. This method takes into account all the test bench uncertainties are propagate them as exactly as possible to the final result. It is currently being applied to the real SkyDICE dataset, and what has been presented here is a set of tests performed on (realistic) simulated datasets. A important result of this work is that, despite the fact that the LEDs are not monochromatic sources, we are able to control the position of the filter fronts with an accuracy well below 1 -nm. Regarding the passband inter-calibration, we have computed the expected uncertainties affecting our estimates of the passband normalisation, relative to the r-band. These uncertainties actually depend on how we interpret the uncertainties that affect the calibration of the NIST photodiode. In the best-case scenario, where the NIST uncertainties are all positively correlated, we have shown that after a few calibration runs, we get down to a precision of 0. 4 % in the u and v-bands (near-UV) and of 0. 3 % in the other bands. Depending on how we estimate the CALSPEC uncertainties (which are themselves uncertain), this result is either a major improvement on CALSPEC, or on par with what can be obtained with CALSPEC. In any case, this means that by using routinely a DICE source to calibrate a survey telescope, we should be able to test the CALSPEC flux scale. The analysis of the SkyDICE commissioning dataset is still ongoing. The main missing ingredient is the control of the relative positions of the telescope and the sources, as well as an estimate of the pollution of the calibration frames. These two aspects of the analysis are actively worked on, and the first constrains should be published soon. La cosmologie est maintenant entré dans une ère de mesures de précision, et l'objectif des observations est maintenant la chasse aux contradictions au sein du Modèle Cosmologique. La mesure des distances de luminosité de SNe Ia en fonction de leur décalage vers le rouge a permis de découvrir l'accélération de l'expansion cosmique. Aujourd'hui, les SNe-Ia sont encore la sonde la plus sensible à w, l'équation d'état de l'énergie noire, et le nombre croissant de SNe-Ia sont détectés et étudiés par plusieurs collaborations partout dans le monde, afin d'affiner la mesure du valeur de w. La précision sur w est maintenant aussi bas que 7 %, avec près de 1000 SNe-Ia dans le diagramme de Hubble. <b>Malheureusement,</b> la mesure est désormais dominé par les incertitudes systématiques, la principale source de la systématique en étant l'étalonnage photométrique des imageurs utilisés pour mesurer le flux des SNe Ia. Ce travail de thèse a pour sujet l'étalonnage photométrique. Pour améliorer les résultats actuels, les astronomes n'ont pas d'autre choix que de revoir les systèmes d'étalonnage anciens. Depuis 2005, les collaborations sur l'énergie noire ont lancé des efforts d'étalonnage ambitieux, redéfini les standards primaires et la métrologie entre ces standards et leurs images scientifiques pour pousser le budget d'erreur bien inférieure à 1 %. Depuis 2008, le groupe de Cosmologie de l'LPNHE a été impliqué dans la construction d'un système d'étalonnage spectrophotométrique pour la dernière génération des imageurs grand-champ. En particulier, l'équipe a conçu et construit SkyDICE (SkyMapper Direct Illumination Calibration Experiment), installé dans le dôme du télescope SkyMapper (Observatoire Siding Springs, Australie). Dans ce projet nous avons montré qu'il est possible de construire une source lumineuse à base des LEDs qui échantillonnent uniformément toute la gamme des longueur d'ondes visible du télescope SkyMapper. La stabilité de la source est remarquable, allant de quelques 10 - 4 pour la majorité des LEDs, à 10 − 3 pour les canaux les moins stables. J'ai détaillé l'étalonnage spectrophotométrique de l'appareil sur notre banc de test au LPNHE. Plus important encore, j'ai montré qu'il est possible de construire un modèle spectrophotométrique de chaque LED, qui peut prédire le spectre des LEDs à n'importe quelle température T. Chacun de ces modèles est livré avec un budget d'incertitude que représente (1) -le nombre limité de mesures spectroscopiques et photométriques et (2) -les incertitudes du banc de test. Enfin, j'ai décrit une méthode pour calibrer les bandes passantes effectives de l'imageur, et de surveiller les filtre avec des sériés d'images d'étalonnage prises avec SkyDICE. Cette méthode prend en compte toutes les incertitudes du banc d'essai et le propage aussi exactement que possible. Le méthode est actuellement appliqué à l'ensemble de données réelles de SkyDICE, et ce qui a été présenté ici est un ensemble de tests effectués sur des ensembles de données simulées. Un résultat important de ce travail est que, malgré le fait que les LEDs ne sont pas des sources monochromatiques, nous sommes en mesure de contrôler la position des fronts de filtre avec une précision bien inférieure à 1 -nm. En ce qui concerne la bande passante étalonné, nous avons calculé les incertitudes affectant nos estimations sur la normalisation de la bande passante, par rapport à la bande r. Dans le meilleur scénario, où les incertitudes sont tous corrélés positivement, nous avons montré que, après quelques analyses d'étalonnage, nous nous attelons à une précision d'environ 0, 4 % dans les bandes u et v et d'environ 0, 3 % dans les autres bandes. L'analyse de l'ensemble de données des SkyDICE est toujours en cours et le premier contraintes seront publiés bientôt...|$|E
40|$|This {{survey of}} {{literature}} was undertaken by ARTEP- the French Research Association for Oil Exploration and Production Techniques- {{at the beginning}} of STAR (=STabilité des ARgiles), a project on the influence of clays on borehole stability. Knowledge of theories and laboratory experiments was indeed felt very necessary to help understanding, and thus becoming able to prevent, quite damaging phenomena. During the time spent on this project, ideas and interpretations of all participants underwent some evolution due to the comparison between theories and experiment, and new procedures and interpretations are being proposed elsewhere. The survey is divided in four sections :The first section recalls the specific problems caused by the occurrence of shales during drilling operations for oil or gas : about 90 % of the problems, for about 70 % of the drilled formations. The behaviour of the shales leads to a classification in four different classes : dispersive, swelling, heaving and brittle. They are spread all over the world, not only on the Gulf Coast of the USA and in the North Sea, where they have been more extensively studied, but also in former USSR, in Asia and Africa. Due to deposit conditions and diagenetic history, they occur at different depths, with different properties : reactive shales, at shallow depths, under-, over- or normally-compacted formations, and reservoir caps. Besides their mineralogical and textural properties, due to the large proportion of clays, they have damaging properties as the low permeability (10 to the power of (- 6) to 10 to the power of (- 12) D). The variety of reactions with water extends from a complete dispersion in mud to cavings or swelling of the borehole, with cuttings ranging from less than 1 mm to more than several cm. This has led to use of various empirical solutions to protect the borehole, with mainly mechanical or chemical objectives. However, it is felt that the general solutions can arise only from a synergistic effort of both rock mechanics and physico-chemistry, hence the STAR program. The second section is devoted to a survey of physico-chemical reactions between clays and water. It begins with some definitions of the clays as solids, from the rock to the atomic level. All clays are characterised by their small size, in the range of 1 µm, and thus, their high surface area : from one to several hundred M²/g. Existence of a layer charge, and of compensating cations is the key of the behaviour of the swelling clays (i. e. smectites) versus water, pure or with cations, which deserve particular attention. Description of the various kinds of water associated, more or less energetically, with clays, as a function of relative humidity RH, or water activity a index (w), helps to distinguish their effect on porosity and texture. Hydration and dehydration behaviour, with a particular hysteresis, is described, in solid-gas systems as well as in solid-liquid, closer to field conditions. Two main domains are distinguished : crystalline swelling, up to a water content of circa 50 vol. -%, inducing swelling pressures in the range of several thousand atmospheres, and osmotic swelling, for higher water contents, inducing pressures in the range of several atmospheres. Influence of nature and amount of cations is examined, mainly for calcium, which induces limited swelling, and for potassium which is both less hydratable and proner to irreversible fixation on clay. Little work has already been performed on the influence of temperature and pressure. Mechanisms of water and/or ionic species transport are reviewed: diffusion and osmosis, without applied pressure, and effect of pressure. Behaviour on compaction, always showing hysteresis, depends {{on the nature of the}} clays, and of the cations, but also on the composition of the solution. It is described using suction pressures or mechanical stresses, which induce different properties of the final solid. Experiments in soil literature, generally performed in the presence of a gas phase, cannot be readily compared to the in situ behaviour of the shales, but give insights on the possible artefacts of laboratory experiments. Caution is thus necessary before any application of literature results to real samples, all preliminary conditioning (initial state and composition of the clay and the water, way of hydration/dehydration, or compaction) being able to modify the behaviour of the clay-water system. The third section sets the problem of describing the mechanical behaviour of the rock formation on drilling. This behaviour depends on initial in situ stresses, pore pressure and temperature, and on the constitutive law of the rock, i. e. the relation between stress and strain. As an example, the Cam Clay elasto-plastic law is developed. Then the laboratory experimental sets used to identify mechanical properties are described : triaxial tests, drained or undrained, oedometric tests, and hollow cylinder tests, the first ones being used to calibrate borehole stability, while the latter simulate drilled boreholes. Specific aspects of shales are then recalled : dependence of mechanical properties on the water content, anisotropy and influence of time. Coupling between physico-chemistry and mechanics arises from the lack of chemical equilibrium between the solid and the liquid. This desequilibrium induces a transfer of water and chemical species in solution, modifying the pore pressure, thus the stress on the rock, and leading to chemical reactions, which have been described in section III. Follows a description of stability models, which should be able to predict mud characteristics for the drilling as well as evolution of the borehole with time. Stability models intend to calculate the maximum/minimum mud weight, from a relevant instability criterion, drawn from well data, mechanical data and fluid properties. The choice of the constitutive law is thus important, and elasto-plastic ones seem the more relevant. Taking into account physico-chemistry has been done generally using an osmoticpressure, with the assumption that the shales behave as a semi-permeable membrane. Even if this assumption is too simplistic, it is still used, but more refined models are being studied, which take into account variations in pore pressures and salinities. The fourth section deals with what actually occurs on application to real wellbore. Improvement of mud formulation tries to prevent any problem occurring during drilling. Evolution of formulation is described, from lime, oil, KCI to polymers additions, and to nowadays constraints brought by environment concerns : requirements may be opposite, and thus compromises must be found. Mud monitoring seems a good prospect. In a second part, availability of representative samples, artefacts related to the recovery and storage of samples as well as choice of experimental conditions are reviewed. It is recalled that downhole conditions are rarely taken into account, and problems like drying of the samples, which induces a suction pressure, anisotropy and cohesion of the samples are rarely considered. The conclusion emphasises that if swelling pressures can now be more precisely defined in hydration domains, the effects of physicochemistry on mechanical properties are still to be investigated more thoroughly. Further experiments should be set in conditions closer to downhole ones, and teams must work together to get all the data needed. This will allow proper stability modelling, with coupling of physico-chemistry and mechanics, to become a predictive tool. Cette revue bibliographique a été réalisée dans le cadre du programme STAR (STabilité des ARgiles) entrepris par l'ARTEP (Association de Recherche sur les Techniques d'Exploitation du Pétrole). Elle se divise en quatre sections: La première section rappelle les problèmes spécifiques rencontrés lors du forage des argiles, qui représentent environ 70 % des formations traversées, et sont répandues dans le monde entier, à diverses profondeurs, donc dans des états de compaction et d'évolution diagénétique différents. Leur comportement a conduit à une classification opérationnelle. Des solutions empiriques, destinées à protéger les parois de puits, ont été mises en oeuvre. Toutefois, il apparaît évident que des solutions de caractère général ne peuvent résulter que d'un effort conjoint en mécanique des roches et en physico-chimie : tel est l'objectif du programme STAR. La seconde section est consacrée à la description des réactions physico-chimiques entre les argiles et l'eau. Après avoir décrit les argiles en tant que solides, et plus particulièrement les argiles gonflantesqui ont une charge structurale faible compensée par un cation interfoliaire échangeable, on s'attache au comportement de l'eau qui leur est associée en fonction du degré d'humidité et de la nature et de la concentration des sels dans l'eau. Le gonflement cristallin, limité à une teneur en eau de 50 % environ, et correspondant à des pressions de gonflement de milliers d'atmosphères, est distingué du gonflement osmotique, qui intervient à de plus fortes hydratations, et induit des pressions de l'ordre de la dizaine d'atmosphères au maximum. L'influence spécifique des cations comme le potassium et le calcium est décrite. Des exemples de comportement à la compaction, fonction de la nature des argiles et des cations sont décrits. Il apparaît qu'une description très précise des conditions de départ et du déroulement des phénomènes est indispensable pour l'interprétation, et qu'elle est <b>malheureusement</b> absente dans nombre de publications. La troisième section traite de la description mécanique du comportement des roches argileuses lors du forage. La possibilité de modéliser ce comportement dépend de l'acquisition des paramètres pertinents, et du choix du modèle : le modèle Cam Clay, qui utilise une loi élasto-plastique est donné en exemple. Les tests de laboratoire sont décrits. Dans les argilites, les déséquilibres chimiques entre solide et fluide induisent des transferts d'eau et d'espèces chimiques, modifiant la pression de pore, donc la contrainte à laquelle est soumise la roche, et conduisant à des réactions chimiques décrites dans la deuxième section. Les modèles de stabilité prennent en compte ce couplage par l'usage d'une pression osmotique, avec l'hypothèse simpliste que les argilites se comportent comme des membranes semi-perméables. Des modèles plus affinés sont en cours d'élaboration pour prendre en compte les variations de pression de pore et de salinité. La quatrième section traite des pratiques de terrain, en particulier des améliorations apportées par une formulation de plus en plus élaborée des boues, pour répondre à la fois aux problèmes de forage, et aux exigences de préservation de l'environnement. La question de représentativité des échantillons destinés au laboratoire, et des artefacts liés à la récupération et au stockage est traitée : en particulier les variations de pression capillaire lors du séchage et de la réhydratation imposent des protocoles stricts. Des conditions expérimentales proches des conditions de fond sont rarement employées. En conclusion, si la notion de gonflement, et les conditions dans lesquelles il peut intervenir, apparaissent plus précisément, l'effet sur les propriétés mécaniques reste encore largement à étudier. Une approche multidisciplinaire est indispensable à l'élaboration de modèles de stabilité qui se veulent prédictifs...|$|E
40|$|China {{realized}} {{during the}} last 15 years spectacular economic growth success. However, its economic growth was also accompanied by serious environmental degradation problems. China has been ranked {{as one of the}} most polluted countries in the world by some international organizations, particularly on the aspect of air pollution. Although many scholars start to consider China as the future number one economic power given its current marvelous economic success, the verification of such hypothesis closely depends on the sustainability of China's future growth path—its actual economic growth speed can be sustainable only if today's economic achievement is not obtained by mortgaging that of tomorrow. This dissertation, focusing on the case of industrial SO 2 emission in China, aims to study the potential relationship between economic growth, trade liberalization and environment in China, in the aims of identifying the possibility, the sufficient and necessary conditions for China to realize its sustainable development. After a comprehensive literature review on the existing Environmental Kuznets Curve studies, I started my analysis by a reduced-form Environmental Kuznets Curve analysis (Chapter 2). The results showed that although the EKC analysis predicts a turning point at about 9000 yuan (1990 price) for the case of per capita industrial SO 2 emission, the evolution of total industrial SO 2 emission seems to continue its increasing tendency. To understand the underlying reasons for the increasing tendency in total SO 2 emission, I further carried out two structural analyses, in which the structural determinants of SO 2 emission are decomposed either parametrically (Chapter 3) or non-parametrically (Divisia index decomposition method based on the detailed data on production and SO 2 emission intensity of 13 industrial sectors in each province during 1991 - 2001, Chapter 4) into scale, composition and technique effects. The results showed that, the per capita income, acting as an omnibus variable representing all the three aspects of underlying structural determinants, only impart a “net-effect” of income growth on environment. The real reason for the ever-increasing trend of total industrial SO 2 emission in China is actually due to the domination of pollution-increasing impact of scale enlargement over the pollution-reducing contribution from technical progress, combined with a province-specific composition transformation which exerts modest pollution-increasing impact in most of the Chinese provinces, given their current industrialization process. The second part of this dissertation further amplified my decomposition efforts by giving particular attention to the emission determination role of international trade. Previously redeemed by some pessimistic economists as a channel for the richer developed countries to discharge their pollution burdens to their poorer trade-partners, international trade has been considered as a static explanation for the formation of the inverted-U-shaped growth-pollution relationship. Nevertheless, all three analyses carried out in this part, by investigating the different channels through which international trade can exerts impact on the three determinants of emission, only provide very limited supportive evidence for the “pollution haven” hypothesis in China. As China's factor-endowment-based comparative advantages are much attractive than its potential as a “pollution haven”, the conclusion of the ACT-style (Antweiler, Copeland and Taylor, 2001) model estimation in Chapter 5 shows that trade liberalization can actually reduce the pollution burden of China's industrialization process by deviating its industrial composition transformation towards less polluting labour-intensive sectors. By noticing that the actual role of trade is more complicated, in Chapter 6, I re-employed the decomposed results of Chapter 4 and further checked the indirect impact of trade (export and import separated this time) on industrial emission through its three structural determinants. This study confirmed the significantly positive impact of trade in both scale enlargement and technical progress. The analysis based on a simultaneous system in Chapter 7 permits me to combine these three aspects' indirect impact of international trade on emission into the same estimation. Its results reveal the total role of export in China is environment-friendly while that of import (measured by the accumulation of imported machinery and equipments) is pollution-enhancing. In the CGE model analysis in Chapter 8 (Part 3), I related emission results directly to energy input used in production activities and included the principal coefficients estimated in the previous into the modelling and simulation work. This model offered me an opportunity to parameterise the multiple aspects of trade-pollution and growth-pollution nexus and to finally obtain an explicit numerical comparison between the magnitude of environmental impact of trade and economic growth. This analysis reveals that, compared to the scale effect resulting from rapid economic expansion in China, the actual pollution-increasing impact of trade liberalization is very small. The most important pollution reduction contribution actually comes from efficiency improvement in energy uses and depends largely on the existence of a more stringent and efficient pollution control policies combined with a flexible energy substitution process. Facing the potential dangers for China's future environmental situation, I investigated in the chapter 9 (Part 3) the potential feedback effect from pollution to China's future growth sustainability. The analysis reveals a significant negative relationship between industrial SO 2 emission and public health after the industrial SO 2 emission density attains the critical threshold of 8 g/m 2. Fortunately, the estimated model inn this chapter seems also to reveal some possible dynamism through which the significant negative impact of industrial SO 2 emission on public health status can be gradually reduced with economic growth. But to realize this dynamism, China need to realise a more-than-proportion increase in de-sulfur technology investment with respect to its economic growth rate in the coming years. To sum up, the analyses carried out in this dissertation actually indicate both opportunity and challenge for China's pursuit for a sustainable development path. Given the current environmental deterioration tendency, whether China can preserve its future growth sustainability actually depends on the existence of the technological capacities to improve the pollution abatement efficiency (sufficient condition) and the correct function of a stricter pollution control policies (necessary condition). Both aspects further put forward the requirement for the availability of efficient institutional and market system in China. La Chine, pays le plus peuplé du monde, connaît depuis une quinzaine d'années des taux de croissance spectaculaires. <b>Malheureusement,</b> cette croissance a aussi été accompagnée d'une très forte dégradation de l'environnement et a positionné la Chine parmi les pays les plus pollués du monde, notamment au niveau de son atmosphère. Ainsi, le considérable succès que représente cette croissance économique, au point de vouloir présenter la Chine comme la prochaine première puissance économique mondiale, conduit à poser la question de sa soutenabilité. Nous postulons que la croissance actuelle de la Chine ne pourra être durable que dans la mesure où elle n'hypothèque pas celle de son futur. Cette thèse se base sur le cas de l'émission industrielle de SO 2 – la pollution aérienne la plus importante en Chine. En analysant son évolution au cours des années 1990 et en se focalisant sur ses relations avec la croissance économique, l'industrialisation et l'ouverture commerciale – les trois caractères les plus évidents du développement économique chinois –, cette thèse vise à identifier la possibilité et les conditions nécessaires et suffisantes pour la Chine de réaliser un développement soutenable. Après une revue de la littérature sur la Courbe de Kuznets Environnementale (CKE), nous commençons notre analyse par l'étude d'une CKE de forme réduite (Chap. 2), qui révèle une relation assez optimiste entre la croissance chinoise et l'émission industrielle de SO 2 par tête. Cependant, cette CKE « par tête » ne garantit pas un même retournement de trajectoire pour l'émission industrielle totale de SO 2. Bien que notre analyse prédise ce retournement aux environs de 9000 yuan par tête (prix constants 1990) pour le cas de l'émission industrielle de SO 2 par tête, l'évolution de l'émission industrielle de SO 2 totale semble continuer à augmenter. Pour comprendre les raisons de cette tendance à la hausse de l'émission industrielle de SO 2 totale, nous menons deux analyses structurelles, dans lesquelles les variations de l'émission sont décomposées de façon paramétrique (Chap. 3) et non-paramétrique (la méthode de décomposition de l'indice de Divisia basée sur des données détaillées de la production et de l'intensité d'émission de SO 2 de 13 secteurs industriels dans chaque province entre 1991 et 2002, Chap. 4) en contributions de ses trois déterminants structurels - effets d'échelle, de composition et de technique. Les résultats montrent que le revenu par tête fonctionne comme une variable omnibus qui capte les effets des trois déterminants structurels et révèle seulement un « effet net » de la croissance sur l'environnement. Le détail de nos analyses permet cependant de trouver les véritables raisons de cette tendance à la hausse de l'émission industrielle de SO 2. Cela serait principalement dû à une domination de l'effet d'échelle sur l'effet réducteur d'émission issu des progrès techniques, le tout combiné à une transformation de la composition industrielle exerçant un modeste impact à la hausse des émissions dans la plupart des provinces chinoises. La seconde partie de cette thèse accroît ses efforts de décomposition en donnant une attention particulière au rôle déterminant du commerce international sur l'émission. Perçu par certains économistes pessimistes comme un canal à travers lequel les pays riches déchargeaient leurs fardeaux de pollution sur leurs partenaires commerciaux relativement plus pauvres (hypothèse de « havre de pollution »), le commerce international a ainsi souvent été considéré comme une explication statique de la formation d'une relation en U inversé entre le revenu et la pollution. Cependant, les trois analyses menées dans cette partie, en recherchant les différents canaux de transmission à travers lesquels le commerce affecte les trois déterminants structurels de l'émission, nous offrent pour la Chine des preuves très limitées en faveur de l'hypothèse de « havre de pollution ». Etant donné que l'avantage comparatif de la Chine basé sur sa dotation extrêmement riche en travail est beaucoup plus attractif que son avantage de « havre de pollution », la conclusion d'une analyse de style Antweiler, Copeland et Taylor (ACT, 2001) au Chapitre 5 nous montre que la libéralisation commerciale peut réduire les dangers d'une augmentation de pollution liée à l'effet de « havre de pollution » en guidant la transformation de la composition industrielle chinoise vers les secteurs intensifs en travail, souvent considérés comme moins polluants. Le rôle du commerce sur l'environnement n'est cependant pas aussi simple. Dans le Chapitre 6, en re-employant les résultats de la décomposition de Divisia du Chapitre 4, nous vérifions les impacts indirects du commerce (exportations et importations introduites de façon séparées) sur l'émission à travers les trois déterminants structurels. Cette analyse confirme l'impact significativement positif du commerce dans l'élargissement de l'échelle de production et sur les progrès techniques. Les analyses basées sur un système d'équations simultanées au Chapitre 7 nous permettent de combiner ces trois aspects des impacts indirects du commerce sur l'émission et d'inclure leurs interactions potentielles dans une même estimation. Les résultats révèlent que le rôle total des exportations est positif pour l'environnement mais que celui des importations (mesurées par l'accumulation de machines et d'équipements importés) est négatif. Dans le modèle en Equilibre Général Calculable (EGC) du Chapitre 8 (Partie 3), nous relions directement les résultats de l'émission à la combustion des énergies dans les activités productives et incluons toutes les interactions entre les variables économiques et l'environnement dont nous avons pu discuter dans les chapitres précédents pour la spécification du modèle. Ce modèle nous offre l'opportunité de paramétrer et de simuler de multiples aspects des relations entre croissance, ouverture commerciale et émission. Les simulations basées sur ce modèle nous permettent d'obtenir des comparaisons numériques de l'ampleur des impacts environnementaux du commerce et de la croissance économique. La conclusion de ce chapitre montre que, sans une politique de contrôle de pollution plus efficace, la croissance économique chinoise devrait s'avérer très polluante et que l'accession à l'OMC devrait provoquer une hausse supplémentaire mais marginale de pollution. En considérant les dangers potentiels de cette situation sur l'environnement chinois, nous décidons de rechercher dans le dernier chapitre de cette thèse (Chap. 9) l'effet de « feedback » potentiel de la pollution sur la capacité de croissance de l'économie chinoise. Les résultats confirment un effet négatif de l'émission de SO 2 sur la prévalence des maladies chroniques au sein de la population. Une fois dépassé le seuil de 8 g/m 2, une augmentation de 1 g/m 2 de la densité de l'émission industrielle de SO 2 accroît la probabilité pour une personne représentative de souffrir de maladies chroniques de 0, 25 %. Cependant, si les progrès techniques réalisés dans les activités de contrôle de la pollution augmentent de façon continue dans le temps, nos résultats indiquent également une dynamique potentielle pouvant réduire de façon graduelle l'impact négatif de la pollution sur la santé avec la croissance économique. En résumé, les analyses menées dans cette thèse présentent un certain nombre de défis à relever et d'opportunités à saisir pour que la Chine puisse poursuivre un chemin de développement qui soit soutenable. Etant donnée la tendance actuelle à la détérioration de son environnement, la capacité de la Chine à préserver une croissance soutenable dans le futur dépendra étroitement de l'adoption de progrès techniques (conditions suffisantes) et d'un fonctionnement efficace et plus strict des politiques de contrôle de pollution (conditions nécessaires), ainsi que d'une meilleure efficacité des systèmes institutionnels et de marché...|$|E
40|$|The {{present report}} surveys the {{essentials}} of my research activity since my PhD thesis [53], which was mainly devoted to extend the use of recent advances in Computational Harmonic Analysis (such as wavelet analysis) for adaptive nonparametric estimation methods in the i. i. d. setting to statistical estimation based on Markovian data. As explained at length in [123], certain concentration of measure properties (i. e. deviation probability and moment inequalities over functional classes, specifically tailored for nonlinear approximation) are crucially required for taking advantages of these analytical tools in statistical settings and getting estimation procedures with convergence rates surpassing the ones of older methods. In [53] (see also [54], [55] and [56]), the regenerative method (refer to [185]), consisting in dividing Harris Markov sample paths into asymptotically i. i. d. blocks, has been crucially exploited for establishing the required probabilistic results, the long term behavior of Markov processes being governed by certain renewal processes (the blocks being actually determined by renewal times). But having constructed an estimator, estimation of the accuracy (measured by the variance, particular quantiles or any functional of the distribution function) of the computed statistic is next of crucial importance. In this respect and beyond its practical simplicity (it consists in resampling data by making i. i. d. draws in the original data sample and recompute the statistic from the bootstrap data sample), the bootstrap {{is known to have}} major theoretical advantages over asymptotic normal approximation in the i. i. d. setting (it automatically approximates the second order structure in the Edgeworth expansion of the statistic distribution). I then turned naturally to the problem of extending the popular bootstrap procedure to markovian data. Through the works I and Patrice Bertail have jointly carried out, the regenerative method was revealed to be not solely a powerful analytical tool for proving probabilistic limit theorems or inequalities, but also to be of practical use for statistical estimation: our proposed bootstrap generalization is based on the resampling of (a random number of) regeneration data blocks (or of approximation of the latter) so as to mimick the renewal structure of the data. This method has also been shown to be advantageous for many other statistical purposes. And {{the first part of the}} report strives to present the principle of regeneration-based statistical methods for Harris Markov chains, as well as some of the various results obtained this way, in a comprehensive manner. The second part of the report is devoted to the problem of learning how to order instances, instead of classifying them only, in a supervised setting. This dicult problem is of practical importance in many areas, ranging from medical diagnosis to information retrieval (IR) and asks challenging theoretical and algorithmic questions, with no entirely satisfactory answers yet. A possible approach to this subject consists in reducing the problem to a pairwise classification problem, as suggested by a popular criterion (namely, the AUC criterion) widely used for evaluating the pertinence of an ordering. In this context some results have been obtained in a joint work with Gabor Lugosi and Nicolas Vayatis, involving the study of U-processes: the major novelty consisting in the fact that here natural estimates of the risk are of the form of a U-statistic. However, in many applications such as IR, only top ranked instances are eectively scanned and a criterion corresponding to such local ranking problems as well as methods for computing optimal ordering rules with respect to the latter are crucially needed. Further developments in this direction have been considered in a (continuing) series of works in collaboration with Nicolas Vayatis. Finally, the last part of the report reflects my interest in practical applications of probabilistic concepts and statistical tools. My personal background lead me to consider first applications in finance. Although historical approaches are not preferred in this domain, I have been progressively convinced that nonparametric statistics could play a major role in analyzing the massive (of very large dimension and high-frequency) financial data for detecting hidden structure in the latter and gaining advantage of the latter in risk assesment or portfolio selection for instance. As an illustration, the works I have carried out with Skander Slim in that direction are described in a word in this third part. Recently, I also happened to meet applied mathematicians or scientists working in other fields, which may naturally interface with applied probability ans statistics. Hence, applications to Toxicology, and in particular to toxic chemicals dietary exposure, has also been one of my concern this last year, which I have spent in the pluridisciplinary research unity Metarisk of the National Research Agronomy Institute, entirely dedicated to dietary risk analysis. I could thus make use of my skills in Markov modelling for proposing a stochastic model describing the temporal evolution of the total body burden of chemical (in a way that both the toxicokinetics and the dietary behavior may be taken into account) and adequate inference methods for the latter in a joint work with P. Bertail and J. Tressou. This line of research is still going on and will hopefully provide practical insight and guidance for dietary contamination control in public health practice. It is also briefly presented in this last part. Besides, I have the great opportunity to work currently on the modelling of the AIDS epidemic with H. de Arazoza, B. Auvert, P. Bertail, R. Lounes and C. Tran based on the cuban epidemic data available, which form one of the most informed database on any HIV epidemic. While such a research project (taking place in the framework of the ACI-NIM "Epidemic Modelling") aims at providing a numerical model (for computing incidence predictions on short horizons for instance, so as to plan the quantity of antiretrovirals required), it also poses very challenging probabilistic and statistical problems, ranging from the proof for the existence of a quasi-stationary distribution describing the long term behavior of the epidemic to the diculties encountered due to the incomplete character of the epidemic data available. Unfortunately, they are not discussed here, presenting the wide variety of mathematical problems arising in this project without denaturing it would have deserved a whole report. Ce rapport présente brièvement l'essentiel de mon activité de recherche depuis ma thèse de doctorat [53], laquelle visait principalement à étendre l'utilisation des progrès récents de l'Analyse Harmonique Algorithmique pour l'estimation non paramétrique adaptative dans le cadre d'observations i. i. d. (tels que l'analyse par ondelettes) à l'estimation statistique pour des données markoviennes. Ainsi qu'il est éxpliqué dans [123], des résultats relatifs aux propriétés de concentration de la mesure (i. e. des inégalités de probabilité et de moments sur certaines classes fonctionnelles, adaptées à l'approximation non linéaire) sont indispensables pour exploiter ces outils d'analyse dans un cadre probabiliste et obtenir des procédures d'estimation statistique dont les vitesses de convergence surpassent celles de méthodes antérieures. Dans [53] (voir également [54], [55] et [56]), une méthode d'analyse fondée sur le renouvellement, la méthode dite 'régénérative' (voir [185]), consistant à diviser les trajectoires d'une chaîne de Markov Harris récurrente en segments asymptotiquement i. i. d., a été largement utilisée pour établir les résultats probabilistes requis, le comportement à long terme des processus markoviens étant régi par des processus de renouvellement (définissant de façon aléatoire les segments de la trajectoire). Une fois l'estimateur construit, il importe alors de pouvoir quantifier l'incertitude inhérente à l'estimation fournie (mesurée par des quantiles spécifiques, la variance ou certaines fonctionnelles appropriées de la distribution de la statistique considérée). A cet égard et au delà de l'extrême simplicité de sa mise en oeuvre (puisqu'il s'agit simplement d'eectuer des tirages i. i. d. dans l'échantillon de départ et recalculer la statistique sur le nouvel échantillon, l'échantillon bootstrap), le bootstrap possède des avantages théoriques majeurs sur l'approximation asymptotique gaussienne (la distribution bootstrap approche automatiquement la structure du second ordre dans le développement d'Edegworth de la distribution de la statistique). Il m'est apparu naturel de considérer le problème de l'extension de la procédure traditionnelle de bootstrap aux données markoviennes. Au travers des travaux réalisés en collaboration avec Patrice Bertail, la méthode régénérative s'est avérée non seulement être un outil d'analyse puissant pour établir des théorèmes limites ou des inégalités, mais aussi pouvoir fournir des méthodes pratiques pour l'estimation statistique: la généralisation du bootstrap proposée consiste à ré-échantillonner un nombre aléatoire de segments de données régénératifs (ou d'approximations de ces derniers) de manière à imiter la structure de renouvellement sous-jacente aux données. Cette approche s'est révélée également pertinente pour de nombreux autres problèmes statistiques. Ainsi la première partie du rapport vise essentiellement à présenter le principe des méthodes statistiques fondées sur le renouvellement pour des chaînes de Markov Harris. La seconde partie du rapport est consacrée à la construction et à l'étude de méthodes statistiques pour apprendre à ordonner des objets, et non plus seulement à les classer (i. e. leur aecter un label), dans un cadre supervisé. Ce problème difficile est d'une importance cruciale dans de nombreux domaines d' application, allant de l'élaboration d'indicateurs pour le diagnostic médical à la recherche d'information (moteurs de recherche) et pose d'ambitieuses questions théoriques et algorithmiques, lesquelles ne sont pas encore résolues de manière satisfaisante. Une approche envisageable consiste à se ramener à la classification de paires d'observations, ainsi que le suggère un critère largement utilisé dans les applications mentionnées ci-dessus (le critère AUC) pour évaluer la pertinence d'un ordre. Dans un travail mené en collaboration avec Gabor Lugosi et Nicolas Vayatis, plusieurs résultats ont été obtenus dans cette direction, requérant l'étude de U-processus: l'aspect novateur du problème résidant dans le fait que l'estimateur naturel du risque a ici la forme d'une U-statistique. Toutefois, dans de nombreuses applications telles que la recherche d'information, seul l'ordre relatif aux objets les plus pertinents importe véritablement et la recherche de critères correspondant à de tels problèmes (dits d'ordre localisé) et d'algorithmes permettant de construire des règles pour obtenir des 'rangements' optimaux à l'égard de ces derniers constitue un enjeu crucial dans ce domaine. Plusieurs développements en ce sens ont été réalisés dans une série de travaux (se poursuivant encore actuellement) en collaboration avec Nicolas Vayatis. Enfin, la troisième partie du rapport reflète mon intérêt pour les applications des concepts probabilistes et des méthodes statistiques. Du fait de ma formation initiale, j'ai été naturellement conduit à considérer tout d'abord des applications en finance. Et bien que les approches historiques ne suscitent généralement pas d'engouement dans ce domaine, j'ai pu me convaincre progressivement du rôle important que pouvaient jouer les méthodes statistiques non paramétriques pour analyser les données massives (de très grande dimension et de caractère 'haute fréquence') disponibles en finance afin de détecter des structures cachées et en tirer partie pour l'évaluation du risque de marché ou la gestion de portefeuille par exemple. Ce point de vue est illustré par la brève présentation des travaux menés en ce sens en collaboration avec Skander Slim dans cette troisième partie. Ces dernières années, j'ai eu l'opportunité de pouvoir rencontrer des mathématiciens appliqués et des scientifiques travaillant dans d'autres domaines, pouvant également bénéficier des avancées de la modélisation probabiliste et des méthodes statistiques. J'ai pu ainsi aborder des applications relatives à la toxicologie, plus précisément au problème de l'évaluation des risque de contamination par voie alimentaire, lors de mon année de délégation auprès de l'Institut National de la Recherche Agronomique au sein de l'unité Metarisk, unité pluridisciplinaire entièrement consacrée à l'analyse du risque alimentaire. J'ai pu par exemple utiliser mes compétences dans le domaine de la modélisation maarkovienne afin de proposer un modèle stochastique décrivant l'évolution temporelle de la quantité de contaminant présente dans l'organisme (de manère à prendre en compte à la fois le phénomène d'accumulation du aux ingestions successives et la pharmacocinétique propre au contaminant régissant le processus d'élimination) et des méthodes d'inférence statistique adéquates lors de travaux en collaboration avec Patrice Bertail et Jessica Tressou. Cette direction de recherche se poursuit actuellement et l'on peut espérer qu'elle permette à terme de fonder des recommandations dans le domaine de la santé publique. Par ailleurs, j'ai la chance de pouvoir travailler actuellement avec Hector de Arazoza, Bertran Auvert, Patrice Bertail, Rachid Lounes et Viet-Chi Tran sur la modélisation stochastique de l'épidémie du virus VIH à partir des données épidémiologiques recensées sur la population de Cuba, lesquelles constituent l'une des bases de données les mieux renseignées sur l'évolution d'une épidémie de ce type. Et bien que ce projet vise essentiellement à obtenir un modèle numérique (permettant d'effectuer des prévisions quant à l'incidence de l'épidémie à court terme, de manière à pouvoir planifier la fabrication de la quantité d'anti-rétroviraux nécéssaire par exemple), il nous a conduit à aborder des questions théoriques ambitieuses, allant de l'existence d'une mesure quasi-stationnaire décrivant l'évolution à long terme de l'épidémie aux problèmes relatifs au caractère incomplet des données épidémiologiques disponibles. Il m'est <b>malheureusement</b> impossible d'évoquer ces questions ici sans risquer de les dénaturer, la présentation des problèmes mathématiques rencontrés dans ce projet mériterait à elle seule un rapport entier...|$|E
