14|55|Public
60|$|Sally {{had spoken}} softly, but a {{dynamite}} explosion {{could not have}} shattered her brother's composure with <b>more</b> <b>completeness.</b> In the leaping twist which brought him facing her, he rose a clear three inches from the floor. He had a confused sensation, as though his nervous system had been stirred up with a pole. He struggled for breath and moistened his lips {{with the tip of}} his tongue, staring at her continuously during the process.|$|E
60|$|It was an {{unpleasant}} surprise to Deronda {{when he returned}} from the Abbey to find the undesirable father installed in the lodgings at Brompton. Mirah had felt it necessary to speak of Deronda to her father, and even to make him as fully aware as she could {{of the way in which}} the friendship with Ezra had begun, and of the sympathy which had cemented it. She passed more lightly over what Deronda had done for her, omitting altogether the rescue from drowning, and speaking of the shelter she had found in Mrs. Meyrick's family so as to leave her father to suppose that it was through these friends Deronda had become acquainted with her. She could not persuade herself to <b>more</b> <b>completeness</b> in her narrative: she could not let the breath of her father's soul pass over her relation to Deronda. And Lapidoth, for reasons, was not eager in his questioning about the circumstances of her flight and arrival in England. But he was much interested in the fact of his children having a beneficent friend apparently high in the world.|$|E
60|$|But it was {{astonishing}} {{how little}} time she found for these vast mental excursions. Constantly {{she had to}} be on the scene as Mrs. Grandcourt, and to feel herself watched in that part by the exacting eyes of a husband who had found a motive to exercise his tenacity--that of making his marriage answer all the ends he chose, and with the <b>more</b> <b>completeness</b> the more he discerned any opposing will in her. And she herself, whatever rebellion might be going on within her, could not have made up her mind to failure in her representation. No feeling had yet reconciled her for a moment to any act, word, or look that would be a confession to the world: and what she most dreaded in herself was any violent impulse that would make an involuntary confession: it was the will to be silent in every other direction that had thrown the more impetuosity into her confidences toward Deronda, to whom her thought continually turned as a help against herself. Her riding, her hunting, her visiting and receiving of visits, were all performed in a spirit of achievement which served instead of zest and young gladness, so that all around Diplow, in those weeks of the new year, Mrs. Grandcourt was regarded as wearing her honors with triumph.|$|E
30|$|Now we {{introduce}} the following <b>more</b> extensive <b>completeness</b> for dislocated quasi-metric spaces, which is weaker than the 0 -σ-completeness.|$|R
50|$|See Game {{complexity}} for <b>more</b> games whose <b>completeness</b> for PSPACE {{or other}} complexity classes has been determined.|$|R
30|$|Kirk {{showed in}} [10] that the {{validity}} of Caristi’s fixed point theorem in a metric space characterizes its <b>completeness.</b> <b>More</b> exactly, he proved the following.|$|R
5000|$|Famed {{animator}} Joseph Barbera wrote {{a romantic}} comedy play which debuted on stage in Los Angeles in 1952 called The Maid and the Martian. It was about Captain Derro, a scout from Mars, who goes to Earth to help plan an invasion, but {{falls in love with}} a girl from Earth. The Los Angeles Times said the play [...] "has strong elements and might even go to Broadway... provided it gains <b>more</b> <b>completeness</b> in plot and situation." [...] The production was directed by Gordon Hunt and starred Pat Priest and ran successfully for seven weeks. The play was revived in 1954 with James Arness in the lead.|$|E
40|$|AbstractThis paper {{presents}} the navigation for a floor-mopping robot by new navigation algorithm. This algorithm {{is used for}} a robot that design of cleaning with rags. Which it makes a floor-mopping robot improved efficient. We designed the navigation algorithm using the behaviour-based structure to cover a working area {{as much as possible}} without wetting the floor too much. The effectiveness of the algorithm is tested on simulation only. The results indicated that the behaviour-based algorithm provided <b>more</b> <b>completeness</b> while maintaining low energy compared to other methods. We study other navigation algorithm compared with our algorithm. It includes with zigzag, random and our behaviour-based algorithms. From simulation results, I confirm that behaviour-based algorithm is better than zigzag and random algorithms...|$|E
40|$|Abstract- Semantic {{similarity}} {{measures have}} been used successfully and extensively in the biomedical research with various applications. As the biomedical ontologies, which form the main ground {{for most of the}} similarity measures, are growing and progressing towards <b>more</b> <b>completeness</b> and higher accuracy, the results and outcomes of these semantic similarity measures become more acceptable and more reliable in the field. In this paper, we investigate a path length based measure for prioritization of disease proteins and for computing the similarity between diseases and proteins. Our measure is based on the GO annotation terms of the proteins and uses a simple exponential transfer function to convert the path length to similarity score. The evaluation results prove that this similarity measure is fairly effective in assessing the closeness of proteins and diseases in the disease protein ranking and protein prioritization experiments. 1...|$|E
40|$|Abstract The paper {{describes}} tableaux based proof methods fortemporal logics {{of knowledge}} allowing interaction axioms between the modal and temporal components. Such logicscan {{be used to}} specify systems that involve the knowledge of processes or agents and which change over time, for exam-ple agent based systems or knowledge games. The interaction axioms allow the description of how knowledge evolvesover time and makes reasoning in such logics theoretically <b>more</b> complex. <b>Completeness</b> arguments for the tableauxare discussed. ...|$|R
40|$|In {{this paper}} we show how {{abstract}} interpretation, and <b>more</b> specifically <b>completeness,</b> provides an adequate model for reasoning about code obfuscation and watermarking. The {{idea is that}} making a program obscure, or equivalently hiding information in it, corresponds to force an interpreter (the attacker) to become incomplete in its attempts interpretation provides {{the model of the}} attacker (malicious host) and abstract interpretation transformers provide driving methods for understanding and designing new obfuscation and watermarking strategies: Obfuscation corresponds to make the malicious host incomplete and watermarking corresponds to hide secrets where incomplete attackers cannot extract them unless some secret key is given. 1...|$|R
6000|$|... § 14. What is {{here said}} {{respecting}} {{the succession of}} the adjective and substantive is obviously applicable, by change of terms, to the adverb and verb. And without further explanation, it will be manifest, that {{in the use of}} prepositions and other particles, most languages spontaneously conform with <b>more</b> or less <b>completeness</b> to this law.|$|R
40|$|Early {{orientation}} towards {{tasks of}} the application domain to be supported by a software system has been proposed as a fruitful means for achieving more appropriate and usable systems {{as well as for}} focusing the requirements engineering process. Besides goal orientation, task orientation has therefore been recognized as a promising concept for assuring <b>more</b> <b>completeness</b> and correctness of requirements specifications, and better integration with usability engineering issues. In this paper, we present experiences made with the task-oriented requirements engineering framework "TORE" in four different case studies. These case studies were selected based on their contextual specifics, like service orientation or ambient intelligence,that might impose certain challenges on task-oriented RE. As a lesson learned, we experienced TORE to be highly beneficial even in systems that {{do not seem to be}} "traditional" information systems at first glance. However, we also identified limitations that call for necessary adaptations...|$|E
40|$|The {{concept of}} Enhanced Maps (Emaps) was {{introduced}} with one main objective : it should characterize roads, first, with <b>more</b> <b>completeness,</b> and second, with more accuracy than standard maps, {{in order to}} fulfill the requirements of new challenging road safety applications and advanced driver assistance systems. This article introduces a paradigm for Emap definition and creation on which every road lane is represented and topologically connected to the rest of lanes. A number of Emaps have been created in France, Germany and Sweden following this approach. The experiments carried out in these test sites with the Emaps show the capability of our Emap definition to assist the determination of the vehicle position at the lane level. Details of the processes of extraction and connection of the road segments are given in the core of the article, as well as a discussion of the elaboration process and future guidelines in conclusion...|$|E
40|$|AbstractThe article {{presents}} {{the problem of}} knowledge in knowledge-based systems, such as advisory systems used in construction engineering. The unique characteristics of construction engineering translate directly into unique characteristics of knowledge resources, which {{is evident in the}} potential sources of knowledge. Many of them are not open, uncertain, fuzzy, of different credibility, and incomplete. One of the knowledge sources is the mental models of experts working in specific fields of construction engineering. Based on the knowledge acquisition sessions that have been completed, it can be concluded that only a certain part of the knowledge contained in mental models has been acquired. In order to ensure <b>more</b> <b>completeness</b> of the knowledge and explain the mechanism of inference, the KBANN (Knowledge Based Artificial Neural Network) algorithm was used, which enables extracting rules that are {{not a part of the}} original state of knowledge using trained neural networks. This method effectively supports the process of construction of advisory systems...|$|E
30|$|Proof The {{following}} proof {{follows the}} same lines as previous proofs of related results in [7, 13], but we reproduce it {{for the sake of}} <b>completeness.</b> <b>More</b> precisely, {{the first part of the}} proof for Theorem 19, the proof up to equation (2.13), is analogous to the corresponding proof of Harjani et al. in [7]. But, for the general readership, we give all the details here.|$|R
40|$|The {{purpose of}} this thesis is to {{investigate}} a topological construction using ele-mentary submodels. In the second chapter, the construction is used to obtain some results on realcompactness and <b>completeness.</b> <b>More</b> interestingly, the construction is used to prove the separable case of Nikiel’s conjecture that a space is compact and monotonically normal {{if and only if}} it is the continuous image of some compact, linearly ordered space. This result is originally due to M. E. Rudin by a more difficult method. Approved...|$|R
40|$|Three-dimensional {{representation}} {{is becoming an}} effective supportfor the documentation {{of the state of}} conservation of heritage artefacts, for thestudy of its transformations and for cultural diffusion. 3 D digitizationtechnologies now offer effective means to observe and analyze historicbuildings with <b>more</b> accuracy, <b>completeness</b> and timeliness. Nevertheless, itproduces a real problem of information overload. The growing mass of uninterpreteddata make emerge a need for innovative methodologies assistingdata processing, sorting and analysis by researchers who want to use it foradvancing the knowledge of cultural heritage. Exploring the informationalvalue of these new representation systems allows introducing new approachesto the analysis of artefacts so distant in space but so close in features(typologies, styles, compositional rules, etc.). This chapter presents someresearch avenues for defining a geometric /semantic description model ofarchitectural elements in order to integrate the informative value of 3 Ddigitization in intelligible representations. info:eu-repo/semantics/publishe...|$|R
30|$|However, {{most of the}} {{researches}} of UCON {{are concentrated}} on the basic conceptual level at present, where the theory is almost not considered of cloud computing. Therefore, it will be great significance to resolve the existing questions of secure access control in cloud computing by building a usage control model with dynamic license certification [19, 20] and security access control mechanism. In this paper, we proposed a new usage control protocol model, namely multi-UCON (MUCON) with secure dynamic authentication and authorization mechanisms, including three authorization models, machine to machine coding, sharing with one user domain, and making the rights transfer under controlled as well as flexible accrediting which can be applied. Permissions monitor embedded in the client is used to monitor and control the user’s activity; when the user accesses beyond his rights, the monitor will cooperate with the content server to investigate and affix {{the responsibility of the}} user’s illegal activity, so as to reduce pressure on servers and prevent illegal violations of user privacy. Meanwhile, embedded in the digital content with digital watermarking of strong robustness for copyright information is to ensure that digital content is illegal in cracking the case of the responsibility of the user, thus making the protection of digital content <b>more</b> <b>completeness.</b>|$|E
40|$|The {{object of}} this study is to look at {{categorical}} approaches to many valued logic, both propositional and predicate, to see how different logical properties result from different parts of the situation. In particular, the relationship between the categorical fabric I introduced at Linz in 2004 and the Fuzzy Logics studied by Hajek (2003) [5], Esteva et al. (2003) [1], and Hajek (1998) [4], comes from restricting the kind of structures used for truth values. We see how the structure of the various kinds of algebras shows up in the categorical logic, giving a variant on natural deduction for these logics. Quantification typically needs <b>more</b> <b>completeness</b> than is present in the algebras used in Hajek (1998) [4], hence the need for safe interpretations. The categorical setting gives a predicate logic without variables. The language in the more traditional sense comes from a structure built on a particular freely generated Cartesian category. Formulas have a clear meaning in that more restricted context. Interpretation of the language in other categorical fabrics is given by application of a product preserving functor. Traditional completeness results relate to this kind of interpretation. Completeness can also be understood as showing that the derivable truths in the general fabric are the necessary truths: those which are true in all of the possible worlds...|$|E
40|$|Purpose: In {{life cycle}} {{assessment}} (LCA), literature suggests accounting for land as a resource either by what it delivers (e. g., biomass content) or the time and space needed to produce biomass (land occupation), {{in order to avoid}} double-counting. This paper proposes and implements a new framework to calculate exergy-based spatial explicit characterization factors (CF) for land as a resource, which deals with both biomass and area occupied on the global scale. Methods: We created a schematic overview of the Earth, dividing it into two systems (human-made and natural), making it possible to account for what is actually extracted from nature, i. e., the biomass content was set as the elementary flow to be accounted at natural systems and the land occupation (through the potential natural net primary production) was set as the elementary flow at human-made systems. Through exergy, we were able to create CF for land resources for these two different systems. The relevancy of the new CF was tested for a number of biobased products. Results and discussion: Site-generic CF were created for land as a resource for natural systems providing goods to humans, and site-generic and site-dependent CF (at grid, region, country, and continent level) were created for land as a resource within human-made systems. This framework differed from other methods in the sense of accounting for both land occupation and biomass content but without double-counting. It is set operationally for LCA and able to account for land resources with <b>more</b> <b>completeness,</b> allowing spatial differentiation. When site-dependent CF were considered for land resources, the overall resource consumption of certain products increased up to 77 % in comparison with site-generic CF-based data. Conclusions: This paper clearly distinguished the origin of the resource (natural or human-made systems), allowing consistent accounting for land as a resource. Site-dependent CF for human-made systems allowed spatial differentiation, which was not considered in other resource accounting life cycle impact assessment methods...|$|E
30|$|We {{are aware}} that the {{evaluation}} approach we used for mapping to phenotype is biased toward detecting erroneous information and that errors of omission can be missed with this approach. However, since {{a large body of}} information was captured, and since no other knowledge-base of age and phenotypes currently exists, we consider accuracy to be <b>more</b> important than <b>completeness.</b> The APK can now serve as a baseline for future efforts to improve the completeness, as well as the accuracy of age-phenotype extraction from PubMed.|$|R
40|$|The maximum clique problem {{provides}} a classic framework for detecting cohesive sub-graphs. However, this approach can fail to detect {{much of the}} cohesive structure in a graph. To address this issue, Seidman and Foster introduced k-plexes as a degree-based relaxation of graph <b>completeness.</b> <b>More</b> recently, Balasundaram et al. formulated the maximum k-plex problem as an integer program and designed a branch-and-cut algorithm. This paper derives a new upper bound on the cardinality of k-plexes and adapts combinatorial clique algorithms to find maximum k-plexes. 1...|$|R
40|$|The simply typed -calculus {{is known}} to be {{complete}} with respect to models of the form Sets. More formally, that means that given any simply typed - theory T, T ` t 1 = t 2 i for all T-models [[]] in Sets we have [[t 1]] = [[t 2]]. It follows from a result by Awodey that it is enough to look at models in Sets for P a poset. For my thesis, I will describe explicitly how this <b>more</b> powerful <b>completeness</b> result follows from his recent paper. As models of the form for P a poset resemble the Kripke models familiar from intuitionistic logic, they are relatively easy for non-category theorists to understand. We hope that the simpler semantics result in new applications of the simply typed -calculus. We also describe how this gives a complete semantics of the simply typed -calculus in Pos=P...|$|R
40|$|The usual {{operations}} {{on the distribution}} network such as switching loads and circuits, the proliferation of power electronic equipment and non-linear loads and the distributed generation with renewable energy are several {{of the most common}} causes that are leading to an increasing polluted power system in terms of voltage signal distortion. One way of improving the power quality (PQ) parameters consists of analyzing these disturbances efficiently and understanding them deeply and PQ monitoring is one major task in order to achieve it. PQ monitoring is not an easy task usually involving sophisticated hardware instrumentation and software packages. Many recent approaches in PQ monitoring try to achieve it through the automated classification of different disturbances. The different approaches in this field lead their efforts in two directions, the main parts that form an automated classification. The first make focus to obtain a suitable pattern that allow distinguish clearly each disturbance, by the use of time-frequency transforms to get feature extraction, as Wavelet transform (WT) and S-transform (ST). The second is oriented to use a classifier able to assign each disturbance correctly in its class, so the most of the artificial intelligent techniques have been combined with WT or ST, as Artificial Neural Networks (ANN), Decision Tree, Fuzzy Logic, Hidden Markov Model, Support Vector Machines, etc. In this work an automated classification system based on time-frequency transform as a feature extraction tool in combination with Artificial Neural Network (ANN) as algorithm classifier is presented. As feature extraction tool have been used both, WT and ST, {{in order to make a}} comparison between them. ANN is discussed as an example of a robust classification algorithm, so it is chosen for obtaining experimental results. In addition, two variants, backpropagation (BP) and probabilistic (PNN) have been used for <b>more</b> <b>completeness</b> of the results. An automated classification system based on Wavelet transform as a feature extraction tool in combination with Artificial Neural Network as algorithm classifier is presented. The most usual PQ disturbances have been generated according to mathematical models to obtain experimental results. Noise is added to the signals from 40 dB to 20 dB. At last, signals generated by power network simulation using PSCAD/EMTDC environment have been used to check the reliability of the resulting systems based on different time-frequency transforms...|$|E
40|$|Objetivo: Estudo descritivo da morbidade por acidentes de tr??nsito (atropelamentos, colis??es e outros) em hospitais, tendo como objetivo a caracteriza????o das v??timas, dos diferentes agravos sofridos e do atendimento prestado. M??todos: Foram estudados 2 hospitais municipais do Rio de Janeiro, RJ, Brasil. Definiram-se como acidentes de tr??nsito os eventos: atropelamentos, colis??es e outros acidentes de tr??nsito. A coleta de dados referiu-se ao atendimento dos meses de maio (Hospital 1) e junho (Hospital 2) de 1996 e foi realizada por 7 equipes que se revesaram em plant??es de 12 horas ao longo de todo per??odo estudado, perfazendo 24 horas/dia de coleta. As causas declaradas pelos pacientes ou, na impossibilidade deste, pelo socorrista ou acompanhante, foram a fonte de classifica????o dos eventos. Resultados: Foram atendidos 320 casos no Hospital 1 e 290 no Hospital 2. Em ambos os hospitais, os homens foram os mais atingidos (69, 3 por cento) e a faixa et??ria mais afetada foi a de 20 a 39 anos (60, 5 por cento no Hospital 1 e 47, 5 por cento no Hospital 2). Dos 610 casos, a maioria foi devido a atropelamentos (49, 3 por cento), seguidos pelas colis??es (35, 6 por cento) e outros acidentes (15, 1 por cento). Conclus??es: A an??lise da distribui????o dos casos e do tipo de atendimento demandado corrobora a sugest??o de uma melhor organiza????o das equipes, maior rigor no registro hospitalar e os aspectos que deveriam ser mais enfatizados nas campanhas de preven????o. Introduction Morbidity due {{to motor}} vehicle {{accidents}} was analysed in a descriptive study based on reports of hospitals {{with the objective}} to characterize their victims, types of injuries, and the health care provided. Methods The study {{was carried out in}} two municipal hospitals of Rio de Janeiro, Brazil. Motor vehicle accidents were defined as a run over, a collision, and ???other traffic accidents???. Data was collected from cases seen in May (Hospital 1) and June (Hospital 2) of 1996 by 7 teams that alternated in shifts of 12 hours, covering the collection 24 hours per day. The events were classified according to the victim or, when it was impossible, to the person who helped them or accompanied them to the hospital. Results In the study period, 320 cases were seen in Hospital 1 and 290 in Hospital 2. Most of the victims were men (69. 3 %) in both of hospitals and the most affected group was young adults between 20 to 39 years (60. 5 % in Hospital 1 and 47. 5 % in Hospital 2). Of all 610 cases, the main cause of injury was run overs (49. 3 %), followed by collisions (35. 6 %), and ???other traffic accidents??? (15. 1 %). Conclusions The analysis of the cases distributions and the type of care required corroborates with the need of a better organization of hospital care teams, <b>more</b> <b>completeness</b> of hospital reports and emphasis on some aspects in prevention campaigns...|$|E
40|$|The abductive {{procedure}} SLDNFA {{is presented}} for normal abductive programs. The procedure {{is a natural}} extension of SLDNF-resolution and incorporates both abduction and negation as failure. The main difference between our approach and existing procedures is the treatment of non-ground abductive goals. The soundness of SLDNFA wrt 3 -valued completion semantics is proven. Also a completeness theorem is provided (where completeness here refers to a specific notion of generating all minimal abductive solutions). Several variants of SLDNFA are given, which generate even more solutions. For these procedures, <b>more</b> powerful <b>completeness</b> theorems can be formulated. The research presented here, provides a simple framework of abductive procedures, in which a number of parameters can be set, in order to fit the abduction procedure to the application under consideration. 1 Introduction Negation as failure and abduction have been recognized as important forms of non-monotonic reasoning [23], [4], [36 [...] ...|$|R
40|$|The {{purpose of}} the study is to revisit the concept of {{marketing}} from system perspective in the backdrop of increased environmental dynamism. System view, with its holistic and diversified characteristic helps to bring in <b>more</b> agility and <b>completeness</b> in overall marketing efforts of the firmby effectively coordinating and efficiently integrating the efforts ofvarious contributing elements of marketing. With the insights based on extensive literature review of system and marketing, paper conceptualizes the framework for marketing from system perspective and concludes with the benefits associated with this holistic approach through the help of real world examples...|$|R
40|$|Within the {{mathematical}} logic field, much {{effort has been}} devoted to prove completeness of different axiomatizations with respect to classes of algebras de-fined on the real unit interval [0, 1] (see for instance [1] and [2]), but in general, what has been mainly achieved are axiomatizations and results concerning fini-tary completeness, that is, for deductions from a finite number of premises. In this work we are concerned with the problem of strong completeness, i. e., completeness for deductions from an arbitrary number of premises. In particular, we will focus on showing strong completeness for logics of a left-continuous t-norm. These will be extensions of the monoidal t-norm based logic, MTL, the logic of prelinear, bounded, commutative and integral residuated lattices [3], expanded with rational truth-constants and with an arbitrary set of connectives under some constraints. It is known that MTL is strongly complete with respect to the class of all standard algebras based of left-continuous t-norms [2]. Some particular exten-sions of MTL enjoy <b>more</b> concrete <b>completeness</b> results: BL, Gödel, Product o...|$|R
40|$|Abstract. Motivated by {{the fact}} that nearly all {{conditional}} logics are axiomatised by so-called shallow axioms (axioms with modal nesting depth ≤ 1) we investigate sequent calculi and cut elimination for modal logics of this type. We first provide a generic translation of shallow axioms to (one-sided, unlabelled) sequent rules. The resulting system is complete if we admit pseudo-analytic cut, i. e. cuts on modalised propositional combinations of subformulas, leading to a generic (but sub-optimal) decision procedure. In a next step, we show that, for finite sets of axioms, {{only a small number of}} cuts is needed between any two applications of modal rules. <b>More</b> precisely, <b>completeness</b> still holds if we restrict to cuts that form a tree of logarithmic height between any two modal rules. In other words, we obtain a small (PSPACE-computable) representation of an extended rule set for which cut elimination holds. In particular, this entails PSPACE decidability of the underlying logic if contraction is also admissible. This leads to (tight) PSPACE bounds for various conditional logics. ...|$|R
40|$|The {{completeness}} {{of buildings}} in OpenStreetMap (OSM) is estimated for a medium-sized German {{city and its}} surroundings by comparing the OSM data with data from an official building cadastre. As completeness measures we apply two unit-based methods that are frequently applied in similar studies. It is found that the estimation of OSM building completeness strongly differ between the methods. A count ratio (number of OSM buildings / number of reference buildings) tends to underestimate the actual building completeness and an area ratio (total OSM building area / total reference building area) instead tends to overestimate the completeness within the study area. It is argued that a simple pre-processing of the building footprint polygons leads to a <b>more</b> accurate <b>completeness</b> estimation when applying the count ratio. It is also suggested to more carefully examine the areas that have been mapped in OSM {{but not in the}} reference data set (false positives). In the present study region, these values are mainly due to simplified OSM polygons and they contribute to an overestimation of the OSM building completeness when applying the area ratio...|$|R
40|$|Abstract: Geology is {{the base}} for {{highways}} and tunnels construction. With the fast development of national highway construction, highway tunnel construction project are more and <b>more</b> complex. The <b>completeness</b> and accuracy are essential for the planning, design and construction of projects, while the ground information is quite poor in systematic, reliable and timely aspects. Therefore, the development of underground road tun-nels, and the implementation of informationized spatial information management is urgent for highway con-struction. 3 D geological tunnel model is intuitive, high efficient and convenience which greatly facilitates the maintenance and security of highway tunnels construction {{and it will be}} the trend for the future highway tun-nel development...|$|R
40|$|Insights from {{transaction}} cost economics {{were used to}} study the boundary conditions underlying the role conflict and ambiguity of 265 CEOs in Chinese-based international joint ventures. Role conflict and ambiguity were lower when the contract between parents was <b>more</b> complete. Contract <b>completeness</b> fully mediated the effects of parent objective gap and parent formalization on role ambiguity but only partially so {{in the case of}} role conflict. Role conflict was lower when the foreign parent was dominant in the venture but higher when the local parent was dominant. Role conflict and ambiguity were inversely related to cultural distance. Neither construct had a detrimental effect on international joint venture performance. Implications for role theory are discussed...|$|R
40|$|We are {{indebted to}} Dr. O’Brien [l] for calling {{the work of}} Carre [2] to the {{attention}} of us and perhaps of other investigators. Our interest in the omega equation stems from a desire to use this equation to obtain diagnostic vertical niotions for careful analysis of the three-dimensional structure of atmospheric systems. Having started such studies late in 1960 prior to the work of Carre and prior to knowledge of Miyakoda’s [3] work, I was forced into the more empirical approach for finding the optimum over-relaxation factor, aopr, as mentioned in our paper (Stuart and O’Neill [4]). Since some knowledge of the over-relaxation factor was needed for our work, we decided to search a bit for aopl. We tested the 1 -D and 2 -D cases <b>more</b> for <b>completeness</b> and to show that our relaxation technique fitted the known theoretical results. Suffice it to say that ol,r paper reports on the results of an (yopl study for a particular grid and stability profile and we hope other investigators will benefit by Carre’s paper and our results. Indeed lve have follolved the approach suggested by O’Brien {{at the end of his}} section 4, but without the aid o...|$|R
40|$|The Sloan Digital Sky Survey (SDSS) will observe around 10 ^ 6 spectra from targets {{distributed}} over an area {{of about}} 10, 000 square degrees, using a multi-object fiber spectrograph which can simultaneously observe 640 objects in a circular field-of-view (referred to as a ``tile'') 1. 49 degrees in radius. No two fibers can be placed closer than 55 '' during the same observation; multiple targets closer than this distance are said to ``collide. '' We present here a method of allocating fibers to desired targets given a set of tile centers which includes the effects of collisions and which is nearly optimally efficient and uniform. Because of large-scale structure in the galaxy distribution (which form {{the bulk of the}} SDSS targets), a naive covering the sky with equally-spaced tiles does not yield uniform sampling. Thus, we present a heuristic for perturbing the centers of the tiles from the equally-spaced distribution which provides <b>more</b> uniform <b>completeness.</b> For the SDSS sample, we can attain a sampling rate greater than 92 % for all targets, and greater than 99 % for the set of targets which do not collide with each other, with an efficiency greater than 90 % (defined as the fraction of available fibers assigned to targets) ...|$|R
40|$|AbstractIn this paper, {{a general}} {{solution}} for three-dimensional transversely isotropic piezoelectricity {{in terms of}} four quasi-quadri-harmonic functions is established first. Owing to complexity of the higher-order equation, {{it is difficult to}} obtain rigorous analytic solutions and in most cases this general solution is not applicable. By virtue of the generalized Almansi’s Theorem, the simplified generalized LHN solution and E–L solution expressed by lower order functions are achieved, respectively, by taking a decomposition and superposition technique. In the absence of piezoelectric coupling, these two simplified general solutions can be degenerated into those for transversely isotropic elasticity, i. e. LHN and E–L solutions. <b>More</b> importantly, the <b>completeness</b> of these two generalized solutions is proved if the domain is z-convex, no matter whether the characteristic roots are distinct or possibly equal to each other...|$|R
