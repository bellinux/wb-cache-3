4|55|Public
40|$|Abstract:Optimization of SRAM (Static Random Access Memory) array design {{can be done}} {{at three}} domains namely bit cell optimization, sense {{amplifier}} optimization and <b>memory</b> <b>decoder</b> optimization. In this paper, we focused on <b>memory</b> <b>decoder</b> optimization. The objective of the paper is to design speed and power efficient <b>memory</b> <b>decoder</b> structure and to implement 4 Kb SRAM array controller. We compared four NAND gate based decoder structures at TSMC 28 nm technology and OR style NAND decoder structure {{is found to be}} efficient. We also implemented 4 Kb SRAM array controller which is a byte accessible using binary decoder tree. All the logics have been designed using Cadence Virtuoso schematic editor and simulated using Spectre simulator with operating voltage of 0. 9 V...|$|E
40|$|This paper {{presents}} a low energy <b>memory</b> <b>decoder</b> architecture for ultra-low-voltage systems containing multiple voltage domains. Due to limitations in scalability of memory supply voltages, these systems typically contain a core operating at subthreshold voltages and memories operating {{at a higher}} voltage. This difference in voltage provides a timing slack on the memory path as the core supply is scaled. The paper analyzes the feasibility and trade-offs in utilizing this timing slack to operate a greater section of <b>memory</b> <b>decoder</b> circuitry at the lower supply. A 256 x 16 -bit SRAM interface has been designed in UMC 65 nm low-leakage process to evaluate the above technique with the core and memory operating at 280 mV and 500 mV respectively. The technique provides a reduction of up to 20 % in energy/cycle of the row decoder without any penalty in area and system-delay...|$|E
40|$|Abstract—Two novel <b>memory</b> <b>decoder</b> {{designs for}} {{reducing}} energy consumption and delay {{are presented in}} this paper. These two decoding schemes are compared to the conventional NOR decoder. Fewer word lines are charged and discharged by the proposed schemes which leads to less energy dissipation. Energy, delay, and area calculations are provided for all three designs under analysis. The two novel decoder schemes range from dissipating 3. 9 % to 23. 6 % of the energy dissipated by the conventional decoder. The delays of these designs are 80. 8 % of the conventional decoder delay. Simulations of the three decoders are performed using a 90 nm CMOS technology. I...|$|E
40|$|The main {{objective}} of this project is based upon majority decision decoding. A new approach to design fault-secure encoder and <b>decoder</b> circuitry for <b>memory</b> designs is introduced. In this project, synchronization between encoder <b>memory</b> and <b>decoder</b> takes placed properly. An 8 bit encoder connected with nano <b>memory</b> and <b>decoder</b> for synchronization purpose. LDPC codes satisfies a new, restricted definition for ECCs which guarantees that the ECC codeword has an appropriate redundancy structure such that it can detect multiple errors occurring in both the stored codeword in memory and the surrounding circuitries...|$|R
50|$|In CPU design, {{the use of}} a Sum Addressed Decoder (SAD) or Sum Addressed <b>Memory</b> (SAM) <b>Decoder</b> is {{a method}} of {{reducing}} the latency of the CPU cache access. This is achieved by fusing the address generation sum operation with the decode operation in the cache SRAM.|$|R
40|$|Upper {{and lower}} bounds for the {{probability}} of a decoding error event and for symbol error probability are developed for the class of time-varying phase codes known as multi-h codes. The effect of finite <b>decoder</b> <b>memory</b> is also treated. The analysis is illustrated with numerical examples, and simulation results are compared...|$|R
40|$|Proper {{encoding}} of {{transmitted information}} {{can improve the}} performance of a communication system. To recover the information at the receiver it is necessary to decode the received signal. For many codes the complexity and slowness of the decoder is so severe that the code is not feasible for practical use. This thesis considers the decoding problem for one such class of codes, the comma-free codes related to the first-order Reed-Muller codes. A factorization of the code matrix is found which leads to a simple, fast, minimum <b>memory,</b> <b>decoder.</b> The decoder is modular and only n modules are needed to decode a code of length 2 n. The relevant factorization is extended to any code defined by a sequence of Kronecker products. The problem of monitoring the correct synchronization position is also considered. A general answer seems to depend upon more detailed knowledge of the structure of comma-free codes. However, a technique is presented which gives useful results in many specific cases. ...|$|E
40|$|Abstract — A rate 3 / 8 (1, 3) {{constrained}} noncatastrophic encoder {{is constructed}} with free Hamming distance 3 and 4 encoder states. In comparison, a comparable code constructed by a conventional approach requires 8 encoder states {{and has a}} longer <b>decoder</b> <b>memory.</b> The code described herein was originally presented at th...|$|R
50|$|Decoding {{slices in}} the order they are {{received}} can result in additional memory consumption or impose higher throughput requirements on the <b>decoder</b> and local <b>memory</b> to run at higher clock speed. Consider an application in which the display operation reads the pictures to be displayed right from the section of <b>memory</b> where the <b>decoder</b> stored the pictures.|$|R
40|$|This paper {{presents}} a high-throughput <b>memory</b> efficient <b>decoder</b> for Low Density Parity Check (LDPC) codes in the high-rate wireless {{personal area network}} application. The novel techniques which can apply to our selected LDPC code is proposed, including parallel blocked layered decoding architecture and simplification of the WiGig networks. We use Real Time - Performance Evaluation Process Algebra (RTPEPA) to evaluate a typical LDPC Decoder system’s performance. The approach is more convenient, flexible, and lower cost than the former simulation method which needs develop special hardware and software tools. Moreover, we can easily analysis how changes in performance depend on changes in a particular modes by supplying ranges for parameter values...|$|R
40|$|This paper {{presents}} a cost-effective scalable quasi-cyclic LDPC (QC-LDPC) decoder architecture for non-volatile memory systems (NVMS). A re-arranged architecture is proposed {{to eliminate the}} first-in-first-out (FIFO) memory in conventional decoders, where the FIFO size is linearly proportional to the codeword size. The area reduction is 18. 5 % compared to the conventional decoder architecture. The scalable datapaths of the proposed decoder reduce the re-design cost and enable the flexibility of using QC-LDPC codes for NVMS. A prototyping decoder with maximum codeword size of 9280 bits is implemented in TSMC 90 nm CMOS technology, and the core area is only 2. 52 mm 2 at 138. 8 MHz. Index Terms—Non-volatile <b>memory,</b> scalable <b>decoder,</b> re-arranged architecture, QC-LDPC codes, TDMP algorithm. 1...|$|R
30|$|Because of this {{propagation}} time, consistency conflicts (i.e., {{a component}} decoder performs a read access before the write access {{of the other}} component decoder is completed.) may occur in extrinsic information memory. Hence, the symbols suffering from consistency conflict must have one <b>memory</b> bank per <b>decoder</b> and their extrinsic information values are exchanged in the time interval of two iterations instead of one for other symbols. Consequently, the convergence of the shuffled decoding process is slowed down.|$|R
40|$|We {{present a}} method for {{ordering}} the wavelet coefficient information in a compressed bitstream that allows an image to be sequentially decoded, with lower memory requirements than conventional wavelet decompression schemes. We also introduce a hybrid filtering scheme that uses different horizontal and vertical filters, each with different depths of wavelet decomposition. This reduces <b>decoder</b> <b>memory</b> requirements by reducing the instantaneous number of wavelet coefficients needed for inverse filtering. Index Terms [...] - Color image coding, data compression, source coding, wavelet transforms, zerotrees...|$|R
40|$|Abstract. Recently Network-on-Chip (NoC) {{technique}} {{has been proposed}} as a promising solution for on-chip interconnection network. However, different interface specification of integrated components raises a considerable difficulty for adopting NoC techniques. In this paper, we present a generic architecture for network interface (NI) and associated wrappers for a networked processor array (NoC based multiprocessor SoC) {{in order to allow}} systematic design flow for accelerating the design cycle. Case studies for <b>memory</b> and turbo <b>decoder</b> IPs show the feasibility and efficiency of our approach...|$|R
40|$|Abstract: Low-Density Parity-Check (LDPC) code is {{one kind}} of {{prominent}} error correcting codes (ECC) being considered in next generation industry standards. The decoder implementation complexity has been the bottleneck of its application. This paper presents {{a new kind of}} high-throughput and <b>memory</b> efficient LDPC <b>decoder</b> architecture. In general, more than fifty percent of memory can be saved over conventional partially parallel decoder architectures. It is shown that this presented hardware structure will be highly competent in high throughput and low decoding latency applications. Key-Words:- Low-density parity-check (LDPC) codes, VLSI architecture, decoder, shift LDP...|$|R
40|$|In {{this paper}} we propose low {{complexity}} LDPC code design and decoding in Z_ 4 which {{may be useful to}} combat phase ambiguities in wireless links affected by strong phase noise. We approximate messages exchanged on the Tanner graph using separable probability density functions. This allows a substantial reduction of <b>decoder</b> <b>memory</b> and complexity, with a negligible performance penalty, compared to ideal Z_ 4 decoding. Furthermore, we show that the Density Evolution analysis of this suboptimal decoder leads to irregular LDPC designs matching the criteria of binary LDPC codes...|$|R
40|$|We {{present a}} design method (called STD architecture) to design large {{memories}} {{so that the}} test time does not increase with the increasing size of memory. Large memories can be constructed by using several small blocks of memory. The <b>memory</b> address <b>decoder</b> {{is divided into two}} or more levels and designed such that during the test mode all small memory blocks are accessed together. With the help of modified <b>decoder,</b> all small <b>memory</b> blocks are tested in parallel using any standard test algorithm. In this design, time to test the whole memory is equal to the time required to test one small block. The proposed design is highly structured and hardware overhead is negligible. The basic idea is to exploit internal hardware for testing purpose. With the proposed method a constant test time can be achieved irrespective of the memory size. STD architecture is applicable to memory chips as well as memory boards, and the design is suitable for fault detection as well as for fault diagnosis...|$|R
40|$|Abstract — We {{consider}} zero-delay or fixed finite-delay joint {{source channel}} coding of Markov sources using finite <b>memory</b> encoder and <b>decoder.</b> The {{objective is to}} choose designs that minimize expected total distortion over a finite horizon, expected discounted distortion over an infinite horizon and average distortion per unit time over an infinite horizon. The above problem is a dynamic team with non-classical information structure. We develop a sequential decomposition for this problem. The main contribution {{of this paper is}} to provide a systematic methodology for determination of optimal joint source-channel encoding-decoding strategies for zero-delay or fixed finite-delay point-to-point communication with limited memory. I...|$|R
40|$|International Telemetering Conference Proceedings / September 28 - 30, 1976 / Hyatt House Hotel, Los Angeles, CaliforniaThe {{probability}} of error for a convolutional coded communication system can be predicted using three approaches. The first approach is using Gallager's [1] exponential bound for random codes. The second technique {{is the use of}} transfer function union bounds developed by Viterbi [2]. Finally, an approximation to the transfer function developed by Huth and Weber [3] can be used. This paper compares the three approaches for predicting performance and presents results for Viterbi decoding bit error probability, burst error probability, and <b>decoder</b> <b>memory</b> length...|$|R
40|$|Abstract. This paper {{investigates the}} {{security}} of FTT (fully collusion resistant traitor tracing) schemes in terms of DOT (Denial Of Tracing) and framing. With DOT attack, a decoder is able to detect tracing activity, and then prolongs the tracing process such that the tracer is unable to complete tracing job in a realistic time duration and hence has to abort his effort. On the other hand, by merely embedding several bytes of non-volatile <b>memory</b> in the <b>decoder,</b> we demonstrate, for the FTT schemes, how the decoder can frame innocent users at will. Furthermore, we propose a countermeasure on the framing attack. ...|$|R
40|$|Machine {{translation}} {{is going through}} a radical revolution, driven by the explosive development of deep learning techniques using Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN). In this paper, we consider a special case in machine translation problems, targeting to translate natural language into Structural Query Language (SQL) for data retrieval over relational database. Although generic CNN and RNN learn the grammar structure of SQL when trained with sufficient samples, the accuracy and training efficiency of the model could be dramatically improved, when the translation model is deeply integrated with the grammar rules of SQL. We present a new encoder-decoder framework, with a suite of new approaches, including new semantic features fed into the encoder as well as new grammar-aware states injected into the <b>memory</b> of <b>decoder.</b> These techniques help the neural network focus on understanding semantics of the operations in natural language and save the efforts on SQL grammar learning. The empirical evaluation on real world database and queries show that our approach outperform state-of-the-art solution by a significant margin...|$|R
40|$|Abstract—Focusing on {{internal}} high-voltage () switching and generation for low-voltage NAND flash memories, {{this paper describes}} a switch, row decoder, and charge-pump circuit. The proposed nMOS switch is composed of only intrinsic high-voltage transistors without channel implantation, which realizes both reduction of the minimum operating voltage and elimination of the leakage current. The proposed row decoder scheme is described in which all blocks are in selected state in standby so as to prevent standby current from flowing through the proposed switches in the row decoder. A merged charge-pump scheme generates a plurality of voltage levels with an individually optimized efficiency, which reduces circuit area {{in comparison with the}} conventional scheme that requires a separate charge-pump circuit for each voltage level. The proposed circuits were implemented on an experimental NAND flash memory. The charge pump and switch successfully operated at a supply voltage of 1. 8 V with a standby current of 10 A. The proposed pump scheme reduced the area required for charge-pump circuits by 40 %. Index Terms—Charge-pump circuit, high-voltage switching, low supply voltage, NAND flash <b>memories,</b> row <b>decoder.</b> I...|$|R
30|$|Meanwhile, {{the error}} {{correction}} capability of LDPC code decoding gradually improves as we increase the memory sensing precision. If NAND flash memory uses conventional hard-decision memory sensing (i.e., {{there are only}} l[*]−[*] 1 sensing quantization levels for l-level per cell NAND flash <b>memory),</b> LDPC code <b>decoder</b> can only carry out hard-decision decoding (e.g., using the hard-decision bit-flipping decoding algorithm) and achieve relatively poor error correction capability. As NAND flash memory uses soft-decision memory sensing with higher and higher sensing quantization granularity, LDPC code decoder can carry out soft-decision decoding (e.g., using the sum–product or min–sum decoding algorithm) and achieve stronger and stronger error correction capability.|$|R
40|$|This paper {{presents}} a low power LDPC decoder design based on {{reducing the amount}} of memory access. By utilizing the column overlapping of the LDPC parity check matrix, the amount of access for the memory storing the posterior values is minimized. In addition, a thresholding decoding scheme is proposed which reduces the memory access by trading off the error correcting performance. The decoder was implemented in TSMC 0. 18 μm CMOS process. Experimental results show that for a LDPC decoder targeting for IEEE 802. 1 In, the power consumption of the <b>memory</b> and the <b>decoder</b> can be reduced by 72 % and 24 %, respectively. Copyright 2008 ACM...|$|R
40|$|International audienceA {{particular}} type of conflict due to multiple-diagonal sub-matrices in the DVB-S 2 parity-check matrices is known to complicate {{the implementation of the}} layered decoder architecture. The new matrices proposed in DVB-S 2 X no longer use such sub-matrices. For implementing a decoder compliant both with DVB-S 2 and DVB-S 2 X, we propose an elegant solution which overcomes this conflicts relying on an efficient write disable of the memories, allowing a straightforward implementation of layered LDPC decoders. The complexity and latency are further reduced by eliminating one barrel shifter. Compared with the existing solutions, complexity is reduced without performance degradation. Keywords—Low-Density Parity-Check (LDPC) code, <b>memory</b> conflict, layered <b>decoder,</b> DVB-S 2, DVB-S 2 X...|$|R
40|$|In this paper, {{we present}} a novel fault {{detection}} and fault diagnosis technique for Field Programmable Gate Arrays (FPGAs). The cell is configured to implement a bijective function to simplify the testing of the whole cell array. The whole chip is partitioned into disjoint one-dimensional arrays of cells. For the lookup table (LUT), a fault may occur at the <b>memory</b> matrix, <b>decoder,</b> input or output lines. The input patterns can be easily generated with a k-bit binary counter, where k denotes the number of input lines of a configurable logic block (CLB). Theoretical proofs show that the resulting fault coverage is 100 %. According to {{the characteristics of the}} bijective cell function, a novel built-in self-test structure is also proposed. Our BIST approaches have the advantages of requiring less hardware resources for test pattern generation and output response analysis. To locate a faulty CLB, two diagnosis sessions are required. However, the maximum number of configurations is k + 4 for diagnosing a faulty CLB. The diagnosis complexity of our approach is also analyzed. Our results show that the time complexity is independent of the array size of the FPGA. In other words, we can make the FPGA array C-diagnosable...|$|R
40|$|Attention-based Neural Machine Translation (NMT) models {{suffer from}} {{attention}} deficiency issues {{as has been}} observed in recent research. We propose a novel mechanism {{to address some of}} these limitations and improve the NMT attention. Specifically, our approach memorizes the alignments temporally (within each sentence) and modulates the attention with the accumulated temporal <b>memory,</b> as the <b>decoder</b> generates the candidate translation. We compare our approach against the baseline NMT model and two other related approaches that address this issue either explicitly or implicitly. Large-scale experiments on two language pairs show that our approach achieves better and robust gains over the baseline and related NMT approaches. Our model further outperforms strong SMT baselines in some settings even without using ensembles. Comment: 8 page...|$|R
40|$|This paper {{demonstrates}} our {{implementation of}} a dynamically reconfigurable network on chip router with bus based interface. Our work targets heterogeneous integration of components in NoC architecture and includes modeling of reconfigurable components, processor cores and fixed IPs. The novelty of the proposed NoC lies {{in its ability to}} integrate standard non-packet based components thus reducing design time and ease of integration. A system consisting of an ARM processor, reconfigurable FFT, reconfigurable Viterbi <b>decoder,</b> <b>memory</b> controller and peripherals is considered with the option of system scalability for future upgrades. A framework for system level modeling of reconfigurable NoC with reconfigurable components is also proposed and demonstrated in systemC. Results are compared with implementation of the same system with conventional NoC to demonstrate advantages of the proposed NoC architecture. I...|$|R
40|$|We {{present a}} method for {{ordering}} the wavelet coefficient information in a compressed bit stream that allows an image to be sequentially decoded, with lower memory requirements than conventional wavelet decompression schemes. We also introduce a hybrid filtering scheme that uses different horizontal and vertical filters, each with different depths of wavelet decomposition. This reduces <b>decoder</b> <b>memory</b> requirements by reducing the instantaneous number of wavelet coefficients needed for inverse filtering. 1 Introduction We consider the transmission of compressed image or video to inexpensive output devices, such as color printers and wireless videophones, where the amount of on-board memory is tightly constrained. While some existing compression algorithms have low memory requirements, many "high-memory" wavelet-based algorithms have superior distortion vs. rate performance, such as embedded zerotree wavelet (EZW) coding, introduced by Shapiro [1] and later refined by Said and Pearlman [...] ...|$|R
40|$|We {{investigate}} {{the impact of}} multimedia applications on the cache behavior of desktop systems. Specifically, we consider the memory bandwidth and data cache challenges associated with MPEG- 2 software decoding. Recent extensions to instruction set architectures, including Intel's MMX, address the computational aspects of MPEG decoding. The large amount of data traffic generated, however, has received little attention. Standard data caches consistently generate an excess of cache-memory traffic. Varying basic cache parameters only reduces traffic to double the minimum required at best. Incremental changes in cache size have a negligible effect for most feasible values. Increasing set associativity yields rapidly diminishing returns, and manipulating line size is similarly unproductive. Achieving higher efficiency requires understanding the composition and behavior of the decoder data set. We present a model of MPEG- 2 <b>decoder</b> <b>memory</b> behavior and describe how to exploit this knowledge to m [...] ...|$|R
40|$|As an {{enhancement}} of the state-of-the-art solutions, a high-throughput architecture of a decoder for structured LDPC codes {{is presented in}} this paper. Thanks to the peculiar code definition and to the envisaged architecture featuring <b>memory</b> paging, the <b>decoder</b> is very flexible, {{and the support of}} different code rates is achieved with no significant hardware overhead. A top-down design flow of a real decoder is reported, starting from the analysis of the system performance in finite-precision arithmetic, up to the VLSI implementation details of the elementary modules. The synthesis of the whole decoderon 0. 18 mu m standard cells CMOS technology showed remarkable performances: small implementation loss (0. 2 dB down to BER = 10 (- 8)), low latency (less than 6. 0 mu s), high useful throughput (up to 940 Mbps) and low complexity (about 375 Kgates) ...|$|R
40|$|A {{microprocessor}} system {{is provided with}} added memories to expand its address spaces beyond its address word length capacity by using indirect addressing instructions of a type having a detectable operations code and dedicating designated address spaces of memory {{to each of the}} added memories, one space to a memory. By decoding each operations code of instructions read from main <b>memory</b> into a <b>decoder</b> to identify indirect addressing instructions of the specified type, and then decoding the address that follows in a decoder to determine which added memory is associated therewith, the associated added memory is selectively enabled through a unit while the main memory is disabled to permit the instruction to be executed on the location to which the effective address of the indirect address instruction points, either before the indirect address is read from main memory or afterwards, depending on how the system is arranged by a switch...|$|R
40|$|This paper {{considers}} optimal decoding for {{vector quantization}} over a noisy channel with <b>memory.</b> The optimal <b>decoder</b> is soft {{in the sense}} that the unquantized channel outputs are utilized directly for decoding, and no decisions are taken. Since the complexity of optimal decoding is high, we also present an approach to sub-optimal decoding, of lower complexity, being based on Hashimoto's generalization of the Viterbi algorithm. We furthermore study optimal encoding and combined source [...] channel coding. Numerical simulations demonstrate that both optimal and sub-optimal soft decoding give prominent gain over decision-based decoding. 1 INTRODUCTION The study of combined source-channel coding has become a major field of research, partly motivated by the increasing importance of wireless communications. The field is, however, also interesting from a more fundamental point of view: Implicit in Shannon's work is the fact that the source and channel coding can be separated without loss of opti [...] ...|$|R
40|$|Transform coding {{methods such as}} JPEG which {{operate on}} small blocks tend to have poorer {{performance}} than full-frame transform methods based on wavelet decompositions, but the memory requirements are much lower. We present a method for ordering the wavelet coefficient information in a compressed bit stream that allows the image to be sequentially decoded, with lower memory requirements than conventional wavelet decompression schemes. In addition, we introduce a hybrid filtering scheme that uses different horizontal and vertical filters, each with different depths of wavelet decomposition. This reduces the <b>decoder</b> <b>memory</b> requirements by reducing the instantaneous number of wavelet coefficients needed to perform inverse filtering. 1 Introduction We consider applications in which an image or video is to be transmitted in compressed format to an inexpensive output device, such as a low-end color printer or a wireless hand-held videophone. For such devices, the amount of on-board memory is [...] ...|$|R
40|$|The {{development}} of two programmable memory BIST architectures is first reported. A memory synthesis framework which can automatically generate, verify and insert programmable {{as well as}} non-programmable BIST units is developed {{as a vehicle to}} efficiently integrate BIST architectures in today's memory-intensive systems. Custom memory test algorithms could be loaded in the developed programmable BIST unit and therefore any type of memory test algorithm could be realized. The flexibility and efficiency of the framework are demonstrated by showing that these memory BIST units could be generated, functionally verified and inserted in a short time. 1 Introduction Memories are fabricated much denser than random logic and therefore are more prone to defects and failures. Defects in memories are due to shorts and opens in <b>memory</b> cells, address <b>decoder</b> and read/write logic. These defects are modeled as single and multi-cell memory faults and different classes of memory test algorithms have bee [...] ...|$|R
40|$|Polar {{codes are}} newly {{discovered}} capacity-achieving codes, which have attracted lots of research efforts. Polar codes can be efficiently decoded by the low-complexity successive cancelation (SC) algorithm and the SC list (SCL) decoding algorithm. The belief propagation (BP) decoding algorithm {{not only is}} {{an alternative to the}} SC and SCL decoders, but also provides soft outputs that are necessary for joint detection and decoding. Both the BP decoder and the soft cancelation (SCAN) decoder were proposed for polar codes to output soft information about the coded bits. In this paper, first a belief propagation decoding algorithm, called reduced complexity soft cancelation (RCSC) decoding algorithm, is proposed. Let N denote the block length. Our RCSC decoding algorithm needs to store only 5 N- 3 log-likelihood ratios (LLRs), significantly less than 4 N- 2 +N_ 2 N/ 2 and N(_ 2 N+ 1) LLRs needed by the BP and SCAN decoders, respectively, when N≥ 64. Besides, compared to the SCAN decoding algorithm, our RCSC decoding algorithm eliminates unnecessary additions over the real field. Then the simplified SC (SSC) principle is applied to our RCSC decoding algorithm, and the resulting SSC-aided RCSC (S-RCSC) decoding algorithm further reduces the computational complexity. Finally, based on the S-RCSC decoding algorithm, we propose a corresponding <b>memory</b> efficient <b>decoder</b> architecture, which has better error performance than existing architectures. Besides, our decoder architecture consumes less energy on updating LLRs. Comment: accepted by the IEEE 2015 workshop on signal processing systems (SiPS...|$|R
