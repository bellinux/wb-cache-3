2228|7505|Public
5|$|Software {{transactional}} {{memory is}} a common type of consistency model. Software transactional memory borrows from database theory the concept of atomic transactions and applies them to <b>memory</b> <b>accesses.</b>|$|E
5|$|Linear probing {{provides}} good locality of reference, {{which causes}} it to require few uncached <b>memory</b> <b>accesses</b> per operation. Because of this, for low to moderate load factors, {{it can provide}} very high performance. However, compared to some other open addressing strategies, its performance degrades more quickly at high load factors because of primary clustering, a tendency for one collision to cause more nearby collisions. Additionally, achieving good performance with this method requires a higher-quality hash function than for some other collision resolution schemes. When used with low-quality hash functions that fail to eliminate nonuniformities in the input distribution, linear probing can be slower than other open-addressing strategies such as double hashing, which probes a sequence of cells whose separation is determined by a second hash function, or quadratic probing, where the size of each step varies depending on its position within the probe sequence.|$|E
25|$|The MMU {{may also}} {{generate}} illegal access error conditions or invalid page faults upon illegal or non-existing <b>memory</b> <b>accesses,</b> respectively, leading to segmentation fault or bus error conditions when {{handled by the}} operating system.|$|E
50|$|There {{are many}} {{examples}} of shared memory (multiprocessors): UMA (Uniform <b>Memory</b> <b>Access),</b> COMA (Cache Only <b>Memory</b> <b>Access)</b> and NUMA (Non-Uniform <b>Memory</b> <b>Access).</b>|$|R
50|$|Reducing Memory Access: Changing the {{structure}} of the program by replacing the operations which require frequent <b>memory</b> <b>access</b> with those need less <b>memory</b> <b>access</b> is also profitable as <b>memory</b> <b>access</b> is a costly operation.|$|R
5000|$|... {{non-uniform}} <b>memory</b> <b>access</b> (NUMA): <b>memory</b> <b>access</b> time {{depends on}} the memory location relative to a processor; ...|$|R
25|$|No {{support for}} unaligned <b>memory</b> <b>accesses</b> in the {{original}} version of the architecture. ARMv6 and later, except some microcontroller versions, support unaligned accesses for half-word and single-word load/store instructions with some limitations, such as no guaranteed atomicity.|$|E
25|$|Some aspects {{attributed}} to the first RISC-labeled designs around 1975 include the observations that the memory-restricted compilers of the time were often unable {{to take advantage of}} features intended to facilitate manual assembly coding, and that complex addressing modes take many cycles to perform due to the required additional <b>memory</b> <b>accesses.</b> It was argued that such functions would be better performed by sequences of simpler instructions if this could yield implementations small enough to leave room for many registers, reducing the number of slow <b>memory</b> <b>accesses.</b> In these simple designs, most instructions are of uniform length and similar structure, arithmetic operations are restricted to CPU registers and only separate load and store instructions access memory. These properties enable a better balancing of pipeline stages than before, making RISC pipelines significantly more efficient and allowing higher clock frequencies.|$|E
25|$|Yet another impetus of both RISC {{and other}} designs came from {{practical}} measurements on real-world programs. Andrew Tanenbaum summed up many of these, demonstrating that processors often had oversized immediates. For instance, he showed that 98% {{of all the}} constants in a program would fit in 13 bits, yet many CPU designs dedicated 16 or 32 bits to store them. This suggests that, {{to reduce the number}} of <b>memory</b> <b>accesses,</b> a fixed length machine could store constants in unused bits of the instruction word itself, so that they would be immediately ready when the CPU needs them (much like immediate addressing in a conventional design). This required small opcodes in order to leave room for a reasonably sized constant in a 32-bit instruction word.|$|E
40|$|Algorithms that exhibit {{irregular}} <b>memory</b> <b>access</b> {{patterns are}} known to show poor performance on multiprocessor architectures, particularly when <b>memory</b> <b>access</b> latency is variable. Many common data structures, including graphs, trees, and linked-lists, exhibit these irregular <b>memory</b> <b>access</b> patterns. While FPGA-based code accelerators have been successful on applications with regular <b>memory</b> <b>access</b> patterns, {{they have not been}} further explored for irregular <b>memory</b> <b>access</b> patterns. Multithreading {{has been shown to be}} an eâ†µective technique in masking long latencies. We describe the compiler generation of concurrent hardware threads for FPGAs with the objective of masking the memory latency caused by irregular <b>memory</b> <b>access</b> patterns. We extend the ROCCC compiler to generate customized state information for each dynamically generated thread...|$|R
30|$|The {{total energy}} {{consumption}} of NAND Flash <b>memory</b> <b>access</b> {{can be obtained}} by adding that for <b>memory</b> <b>access</b> and that for error correction. We observe that high output precision increases the energy for <b>memory</b> <b>access,</b> while it can reduce the LDPC decoding energy.|$|R
30|$|This {{equation}} {{can be used}} {{to compute}} individually yLimfor identical arithmetic, identical read <b>memory</b> <b>access</b> or identical write <b>memory</b> <b>access</b> operations.|$|R
500|$|Due {{to their}} {{necessity}} of {{huge numbers of}} rapidly performed DRAM row activations, row hammer exploits issue large numbers of uncached <b>memory</b> <b>accesses</b> that cause cache misses, which can be detected by monitoring the rate of cache misses for unusual peaks using hardware performance counters. [...] Version 6.0.0 of the memtest86 memory diagnostic software, released on February 13, 2015, includes a so-called hammer test that checks whether computer hardware is susceptible to disturbance errors.|$|E
500|$|An early {{result in}} this {{direction}} was provided by [...] using the cell probe model of computation (an artificial model in which the complexity of an algorithm is measured only {{by the number of}} <b>memory</b> <b>accesses</b> it performs). Building on their work, [...] described two data structures, the Q-heap and the atomic heap, that are implementable on a random access machine. The Q-heap is a bit-parallel version of a binary trie, and allows both priority queue operations and successor and predecessor queries to be performed in constant time for sets of [...] items, where [...] is the size of the precomputed tables needed to implement the data structure. The atomic heap is a B-tree in which each tree node is represented as a Q-heap; it allows constant time priority queue operations (and therefore sorting) for sets of [...] items.|$|E
500|$|In July 2015, a {{group of}} {{security}} researchers published a paper that describes an architecture- and instruction-set-independent way for exploiting the row hammer effect. [...] Instead of relying on the clflush instruction to perform cache flushes, this approach achieves uncached <b>memory</b> <b>accesses</b> by causing a very high rate of cache eviction using carefully selected memory access patterns. [...] Although the cache replacement policies differ between processors, this approach overcomes the architectural differences by employing an adaptive cache eviction strategy algorithm. [...] The proof of concept for this approach is provided both as a native code implementation, and as a pure JavaScript implementation that runs on Firefox39. [...] The JavaScript implementation, called Rowhammer.js, uses large typed arrays and relies on their internal allocation using large pages; as a result, it demonstrates a very high-level exploit of a very low-level vulnerability.|$|E
5000|$|... and programmers understand,analyse {{and improve}} the <b>memory</b> <b>access</b> pattern, e.g., VTune, Vectorization Advisor, and others, {{including}} tools to address GPU <b>memory</b> <b>access</b> patterns ...|$|R
40|$|This paper {{presents}} an effective <b>memory</b> <b>access</b> method for a high-speed data transfer on mobile systems using a direct <b>memory</b> <b>access</b> controller that considers {{the characteristics of}} a multi-port memory controller. The direct <b>memory</b> <b>access</b> controller has an integrated channel management function to control multiple direct <b>memory</b> <b>access</b> channels. The channels are physically separated and operate independently from each other. Experimental results show that the proposed direct <b>memory</b> <b>access</b> method improves the transfer performance by up to 72 % and 69 % on read and write transfer cycles, respectively. The total number of transfer cycles of the proposed method is 63 % less than in a commercial method under 4 -channel access...|$|R
50|$|Locality of {{reference}} {{refers to a}} property exhibited by <b>memory</b> <b>access</b> patterns.A programmer will change the <b>memory</b> <b>access</b> pattern (by reworking algorithms) to improve the locality {{of reference}}, and/or to increase potential for parallelism. A programmer or system designer may create frameworks or abstractions (e.g. C++ templates or higher-order functions) that encapsulate a specific <b>memory</b> <b>access</b> pattern.|$|R
2500|$|Supports {{speculative}} pre-fetching of the interrupt vector address. Reduces {{the number}} of interrupt service cycles by overlapping <b>memory</b> <b>accesses</b> with pipeline flushes and exception prioritization ...|$|E
2500|$|The cache {{would watch}} all <b>memory</b> <b>accesses,</b> without asserting DEVSEL#. [...] If it noticed an access {{that might be}} cached, it would drive SDONE low (snoop not done). [...] A coherence-supporting target would avoid {{completing}} a data phase (asserting TRDY#) until it observed SDONE high.|$|E
2500|$|MIPS I has {{instructions}} that load and store 8-bit bytes, 16-bit halfwords, and 32-bit words. Only one addressing mode is supported: base + displacement. Since MIPS I is a 32-bit architecture, loading quantities fewer than 32 bits requires the datum {{to be either}} signed- or zero-extended to 32 bits. The load instructions suffixed by [...] "unsigned" [...] perform zero extension; otherwise sign extension is performed. Load instructions source the base from {{the contents of a}} GPR (rs) and write the result to another GPR (rt). Store instructions source the base from the contents of a GPR (rs) and the store data from another GPR (rt). All load and store instructions compute the memory address by summing the base with the sign-extended 16-bit immediate. MIPS I requires all <b>memory</b> <b>accesses</b> to be aligned to their natural word boundaries, otherwise an exception is signaled. To support efficient unaligned <b>memory</b> <b>accesses,</b> there are load/store word instructions suffixed by [...] "left" [...] or [...] "right". All load instructions are followed by a load delay slot. The instruction in the load delay slot cannot use the data loaded by the load instruction. The load delay slot can be filled with an instruction that is not dependent on the load; a nop is substituted if such an instruction cannot be found.|$|E
5|$|Computer {{architectures}} {{in which}} each element of main <b>memory</b> can be <b>accessed</b> with equal latency and bandwidth are known as uniform <b>memory</b> <b>access</b> (UMA) systems. Typically, {{that can be achieved}} only by a shared memory system, in which the memory is not physically distributed. A system that does not have this property is known as a non-uniform <b>memory</b> <b>access</b> (NUMA) architecture. Distributed memory systems have non-uniform <b>memory</b> <b>access.</b>|$|R
30|$|The {{ratio of}} {{computation}} to <b>memory</b> <b>access</b> is N: 4 for basic dGEMM(N, N, N) and N: 2 for zGEMM(N, N, N). Compared with the computational amount, {{the amount of}} <b>memory</b> <b>access</b> is very small. Because of the rapid computational power and slow <b>memory</b> <b>access</b> performance, the <b>memory</b> wall is still the bottleneck of GEMM performance. Many {{attempts have been made}} to optimize the BLAS 3 with the normal optimization technologies such as loop unrolling, software pipelining or data prefetching of processor. Loop unrolling is used to enhance the re-use of the data in caches to reduce the accounts of <b>memory</b> <b>access.</b> Software pipelining is used to eliminate the correlation between the execution and <b>memory</b> <b>access,</b> and the execution and <b>memory</b> <b>access</b> units can progress in parallel. However, the theoretical peak performance is too high, and the time of <b>memory</b> <b>access</b> of processors cannot be concealed by the execution. Only approximately 35 % of the theoretical peak performance can be obtained. Moreover, the parameters of loop unrolling have been adjusted, and the performance is still very low. Therefore, the bottleneck cannot be solved by using normal optimization technologies.|$|R
30|$|As {{shown in}} (8), {{in order to}} enhance the {{performance}} P, the variables T_shuffle, T_sync, T_extra and Î» _ 1 should be reduced, while Ï± and v should be increased. In the DAE architecture, APs and EPs can work in parallel. To reduce the <b>memory</b> <b>access</b> overhead, APs accomplish most missions of <b>memory</b> <b>access,</b> and the normal <b>memory</b> <b>access</b> unit is responsible for the remaining missions of <b>memory</b> <b>access.</b> Most GEMM tasks are computations, and extra overhead makes little difference to the overall runtime. Ï± is influenced by the computation to <b>memory</b> <b>access</b> overhead ratio, and it is mainly determined by the features of the algorithm and hardware. Variables Ï± and T_extra will not be discussed in this paper. In the following subsections, the optimizations of v, T_shuffle, Î» _ 1, and T_sync are mainly discussed.|$|R
2500|$|The {{prefetch}} architecture {{takes advantage}} of the specific characteristics of <b>memory</b> <b>accesses</b> to DRAM. Typical DRAM memory operations involve three phases: bitline precharge, row access, column access. Row access is the heart of a read operation, as it involves the careful sensing of the tiny signals in DRAM memory cells; it is the slowest phase of memory operation. However, once a row is read, subsequent column accesses to that same row can be very quick, as the sense amplifiers also act as latches. For reference, a row of a 1 Gbit DDR3 device is 2,048 bits wide, so internally 2,048 bits are read into 2,048 separate sense amplifiers during the row access phase. [...] Row accesses might take 50 ns, depending on the speed of the DRAM, whereas column accesses off an open row are less than 10 ns.|$|E
50|$|Most modern CPUs reorder <b>memory</b> <b>accesses</b> {{to improve}} {{execution}} efficiency (see memory ordering for types of reordering allowed). Such processors invariably give {{some way to}} force ordering in a stream of <b>memory</b> <b>accesses,</b> typically through a memory barrier instruction. Implementation of Peterson's and related algorithms on processors which reorder <b>memory</b> <b>accesses</b> generally requires use of such operations to work correctly to keep sequential operations from happening in an incorrect order. Note that reordering of <b>memory</b> <b>accesses</b> can happen even on processors that don't reorder instructions (such as the PowerPC processor in the Xbox 360).|$|E
50|$|These prefetches are {{non-blocking}} memory operations, i.e. these <b>memory</b> <b>accesses</b> do {{not interfere}} with actual <b>memory</b> <b>accesses.</b> They do not change {{the state of the}} processor or cause page faults.|$|E
40|$|Some typical <b>memory</b> <b>access</b> {{patterns}} are provided and programmed in C, {{which can be}} used as benchmark to characterize the various techniques and algorithms aim to improve the performance of NUMA <b>memory</b> <b>access.</b> These access patterns, called MAP-numa (<b>Memory</b> <b>Access</b> Patterns for NUMA), currently include three classes, whose working data sets are corresponding to 1 -dimension array, 2 -dimension matrix and 3 -dimension cube. It is dedicated for NUMA <b>memory</b> <b>access</b> optimization other than measuring the memory bandwidth and latency. MAP-numa is an alternative to those exist benchmarks such as STREAM, pChase, etc. It is used to verify the optimizations' (made automatically/manually to source code/executive binary) capacities by investigating what locality leakage can be remedied. Some experiment results are shown, which give an example of using MAP-numa to evaluate some optimizations based on Oprofile sampling. Â© IFIP International Federation for Information Processing 2012. Some typical <b>memory</b> <b>access</b> {{patterns are}} provided and programmed in C, {{which can be used}} as benchmark to characterize the various techniques and algorithms aim to improve the performance of NUMA <b>memory</b> <b>access.</b> These access patterns, called MAP-numa (<b>Memory</b> <b>Access</b> Patterns for NUMA), currently include three classes, whose working data sets are corresponding to 1 -dimension array, 2 -dimension matrix and 3 -dimension cube. It is dedicated for NUMA <b>memory</b> <b>access</b> optimization other than measuring the memory bandwidth and latency. MAP-numa is an alternative to those exist benchmarks such as STREAM, pChase, etc. It is used to verify the optimizations' (made automatically/manually to source code/executive binary) capacities by investigating what locality leakage can be remedied. Some experiment results are shown, which give an example of using MAP-numa to evaluate some optimizations based on Oprofile sampling. Â© IFIP International Federation for Information Processing 2012...|$|R
40|$|The {{available}} memory bandwidth {{of existing}} high performance computing platforms turns out {{as being more}} and more the limitation to various applications. Therefore, modern microarchitectures integrate the memory controller on the processor chip, {{which leads to a}} non-uniform <b>memory</b> <b>access</b> behavior of such systems. This access behavior in turn entails major challenges in the development of shared memory parallel applications. An improperly implemented <b>memory</b> <b>access</b> functionality results in a bad ratio between local and remote <b>memory</b> <b>access,</b> and causes low performance on such architectures. To address this problem, the developers of such applications rely on tools to make these kinds of performance problems visible. This work presents a new tool for the visualization of performance data of the non-uniform <b>memory</b> <b>access</b> behavior. Because of the visual design of the tool, the developer is able to judge the severity of remote <b>memory</b> <b>access</b> in a time-dependent simulation, which is currently not possible using existing tools...|$|R
50|$|In computing, remote direct <b>memory</b> <b>access</b> (RDMA) is {{a direct}} <b>memory</b> <b>access</b> from the <b>memory</b> of one {{computer}} into that of another without involving either one's operating system. This permits high-throughput, low-latency networking, which is especially useful in massively parallel computer clusters.|$|R
50|$|It is {{noticeable}} that correctness is {{not affected}} if <b>memory</b> <b>accesses</b> following the unlock issue before the unlock complete or <b>memory</b> <b>accesses</b> {{prior to a}} lock issue after the lock acquisition. However, the code in critical section can not be issued prior to the lock acquisition is complete because mutual exclusion may not be guaranteed.|$|E
5000|$|Minimize host <b>memory</b> <b>accesses,</b> fully {{eliminating}} {{them when}} USB devices are idle ...|$|E
5000|$|RISC-V {{supports}} {{computers that}} share memory between multiple CPUs and threads. RISC-V's standard memory consistency model is release consistency. That is, loads and stores may generally be reordered, but some loads are [...] "acquire" [...] operations which must precede later <b>memory</b> <b>accesses,</b> and some stores are [...] "release" [...] operations which must follow earlier <b>memory</b> <b>accesses.</b>|$|E
5000|$|<b>Memory</b> <b>access</b> ... Nexus {{supports}} <b>memory</b> <b>access</b> {{while the}} processor is running. Such access is required when debugging systems {{where it is}} not possible to halt the system under test. Examples include Engine Control, where stopping digital feedback loops can create physically dangerous situations.|$|R
30|$|GPU <b>memory</b> <b>access</b> is {{optimized}} using texture memory which fully leverage {{the parallel}} processing power. In our implementation and during kernel computation, texture <b>memory</b> <b>access</b> patterns has a spatial locality. In other words, a processing unit running a fragment shader {{is likely to}} read from an address near the address that nearby processing unit read. Texture memory is cached on a chip and has a specialized caching scheme optimized for spatial locality which provides effective bandwidth advantage and reduce <b>memory</b> <b>access.</b>|$|R
40|$|Many {{distributed}} system technologies and programming frameworks are developed to support parallelized computation on the datacenter platform. While simplifying data-centric parallel computation, existing system technologies and application frameworks, however, exhibit limitation in their capability, efficacy and programmability in supporting sophisticated applications. In the thesis, we present the new principle of disciplined <b>memory</b> <b>access,</b> {{together with the}} new C 0 programming language and a compiler, to support large-scale parallel computing on compute clusters with loosely-coupled commodity servers. Our prototype implementation, called Disciplined Runtime, is built under the principle of a disciplined <b>memory</b> <b>access</b> model, which means the programmer and the compiler furnish <b>memory</b> <b>access</b> information to the system so that the latter can detect <b>memory</b> <b>access</b> patterns and locality preferences at run time. Without limiting the expressing capability, this makes the <b>memory</b> <b>access</b> behavior known to the memory sub-system and the scheduler, and this information can be utilized to significantly improve the efficiency and performance of the system. The Disciplined Runtime provides distributed multitasking in a large uniform memory space with transactional semantics, and the new programming language enables programmers to provide <b>memory</b> <b>access</b> information of parallel tasks in a straightforward way so that the programs naturally follow the disciplined <b>memory</b> <b>access</b> model. Evaluation on both our research testbed and Amazon EC 2 show that the new multitasking engine provides high performance and scalability on 32 physical compute servers and 256 large EC 2 instances...|$|R
