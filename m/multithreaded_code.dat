76|63|Public
5000|$|Helgrind and DRD, detect race {{conditions}} in <b>multithreaded</b> <b>code</b> ...|$|E
50|$|In <b>multithreaded</b> <b>code,</b> {{access to}} lazy-initialized objects/state must be {{synchronized}} {{to guard against}} race conditions.|$|E
50|$|Programs {{written in}} SequenceL can be {{compiled}} to <b>multithreaded</b> <b>code</b> that runs in parallel, with no explicit indications from a programmer of how {{or what to}} parallelize. As of 2015, versions of the SequenceL compiler generate parallel code in C++ and OpenCL, which allows it to work with most popular programming languages, including C, C++, C#, Fortran, Java, and Python. A platform-specific runtime manages the threads safely, automatically providing parallel performance according {{to the number of}} cores available.|$|E
5000|$|Portable <b>multithreading</b> <b>code</b> (in C/C++ {{and other}} languages, one {{typically}} has to call platform-specific primitives {{in order to}} get multithreading).|$|R
50|$|OOP was {{developed}} to increase the reusability and maintainability of source code. Transparent representation of the control flow had no priority and {{was meant to be}} handled by a compiler. With the increasing relevance of parallel hardware and <b>multithreaded</b> <b>coding,</b> developing transparent control flow becomes more important, something hard to achieve with OOP.|$|R
40|$|To {{provide a}} {{standard}} method for using threads in C, a threads interface {{had been added}} to C 11. It unifies various existing application programming interfaces (APIs) so that software developers could write portable <b>multithreading</b> <b>code.</b> C 11 threads were deliberately defined with the least constraints such {{that they could be}} implemented as a standard veneer over native threads of a given platform. However, in some cases too much was left out of the specification. This document lists such cases and tracks the proposed solutions by the C standards committee...|$|R
50|$|Programs {{written in}} SequenceL can be {{compiled}} to <b>multithreaded</b> <b>code</b> that runs in parallel, with no explicit indications from a programmer of how {{or what to}} parallelize. , versions of the SequenceL compiler generate parallel code in C++ and OpenCL, which allows it to work with most popular programming languages, including C, C++, C#, Fortran, Java, and Python. A platform-specific runtime manages the threads safely, automatically providing parallel performance according {{to the number of}} cores available, currently supporting x86, OpenPOWER/POWER8, and ARM platforms.|$|E
50|$|An {{outdated}} {{version of}} an anti-virus application may create a new thread for a scan process, while its GUI thread waits for commands from the user (e.g. cancel the scan). In such cases, a multi-core architecture is of little benefit for the application itself due to the single thread doing all the heavy lifting and the inability to balance the work evenly across multiple cores. Programming truly <b>multithreaded</b> <b>code</b> often requires complex co-ordination of threads and can easily introduce subtle and difficult-to-find bugs due to the interweaving of processing on data shared between threads (see thread-safety). Consequently, such code {{is much more difficult}} to debug than single-threaded code when it breaks. There has been a perceived lack of motivation for writing consumer-level threaded applications because of the relative rarity of consumer-level demand for maximum use of computer hardware. Although threaded applications incur little additional performance penalty on single-processor machines, the extra overhead of development has been difficult to justify due to the preponderance of single-processor machines. Also, serial tasks like decoding the entropy encoding algorithms used in video codecs are impossible to parallelize because each result generated is used to help create the next result of the entropy decoding algorithm.|$|E
40|$|This {{position}} paper argues for {{an approach to}} bring several techniques successful for (regression) testing of sequential code over to <b>multithreaded</b> <b>code.</b> <b>Multithreaded</b> <b>code</b> is getting increasingly important but remains extremely hard to develop and test. Most recent research on testing <b>multithreaded</b> <b>code</b> focuses solely on finding bugs in one given version of code. While there are many promising results, the tools are fairly slow (as they, conceptually, explore {{a large number of}} schedules) and do not exploit the fact that code evolves over several versions during development and maintenance. Our proposal is to allow explicit specification of relevant schedules (either manually written or automatically generated) for multithreaded tests, which can substantially speed up testing, especially for evolving code. To enable the use of schedules, we propose to design a novel language for specifying schedules in multithreaded tests, and to develop tools for automatic generation of multithreaded tests and for improved regression testing with multithreaded tests. 1...|$|E
40|$|While {{removing}} software bugs consumes {{vast amounts}} of human time, hardware support for debugging in modern computers remains rudimentary. Fortunately, we show that mechanisms for Thread-Level Speculation (TLS) can be reused to boost debugging productivity. Most notably, TLS's rollback capabilities can be extended to support rolling back recent buggy execution and repeating it {{as many times as}} necessary until the bug is fully characterized. These incremental re-executions are deterministic even in <b>multithreaded</b> <b>codes.</b> Importantly, this operation can be done automatically on the fly, and is compatible with production runs...|$|R
40|$|International audienceAs a {{solution}} {{for dealing with the}} design complexity of multiprocessor SoC architectures, we present a joint Simulink-SystemC design flow that enables mixed hardware/software refinement and simulation in the early design process. First, we introduce the Simulink combined algorithm/architecture model (CAAM) unifying the algorithm and the abstract target architecture. From the Simulink CAAM, a hardware architecture generator produces architecture models at three different abstract levels, enabling a trade-off between simulation time and accuracy. A <b>multithread</b> <b>code</b> generator produces memory-efficient multithreaded programs to be executed on the architecture models. To show the applicability of the proposed design flow, we present experimental results on two real video applications...|$|R
40|$|Multithreading as {{attractive}} in In a large-scale parallel system since it allows split-phase memory operations and fast context switching between computations without blocking the processor. Performance of multithreaded archifectures depends significantly {{on the quality}} of <b>multithreaded</b> <b>codes.</b> this paper, we describe the enhanced thread formation scheme to produce efficient sequential threads from programs written, in a len. zent parallel language Id-[I]. This scheme features graph partitaoning based 011 only long latency instructions, combination. of multiple switches and merges introducing a generalized switch and merge, thread merging, and redundant arc eliminataon using thread precedence relation. The simulataon results show that our scheme reduces control and branch instructions effectively. ...|$|R
40|$|International audienceCurrent {{multicore}} {{systems are}} nondeterministic. Each time they execute a multithreaded application, even if supplied {{with the same}} input, they can produce a different output. This frustrates debugging, limits the ability to properly test <b>multithreaded</b> <b>code</b> and hinders fault-tolerant scenarios...|$|E
40|$|It is {{difficult}} to write correct <b>multithreaded</b> <b>code.</b> Th{{is difficult}}y {{is compounded by the}} weak memory model [1] provided to multithreaded applications running on commodity multicore hardware, where there is not an easily understood semantics for applications containing data races. For example, the DRF 0 memory model only guarantees sequential consistency to data-race free programs [2], and while the Java memory model does specify behavior for racy programs, these semantics are difficult to understand [4]. Without clear semantics, it {{is difficult to}} write, test, and debug <b>multithreaded</b> <b>code</b> running on commodity multicore hardware. While sequential consistency is often regarded as the preferred semantics for buggy lock-based programs, it only guarantees a global ordering at the granularity of instructions...|$|E
40|$|Abstract Emerging {{embedded}} systems require heterogeneous multiprocessor SoC archi-tectures that can satisfy both high-performance and programmability. However, as the com-plexity of {{embedded systems}} increases, software programming on {{an increasing number}} of multiprocessors faces several critical problems, such as <b>multithreaded</b> <b>code</b> generation, heterogeneous architecture adaptation, short design time, and low cost implementation. In This manuscript has been extended with <b>multithreaded</b> <b>code</b> generation based on “Buffer memory optimization for video codec application modeled in Simulink ” by Sang-Il Han, Ahmed A. Jerraya, et. al., which appeared in the Proceedings of the DAC 2006 and “Functional modeling techniques for efficient SW code generation of video codec application ” by Sang-Il Han, Ahmed A. Jerraya, et. al., which appeared in the Proceedings of the ASPDAC 2006...|$|E
50|$|MPI-1 and MPI-2 both enable {{implementations}} that overlap {{communication and}} computation, but practice and theory differ. MPI also specifies thread safe interfaces, which have cohesion and coupling strategies that help avoid hidden state within the interface. It {{is relatively easy}} to write <b>multithreaded</b> point-to-point MPI <b>code,</b> and some implementations support such <b>code.</b> <b>Multithreaded</b> collective communication is best accomplished with multiple copies of Communicators, as described below.|$|R
40|$|It is {{generally}} acknowledged that developing correct <b>multithreaded</b> <b>codes</b> is difficult, because threads may {{interact with each}} other in unpredictable ways. The goal of this work is to discover common multi-threaded programming pitfalls, the knowledge of which will be useful in instructing new programmers and in developing tools to aid in multi-threaded programming. To this end, we study multi-threaded applications written by students from introductory operating systems courses. Although the applications are simple, careful inspection and the use of an automatic race detection tool reveal a surprising quantity and variety of synchronization errors. We describe and discuss these errors, evaluate the role of automated tools, and propose new tools for use in the instruction of multi-threaded programming. ...|$|R
40|$|Multithreading is {{attractive}} in a large-scale parallel system since it allows split-phase memory operations and fast context switching between computations without blocking the processor. Performance of multithreaded architectures depends significantly {{on the quality}} of <b>multithreaded</b> <b>codes.</b> In this paper, we describe the enhanced thread formation scheme to produce efficient sequential threads from programs written in a lenient parallel language Id Γ [1]. This scheme features graph partitioning based on only long latency instructions, combination of multiple switches and merges introducing a generalized switch and merge, thread merging, and redundant arc elimination using thread precedence relation. The simulation results show that our scheme reduces control and branch instructions effectively. 1 Introduction Parallel architectures with a number of off-the-shelf microprocessors should address performance degradation due to increased latency and synchronization overheads[2]. Curr [...] ...|$|R
40|$|Abstract In {{compiling}} or developing {{applications for}} execution on distributed memory machines, communication optimizations {{are critical for}} performance. Multithreaded architectures support multiple threads of execution on each processor, with low-cost thread initiation, low-overhead communication, and efficient data transfer and synchronization between threads on different processors. These mechanisms {{can be used for}} achieving an effective overlap between communication and computation, and therefore, good performance on communication intensive parallel applications. We focus on generating correct and efficient <b>multithreaded</b> <b>code</b> for array based programs that involve different classes of communication patterns. We consider producer-consumer, scalar reductions, and nearneighbor communication. We describe multithreaded programming methodologies suitable for handling loops with each of these patterns. We further show how a compiler can generate threaded code for loops with such patterns. We present a cost model to guide thread granularity selection while generating <b>multithreaded</b> <b>code.</b> We present experimental results from three benchmark programs, CG, Tomcatv, and Jacobi. Our results show that: 1) the compiler generated <b>multithreaded</b> <b>code,</b> particularly for CG and Tomcatv, achieves high performance, not previously seen from distributed memory compilers, 2) the performance of compiler generated code is comparable to the performance of hand-written multithreaded codes, and 3) the cost model accurately guides the thread granularity selection (for Tomcatv and Jacobi) ...|$|E
40|$|The {{computational}} {{power of}} multicores presents new opportunities to provide increased performance with mul-tithreaded software. At the same time, {{the complexities of}} <b>multithreaded</b> <b>code</b> {{has given rise to}} great demand for software tools that provide better debugging capabilities, and that ensure reliability and security. Dynamic moni-toring of program executions is essential to track information that can {{serve as the basis for}} aggressive performanc...|$|E
40|$|Most of the {{research}} effort towards verification of concurrent software has focused on <b>multithreaded</b> <b>code.</b> On the other hand, concurrency in low-end embedded systems is predominantly based on interrupts. Low-end embedded systems are ubiquitous in safety-critical applications such as those supporting transportation and medical automation; their verification is important. Although interrupts are superficially similar to threads, there are subtle semantic {{differences between the two}} abstractions. This paper compares and contrasts threads and interrupts {{from the point of view}} of verifying the absence of race conditions. We identify a small set of extensions that permit thread verification tools to also verify interrupt-driven software, and we present examples of source-to-source transformations that turn interrupt-driven code into semantically equivalent thread-based code that can be checked by a thread verifier. Finally, we demonstrate a proofof-concept program transformation tool that converts interrupt-driven sensor network applications into <b>multithreaded</b> <b>code,</b> and we use an existing tool to find race conditions in these applications. Keywords: race conditions, interrupts, threads, embedded softwar...|$|E
40|$|Abstract. Failing to {{find the}} best {{optimization}} sequence for a given application code can lead to compiler generated codes with poor performances or inappro-priate code. It is necessary to analyze performances from the assembly generated code to improve over the compilation process. This paper presents a tool for the performance analysis of <b>multithreaded</b> <b>codes</b> (OpenMP programs support at the moment). MAQAO relies on static performance evaluation to identify compiler optimizations and assess performance of loops. It exploits static binary rewriting for reading and instrumenting object files or executables. Static binary instrumen-tation allows the insertion of probes at instruction level. Memory accesses can be captured to help tune the code, but such traces require to be compressed. MAQAO can analyze the results and provide hints for tuning the code. We show on some examples how this can help users improve their OpenMP applications...|$|R
40|$|Data-parallel {{model is}} {{attractive}} in {{a point that}} data-parallelism is easily expressed in loops and the efficiency {{is likely to be}} the highest if processors perform computations on data elements stored in local memory. However, communication is necessary in many cases and single-threaded data-parallel programs have a limitation in efficient communication. In this paper, we introduce multithreading concept to overcome the limitation. We present a new representation, PCFG(Parallel Control Flow Graph) and show how to construct it. We also show <b>multithreaded</b> <b>codes</b> can be easily transformed from single-threaded data-parallel codes using PCFGs. Keywords: data-parallel, multithreading, control dependence, data dependence, parallel control flow graph. Technical area: Compilers and Languages. 1 Introduction Data-parallel loops, which have no-loop carried dependence, are the primary target of Fortran D[1] or HPF(High Performance Fortran) [2] compilers. Most of parallel loops are related to arr [...] ...|$|R
40|$|In multithreaded computers, the per-iteration cycle cost {{is largely}} {{variable}} according to loop implementation schemes. Especially, when sequential loops {{with a lot}} of loop carried dependences in their bodies are unfolded, a number of value movements are required between frames, and a lot of synchronizations must be performed between threads, causing considerable overhead. However, it has been overlooked how to transform sequential loops efficiently into <b>multithreaded</b> <b>codes.</b> In this paper, we present a scheme to execute sequential loops efficiently in multithreaded computers and compare it with other schemes such as recursion and k-bounded loop by simulation over several benchmarks. Simulation results show that the proposed scheme gives the best performance under the same condition. Keywords: multithreading, loop unfolding, sequential loops, synchronization, communication 1 Introduction Recently, several multithreaded architectures such as *T[1], TAM[2], P-RISC[3], and DAVRID[4] have b [...] ...|$|R
40|$|AbstractMost of the {{research}} effort towards verification of concurrent software has focused on <b>multithreaded</b> <b>code.</b> On the other hand, concurrency in low-end embedded systems is predominantly based on interrupts. Low-end embedded systems are ubiquitous in safety-critical applications such as those supporting transportation and medical automation; their verification is important. Although interrupts are superficially similar to threads, there are subtle semantic {{differences between the two}} abstractions. This paper compares and contrasts threads and interrupts {{from the point of view}} of verifying the absence of race conditions. We identify a small set of extensions that permit thread verification tools to also verify interrupt-driven software, and we present examples of source-to-source transformations that turn interrupt-driven code into semantically equivalent thread-based code that can be checked by a thread verifier. Finally, we demonstrate a proof-of-concept program transformation tool that converts interrupt-driven sensor network applications into <b>multithreaded</b> <b>code,</b> and we use an existing tool to find race conditions in these applications...|$|E
40|$|This bachelor´s thesis {{deals with}} {{parallel}} processing {{programs at the}} platform EdkDSP. The most important parts of this thesis are an analyzation of the target platform and a design of the translator. The design is aimed at translation of the OpenMP pragmas to a <b>multithreaded</b> <b>code</b> and a transformation of the specific types of cycles. The translator was implemented using the framework ROSE compiler and than tested...|$|E
40|$|International audienceSIGNAL {{belongs to}} the {{synchronous}} languages family which are widely used {{in the design of}} safety-critical real-time systems such as avionics, space systems, and nuclear power plants. This paper reports a compiler prototype for SIGNAL. Compared with the existing SIGNAL compiler, we propose a new intermediate representation (named S-CGA, a variant of clocked guarded actions), to integrate more synchronous programs into our compiler prototype in the future. The front-end of the compiler, i. e., the translation from SIGNAL to S-CGA, is presented. As well, the proof of semantics preservation is mechanized in the theorem prover Coq. Moreover, we present the back-end of the compiler, including sequential code generation and <b>multithreaded</b> <b>code</b> generation with time-predictable properties. With the rising importance of multi-core processors in safety-critical embedded systems or cyber-physical systems (CPS), there is a growing need for model-driven generation of <b>multithreaded</b> <b>code</b> and thus mapping on multi-core. We propose a time-predictable multi-core architecture model in architecture analysis and design language (AADL), and map the multi-threaded code to this model...|$|E
40|$|The {{architecture}} of a programmable video codec IC is described that employs multiple vector processors {{in a single}} chip. The vector processors operate in parallel and {{communicate with one another}} through on-chip shared memories. A single scalar control processor schedules each vector processor independently to achieve real-time video coding with special vector instructions. With programmable interconnection buses, the proposed architecture performs multi-processing of tasks and data in video coding. Therefore, it can provide good parallelism as well as good programmability. Especially, it can operate <b>multithread</b> video <b>coding,</b> which processes several independent image sequences simultaneously. We explain its scheduling, <b>multithread</b> video <b>coding,</b> and vector processor architectures. We implemented a prototype video codec with a 0. 8 um CMOS cell based technology for the multi-standard videophone. This codec can execute video encoding and decoding simultaneously for the QCIF image at a frame [...] ...|$|R
40|$|Multithreading is {{attractive}} {{in that it}} can tolerate latency and synchronization, which are the two fundamental problems for scalability in parallel architectures, by effectively overlapping communication with computation. While several compiler techniques {{have been developed to}} produce <b>multithreaded</b> <b>codes</b> from functional language programs, we can hardly find effective loop implementation for the multithreaded environment. In this paper, we will suggest a loop unfolding scheme, KU-Loop, to effectively unfold loops in the multithreaded environment. In this scheme, parallel loops are implemented more efficiently than sequential loops by effectively overlapping loop-setup with loop execution. In addition, we will suggest the equations to statically approximate the optimal loop unfolding degree for each loop class using profile information. Finally, we will analyze our loop unfolding scheme by simulation over several benchmarks, and compare simulation results with the equations. Keywords: [...] ...|$|R
40|$|Java 8 has {{introduced}} a new abstraction called a stream to represent an immutable sequence of elements {{and to provide a}} variety of operations to be executed on the elements in series or in parallel. By processing a collection of data in a declarative way, it enables one to write more concise and clean code that can also leverage multi-core architectures without needing a single line of <b>multithread</b> <b>code</b> to be written. In this document, we describe our preliminary work on systematically refactoring loops with Java 8 streams to produce more concise and clean code. Our idea is to adapt existing work on analyzing loops and deriving their specifications written in a functional program verification style. Our extension is to define a set of transformation rules for loop patterns, one for each pattern, by decomposing the derived specification function into a series of stream operations...|$|R
40|$|Developing multithreaded {{software}} {{has proven}} to be much more difficult than writing single-threaded code. One major challenge is that current multicore processors execute <b>multithreaded</b> <b>code</b> nondeterministically: given the same input, threads can interleave their memory and I/O operations differently in each execution. Nondeterminism in multithreaded execution arises from small perturbations in the execution environment—for example, other processes executing simultaneously, differences in the operating system’s resource allocation, the state of caches, TLBs, buses, and other microarchitectural structures...|$|E
40|$|This paper {{introduces}} the Encog library for Java and C#, a scalable, adaptable, multiplatform machine learning framework that was 1 st released in 2008. Encog allows {{a variety of}} machine learning models {{to be applied to}} datasets using regression, classification, and clustering. Various supported machine learning models can be used interchangeably with minimal recoding. Encog uses efficient <b>multithreaded</b> <b>code</b> to reduce training time by exploiting modern multicore processors. The current version of Encog can be downloaded from [URL]...|$|E
40|$|International audienceEmerging {{embedded}} systems require heterogeneous multiprocessor SoC architectures that can satisfy both high-performance and programmability. However, as {{the complexity of}} {{embedded systems}} increases, software programming on {{an increasing number of}} multiprocessors faces several critical problems, such as <b>multithreaded</b> <b>code</b> generation, heterogeneous architecture adaptation, short design time, and low cost implementation. In this paper, we present a software code generation flow based on Simulink to address these problems. We propose a functional modeling style to capture data-intensive and control-dependent target applications, and a system architecture modeling style to seamlessly transform the functional model into the target architecture. Both models are described using Simulink. From a system architecture Simulink model, a code generator produces a <b>multithreaded</b> <b>code,</b> inserting thread and communication primitives to abstract the heterogeneity of the target architecture. In addition, the multithread code generator called LESCEA applies the extensions of dataflow based memory optimization techniques, considering both data and control dependency. Experimental results on a Motion-JPEG decoder and an H. 264 decoder show that the proposed multithread code generator enables easy software programming on different multiprocessor architectures with substantially reduced data memory size (up to 68. 0 %) and code memory size (up to 15. 9 %) ...|$|E
40|$|The Hewlett-Packard X- and V-Class ccNUMA systems appear {{well suited}} to {{exploiting}} coarse and fine-grained parallelism, using multithreading techniques. This paper briefly summarizes the multilevel memory subsystem for the X- and V-Class platforms. Typical MPP distributed memory programming concerns for the codes under investigation, such as explicit memory localization and load balancing, are compared to relevant issues when porting and tuning for the X- and V-Class. This paper uses two small benchmarks {{as the basis for}} investigating differences running <b>multithreaded</b> <b>codes</b> in SPP-UX and HP-UX environments. One code is from the Command, Control, Communication and Intelligence (C 3 I) Parallel Benchmark suite, shown to have the potential for large-scale parallelization with straightforward multithreading techniques. The second benchmark exhibits the computationally dynamic behavior of a thermally-driven explosion model. Both codes are shown to stress the HP systems' ability to keep memory close to processors and appropriate threads of execution...|$|R
40|$|Abstract. We present Goblint, {{a static}} {{analyzer}} for detecting potential data {{races in the}} <b>multithreaded</b> C <b>code.</b> The implemented analysis is sound on a “safe ” subset of C and sufficiently efficient {{to be used for}} race-detection of multithreaded programs up to about 25 thousand lines of code. It uses a global invariant approach to avoid the state space explosion problem and is both context- and path-sensitive. ...|$|R
50|$|It {{is part of}} Allinea Forge - a {{suite of}} tools for {{developing}} code in high performance computing - which also includes the performance profiler for scalar, <b>multithreaded</b> and parallel <b>codes</b> - Allinea MAP.|$|R
