10000|2175|Public
5|$|Watson {{was created}} as a {{question}} answering (QA) computing system that IBM built to apply advanced natural language processing, information retrieval, knowledge representation, automated reasoning, and <b>machine</b> <b>learning</b> technologies {{to the field of}} open domain question answering.|$|E
5|$|According to analyst firm Ovum, the {{software}} is made possible through advances in predictive analytics, <b>machine</b> <b>learning</b> and the NoSQL data caching methodology. The software uses semantic algorithms to {{understand the meaning of}} a data table's columns and pattern recognition algorithms to find potential duplicates in a data-set. It also uses indexing, text pattern recognition and other technologies traditionally found in social media and search software.|$|E
5|$|In recent years, the Watson {{capabilities}} {{have been}} extended {{and the way}} in which Watson works has been changed to take advantage of new deployment models (Watson on IBM Cloud) and evolved <b>machine</b> <b>learning</b> capabilities and optimised hardware available to developers and researchers. It is no longer purely a question answering (QA) computing system designed from Q pairs but can now 'see', 'hear', 'read', 'talk', 'taste', 'understand', 'reason', 'interpret', 'learn' and 'recommend'.|$|E
5000|$|Non {{parametric}} decision {{rules for}} <b>machine</b> <b>learned</b> classification ...|$|R
40|$|We {{consider}} {{the problem of}} distributed multi-task <b>learning,</b> where each <b>machine</b> <b>learns</b> a separate, but related, task. Specifically, each <b>machine</b> <b>learns</b> a linear predictor in high-dimensional space,where all tasks share the same small support. We present a communication-efficient estimator based on the debiased lasso and show that it is comparable with the optimal centralized method...|$|R
40|$|We {{construct}} <b>machine</b> <b>learned</b> regressors {{to predict}} the behaviour of DNA sequencing data from the fluorescent labelled Sanger method. These predictions are used to assess hypotheses for sequence composition through calculation of likelihood or deviation evidence from the comparison of predictions from the hypothesized sequence with target trace data. We <b>machine</b> <b>learn</b> a means for comparing the measures taken from competing hypotheses for the sequence. This is a <b>machine</b> <b>learned</b> implementation of our proposal for abductive DNA basecalling. The {{results of the present}} experiments suggest that neural nets are a more effective means for predicting peak sizes than decision tree regressors, and for assembling evidence for competing hypotheses in this context. This is despite the availability of variance estimates in our decision tree regressors. ...|$|R
25|$|Quantum <b>machine</b> <b>learning</b> is an {{emerging}} interdisciplinary research {{area at the}} intersection of quantum physics and <b>machine</b> <b>learning.</b>|$|E
25|$|It {{also is a}} buzzword and is {{frequently}} applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) {{as well as any}} application of computer decision support system, including artificial intelligence, <b>machine</b> <b>learning,</b> and business intelligence. The book Data mining: Practical <b>machine</b> <b>learning</b> tools and techniques with Java (which covers mostly <b>machine</b> <b>learning</b> material) was originally to be named just Practical <b>machine</b> <b>learning,</b> and the term data mining was only added for marketing reasons. Often the more general terms (large scale) data analysis and analytics – or, when referring to actual methods, artificial intelligence and <b>machine</b> <b>learning</b> – are more appropriate.|$|E
25|$|Data {{science and}} <b>machine</b> <b>learning</b> {{analysis}} and modelling methods are being increasingly employed in portfolio performance and portfolio risk modelling, {{and as such}} data science and <b>machine</b> <b>learning</b> Master's graduates are also in demand as quantitative analysts.|$|E
50|$|In January 2017 the {{technology}} {{was included in the}} open source search engine Apache Solr™, thus making <b>machine</b> <b>learned</b> search rank widely accessible also for enterprise search.|$|R
40|$|In this paper, we {{deal with}} {{efficiency}} of the diversification of results returned by Web Search Engines (WSEs). We extend a search architecture based on additive <b>Machine</b> <b>Learned</b> Ranking (MLR) systems with a new module computing the diversity score of each retrieved document. Our proposed solution {{is designed to be}} used with other techniques, (e. g. early termination of rank computation, etc.). Furthermore, we use an efficient state-of-the-art diversification approach based on knowledge extracted from query logs, and prove that it can efficiently works in a additive <b>machine</b> <b>learned</b> ranking system, and we study its feasibility...|$|R
5000|$|During the Fear Itself storyline, War Machine {{is seen in}} Washington DC helping Ant-Man and Beast. War <b>Machine</b> <b>learns</b> {{from the}} Prince of Orphans that the [...] "Eighth City" [...] has been opened.|$|R
25|$|ROC curves {{also proved}} {{useful for the}} {{evaluation}} of <b>machine</b> <b>learning</b> techniques. The first application of ROC in <b>machine</b> <b>learning</b> was by Spackman who demonstrated the value of ROC curves in comparing and evaluating different classification algorithms.|$|E
25|$|Quantum-enhanced <b>machine</b> <b>learning</b> {{refers to}} quantum {{algorithms}} that solve tasks in <b>machine</b> <b>learning,</b> thereby improving a classical <b>machine</b> <b>learning</b> method. Such algorithms typically require one to encode the given classical dataset into a quantum computer, {{so as to}} make it accessible for quantum information processing. After this, quantum information processing routines can be applied and the result of the quantum computation is read out by measuring the quantum system. For example, the outcome of the measurement of a qubit could reveal the result of a binary classification task. While many proposals of quantum <b>machine</b> <b>learning</b> algorithms are still purely theoretical and require a full-scale universal quantum computer to be tested, others have been implemented on small-scale or special purpose quantum devices.|$|E
25|$|Classifying data is {{a common}} task in <b>machine</b> <b>learning.</b>|$|E
40|$|We {{propose a}} novel {{notion of a}} quantum <b>learning</b> <b>machine</b> for {{automatically}} controlling quantum coherence and for developing quantum algorithms. A quantum <b>learning</b> <b>machine</b> can be trained to learn a certain task with no a priori knowledge on its algorithm. As an example, it is demonstrated that the quantum <b>learning</b> <b>machine</b> <b>learns</b> Deutsch's task and finds itself a quantum algorithm, {{that is different from}} but equivalent to the original one. Comment: 4 pages, 8 figures; Revisions made to improve the introduction and motivatio...|$|R
40|$|We {{consider}} a novel use of mostly-correct reactive policies. In classical planning, reactive policy learning approaches could find good policies from solved trajectories of small problems and such {{policies have been}} successfully applied to larger problems of the target domains. Often, due to the inductive nature, the learned reactive policies are mostly cor-rect but commit errors on some portion of the states. Discrep-ancy search has been developed to explore {{the structure of the}} heuristic function when it is mostly-correct. In this paper, to improve the performance of <b>machine</b> <b>learned</b> reactive poli-cies, we propose to use such policies in discrepancy search. In our experiments on benchmark planning domains, our pro-posed approach is effective in improving the performance of the <b>machine</b> <b>learned</b> reactive policies. The proposed approach outperformed the policy rollout with the learned policies as well as the <b>machine</b> <b>learned</b> policies themselves. As an exten-sion, we consider using reactive policies in heuristic search. During a node expansion in a heuristic search, we added to the search queue all the states that occur along the trajectory of the given policy from the node. Experiments show that this approach greatly improves the performance of heuristic search on benchmark planning domains...|$|R
30|$|Due to the {{involvement}} of the ℓ_ 1 norm, which promotes the sparsity phenomena of many real world problems arising from image/signal processing, statistical regression, <b>machining</b> <b>learning</b> and so on, the lasso receives much attention (see Combettes and Wajs [2], Xu [3], and Wang and Xu [4]).|$|R
25|$|One newer {{method for}} model {{assessment}} relies on <b>machine</b> <b>learning</b> {{techniques such as}} neural nets, which may be trained to assess the structure directly or to form a consensus among multiple statistical and energy-based methods. Results using support vector machine regression on a jury of more traditional assessment methods outperformed common statistical, energy-based, and <b>machine</b> <b>learning</b> methods.|$|E
25|$|Weka: A {{suite of}} <b>machine</b> <b>learning</b> {{software}} applications {{written in the}} Java programming language.|$|E
25|$|MLPACK library: a {{collection}} of ready-to-use <b>machine</b> <b>learning</b> algorithms written in the C++ language.|$|E
50|$|Commercial web {{search engines}} began using <b>machine</b> <b>learned</b> ranking systems since the 2000s (decade). One {{of the first}} search engines to start using it was AltaVista (later its {{technology}} was acquired by Overture, and then Yahoo), which launched a gradient boosting-trained ranking function in April 2003.|$|R
40|$|Extreme <b>learning</b> <b>machine</b> (ELM) is a {{new class}} of single-hidden layer {{feedforward}} neural network (SLFN), which is simple in theory and fast in implementation. Zong et al. propose a weighted extreme <b>learning</b> <b>machine</b> for <b>learning</b> data with imbalanced class distribution, which maintains the advantages from original ELM. However, the current reported ELM and its improved version are only based on the empirical risk minimization principle, which may suffer from overfitting. To solve the overfitting troubles, in this paper, we incorporate the structural risk minimization principle into the (weighted) ELM, and propose a modified (weighted) extreme <b>learning</b> <b>machine</b> (M-ELM and M-WELM). Experimental results show that our proposed M-WELM outperforms the current reported extreme <b>learning</b> <b>machine</b> algorithm in image quality assessment...|$|R
40|$|In [1], we {{introduced}} mechanical learning and proposed 2 approaches to mechanical learning. Here, we follow one such approach to well describe the objects and {{the processes of}} learning. We discuss 2 kinds of patterns: objective and subjective pattern. Subjective pattern is crucial for <b>learning</b> <b>machine.</b> We prove that for any objective pattern {{we can find a}} proper subjective pattern based upon least base patterns to express the objective pattern well. X-form is algebraic expression for subjective pattern. Collection of X-forms form internal representation space, which is center of <b>learning</b> <b>machine.</b> We discuss <b>learning</b> by teaching and without teaching. We define data sufficiency by X-form. We then discussed some learning strategies. We show, in each strategy, with sufficient data, and with certain capabilities, <b>learning</b> <b>machine</b> indeed can <b>learn</b> any pattern (universal <b>learning</b> <b>machine).</b> In appendix, with knowledge of <b>learning</b> <b>machine,</b> we try to view deep learning from a different angle, i. e. its internal representation space and its learning dynamics...|$|R
25|$|Different from {{traditional}} <b>machine</b> <b>learning</b> approaches, an approach {{based on data}} mining has been recently proposed.|$|E
25|$|In September 2017, Deere & Company {{signed a}} {{definitive}} agreement to acquire Blue River Technology, {{which is based}} in Sunnyvale, California and is a leader in applying <b>machine</b> <b>learning</b> to agriculture. Blue River has designed and integrated computer vision and <b>machine</b> <b>learning</b> technology that will enable growers to reduce the use of herbicides by spraying only where weeds are present, optimizing the use of inputs in farming.|$|E
25|$|Also in December 2011, Tagged {{acquired}} Topicmarks, {{a natural}} language processing and <b>machine</b> <b>learning</b> company.|$|E
5000|$|A <b>machine</b> that <b>learns,</b> Scientific American (1951) 185(2): 60—63 ...|$|R
5000|$|For example, in the {{materials}} handling industry, Neocortex helps machines adapt to mixed-size boxes for palletizing and de-palletizing. Traditional AI has not typically worked {{in this area}} {{because of the difficulty}} in programming for every circumstance. [...] Since Neocortex helps <b>machines</b> <b>learn,</b> it can adapt to a highly variable palletization process.|$|R
40|$|Abstract. The recent {{developments}} of statistical learning focused on vector <b>machines,</b> which <b>learn</b> from examples that are described by vectors of features. However, {{there are many}} fields where structured data must be handled; there-fore, it would be desirable to learn from examples described by graphs. Graph <b>machines</b> <b>learn</b> real numbers from graphs. Basically, for each input graph, a separate <b>learning</b> <b>machine</b> is built, whose algebraic structure contains the same information as the graph. We describe the training of such machines, and show that virtual leave-one-out, a powerful method for assessing the generalization capabilities of conventional vector machines, can be extended to graph ma-chines. Academic examples are described, together with applications to the pre-diction of pharmaceutical activities of molecules and to the classification of properties; the potential of graph machines for computer-aided drug design is highlighted...|$|R
25|$|Orange: A component-based {{data mining}} and <b>machine</b> <b>learning</b> {{software}} suite {{written in the}} Python language.|$|E
25|$|Parekh, R. and Honavar, V. (2001). DFA Learning from Simple Examples. <b>Machine</b> <b>Learning.</b> Vol. 44. pp.9–35.|$|E
25|$|The <b>machine</b> <b>learning</b> work {{on human}} motion capture within Kinect won the 2011 MacRobert Award for {{engineering}} innovation.|$|E
40|$|Abstract. Based on the {{research}} of extreme <b>learning</b> <b>machine</b> and support vector machine, this paper does {{the research}} on the optimization method on extreme <b>learning</b> <b>machine.</b> This paper suggests an optimization model of extreme <b>learning</b> <b>machine</b> based on {{the improvement of the}} old model, and this model has obvious improvement on generalization ability and learning parameter ability...|$|R
30|$|According to {{the theory}} of the uniform {{convergence}} of empirical risk to actual risk [26], the convergence rate bounds are based on the capacity of the set of functions that are implemented by the <b>learning</b> <b>machine.</b> The capacity of the <b>learning</b> <b>machine</b> is referred to as VC-dimension (for Vapnik-Chervonenkis dimension) [27] that represents the complexity of the <b>learning</b> <b>machine.</b>|$|R
40|$|Human {{perceive}} {{and sense}} {{the world around}} them by using their brains and eyes whereas computer vision is a science which provides all these capabilities to machines. It is a discipline in which a <b>machine</b> <b>learns</b> how to see and visualize environment around it. In this paper, we describe what Periodicity of Facial Patterns are, their advantages and limitations...|$|R
