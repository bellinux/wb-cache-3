39|1|Public
50|$|Goobi (Abbr. of Göttingen online-objects binaries) is an {{open-source}} {{software suite}} intended to support <b>mass</b> <b>digitisation</b> projects for cultural heritage institutions. The software implements international standards such as METS, MODS and other formats {{maintained by the}} Library of Congress. Goobi consists of several independent modules serving different purposes such as controlling the digitization workflow, enriching descriptive and structural metadata, and presenting the results {{to the public in}} a modern and convenient way. It is used by archives, libraries, museums, publishers and scanning utilities.|$|E
5000|$|The first {{extended}} collective licensing (ECL) {{laws were}} established in Denmark, Finland, Iceland, Norway and Sweden (Nordic countries) in the 1960s. Committees in Denmark, Finland, Norway and Sweden, with participation from Iceland, reviewed copyright laws and proposed ECL {{for the use of}} literary and music works under copyright in radio and TV broadcasting. In subsequent years ECL has been extended to other copyrighted works and areas of use, including the reuse of broadcasts through re-broadcast, on demand services and <b>mass</b> <b>digitisation</b> by libraries.|$|E
5000|$|The {{transformative}} {{nature of}} computer based analytical {{processes such as}} text mining, web mining and data mining has led many to form the view that such uses would be protected under fair use. This view was substantiated by the rulings of Judge Denny Chin in Authors Guild, Inc. v. Google, Inc., a case involving <b>mass</b> <b>digitisation</b> of millions of books from research library collections. As part of the ruling that found the book digitisation project was fair use, the judge stated [...] "Google Books is also transformative {{in the sense that}} it has transformed book text into data for purposes of substantive research, including data mining and text mining in new areas".|$|E
5000|$|Many <b>mass</b> <b>digitisations</b> of in-copyright {{works by}} {{universities}} and libraries in Scandinavia {{are based on}} ECL. One {{example of this is}} the Bøkhylla project by the National Library of Norway which intends to digitise all books in the national library in Norwegian and make them available online.|$|R
40|$|The {{focus of}} this paper is on which digital objects to {{preserve}} when preserving digital library materials derived from original paper materials. It will investigate preservation strategies for digital objects from digitised paper material that must both be preserved and simultaneously retain a short route to dissemination. The investigation is based on a study of digitisation done a decade ago and digitisation done today. In the last decade <b>mass</b> <b>digitisation</b> has become more commonly used since technological evolution has made it cheaper and quicker. The paper explores whether there are parts of digital material digitised a decade ago worth preserving, or whether a re-digitisation via <b>mass</b> <b>digitisation</b> today can create a relevant alternative. The results presented show that the old digitised objects are worth preserving, although new digitisation can contribute additional information. A supplementary result is that investment in digitisation can mean lower costs in the long term. Manual adjustments for the image processing can result in considerably smaller images than images made in cheap <b>mass</b> <b>digitisation.</b> Although initial manual work is more expensive, the storage and bit preservation expenses are lower over a long period...|$|E
40|$|This article {{provides}} an overview of recent developments in digitizing nineteenth-century printed and archived material, and argues that while <b>mass</b> <b>digitisation</b> offers incredible opportunities for research and scholarship, it also increases the considerable distance between nineteenth-century reading practices and our own. Digitized content implicitly privileges certain types of reading practice and use which the extant printed material does not...|$|E
40|$|AV Digitisation {{between two}} Stools – a Progress Report on Digital Preservation in Higher Education (translation of the title). The {{deterioration}} and decay of analogue AV media present a considerable {{problem that is}} not limited to commercial environments but also affects public organisations such as higher education institutions, libraries and archives. In light of this, and because there are no notable affordable solutions to this problem as far as the scenario in hand is concerned, a pertinent pilot project {{within the framework of the}} "PrestoPRIME" (see 2009) EU project has been initiated by the Univeristy of Innsbruck. The project deals with <b>mass</b> <b>digitisation</b> of AV media of so called consumer grade whose characteristics differ significantly from those of professional settings such as broadcasting. The main focus of the project is <b>mass</b> <b>digitisation</b> as certain issues only arise in connection with larger quantities of material...|$|E
40|$|<b>Mass</b> <b>digitisation</b> of the {{collections}} held by GLAMs (Galleries, Libraries, Archives and Museums) provide the users with huge historical, cultural, linguistical informational resources in digital format, {{which are more}} and more used in educational activities all over the world. The paper describes the experience and the outcomes of some non formal educational projects and activities associated with the creation and re-use of the digital archives Europeana 1914 - 1918 and Europeana 1989...|$|E
40|$|International audienceFor many libraries, <b>mass</b> <b>digitisation</b> {{has become}} routine. Digitisation centres are {{available}} in many places {{and there is a}} wealth of online platforms for the presentation {{of a wide variety of}} different media. Current projects from ETH Library reveal the directions in which the enormous potential harboured in these platforms and the millions of digital copies already produced may evolve. Research partnerships play just as important a role here as active user participation and intensified outreach...|$|E
40|$|The {{large-scale}} digitisation {{of early}} modern Dutch texts {{has proved to}} be a controversial enterprise: despite its ambitious scope, the project’s efficacy has met with widespread criticism. This article attempts to assess the methodological advantages and drawbacks of <b>mass</b> <b>digitisation</b> by combing the Dagverhaal, a late eighteenth-century periodical detailing the proceedings of the National Assembly of the Batavian Republic, for strategic invocations of Johan van Oldenbarnevelt, the eminent Dutch states-man who had been executed eighty years earlier...|$|E
40|$|In recent years, {{cultural}} institutions and commercial providers have created extensive digitised newspaper collections. This book asks the timely question: {{what can the}} large-scale digitisation of newspapers {{tell us about the}} wider cultural phenomenon of <b>mass</b> <b>digitisation?</b> The unique form and materiality of newspapers, and their grounding in a particular time and place, provide challenges for researchers and digital resource creators alike. At the same time, the wider context in which digitisation of cultural heritage occurs shapes the impact of digital resources in ways which fall short of the grand ambitions of the wider theoretical discourse. Drawing on case studies from leading digitised newspaper collections, the book aims to provide a bridge between the theory and practice of how these digitised collections are being used. Beginning with an exploration of the hyperbolic nature of technological discourses, the author explores how web interfaces, funding models and the realities of contemporary user behaviour, contrast with the hyperbolic discourse surrounding <b>mass</b> <b>digitisation.</b> This book will be of particular interest to those who want to investigate how user studies can inform our understanding of technological phenomena, including digital resource creators, information professionals, students and researchers in universities, libraries, museums and archives...|$|E
40|$|Future Media Internet {{has been}} {{designed}} to overcome current limitations and address emerging trends including: network architecture, content and service delivery across heterogeneous networks, diffusion of heterogeneous nodes and devices, <b>mass</b> <b>digitisation,</b> new forms of (3 D) user centric/user generated content provisioning, emergence of software as a service and interaction with improved security, trustworthiness and privacy. This paper presents current and future research trends for 3 D Video Delivery across the entire networked-media ecosystem (from the encoding/packetisation, through the transmission and up to end-user experience) ...|$|E
40|$|It {{has been}} said that media is an {{important}} but mostly overlooked player in European integration history. Now, the <b>mass</b> <b>digitisation</b> of newspapers and the introduction of new digital techniques promise great potential to remedy this inattention. With the conjecture that people are drivers and carriers of change, we propose a people-centric approach to mine news articles in a way that can be most useful to further historical research. In this paper, we describe a methodology for building social networks from unstructured news stories, with the European integration scenario serving as a case study...|$|E
40|$|This {{paper is}} a short {{introductory}} policy paper about the state-of-the-art of digitisation of library material in Europe, seen from the chief executive {{point of view of}} a big national and university library in the autumn of 2007. It focuses on current problems, obstacles, and some perspectives. What has been achieved, what are the problems and obstacles in terms of especially <b>mass</b> <b>digitisation</b> in the light of the so-called Google challenge and the response by the Commission of the European Union, and what are the consequences likely to be...|$|E
40|$|The IMPACT Centre of Competence in Digitisation is a not {{for profit}} {{organisation}} {{with the mission}} to make the digitisation of historical printed text “better, faster, cheaper”. This training includes twelve Xerte Learning objects to support anyone wanting {{to learn more about}} the digitisation of historic text. Subjects covered include: Introduction, Project Planning, Material Selection, Image Capture, OCR Dictionaries & Lexica, Image Optimisation, Optical Character Recognition, Project Management, Metadata Creation & Management, Migration, Storage & Preservation, IPR & Copyright for Text based <b>Mass</b> <b>Digitisation</b> and Outsourcing. Further details can be found at [URL]...|$|E
40|$|The ARROW rights {{infrastructure}} {{provides the}} means to support <b>mass</b> <b>digitisation</b> projects by finding automated ways to clear the rights situation of books to be digitised. ARROW provides seamless interoperability across a distributed network of national data sources, which contain essential information for determining the rights status of works, including national bibliographies from national libraries, books-in-print databases, and rights-holders databases. This paper presents how open data about authors, from the Virtual International Authority File (VIAF) is being used in ARROW to support the data interoperability across ARROW data sources, {{and how it is}} being used for the outputs of the rights clearance process...|$|E
40|$|The <b>mass</b> <b>digitisation</b> of {{analogue}} archive holdings {{plus the}} transition to tapeless production for new content means AV archives inevitably {{face the prospect of}} file-based archiving solutions using IT storage technology. But what is the long-term Total Cost of Ownership (TCO) of these systems, which file formats should be used, what storage technologies make sense, what are the risks involved, what is the additional cost of managing these risks, and what new software approaches can be applied? These are all issues being explored by major broadcasters, national archives and technology specialists in the European PrestoPrime project and the UK AVATAR-m project...|$|E
40|$|This poster {{describes}} a <b>mass</b> <b>digitisation</b> project {{led by the}} National Library of Wales to digitize archives and special collections about the Welsh experience of the First World War. The digital archive that will be created by the project will be a cohesive, digitally reunified archive that has value for research, education, and public engagement {{in time for the}} hundredth anniversary of the start of the First World War. In order to maximize impact of the digital outputs of the project, it has actively sought to embed methods that will increase its value to the widest audience. This paper describes these approaches and how they sit within the digital life cycle of project development...|$|E
40|$|The {{number of}} digital objects (and digital collections) will {{increase}} rapidly {{within the next}} years since <b>mass</b> <b>digitisation</b> activities have started all over the world. Although {{it is obvious that}} these objects are of enormous scientific and cultural value, some crucial aspects of ensuring their long-term preservation and access to them have so far not been thoroughly addressed. This means that there is an urgent need for developing (and implementing) new and reliable models in order to deliver a sound organisational and financial framework for institutions (and enterprises), that are concerned with digitisation and long-term preservation of digital objects. To this purpose the Bavarian State Library (BSB) and the University of the Federal Armed Forces Munich, ar...|$|E
40|$|The Natural History Museum, London (NHMUK) has {{embarked}} on an ambitious programme to digitise its collections. The first phase of this programme has been to undertake a series of pilot projects that will develop the necessary workflows and infrastructure development needed to support <b>mass</b> <b>digitisation</b> of very large scientific collections. This paper {{presents the results of}} one of the pilot projects – iCollections. This project digitised all the lepidopteran specimens usually considered as butterflies, 181, 545 specimens representing 89 species from the British Isles and Ireland. The data digitised includes, species name, georeferenced location, collector and collection date - the what, where, who and when of specimen data. In addition, a digital image of each specimen was taken. This paper explains the way the data were obtained and the background to the collections which made up the project. Specimen-level data associated with British and Irish butterfly specimens have not been available before and the iCollections project has released this valuable resource through the NHM data portal...|$|E
40|$|The <b>mass</b> <b>digitisation</b> of our {{literary}} heritage {{has resulted}} in both possibilities and problems for the literary scholar. With the availability large-scale literary corpora comes the implicit perception that digital surrogates yield the same information as the physical object from which they were produced. But {{this is not the}} case. Scholars stand to lose a great deal, including accuracy and credibility, by turning our backs on the physical book. At the same time, however, increased access to masses of literary data enables scholars to make computer-assisted queries that are otherwise impossible. Computers can read (for example) the literature of the eighteenth century and with scholarly guidance and interpretation provoke fresh insights into our understanding of literary history. This talk describes examples of two apparently contradictory approaches to literary study represented by the computer and the book, and suggests that they are more similar than they appear. Both are at heart inspired by a philological imperative to preserve our cultural heritage and provide a means for its investigation...|$|E
40|$|Orphan {{works are}} works that are {{protected}} by copyright but whose right holders are not known or cannot be located. Because obtaining copyright permissions from the right holders is impossible, orphan works often cannot be used in secondary works nor they can be digitised. During the last ten years, there have been presented or adopted a multitude of solutions to address the orphan works problem. In Europe, the current solutions include {{an exception to the}} copyright, extended collective licensing and mandatory collective management of rights. In the United States, a model that limits the remedies available to the right holders has been suggested. In some US cases, fair use has been applied to <b>mass</b> <b>digitisation</b> projects involving orphan works. The {{purpose of this study is}} to compare the European and US orphan works solutions in order to gain a better understanding of their capability and effectiveness to address the orphan works problem. Moreover, this comparison between the European and US solutions makes possible to identify problems in the European orphan works regime and recommend on how to improve European law on orphan works...|$|E
40|$|Normal 0 21 false false false NL X-NONE X-NONE MicrosoftInternetExplorer 4 &# 13; /* Style Definitions */&# 13; table. MsoNormalTable&# 13; {mso-style-name:Standaardtabel;&# 13; mso-tstyle-rowband-size: 0;&# 13; mso-tstyle-colband-size: 0;&# 13; mso-style-noshow:yes;&# 13; mso-style-priority: 99;&# 13; mso-style-qformat:yes;&# 13; mso-style-parent:"";&# 13; mso-padding-alt: 0 cm 5. 4 pt 0 cm 5. 4 pt;&# 13; mso-para-margin-top: 0 cm;&# 13; mso-para-margin-right: 0 cm;&# 13; mso-para-margin-bottom: 10. 0 pt;&# 13; mso-para-margin-left: 0 cm;&# 13; line-height: 115 %;&# 13; mso-pagination:widow-orphan;&# 13; font-size: 11. 0 pt;&# 13; font-family:"Calibri","sans-serif";&# 13; mso-ascii-font-family:Calibri;&# 13; mso-ascii-theme-font:minor-latin;&# 13; mso-hansi-font-family:Calibri;&# 13; mso-hansi-theme-font:minor-latin;&# 13; mso-fareast-language:EN-US;}&# 13; The {{large-scale}} digitisation {{of early}} modern Dutch texts {{has proved to}} be a controversial enterprise: despite its ambitious scope, the project’s efficacy has met with widespread criticism. This article attempts to assess the methodological advantages and drawbacks of <b>mass</b> <b>digitisation</b> by combing the Dagverhaal, a late eighteenth-century periodical detailing the proceedings of the National Assembly of the Batavian Republic, for strategic invocations of Johan van Oldenbarnevelt, the eminent Dutch states-man who had been executed eighty years earlier. </span...|$|E
40|$|Current EU (FP 7) funded {{research}} {{priorities in}} the cultural heritage domain understandably favour projects that address generic and large scale systemic issues such as barriers to <b>mass</b> <b>digitisation,</b> automated content capture and data mining, resource sharing, multilingual access and broad frameworks for long term data preservation. While these are undeniably important, access to and use of cultural and scientific resources also depend on the usabilty of individual Web sites. The majority of Web sites are difficult to use, resulting in frustration, unnecessary costs and loss of repeat visits. Users {{do not wish to}} invest significant time in learning how to get the best out of a site. So interface design is a significant factor in determining levels of access and use. This paper examines some interface design challenges encountered in the context of developing an online database of historical exhibition catalogues. It identifies issues that distinguish event-based data sets from other database design projects and discusses {{the extent to which a}} user-centred design approach and paper prototyping in particular can help to address these issues...|$|E
40|$|Digitised {{versions}} of archival fonds with micro or regional significance daily join <b>mass</b> <b>digitisation</b> projects {{of books and}} documents in the global digital space. For historians, the exponential expansion of searchable digital archival material has required the revision of traditional research methods. The digital age has also shifted disciplinary boundaries such as the distinction between historian and archivist. This article concerns a micro digitisation involving collaboration between historian and archivist, not in archive access as is usually the case, but in archive creation. The experience of this collaboration is generalisable to other micro-scale uploads of scanned material enabled by digital technologies. This article is {{a case study of}} this experience. It uses autoethnography to explore the practicalities and ethical processes of decision-making to create a new digital archive of wine history during the pilot stage of an Australian Research Council Industry Linkage Grant. The decision-making process that transformed a historian as traditional archive end-user to archive creator highlights the challenges for both professions in the decision to digitise, the implications for expenditure of public funds and questions of digitisation and environmental sustainability...|$|E
40|$|Public Sector Information (PSI) is a {{strategic}} resource without which government could not function. It feeds directly into policy formation and growing volumes {{of personal and}} cultural information are in constant demand, fulfilling {{a wide variety of}} personal and business needs. Whereas traditional conduits for the supply and publication of such material were confined to the printed page and broadcast media, digitisation has opened up substantial new means for delivering access to these resources. This ‘access revolution’ has itself created a new array of issues about what types of material should be captured to form the national archive and what intellectual property rights apply. One important aspect is the less discussed issue of policy for the maintenance and development of the public archive. <b>Mass</b> <b>digitisation</b> of the public record requires investment and secure maintenance if the archive is to be sustained. So what kind of regulatory structure needs to be established to facilitate this? Moreover, how far should private sector participation and expertise be utilised? On what basis should proprietary and user rights be granted where such collaborations take place? Finally, what message comes out of this as regards regulatory reform for libraries and archives?<br/...|$|E
40|$|Copyright @ 2011 The AuthorsWith {{the advent}} of <b>mass</b> <b>digitisation</b> projects, such as the Google Book Search, a {{peculiar}} shift {{has occurred in the}} way that copyright works are dealt with. Contrary to what has so far been the case, works are turned into machine-readable data to be automatically processed for various purposes without the expression of works being displayed to the public. In the Google Book Settlement Agreement, this new kind of uses is referred to as “non-display uses” of digital works. The legitimacy of these uses has not yet been tested by Courts and does not comfortably fit in the current copyright doctrine, plainly because the works are not used as works but as something else, namely as data. Since non-display uses may prove to be a very lucrative market in the near future, with the potential to affect the way people use copyright works, we examine non-display uses under the prism of copyright principles to determine the boundaries of their legitimacy. Through this examination, we provide a categorisation of the activities carried out under the heading of “non-display uses”, we examine their lawfulness under the current copyright doctrine and approach the phenomenon from the spectrum of data protection law as could apply, by analogy, to the use of copyright works as processable data...|$|E
40|$|Writer {{identification}} {{is the task}} of associating a handwriting sample with {{the identity of the}} correct writer. It can be used to confirm or refute the authenticity of a document, or to link together documents produced by the same writer. This problem has applications in several areas, including forensics and palaeography [...] the study of historical books and writings. Rigorous manual writer identification requires the exhaustive comparison of character details, and is very time-consuming, making computer automation of all or part of this process attractive. Most research into automated writer identification has originated in forensic science, although more recently applications to historical texts are increasing. With <b>mass</b> <b>digitisation</b> of texts on the rise in libraries and collections, organising this new data is a growing problem. However, different types of writing have different characteristics, and require different handling. This thesis focuses on how medieval English manuscripts from the 14 th [...] 15 th centuries compare to the contemporary handwriting datasets used for much of the research and feature development in this area. The work presented here is based on an in-depth application of the grapheme codebook approach to offline writer identification. It finds domain-specific considerations throughout the process, particularly in grapheme creation and comparison and in the influence of document sources on system accuracy. Additionally, {{over the course of the}} data analysis, methods are proposed for the visualisation of extracted features, for quantifying the impact of sample source on identification accuracy, and for a nearest-neighbour-based verification system. ...|$|E
40|$|In an {{age where}} works are {{increasingly}} being used, not only as works in the traditional sense, but also as carriers of data from which information may be automatically extracted for various purposes, Borghi and Karapapa consider whether <b>mass</b> <b>digitisation</b> is consistent with existing copyright principles, and ultimately whether copyright protection needs to be redefined, and if so how? The work considers the activities {{involved in the process}} of mass digitization identifying impediments to the increasing number of such projects such as the inapplicability of copyright exceptions, difficulties in rights clearance, and the issue of 'orphan' and out-of-print works. It goes on to examine the concept of 'use' of works in light of mass digital technologies and how it impinges on copyright law and principles; for example considering whether scanning and using optical character recognition in mass digital projects qualify as transformative use, or whether text mining on digitial repositories should be a permitted activity. These issues are considered in the context of both European and US law. Consideration is also given to mass digitization in the wider context of 'law and technology', comparing mass digitization issues with those of genetic databases, online privacy and data protection. Illustrating how mass digitization unveils a number of unsettled theoretical issues within copyright, the book proposes a new regulatory framework for the use of works in the context of emerging technologies, providing a new rights-based approach to dealing with copyright...|$|E
40|$|The {{process of}} {{clearing}} the rights situation {{of a work}} for digitisation {{is very difficult to}} be performed for works that have been published during the 20 th and 21 st centuries. Although libraries hold materials of public interest, which should be made digitally available to a broader public, legal issues make it necessary to determine their exact copyright status before a library can digitise it. One of the major challenges in rights clearance is the significant fragmentation of rights information across multiple data sources, some of which are not remotely accessible. This makes the rights clearance process very demanding and expensive for libraries. Large-scale digitisation projects can digitise thousands of books per week, and therefore there is a significant need to develop faster ways to clear the copyright status of the books. In the ARROW and ARROW Plus projects (Accessible Registries of Rights Information and Orphan Works), a single framework is being established to combine and access rights information. It proposes to create a seamless service across a distributed network of national databases containing information that will assist in determining the rights status of works. Its goal is to support <b>mass</b> <b>digitisation</b> projects by finding automated ways to clear the rights of the books to be digitised. This paper describes the ARROW service from the perspective of libraries undertaking digitisation projects. It presents the complete ARROW workflow, how national bibliographies are used in ARROW, and the services that ARROW offers particularly for libraries. </p...|$|E
40|$|This paper {{considers}} {{the place of}} the archive sector within the copyright regime, and how copyright impacts upon the preservation, access to, and use of archival holdings. It will begin with a critical assessment of the current parameters of the UK copyright regime as it applies to the work of archivists, including recommendations for reform that have followed {{in the wake of the}} Gowers Review of Intellectual Property (2006 - 2010), the Hargreaves Review of Intellectual Property and Growth (2010 - 2011), the recent Consultation on Copyright (2011 - 12), as well as the government’s response thereto: Modernising Copyright (2012). It {{considers the}} various problems the copyright regime presents for archives undertaking <b>mass</b> <b>digitisation</b> projects as well as recent European and UK initiatives in this domain. It argues that the UK copyright regime, even when read in conjunction with current national and regional recommendations for reform, falls short of delivering a legal framework that would enable archivists to realise the full potential that comprehensive, universal online access to the country’s archival holdings would contribute to local and national democracy and accountability, to education, learning, and culture, and to the sense of identity and place for local people, communities and organisations. Ultimately, a case is made for the differential treatment of archives within the copyright regime – different, that is, from libraries and other related institutions operating within the cultural sector. The paper concludes with a policy recommendation that would greatly enhance the ability of archives to provide online access to their holdings, while at the same time safeguarding the economic interests of the authors and owners of copyright-protected work...|$|E
40|$|In this workshop, the presenters {{will discuss}} their recent {{experiences}} with international initiatives and collaboration, and consider future developments. Ross Coleman will {{report on a}} recent Library-funded trip to institutions in the United States, United Kingdom and Canada to present on, and monitor, developments in digital libraries and eResearch. He will discuss some issues related to <b>mass</b> <b>digitisation</b> programs (e. g., the Google project in Michigan and Oxford, and the Internet Archive in Cornell and Toronto), scholarly publishing, open access and repositories. He will consider some collaboration platforms {{as well as the}} impact of new research funding mandates in North America and Australia. Ian Johnson will talk about the Archaeological Computing Laboratory's (ACL) long-term relationship with the Electronic Cultural Atlas Initiative (ECAI), ACL’s work for a number of ECAI's international partners, and recent work for the network of Digital Humanities Centers (CenterNet). Steven Hayes will talk about the DIU's role as a partner in an EU project and collaboration with the Centre for Computing in the Humanities at King's College, London. He will discuss the difficulties and solutions to long-range collaboration and provide some perspective on EU-funded projects. Willard McCarty will discuss opportunities for the digital humanities in relation to experiences with previous programs. Professor McCarty suggests that the most important initiative is the establishment of institutional models for humanities computing. He will discuss why the great centres of yore have failed and are all now gone. Professor McCarty’s second initiative is to establish what a colleague has called "evidence of value", or rather, to discuss how humanists go about constructing a disciplinary rhetoric {{that would allow them to}} understand what is happening in contemporary scholarship. The participants will invite the audience to participate in a discussion about international initiatives in the digital humanities and eScholarship. Digital Innovation Unit for the Humanities and Social Science...|$|E
40|$|The {{paper will}} {{describe}} how web-based collaboration tools can engage {{users in the}} building of historical printed text resources created by <b>mass</b> <b>digitisation</b> projects. The drivers for developing such tools will be presented, identifying the benefits that can be derived for both the user community and cultural heritage institutions. The perceived risks, such as new errors introduced by the users, and the limitations of engaging with users in this way will be set out with the lessons that can be learned from existing activities, such as the National Library of Australia's newspaper website which supports collaborative correction of Optical Character Recognition (OCR) output. The paper will present the work of the IMPACT (Improving Access to Text) project, a large-scale integrating project funded by the European Commission as part of the Seventh Framework Programme (FP 7). One of the aims of the project is to develop tools that help improve OCR results for historical printed texts, specifically those works published before the industrial production of books {{from the middle of the}} 19 th century. Technological improvements to image processing and OCR engine technology are vital to improving access to historic text, but engaging the user community also has an important role to play. Utilising the intended user can help achieve the levels of accuracy currently found in born-digital materials. Improving OCR results will allow for better resource discovery and enhance performance by text mining and accessibility tools. The IMPACT project will specifically develop a tool that supports collaborative correction and validation of OCR results and a tool to allow user involvement in building historical dictionaries which can be used to validate word recognition. The technologies use the characteristics of human perception as a basis for error detection...|$|E
40|$|The {{on-going}} {{process of}} the <b>mass</b> <b>digitisation</b> of Dutch literature {{during the past two}} decades implies an increase in the digital analysis of such resources. This situation is not particular for the Netherlands: given the growing amount of the electronic book stack worldwide, one of the core questions in the field of digital humanities revolves around how to read a million books. As ‘opposed’ to the close reading practice we are used to in literary theory, Franco Moretti’s ‘distant reading’ focusses on a large group of texts, and the zooming out of that corpus by means of visualisations is bound to bring into view patterns that were not observable from close by. Distant reading is therefore rereading and rewriting literary history, an explaining of patterns rather than an interpreting of individual texts. This thesis serves as a test of these close and distant reading techniques, to see if we can indeed affirm, refute or nuance some assumptions in Dutch Literary Studies using digital reading methods. Generic assumptions can function as a good set of hypotheses for this purpose, as these make out a small portion of literary history, and as their definitions are often based on a few exemplary cases rather than all specimens attributed to the genre. The case study in this thesis involves the genre of Dutch mendacious songs, a relatively small genre of which only a handful of assumptions on its definition exist. Mendacious songs contain impossibilities and a lying narrative instance, and are set in a wonderland, such as ‘Luilekkerland’: ‘’k Zag twee beren broodjes smeren’ is an example of such songs. How can we (re) define this genre by close and distant reading a small and large corpus of these songs respectively...|$|E
40|$|About This {{report is}} a {{collaboration}} between the Centre for Intellectual Property Policy & Management (www. cippm. org. uk), Bournemouth University (BU), the Department for Human Resources & Organisational Behaviour, The Business School, BU, and CREATe, the RCUK Centre for Copyright & New Business Models (www. create. ac. uk). The Hargreaves Review stated: “The problem of orphan works – works to which access is effectively barred because the copyright holder cannot be traced – represents the starkest failure of the copyright framework to adapt. ” (Digital Opportunity: A Review of Intellectual Property and Growth; London: Intellectual Property Office; 2011; p. 38). This report was commissioned by the Intellectual Property Office to support {{the implementation of the}} Hargreaves Review. It aims to offer a clearer understanding of how orphan works are regulated and priced in other jurisdictions, and how a pricing system could be structured to ensure that “parents” are fairly remunerated if they re-appear, and users are incentivised to access and exploit registered orphan works. Executive Summary ‘Orphan works’ are works in which copyright still subsists, but where the rightholder, whether it be the creator of the work or successor in title, cannot be located. This report was commissioned to assist the UK government in evaluating policy options in the implementation of the Hargreaves Review of Intellectual Property & Growth (2011) to enable and price the use of orphan works. The research proceeded in two stages. Study I undertook a comparative international review of actual or proposed orphan works legislation, and identified key characteristics of orphan works licensing schemes. Study II investigated the potential effects of such schemes by conducting a simulated rights clearance exercise for six scenarios (establishing licence terms and fees for specific commercial and non-commercial uses), and analysing the resulting dataset for effects of the characteristics identified in Study I. I. Comparative Review The comparative review relied on a close scrutiny of actual or proposed legislation and considered government reports, draft bills, publications and other commentaries on the orphan works issue. The countries reviewed included jurisdictions with operational orphan works regulations: Canada, Denmark, Hungary, India, and Japan; as well as provisions at the EU level and in the US (draft legislation and current practice). The purpose of the review was to identify key features of legal regimes with respect to factors such as – (i) categories of works covered; (ii) standards of diligent search; (iii) the mechanism for obtaining permission; (iv) the existence of a register or database of recording suspected orphan works; (v) the role of collecting societies;(vi) tariffs set by category of work; (vii) mechanisms for challenging tariffs; (viii) remedies for reappearing authors and case law, if any, on damages for infringing use. Findings Study I: (1) Two distinct approaches appear to be used for governing orphan works in the jurisdictions reviewed. The first may be labelled ‘ex-ante’, and involves rights clearing before a work is used, the second is ‘ex-post’ and typically involves the management of infringement risks by the user. In the former an applicant is required to engage with an authorising body or collecting society in order to receive a licence to make use of an orphan work. In contrast, the latter involves either the creation of a statutory copyright exception, or a limitation of liability where an applicant makes use of an orphan work after having exerted some effort to identify the potential rightholder (e. g. diligent search, attribution). These regimes provide different levels of protection for authors and users. The ex-ante approach is exemplified by Canada, Japan and India where a potential user has to discuss terms with a copyright board. In ex-post systems payment is only due in case an author reappears. This approach is exemplified by the US. The analysis shows a strong protection for rightholders, both ex ante and ex post, in India, and Japan; and a relatively lower protection in the US, and to a certain extent Denmark (under the system of Extended Collective Licensing). Canada and Hungary are intermediate cases (e. g. no advertising requirements prior to use, but public listing of granted licences). (2) While most jurisdictions require a diligent search to be conducted by the applicant there is no uniform standard constituting a diligent search. Across jurisdictions the specifications for diligent search vary considerably. Requirements involving the preventive search of the author range from the weak provisions of Denmark (no search required), Canada (requiring “reasonable effort”), and the US (“reasonable search” required) to the strong provisions of India, Japan, Hungary and the EU (but not France) providing a duty for the user of performing a “diligent search” or “due diligence search” (India) accompanied by some form of record tracking of the steps performed. The EU lists minimum sources for a diligent search in the Annex to the Orphan Works Directive. Advertising requirements (in the national press or equivalent) are provided in Japan and India. (3) The United States had proposed a “limited liability” approach, under which the use of orphan works is possible after a reasonable search. In the case of an infringement claim orphan users are liable only for a reasonable compensation. Denmark uses an Extended Collective Licensing system, which involves collective negotiation with users (normally for multiple licensing) valid also for non-represented authors. In turn, the EU leaves Member States free to choose their regulatory approach (for example, France has chosen a central licensing system in its forthcoming legislation). All the other countries reviewed implement the central licensing system, with a central public authority granting copyright licences on orphan works. (4) Prices are set by central authorities in the countries that have a central licensing system, and by collecting societies in Denmark. Interestingly, national central authorities have claimed that although no official negotiation process is provided by law, the price of licences is set on a case by case basis, after considering the individual circumstances of the applicant. Set prices can be challenged mostly in an ordinary court of law in the examined countries, or alternatively before the licensing authority with a quasi-judicial procedure (e. g. Canada). Infringement claims are handled by ordinary courts in all countries (including the US) or by licensing authorities with quasi-judicial procedures (in Hungary). In Denmark, both prices and infringement claims are under the jurisdiction of a special tribunal (the Copyright Licensing Tribunal). The above rules on price, infringement, and legal remedies do not derive from EU law, which leaves these matters to Member States. (5) In Canada, Japan, India, Denmark and France an upfront payment is normally required byte applicant in exchange for using orphan works. In Canada, payment is upfront in approximately two-thirds of cases, whilst it is contingent on the rightholder reappearing in the remaining third. See De Beer and Bouchard (2010). In Hungary the amount is identified but may not be deposited (for non-profit licensees). It will be paid directly to the rightholder, in the event that he or she reappears. In the US, no payment is made until a court decision is issued, following an infringement claim. Particular roles are envisaged for collecting societies in Denmark, in which they handle the whole system, Hungary, where collecting societies retain unclaimed revenues after five years from expiry of licence, and Canada, where collecting societies are consulted during tariff setting, and hold collected fees (to be used as they see fit). (6) In the US, in Hungary, and France a voluntary public online register for suspected orphan works is established. The EU is establishing a register at the Office for Harmonization in the Internal Market (OHIM). In Japan, some institutions have their own register of orphan works. No register is envisaged in India, Canada or Denmark (prior to the Orphan Works Directive). However details of all licences granted are available on-line in Canada. II. Rights Clearance Simulation The “Rights Clearance Exercise” reported in Study II is a combination of various methods. In a first step a simulation approach is employed to collect a unique dataset on actual or potential licence fees for orphan works. Representatives of rights clearance authorities from countries covered in the comparative legal review (Canada, Denmark, France, Hungary, India, Japan, supplemented by some US data) were asked to provide a licence fee for each of six scenarios that are likely to occur in reality (from creating a small online resource to <b>mass</b> <b>digitisation</b> projects). The identification of the various scenarios was the outcome of a rigorous methodological procedure. The six identified scenarios were: 	Historical geographic maps for a video game for mobile phones (up to 50 maps 	A vintage postcard collection for web publication and eventual sale of prints (up to 50 cards) 	National folk tune recordings for multimedia/teaching (DVD) (up to 50) 	Re-issuing a 1960 / 70 s TV series as part of a digital on-demand service (one series) 	<b>Mass</b> <b>digitisation</b> of photographs (archives) by a public non-profit institution, with possible sale of prints (above 100, 000 items) 	<b>Mass</b> <b>digitisation</b> of books by a private for-profit institution, with possible sale of books (above 100, 000 items) Two rates for each scenario were sought, for commercial and non-commercial use. In a second step the dataset is subjected to various analytical techniques, including a regression approach and a comparison of collective licensing systems against others through the computation of effect sizes. Findings Study II: (1) There {{does not appear to be}} a standard price for licensing orphan works. In fact tariffs vary widely. For example, to clear 50 items from a folk tune archive for commercial use will cost the equivalent of £ 188 per year in Canada, and (under reasonable assumptions) £ 9, 312 per year in France. In fact, the only consistent finding appears to be that in almost all cases commercial licence fees tend to exceed non-commercial ones. (2) Licences were not available for all scenarios. Re-issuing orphaned broadcasts seems particularly problematic, with no licence offered in any of the countries investigated. (3) There is no systematic recognition of what may constitute an appropriate duration for licences. Licences were very variable from country to country, ranging from a monthly to a five-year licence, without the provision of a permanent licence. (4) We find high tariffs that discourage <b>mass</b> <b>digitisation</b> projects. Per item fees initially appearing very low and thus sustainable turn out to render mass-digitisation unviable for public and non-profit institutions when scaled up under reasonable assumptions. <b>Mass</b> <b>digitisation</b> projects involving 100, 000 items may incur annual licensing fees exceeding £ 1 million per year. (5) The average level of fees imposed on a potential user of an orphan work is similar in collective and individual licensing regimes. This is an interesting finding because it mitigates arguments that one of the regimes will lead to higher fees. The operating costs involved in running an orphan works scheme appear therefore an important factor when choosing between individual and collective approaches. (6) A limited liability system seems to have advantages for archives and other non-profit institutions exposed to orphan works, enabling those organisations to share their stock of orphaned artefacts with the public. In contrast, the up-front rights clearing seems to provide more appropriate incentives for commercial uses of orphaned artefacts, guaranteeing that a reappearing rightholder will be compensated for the exploitation of any work. Together, the findings from both studies indicate the need for a more structured and consistent approach in governing orphan works that is reflected in the pricing and duration of licences, and in the costs of running any licensing system...|$|E
40|$|Purpose – The {{inability}} of cultural institutions {{to make available}} digital reproductions of collected material highlights a shortcoming with the existing copyright framework {{in a number of}} national jurisdictions. Overlapping efforts to remedy the situation were recently undertaken in the form of EU Directive 2012 / 28 /EU, the ‘Orphan Works’ directive, and a new licensing scheme introduced by the UK Intellectual Property Office (UKIPO). This study empirically evaluates both the EU and UK policy approaches, drawing on data collected during a live rights clearance simulation. Design/methodology/approach – The authors attempted to clear rights in a sample of 432 items contained in the mixed-media Edwin Morgan Scrapbooks collection held by the University of Glasgow Library. Data were collected on the resource costs incurred at each stage of the rights clearance process, from initial audit of the collection, through to compliance with diligent search requirements under EU Directive 2012 / 28 /EU and the UKIPO licensing procedures. Findings – Comparing results against the two current policy options for the use of orphan works, we find that the UKIPO licensing scheme offers a moderate degree of legal certainty but also the highest cost to institutions (the cost of diligent search in addition to license fees). The EU exception to copyright provides less legal certainty in the case of rightsholder re-emergence, but also retains high diligent search costs. Both policy options may be suitable for institutions wishing to make use of a small number of high-risk works, but neither approach is currently suitable for <b>mass</b> <b>digitisation.</b> Research limitations/implications – This rights clearance exercise is focused on a single case study with unique properties (with a high proportion of partial works embedded in a work of bricolage). Consequently, the results obtained in this study reflect differences from simulation studies on other types of orphan works. However, by adopting similar methodological and reporting standards to previous empirical studies, we can compare rights clearance costs between collections of different works. Originality/value – This study is the first to empirically assess the 2014 UK orphan works licensing scheme from an institutional perspective. We hope that it will contribute to understanding of how policy could more effectively assist libraries and archives in their digitisation efforts...|$|E
