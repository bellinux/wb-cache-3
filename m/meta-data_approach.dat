3|14|Public
40|$|Asynchronous {{learning}} {{networks are}} facilities and procedures to allow members of learning communities {{to be more}} effective and efficient in their learning. One approach is to see how the `sharing' of knowledge can be augmented through meta-data descriptions attached to portfolios and project work. Another approach is to facilitate the reflection upon individual or collaborative learning experiences (Okamoto, Cristea, Matsui, & Miwata, 2000). The position that I defend in this paper is that both the <b>meta-data</b> <b>approach</b> and the attempts to capture the students' meta-knowledge are rather complicated because of social and emotional reason...|$|E
40|$|This paper {{describes}} the {{human computer interface}} of a new emergency medical services application, which uses several emerging technologies to clarify the inherent complexity and uncertainty of emergency pre-hospital patient care. These technologies include: 1) the integration and display of real-time physiological sensor data in parallel with manually entered human observations and treatments; 2) a <b>meta-data</b> <b>approach</b> to screen definitions to allow dynamic configuration; and 3) a rule-based data capture system to ensure complete and accurate data collection. These new technologies led us to design a flexible and efficient GUI {{that is capable of}} evolving as technology and medical science advance. ...|$|E
40|$|Abstract. The World Wide Web {{provides}} a vast resource to genomics researchers, with Web-based access to distributed data {{sources such as}} BLAST sequence homology search interfaces. However, finding the desired scientific information can still be very tedious and frustrating. While there are several known servers on genomic data (e. g., GeneBank, EMBL, NCBI) that are shared and accessed frequently, new data sources are created each day in laboratories all over the world. Sharing these new genomics results is hindered {{by the lack of}} a common interface or data exchange mechanism. Moreover, the number of autonomous genomics sources and their rate of change outpace the speed at which they can be manually identified, meaning that the available data is not being utilized to its full potential. An automated system that can find, classify, describe, and wrap new sources without tedious and low-level coding of source-specific wrappers is needed to assist scientists in accessing hundreds of dynamically changing bioinformatics Web data sources through a single interface. A correct classification of any kind of Web data source must address both the capability of the source and the conversation/interaction semantics inherent {{in the design of the}} data source. We propose a service class description (SCD) -a <b>meta-data</b> <b>approach</b> for classifying Web data sources that takes into account both the capability and the conversational semantics of the source. The ability to discover the interaction pattern of a Web source leads to increased accuracy in the classification process. Our result...|$|E
40|$|As a {{consequence}} of a rapidly changing environment, the success of organizations is dependent upon the ongoing and immediate adjustments of their information systems as reactions to these changes. Therefore, flexibility becomes {{one of the most}} crucial features in information systems. This paper specifies a data model oriented framework for the development of flexible information systems. The result of such process is a system that is sufficiently general and flexible in relation to solving problems related to changing environment. From general requirements related to data layer of information systems the paper discusses the definition of major elements of logical data model (entities and their hierarchical arrangement, attributes of the entities and their important characteristics, relationships among entities and their characteristics) as well as possibilities of implementation and definition of application logic and data presentation. Proposed framework enables the organization to specify its own database structure, which best matches the situation of the organization and its environment. Because an <b>approach</b> similar to <b>meta-data</b> <b>approaches</b> is applied, methods for information sharing and interchange can be easily specified as well as program logic for manipulation with the data base on the application layer and data presentation...|$|R
40|$|The Grid {{technology}} is evolving into a global, service-orientated architecture, a universal platform for delivering future high demand computational services. Strong {{adoption of the}} Grid and the utility computing concept is leading to {{an increasing number of}} Grid installations running a wide range of applications of different size and complexity. In this paper we address the problem of elivering deadline/economy based scheduling in a heterogeneous application environment using statistical properties of job historical executions and its associated <b>meta-data.</b> This <b>approach</b> is motivated by a study of six-month computational load generated by Grid applications in a multi-purpose Grid cluster serving a community of twenty e-Science projects. The observed job statistics, resource utilisation and user behaviour is discussed in the context of management approaches and models most suitable for supporting a probabilistic and autonomous scheduling architecture...|$|R
40|$|Model {{management}} is {{a framework for}} supporting meta-data related applications where models and mappings are manipulated as first class objects using operations such as Match, Merge, ApplyFunction, and Compose. To demonstrate the approach, we show how to use model management in two scenarios related to loading data warehouses. The case study illustrates the value of model management as a methodology for <b>approaching</b> <b>meta-data</b> related problems. It also helps clarify the required semantics of key operations. These detailed scenarios provide evidence that generic model {{management is}} useful and, very likely, implementable...|$|R
40|$|A data {{warehouse}} that presents data {{from many of}} the genomics community data sources in a consistent, intuitive fashion has long been a goal of bioinformatics. Unfortunately, {{it is one of the}} goals that has not yet been achieved. One of the major problems encountered by previous attempts has been the high cost of creating and maintaining a warehouse in a dynamic environment. In this abstract we have outlined a <b>meta-data</b> based <b>approach</b> to integrating data sources that begins to address this problem. We have used this infrastructure to successfully integrate new sources into an existing warehouse in substantially less time than would have traditionally been required [...] and the resulting mediators are more maintainable than the traditionally defined ones would have been. In the final paper, we will describe in greater detail both our architecture and our experiences using this framework. In particular, we will outline the new, XML based representation of the meta-data, describe how the mediator generator works, and highlight other potential uses for the meta-data...|$|R
40|$|Emerging {{languages}} are often source-to-source compiled to mainstream ones, which offer standardized, fine-tuned im-plementations of non-functional concerns (NFCs) —including persistence, security, transactions, and testing. Because these NFCs are specified through metadata such as XML config-uration files, compiling an emerging language to a main-stream {{one does not}} include NFC implementations. Unable to access the mainstream language’s NFC implementations, emerging language programmers waste development effort reimplementing NFCs. In this paper, we present a novel ap-proach to reusing NFC implementations across languages by automatically translating metadata. To add an NFC to an emerging language program, the programmer declares meta-data, which is then translated to reuse the specified NFC im-plementation in the source-to-source compiled mainstream target language program. By automatically translating <b>meta-data,</b> our <b>approach</b> eliminates the need to reimplement NFCs in the emerging language. As a validation, we add unit test-ing and transparent persistence to X 10 by reusing imple-mentations of these NFCs in Java and C++, the X 10 back-end compilation targets. The reused persistence NFC is effi-cient and scalable, {{making it possible to}} checkpoint and mi-grate processes, as demonstrated through experiments with third-party X 10 programs. These results indicate that our approach can effectively reuse NFC implementations across languages, thus saving development effort...|$|R
40|$|The paper {{starts with}} a generic {{discussion}} on the cloud application services and security concerns then expands the concepts with 3 main data management approaches of multi-tenant data management. After that paper describes reference architecture including standard cloud computing taxonomy for <b>meta-data</b> driven architecture <b>approach</b> and a conceptual data model to support the architecture. At the end it incorporates a comparison between green field application verses existing application migration assessment for target Software as a Service (SaaS) environment. WHAT ARE THE CLOUD APPLICATION (SAAS) SERVICES? Now a days the availability of reliable high speed broadband Internet access, service-oriented architectures (SOAs), and the economic management driving a transition toward the delivery of Cloud applications-of dedicated on-premises applications are “Cloud applications or "Software as a Service (SaaS) applications deliver {{software as a service}} over the Internet, eliminating the need to install and run the application on the customer's own computers and simplifying maintenance and support, and equipped with decomposable applications, managed services, shared hardware / software /admin resources and Web-based services”. It’s a paradigm shift which imposes a new set of technical challenges. Existing applicatio...|$|R
40|$|Abstract Background Investigators in the {{biological}} sciences continue to exploit laboratory automation methods and have dramatically increased the rates at which they can generate data. In many environments, the methods themselves also evolve in a rapid and fluid manner. These observations point {{to the importance of}} robust information management systems in the modern laboratory. Designing and implementing such systems is non-trivial and it appears that in many cases a database project ultimately proves unserviceable. Results We describe a general modeling framework for laboratory data and its implementation as an information management system. The model utilizes several abstraction techniques, focusing especially on the concepts of inheritance and <b>meta-data.</b> Traditional <b>approaches</b> commingle event-oriented data with regular entity data in ad hoc ways. Instead, we define distinct regular entity and event schemas, but fully integrate these via a standardized interface. The design allows straightforward definition of a "processing pipeline" as a sequence of events, obviating the need for separate workflow management systems. A layer above the event-oriented schema integrates events into a workflow by defining "processing directives", which act as automated project managers of items in the system. Directives can be added or modified in an almost trivial fashion, i. e., without the need for schema modification or re-certification of applications. Association between regular entities and events is managed via simple "many-to-many" relationships. We describe the programming interface, as well as techniques for handling input/output, process control, and state transitions. Conclusion The implementation described here has served as the Washington University Genome Sequencing Center's primary information system for several years. It handles all transactions underlying a throughput rate of about 9 million sequencing reactions of various kinds per month and has handily weathered a number of major pipeline reconfigurations. The basic data model can be readily adapted to other high-volume processing environments. </p...|$|R
40|$|Communication about {{requirements}} {{is often}} handled in issue tracking systems, {{especially in a}} distributed setting. As issue tracking systems also contain bug reports or programming tasks, the software feature requests of the users are often difficult to identify. This paper investigates natural language processing and machine learning features to detect software feature requests in natural language data of issue tracking systems. It compares traditional linguistic machine learning features, such as "bag of words", with more advanced features, such as subject-action-object, and evaluates combinations of machine learning features derived from the natural language and features taken from the issue tracking system meta-data. Our investigation shows that some combinations of machine learning features derived from natural language and the issue tracking system <b>meta-data</b> outperform traditional <b>approaches.</b> We show that issues or data fields (e. g. descriptions or comments), which contain software feature requests, can be identified reasonably well, but hardly the exact sentence. Finally, we show that the choice of machine learning algorithms should depend on the goal, e. g. maximization of the detection rate or balance between detection rate and precision. In addition, the paper contributes a double coded gold standard and an open-source implementation to further pursue this topic...|$|R
40|$|Breast cancer (BC) takes {{thousands}} of woman’s lives yearly. Several {{factors have been}} found to influence initiation and development of breast cancer, and to affect prognosis and treatment of this disease. This thesis is focuses on opening-out this complexity and search for approaches that may lead to individualized treatment of breast cancer patients. We studied clinical samples of breast tumors and adjacent normal tissues using protein-based proteomics. By studying each patient individually, we identified proteins that changed expression during carcinogenesis (p 53, Smad 2, etc). We observed significant differences in the lists of cancer-related proteins between individual patients. We demonstrated that meta-data analysis of the identified proteins is {{the most efficient way to}} describe common and individual features of tumors from different patients. Our validation study by immunohistochemistry analysis of identified molecules (PYK, Smad 2, CK 2 α) confirmed the changed expressions between tumor and normal tissue, and thereby confirmed the conclusions obtained with proteomics analysis. Thus, we found that <b>meta-data</b> analysis <b>approach</b> is suitable for improved and individualized diagnostics and selection of treatment. Transforming growth factor-β (TGFβ) is a potent regulator of tumorigenesis. In our study of the clinical cases, we demonstrated that TGFβ signaling might be influenced in breast tumorigenesis. Phosphoproteomics analysis of TGFβ action on MCF 10 A human breast epithelial cells showed a complex regulation of cell signaling, with strong representation of functional domains such as metabolism. One of the targets of TGFβ is 14 - 3 - 3 σ protein, and we found that 14 - 3 - 3 σ was of a crucial importance for the cross-talk between TGFβ and p 53 signaling. We reported also proteins identified by expression proteomics, which are regulated by TGFβ in human breast epithelial cells that have phenotype similar to normal breast epithelial cells. We found more than 100 proteins that were regulated by TGFβ. Among them, Casein Kinase 2 α (CK 2 α), Structure-Specific Recognition Protein- 1 (SSRP 1) and protein convertase- 4 (PC 4) may be involved in TGF!- dependent inhibition of cell proliferation by modulating p 53 phosphorylation. Therefore, presented here study describes development of tools for individualized treatment of patients, and provides insights in the complexity of cancer related signaling in breast epithelial cells...|$|R
40|$|Ankara : The Department of Computer Engineering and the Graduate School of Engineering and Science of Bilkent University, 2012. Thesis (Master's) [...] Bilkent University, 2012. Includes bibliographical refences. There is a {{huge amount}} of data which is {{collected}} from the Earth observation satellites and they are continuously sending data to Earth receiving stations day by day. Therefore, mining of those data becomes more important for effective processing of collected multi-spectral images. The most popular approaches for this problem use the meta-data of the images such as geographical coordinates etc. However, these approaches do not offer a good solution for determining what those images contain. Some researches make a big step from the <b>meta-data</b> based <b>approaches</b> in this area by moving the focus of the study to content based approaches such as utilizing the region information of the sensed images. In this thesis, we propose a novel, generic and extendable image information mining system that uses spatial relationship constraints. In this system, we use not only the region content, but also relationships of those regions. First, we extract the region information of the images and then extract pairwise relationship information of those regions such as left, right, above, below, near, far and distance etc. This feature extraction process is defined as a generic process which is independent from how the region segmentation is obtained. In addition to these, since new features and new approaches are continuously being developed by the image information mining researchers, extendability feature of the our system plays a big role while we are designing our system. In this thesis, we also propose a novel feature vector structure in which a feature vector consists of several sub-feature vectors. In the proposed feature vector structure, each sub-feature vector can be exclusively selected to be used for search process and they can have different distance metrics to be used in comparisons between the same sub-feature vector of the other feature vector structures. Therefore, the system gives ability to users to choose which information about the region and its pairwise relationship with other regions to be used when they perform a search on the system. The proposed system is illustrated by using region based retrieval scenarios on very high spatial resolution satellite images. Karakuş, FatihM. S...|$|R
40|$|System {{dynamics}} is {{a mathematical}} modeling ap-proach {{widely used in}} environmental studies for repres-enting and simulating ecological systems, giving sup-port to prediction and decision making. The know-ledge sources for model design are, essentially, human expertise and ecological data at various levels of ab-straction. We observe that property descriptions of eco-logical data (ecological meta-data), such as functional, temporal and spatial relations between variables, seem to be cognitively close to the concepts and reasoning used by model designers. Can the process of linking ecological meta-data to model design be automated? We aim {{to answer this question}} by: (1) develop-ing a formal language for expressing ecological meta-data; (2) building a logic-based formalisation of con-nections between the meta-data descriptions and model design; and (3) semi-automating model design endorsed by <b>meta-data</b> descriptions. Logic-based <b>approaches</b> for ecological modeling have been proposed in (Robertson et al. 1991). Emphasis is placed on the use of domain knowledge to support modeling automation, making model assumptions ex-plicit to enable more informed model analysis. Our work evolves from these ideas, adding to them by in-vestigating how ecological meta-data (which play a part in domain knowledge) can be conducive to model con-struction. Automated modeling based on meta-data can only be attractive if the mechanisms supporting it are not restrictive to specic datasets. Pre-dening a detailed knowledge representation system still general enough to express properties of every ecological dataset is in-feasible. Thus, what is needed is a general framework for meta-data description which can be instantiated to speci c datasets and model purposes. We have a proto-type ontology, named Ecolingua, for such a framework, providing a vocabulary and axioms for ecological meta-data description. Ecolingua has been designed by trying and describ-ing a diverse dataset generated by a tropical forest log-ging experiment in the Amazon, Brazil (Biot 1995). As design tool we have been using the www-based Ontolin-gua Server (Farquhar, Fikes, & Rice 1996) for ontolog...|$|R
40|$|Lack of {{knowledge}} about emerging issues limits {{the ability of a}} discipline to accurately target resources to support effective and sustainable practice. There is evidence to suggest that human factors and ergonomics (HFE) professionals do not have the specific futures thinking skills and methodologies to allow these issues to be identified. This study examined options for the professional not trained in futures studies to explore emerging issues in order to support an interdisciplinary approach to complex problems. The focus of the study was on emerging issues in the ergonomics of office work. The study took an action research/learning and largely phenomenological approach: four narrative methodologies were tested and their strengths and weaknesses compared. These included a literature review/environmental scanning (4 years' duration), expert interviews (19 interviews), a scenarios workshop (16 participants) and a naturalistic sensemaking survey using the software SenseMakerTM (99 participants). The latter two case studies were conducted in a large public sector organisation. Of the four methods, naturalistic sensemaking resulted in the least bias and was the most sustainable. A model– the Sensemaking Spiral– and methodological framework– Context for Action (CA) – were developed through the comparison of the case studies, supporting theory and researcher reflection. The five elements in the CA framework are perspective, momentum, narrative, patterns and meaning. These elements allow a multi-ontology and multi-epistemology approach; a balanced research design should select methodologies which cover all five elements in order to be valid and sustainable. Context for Action allows different methodologies to be 'scaffolded' together without subsuming one into the other. Performance criteria for each element are proposed. These allow the comparison of different research designs to explore emerging issues. This study has argued that meta-data is important and demonstrated the potential for combining <b>meta-data</b> and narrative <b>approaches</b> in ergonomics, allowing quantitative as well as qualitative analysis. Like many problems in HFE, the ergonomics of office work was found to be complex. Evidence that complexity theory contributes {{to a better understanding of}} complex problems in ergonomics has been presented. The Sensemaking Spiral allows a research project to be positioned with regards to different frameworks for ignorance and knowledge and with respect to time. An approach using ignorance based learning and narrative was found to be effective for exploring emerging issues. Frameworks for ignorance are suggested as a powerful way to locate and manage a futures focussed project in an interdisciplinary context...|$|R

