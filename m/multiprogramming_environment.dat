43|68|Public
5000|$|Its queues are non-blocking. While on {{dedicated}} processors, {{access to}} the queues can be synchronized using locks, this is not advisable in a <b>multiprogramming</b> <b>environment</b> since the operating system might preempt the worker thread holding the lock, blocking the progress of any other workers that try to access the same queue.|$|E
5000|$|The cache miss rate of {{recursive}} matrix multiplication is {{the same}} as that of a tiled iterative version, but unlike that algorithm, the recursive algorithm is cache-oblivious: there is no tuning parameter required to get optimal cache performance, and it behaves well in a <b>multiprogramming</b> <b>environment</b> where cache sizes are effectively dynamic due to other processes taking up cache space.(The simple iterative algorithm is cache-oblivious as well, but much slower in practice if the matrix layout is not adapted to the algorithm.) ...|$|E
40|$|Fairness is a {{mathematical}} abstraction: in a <b>multiprogramming</b> <b>environment,</b> fairness abstracts {{the details of}} admissible ("fair") schedulers; in a distributed environment, fairness abstracts the independent processor speeds. We argue that the standard definition of fairness often is unnecessarily weak and can {{be replaced by the}} stronger, yet still abstract, notion of finitary fairness. While standard weak fairness requires that no enabled transition is postponed forever, finitary weak fairness requires that for every computation of a system there is an unknown bound k such that no enabled transition is postponed more than k consecutive times. In general, the finitary restriction fin(F) of any given fairness assumption F is the union of all !-regular safety properties contained in F. The adequacy of the proposed abstraction is shown in two ways. Suppose we prove a program property under the assumption of finitary fairness. In a <b>multiprogramming</b> <b>environment,</b> the program then satisfies [...] ...|$|E
40|$|We {{investigate}} proactive {{dynamic load}} balancing on multicore systems, in which threads are continually migrated {{to reduce the}} impact of processor/thread mismatches to enhance {{the flexibility of the}} SPMD-style programming model, and enable SPMD applications to run efficiently in <b>multiprogrammed</b> <b>environments.</b> We present Juggle, a practical decentralized, user-space implementation of a proactive load balancer that emphasizes portability and usability. Juggle shows performance improvements of up to 80 % over static balancing for UPC, OpenMP, and pthreads benchmarks. We analyze the impact of Juggle on parallel applications and derive lower bounds and approximations for thread completion times. We show that results from Juggle closely match theoretical predictions across a variety of architectures, including NUMA and hyper-threaded systems. We also show that Juggle is effective in <b>multiprogrammed</b> <b>environments</b> with unpredictable interference from unrelated external applications...|$|R
40|$|Existing user-level network {{interfaces}} deliver high bandwidth, {{low latency}} performance to applications, but are typically unable to support diverse styles {{of communication and}} are unsuitable for use in <b>multiprogrammed</b> <b>environments.</b> Often {{this is because the}} network abstraction is presented at too high a level, and support for synchronisation is inflexible. In thi...|$|R
40|$|In this paper, {{we present}} an OpenMP {{implementation}} designed for <b>multiprogrammed</b> <b>environments</b> on Intel-based SMPs. This implementation {{consists of a}} runtime system and a resource manager, while we use the NanosCompiler to transform OpenMP applications into code with calls to our runtime system. The resource manager acts as the operating system scheduler for the applications built with our runtime. The runtime system cooperates with the resource manager in order to adapt each application's generated parallelism {{to the number of}} processors allocated to it, according to the resource manager scheduling policy. We use the OpenMP version of the NAS Parallel Benchmark suite in order to evaluate the performance of our implementation. In our experiments we compare the performance of our implementation with that of a commercial OpenMP implementation. The comparison proves that our approach performs better both on a dedicated and on a <b>multiprogrammed</b> <b>environment...</b>|$|R
40|$|Recent superscalar {{processors}} {{that use}} deep pipelines and wide issue rates, highly depend on efficient branch prediction {{to obtain a}} large amount of instruction level parallelism. Much work has been carried out in this area and the outcome of branches is very predictable. Mostly, superscalar processors are used in a <b>multiprogramming</b> <b>environment</b> to obtain higher throughput. In this paper, we analyze the impact of process switches on prediction accuracy and show that the accuracy of branch prediction is still high in a <b>multiprogramming</b> <b>environment</b> where process switches occur in the order of milliseconds. However, in multi-threading architectures, the prediction accuracy will be severely degraded due to the high frequency of process switches. 1 Introduction In order to fully exploit the performance of recent superscalar processors, effective branch prediction schemes are essential. To improve branch prediction accuracy, many hardware and software schemes have been proposed. In [11] [12] [...] ...|$|E
40|$|Today {{most of the}} {{multiprocessor}} supercomputer {{systems are}} still used within a <b>multiprogramming</b> <b>environment,</b> where the individual processors execute different jobs which are totally independent. All programs compete for the available resources. With multitasking, processors may also execute different parts of one program in parallel. The behaviour of parallel programs within a <b>multiprogramming</b> <b>environment</b> {{and the influence of}} these programs on the overall workload is of great interest for the rating of multitasking concepts with respect to their efficiency in practice. While the efficiency of multitasking is proved for parallel programs running in a dedicated environment, this paper leads to a new approach for the assessment of multitasking. It shortly describes the benchmark generation environment PAR-Bench, which is implemented on a Cray Y-MP under the UNICOS operating system; based on full information about system activities, this system enables reasonable measurements of parallel programs in a multiprogramming mode. Additionally, a few typical results obtained from first measurements are summarized...|$|E
40|$|Today, {{most of the}} Cray {{multiprocessor}} {{systems are}} still used within a <b>multiprogramming</b> <b>environment.</b> In such environments, {{there are two main}} issues contributing to whether or not production codes should exploit parallelism. Firstly, in terms of turnaround time, the parallel program should run faster than the single-tasked program version, and secondly, the costs, i. e. CPU-time from the user's point of view as well as system throughput from the computer center's point of view, should remain reasonably constant. This become...|$|E
40|$|In this work, {{we present}} an OpenMP {{implementation}} suitable for <b>multiprogrammed</b> <b>environments</b> on Intel-based SMP systems. This implementation {{consists of a}} runtime system and a resource manager, while we use the NanosCompiler to transform OpenMP-coded applications into code with calls to our runtime system. The resource manager acts as the operating system scheduler for the applications built with our runtime system. It executes a custom made scheduling policy to distribute the available physical processors to the active applications. The runtime system cooperates with the resource manager in order to adapt each application's generated parallelism {{to the number of}} processors allocated to it, according to the resource manager scheduling policy. We use the OpenMP version of the NAS Parallel Benchmark suite in order to evaluate the performance of our implementation. In our experiments we compare the performance of our implementation with that of a commercial OpenMP implementation. The comparison proves that our approach performs better both on a dedicated and on a heavily <b>multiprogrammed</b> <b>environment...</b>|$|R
50|$|On a {{computer}} with multiple processors different processes can be allocated to different processors {{so that the}} computer can truly multitask. Some programs, such as Adobe Photoshop and YafRay, which can require intense processing power, have been coded {{so that they are}} able to run on more than one processor at once, thus running more quickly and efficiently. This method is generally suitable for <b>multiprogramming</b> <b>environments</b> and is actually very helpful.|$|R
40|$|Abstract We {{investigate}} proactive {{dynamic load}} balancing on multicore systems, in which threads are continually migrated {{to reduce the}} impact of processor/thread mismatches. Our goal is to enhance {{the flexibility of the}} SPMD-style programming model and enable SPMD applications to run efficiently in <b>multiprogrammed</b> <b>environments.</b> We present Juggle, a practical decentralized, user-space implementation of a proactive load balancer that emphasizes portability and usability. In this paper we assume perfect intrinsic load balance and focus on extrinsic imbalances caused by OS noise, multiprogramming and mismatches of threads to hardware parallelism. Juggle shows performance improvements of up to 80 % over static load balancing for oversubscribed UPC, OpenMP, and pthreads benchmarks. We also show that Juggle is effective in unpredictable, <b>multiprogrammed</b> <b>environments,</b> with up to a 50 % performance improvement over the Linux load balancer and a 25 % reduction in performance variation. We analyze the impact of Juggle on parallel applications and derive lower bounds and approximations for thread completion times. We show that results from Juggle closely match theoretical predictions across a variety of architectures, including NUMA and hyper-threaded systems...|$|R
40|$|This report {{describes}} the program description of an interactive graphics package interfacing the Vector General Graphics Display Unit and a Digital Equipment Corporation PDP- 11 / 50 computer. The program {{was written in}} the C-programming language and designed {{to be used in the}} <b>multiprogramming</b> <b>environment</b> of the UNIX Timesharing operating system. Included is a description of the Vector General, operating system modifications, device driver, and interface routines. (Author) Prepared for: Naval Electronics Systems Command (ELEX 320) Washington, D. C. [URL] 59051 N...|$|E
40|$|Barrier synchronizations {{can be very}} {{expensive}} on <b>multiprogramming</b> <b>environment</b> because no process can go past a barrier until all the processes have arrived. If a process participating at a barrier is swapped out by the operating system, the rest of participating processes end up waiting for the swapped-out process. This paper presents a compile-time/run-time system that uses a dependence-driven execution to overlap the execution of computations separated by barriers so that the processes do not {{spend most of the}} time idling at the synchronization point...|$|E
40|$|Abstract − This paper {{presents}} an efficient system level power saving method for DRAM with multiple power modes. The proposed method {{is based on}} the power aware scheduling algorithm that controls DRAM modules in coarse grain in which the scheduler assigns appropriate power modes to memory banks at context switching time. The method controls the transition of multiple power modes, which is currently available technology, based on the history of gaining processor and memory bank usage of each process. The experimental results demonstrate the efficiency of the proposed schemes in <b>multiprogramming</b> <b>environment...</b>|$|E
40|$|NthLib is the {{concrete}} n-RTL implementation {{done in the}} context of the NANOS project to support multi-level and fine-grain parallelism in a <b>multiprogrammed</b> <b>environment.</b> This deliverable presents the design decisions we have adopted in order to achieve the previous goals, the structure and implementation of the library and some starting evaluation measurements. 1 Contents 1. Introduction................................................ 2 2. Outline of the Programming Model.............................. 2 3. Design Decisions............................................ 3 4. NthLib Interface............................................. 5 5. NthLib Implementation....................................... 5 5. 1. Data structures.............................. [...] . ...|$|R
40|$|Abstract—The paper {{gives an}} {{overview}} of the current computer hardware trends and software issues. The optimal utilization of a <b>multiprogrammed</b> <b>environment</b> with multi-core processors is an extremely complex job. The scheduling in such an environment is an NP hard problem. The authors of the paper propose a novel approach to model the behaviour of the computing environment. Furthermore an application modelling scheme is introduced. The proposed approach may allow the operating system to react to the changes in a reconfigurable environment. By using the introduced solution {{it would be possible to}} make the operating system more proactive. I...|$|R
40|$|This paper overviews {{results from}} our recent work on {{building}} customized system software support for Distributed Shared Memory Multiprocessors. The mechanisms and policies outlined {{in this paper}} are connected with a single conceptual thread: they all attempt to reduce the memory latency of parallel programs by optimizing critical system services, while hiding the complex architectural details of Distributed Shared Memory from the programmer. We present four techniques that exhibit solid performance improvements: Efficient memory management for lightweight multithreading, highly scalable hybrid synchronization primitives, a virtual memory management scheme for DSM systems and transparent operating system services for adapting parallel programs to <b>multiprogrammed</b> <b>environments...</b>|$|R
40|$|The {{emergence}} of multicore and manycore processors {{is set to}} change the parallel computing world. Applications are shifting towards increased parallelism in order to utilise these architectures efficiently. This leads to a situation where every application creates its desirable number of threads, based on its parallel nature and the system resources allowance. Task scheduling in such a multithreaded <b>multiprogramming</b> <b>environment</b> is a significant challenge. In task scheduling, not only {{the order of the}} execution, but also the mapping of threads to the execution resources is of a great importance. In this paper we state and discuss some fundamental rules based on results obtained from selected applications of the BOTS benchmarks on the 64 -core TILEPro 64 processor. We demonstrate how previously efficient mapping policies such as those of the SMP Linux scheduler become inefficient when the number of threads and cores grows. We propose a novel, low-overhead technique, a heuristic based on {{the amount of time spent}} by each CPU doing some useful work, to fairly distribute the workloads amongst the cores in a <b>multiprogramming</b> <b>environment.</b> Our novel approach could be implemented as a pragma similar to those in the new task-based OpenMP versions, or can be incorporated as a distributed thread mapping mechanism in future manycore programming frameworks. We show that our thread mapping scheme can outperform the native GNU/Linux thread scheduler in both single-programming and multiprogramming environments. Comment: ParCo Conference, Munich, Germany, 201...|$|E
40|$|As we {{know there}} is an {{increasing}} demand from the software industry to develop software model which can communicate and exchange information concurrently in <b>multiprogramming</b> <b>environment.</b> This {{is very difficult to}} find and eliminate concurrency problem like deadlock in a large and complex system. There is a need of model to identify and recognize concurrency problem such as deadlocks in the early stage in system and design model to get rid of these kinds of problems. In this paper author proposed a UML model for detecting the concurrency in early stage of design system. The author has also proposed a sequence diagram, activity diagram and use case diagram for the above model...|$|E
40|$|Computer job {{scheduling}} is often performed with little {{understanding of the}} formal properties of the jobs being scheduled. One {{reason for this is}} that optimal solutions for {{job scheduling}} on computers are difficult to obtain if the job stream has mixed objectives, i. e., it consists of some jobs whose turnaround time has to be minimized and others whose deadlines must be met. A practical algorithm for scheduling mixed job streams on monoprogrammed computers, with potential application to a <b>multiprogramming</b> <b>environment</b> is presented. The algorithm takes into account variable cost rates for each job. Experimental results illustrate the efficiency of the algorithm in terms of both its proximity to optimal solutions and its low computational complexity. 1...|$|E
40|$|In {{order to}} improve the {{implementation}} of parallel programs into <b>multiprogramming</b> <b>environments,</b> a new scheduling system has been developed by which in interaction of load adaptive algorithms within the components, cooperative decisions are derived under consideration of the specialities of parallel programs in non-dedicated environments. Basic, results for this system were obtained {{by means of an}} event-controlled simulator into which the cooperative scheduling components had been integrated. Using the cooperative scheduling strategy in the parallel processing, the average waiting time of the programs could be considerably reduced. (WEN) Available from TIB Hannover: RA 831 (2850) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekSIGLEDEGerman...|$|R
40|$|This paper studies compile-time and runtime {{techniques}} for enhancing performance portability of MPI code running on multiprogrammed shared memory machines. The proposed techniques allow MPI nodes {{to be executed}} safely and efficiently as threads. Compile-time transformation eliminates global and static variables in C code using node-specific data. The runtime support includes an efficient and provablycorrect communication protocol that uses lock-free data structure and takes advantage of address space sharing among threads. The experiments on SGI Origin 2000 show that our MPI prototype called TMPI using the proposed techniques is competitive with SGI's native MPI implementation in a dedicated environment, {{and that it has}} significant performance advantages in a <b>multiprogrammed</b> <b>environment...</b>|$|R
40|$|This paper {{surveys the}} {{deterministic}} scheduling of jobs m uniprocessor, multiprocessor, and job-shop environments. The survey {{begins with a}} brief introduction to the representation of task or job sets, followed by a discussion of classification categories. These categories include number of processors, task interruptlbility, job periodicity, deadlines, and number of resources. Results are given for single-processor schedules in job-shop and <b>multIprogramming</b> <b>environments,</b> flow-shop schedules, and multiprocessor schedules. They are {{stated in terms of}} optimal constructive algorithms and suboptimal heuristics. In most cases the latter are stated in terms of performance bounds related to optimal results. Annotations for most of the references are provided {{in the form of a}} table classifying the referenced studies m terms of various parameters...|$|R
40|$|This study {{describes}} real-time kernel essential {{mechanisms and}} {{deals with the}} implementation of a real-time multitasks executive. To make more advantage of microprocessors in applications involving many functions simultaneously, our real-time kernel provides a <b>multiprogramming</b> <b>environment</b> in which many independent multitasking application programmes may execute and provides facilities to manage efficiently the processes (tasks) and communicate between them. These facilities are provided by system calls that handle data structures namely tasks, semaphores, messages, events flag, resources, mail-boxes, queues and interruptions. Present kernel is preemptive and priorities assigned to tasks are dynamic, the kernel manages up to 63 task levels (63 is the lowest priority level assigned to the idle task). Round robin scheduling is not used here...|$|E
40|$|The {{overall system}} {{performance}} {{is of interest}} when multitasking concepts are used in a <b>multiprogramming</b> <b>environment.</b> To get reliable results, a programming system is developed, generating synthetic user programs on a CRAY multiprocessor which simulate a given work load in a flexible way by using the hardware performance monitor (HPM). The resulting benchmark environment is used to insert additional sequential as well as parallel programs. First results obtained from these investigations seem to prove the efficiency especially for the fine-grain multitasking concepts which provide good performance in dedicated as well as batch oriented multiprogramming environments; for selected production codes on the CRAY Y-MP these concepts now are used to evaluate their effects on a loaded system...|$|E
40|$|An {{analysis}} {{was made of}} the correlation between performance and resource usage variables of a given computer job and between the performance variables of a given job and the resource usage variables of other jobs in a <b>multiprogramming</b> <b>environment.</b> This {{analysis was}} performed in order to: (1) determine the mix and characteristics of jobs which lead to high performance, (2) provide regression equation predictors of performance for given resource utilizations, and (3) provide performance and resource utilization coefficients for use in a linear programming resource allocation model. The linear programming model is used to select an optimum job mix subject to production, resource usage and budgetary constraints. (Author) Nava 1 Weapons Center, China Lake, California[URL] 72 PO- 2 - 0056 N...|$|E
40|$|We {{study the}} {{performance}} of user-level thread schedulers in <b>multiprogrammed</b> <b>environments.</b> Our goal is a user-level thread scheduler that delivers efficient performance under multiprogramming without any need for kernel-level resource management, such as coscheduling or process control. We show that a non-blocking implementation of the work-stealing algorithm achieves this goal. With this implementation, the execution time of a computation running with arbitrarily many processes on arbitrarily many processors can be modeled as a simple function of work and critical-path length. This model holds even when the processes run {{on a set of}} processors that arbitrarily grows and shrinks over time. We observe linear speedup whenever the number of processes is small relative to the average parallelism. ...|$|R
40|$|In practice, the {{performance}} evaluation of supercomputers is still substantially driven by singlepoint estimates of metrics (e. g., MFLOPS) obtained by running characteristic benchmarks or workloads. With the {{rapid increase in}} the use of time-shared multiprogramming in these systems, such measurements are clearly inadequate. This is because multiprogramming and system overhead, as well as other degradations in performance due to time varying characteristics of workloads, are not taken into account. In <b>multiprogrammed</b> <b>environments,</b> multiple jobs and users can dramatically increase the amount of system overhead and degrade {{the performance}} of the machine. Performance techniques, such as benchmarking, which characterize performance on a dedicated machine ignore this major component of true computer performance. Due to the complexity of analysis, there has been little work done in analyzing, modeling, and predicting the performance of applications in <b>multiprogrammed</b> <b>environments.</b> This is especially true for parallel processors, where {{the costs and benefits of}} multi-user workloads are exacerbated. While some may claim that the issue of multiprogramming is not a viable one in the supercomputer market, experience shows otherwise. Even in recent massively parallel machines, multiprogramming is a key component. It has even been claimed that a partial cause of the demise of the CM 2 was the fact that it did not efficiently support time-sharing. In the same paper, Gordon Bell postulates that, multicomputers will evolve to multiprocessors in order to support efficient multiprogramming. Therefore, it is clear that parallel processors of the future will be required to offer the user a time-shared environment with reasonable response times for the applications. In this type of environment, the most important performance metric is the completion of response time of a given application. However, there are a few evaluation efforts addressing this issue...|$|R
40|$|We {{present the}} first lock-free {{implementation}} of an extensible hash table running on current architectures. Our algorithm provides concurrent insert, delete, and search operations with an expected O(1) cost. It consists of very simple code, easily implementable using only load, store, and compare-and-swap operations. The new mathematical structure {{at the core}} of our algorithm is recursive split-ordering, a way of ordering elements in a linked list {{so that they can be}} repeatedly “split ” using a single compare-and-swap operation. Though lock-free algorithms are expected to work best in <b>multiprogrammed</b> <b>environments,</b> empirical tests we conducted on a large shared memory multiprocessor show that even in non-multiprogrammed environments, the new algorithm performs as well as the most efficient known lock-based resizable hash-table algorithm, and in high load cases it significantly outperforms it...|$|R
40|$|Abstract Barrier synchronizations {{can be very}} {{expensive}} on <b>multiprogramming</b> <b>environment</b> because no process can go past a barrier until all the processes have arrived. If a process participating at a barrier is swapped out by the operating system, the rest of participating processes end up waiting for the swapped-out process. This paper presents a compile-time/run-time system that uses a dependence-driven execution to overlap the execution of computations separated by barriers so that the processes do not {{spend most of the}} time idling at the synchronization point. Keywords: Run-time systems, multiprogramming, loop scheduling, dependence-driven execution, barrier synchronization, coarse-grain dataflow. The parallel execution of a sequence of loop nests is typically broken into phases, each phase consisting of a simple loop separated by a barrier synchronization to ensure that the execution respect...|$|E
40|$|Although {{previous}} {{studies have shown that}} a large file of overlapping register windows can greatly reduce procedure call/return overhead, the effects of register windows in a <b>multiprogramming</b> <b>environment</b> are poorly understood. This paper investigates the performance of multiprogrammed, reduced instruction set computers (RISCs) as a function of window management strategy. Using an analytic model that reflects context switch and procedure call overheads, we analyze the performance of simple, linearly self-recursive programs. For more complex programs, we present the results of a simulation study. These studies show that a simple strategy that saves all windows prior to a context switch, but restores only a single window following a context switch, performs near optimally. Champaign. tSupported by a Senior Fulbright Scholarship while on leave at the University of Illinoisat Urbane...|$|E
40|$|Systems {{with large}} numbers of cores have become commonplace. Accordingly, {{applications}} are shifting towards increased parallelism. In a general-purpose system, applications residing in the system compete for shared resources. Thread and task scheduling in such a multithreaded <b>multiprogramming</b> <b>environment</b> is a significant challenge. In this study, we have chosen the Intel Xeon Phi system as a modern platform to explore how popular parallel programming models, namely OpenMP, Intel Cilk Plus and Intel TBB (Threading Building Blocks) scale on manycore architectures. We have used three benchmarks with different features which exercise different aspects of the system performance. Moreover, a multiprogramming scenario is used to compare the behaviours of these models when all three applications reside in the system. Our initial results show that it is to some extent possible to infer multiprogramming performance from single-program cases...|$|E
40|$|Introduction Traditional {{benchmarks}} such as SPEC model {{a simple}} workload: a single address space, a single thread of control, no input beyond an initial data set, and {{little if any}} use of system routines. Increasingly common computer usage includes <b>multiprogrammed</b> <b>environments,</b> interactivity, and multiprocessor machines. Workloads that represent these environments have significantly different characteristics than single-process batch workloads [1]. Computer architects often rely on measurements of existing systems to drive studies of system performance. These measurements are gathered by behavior monitoring systems and might include information such as execution profiles, address traces, and instruction traces. Existing behavior monitoring techniques encounter difficulty when applied {{to the task of}} gathering information about execution behavior of complex workloads on a variety of computers. Hardware techniques often cause little, if any, disturbance of the system, but lack b...|$|R
40|$|Parallel {{execution}} of application programs on a multiprocessor system {{may lead to}} performance degradation if the workload of a parallel region is not large enough to amortize the overheads associated with the parallel execution. Furthermore, if too many processes are running on the system in a <b>multiprogrammed</b> <b>environment,</b> {{the performance of the}} parallel application may degrade due to resource contention. This work proposes a comprehensive dynamic processor allocation scheme that takes both program behavior and system load into consideration when dynamically allocating processors. This mechanism was implemented on the Solaris operating system to dynamically control the {{execution of}} parallel C and Java application programs. Performance results show the effectiveness of this scheme in dynamically adapting to the current execution environment and program behavior, and that it outperforms a conventional time-shared system. ...|$|R
40|$|Supercomputers run multiprogrammed {{time-sharing}} operating systems, {{so their}} facilities {{can be shared}} by many local and remote users. Therefore, {{it is important to}} be able to assess the performance of supercomputers in <b>multiprogrammed</b> <b>environments.</b> Analytic models based on Queueing Networks (QNs) and Stochastic Petri Nets (SPNs) are used in this paper with two purposes: to evaluate the performance of supercomputers in multi-programmed environments, and to compare, perfor-mance-wise, conventional supercomputer architectures with a novel architecture proposed here. It is shown, with the aid of the analytic models, that the proposed architecture is preferable performance-wise over the existing conventional supercomputer architectures. A three-level workload characterization model for super-computers is presented. Input data for the numerical ex-amples discussed here are extracted from the well-known Los Alamos benchmark, and the results are vali-dated by simulation...|$|R
