1|12|Public
5000|$|Currently, the World Wide Web {{is based}} mainly on {{documents}} written in Hypertext Markup Language (HTML), a <b>markup</b> <b>convention</b> {{that is used}} for coding a body of text interspersed with multimedia objects such as images and interactive forms. Metadata tags provide a method by which computers can categorise the content of web pages, for example: ...|$|E
5000|$|... noweb is {{independent}} of the programming language of the source code. It {{is well known for}} its simplicity, given the need of using only two text <b>markup</b> <b>conventions</b> and two tool invocations, and it allows for text formatting in HTML rather than going through the TeX system.|$|R
40|$|We propose {{some simple}} design rules for {{encoding}} multi-lingual texts for flexibility of further automatic processing. Our main recommendation is {{to include the}} available descriptive information with the data, and to use symbolic <b>markup</b> <b>conventions.</b> These rules have been applied successfully to the first steps of compiling a multi-lingual dictionary...|$|R
25|$|HyperText Markup Language (HTML), {{one of the}} {{document}} formats of the World Wide Web, is an instance of SGML (though, strictly, it does not comply with all the rules of SGML), and follows many of the <b>markup</b> <b>conventions</b> used in the publishing industry in the communication of printed work between authors, editors, and printers.|$|R
40|$|Armstrong and Russell (1990). On clean inputs, {{such as the}} Canadian Hansards, {{these methods}} have been very {{successful}} (at least 96 % correct by sentence). Unfortunately, if the input is noisy (due to OCR and/or unknown <b>markup</b> <b>conventions),</b> then these methods tend to break down because the noise can {{make it difficult to}} find paragraph boundaries, let alone sentences. This paper describes a new program, charalign, that aligns texts at the character level rather than at the sentence/paragraph level, based on the cognate approach proposed by Simard et al. 1...|$|R
40|$|Mariano P. Consens Department of Computer Science University of Waterloo Waterloo, Canada N 2 L 3 G 1 mconsens@uwaterloo. ca Tova Milo Department of Computer Science Tel Aviv University Tel Aviv, Israel 69978 milo@math. tau. ac. il Abstract There is a {{significant}} amount of interest in combining and extending database and information retrieval technologies to manage textual data. The challenge is becoming more relevant due to the increased availability of documents in digital form. Document data has a natural hierarchical structure, which may be made explicit due to the use of <b>markup</b> <b>conventions</b> (as it is the case with SGML). An important aspect of managing structured and semi-structured textual data consists of supporting the efficient retrieval of text components based both on their content and structure. In this paper we study issues related to the expressive power and optimization of a class of algebras that support combining string (or pattern) searches with queries on the h [...] ...|$|R
40|$|The Standard Generalized Markup Language (SGML), an ISO standard, {{has become}} the {{accepted}} method of defining <b>markup</b> <b>conventions</b> for text files. SGML is a metalanguage for defining grammars for textual markup {{in much the same}} way that Backus [...] Naur Form is a metalanguage for defining programming-language grammars. Indeed, HTML, the method of marking up a hypertext documents for the World Wide Web, is an SGML grammar. The underlying assumptions of the SGML initiative are that a logical structure of a document can be identified and that it can be indicated by the insertion of labeled matching brackets (start and end tags). Moreover, it is assumed that the nesting relationships of these tags can be described with an extended context-free grammar (the right-hand sides of productions are regular expressions). In this survey of some of the issues raised by the SGML initiative, I reexamine the underlying assumptions and address some of the theoretical questions that SGML raises [...] . ...|$|R
40|$|There {{have been}} a number of recent papers on {{aligning}} parallel texts at the sentence level, e. g., Brown et al (1991), Gale and Church (to appear), Isabelle (1992), Kay and Ro [...] senschein (to appear), Simard et al (1992), WarwickArmstrong and Russell (1990). On clean inputs, such as the Canadian Hansards, these methods have been very successful (at least 96 % correct by sentence). Unfortunately, if the input is noisy (due to OCR and/or unknown <b>markup</b> <b>conventions),</b> then these methods tend to break down because the noise can make it difficult to find paragraph boundaries, let alone sentences. This paper describes a new program, char_align, that aligns texts at the character level rather than at the sentence/paragraph level, based on the cognate approach proposed by Simard et al. 1. Introduction Parallel texts have recently received considerable attention in machine translation (e. g., Brown et al, 1990), bilingual lexicography (e. g., Klavans and Tzoukermann, 1990), and terminology resea [...] ...|$|R
5000|$|LaTeX ( [...] , , also {{pronounced}} as , , a shortening of Lamport TeX) is {{a document}} preparation system. When writing, the writer uses plain text {{as opposed to}} the formatted text found in WYSIWYG word processors like Microsoft Word, LibreOffice Writer and Apple Pages. The writer uses <b>markup</b> tagging <b>conventions</b> to define the general structure of a document (such as article, book, and letter), to stylise text throughout a document (such as bold and italics), and to add citations and cross-references. A TeX distribution such as TeX Live or MikTeX is used to produce an output file (such as PDF or DVI) suitable for printing or digital distribution. Within the typesetting system, its name is stylised as LaTeX.|$|R
40|$|AbstractThere is a {{significant}} amount of interest in combining and extending database and information retrieval technologies to manage textual data. The challenge is becoming more relevant due to increased availability of documents in digital form. Document data has a natural hierarchical structure, which may be made explicit due to the use of <b>markup</b> <b>conventions</b> (as with SGML). An important aspect of managing structured and semistructured textual data consists of supporting the efficient retrieval of text components based both on their content and on their structure. In this paper we study issues related to the expressive power and optimization of a class of algebras that support combining string (or pattern) searches with queries on the hierarchical structure of the text. Theregion algebrastudied is a set-at-a-time algebra for manipulatingtext regions(substrings of the text) that supports finding out nesting and ordering properties of the text regions. This algebra is part of the language in use in commercial text retrieval systems and can form the basis for supporting SQL-like access to textual data. By presenting a close relationship between the region algebra and the monadic first order theory of finite binary trees, we show that queries in the algebra can be optimized, in the sense that equivalence to less expensive expressions can be tested. This optimization can be difficult (co-NP-hard in the general case), but there is an important class of queries that can be optimized in polynomial time. On the negative side, we show that the language is incapable of capturing some important properties of the text structure, related to the nesting and ordering of text regions. We conclude by suggesting possible extensions to increase the expressive power of the language and consider one such example...|$|R
40|$|Written learner corpora {{have become}} a {{relatively}} common resource on the market of corpora. Spoken learner corpora, on the other hand, are still very rare. The LINDSEI (Louvain International Database of Spoken English Interlanguage) project was meant to fill this gap. It was launched in 1995 at the University of Louvain as the spoken counterpart of the International Corpus of Learner English (ICLE). LINDSEI {{is a collection of}} spoken data produced by higher intermediate to advanced learners of English as a foreign language. The collaboration with several universities internationally made it possible to include data from learners {{with a wide variety of}} mother tongue backgrounds. To date, eleven mother tongues are represented: Bulgarian, Chinese, Dutch, French, German, Greek, Italian, Japanese, Polish, Spanish and Swedish. The data consist of informal interviews that were transcribed orthographically (with some prosodic and phonetic information like pauses or syllable lengthening), following guidelines which were specifically designed for the project and which were unified across the subcorpora to ensure perfect comparability of the data. The transcripts also include some special markup, for example for overlapping speech or foreign words. Each interview is accompanied by a learner profile recording a number of variables such as the learner’s age, his/her knowledge of other foreign languages, the interviewer’s mother tongue or the duration of the interview. In this workshop I will describe the genesis of LINDSEI and the steps involved in its compilation. I will also present its structure, the transcription and <b>markup</b> <b>conventions</b> it relies on, and the metalinguistic data it includes. I will then demo the LINDSEI software, which allows users to compile their own tailor-made corpus on the basis of a set of predefined variables and to extract useful statistics. It will be shown how the corpus thus compiled can be imported into a concordancer like WordSmith Tools for further analysis. Several methods of analysis of the LINDSEI data will be mentioned, in particular when used in combination with LOCNEC (Louvain Corpus of Native English Conversation), a corpus of spoken data produced by native British English students which is the exact replica of LINDSEI. Finally, I will give an overview of the contents of the handbook that accompanies the LINDSEI CD-ROM...|$|R
40|$|Approved {{for public}} release, {{distribution}} unlimitedS) technologies {{to demonstrate a}} prototypical planning tool {{that can be used}} by today's deployed warfighter. All research and work is conducted in a web-based, 'user-centric' fashion utilizing a combination of user-driven and agentbased control of entities for simulation iterations, along with various open source technologies which include Extensible 3 D Graphics (X 3 D), Scalable Vector Graphics (SVG), and Extensible <b>Markup</b> Language (XML). <b>Conventions</b> are demonstrated for the integration of the many academic disciplines utilized during this research to achieve automatic generation of tactically significant scenarios. In order to give the end-user the greatest insight towards potential drawbacks in the tactical planning against surface-borne terrorist threats, various 2 D and 3 D media provide both real-time and non-real time scenario playback. The result of this work is a fully integrated, prototypical, Java-based application that demonstrates how various Open-Source, web-based technologies can be applied in order to provide the tactical operator with tools to aid in Force Protection planning. Scenarios can be auto generated, viewed, analyzed, and manipulated by end users with little to no computer experience necessary beyond requirements for operation of a desktop personal computer (PC) in the Information Technology for the 21 st Century (IT- 21) environment at sea. This approach has broad applicability to improve the tactical awareness and defensive posture of ships defending against terrorist attacks in port. Lieutenant, United States Nav...|$|R

