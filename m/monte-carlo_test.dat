14|56|Public
5000|$|A <b>Monte-Carlo</b> <b>test</b> (Allen and Smith, 1996; Allen and Robertson, 1996; Groth and Ghil, 2015) can {{be applied}} to {{ascertain}} the statistical significance of the oscillatory pairs detected by SSA. The entire time series or parts of it that correspond to trends, oscillatory modes or noise can be reconstructed by using linear combinations of the PCs and EOFs, which provide the reconstructed components (RCs) : ...|$|E
40|$|Several {{problems}} with the diagnostic check suggested by Peňa and Rodriguez [2002. A powerful portmanteau test of lack of fit for time series. J. Amer. Statist. Assoc. 97, 601 – 610. ] are noted and an improved Monte-Carlo version of this test is suggested. It is shown that quite often the test statistic recommended by Peňa and Rodriguez [2002. A powerful portmanteau test of lack of fit for time series. J. Amer. Statist. Assoc. 97, 601 – 610. ] may not exist and their asymptotic distribution of the test does {{not agree with the}} suggested gamma approximation very well if the number of lags used by the test is small. It is shown that the convergence of this test statistic to its asymptotic distribution may be quite slow when the series length is less than 1000 and so a <b>Monte-Carlo</b> <b>test</b> is recommended. Simulation experiments suggest the <b>Monte-Carlo</b> <b>test</b> is usually more powerful than the test given by Peňa and Rodriguez [2002. A powerful portmanteau test of lack of fit for time series. J. Amer. Statist. Assoc. 97, 601 – 610. ] and often much more powerful than the Ljung–Box portmanteau test. Two illustrative examples of enhanced diagnostic checking with the <b>Monte-Carlo</b> <b>test</b> are given. © 2006 Elsevier B. V. All rights reserved. Keywords: ARMA residual diagnostic test; Imhof distribution; Monte-Carlo test; Portmanteau diagnostic chec...|$|E
40|$|Suggested by Scullard's recent star-triangle {{relation}} for bond correlated systems, {{we propose}} a general "cell/dual-cell" transformation, which allows in principle an infinite variety of lattices with exact percolation thresholds to be generated. We directly verify Scullard's new site percolation thresholds, and derive the bond thresholds for his "martini" lattice (pc = 1 /sqrt(2)) and the "A" lattice (pc = 0. 625457 [...] ., solution to p^ 5 - 4 p^ 4 + 3 p^ 3 + 2 p^ 2 - 1 = 0). We also present a precise <b>Monte-Carlo</b> <b>test</b> the site threshold for the "A" lattice. Comment: Accepted for publication, PRE. 9 figure...|$|E
3000|$|... {{in order}} to {{increase}} the number of samples; however, the <b>Monte-Carlo</b> <b>tests</b> only provide the “discrete-approximate” value. As a result, there can appear some difference between the P [...]...|$|R
40|$|Air {{situation}} histories {{are represented}} by sequence set trees. These structures provide the representational power to model a rich variety of situations with {{only a small number}} of rules. The model's generative power makes it a candidate for use in <b>Monte-Carlo</b> <b>testing</b> of planning and surveillance regimes in the air defence domain. Sequence set...|$|R
3000|$|... 0). However, it can’t be {{analytically}} obtained {{because of}} {{a large amount of}} nonlinear operations in calculation of the WCENs. In fact, this is a common problem in nonparametric detectors. In this case, the decision thresholds have to be determined by <b>Monte-Carlo</b> <b>tests</b> to clutter-only data and the performance evaluations have to be performed by the experimental results to real data rather than rigorous theoretical analysis [27].|$|R
40|$|We use the Monte-Carlo (MC) test {{technique}} to find valid p-values when testing for discontinuities in jump-diffusion models. While {{the distribution of}} the LR statistic for this test is typically non-standard, we show that the MC p-value is finite sample exact if no other (identified) nuisance parameter is present. Otherwise, we derive nuisance-parameter free bounds and obtain exact bounds p-values. We illustrate our approach on four classes of jump-diffusion models we use to model spot prices of copper, nickel, gold, and crude oil. We find significant jumps in all weekly time series and in a few monthly time series. <b>Monte-Carlo</b> <b>test,</b> bounds test, discontinuous process, conditional heteroscedasticity...|$|E
40|$|A new {{portmanteau}} {{diagnostic test}} for vector {{autoregressive moving average}} (VARMA) models {{that is based on}} the determinant of the standardized multivariate residual autocorrelations is derived. The new test statistic may be considered an extension of the univariate portmanteau test statistic suggested by Pena and Rodriguez (2002, A Powerful Portmanteau Test of Lack of Test for Time Series, Journal of American Statistical Association) The asymptotic distribution of the test statistic is derived as well as a chi-square approximation. However, the <b>Monte-Carlo</b> <b>test</b> is recommended unless the series is very long. Extensive simulation experiments demonstrate the usefulness of this test as well as its improved power performance compared to widely used previous multivariate portmanteau diagnostic check. Two illustrative applications are given. Comment: 25 pages, 1 figure, 5 table...|$|E
30|$|The {{objective}} of our simulations {{is to explore}} {{the performance of the}} proposed portmanteau seasonal tests, Q_m(s),Q̂_m(s),Q̃_m(s), and D_m(s), in finite samples and when the sample size grow. We study the empirical type I and type II error rates demonstrating the accuracy of the approximation distributions of the proposed seasonal tests in producing the correct sizes and conducting a power comparison studies. For each simulation experiment, we determine the critical values from the corresponding asymptotic distributions of the proposed seasonal test statistics. One can use the <b>Monte-Carlo</b> <b>test</b> procedures, as described by Lin and McLeod (2006) and Mahdi and McLeod (2012), to compute these critical values instead of using the approximation distributions. The simulations were run on a modern quad-core personal computer using the R package portes (Mahdi and McLeod 2015) and WeightedPortTest (Fisher and Gallagher 2012) that are available from the CRAN website (R Development Core Team 2015).|$|E
3000|$|... is set as a {{constant}} value. Because {{the first order}} of detector has no analytic expression, we cannot obtain the accurate value of the first detection threshold η 1 by calculation, but it can be obtained by the widely used <b>Monte-Carlo</b> <b>tests</b> on the pure clutter for the false alarm probability Pfa 1. Also, due to the pulse sampling, the correlation time of return fluctuate extracted from sea clutter are discrete values. We define the γ [...]...|$|R
40|$|Abstract — This paper {{develops}} an optimization {{method to}} synthesize trajectories {{for use in}} the identification of system parameters. Using widely studied techniques to compute Fisher information based on observations of nonlinear dynamical systems, an infinite-dimensional, projection-based optimization algorithm is formulated to optimize the system trajectory using eigenvalues of the Fisher information matrix as the cost metric. An example of a cart-pendulum simulation demonstrates a sig-nificant increase in the Fisher information using the optimized trajectory with decreased parameter variances shown through <b>Monte-Carlo</b> <b>tests</b> and computation of the Cramer-Rao lower bound. I...|$|R
40|$|Description eRm fits Rasch models (RM), linear {{logistic}} test models (LLTM), {{rating scale}} model (RSM), linear rating scale models (LRSM), partial credit models (PCM), and linear partial credit models (LPCM). Missing values are {{allowed in the}} data matrix. Additional features are the ML estimation of the person parameters, Andersen’s LR-test, itemspecific Wald test,Martin-Loef-Test, nonparametric <b>Monte-Carlo</b> <b>Tests,</b> itemfit and personfit statistics including infit and outfit measures,various ICC and related plots, automated stepwise item elimination, simulation module for various binary data matrices. An eRm platform is provided at R-forge (see URL) ...|$|R
40|$|This paper {{presents}} a hybrid route-path planning model for an Autonomous Underwater Vehicle's task assignment and management while the AUV is operating through the variable littoral waters. Several prioritized tasks distributed {{in a large}} scale terrain is defined first; then, considering the limitations over the mission time, vehicle's battery, uncertainty and variability of the underlying operating field, appropriate mission timing and energy management is undertaken. The proposed objective is fulfilled by incorporating a route-planner that is in charge of prioritizing the list of available tasks according to available battery and a path-planer that acts in a smaller scale to provide vehicle's safe deployment against environmental sudden changes. The synchronous process of the task assign-route and path planning is simulated using a specific composition of Differential Evolution and Firefly Optimization (DEFO) Algorithms. The simulation results indicate that the proposed hybrid model offers efficient performance in terms of completion of maximum number of assigned tasks while perfectly expending the minimum energy, provided by using the favorable current flow, and controlling the associated mission time. The <b>Monte-Carlo</b> <b>test</b> is also performed for further analysis. The corresponding results show the significant robustness of the model against uncertainties of the operating field and variations of mission conditions...|$|E
40|$|In this thesis, a new univariate-multivariate {{portmanteau}} test is derived. The proposed {{test statistic}} {{can be used}} for diagnostic checking ARMA, VAR, FGN, GARCH, and TAR time series models as well as for checking randomness of series and goodness-of- fit VAR models with stable Paretian errors. The asymptotic distribution of the test statistic is derived as well as a chi-square approximation. However, the <b>Monte-Carlo</b> <b>test</b> is recommended unless the series is very long. Extensive simulation experiments demonstrate the usefulness of this test and its improved power performance compared to widely used previous multivariate portmanteau diagnostic check. The contributed R package portes is also introduced. This package can utilize multi-core CPUs often found in modern personal computers as well as a computer cluster or grid. The proposed package includes the most important univariate and multivariate diagnostic portmanteau tests with the new test statistic given in this thesis. It is also useful for simulating univariate/multivariate data from nonseasonal ARIMA/VARIMA process with nite or in nite variances, testing for stationarity and invertibility, and estimating parameters from stable distributions. Many illustrative applications are given. In this thesis, {{it has been shown that}} the classical ordinary least squares regression may produce smaller p-values than it should due to the lack of statistical independency in the tted model which may invalidate the statistical inferences. The Poincare plots are suggested to check for such hidden positive correlations...|$|E
40|$|De-noising is a {{substantial}} issue in hydrologic time series analysis, {{but it is a}} difficult task due to the defect of methods. In this paper an energy-based wavelet de-noising method was proposed. It is to remove noise by comparing energy distribution of series with the background energy distribution, which is established from <b>Monte-Carlo</b> <b>test.</b> Differing from wavelet threshold de-noising (WTD) method with the basis of wavelet coefficient thresholding, the proposed method is based on energy distribution of series. It can distinguish noise from deterministic components in series, and uncertainty of de-noising result can be quantitatively estimated using proper confidence interval, but WTD method cannot do this. Analysis of both synthetic and observed series verified the comparable power of the proposed method and WTD, but de-noising process by the former is more easily operable. The results also indicate the influences of three key factors (wavelet choice, decomposition level choice and noise content) on wavelet de-noising. Wavelet should be carefully chosen when using the proposed method. The suitable decomposition level for wavelet de-noising should correspond to series' deterministic sub-signal which has the smallest temporal scale. If too much noise is included in a series, accurate de-noising result cannot be obtained by the proposed method or WTD, but the series would show pure random but not autocorrelation characters, so de-noising is no longer needed...|$|E
40|$|Constraint-based {{planning}} systems, {{especially those}} used for real-world applications, {{have a very}} large space of possible states. This makes them very hard to test, as only one path through the state space can be tested at once, and it is tricky to manually select a small set of paths that will thoroughly test a system. This paper describes my implementation of a <b>Monte-Carlo</b> <b>testing</b> system for the T-REX planner, which was developed at MBARI for AUV control. The test harness performs repeated runs of the planner, forcing each run {{through a series of}} randomly sampled states, in an attempt to uncover bugs in the system that manual methods may fail to detect...|$|R
40|$|This {{document}} {{presents a}} solution to the problem of determining the composi-tion of linearly combined statistical samples (histograms) formed by multiple data generating processes. In this problem the probability distributions of individual processes are either known, or need to be estimated, and their relative proportions need to be determined. The method is based upon Bayes Theorem implemented in the form of an Expectation Maximisation (EM). Components of the theory have been developed which deal with uncertainty due to perturbation in incoming (mea-sured) data and errors in the model. This allows the techniques to be used for the quantitative analysis of data (i. e. quantity estimates and associated measurement covariances). Background theory is presented along with <b>Monte-Carlo</b> <b>tests</b> which illustrate quantitative agreement within statistical limits. ...|$|R
40|$|A unique {{parameterization}} of {{the perspective}} projections in all whole-numbered dimensions is reported. The algorithm for generating a perspective transformation from parameters and for recovering parameters from a transformation is {{a modification of}} the Givens orthogonalization algorithm. The algorithm for recovering a perspective transformation from a perspective projection is a modification of Roberts' classical algorithm. Both algorithms have been implemented in Pop- 11 with call-out to the NAG Fortran libraries. Preliminary <b>monte-carlo</b> <b>tests</b> show that the transformation algorithm is highly accurate, but that the projection algorithm cannot recover magnitude and shear parameters accurately. However, {{there is reason to}} believe that the projection algorithm might improve significantly with the use of many corresponding points, or with multiple perspective views of an object. Previous parameterizations of the perspective transformations in the computer graphics and computer vision literature are discussed...|$|R
40|$|We have {{compiled}} {{and analyzed}} historical Korean meteor and meteor shower records in three Korean official history books, Samguksagi which covers the three Kingdoms period (57 B. C [...] A. D. 935), Goryeosa of Goryeo dynasty (A. D. 918 [...] 1392), and Joseonwangjosillok of Joseon dynasty (A. D. 1392 [...] 1910). We have found 3861 meteor and 31 meteor shower records. We have confirmed {{the peaks of}} Perseids and an excess due to the mixture of Orionids, north-Taurids, or Leonids through the <b>Monte-Carlo</b> <b>test.</b> The peaks persist from the period of Goryeo dynasty to that of Joseon dynasty, for almost one thousand years. Korean records show a decrease of Perseids activity and an increase of Orionids/north-Taurids/Leonids activity. We have also analyzed seasonal variation of sporadic meteors from Korean records. We confirm the seasonal variation of sporadic meteors from the records of Joseon dynasty with {{the maximum number of}} events being roughly 1. 7 times the minimum. The Korean records are compared with Chinese and Japanese records for the same periods. Major features in Chinese meteor shower records are quite consistent with those of Korean records, particularly for the last millennium. Japanese records also show Perseids feature and Orionids/north-Taurids/Leonids feature, although they are less prominent compared to those of Korean or Chinese records. Comment: 29 pages, 7 figures. To appear in Icaru...|$|E
40|$|Because of {{the highly}} {{accurate}} accelerometers, the GOCE mission {{has proven to be}} a unique source of thermosphere neutral density and cross-wind data. In the current methods, in which only the horizontal linear accelerations are used, the vertical winds cannot be obtained. In the algorithm proposed in this paper, angular accelerations derived from the individual gradiometer accelerations are used to obtain the vertical wind speeds as well. To do so, the measured angular rate and acceleration are combined to find a measurement of the torque acting on the spacecraft. This measurement is then corrected for modeled control torque applied by the magnetic torquers, aerodynamic torque, gravity gradient torque, solar radiation pressure torque, the torque caused by the misalignment of the thrust with respect to the center of gravity, and magnetic torque caused by the operation of several different subsystems of the spacecraft bus. Since the proper documentation of the magnetic properties of the payload were not available, a least squares estimate is made of one hard- and one soft-magnetic dipole pertaining to the payload, on an aerodynamically quiet day. The model for aerodynamic torque uses moment coefficients from <b>Monte-Carlo</b> <b>Test</b> Particle software ANGARA. Finally the neutral density, horizontal cross-wind, and vertical wind are obtained from an iterative process, in which the residual forces and torques are minimized. It is found that, like horizontal wind, the vertical wind responds strongly to geomagnetic storms. This response is observed over the whole latitude range, and shows seasonal variations. Astrodynamics & Space MissionsControl & Simulatio...|$|E
40|$|International audienceA {{model of}} the martian {{exosphere}} is built for average solar conditions. A Chamberlain's approach (Chamberlain, 1963) is {{used to describe the}} O, CO, CO 2, and O 2 thermal exospheric components. The average thermal oxygen density at 300 km in altitude varies by about one order of magnitude with seasons. A <b>Monte-Carlo</b> <b>test</b> particle simulation is also developed in order to estimate the non-thermal oxygen component of the exosphere. The seasonal variation of the non-thermal oxygen average density is much less than the thermal component but displays clear seasonal variations of its spatial distribution. The neutral oxygen atomic escaping flux varies from 2. 9 to 5. 3 × 1025 s- 1 in good agreement with Valeille et al. (2009). Mars's oxygen exosphere is thermal below 600 km and non-thermal above 700 km at all seasons. The typical scale height is ∼ 45 km for thermal O and ∼ 500 km for the non-thermal oxygen density. The total photoionization rate above 300 km corresponds to a CO 2 +/O+ total production ratio between 0. 004 and 0. 02. When compared to the composition of the escaping flux measured by ASPERA- 3 /Mars Express, this suggests that ions formed below 300 km should significantly contribute to the escaping ion flux and/or that {{a significant part of the}} newly O+ ions reimpacts Mars. The simulated oxygen density profile is also compared to the recent observed profile by Alice/Rosetta (Feldman et al., 2011). Although the scale height of our simulated non-thermal oxygen exosphere and the transition from thermal to non-thermal dominated exospheres are slightly higher than suggested by Feldmann et al. (2011), a good agreement is found when taking into account the uncertainties of ALICE/ROSETTA observations...|$|E
40|$|Multiple {{hypothesis}} testing {{is widely used}} to evaluate scientific studies involving statistical tests. However, {{for many of these}} tests, exact p-values are not available and thus p-values are often approximated using <b>Monte-Carlo</b> <b>tests</b> such as permutation tests or bootstrap tests. This is typically done by drawing a constant number of replicates or permutations to estimate the p-value of each hypothesis. The estimates are then used as input for procedures which correct for the multiplicity, for instance by controlling the Familywise Error Rate or the False Discovery Rate. This article introduces QuickMMCTest, a new heuristic method to assess the statistical significance of multiple hypotheses. Moreover, this article compares QuickMMCTest to three other methods in a simulation study, namely to a naive approach which draws a constant number of replicates or permutations for each hypothesis, to the recent MCFDR algorithm and to the MMCTest algorithm. Using the same number of replicates or permutations QuickMMCTest yields considerably better results in terms of misclassifications than the other three methods...|$|R
40|$|International audienceWe {{examined}} fine-scale {{heterogeneity of}} environmental conditions {{in a primary}} rain forest in French Guiana to describe variation in microhabitats that plants may experience during establishment. We characterized both the range {{as well as the}} spatial structuring of 11 environmental factors important for seedling establishment in six hexagonal sampling grids, one each in gap and understory sites at three points representing the predominant geomorphic units in this primary forest. Each grid contained 37 sampling points separated by 31 cm- 20 m. <b>Monte-Carlo</b> <b>tests</b> of semivariograms against complete spatial randomness indicated that for many variables in all six sampling grids, spatial dependence did not exceed 1 m. A principal component analysis of all sampling points revealed a lack of spatial microhabitat structure, rather than homogeneous patches associated with canopy structure or geomorphology. Our results suggest that ample fine-scale spatial heterogeneity exists to support the coexistence of plant species with differential abiotic requirements for regeneration...|$|R
40|$|One of the {{challenges}} in designing low level control loops for Micro Air Vehicles (MAVs) is that the manufacturing process for airframes is not consistent enough to ensure uniform aerodynamic properties. Therefore, {{there is a significant}} need for robust adaptive control techniques that are computationally simple. Conventional Model Reference Adaptive Con-trollers (MRAC) have proved to be very useful in a number of flight tests over the past years. However, a major drawback of this control architecture is that during the transient the control signal or the system output can exhibit large oscillations. This requires in-tensive <b>Monte-Carlo</b> <b>testing</b> for all possible variations in all possible scenarios before each flight test. This paper presents preliminary results for a novel adaptive control technique that is both computationally simple, and has uniform bounded transient response. The effectiveness of the proposed control scheme is demonstrated through simulation results produced by a medium-fidelity hardware-in-the-loop simulator as well as flight test results on a five foot wingspan unmanned air vehicle. I...|$|R
40|$|The {{identification}} of relevant spatial scales in organism-environment relationships {{is a key}} step {{in the understanding of}} an ecosystem. To study this concern, the hierarchical patch dynamic theory offers a unique concept to relate the ecological processes, patterns and scales. Based on this theory, we approach the {{identification of}} relevant spatial scale in organism-environment relationships, with the intention to explicitly account for the spatial component of ecological processes. From the comparison between the PCNM method and the geostatistical approach applied to spruce’s defoliation caused by spruce budworm in Ontario (Canada), the geostatistical approach is more efficient to identify relevant spatial scale in organism-environment relationships. Even so, to answer the question of significance of identified scales, we developed a <b>Monte-Carlo</b> <b>test</b> for nested variogram models. However classical geostatistics could be difficult to use with ecological data which have positive, skewed distribution. To consider this problem, we developed a hierarchical model associated to geostatistics and applied it to modelling spatial distribution of auks (Uria spp) in the Bay of Biscay. Tree levels of patches were characterized : a very broad scale patch (200 km), broad scale patches (50 km) and fine scale patches (10 km). The spatio-temporal analysis of links between auk distribution and the oceanographic landscape discriminates between the ”process variables” which feature a stable link in term of sign and correlation among time and ”circumstance variables” which have unstable link. At very broad scale, the surface salinity, the mixed layer depth and the chlorophyll a, could be view as ”process variables” which have been used to define the potential habitat of auks during the wintering season. At broad scale, only chlorophyll a is selected as a process variable, giving a path to model preferential habitats. These two kinds of habitats can then be connected to different levels of a hierarchical patch dynamic syste...|$|E
40|$|This {{dissertation}} aims {{to augment}} current structural health monitoring (SHM) practice with {{an approach to}} model and quantify uncertainty to enable confidence-based decision-making. The SHM application domain is vibration data-based system identification, and more specifically, transmissibility and frequency response function (FRF) estimations are considered, as these are the primary forms of transfer function estimation in the frequency domain. A finite element (FE) model is established in order to supply a benchmark of transmissibility evaluations, and by tuning the FE model, structural damages can be simulated. Two SHM features are proposed to detect and localize defects by analyzing the features calculated at certain interest point arrays. Considering a realistic test condition, all of the model parameters and data are subject to uncertainty from various sources leading to ambiguous system identification results that cause false alarms (Type-I error) when evaluating hypothesis testing for damage. Based upon stationary Gaussian random process, this dissertation statistically establishes uncertainty quantification (UQ) models for different estimators, and uncertainties of transmissibility and FRF are therefore quantified. A perturbation approach is implemented ending up with standard deviation and bias coefficient of transmissibility magnitude estimations. Probability density functions (PDFs) of transmissibility and FRF estimation are derived, for both magnitude and phase, via different methods, namely Chi-square and Gaussian bivariate approach. The proposed statistical models are validated by <b>Monte-Carlo</b> <b>test</b> on both FE simulation model and real lab-scale structure. To obtain a more stringent validation condition, extraneous artificial noise is added onto the raw measurements. Compared to the pre-set confidence interval, validation results are illustrated via outlier percentage, which is the observed outlier amount, at each frequency line, normalized {{by the number of}} total test cases. Comparison of the UQ results among different statistical models, estimators, and noise contamination levels is presented, for the purpose of guiding users towards using optimal estimators under certain circumstance. Hypothesis tests are implemented, with statistical models available, and the detection performance is compared for different detectors, damage levels, and noise contaminations. Receiver operating characteristic curves are used for quantitative visualization of the abovementioned performance qualities. Using area under curve (AUC) metric, it is concluded how detection rates trend as damage level and signal-to-noise condition changes, suggesting optimal frequency bands for implementing detection. For example, even for heavily- contaminated cases, there is still acceptable detectability at resonances. As a decision-making problem, SHM probabilistically involves making correct decisions with acceptable (application-dependent) type-I errors. In the end of this dissertation, probability of detection for different cases and test conditions are optimized and compared, as given certain false alarm tolerance thresholds. By having optimal detections, the damage identification problems have a clearer outline with respect to different hypothesis design...|$|E
40|$|Socioeconomic inequalities in children’s {{skills and}} {{capabilities}} begin {{early in life}} and can have detrimental effects on future success in school. The present study examines the relationships between school readiness and sociodemographic inequalities using teacher reports of the Short Early Development Instrument in a disadvantaged urban area of Ireland. It specifically examines socioeconomic (SES) differences in skills within a low SES community in order to investigate the role of relative disadvantage on children’s development. Differences across multiple domains of school readiness are examined using <b>Monte-Carlo</b> permutation <b>tests.</b> The results show that child, family and environmental factors {{have an impact on}} children’s school readiness, with attendance in centre-based childcare having the most consistent relationship with readiness for school. In addition, the findings suggest that social class inequalities in children’s skills still exist within a disadvantaged community. These results are discussed in relation to future intervention programmes. School readiness, Socioeconomic inequalities, <b>Monte-Carlo</b> permutation <b>tests...</b>|$|R
40|$|We {{examined}} fine-scale {{heterogeneity of}} environmental conditions {{in a primary}} rain forest in French Guiana to describe variation in microhabitats that plants may experience during establishment. We characterized both the range {{as well as the}} spatial structuring of 11 environmental factors important for seedling establishment in six hexagonal sampling grids, one each in gap and understory sites at three points representing the predominant geomorphic units in this primary forest. Each grid contained 37 sampling points separated by 31 cm- 20 m. <b>Monte-Carlo</b> <b>tests</b> of semivariograms against complete spatial randomness indicated that for many variables in all six sampling grids, spatial dependence did not exceed 1 m. A principal component analysis of all sampling points revealed a lack of spatial microhabitat structure, rather than homogeneous patches associated with canopy structure or geomorphology. Our results suggest that ample fine-scale spatial heterogeneity exists to support the coexistence of plant species with differential abiotic requirements for regeneration...|$|R
40|$|Abstract Background Mathematical {{models of}} the immune {{response}} to the Human Immunodeficiency Virus demonstrate the potential for dynamic schedules of Highly Active Anti-Retroviral Therapy to enhance Cytotoxic Lymphocyte-mediated control of HIV infection. Methods In previous work we have developed a model predictive control (MPC) based method for determining optimal treatment interruption schedules for this purpose. In this paper, we introduce a nonlinear observer for the HIV-immune response system and an integrated output-feedback MPC approach for implementing the treatment interruption scheduling algorithm using the easily available viral load measurements. We use <b>Monte-Carlo</b> approaches to <b>test</b> robustness of the algorithm. Results The nonlinear observer shows robust state tracking while preserving state positivity both for continuous and discrete measurements. The integrated output-feedback MPC algorithm stabilizes the desired steady-state. <b>Monte-Carlo</b> <b>testing</b> shows significant robustness to modeling error, with 90 % success rates in stabilizing the desired steady-state with 15 % variance from nominal on all model parameters. Conclusions The possibility of enhancing immune responsiveness to HIV through dynamic scheduling of treatment is exciting. Output-feedback Model Predictive Control is uniquely well-suited to solutions {{of these types of}} problems. The unique constraints of state positivity and very slow sampling are addressable by using a special-purpose nonlinear state estimator, as described in this paper. This shows the possibility of using output-feedback MPC-based algorithms for this purpose. </p...|$|R
40|$|A {{problem in}} the archaeometric {{classification}} of Catalan Renaissance pottery is the fact, that the clay supply of the pottery workshops was centrally organized by guilds, and therefore usually all potters of a single production centre produced chemically similar ceramics. However, analysing the glazes of the ware usually {{a large number of}} inclusions in the glaze is found, which reveal technological differences between single workshops. These inclusions have been used by the potters in order to opacify the transparent glaze and to achieve a white background for further decoration. In order to distinguish different technological preparation procedures of the single workshops, at a Scanning Electron Microscope the chemical composition of those inclusions as well as their size in the two-dimensional cut is recorded. Based on the latter, a frequency distribution of the apparent diameters is estimated for each sample and type of inclusion. Following an approach by S. D. Wicksell (1925), it is principally possible to transform the distributions of the apparent 2 D-diameters back to those of the true three-dimensional bodies. The applicability of this approach and its practical problems are examined using different ways of kernel density estimation and <b>Monte-Carlo</b> <b>tests</b> of the methodology. Finally, it i...|$|R
40|$|This paper shows {{a general}} {{non-parametric}} unfolding technique for maximizing the correct classification of binary choice or two-category data. The motivation {{for and the}} primary focus of the unfolding technique is parliamentary roll call voting data. However, the procedures that implement the unfolding also {{can be applied to the}} problem of unfolding rank order data as well as analyzing a data set that would normally be the subject of a probit, logit, or linear probability analysis. To unfold binary choice data two subproblems must be solved. First, given a set of chooser or legislator points a cutting plane must be found such that it divides the legislators/choosers into two sets that reproduce the actual choices as closely as possible. Second, given a set of cutting planes for the binary choices a point for each chooser or legislator must be found which reproduces the actual choices as closely as possible. Solutions for these two problems are shown in this paper. <b>Monte-Carlo</b> <b>tests</b> of the procedure shows it to be highly accurate in the presence of error and missing data. The purpose of this paper is to show a general non-parametric unfolding techniqu...|$|R
40|$|OBJECTIVES: Combining the {{analysis}} of family-based samples with unrelated individuals can enhance the power of genetic association studies. Various combined analysis techniques have been recently developed; as yet, {{there have been no}} comparisons of their power, or robustness to confounding factors. We investigated empirically the power of up to six combined methods using simulated samples of trios and unrelated cases/controls (TDTCC), trios and unrelated controls (TDTC), and affected sibpairs with parents and unrelated cases/controls (ASPFCC). METHODS: We simulated multiplicative, dominant and recessive models with varying risk parameters in single samples. Additionally, we studied false-positive rates and investigated, if possible, the coverage of the true genetic effect (TDTCC). RESULTS/CONCLUSIONS: Under the TDTCC design, we identified four approaches with equivalent power and false-positive rates. Combined statistics were more powerful than single-sample statistics or a pooled chi(2) -statistic when risk parameters were similar in single samples. Adding parental information to the CC part of the joint likelihood increased the power of generalised logistic regression under the TDTC but not the TDTCC scenario. Formal testing of differences between risk parameters in subsamples was the most sensitive approach to avoid confounding in combined analysis. Non-parametric analysis based on <b>Monte-Carlo</b> <b>testing</b> showed the highest power for ASPFCC samples...|$|R
40|$|The {{standard}} multi-detector F-statistic {{for continuous}} gravitational waves {{is susceptible to}} false alarms from instrumental artifacts, for example monochromatic sinusoidal disturbances (lines). This vulnerability to line artifacts arises because the F-statistic compares the signal hypothesis to a Gaussian-noise hypothesis, and hence is triggered by anything that resembles the signal hypothesis more than Gaussian noise. Various ad-hoc veto methods {{to deal with such}} line artifacts have been proposed and used in the past. Here we develop a Bayesian framework that includes an explicit alternative hypothesis to model disturbed data. We introduce a simple line model that defines lines as signal candidates appearing only in one detector. This allows us to explicitly compute the odds between the signal hypothesis and an extended noise hypothesis, resulting in a new detection statistic that is more robust to instrumental artifacts. We present and discuss results from <b>Monte-Carlo</b> <b>tests</b> on both simulated data and on detector data from the fifth LIGO science run. We find that the line-robust detection statistic retains the detection power of the standard F-statistic in Gaussian noise, while it can be substantially more sensitive in the presence of line artifacts. This new statistic also equals or surpasses the performance of the popular F-statistic consistency veto...|$|R
40|$|It {{has been}} broadly {{acknowledged}} that vortex detection algorithms, usually based on linear-algebraic {{properties of the}} velocity gradient tensor, can be plagued with severe shortcomings and may become, in practical terms, dependent on the choice of subjective threshold parameters in their implementations. In two-dimensions, a large class of standard vortex identification prescriptions {{turn out to be}} equivalent to the "swirling strength criterion" (λ_ci-criterion), which is critically revisited in this work. We classify the instances where the accuracy of the λ_ci-criterion is affected by nonlinear superposition effects and propose an alternative vortex detection scheme based on the local curvature properties of the vorticity graph (x,y,ω) [...] the "vorticity curvature criterion" (λ_ω-criterion) [...] which improves over the results obtained with the λ_ci-criterion in controlled <b>Monte-Carlo</b> <b>tests.</b> We show that the λ_ω-criterion is able to cope with strong shear effects, if a subtraction of the mean velocity field background is performed, {{in the spirit of the}} Reynolds decomposition procedure. A realistic comparative study for vortex identification is then carried out for a direct numerical simulation (DNS) of a turbulent channel flow, including a three-dimensional extension of the λ_ω-criterion. In contrast to the λ_ci-criterion, the λ_ω-criterion indicates in a consistent way the existence of small scale isotropic turbulent fluctuations in the logarithmic layer, in consonance with long-standing assumptions commonly taken in turbulent boundary layer phenomenology. Comment: 20 pages, 23 figure...|$|R
40|$|In {{testing a}} {{structural}} change, the approximated confidence intervals are conventionally used for CUSUM and CUSUMSQ tests. This paper numerically derives the asymptotically exact confidence intervals of CUSUM and CUSUMSQ tests. It {{can be easily}} extended to nonnormal and/or nonlinear models. KEY WORDS: CUSUM <b>test,</b> CUSUMSQ <b>test,</b> <b>Monte-Carlo</b> simulation, Asymptotically exact confidence interval...|$|R
40|$|We {{study the}} {{distribution}} of Durbin-Wu-Hausman (DWH) and Revankar-Hartley (RH) tests for exogeneity from a finite-sample viewpoint, under the null and alternative hypotheses. We consider linear structural models with possibly non-Gaussian errors, where structural parameters may not be identified and where reduced forms can be incompletely specified (or nonparametric). On level control, we characterize the null distributions of all the test statistics. Through conditioning and invariance arguments, we show that these distributions do not involve nuisance parameters. In particular, this applies to several test statistics for which no finite-sample distributional theory is yet available, such as the standard statistic proposed by Hausman (1978). The distributions of the test statistics may be non-standard [...] so corrections to usual asymptotic critical values are needed [...] but the characterizations are sufficiently explicit to yield finite-sample (<b>Monte-Carlo)</b> <b>tests</b> of the exogeneity hypothesis. The procedures so obtained are robust to weak identification, missing instruments or misspecified reduced forms, and can easily be adapted to allow for parametric non-Gaussian error distributions. We give a general invariance result (block triangular invariance) for exogeneity test statistics. This property yields a convenient exogeneity canonical form and a parsimonious reduction of the parameters on which power depends. In the extreme case where no structural parameter is identified, the distributions under the alternative hypothesis and the null hypothesis are identical, so the power function is flat, for all the exogeneity statistics. However, as soon as identification does not fail completely, this phenomenon typically disappears...|$|R
