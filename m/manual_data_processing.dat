34|10000|Public
50|$|This method can {{automate}} {{data processing}} by using pre-defined templates and configurations. A template in this case, {{would be a}} map of the document, detailing where the data fields are located within the form or document. As compared to the manual data entry process, automatic form input systems are more preferable, since they help reduce the problems faced during <b>manual</b> <b>data</b> <b>processing.</b>|$|E
5000|$|Thompson, who ran {{the label}} Crowded Elevator, saw {{the need for a}} {{solution}} in managing inventory administration and made the decision to create one. “Running my clothing label was exciting and I enjoyed it a lot. But we grew very fast and the amount of administration quickly overwhelmed us. I couldn’t find a solution to pull it all together. Spreadsheets and <b>manual</b> <b>data</b> <b>processing</b> turned into an absolute nightmare," [...] he said.|$|E
30|$|The {{difference}} in algorithm recognition quality K 0.8 obtained for records for August and July 2009 is {{likely due to}} the fact that it was easier to carry out <b>manual</b> <b>data</b> <b>processing</b> by eye having at the disposal the results of the algorithm recognition (August data), rather than to analyze raw magnetograms “from scratch” (July data). Thus for July data the quality of manual recognition of spikes turned to be worse. This shows that the algorithm significantly helped the recognition by eye. It also provides some estimate of the amount of errors made when relying on manual spike detection.|$|E
5000|$|Instructors <b>Manual,</b> Introduction to <b>data</b> <b>processing,</b> {{second edition}} (BASIC version also) ...|$|R
5000|$|All {{these data}} collection, analysis, and {{visualization}} tasks are highly automated in modern software. In the past, however, these tasks required <b>manual</b> <b>data</b> recording and <b>processing</b> [...]|$|R
40|$|The {{processed}} data with computer {{can be more}} convenient and practical compared with <b>manual</b> system. <b>Data</b> <b>processing</b> is done manually takes time and labor so much, that when dealing with complex <b>data</b> <b>processing</b> and large less efficient. Thus {{the use of a}} computer can be optimized. Therefore the "Web Based Information System Report Cards at SMP N 4 Temanggung" and as the title made in this thesis. Design of the information system of <b>data</b> <b>processing</b> was designed in order to process data effectively and efficiently in the process of inputting student data, teacher data, the value of data subjects as well as report cards, in addition to optimizing the use of existing computer in SMP N 4 Temanggung...|$|R
40|$|Often data {{processing}} is not implemented by a work ow system or an integration application but is performed manually by humans {{along the lines}} of a more or less specified procedure. Collecting provenance information during <b>manual</b> <b>data</b> <b>processing</b> can not be automated. Further, manual collection of provenance information is error prone and time consuming. Therefore, we propose to infer provenance information based on the read and write access of users. The derived provenance information is complete, but has a low precision. Therefore, we propose further to introducing organizational guidelines in order to improve the precision of the inferred provenance information...|$|E
40|$|Hospital {{personnel}} are exploring {{ways to increase}} both production and clinical efficiency {{in the delivery of}} health care. Because laboratory information systems (LISs) will play such a critical role in this quest, these systems must perform optimally. The author discusses whether the persistence of older LISs and <b>manual</b> <b>data</b> <b>processing</b> systems within hospital clinical laboratories is related to the “competency trap. ” A competency trap occurs when continuing favorable performance with an inferior procedure leads an organization to accumulate more experience with it, thus avoiding experience with a superior procedure or keeping such experience at a low level...|$|E
40|$|A Microsoft Excel utility, HX-Express, that {{significantly}} accelerates {{the analysis of}} hydrogen exchange mass spectrometry data is described. HX-Express generates deuterium uptake and peak width plots from peaks in mass spectral data. Data analysis is intentionally semi-automated, requiring that the user find the peaks to be analyzed. The peaks are entered {{in the form of}} x, y lists of m/z versus intensity or can be directly imported from Waters MassLynx software. Analysis of data with HX-Express provides the same results as <b>manual</b> <b>data</b> <b>processing</b> but is substantially faster. In addition to speed, HX-Express provides and preserves visual and numeric displays of the analysis process for quality control...|$|E
40|$|The CRI {{began using}} {{microcomputers}} {{for the analysis}} of their experimental data in 1986. Four case studies of typical sets of field experimental data from the CRI are used to illustrate the potential benefits and possible problems in moving from a <b>manual</b> system of <b>data</b> <b>processing</b> to one which uses a powerful statistical package...|$|R
40|$|These authors {{discuss the}} system of loaning and book return {{services}} at the library aredepicted with Files Of Document, Data Flow Diagram, Entity Relationship Diagramsand Normalization, which then created a computer application program to processloaning and book return data, which formed as a data member, data books, databooks borrowed and returned data by using Visual Basic 6. 0 programming languagethat {{is better than the}} <b>manual</b> way. <b>Data</b> <b>processing</b> using a computer system at the rental data and return data areintended to determine the number of members, number of books borrowed and thenumber of books returned...|$|R
50|$|Blue Prism is {{the trading}} {{name of the}} Blue Prism Group, a UK {{multinational}} software corporation that pioneered and makes enterprise Robotic Process Automation software to eliminate low-return, high-risk, <b>manual</b> <b>data</b> entry and <b>processing</b> work. Blue Prism is headquartered {{in the north of}} England, UK with regional offices in the US and Australia. The company is listed on the London Stock Exchange AIM market.|$|R
40|$|This paper {{discusses}} {{the need for}} automatic data extraction, processing, and transfer [computer-aided design and computer-aided manufacturing (CAD/CAM) interfacing] between the design and constructionphases as {{a key element in}} computer-integrated construction (CIC). The CAD/CAM interface eliminates the need for manual interfacing between these two computer-aided phases. The CAD/CAM interfaceimproves the process of the constructed facility realization by eliminating the <b>manual</b> <b>data</b> <b>processing</b> and thereby reducing many sources of errors. It also makes the process more cost-effective because it reduceslabor inputs, especially those presently invested in robot programming. A model for automatic data extraction, processing, and transfer is proposed for the tile-setting paradigm. The model generates constructionmanagement data as well as data needed for automatic on-site construction (robotics). The model is implemented in the AutoCAD and AutoLISP environments. The model and the implementation system weretested in the laboratories with a scaled robot adapted to perform interior finishing tasks...|$|E
40|$|The {{potential}} and current limitations of comprehensive two- dimensional gas chromatography coupled to time-of-flight mass spectrometry (GCxGC-TOF-MS) {{for the analysis}} of very complex samples were studied with the separation of cigarette smoke as an example. Because of the large number of peaks in such a GCxGC chromatogram {{it was not possible to}} perform <b>manual</b> <b>data</b> <b>processing.</b> Instead, the GC-TOF-MS software was used to perform peak finding, deconvolution and library search in an automated fashion; this resulted in a peak table containing some 30 000 peaks. Mass spectral match factors were used to evaluate the library search results. The additional use of retention indices and information from second-dimension retention times can substantially improve the identification. The combined separation power of the GCxGC-TOF-MS system and the deconvolution algorithm provide a system with a most impressive separation power. (C) 2002 Elsevier Science B. V. All rights reserved...|$|E
40|$|Being able to {{efficiently}} compare as-built against as-planned 3 D {{states is}} critical for performing efficient building and infrastructure construction, maintenance, and management. Three-dimensional (3 D) laser scanners {{have the potential to}} be successfully applied to these tasks. Recent commercial products allow the comparison of 3 D scanned and 3 D CAD data based on CAD forms. Their current use is however limited due to the large amounts of <b>manual</b> <b>data</b> <b>processing</b> required for extracting useful information. By using 3 D Computer Aided Design (CAD) models as representations of 3 D specifications and Global Positioning System (GPS) technologies, the authors present an approach for automating the comparison of 3 D sensed data and 3 D CAD data. This new approach does not perform this data comparison based on CAD forms but on point-clouds. This paper discusses the fundamental differences between the two approaches, describes the theoretical implementation of the proposed approach, and presents laboratory experimental results confirming the potential impact of the proposed method on industry’s practices...|$|E
40|$|Individual-level {{immunization}} data captured electronically {{can facilitate}} evidence-based decision-making and planning. Populating individual-level records through <b>manual</b> <b>data</b> entry is time-consuming. An {{alternative is to}} use scannable forms, completed {{at the point of}} vaccination and subsequently scanned and exported to a database or registry. To explore the suitability of this approach for collecting immunization data, we conducted a feasibility study in two settings in Ontario, Canada. Prior to the 2011 - 2012 influenza vaccination campaign, we developed a scannable form template and a corresponding database that captured required demographic and clinical data elements. We examined efficiency, data quality, and usability through time observations, record audits, staff interviews, and client surveys. The mean time required to scan and verify forms (62. 3 s) was significantly shorter than <b>manual</b> <b>data</b> entry (69. 5 s) in one organization, whereas there was no difference (36. 6 s vs. 35. 4 s) in a second organization. Record audits revealed no differences in data quality between records populated by scanning versus <b>manual</b> <b>data</b> entry. <b>Data</b> <b>processing</b> personnel and immunized clients found the processes involved to be straightforward, while nurses and managers had mixed perceptions regarding the ease and merit of using scannable forms. Printing quality and other factors rendered some forms unscannable, necessitating manual entry. Scannable forms can facilitate efficient data entry, but certain features of the forms, as well as the workflow and infrastructure into which they are incorporated, should be evaluated and adapted if scannable forms are to be a meaningful alternative to <b>manual</b> <b>data</b> entry...|$|R
40|$|AbstractEstablishing a {{water balance}} for a water {{distribution}} network {{with the majority}} of available tools requires <b>manual</b> <b>data</b> collection and <b>processing.</b> This is why water balances in most cases are determined for a whole network and only at long intervals. Unmetered components are often neglected or based on rough estimates. This paper presents an approach for automated establishing zonal water balances for variable balancing periods. The application of the approach is demonstrated for the case of Pforzheim, Germany...|$|R
40|$|AbstractEstimating {{transpiration}} from {{woody plants}} using thermal dissipation sap flux sensors requires careful <b>data</b> <b>processing.</b> Currently, researchers accomplish this using spreadsheets, or by personally writing scripts for statistical software programs (e. g., R, SAS). We developed the Baseliner software to help establish a standardized protocol for <b>processing</b> sap flux <b>data.</b> Baseliner enables users to QA/QC data and process data {{using a combination}} of automated steps, visualization, and <b>manual</b> editing. <b>Data</b> <b>processing</b> requires establishing a zero-flow reference value, or “baseline”, which varies among sensors and with time. Since no set of algorithms currently exists to reliably QA/QC and estimate the zero-flow baseline, Baseliner provides a graphical user interface to allow visual inspection and manipulation of data. Data are first automatically processed using a set of user defined parameters. The user can then view the <b>data</b> for additional, <b>manual</b> QA/QC and baseline identification using mouse and keyboard commands. The open-source software allows for user customization of <b>data</b> <b>processing</b> algorithms as improved methods are developed...|$|R
40|$|Abstract Background Particle {{tracking}} passive microrheology relates recorded {{trajectories of}} microbeads, embedded in soft samples, {{to the local}} mechanical properties of the sample. The method requires intensive numerical data processing and tools allowing control of the calculation errors. Results We report {{the development of a}} software package collecting functions and scripts written in Python for automated and <b>manual</b> <b>data</b> <b>processing,</b> to extract viscoelastic information about the sample using recorded particle trajectories. The resulting program package analyzes the fundamental diffusion characteristics of particle trajectories and calculates the frequency dependent complex shear modulus using methods published in the literature. In order to increase conversion accuracy, segmentwise, double step, range-adaptive fitting and dynamic sampling algorithms are introduced to interpolate the data in a splinelike manner. Conclusions The presented set of algorithms allows for flexible data processing for particle tracking microrheology. The package presents improved algorithms for mean square displacement estimation, controlling effects of frame loss during recording, and a novel numerical conversion method using segmentwise interpolation, decreasing the conversion error from about 100 % to the order of 1 %. </p...|$|E
40|$|New {{information}} architectures enable {{new approaches}} to publishing and accessing valuable data and programs. So-called service-oriented architectures define standard interfaces and protocols that allow developers to encapsulate information tools as services that clients can access without knowledge of, or control over, their internal workings. Thus, tools formerly only accessible to the specialist can {{be made available to}} all, previously <b>manual</b> <b>data</b> <b>processing</b> and analysis tasks can be automated by having services access services. Such service-oriented approaches to science are already being applied successfully, in some cases at substantial scales, but significant further effort is required before these approaches are applied routinely across many disciplines. Grid technologies can accelerate the development and adoption of service-oriented science, by enabling a separation of concerns between discipline-specific content and domain-independent software and hardware infrastructure. Paul Erdös claimed that a mathematician is a machine for turning coffee into theorems. The scientist is arguably a machine for turning data into insight. However, advances in information technology are changing the way in which this role is fulfilled, by automating time-consumin...|$|E
40|$|Background Particle {{tracking}} passive microrheology relates recorded {{trajectories of}} microbeads, embedded in soft samples, {{to the local}} mechanical properties of the sample. The method requires intensive numerical data processing and tools allowing control of the calculation errors. Results We report {{the development of a}} software package collecting functions and scripts written in Python for automated and <b>manual</b> <b>data</b> <b>processing,</b> to extract viscoelastic information about the sample using recorded particle trajectories. The resulting program package analyzes the fundamental diffusion characteristics of particle trajectories and calculates the frequency dependent complex shear modulus using methods published in the literature. In order to increase conversion accuracy, segmentwise, double step, range-adaptive fitting and dynamic sampling algorithms are introduced to interpolate the data in a splinelike manner. Conclusions The presented set of algorithms allows for flexible data processing for particle tracking microrheology. The package presents improved algorithms for mean square displacement estimation, controlling effects of frame loss during recording, and a novel numerical conversion method using segmentwise interpolation, decreasing the conversion error from about 100 % to the order of 1 %...|$|E
40|$|Abstract:- Data mining is {{essentially}} employed in getting ready information sets for <b>data</b> <b>processing</b> analysis. However it's most time overwhelming method. It needs {{great deal of}} <b>manual</b> effort. <b>Data</b> <b>processing</b> {{is essentially}} used domain for obtaining the patterns from historical info or keep info. Great deal of effort is needed to arrange datasets that will be input for <b>data</b> <b>processing</b> algorithmic program. As we tend to have already got some aggregation operate easy lay, MIN, SUM, COUNT, AVG that aren't economical for creating datasets in <b>data</b> <b>processing</b> analysis. This combination operate have disadvantage as they come single price single price per mass cluster in this table. In <b>data</b> <b>processing</b> analysis when we tend to needs information in horizontal layout that prong we need arduous effort. therefore we tend to are developing straightforward however powerful tool to urge SQL code to come combined columns in horizontal layout type, that returns cluster of varieties rather than one number per row. This new cluster of tool or operate is claimed to be horizontal aggregation. From third queries we'll get output information that is appropriate for varied <b>data</b> <b>processing</b> operations. It means that this paper provides horizontal aggregation victimization some constructs that embody SQL queries. Here we tend to are victimization 3 functions that is Grouping column, Horizontal column, combination column. Users need to provide this as input. So user gets the output that is appropriate for <b>data</b> <b>processing</b> analysis...|$|R
40|$|The {{simultaneous}} {{measurement of}} detrusor pressure and flow rate during voiding is at present {{the only way}} to measure or grade infravesical obstruction objectively. Numerous methods have been introduced to analyze the resulting data. These methods differ in aim (measurement of urethral resistance and/or diagnosis of obstruction), method (<b>manual</b> versus computerized <b>data</b> <b>processing),</b> theory or model used, and resolution (continuously variable parameters or a limited number of classes, the so-called monogram). In this paper, some aspects of these fundamental differences are discussed and illustrated. Subsequently, the properties and clinical performance of two computer-based methods for deriving continuous urethral resistance parameters are treated...|$|R
40|$|Technological {{developments}} {{in this era}} of globalization it is so fast. Almost all of the activities carried out by a computerized human. With the system terkomputerisasi to all lini dapat make it all work faster. Even using a computerized system can meminimalkan possibility of error in using a <b>manual</b> system. <b>Data</b> <b>processing</b> in the field of education is to help accelerate time to menyelesaikan task of TU, the teacher or the principal. SDN 2 Biting Purwantoro Wonogiri, is one of the schools that still use manual systems. With the new system created to provide benefits in improving the performance of the agency or company sehiingga sistem error of manual labor can be minimized. The scoring system is also able management designed toevakuate the level of intelligence to the student. This system includes the process of recording data Teachers, Student, Subjects, Home room teacher and the school-related...|$|R
40|$|Virtual {{methods to}} assess the fitting of a {{fracture}} fixation plate were proposed recently, however with limitations such as simplified fit criteria or <b>manual</b> <b>data</b> <b>processing.</b> This study aims to automate a fit analysis procedure using clinical-based criteria, and then to analyse the results further for borderline fit cases. Three dimensional (3 D) models of 45 bones and of a precontoured distal tibial plate were utilized {{to assess the}} fitting of the plate automatically. A Matlab program was developed to automatically measure the shortest distance between the bone and the plate at three regions of interest and a plate-bone angle. The measured values including the fit assessment results were recorded in a spreadsheet {{as part of the}} batch-process routine. An automated fit analysis procedure will enable the processing of larger bone datasets in a significantly shorter time, which will provide more representative data of the target population for plate shape design and validation. As a result, better fitting plates can be manufactured and made available to surgeons, thereby reducing the risk and cost associated with complications or corrective procedures. This in turn, is expected to translate into improving patients' quality of life...|$|E
40|$|This paper {{draws on}} the need to {{understand}} how mobile technology is implemented and used at the organisational level. IT is a general-purpose technology and therefore its use involves a high degree of uncertainty and ambiguity. Moreover, IT vendors and system developers tend to be very unambiguous in their rhetoric about mobile technology opportunities. Therefore, managers have trouble to identify the real scope, the functionality and the impact of new mobile applications. However, these three types of uncertainties need to be handled in change management projects where new information technology is involved. Gradual uncertainty reduction at these three different levels, i. e. what technology can do; will technology work; and will users adopt it, is studied in this paper. This is achieved through an analysis of the implementation process of an information system where mobile terminals are used to give service technicians access to the ERP system at BT Europe, a leading supplier of forklift trucks. The analysis shows how the three levels of uncertainty interact, and how the computerised parts of the information system are complemented by mindful intertwining with the non-computerised communication and <b>manual</b> <b>data</b> <b>processing,</b> in order for the information system to work...|$|E
40|$|The Atacama Large Millimeter Array (ALMA) is {{a project}} to build a radio {{interferometric}} telescope containing {{a large number of}} antennas (nominally 64) at a high site in Chile operating in the mm and sub-mm spectral region. With the addition of a Japan in the last year, ALMA is now a global partnership with participation by institutes in Asia, Europe, and North America. The scope of the ALMA Software includes all aspects: observing script creation through GUIs, dynamic scheduling depending on weather and instrumental parameters, instrument control (including the correlator device, capable of producing data at more than 1 GB/s), data handling and formatting, data archiving and retrieval, and automatic and <b>manual</b> <b>data</b> <b>processing</b> systems. The scope has recently been increased to support telescope operations (e. g., referee support). This ambitious scope is being implemented by a very distributed team, with approximately 60 members at institutes in 10 countries. This paper will describe some technical highlights of the software system, some technical lessons learned after the initial deployments to support initial antenna prototype tests, and will describe some of the management approaches used to keep the software effort coherent across the entire project...|$|E
40|$|Includes bibliographical {{references}} (pages 106 - 109) As <b>data</b> <b>processing</b> systems {{evolved from}} <b>manual</b> to Electronic <b>Data</b> <b>Processing</b> (EDP) methods, <b>data</b> <b>processing</b> documentation was changed and improved accordingly. The {{purpose of this}} thesis is to study the evolution of <b>data</b> <b>processing</b> documentation from <b>manual</b> to computer-aided techniques, and to outline an effective documentation framework. So long as the process of system development in a <b>data</b> <b>processing</b> department was <b>manual,</b> the method for documentation remained manual as well. The documentation tools included narrative text, flowcharts and tables. Since these tools are not fully capable of describing all the aspects of a large system in a consistent and comprehensive manner, they were supplemented by formal methods such as HIPO and Information Mapping. These can represent design information in a hierarchical or modular format which permits both an instantaneous overview and detail focusing. Subsequently more formal methods for stating requirements were implemented such as the Problem Statement Language/Problem Statement Analyzer (PSL/PSA) and the Automated Documentation System (ADS). (See more in text. ...|$|R
40|$|<b>Data</b> <b>processing</b> in {{environmental}} science {{is essential for}} doing science. The heterogeneity of <b>data</b> sources, <b>data</b> <b>processing</b> operations and infrastructures results {{in a lot of}} <b>manual</b> <b>data</b> and process integration work done by each scientist individually. This is very inefficient and time consuming. The aim is to provide a view based approach on accessing and <b>processing</b> <b>data</b> supporting a more generic infrastructure to integrate processing steps from different organizations, systems and libraries. We propose an approach modeled in Colored Place/Transition Nets which has been implemented in a Web Service infrastructure...|$|R
40|$|The {{paper is}} {{highlighting}} {{the advantages of}} integrating technologies and digital standards in public politics for protecting consumer rights. Today’s globalization of food production chains, their extensive complexity and limitations arising from <b>manual</b> insertion and <b>data</b> <b>processing</b> of products information {{make it impossible for}} consumers the task of being informed in real-time. This obstacle can be overcome, and the current level of technology and low implementation costs allow full automation of this process. We propose a sensors network architecture based on Internet of Things (IoT) components, which is using autonomous embedded modules and radio identification tags (RFID) that will automatically collect data, covering the entire life cycle of the food product and all the factors that influence its chemical composition. This architecture offers consumers complete data about their food products and how their components were obtained...|$|R
40|$|Abstract. Advances in {{wireless}} {{and mobile}} technology flood us with amounts of moving object data that preclude all means of <b>manual</b> <b>data</b> <b>processing.</b> The {{volume of data}} gathered from position sensors of mobile phones, PDAs, or vehicles, defies human ability to analyze the stream of input data. On the other hand, vast amounts of gathered data hide interesting and valuable knowledge patterns describing the behavior of moving objects. Thus, new algorithms for mining moving object data are required to unearth this knowledge. An important function of the mobile objects management system is the prediction of the unknown location of an object. In this paper we introduce a data mining approach {{to the problem of}} predicting the location of a moving object. We mine the database of moving object locations to discover frequent trajectories and movement rules. Then, we match the trajectory of a moving object with the database of movement rules to build a probabilistic model of object location. Experimental evaluation of the proposal reveals prediction accuracy close to 80 %. Our original contribution includes the elaboration on the location prediction model, the design of an efficient mining algorithm, introduction of movement rule matching strategies, and a thorough experimental evaluation of the proposed model. ...|$|E
40|$|One of {{the medical}} support service at a {{hospital}} is radiology services, provided by Radiology Installation. Information related problems faced by the Radiology Installation of dr. Ario Wirawan Lung Hospital in Salatiga are the incomplete data input, <b>manual</b> <b>data</b> <b>processing,</b> and troubles in reporting process. Information system at the Radiology Installation is generally accomplished through development of Radiology Information System (RIS). This system helps in scheduling, patients tracing, maintaining, film finding, coding, reporting, and bill generating. The objective {{of this research was}} to produce an information system model ata Radiology Installation to support the service evaluation of Dr. Ario Wirawan Lung Hospital in Salatiga. The result of this research showed that the development system model of Radiology Installation could overcome the problems on the old system. The considered average on the old system was equal to 1. 95 and the considered average on the new system was equel to 3. 40. It means that there was some improvement of the respondents perception on the new system. Based on statistical analysis through Sign Test, there were significant differences on the information quality between {{the old and the new}} system (p value = 0. 0001 or p < 0. 05). Sumber Utama : www. mikm. undip. ac. id...|$|E
40|$|Abstract Background Widespread in the tropics, the {{mosquito}} Aedes aegypti {{is an important}} vector of many viruses, posing a significant threat to human health. Vector monitoring often requires fecundity estimation by counting eggs laid by female mosquitoes. Traditionally, manual data analyses have been used but this {{requires a lot of}} effort and is the methods are prone to errors. An easy tool to assess the number of eggs laid would facilitate experimentation and vector control operations. Results This study introduces a built-in software called ICount allowing automatic egg counting of {{the mosquito}} vector, Aedes aegypti. ICount egg estimation compared to manual counting is statistically equivalent, making the software effective for automatic and semi-automatic data analysis. This technique also allows rapid analysis compared to manual methods. Finally, the software has been used to assess p-cresol oviposition choices under laboratory conditions in order to test the system with different egg densities. Conclusions ICount is a powerful tool for fast and precise egg count analysis, freeing experimenters from <b>manual</b> <b>data</b> <b>processing.</b> Software access is free and its user-friendly interface allows easy use by non-experts. Its efficiency has been tested in our laboratory with oviposition dual choices of Aedes aegypti females. The next step will be the development of a mobile application, based on the ICount platform, for vector monitoring surveys in the field...|$|E
40|$|The present work gathers {{together}} numerous papers {{describing the}} use of remote sensing technology for mapping, monitoring, and management of earth resources and man's environment. Studies using various types of sensing equipment are described, including multispectral scanners, radar imagery, spectrometers, lidar, and aerial photography, and both <b>manual</b> and computer-aided <b>data</b> <b>processing</b> techniques are described. Some of the topics covered include: estimation of population density in Tokyo districts from ERTS- 1 data, a clustering algorithm for unsupervised crop classification, passive microwave sensing of moist soils, interactive computer processing for land use planning, {{the use of}} remote sensing to delineate floodplains, moisture detection from Skylab, scanning thermal plumes, electrically scanning microwave radiometers, oil slick detection by X-band synthetic aperture radar, {{and the use of}} space photos for search of oil and gas fields. Individual items are announced in this issue...|$|R
30|$|In {{order to}} address all these {{challenges}} we developed a generic, ontology-centered research infrastructure. The main principle is the following: By modeling the actual research domain {{in the form of}} a domain ontology (Step 1 of the process), the domain-experts build the base for all subsequent steps. The whole research infrastructure derives its structure and behavior from the central domain ontology—at run-time. Changes to the ontology have immediate effects on the whole system, which consists of three main modules. Firstly, a management tool, which allows the user to model and maintain the domain ontology, but also process and analyze the research data. The other two components are an ontology-derived electronic data interface based upon and open-source ETL (Extract-Transform-Load) suite, and an ontology-derived web interface for <b>manual</b> <b>data</b> input and <b>processing.</b> Wherever possible the elaborate structural meta-information is used to actively support the user in <b>data</b> handling, <b>processing</b> and analyzing. The system always appears to the user as if it was especially tailored for his domain. For further information in more detail on the infrastructure itself, the reader is kindly referred to [29, 30, 31].|$|R
5000|$|This {{method of}} <b>data</b> <b>processing</b> {{involves}} human operators keying in data {{found on the}} form. The <b>manual</b> process of <b>data</b> entry has many disadvantages in speed, accuracy and cost. Based on average professional typist speeds of 50 to 80 wpm, one could generously estimate about two hundred pages per hour for forms with fifteen one-word fields (not counting the time for reading and sorting pages). In contrast, modern commercial scanners can scan and digitize up to 200 pages per minute. [...] The second major disadvantage to <b>manual</b> <b>data</b> entry is the likelihood of typographical errors. When factoring {{in the cost of}} labor and working space, <b>manual</b> <b>data</b> entry is a very inefficient process.|$|R
