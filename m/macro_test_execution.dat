0|1327|Public
40|$|Multi-modal <b>test</b> <b>{{execution}}</b> allows {{execution of}} the same test against various layers and components of a software system. This paper presents a method that effectively encodes one-to-many test definition and <b>test</b> <b>execution</b> relationship of multi-modal functional tests without creating large test maintenance overhead. Our approach extends the Fit table specification structure by multi-modal fixtures in Fitclipse and {{presents the results of}} <b>test</b> <b>execution</b> in a way that can help debugging and progress reporting. We analyze the application of multi-modal <b>test</b> <b>execution</b> and the potential benefits of using multi-modal <b>test</b> <b>execution</b> in a multi-functional team. 1...|$|R
40|$|A methyl green {{decolorization}} micro {{test for}} the determination of streptococcal anti-deoxyribonuclease B titers in serum is described. The micro test was compared with an alcohol precipitation <b>macro</b> <b>test</b> on 112 human sera. The anti-deoxyribonuclease B titers agreed within one dilution step for 85. 7 % of the 112 sera examined by both techniques. Both tests were about equal in the detection of titers above the upper limits of normal. The micro test requires smaller amounts of reagents and fewer man-hours to perform than the <b>macro</b> <b>test.</b> This translates into lower cost per test and more tests per man-hour...|$|R
5000|$|Multilingual tests (change the {{language}} during <b>test</b> <b>execution)</b> ...|$|R
30|$|The {{next phase}} of a generic testing process {{consists}} of <b>test</b> case <b>execution</b> and evaluation. At this point, the team runs the tests and, eventually, creates the defect reports. The evaluation aims to assure the test goals were achieved and to inform the results to stakeholders (Hass 2008. For this phase, Höhn (2011) identified 13 TMMi practices, which are related to <b>test</b> <b>execution</b> goals, management of incidents, non-functional <b>test</b> <b>execution</b> and peer reviews. This {{can be seen in}} Fig. 11. As the reader can notice, only four practices were not ranked as mandatory. This makes evident the relevance of this phase, since it encompasses activities which are related to <b>test</b> <b>execution</b> and management of incidents.|$|R
5000|$|Automatic <b>test</b> <b>execution</b> and {{evaluation}} (assessment) in a Test campaign ...|$|R
5000|$|Test {{planning}} through {{test set}} configuration and <b>test</b> <b>execution</b> configuration ...|$|R
50|$|The {{difference}} between the concept of <b>test</b> <b>execution</b> engine and operation system is that the <b>test</b> <b>execution</b> engine monitors, presents and stores the status, results, time stamp, length and other information for every Test Step of a Test Sequence, but typically an operation system does not perform such profiling of a software execution.|$|R
40|$|Abstract. Multi-modal <b>test</b> <b>{{execution}}</b> allows {{execution of}} the same test against various layers of a software system, e. g. the GUI layer, the web service layer and the business logic layer. Multi-modal <b>test</b> <b>execution</b> helps with identifying the location of software bugs during debugging and maintenance {{as well as in}} tracking the progress of the development effort. This paper presents a method that effectively encodes multi-modal functional tests without creating large test maintenance overhead. Our approach extends the Fit table specification structure by multi-modal fixtures and presents the results of <b>test</b> <b>execution</b> in a way to help with debugging and progress reporting...|$|R
5000|$|An {{advanced}} <b>test</b> <b>execution</b> engine {{may have}} additional functions, such as: ...|$|R
5000|$|Cross-browser <b>test</b> <b>execution</b> - Internet Explorer, Firefox, Chrome and Safari (web browser) ...|$|R
50|$|Test {{specification}} is selected, {{loaded and}} {{executed by the}} <b>test</b> <b>execution</b> engine similarly, as application software is selected, loaded and executed by operation systems. The <b>test</b> <b>execution</b> engine should not operate on the tested object directly, but though plug-in modules similarly as an application software accesses devices through drivers which are installed on the operation system.|$|R
5000|$|Grid {{benchmarking}} {{and comparison}} of <b>test</b> <b>execution</b> days reduces analysis and review effort.|$|R
40|$|We {{present a}} {{scalable}} BIST (Built-In Self Test) architecture {{that provides a}} tunable trade-off between on-chip area demand and <b>test</b> <b>execution</b> time for delay fault testing. So, the architecture can meet <b>test</b> <b>execution</b> time requirements, area requirements, or any target in between. Experiments show the scalability of our approach, e. g. that considerably shorter <b>test</b> <b>execution</b> time {{can be achieved by}} storing only a few additional input vectors of the BIST architecture. The gain of <b>test</b> <b>execution</b> time possible with the proposed method ranges from a factor of 2 up to a factor of more than 800000. 1 Introduction Delay fault testing is likely to become industrially accepted in the near future. However, there is no single delay fault model, but several models that compete for acceptance. A discussion of {{the advantages and disadvantages of}} these fault models is published e. g. in [12]. Besides their differences, all these models have in common that a test for a fault consists of two successiv [...] ...|$|R
5000|$|... a {{consistency}} oracle {{that compares}} {{the results of}} one <b>test</b> <b>execution</b> to another for similarity ...|$|R
5000|$|TCI: TTCN-3 Control Interfaces is the {{interface}} {{to control the}} <b>test</b> <b>execution.</b> It is divided in: ...|$|R
50|$|Exploratory {{testing means}} {{simultaneous}} test design and <b>test</b> <b>execution</b> {{with an emphasis}} on learning. Scripted testing means that learning and test design happen prior to <b>test</b> <b>execution,</b> and quite often the learning has to be done again during <b>test</b> <b>execution.</b> Exploratory <b>testing</b> is very common, but in most writing and training about testing it is barely mentioned and generally misunderstood. Some writers consider it a primary and essential practice. Structured exploratory testing is a compromise when the testers are familiar with the software. A vague test plan, known as a test charter, is written up, describing what functionalities need to be tested but not how, allowing the individual testers to choose the method and steps of testing.|$|R
40|$|Test {{prioritization}} aims {{at reducing}} <b>test</b> <b>execution</b> costs. There are several approaches to prioritize test cases based on collected data of previous test runs, e. g., in regression testing. In this paper, {{we present a}} new approach to test prioritization for efficient <b>test</b> <b>execution</b> that is focused on the artifacts used in model-based test generation from state machines. We propose heuristics for test goal prioritizations and evaluate them using two different test models. Our finding is that the prioritizations can have a positive impacton the <b>test</b> <b>execution</b> efficiency. This impact, however, is hard to predict for a concrete situation. Thus, the question for the general gain of test goal prioritizations is still open...|$|R
50|$|A <b>test</b> <b>execution</b> {{engine is}} a type of {{software}} used to test software, hardware or complete systems.|$|R
5000|$|Oracle Test Manager for test process management, {{including}} {{test requirements}} management, <b>test</b> management, <b>test</b> <b>execution</b> and defect tracking.|$|R
40|$|In our researches, we {{developed}} a method to size tests based on their specifications. This measure, called execution points, {{can be used as}} input for <b>test</b> <b>execution</b> effort estimation models. Here, we present our method for sizing tests that is based on test specifications written in natural language. We also presents the main functionalities of a tool developed for supporting our measurement method. In addition, we discuss how some techniques can be used for estimating <b>test</b> <b>execution</b> effort based on the proposed test size measure. Some interesting results of an empirical study run on the mobile application domain are also discussed. For instance, we verified in this empirical study a high linear correlation between <b>test</b> <b>execution</b> effort and execution points...|$|R
50|$|Rational Quality Manager {{includes}} an integrated <b>test</b> <b>execution</b> environment for running tests developed within the product {{as well as}} running tests created in other manual, functional, performance, and security testing tools. Options for <b>test</b> <b>execution</b> include running a test case directly, grouping test cases into test suites for parallel or sequential <b>execution,</b> or creating <b>test</b> case and test-suite execution records to map test environment information directly to the test cases and test suites.|$|R
30|$|Finally, we {{performed}} a cost analysis, to compare the expenses for monitoring <b>tests</b> <b>execution</b> and heparin infusion between the study groups.|$|R
5000|$|A <b>test</b> <b>execution</b> engine by {{executing}} a test specification, it may perform {{different types of}} operations on the product, such as: ...|$|R
5000|$|Test {{automation}} {{management systems}} leverage automation effort towards efficient and continuous processes of delivering <b>test</b> <b>execution</b> and new working tests by: ...|$|R
40|$|Simultaneous <b>test</b> design, <b>test</b> <b>execution,</b> and learning. • James Bach, 1995 But {{maybe it}} would be a good idea to {{underscore}} why that’s important… What IS Exploratory Testing? •Simultaneous <b>test</b> design, <b>test</b> <b>execution,</b> and learning, with an emphasis on learning. •Cem Kaner, 2005 But {{maybe it would}} be a good idea to be really explicit about what goes on… What IS Exploratory Testing? • I follow (and to some degree contributed to) Kaner’s definition, which was refined over several peer conferences through 2007 : Exploratory software testing is… • a style of software testing • that emphasizes the personal freedom and responsibility • of the individual tester • to continually optimize the value of his or her work • by treating <b>test</b> design, <b>test</b> <b>execution,</b> <b>test</b> result interpretation, and test-related learning • as mutually supportive activities • that run in parallel • throughout the project...|$|R
40|$|Testing {{is one of}} {{the most}} {{expensive}} tasks in today's software development cycle and it is very important to devise techniques that speed up the whole testing process. In a recent paper [9], it has been shown that tests on database applications can be speeded up by using proper <b>test</b> <b>execution</b> strategies and <b>test</b> optimization algorithms. This papers proposes a new <b>test</b> <b>execution</b> strategy called SAFE-OPTIMISTIC and a new test optimization algorithm called SLICEDepartment of ComputingRefereed conference pape...|$|R
40|$|Multithreaded code is {{notoriously}} hard {{to develop and}} test. A multithreaded test exercises the code under test with two or more threads. Each <b>test</b> <b>execution</b> follows some schedule/interleaving of the multiple threads, and different schedules can give different results. Developers often want to enforce a particular schedule for <b>test</b> <b>execution,</b> and to do so, they use time delays (Thread. sleep in Java). Unfortunately, this approach can produce false positives or negatives, and can result in unnecessarily long testing time. This paper presents IMUnit, a novel approach to specifying and executing schedules for multithreaded tests. We introduce anewlanguage thatallows explicit specification of schedules as orderings on events encountered during <b>test</b> <b>execution.</b> We present a tool that automatically instruments the code to control <b>test</b> <b>execution</b> to follow the specified schedule, and a tool that helps developers migrate their legacy, sleep-based tests into event-based tests in IMUnit. The migration tool uses novel techniques for inferring events and schedules from the <b>executions</b> of sleep-based <b>tests.</b> We describe our experience in migrating over 200 tests. The inference techniques have high precision and recall of over 75 %, and IMUnit reduces testing time compared to sleepbased tests on average 3. 39 x. Categories andSubject Descriptor...|$|R
40|$|Abstract — Software testing {{efficiency}} {{is very important}} factor for all test organizations. By conducting {{a case study in}} data base environment. this paper is talking about how we can improve the <b>test</b> <b>execution</b> process by considering the factors like automation using standard framework, introducing risk based <b>testing,</b> parallel <b>execution,</b> modularization, avoiding code redundancy and thr ough proper test management. The recommendation given {{at the end of the}} document can easily implement for any domains of testing for getting an improvement in <b>test</b> <b>execution.</b> Index Terms — Indexing tool, test efficiency, test automation, process improvement, risk based testing, test tracking tool, test point method, parallel execution. ...|$|R
5000|$|HP Business Process Testing {{software}}: Automated and manual testing {{software for}} test design, test creation, <b>test</b> maintenance, <b>test</b> <b>execution,</b> and <b>test</b> data management ...|$|R
50|$|Advanced {{functions}} of the <b>test</b> <b>execution</b> engine maybe less important for software testing, but these advanced features could be essential when executing hardware/system tests.|$|R
30|$|This paper {{presents}} {{the application of}} a formal testing methodology to protocols and services for wireless telephony networks. The methodology provides a complete and integrated coverage of all phases of the testing procedure: specification, test generation, and <b>test</b> <b>execution</b> on a given architecture. It permits to perform conformance and interoperability testing detecting different kinds of implementation faults, as for instance output and transmission faults. The <b>test</b> <b>execution</b> is performed in the framework of a set of architectures capable to deal with different environments.|$|R
40|$|Abstract — This work {{considers}} {{conformance testing}} (functional testing). The main distinction {{from the other}} works is the availability of mapping from implementation states to specification ones. This information allows us to simplify test development and to reduce <b>test</b> <b>execution</b> time. We introduce a novel implementation relation called conff and composite test suites. The conff relation minimizes the size of generated test suite. Composite test suite is a compact representation of traditional test suite. Furthermore, it allows to reduce <b>test</b> <b>execution</b> time. I...|$|R
40|$|The use {{of formal}} system {{specifications}} {{makes it possible}} to automate the derivation of test cases from specifications. This allows to automate the whole testing process, not only the <b>test</b> <b>execution</b> part of it. This paper presents {{the state of the art}} and future perspectives in testing based on formal methods. The theory of formal testing is briefly outlined, a test tool is presented which automates both test derivation and <b>test</b> <b>execution</b> on-the-fly, and an application case study is discussed. ...|$|R
50|$|The {{expected}} system behaviour {{for individual}} test cases {{should also be}} automatically tested to assure efficient test processes. TPT offers the possibility to compute the properties for the expected behaviour online (during <b>test</b> <b>execution)</b> and offline (after <b>test</b> <b>execution).</b> While online evaluation uses the same modelling techniques as test modelling, offline evaluation offers decidedly more far-reaching possibilities for more complex evaluations, including operations such as comparisons with external reference data, limit-value monitoring, signal filters, analyses of state sequences and time conditions.|$|R
5000|$|Exploratory {{testing is}} an {{approach}} to software testing that is concisely described as simultaneous learning, test design and <b>test</b> <b>execution.</b> Cem Kaner, who coined the term in 1984, defines exploratory testing as [...] "a style of software testing that emphasizes the personal freedom and responsibility of the individual tester to continually optimize the quality of his/her work by treating test-related learning, <b>test</b> design, <b>test</b> <b>execution,</b> and <b>test</b> result interpretation as mutually supportive activities that run in parallel throughout the project." ...|$|R
40|$|Verifying that <b>test</b> <b>executions</b> {{are correct}} {{is a crucial}} step in the testing process. Unfortunately, {{it can be a}} very arduous and {{error-prone}} step, especially when testing a concurrent system. System developers can therefore benefit from oracles automating the verifi-cation of <b>test</b> <b>executions.</b> This paper examines the use of Graphical Interval Logic (GIL) for specifying temporal properties of con-current systems and describes a method for construct-ing oracles from GIL specifications. The visually intuitive representation of GIL specifications makes them easier to develop and to understand than spec-ifications written in more traditional temporal logics. Additionally, when a <b>test</b> <b>execution</b> violates a GIL specification, the associated oracle provides informa-tion about a fault. This information can be displayed visually, together with the execution, to help the sys-tem developer see where in the execution a fault was detected {{and the nature of the}} fault. ...|$|R
