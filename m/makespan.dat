3675|38|Public
25|$|Vertex {{coloring}} {{models to}} a number of scheduling problems. In the cleanest form, a given set of jobs need to be assigned to time slots, each job requires one such slot. Jobs can be scheduled in any order, but pairs of jobs may be in conflict {{in the sense that they}} may not be assigned to the same time slot, for example because they both rely on a shared resource. The corresponding graph contains a vertex for every job and an edge for every conflicting pair of jobs. The chromatic number of the graph is exactly the minimum <b>makespan,</b> the optimal time to finish all jobs without conflicts.|$|E
5000|$|An optimal {{allocation}} is an allocation of jobs to workers {{in which the}} <b>makespan</b> is minimized. The minimum <b>makespan</b> is denoted by [...]|$|E
5000|$|Again, {{determine}} which machine {{is the new}} bottleneck. The new <b>makespan</b> is the old <b>makespan</b> plus the maximum lateness from the new bottleneck. Again, if the maximum lateness on all machines is zero then use all the paths for the disjunctive constraints on the drawing and the <b>makespan</b> {{is still the same}} as it was before.|$|E
40|$|We {{propose a}} novel replication-based {{two-phase}} scheduling algorithm designed to achieve DAG scheduling with small <b>makespans</b> and high efficiency. In the first phase, the schedule {{length of the}} application is minimized using a novel approach that utilizes partial schedules. In the second phase, the number of processors required is minimized by eliminating and merging these partial schedules. Experimental results on random DAGs show that the <b>makespans</b> generated by the proposed algorithm are slightly better than those generated by the well known CPFD algorithm whereas the number of processors used {{is less than half}} of what is needed by CPFD solutions. ...|$|R
40|$|We {{present a}} three-step binding {{algorithm}} for {{applications in the}} form of directed acyclic graphs (DAGs) of tasks with deadlines, that need to be bound to a shared memory multiprocessor platform. The aim of the algorithm is to obtain a good binding that results in low <b>makespans</b> of the schedules of the DAGs. It first clusters tasks assuming unlimited resources using a deadline-aware shared memory extension of the existing dominant sequence clustering algorithm. Second, the clusters produced are merged based on communication dependencies to fit into the number of available platform resources. As a final step, the clusters are allocated to the available resources by balancing the workload. The approach is compared to {{the state of the art}} bounded dominant sequence clustering (BDSC) algorithm that also performs clustering on a limited number of resources. We show that our three-step algorithm makes better use of the shared memory communication structure and produces significantly lower <b>makespans</b> than BDSC on benchmark cases...|$|R
40|$|Large-scale donation-based {{distributed}} infrastructures need to {{cope with}} the inherent unreliability of participant nodes. A widely-used work scheduling technique in such environments is to redundantly schedule the outsourced computations to a number of nodes. We present the design and implementation of RIDGE, a reliability-aware system which uses a node’s prior performance and behavior to make more effective scheduling decisions. We have implemented RIDGE on top of the BOINC distributed computing infrastructure and have evaluated its performance on a live PlanetLab testbed. Our experimental results show that RIDGE is able to match or surpass the throughput of the best BOINC configuration by automatically adapting to the characteristics of the underlying environment. In addition, RIDGE is able to provide much lower workunit <b>makespans</b> compared to BOINC. RIDGE is also able to produce significantly lower communication <b>makespans</b> for downloading clients. Collectively, the results suggest that RIDGE has great promise for service-oriented environments with time constraints. ...|$|R
5000|$|Then, the VCG {{mechanism}} allocates all {{tasks to}} worker 1. Both the [...] "make-total" [...] and the <b>makespan</b> are [...] But, when each job {{is assigned to}} a different worker, the <b>makespan</b> is [...]|$|E
50|$|In {{operations}} research, Johnson's rule is {{a method}} of scheduling jobs in two work centers. Its primary objective is to find an optimal sequence of jobs to reduce <b>makespan</b> (the total {{amount of time it}} takes to complete all jobs). It also reduces the amount of idle time between the two work centers.The method minimizes the <b>makespan</b> in the case of two work centers. Furthermore, the method finds the shortest <b>makespan</b> in the case of three work centers if additional constraints are met.|$|E
50|$|Once the {{bottleneck}} {{has been}} determined, the path for the machine {{needs to be}} included in the drawing of jobs (See Iteration 1 Drawing, where the colored arrows represent disjunctive constraints). These new paths can be considered the disjunctive constraints and they need to be taken into consideration when determining the new <b>makespan.</b> The disjunctive constraints are the machine constraints in our job shop. The new <b>makespan</b> will be the old <b>makespan</b> plus the maximum lateness of the machine determined to be the bottleneck.|$|E
40|$|International audienceHadoop {{has been}} {{recently}} used to process a diverse variety of applications, {{sharing the same}} execution infrastructure. A practical problem facing the Hadoop community is how to reduce job <b>makespans</b> by reducing job waiting times and ex- ecution times. Previous Hadoop schedulers have focused on improving job execution times, by improving data locality but not considering job waiting times. Even worse, enforcing data locality according to the job input sizes can be ineffi- cient: {{it can lead to}} long waiting times for small yet short jobs when sharing the cluster with jobs with smaller input sizes but higher execution complexity. This paper presents hSRTF, an adaption of the well-known Shortest Remaining Time First scheduler (i. e., SRTF) in shared Hadoop clus- ters. hSRTF embraces a simple model to estimate the re- maining time of a job and a preemption primitive (i. e., kill) to free the resources when needed. We have implemented hSRTF and performed extensive evaluations with Hadoop on the Grid’ 5000 testbed. The results show that hSRTF can significantly reduce the waiting times of small jobs and therefore improves their <b>makespans,</b> but at the cost of a rel- atively small increase in the <b>makespans</b> of large jobs. For instance, a time-based proportional share mode of hSRTF (i. e., hSRTF-Pr) speeds up small jobs by (on average) 45 % and 26 % while introducing a performance degradation for large jobs by (on average) 10 % and 0. 2 % compared to Fifo and Fair schedulers, respectively...|$|R
40|$|We {{propose a}} new duplication-based DAG {{scheduling}} algorithm for heterogeneous computing environments. Contrary {{to the traditional}} approaches, proposed algo-rithm traverses the DAG in a bottom-up fashion while taking advantage of task duplication and task insertion. Experimental results on random DAGs and three differ-ent application DAGs show that the <b>makespans</b> gener-ated by the proposed DBUS algorithm are much better than those generated by the existing algorithms, HEFT, HCPFD and HCNF. ...|$|R
40|$|Many {{scheduling}} {{problems are}} NP-hard on classical computers. Using quantum parallelism and entanglement, a quantum schedule algorithm {{may be able}} to lead to an exponential improvement. The algorithm presented in this paper constructs a superposition of all schedules and a superposition of their <b>makespans</b> and then amplifies the one that corresponds to the solution. We perform O(√(2 ^n+q)) Grover iterations. The time complexity of the quantum scheduling algorithm for an R||C_max problem is O(√(2 ^n+q)) while the complexity of a classical algorithm is O(2 ^m 2 ^n) ...|$|R
50|$|The <b>makespan</b> {{with this}} scheme is K · max(M, N), where K is the {{duration}} of one pairwise encounter. Note that {{this is exactly the}} same <b>makespan</b> if MN gloves were used. Clearly in this case, increasing capital cost has not produced a shorter operation time.|$|E
5000|$|NPO(II): Equals PTAS. Contains the <b>Makespan</b> {{scheduling}} problem.|$|E
50|$|At {{this point}} the Shifting Bottleneck Heuristic is complete. The drawing should now include all {{precedence}} constraints and all disjunctive constraints. The final <b>makespan</b> is the original <b>makespan</b> plus all of the maximum latenesses {{from each of the}} respective bottlenecks. It is the lowest amount of time needed complete all of the jobs given these machine and precedence constraints.|$|E
40|$|Abstract. This paper {{describes}} a preliminary work on Machine T, a temporal partial order planner {{based on the}} use of temporal constraint networks to represent and propagate temporal constraints over the actions of a plan. The use of this formalism provide several advantages like the possibility of including deadline goals, <b>makespans</b> and imprecise temporal references able to obtain a flexible timeline for the execution of temporal plans. The expressiveness of the approach also meet the main temporal specifications of PDDL 2. 1 level 3 like durative actions and invariant conditions...|$|R
40|$|This paper {{presents}} {{an adaptation of}} the Multi-Objective Simulated Annealing method to the Resource-Constrained Multi-Project Scheduling Problem. The types of projects used are the single mode and multiple mode projects. The objective functions to optimise are the projects <b>Makespans.</b> The study consisted to design the solution representation and the procedures, to generate some test instances, to do a preliminary tuning test and finally to perform the computation experiment for the performance evaluation. The results are satisfying and prove {{the efficiency of the}} MOSA method for this complex category of the project scheduling problem...|$|R
40|$|Many {{scheduling}} {{problems are}} N P − hard on classical computers. Using quantum parallelism and entanglement, a quantum schedule algorithm {{may be able}} to lead to an exponential improvement. The algorithm presented in this paper constructs a superposition of all schedules and a superposition of their <b>makespans</b> and then amplifies the one that corresponds to the solution. We perform O (√ 2 m 2 n) Grover iterations. The time complexity of the quantum scheduling algorithm for an R||Cmax problem is O (√ 2 m 2 n) while the complexity of a classical algorithm is O(2 m 2 n...|$|R
5000|$|Here [...] is {{the idle}} time of machine , [...] is the <b>makespan</b> and [...] {{is the number}} of machines. Notice that with the above definition, {{scheduling}} efficiency is simply the <b>makespan</b> normalized to the number of machines and the total processing time. This makes it possible to compare the usage of resources across JSP instances of different size.|$|E
5000|$|Given an {{allocation}} [...] of jobs to workers, The <b>makespan</b> of {{a project}} is: ...|$|E
50|$|So in this case, {{the second}} {{schedule}} attains the shortest <b>makespan,</b> which is 20.|$|E
40|$|In {{this paper}} we give online {{algorithms}} and competitive ratio bounds for a scheduling {{problem on the}} following two-layer architecture. The architecture consists of two sets of processors; within each set the processors are identical while both the processors themselves and their numbers may differ between the sets. The scheduler has to make an online assigment of jobs {{to one of the}} two processor sets. Jobs, assigned to a processor set, are then sceduled in an optimal o ine preemptive way within the processor set considered. The scheduler's task is to minimize the maximum of the two <b>makespans</b> of the processor sets...|$|R
40|$|AbstractIn practice, {{processing}} {{times can}} be more accurately represented as intervals with the most probable completion time somewhere near {{the middle of the}} interval. A fuzzy number which is essentially a generalized interval can represent this processing time interval exactly and naturally. In this work, triangular and trapezoidal fuzzy numbers are used to represent those vague job processing times in job shop production systems. The job sequencing algorithms of Johnson and Ignall and Schrage are modified to accept fuzzy job processing times. Fuzzy <b>makespans</b> and fuzzy mean flow times are then calculated for greater decision-making information. Numerous examples are used to illustrate the approach...|$|R
40|$|The Vienna Platform for Elastic Processes (ViePEP) is a {{research}} Busi-ness Process Management System (BPMS) which additionally provides the functionalities of a cloud resource controller. As the name im-plies, the system is able to plan, schedule, and enact elastic processes in the cloud. ViePEP allows the integration of di↵erent optimization approaches, which could aim, e. g., at minimum <b>makespans,</b> minimum costs, or a combination thereof. Within this Technical Report we present an optimization model to tackle the challenges of scheduling service in-vocations among cloud-based computational resources. This specific opti-mization approach – the Service Instance Placement Problem – considers di↵erent kinds of QoS attributes and aims at cost-eciency...|$|R
5000|$|If we let Shifra feed all goats, {{then the}} <b>makespan</b> is 36 (3×12 for Shifra).|$|E
5000|$|If we let Shmuel feed all goats, {{then the}} <b>makespan</b> is 30 (3×10 for Shmuel, 0 for Shifra); ...|$|E
50|$|The {{number of}} product batches to produce is set, {{and the goal}} is to {{minimize}} the <b>makespan</b> (processing time).|$|E
40|$|International audienceIn this article, we {{revisit the}} problem of {{scheduling}} dynamically generated directed acyclic graphs (DAGs) of multi-processor tasks (M-tasks). A DAG is a basic model for expressing workflows applications where each node represents a task of the workflow. We present a novel algorithm (DMHEFT) for scheduling dynamically generated DAGs onto a heterogeneous collection of clusters. The scheduling decisions {{are based on the}} predicted runtime of an M-task as well as the estimation of the redistribution costs between data-dependent tasks. The algorithm also takes care of unfavorable placements of M-tasks by considering the postponing of ready tasks even if idle processors are available. We evaluate the scheduling algorithm by comparing the resulting <b>makespans</b> to the results obtained by using other scheduling algorithms, such as RePA and MHEFT...|$|R
40|$|The {{purpose of}} grid {{computing}} {{is to produce}} a virtual supercomputer by using free resources available through widespread networks such as the Internet. This resource distribution, changes in resource availability, and an unreliable communication infrastructure pose a major challenge for efficient resource allocation. Because of the geographical spread of resources and their distributed management, grid scheduling {{is considered to be}} a NP-complete problem. It has been shown that evolutionary algorithms offer good performance for grid scheduling. This article uses a new evaluation (distributed) algorithm inspired by the effect of leaders in social groups, the group leaders 2 ̆ 7 optimization algorithm (GLOA), to solve the problem of scheduling independent tasks in a grid computing system. Simulation results comparing GLOA with several other evaluation algorithms show that GLOA produces shorter <b>makespans...</b>|$|R
40|$|Many {{scheduling}} {{problems are}} N P − hard. A quantum scheduling algorithm which exploits quantum parallelism and entanglement {{has the potential}} to lead to a significant improvement over a classical one. Given a deadline, or a range of deadlines, the algorithm presented in this paper allows us to determine if a solution to an R||Cmax problem with N = 2 n jobs and M = 2 m machines exists, and if so, it provides the schedule. The algorithm constructs a superposition of all schedules and a superposition of their <b>makespans</b> and then amplifies the one that corresponds to the solution. We perform O (√ 2 m 2 n) Grover iterations. The time complexity of the quantum scheduling algorithm for an R||Cmax problem is O (√ 2 m 2 n) while the complexity of a classical algorithm is O(2 m 2 n...|$|R
5000|$|Theorem: If [...] has a {{feasible}} solution then {{a schedule}} {{can be constructed}} with <b>makespan</b> [...] and cost [...]|$|E
5000|$|We {{guess the}} optimum value of <b>makespan</b> [...] {{and write the}} {{following}} LP. This technique is known as parametric pruning.|$|E
50|$|A {{lower bound}} of 1.852 was {{presented}} by Albers.Taillard instances {{has an important}} role in developing job shop scheduling with <b>makespan</b> objective.|$|E
40|$|Scientific {{workflows}} {{are often}} used to automate large-scale data analysis pipelines on clusters, grids, and clouds. However, because workflows can be extremely data-intensive, and are often executed on shared resources, {{it is critical to}} be able to limit or minimize the amount of disk space that workflows use on shared storage systems. This paper proposes a novel and simple approach that constrains the amount of storage space used by a workflow by inserting data cleanup tasks into the workflow task graph. Unlike previous solutions, the proposed approach provides guaranteed limits on disk usage, requires no new functionality in the underlying workflow scheduler, and does not require estimates of task runtimes. Experimental results show that this algorithm significantly reduces the number of cleanup tasks added to a workflow and yields better workflow <b>makespans</b> than the strategy currently used by the Pegasus Workflow Management System...|$|R
40|$|Rather than blindly {{following}} a predetermined schedule, human workers often change tasks {{in order to}} assist a coworker experiencing difficulties. We are examining how this notion of “helpful ” behavior can inspire new approaches to online plan execution and repair in multi-robot systems. Specifically, we are investigating proactive replanning, which attempts to predict problems or opportunities and adapt to them by shifting agents between executing tasks. By continuously predicting a task’s remaining duration, a proactive replanner is able to accommodate upcoming problems or opportunities before they manifest themselves. One way {{to do so is}} by adding or removing agents to or from the various executing tasks, allowing the planner to balance a schedule in response to the realities of execution. We have developed a proof-of-concept system that implements duration prediction and modification of existing tasks, yielding simulated executed <b>makespans</b> 1 as much as 32 % shorter than possible without these capabilities...|$|R
40|$|Abstract—Scientific {{workflows}} {{are often}} used to automate large-scale data analysis pipelines on clusters, grids, and clouds. However, because workflows can be extremely data-intensive, and are often executed on shared resources, {{it is critical to}} be able to limit or minimize the amount of disk space that workflows use on shared storage systems. This paper proposes a novel and simple approach that constrains the amount of storage space used by a workflow by inserting data cleanup tasks into the workflow task graph. Unlike previous solutions, the proposed approach provides guaranteed limits on disk usage, requires no new functionality in the underlying workflow scheduler, and does not require estimates of task runtimes. Experimental results show that this algorithm significantly reduces the number of cleanup tasks added to a workflow and yields better workflow <b>makespans</b> than the strategy currently used by the Pegasus Workflow Management System. I...|$|R
