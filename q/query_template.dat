29|150|Public
50|$|MemSQL {{combines}} lock-free data {{structures and}} a just-in-time compilation (JIT) to process highly volatile workloads. More specifically, MemSQL implements lock-free hash tables and lock-free skip lists in memory for fast random access to data. SQL queries {{sent to the}} MemSQL server are converted into byte code and compiled through LLVM into machine code. Queries are then stripped of their parameters and the <b>query</b> <b>template</b> is stored as a shared object which is subsequently matched against incoming queries to the system. Executing pre-compiled query plans removes interpretation along hot code paths, providing highly efficient code paths that minimize the number of central processing unit (CPU) instructions required to process SQL statements.|$|E
5000|$|The SAM format {{consists}} of a header and an alignment section. The binary representation of a SAM file is a BAM file, which is a compressed SAM file. SAM files can be analysed and edited with the software SAMtools. The header section must be prior to the alignment section if it is present. Heading's begin with the '@' symbol, which distinguishes them from the alignment section. Alignment sections have 11 mandatory fields, {{as well as a}} variable number of optional fields. 1. QNAME: <b>Query</b> <b>template</b> NAME. Reads/segments having identical QNAME are regarded to come from the same template. A QNAME ‘*’ indicates the information is unavailable. In a SAM file, a read may occupy multiple alignment lines, when its alignment is chimeric or when multiple mappings are given.|$|E
3000|$|In this section, {{we use the}} {{following}} <b>query</b> <b>template</b> in our experiments to demonstrate our implementation: [...]...|$|E
40|$|In {{this paper}} we {{investigate}} {{the effectiveness of}} topic-independent <b>query</b> <b>templates</b> {{as a tool for}} assisting users in articulating their information needs. We hypothesize that topic-independent <b>query</b> <b>templates</b> can help users with complex information needs to express their requirements more accurately and in greater detail. We developed a set of <b>query</b> <b>templates</b> representing general semantic relationships between concepts, such as cause-effect and problem-solution. Each template was written {{in the form of a}} fill-in-the-blanks question. A user study was performed comparing the template-based interface with a single-textbox search interface. Results demonstrate that, while users found the template-based query formulation less easy to use, the <b>queries</b> written using <b>templates</b> performed better than the queries written using the control interface with one query textbox...|$|R
40|$|LDAP {{directories}} {{have recently}} proliferated {{with the growth}} of the Internet, and are being used {{in a wide variety of}} network-based applications. In this paper, we propose the use of generalized queries, referred to as <b>query</b> <b>templates,</b> obtained by generalizing individual user queries, as the semantic basis for low overhead, high benefit LDAP directory caches for handling declarative queries. We present efficient incremental algorithms that, given a sequence of user queries, maintain a set of potentially beneficial candidate <b>query</b> <b>templates,</b> and select a subset of these candidates for admission into the directory cache. A novel feature of our algorithms is their ability to deal with overlapping <b>query</b> <b>templates.</b> Finally, we demonstrate the advantages of <b>template</b> caches over <b>query</b> caches, with an experimental study based on real data and a prototype implementation of the LDAP directory cache...|$|R
40|$|<b>Queries</b> in <b>template</b> form {{are gaining}} in {{popularity}} {{as a means}} of conveying specific information needs to search engines. We explore the utility of Information Retrieval (IR) techniques in the context of <b>templated</b> <b>queries.</b> Our investigations show that IR techniques known to be well-suited for ad hoc retrieval don’t seamlessly extend to the case of <b>templated</b> <b>queries.</b> We show that what works is a combination of IR techniques and intuition-driven modifications to the <b>templated</b> <b>queries,</b> resulting in statistically significant improvements over the baseline...|$|R
40|$|This paper {{presents}} a new method for ontology-based question answering (QA) {{with the use}} of <b>Query</b> <b>template</b> for Dining Ontology as the domain (Service of the restaurant is called dining). Hypothesis questions and query templates can be produced from domain ontology. Pre-generation of hypothesis questions {{on the basis of the}} domain ontology and textual entailment. Semantic web is the expression of meaning of the content. Semantic web based gives instant access. Reusability is enhanced. This method is not domain specific. It focuses on the RDF, Ontology model and Web ontology language. Design of the ontology created can be viewed. Query processing is faster using text pool, dining ontology is developed. Keywords: Ontology, RDF, Semantic web, <b>query</b> <b>template...</b>|$|E
40|$|We {{consider}} how to support interactive querying over webscale data. The basic {{approach is to}} view querying as a two-phase activity: first supply a <b>query</b> <b>template,</b> and later supply specific instantiations of the template. Interactive responsiveness is offered {{in the second phase}} only. While instances of this problem have been studied in the past, e. g., OLAP and web search, we pursue a more general formulation. Our aim is to build a general two-phase query system. 1...|$|E
40|$|Nowadays, many complex text search systems, such as Entity Search or Topic Search, {{have been}} {{proposed}} to allow users to retrieve fine granularity units (e. g., entities or topics) inside documents directly. As those search systems target on more complex search tasks, the traditional query processing method purely based on an inverted index can not execute those search queries efficiently. New execution algorithms and index structures need to be proposed. In this paper, we study the problem of automatically deriving an efficient execution algorithm and indexes that support the algorithm for those systems. We take a relational view {{of the problem and}} model it as optimizing a <b>query</b> <b>template</b> with views. This <b>query</b> <b>template</b> optimization problem raises new challenges including enumerating plans with views and selecting plans for answering a template for a query optimizer. We present a novel optimization framework with a new set of transformation rules and an efficient selection strategy to deal with those two challenges. We systematically evaluate our framework in two concrete application settings. Experiments show that: (1) The derived algorithm and indexes significantly improve the efficiency the keyword-based baseline method. (2) Our framework can automatically derive plans and indexes that are manually optimized for a system. (3) Our approach is general enough to be applied to different search systems...|$|E
40|$|In this paper, {{we present}} our {{solution}} for the first task of the second Semantic Publishing Challenge. The task requires extracting and semantically annotating information regarding CEUR-WS workshops, their chairs and conference affiliations, {{as well as their}} papers and their authors, from a set of HTML-encoded workshop proceedings volumes. Our solution builds on last year's submission, while we address a number of shortcomings, assess the generated dataset for its quality and publish the queries as SPARQL <b>query</b> <b>templates.</b> This is accomplished using the RDF Mapping Language (RML) to define the mappings, the RMLPROCESSOR to execute them, the RDFUnit to both validate the mapping documents and assess the generated dataset's quality, and the DATATANK to publish the SPARQL <b>query</b> <b>templates.</b> This results in an overall improved quality of the generated dataset that is reflected in the query results...|$|R
50|$|The InterMine web {{application}} allows creation of custom bioinformatics <b>queries,</b> includes <b>template</b> <b>queries</b> (web forms to run 'canned' queries). Users can upload and operate on lists of data. It {{is possible to}} configure/create widgets to analyse lists with graphs and enrichment statistics.|$|R
40|$|While {{many users}} have {{relatively}} general information needs, users {{who are familiar}} with a certain topic may have more specific or complex information needs. Such users already have some knowledge of a subject and its concepts, and they need to find information on a specific aspect of a certain entity, such as its cause, effect, and relationships between entities. To successfully resolve this kind of complex information needs, in our study, we investigated the effectiveness of topic-independent <b>query</b> <b>templates</b> as a tool for assisting users in articulating their information needs. A set of <b>query</b> <b>templates,</b> which were written in the form of fill-in-the-blanks was designed to represent general semantic relationships between concepts, such as cause-effect and problem-solution. To conduct the research, we designed a control interface with a single query textbox and an experimental interface with the <b>query</b> <b>templates.</b> A user study was performed with 30 users. Okapi information retrieval system was used to retrieve documents in response to the users’ queries. The analysis in this paper indicates that while users found the template-based query formulation less easy to use, the <b>queries</b> written using <b>templates</b> performed better than the queries written using the control interface with one query textbox. Our analysis of a group of users and some specific topics demonstrates that the experimental interface tended to help users create more detailed search queries and the users were able to think about different aspects of their complex information needs and fill in many templates. In the future, an interesting research direction would be to tune the templates, adapting them to users’ specific query requests and avoiding showing non-relevant templates to users by automatically selecting related templates from a larger set of templates...|$|R
40|$|A new {{approach}} for biometric database management is presented with enhanced security features. This paper attempts {{to improve the}} security of biometric database system by interfacing with MySQL server. The signature templates are acquired for the database using digital pen tablet, encrypted using a simple pseudo noise based algorithm and then stored on the MySQL server. During the authentication phase, the signatures are brought from the database for feature extraction. After decryption, they are compared with <b>query</b> <b>template.</b> Since, the database is password protected, it provides double security against adversary attacks. 1...|$|E
40|$|Most {{biometric}} verification techniques {{make decisions}} based solely on a score that represents the similarity of the <b>query</b> <b>template</b> with the reference template of the claimed identity stored in the database. When multiple templates are available, a fusion scheme can be designed using the similarities with these templates. Combining several templates to construct a composite template and selecting a set of useful templates has also been reported in addition to usual multi-classifier fusion methods when multiple matchers are available. These commonly adopted techniques rarely {{make use of the}} large number of non-matching templates in the database or training set. In this paper, we highlight the usefulness of such a fusion scheme while focusing on the problem of fingerprint verification. For each enrolled template, we identify its cohorts (similar fingerprints) based on a selection criterion. The similarity scores of the <b>query</b> <b>template</b> with the reference template and its cohorts from the database are used to make the final verification decision using two approaches: a likelihood ratio based normalization scheme and a Support Vector Machine (SVM) -based classifier. We demonstrate the accuracy improvements using the proposed method with no a priori knowledge about the database or the matcher under consideration using a publicly available database and matcher. Using our cohort selection procedure and the trained SVM, we show that accuracy can be significantly improved at the expense of few extra matches. 1...|$|E
40|$|The {{automatic}} QA system {{described in}} this paper uses a reference interview model to allow the user to guide {{and contribute to the}} QA process. A set of system capabilities was designed and implemented that defines how the user’s contributions can help improve the system. These include tools, called the <b>Query</b> <b>Template</b> Builder and the Knowledge Base Builder, that tailor the document processing and QA system to a particular domain by allowing a Subject Matter Expert to contribute to the query representation and to the domain knowledge. During the QA process, the system can interact with the user to improve query terminology b...|$|E
40|$|The {{advanced}} proxy-caching techinques {{used for}} function-embedded queries are discussed. The function templates capture the semantics of user-defined functions and the function-embedded <b>query</b> <b>templates</b> define {{the types of}} queries that the proxy answers. These templates allow the proxy to optimize its cache organization and its limited query processing logic accordingly. The table-valued functions which are opposite to scalar functions, provide function proxy additional challenges and opportunities...|$|R
40|$|The {{massive and}} diverse data sources on the Deep Web {{presents}} a serious data integration challenge. Existing vir-tual integration approaches suffer from slow query response, while surfacing approaches demand hefty storage space and incur huge costs in maintaining data freshness. We pro-pose a novel hybrid integration approach that strikes a bal-ance between the virtual and surfacing approaches. The key {{idea is to}} capture user needs in <b>query</b> <b>templates</b> and focus the integration efforts on the templates. However, realizing this approach requires innovations in template-driven query planning, <b>query</b> parsing, and <b>template</b> discovery. We elabo-rate on these challenges and propose our solutions...|$|R
40|$|In this paper, {{we address}} the problem of query {{formulation}} in the context of multi-domain integration of heterogeneous data on the Web. We argue that effectively tackling this problem requires solutions to query specification and refinement, development and organization of domain taxonomies, and designing <b>query</b> <b>templates</b> to incorporate spatial and temporal conditions across multiple domains. We discuss our approaches in designing the query formulation component for InfoMosaic, our proposed framework for multi-domain information integration. ...|$|R
40|$|This paper {{presents}} a generic pre-processor for expediting conventional template matching techniques. Instead of locating the best matched patch in the reference image to a <b>query</b> <b>template</b> via exhaustive search, the proposed algorithm rules out regions with no possible matches with minimum computational efforts. While working on simple patch features, such as mean, variance and gradient, the fast pre-screening is highly discriminative. Its computational efficiency is gained {{by using a}} novel octagonal-star-shaped template and the inclusion-exclusion principle to extract and compare patch features. Moreover, it can handle arbitrary rotation and scaling of reference images effectively. Extensive experiments demonstrate that the proposed algorithm greatly reduces the search space while never missing the best match...|$|E
40|$|We {{present a}} {{question}} answering system architecture which processes natural language {{questions in a}} pipeline consisting of five steps: i) question parsing and <b>query</b> <b>template</b> generation, ii) lookup in an inverted index, iii) string similarity computation, iv) lookup in a lexical database {{in order to find}} synonyms, and v) semantic similarity computation. These steps are ordered with respect to their computational effort, following the idea of layered processing: questions are passed on along the pipeline only if they cannot be answered on the basis of earlier processing steps, thereby invoking computationally expensive operations only for complex queries that require them. In this paper we present an evaluation of the system on the dataset provided by the 2 nd Open Challenge on Question Answering over Linked Data (QALD- 2). The main, novel contribution is a systematic empirical investigation {{of the impact of the}} single processing components on the overall performance of question answering over linked data. ...|$|E
40|$|Abstract: The {{authentication}} {{based on}} biometric information has several advantages compared to solely password-based systems, {{which has led}} to a growing interest of in-dustry and of public authorities in biometric-based systems. To meet the high security standards concerning biometric data, template protection systems such as the fuzzy vault are indispensable to maintain the secrecy of the critical information. Several publications have discussed the application of fuzzy vault to fingerprint authentica-tion systems. However, for identification purposes in large databases the fuzzy vault protection of the biometric reference data poses severe efficiency challenges. In this work, we examine and compare the performance of three different ap-proaches to enable the identification based on protected fingerprint minutiae templates also for large databases. All three approaches calculate a prioritization of the database entries exploiting filtering techniques and indexing structures. Based on this prior-itization a search performing the complex exact comparison of database and <b>query</b> <b>template</b> is steered and thus will faster find a match. ...|$|E
40|$|Understanding {{crosscutting}} {{concerns is}} difcult because their underlying relations remain {{hidden in a}} class-based de-composition of a system. Based on an extensive investigation of crosscutting concerns in existing systems and literature, we identied a number of typical implementation idioms and relations {{that allow us to}} group such concerns around so-called sorts. In this paper, we present SOQUET, a tool that uses sorts to support the consistent description and doc-umentation of crosscutting relations using pre-dened, sort-specic <b>query</b> <b>templates.</b> 1...|$|R
40|$|Tuning a DBMS that {{experiences}} varying workload is challenging. Database administrators {{cannot be}} expected to monitor the workload and react with appropriate tunings, therefore automation is essential. In this report we outline a new method for automatic physical DBMS tuning that uses machine learning to model and predict workloads, and tune for the future. Our method builds on previous approaches by exploiting predictability of DBMS workload without assuming full knowledge. We present results {{on a set of}} synthetic workloads generated from the TPCH <b>query</b> <b>templates.</b> ...|$|R
40|$|Work in hybrid human-machine query {{processing}} {{has thus}} far focused on the data: gathering, cleaning, and sorting. In this paper, we address a missed opportunity to use crowdsourcing to understand the query itself. We propose a novel hybrid human-machine approach that leverages the crowd to gain knowledge of query structure and entity relationships. The proposed system exploits a combination of query log mining, natural language processing (NLP), and crowdsourcing to generate <b>query</b> <b>templates</b> {{that can be used}} to answer whole classes of different questions rather than focusing on just a specific question and answer. 1...|$|R
30|$|In the Clinical E-Science Framework (CLEF), {{which is}} a {{repository}} of clinical histories for biomedical research and medical care, a technique known as WYSIWYM (Hallet et al. 2007) is used, which uses a semantic graph inspired by the one described in Zhang et al. (1999). The customization {{was carried out by}} the implementers by supplying a knowledge base of the semantic part of the database (semantic graph), which permits generating automatically the components that users may choose. The experiment for that system involved 15 physicians with knowledge of the domain, which were given a 5 -to- 10 -min briefing on the interface operation and a set of four queries. The average time that took them to formulate a query was 3.9  min and the reported recall was 100  %. Finally, it is important to mention that WYSIWYM does not permit formulating queries in unrestricted natural language, but rather the user starts by editing a basic <b>query</b> <b>template,</b> where concepts to be instantiated are clickable spans of text with associated pop-up menus containing options for expanding the query.|$|E
30|$|As an example, in Figure 2 A, {{a certain}} Bt-maize field is {{selected}} {{to generate a}} buffer zone of 2, 500 m around this field. As a result, {{the extent of the}} buffer appears in the map as a blue polygon (see Figure 3). In the next step, the user extracts geo-objects from the FFH layer by clipping with the buffer layer generated before (see Figure 2 C, F). In the result, one single FFH area is highlighted (red outline) being located within the buffer zone (see Figure 3). Additionally, the extracted FFH area is linked to a <b>query</b> <b>template</b> to provide specific information, for instance, on protected species housed in this FFH area. This spatial investigation whether the Bt-maize fields are within or near a conservation area is relevant since protected non-target organisms might be affected by toxins produced by Bt-maize or a change in biodiversity might be induced. Furthermore, it is possible to calculate the distance between the selected Bt-maize field and the respective conservation area (see Figure 2 E) and to identify other relevant geodata located within the buffer zone (see Figure 2 D).|$|E
40|$|Natural {{language}} question-answering over RDF {{data has}} received widespread attention. Although {{there have been}} several studies that have dealt with a small number of aggregate queries, they have many restrictions (i. e., interactive information, controlled question or <b>query</b> <b>template).</b> Thus far, there has been no natural language querying mechanism that can process general aggregate queries over RDF data. Therefore, we propose a framework called NLAQ (Natural Language Aggregate Query). First, we propose a novel algorithm to automatically understand a users query intention, which mainly contains semantic relations and aggregations. Second, to build a better bridge between the query intention and RDF data, we propose an extended paraphrase dictionary ED to obtain more candidate mappings for semantic relations, and we introduce a predicate-type adjacent set PT to filter out inappropriate candidate mapping combinations in semantic relations and basic graph patterns. Third, we design a suitable translation plan for each aggregate category and effectively distinguish whether an aggregate item is numeric or not, which will greatly affect the aggregate result. Finally, we conduct extensive experiments over real datasets (QALD benchmark and DBpedia), and the experimental results demonstrate that our solution is effective...|$|E
30|$|Q {{indicates}} the query fingerprint, D* 1, D* 2, D* 3 indicate the similarity {{distance between the}} <b>query</b> and <b>templates,</b> and D*c{{indicates the}} distance between the query and the centroid.|$|R
40|$|Web {{search queries}} are an {{encoding}} of the user’s search intent and extracting structured information from them can facilitate central search engine operations like improving the ranking of search results and advertisements. Not surprisingly, {{this area has}} attracted {{a lot of attention}} in the research community in the last few years. The problem is, however, made challenging by the fact that search queries tend to be extremely succinct; a condensation of user search needs to the bare-minimum set of keywords. In this paper we consider the problem of extracting, with no manual intervention, the hidden structure behind the observed search queries in a domain: the origins of the constituent keywords as well as the manner the individual keywords are assembled together. We formalize important properties of the problem and then give a principled solution based on generative models that satisfies these properties. Using manually labeled data we show that the <b>query</b> <b>templates</b> extracted by our solution are superior to those discovered by strong baseline methods. The <b>query</b> <b>templates</b> extracted by our approach have potential uses in many search engine tasks; query answering, advertisement matching and targeting, to name a few. In this paper we study one such task, estimating Query-Advertisability, and empirically demonstrate that using extracted template information can improve performance over and above the current state-of-the-art. Categories and Subject Descriptors...|$|R
40|$|Abstract- This paper {{presents}} an Ontology development based on images. A set of question patterns, called predictive questions, which are predicted {{to be asked}} by users in a domain, were generated {{on the basis of}} a domain ontology. Their corresponding <b>query</b> <b>templates,</b> which can be used to extract answers to the predictive questions from a knowledge base, were generated as well. The process of producing these question patterns and <b>query</b> <b>templates</b> is described. To retrieve images from a database using pattern matching techniques, but usually textual descriptions attached to the images are used. Semantic web ontology and metadata languages provide a new way to annotating and retrieving images. This paper considers the situation when a user is faced with an image repository whose content is complicated and semantically unknown to some extent. We show how ontologies can then be of help to the user in formulating the information need, the query, and the answers. As a proof of the concept, we have shown photos of restaurant. In this system, images are annotated according to ontologies When generating answers to the queries, the ontology combined with the image data also facilitates. In survey of the technical achievements in the research area of Image Retrieval, is especially Content-Based Image Retrieval. The survey covers image feature representation and extraction, multi-dimensional indexing, and system design...|$|R
40|$|Many {{approaches}} for converting keyword queries to formal query languages are presented for natural language interfaces to ontologies. Some approaches present fixed formal query templates, so they lack in providing support with {{increasing number of}} words in the user query. Other approaches work on constructing and manipulating subgraphs from RDF graphs so their processing is complex with respect to time and space. Techniques are presented to perform operations by obtaining a reduced RDF graph but they limit the input to some type of resources so their complete complexity with all type of input resources is unknown. For formal query generation, we present a variable <b>query</b> <b>template</b> whose computation is facilitated by less complex and distributed RDF property and relation graphs. A prototype QuriOnto is developed to evaluate our design. The user can query QuriOnto with any number of words and resource types. Also, {{to the best of}} our knowledge, it is the first system that can handle quantitative restrictions with keyword queries. As QuriOnto has no support for semantic similarity at this time except for rdfs labels so its recall is low but high precision shows that the approach is promising for the generation of corresponding formal queries...|$|E
40|$|I {{would like}} to thank my advisor Prof. Jayant Haritsa for helping me to take {{the first step in the}} world of {{scientific}} research. I {{would like to}} sincerely acknowledge his invaluable guidance and encouragement in all forms through out my stay in IISc. I do convey my gratitude to Atreyee and Harish for their valuable contributions and suggestions in structuring the Joint Paper. I {{would like to thank}} all the members of DSL who have made my stay at IISc memorable. I thank my family and friends for their continued support throughout my career. i Given a parameterized n-dimensional SQL <b>query</b> <b>template</b> and a choice of query optimizer, a plan diagram is a color-coded pictorial enumeration of the execution plan choices of the optimizer over the query parameter space. Similarly, we can define cost diagram and cardinality diagram as the pictorial enumerations of cost and cardinality estimations of the optimizer over the same space. These three diagrams are collectively called “optimizer diagrams”. These diagrams have proved to be very useful for the analysis and redesign of modern optimizers but their utility is adversel...|$|E
40|$|Today {{is one of}} {{the reasons}} that Sweden works for {{improved}} public health to reduce the incidence of the most common diseases. The eleven objectives are the key determinants of public health work and are thus assigned as the common positions for players to work towards the same goal. The purpose of this study was to examine which aspects of health promotion in Sweden examined in the scientific articles. The method is a literature review based on scientific articles. Searches have been conducted in the databases SAGE Journals Online, ASSIA and Cinahl. The exclusion and inclusion of the articles have a <b>query</b> <b>template</b> obtained form the basis for assessing the quality of scientific articles. This study aimed to investigate which aspects of health promotion in Sweden as described in the scientific articles. None of the aspect that was presented in the literature can be regarded as sensational addition to the aspect of humor that was considered could be regarded as a possible factor in health promotion. The reviewed articles shows that school, workplace and health care seen as arenas where it is possible to carry out work on health while reach out to many people. The results also showed that the factors self-esteem, confidence and empowerment are important to the work of health promotion...|$|E
40|$|When {{integrating}} heterogeneous information re-sources, it {{is often}} the case that the source is rather limited in the kinds of queries it can answer. If a query is asked of the entire system, we have a new kind of optimization problem, in which we must try to express the given query in terms of the lim-ited <b>query</b> <b>templates</b> that this source can answer. For the case of conjunctive queries, we show how to decide with a nondeterministic polynomial-time al-gorithm whether the given query can be answered. We then extend our results to allow arithmetic comparisons in the given query and in the tem-plates...|$|R
40|$|International audienceIn {{this paper}} we present our ongoing work on {{integrating}} large-scale terminological information into NLP tools. We {{focus on the}} problem of selecting and generating a set of suitable terms from the resources, based on deletion, modification and addition rules. We propose a general framework in which the raw data of the resources are first loaded into a knowledge base (KB). The selection and generation rules are then defined in a declarative way using <b>query</b> <b>templates</b> in the <b>query</b> language of the KB system. We illustrate the use of this framework to select and generate term sets from a UMLS dataset...|$|R
40|$|Abstract. We {{present an}} {{architecture}} for combining wikis containing hypertext with ontologies containing formal, structured information. A web-based ontology editor that supports collaborative work through versioning, transactions {{and management of}} simultaneous modifications is used for ontology evolution. In wiki pages, ontology information {{can be used to}} render dynamic content and answer user <b>queries.</b> Furthermore, <b>query</b> <b>templates</b> are introduced that simplify the use of queries for inexperienced users. The architecture allows easy integration with existing ontology frameworks and wiki engines. The usefulness of the approach is demonstrated by a prototypical implementation as well as a small case study. ...|$|R
