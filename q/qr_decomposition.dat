741|86|Public
25|$|The {{first stage}} in the {{calculation}} of a thin SVD will usually be a <b>QR</b> <b>decomposition</b> of M, which can make for a significantly quicker calculation ifn≪m.|$|E
25|$|A matrix {{can also}} be factorized into a product of {{matrices}} of special types, for an application in which that form is convenient. One major example of this uses an orthogonal or unitary matrix, and a triangular matrix. There are different types: <b>QR</b> <b>decomposition,</b> LQ, QL, RQ, RZ.|$|E
25|$|Much {{effort has}} been put in the {{development}} of methods for solving systems of linear equations. Standard direct methods, i.e., methods that use some matrix decomposition are Gaussian elimination, LU decomposition, Cholesky decomposition for symmetric (or hermitian) and positive-definite matrix, and <b>QR</b> <b>decomposition</b> for non-square matrices. Iterative methods such as the Jacobi method, Gauss–Seidel method, successive over-relaxation and conjugate gradient method are usually preferred for large systems. General iterative methods can be developed using a matrix splitting.|$|E
3000|$|... for i= 1, 2 are the unitary {{matrices}} which {{relate to}} QL and <b>QR</b> <b>decompositions</b> for the dependent channel coefficients.|$|R
50|$|Since in {{the modern}} {{implicit}} version of the procedure no <b>QR</b> <b>decompositions</b> are explicitly performed, some authors, for instance Watkins, suggested changing its name to Francis algorithm. Golub and Van Loan use the term Francis QR step.|$|R
50|$|<b>QR</b> <b>decompositions</b> {{can also}} be {{computed}} {{with a series of}} Givens rotations. Each rotation zeroes an element in the subdiagonal of the matrix, forming the R matrix. The concatenation of all the Givens rotations forms the orthogonal Q matrix.|$|R
2500|$|Note {{that the}} {{singular}} values are real and right- and left- singular vectors {{are not required}} to form any similarity transformation. Alternating <b>QR</b> <b>decomposition</b> and LQ decomposition can be claimed to use iteratively to find the real diagonal matrix with Hermitian matrices. <b>QR</b> <b>decomposition</b> gives [...] and LQ decomposition of [...] gives [...] Thus, at every iteration, we have , update [...] and repeat the orthogonalizations.|$|E
2500|$|Eventually, <b>QR</b> <b>{{decomposition}}</b> and LQ decomposition iteratively provide unitary matrices for left- and right- singular matrices, respectively.|$|E
2500|$|Given {{a matrix}} A, some methods compute its {{determinant}} by writing A {{as a product}} of matrices whose determinants can be more easily computed. Such techniques are referred to as decomposition methods. Examples include the LU decomposition, the <b>QR</b> <b>decomposition</b> or the Cholesky decomposition (for positive definite matrices). These methods are of order O(n3), which is a significant improvement over O(n!) ...|$|E
50|$|Householder transformations {{are widely}} used in {{numerical}} linear algebra, to perform <b>QR</b> <b>decompositions</b> and {{is the first step}} of the QR algorithm. They are also widely used for tridiagonalization of symmetric matrices and for transforming non-symmetric matrices to a Hessenberg form.|$|R
5000|$|Householder {{reflections}} {{can be used}} {{to calculate}} <b>QR</b> <b>decompositions</b> by reflecting first one column of a matrix onto a multiple of a standard basis vector, calculating the transformation matrix, multiplying it with the original matrix and then recursing down the [...] minors of that product.|$|R
40|$|The {{problems}} of blind timing acquisition and channel estimation for DS-CDMA signals in multipath fading channels are investigated. Methods based on <b>QR</b> <b>decompositions</b> are proposed. These methods perform comparable or {{even better than}} subspace based methods with an order lower complexity. Furthermore, the methods exhibit significantly more robustness to channel order mismatch. Based on the acquired timing information, channel estimation algorithms are also developed which are competitive with the previously proposed subspace based channel estimation algorithms. In addition, a channel order estimation algorithm is proposed for the scenario where the order is unknown. Performance of the proposed algorithms is evaluated through simulation. Keywords: DS-CDMA, multipath fading, synchronization, timing acquisition, multiuser systems, blind algorithms, channel estimation, wideband CDMA, <b>QR</b> <b>decompositions.</b> y This work {{was supported by the}} National Science Foundation under CAREER Grant NCR 9624 [...] ...|$|R
2500|$|If the [...] matrix [...] is nonsingular, its columns are linearly {{independent}} vectors; {{thus the}} Gram–Schmidt process can adjust {{them to be}} an orthonormal basis. Stated in terms of numerical linear algebra, we convert [...] to an orthogonal matrix, , using <b>QR</b> <b>decomposition.</b> However, we often prefer a [...] closest to , which this method does not accomplish. For that, the tool we want is the polar decomposition ( [...] ; [...] ).|$|E
2500|$|The {{second step}} {{can be done}} by a variant of the QR {{algorithm}} for the computation of eigenvalues, which was first described by [...] The LAPACK subroutine DBDSQR implements this iterative method, with some modifications to cover the case where the singular values are very small [...] Together with a first step using Householder reflections and, if appropriate, <b>QR</b> <b>decomposition,</b> this forms the DGESVD routine for the computation of the singular-value decomposition.|$|E
2500|$|The {{first step}} {{can be done}} using Householder {{reflections}} for a cost of 4mn2 − 4n3/3 flops, assuming that only the singular values are needed and not the singular vectors. If m is much larger than n then it is advantageous to first reduce the matrix M to a triangular matrix with the <b>QR</b> <b>decomposition</b> and then use Householder reflections to further reduce the matrix to bidiagonal form; the combined cost is 2mn2 + 2n3 flops [...]|$|E
40|$|We {{address the}} task of higher-order {{derivative}} evaluation of computer programs that contain <b>QR</b> <b>decompositions</b> and real symmetric eigenvalue decompositions. The approach {{is a combination of}} univariate Taylor polynomial arithmetic and matrix calculus in the (combined) forward/reverse mode of Algorithmic Differentiation (AD). Explicit algorithms are derived and presented in an accessible form. The approach is illustrated via examples...|$|R
40|$|It {{has become}} a commonplace that {{triangular}} systems are solved to higher accuracy than their condition would warrant. This observation is not true in general, and counterexamples are easy to construct. However, it is often true of the triangular matrices from pivoted LU or <b>QR</b> <b>decompositions.</b> It is shown that this fact is closely connected with the rank-revealing character of these decompositions...|$|R
30|$|In our {{proposed}} QL-QR algorithm, we {{take advantage}} of QL and <b>QR</b> <b>decompositions</b> instead of the SVD operation, and then we compute an efficient channel as well as decompose a noise covariance matrix by the Choleskey decomposition. Finally, we calculate the determinant of B^ 2 _i to solve an optimization problem. Obviously, our proposed QL-QR algorithm outperforms conventional algorithms {{in the light of}} the computational complexity.|$|R
5000|$|<b>QR</b> <b>decomposition</b> (see Householder {{transformation}} and Gram-Schmidt decomposition); ...|$|E
50|$|Iwasawa {{decomposition}} generalizes <b>QR</b> <b>decomposition</b> to semisimple Lie groups.|$|E
50|$|<b>QR</b> <b>decomposition</b> is Gram-Schmidt {{orthogonalization}} {{of columns}} of A, started {{from the first}} column.|$|E
40|$|AbstractWe {{show how}} to compactly {{represent}} any n-dimensional subspace of Rm as a banded product of Householder reflections using n(m-n) floating point numbers. This is optimal since these subspaces form a Grassmannian space Grn(m) of dimension n(m-n). The representation is stable {{and easy to}} compute: any matrix can be factored into {{the product of a}} banded Householder matrix and a square matrix using two to three <b>QR</b> <b>decompositions...</b>|$|R
40|$|It {{has become}} a commonplace that {{triangular}} systems are solved to higher accuracy than their condition would warrant. This observation is not true in general, and counterexamples are easy to construct. However, it is often true of the triangular matrices from pivoted LU or <b>QR</b> <b>decompositions.</b> It is shown that this fact is closely connected with the rank-revealing character of these decompositions. (Also cross-referenced as UMIACS-TR- 95 - 91...|$|R
40|$|We {{consider}} {{the solution of}} the?-Sylvester equations AX±X?B? = C, for? = T,H and A,B, ∈ Cn×n, and the related linear matrix equations AXB?±X? = C, AXB?±CX?D? = E and AX±X?A? = C. Solvability conditions and stable numerical methods are considered, {{in terms of the}} (generalized and periodic) Schur and <b>QR</b> <b>decompositions.</b> We emphasize on the square cases where m = n but the rectangular cases will be considered...|$|R
50|$|In linear algebra, a <b>QR</b> <b>{{decomposition}}</b> (also {{called a}} QR factorization) of a matrix is a decomposition of a matrix A into a product A = QR of an orthogonal matrix Q and an upper triangular matrix R. <b>QR</b> <b>decomposition</b> {{is often used}} to solve the linear least squares problem, and is the basis for a particular eigenvalue algorithm, the QR algorithm.|$|E
5000|$|The matrix X is {{subjected}} to an orthogonal decomposition, e.g., the <b>QR</b> <b>decomposition</b> as follows.|$|E
50|$|This completes two {{iterations}} of the Givens Rotation and {{calculating the}} <b>QR</b> <b>decomposition</b> {{can now be}} done.|$|E
40|$|We {{show how}} to compactly {{represent}} any n-dimensional subspace of Rm as a banded product of Householder reflections using n(m − n) floating point num-bers. This is optimal since these subspaces form a Grassmannian space Grn(m) of dimension n(m − n). The representation is stable {{and easy to}} compute: any matrix can be factored into {{the product of a}} banded Householder matrix and a square matrix using two to three <b>QR</b> <b>decompositions...</b>|$|R
40|$|Long-run {{recursive}} identification schemes {{are very}} popular in the structural VAR literature. This note suggests a two-step procedure based on <b>QR</b> <b>decompositions</b> as a solution algorithm {{for this type of}} identification problem. Our procedure will always deliver the exact solution and {{it is much easier to}} implement than a Newton-type iteration algorithm. It may therefore be very useful whenever quick and precise solutions of a long-run recursive scheme are required, e. g. in bootstrapping confidence intervals for impulse responses...|$|R
30|$|The {{fact that}} we obtain a square {{equivalent}} channel matrix also allows {{the possibility of using}} lattice reduction (LR)-based detectors which have a better performance for square channel matrices [27]. Further, the computational complexity to compute the channel inversion (13) and N <b>QR</b> <b>decompositions</b> (15) of matrices with dimensions N_t_n× N_r is much lower than the computational cost of computing N SVD transformations (19) of matrices with dimensions i̇N_r× (N_t-N_t_n). For these reasons, in this paper we will focus on the first alternative.|$|R
50|$|Common {{problems}} in numerical linear algebra include computing the following: LU decomposition, <b>QR</b> <b>decomposition,</b> singular value decomposition, eigenvalues.|$|E
5000|$|In this crude {{form the}} {{iterations}} are relatively expensive. This can be mitigated by first bringing the matrix A to upper Hessenberg form (which costs [...] arithmetic operations using a technique based on Householder reduction), with a finite sequence of orthogonal similarity transforms, somewhat like a two-sided <b>QR</b> <b>decomposition.</b> [...] (For <b>QR</b> <b>decomposition,</b> the Householder reflectors are multiplied {{only on the}} left, but for the Hessenberg case they are multiplied on both left and right.) Determining the <b>QR</b> <b>decomposition</b> of an upper Hessenberg matrix costs [...] arithmetic operations. Moreover, because the Hessenberg form is already nearly upper-triangular (it has just one nonzero entry below each diagonal), {{using it as a}} starting point reduces the number of steps required for convergence of the QR algorithm.|$|E
5000|$|After [...] {{iterations}} of this process, ,is {{an upper}} triangular matrix. So, {{with is a}} <b>QR</b> <b>decomposition</b> of [...]|$|E
40|$|This article {{describes}} Fortran 77 subroutines for computing eigenvalues and invariant subspaces of Hamiltonian and skew-Hamiltonian matrices. The implemented algorithms {{are based on}} orthogonal symplectic decompositions, implying numerical backward stability as well as symmetry preservation for the computed eigenvalues. These algorithms are supplemented with balancing and block algorithms, {{which can lead to}} considerable accuracy and performance improvements. As a by-product, an efficient implementation for computing symplectic <b>QR</b> <b>decompositions</b> is provided. We demonstrate the usefulness of the subroutines for several, practically relevant examples...|$|R
40|$|We {{show how}} to compactly {{represent}} any $n$-dimensional subspace of $R^m$ as a banded product of Householder reflections using $n(m - n) $ floating point numbers. This is optimal since these subspaces form a Grassmannian space $Gr_n(m) $ of dimension $n(m - n) $. The representation is stable {{and easy to}} compute: any matrix can be factored into {{the product of a}} banded Householder matrix and a square matrix using two to three <b>QR</b> <b>decompositions.</b> Comment: 5 pages, 1 figure, submitted to Linear Algebra and its Application...|$|R
40|$|The Gram-Schmidt {{method is}} a {{classical}} method for determining <b>QR</b> <b>decompositions,</b> which {{is commonly used}} in many applications in computational physics, such as orthogonalization of quantum mechanical operators or Lyapunov stability analysis. In this paper, we discuss how well the Gram-Schmidt method performs on different hardware architectures, including both state-of-the-art GPUs and CPUs. We explain, in detail, how a smart interplay between hardware and software {{can be used to}} speed up those rather compute intensive applications as well as the benefits and disadvantages of several approaches...|$|R
