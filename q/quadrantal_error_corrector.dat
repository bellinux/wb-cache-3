0|54|Public
50|$|It is {{sometimes}} wrongly confused with <b>quadrantal</b> <b>error,</b> {{which is the}} result of radio waves being bounced and reradiated by the airframe. <b>Quadrantal</b> <b>error</b> does not affect signals from straight ahead or behind, nor on the wingtips. The further from these cardinal points and the closer to the quadrantal points (i.e. 45°, 135°, 225° and 315° from the nose) the greater the effect, but <b>quadrantal</b> <b>error</b> is normally much less than dip error, which is always present when the aircraft is banked.|$|R
40|$|It {{is shown}} that the dicode pulse {{position}} modulation (DiPPM) scheme, presented in this study, demonstrates several key advantages in comparison with other existing PPM schemes, because of the compact structure and efficient recovery process of the clock signal. An <b>error</b> <b>corrector</b> for the DiPPM scheme is presented, where experimental results prove the theoretical expectations of a previous publication. Thus, it is demonstrated that DiPPM {{with the use of}} a maximum likelihood sequence detector (MLSD) <b>error</b> <b>corrector,</b> significantly reduces or eliminates errors incurred during the transmission. Comparison of and discussion on the outcomes of a DiPPM system with and without the <b>error</b> <b>corrector</b> are include...|$|R
40|$|This paper {{presents}} {{particularly a}} contextual post processing subsystem for a Turkish machine printed character recognition system. The contextual post processing subsystem {{is based on}} positional binary 3 -gram statistics for Turkish language, an <b>error</b> <b>corrector</b> parser and a lexicon, which contains root words and the inflected forms of the root words. <b>Error</b> <b>corrector</b> parser is used for correcting CR alternatives using Turkish Morphology. 1...|$|R
25|$|Interleaving is used {{to convert}} {{convolutional}} codes from random <b>error</b> <b>correctors</b> to burst <b>error</b> <b>correctors.</b> The basic idea behind the use of interleaved codes is to jumble symbols at the receiver. This leads to randomization of bursts of received errors which are closely located and we can then apply the analysis for random channel. Thus, the main function performed by the interleaver at transmitter is to alter the input symbol sequence. At the receiver, the deinterleaver will alter the received sequence to get back the original unaltered sequence at the transmitter.|$|R
40|$|With an <b>error</b> <b>corrector</b> via {{principal}} {{branch of}} the mth root of a function-to-function ratio, we propose optimal quartic-order multiple-root finders for nonlinear equations. The relevant optimal order satisfies Kung-Traub conjecture made in 1974. Numerical experiments performed for various test equations demonstrate convergence behavior agreeing with theory and the basins of attractions for several examples are presented...|$|R
40|$|Summary: We {{present a}} new tool to correct {{sequencing}} errors in Illumina data produced from high-coverage whole-genome shotgun resequencing. It uses a non-greedy algorithm and shows comparable performance and higher accuracy in an evaluation on real human data. This evaluation {{has the most}} complete collection of high-performance <b>error</b> <b>correctors</b> so far. Availability and implementation: [URL] Contact: hengli@broadinstitute. or...|$|R
40|$|The {{introduction}} of second-generation DNA sequencers has enabled researchers to explore biological information in ways never before possible. These sequencers provide increased throughput over first-generation sequencers at decreasing costs. However, the information produced by these sequencing technologies contains errors which may complicate downstream analyses. The error correction problem involves locating sequencing errors and making edits that correct or remove errors. We introduce Pollux, a platform-independent <b>error</b> <b>corrector</b> which identifies and fixes errors produced by second-generation sequencing technologies. We evaluate Pollux on several diploid bacterial data sets. Using standardized test data, Pollux corrects 85 % of Roche 454 GS Junior, 86 % of Ion Torrent PGM, and 94 % of Illumina MiSeq errors. We compare Pollux to several current <b>error</b> <b>correctors.</b> Pollux performs comparably {{with the most}} effective correctors when correcting Illumina data and makes significant improvements when correcting Roche 454 and Ion Torrent PGM data. Furthermore, we provide evidence that Pollux can correct errors {{in the presence of}} varying coverage and improves the quality of sequence assemblies...|$|R
40|$|We {{introduce}} {{an improved}} version of RECKONER, an <b>error</b> <b>corrector</b> for Illumina whole genome sequencing data. By modifying its workflow we reduce the computation time even 10 times. We also propose a new {{method of determination}} of $k$-mer length, the key parameter of $k$-spectrum-based family of correctors. The correction algorithms are examined on huge data sets, i. e., human and maize genomes for both Illumina HiSeq and MiSeq instruments...|$|R
40|$|A {{model of}} error {{correction}} is presented. Upon detection of a syntax error, a locally least-cost corrector operates by deleting 0 or more input symbols and inserting a terminal string that guarantees {{that the first}} non-deleted symbol will {{be accepted by the}} parser. The total correction cost, as defined by a table of deletion and insertion costs, is minimized. Previous work with the LL(1) parsing technique is summarized and a locally least-cost <b>error</b> <b>corrector</b> for LR(1) -based parsers is developed. Correctness as well as time and space complexity are discussed. In particular, linearity is established {{in the case of a}} bounded depth parse stack. Implementation results are presented. Attributed grammars can be used to specify the context-sensitive syntax of programming languages. A formal presentation of Attribute-Free LL(1) parsing is given and a locally least-cost <b>error</b> <b>corrector</b> for AF-LL(1) parsers is developed for the case in which the attributes that control context-sensitive correctness have finite domains. The algorithm is shown to have the same properties as its LL(1) and LR(1) counterparts...|$|R
40|$|An <b>error</b> <b>corrector</b> {{working with}} LR(1) parsers and {{variations}} such as SLR(1) and LALR(1) is studied. The corrector {{is able to}} correct and parse any input string. Upon detection of a syntax error, it operates by deleting 0 or more input symbols and inserting a terminal string that guarantees the first non-deleted symbol {{to be accepted by}} the parser. The total correction costs, as defined by a table of deletion and insertion costs, is minimized...|$|R
40|$|We {{present a}} novel OCR error {{correction}} method for languages without word delimiters {{that have a}} large character set, such as Japanese and Chinese. It consists of a statistical OCR model, an approxi-mate word matching method using character shape similarity, and a word segmentation algorithm us-ing a statistical language model. By using a sta-tistical OCR model and character shape similarity, the proposed <b>error</b> <b>corrector</b> outperforms the previ-ously published method. When the baseline char-acter recognition accuracy is 90 %, it achieves 97. 4 % character recognition accuracy. ...|$|R
40|$|If a VLSI chip is {{partitioned}} into functional units (FU's) and redundant FU's are added, error correcting codes may {{be employed}} to increase the yield and/or reliability of the chip. Acceptable testing is defined to be testing the chip with the <b>error</b> <b>corrector</b> functioning, thus obtaining the maximum increase in yield afforded by the error correction. The acceptable testing theorem shows {{that the use of}} coding and error correction in conjunction with acceptable testing can significantly increase the yield of VLSI chips without seriously compromising their reliability...|$|R
40|$|This work {{introduces}} a new data transmission {{system in which}} the main blocks, coding, modulation and channel are designed on a Riemannian manifolds. An intrinsic algebraic structure to the manifolds (surface), the homology group will be used to compose an <b>error</b> <b>corrector</b> code, a partition on the surface extracted from the embedding of a graph, which will compose the modulation design, and the channel design is the result of an association rule applied to the embedded graph. Mathematical subject classification: Primary: 06 B 10; Secondary: 06 D 05...|$|R
40|$|After {{defining}} appropriate metrics on {{strings and}} parse trees, the classic definition of continuity is adapted {{and applied to}} functions from strings to parse trees. Grammars that yield continuous mappings are of special interest, because they provide a sound theoretical framework for syntax error correction. Continuity justifies the approach, taken by many <b>error</b> <b>correctors,</b> to use the function output (the parse tree), and all the additional information it provides, {{in order to find}} corrections to the function input (the string). We prove that all Bounded Context grammars are continuous and that all continuous grammars are Bounded Context Parseable grammars, giving a characterization of continuous grammars in terms of possible parsing algorithms...|$|R
40|$|A fully {{asynchronous}} {{implementation of}} a complete DCC <b>error</b> <b>corrector</b> is presented that consumes 10 mW at 5 V, only a fifth of its synchronous counterpart, This is achieved by eliminating clocks, and exploiting the additional freedom in architecture provided {{by the absence of}} a clock. The corrector has been integrated in an experimental player and is both functionally and audibly correct. Handshake circuits are proposed as an architecture that enables structured design of asynchronous circuits through a consistent application of handshake signaling at all design levels. Handshake circuits are compiled fully automatically from high-level descriptions, and are implemented quasi delay insensitively using 4 -phase handshake signaling and double-rail data encoding. The resulting circuits are self-initializable and testabl...|$|R
40|$|This book {{presents}} {{the investigation of}} different architectures of integrating hydrological knowledge and models with data-driven models {{for the purpose of}} hydrological flow forecasting. The models resulting from such integration are referred to as hybrid models. The book addresses the following topics: A classification of different hybrid modelling approaches in the context of flow forecasting. The methodological development and application of modular models based on clustering and baseflow empirical formulations. The integration of hydrological conceptual models with neural network <b>error</b> <b>corrector</b> models and the use of committee models for daily streamflow forecasting. The application of modular modelling and fuzzy committee models to the problem of downscaling weather information for hydrological forecasting. WatermanagementCivil Engineering and Geoscience...|$|R
40|$|In this paper, {{we present}} a new phrase break {{detection}} architecture that integrates probabilistic approach with rule-based error correction. The architecture consists of a probabilistic phrase break detector and a transformational rule-based post <b>error</b> <b>corrector.</b> The probabilistic method alone usually suffers from performance degradation due to inherent data sparseness problems. So we adopted transformational rule-based error correction to overcome these training data limitations. The probabilistic phrase break detector segments the POS sequences into several phrases according to word trigram probabilities. The probabilistic phrase break detection only covers a limited range of contextual information. Moreover, the module does not see the morpheme tag selectively and relative distance to the other phrase breaks. The initially phrase break tagged morpheme sequence is corrected with error correcting rules. The rules are learned by comparing the correctly tagged phrase break corpus with t [...] ...|$|R
40|$|In {{this paper}} we develop a {{framework}} for constructing fault-tolerant dynamic systems, focusing primarily on linear finite state machines (LFSM's). Modular redundancy, the traditional approach to fault tolerance, is expensive because of the overhead in replicating the hardware and its reliance {{on the assumption that}} the error-correcting (voting) mechanism is fault-free. Our approach is more general, makes efficient use of redundancy, and relaxes the strict requirements regarding the reliability of the <b>error</b> <b>corrector.</b> By combining linear coding techniques and dynamic system theory, we characterize the class of all appropriate redundant implementations. Furthermore, we construct reliable LFSM's assembled exclusively from unreliable components, including unreliable voters and parity checkers in the error correcting mechanism. Using constant redundancy per system, we obtain implementations of identical LFSM's that operate in parallel on distinct input sequences and achieve arbitrarily low p [...] ...|$|R
40|$|Advances in {{high-throughput}} {{next-generation sequencing}} (NGS) technologies have enabled {{the determination of}} millions of nucleotide sequences in massive parallelism at affordable costs. Many studies have shown increased error rates over Sanger sequencing, in sequencing data produced by mainstream next-generation sequencing platforms, and have demonstrated the negative impacts of sequencing errors {{on a wide range}} of applications of NGS. Thus, it is critically important for primary analysis of sequencing data to produce accurate, high-quality nucleotides for downstream bioinformatics pipelines. Two bioinformatics problems are dedicated to the direct removal of sequencing errors: base-calling and error-correction. However, existing error correction methods are mostly algorithmic and heuristics. Few methods can address insertion and deletion errors, the dominant error type produced by many platforms. On the other hand, most base-callers do not model the underlying genome structures of the sequencing data, which are necessary for improving base-calling quality especially in low-quality regions. The sequential application of base-caller and error-corrector do not fully offset their shortcomings. In recognition of these issues, in this dissertation, we propose a probabilistic framework that closely emulate the sequencing-by-synthesis (SBS) process adopted by many NGS platforms. The core idea is to model sequencing data (individual reads, or fluorescent intensities) as independent emissions from a Hidden Markov model (HMM) with transition distributions to model local and double-stranded dependence in the genome, and emission distributions to model the subtle error characteristics of the sequencers. Deriving from this backbone, we develop three novel methods for improving the data quality of high-throughput sequencing: 1) PREMIER, an accurate probabilistic <b>error</b> <b>corrector</b> of substitution <b>errors</b> in Illumina data, 2) PREMIER-bc, an integrated base-caller and <b>error</b> <b>corrector</b> that significantly improves base-calling quality, and 3) PREMIER-indel, an extended error correction method that addresses substitution, insertion and deletion errors for SBS-based sequencers with good empirical performance. Our foray of using probabilistic methods for base-calling and error correction provides the immediate benefits to downstream analyses with increased sequencing data quality, and more importantly, a flexible and fully-probabilistic basis to go beyond primary analysis...|$|R
40|$|Two-Line Elements (TLEs) {{continue}} to be the sole public source of orbiter observations. The accuracy of TLE propagations through the Simplified General Perturbations- 4 (SGP 4) software decreases dramatically as the propagation horizon increases, and thus the period of validity of TLEs is very limited. As a result, TLEs are gradually becoming insufficient for the growing demands of Space Situational Awareness (SSA). We propose a technique, based on the hybrid propagation methodology, aimed at extending TLE validity with minimal changes to the current TLE-SGP 4 system in a non-intrusive way. It requires that the institution in possession of the osculating elements distributes hybrid TLEs, HTLEs, which encapsulate the standard TLE and the model of its propagation error. The validity extension can be accomplished when the end user processes HTLEs through the hybrid SGP 4 propagator, HSGP 4, which comprises the standard SGP 4 and an <b>error</b> <b>corrector.</b> Comment: 18 pages, 4 figure...|$|R
50|$|After demodulating, a CIRC <b>error</b> <b>corrector</b> takes each {{audio data}} frame, stores it in a SRAM memory and verifies {{that it has}} been read correctly, if it is not, it takes the parity and {{correction}} bits and fixes the data, then it moves it out to a DAC to be converted to an analog audio signal. If the data missing is enough to make recovery impossible, the correction is made by interpolating the data from subsequent frames so the missing part is not noticed. Each player has a different interpolation ability so if enough data frames are missing or unrecoverable, it may be impossible to fix by interpolation so an audio mute flag is raised to mute the DAC to avoid invalid data to be played back.The Redbook standard dictates that, if there is invalid, erroneous or missing audio data, it cannot be output to the speakers as digital noise, it has to be muted.|$|R
40|$|AbstractDesigns {{are derived}} for three error-handling {{functions}} in the compact disc player: the first decoder in the <b>error</b> <b>corrector,</b> the interpolator and the muter. All three functions are performed by autonomous processes, which interact with their environment by communication only. The designs are derived in a transformational way, {{which means that the}} ultimate design is derived by applying a series of correctness-preserving transformations on an initial design. Initial designs are fairly direct translations of functional specifications. A functional way of programming is applied. An automaton is conceived as a function from states to behaviours (processes), where states are equivalence classes of communication histories. A program then consists of two parts: a function definition of the automaton and an application of that function to the initial state. States are very useful in the verification of program transformations, since they facilitate the definition of a function mapping the states of the transformed automaton onto the states of the original one...|$|R
40|$|A central {{result that}} arose in {{applying}} information theory to the stochastic thermodynamics of nonlinear dynamical systems is the Information-Processing Second Law (IPSL) : the physical entropy {{of the universe}} can decrease if compensated by the Shannon-Kolmogorov-Sinai entropy change of appropriate information-carrying degrees of freedom. In particular, the asymptotic-rate IPSL precisely delineates the thermodynamic functioning of autonomous Maxwellian demons and information engines. How do these systems begin to function as engines, Landauer erasers, and <b>error</b> <b>correctors?</b> Here, we identify a minimal, inescapable transient dissipation engendered by physical information processing not captured by asymptotic rates, but critical to adaptive thermodynamic processes such as found in biological systems. A component of transient dissipation, we also identify an implementation-dependent cost that varies from one physical substrate to another for the same information processing task. Applying these results to producing structured patterns from a structureless information reservoir, we show that "retrodictive" generators achieve the minimal costs. The results establish the thermodynamic toll imposed by a physical system's structure as it comes to optimally transduce information. Comment: 8 pages, 2 figures, supplementary material; [URL]...|$|R
40|$|Empirical {{experience}} and observations have shown us when powerful and highly tunable classifiers such as maximum entropy classifiers, boosting and SVMs {{are applied to}} language processing tasks, {{it is possible to}} achieve high accuracies, but eventually their performances all tend to plateau out at around the same point. To further improve performance, various error correction mechanisms have been developed, but in practice, most of them cannot be relied on to predictably improve performance on unseen data; indeed, depending upon the test set, they are as likely to degrade accuracy as to improve it. This problem is especially severe if the base classifier has already been finely tuned. In recent work, we introduced N-fold Templated Piped Correction, or NTPC (“nitpick”), an intriguing <b>error</b> <b>corrector</b> that is designed to work in these extreme operating conditions. Despite its simplicity, it consistently and robustly improves the accuracy of existing highly accurate base models. This paper investigates some of the more surprising claims made by NTPC, and presents experiments supporting an Occam’s Razor argument that more complex models are damaging or unnecessary in practice. ...|$|R
40|$|This paper {{examines}} the short- and long-run daily relationships for a grain-energy nexus {{that includes the}} prices of corn, crude oil, ethanol, gasoline, soybeans, and sugar, and their open interest. The empirical results demonstrate {{the presence of these}} relationships in this nexus, and underscore the importance of ethanol and soybeans in all these relationships. In particular, ethanol and be considered as a catalyst in this nexus because of its significance as a loading factor, a long-run <b>error</b> <b>corrector</b> and a short-run adjuster. Ethanol leads all commodities in the price discovery process in the long run. The negative cross-price open interest effects suggest that there is a money outflow from all commodities in response to increases in open interest positions in the corn futures markets, indicating that active arbitrage activity takes place in those markets. On the other hand, an increase in the soybean open interest contributes to fund inflows in the corn futures market and the other futures markets, leading to more speculative activities in these markets. In connection with open interest, the ethanol market fails because of its thi...|$|R
40|$|This paper {{describes}} a robust architecture for high speed serial links for embedded SoC applications, implemented {{to satisfy the}} 1. 5 Gb/s and 3 Gb/s Serial-ATA PHY standards. To meet the primary design requirements of a sub-system that is very tolerant of device variability and is easy to port to smaller nanometre CMOS technologies, a minimum of precision analog functions are used. All digital functions are implemented in rail-to-rail CMOS with maximum use of synthesized library cells. A single fixed frequency low-jitter PLL serves the transmit and receive paths in both modes so that tracking and lock time issues are eliminated. A new oversampling CDR with a simple feed-forward error correction scheme is proposed which relaxes the requirements for the analog front-end {{as well as for}} the received signal quality. Measurements show that the <b>error</b> <b>corrector</b> can almost double the tolerance to incoming jitter and to DC offsets in the analog front-end. The design occupies less than 0. 4 mm 2 in 90 nm CMOS and consumes 75 m...|$|R
40|$|The {{objective}} {{of this paper is}} to report the results from the research being conducted in reconfigurable fight controls at NASA Ames. A study was conducted with three NASA Dryden test pilots to evaluate two approaches of reconfiguring an aircraft's control system when failures occur in the control surfaces and engine. NASA Ames is investigating both a Neural Generalized Predictive Control scheme and a Neural Network based Dynamic Inverse controller. This paper highlights the Predictive Control scheme where a simple augmentation to reduce zero steady-state error led to the neural network predictor model becoming redundant for the task. Instead of using a neural network predictor model, a nominal single point linear model was used and then augmented with an <b>error</b> <b>corrector.</b> This paper shows that the Generalized Predictive Controller and the Dynamic Inverse Neural Network controller perform equally well at reconfiguration, but with less rate requirements from the actuators. Also presented are the pilot ratings for each controller for various failure scenarios and two samples of the required control actuation during reconfiguration. Finally, the paper concludes by stepping through the Generalized Predictive Control's reconfiguration process for an elevator failure...|$|R
40|$|We {{describe}} {{a set of}} tools that interface to the Tracy particle tracking library. The state of the machine including misalignments, multipole <b>errors</b> and <b>corrector</b> settings is captured in a 'flat' file, or 'machine' file. There are three types of tools designed around this flat file: (1) flat file creation tools. (2) flat file manipulation tools. (3) tracking tools. We describe the status of these tools, and give some examples of how they {{have been used in}} the design process for NSLS-II...|$|R
40|$|Limited {{studies have}} been {{conducted}} on the effects of error correction on acquiring oral proficiency and the teacher’s role as <b>error</b> <b>corrector.</b> Thus, the present study aims to investigate English as a foreign language (EFL) teachers’ perceptions of error correction in their speaking classes, reasons and types of errors they correct and their error correction strategies. The sample group in the study consisted of 15 English instructors working at a state university in Turkey. The data collection instruments consisted of a background questionnaire, reflections, interviews and essay papers. Results showed that EFL teachers seem to make corrections to improve learners’ accuracy during speaking, grammar and vocabulary knowledge and pronunciation skills and that EFL teachers believe that error correction may contribute to habit formation in terms of self-correction among students, pragmatic and appropriate use of the target language, learners’ accuracy and fluency. Another conclusion was concluded that teachers concentrate on pronunciation, grammar and vocabulary errors that directly distort meaning while speaking, and that they seem to use various strategies to correct errors. It was recommended that the curricula of teacher training programs should include topics to raise awareness of the issues such as reasons to make corrections, situations that require corrections, error types and correction strategies...|$|R
40|$|RePEc Working Paper Series: No. 24 / 2011 This paper {{examines}} the short- and long-run daily relationships for a grain-energy nexus {{that includes the}} prices of corn, crude oil, ethanol, gasoline, soybeans, and sugar, and their open interest. The empirical results demonstrate {{the presence of these}} relationships in this nexus, and underscore the importance of ethanol and soybeans in all these relationships. In particular, ethanol and be considered as a catalyst in this nexus because of its significance as a loading factor, a long-run <b>error</b> <b>corrector</b> and a short-run adjuster. Ethanol leads all commodities in the price discovery process in the long run. The negative cross-price open interest effects suggest that there is a money outflow from all commodities in response to increases in open interest positions in the corn futures markets, indicating that active arbitrage activity takes place in those markets. On the other hand, an increase in the soybean open interest contributes to fund inflows in the corn futures market and the other futures markets, leading to more speculative activities in these markets. In connection with open interest, the ethanol market fails because of its thin market. Finally, {{it is interesting to note}} that the long-run equilibrium (cointegrating relationship), speeds of adjustment and open interest across markets have strengthened significantly during the 2009 - 2011 economic recovery period, compared with the full and 2007 - 2009 Great Recession periods...|$|R
40|$|In computing, spell {{checking}} is {{the process}} of detecting and sometimes providing spelling suggestions for incorrectly spelled words in a text. Basically, a spell checker is a computer program that uses a dictionary of words to perform spell checking. The bigger the dictionary is, the higher is the error detection rate. The fact that spell checkers are based on regular dictionaries, they suffer from data sparseness problem as they cannot capture large vocabulary of words including proper names, domain-specific terms, technical jargons, special acronyms, and terminologies. As a result, they exhibit low error detection rate and often fail to catch major errors in the text. This paper proposes a new context-sensitive spelling correction method for detecting and correcting non-word and real-word errors in digital text documents. The approach hinges around data statistics from Google Web 1 T 5 -gram data set which consists of a big volume of n-gram word sequences, extracted from the World Wide Web. Fundamentally, the proposed method comprises an error detector that detects misspellings, a candidate spellings generator based on a character 2 -gram model that generates correction suggestions, and an <b>error</b> <b>corrector</b> that performs contextual error correction. Experiments conducted on a set of text documents from different domains and containing misspellings, showed an outstanding spelling error correction rate and a drastic reduction of both non-word and real-word errors. In a further study, the proposed algorithm is to be parallelized so as to lower th...|$|R
40|$|The {{traditional}} {{approaches to}} decoding shortened block codes are the padding of received codeword with zeros or correction of the syndrome polynomial values. Such approaches make stream-oriented decoding of continuous input data stream impossible. The paper {{offers a new}} technique for decoding the shortened Reed-Solomon codes. Its hardware implementation allows us to provide stream processing, reduce latency in clock cycles, and decrease a required quantity of hardware resources as compared to decoder implementation with padding of received packet with zeros. A distinctive feature of the proposed technique is that the decoder processes a stream of code words of different lengths without changing their structure and insertion of additional delays {{and that it is}} possible to use the modules of existing Reed-Solomon decoders for full codeword length. To enable this, a notion of <b>error</b> locator <b>corrector</b> for shortened code is introduced and a technique of their calculation is offered. <b>Error</b> locator <b>correctors</b> are calculated during the reception of a codeword in parallel to syndrome polynomial calculation. Their values allow us to modify the polynomial values of locators and errors at the output of the key equation solver. The paper considers a shortened code decoder that is implemented on the full code decoder modules based on Berlekamp-Massey algorithm, describes architecture of additional modules and required modifications of the algorithm. It shows the way to save hardware resources by using the multipliers in Berlekemp-Massey key equation solver to correct values. The Altera FPGA-based decoder has been tested in and compared to the Reed-Solomon II decoder IP from Altera...|$|R
40|$|Current {{approaches}} to fault-tolerant quantum computation will not enable useful quantum computation on near-term devices of 50 to 100 qubits. Leading proposals, {{such as the}} color code and surface code schemes, must devote a large fraction of their physical quantum bits to quantum error correction. Building from recent quantum machine learning techniques, we propose an alternative approach to quantum error correction aimed at reducing this overhead, which can be implemented in existing quantum hardware and on a myriad of quantum computing architectures. This method aims to optimize the average fidelity of encoding and recovery circuits {{with respect to the}} actual noise in the device, as opposed to that of an artificial or approximate noise model. The quantum variational <b>error</b> <b>corrector</b> (QVECTOR) algorithm employs a quantum circuit with parameters that are variationally-optimized according to processed data originating from quantum sampling of the device, so as to learn encoding and error-recovery gate sequences. We develop this approach for the task of preserving quantum memory and analyze its performance with simulations. We find that, subject to phase damping noise, the simulated QVECTOR algorithm learns a three-qubit encoding and recovery which extend the effective T 2 of a quantum memory six-fold. Subject to a continuous-time amplitude- plus phase-damping noise model on five qubits, the simulated QVECTOR algorithm learns encoding and decoding circuits which exploit the coherence among Pauli errors in the noise model to outperform the five-qubit stabilizer code and any other scheme that does not leverage such coherence. Both of these schemes can be implemented with existing hardware. Comment: 11 + 5 pages, 5 figures, 2 table...|$|R
500|$|While {{working on}} the translation, Wright died in late November 1615 and was buried on 2 December 1615 at St. Dionis Backchurch (now demolished) in the City of London. The Caius annals noted that {{although}} he [...] "was rich in fame, and in the promises of the great, yet he died poor, to the scandal of an ungrateful age". Wright's translation of Napier, which incorporated tables that Wright had supplemented and further information by Henry Briggs, was completed by Wright's son Samuel and arranged to be printed by Briggs. It appeared posthumously as A Description of the Admirable Table of Logarithmes in 1616, and in it Wright was lauded in verse as [...] "hat famous, learned, <b>Errors</b> true <b>Corrector,</b> / England's great Pilot, Mariners Director".|$|R
40|$|International audienceThis paper {{presents}} a new cross-layer (PHY/MAC) resource allocation {{scheme based on}} Multiple Input Multiple Output (MIMO) and Orthogonal Frequency Division Multiple Access (OFDMA) system with a multi-service (MS) and a multiuser (MU) configuration. It is about a downlink transmission chain based on the IEEE 802. 16 m specifications, in which the available resources (power and bandwidth) are dynamically allocated according to the system parameters such as Channel State Information (CSI), spectral efficiency, <b>error</b> code <b>corrector</b> rate, Quality of Service (QoS) requirements and services scheduling. The specificity of our system is the joint parameterization of these elements according to the total power and threshold rate constraints in order to guarantee a good trade-off between users QoS requirements and sub-carriers distribution. Simulation {{results show that the}} proposed scheme offers better performances in terms of average throughput and users satisfaction than previous resource allocation schemes...|$|R
40|$|Illumina Sequencing {{data can}} provide high {{coverage}} of a genome by relatively short (most often 100 bp to 150 bp) reads at a low cost. Even with low (advertised 1 %) error rate, 100 × coverage Illumina data on average has an error in some read at every base in the genome. These errors make handling the data more complicated because they result in {{a large number of}} low-count erroneous k-mers in the reads. However, there is enough information in the reads to correct most of the sequencing errors, thus making subsequent use of the data (e. g. for mapping or assembly) easier. Here we use the term "error correction" to denote the reduction in errors due to both changes in individual bases and trimming of unusable sequence. We developed an error correction software called QuorUM. QuorUM is mainly aimed at error correcting Illumina reads for subsequent assembly. It is designed around the novel idea of minimizing the number of distinct erroneous k-mers in the output reads and preserving the most true k-mers, and we introduce a composite statistic π that measures how successful we are at achieving this dual goal. We evaluate the performance of QuorUM by correcting actual Illumina reads from genomes for which a reference assembly is available. We produce trimmed and error-corrected reads that result in assemblies with longer contigs and fewer errors. We compared QuorUM against several published <b>error</b> <b>correctors</b> and found that it is the best performer in most metrics we use. QuorUM is efficiently implemented making use of current multi-core computing architectures and it is suitable for large data sets (1 billion bases checked and corrected per day per core). We also demonstrate that a third-party assembler (SOAPdenovo) benefits significantly from using QuorUM error-corrected reads. QuorUM error corrected reads result in a factor of 1. 1 to 4 improvement in N 50 contig size compared to using the original reads with SOAPdenovo for the data sets investigated. QuorUM is distributed as an independent software package and as a module of the MaSuRCA assembly software. Both are available under the GPL open source license at [URL]...|$|R
