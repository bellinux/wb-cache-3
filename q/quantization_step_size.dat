88|6240|Public
5000|$|As an example, {{rounding}} a {{real number}} [...] {{to the nearest}} integer value forms a very basic type of quantizer - a uniform one. A typical (mid-tread) uniform quantizer with a <b>quantization</b> <b>step</b> <b>size</b> equal to some value [...] can be expressed as ...|$|E
5000|$|JPEG uses {{a single}} <b>quantization</b> <b>step</b> <b>size</b> per DC/AC {{component}} per color plane per image. JPEG XR allows {{a selection of}} DC quantization step sizes on a tile region basis, and allows lowpass and AC quantization step sizes to vary from macroblock to macroblock.|$|E
5000|$|When the <b>quantization</b> <b>step</b> <b>size</b> {{is small}} (relative to the {{variation}} in the signal being measured), it is relatively simple {{to show that the}} mean squared error produced by such a rounding operation will be approximately [...] Mean squared error is also called the quantization noise power. Adding one bit to the quantizer halves the value of Δ, which reduces the noise power by the factor ¼. In terms of decibels, the noise power change is ...|$|E
40|$|Image sizes have {{increased}} exponentially in recent years. The resulting high-resolution images are often viewed via remote image browsing. Zooming and panning are desirable features in this context, which result in disparate spatial regions {{of an image}} being displayed {{at a variety of}} (spatial) resolutions. When an image is displayed at a reduced resolution, the <b>quantization</b> <b>step</b> <b>sizes</b> needed for visually lossless quality generally increase. This paper investigates the <b>quantization</b> <b>step</b> <b>sizes</b> needed for visually lossless display as a function of resolution, and proposes a method that effectively incorporates the resulting (multiple) <b>quantization</b> <b>step</b> <b>sizes</b> into a single JPEG 2000 codestream. This codestream is JPEG 2000 Part 1 compliant and allows for visually lossless decoding at all resolutions natively supported by the wavelet transform as well as arbitrary intermediate resolutions, using {{only a fraction of the}} full-resolution codestream. When images are browsed remotely using the JPEG 2000 Interactive Protocol (JPIP), the required bandwidth is significantly reduced, as demonstrated by extensive experimental results...|$|R
30|$|The {{freedom to}} choose <b>quantization</b> <b>step</b> <b>sizes</b> allows for a {{trade-off}} between reconstruction performance and leakage [4]. The alternating labeling was adopted to reduce leakage but sacrifices {{a large part of}} the source’s entropy.|$|R
40|$|ITC/USA 2010 Conference Proceedings / The Forty-Sixth Annual International Telemetering Conference and Technical Exhibition / October 25 - 28, 2010 / Town and Country Resort & Convention Center, San Diego, CaliforniaAerial image {{collections}} {{have experienced}} exponential growth in size in recent years. These high resolution images are often viewed {{at a variety}} of scales. When an image is displayed at reduced scale, maximum <b>quantization</b> <b>step</b> <b>sizes</b> for visually lossless quality become larger. However, previous visually lossless coding algorithms quantize the image with a single set of <b>quantization</b> <b>step</b> <b>sizes,</b> optimized for display at the full resolution level. This implies that if the image is rendered at reduced resolution, there are significant amounts of extraneous information in the codestream. Thus, in this paper, we propose a method which effectively incorporates multiple <b>quantization</b> <b>step</b> <b>sizes,</b> for various display resolutions, into the JPEG 2000 framework. If images are browsed from a remote location, this method can significantly reduce bandwidth usage by only transmitting the portion of the codestream required for visually lossless reconstruction at the desired resolution. Experimental results for high resolution color aerial images are presented...|$|R
50|$|In {{contrast}} with older MPEG-1/2/4 standards, the H.264 deblocking filter {{is not an}} optional additional feature in the decoder. It is a feature on both the decoding path and on the encoding path, so that the in-loop effects of the filter {{are taken into account}} in reference macroblocks used for prediction. When a stream is encoded, the filter strength can be selected, or the filter can be switched off entirely. Otherwise, the filter strength is determined by coding modes of adjacent blocks, <b>quantization</b> <b>step</b> <b>size,</b> and the steepness of the luminance gradient between blocks.|$|E
50|$|The filter {{operates}} {{on the edges}} of each 4×4 or 8×8 transform block in the luma and chroma planes of each picture. Each small block's edge is assigned a boundary strength based on whether it is also a macroblock boundary, the coding (intra/inter) of the blocks, whether references (in motion prediction and reference frame choice) differ, and whether it is a luma or chroma edge. Stronger levels of filtering are assigned by this scheme where there is likely to be more distortion. The filter can modify as many as three samples on either side of a given block edge (in the case where an edge is a luma edge that lies between different macroblocks and {{at least one of them}} is intra coded). In most cases it can modify one or two samples on either side of the edge (depending on the <b>quantization</b> <b>step</b> <b>size,</b> the tuning of the filter strength by the encoder, the result of an edge detection test, and other factors).|$|E
5000|$|Often {{the design}} of a {{quantizer}} involves supporting only a limited range of possible output values and performing clipping to limit the output to this range whenever the input exceeds the supported range. The error introduced by this clipping is referred to as overload distortion. Within the extreme limits of the supported range, the amount of spacing between the selectable output values of a quantizer is referred to as its granularity, and the error introduced by this spacing is referred to as granular distortion. It is common for {{the design of}} a quantizer to involve determining the proper balance between granular distortion and overload distortion. For a given supported number of possible output values, reducing the average granular distortion may involve increasing the average overload distortion, and vice versa. A technique for controlling the amplitude of the signal (or, equivalently, the <b>quantization</b> <b>step</b> <b>size</b> [...] ) to achieve the appropriate balance is the use of automatic gain control (AGC). However, in some quantizer designs, the concepts of granular error and overload error may not apply (e.g., for a quantizer with a limited range of input data or with a countably infinite set of selectable output values).|$|E
30|$|The KFs are decoded first, {{using an}} H. 264 /AVC decoder, obtaining I c,t − 1 and I c,t + 1. In addition, the key frame quality should match {{the quality of}} the reconstructed WZ frame on average. Thus, to avoid quality {{fluctuations}} appropriate <b>quantization</b> <b>step</b> <b>sizes</b> for the WZ and KF DCT coefficients must be selected.|$|R
40|$|Transform coding is {{routinely}} used for lossy compression of discrete sources with memory. The input signal {{is divided into}} N-dimensional vectors, which are transformed {{by means of a}} linear mapping. Then, transform coefficients are quantized and entropy coded. In this paper we consider the problem of identifying the transform matrix as well as the <b>quantization</b> <b>step</b> <b>sizes.</b> We study the challenging case in which the only available information is a set of P transform decoded vectors. We formulate the problem in terms of finding the lattice with the largest determinant that contains all observed vectors. We propose an algorithm that is able to find the optimal solution and we formally study its convergence properties. Our analysis shows {{that it is possible to}} identify successfully both the transform and the <b>quantization</b> <b>step</b> <b>sizes</b> when P >= N + d where d is a small integer, and the probability of failure decreases exponentially to zero as P - N increases. Comment: Submitted to IEEE Transactions on Information Theor...|$|R
40|$|In this paper, {{we study}} the best rate {{distortion}} performance that an H. 264 encoder can possibly achieve. Using soft decision quantization {{rather than the}} standard hard decision quantization, we first establish a general framework for jointly designing motion compensation, quantization, and entropy coding in the hybrid coding structure of H. 264 to minimize a true rate distortion cost. We then propose three rate distortion optimization algorithms—a graph-based algorithm for optimal soft decision quantization in H. 264 baseline profile encoding given motion compensation and <b>quantization</b> <b>step</b> <b>sizes,</b> an iterative algorithm for optimal residual coding in H. 264 baseline profile encoding given motion compensation, and an iterative overall algorithm for optimal H. 264 baseline profile encoding—with them embedded in the indicated order. The graph-based algorithm for optimal soft decision quantization is the core; given motion compensation and <b>quantization</b> <b>step</b> <b>sizes,</b> it is guaranteed to perform optimal soft decision quantization to certain degree. The proposed iterative overall algorithm has been implemented based on the reference encoder JM 82 of H. 264. Comparative studies show that it achieves a significant performance gain, which can {{be as high as}} 25 % rate reduction at the same PSNR when compared to the reference encoder...|$|R
30|$|For the JPEG {{compression}} standard, {{we can use}} {{the visual}} model to calculate the <b>quantization</b> <b>step</b> <b>size.</b> The quantization error of the quantized DCT coefficient is within JND. Watson have presented optimal JPEG <b>quantization</b> <b>step</b> <b>size.</b>|$|E
40|$|Rate control {{plays an}} {{important}} role in regulating the en-coding bit rate to meet the bandwidth and storage require-ment. Most existing works regulate the bit rate by adjusting the <b>quantization</b> <b>step</b> <b>size.</b> We propose to incorporate a new dimension: the quantization rounding offset into a rate control algorithm. Based on our previous work of multi-pass fine rate control, in this work, we present a unified one-pass rate con-trol algorithm that jointly adjusts the <b>quantization</b> <b>step</b> <b>size</b> and the rounding offset for high bit rate accuracy. Unlike the <b>quantization</b> <b>step</b> <b>size</b> that has a limited number of choices, the rounding offset is a continuously adjustable variable that allows the rate control algorithm to reach any precision in principle. Our extensive experimental results show that the proposed algorithm greatly improves the rate control accu-racy at almost no extra computational complexity. Index Terms — Video coding, rate control, quantization, rounding offse...|$|E
3000|$|... [...]), {{and used}} to form a block of symbols that is passed onto the Turbo encoder. The <b>quantization</b> <b>step</b> <b>size</b> for {{processing}} the transform coefficients is therefore [...]...|$|E
30|$|Refine {{quantization}} In scalar <b>quantization,</b> the <b>step</b> <b>size</b> {{is lower}} bounded by the correlation noise N, which {{is crucial to}} avoid decoding errors (Fig.  6). Depending on the target reconstruction quality, <b>step</b> <b>size</b> is chosen to be fine or coarse. Therefore, the refined <b>quantization</b> <b>step</b> is applied to coefficients to fine-tune codewords such that the desired target quality is achieved.|$|R
30|$|In this section, {{we provide}} {{simulation}} {{studies of the}} proposed coder. We simulate an environment with packet losses of 0.1 %, 1 %, and 10 %. We restrict the <b>quantization</b> <b>step</b> <b>sizes</b> to Δ∈{ 0.01, 0.05 }, the block size upon which the predictor is used to { 64, 128, 256, 512, 1024, 2048 }, and the LPC filter order to plpc∈{ 5, 10 }. Finally, in all simulations, the low-pass filters used for resampling are of order 200, the noise-shaping filter is of order 10, and the noise-shaping ratio λ= 0.01.|$|R
40|$|Bit rate {{adaptation}} {{is one of}} the most important types of video transcoding. With H. 264 becoming the predominant video codec of choice in video coding and streaming, prudent rateadaptation techniques should be developed. In this paper, we investigate certain critical points in the spectrum of rate shaping requests. We show that the selection of <b>quantization</b> <b>step</b> <b>sizes</b> may not have monotonic effects on rate-distortion characteristics in the transcoding sense. In other words, distortion in the regular sense can be different from the distortion in the context of transcoding in which a requantization process is carried out. We show in a generic form that careful selections of the <b>step</b> <b>size</b> can lead to much improved performance. Experiments based on both simulation and real transcoding show the effectiveness of the proposed solution. 1...|$|R
40|$|Abstract — The {{rate control}} {{algorithm}} is of essential importance to a video encoder. It enables the encoded bitstream {{to meet the}} bandwidth and storage requirement while maintaining good video quality. Most existing works adjust the <b>quantization</b> <b>step</b> <b>size</b> to achieve the required bit rate accuracy. This paper introduces a new dimension: the quantization rounding offset into a frame-level fine rate control algorithm. Specifically, we propose a novel fine rate control algorithm based on a linear model between the bit rate and the rounding offset. Unlike the <b>quantization</b> <b>step</b> <b>size</b> that has {{a limited number of}} choices, the quantization rounding offset is a continuously adjustable variable which allows the rate control algorithm to reach any precision in principle. Extensive experiment results show that the proposed algorithm greatly improves the bit rate accuracy and provides better visual quality by fine tuning of the rounding offset in addition to the <b>quantization</b> <b>step</b> <b>size.</b> I...|$|E
40|$|This paper envisages a {{cellular}} {{system based on}} code-division multiple access and investigates {{the performance of a}} strength-based closed-loop power control (CLPC) scheme on the basis of different parameters, such as the number of bits of the power command, the <b>quantization</b> <b>step</b> <b>size,</b> and the user speed. On the basis of a log-linear CLPC model, an analytical approach has been developed that has allowed to determine the optimum <b>quantization</b> <b>step</b> <b>size</b> to be used for each value of the number of power command bits. Simulation results have permitted to support the analytical framework developed in this paper...|$|E
3000|$|... [...]. We only {{consider}} the <b>quantization</b> <b>step</b> <b>size</b> in powers of two {{so that the}} quantization process can be performed in the encrypted domain. First, we use the secure bit extraction routine EXTRACT to compute the binary representation of [...]...|$|E
40|$|Abstract—The paper {{presents}} a non-linear quantization method for detail {{components in the}} JPEG 2000 standard. The <b>quantization</b> <b>step</b> <b>sizes</b> are determined by actual statistics of the wavelet coefficients. Mean and standard deviation are the two statistical parameters used to obtain the <b>step</b> <b>sizes.</b> Moreover, weighted mean of the coefficients lying within the <b>step</b> <b>size</b> is chosen as quantized value- contrary to the uniform quantizer. The empirical results are compared using mean squared error of uniformly quantized and non-linearly quantized wavelet co-efficients w. r. t. original wavelet coefficients. Through empirical results, it has been concluded that for low bit rate compression, the quantization error introduced by uniform quantizer is {{higher than that of}} non-linear quantizer, and thus suggesting the use of non-linear quantizer for low bit rate lossy compression. Index Terms—Mean and standard deviation, quantization, discrete wavelet transform, image compression, image processing, JPEG 2000 standard. I...|$|R
40|$|We propose an {{improved}} multiresolution two-layer video codec that can operate at a base layer of 64 kbps and a {{variable bit rate}} second layer (VBR). The basic idea {{is to use a}} multiresolution technique to reduce the original CIF format video to the QCIF format. The low-resolution video is then encoded with {{an improved}} H. 261 codec. To reduce the bit rate of the second layer, the residuals are encoded with the discrete cosine transform (DCT) using perceptually weighted <b>quantization</b> <b>step</b> <b>sizes.</b> Simulation results show that the Miss America sequence can be transmitted at 64 kbps in the CIF format with a reasonably good picture quality. For a good picture quality, the second layer would typically require 190 kbps. published_or_final_versio...|$|R
30|$|The {{experimental}} results {{also show that}} the PSNR improvement obtained from the set of low-bit rate inputs is {{lower than that of}} the high-bit rate set. This can be explained as at low-bit rate range, coarse <b>quantization</b> <b>step</b> <b>sizes</b> are generally used for encoding, resulting in a large QCS for each integer transform coefficient. Furthermore, the QCSs of the low-quality copies (e.g., copies 1 and 2 in Set 1) do not contribute much in reducing the size of the narrow QCS obtained by the proposed method. This is because the <b>quantization</b> <b>step</b> <b>sizes</b> used in these copies are generally too large compared to that of the best copy. As a result, the size of the narrow QCS cannot be significantly reduced, and hence it usually remains {{the same as that of}} the best copy. Thus, not much quality improvement compared to the best copy can be obtained (see the results of Set 1 in Tables 4 and 5 and discussion in Section 5). Furthermore, we can generally obtain better PSNR gain in the case where the similar frames from the available copies are coded using different picture coding types (Case 2 in Table 5) compared with that of using the same picture coding types (Case 1 in Table 4). Note that the size of the narrow QCS depends not only on the relation among the sizes of QCSs from multiple compressed copies, but also the relative position of the QCSs' intervals. As explained in Section 5, this relative position of each independent QCS is partly determined by the prediction value, which can be much different when different picture types are used to code similar frames from the available copies. This will help to reduce the size of the narrow QCS obtained by the proposed method significantly, resulting in more distortion reduction.|$|R
40|$|Abstract: Traditional Spread Transform Dither Modulation (ST-DM) {{is based}} on the fixed <b>quantization</b> <b>step</b> <b>size,</b> which does not exploit the {{features}} of the video contents. Spatio-temporal masking function represents the visual threshold under which human eyes are usually not sensitive. In this paper, we adopts not only the spatial contrast sensitivity function (CSF), the luminance masking function, the contrast masking function but also the spatio-temporal masking function to account for the model of human eyes to the video. we propose using spatio-temporal perceptual model to adaptively select the <b>quantization</b> <b>step</b> <b>size</b> based on the calculated masking function. Experimental results show that the proposed scheme guarantees high peak signal to noise ratio (PSNR). Furthermore, compared with the scheme with fixed step size, the proposed scheme is more robust against temporal frame averaging (TFA) attack...|$|E
40|$|The {{existing}} {{rate control}} schemes {{in the literature}} calculate quantization parameters of macro-blocks (MB) based on the quadratic/log rate models. Actually, this model may not be good because this model does not estimate well at some particular bit rates. This work investigates the relationships of bit rate and distortion with MB <b>quantization</b> <b>step</b> <b>size</b> and standard deviation of MB prediction errors. We find that linear rate and linear distortion models are established. Then we present the proposed rate control schemes based on these models. The proposed algorithm introduces a low quantization overhead, low computation complexity to calculate the optimal <b>quantization</b> <b>step</b> <b>size,</b> independence of the distortion factors and low memory usage compared with TMN 8 rate control. The experimental results suggest that our scheme can achieve PSNR gain over TMN 8...|$|E
40|$|Abstract — Multiplierless {{filtering}} is {{very attractive}} for digital signal processing, for the coefficient multiplier {{is the most}} complex and the slowest component. An alternative way to achieve multiplierless filtering other than designing digital filters with signed power-of-two (SPT) coefficient values is to round each input data to a sum of {{a limited number of}} SPT terms. However, a roundoff noise representing the roundoff error is introduced when signal data are rounded to SPT numbers. In the SPT space, the <b>quantization</b> <b>step</b> <b>size</b> is nonuniform and thus the roundoff noise characteristic is different from that produced when the <b>quantization</b> <b>step</b> <b>size</b> is uniform. This paper presents an analysis for the roundoff noise of signal represented using a limited number of SPT terms. Roundoff errors for Gaussian distributed inputs are estimated by using our analysis. Examples show that the estimated errors are very close to the actual ones. I...|$|E
3000|$|... where δstep is the <b>step</b> <b>size</b> of the quantizer, {{which may}} be {{adjusted}} individually for each channel component. If the <b>quantization</b> granularity (<b>step</b> <b>size)</b> is individually controlled by the standard deviation of the prediction error, then the quantization error term in (2) can be kept small relative to the prediction error term in an efficient way. The quantization errors would then have negligible impact on the performance metric.|$|R
40|$|Digital image {{compression}} algorithms {{have become increasingly}} popular due {{to the need to}} achieve costeffective solutions in transmitting and storing images. In order to meet various transmission and storage requirements, the compression algorithm should allow a range of compression ratios, thus providing images of different visual quality. This paper presents a modified JPEG algorithm that provides better visual quality than the Q-factor scaling method commonly used with JPEG implementations. The <b>quantization</b> <b>step</b> <b>sizes</b> are adapted to the activity level of the block, and the activity selection is based on an edge-driven quadtree decomposition of the image. This technique achieves higher visual quality than standard JPEG compression at the same bit rate. Keywords: JPEG, edge-adaptive quantization, quadtree decomposition, activity selection 1. INTRODUCTION The recent popularity of personal computing has brought about a revolution in the way information is transmitted and stored. With t [...] ...|$|R
40|$|We use {{multiple}} bits {{to improve the}} accuracy in encoding the relationships between paired transform coefficients. We use non-uniform quantizers to explore the non-linear mapping between the coefficient differences in the original image and the compressed image. After thoroughly analyzing the properties of different distortions caused by acceptable (e. g., lossy compression) and unacceptable (e. g. copy-paste) manipulations, we use a non-uniform quantizer to generate the raw signature, which is comprised of several bits instead of one single bit, from the difference between two wavelet coefficients. The non-uniform quantization consists of two steps. First, we use different <b>quantization</b> <b>step</b> <b>sizes</b> to quantize {{the difference between these}} two selected coefficients according to the magnitude of the difference. Second, we assign different <b>quantization</b> <b>step</b> <b>sizes</b> for the signature generator and the signature verifier which is guided by the observations on the difference of distortion properties from different manipulations especially on lossy compression. Obviously these raw signatures require much more capacities for signature storage as well as for watermark embedding. Therefore, for this method, we also propose a codeword table-based solution to shorten the length of the generated signature. 2. PREVIOUS WORK ON THE DCT-BASED SEMI-FRAGILE WARERMARKING In this section, we review the main approach to achieve the semi-fragile watermarking proposed by Lin and Chang [1]. This unique semi-fragile authentication watermarking technique is well recognized for its capability of providing deterministic guarantee of zero false alarm rate and statistical guarantee of miss rate in distinguishing malicious attacks from JPEG lossy compression. The authors have deployed a popular software that’s freely downloadable and available for testing from an online web site [7]. When an image is compressed with JPEG, its image pixels are transformed to DCT coefficients, and then quantized. Lin and Chang found that the magnitude relationship between two coefficients remains invariable through repetitive JPEG compression. They demonstrated that semi-fragile image authentication for JPEG is feasible using this property [1]. In their method, the authenticity of the image is verified by 1 -bit signature bit which represents the magnitude relationship between two DCT coefficients...|$|R
30|$|The {{conventional}} QIM has {{a serious}} drawback, i.e., the weakness against valumetric scaling. Spherical codes were utilized {{to cope with this}} problem in [8]. However, watermark embedding and recovery get very complicated [9]. Oostveen et al. [10] proposed to choose the adaptive <b>quantization</b> <b>step</b> <b>size</b> to be proportional to a local average of the host signal samples. Despite its robustness against valumetric scaling, the method presents a nonzero probability of error even for null distortions and becomes more sensitive to constant change. Rational dithered modulation (RDM) was developed in [9] using a gain-invariant adaptive <b>quantization</b> <b>step</b> <b>size</b> at both embedder and decoder. The RDM achieves invariance to valumetric scaling, but is still sensitive to constant change. Li and Cox [11] applied the modified Watson's perceptual model to provide resistance to valumetric scaling for QIM watermarking. The modification to Watson's model results in the degradation in quality and performance loss with respect to constant change.|$|E
3000|$|Since each integer {{transform}} coefficient is quantized independently by a <b>quantization</b> <b>step</b> <b>size,</b> minimizing the MSE {{subject to the}} constraints specified in (9) is equivalent to minimizing the distortion caused by each integer {{transform coefficient}}. In {{order to have a}} quantization that better fits to a nonuniform distribution of the integer transform coefficient over the QCS, H. 264 /AVC decoder uses the rounding control parameter [...]...|$|E
30|$|To {{account for}} this, we propose {{to use a}} method to {{automatically}} adapt the <b>quantization</b> <b>step</b> <b>size</b> at each sample. First, the step value {{is controlled by the}} wavelet coefficients that measure the spatial local activity. Second, the watermark insertion process is based upon moving one of the three color vectors (R, G, and B). A better candidate is defined in order to minimize the distortion at each insertion space.|$|E
40|$|We {{propose a}} method of {{reducing}} the computational complexity of iterative video post-processing algorithms that {{make use of the}} convex quantization constraint set. Rather than constraining all DCT coefficients in a block, this method constrains only a subset of a block's DCT coefficients, and hence eliminates the need to evaluate all coefficients. 1. INTRODUCTION There are numerous video post-processing algorithms that attempt to remove or alleviate compression artifacts from the sequence prior to viewing. Many iterative techniques have been shown to do an excellent job of removing these artifacts; an underlying property of these algorithms is that the image estimate is improving with each iteration. However, the advantages of iterative techniques can be offset by the computational requirements of performing perhaps many iterations. Many iterative techniques of post-processing make use of a quantization constraint set, which is determined from the <b>quantization</b> <b>step</b> <b>sizes.</b> One example [...] ...|$|R
40|$|The {{widespread}} {{popularity of}} transform coding {{has made it}} central {{to a wide range}} of methods in forensics, quality assessment and digital restoration. However, most approaches require prior knowledge of the transform coding parameters. In this paper, we consider the challenging problem of identifying the transform matrix as well as the <b>quantization</b> <b>step</b> <b>sizes</b> of a transform coder, given a set of P non-overlapping N-dimensional vectors observed as its output. We formulate the problem in terms of finding the lattice with the largest determinant that contains all observed vectors and we propose an algorithm that is able to find the optimal solution. Our experimental analysis shows that the probability of success of the algorithm quickly approaches 1 for small values of (P − N). The complexity of the proposed algorithm grows linearly with the dimensionality N. Index Terms — Transform coding, lattice theory. 1...|$|R
40|$|A {{method is}} {{described}} {{for reducing the}} visibility of artifacts arising in the display of quantized color images on CRT displays. The method {{is based on the}} differential spatial sensitivity of the human visual system to chromatic and achromatic modulations. Because the visual system has the highest spatial and temporal acuity for the luminance component of an image, a technique which will reduce luminance artifacts at the expense of introducing high-frequency chromatic errors is sought. A method based on controlling the correlations between the quantization errors in the individual phosphor images is explored. The luminance component is greatest when the phosphor errors are positively correlated, and is minimized when the phosphor errors are negatively correlated. The greatest effect of the correlation is obtained when the intensity <b>quantization</b> <b>step</b> <b>sizes</b> of the individual phosphors have equal luminances. For the ordered dither algorithm, a version of the method can be implemented by simply inverting the matrix of thresholds for one of the color components...|$|R
