0|153|Public
50|$|The {{aircraft}} <b>quantities</b> are <b>approximate,</b> {{and estimated}} to be 68 airplanes and 54 helicopters.|$|R
50|$|Always {{involves}} the measurement and pricing of <b>approximate</b> <b>quantities</b> at some {{stage of the}} process.|$|R
5000|$|The <b>approximate</b> <b>quantity</b> of {{caryophyllene}} in {{the essential}} oil of each source {{is given in}} square brackets (...) : ...|$|R
25|$|The <b>approximate</b> <b>quantity</b> and guarded {{wording of}} the UN {{statement}} indicated a voter turnout of no higher than 33%.|$|R
30|$|A {{fuzzy set}} A in X {{is said to}} be an <b>approximate</b> <b>quantity</b> if and only if A_α is compact and convex in X for each α∈ (0, 1] and _x∈ XA(x)= 1. We denote by W(X) the family of all <b>approximate</b> <b>quantities</b> in X. Let A, B∈ W(X), then A {{is said to be}} more {{accurate}} A than B, denoted by A⊂ B, if and only if A(x)≤ B(x) for each x∈ X.|$|R
3000|$|... in a metric space X respectively. This {{result is}} {{significant}} {{as it does}} not require the condition of <b>approximate</b> <b>quantity</b> for [...]...|$|R
40|$|A dedicated, non-symbolic, system {{yielding}} imprecise {{representations of}} large <b>quantities</b> (<b>Approximate</b> Number System, or ANS) {{has been shown}} to support arithmetic calculations of addition and subtraction. In the present study, 5 – 7 -year-old children without formal schooling in multiplication and division were given a task requiring a scalar transformation of large approximate numerosities, presented as arrays of objects. In different conditions, the required calculation was doubling, quadrupling, or increasing by a fractional factor (2. 5). In all conditions, participants were able to represent the outcome of the transformation at above-chance levels, even on the earliest training trials. Their performance could not be explained by processes of repeated addition, and it showed the critical ratio signature of the ANS. These findings provide evidence for an untrained, intuitive process of calculating multiplicative numerical relationships, providing a further foundation for formal arithmetic instruction...|$|R
40|$|Metric {{estimates}} are <b>quantities</b> that <b>approximate</b> the word metric of a finitely presented group up to multiplicative constants. In this paper, they are computed for some nilpotent groups {{and used to}} compute the distortion functions of several embeddings between them. Comment: The paper has been withdrawn because of substantial coincidence with a previously published pape...|$|R
3000|$|... be {{a family}} of all <b>approximate</b> <b>quantities</b> in X. A fuzzy set A {{is said to be}} more {{accurate}} than a fuzzy set B denoted by [...]...|$|R
40|$|Since the Lucas-Kanade {{algorithm}} {{was proposed}} in 1981 image alignment {{has become one of}} the most widely used techniques in computer vision. Applications range from optical flow and tracking to layered motion, mosaic construction, and face coding. Numerous algorithms have been proposed and a wide variety of extensions have been made to the original formulation. We present an overview of image alignment, describing most of the algorithms and their extensions in a consistent framework. We concentrate on the inverse compositional algorithm, an efficient algorithm that we recently proposed. We examine which of the extensions to Lucas-Kanade can be used with the inverse compositional algorithm without any significant loss of efficiency, and which cannot. In this paper, Part 1 in a series of papers, we cover the <b>quantity</b> <b>approximated,</b> the warp update rule, and the gradient descent approximation. In future papers, we will cover the choice of the error function, how to allow linear appearance variation, and how to impose priors on the parameters...|$|R
25|$|Approximate FFTs: For {{applications}} such as MRI, {{it is necessary to}} compute DFTs for nonuniformly spaced grid points and/or frequencies. Multipole based approaches can compute <b>approximate</b> <b>quantities</b> with factor of runtime increase.|$|R
40|$|In {{this paper}} we {{consider}} a weighted harmonic mean of two inconsistent estimators {{to propose a}} new estimator of the coefficient of a linear regression model with measurement errors. The proposed estimator is simple {{and it does not}} depend on any unknown <b>quantity.</b> The <b>approximate</b> bias and MSE of the estimator are derived. Further, an empirical application is also presented. c ○ 2001 Peking University Pres...|$|R
40|$|A {{review of}} the meshless {{formulations}} based on local boundary integral equation (LBIE) methods is presented. Physical <b>quantities</b> are <b>approximated</b> by the moving least-squares method. A summary of recent developments {{in the application of}} the LBIE method to potential problems, elastostatics, elastodynamics, thermoelasticity, and plate bending problems is given. The efficiency and generality of the present formulation in a wide class of engineering problems are confirmed. 1. Introduction. Th...|$|R
40|$|Abstract: This paper {{introduce}} the new solution of integrated system to acoustic echo and noise reduction {{in order to}} improve the speech intelligibility in communication with used of the hands-free devices. The filter bank is proposed with minimum requirement of the band <b>quantity</b> to <b>approximate</b> psychoacoustic scale with nonuniform bands design using the first order all-pass transformation. Proposed solution fit in application of the 16 -bit signal and sampling rate from 8 kHz. ...|$|R
40|$|We {{introduce}} a simple image descriptor {{referred to as}} the image signature. We show, within the theoretical framework of sparse signal mixing, that this <b>quantity</b> spatially <b>approximates</b> the foreground of an image. We experimentally investigate whether this approximate foreground overlaps with visually conspicuous image locations by developing a saliency algorithm based on the image signature. This saliency algorithm predicts human fixation points best among GBVS (Graph based visual saliency), Itti/Koch saliency map and sun saliency does so in muc...|$|R
40|$|Abstract. We {{describe}} an infinite dimensional field theory based on invariant geometric <b>quantities</b> which <b>approximates</b> to 4 dimensional field {{theory in the}} large distance limit. We show that by allowing quantum uncertainty in the geometric constraints of the underlying equations we naturally introduce a gravitational force into the system. Gravity {{is included in the}} model by inserting certain invariant factors over the 6 -simplices in the Feynman graphs of the system in place of delta-function constraints. 2 1...|$|R
40|$|The am 1 and am 3 mutational {{variants}} of the Neurospora crassa NADP-specific glutamate dehydrogenase show complementation activity in hybrid hexamers. A freeze-thaw hybridization method {{was used to}} construct hybrids from purified enzymes and the products were separated into species of different monomer ratio by affinity chromatography. Hexamers with am 1 :am 3 ratios of 1 : 5, 2 : 4, 3 : 3, 4 : 2 and 5 : 1 were all recovered as resolved or partially resolved peaks in <b>quantities</b> <b>approximating</b> to a binomial distribution. Reassociation of monomers during the hybridization process was random, except for some differential loss of am 3 protein by precipitation and an apparent absence of reassociated am 1 homohexamers. Complementation activity was shown by hybrids of all five monomer ratios, owing to activation of am 3 monomers by conformational constraints arising from the intrinsically inactive am 1 monomers. The activating effect of such constraints was greatest in hexamers containing only a single am 1 monomer and least in the 5 am 1 : 1 am 3 species. When fully activated by L-glutamate all am 3 monomers were equivalent in intrinsic catalytic activity, irrespective {{of the number of}} am 1 monomers per hexamer...|$|R
40|$|The exact Green's {{functions}} of the periodic Anderson model for U→∞ are formally expressed within the cumulant expansion {{in terms of an}} effective cumulant. Here we resort to a calculation in which this <b>quantity</b> is <b>approximated</b> by the value it takes for the exactly soluble atomic limit of the same model. In the Kondo region a spectral density is obtained that shows near the Fermi surface a structure with the properties of the Kondo peak. Approximate expressions are obtained for the static conductivity...|$|R
5000|$|Even {{though it}} is often {{stressed}} that counting to precisely a million would be an exceedingly tedious task due to the time and concentration required, {{there are many ways}} to bring the number [...] "down to size" [...] in <b>approximate</b> <b>quantities,</b> ignoring irregularities or packing effects.|$|R
40|$|Abstract. In {{this paper}} {{classical}} matrix perturbation theory is approached from a probabilistic point of view. The perturbed <b>quantity</b> is <b>approximated</b> by a rst-order perturbation expansion, {{in which the}} perturbation {{is assumed to be}} random. This permits the computation of statistics estimating the variation in the perturbed quantity. Up to the higher-order terms that are ignored in the expansion, these statistics tend to be more realistic than perturbation bounds obtained in terms of norms. The technique is applied to a number of problems in matrix perturbation theory, including least squares and the eigenvalue problem...|$|R
40|$|The γ_ 2 norm {{of a real}} m× n matrix A is {{the minimum}} number t such that the column vectors of A are {{contained}} in a 0 -centered ellipsoid E⊆R^m {{which in turn is}} contained in the hypercube [-t, t]^m. We prove that this classical <b>quantity</b> <b>approximates</b> the hereditary discrepancy herdisc A as follows: γ_ 2 (A) = O(m) ·herdisc A and herdisc A = O(√(m)) ·γ_ 2 (A). Since γ_ 2 is polynomial-time computable, this gives a polynomial-time approximation algorithm for hereditary discrepancy. Both inequalities are shown to be asymptotically tight. We then demonstrate on several examples the power of the γ_ 2 norm as a tool for proving lower and upper bounds in discrepancy theory. Most notably, we prove a new lower bound of Ω(^d- 1 n) for the d-dimensional Tusnády problem, asking for the combinatorial discrepancy of an n-point set in R^d with respect to axis-parallel boxes. For d> 2, this improves the previous best lower bound, which was of order approximately ^(d- 1) / 2 n, and it comes close to the best known upper bound of O(^d+ 1 / 2 n), for which we also obtain a new, very simple proof. Comment: This is an expanded and simplified version, which also mostly subsumes arXiv: 1311. 6204. The "ellipsoid infinity norm" terminology is replaced by the standard factorization norm terminolog...|$|R
40|$|We {{will extend}} the {{definition}} of antieigenvalue of an operator to antieigenvalue-type quantities, {{in the first section}} of this paper, {{in such a way that}} the relations between antieigenvalue-type quantities and their corresponding Kantorovich-type inequalities are analogous to those of antieigenvalue and Kantorovich inequality. In the second section, we <b>approximate</b> several antieigenvalue-type <b>quantities</b> for arbitrary accretive operators. Each antieigenvalue-type <b>quantity</b> is <b>approximated</b> in terms of the same quantity for normal matrices. In particular, we show that for an arbitrary accretive operator, each antieigenvalue-type quantity is the limit of the same quantity for a sequence of finite-dimensional normal matrices...|$|R
40|$|Simulating the {{stochastic}} {{evolution of}} real quantities on a digital computer requires a trade-off between the precision {{to which these}} <b>quantities</b> are <b>approximated,</b> and the memory required to store them. The statistical accuracy of the simulation is thus generally limited by the internal memory available to the simulator. Here, using tools from computational mechanics, we show that quantum processors with a fixed finite memory can simulate stochastic processes of real variables to arbitrarily high precision. This demonstrates a provable, unbounded memory advantage that a quantum simulator can exhibit over its best possible classical counterpart. Comment: 5 pages + appendix; 3 figure...|$|R
5000|$|Some {{models of}} human {{behavior}} {{in the social sciences}} assume that humans can be reasonably approximated or described as [...] "rational" [...] entities (see for example rational choice theory, or Downs Political Agency Models). Many economics models assume that people are on average rational, and can in large enough <b>quantities</b> be <b>approximated</b> to act according to their preferences. The concept of bounded rationality revises this assumption to account for the fact that perfectly rational decisions are often not feasible in practice because of the intractability of natural decision problems and the finite computational resources available for making them.|$|R
40|$|Involvement of {{the cost}} {{advisers}} -in the early stage of the building design process (briefing stage) for the forecasting of probable cost of projects is often minimal. The traditional pre tender estimating techniques such as unit method, cost per unit floor area method, cube method, <b>approximate</b> <b>quantities</b> method and the like used to forecast the probable cost of projects are not satisfactory. This bars the cost adviser to make the fullest contribution in the early design stage decisions. The surveys conducted in this research revealed the fact that local design practice with respect to design stage estimating is limited to unit method, cost per unit floor area method and later stage <b>approximate</b> <b>quantities</b> method and elemental cost method. The use of computer facility to automate tasks in local practice is very low. Quantity surveyors in design offices use spreadsheet software for calculations. Use of database software or other programming languages {{in the process of}} cost advice was not evidenced. This research is, thus, aimed at establishing a computer aided cost model based on bill of <b>approximate</b> <b>quantities</b> properly linked to Standard Method of Measurement (SMM) with the flexibility in application during briefing, sketch plan and working drawing stages. Being linked to SMM, the proposed model ensures transformation of design and cost data into a BOQ prepared in accordance with SMM. The current practice of using bill of <b>approximate</b> <b>quantities</b> for cost forecasting is limited to later stages of the design process since the method requires design information to work upon. This difficulty has been overcome in the proposed model by incorporating design and cost data libraries with computer manipulation. [...] A case study was used to generate quantities, compile historical design data, and provide causal relationships to the model. These data coupled with solution neutral design information were used to develop design data library and cost data library of the model which provide sufficient information to make professional judgements at the briefing stage...|$|R
40|$|Time-independent Hamiltonian flows {{are viewed}} as {{geodesic}} flows in a curved manifold, so that the onset of chaos hinges on properties of the curvature two-form entering into the Jacobi equation. Attention focuses on ensembles of orbit segments evolved in 2 -D potentials, examining how various orbital properties correlate with the mean value and dispersion, and k, of the trace K of the curvature. Unlike most analyses, which have attributed chaos to negative curvature, this work exploits the fact that geodesics can be chaotic even if K is everywhere positive, chaos arising as a parameteric instability triggered by regular variations in K along the orbit. For ensembles of fixed energy, with both regular and chaotic segments, simple patterns connect the values of and k for different segments, both {{with each other and}} with the short time Lyapunov exponent X. Often, but not always, there is a near one-to- one correlation between and k, a plot of these <b>quantities</b> <b>approximating</b> a simple curve. X varies smoothly along this curve, chaotic segments located furthest from the regular regions tending systematically to have the largest X's. For regular orbits, and k also vary smoothly with ``distance'' from the chaotic phase space regions, as probed, e. g., by the location of the initial condition on a surface of section. Many of these observed properties can be understood qualitatively in terms of a one-dimensional Mathieu equation. Comment: 16 pages plus 9 figures, LaTeX, no macros required to appear in Physical Review...|$|R
50|$|In {{his other}} works, Archimedes often proves the {{equality}} of two areas or volumes with Eudoxus' method of exhaustion, an ancient Greek counterpart of the modern method of limits. Since the Greeks were aware that some numbers were irrational, their notion of a real number was a <b>quantity</b> Q <b>approximated</b> by two sequences, one providing an upper bound {{and the other a}} lower bound. If you find two sequences U and L, with U always bigger than Q, and L always smaller than Q, and if the two sequences eventually came closer together than any prespecified amount, then Q is found, or exhausted, by U and L.|$|R
50|$|Though it is {{regularly}} minted, {{it is not}} made in large <b>quantities</b> (<b>approximate</b> annual average production of 150,000), and since 2004 has only been {{available to the public}} directly from the mint. It is very rare to encounter this denomination in everyday transactions, since there seems to be the mistaken belief among many Canadians that the coin itself is rare and thus of value in excess of 50 cents. Most times, when a 50-cent piece is exchanged in a transaction, it is saved by its recipient. People quite commonly, upon being presented with 50-cent pieces, question the legality of the coin, because of the non-circulating status of the denomination. The coin occupies a similar status to that of the United States half-dollar coin. Newer vending machines do not generally accept it, even when they accept coins of both higher and lower value, but many older machines that were retooled to accept loonies will misidentify a 50-cent piece as a loonie, thus allowing the value of the coin to be doubled. A largely unsuccessful attempt was made by the Royal Canadian Mint to promote the use of the coin when a special edition was released in 2002 marking the 50th anniversary of the accession of Elizabeth II to the throne. After this failed promotion, the mint stopped distributing 50 cent pieces to banks, and now only sells them in rolls or in coin sets available directly from their Numismatic Department at twice their face value, or $25 per roll of 25 coins.|$|R
40|$|In C 4 plants, {{water deficit}} may {{decrease}} photosynthetic CO 2 assimilation independently {{of changes in}} stomatal conductance, suggesting decreased turnover by ribulose- 1, 5 -bisphosphate carboxylase/oxygenase (Rubisco). The activity and biochemistry of Rubisco was studied in three different C 4 grasses: Paspalum dilatatum, Cynodon dactylon, and Zoysia japonica. The objectives were to characterize the C 4 Rubisco in these species and to identify factors associated with decreased photosynthetic rates caused by drought. Rubisco isolated {{from each of the}} three C 4 grasses was characterized by smaller specificity factors (S C/O), larger Michaelis-Menten constants for CO 2 (K c) and O 2 (Ko), and larger maximum carboxylation velocities (Vc) than Rubisco from wheat, which can be rationalized in terms of the CO 2 -rich environment of C 4 Rubisco in the bundle sheath. During leaf dehydration the quantity and maximum activity of Rubisco remained unchanged but the initial and total activities declined slightly, possibly due to increased inhibition. Tight-binding inhibitors were present in the light but were more abundant in the dark, especially in Z. japonica, and increased in quantity with drought stress. The inhibitor from darkened leaves of Z. japonica was identified as 2 -carboxyarabinitol- 1 -phosphate (CA 1 P). Consistent with the presence of CA 1 P, the total activity of Rubisco was decreased after 12 h darkness in Z. japonica. Ribulose- 1, 5 -bisphosphate (RuBP) in the leaves decreased with drought stress, to <b>quantities</b> <b>approximating</b> those of Rubisco catalytic sites. The magnitude of the decrease in RuBP suggested that, at least in C. dactylon and Z. japonica, it could contribute to the drought-induced decrease in photosynthesis. © 2010 The Author(s) ...|$|R
3000|$|The {{collection}} {{of all the}} <b>approximate</b> <b>quantities</b> in a metric linear space X is denoted by W ([...] X [...]). T:X → F ([...] Y [...]) is a fuzzy mapping from an arbitrary set X to F ([...] Y [...]), which is a fuzzy subset in X × Y, and the grade of membership of y in T ([...] x [...]) is T ([...] x [...]) ([...] y [...]).|$|R
50|$|The tender {{is treated}} as an offer {{to do the work}} for {{a certain amount of money}} (firm price), or a certain amount of profit (cost {{reimbursement}} or cost plus). The tender, which is submitted by the competing firms, is generally based on a bill of quantities, a bill of <b>approximate</b> <b>quantities</b> or other specifications which enable the tenders to attain higher levels of accuracy, the statement of work.|$|R
40|$|As online social {{networking}} emerges, {{there has been}} increased interest to utilize the underlying social structure {{as well as the}} available social information to improve search. In this paper, we focus on improving the performance of information collection from the neighborhood of a user in a dynamic social network. To this end, we introduce sampling based algorithms to quickly <b>approximate</b> <b>quantities</b> of interest from the vicinity of a user’s social graph. We then introduce and analyze variants of this basic scheme exploring correlations across our samples. Models of centralized and distributed social networks are considered. We show that our algorithms can be utilized to rank items in the neighborhood of a user, assuming that information for each user in the network is available. Using real and synthetic data sets, we validate the results of our analysis and demonstrate the efficiency of our algorithms in <b>approximating</b> <b>quantities</b> of interest. The methods we describe are general and can probably be easily adopted in a variety of strategies aiming to efficiently collect information from a social graph...|$|R
40|$|We {{introduce}} a simple image descriptor {{referred to as}} the image signature. We show, within the theoretical framework of sparse signal mixing, that this <b>quantity</b> spatially <b>approximates</b> the foreground of an image. We experimentally investigate whether this approximate foreground overlaps with visually conspicuous image locations by developing a saliency algorithm based on the image signature. This saliency algorithm predicts human fixation points best among competitors on the Bruce and Tsotsos [1] benchmark data set and does so in much shorter running time. In a related experiment, we demonstrate with a change blindness data set that the distance between images induced by the image signature is closer to human perceptual distance than can be achieved using other saliency algorithms, pixel-wise, or GIST [2] descriptor methods...|$|R
40|$|Abstract: We {{introduce}} an algorithm {{that generates}} an optimal controller for stochastic nonlinear {{problems with a}} periodic solution, e. g. locomotion. Uniquely, the <b>quantity</b> we <b>approximate</b> is neither the Value nor Policy functions, but rather the stationary statedistribution of the optimally-controlled process. We recast the control problem as Bayesian inference over a graphical model with a ring topology. The posterior approximates the controlled stationary distribution with local gaussians along the optimal limit-cycle. Linear-feedback gains and open-loop controls are extracted from the covariances and the means, respectively. Complexity scales linearly or quadratically with the state dimension, depending on the dynamics approximation. We demonstrate our algorithm on a toy 2 -dimensional problem and then on a challenging 23 -dimensional simulated walking robot...|$|R
40|$|Appeared in SIAM Review 32 (1990) 576 [...] 610. In {{this paper}} {{classical}} matrix perturbation theory is approached from a probabilistic point of view. The perturbed <b>quantity</b> is <b>approximated</b> by a first order perturbation expansion, {{in which the}} perturbation {{is assumed to be}} random. This permits the computation of statistics estimating the variation in the perturbed quantity. Up to the higher order terms that are ignored in the expansion, these statistics tend to be more realistic than perturbation bounds obtained in terms of norms. The technique is applied to a number of problems in matrix perturbation theory, including least squares and the eigenvalue problem. Additional files are available via anonymous ftp at: thales. cs. umd. edu in the directory /ftp/pub/reports (Also cross-referenced as UMIACS-TR- 88 - 76...|$|R
40|$|Interval {{arithmetic}} is {{the mathematical}} structure, which for real intervals defines operations analogous to ordinary arithmetic ones. This field of mathematics is also called interval analysis or interval calculations. The given math model is convenient for investigating various applied objects: the <b>quantities,</b> the <b>approximate</b> values {{of which are}} known; the quantities obtained during calculations, the values of which are not exact because of rounding errors; random quantities. As a whole, the idea of interval calculations {{is the use of}} intervals as basic data objects. In this paper, we considered the definition of interval mathematics, investigated its properties, proved a theorem, and showed the efficiency of the new interval arithmetic. Besides, we briefly reviewed the works devoted to interval analysis and observed basic tendencies of development of integral analysis and interval calculations...|$|R
