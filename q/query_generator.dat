34|24|Public
50|$|Instantiation {{questions}} {{are composed of}} both the existence and the uniqueness questions, {{but it is the}} uniqueness questions that actually instantiate an object if they get a positive response. So if the <b>query</b> <b>generator</b> has to randomly pick an instantiation question, it prefers to pick an unpredictable uniqueness question if present. If such a question is not present, the <b>query</b> <b>generator</b> picks an existence question such that it will lead to a uniqueness question with a high probability in the future. Thus the <b>query</b> <b>generator</b> performs a look-ahead search in this case.|$|E
50|$|Simplicity {{preference}} {{states that}} the <b>query</b> <b>generator</b> should pick simpler questions over the more complicated ones. Simpler questions {{are the ones that}} have less number of attributes in them. So this gives an ordering to the questions {{based on the number of}} attributes, and the <b>query</b> <b>generator</b> prefers the simpler ones.|$|E
50|$|The {{built-in}} SQL backend of {{the framework}} provides ROLAP functionality on top a relational database. Cubes contains a SQL <b>query</b> <b>generator</b> that translates the reporting queries into SQL statements. The <b>query</b> <b>generator</b> {{takes into account}} topology of the star or snowflake schema and executes only joins {{that are necessary to}} retrieve attributes required by the data analyst.|$|E
40|$|Dynamic web {{applications}} such as mashups need efficient access to web data that is only accessible via entity search engines (e. g. product or publication search engines). However, most current mashup systems and applications only support simple keyword searches for retrieving data from search engines. We propose the use of more powerful search strategies building on so-called <b>query</b> <b>generators.</b> For a given set of entities <b>query</b> <b>generators</b> are able to automatically determine a set of search queries to retrieve these entities from an entity search engine. We demonstrate the usefulness of <b>query</b> <b>generators</b> for on-demand web data integration and evaluate the effectiveness and efficiency of <b>query</b> <b>generators</b> for a challenging real-world integration scenario. 1...|$|R
40|$|Abstract—Programmatic data {{integration}} approaches such as mashups {{have become a}} viable approach to dynamically integrate web data at runtime. Key data sources for mashups include entity search engines and hidden databases {{that need to be}} queried via source-specific search interfaces or web forms. Current mashups are typically restricted to simple query approaches such as using keyword search. Such approaches may need a high number of queries if many objects have to be found. Furthermore, the effectiveness of the queries may be limited, i. e., they may miss relevant results. We therefore propose more advanced search strategies that aim at finding a set of entities with high efficiency and high effectiveness. Our strategies use different kinds of queries that are determined by source-specific <b>query</b> <b>generators.</b> Furthermore, the <b>queries</b> are selected based on the characteristics of input entities. We introduce a flexible model for entity search strategies that includes a ranking of candidate queries determined by different <b>query</b> <b>generators.</b> We describe different <b>query</b> <b>generators</b> and outline their use within four entity search strategies. These strategies apply different query ranking and selection approaches to optimize efficiency and effectiveness. We evaluate our search strategies in detail for two domains: product search and publication search. The comparison with a standard keyword search shows that the proposed search strategies provide significant improvements in both domains. I...|$|R
5000|$|... fP SQL {{support is}} {{read-only}} ( [...] fP SQL <b>Query</b> and Report <b>Generator</b> [...] ) ...|$|R
50|$|As {{mentioned}} before {{the core of}} the Visual Turing Test is the <b>query</b> <b>generator</b> which generates a sequence of binary questions such that the answer to any question k is unpredictable given the correct answers to the previous k-1 questions. This is a recursive process, given a history of questions and their correct answers, the <b>query</b> <b>generator</b> either stops because there are no more unpredictable questions, or randomly selects an unpredictable question and adds it to the history.|$|E
50|$|It {{is clear}} that the {{interesting}} questions about the attributes and the relations come after the instantiation questions, and so the <b>query</b> <b>generator</b> aims at instantiating as many objects as possible.|$|E
50|$|An {{integral}} part of the ultimate aim of building systems that can understand images the way humans do, is the story line. Humans try to figure out a story line in the Image they see. The <b>query</b> <b>generator</b> achieves this by a continuity in the question sequences.|$|E
40|$|Abstract. Semantic query {{optimization}} (SQO) {{has been}} proved to be quite useful in various applications (e. g., data integration, graphical <b>query</b> <b>generators,</b> caching, etc.) and has been extensively studied for relational, deductive, object, and XML databases. However, less attention to SQO has been devoted {{in the context of}} the Semantic Web. In this paper, we present sound and complete algorithms for the containment and minimization of RDF/S query patterns. More precisely, we consider two widely used RDF/S query fragments supporting pattern matching at the data, but also, at the schema level. To this end, we advocate a logic framework for capturing the RDF/S data model and semantics and we employ well-established techniques proposed in the relational context, in particular, the Chase and Backchase algorithms. ...|$|R
40|$|<b>Query</b> <b>generators</b> {{producing}} {{sequences of}} SQL statements {{are embedded in}} many applications. As the execution time of such sequences is often far from optimal, their optimization is an important issue. Therefore, in [5] we proposed a rule-based optimization approach, which we called CGO (Coarse-Grained Optimization). Our first prototype used a heuristic, priority-based control strategy to choose the rewrite rules that should be applied to a given statement sequence. This worked well {{but there is still}} potential for improvements. Thus, in [4] we have introduced an approach to provide cost estimates for statement sequences which is the basis for a cost-based CGO optimizer. It exploits histogram propagation and the optimizer of the underlying database system for this purpose. In this demonstration, we want to showcase the functionality and the effectiveness of our approach. Thereto, we present a prototype of a cost-estimation component for statement sequences which implements this approach. It includes a graphical user interface to explain the histogram-propagation process and to report the results of the cost-estimation process. In the setup for this demonstration, we use a TPC-H benchmark database with an appropriate set of sequences as sample scenario. 1...|$|R
40|$|Abstract: To aid {{job seekers}} searching for job vacancies, we have {{developed}} a new configurable meta-search engine for the human resources domain. In this paper we describe the three main components of our meta-search engine – the <b>query</b> interface <b>generator,</b> <b>query</b> dispatcher and information extractor – which collectively support the meta-search engine creation and usage. One of the important challenges in accessing heterogeneous and distributed data via a meta-search engine is schema/data matching and integration. We describe an approach to schema and data integration for meta-search engines that helps to resolve the semantic heterogeneities between different source search engines. Our approach is a hybrid one, in that we use multiple matching criteria and multiple matchers. A domain-specific ontology serves as a global ontology and allows us to resolve semantic heterogeneities by deriving mappings between the source search engine interfaces and the ontology. The mappings are used to generate an integrated meta-search query interface, to support query processing within the meta-search engine, and to resolve semantic conflicts arising durin...|$|R
50|$|This {{means that}} once the object has been {{instantiated}} it tries to explore it in more details. Apart from finding its attributes and relation to the other objects, localisation {{is also an important}} step. Thus, as a next step the <b>query</b> <b>generator</b> tries to localise the object in the region it was first identified, so it restricts the set of instantiation questions to the regions within the original region.|$|E
50|$|As {{described}} in, it is “an operator-assisted {{device that}} produces a stochastic sequence of binary questions from a given test image”. The query engine produces a sequence of questions that have unpredictable answers given the history of questions. The test is only about vision and does not require any natural language processing. The job of the human operator is to provide the correct {{answer to the question}} or reject it as ambiguous. The <b>query</b> <b>generator</b> produces questions such that they follow a “natural story line”, similar to what humans do when they look at a picture.|$|E
30|$|For each experiment, we {{generated}} {{five different}} datasets, each one containing 50 [*]~[*] 100 data records for broadcasting. In addition, for each data set, we generated 5, 000 queries and calculated the corresponding access time and tune-in time to evaluate access efficiency and power conservation. The inter-arrival time of queries followed an exponential distribution with an arrival rate of λ[*]=[*] 1. The simulator and <b>query</b> <b>generator</b> were coded in MATLAB.|$|E
40|$|This paper {{presents}} {{the design and}} an initial performance evaluation of the <b>query</b> optimizer <b>generator</b> designed for the EXODUS extensible database system. Algebraic transformation rules are translated into an executable query optimizer, which transforms query trees and selects methods for executing operations according to cost functions associated with the methods. The search strategy avoids exhaustive search and it modifies itself {{to take advantage of}} past experience. Computational results show that an optimizer generated for a relational system produces access plans almost as good as those produced by exhaustive search, with the search time cut to a small fraction...|$|R
40|$|This paper {{presents}} {{the design and}} an mmal perfor-mance evaluation of the <b>query</b> ophrmzer <b>generator</b> designed for the EXODUS extensible database system. Algetic transformaaon rules are translated mto an executable query optmuzer. which transforms query trees and selects methods for executmg operattons accordmg to cost funcaons associ-ated with the methods The search strategy avoids exhaus-ave search and it mties Itself {{to take advantage of}} past expenence Computattonal results show that an opmzer generated for a relational system produces access plans almost as good as those produced by exhaushve search, ~rlth the search tune cut to a small fraction...|$|R
40|$|Design {{patterns}} shall {{support the}} reuse of a software architecture in different application domains {{as well as}} the flexible reuse of components. In this paper, we propose design patterns for meta-search engines. We also introduce design patterns for common components of meta-search engines e. g. <b>query</b> interface <b>generator,</b> information extraction, result merger and result ranker. Presented design patterns for meta-search engines and their components are reusable, extendable and flexible. These design patterns accelerate the development process in meta-search domain and other related domains. Moreover, it promises higher quality of developed solutions. These design patterns also provide developers with a shared vocabulary for easy communication...|$|R
40|$|Abstract—We {{describe}} an automated approach for systematic black-box testing of database management systems (DBMS) using a relational constraint solver. We reduce {{the problem of}} automated database testing into generating three artifacts: (1) SQL queries for testing, (2) meaningful input data to populate test databases, and (3) expected results of executing the queries on the generated data. We leverage our previous work on ADUSA and the Automated SQL <b>Query</b> <b>Generator</b> to form high-quality test suites for testing DBMS engines. This paper presents {{a detailed description of}} our framework for Automated SQL Query Generation using the Alloy tool-set, and experimental results of testing database engines using our framework. We show how the main SQL grammar constraints can be solved by translating them to Alloy constraints to generate semantically and syntactically correct SQL queries. We also present experimental results of combining ADUSA and the Automated SQL <b>Query</b> <b>Generator,</b> and applying our framework to test the Oracle 11 g database. Our framework generated 5 new queries, which reveal erroneous behavior of Oracle 11 g. I...|$|E
40|$|As {{expert systems}} become more widely used, {{their access to}} large amounts of {{external}} information becomes increasingly important. This information exists in several forms such as statistical, tabular data, knowledge gained by experts and large databases of information maintained by companies. Because many expert systems, including CLIPS, do not provide access to this external information, much of the usefulness of expert systems is left untapped. The scope {{of this paper is}} to describe a database extension for the CLIPS expert system shell. The current industry standard database language is SQL. Due to SQL standardization, large amounts of information stored on various computers, potentially at different locations, will be more easily accessible. Expert systems should be able to directly access these existing databases rather than requiring information to be re-entered into the expert system environment. The ORACLE relational database management system (RDBMS) was used to provide a database connection within the CLIPS environment. To facilitate relational database access a query generation system was developed as a CLIPS user function. The queries are entered in a CLlPS-like syntax and are passed to the <b>query</b> <b>generator,</b> which constructs and submits for execution, an SQL query to the ORACLE RDBMS. The query results are asserted as CLIPS facts. The <b>query</b> <b>generator</b> was developed primarily for use within the ICADS project (Intelligent Computer Aided Design System) currently being developed by the CAD Research Unit in the California Polytechnic State University (Cal Poly). In ICADS, there are several parallel or distributed expert systems accessing a common knowledge base of facts. Expert system has a narrow domain of interest and therefore needs only certain portions of the information. The <b>query</b> <b>generator</b> provides a common method of accessing this information and allows the expert system to specify what data is needed without specifying how to retrieve it...|$|E
40|$|ABSTRACT The {{three-dimensional}} environ-ments of {{ligand binding}} sites have been {{derived from the}} parsing and loading of the PDB entries into a relational database. For each bound molecule the biological assembly of the quaternary structure {{has been used to}} determine all contact residues and a fast interactive search and retrieval system has been developed. Prosite pattern and short sequence search options are available together with a novel graphical <b>query</b> <b>generator</b> for inter-residue con-tacts. The database and its query interface are accessible from the Internet through a web serve...|$|E
40|$|The {{aim of this}} {{tutorial}} is to help {{users to}} formulate and solve a large variety of problems through the analytic center cutting plane method. We first briefly review the class of problems that are solvable by accpm and sketch the principles that underlie accpm. We discuss the organization of accpm as a general cutting plane method, and present its three modules: the oracle, the coordinator and the <b>query</b> point <b>generator.</b> The main interface is between the oracle that is problem-dependent, and thus programmed by the user, and the last two modules whose internal functions are hidden in most part to the user. We finally present some examples of optimization problems that can be solved by accpm, directly or after some reformulation...|$|R
40|$|This paper {{presents}} {{an overview of}} EXODUS, an extensible database system project that is addressing data management problems posed {{by a variety of}} challenging new applications. The goal of the project is to facilitate the fast development of high-performance, application-specific database systems. EXODUS provides certain kernel facilities, including a versatile storage manager. In addition, it provides an architectural framework for building application-specific database systems; powerful tools to help automate the generation of such systems, including a rule-based <b>query</b> optimizer <b>generator</b> and a persistent programming language; and libraries of generic software components (e. g., access methods) that are likely to be useful for many application domains. We briefly describe each of the components of EXODUS in this paper, and we also describe a next-generation DBMS that we are now building using the EXODUS tools. 1...|$|R
40|$|This paper {{describes}} {{the design and}} implementation of a kernel for an OODBMS, namely the METU Object-Oriented DBMS (MOOD). MOOD is developed on the Exodus Storage Manager (ESM). MOOD kernel provides the optimization and interpretation of SQL statements, dynamic linking of functions, and catalog management. SQL statements are interpreted whereas functions (which have been previously compiled with C++) within SQL statements are dynamically linked and executed. Thus the interpretation of functions are avoided increasing {{the efficiency of the}} system. A query optimizer is implemented by using the Volcano <b>Query</b> Optimizer <b>Generator.</b> A graphical user interface, namely MoodView, is developed using Motif. MoodView displays both the schema information and the query results graphically. Additionally it is possible to update the database schema and to traverse the references in query results graphically. Keywords: OODBMS kernel implementation, query optimization in OODBMSs, dynamic function link [...] ...|$|R
40|$|This paper {{presents}} a similarity <b>query</b> <b>generator</b> for DBMSs. A user query which {{turns out to}} be too restrictive and returns an empty set of rows is relaxed and transformed into a similar one: the resulting set of tuples will resemble, at some degree, the set defined by the original query. The relaxing activity is based on fuzzy logic and the system provides a user interface to express the query, to obtain suggestions on possible search values and to validate, on the basis of semantic integrity rules, the expressed conditions...|$|E
40|$|The Chem 2 Bio 2 RDF portal is a Linked Open Data (LOD) portal for systems {{chemical}} biology aiming for facilitating drug discovery. It converts around 25 different datasets on genes, compounds, drugs, pathways, side effects, diseases, and MEDLINE/PubMed documents into RDF triples and links {{them to other}} LOD bubbles, such as Bio 2 RDF, LODD and DBPedia. The portal is based on D 2 R server and provides a SPARQL endpoint, but adds on few unique features like RDF faceted browser, user-friendly SPARQL <b>query</b> <b>generator,</b> MEDLINE/PubMed cross validation service, and Cytoscape visualization plugin. Three use cases demonstrate the functionality and usability of this portal. Comment: 8 pages, 10 figure...|$|E
40|$|International audienceIn this {{presentation}} we {{show our}} ongoing work {{to develop a}} testbed [...] based on software and commodity hardware [...] to research on flooding attacks against DNS infrastructure. We have currently developed two prototype components: a flooding DNS <b>query</b> <b>generator,</b> able to saturate 10 GbE links with 11 Mrps, and an online detector of overabundant queried domains at reception. Relying on DPDK and libmoon (a LuaJIT framework for DPDK), these two tools run on commodity hardware, while optimizing the number of packets that we can handle at transmission and reception. Both generation and reception tools run Lua scripts, achieving {{a high level of}} flexibility. In this presentation we show some lessons we are learning, we compare the generator against other available tools, and present some unexpected results. For example, how a slower software <b>query</b> <b>generator</b> has a stronger impact on a Bind server than our current flooding tool (650 Krps versus 10 Mrps). We also describe how we count the number of queries per domain at reception under 11 Mrps traffic, with reduced packet losses. Given the high number of possible elements to analyse from the DNS messages (IP addresses, random qnames) we make use of statistical tools, mainly CountMin-Sketch, to restrict the use of memory space. This tool can trigger an alarm when a domain exceeds a threshold of queries per a small interval of time. In this presentation we also look for feedback from the DNS-OARC community about possible strategies to use this tool to countermeasure flooding attacks...|$|E
40|$|International audienceMassive graph {{data sets}} are {{pervasive}} in contemporary application domains. Hence, graph database systems {{are becoming increasingly}} important. In the experimental study of these systems, {{it is vital that}} the research community has shared solutions for the generation of database instances and query workloads having predictable and controllable properties. In this paper, we present the design and engineering principles of gMark, a domain- and query language-independent graph instance and <b>query</b> workload <b>generator.</b> A core contribution of gMark is its ability to target and control the diversity of properties of both the generated instances and the generated workloads coupled to these instances. Further novelties include support for regular path queries, a fundamental graph query paradigm, and schema-driven selectivity estimation of queries, a key feature in controlling workload chokepoints. We illustrate the flexibility and practical usability of gMark by showcasing the framework's capabilities in generating high quality graphs and workloads, and its ability to encode user-defined schemas across a variety of application domains...|$|R
40|$|The discriminative {{approach}} to classification using deep neural networks {{has become the}} de-facto standard in various fields. Complementing recent reservations about safety against adversarial examples, we show that conventional discriminative methods can easily be fooled to provide incorrect labels with very high confidence to out of distribution examples. We posit that a generative approach is the natural remedy for this problem, and propose a method for classification using generative models. At training time, we learn a generative model for each class, while at test time, given an example to classify, we <b>query</b> each <b>generator</b> for its most similar generation, and select the class corresponding to the most similar one. Our approach is general {{and can be used}} with expressive models such as GANs and VAEs. At test time, our method accurately "knows when it does not know," and provides resilience to out of distribution examples while maintaining competitive performance for standard examples...|$|R
40|$|The {{increasing}} {{attention for}} federated SPARQL query sys-tems emphasize necessity for benchmarking systems to eval-uate their performance. Most {{of the existing}} benchmark systems rely {{on a set of}} predefined static queries over a par-ticular set of data sources. Such benchmark are useful for comparing general purpose SPARQL query federation sys-tems such as FedX, SPLENDID etc. However, special pur-pose federation systems such as TopFed, SAFE etc. cannot be tested with these static benchmarks since these systems only operate on a specific data sets and the corresponding queries. To facilitate the process of benchmarking for such special purpose SPARQL query federation systems, we pro-pose QFed, a dynamic SPARQL <b>query</b> set <b>generator</b> that takes into account the characteristics of both dataset and queries along with the cost of data communication. Our experimental results show that QFed can successfully gen-erate a large set of meaningful federated SPARQL queries to be considered for the performance evaluation of different federated SPARQL query engines...|$|R
40|$|The {{growth of}} web data has {{presented}} new chal-lenges regarding {{the ability to}} effectively query RDF data. Traditional relational database systems effi-ciently scale and query distributed data. With the development of Hadoop its implementation of the MapReduce Framework along with HBase, a NoSQL data store, the semantics of processing and querying data has changed. Given the existing structure of SPARQL, we devised and are continuing to develop a means to query RDF data effectively. By using HBase as a data store and HiveQL as the <b>query</b> <b>generator,</b> we implement a prototype intermediate translator which will generate HiveQL given a SPARQL query. This HiveQL query will then return data from our distributed HBase store. ...|$|E
40|$|We {{showcase}} QUEST (<b>QUEry</b> <b>generator</b> for STructured sources), {{a search}} engine for relational databases that combines semantic and machine learning techniques for transforming keyword queries into meaningful SQL queries. The search engine relies on two approaches: the forward, providing mappings of keywords into database terms (names of tables and attributes, and domains of attributes), and the backward, computing the paths joining the data structures identified in the forward step. The results provided by the two approaches are combined within a probabilistic framework based on the Dempster-Shafer Theory. We demonstrate QUEST capabilities, and we show how, thanks to the flexibility obtained by the probabilistic combination of different techniques, QUEST is able to compute high quality results even with few training data and/or with hidden data sources such as {{those found in the}} Deep Web. 1...|$|E
40|$|International audienceBeing DNS an {{essential}} service for Internet reliability, {{it is an}} attractive target for malicious users. The constantly increasing Internet traffic rate challenges DNS services and their attack detection methods to handle actual queries while being flooded by {{tens of millions of}} malicious requests per second. Moreover, state of the art on hostile actions evolve fast. DNS administrators continuously face new kinds of attacks and they regularly need to evaluate their detection systems. We have studied different approaches to develop a tool able to reproduce state-of-the-art attacks, aiming to make it easy to evaluate countermeasure strategies. We have focused on commodity-hardware, DPDK and MoonGen to build a flexible flood <b>query</b> <b>generator.</b> The described tool can saturate a 10 Gbps link, sending more than 12 million attack-like random DNS requests per second...|$|E
40|$|A data {{dictionary}} {{system with a}} query compiler is implemented in a symbol manipulation language, separate from the underlying database system. The query compiler (or program generator) generates COBOL programs for database access. These programs are optimized at generation time using information from the {{data dictionary}}. The implementation technique {{makes it possible to}} combine pilot implementation with production implementation of database application programs. Furthermore, an example is given of how the architecture of the system is convertible to different underlying database systems. Key concepts: data dictionary, program <b>generator,</b> <b>query</b> language compilation, query language interpretation, non-procedural query language. I...|$|R
40|$|Massive graph {{data sets}} are {{pervasive}} in contemporary application domains. Hence, graph database systems {{are becoming increasingly}} important. In the experimental study of these systems, {{it is vital that}} the research community has shared solutions for the generation of database instances and query workloads having predictable and controllable properties. In this paper, we present the design and engineering principles of gMark, a domain- and query language-independent graph instance and <b>query</b> workload <b>generator.</b> A core contribution of gMark is its ability to target and control the diversity of properties of both the generated instances and the generated workloads coupled to these instances. Further novelties include support for regular path queries, a fundamental graph query paradigm, and schema-driven selectivity estimation of queries, a key feature in controlling workload chokepoints. We illustrate the flexibility and practical usability of gMark by showcasing the framework's capabilities in generating high quality graphs and workloads, and its ability to encode user-defined schemas across a variety of application domains. Comment: Accepted in November 2016. URL: [URL] in IEEE Transactions on Knowledge and Data Engineering 201...|$|R
40|$|The query optimizer {{plays an}} {{important}} role in a database management system supporting a declarative query language, such as SQL. One of its central components is the plan generator, which is responsible for determining the optimal join order of a <b>query.</b> Plan <b>generators</b> based on dynamic programming have been known for several decades. However, some significant progress in this field has only been made recently. This includes the emergence of highly efficient enumeration algorithms and the ability to optimize a wide range of queries by supporting complex join predicates. This thesis builds upon the recent advancements by providing a framework for extending the aforementioned algorithms. To this end, a modular design is proposed that allows for the exchange of individual parts of the plan generator, thus enabling the implementor to add new features at will. This is demonstrated by taking the example of two previously unsolved problems, namely the correct and complete reordering of different types of join operators as well as the efficient reordering of join operators and grouping operators...|$|R
