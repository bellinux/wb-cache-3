17|91|Public
40|$|This unit {{covers the}} basics of numeracy, {{arithmetic}} and algebra which are required to support the food technology units and <b>quantitative</b> <b>units</b> in food chemistry {{as part of the}} Certificate in Food Manufacture. The student is encouraged to use quantities with both precision and accuracy;to manipulate simple algebraic expressions and to plot simple linear graphs...|$|E
40|$|The {{article in}} {{question}} considers {{the functioning of}} <b>quantitative</b> <b>units,</b> their language and speech aspects. Introduction focuses upon the major items of paper - definition of numerals, words of weight and measure, aims, methods of investigation, empiric material applied, evoluation modifications, and perspectives of further study. When you are citing the document, use the following link [URL]...|$|E
40|$|Polyfunctionality of the English {{quantitative}} words The {{article in}} question considers {{the functioning of}} <b>quantitative</b> <b>units,</b> their language and speech aspects. Introduction focuses upon the major items of paper – de nition of numerals, words of weight and measure, aims, methods of investigation, empiric mate-rial applied, evolution modi cations, and perspectives of further study. Key words: quantitative words, numerals, words of weight and measure, polyfunc...|$|E
40|$|REASONS FOR PERFORMING STUDY: Reference {{values for}} {{quantitative}} electromyography (QEMG) in shoulder and hindlimb muscles of horses are limited. : To determine normative data on QEMG analysis of supraspinatus (SS), infraspinatus (IS), deltoideus (DT) and biceps femoris (BF) muscles. DESIGN: Experimental observational study and retrospective case series. : Seven adult healthy Royal Dutch sport horses underwent <b>quantitative</b> motor <b>unit</b> action potential analysis of each muscle using commercial electromyography equipment. Measurements were made according to published methods. One-way ANOVA {{was used to}} compare <b>quantitative</b> motor <b>unit</b> action potential variables between muscles, with post hoc testing according to Bonferroni, with significance set at P 15...|$|R
40|$|Abstract: A {{statistical}} comparison {{was carried}} out among 3 different parameters, i. e., egg count/volume, total egg countlsample and eggcountlhour, in regards to day-to-day variations of S. haematobium egg output in midday urination. Among 3 parameters, the egg countlhour showed the most stable value. In addition, the total egg cC?unt in a urine samplf 7. was not correlated with the sample volume of the urine in the same individuals. We conclude, therefore, that {{the adoption of the}} egg countlhour was best as a parameter for a <b>quantitative</b> <b>unit</b> of intensity of infection for cohort studies where the changes of intensity of infection are monitored for a long period. The existing parameter for egg output expressed in terms of the egg countl 10 mt volume of urine seems to be a less reliable reflection of the intensity of infection...|$|R
40|$|The {{qualitative}} {{nature of}} memory remains unknown, but common experience and laboratory observations attest to its quantitative character. Memories may be "strong" or "weak," as judged subjectively or as inferred from animal experiments. The typical experimental {{measure of a}} memory is the probability of emission of a learned behavioral response, the observable indicant of the memory. Each indicant is considered to reflect a corresponding memory trace in the brain, the so-called engram. The indicant is quantified, but the corresponding engram is not. This dichotomy adds a conceptual gap to the physiological gap that separates an engram from its indicant. There {{is a need for}} language to describe the engram in a quantitative sense, in order to link it more definitely with its measured indicant. It seems timely to introduce a <b>quantitative</b> <b>unit</b> of memory, even though such a unit must as yet be hypothetical, speculative, and tentative...|$|R
40|$|Financial risk {{modeling}} and management {{are very important}} and challenging tasks for financial institutions’ <b>quantitative</b> <b>units.</b> Owing to the complex nature of portfolios, and given recent financial market developments, contemporary research is focused on tail {{modeling and}}/or dependency modeling. The main objective {{of this paper is}} to examine the potential contribution of Lévy-based subordinated models coupled by ordinary elliptical copula functions to the estimation of the distribution pattern of international equity portfolios. We observe that the subordinated NIG model coupled with the Student copula function, and in particular its combined estimation version, allows us to get very good estimates of portfolio risk measures. Web of Science 62216114...|$|E
40|$|While {{the continual}} service {{improvement}} (CSI) journey embarked upon by each organisation is unique, {{one aspect of}} service improvement that should be standardised is the measurement of service improvements. In other words, units of service measurement {{should be based on}} standards that communities can use for common benchmarks. Classic examples include hours and minutes as the units of measurement for time and kilometres to measure distance. Unfortunately such <b>quantitative</b> <b>units</b> of measurement are not easy to determine for IT service improvements. The University of Southern Queensland initiated an Australian Research Council (ARC) Linkage project in 2011 to develop a standard approach to determine process capabilities for service improvement in ITSM. The research team includes ITSM practitioners and International Standards committee members. This article proposes the use of the international standard of process assessment (ISO/IEC 15504) in measuring ITSM improvements. ...|$|E
40|$|There is a {{consensus}} in the literature that mathematical ability contributes to student success in tertiary education. More importantly, mathematical skills are necessary when successfully completing mathematics- and/or science-based degrees. Social sciences such as psychology and economics require statistical skills which also require knowledge of mathematics. Even business students such as marketing and accounting students need the necessary mathematical skills to successfully complete their degrees at university. This paper suggests that student success in a core business subject is dependent on their mathematical aptitude, attitude and type of secondary schooling whether government or non-government schools. There is urgency for universities to recognise that high failure rates are due to insufficient mathematics exposure in secondary schooling and remedial classes might not be enough. Specifying a minimum (maths, e. g. 2 unit) requirement for entry and/or providing bridging programmes to ensure students have the necessary basic mathematical skills would increase student success in <b>quantitative</b> <b>units...</b>|$|E
40|$|Ntople {{assess the}} quality of the air indos primaly {{on the basis of its}} odors and on their percption of {{associated}} health risk. The majorcurrent contributors to indoor odorants are human occupant odors (body odor), environmental tobacco smoke, volatile building materials, bio-odorants (particularly mold and animal-derived materials), air fresheners, deodorants, and perfunes. These are most often present as complex mixtures, making measurement of the total odorant problem difficult. There is no current method ofmeasuring human body odor, other than by human panel studies of expertjudges of air quality. Human body odors have been qu d in terms of the "olf " which is the amount ofair polution produced by the average person. Another <b>quantitative</b> <b>unit</b> of odorants is the "decipol, " which is the perceived level of pollution produced by the average human ventilated by 10 L/sec of unpoDluted air or its equivalent klvel of dissatisfaction from nonhuman air pollutants. The standard regulatory approach, focining on individual constituents or chemicals, is not likely to be s in adequately contbli odoans in indoor air. Besides the current approach ofsefting minimum ventition stndards to prevent health effects due to indoor air pollution, a standard based on the olf or decipol unit might be more efficacious as well as simpler to measure...|$|R
40|$|This {{study used}} a {{contingent}} choice method {{to determine the}} economic value of improving various ecosystem services (ESs) of the Blue Network of Greater Montreal (Quebec, Canada). Three real projects were used and the evaluation focused on six ESs {{that are related to}} freshwater aquatic ecosystems: biodiversity, water quality, carbon sequestration, recreational activities, landscape aesthetics and education services. We also estimated the value associated with the superficies of restored sites. We calculated the monetary value that a household {{would be willing to pay}} for each additional qualitative or <b>quantitative</b> <b>unit</b> of different ESs, and these marginal values range from $ 0. 11 to $ 15. 39 per household per unit. Thus, under certain assumptions, we determined the monetary values that all Quebec households would allocate to improve each ES in Greater Montreal by one unit. The most valued ES was water quality ($ 13. 5 million), followed by education services ($ 10. 7 million), recreational activities ($ 8. 9 million), landscape aesthetics ($ 4. 1 million), biodiversity ($ 1. 2 million), and carbon sequestration ($ 0. 1 million). Our results ascribe monetary values to improved (or degraded) aquatic ecosystems in the Blue Network of Greater Montreal, but can also enhance economic analyses of various aquatic ecosystem restoration and management projects...|$|R
40|$|Target {{definition}} {{is the largest}} source of geometric uncertainty in radiation therapy. This {{is partly due to}} a lack of contrast between tumor and healthy soft tissue for computed tomography (CT) and due to blurriness, lower spatial resolution, and lack of a truly <b>quantitative</b> <b>unit</b> for positron emission tomography (PET). First-, second-, and higher-order statistics, Tamura, and structural features were characterized for PET and CT images of lung carcinoma and organs of the thorax. A combined decision tree (DT) with K-nearest neighbours (KNN) classifiers as nodes containing combinations of 3 features were trained and used for segmentation of the gross tumor volume. This approach was validated for 31 patients from two separate institutions and scanners. The results were compared with thresholding approaches, the fuzzy clustering method, the 3 -level fuzzy locally adaptive Bayesian algorithm, the multivalued level set algorithm, and a single KNN using Hounsfield units and standard uptake value. The results showed the DTKNN classifier had the highest sensitivity of 73. 9 %, second highest average Dice coefficient of 0. 607, and a specificity of 99. 2 % for classifying voxels when using a probabilistic ground truth provided by simultaneous truth and performance level estimation using contours drawn by 3 trained physicians...|$|R
40|$|The {{presence}} of qualitative {{changes in the}} nature of many commodities hinders our ability to construct meaningful price and quantity indices. This paper assesses some of the quality-change literature that seeks to resolve this problem. Several writers have endeavoured to develop objective, theory-neutral procedures designed to measure qualitative changes in some timeless, <b>quantitative</b> <b>units.</b> These measures, they argued, could be used to properly adjust ordinary price and quantity statistics for distortions introduced by quality changes. A careful examination suggests, however, that such procedures are neither objective nor free of theoretical biases. First, all existing attempts to develop ‘objective’ commodity measures in the {{presence of}} quality changes are besieged by a constant resort – explicit or implicit – to ‘subjective’ considerations. Second, both the idea that quality can be measured and the methods developed for that purpose are closely tied with the neoclassical theoretical paradigm, particularly with its emphasis on perfect competition and equilibrium...|$|E
40|$|Progress in {{autonomy}} {{cannot be}} – nor historically has it ever been – measured in <b>quantitative</b> <b>units.</b> Rather, {{the need for}} autonomy is repositioned in relation to society’s political, economic, and cultural developments on an ongoing basis. What do we mean when we speak of ‘autonomy’ and ‘reproduction’ {{in the field of}} contemporary art? What kind of objects do these terms encompass, what are their histories, and what internal logical relations can we identify between these concepts? How do they operate in a philosophical discourse about art and in political theory and practice? In this book, Marina Vishmidt and Kerstin Stakemeier analyse ‘autonomy’ and then ‘reproduction’, in the understanding that this method of categorical isolation must be overcome if we are to reach towards the relationship of the two terms. These three essays establish a new framework to locate notions of artistic autonomy and autonomies of art. The texts not only offer an entrance into thinking about the role that autonomy has occupied in modern European intellectual history; they also put forward an original thesis...|$|E
40|$|This study {{examines}} 8 th grade students ' coordination of <b>quantitative</b> <b>units</b> arising from word {{problems that can}} be solved via a set of equations that are reducible to a single equation with a single unknown. Quantitative unit conservation also emerges as a necessary construct in dealing with such problems. We introduce a theoretical framework that encompasses these two constructs. Our data consist of videotaped classroom lessons, student interviews and teacher interviews. We generated a thematic analysis by undertaking a retrospective analysis, using constant comparison methodology. Our first result is about students ' coordination of pairs of units (e. g. dime standing {{for the name of}} the coin and/or the number of dimes, the value of a dime being the second unit). Our second result is about students ' attempts to balance the two sides of an equation by conserving units. Theoretical Background This study is part of Project CoSTAR (Coordinating Students ’ and Teachers ’ Algebraic Reasoning) 1 that has as its main purpose the coordination of research on students’ understandings and teachers ’ practices and interpretations of students ’ actions relative to algebraic reasoning. This particular study is informed by recent research that coordinate...|$|E
50|$|NUSAP is a {{notational}} {{system for}} the management and communication of uncertainty in science for policy, based on five categories for characterizing any <b>quantitative</b> statement: Numeral, <b>Unit,</b> Spread, Assessment and Pedigree. NUSAP was introduced by Silvio Funtowicz and Jerome Ravetz in the 1990 book Uncertainty and quality in science for policy. See also van der Sluijs et al. 2005.|$|R
40|$|Copyright © 2013 Daniel Markel et al. This is an {{open access}} article {{distributed}} under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Target definition is the largest source of geometric uncertainty in radiation therapy. This {{is partly due to}} a lack of contrast between tumor and healthy soft tissue for computed tomography (CT) and due to blurriness, lower spatial resolution, and lack of a truly <b>quantitative</b> <b>unit</b> for positron emission tomography (PET). First-, second-, and higher-order statistics, Tamura, and structural features were characterized for PET andCT images of lung carcinoma and organs of the thorax. A combined decision tree (DT) with K-nearest neighbours (KNN) classifiers as nodes containing combinations of 3 features were trained and used for segmentation of the gross tumor volume. This approach was validated for 31 patients from two separate institutions and scanners. The results were compared with thresholding approaches, the fuzzy clustering method, the 3 -level fuzzy locally adaptive Bayesian algorithm, the multivalued level set algorithm, and a single KNN using Hounsfield units and standard uptake value. The results showed the DTKNN classifier had the highest sensitivity of 73. 9 %, second highest average Dice coefficient of 0. 607, and a specificity of 99. 2 % for classifying voxels when using a probabilistic ground truth provided by simultaneous truth and performance level estimatio...|$|R
40|$|Identifying {{the most}} {{suitable}} risk-reduction measures in drinking water systems requires a thorough analysis of possible alternatives. In addition to the effects on the risk level, also the economic aspects of the risk-reduction alternatives are commonly considered important. Drinking water supplies are complex systems and to avoid sub-optimisation of risk-reduction measures, the entire system from source to tap needs to be considered. There {{is a lack of}} methods for quantification of water supply risk reduction in an economic context for entire drinking water systems. The aim {{of this paper is to}} present a novel approach for risk assessment in combination with economic analysis to evaluate risk-reduction measures based on a source-to-tap approach. The approach combines a probabilistic and dynamic fault tree method with cost-effectiveness analysis (CEA). The developed approach comprises the following main parts: (1) quantification of risk reduction of alternatives using a probabilistic fault tree model of the entire system; (2) combination of the modelling results with CEA; and (3) evaluation of the alternatives with respect to the risk reduction, the probability of not reaching water safety targets and the cost-effectiveness. The fault tree method and CEA enable comparison of risk-reduction measures in the same <b>quantitative</b> <b>unit</b> and consider costs and uncertainties. The approach provides a structured and thorough analysis of risk-reduction measures that facilitates transparency and long-term planning of drinking water systems in order to avoid sub-optimisation of available resources for risk reduction...|$|R
40|$|Print {{this page}} The {{presence}} of HBV DNA in peripheral blood is a reliable marker of active HBV replication. HBV DNA detection and quantification {{can be achieved}} by means of signal amplification or target amplification methods. HBV DNA <b>quantitative</b> <b>units</b> used in the various assays are not yet standardized, but an HBV DNA international unit (IU) has been defined that should now be implemented in all commercial HBV DNA quantitative assays. HBV DNA detection and quantification are useful in the diagnosis of infection, therapeutic decision-making, and assessment of the response to therapy. However, clinically relevant decision thresholds have not been precisely defined, preventing from drawing simple management guidelines based on HBV DNA level assessment. Sensitive and accurate HBV DNA quantification methods can be used to study HBV kinetics in various clinical settings. Progress in standardization and better understanding of the mechanisms of HBV infection and response to therapy should help improve the practical use of HBV DNA assays in the future. Virological diagnosis and monitoring of hepatitis B virus (HBV) infection are based on serologic assays detecting specific anti-HBV antibodies, and assays that can detect, quantify or characterize the component...|$|E
40|$|This project {{research}} {{seeks to}} analyse {{the impact of}} variance in the dry concrete manufacturing processing times with a simulation output performance measure, by studying the simulation model results and answering with <b>quantitative</b> <b>units,</b> how much the overall system performance gets affected if the variance is increased by one unit. Three main challenges provide difficulty to this study: to develop {{a clear understanding of}} the system to simulate, to represent it in a computational model and to draw valid conclusions on the measured impact based on the model results. Three groups of experiments establish the conditions of how the variability is inserted to the system stages by the creation of different simulation models. The first group of experiments considers variance modifications in all activities, the second group has variability changes in critical activities and the last group simulates machines breakdown and repairing time’s variability. An interesting relationship between the variance and some preselected performance measures is noticed. Graphs are plotted and most of the provided analysis is supported by tendency lines and statistical procedures. Finally, Conclusions are presented and a set of project proposals are identified for achieving a full understanding on the topic...|$|E
40|$|The Sustainable Livelihoods Approach (SLA) is {{promoted}} {{as a useful}} way to centre development {{on the needs of}} those who are most vulnerable, but is critiqued for inflexibility and ignoring important power relations. In light of the rigidities in using a formulaic SLA, this conceptual paper suggests a practical suite of tools that are in use in livelihoods research and development practice, and refocuses them to include adaptive strategies of vulnerable peoples to resource management pressures. The Pentagon Prison of five capitals can overwhelm both the researcher and researched and while possibly useful in identifying livelihoods gaps, misses all-encompassing power relations, and reduces complexities to <b>quantitative</b> <b>units.</b> In arguing for a shift from the Pentagon Prison of SLA towards a flexible livelihoods trajectory approach, in particular for a research project on livelihood adaptations in Lao PDR, I identify a feasible research and development approach that more meaningfully reflects the lives of those participating in that research. I propose that the livelihood trajectories approach opens up the way data are gathered and can lead to holistic understandings of the complex realities of peoples' adaptive strategies, incorporating strategies from short to long term, and proactive, reactive and inactive techniques. 12 page(s...|$|E
40|$|A systems {{approach}} {{is applied to}} UK personal sector holdings of unit trusts, UK company securities, public-sector long-term debt and overseas securities. In the long run, asset holdings are determined primarily by hedging considerations but {{in the short run}} there is evidence of speculative activity. Asset shares are influenced by relative yields (including capital gains), inflation, and real expenditure. A two-step estimation procedure is used: a set of cointegration vectors are estimated for asset shares and dynamics are represented by a systems error feedback model. The four equation system is broadly consonant with the data and coefficient estimates are intuitively acceptable. Econometrics, Fixed assets, Models, <b>Quantitative</b> techniques, <b>Unit</b> trusts, United Kingdom...|$|R
5000|$|Dirk Geeraerts (born 24 October 1955, PhD 1981) {{holds the}} chair of {{theoretical}} linguistics at the University of Leuven, Belgium. He {{is the founder of}} the research <b>unit</b> <b>Quantitative</b> Lexicology and Variational Linguistics (QLVL). [...] His main research interests involve the overlapping fields of lexical semantics, lexicology, and lexicography, with a theoretical focus on cognitive semantics. His publications include the following monographs: ...|$|R
50|$|Two {{fundamental}} {{pillars of}} education and research are both necessary and concomitant factors. Without proper training, research and research training objectives have been drawn without the production {{of science and technology}} will be terminated. Double steps have been taken to promote research in Zanjan <b>unit.</b> <b>Quantitative</b> and qualitative indicators indicate the seriousness of the unit's research activities.recognized and awarded a plaque of appreciation from the President of Islamic Republic of Iran.|$|R
40|$|Summary: External quality {{assessment}} programmes for specific IgE have been organised {{for some years}} in the United Kingdom, Belgium and the Netherlands but independently. This paper describes a co-operation scheine whereby the same samples were circulated simultaneously {{from each of the}} three countries and subsequent results combined to produce a single "EURO EQAS " report. Serum pools were prepared each containing antibodies, at differing concentrations, to 4 different allergens. The allergens surveyed represented the 10 most commonly encountered in Northern Europe. Results were submitted in grades (or classes) and in <b>quantitative</b> <b>units</b> and they showed some similarity by grade regardless of method used but differed greatly in units probably due to method differences. This paper shows how results could be treated to produce statistical data for the participants to help them be aware of their performance internally and also in comparison to other users. Introduction ment schemes. The good inter-assay and inter-laboratory agreement of total serum IgE measurements indicates In 1967, IgE was defined äs a fifth distinct human im- ^ ̂ measurement does not present a major technical munoglobulin and it was shown to be the reaginic anti...|$|E
40|$|Accurate {{conversion}} of magnetic resonance spectra to <b>quantitative</b> <b>units</b> of concentration generally requires compensation {{for differences in}} coil loading conditions, the gains of the various receiver amplifiers, and rescaling that occurs during post-processing manipulations. This can be efficiently achieved by injecting a precalibrated, artificial reference signal, or pseudo-signal into the data. We have previously demonstrated, using in vitro measurements, that robust pseudo-signal injection can be accomplished using a second coil, called the injector coil, properly designed and oriented so that it couples inductively with the receive coil used to acquire the data. In this work, we acquired nonlocalized phosphorous magnetic resonance spectroscopy measurements from resting human tibialis anterior muscles and used pseudo-signal injection to calculate the Pi, PCr, and ATP concentrations. We compared these results to parallel estimates of concentrations obtained using the more established phantom replacement method. Our results demonstrate that pseudo-signal injection using inductive coupling provides a robust calibration factor that is immune to coil loading conditions and suitable for use in human measurements. Having benefits in terms of ease of use and quantitative accuracy, this method is feasible for clinical use. The protocol we describe could be readily translated for use in patients with mitochondrial disease, where sensitiv...|$|E
40|$|The {{curves of}} growth and of {{regeneration}} follow the same course, and can be represented by the same exponential equation. This is taken to substantiate the theory that growth and regeneration are essentially identical processes governed by the same laws. A common peculiarity of the curves {{of growth and}} of regeneration is that during a short period {{in the early stages}} of regeneration and of growth, the apparent observed speed of these processes seems to be relatively slow. As a result, the curve of the fitted equation cuts the time axis not at zero, the beginning of growth or regeneration, but somewhat later. Data on regeneration are cited indicating that the initial slow phase of regeneration is due to the time required for the formation of a cap of embryonic cells which serves as a basis for the more active later regeneration; in other words, to qualitative growth which cannot be expressed in terms of <b>quantitative</b> <b>units.</b> It is suggested that the apparent initial slow phase of growth of the individual from the fertilized egg is due to a similar qualitative growth. It is suggested that if the initial qualitative changes could be converted into some common unit with the subsequent quantitative changes, the apparent initial lag would disappear, and the exponential equation representing the course of these processes would then be the same as the equation used to represent the course of a monomolecular chemical reaction. Certain implications of this reasoning are discussed in the text...|$|E
40|$|In this paper, I {{examined}} {{characteristics of}} -lkAn, a proprietive suffix in Ewen. Added to nominal stems, this suffix expresses possession. Nominals suffixed with -lkAn (N-lkAn) function as adnominals, predicates, and adverbials. The semantic {{extent of the}} suffix -lkAn {{is not limited to}} possession, but covers various meanings. In adverbial usage, the suffix conveys the meaning of “possession at that very moment”. The other meanings expressed by N-lkan are as follows: (i) An accompanier, when added to a noun that signifies a person, a personal pronoun, or a proper noun of person name (in adverbial usage). (ii) Clothing in a state of “on (being worn) ” or “in use”, when added to a noun that signifies clothing. (iii) A <b>quantitative</b> <b>unit,</b> when added to a noun that signifies containers. (iv) A person’s age, when added to a numeral. Although the meaning of “accompanying” can be expressed by the comitative case as well as the suffix -lkAn, these two constructions are significantly different. On one hand, the comitative case suffix can co-occur with a possessive person suffix; on the other, the suffix -lkAn cannot. N-lkAn can serve as adnominals either with or without an instrumental case suffix. Without the instrumental case, it means a state of “on” or “in use”, whereas, with the instrumental case, it does not necessarily imply the state. In addition, there is an abessive form, aač -LA phrase, which is semantically opposite to the proprietive suffix -lkAn. The meaning of the aač -LA phrase covers absence, non-possession, and non-accompanying. The proprietive suffix and the abessive form share certain similarities in that they both have adnominal, predicative, and adverbial usage. However, in terms of morphosyntax, they show some differences for instance, in adverbial usage, the aač -LA phrase requires an instrumental case suffix while N-lkAn can be used as an adverbial either with or without the instrumental case suffix. 特集 所有表...|$|R
40|$|Food {{problem has}} been and remains a major concern, both nationally and globally, and the {{population}} has or {{does not have enough}} income to ensure structural physichological requirements of consumption. In the context {{of the importance of the}} role incumbent main categories of food consumption, the paper aims to study the level both total consumption in Romania and derivatives products and cereals, vegetables and fruits. Comparative dynamics shown by appropriate indicators (<b>quantitative</b> physical <b>units</b> and calories) report highlights that resources/total consumption is assured, but the differences in the levels of consumption of fruits and vegetables are considered significant. Consumption aspect qualitative comparisons analyzed by means a reduction in the level of human consumption, but there are slight differences (insignificant) for cereals, vegetables and very sharp significant fruit. For dynamic analysis period can be reported elements of concern, the existence of annual decreases in the consumption of fruit and vegetables...|$|R
40|$|Copyright © 2004 by the American Physiological Society. Cortical binocularity is {{abolished}} by monocular deprivation {{during a}} critical period of development lasting from approximately postnatal day (P) 35 to P 70 in ferrets. Although {{this is one}} of the best-characterized models of neural plasticity and amblyopia, very few studies have examined the requirements for recovery of cortical binocularity and orientation selectivity of deprived eye responses. Recent studies indicating that different mechanisms regulate loss and recovery of binocularity raise the possibility that different sensitive periods characterize loss and recovery of deprived eye responses. In this report, we have examined whether the potential for recovery of binocularity and orientation selectivity is restricted to the critical period. <b>Quantitative</b> single <b>unit</b> recordings revealed recovery of cortical binocularity and full recovery of orientation selectivity of deprived eye responses following prolonged periods of monocular deprivation (i. e., longer than 3 weeks) starting at P 49, near the peak of plasticity. Surprisingly, recovery was present when binocular vision was restored after the end o...|$|R
40|$|BACKGROUND AND OBJECTIVES: Flow cytometry is {{nowadays}} {{the preferred}} method for immunophenotypic identification, enumeration and characterization of blast cells at diagnosis. Despite widespread application of standardized protocols, inter-laboratory reproducibility {{has still not}} been achieved. The complexity of diagnosis and evaluation of minimal residual disease, in immunophenotyping acute leukemia, demands {{the use of a}} test that provides all the necessary information. DATA SOURCES AND METHODS: The information given here is derived from the experience of the authors and from literature files. The most relevant studies with adequate conclusions were considered. We report on the current status of multiparametric immunophenotyping using simultaneous three and four-color staining and the applications of this technique. RESULTS: Multiparametric immunophenotyping is a powerful method for achieving a clear discrimination between normal and pathologic cells. The specific identification of leukemic cells by immunologic gating forms the basis for immunophenotypic diagnosis, classification as well as prognostic evaluation of patients with acute leukemias. The performance of the procedure with regards to the panels of reagents and the analytic processes, is necessarily different in lymphoblastic and myeloblastic leukemias, since the diagnostic questions are different. Phenotypic information should be specifically provided for the blast cells and antigen expression should preferably be reported in <b>quantitative</b> <b>units</b> and CV. This would allow a standardized cross evaluation of immunophenotypic results between different investigators and laboratories. INTERPRETATION AND CONCLUSIONS: Recent reports indicate that phenotypic aberrations reflect genetic abnormalities of leukemic cells and therefore their definition and identification is of clinical relevance not only for minimal residual disease monitoring but also for subclassifying acute myeloid and lymphocytic leukemias...|$|E
40|$|International audienceChlordecone is an {{organochlorine}} pesticide that was extensively {{used in the}} French West Indies to fight weevils in banana plantations from 1973 to 1993. This {{has led to a}} persistent pollution of the environment and to the contamination of the local population for several decades with effects demonstrated on human health. Chlordecone accumulates mainly in the liver where it is known to potentiate the action of hepatotoxic agents. However, there is currently no information on its in situ localization in the liver. We have thus evaluated a matrix-assisted laser desorption ionization (MALDI) imaging quantification method based on labeled normalization for the in situ localization and quantification of chlordecone. After validating the linearity and the reproducibility of this method, quantitative MALDI imaging was used to study the accumulation of chlordecone in the mouse liver. Our results revealed that normalized intensities measured by MALDI imaging could be first converted in <b>quantitative</b> <b>units.</b> These quantities appeared to be different from absolute quantities of chlordecone determined by gas chromatography (GC), but they were perfectly correlated (R(2) = 0. 995). The equation of the corresponding correlation curve was thus efficiently used to convert quantities measured by MALDI imaging into absolute quantities. Our method combining labeled normalization and calibration with an orthogonal technique allowed the in situ absolute quantification of chlordecone by MALDI imaging. Finally, our results obtained on the pathological mouse liver illustrate the advantages of quantitative MALDI imaging which preserves information on in situ localization without radioactive labeling and with a simple sample preparation...|$|E
40|$|OBJECTIVE: The {{technical}} feasibility of virtual noncontrast (VNC) images from dual-energy computed tomography (DECT) {{for the detection}} of the hyperdense artery sign (HAS) in ischemic stroke patients was investigated. METHODS: True noncontrast (TNC) scans of 60 patients either with or without HAS (n = 30 each) were investigated. Clot presence and characteristics were assessed on VNC images from DECT angiography and compared with TNC images. Clot characterization included the level of confidence for diagnosing HAS, a qualitative clot burden score, and <b>quantitative</b> attenuation (Hounsfield <b>unit</b> [HU]) measurements. RESULTS: Sensitivity, specificity, and accuracy of VNC for diagnosing HAS were 97...|$|R
40|$|Research {{within the}} {{increasingly}} competitive global {{higher education sector}} has identified a significant shift towards improving students' perceptions of quality, as these perceptions are a basis for student promoter or detractor behaviours. For Australia's universities, managing these perceptions is critical since servicing the international student market {{is worth more than}} Australia's beef, wheat and wool industries (Linacre, 2007). This paper critically reflects on a change intervention being run in an Australian university, for an undergraduate <b>quantitative</b> marketing metrics <b>unit</b> which has experienced low quality unit student evaluations, and presents a rationale for preparing and designing a change intervention...|$|R
40|$|A Shuttle Imaging Radar (SIR-A) {{image of}} the {{southern}} portion of the San Rafael Swell in Utah has been digitized and registered to coregistered Landsat, Seasat, and HCMM thermal inertia images. The addition of the SIR-A image to the registered data set improves rock type discrimination in both qualitative and <b>quantitative</b> analyses. Sedimentary <b>units</b> can be separated in a combined SIR-A/Seasat image that cannot be seen in either image alone. Discriminant Analyses show that the classification accuracy is improved with addition of the SIR-A image to Landsat images. Classification accuracy is further improved when texture information from the Seasat and SIR-A images is included...|$|R
40|$|Objective – To analyse the {{efficiency}} levels of company owners {{of four and}} five-star hotels in the Algarve in Portugal for the years 2005 to 2007. Variables with <b>quantitative</b> and monetary <b>units</b> were used corresponding to two different models applied. An analysis to the slacks and peers of companies was also performed. Design/methodology/approach – Using data envelopment analysis (DEA) methodology to evaluate {{the efficiency}} of 28 hotels in the Portuguese Algarve. Findings - Tourism has an importance universally recognized. This sector has an high economic, social, cultural and environmental impact level. Global destinations and greater competitiveness have turned efficiency into {{a major issue in}} the hotel industry nowadays. In Portugal, the Algarve is the region with more tourism (70...|$|R
