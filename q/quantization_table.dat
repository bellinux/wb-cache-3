92|59|Public
40|$|In this paper, we {{proposed}} a passive scheme to achieve image forgery. The inconsistent measure of <b>quantization</b> <b>table</b> is characterized to develop the proposed scheme. The proposed scheme is composed of candidate region selection, <b>quantization</b> <b>table</b> estimation, and forgery detection. To select candidate regions for estimating <b>quantization</b> <b>table,</b> a split-and-merge algorithm based on quad-tree decomposition is devised. To estimate the <b>quantization</b> <b>table,</b> we classify the type of PSD and then adjust the estimation algorithm. After <b>quantization</b> <b>table</b> estimation, the variation resulting from the inconsistent of <b>quantization</b> <b>table</b> is utilized to detect tampered regions. The experimental results show that our proposed scheme can not only estimates <b>quantization</b> <b>table</b> correctly but also detect tampered regions well. 1...|$|E
40|$|AbstractThe DCT (Discrete Cosine Transform) based coding {{process of}} full color images is {{standardized}} by the JPEG (Joint Photographic Expert Group). The JPEG method is applied widely, for example a color facsimile. The <b>quantization</b> <b>table</b> in the JPEG coding influences image quality. However, detailed research is not accomplished sufficiently about a <b>quantization</b> <b>table.</b> Therefore, we study {{the relations between}} <b>quantization</b> <b>table</b> and image quality. We examine first the influence to image quality given by <b>quantization</b> <b>table.</b> <b>Quantization</b> <b>table</b> is grouped into four bands by frequency. When each value of bands is changed, the merit and demerit of color image are examined. At the present time, we analyze the deterioration component of a color image. We study {{the relationship between the}} <b>quantization</b> <b>table</b> and the restoration image. Color image is composed of continuoustone level and we evaluate the deterioration component visually. We also analyze it numerically. An analysis method using the 2 -D FFT (Fast Fourier Transform) can catch a change of a color image data by a <b>quantization</b> <b>table</b> change. On the basis of these results, we propose a <b>quantization</b> <b>table</b> using Fibonacci numbers...|$|E
40|$|This paper {{discusses}} lossy {{image compression}} using Hadamard Transform (HT). <b>Quantization</b> <b>table</b> plays {{significant role in}} image compression (lossy) that improves the compression ratio without sacrificing visual quality. In this work, Human Visual system (HVS) is considered to derive the <b>quantization</b> <b>table,</b> which is applicable for Hadamard Transform. By incorporating the human visual system with the uniform quantizer, a perceptual <b>quantization</b> <b>table</b> is derived. This <b>quantization</b> <b>table</b> is easy {{to adapt to the}} specified resolution for viewing. Results show that this <b>quantization</b> <b>table</b> is good in terms of improving peak signal to noise ratio (PSNR), Normalized Cross correlation (NCC) and to reduce blocking artifacts. This work is extended to test the robustness of watermarking against various attacks...|$|E
40|$|The study {{presented}} in this paper aims at quantifying the empirical limits of JPEG optimization, when the compressed stream is standard compliant and only the <b>quantization</b> <b>tables</b> are optimized. Image-dependent <b>quantization</b> <b>tables,</b> which minimize the bitrate of the compressed image while maintaining transparent visual quality, are identified by means of a psychovisual experiment. The results demonstrate that significant room for improving JPEG compression efficiency is available. 1...|$|R
40|$|Part 3 : FORENSIC TECHNIQUESInternational audienceMany {{techniques}} {{have been proposed}} for file recovery, but recovering fragmented files is still a challenge in digital forensics, especially when the files are damaged. This chapter focuses on JPEG files, {{one of the most}} popular photograph formats, and proposes techniques for recovering partially-damaged standalone JPEG fragments by reconstructing pseudo headers. The techniques deal with missing Huffman tables and sub-sampling factors, estimate the resolution of fragments, assess the image quality of JPEG files with incorrect <b>quantization</b> <b>tables,</b> and create <b>quantization</b> <b>tables</b> that are very close to the correct <b>quantization</b> <b>tables</b> in a reasonable amount of time. Experiments with real camera pictures demonstrate that the techniques can recover standalone fragments accurately and efficiently...|$|R
30|$|The {{conventional}} DVC systems usually {{focus on}} the performance improvement [11 – 13, 16, 17] or management of feedback channel [6 – 9]. The proposed DVC system deals with the precise bit rate control method and performance improvement. The proposed system provides the efficient bit rate control method by using the bitplanewise zigzag scanning method. While seven or eight fixed <b>quantization</b> <b>tables</b> {{are used in the}} conventional system, the number of available <b>quantization</b> <b>tables</b> in the proposed system is equal to the number of bitplanes. For example, for the seventh quantizer table in the conventional system, the number of available <b>quantization</b> <b>tables</b> in the proposed system is 63, which is the number of bitplanes. Thus, more <b>quantization</b> <b>tables</b> should be made in the proposed system. The increased decoder complexity for making more <b>quantization</b> <b>tables</b> is almost negligible, since they can be easily generated using the bitplanewise zigzag scanning, which is shown in Figure[*] 6 and Figure[*] 7. The decoder complexity for refining side information doesn’t increase compared to the conventional DVC systems, since the same dequantization method as the conventional DVC systems is used for refining. By setting the initial decoded Wyner-Ziv frame as the side information and refining the side information progressively, the proposed system shows better performance than the conventional DVC systems at low bit rates.|$|R
40|$|Keypoint or {{interest}} point detection {{is the first}} step in many com-puter vision algorithms. The detection performance of the state-of-the-art detectors is, however, strongly influenced by compression ar-tifacts, especially at low bit rates. In this paper, we design a novel <b>quantization</b> <b>table</b> for the widely-used JPEG compression standard which leads to improved feature detection performance. After an-alyzing several popular scale-space based detectors, we propose a novel <b>quantization</b> <b>table</b> which is based on the observed impact of scale-space processing on the DCT basis functions. Experimental results show that the novel <b>quantization</b> <b>table</b> outperforms the JPEG default <b>quantization</b> <b>table</b> in terms of feature repeatability, number of correspondences, matching score, and number of correct matches...|$|E
30|$|Additionally, {{in order}} to compare the {{efficiency}} of the above components, we alter the DCT <b>quantization</b> <b>table</b> to control the quality of the JPEG image based on the Y coefficient and the UV coefficient. There are two types of a DCT <b>quantization</b> <b>table</b> such as the table for UV component and the table for Y component. In each <b>quantization</b> <b>table,</b> there are 64 quantized coefficients. If we alter these quantization tables, we can easily control the quality of digital content with low computational cost.|$|E
30|$|By using <b>quantization</b> <b>table,</b> we {{can obtain}} the {{quantized}} ‘residual noise’.|$|E
40|$|The JPEG {{algorithm}} {{is one of}} the most used tools for compressing images. The main factor affecting the performance of the JPEG compression is the quantization process, which exploits the values contained in two <b>tables,</b> called <b>quantization</b> <b>tables.</b> The compression ratio and the quality of the decoded images are determined by these values. Thus, the correct choice of the <b>quantization</b> <b>tables</b> is crucial to the performance of the JPEG algorithm. In this paper, a two-objective evolutionary {{algorithm is}} applied to generate a family of optimal <b>quantization</b> <b>tables</b> which produce different trade-offs between image compression and quality. Compression is measured in terms of difference in percentage between the sizes of the original and compressed images, whereas quality is computed as mean squared error between the reconstructed and the original images. We discuss the application of the proposed approach to well-known benchmark images and show how the <b>quantization</b> <b>tables</b> determined by our method improve the performance of the JPEG algorithm with respect to the default tables suggested in Annex K of the JPEG standard...|$|R
40|$|In {{this paper}} we propose a method for {{computing}} optimal <b>quantization</b> <b>tables</b> for specific images. The main criterion for this processing is the allocation of bandwidth in frequency subspaces in the DCT-domain according to power metrics obtained from the transform coefficients. Choice of the weights determines the subjective importance of each frequency coefficient as well as its contribution the finally perceived image. The simultaneous requirement that the <b>quantization</b> <b>tables</b> yield data compression comparable to the one achieved by the baseline JPEG scheme at various quality factors (QF) imposes an additional constraint to the proposed model...|$|R
40|$|JPEG {{is one of}} {{the most}} widely used image formats, but in some ways remains {{surprisingly}} unoptimized, perhaps because some natural optimizations would go outside the standard that defines JPEG. We show how to improve JPEG compression in a standard-compliant, backward-compatible manner, by finding improved default <b>quantization</b> <b>tables.</b> We describe a simulated annealing technique that has allowed us to find several <b>quantization</b> <b>tables</b> that perform better than the industry standard, in terms of both compressed size and image fidelity. Specifically, we derive tables that reduce the FSIM error by over 10 % while improving compression by over 20 % at quality level 95 in our tests; we also provide similar results for other quality levels. While we acknowledge our approach can in some images lead to visible artifacts under large magnification, we believe use of these <b>quantization</b> <b>tables,</b> or additional tables that could be found using our methodology, would significantly reduce JPEG file sizes with improved overall image quality. Comment: Appendix not included in arXiv version due to size restrictions. For full paper go to: [URL]...|$|R
40|$|Steganography {{techniques}} are useful to convey hidden information using {{various types of}} typically-transmitted multimedia data as cover file to conceal communication. When using JPEG as a cover file, normally message is hidden in the AC values of quantized DCT coefficients. However, concealment of message in <b>quantization</b> <b>table</b> {{is yet to be}} done. In this paper, a novel in-DQT technique for message hiding in JPEG’s <b>quantization</b> <b>table</b> using bytewise insertion is proposed. By using in-DQT technique on standard JPEG test images for cover files with up to 24 bytes of message, results show that steganography image with minimal acceptable image distortion. Thus, in-DQT technique provides alternative for embedding message in <b>quantization</b> <b>table...</b>|$|E
30|$|The {{second is}} a <b>quantization</b> <b>table</b> {{modification}} approach proposed by Fridrich et al. [21] and later improved by Wang et al. [22]. Their techniques work by preprocessing the quantized DCT coefficients and modifying the <b>quantization</b> <b>table</b> to create space for data hiding. Although the experimental results of [22] achieve high peak {{signal to noise}} ratio (PSNR), the file size increases greatly.|$|E
3000|$|... is the <b>quantization</b> <b>table</b> entry at {{position}} i. Note that the nominator {{of the equation}} represents the total embedding capacity when AC coefficients in position i are used for embedding, whereas the denominator represents the corresponding estimated distortion: sum of total shiftable AC coefficients {{and half of the}} embedding capacity (assuming the payload is pseudorandom, approximately half of the embeddable will cause distortion) with the squared quantization term of penalty. The reason to squaring the <b>quantization</b> <b>table</b> entry is, to consider its effect on de-quantization phase. Although the number of embeddable coefficients are many and shiftable coefficients are small on a given position, if the <b>quantization</b> <b>table</b> entry is a large number, then image may be highly distorted after data embedding. To make clear the difference between positions with respect to the value of R, this method squared the <b>quantization</b> <b>table</b> values. So, from two positions which have similar distortion (denominator result from Eq. (4) without squaring Q [...]...|$|E
40|$|An {{extension}} of the standard JPEG image compression known as JPEG- 3 allows rescaling of the quantization matrix to achieve a certain image output quality. Recently, Tchebichef Moment Transform (TMT) has been introduced {{in the field of}} image compression. TMT has been shown to perform better than the standard JPEG image compression. This study presents an adaptive TMT image compression. This task is obtained by generating custom <b>quantization</b> <b>tables</b> for low, medium and high image output quality levels based on a psychovisual model. A psychovisual model is developed to approximate visual threshold on Tchebichef moment from image reconstruction error. The contribution of each moment will be investigated and analyzed in a quantitative experiment. The sensitivity of TMT basis functions can be measured by evaluating their contributions to image reconstruction for each moment order. The psychovisual threshold model allows a developer to design several custom TMT <b>quantization</b> <b>tables</b> for a user to choose from according to his or her target output preference. Consequently, these <b>quantization</b> <b>tables</b> produce lower average bit length of Huffman code while still retaining higher image quality than the extended JPEG scaling scheme...|$|R
40|$|The psychovisual {{technique}} {{has brought about}} significant improvement in pursuing image analysis and image reconstruction. The psychovisual threshold can be utilized to find the optimal bits-budget for image signals. The psychovisual system is developed based on noticeable distortion of the compressed image from an original image in frequency order. This paper proposes an image compression technique using Tchebichef psychovisual threshold for generating an optimal bits-budget of image signals. The bits-budget is designed to replace the main role of <b>quantization</b> <b>tables</b> in image compression. The experimental {{results show that the}} proposed bits-budget technique can improve the visual quality of image output by 42 of JPEG compression. The visual image quality of Thcebichef bits allocation produces less artifact effect and distortion of the image pixels. A set of bits-budgets gives excellent improvement in the image quality at a low average bit length of Huffman code than the image coding using <b>quantization</b> <b>tables...</b>|$|R
40|$|We {{describe}} effective {{channel coding}} strategies {{which can be}} used in conjunction with linear programming optimization techniques for the embedding of robust perceptually adaptive DCT domain watermarks. The main contributions lie in the proposal of a coding strategy based on the magnitude of a DCT coefficient, the use of turbo codes for effective error correction, and finally the incorporation of JPEG <b>quantization</b> <b>tables</b> at embedding. 1...|$|R
40|$|An image {{enhancement}} algorithm for low-vision television viewing was developed. The algorithm enhances {{the images in}} the JPEG/MPEG domain by weighting the <b>quantization</b> <b>table.</b> The advantages of our algorithm are threefold: (1) Low computation (only a change of the 8 × 8 <b>quantization</b> <b>table</b> in the decoding stage); (2) Suitable for real–time application; and (3) easily manipulated by individual users, because the enhancement level can be controlled at the receiver. 1...|$|E
40|$|In this paper, {{we propose}} a passive-blind scheme for {{detecting}} forged images. The scheme leverages <b>quantization</b> <b>table</b> estimation {{to measure the}} inconsistency among images. To improve {{the accuracy of the}} estimation process, each AC DCT coefficient is first classified into a specific type; then the corresponding quantization step size is measured adaptively from its energy density spectrum (EDS) and the EDS's Fourier transform. The proposed content-adaptive <b>quantization</b> <b>table</b> estimation scheme is comprised of three phases: pre-screening, candidate region selection, and tampered region identification. In the pre-screening phase, we determine whether an input image has been JPEG compressed, and count the number of quantization steps whose size is equal to one. To select candidate regions for estimating the <b>quantization</b> <b>table,</b> we devise a candidate region selection algorithm based on seed region generation and region growing. First, the seed region generation operation finds a suitable region by removing suspect regions, after which the selected seed region is merged with other suitable regions to form a candidate region. To avoid merging suspect regions, a candidate region refinement operation is performed in the region growing step. After estimating the <b>quantization</b> <b>table</b> from the candidate region, an maximum-likelihood-ratio classifier exploits the inconsistency of the <b>quantization</b> <b>table</b> to identify tampered regions block by block. To evaluate the scheme's performance in terms of tampering detection, three common forgery techniques, copy-paste tampering, inpainting, and composite tampering, are used. Experiment results demonstrate that the proposed scheme can estimate quantization tables and identify tampered regions effectively...|$|E
40|$|Abstract: Blind {{watermarking}} {{utilized the}} <b>quantization</b> <b>table</b> to embed the watermark by adding or subtracting a quantized constant value. When the watermarked image processes malicious manipulations the constant <b>quantization</b> <b>table</b> is not robust. In this paper, a new adaptive and duplicate quantized blind watermarking system is proposed to attend higher transparency. We use an adaptive <b>quantization</b> <b>table</b> {{to advance the}} robustness according to {{the properties of the}} human visual system and just noticeable distortion. And the error-correcting based watermarking will have the property that corrects errors of the extracted watermark automatically. A multiresolution watermarking based on the wavelet transformation is selected and therefore it can resist the destruction of image processing. In our experiments, the results show that the robustness and transparency of the adaptive watermarking is much better than the traditional one...|$|E
50|$|The header {{consists}} of many parts and may include <b>quantization</b> <b>tables</b> and 2048 bits of user data. Each frame also has two GUIDs and timestamp. The frame header is packed into big-endian dwords. Actual frame data {{consists of}} packed macroblocks using a technique {{almost identical to}} JPEG: DC prediction and variable-length codes with run length encoding for other 63 coefficients.DC coefficient is not quantized.|$|R
40|$|We {{propose a}} tracking-aware system that removes video {{components}} of low tracking interest and optimizes the quantization during compression of frequency coefficients, {{particularly those that}} most influence trackers, significantly reducing bitrate while maintaining comparable tracking accuracy. We utilize tracking accuracy as our compression criterion in lieu of mean squared error metrics. The process of optimizing <b>quantization</b> <b>tables</b> suitable for automated tracking can be executed online or offline. The online implementation initializes the encoding procedure for a specific scene, but introduces delay. On the other hand, the offline procedure produces globally optimum <b>quantization</b> <b>tables</b> where the optimization occurs for a collection of video sequences. Our proposed system is designed with low processing power and memory requirements in mind, and as such can be deployed on remote nodes. Using H. 264 /AVC video coding and a commonly used state-of-the-art tracker we show that while maintaining comparable tracking accuracy our system allows for over 50 % bitrate savings on top of existing savings from previous work. Index Terms — Urban traffic video tracking, transportation, video compression, quantization, preprocessing, postprocessin...|$|R
40|$|A psychovisual {{experiment}} prescribes the quantization {{values in}} image compression. The quantization process {{is used as}} a threshold of the human visual system tolerance {{to reduce the amount of}} encoded transform coefficients. It is very challenging to generate an optimal quantization value based on the contribution of the transform coefficient at each frequency order. The psychovisual threshold represents the sensitivity of the human visual perception at each frequency order to the image reconstruction. An ideal contribution of the transform at each frequency order will be the primitive of the psychovisual threshold in image compression. This research study proposes a psychovisual threshold on the large discrete cosine transform (DCT) image block which will be used to automatically generate the much needed <b>quantization</b> <b>tables.</b> The proposed psychovisual threshold will be used to prescribe the quantization values at each frequency order. The psychovisual threshold on the large image block provides significant improvement in the quality of output images. The experimental results on large <b>quantization</b> <b>tables</b> from psychovisual threshold produce largely free artifacts in the visual output image. Besides, the experimental results show that the concept of psychovisual threshold produces better quality image at the higher compression rate than JPEG image compression...|$|R
40|$|To process {{previously}} JPEG coded images {{the knowledge}} of the <b>quantization</b> <b>table</b> used in compression is sometimes required. This happens for example in JPEG artifact removal and in JPEG re-compression. However, the <b>quantization</b> <b>table</b> might not be known due to various reasons. In this paper, a method is presented for the maximum likelihood estimation (MLE) of the JPEG quantization tables. An efficient method is also provided to identify if an image has been previously JPEG compressed. 1...|$|E
3000|$|... where Tc is {{the matrix}} of {{correction}} factors {{for each of}} the 8 [*]×[*] 8 DCT coefficients, which was normalized based on the JPEG <b>quantization</b> <b>table</b> in [22].|$|E
30|$|In the {{existing}} low power line compression method, Ham et al. [31] have expanded 16 quantization levels of {{the existing}} single line-based line compression study [27] to 89 levels for a more precise bit rate control. In the proposed method, the number of quantization levels is reduced to 80 to decrease transmission cost. An additional 80 -level <b>quantization</b> <b>table</b> for 2 D DWT coefficients is also included and quantization parameters of each level are defined to be compatible to those of PSNR of the same quantization level in the 1 L mode <b>quantization</b> <b>table.</b>|$|E
40|$|Abstract- A complete, {{low cost}} {{baseline}} JPEG encoder soft IP and its chip implementation {{are presented in}} this paper. It features user-defined, run-time re-configurable <b>quantization</b> <b>tables,</b> highly modularized and fully pipelined architecture. A prototype, synthesized with COMPASS cell library, has been implemented in TSMC 0. 6 -pm single-poly, triple-metal process. It can run up to 40 MHz at 3. 3 V. This IP can be easily integrated into various application systems, such as scanner, PC camera and color FAX, etc. I...|$|R
40|$|A {{method for}} {{simultaneously}} estimating the high-resolution frames {{and the corresponding}} motion field from a compressed low-resolution video sequence is presented. The algorithm incorporates knowledge of the spatio-temporal correlation between low and high-resolution images to estimate the original high-resolution sequence from the degraded low-resolution observation. Information from the encoder is also exploited, including the transmitted motion vectors, <b>quantization</b> <b>tables,</b> coding modes and quantizer scale factors. Simulations illustrate an improvement in the peak signal-to-noise ratio when compared with traditional interpolation techniques and are corroborated with visual results...|$|R
40|$|Abstract. The Joint Photographic Experts Group (JPEG) {{baseline}} system, {{which is}} scheduled to be standardized in 1992, is applied to character images, and the characteristics of the application are investigated. The JPEG system is suitable for continuous-tone im-ages, however, continuous-tone images are usually accompanied by characters. The image quality of characters is investigated on various magnitudes of <b>quantization</b> <b>tables</b> and the deterioration mechanisms are discussed. A method ofimage quality improvement that is accomplished by density transformation after decoding is proposed and its effects are confirmed. ...|$|R
40|$|In this paper, {{we propose}} a scheme of {{watermark}} embedding and extracting based on DCT transform and JPEG <b>quantization</b> <b>table.</b> Firstly, {{the image is}} divided into non-overlapping 8 × 8 blocks, and each block is transformed by DCT. Then, a pair of points with the same quantization value is selected by the JPEG <b>quantization</b> <b>table</b> in order to embed one watermark bit, and the adjustment coefficients are adaptively selected by using the visual masking property of HVS. The experimental results {{presented in this paper}} showed that our proposed algorithm is robust to JPEG compression under different quality factors...|$|E
3000|$|... and, respectively, PSNR-HVS {{account for}} {{different}} sensitivity of human eyes to distortions in {{low and high}} spatial frequencies similarly to what JPEG standard does by using nonuniform <b>quantization</b> <b>table</b> [11, 14]. The metrics [...]...|$|E
3000|$|... are preferred. For {{any value}} of the quality factor, Q[*]=[*][qm,n][*]=[*]λQdefault, where Qdefault is the default <b>quantization</b> <b>table</b> defined by the Independent JPEG Group (IJG) and λ is a {{constant}} determined by the quality factor. Hence, we have [...]...|$|E
3000|$|... [...]. Thus, if ICC is 1 for {{all frames}} and {{parameter}} bands, exact OPDs {{can be obtained}} from IID and IPD parameters using Equation (6), which results in bit saving, since we do not need to quantize the OPDs [13]. But if the ICC is not 1, this method may lead to the wrong OPD and, in turn, cause degradation of audio quality, which will be explained in more detail in the next section. The above-mentioned OPD estimation methods were developed using the <b>quantization</b> <b>tables</b> specified in PS [5].|$|R
40|$|The {{compression}} of video {{can reduce the}} accuracy of post-compression tracking algorithms. This is problematic for centralized applications such as traffic surveillance systems, where remotely captured and compressed video is transmitted to a central location for tracking. We propose a low complexity optimization framework that automatically identifies video features critical to tracking and concentrates bitrate on these features via <b>quantization</b> <b>tables.</b> Using the H. 264 video coding standard and two commonly used state-of-the-art trackers we show that our algorithm allows for over 60 bitrate savings while maintaining comparable tracking accuracy...|$|R
40|$|The Information {{exchange}} via any media needs {{privacy and}} secrecy. Cryptography {{is widely used}} for providing privacy and secrecy between the sender and receiver. But, now, along with Cryptography, we are using Steganography to have more protection to our hidden data. In this paper, we show how a JPEG {{can be used as}} an embedding space for a message by adjusting the values in the JPEG <b>Quantization</b> <b>tables</b> (QTs). This scheme also uses some permutation algorithms and it can be widely used for secret communication. This JPEG double compression will give satisfactory decoded results...|$|R
