333|2085|Public
50|$|According to GMAC, {{the reading}} {{comprehension}} <b>question</b> <b>type</b> tests ability to analyze information and draw a conclusion. Reading comprehension passages can be anywhere {{from one to}} several paragraphs long. According to GMAC, the critical reasoning <b>question</b> <b>type</b> assesses reasoning skills. According to GMAC, the sentence correction <b>question</b> <b>type</b> tests grammar and effective communication skills. From the available answer options, the test taker should select the most effective construction that best expresses {{the intent of the}} sentence.|$|E
5000|$|Medium <b>Question</b> <b>type</b> - four {{questions}} per item, 12 questions (3 items) ...|$|E
50|$|Known {{locally as}} PhilNITS IP, the Information Technology Passport Exam is for {{individuals}} who have basic knowledge in IT that all business workers should commonly possess, and who are doing information technology related tasks or trying to utilize IT related technology in their tasks. The exam lasts about 165 minutes (conducted during a morning schedule). It consists of 100 questions in multiple choice (one per four choices) broke down into two types: the short <b>question</b> <b>type,</b> one question per item, 88 questions; and medium <b>question</b> <b>type,</b> four questions per item, 12 questions (3 items).|$|E
40|$|The authors {{investigate}} {{the interplay between}} answer quality and answer speed across <b>question</b> <b>types</b> in community question-answering sites (CQAs). The research questions addressed are the following: (a) How do answer quality and answer speed vary across <b>question</b> <b>types?</b> (b) How do the relationships between answer quality and answer speed vary across <b>question</b> <b>types?</b> (c) How do the best quality answers and the fastest answers differ in terms of answer quality and answer speed across <b>question</b> <b>types?</b> (d) How do trends in answer quality vary over time across <b>question</b> <b>types?</b> From the posting of 3, 000 questions in six CQAs, 5, 356 answers were harvested and analyzed. There {{was a significant difference}} in answer quality and answer speed across <b>question</b> <b>types,</b> and there were generally no significant relationships between answer quality and answer speed. The best quality answers had better overall answer quality than the fastest answers but generally took longer to arrive. In addition, although the trend in answer quality had been mostly random across all <b>question</b> <b>types,</b> the quality of answers appeared to improve gradually when given time. By highlighting the subtle nuances in answer quality and answer speed across <b>question</b> <b>types,</b> this study is an attempt to explore a territory of CQA research that has hitherto been relatively uncharted...|$|R
40|$|In {{the context}} of online {{learning}} environments, the argumentation skill becomes an important priority for sustained engagement of students in productive negotiation of ideas and evidence based reasoning (Siakidou, Papadouris, & Constantinou, 2014). The {{aim of this study}} is to reveal the quality of argumentation skills stimulation used a variety of questions in Facebook group discussion for Biology student teachers. The method of this work was using quasi experiment with time series design. This study involved 24 Biology student teachers who contacted in online and expressed willingness to take part in Facebook group discussions. Results indicate that divergence <b>questions</b> <b>types</b> have a greater influence on the argumentation skills quantity rather than focal and brainstorming <b>questions</b> <b>types,</b> while the focal <b>question</b> <b>types</b> have a greater influence on the quantity of argumentation skills rather than brainstorming <b>questions</b> <b>types.</b> In addition, this study shows that brainstorming <b>questions</b> <b>types</b> (FA) have a better effect to coding scheme for individual arguments rather than focal <b>questions</b> <b>types</b> (NFA) and the divergent <b>questions</b> <b>types</b> (NJA) to stimulating the quality of argumentation skills of Biology student teachers. Further research to uncover the effect of topic discussion on the arguments quality is expected to enrich the findings in this study...|$|R
30|$|Effect of <b>question</b> <b>types</b> on {{accuracy}} score.|$|R
5000|$|The system takes {{a natural}} {{language}} question as an input {{rather than a}} set of keywords, for example, [...] "When is the national day of China?" [...] The sentence is then transformed into a query through its logical form. Having the input {{in the form of}} a natural language question makes the system more user-friendly, but harder to implement, as there are various question types and the system will have to identify the correct one in order to give a sensible answer. Assigning a <b>question</b> <b>type</b> to the question is a crucial task, the entire answer extraction process relies on finding the correct <b>question</b> <b>type</b> and hence the correct answer type.|$|E
50|$|The verbal {{section of}} the GMAT exam {{includes}} the following question types: reading comprehension, critical reasoning, and sentence correction. Each <b>question</b> <b>type</b> gives five answer options from which to select. Verbal scores range from 0 to 60; however, scores below 9 or above 44 are rare.|$|E
5000|$|To {{select the}} next {{question}} in the sequence, VTT has to estimate the predictability of every proposed question. This is done using the annotated training set of Images. Each Image is annotated with bounding box around the objects and labelled with the attributes, and pairs of objects are labelled with the relations.Let us consider each <b>question</b> <b>type</b> separately: ...|$|E
5000|$|Assessments and self-tests with {{different}} <b>question</b> <b>types,</b> surveys ...|$|R
40|$|Current-generation {{assessment}} tools used in K- 12 and post-secondary education {{are limited in}} the <b>type</b> of <b>questions</b> they support; this limitation {{makes it difficult for}} instructors to navigate their assessment engines. Furthermore, the <b>question</b> <b>types</b> tend to score low on Bloom’s Taxonomy. Dedicated learning management systems (LMS) such as Blackboard, Moodle and Canvas are somewhat better than informal tools as they offer more <b>question</b> <b>types</b> and some randomization. Still, <b>question</b> <b>types</b> in all the major LMS assessment engines are limited. Additionally, LMSs place a heavy burden on teachers to generate online assessments. In this study we analyzed the top three LMS providers to identify inefficiencies. These inefficiencies in LMS design, point us to ways to ask better questions. Our findings show that teachers have not adopted current tools because they do not offer definitive improvements in productivity. Therefore, we developed LiquiZ, a design for a next-generation assessment engine that reduces user effort and provides more advanced <b>question</b> <b>types</b> that allow teachers to ask questions that can currently only be asked in one-on-one demonstration. The initial LiquiZ project is targeted toward STEM subjects, so the <b>question</b> <b>types</b> are particularly advantageous in math or science subjects...|$|R
40|$|Any {{attempt to}} compare {{assessment}} engines {{is difficult for}} several reasons. One {{of these is the}} terminology that is used to identify <b>question</b> <b>types.</b> As computer assisted assessment develops and extends, new assessment systems are introduced. It is a competitive sector and for those commercial companies involved, a measure of uniqueness is advantageous. All too often this can result in an undue emphasis on finding ways of naming <b>question</b> <b>types</b> to produce the largest number. Close scrutiny reveals that many of these types are derived from the same basic structure with different formatting. The clear cut naming of the initial <b>question</b> <b>types</b> during {{the first few years of}} computer assisted assessment worked well but advances in the technology and innovative approaches to assessment are making this convention difficult to sustain. The work of the IMS QTI group (IMS QTI project 2002) is very valuable and the issue of <b>question</b> <b>types</b> is partly addressed by them. A new structure and naming convention for <b>question</b> <b>types</b> that can be implemented by all interested parties is needed urgently. There are two aspects to this. 1. A naming convention that would interest those involved in IMS QTI standards and build on the work already undertaken. (the technical sector) 2. A naming convention for the authors, users, academics and researchers interested in what <b>question</b> <b>types</b> are available. (the non-technical sector) The advantages of such a hierarchy would include • progress in interoperability • progress in the use of item banking • stronger focus on the aims of assessment • greater awareness of the true <b>question</b> <b>types</b> available This paper proposes such a hierarchy developed from a non-technical viewpoint but with a sound structure as a basis for discussion, development and to motivate interest in research in this area...|$|R
5000|$|Borderline groups method (person-centered): A {{description}} {{is prepared for}} each performance category. SMEs are asked to submit a list of participants whose performance on the test should {{be close to the}} performance standard (borderline). The test is administered to these borderline groups and the median test score is used as the cut score. This method can be used with virtually any <b>question</b> <b>type</b> (e.g., multiple-choice, multiple response, essay, etc.).|$|E
50|$|Gray et al. (2013) {{explored}} how bridging social capital, <b>question</b> <b>type</b> {{and relational}} closeness influence the perceived usefulness and satisfaction of information obtained through questions asked on Facebook. Their {{results indicated that}} bridging social capital could positively predict the perceived utility of the acquired information, meaning that information exchanges on social networks is an effective way of social capital conversion. Also, useful answers {{are more likely to}} be received from weak ties than strong ties.|$|E
50|$|Problem solving {{questions}} {{are designed to}} test the ability to reason quantitatively and to solve quantitative problems. Data sufficiency is a <b>question</b> <b>type</b> unique to the GMAT designed to measure the ability to understand and analyze a quantitative problem, recognize what information is relevant or irrelevant and determine at what point there is enough information {{to solve a problem}} or recognize {{the fact that there is}} insufficient information given to solve a particular problem.|$|E
40|$|This is {{a conference}} paper. One {{of these is}} the {{terminology}} that is used to identify <b>question</b> <b>types.</b> As computer assisted assessment develops and extends, new assessment systems are introduced. It is a competitive sector and for those commercial companies involved, a measure of uniqueness is advantageous. All too often this can result in an undue emphasis on finding ways of naming <b>question</b> <b>types</b> to produce the largest number. Close scrutiny reveals that many of these types are derived from the same basic structure with different formatting. The clear cut naming of the initial <b>question</b> <b>types</b> during {{the first few years of}} computer assisted assessment worked well but advances in the technology and innovative approaches to assessment are making this convention difficult to sustain. The work of the IMS QTI group (IMS QTI project 2002) is very valuable and the issue of <b>question</b> <b>types</b> is partly addressed by them. A new structure and naming convention for <b>question</b> <b>types</b> that can be implemented by all interested parties is needed urgently. There are two aspects to this. 1. A naming convention that would interest those involved in IMS QTI standards and build on the work already undertaken. (the technical sector) 2. A naming convention for the authors, users, academics and researchers interested in what <b>question</b> <b>types</b> are available. (the non-technical sector) The advantages of such a hierarchy would include • progress in interoperability • progress in the use of item banking • stronger focus on the aims of assessment • greater awareness of the true <b>question</b> <b>types</b> available This paper proposes such a hierarchy developed from a non-technical viewpoint but with a sound structure as a basis for discussion, development and to motivate interest in research in this area...|$|R
3000|$|... 2008) and Isozaki and Higashinaka (2008) {{reported}} that {{the performance of the}} system improved when the <b>question</b> <b>types</b> were classified into classes such as “how-questions” and “why-questions” in advance. However, Ishioroshi et al. (2009) and Soricut and Brill (2006) developed a QA system without classification of the <b>question</b> <b>types.</b> Ishioroshi et al. (2009) estimated the topic relevance by relevance feedback from the Web.|$|R
40|$|A study {{investigated}} the relationship between relative language proficiency and the <b>types</b> of <b>questions</b> produced by bilingual children in different settings and situations, {{taking into account the}} whole language repertoire in both languages and in different settings. The subjects were six third-graders in a self-contained maintenance bilingual education program in a midwestern city. Video- and audio-taped data collected in the classroom, at home, and in the park were analyzed. The data show that the children produced the same question repertoire previously found in English monolingual children and adults. Children asked more questions in the language in which they were more proficient. Certain <b>question</b> <b>types</b> appeared only in children who were proficient in a language, while other <b>question</b> <b>types</b> were characteristic of the speech of limited-proficiency children. The use of <b>question</b> <b>types</b> wa...|$|R
5000|$|Contrasting groups method (person-centered): SMEs {{are asked}} to {{categorize}} the participants in their classes according to the performance category descriptions. The test is administered {{to all of the}} categorized participants and the test score distributions for each of the categorized groups are compared. Where the distributions of the contrasting groups intersect is where the cut score would be located. This method can be used with virtually any <b>question</b> <b>type</b> (e.g., multiple-choice, multiple response, essay, etc.).|$|E
5000|$|Whatshisname Question: In this question, {{the host}} {{is trying to}} {{remember}} a certain name. A clue is provided every few seconds, and the players must buzz in and type the name. In HeadRush, this <b>question</b> <b>type</b> is known as Old Man's Moldy Memories and in YDKJ - 2015 as Foggy Facts with Old Man both featuring the character of [...] "Old Man", voiced by Andy Poland in which he hosts the question.|$|E
5000|$|Keyword {{extraction}} is {{the first}} step for identifying the input <b>question</b> <b>type.</b> In some cases, there are clear words that indicate the <b>question</b> <b>type</b> directly. i.e. [...] "Who", [...] "Where" [...] or [...] "How many", these words tell the system that the answers should be of type [...] "Person", [...] "Location", [...] "Number" [...] respectively. In the example above, the word [...] "When" [...] indicates that the answer should be of type [...] "Date". POS (Part of Speech) tagging and syntactic parsing techniques {{can also be used to}} determine the answer type. In this case, the subject is [...] "Chinese National Day", the predicate is [...] "is" [...] and the adverbial modifier is [...] "when", therefore the answer type is [...] "Date". Unfortunately, some interrogative words like [...] "Which", [...] "What" [...] or [...] "How" [...] do not give clear answer types. Each of these words can represent more than one type. In situations like this, other words in the question need to be considered. First thing to do is to find the words that can indicate the meaning of the question. A lexical dictionary such as WordNet can then be used for understanding the context.|$|E
50|$|QA {{research}} {{attempts to}} deal {{with a wide range of}} <b>question</b> <b>types</b> including: fact, list, definition, How, Why, hypothetical, semantically constrained, and cross-lingual questions.|$|R
40|$|There is a {{proliferation}} of categorization schemes in the scientific literature that have mostly been developed from psychologists’ {{understanding of the nature}} of linguistic interactions. This has a led to problems in defining <b>question</b> <b>types</b> used by interviewers. Based on the principle that the overarching purpose of an interview is to elicit information and that questions can function both as actions in their own right and as vehicles for other actions, a Conversational Analysis approach was used to analyse a small number of police interviews. The analysis produced a different categorization of <b>question</b> <b>types</b> and, in particular, the conversational turns fell into two functional types: (i) Topic Initiation Questions and (ii) Topic Facilitation Questions. We argue that forensic interviewing requires a switch of focus from the ‘words’ used by interviewers in <b>question</b> <b>types</b> to the ‘function’ of conversational turns within interviews...|$|R
30|$|Finally, in our study, <b>question</b> <b>types</b> indeed had a {{significant}} effect on user performance that may affect the usability and security of smartphone based authentication systems.|$|R
5000|$|Once the <b>question</b> <b>type</b> {{has been}} identified, an Information {{retrieval}} system {{is used to}} find a set of documents containing the correct key words. A tagger and NP/Verb Group chunker {{can be used to}} verify whether the correct entities and relations are mentioned in the found documents. For questions such as [...] "Who" [...] or [...] "Where", a Named Entity Recogniser is used to find relevant [...] "Person" [...] and [...] "Location" [...] names from the retrieved documents. Only the relevant paragraphs are selected for ranking.|$|E
5000|$|Road Kill/Coinkydink: Exists in The Ride (as RoadKill) and Mock 2 (as Coinkydink). In this fast-paced <b>question</b> <b>type,</b> {{players are}} given two clues (Such as [...] "Sexy voice" [...] and [...] "Hefty kid"). Several words fly past in rapid succession, {{and the players}} must buzz in when the word on the screen connects the two clues in a pair (In this case, the answer is [...] "husky"). At {{the end of the}} round, players can earn a bonus for {{choosing}} the category which all the correct answers have in common.|$|E
5000|$|A {{vector space}} {{model can be}} used as a {{strategy}} for classifying the candidate answers. Check if the answer is of the correct type as determined in the <b>question</b> <b>type</b> analysis stage. Inference technique can also be used to validate the candidate answers. A score is then given to each of these candidates according to the number of question words it contains and how close these words are to the candidate, the more and the closer the better. The answer is then translated into a compact and meaningful representation by parsing. In the previous example, the expected output answer is [...] "1st Oct." ...|$|E
3000|$|... [...]). For example, {{response}} time (i.e., the time taken {{to answer the}} questions) can be one such feature. Please note that we create separate models for each <b>question</b> <b>types.</b> For example, for user 1, we have three different models for three different <b>question</b> <b>types.</b> Moreover, we have one model that represents strong adversary, one model that represents naive adversary and one model that represents the community of strong and naive adversaries in the system (see Section “Accuracy of model‐based authentication for Study ‐ I and Study ‐ II”).|$|R
40|$|ArikIturri is {{a system}} {{developed}} for the automatic generation of didactic resources. More specifically, it {{is focused on the}} automatic generation of questions based on NLP tools. In this paper, we present an evaluation of the error correction and multiple-choice <b>question</b> <b>types</b> generated by ArikIturri. This evaluation has been carried out with the collaboration of four expert editors. In the mentioned <b>question</b> <b>types,</b> heuristics have been used in order to generate the distractors automatically. Although the heuristics have been manually defined, a first attempt for their automatic generation is explained...|$|R
40|$|Problem Statement: Since {{the late}} 1990 s, Turkey has {{witnessed}} two major curriculum reforms in English language {{teaching at the}} primary level education. However, {{the situation of the}} assessment practices of the teachers has been unclear due to lack of relevant research, particularly in Turkish context. Purpose of Study: This study aims to investigate the written assessment practices of young English language learners in Grades 4 - 5 in state Turkish primary schools with a focus on comparing the <b>question</b> <b>types</b> posed by the teachers prior to and following the 2005 curriculum innovation in English language teaching (ELT) in primary education in Turkey. Method: In order to identify the teachers’ written assessment practices, 100 written examination papers were collected from 25 teachers who had been teaching in grades 4 and 5 since 1997, the papers were analyzed with regard to the <b>question</b> <b>types</b> based on the categorization suggested by Brown and Hudson (1998), and descriptive statistics was used in comparing the <b>question</b> <b>types.</b> Findings: As a result, no major {{differences were found between the}} <b>types</b> of <b>questions</b> directed to 4 th and 5 th graders prior to and following the 2005 curriculum innovation. Conclusion and Recommendations: Additionally, constructed response <b>question</b> <b>types</b> grew noticeably in number, particularly when the examination papers prepared for 4 th and 5 th graders were compared prior to and after the 2005 curriculum innovation...|$|R
50|$|Previous {{research}} has found mixed results regarding whether behavioral or situational questions will best predict future job performance of an applicant. It is likely that variables unique to each situation, such as the specific criteria being examined, the applicant’s work experience, or the interviewee’s nonverbal behavior make a difference with regard to which <b>question</b> <b>type</b> is the best. It is recommended to incorporate both situational and behavioral questions into the interview {{to get the best}} of both question types. The use of high-quality questions represents an element of structure, and is essential to ensure that candidates provide meaningful responses reflective of their capability to perform on the job.|$|E
5000|$|Questions {{are grouped}} into modules and topics. TCExam can store an {{unlimited}} number of modules. Each module can contain an unlimited number of topics. Each topic can contain an unlimited {{number of questions}} and each question can have an unlimited number of alternative answers. A TCExam test can include several modules and topics. For each module, topic or groups of modules/topics, TCExam randomly extracts a specified number of questions with certain characteristics (i.e.: <b>question</b> <b>type,</b> question difficulty and number of alternative answers to be displayed). If the question bank is large enough, TCExam may generate unique test for each user by randomly selecting and ordering questions and alternative answers. This drastically reduces or eliminates the risk of cheating.|$|E
5000|$|Episodes {{consist of}} five {{questions}}. Most questions are multiple choice, where the player is given four answers {{to choose from}} within {{a short amount of}} time once the emcee has finished reading the question. The player is awarded money for selecting the right answer, and a bonus for how fast they responded; getting the wrong answer will cost the player a similar amount of money; failing to answer does not affect a player's score. Like other games in the series, the questions mix general knowledge with contemporary events, as per its tagline [...] "where high culture meets pop culture". For example, the player may be asked how many alcoholic shots they would have to drink for each cannon shot in the 1812 Overture. Some of the multiple choice questions are based on a recurring <b>question</b> <b>type</b> throughout the episodes as well as those featured in earlier You Don't Know Jack games. For example, [...] "Funky Trash" [...] questions will list three items claimed to be in some celebrity's garbage, and the player would be required to identify the celebrity. A new variation introduced in the Facebook game is [...] "Elephant, Mustard, Teddy Roosevelt, or Dracula", where the answer to the question will be one of those four items.|$|E
40|$|The TREC 2003 {{question}} answering track contained two tasks, the passages task {{and the main}} task. In the passages task, systems returned a single text snippet in response to factoid questions; the evaluation metric {{was the number of}} snippets that contained a correct answer. The main task contained three separate <b>types</b> of <b>questions,</b> factoid questions, list questions, and definition questions. Each of the questions was tagged as to its type and the different <b>question</b> <b>types</b> were evaluated separately. The final score for a main task run was a combination of the scores for the separate <b>question</b> <b>types...</b>|$|R
5000|$|Triple Threat and Top Ten <b>question</b> <b>types</b> {{have been}} eliminated, {{but there are}} now six Battle {{questions}} for each set of 30. (Contestants can ask for how many battles are left at any point.) ...|$|R
40|$|Data-driven {{approaches}} in question answering (QA) are increasingly common. Since availability of training data for such approaches is very limited, we propose an unsupervised algorithm that generates high quality question-answer pairs from local corpora. The algorithm is ontology independent, requiring very small seed data as its starting point. Two alternating {{views of the}} data make learning possible: 1) <b>question</b> <b>types</b> are viewed as relations between entities and 2) <b>question</b> <b>types</b> are described by their corresponding question-answer pairs. These two aspects of the data allow us to construct an unsupervised algorithm that acquires high precision question-answer pairs. We show {{the quality of the}} acquired data for different <b>question</b> <b>types</b> and perform a task-based evaluation. With each iteration, pairs acquired by the unsupervised algorithm are used as training data to a simple QA system. Performance increases with the number of question-answer pairs acquired confirming the robustness of the unsupervised algorithm. We introduce the notion of semantic drift and show that it is a desirable quality in training data for question answering systems...|$|R
