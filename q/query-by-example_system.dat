4|16|Public
40|$|Graduation date: 1987 Allegro is {{a network}} {{database}} management system being developed at Oregon State University. This project adds a user friendly query facility to the system. The user is presented with pictorial display of the network records and a query interface modeled on the <b>Query-By-Example</b> <b>system.</b> By request the user may be shown the network sets of the query schema. When necessary the user may specify query navigation of the network schema. While implemented and functional, this facility {{should be considered as}} a feasibility study for a full query system on a network data base. To provide the desired display this facility is implemented on a system separate from the main Allegro system and uses a communication interface to it. This facility is a Smalltalk implementation...|$|E
40|$|We {{consider}} {{the problem of}} learning a mapping function from low-level feature space to high-level semantic space. Under {{the assumption that the}} data lie on a submanifold embedded in a high dimensional Euclidean space, we propose a relevance feedback scheme which is naturally conducted only on the image manifold in question rather than the total ambient space. While images are typically represented by feature vectors in R n, the natural distance is often different from the distance induced by the ambient space R n. The geodesic distances on manifold are used to measure the similarities between images. However, when the number of data points is small, it is hard to discover the intrinsic manifold structure. Based on user interactions in a relevance feedback driven <b>query-by-example</b> <b>system,</b> the intrinsic similarities between images can be accurately estimated. We then develop an algorithmic framework to approximate the optimal mapping function by a Radial Basis Function (RBF) neural network. The semantics of a new image can be inferred by the RBF neural network. Experimental results show that our approach is effective in improving the performance of content-based image retrieval systems...|$|E
40|$|One key {{challenge}} in talent search is to translate complex criteria of a hiring position into a search query, {{while it is}} relatively easy for a searcher to list examples of suitable candidates for a given position. To improve search efficiency, we propose {{the next generation of}} talent search at LinkedIn, also referred to as Search By Ideal Candidates. In this system, a searcher provides one or several ideal candidates as the input to hire for a given position. The system then generates a query based on the ideal candidates and uses it to retrieve and rank results. Shifting from the traditional Query-By-Keyword to this new <b>Query-By-Example</b> <b>system</b> poses a number of challenges: How to generate a query that best describes the candidates? When moving to a completely different paradigm, how does one leverage previous product logs to learn ranking models and/or evaluate the new system with no existing usage logs? Finally, given the different nature between the two search paradigms, the ranking features typically used for Query-By-Keyword systems might not be optimal for Query-By-Example. This paper describes our approach to solving these challenges. We present experimental results confirming the effectiveness of the proposed solution, particularly on query building and search ranking tasks. As of writing this paper, the new system has been available to all LinkedIn members...|$|E
40|$|Includes bibliographical {{references}} (leaves 43 - 45) Music {{information retrieval}} (MIR) is the fi eld {{dedicated to the}} organization and retrieval of music data. One of the eventual goals of MlR research is automatic transcription, where any musical recording can be transcribed to accurate sheet music with no human intervention, allowing easy cataloging of music and facilitating the creation of music databases for <b>query-by-example</b> <b>systems.</b> Towards this goal, l have investigated pitch detection algorithms for hummed or instrumental input and created an open source tool to transcribe monophonic melodies using the Java programming language. Additional goals include examining {{the current state of}} MlR research and encouraging future graduate students to explore MIR and expand on this tool...|$|R
30|$|With {{respect to}} the {{language}}, English is the language with more resources and for which more research has been done. When applying the similar technology to languages with fewer resources or for which less specific research has been devoted, performance decreases are observed. In {{the case of the}} NIST STD 2006 evaluation, very important performance decreases are observed when moving from English to other languages. In the case of our evaluation, we should not expect important decreases due to the use of Spanish since we are conducting a query-by-example evaluation in which language resources are less important and the technology is relatively more language independent. However, we will probably lose some performance due to using a query-by-example setting. In fact, we see that this happens in the particular setting of our evaluation by comparing results of the <b>query-by-example</b> <b>systems</b> with the performance obtained by a text-based spoken term detection system that is more comparable to the systems participating in the NIST STD 2006 evaluation.|$|R
40|$|Abstract: Currently {{large scale}} {{multimodal}} image databases have become widely available, for example via photo sharing sites where images {{come along with}} textual descriptions and keyword annotations. Most existing work on image retrieval and image auto-annotation has considered uni-modal techniques, either focusing on <b>query-by-example</b> <b>systems</b> or query-by-text systems for image retrieval, and mono modal classification for image auto-annotation. However recent state-of-the-art multimodal image retrieval and image auto-annotation systems combine different uni-modal models using late-fusion techniques. In addition, significant advances {{have been made by}} using pseudo-relevance feedback techniques, as well as using transmedia relevance models that swap modalities in the query expansion step of pseudo-relevance methods. While these techniques are promising it is not trivial to set the parameters that control the late fusion and pseudo/cross relevance models. In this paper, we therefore propose approaches to learn these parameters from a labeled training set: queries with relevant and non-relevant documents, or images with relevant and non-relevant keywords. Three additional contributions are the introduction of (i) two new parameterizations of transmedia and pseudo-relevance models, (ii) correction parameters for inter-query variations in the distribution of retrieval scores for both relevant and non-relevant documents, and (iii) the extension of TagProp, a nearest neighbor based image annotation method to exploit transmedia relevance feedback. W...|$|R
40|$|We {{present a}} <b>query-by-example</b> <b>system</b> for {{content-based}} music information retrieval by ranking items in a database based on semantic similarity, rather than acoustic similarity, to a query example. The retrieval system {{is based on}} semantic concept models that are learned from the CAL- 500 data set containing both audio examples and their text captions. Using the concept models, the audio tracks are mapped into a semantic feature space, where each dimension indicates {{the strength of the}} semantic concept. Audio similarity and retrieval is then based on ranking the database tracks by their similarity to the query in the semantic space. 1 MODELING AUDIO AND SEMANTICS Our query-by-example music information retrieval (MIR) system takes an audio track as a query and retrieves new audio tracks that have similar semantic descriptions to the query track. For example, given a piece of music that a listener might describe as “crazy guitar rock with a screaming female singer that makes me want to get up and dance”, our system ranks all retrievable songs by how well they fit this description. The system is based on the models of [9, 3] which have shown promise in the domains of audio and image retrieval. Audio models are learned from a database of audio tracks with associated text captions that describe the audio content: D = {(A (1), c (1)), [...] ., (A (|D|), c (|D|)) } (1) where A (d) and c (d) represent the d-th audio track and the associated text caption, respectively. Each caption is a set of words from a fixed vocabulary, V. We train our system using the semantic labels from the CAL- 500 data set [9] of 500 songs, each annotated by at least 3 humans using up to 200 words. We require that each word be positively associated with at least 10 songs, resulting in a vocabulary of 146 words (|V | = 146) ...|$|E
40|$|This {{working paper}} {{provides}} the basic information about experiments conducted on audio documents within the MediaEval 2012 spoken web search evaluation project. The {{main purpose of}} these experiments was to build a robust and language independent system for spoken term detection. Therefore we have proposed <b>query-by-example</b> searching <b>system</b> based on the minimum-cost alignment of DTW algorithm and unsupervised SVM misclassification rate. Results show that our system is liable to variable length of queries with similar spectral characteristics that results in poor detection performance with high number of insertions and misdetections. There were no other resources used during the development...|$|R
40|$|Currently {{large scale}} {{multimodal}} image databases have become widely available, for example via photo sharing sites where images {{come along with}} textual descriptions and keyword annotations. Most existing work on image retrieval and image auto-annotation has considered uni-modal techniques, either focusing on <b>query-by-example</b> <b>systems</b> or query-by-text systems for image retrieval, and mono modal classification for image auto-annotation. However recent state-of-the-art multimodal image retrieval and image auto-annotation systems combine different uni-modal models using late-fusion techniques. In addition, significant advances {{have been made by}} using pseudo-relevance feedback techniques, as well as using transmedia relevance models that swap modalities in the query expansion step of pseudo-relevance methods. While these techniques are promising it is not trivial to set the parameters that control the late fusion and pseudo/cross relevance models. In this paper, we therefore propose approaches to learn these parameters from a labeled training set: queries with relevant and non-relevant documents, or images with relevant and non-relevant keywords. Three additional contributions are the introduction of (i) two new parameterizations of transmedia and pseudo-relevance models, (ii) correction parameters for inter-query variations in the distribution of retrieval scores for both relevant and non-relevant documents, and (iii) the extension of TagProp, a nearest neighbor based image annotation method to exploit transmedia relevance feedback. We evaluate our models using public benchmark data sets for image retrieval and annotation. Using the data set of the ImageClef 2008 Photo Retrieval task, our retrieval experiments show that our learned models lead to significant improvements of retrieval performance over the current state-of-the-art. In our experiments on image annotation we use the COREL and IAPR data sets, and also here we observe annotation accuracies that improve over the current state-of-the-art results on these data sets...|$|R
40|$|Retrieving spoken {{content with}} spoken queries, or query-by- example spoken term {{detection}} (STD), is attractive {{because it makes}} possible the matching of signals directly on the acoustic level without transcribing them into text. Here, we propose an end-to-end query-by-example STD model based on an attention-based multi-hop network, whose input is a spoken query and an audio segment containing several utterances; the output states whether the audio segment includes the query. The model can be trained in either a supervised scenario using labeled data, or in an unsupervised fashion. In the supervised scenario, {{we find that the}} attention mechanism and multiple hops improve performance, and that the attention weights indicate the time span of the detected terms. In the unsupervised setting, the model mimics the behavior of the existing <b>query-by-example</b> STD <b>system,</b> yielding performance comparable to the existing system but with a lower search time complexity...|$|R
40|$|Abstract — While {{there have}} been {{advances}} in visualization systems, particularly in multi-view visualizations and visual exploration, {{the process of building}} visualizations remains a major bottleneck in data exploration. We show that provenance metadata collected during the creation of pipelines can be reused to suggest similar content in related visualizations and guide semi-automated changes. We introduce the idea of query-by-example {{in the context of an}} ensemble of visualizations, and the use of analogies as first-class operations in a system to guide scalable interactions. We describe an implementation of these techniques in VisTrails, a publiclyavailable, open-source system. Index Terms—visualization <b>systems,</b> <b>query-by-example,</b> analogy...|$|R
40|$|This paper {{describes}} a music remixing interface, called Instrument Equalizer, that {{allows users to}} control the volume of each instrument part within existing audio recordings in real time. Although <b>query-by-example</b> retrieval <b>systems</b> need a user to prepare favorite examples (songs) in general, our interface gives a user to generate examples from existing ones by cutting or boosting some instrument/vocal parts, resulting {{in a variety of}} retrieved results. To change the volume, all instrument parts are separated from the input sound mixture using the corresponding standard MIDI file. For the separation, we used an integrated tone (timbre) model consisting of harmonic and inharmonic models that are initialized with template sounds recorded from a MIDI sound generator. The remaining but critical problem here is to deal with various performance styles and instrument bodies that are not given in the template sounds. To solve this problem, we train probabilistic distributions of timbre features by using various sounds. By adding a new constraint of maximizing the likelihood of timbre features extracted from each tone model, we succeeded in estimating model parameters that better express actual timbre. ...|$|R
40|$|A novel {{technique}} for Content-Based Image Retrieval (CBIR) that employs both {{the color and}} spatial information of images is proposed. A maximum of three dominant color regions in an image together with its respective coordinates of the Minimum-Bounding Rectangle (MBR) are first extracted. Next, the Sub-Block technique is then {{used to determine the}} location of the dominant regions by comparing the coordinates of the region’s MBR with the four corners of the center of the location map. The cell number that is maximally covered by the region is supposedly to be assigned as the location index. However, the Sub-Block technique is not reliable because in most cases, the location index assigned is not the cell number that is maximally covered by the region and sometimes a region does not overlap with the cell number assigned at all. The effectiveness of this technique has been improved using the Improved Sub-Block technique by taking into consideration the total horizontal and vertical distances of a region at each location where it overlaps. The color-spatial technique is accessed on a <b>Query-by-Example</b> CBIR <b>system</b> consisting of 900 images. From the experiments it is shown that retrieval effectiveness has been significantly improved by 85. 86 %...|$|R
40|$|Hashing-based {{similarity}} {{search is}} an important technique for large-scale <b>query-by-example</b> image retrieval <b>system,</b> since it provides fast search with computation and memory efficiency. However, {{it is a challenge}} work to design compact codes to represent original features with good performance. Recently, a lot of unsupervised hashing methods have been proposed to focus on preserving geometric structure similarity of the data in the original feature space, but they have not yet fully refined image features and explored the latent semantic feature embedding in the data simultaneously. To address the problem, in this paper, a novel joint binary codes learning method is proposed to combine image feature to latent semantic feature with minimum encoding loss, which is referred as latent semantic minimal hashing. The latent semantic feature is learned based on matrix decomposition to refine original feature, thereby it makes the learned feature more discriminative. Moreover, a minimum encoding loss is combined with latent semantic feature learning process simultaneously, so as to guarantee the obtained binary codes are discriminative as well. Extensive experiments on several wellknown large databases demonstrate that the proposed method outperforms most state-of-the-art hashing methods. </p...|$|R
40|$|Dealing with {{ambiguous}} queries is {{an important}} challenge in information retrieval (IR). While this problem is well understood in text retrieval, {{this is not the}} case in video retrieval, especially when multi-modal queries have to be considered as for instance in <b>Query-by-Example</b> or Query-by-Sketch. <b>Systems</b> supporting such query types usually consider dedicated features for the different modalities. This can be intrinsic object features like color, edge, or texture for the visual modality or motion for the kinesthetic modality. Sketch-based queries are naturally inclined to be ambiguous as they lack specification in some information channels. In this case, the IR system has to deal with the lack of information in a query, as it cannot deduce whether this information should be absent in the result or whether it has simply not been specified, and needs to properly select the features to be considered. In this paper, we present an approach that deals with such ambiguous queries in sketch-based multimodal video retrieval. This approach anticipates the intent(s) of a user based on the information specified in a query and accordingly selects the features to be considered for query execution. We have evaluated our approach based on Cineast, a sketch-based video retrieval system. The evaluation results show that disregarding certain features based on the anticipated query intent(s) can lead to an increase in retrieval quality of more than 25 % over a generic query execution strategy...|$|R
40|$|Abstract—Embedding image {{features}} into a binary Hamming space can improve both {{the speed and}} accuracy of large-scale <b>query-by-example</b> image retrieval <b>systems.</b> Supervised hashing aims to map the original features to compact binary codes in a manner which preserves the label-based similarities of the original data. Most existing approaches apply a single form of hash function, and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of those methods, and can result in complex optimization problems {{that are difficult to}} solve. In this work we proffer a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. The proposed framework allows a number of existing approaches to hashing to be placed in context, and simplifies the development of new problem-specific hashing methods. Our framework decomposes the into two steps: binary code (hash bits) learning, and hash function learning. The first step can typically be formulated as a binary quadratic problem, and the second step can be accomplished by training standard binary classifiers. For solving large-scale binary code inference, we show how to ensure that the binary quadratic problems are submodular such that an efficient graph cut approach can be used. To achieve efficiency as well as efficacy on large-scale high-dimensional data, we propose to use boosted decision trees as the hash functions, which are nonlinear, highly descriptive, and very fast to train and evaluate. Experiments demonstrate that our proposed method significantly outperforms most state-of-the-art methods, especially on high-dimensional data...|$|R
40|$|Date of Publication : 18 February 2015 To build {{large-scale}} <b>query-by-example</b> image retrieval <b>systems,</b> embedding {{image features}} into a binary Hamming space provides great benefits. Supervised hashing aims {{to map the}} original features to compact binary codes {{that are able to}} preserve label based similarity in the binary Hamming space. Most existing approaches apply a single form of hash function, and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of those methods, and can result in complex optimization problems that are difficult to solve. In this work we proffer a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. The proposed framework allows a number of existing approaches to hashing to be placed in context, and simplifies the development of new problem-specific hashing methods. Our framework decomposes the hashing learning problem into two steps: binary code (hash bit) learning and hash function learning. The first step can typically be formulated as binary quadratic problems, and the second step can be accomplished by training a standard binary classifier. For solving large-scale binary code inference, we show how it is possible to ensure that the binary quadratic problems are submodular such that efficient graph cut methods may be used. To achieve efficiency as well as efficacy on large-scale high-dimensional data, we propose to use boosted decision trees as the hash functions, which are nonlinear, highly descriptive, and are very fast to train and evaluate. Experiments demonstrate that the proposed method significantly outperforms most state-of-the-art methods, especially on high-dimensional data. Guosheng Lin, Chunhua Shen, and Anton van den Henge...|$|R
40|$|Embedding image {{features}} into a binary Hamming space can improve both {{the speed and}} accuracy of large-scale <b>query-by-example</b> image retrieval <b>systems.</b> Supervised hashing aims to map the original features to compact binary codes in a manner which preserves the label-based similarities of the original data. Most existing approaches apply a single form of hash function, and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of those methods, and can result in complex optimization problems {{that are difficult to}} solve. In this work we proffer a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. The proposed framework allows a number of existing approaches to hashing to be placed in context, and simplifies the development of new problem-specific hashing methods. Our framework decomposes the into two steps: binary code (hash bits) learning, and hash function learning. The first step can typically be formulated as a binary quadratic problem, and the second step can be accomplished by training standard binary classifiers. For solving large-scale binary code inference, we show how to ensure that the binary quadratic problems are submodular such that an efficient graph cut approach can be used. To achieve efficiency as well as efficacy on large-scale high-dimensional data, we propose to use boosted decision trees as the hash functions, which are nonlinear, highly descriptive, and very fast to train and evaluate. Experiments demonstrate that our proposed method significantly outperforms most state-of-the-art methods, especially on high-dimensional data. Comment: 15 pages. Appearing in IEEE T. Pattern Analysis & Machine Intelligence. arXiv admin note: text overlap with arXiv: 1404. 156...|$|R
40|$|Content based image {{retrieval}} (CBIR) {{encompasses a}} variety of techniques with a goal {{to solve the problem of}} searching for digital images in a large database by their visual content. Applications where the retrieval of similar images plays a crucial role include personal photo and art collections, medical imaging, multimedia publications and video surveillance. Main objective of our study was to try to improve the performance of the <b>query-by-example</b> image retrieval <b>system</b> based on texture features – Gabor wavelet and wavelet transform – by augmenting it with color information about the images, in particular color histogram, color autocorrelogram and color moments. Wang image database comprising 1000 natural color images grouped into 10 categories with 100 images was used for testing individual algorithms. Each image in the database served as a query image and the retrieval performance was evaluated by means of the precision and recall. e number of retrieved images ranged from 10 to 80. e best CBIR performance was obtained when implementing a combination of all 190 texture- and color features. Only slightly worse were the average precision and recall for the texture- and color histogram-based system. is result was somewhat surprising, since color histogram features provide no color spatial informa- tion. We observed a 23 % increase in average precision when comparing the system containing a combination of texture- and all color features with the one consisting of exclusively texture descriptors when using Euclidean distance measure and 20 retrieved images. Addition of the color autocorrelogram features to the texture de- scriptors had virtually no e ect on the performance, while only minor improvement was detected when adding rst two color moments – the mean and the standard deviation. Similar to what was found in the previous studies with the same image database, average precision was very high in case of dinosaurs and owers and very low with beach, food, monuments and mountains images...|$|R
40|$|In this thesis, {{we cover}} {{what we believe}} would be the main {{ingredients}} of an exploratory search system (ESS). In a nutshell, these are textual queries, facets, visual results, social search and query-by-example. The goal of the thesis is to show how all of these elements could readily be integrated into a typical faceted search system that users are already accustomed to. In this respect, we propose {{that the future of}} exploratory search might be a traditional faceted search system, but with the added ingredients of information visualizations and query-by-example. To illustrate our ideas we have built two freely available web applications. The first one, Biomed Search, has been positively received by the community and offers some novel characteristics. First, in order to improve on both precision and recall, Biomed Search indexes not only the text caption but also the text that refers to the image. Second, the interface uses a common pattern of zooming in on a particular search result in order to display more information. User feedback on Biomed Search has hinted towards faceted search, visual search results and <b>query-by-example.</b> The second <b>system,</b> Cloud Mining, is an attempt at implementing the vision set forth in this thesis. The system is a framework used to instantiate ESSs. It offers the novel characteristics of facet views as well as multiple-item based searches combined with textual queries. Cloud Mining paves the way to a completely pluggable search framework, in which every component would be driven by a community of users. The system was tested on large publicly available datasets and all its software components are available under an open source license. The main contributions of this thesis come as lessons learned, suggestions or recommendations as to how to extend the current paradigm of faceted search into the one of exploratory search. The search results and facets should be extended with different views. Query by example should be integrated with Bayesian Sets as it reduces the handling of complex content based searches to choosing the right plugin. Finally, the system should be thought as a framework to instantiate ESSs, in which every one of its component is a community driven plugin. These customized tailored tools, when applied to a dataset of interest, could offer a collective intelligence approach to information overload...|$|R

