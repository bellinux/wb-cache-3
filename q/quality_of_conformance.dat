10|10000|Public
40|$|Abstract—Trajectory {{conformance}} monitoring {{is important}} for future air traffic control for reasons associated with optimal operation, increased safety, and improved efficiency. In this study conformance monitoring is considered with respect to preas-signed 4 D (space and time) trajectories and their margins (4 D contracts), and an adaptive time series probabilistic framework is postulated. Two problems are tackled and proper methods are developed: (a) present conformance monitoring and <b>quality</b> <b>of</b> <b>conformance</b> evaluation via statistical tools, which leads to abnormal event detection, and (b) future conformance monitoring in which conformance is predicted ahead of time allowing for the early initiation of corrective actions. The framework is based on Recursive Integrated AutoRegressive (RIAR) modeling of contract deviations alone, with the underlying dynamics and non-stationarity accounted for. An initial assessment {{of the performance of}} the framework is based on two simulation scenarios. Through them, present conformance monitoring is shown to lead to quality assessment and the declaration of an alarm immediately following the emergence of an abnormal event. Future conformance monitoring is shown to lead to an early non-conformance alarm, with the lead time shown to be significantly longer than that achieved by a current probabilistic benchmark scheme. Index Terms—conformance monitoring, adaptive models, time series models, <b>quality</b> <b>of</b> <b>conformance,</b> trajectory monitoring I...|$|E
40|$|In today’s {{world of}} rapidly {{advancing}} technology, {{for any kind}} of software product, quality is the prime concern. A software product {{is said to be a}} quality product if it justifies all the customer requirements. The quality of software can be categorized broadly into two ways, one quality of the design and the second <b>quality</b> <b>of</b> <b>conformance.</b> Quality of design encompasses requirements specification and design of the system and <b>quality</b> <b>of</b> <b>conformance</b> focuses on implementation. The quality of software is intricately connected to the underlying architecture because architecture is the base for further development of the project (Analysis, design and implementation). The work products of architecture refinement are the high level design, low level design, implementation and test cases. The software architecture is the structure of the system which comprises software components, their externally visible properties and the relationship among them. The software architecture is a meta-structure created based on the requirement and specifications. Architecture aids the software engineers to make early design decisions so that it will have a profound impact on software engineering activities that involve in success of the system. The major work to be done in modeling the architecture is to identify qualified, adaptable, updatable components and how these components are associated to build a new product. In the current work, we provide innovative patterns which help in developing the product using component based software engineering. Here we discuss how to create components by applying various innovative patterns like subtraction, multiplication, division, task unification and attribute dependency change patterns. We will also depict the results of applying these patterns to some of the software projects...|$|E
40|$|Total {{product quality}} {{involves}} three strongly inter-related notions of quality - (i) quality of performance, (ii) <b>quality</b> <b>of</b> <b>conformance</b> and (iii) {{quality of service}} (repairs during post-sale period). As such, study of total product quality requires a framework which integrates the actions of manufacturer and retailer {{and their impact on}} the different dimensions of total product quality. We develop a general game-theoretic model for such a study. For a special case, we derive the optimal manufacturer and retailer actions and discuss various structural, and economic implications of the model such as the inverse relationships between (i) quality of service and quality of performance and (ii) quality of service and retail price. (C) 2000 Elsevier Science B. V. All rights reserved...|$|E
5000|$|... #Subtitle level 4: 1. [...] "The {{definition}} <b>of</b> <b>quality</b> is <b>conformance</b> to requirements" ...|$|R
5000|$|The {{definition}} <b>of</b> <b>quality</b> is <b>conformance</b> to requirements (requirements meaning {{both the}} product and the customer's requirements):#The system <b>of</b> <b>quality</b> is prevention ...|$|R
50|$|Rear Admiral Bird became Commander of Defense Supply Center, Columbus in Columbus, Ohio, on July 18, 2003. As DSCC's Commander, Rear Adm. Bird oversaw an {{operating}} budget of $2.8 billion. She also directed {{the functions of}} 2,300 associates involved in purchasing materiel, monitoring inventory levels, maintaining technical data and assuring <b>quality</b> <b>conformance</b> <b>of</b> more than 1.6 million spare and repair parts used by over 24,000 military units and civilian federal agencies.|$|R
40|$|Abstract — The general {{problem of}} {{conformance}} monitoring {{with respect to}} preassigned 4 -dimensional (4 D) trajectories equipped with corresponding 4 D margins (4 D contracts) is considered within an adaptive statistical time series framework. The specific issues tackled within this context are: (a) Present conformance monitoring and <b>quality</b> <b>of</b> <b>conformance</b> evalua-tion via statistical tools, which also leads to abnormal event detection; (b) future conformance monitoring, in which the conformance is predicted ahead of time, allowing for potentially corrective or other actions. The performance of the developed methods is assessed via simulations. In present conformance monitoring, an alarm is shown to be issued instantaneously, following {{the emergence of an}} abnormal event. In future conformance monitoring, the comparison with a scheme based on nominal probabilistic trajectory prediction demonstrates the benefits of the adaptive statistical time series framework. I...|$|E
40|$|The {{purpose of}} this paper is to study the links between quality and consumers’ {{emotions}} and eventually with their satisfaction. There is mounting evidence that, while quality evaluation may be strictly cognitive, satisfaction gauges customers’ emotions towards the product/service provider’s performance. The existing approaches used in measuring quality have certain limitations. This study introduces two components of total quality structure – quality of design and <b>quality</b> <b>of</b> <b>conformance</b> – for analysis of the link between quality and customer emotions.   Undergraduate college students were surveyed in regard to four scenarios for food establishments. Respondents were asked to select, from a pre-tested list of emotions, those arising from these scenarios. The equality of means of the valence and the intensity of emotions among the four scenarios were tested using ANOVA and Scheffe paired comparisons. The results show that there is a significant relationship between possible combinations of two quality dimensions and customers’ affective responses in terms of both their valence and intensity. Important managerial implications are discussed...|$|E
40|$|Abstract: When making road pavements, {{construction}} errors may occur. It {{determines the}} <b>quality</b> <b>of</b> <b>conformance</b> of finished product {{to the requirements}} of regulatory documents. Up to the present day theoretical and applied research is oriented to the assessment of impact of such deviations on road pavement behavior in operating period have not been conducted. The point is that the change of layer thickness affects both consumption of construction materials and strength of the whole road structure and, subsequently, its working efficiency, durability, repair size and time. The deviations can have a scope (a size) and a certain part in the total volume of a sample. Road pavement consists of various layers. The more number of layers is, the more combinations of deviations have been. In the present article we give the solution with the use of probability theory propositions. This solution, that became the base of a software product, lets evaluate the construction heterogeneity of a road pavement by the thickness of its layers. Key words: Probability theory Road pavement Quality of roadwork INTRODUCTION these deviations through the layers will make it It is common knowledge that one can ensure the high (constructional) strength heterogeneity in models of level of support and roadwork quality control relying non-rigid pavement behavior forecasting and residual only on the statistical method of describing properties life assessment. In the long run, such solutions wil...|$|E
40|$|Web {{accessibility}} evaluations {{are significantly}} different from document validations; they are not based on determining <b>conformance</b> with specifications <b>of</b> formal grammars but are often more rule-based methodologies encompassing sequences of atomic tests to determine conformance to guidelines. While some of these tests may not be automatable by current computer technology and require human judgment to determine the results, evaluation tools often vary considerably in their coverage and reliability for tests that are automatable. These differences in tool performances are probably unavoidable in practice; however, they cause notable discrepancies in the efficiency and <b>quality</b> <b>of</b> the <b>conformance</b> evaluations. This paper discusses the practical implications of the Evaluation and Report Language; a vendor neutral vocabulary to facilitate the aggregation of test results generated by different evaluation tools {{in order to maximize}} the benefit from their respective features. The paper will also highlight the importance of this language to the objectives of the Dublin Core Metadat...|$|R
40|$|The Healthy Eating Index (HEI) is {{a measure}} <b>of</b> diet <b>quality</b> in terms <b>of</b> <b>conformance</b> with federal dietary guidance. Publication of the Dietary Guidelines for Americans, 2010 {{prompted}} an interagency working group to update the HEI. The HEI- 2010 retains several features of the 2005 version: (1) it has 12 components, many unchanged, including 9 adequacy and 3 moderation components; (2) it uses a density approach to set standards, e. g., per 1000 calories or {{as a percent of}} calories; and (3) it employs least-restrictive standards, i. e., those that are easiest to achieve among recommendations that vary by energy level, sex, and/or age. Changes to the index include: (1) Greens and Beans replaces Dark Green and Orange Vegetables and Legumes; (2) Seafood and Plant Proteins has been added to capture specifi...|$|R
40|$|AbstractThe aim of {{this paper}} is to present the {{connection}} between the <b>Quality</b> Cost (Cost <b>of</b> <b>Conformance)</b> and the Non <b>Quality</b> Cost (Cost <b>of</b> Poor <b>Quality).</b> The major objective of each production company is to produce added value, and this is possible if they have like ultimate goal, the reduction <b>of</b> waste. <b>Quality</b> Cost are describe in the specific literature as an iceberg, that containing a conglomeration of waste, and we can see only a little part of it. Usually what we can see are a part <b>of</b> Non <b>Quality</b> cost as rejection parts and the customer returns or official claims. This is the reason we start our analysis with this variables that are the most important component <b>of</b> Non <b>Quality</b> Cost. Because the decrease <b>of</b> Non <b>Quality</b> Cost is proportional with the increasing <b>of</b> <b>Quality</b> Cost, we chose to exanimate the influence of introduction Statistical Control in production– Categories <b>of</b> cost <b>of</b> <b>Conformance</b> (Prevention Cost), in Non Quality Cost. The above study was performed in manufacturing aluminum components during nineteen months. The collected data was then analyzed with a descriptive statistics in order to see the relation between the quality and non quality costs. Results are showing that the Non Quality Cost can be improve if we introduce in production the Statistical Control, that's the reason we propose to plan in advance this quality tools for all the process of production...|$|R
40|$|An {{increasing}} number of competition every year causing all business entities {{must be able to}} improve the operational quality of their company. Improvement of operational quality will be more effective if the management can analyze the critical success factors that are owned by the business entity. Critical success factor of every business entity can not be the same as other, management needs to look at the factors that will be supporting the achievement of the objectives of the business entity. By performing this analysis, the company can identify the weaknesses and advantages their company and the sustainable improvements to improve the operational quality. Operational quality improvement is not only required by the company who produce products but also very necessary for service company such as hospitality. In hospitality, improvement of operational quality can be done by looking at the dimensions of the quality of services available of the hotel. Improved <b>quality</b> <b>of</b> <b>conformance</b> will give effect to the improvement of the quality of services to hotel guests and if the hotel guests felt the quality of services satisfy them, they will use that hotel again. Now the development of the hospitality industry is not only controlled by full service hotel, but also begin to develop a budget hotel that has own market segment. Each segment hotel will have different critical success factors that are depending of the conditions of their hotel...|$|E
40|$|AbstractComputer {{software}} {{is a collection}} of computer programs or set of programs, procedures, algorithms and its documentation and became a part of daily life. Software performs the function of the program it implements, either by directly providing instructions to the computer hardware or by serving as input to another piece of software. Now a day's more and more software problems are caused trial-and-error programming. Solving the problems of software seems to be as an art and this art is a decent business for big software companies that provide technical support subscriptions so you can debug their products for them. Over the last few years, I found many ways to solve the Computer Science & Information Technology problems by TRIZ. Software quality management is also a part of Computer Science & Information Technology or Software Engineering; even I noticed, we can solve the Software Engineering problem also. It has been desired {{for a long time to}} make TRIZ applicable to the issues related to software quality applications and software-based technology systems. How well {{software is}} designed, measured by Software quality called quality of design, and software conformation to that design, called <b>quality</b> <b>of</b> <b>conformance.</b> In this paper, we show the structure of TRIZ, examine the 40 principles of TRIZ and later try to solve the software quality problem of software engineering with the help of TRIZ...|$|E
40|$|AbstractDemonstrating {{conformity}} between {{observed and}} simulated plume behaviour {{is one of}} the main high-level requirements, which have to be fulfilled by an operator of a CO 2 storage site in order to assure safe storage operations and to be able to transfer liability to the public after site closure. The observed plume behaviour is derived from geophysical and/or geochemical monitoring. Repeated 3 D seismic observations have proven to provide the most comprehensive image of a CO 2 plume in various projects such as Sleipner, Weyburn, or Ketzin. The simulated plume behaviour is derived from reservoir simulation using a model calibrated with monitoring results. Plume observations using any monitoring method are always affected by limited resolution and detection ability, and reservoir simulations will only be able to provide an approximated representation of the occurring reservoir processes. Therefore, full conformity between observed and simulated plume behaviour is difficult to achieve, if it is at all. It is therefore of crucial importance for each storage site to understand to what degree conformity can be achieved under realistic conditions, comprising noise affected monitoring data and reservoir models based on geological uncertainties. We applied performance criteria (plume footprint area, lateral migration distance, plume volume, and similarity index) for a comparison between monitoring results (4 D seismic measurements) and reservoir simulations, considering a range of seismic amplitude values as noise threshold and a range of minimum thickness of the simulated CO 2 plume. Relating the performance criteria to the noise and thickness threshold values allows assessing the <b>quality</b> <b>of</b> <b>conformance</b> between simulated and observed behaviour of a CO 2 plume. The Ketzin site is provided with a comprehensive monitoring data set and a history-matched reservoir model. Considering the relatively high noise level, which is inherent for land geophysical monitoring data, a reasonable conformance between the observed and simulated plume behaviour is demonstrated...|$|E
40|$|The 5300. 4 {{series of}} NASA Handbooks for Reliability and Quality Assurance Programs have {{provisions}} {{for the establishment}} and utilization of a documented metrology system to control measurement processes and to provide objective evidence <b>of</b> <b>quality</b> <b>conformance.</b> The intent <b>of</b> these provisions is to assure consistency and conformance to specifications and tolerances of equipment, systems, materials, and processes procured and/or used by NASA, its international partners, contractors, subcontractors, and suppliers. This Measurement Assurance Program (MAP) guideline has the specific objectives to: (1) ensure the <b>quality</b> <b>of</b> measurements made within NASA programs; (2) establish realistic measurement process uncertainties; (3) maintain continuous control over the measurement processes; and (4) ensure measurement compatibility among NASA facilities. The publication addresses MAP methods as applied within and among NASA installations {{and serves as a}} guide to: control measurement processes at the local level (one facility); conduct measurement assurance programs in which a number of field installations are joint participants; and conduct measurement integrity (round robin) experiments in which a number of field installations participate to assess the overall <b>quality</b> <b>of</b> particular measurement processes at a point in time...|$|R
40|$|Popular Open Source Software (OSS) {{development}} platforms like GitHub, Google Code, and Bitbucket {{take advantage}} of some best practices of traditional software development like version control and issue tracking. Current major open source software environments, including IDE tools and online code repositories, do not provide support for visual architecture modeling. Research has shown that visual modeling of complex software projects has benefits throughout the software lifecycle. Then {{why is it that}} software architecture modeling is so conspicuously missing from popular online open source code repositories? How can including visual documentation improve the overall <b>quality</b> <b>of</b> open source software projects? Our goal is to answer both of these questions and bridge the gap between traditional software engineering best practices and open source development by applying a software architecture documentation methodology using Unified Modeling Language, called 5 W 1 H Re-Doc, on a real open source project for managing identity and access, MITREid Connect. We analyze the effect of a model-driven software engineering approach on collaboration of open source contributors, <b>quality</b> <b>of</b> specification <b>conformance,</b> and state-of-the-art <b>of</b> architecture modeling. Our informal experiment revealed that in some cases, having the visual documentation can significantly increase comprehension of an online OSS project over having only the textual information that currently exists for that project. We recommend more rigorous evaluation of these claims as future work...|$|R
40|$|The {{variety of}} design {{artefacts}} (models) produced in a model-driven design process {{results in an}} intricate relationship between requirements and the various models. This paper proposes a methodological framework that simplifies management of this relationship. This framework is a basis for tracing requirements, assessing the <b>quality</b> <b>of</b> model transformation specifications, metamodels, models and realizations. We propose a notion <b>of</b> <b>conformance</b> between application models which reduces the effort needed for assessment activities. We discuss how this notion <b>of</b> <b>conformance</b> can be integrated with model transformations...|$|R
40|$|The {{development}} {{management quality}} assessment methodology {{in the public}} sector is relevant scientific and practical problem of economic research. The utilization {{of the results of the}} assessment on the basis of the authors’ methodology allows us to rate the public sector organizations, to justify decisions on the reorganization and privatization, and to monitor changes in the level of the management quality of the public sector organizations. The study determined the place of the quality of the control processes of the public sector organization in the system of “Quality of public administration — the effective operation of the public sector organization,” the contradictions associated with the assessment of management quality are revealed, the conditions for effective functioning of the public sector organizations are proved, a mechanism of comprehensive assessment and algorithm for constructing and evaluating the control models of management quality are developed, the criteria for assessing the management quality {{in the public sector}} organizations, including economic, budgetary, social and public, informational, innovation and institutional criteria are empirically grounded. By utilizing the proposed algorithm, the assessment model of quality management in the public sector organizations, including the financial, economic, social, innovation, informational and institutional indicators is developed. For each indicator of quality management, the coefficients of importance in the management quality assessment model, as well as comprehensive and partial evaluation indicators are determined on the basis of the expert evaluations. The main conclusion of the article is that management quality assessment for the public sector organizations should be based not only on the indicators achieved in the dynamics and utilized for analyzing the effectiveness of management, but also should take into account the reference levels for the values of these parameters. The comparison of the indicators will allow to assess not only the degree of the achievement of objectives and results but also the <b>quality</b> <b>of</b> <b>conformance</b> of the chosen development strategy from a position of management quality...|$|E
40|$|The method {{involves}} providing {{digital values}} of a reference signal based on a test signal which quality is to be evaluated. The digital values of the test signal are stored temporary. The degree <b>of</b> <b>conformance</b> <b>of</b> {{a section of the}} reference signal values with the test signal values is determined. The measuring process is triggered when a predetermined degree <b>of</b> <b>conformance</b> is exceeded. USE - E. g. for <b>quality</b> evaluation <b>of</b> audio and voice signals. ADVANTAGE - Provides accurate triggering of measuring process...|$|R
40|$|Employers {{are looking}} for {{reducing}} execution time and maintaining the <b>quality</b> <b>of</b> the projects that are the main objective of the projects. In this article, we focus on crashing projects by con-sidering different factors such as cost, time, quality and risk. For the proposed integer linear model, cost <b>of</b> <b>conformance</b> and cost <b>of</b> non-conformance are considered as parts of the costs <b>of</b> <b>quality</b> <b>of</b> deliverables in projects. The cost <b>of</b> <b>conformance</b> consists <b>of</b> the costs of training the project team, inspection and test of deliverables. The cost of non-conformance also includes costs of rework and scrap. Project risk management {{is one of the}} important aspects of the pro-jects. The present study also considers the impact of risks, which is highly applicable in projects {{with a high level of}} uncertainty. Results are presented using integer programming approach with the aim of minimizing the costs of the project...|$|R
40|$|Abstract—Developing mixed object-relational (OR) {{mappings}} for object-oriented application {{models is}} a tedious, costly, intuitive, and error-prone manual process. The technical contribution {{of this paper}} is a substantially automated technique for exhaustive, formally precise synthesis <b>of</b> <b>quality</b> equivalence classes <b>of</b> such mappings. We employ a constraint-solving technique to generate candidate mappings, compute six metrics related to three major quality attributes for each candidate, and cluster results into quality equivalence classes. We claim that this work has the potential to significantly reduce the cost and time required to develop mixed OR mappings, to increase the <b>quality</b> <b>of</b> mappings (in <b>conformance</b> to formal correctness constraints), and to provide engineers with a better and quantitative basis for making tradeoffs in selecting among available mappings. To evaluate these claims, we conducted several experiments, including over an object model of a real e-commerce application. The resulting data strongly support our claims {{that we were able to}} generate hundreds of thousands of candidates, assess them, and reduce them to equivalence classes, in the order of minutes. Index Terms—Design; Database; Object-relational mapping; Design space exploration; Alloy Language...|$|R
40|$|Industrial {{inspection}} {{is one of}} {{the crucial}} tasks to ensure <b>quality</b> <b>conformance</b> <b>of</b> products. The inspection tasks can be done by using several methods like non-scaled go/not go gauging, measuring instruments, or advanced non-touching tools. In this research visual inspection using a developed optical system is conducted. One of the aims of this research is to design an on line visual inspection system that is capable to test geometrical <b>quality</b> characteristics <b>of</b> 2 -D machined products. The design process includes developing an economical optical system to acquire inspected product’s images. Image processing tools are utilized to deal with the product image; and extract features of its geometrical characteristics. A neural network-based methodology is developed and applied to decide whether the product conforms to pre-specified tolerances. The results of the developed methodology are compared to some statistics based visual approaches from the literature. The results show the goodness of the system as an automated visual inspection system and prove its superior performance with respect to other methods...|$|R
40|$|Several notions <b>of</b> <b>conformance</b> {{have been}} {{proposed}} for checking the behavior of cyber-physical systems against their hybrid systems models. In this paper, we explore the initial idea of a notion <b>of</b> approximate <b>conformance</b> that allows for comparison of both observable discrete actions and (sampled) continuous trajectories. As such, this notion will consolidate two earlier notions, namely the notion <b>of</b> Hybrid Input-Output <b>Conformance</b> (HIOCO) by M. van Osch and the notion <b>of</b> Hybrid <b>Conformance</b> by H. Abbas and G. E. Fainekos. We prove that our proposed notion <b>of</b> <b>conformance</b> satisfies a semi-transitivity property, which makes it suitable for a step-wise proof <b>of</b> <b>conformance</b> or refinement. Comment: In Proceedings V 2 CPS- 16, arXiv: 1612. 0402...|$|R
40|$|M. Mohaqeqi and M. R. Mousavi. Several notions <b>of</b> <b>conformance</b> {{have been}} {{proposed}} for checking the behavior of cyber-physical systems against their hybrid systems models. In this paper, we explore the initial idea of a notion <b>of</b> approximate <b>conformance</b> that allows for comparison of both observable discrete actions and (sampled) continuous trajectories. As such, this notion will consolidate two earlier notions, namely the notion <b>of</b> Hybrid Input-Output <b>Conformance</b> (HIOCO) by M. van Osch and the notion <b>of</b> Hybrid <b>Conformance</b> by H. Abbas and G. E. Fainekos. We prove that our proposed notion <b>of</b> <b>conformance</b> satisfies a semi-transitivity property, which makes it suitable for a step-wise proof <b>of</b> <b>conformance</b> or refinement. Peer-reviewedPost-printV 2 CPS- 16, Verification and Validation of Cyber-Physical Systems, Reykjavík, Iceland, June 4 - 5, 201...|$|R
40|$|We {{review and}} compare three notions <b>of</b> <b>conformance</b> testing for cyber-physical systems. We {{begin with a}} review of their {{underlying}} semantic models and present conformance-preserving translations between them. We identify {{the differences in the}} underlying semantic models and the various design decisions that lead to these substantially different notions <b>of</b> <b>conformance</b> testing. Learning from this exercise, we reflect upon the challenges in designing an "ideal" notion <b>of</b> <b>conformance</b> for cyber-physical systems and sketch a roadmap of future research in this domain...|$|R
40|$|Abstract. Conformance {{checking}} {{is becoming}} {{more important for the}} analysis of business processes. While the diagnosed results <b>of</b> <b>conformance</b> checking tech-niques are used in diverse context such as enabling auditing and performance analysis, the <b>quality</b> and reliability <b>of</b> the <b>conformance</b> checking techniques them-selves have not been analyzed rigorously. As the existing conformance checking techniques heavily rely on the total ordering of events, their diagnostics are un-reliable and often even misleading when the timestamps of events are coarse or incorrect. This paper presents an approach to incorporate flexibility, uncertainty, concurrency and explicit orderings between events in the input {{as well as in the}} output <b>of</b> <b>conformance</b> checking using partially ordered traces and partially or-dered alignments, respectively. The paper also illustrates various ways to acquire partially ordered traces from existing logs. In addition, a quantitative-based qual-ity metric is introduced to objectively compare the results <b>of</b> <b>conformance</b> check-ing. The approach is implemented in ProM plugins and has been evaluated using artificial logs. ...|$|R
5000|$|A {{certificate}} <b>of</b> <b>conformance</b> {{is defined}} in American commerce as a document certified by a competent authority that the supplied good or service meets the required specifications. A certificate <b>of</b> <b>conformance</b> is a lot/datecode specific certification that provides traceability of the goods {{back to the point}} of manufacture.|$|R
40|$|International audienceThis paper {{presents}} an ontological method {{aimed at the}} capitalisation and organisation of conformance-related knowledge for semi-automatic checking model <b>of</b> the <b>conformance</b> <b>of</b> construction projects against a set <b>of</b> <b>conformance</b> requirements. We start by {{developing a method for}} knowledge representation of explicit knowledge taking part in checking, and capitalisation of tacit knowledge defining checking practices. Then we explain our approach for classification, organisation and retrieval <b>of</b> <b>conformance</b> requirements and propose an approach for scheduling these requirements to optimise the checking process. Finally, we introduce our reasoning model that is based on matching graph representations of construction projects and conformance queries, and interpret the acquired results in terms <b>of</b> <b>conformance</b> checking in construction. To validate our approach, we describe a prototype illustrating its feasibility, and discuss ongoing works and perspectives of our research...|$|R
5000|$|ISO/IEC/IEEE 42010 defines {{four cases}} <b>of</b> <b>conformance</b> to the standard: ...|$|R
50|$|Industry {{collaboration}} {{in the original}} PDF/A Competence Center {{led to the development}} of the Isartor Test Suite in 2007 and 2008. The test suite consists of 204 PDF files intentionally constructed to systematically fail each of the requirements for PDF/A-1b conformance, allowing developers to test the ability of their software to validate against the standard's most basic level <b>of</b> <b>conformance.</b> By mid-2009 the test suite had already made an appreciable difference in the general <b>quality</b> <b>of</b> PDF/A validation software.|$|R
5000|$|Design output, {{including}} evaluation <b>of</b> <b>conformance</b> {{to design}} input requirements through: ...|$|R
5000|$|The purpose <b>of</b> <b>conformance</b> {{checking}} is {{to identify}} two types of discrepancies: ...|$|R
50|$|The VCCI mark <b>of</b> <b>conformance</b> {{also appears}} on some {{electrical}} equipment sold outside Japan.|$|R
40|$|Abstract: For {{systematic}} and automatic testing of cyber-physical systems, {{in which a}} set of test cases is generated based on a formal specification, a number <b>of</b> notions <b>of</b> <b>conformance</b> testing have been proposed. In this paper, we review two existing theories <b>of</b> <b>conformance</b> testing for cyber-physical systems and compare them. We point out their fundamental differences, and prove under which assumptions they coincide...|$|R
5000|$|... "The {{purpose of}} a {{software}} audit is to provide an independent evaluation <b>of</b> <b>conformance</b> <b>of</b> software products and processes to applicable regulations, standards, guidelines, plans, and procedures". [...] The following roles are recommended: ...|$|R
