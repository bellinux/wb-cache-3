5|47|Public
50|$|In {{variance}} analysis, {{direct material}} usage (efficiency, <b>quantity)</b> <b>variance</b> {{is the difference}} between the standard quantity of materials that should have been used for the number of units actually produced, and the actual quantity of materials used, valued at the standard cost per unit of material. It is one of the two components (the other is direct material price variance) of direct material total variance.|$|E
40|$|When {{dealing with}} time series, the {{one step ahead}} {{forecasting}} problem based on experimental data {{is the problem of}} estimating the autoregression function of the underlying process. When minimizing the expected forecasting error is the main goal the flexible approach has to be {{used to be able to}} adjust the complexity of the model to the complexity of the data. Multilayered perceptrons are a popular example of such a flexible approach but not the only one. Other methods such as kernel approximator (e. g. Naradaya Watson regressor), regression spline or wavelet regressor can also be used. But whatever flexible approach is, the main issue remains the control of the complexity of the flexible approximator. Noise injection in the inputs is an efficient technique to do so. The complexity of the regessor is then adjusted thanks to the <b>quantity</b> (<b>variance)</b> of injected noise. This quantity is tuned using a bootstrap estimation of the forecasting error. One unexpected effect of this ap [...] ...|$|E
40|$|According to {{the last}} {{revision}} of international recommendations, numerical methods are nowadays claimed/or the estimation of measurement uncertainty in indirect measurements. The paper, in particular, proposes the use of numeric integration of measurement models as a fast and reliable method for uncertainty estimation. Starting from {{the knowledge of the}} probability density functions of the input quantities, the method applies traditional techniques of numeric integration to the measurement model in order to achieve an estimate the output <b>quantity</b> <b>variance,</b> and, consequently, of the output standard uncertainty. A number of tests have been carried out to assess the performance of the proposed method. In particular, the concurrence between the estimates of output quantity expectation and standard uncertainty provided by the method and those granted by Monte Carlo simulations or method based on unscented transform has been verified along with a comparison of computational times. The obtained results highlight the efficacy of the method and suggest it as an attractive alternative to other approaches currently adopted...|$|E
50|$|Purchase Order Processing: enter {{purchase}} transactions {{earlier in}} the cycle to better manage costs and improve efficiency. Complete purchasing audit control with comprehensive selection of reports to track and analyze purchase activity (full historical and performance reporting). Option to print or e-mail purchase order documents. Other features include Auto-Receive, Auto-Invoice, VAT tracking, facility to handle price and <b>quantity</b> <b>variances,</b> approvals option, commitment reporting. Integrates smoothly with Payables Management, General Ledger, Sales Order Processing and Inventory.|$|R
3000|$|... (i) is the {{decision}} <b>quantity,</b> the <b>variance</b> {{of the power}} becomes the corresponding cost function, and eq. (7) is the dynamic equation.|$|R
40|$|Traditional {{standard}} cost variance analysis procedures are examined as motivational devices in a principal-agent model. The reexpressing of a cost realization into components (such as individual factor price and <b>quantity</b> <b>variances)</b> {{is shown to}} be useful if an incentive problem exists and if the separate components are differentially correlated with the agent's behavior. Similarly, the investigation of selected variances is shown to have desirable motivational effects. However, the optimal variance investigation policy is shown {{to be far more}} subtle than those found in single-person analyses. accounting, information systems: management...|$|R
40|$|The direct {{material}} cost variance can be subdivided into a price variance, a <b>quantity</b> <b>variance</b> and a price-quantity interaction variance. The price-quantity interaction variance is rarely {{mentioned in the}} literature because the traditional price variance does not acknowledge an interaction variance. For a number of pragmatic reasons, this approach may be justified for the {{direct material}} price variance. The direct labor cost variance is conceptually similar to the direct material cost variance. Accordingly, the traditional direct labor rate variance also includes a rate-efficiency interaction variance. However, the justifications for incorporating the interaction variance into the direct material price variance {{do not apply to}} the direct labor rate variance. This paper explores the possibility of separating the rate-efficiency interaction variance from the direct labor rate variance. This approach may be more aligned with the concept of responsibility accounting than the traditional method of calculating the direct labor rate variance. Thus, it may provide more reliable information feedback for decision-making purposes...|$|E
40|$|Previous {{research}} has shown that individuals systematically and persistently deviate from the profit maximizing quantity when solving a newsvendor problem. This research posits that Dual Process Theory provides an underlying cognitive explanation for why individuals deviate from optimality. More specifically, this research explores the relationship between individual performance and a Dual Process Theory construct called cognitive reflection, which can be measured by the Cognitive Reflection Test (CRT). We experimentally test the relationship between cognitive reflection and newsvendor decision-making using 313 experienced supply chain professionals. We find statistically significant results showing that cognitive reflection is related to performance as measured by expected profit, order quantity, and order <b>quantity</b> <b>variance.</b> Cognitive reflection is also related to anchoring heuristics and preference to reduce ex post inventory error. Other potential explanations of individual heterogeneity, including college major, years of experience, and managerial position, are also evaluated and found to be less informative than CRT scores. These results suggest that Dual Process Theory contributes to a theoretical understanding of supply chain decision-making. These results can be used to infor...|$|E
50|$|P-boxes are {{specified}} by {{left and right}} bounds on the cumulative probability distribution function (or, equivalently, the survival function) of a quantity and, optionally, additional information about the <b>quantity’s</b> mean, <b>variance</b> and distributional shape (family, unimodality, symmetry, etc.). A p-box represents a class of probability distributions consistent with these constraints.|$|R
40|$|Policy {{makers are}} often {{interested}} in how values for an environmental asset may be disaggregated into component pieces and transferred from one site to another. This issue {{can be described in}} relation to environmental values for the Fitzroy Basin in Central Queensland. The Basin comprises several smaller catchments that share similar development opportunities, environmental issues and water resource constraints. This paper describes the application of Choice Modelling to estimate nvironmental and social values for the Basin as a whole and two of its component sub-catchments. The results of the application provide an understanding of how the values estimated at different scales may be related. If there are no significant differences in values estimated at different scales, it implies that benefit transfer may be undertaken for different sites at different scales without adjustment. Various experiments using scale and <b>quantity</b> <b>variances</b> are reported to allow comparison of the estimates...|$|R
40|$|Using {{refractive}} index and density data for nematogenic compounds Octyl benzoic acid (OBA) and Nonyl benzoic acid (NBA), orientational order and hence distribution function has been determined. From this, the higher order parameter [P- 4] {{has been estimated}} for these two compounds. The physical interpretation of these parameters [P- 2] and [P- 4] has been given in terms of statistical <b>quantities</b> like <b>variance</b> and kurtosis...|$|R
40|$|The {{purpose of}} this paper is to {{investigate}} the impact of production cost variability upon hedging decision when the firm is a risk minimizer agent facing both price and quantity uncertainties. We show, under a perfect flexibility assumption, that considering cost variability leads to a lower [higher] optimal hedge ratio assuming a positive [negative] relation between prices and <b>quantities.</b> MINIMUM <b>VARIANCE</b> HEDGE; UNCERTAIN DEMAND; PERFECT FLEXIBILITY; COST FUNCTION. ...|$|R
5000|$|By {{equalizing}} this <b>quantity</b> {{with the}} <b>variance</b> associated {{to the position}} of the random walker, one obtains the equivalent diffusion coefficient to be considered for the asymptotic Wiener process toward which the random walk converges after a large number of steps: ...|$|R
40|$|MBA Professional ReportFor six of {{the past}} eight years, naval {{aviation}} depot-level maintenances activities have encountered operating losses that were not anticipated in the Navy Working Capital Fund (NWCF) budgets. These unanticipated losses resulted in increases or surcharges to the stabilized rates as an offset. This project conducts a variance analysis to uncover possible causes of the unanticipated losses. The variance analysis between budgeted (projected) and actual financial results was performed on financial data collected on the E- 2 C aircraft program from Fleet Readiness Center Southwest (FRCSW) located in San Diego, California. The results of the variance analysis are interpreted and {{discussed in terms of}} labor sales quantity, mix, and rate variances, material sales variance, material expense variance, labor, production overhead, and general and administrative rate/spending and <b>quantity</b> <b>variances.</b> The results of this project reveal the factors that created the greatest variance in FRCSW's net operating results. The variance analysis suggests that the factors having the greatest affect on the operating results were the material sales variances, material expense variances, and the variances due to the quantity of work. Additionally, the analysis revealed that during the year analyzed (FY 2007) FRCSW was not reimbursed for 21 percent of its material costs...|$|R
5000|$|The {{previous}} variance term {{alludes to}} how moments measures, like moments of random variables, {{can be used}} to calculate <b>quantities</b> like the <b>variance</b> of point processes. A further example is the covariance of a point process [...] for two Borel sets [...] and , which is given by: ...|$|R
40|$|We {{investigate}} {{the role of}} energy price shocks on business cycle fl uctuations in Bangladesh. In doing so, we calibrate a Dynamic Stochastic General Equilibrium (DSGE) model, allowing for both energy consumption by households and as an input in production. We fi nd that qualitatively temporary energy price shocks and technology shocks produce similar impulse response functions, as well as similar (quantitatively) autocorrelations in aggregate <b>quantities.</b> The <b>variance</b> in aggregate <b>quantities</b> are better explained by technology shocks than by energy price shocks, suggesting that technology shocks are more important source of fl uctuations in Bangladesh...|$|R
40|$|Estimates {{of genetic}} {{variance}} components {{depend on the}} type of marker used, the definitions of geographic regions, the populations sampled within these regions, the relative sample sizes from the populations, {{and the way in which}} information is combined across loci. For microsatellite markers, estimates also depend on whether the <b>quantity</b> whose <b>variance</b> is partitioned is an allele-size variable or an indicator variable for allelic presence or absence. A main purpose of our variance component estimation was to provide insight into the finescale population structure analysis in (1). Because the structure algorithm uses onl...|$|R
40|$|Neutron {{fluctuations}} {{in a constant}} multiplying medium (zero power noise) and those in a fluctuating medium (power reactor noise) have been traditionally considered as two separate disciplines that exist in two opposing limiting areas of operation (low and high power, respectively). They have also been treated by different mathematical methods, i. e., master equations and Langevin equation, respectively. In this paper we develop a theory of neutron {{fluctuations in}} a medium randomly varying in time, based on a forward-type master equation approach. This method accounts for both the zero power and the power reactor noise simultaneously. Factorial moments and related <b>quantities</b> (<b>variance,</b> power spectrum, etc.) {{of the number of}} the neutrons are calculated in subcritical systems with a stationary external source. It is shown that the pure zero power and power reactor noise results can be reconstructed in the cases of vanishing system fluctuations and high power, respectively, the latter being a nontrivial result. Further, it is shown that the effect of system fluctuations on the zero power noise is retained even in the limit of vanishing neutron number (reactor power). The results have thus even practical significance for low-power systems with fluctuating properties. The results also have a bearing on other types of branching processes such as evolution of biological systems, germ colonies, epidemics, etc., which take place in a time-varying environment...|$|R
30|$|Objective {{function}} is generally composed of design variables {{as well as}} control variables {{in the form of}} Θ[*]=[*]f(x, y). Thus, the objective function for each scenario S is ΘS[*]=[*]f(x, yS). High <b>quantity</b> for the <b>variance</b> illustrates the fact that a small variation in parameters with uncertainty may end up in substantial alteration in value of measuring function.|$|R
5000|$|The great body of {{available}} statistics {{show us that}} the deviations of a human measurement from its mean follow very closely the Normal Law of Errors, and, therefore, that the variability may be uniformly measured by the standard deviation corresponding to the square root of the mean square error. When there are two independent causes of variability capable of producing in an otherwise uniform population distributions with standard deviations [...] and , {{it is found that}} the distribution, when both causes act together, has a standard deviation [...] It is therefore desirable in analysing the causes of variability to deal with the square of the standard deviation as the measure of variability. We shall term this <b>quantity</b> the <b>Variance...</b>|$|R
25|$|In physics or {{engineering}} education, a Fermi problem, Fermi quiz, Fermi question, Fermi estimate, {{or order}} estimation is an estimation problem {{designed to teach}} dimensional analysis, approximation, and such a problem is usually a back-of-the-envelope calculation. The estimation technique is named after physicist Enrico Fermi as he {{was known for his}} ability to make good approximate calculations with little or no actual data. Fermi problems typically involve making justified guesses about <b>quantities</b> and their <b>variance</b> or lower and upper bounds.|$|R
40|$|A {{confidence}} interval {{is a standard}} way of expressing our uncertainty {{about the value of}} a population parameter. In survey sampling most methods of {{confidence interval}} estimation rely on “reasonable” assumptions to be true in order to achieve nominal coverage levels. Typically these correspond to replacing complex sample statistics by large sample approximations and invoking central limit behaviour. Unfortunately, coverage of these intervals in practice is often much less than anticipated, particularly in unbalanced samples. This paper explores an alternative approach, based on a generalisation of quantile regression analysis, to defining an interval estimate that captures our uncertainty about an unknown population quantity. These quantile-based intervals seem more robust and stable than confidence intervals, particularly in unbalanced situations. Furthermore, they do not involve estimation of second order <b>quantities</b> like <b>variances,</b> which is often difficult and time-consuming for non-linear estimators. We present empirical results illustrating this alternative approach and discuss implications for its use...|$|R
40|$|Introduction Online Monitoring is {{a rapidly}} {{expanding}} field {{in different areas}} such as quality control, finance and navigation. The automated detection of so-called changepoints is playing {{a prominent role in}} all these fields, be it the detection of sudden shifts of the mean of a continuously monitored <b>quantity,</b> the <b>variance</b> of stock quotes or the change of some characteristic features indicating the malfunctioning of one of the detectors used for navigation (the "faulty sensor problem"). A prominent example for the application of advanced statistical methods for the detection of changepoints in biomedical time series is the multi-process Kalman filter used by Smith and West [Smith 1983] to monitor renal transplants. However, {{despite the fact that the}} algorithm could be tuned in such a way that the computer could predict dangerous situations on the average one day before the human experts it has nevertheless become superfluous as soon as new diagnsoic tools became available. ...|$|R
40|$|Includes bibliographical {{references}} (pages 49 - 50) The {{effect of}} variance of data, horizontal scale size, and {{the quantity of}} data points plotted, on trend estimation accuracy was examined using two graph sizes as pictorial displays. A factorial design was employed using two groups of eighteen subjects. The dependent variables were the slope, intercept, and the sum of squared deviations about the least squared error regression line as measured in millimeters difference between the estimated and true lines. The results indicated that {{no significant difference in}} estimation accuracy occurred due to graph size except by one measure, the absolute intercept. The implications of this finding are discussed. Estimation accuracy increased significantly as the quantity of data plotted increased. Estimation accuracy decreased significantly as the variance of the plotted data increased. A <b>quantity</b> by <b>variance</b> interaction was found with the absolute intercept measure. The intermediate quantity condition (12 data points) was most sensitive to the detrimental influence of increased variance. Intercepts were underestimated, and slopes were overestimated significantly for both graph sizes. Methodological problems and suggestions for future research are discussed...|$|R
40|$|In using entropy {{maximization}} {{models to}} forecast locational and travel behaviour, one is {{confronted with the}} problem of delineating the choice process as precisely as possible. In addition to defining a fine-grain choice structure implying individuals seeking distinct location sites within residential zones and travelling to distinct jobs or shops within destination zones, this note also accounts {{for the fact that the}} location choice is of a site for a household or firm, but the corresponding travel choices are by individual members of a household. In conjunction with disaggregation across <b>quantities</b> with large <b>variance,</b> the above principles are applied to formulate improved versions of residential and shopping location models. ...|$|R
50|$|Checking {{the amount}} of {{variance}} in the data after computing the SVD {{can be used to}} determine the optimal number of dimensions to retain. The variance contained in the data can be viewed by plotting the singular values (S) in a scree plot. Some LSI practitioners select the dimensionality associated with the knee of the curve as the cut-off point for the number of dimensions to retain. Others argue that some <b>quantity</b> of the <b>variance</b> must be retained, and {{the amount of}} variance in the data should dictate the proper dimensionality to retain. Seventy percent is often mentioned as the amount of variance in the data that should be used to select the optimal dimensionality for recomputing the SVD.|$|R
40|$|The {{traditional}} {{techniques of}} calculating <b>quantity</b> and price <b>variances</b> for analyzing deviations of realized profit contribution (actual) from the planned profit contribution only offer {{the benefit of}} identifying areas where problems may exist, rather than diagnosing the causes of these problems. Therefore, it is proposed to base profit contribution variance analysis on market response functions which have been assumed when fixing the planned marketing budgets, prices and quantities. This allows for a decomposition of total contribution variance into causal sources such as realization, effectiveness, reaction and planning variance. It is also shown how the variance of any such source can be separated into effects caused by exogenous factors as well as single market instruments...|$|R
40|$|The gmwm R {{package for}} {{inference}} on time series models is mainly {{based on the}} <b>quantity</b> called wavelet <b>variance</b> which is derived from a wavelet decomposition of a time series. This quantity provides a means to summarize and graphically represent the features of time series {{in order to identify}} possible models. Moreover, it is used as a moment condition for model estimation through the generalized method of wavelet moments. Based on the latter method, this package not only provides an alternative method to estimate classical ARMA models but also delivers a general framework for the robust estimation of many time series models as well as a quick and efficient estimation of many linear state-space models...|$|R
30|$|The {{average is}} the first moment of a random variable. Often {{it is useful to}} also examine <b>quantities</b> such as <b>variance,</b> which are derived from higher moments; for example, it might be {{interesting}} to examine whether some types of input produce a noisier output than others. Certainly, it is easy to estimate the variance in the function space, giving a form of the peri-stimulus variance histogram [16]. However, this is a function and {{it is not clear how}} to interpret the function variance in terms of spike trains. This would be an interesting topic for further work. Another approach would be to examine the distribution of displacements between the central spike train and the spike trains in the collection, something that has previously been considered using pair-wise displacement in the collection [9].|$|R
40|$|The {{realization}} that electron localization in disordered systems (Anderson localization) is ultimately a wave phenomenon {{has led to}} the suggestion that photons could be similarly localized by disorder. This conjecture attracted wide interest because the differences between photons and electrons - in their interactions, spin statistics, and methods of injection and detection - may open a new realm of optical and microwave phenomena, and allow a detailed study of the Anderson localization transition undisturbed by the Coulomb interaction. To date, claims of three-dimensional photon localization have been based on observations of the exponential decay of the electromagnetic wave as it propagates through the disordered medium. But these reports have come under close scrutiny because of the possibility that the decay observed may be due to residual absorption, and because absorption itself may suppress localization. Here we show that the extent of photon localization can be determined by a different approach - measurement of the relative size of fluctuations of certain transmission <b>quantities.</b> The <b>variance</b> of relative fluctuations accurately reflects the extent of localization, even in the presence of absorption. Using this approach, we demonstrate photon localization in both weakly and strongly scattering quasi-one-dimensional dielectric samples and in periodic metallic wire meshes containing metallic scatterers, while ruling it out in three-dimensional mixtures of aluminum spheres. Comment: 5 pages, including 4 figure...|$|R
40|$|Retail {{business}} {{competition in}} Indonesia is complex {{to support the}} traditional retail sustainabilityefforts in indonesia. So, it’s necessary to develop the partnership pattern that is oriented in entrepeneurship and business ethics for the small business player to build the marketing performance and the sustainabilityin the retail busines. This research is a qualitative deskriptive research, that using the focus group discusion and interview with retailer respondent in the East Java. The result of this research are 1) the exploration result for the retail partnership pattern in East Java small market scale this time is running on the exclusive mutual pattern 2) market performance and sustainabilityof retail of small scale market in East Java is showing the sustainabilityaspect, with the income average per month/day is always increase. From The aspect of quantity and goods completeness, the total of good is increase from the <b>quantity</b> and <b>variance.</b> 3) creating a pattern of partnership-based on the entrepeneurship oriented and business ethics will be done by several ways: {{taking advantage of the}} existance of cooperative/association on creating relation with the supplier, gaining solidarity one another and aware the importance of unity between retailer to build the pattern that balanc. especially on determining the bargaining set position against the modern retail. Intensive giving the information to the retailer about the awareness of retail management based on entrepreneurship and business ethics...|$|R
40|$|A {{quantity}} exists {{by which}} one can identify the approach of a dynamical system {{to the state of}} criticality, which is hard to identify otherwise. This <b>quantity</b> is the <b>variance</b> of natural time χ, where and pk is the normalized energy released during the kth event of which the natural time is defined as χk = k/N and N stands for the total number of events. Then we show that κ 1 becomes equal to 0. 070 at the critical state for a variety of dynamical systems. This holds for criticality models such as 2 D Ising and the Bak–Tang–Wiesenfeld sandpile, which is the standard example of self-organized criticality. This condition of κ 1  =  0. 070 holds for experimental results of critical phenomena such as growth of rice piles, seismic electric signals, and the subsequent seismicity before the associated main shock...|$|R
40|$|Abstract Background Operation {{of natural}} {{selection}} can be characterized {{by a variety of}} <b>quantities.</b> Among them, <b>variance</b> of relative fitness V and load L are the most fundamental. Results Among all modes of selection that produce a particular value V of the variance of relative fitness, the minimal value L min of load L is produced by a mode under which fitness takes only two values, 0 and some positive value, and is equal to V/(1 +V). Conclusions Although it is impossible to deduce the load from knowledge of the variance of relative fitness alone, it is possible to determine the minimal load consistent with a particular variance of relative fitness. The concept of minimal load consistent with a particular biological phenomenon may be applicable to studying several aspects {{of natural selection}}. Reviewers The manuscript was reviewed by Sergei Maslov, Alexander Gordon, and Eugene Koonin. </p...|$|R
40|$|We {{present a}} {{stochastic}} formalism for signal transduction processes in bacterial two-component system. Using elementary mass action kinetics, the proposed model {{takes care of}} signal transduction in terms of phosphotransfer mechanism between the cognate partners of a two-component system, viz, the sensor kinase and the response regulator. Based on the difference in functionality of the sensor kinase, the noisy phosphotransfer mechanism has been studied for monofunctional and bifunctional two component system using the formalism of linear noise approximation. Steady state analysis of both models quantifies different physically realizable <b>quantities,</b> e. g., <b>variance,</b> coefficient of variation, mutual information. The resultant data reveals that both systems reliably transfer information of extra-cellular environment under low external stimulus and at high kinase and phosphatase regime. We extend our analysis further by studying the role of two-component system in downstream gene regulation. Comment: Revised version, 11 pages, 7 figure...|$|R
40|$|Test case {{prioritization}} techniques let testers order their {{test cases}} {{so that those}} with higher priority, according to some criterion, are executed earlier than those with lower priority. In previous work, we examined a variety of prioritization techniques to determine their ability to improve the rate of fault detection of test suites. Our studies showed {{that the rate of}} fault detection of test suites could be significantly improved by using more powerful prioritization techniques. In addition, they indicated that rate of fault detection was closely associated with the target program. We also observed a large <b>quantity</b> of unexplained <b>variance,</b> indicating that other factors must be affecting prioritization effectiveness. These observations motivate the following research questions: (1) Are there factors other than the target program and the prioritization technique that consistently affect the rate of fault detection of test suites? (2) What metrics are most representative of each fact [...] ...|$|R
40|$|In this paper, {{we present}} a new {{algorithm}} that adaptively selects the best possible reference frame for the predictive coding of generalized, or multi-view, video signals, based on estimated prediction similarity with the desired frame. We define similarity between two frames as the absence of occlusion, and we estimate this <b>quantity</b> from the <b>variance</b> of composite displacement vector maps. The composite maps are obtained without requiring the computationally intensive process of motion estimation for each candidate reference frame. We provide prediction and compression performance results for generalized video signals using both this scheme and schemes where the reference frames were heuristically pre-selected. When the predicted frames were used in a modified MPEG encoder simulation, the signal compressed using the adaptively selected reference frames required, on average, more than 10 % fewer bits to encode than the non-adaptive techniques; for individual frames, the reduction in bits was sometimes more than 80 %. These gains were obtained with an acceptable computational increase and an inconsequential bit-count overhead...|$|R
