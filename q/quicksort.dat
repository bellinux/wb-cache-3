749|5|Public
25|$|<b>Quicksort</b> is a {{divide and}} conquer algorithm. <b>Quicksort</b> first divides a large array into two smaller sub-arrays: the low {{elements}} and the high elements. <b>Quicksort</b> can then recursively sort the sub-arrays.|$|E
25|$|<b>Quicksort</b> is {{typically}} somewhat faster due to some factors, but the worst-case running time for <b>quicksort</b> is , which is unacceptable for large data sets {{and can be}} deliberately triggered given enough knowledge of the implementation, creating a security risk. See <b>quicksort</b> {{for a detailed discussion}} of this problem and possible solutions.|$|E
25|$|The {{most direct}} {{competitor}} of <b>quicksort</b> is heapsort. Heapsort's running time is , but heapsort's average running time is usually considered slower than in-place <b>quicksort.</b> This result is debatable; some publications indicate the opposite. Introsort is {{a variant of}} <b>quicksort</b> that switches to heapsort when a bad case is detected to avoid quicksort's worst-case running time.|$|E
25|$|Introsort is an {{alternative}} to heapsort that combines <b>quicksort</b> and heapsort to retain advantages of both: worst case speed of heapsort and average speed of <b>quicksort.</b>|$|E
25|$|In 2009, Vladimir Yaroslavskiy {{proposed}} the new dual pivot <b>Quicksort</b> implementation. In the Java core library mailing lists, he initiated a discussion claiming his new algorithm {{to be superior}} to the runtime library’s sorting method, which {{was at that time}} based on the widely used and carefully tuned variant of classic <b>Quicksort</b> by Bentley and McIlroy. Yaroslavskiy’s <b>Quicksort</b> has been chosen as the new default sorting algorithm in Oracle’s Java 7 runtime library after extensive empirical performance tests.|$|E
25|$|The space used by <b>quicksort</b> {{depends on}} the version used.|$|E
25|$|Heapsort {{primarily}} {{competes with}} <b>quicksort,</b> another very efficient general purpose nearly-in-place comparison-based sort algorithm.|$|E
25|$|Related {{problems}} include partial sorting (sorting {{only the}} k smallest {{elements of a}} list, or alternatively computing the k smallest elements, but unordered) and selection (computing the kth smallest element). These can be solved inefficiently by a total sort, but more efficient algorithms exist, often derived by generalizing a sorting algorithm. The most notable example is quickselect, which is related to <b>quicksort.</b> Conversely, some sorting algorithms can be derived by repeated application of a selection algorithm; <b>quicksort</b> and quickselect {{can be seen as}} the same pivoting move, differing only in whether one recurses on both sides (<b>quicksort,</b> divide and conquer) or one side (quickselect, decrease and conquer).|$|E
25|$|Practical general sorting {{algorithms}} {{are almost}} always based on an algorithm with average time complexity (and generally worst-case complexity) O(n log n), of which the most common are heap sort, merge sort, and <b>quicksort.</b> Each has advantages and drawbacks, with the most significant being that simple implementation of merge sort uses O(n) additional space, and simple implementation of <b>quicksort</b> has O(n2) worst-case complexity. These problems can be solved or ameliorated {{at the cost of}} a more complex algorithm.|$|E
25|$|<b>Quicksort</b> is a {{divide and}} conquer {{algorithm}} which relies on a partition operation: to partition an array an element called a pivot is selected. All elements smaller than the pivot are moved before it and all greater elements are moved after it. This can be done efficiently in linear time and in-place. The lesser and greater sublists are then recursively sorted. This yields average time complexity of O(n log n), with low overhead, and thus this is a popular algorithm. Efficient implementations of <b>quicksort</b> (with in-place partitioning) are typically unstable sorts and somewhat complex, but are among the fastest sorting algorithms in practice. Together with its modest O(log n) space usage, <b>quicksort</b> {{is one of the}} most popular sorting algorithms and is available in many standard programming libraries.|$|E
25|$|Another {{technique}} for overcoming the memory-size problem is using external sorting, for example {{one of the}} ways is to combine two algorithms in a way that takes advantage of the strength of each to improve overall performance. For instance, the array might be subdivided into chunks of a size that will fit in RAM, the contents of each chunk sorted using an efficient algorithm (such as <b>quicksort),</b> and the results merged using a k-way merge similar to that used in mergesort. This is faster than performing either mergesort or <b>quicksort</b> over the entire list.|$|E
25|$|Mathematical {{analysis}} of <b>quicksort</b> shows that, on average, the algorithm takes O(nlogn) comparisons to sort n items. In the worst case, it makes O(n2) comparisons, though this behavior is rare.|$|E
25|$|Bottom-up {{heapsort}} {{was announced}} as beating <b>quicksort</b> (with median-of-three pivot selection) on arrays of size ≥16000. This version of heapsort keeps the linear-time heap-building phase, but changes the second phase, as follows.|$|E
25|$|<b>Quicksort</b> gained {{widespread}} adoption, appearing, for example, in Unix as {{the default}} library sort subroutine. Hence, it lent {{its name to}} the C standard library subroutine qsort and in the reference implementation of Java.|$|E
25|$|<b>Quicksort,</b> O(n log n), in its {{randomized}} version, has {{a running}} {{time that is}} O(n log n) in expectation on the worst-case input. Its non-randomized version has a O(n log n) running time only when considering average case complexity.|$|E
25|$|General method: insertion, exchange, selection, merging, etc. Exchange sorts include {{bubble sort}} and <b>quicksort.</b> Selection sorts include shaker sort and heapsort. Also whether the {{algorithm}} is serial or parallel. The {{remainder of this}} discussion almost exclusively concentrates upon serial algorithms and assumes serial operation.|$|E
25|$|The {{important}} caveat about <b>quicksort</b> is {{that its}} worst-case performance is O(n2); while this is rare, in naive implementations (choosing the first or last element as pivot) this occurs for sorted data, which is a common case. The most complex issue in <b>quicksort</b> is thus choosing a good pivot element, as consistently poor choices of pivots can result in drastically slower O(n2) performance, but good choice of pivots yields O(n log n) performance, which is asymptotically optimal. For example, if at each step the median is chosen as the pivot then the algorithm works in O(nlogn). Finding the median, such as by the median of medians selection algorithm is however an O(n) operation on unsorted lists and therefore exacts significant overhead with sorting. In practice choosing a random pivot almost certainly yields O(nlogn) performance.|$|E
25|$|The {{partition}} algorithm returns indices to {{the first}} ('leftmost') and to the last ('rightmost') item of the middle partition. Every item of the partition is equal to p and is therefore sorted. Consequently the items of the partition need not {{be included in the}} recursive calls to <b>quicksort.</b>|$|E
25|$|The same {{as regular}} <b>quicksort</b> except the pivot is {{replaced}} by a buffer. First, read the M/2 first and last elements into the buffer and sort them. Read the next element from the beginning or end to balance writing. If the next element is less than the least of the buffer, write it to available space at the beginning. If greater than the greatest, write it to the end. Otherwise write the greatest or least of the buffer, and put the next element in the buffer. Keep the maximum lower and minimum upper keys written to avoid resorting middle elements that are in order. When done, write the buffer. Recursively sort the smaller partition, and loop to sort the remaining partition. This is a kind of three-way <b>quicksort</b> in which the middle partition (buffer) represents a sorted subarray of elements that are approximately equal to the pivot.|$|E
25|$|<b>Quicksort</b> (sometimes called partition-exchange sort) is an {{efficient}} sorting algorithm, {{serving as a}} systematic method for placing the elements of an array in order. Developed by Tony Hoare in 1959 and published in 1961, {{it is still a}} commonly used algorithm for sorting. When implemented well, it can be about two or three times faster than its main competitors, merge sort and heapsort.|$|E
25|$|The average case is also quadratic, {{which makes}} {{insertion}} sort impractical for sorting large arrays. However, insertion sort {{is one of}} the fastest algorithms for sorting very small arrays, even faster than quicksort; indeed, good <b>quicksort</b> implementations use insertion sort for arrays smaller than a certain threshold, also when arising as subproblems; the exact threshold must be determined experimentally and depends on the machine, but is commonly around ten.|$|E
25|$|<b>Quicksort</b> {{has some}} {{disadvantages}} {{when compared to}} alternative sorting algorithms, like merge sort, which complicate its efficient parallelization. The depth of quicksort's divide-and-conquer tree directly impacts the algorithm's scalability, and this depth is highly dependent on the algorithm's choice of pivot. Additionally, {{it is difficult to}} parallelize the partitioning step efficiently in-place. The use of scratch space simplifies the partitioning step, but increases the algorithm's memory footprint and constant overheads.|$|E
25|$|For example, {{the popular}} {{recursive}} <b>quicksort</b> algorithm provides quite reasonable performance with adequate RAM, {{but due to}} the recursive way that it copies portions of the array it becomes much less practical when the array does not fit in RAM, because it may cause a number of slow copy or move operations to and from disk. In that scenario, another algorithm may be preferable even if it requires more total comparisons.|$|E
25|$|Various {{efforts have}} been made to {{eliminate}} turtles to improve upon the speed of bubble sort. Cocktail sort is a bi-directional bubble sort that goes from beginning to end, and then reverses itself, going end to beginning. It can move turtles fairly well, but it retains O(n2) worst-case complexity. Comb sort compares elements separated by large gaps, and can move turtles extremely quickly before proceeding to smaller and smaller gaps to smooth out the list. Its average speed is comparable to faster algorithms like <b>quicksort.</b>|$|E
25|$|With a {{partitioning}} algorithm such as {{the ones}} described above (even with one that chooses good pivot values), <b>quicksort</b> exhibits poor performance for inputs that contain many repeated elements. The problem is clearly apparent when all the input elements are equal: at each recursion, the left partition is empty (no input values are less than the pivot), and the right partition has only decreased by one element (the pivot is removed). Consequently, the algorithm takes quadratic time to sort an array of equal values.|$|E
25|$|The <b>quicksort</b> {{algorithm}} {{was developed}} in 1959 by Tony Hoare while in the Soviet Union, as a visiting student at Moscow State University. At that time, Hoare worked in a project on machine translation for the National Physical Laboratory. As {{a part of the}} translation process, he needed to sort the words of Russian sentences prior to looking them up in a Russian-English dictionary that was already sorted in alphabetic order on magnetic tape. After recognizing that his first idea, insertion sort, would be slow, he quickly came up with a new idea that was <b>Quicksort.</b> He wrote a program in Mercury Autocode for the partition but couldn't write the program to account for the list of unsorted segments. On return to England, he was asked to write code for Shellsort as part of his new job. Hoare mentioned to his boss that he knew of a faster algorithm and his boss bet sixpence that he didn't. His boss ultimately accepted that he had lost the bet. Later, Hoare learned about ALGOL and its ability to do recursion that enabled him to publish the code in Communications of the Association for Computing Machinery, the premier computer science journal of the time.|$|E
25|$|A {{selection}} algorithm {{chooses the}} kth smallest {{of a list}} of numbers; this is an easier problem in general than sorting. One simple but effective selection algorithm works nearly {{in the same manner}} as <b>quicksort,</b> and is accordingly known as quickselect. The difference is that instead of making recursive calls on both sublists, it only makes a single tail-recursive call on the sublist that contains the desired element. This change lowers the average complexity to linear or O(n) time, which is optimal for selection, but the sorting algorithm is still O(n2).|$|E
25|$|For example, {{consider}} the deterministic sorting algorithm <b>quicksort.</b> This solves {{the problem of}} sorting a list of integers that is given as the input. The worst-case is when the input is sorted or sorted in reverse order, and the algorithm takes time O(n2) for this case. If we assume that all possible permutations of the input list are equally likely, the average time taken for sorting is O(n log n). The best case occurs when each pivoting divides the list in half, also needing O(n log n) time.|$|E
25|$|When the {{bottleneck}} is localized, optimization usually {{starts with}} a rethinking of the algorithm used in the program. More often than not, a particular algorithm can be specifically tailored to a particular problem, yielding better performance than a generic algorithm. For example, the task of sorting a huge list of items is usually done with a <b>quicksort</b> routine, {{which is one of}} the most efficient generic algorithms. But if some characteristic of the items is exploitable (for example, they are already arranged in some particular order), a different method can be used, or even a custom-made sort routine.|$|E
25|$|The only {{significant}} advantage that bubble sort has over most other implementations, even <b>quicksort,</b> but not insertion sort, {{is that the}} ability to detect that the list is sorted efficiently is built into the algorithm. When the list is already sorted (best-case), the complexity of bubble sort is only O(n). By contrast, most other algorithms, even those with better average-case complexity, perform their entire sorting process on the set and thus are more complex. However, not only does insertion sort have this mechanism too, but it also performs better on a list that is substantially sorted (having a small number of inversions).|$|E
25|$|While these {{algorithms}} are asymptotically efficient on random data, {{for practical}} efficiency on real-world data various modifications are used. First, the overhead of these algorithms becomes significant on smaller data, so often a hybrid algorithm is used, commonly switching to insertion sort once {{the data is}} small enough. Second, the algorithms often perform poorly on already sorted data or almost sorted data – these are common in real-world data, and can be sorted in O(n) time by appropriate algorithms. Finally, {{they may also be}} unstable, and stability is often a desirable property in a sort. Thus more sophisticated algorithms are often employed, such as Timsort (based on merge sort) or introsort (based on <b>quicksort,</b> falling back to heap sort).|$|E
500|$|... converges (i.e., gets {{arbitrarily}} close) to {{a number}} known as the Euler–Mascheroni constant. This relation aids in analyzing the performance of algorithms such as <b>quicksort.</b>|$|E
500|$|Three Oxford mathematicians, Michael Atiyah, Daniel Quillen and Simon Donaldson, {{have won}} Fields Medals, {{often called the}} [...] "Nobel Prize for mathematics". Andrew Wiles, who proved Fermat's Last Theorem, was educated at Oxford and is {{currently}} a Royal Society Research Professor at Oxford. Marcus du Sautoy and Roger Penrose are both currently mathematics professors, and Jackie Stedall is a former professor of the university. Stephen Wolfram, chief designer of Mathematica and Wolfram Alpha studied at the university, along with Tim Berners-Lee, inventor of the World Wide Web, Edgar F. Codd, inventor of the relational model of data, and Tony Hoare, programming languages pioneer and inventor of <b>Quicksort.</b>|$|E
2500|$|<b>Quicksort</b> with in-place and {{unstable}} partitioning uses only constant additional space before making any recursive call. <b>Quicksort</b> must store a constant {{amount of information}} for each nested recursive call. Since the best case makes at most [...] nested recursive calls, it uses [...] space. However, without Sedgewick's trick to limit the recursive calls, in the worst case <b>quicksort</b> could make [...] nested recursive calls and need [...] auxiliary space.|$|E
2500|$|<b>Quicksort</b> also {{competes with}} mergesort, another [...] sorting algorithm. Mergesort is a stable sort, unlike {{standard}} in-place <b>quicksort</b> and heapsort, {{and can be}} easily adapted to operate on linked lists and very large lists stored on slow-to-access media such as disk storage or network attached storage. Although <b>quicksort</b> can be implemented as a stable sort using linked lists, it will often suffer from poor pivot choices without random access. The main disadvantage of mergesort is that, when operating on arrays, efficient implementations require O(n) auxiliary space, whereas the variant of <b>quicksort</b> with in-place partitioning and tail recursion uses only [...] space. (Note that when operating on linked lists, mergesort only requires a small, constant amount of auxiliary storage.) ...|$|E
2500|$|Also {{developed}} by Powers as an o(K) parallel PRAM algorithm. This is again {{a combination of}} radix sort and <b>quicksort</b> but the <b>quicksort</b> left/right partition decision is made on successive bits of the key, and is thus O(KN) for N K-bit keys. Note that all comparison sort algorithms effectively assume an ideal K of O(logN) as if k is smaller we can sort in O(N) using a hash table or integer sorting, and if K >> logN but elements are unique within O(logN) bits, the remaining bits will not be looked at by either <b>quicksort</b> or quick radix sort, and otherwise all comparison sorting algorithms will also have the same overhead of looking through O(K) relatively useless bits but quick radix sort will avoid the worst case O(N2) behaviours of standard <b>quicksort</b> and radix <b>quicksort,</b> and will be faster {{even in the best}} case of those comparison algorithms under these conditions of uniqueprefix(K) >> logN. See Powers [...] for further discussion of the hidden overheads in comparison, radix and parallel sorting.|$|E
