26|10000|Public
50|$|He uses a {{collection}} of project elements from various projects his clients have conducted. He uses this data, Project Experience Risk Information Library (PERIL) database, to <b>quantify</b> <b>and</b> <b>rank</b> classes of risk. In {{the early part of}} his book he uses this significantly and the Appendix lists approximately 120 of the element's descriptions.|$|E
30|$|We were {{motivated}} by the remark of Granovetter that local bridges earn their significance by creating more paths. We tested this hypothesis to see whether Edge Gravity could effectively identify and rank the local bridges of Granovetter’s examples. Our results were positive; in fact, they indicate {{that it is very}} practical to use path enumeration to <b>quantify</b> <b>and</b> <b>rank</b> relative edge importance.|$|E
30|$|We {{note that}} social {{information}} (e.g., gossip or disease transmission) {{does not necessarily}} follow a geodesic route {{from one person to}} another. This emphasizes the significance of counting all possible paths instead of just finding the shortest paths. Most of the metrics described in the literature are based on shortest paths (geodesics). However, Stephenson and Zelen (1989) recognized that in a social network, information does not always travel a geodesic route. They proposed a node centrality measure based on the information contained in all paths rather than in just the shortest paths. In contrast to our method, Stephenson and Zelen did not enumerate all paths to <b>quantify</b> <b>and</b> <b>rank</b> important edges. Instead, they defined a special matrix to capture the information contained in all paths and used it to define a metric of node (rather than edge) importance.|$|E
5000|$|The Nelson {{complexity}} index (NCI) is {{a measure}} to compare the secondary conversion capacity of a petroleum refinery with the primary distillation capacity. [...] The index provides an easy metric for <b>quantifying</b> <b>and</b> <b>ranking</b> the complexity of various refineries and units. To calculate the index, {{it is necessary to}} use complexity factors, which compare the cost of upgrading units to the cost of crude distillation unit.|$|R
40|$|The Transboundary Diagnosis Analysis(TDA) <b>quantifies</b> <b>and</b> <b>ranks</b> water-related {{environmental}} transboundary {{issues and}} their causes {{according to the}} severity of environmental and/or socio-economic impacts. The three main issues in BOBLME are; overexploitation of marine living resources; degradation of mangroves, coral reefs and seagrasses; pollution and water quality. Volume 2 contains background material that sets out the bio-physical and socio-economic characteristics of the BOBLME; {{an analysis of the}} legal,policy and administrative context; and an assessment of the status of marine living resources and marine environment in coastal area...|$|R
40|$|Overheating in {{buildings}} {{is expected to}} increase as global warming continues. This could lead to heatrelated problems ranging from thermal-discomfort and productivity-reduction to illness as well as death. From the indoor-overheating point of view, the sensitivity of 9, 216 Dutch dwelling-case to the climate change is <b>quantified</b> <b>and</b> <b>ranked</b> using detailed simulation and post-processing calculations. The results show that the sensitivity depends significantly on the dwelling’s design/operation characteristics. Minimally-ventilated dwellings are the most sensitive ones. According to the ventilation rate, shading type, insulation level, and other building characteristics, the sensitivity of dwellings could range from 0. 15 to 1. 2 indoor-overheating degree /ambient-warming degree...|$|R
40|$|MEG interictal spikes as {{recorded}} in epilepsy patients are {{a reflection of}} intracranial interictal activity. This study investigates {{the relationship between the}} estimated sources of MEG spikes and the location, distribution and size of interictal spikes in the invasive ECoG of a group of 38 epilepsy patients that are monitored for pre-surgical evaluation. An amplitude/surface area measure is defined to <b>quantify</b> <b>and</b> <b>rank</b> ECoG spikes. It is found that all MEG spikes are associated with an ECoG spike that is among the three highest ranked in a patient. Among the different brain regions considered, the fronto-orbital, inter-hemispheric, tempero-lateral and central regions stand out. In an accompanying simulation study it is shown that for hypothesized extended sources of larger sizes, as suggested by the data, source location, orientation and curvature can partly explain the observed sensitivity of MEG for interictal spikes...|$|E
40|$|Ó The Author(s) 2010. This {{article is}} {{published}} with open access at Springerlink. com Abstract MEG interictal spikes as recorded in epilepsy patients are {{a reflection of}} intracranial interictal activity. This study investigates {{the relationship between the}} estimated sources of MEG spikes and the location, distribution and size of interictal spikes in the invasive ECoG of a group of 38 epilepsy patients that are monitored for presurgical evaluation. An amplitude/surface area measure is defined to <b>quantify</b> <b>and</b> <b>rank</b> ECoG spikes. It is found that all MEG spikes are associated with an ECoG spike that is among the three highest ranked in a patient. Among the different brain regions considered, the fronto-orbital, interhemispheric, tempero-lateral and central regions stand out. In an accompanying simulation study it is shown that for hypothesized extended sources of larger sizes, as suggested by the data, source location, orientation and curvature can partly explain the observed sensitivity of MEG for interictal spikes...|$|E
40|$|Many recent {{papers have}} {{documented}} periodicities in returns, return volatility, bid-ask spreads and trading volume, in both equity and foreign exchange markets. We propose and employ a new test for detecting subtle periodicities in time series data {{based on a}} signal coherence function. The technique is applied {{to a set of}} seven half-hourly exchange rate series. Overall, we find the signal coherence to be maximal at the 8 -h and 12 -h frequencies. Retaining only the most coherent frequencies for each series, we implement a trading rule that is based on these observed periodicities. Our results demonstrate in all cases except one that, in gross terms, the rules can generate returns that are considerably greater than those of a buy-and-hold strategy, although they cannot retain their profitability net of transactions costs. We conjecture that this methodology could constitute an important tool for financial market researchers which will enable them to detect, <b>quantify</b> <b>and</b> <b>rank</b> the various periodic components in financial data better. Copyright 2006 Royal Statistical Society. ...|$|E
40|$|Temperature {{conditions}} and climate on Earth {{are controlled by}} the balance between absorbed solar radiation and outgoing terrestrial radiation. The greenhouse effect is a synonym for the trapping of infrared radiation by radiatively active atmospheric constituents. It generally causes a warming of the planet’s surface, compared to the case without atmosphere. Perturbing the radiation balance of the planet, e. g., by anthropogenic greenhouse gas emissions, induces climate change. Individual contributions to a total climate impact are usually <b>quantified</b> <b>and</b> <b>ranked</b> {{in terms of their}} respective radiative forcing. This method involves some limitations, because the effect of the external forcing is modified by radiative feedbacks. Here the current concept of radiative forcing and potential improvements are explained...|$|R
40|$|In {{this paper}} we explore {{the role of the}} river run-off, the tidal regime and the local winds in the {{dynamics}} of the Ria de Arousa estuary during the summer period, using for that purpose the numerical model Delft 3 D. First of all, a simulation under real hydrological and meteorological conditions is conducted in order to validate the ability of the model to reproduce measured salinity and temperature profiles. Subsequently, a series of simulations considering simplified meteorological input data and river discharge conditions are carried out in order to study the sensitivity of stratification conditions in the estuary to the various external forcings of the model. The results are analyzed using potential energy anomaly arguments, which allow <b>quantifying</b> <b>and</b> <b>ranking</b> the contribution of the different processes to the stratification of the estuarine system. Hydraulic EngineeringCivil Engineering and Geoscience...|$|R
40|$|This {{report is}} a {{strategic}} analysis of how an investment management firm can structure its reference data management {{to support a}} “parent-affiliate” corporate structure. This report discusses several approaches that Connor, Clark & Lunn can take with respect to its reference data management processes. Approaches range from centralizing the raw data download to outsourcing the entire reference data management function to a third party vendor. In recommending the best approach for CCL, {{a number of key}} considerations will be discussed. Key considerations mark the important features or characteristics that make a good data management process. In addition, company specific characteristics, or contextual factors, will be used to analyze the overall ‘fit’ of each approach to the company. The final recommendation is arrived at by <b>quantifying</b> <b>and</b> <b>ranking</b> each approach against the key considerations as well as the contextual factors...|$|R
40|$|This paper {{appeared}} in the Journal of Information Warfare, 3 (2), July 2004, 27 - 39. 'Cyberwar' is information warfare directed at the software of information systems. It represents an increasing threat to our militaries and civilian infrastructures. Six principles of military deception are enumerated and applied to cyberwar. Two taxonomies of deception methods for cyberwar are then provided, making both offensive and defensive analogies from deception strategies and tactics in conventional war to this new arena. One taxonomy has {{been published in the}} military literature, and the other is based on case theory in linguistics. The application of both taxonomies to cyberwar is new. We then show how to <b>quantify</b> <b>and</b> <b>rank</b> proposed deceptions for planning using 'suitability' numbers associated with the taxonomies. The paper provides planners for cyberwar with a more comprehensive enumeration than any yet published to the tactics and strategies that they and their enemies may use. Some analogies to deception in conventional warfare hold, but many do not, and careful thought and preparation must be applied to any deception effort...|$|E
40|$|Considerable {{effort has}} been {{expended}} in the UK and elsewhere to <b>quantify</b> <b>and</b> <b>rank</b> PCDD/F primary sources and emissions to the environment, principally the atmosphere, so that cost-effective source reduction measures can be taken. Here, we predict a congener-specific emissions inventory for primary and secondary nondioxin-regulated sources to the UK atmosphere, estimated to have ranged from 3 to 22 kg in 1996. The inventory profile is dominated by OCDD (30 – 40 %), 1, 2, 3, 4, 6, 7, 8 -HpCDD (15 – 19 %) and 1, 2, 3, 4, 6, 7, 8 -HpCDF (14 – 19 %). Congeners 2, 3, 4, 7, 8 -PeCDF and 1, 2, 3, 7, 8 -PeCDD dominate the ΣTEQ composition. Mass balance modelling suggests that the predicted congener pattern in UK air (based on the emission inventory) is similar to observed measurements, with absolute concentrations being estimated within a factor of 2 for most congeners. Calculations taking into account atmospheric weathering processes and long range (advective) transport suggest that PCDD/F sources to ambient air are primarily ongoing and that atmospheric mixing will mask individual emission source profiles/identities. This supports measured evidence for the consistency of PCDD/F air profiles observed around the UK throughout the year...|$|E
40|$|We {{present a}} corpus-based study of musical rhythm, {{based on a}} {{collection}} of 4. 8 million bar-length drum patterns extracted from 48, 176 pieces of symbolic music. Approaches {{to the analysis of}} rhythm in music information retrieval to date have focussed on low-level features for retrieval or on the detection of tempo, beats and drums in audio recordings. Musicological approaches are usually concerned with the description or implementation of manmade music theories. In this paper, we present a quantitative bottom-up approach to the study of rhythm that relies upon well-understood statistical methods from natural language processing. We adapt these methods to our corpus of music, based on the realisation that—unlike words—barlength drum patterns can be systematically decomposed into sub-patterns both in time and by instrument. We show that, in some respects, our rhythm corpus behaves like natural language corpora, particularly in the sparsity of vocabulary. The same methods that detect word collocations allow us to <b>quantify</b> <b>and</b> <b>rank</b> idiomatic combinations of drum patterns. In other respects, our corpus has properties absent from language corpora, in particular, the high amount of repetition and strong mutual information rates between drum instruments. Our findings may be of direct interest to musicians and musicologists, and can inform the design of ground truth corpora and computational models of musical rhythm. 1...|$|E
40|$|The riveted type of {{construction}} is characteristic of older railway bridges. As some of these bridges approach, or have even exceeded their theoretical fatigue life under in-creasing train loads, it is desirable to improve the procedures available {{for the assessment of}} fa-tigue-critical details. The aim {{of this paper is to}} present results through the use of the FE method, which address the manner in which assumed conditions of fixity of bridge riveted joints affect the resulting internal stresses. Different degrees of connection fixity, ranging from fully fixed to partially fixed, are assigned to the connections. A nominal train is traversed over a UK-typical bridge configuration and the resulting stress histories are converted into stress ranges us-ing the rainflow algorithm. By comparing the resulting S-N damage, the extent to which con-nection fixity can affect fatigue life predictions is <b>quantified</b> <b>and</b> <b>ranking</b> of fatigue-critical de-tails is undertaken...|$|R
40|$|This paper {{proposes a}} set of web-based {{indicators}} for <b>quantifying</b> <b>and</b> <b>ranking</b> the relevance of terms related to key-issues in Ecology and Sustainability Science. Search engines that operate in different contexts (e. g. global, social, scientific) are considered as web information carriers (WICs) {{and are able to}} analyse; (i) relevance on different levels: global web, individual/personal sphere, on-line news, and culture/science; (ii) time trends of relevance; (iii) relevance of keywords for environmental governance. For the purposes of this study, several indicators and specific indices (relational indices and dynamic indices) were applied to a test-set of 24 keywords. Outputs consistently show that traditional study topics in environmental sciences such as water and air have remained the most quantitatively relevant keywords, while interest in systemic issues (i. e. ecosystem and landscape) has grown over the last 20 years. Nowadays, the relevance of new concepts such as resilience and ecosystem services is increasing, but the actual ability of these concepts to influence environmental governance needs to be further studied and understood. The proposed approach, which is based on intuitive and easily replicable procedures, can support the decision-making processes related to environmental governance...|$|R
40|$|Systemic risk {{quantification}} {{in the current}} literature is concentrated on market-based methods such as CoVaR(Adrian and Brunnermeier (2016)). Although it is easily implemented, the interactions among the variables of interest and their joint distribution are less addressed. To quantify systemic risk in a system-wide perspective, we propose a network-based factor copula approach to study systemic risk in a network of systemically important financial institutions (SIFIs). The factor copula model {{offers a variety of}} dependencies/tail dependencies conditional on the chosen factor; thus constructing conditional network. Given the network, we identify the most 'connected' SIFI as the central SIFI, and demonstrate that its systemic risk exceeds that of non-central SIFIs. Our identification of central SIFIs shows a coincidence with the bucket approach proposed by the Basel Committee on Banking Supervision, but places more emphasis on modeling the interplay among SIFIs in order to generate systemwide quantifications. The network defined by the tail dependence matrix is preferable to that defined by the Pearson correlation matrix since it confirms that the identified central SIFI through it severely impacts the system. This study contributes to <b>quantifying</b> <b>and</b> <b>ranking</b> the systemic importance of SIFIs...|$|R
40|$|Quantitative Fitness Analysis (QFA) is an {{experimental}} and computational workflow for comparing fitnesses of microbial cultures grown in parallel 1, 2, 3, 4. QFA {{can be applied}} to focused observations of single cultures but is most useful for genome-wide genetic interaction or drug screens investigating up to thousands of independent cultures. The central experimental method is the inoculation of independent, dilute liquid microbial cultures onto solid agar plates which are incubated and regularly photographed. Photographs from each time-point are analyzed, producing quantitative cell density estimates, which are used to construct growth curves, allowing quantitative fitness measures to be derived. Culture fitnesses can be compared to <b>quantify</b> <b>and</b> <b>rank</b> genetic interaction strengths or drug sensitivities. The effect on culture fitness of any treatments added into substrate agar (e. g. small molecules, antibiotics or nutrients) or applied to plates externally (e. g. UV irradiation, temperature) can be quantified by QFA. The QFA workflow produces growth rate estimates analogous to those obtained by spectrophotometric measurement of parallel liquid cultures in 96 -well or 200 -well plate readers. Importantly, QFA has significantly higher throughput compared with such methods. QFA cultures grow on a solid agar surface and are therefore well aerated during growth without the need for stirring or shaking. QFA throughput is not as high as that of some Synthetic Genetic Array (SGA) screening methods 5, 6. However, since QFA cultures are heavil...|$|E
40|$|The {{complexity}} of biological, social and engineering networks makes it desirable to find natural partitions into communities that {{can act as}} simplified descriptions and {{provide insight into the}} structure and function of the overall system. Although community detection methods abound, {{there is a lack of}} consensus on how to <b>quantify</b> <b>and</b> <b>rank</b> the quality of partitions. We show here that the quality of a partition can be measured in terms of its stability, defined in terms of the clustered autocovariance of a Markov process taking place on the graph. Because the stability has an intrinsic dependence on time scales of the graph, it allows us to compare and rank partitions at each time and also to establish the time spans over which partitions are optimal. Hence the Markov time acts effectively as an intrinsic resolution parameter that establishes a hierarchy of increasingly coarser clusterings. Within our framework we can then provide a unifying view of several standard partitioning measures: modularity and normalized cut size can be interpreted as one-step time measures, whereas Fiedler's spectral clustering emerges at long times. We apply our method to characterize the relevance and persistence of partitions over time for constructive and real networks, including hierarchical graphs and social networks. We also obtain reduced descriptions for atomic level protein structures over different time scales. Comment: submitted; updated bibliography from v...|$|E
40|$|Study Design Controlled {{laboratory}} study, cross-sectional. Background The gluteus medius (GMed) and minimus (GMin) provide {{dynamic stability}} of the hip joint and pelvis and are susceptible to atrophy and injury with menopause, ageing and disease. Numerous exercises {{have been reported in}} the literature to elicit high levels of GMed activity, however few have differentiated between the portions of the GMed, and none have examined GMin. Objectives To <b>quantify</b> <b>and</b> <b>rank</b> the level of muscle activity of the 2 segments of GMin (anterior and posterior fibers) and 3 segments of GMed (anterior, middle and posterior fibers) during 4 isometric and 3 dynamic exercises in a group of healthy, post-menopausal women. Methods Intramuscular electrodes were inserted into each segment of GMed and GMin in 10 healthy post-menopausal women. Participants completed 7 gluteal rehabilitation exercises and average normalized muscle activity was used to rank the exercises from highest to lowest. Results The isometric standing hip hitch with contralateral hip swing was the highest ranked exercise for all muscle segments except anterior GMin, where it was ranked 2 nd. The highest ranked dynamic exercise for all muscle segments was the dip test. Conclusion The hip hitch and its variations maximally activate the gluteus medius and minimus muscle segments, and may be useful in hip muscle rehabilitation in post-menopausal women. J Orthop Sports Phys Ther, Epub 15 Oct 2017. doi: 10. 2519 /jospt. 2017. 7229...|$|E
40|$|Overreliance on {{the same}} {{herbicide}} mode of action leads {{to the spread of}} resistant weeds, which cancels the advantages of herbicide-tolerant (HT) crops. Here, the objective was to quantify, with simulations, the impact of glyphosate-resistant (GR) weeds on crop production and weed-related wild biodiversity in HT maize-based cropping systems differing in terms of management practices. We (1) simulated current conventional and probable HT cropping systems in two European regions, Aquitaine and Catalonia, with the weed dynamics model FlorSys; (2) quantified how much the presence of GR weeds contributed to weed impacts on crop production and biodiversity; (3) determined the effect of cultural practices on the impact of GR weeds and (4) identified which species traits most influence weed-impact indicators. The simulation study showed that during the analysed 28 years, the advent of glyphosate resistance had little effect on plant biodiversity. Glyphosate-susceptible populations and species were replaced by GR ones. Including GR weeds only affected functional biodiversity (food offer for birds, bees and carabids) and weed harmfulness when weed effect was initially low; when weed effect was initially high, including GR weeds had little effect. The GR effect also depended on cultural practices, e. g. GR weeds were most detrimental for species equitability when maize was sown late. Species traits most harmful for crop production and most beneficial for biodiversity were identified, using RLQ analyses. None of the species presenting these traits belonged to a family for which glyphosate resistance was reported. An advice table was built; the effects of cultural practices on crop production and biodiversity were synthesized, explained, <b>quantified</b> <b>and</b> <b>ranked,</b> <b>and</b> the optimal choices for each management technique were identified...|$|R
40|$|Abstract Background The {{mechanisms}} of brain injury following intracerebral haemorrhage (ICH) are incompletely understood. Gene expression studies using quantitative real-time RT-PCR following ICH have increased {{our understanding of}} these mechanisms, however the inconsistent results observed {{may be related to}} inappropriate reference gene selection. Reference genes should be stably expressed across different experimental conditions, however, transcript levels of common reference genes have been shown to vary considerably. Reference gene panels have therefore been proposed to overcome this potential confounder. Results The present study evaluated the stability of seven candidate reference genes in the striatum and overlying cortex of collagenase-induced ICH in rodents at survival times of 5 and 24 hours. Transcript levels of the candidate reference genes were <b>quantified</b> <b>and</b> <b>ranked</b> in order of stability using geNorm. When our gene of interest, transient receptor potential melastatin 2 (TRPM 2), was normalised against each reference gene individually, TRPM 2 mRNA levels were highly variable. When normalised to the four most stable reference genes selected for accurate normalisation of data, we found no significant difference between ICH and vehicle rats. Conclusion The panel of reference genes identified in the present study will enable more accurate normalisation of gene expression data in the acute phase of experimental ICH. </p...|$|R
40|$|Background: The {{mechanisms}} of brain injury following intracerebral haemorrhage (ICH) are incompletely understood. Gene expression studies using quantitative real-time RT-PCR following ICH have increased {{our understanding of}} these mechanisms, however the inconsistent results observed {{may be related to}} inappropriate reference gene selection. Reference genes should be stably expressed across different experimental conditions, however, transcript levels of common reference genes have been shown to vary considerably. Reference gene panels have therefore been proposed to overcome this potential confounder. Results: The present study evaluated the stability of seven candidate reference genes in the striatum and overlying cortex of collagenase-induced ICH in rodents at survival times of 5 and 24 hours. Transcript levels of the candidate reference genes were <b>quantified</b> <b>and</b> <b>ranked</b> in order of stability using geNorm. When our gene of interest, transient receptor potential melastatin 2 (TRPM 2), was normalised against each reference gene individually, TRPM 2 mRNA levels were highly variable. When normalised to the four most stable reference genes selected for accurate normalisation of data, we found no significant difference between ICH and vehicle rats. Conclusion: The panel of reference genes identified in the present study will enable more accurate normalisation of gene expression data in the acute phase of experimental ICH. Naomi L Cook, Timothy J Kleinig, Corinna van den Heuvel and Robert Vin...|$|R
40|$|Today, {{organizations}} are generating {{large volumes of}} data. However, the challenge of extracting valuable information from the data has been a large and long-standing problem. Here, we {{address the problem of}} quantifying risks and detecting fraud in heterogeneous financial big data. Great financial losses are pressuring institutions to devise innovative solutions for risk and fraud detection. Current approaches in government suffer from issues such as high false positive rates and low adaptability to the continuous evolution of newer fraud. In this thesis, we propose an open and extensible framework called "Situational Awarness FrAamework for RIsk ranking" (SAFARI). SAFARI aims to <b>quantify</b> <b>and</b> <b>rank</b> risk with unlabeled, complex data in the financial world. The framework integrates and analyzes different perspectives of financial data, and extends risk scores for decision makers. SAFARI also utilizes machine learning techniques to learn from examined cases to improve the calculation of risks and adapt to the changing behavior of fraudulent activities. The work includes designing, implementing, testing, extending and evaluating the proposed framework. In the overpayment detection scenario, results show SAFARI can effectively find overpayments with low false positive rates. Furthermore, SAFARI can be extended to assist decision making in a variety of environment thanks to its general applicability. by Rongsha Li. Thesis: S. M., Massachusetts Institute of Technology, School of Engineering, Center for Computational Engineering, Computation for Design and Optimization Program, 2015. Cataloged from PDF version of thesis. Includes bibliographical references (pages 75 - 76) ...|$|E
40|$|ABSTRACT: In {{the scope}} of a scenario-based risk analysis, this study aims to <b>quantify</b> <b>and</b> <b>rank</b> various types of epistemic {{uncertainties}} that enter into the derivation of fragility functions for common buildings. Using a numerical model of a test structure (a reinforced concrete five-story building with infill panels {{on the first two}} floors), a first type of uncertainty is introduced, consisting of the mechanical properties of the materials (i. e. Young’s modulus and compressive strength for concrete, and Young’s modulus and yield strength for steel). The area of longitudinal reinforcement is also modified in the model, to generate various damage mechanisms for the same structure, depending on which floor first experiences failure. Finally, another source of epistemic uncertainty is studied, by comparing different types of fragility models: fragility curves derived from dynamic analyses and fragility functions generated from a capacity spectrum approach (i. e. use of a set of natural response spectra to identify a series of performance points from the capacity curve). To this end, a ranking of the importance of different sources of uncertainty in the vulnerability analysis (i. e. mechanical properties, structural models and fragility models) is conducted by computing, for each uncertainty source, the Sobol ’ indices (i. e. the main effects and total effects of each source of uncertainty). This variancebased sensitivity technique presents the appealing features of both exploring the influence of input parameters over their whole range of variation and fully accounting for possible interactions between them. Nevertheless...|$|E
40|$|Sustainable {{management}} of tropical forests {{has been identified}} as one of the main objectives for global conservation and {{management of}} carbon stocks. Toward this goal, managers need tools to determine whether current management practices are sustainable. Several international initiatives have been undertaken for the development of criteria and indicators to aid managers in moving toward sustainable practices. Despite these efforts, {{the question of how to}} apply and assess indicators remains to be answered from an operational, field-based perspective. Field surveys are expensive and time-consuming when management areas are large and in the face of logistical constraints. Thus, there is a need for an approach to prioritization. We sought to determine whether satellite imagery can be used, in conjunction with standard forest management data, to identify and rank priority areas for field surveys of bioindicators. The study area in Costa Rica, in forest areas managed by the Fundacion para el Desarrollo de la Cordillera Volcanica Central (FUNDECOR), was imaged by the Landsat- 5 Thematic Mapper in 1986 and 2001. Through spatial statistical analysis applied to the wide dynamic range vegetation index, we were able to <b>quantify</b> <b>and</b> <b>rank</b> changes in canopy spatial structure. The resulting categories can be used by forest managers to identify which areas are in need of field surveys. More generally, we show how to generate a moving baseline for change analysis and evaluate for significant deviations in spatial structure...|$|E
40|$|The {{red-backed shrike}} (Lanius collurio L.) is a bird living in human-altered {{agricultural}} {{areas that are}} managed by extensive farming techniques. This passerine species has declined significantly in Western Europe over the last 30 - 40 years. The development of efficient species-specific conservation strategies relies on fine-grained information about the ecological resources and environmental conditions that constitute its reproductive habitat in this agricultural landscape. Species distribution models are used increasingly in conservation biology to provide such information. Most studies investigate the environmental pattern of species distribution, assuming that species records are reliable indicators of habitat suitability. However, ecological theory on source-sink dynamics and ecological traps points out that some individuals may be located outside the environmental bounds of their species' reproductive niche. Those individuals could reduce model accuracy and limit model utility. Parameters related to the reproductive success of this shrike in Southern Belgium were integrated into a fine-scale presence-only modelling framework to demonstrate this problem and to address critical habitat requirements of this species relative to conservation management. Integrating reproductive parameters into the modelling framework showed that individuals occurred, but did not reproduce successfully, above a certain environmental threshold. This indicated that the reproductive niche of the shrike is ecologically narrower than standard practice in species distribution modelling would suggest. The major resources (nest sites availability, distance to human settlements, suitable perching sites, foraging areas and insect abundance) required for the reproduction of the red-backed shrike were <b>quantified</b> <b>and</b> <b>ranked</b> to offer concrete species -specific conservation management guidelines. (c) 2007 Elsevier Ltd. All right...|$|R
40|$|The {{modification}} of proteins by reducing sugars {{is a process}} that occurs naturally in the body. This process, which is known as glycation, has been linked to many of the chronic complications encountered during diabetes. Glycation has also been linked to changes in the binding of human serum albumin (HSA) to several drugs and small solutes in the body. While these effects are known, there is little information that explains why these changes in binding occur. The goal of this project was to obtain qualitative and quantitative information about glycation that occurs on HSA. ^ The first section of this dissertation examined methods {{that could be used to}} <b>quantify</b> <b>and</b> identify glycation that occurs on HSA. The extent of glycation that occurred on HSA was quantified using 18 O-labeling mass spectrometry and the glycation sites were identified by observing the mass-to-charge (m/z) shifts that occurred in glycated HSA. This initial investigation revealed that 18 O-labeling based quantitation can be improved over previous methods if a relative comparison is done with 18 O-labeled peptides in a control HSA sample. Similarly, the process of making m/z shift-based assignments could be improved if only the peptides that were unique to the glycated HSA samples were used with internal calibration. ^ These techniques were used in subsequent chapters for the assignment of early and late-stage glycation products on HSA. The regions on HSA that contained the highest amount of modification were identified, <b>quantified,</b> <b>and</b> <b>ranked</b> in order of their relative abundance. Of the commonly reported glycation sites, the N-terminus was found to have the highest extent of modification, followed by lysines 525, 199, and 439. The relative amount of modification on lysine 281, with respect to the aforementioned residues, varied with different degrees of glycation. The 18 O approach used for this analysis was novel because it allowed for the simultaneous quantification of all glycation-related modifications that were occurring on HSA. As such, several arginine residues were also found to have high amounts of modification on glycated HSA. ...|$|R
50|$|A {{vulnerability}} assessment {{is the process}} of identifying, <b>quantifying,</b> <b>and</b> prioritizing (or <b>ranking)</b> the vulnerabilities in a system. Examples of systems for which {{vulnerability assessment}}s are performed include, but are not limited to, information technology systems, energy supply systems, water supply systems, transportation systems, and communication systems. Such assessments may be conducted on behalf of a range of different organizations, from small businesses up to large regional infrastructures. Vulnerability from the perspective of disaster management means assessing the threats from potential hazards to the population and to infrastructure.It may be conducted in the political, social, economic or environmental fields.|$|R
40|$|The {{presence}} of uncertainties are inevitable in engineering design and analysis, where failure in understanding their effects {{might lead to}} the structural or functional failure of the systems. The role of global sensitivity analysis in this aspect is to <b>quantify</b> <b>and</b> <b>rank</b> the effects of input random variables and their combinations to the variance of the random output. In problems where the use of expensive computer simulations are required, metamodels are widely used {{to speed up the}} process of global sensitivity analysis. In this paper, a multi-fidelity framework for global sensitivity analysis using polynomial chaos expansion (PCE) is presented. The goal is to accelerate the computation of Sobol sensitivity indices when the deterministic simulation is expensive and simulations with multiple levels of fidelity are available. This is especially useful in cases where a partial differential equation solver computer code is utilized to solve engineering problems. The multi-fidelity PCE is constructed by combining the low-fidelity and correction PCE. Following this step, the Sobol indices are computed using this combined PCE. The PCE coefficients for both low-fidelity and correction PCE are computed with spectral projection technique and sparse grid integration. In order to demonstrate the capability of the proposed method for sensitivity analysis, several simulations are conducted. On the aerodynamic example, the multi-fidelity approach is able to obtain an accurate value of Sobol indices with 36. 66 % computational cost compared to the standard single-fidelity PCE for a nearly similar accuracy...|$|E
40|$|Vibro-acoustic Transfer Path Analysis (TPA) {{is a tool}} to {{evaluate}} the contribution of different energy propagation paths between a source and a receiver, linked to each other {{by a number of}} connections. TPA is typically used to <b>quantify</b> <b>and</b> <b>rank</b> the relative importance of these paths in a given frequency band, determining the most significant one to the receiver. Basically, two quantities have to be determined for TPA: the operational forces at each transfer path and the Frequency Response Functions (FRF) of these paths. The FRF are obtained either experimentally or analytically, and the influence of the mechanical impedance of the source can be taken into account or not. The operational forces can be directly obtained from measurements using force transducers or indirectly estimated from auxiliary response measurements. Two methods to obtain the operational forces indirectly – the Complex Stiffness Method (CSM) and the Matrix Inversion Method (MIM) – associated with two possible configurations to determine the FRF – including and excluding the source impedance – are presented and discussed in this paper. The effect of weak and strong coupling among the paths is also commented considering the techniques previously presented. The main conclusion is that, with the source removed, CSM gives more accurate results. On the other hand, with the source present, MIM is preferable. In the latter case, CSM should be used only if there is a high impedance mismatch between the source and the receiver. Both methods are not affected by a higher or lower degree of coupling among the transfer paths...|$|E
40|$|It is {{expected}} that biodiesel production in the EU will remain the dominant contributor {{as part of a}} 10 % minimum binding target for biofuel in transportation fuel by 2020 within the 20 % renewable energy target in the overall EU energy mix. Life cycle assessments (LCA) of biodiesel to evaluate its environmental impacts have, however, remained questionable, mainly because of the adoption of a traditional process analysis approach resulting in system boundary truncation and because of issues regarding the impacts of land use change and N 2 O emissions from fertiliser application. In this study, a hybrid LCA methodology is used to evaluate the life cycle CO 2 equivalent emissions of rape methyl ester (RME) biodiesel. The methodology uses input-output analysis to estimate upstream indirect emissions in order to complement traditional process LCA in a hybrid framework. It was estimated that traditional LCA accounted for 2. 7 kg CO 2 -eq per kg of RME or 36. 6 % of total life cycle emissions of 28 the RME supply chin. Further to the inclusion of upstream indirect impacts in the LCA system (which accounted for 23 % of the total life cycle emissions), emissions due to direct land use change (6 %) and indirect land use change (16. 5 %) and N 2 O emissions from fertiliser applications (17. 9 %) were also calculated. Structural path analysis is used to decompose upstream indirect emissions paths of the biodiesel supply chain in order to identify, <b>quantify</b> <b>and</b> <b>rank</b> high carbon emissions paths or ‘hot-spots’ in the biodiesel supply chain. It was shown, for instance, that inputs from the ‘Other Chemical Products’ sector (identified as phosphoric acid, H 3 PO 4) into the biodiesel production process represented the highest carbon emission path (or hot-spot) with 5. 35 % of total upstream indirect emissions of the RME biodiesel supply chain...|$|E
40|$|Proceedings of the 2003 Georgia Water Resources Conference, held April 23 - 24, 2003, at the University of Georgia. Stream {{assessments}} and restoration projects {{are becoming increasingly}} important to meet water quality standards and protect biological health. Developing an assessment methodology that is both effective and efficient is the first crucial step in conducting a stream inventory to identify watershed based problems and develop restoration designs to improve stream system health. ENTRIX has worked on several projects involving stream assessment and restoration design and is continually improving {{on the type of}} data collected, how the data is collected, and how project benefits are quantified. This approach includes the following steps: 1) Field investigation to identify and assess watershed conditions 2) Data summary to prioritize <b>and</b> <b>rank</b> stream reaches {{based on the results of}} the field assessment 3) Develop a list of potential improvement projects 4) <b>Quantify</b> benefits <b>and</b> <b>rank</b> projects based on costs and benefits. This presentation will focus on recent improvements in the methods used for stream assessment and restoration design projects...|$|R
40|$|Abstract—Despite {{years of}} {{research}} yielding systems and guidelines to aid visualization design, practitioners still {{face the challenge of}} identifying the best visualization for a given dataset and task. One promising approach to circumvent this problem is to leverage perceptual laws to quantitatively evaluate the effectiveness of a visualization design. Following previously established methodologies, we conduct a large scale (n= 1687) crowdsourced experiment to investigate whether the perception of correlation in nine commonly used visualizations can be modeled using Weber’s law. The results of this experiment contribute to our understanding of information visualization by establishing that: 1) for all tested visualizations, the precision of correlation judgment could be modeled by Weber’s law, 2) correlation judgment precision showed striking variation between negatively and positively correlated data, and 3) Weber models provide a concise means to <b>quantify,</b> compare, <b>and</b> <b>rank</b> the perceptual precision afforded by a visualization. Index Terms—Perception, Visualization, Evaluation. ...|$|R
30|$|Various {{approaches}} {{have been reported}} in assessing the nature of vulnerability of pastoral and agro-pastoral communities to climatic stress and other shocks (O’Brien et al. 2004; Wisner et al. 2004; ActionAid International 2005; Smit and Wandel 2006; Freeman et al. 2008) and how it is distributed socially (gender, age, wealth) and geographically (resource access, access to markets and climate). Vulnerability assessment can be quantitative, qualitative, or a combination of both, depending on the objectives, disciplinary orientation, and scale of analysis, and may entail identifying, documenting, <b>quantifying,</b> <b>and</b> <b>ranking</b> the vulnerabilities to different stresses or hazards in a community or a system (Freeman et al. 2008). Quantitative vulnerability assessment uses measurable characteristics or indicators to establish scores or indices to represent the degree of vulnerability of a system, community, region, or country. For example, Vulnerability Analysis and Mapping (VAM) of food (in)security was developed by the World Food Programme (WFP) and has been used for vulnerability analysis for Niger (WFP 2005). VAM uses a range of monitoring indicators including satellite imagery of rainfall and crops and food prices in local markets to establish the vulnerability of regions in a country to food deficit (WFP 2005). The weakness of VAM is that the indicators used are developed by ‘experts’ with little or no input from the affected communities. Similar approach was used by Freeman et al. (2008) to assess the vulnerability of livestock-based communities in Lesotho, Malawi, and Zambia to economic- or climate-induced shocks. Through this approach, different geographic locations and their varying degrees of food deficit were identified and characterized into ‘hotspot’ and ‘non-hotspot’ of food aid. Deressa et al. (2008) and Opiyo et al. (2014) also used vulnerability indices based on a set of indicators from the literatures to measure household’s vulnerability to climate-induced stresses by farmers and pastoralists in Ethiopia and Kenya, respectively.|$|R
