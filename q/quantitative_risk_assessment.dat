656|10000|Public
5000|$|... iQRAS (ITEMSOFT <b>Quantitative</b> <b>Risk</b> <b>Assessment</b> System) from ItemSoft ...|$|E
5000|$|... #Subtitle level 3: Criticisms of <b>quantitative</b> <b>risk</b> <b>assessment</b> ...|$|E
5000|$|<b>Quantitative</b> <b>risk</b> <b>assessment</b> of {{combined}} {{exposure to}} food contaminants and natural toxins, ...|$|E
50|$|In {{financial}} terms, <b>quantitative</b> <b>risk</b> <b>assessments</b> {{include a}} {{calculation of the}} single loss expectancy of monetary value of an asset.|$|R
5000|$|... #Subtitle level 3: <b>Quantitative</b> {{microbiological}} <b>risk</b> <b>assessment</b> ...|$|R
40|$|It is {{important}} to be able to quantify risks in order to determine LNG behaviour and to ensure its containment. This facilitates comprehensive safety management. <b>Quantitative</b> <b>risk</b> <b>assessments</b> are necessary to enable the selection of preventive and protective measures that suit the situation. Every situation demands a different measure in terms of protection and comfort...|$|R
5000|$|<b>Quantitative</b> <b>risk</b> <b>assessment</b> (QRA) {{software}} and methodologies give [...] estimates of risks, given the parameters defining them. They {{are used in}} the financial sector, the chemical process industry, and other areas.|$|E
5000|$|The primary {{shortcoming}} of {{the technique}} is that, from a Probability Risk assessment (PRA) stance, there is no HEP produced. As a result, {{the ease with which}} this analysis can be fit into a predictive <b>quantitative</b> <b>risk</b> <b>assessment</b> is reduced.|$|E
50|$|Singapore enforces strict {{pollution}} control measures on local companies and factories. They {{are required to}} comply with regulations pertaining to air pollutants, effluent discharge and noise pollution. <b>Quantitative</b> <b>risk</b> <b>assessment</b> studies and extensive plans relating to management and disposal must also be done on hazardous and toxic chemicals.|$|E
5000|$|<b>Quantitative</b> {{microbiological}} <b>risk</b> <b>assessment</b> [...] (QMRA) is {{the process}} of estimating the risk from exposure to microorganisms.|$|R
40|$|Impact at {{management}} level: Qualitative <b>assessment</b> of <b>risk</b> criticality {{in conjunction}} with risk consequence, likelihood, and severity enable development of an "investment policy" towards managing a portfolio of risks. Impact at research level: <b>Quantitative</b> <b>risk</b> <b>assessments</b> enable researchers to develop risk mitigation strategies with meaningful <b>risk</b> reduction results. <b>Quantitative</b> <b>assessment</b> approach provides useful risk mitigation information...|$|R
40|$|The risk of {{antibiotic}} use in food animals for purposes other that disease treatment is receiving renewed scrutiny. In the U. S. {{the question is}} being addressed with qualitative and <b>quantitative</b> <b>risk</b> <b>assessments,</b> in Scandinavia it has been addressed with prohibitions on the uses labeled as growth promotion (Cox and Popken, 2004, Hurd, et al., 2004, US, FDA, 2002) ...|$|R
50|$|In <b>quantitative</b> <b>risk</b> <b>assessment</b> an annualized loss {{expectancy}} (ALE) {{may be used}} to justify the cost of implementing countermeasures to protect an asset. This may be calculated by multiplying the single {{loss expectancy}} (SLE), which is the loss of value based on a single security incident, with the annualized rate of occurrence (ARO), which is an estimate of how often a threat would be successful in exploiting a vulnerability.|$|E
50|$|Risk {{assessment}} is {{the determination of}} an estimate of risk related to a well-defined situation and a recognized set of hazards. <b>Quantitative</b> <b>risk</b> <b>assessment</b> requires calculations of two components of risk : {{the magnitude of the}} potential loss, and the probability that the loss will occur. An acceptable risk is a risk that is understood and tolerated, usually because the cost or difficulty of implementing an effective countermeasure for the associated vulnerability exceeds the expectation of loss.|$|E
5000|$|The FDA was {{the first}} agency to have to confront this problem, {{with respect to the}} use of {{diethylstilbestrol}} to promote the growth of livestock used in meat production, which remained present in the meat. It addressed the issue by using <b>quantitative</b> <b>risk</b> <b>assessment,</b> declaring that if a carcinogenic food additive was present at a concentration of less than 1 part in 1,000,000, the risk was negligible. This standard became known as the [...] "de minimis" [...] exception to the Delaney Rule and was used throughout the FDA and other agencies.|$|E
40|$|<b>Quantitative</b> <b>risk</b> <b>assessments</b> are an {{integral}} part of risk-informed regulation of current and future nuclear plants in the U. S. The Bayesian approach to uncertainty, in which both stochastic and epistemic uncertainties are represented with precise probability distributions, is the standard approach to modeling uncertainties in such <b>quantitative</b> <b>risk</b> <b>assessments.</b> However, there are long-standing criticisms of the Bayesian approach to epistemic uncertainty from many perspectives, and a number of alternative approaches have been proposed. Among these alternatives, the most promising (and most rapidly developing) would appear to be the concept of imprecise probability. In this paper, we employ a performance indicator example to focus the discussion. We first give a short overview of the traditional Bayesian paradigm and review some its controversial aspects, for example, issues with so-called noninformative prior distributions. We then discuss how the imprecise probability approach treats these issues and compare it with two other approaches: sensitivity analysis and hierarchical Bayes modeling. We conclude with some practical implications for risk-informed decision making...|$|R
40|$|Best {{practice}} {{dictates that}} security requirements {{be based on}} risk assessments; however, simplistic <b>risk</b> <b>assessments</b> that result in lists of risks or sets of scenarios do not provide sufficient information to prioritize requirements when faced with resource constraints (e. g., time, money). Multi-attribute <b>risk</b> <b>assessments</b> provide a convenient framework for systematically developing <b>quantitative</b> <b>risk</b> <b>assessments</b> that the security manager can use to prioritize security requirements. This paper presents a multi-attribute <b>risk</b> <b>assessment</b> process and results from two industry case studies that used the process to identify and prioritize their risks. 1...|$|R
50|$|<b>Quantitative</b> {{microbiological}} <b>risk</b> <b>assessments</b> (QMRAs) combine pathogen {{concentrations in}} water with dose-response relationships and data reflecting potential exposure {{to estimate the}} risk of infection.|$|R
5000|$|Risk {{assessment}} is {{the determination of}} quantitative or qualitative estimate of risk related to a well-defined situation and a recognized threat (also called hazard). <b>Quantitative</b> <b>risk</b> <b>assessment</b> requires calculations of two components of risk (R): {{the magnitude of the}} potential loss (L), and the probability (p) that the loss will occur. An acceptable risk is a risk that is understood and tolerated usually because the cost or difficulty of implementing an effective countermeasure for the associated vulnerability exceeds the expectation of loss. [...] "Health risk assessment" [...] includes variations, such as the type and severity of response, with or without a probabilistic context.|$|E
5000|$|In {{an effort}} to be more fair and to avoid adding to already high {{imprisonment}} rates in the US, courts across America have started using <b>quantitative</b> <b>risk</b> <b>assessment</b> software when trying to make decisions about releasing people on bail and sentencing, which are based on their history and other attributes. [...] It analyzed recidivism risk scores calculated by one of the most commonly used tools, the Northpointe COMPAS system, and looked at outcomes over two years, and found that only 61% of those deemed high risk actually committed additional crimes during that period and that African-American defendants were far more likely to be given high scores that white defendants.|$|E
5000|$|Suresh H. Moolgavkar M.D., Ph.D. (born January 3, 1943 in Bombay, India) is a {{mathematician}} and epidemiologist at the University of Washington and the Fred Hutchinson Cancer Research Center in Seattle. Among his many scientific contributions {{is the development}} of the two-stage clonal expansion (TSCE) model of carcinogenesis, also known as the Moolgavkar-Venzon-Knudson (MVK) model, a stochastic cell-level description of carcinogenesis based on Alfred G. Knudson’s two-hit hypothesis. In its original development the TSCE model [...] represents tumor initiation as the first hit, followed by cell proliferation (clonal expansion) and malignant transformation as the second hit. It has been interpreted as describing the initiation-promotion-progression sequence observed in chemical carcinogenesis and has been applied widely for the analysis of both experimental and epidemiological data for purposes of <b>quantitative</b> <b>risk</b> <b>assessment.</b>|$|E
30|$|<b>Quantitative</b> <b>risk</b> <b>assessments</b> are {{becoming}} common practice for projects with high {{risks associated with}} them. A critical step in the <b>risk</b> <b>assessment</b> process is the adoption of risk evaluation criteria. Because of the diverse contexts for which previously defined criteria were proposed, different regions should derive their own criterion or perform {{an assessment of the}} applicability of any criteria to be adopted. As such, development of these criteria becomes necessary at a regional, industry, client, and even case specific scales.|$|R
40|$|Salmonella and Campylobacter are {{estimated}} to cause 3. 9 million illnesses annually in the United States, {{and most of these}} illnesses are food-related. Pigs can be sub-clinically infected with these pathogens and fecal contamination of meat during processing is a food safety <b>risk.</b> <b>Quantitative</b> measures of foodborne safety risk are rarely reported and are a critical data gap for development of <b>quantitative</b> <b>risk</b> <b>assessments.</b> The goal {{of this study was to}} determine the association between the concentration of Salmonella and Campylobacter in porcine feces and hide with concentrations in meat...|$|R
40|$|Those who prepare <b>quantitative</b> <b>risk</b> <b>assessments</b> do {{not always}} {{appreciate}} that those assessments might be used as evidence in civil litigation. This paper suggests that litigation attorneys, judges, and juries be regarded as audiences to whom {{the information in the}} <b>risk</b> <b>assessment</b> must be communicated. The way that a <b>risk</b> <b>assessment</b> is prepared can affect significantly whether litigation is brought at all, the resolution of evidentiary motions involving the <b>risk</b> <b>assessment,</b> as well as the ultimate outcome of the litigation. This paper discusses certain procedural and evidentiary aspects of the civil litigation process in the hope that a better understanding of that process might lead to the preparation of <b>risk</b> <b>assessments</b> that are more adequately understood by juries, judges, and litigants...|$|R
5000|$|In {{an effort}} to be more fair and to avoid adding to already high {{imprisonment}} rates in the US, courts across America have started using <b>quantitative</b> <b>risk</b> <b>assessment</b> software when trying to make decisions about releasing people on bail and sentencing, which are based on their history and other attributes. [...] It analyzed recidivism risk scores calculated by one of the most commonly used tools, the Northpointe COMPAS system, and looked at outcomes over two years, and found that only 61% of those deemed high risk actually committed additional crimes during that period and that African-American defendants were far more likely to be given high scores that white defendants. [...] These results are part of larger questions being raised in the field of machine ethics with regard to the risks of perpetuating patterns of discrimination via the use of big data and machine learning across many fields.|$|E
50|$|Purely <b>quantitative</b> <b>risk</b> <b>assessment</b> is a {{mathematical}} calculation based on security metrics on the asset (system or application).For each risk scenario, {{taking into consideration}} the different risk factors a Single loss expectancy (SLE) is determined. Then, considering the probability of occurrence on a given period basis, for example the annual rate of occurrence (ARO), the Annualized Loss Expectancy is determined {{as the product of}} ARO X SLE.It is {{important to point out that}} the values of assets to be considered are those of all involved assets, not only the value of the directly affected resource.For example, if you consider the risk scenario of a Laptop theft threat, you should consider the value of the data (a related asset) contained in the computer and the reputation and liability of the company (other assets) deriving from the lost of availability and confidentiality of the data that could be involved.It is easy to understand that intangible assets (data, reputation, liability) can be worth much more than physical resources at risk (the laptop hardware in the example).Intangible asset value can be huge, but is not easy to evaluate: this can be a consideration against a pure quantitative approach.|$|E
5000|$|In 1988 he {{published}} an article entitled [...] "The De Minimis Interpretation of the Delaney Clause: Legal and Policy Rationale" [...] in the Journal of the American College of Toxicology (now called the International Journal of Toxicology), which he had previously presented in December 1986 at a symposium on Topics in Risk Analysis, sponsored by International Life Sciences Institute Risk Science Institute, Society for Risk Analysis, and Brookings Institution. [...] The paper was delivered and published during {{the midst of a}} debate and litigation over federal agencies' interpretation of the Delaney clause, a part of a 1958 federal law that prohibits any carcinogenic chemical from being added, in any amount, to food that is processed. As analytical instrumentation increased in power and more agents were found to be carcinogenic at very low levels, the agencies had developed a <b>quantitative</b> <b>risk</b> <b>assessment</b> approach to interpreting the Delaney Clause, which stated that if a carcinogen was present at levels less than 1 in 1,000,000 parts (1ppm), the risk of that carcinogen was [...] "de minimis", and it could be allowed on the market. In his article, Taylor presented arguments in favor of this approach. Advocates in favor of organic food have criticized Taylor for taking this stance and have attributed the stance not to a good faith effort to reasonably regulate, but to an alleged desire to benefit Monsanto financially.|$|E
40|$|Multiple {{software}} products often {{exist on}} the same server and therefore vulnerability in one product might compromise the entire system. It is imperative to perform a security <b>risk</b> <b>assessment</b> during {{the selection of the}} candidate software products that become part of a larger system. Having a <b>quantitative</b> security <b>risk</b> <b>assessment</b> model provides an objective criterion for such assessment and comparison between candidate software systems. In this paper, we present a software product evaluation method using such a <b>quantitative</b> security <b>risk</b> <b>assessment</b> model. This method utilizes prior research in <b>quantitative</b> security <b>risk</b> <b>assessment,</b> which is based on empirical data from the National Vulnerability Database (NVD), and compares the security risk levels of the products evaluated. We introduced topic modeling to build a security <b>risk</b> <b>assessment</b> model. The <b>risk</b> model is created using Latent Dirichlet Allocation (LDA) to classify the vulnerabilities into topics, which are then used as the measurement instruments to evaluate the candidate software product. Such a procedure could supplement the existing selection process, to assist the decision makers to evaluate open-source software (OSS) systems, to ensure that it is safe and secure enough to be put into their environment. Finally, the procedure is demonstrated using an experimental case study...|$|R
40|$|International audienceThis paper {{discusses}} {{an approach}} for treating model uncertainties {{in relation to}} <b>quantitative</b> <b>risk</b> <b>assessments.</b> The analysis {{is based on a}} conceptual framework where a distinction is made between model error–the difference between the model prediction and the true future quantity–and model output uncertainty–the (epistemic) uncertainty about the magnitude of this error. The aim of the paper is to provide further clarifications and explanations of important issues related to the understanding and implementation of the approach, using a detailed study of a Poisson model case as an illustration. Special focus is on the way the uncertainties are assessed...|$|R
40|$|The {{roles that}} {{waterfowl}} in general, and Canada geese in particular, {{have in the}} dissemination and transmission of viral and bacterial diseases of human or agricultural importance are covered in this review. In addition to the biological information about the etiology of the disease, economic impacts and zoonotic potential of viral and bacterial pathogens are considered. In most cases existing evidence suggests the importance of waterfowl in disease dissemination and transmission, however, definitive data are often lacking, indicating {{the need for more}} directed studies before <b>quantitative</b> <b>risk</b> <b>assessments</b> can be made. Finally, a brief assessment of management options is considered...|$|R
50|$|Fenton {{currently}} {{works on}} <b>quantitative</b> <b>risk</b> <b>assessment.</b> This typically involves analysing and predicting the probabilities of unknown events using Bayesian statistical methods including especially causal, probabilistic models (Bayesian networks). This type of reasoning enables improved assessment by taking account of both statistical data and also expert judgment. In April 2014 Fenton was awarded {{one of the}} prestigious European Research Council Advanced Grants to focus on these issues. Fenton's experience in risk assessment covers {{a wide range of}} application domains such as legal reasoning (he has been an expert witness in major criminal and civil cases), medical analytics, vehicle reliability, embedded software, transport systems, financial services, and football prediction. Fenton has a special interest in raising public awareness of the importance of probability theory and Bayesian reasoning in everyday life (including how to present such reasoning in simple lay terms) and he maintains a website dedicated to this and also a blog focusing on probability and the law. In March 2015 Norman presented the BBC documentary Climate Change by Numbers. Fenton has published 7 books and 230 referred articles and has provided consulting to many major companies world-wide. His 2012 book was the first to bring Bayesian networks to a general audience. Fenton's current projects are focused on using Bayesian methods for improved legal reasoning and improved medical decision making. Since June 2011 he has led an international consortium (Bayes and the Law) of statisticians, lawyers and forensic scientists working to improve the use of statistics in court. In 2016, he is leading a prestigious 6-month Programme on Probability and Statistics in Forensic Science at the Isaac Newton Institute for Mathematical Sciences, University of Cambridge. In addition to his research on risk assessment, Fenton is renowned for his work in software engineering (including pioneering work on software metrics); the third edition of his book “Software Metrics: A Rigorous and Practical Approach” was published in November 2014. The book {{is one of the most}} cited in software engineering (5040 citations, Google Scholar, Feb 2016).|$|E
40|$|<b>Quantitative</b> <b>risk</b> <b>assessment</b> and {{reliability}} is most famous approach used for precise {{the values of}} possible outcomes of initiating event. The Event Tree Analysis (ETA) is a graphical logic model for assessing probability of an accident, ETA is either a preincident or a post incident application. In this paper new model proposed for reliability <b>quantitative</b> <b>risk</b> <b>assessment</b> using event tree analysis with a fuzzy sets approach for solve problem of uncertainty and imprecise of outcomes and risk in the chemical industry. The results got by new model is more precise and more powerful to deal with uncertainty in results and helpful for reliability <b>quantitative</b> <b>risk</b> <b>assessment...</b>|$|E
40|$|During {{the hazard}} {{analysis}} {{as part of}} the development of a HACCP-system, first the hazards (contaminants) have to be identified and then the risks have to be assessed. Often, this assessment is restricted to a qualitative analysis. By using elements of <b>quantitative</b> <b>risk</b> <b>assessment</b> (QRA) the hazard analysis can be transformed into a more meaningful managerial tool. In this way the effect of control measures can be quantified, so the occurrence of contaminants in the endproducts can be estimated. Also, the <b>quantitative</b> <b>risk</b> <b>assessment</b> is a tool to derive or validate control measures and critical limits at process steps (CCPs). The practical use of <b>quantitative</b> <b>risk</b> <b>assessment</b> is demonstrated by two examples: the risk of raw fermented sausages and the risk of a pressurized meat product. It can be concluded that <b>quantitative</b> <b>risk</b> <b>assessment</b> is a powerful combination of food microbiology, modelling and applied statistics. It is recommended as the input for managing food safety issues as an extension or validation of the HACCP-system. © 2001 Elsevier Science Ltd...|$|E
40|$|<b>Quantitative</b> <b>risk</b> <b>assessments</b> {{estimate}} {{the probability of}} unwanted events occurring and stochastic modelling can incorporate real-life uncertainty and variability into these estimates. There is now a focus on whether these techniques can be applied successfully to {{the risks associated with}} food-borne microbiological hazards. With microbiological food-risk assessments, in order to assess the risk to human health, it is not only necessary to {{estimate the}} probability of the organisms being present at each stage of the food production pathway, but also to estimate the burden of organisms present at each stage. We are currently undertaking a <b>risk</b> <b>assessment</b> of the <b>risks</b> to human health consequent upon the presence of campylobacters in on-farm poultry. This paper will examine the initial model framework and the methodological issues arising from the complexity of the <b>risk</b> <b>assessment</b> pathway...|$|R
40|$|Subjective risk {{perceptions}} are often encoded as responses to 0 - 1 questions in surveys or other qualitative risk scales. However, reference points for assessing an activity as risky are confounded by various {{characteristics of the}} respondents. This paper uses a sample of workers for whom <b>quantitative</b> <b>risk</b> <b>assessments</b> as well as dichotomous risk perception responses are available. It is shown that, given a <b>quantitative</b> <b>risk</b> measure, the thresholds for assessing an activity as "risky" vary systematically, particularly by education. The differences in such thresholds across worker groups are estimated. The resulting implications of using qualitative risk variables for assessing wage-risk tradeoffs are estimated, yielding results that are also relevant for many other areas involving similar qualitative variables. Risk Perceptions, Risk Thresholds, Risk Premiums, Dichotomous Variables...|$|R
40|$|The {{applications}} of systems biology approaches have greatly {{increased in the}} past decade largely {{as a consequence of the}} human genome project and technological advances in genomics and proteomics. Systems approaches have been used in the medical and pharmaceutical realm for diagnostic purposes and target identification. During this same period, the use of mode of action (MOA) for <b>risk</b> <b>assessment</b> has been increasing and there is a need for <b>quantitative</b> <b>risk</b> <b>assessments</b> on an ever-growing number of environmental chemicals. Genome-wide (i. e., global) measurements provide both a discovery engine for identifying MOA and an information base for subsequent evaluation of MOA when conducting a <b>risk</b> <b>assessment.</b> These genome-wide measure-ments are not chosen based on the hypothesized MOA and therefore represent an unbiased check of the comprehensiveness of an MOA. In addition, optimal design for MOA studies is critica...|$|R
