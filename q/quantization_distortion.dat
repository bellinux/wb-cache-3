79|47|Public
5000|$|... and on-line {{signature}} recognition. In {{pattern recognition}} applications, one codebook is constructed for each class (each class being a user in biometric applications) using acoustic vectors of this user. In the testing phase the <b>quantization</b> <b>distortion</b> of a testing signal is {{worked out with}} the whole set of codebooks obtained in the training phase. The codebook that provides the smallest vector <b>quantization</b> <b>distortion</b> indicates the identified user.|$|E
50|$|The {{addition}} of noise or other outside signals (hum, interference) {{is not considered}} distortion, though the effects of <b>quantization</b> <b>distortion</b> are sometimes included in noise. A quality measure that explicitly reflects both the noise and the distortion is the Signal-to-noise-and-distortion (SINAD) ratio.|$|E
50|$|The {{calculations}} above, however, {{assume a}} completely filled input channel. If {{this is not}} the case - if the input signal is small - the relative <b>quantization</b> <b>distortion</b> can be very large. To circumvent this issue, analog compressors and expanders can be used, but these introduce large amounts of distortion as well, especially if the compressor does not match the expander. The application of such compressors and expanders is also known as companding.|$|E
40|$|The {{additivity}} of wavelet subband <b>quantization</b> <b>distortions</b> {{was investigated}} in an unmasked detection task and in masked detection and discrimination tasks. Contrast thresholds were measured for both simple targets (artifacts induced by uniform quantization of individual discrete wavelet transform subbands) and compound targets (artifacts induced by uniform quantization of pairs of discrete wavelet transform subbands) {{in the presence of}} no mask and eight different natural image maskers. The results were used to assess summation between wavelet subband <b>quantization</b> <b>distortions</b> on orientation and spatial-frequency dimensions. In the unmasked detection experiment, subthreshold <b>quantization</b> <b>distortions</b> pooled in a non-linear fashion and the amount of summation agreed with those of previous summation-atthreshold experiments (= 2. 43; relative sensitivity= 1. 33). In the masked detection and discrimination experiments, suprathreshold <b>quantization</b> <b>distortions</b> pooled in a linear fashion. Summation increased as the distortions became increasingly suprathreshold but quickly settled to near-linear values. Summation on the spatial-frequency dimension was greater than summation on the orientation dimension for all suprathreshold contrasts. A high degree of uncertainty imposed by the natural image maskers precludes quantifying an absolute measure of summation...|$|R
50|$|When {{representing}} the signals in digital form, {{the results are}} scaled and rounded, and offsets are typically added. For example, the scaling and offset applied to the Yâ€² component per specification (e.g. MPEG-2) results {{in the value of}} 16 for black and the value of 235 for white when using an 8-bit representation. The standard has 8-bit digitized versions of CB and CR scaled to a different range of 16 to 240. Consequently, rescaling by the fraction (235-16)/(240-16) = 219/224 is sometimes required when doing color matrixing or processing in YCbCr space, resulting in <b>quantization</b> <b>distortions</b> when the subsequent processing is not performed using higher bit depths.|$|R
40|$|In this paper, {{we present}} a wavelet-based image {{compression}} scheme for perceptually lossless compression of oceanographic imagery. Wavelet coefficients are quantized within visual sensitivity of human eyes for different subbands to mask <b>quantization</b> <b>distortions</b> of compressed images. Large image files are divided into smaller blocks, {{each of which is}} compressed by an embedded tree-based coder which is capable of preserving smooth ocean textures, at the cost of complex land objects. In the mean time, a fast run-length filter detects and preserves image annotations at negligible computing time. Experimental results show that our coder produces perceptually identical image content on different display devices at a compression ratio 2 - 5 times higher than typical lossless compression schemes...|$|R
50|$|DSP-based {{amplifiers}} which {{generate a}} PWM signal {{directly from a}} digital audio signal (e. g. SPDIF) either use a counter to time the pulse length or implement a digital equivalent of a triangle-based modulator. In either case, the time resolution afforded by practical clock frequencies {{is only a few}} hundredths of a switching period, which is not enough to ensure low noise. In effect, the pulse length gets quantized, resulting in <b>quantization</b> <b>distortion.</b> In both cases, negative feedback is applied inside the digital domain, forming a noise shaper which has lower noise in the audible frequency range.|$|E
50|$|Analog {{systems do}} not have {{discrete}} digital levels in which the signal is encoded. Consequently, the original signal can be preserved to an accuracy limited only by the intrinsic noise-floor and maximum signal level {{of the media and}} the playback equipment, i.e., the dynamic range of the system. This form of distortion, sometimes called granular or <b>quantization</b> <b>distortion,</b> has been pointed to as a fault of some digital systems and recordings. Knee & Hawksford drew attention to the deficiencies in some early digital recordings, where the digital release was said to be inferior to the analog version.|$|E
5000|$|In {{an analog}} system, the signal is continuous, {{but in a}} PCM digital system, the {{amplitude}} of the signal out of the digital system is limited to one {{of a set of}} fixed values or numbers. This process is called quantization. Each coded value is a discrete step... if a signal is quantized without using dither, there will be <b>quantization</b> <b>distortion</b> related to the original input signal... In order to prevent this, the signal is [...] "dithered", a process that mathematically removes the harmonics or other highly undesirable distortions entirely, and that replaces it with a constant, fixed noise level.|$|E
40|$|Quantized massive multiple-input-multiple-output (MIMO) {{systems are}} gaining more {{interest}} {{due to their}} power efficiency. We present a new precoding technique to mitigate the multi-user interference and the <b>quantization</b> <b>distortions</b> in a downlink multi-user (MU) multiple-input-single-output (MISO) system with 1 -bit quantization at the transmitter. This work is restricted to PSK modulation schemes. The transmit signal vector is optimized for every desired received vector {{taking into account the}} 1 -bit quantization. The optimization is based on maximizing the safety margin to the decision thresholds of the PSK modulation. Simulation results show a significant gain in terms of the uncoded bit-error-ratio (BER) compared to the existing linear precoding techniques. Comment: Submitted to SPAWC 201...|$|R
40|$|A visually-optimal {{quantization}} and rate-control strategy {{based on}} results of recent contrast sensitivity and suprathreshold summation experiments is proposed. At suprathreshold contrasts, masked detection thresholds for wavelet subband <b>quantization</b> <b>distortions</b> were approximately equal for scale- 3, 4, and 5 distortions; approximately 52 % greater for scale- 2 distortions; and approximately 84 % greater for scale- 1 distortions. Base contrasts for individual subbands are selected to match these contrast ratios, and are adjusted to account both for changes in relative sensitivity at suprathreshold contrasts and for suprathreshold error-pooling effects such that the combined distortions exhibit a target contrast. Quantizer step sizes are then computed from the adjusted base contrasts. Rate control is performed by scaling target contrast linearly until a specific bit rate is met. 1...|$|R
40|$|Product {{quantization}} is {{an effective}} vector quantization approach to compactly encode high-dimensional vectors for fast approximate nearest neighbor (ANN) search. The essence of product quantization is to decompose the original high-dimensional space into the Cartesian product of {{a finite number of}} low-dimensional subspaces that are then quantized separately. Optimal space decomposition is important for the performance of ANN search, but still remains unaddressed. In this paper, we optimize product quantization by minimizing <b>quantization</b> <b>distortions</b> w. r. t. the space decomposition and the quantization codebooks. We present two novel methods for optimization: a nonparametric method that alternatively solves two smaller sub-problems, and a parametric method that is guaranteed to achieve the optimal solution if the input data follows some Gaussian distribution. We show by experiments that our optimized approach substantially improves the accuracy of product quantization for ANN search. 1...|$|R
5000|$|Noise shaping in audio is most {{commonly}} applied as a bit-reduction scheme. The most basic form of dither is flat, white noise. The ear, however, is less sensitive to certain frequencies than others at low levels (see Fletcher-Munson curves). By using noise shaping we can effectively spread the quantization error around {{so that more}} of it is focused on frequencies that we can't hear as well and less of it is focused on frequencies that we can hear. The result is that where the ear is most critical the quantization error can be reduced greatly and where our ears are less sensitive the noise is much greater. This can give a perceived noise reduction of 4 bits compared to straight dither. [...] While 16-bit audio is typically thought to have 96 dB of dynamic range (see <b>quantization</b> <b>distortion</b> calculations), it can actually be increased to 120 dB using noise-shaped dither.|$|E
30|$|Noise {{produced}} by soundcard during AD conversion including <b>quantization</b> <b>distortion.</b>|$|E
3000|$|... which {{depends on}} the {{transmit}} power, average CDI <b>quantization</b> <b>distortion</b> and target SINR of the user.|$|E
40|$|Asymptotic {{expressions}} for {{the optimal}} scaling factor and resulting minimum distortion, {{as a function}} of codebook size N, are found for fixed-rate k-dimensional lattice vector quantization of generalized Gaussian sources with decay parameter ff 1. These expressions are derived by minimizing upper and lower bounds to distortion. It is shown that the optimal scaling factor aN decreases as (ln N) 1 =ff N Î“ 1 =k and that for scale-optimized lattice <b>quantization,</b> granular <b>distortion</b> asymptotically dominates overload distortion. Consequently, the minimum distortion is DN Â¸ = c (ln N) 2 =ff N Î“ 2 =k. This result indicates that the distortion of optimal lattice quantizers diverges from that of asymptotically optimal vector quantization, as N increases. Index terms: optimal lattice scaling, granular distortion, overload <b>distortion,</b> high resolution <b>quantization</b> theory, <b>distortion</b> bounds. This work was supported by an NSF Graduate Fellowship and by NSF Grant NCR- 9415754. Part [...] ...|$|R
40|$|Modern lossy image {{compression}} algorithms exploit {{characteristics of the}} human visual system; however, whereas psychophysical results are commonly reported in terms of contrast, the majority of compression strategies incorporate contrast sensitivity only in an implicit fashion (e. g., by weighting quantizer step sizes). This paper presents a contrast-based quantization strategy for use in wavelet-based lossy {{image compression}}. Based {{on the results of}} recent psychophysical experiments using wavelet subband <b>quantization</b> <b>distortions</b> and natural-image backgrounds, subbands are quantized such that the distortions in the reconstructed image exhibit specific root-mean squared (RMS) contrasts, and such that edge-structure is preserved across scale-space. Within a single, unified framework, the proposed contrast-based strategy yields images which are competitive in visual quality with results from current visually lossless approaches at high bit-rates, and which demonstrate improved visual quality over current visually lossy approaches at low bit-rates. This strategy operates in the context of non-embedded and embedded quantization, the latter of which yields a highly scalable codestream which provides the best-possible visual quality at all bit-rates; a specific application of the proposed algorithm to JPEG- 2000 is presented. I...|$|R
40|$|Abstractâ€”Product {{quantization}} (PQ) is {{an effective}} vector quantization method. A product quantizer can generate an exponentially large codebook at very low memory/time cost. The essence of PQ is to decompose the high-dimensional vector space into the Cartesian product of subspaces and then quantize these subspaces separately. The optimal space decomposition {{is important for the}} PQ performance, but still remains an unaddressed issue. In this paper, we optimize PQ by minimizing <b>quantization</b> <b>distortions</b> w. r. t the space decomposition and the quantization codebooks. We present two novel solutions to this challenging optimization problem. The first solution iteratively solves two simpler sub-problems. The second solution is based on a Gaussian assumption and provides theoretical analysis of the optimality. We evaluate our optimized product quantizers in three applications: (i) compact encoding for exhaustive ranking [1], (ii) building inverted multi-indexing for non-exhaustive search [2], and (iii) compacting image representations for image retrieval [3]. In all applications our optimized product quantizers outperform existing solutions. Index Termsâ€”Vector quantization, nearest neighbor search, image retrieval, compact encoding, inverted indexing Ã‡...|$|R
3000|$|... in (22) is {{complicated}} and nonlinear with the CDI <b>quantization</b> <b>distortion.</b> In the sequel, we study the bit allocation problem resorting to asymptotic analysis.|$|E
30|$|Similarly, we {{can obtain}} the average CDI <b>quantization</b> <b>distortion</b> for the interfering channel h_c,b_k as E{sin ^ 2 Î¸ _c,b_k}= N_t- 1 /N_t 2 ^-B_c,b_k/N_t- 1 for câ‰ b, where B_c,b_k {{is the number}} of bits for {{quantizing}} the CDI of h_c,b_k.|$|E
40|$|Abstract-The {{effect of}} bit robbing in tandem digital {{switches}} is shoh {{to consist of}} a time-varying hear distortion comparable to aliasing distortion in addition to increased <b>quantization</b> <b>distortion.</b> Alternative bit-robbing strategies in which this linear distortion can be reduced or eliminated are evaluated, and recommendations are made for future digital switch designs. ...|$|E
3000|$|... [*]=[*] 1. We {{note that}} QIM can be {{regarded}} as a special case of dither modulation [30, 31], wherein dither noise is added first and then quantized. The distortion compensation technique introduced in [30, 32] may also be incorporated into the <b>quantization</b> realization. <b>Distortion</b> compensated QIM allows the adjustment of the quantization steps without introducing extra distortion while pursuing robustness. In our former studies [33, 34], we have shown that the incorporation of distortion compensation into QIM can successfully enhance the robustness of the watermark while the imperceptibility is still maintained.|$|R
40|$|A fast {{rate-distortion}} analytical {{framework for}} the PSNR-based optimization of JPEG baseline compression on color images is proposed in this paper. The analysis is conducted in the luminance-chrominance color space and based upon the È¡-domain analysis method [1]. The simulation {{results show that the}} proposed scheme can dramatically improve the performance of the JPEG baseline compression on color images in terms of peak signal-to-noise ratio (PSNR). Meanwhile, while the complexity of the proposed scheme is kept low, its results are very close to that from exhaustive searching method. Index Termsâ€”Image coding, <b>quantization,</b> rate <b>distortion</b> theory 1...|$|R
40|$|Foraprobability measure P on Rd âˆ« and nâˆˆN {{consider}} en = inf min V (â€–xâˆ’aâ€–) dP (x) where aâˆˆÎ± the infimum {{is taken}} over all subsets Î± of Rd with card(Î±) â‰¤ n and V is a nondecreasing function. Under certain conditions on V,wederive the precise n-asymptotics of en for nonsingular and for (singular) self-similar distributions P and we find the asymptotic performance of optimal quantizers using weighted empirical measures. Key words: High-rate vector <b>quantization,</b> norm-difference <b>distortion,</b> empirical measure, weak convergence, local distortion, point density measure. 2001 AMS classification: 60 E 99, 94 A 29, 28 A 80. ...|$|R
40|$|ITC/USA 2009 Conference Proceedings / The Forty-Fifth Annual International Telemetering Conference and Technical Exhibition / October 26 - 29, 2009 / Riviera Hotel & Convention Center, Las Vegas, NevadaThis paper {{describes}} a psychophysical experiment to measure visibility thresholds (VT) for <b>quantization</b> <b>distortion</b> in JPEG 2000 and an associated quantization algorithm for visually lossless coding of color aerial images. The visibility thresholds are {{obtained from a}} <b>quantization</b> <b>distortion</b> model based on the statistical characteristics of wavelet coefficients and the deadzone quantizer of JPEG 2000, and the resulting visibility thresholds are presented for the luminance component (Y) and two chrominance components (Cb and Cr). Using the thresholds, we have achieved visually lossless coding for 24 -bit color aerial images at an average bitrate of 4. 17 bits/pixels, which is approximately 30 % of the bitrate required for numerically lossless coding...|$|E
30|$|The LSFs {{are ideal}} for split quantization. This is because the {{spectral}} sensitivity of these parameters is localized; that is, a change in a given LSF merely affects neighboring frequency regions of the LPC power spectrum. Hence, split quantization of the LSFs cause negligible leakage of the <b>quantization</b> <b>distortion</b> from one spectral region to another [19].|$|E
3000|$|K {{introduces}} extra circuit powers {{because of}} extra radio-frequency (RF) chains at the BSs [30]. On the other hand, the <b>quantization</b> <b>distortion</b> in (2) grows exponentially {{as the number}} of antennas increases, which can greatly degrade the user SINR [31] and eventually bring down the system EE. Therefore, in this work, we consider only the case of full multiplexing. pause [...]...|$|E
40|$|This paper {{addresses}} {{the problem of}} the detection of speaker changes and clustering speakers when no information is available regarding speaker classes or even the total number of classes. We assume that no previous information on speakers is available (no speaker model, no training phase) and that people do not speak simultaneously. The aim is to apply speaker grouping information to speaker adaptation for speech recognition. WeuseVector <b>Quantization</b> (VQ) <b>distortion</b> as the criterion. A speaker model is created from successive utterances as a codebook by a VQ algorithm, and the VQ distortion is calculated between the model and an utterance...|$|R
40|$|Abstractâ€”It {{has been}} shown by earlier results that for fixed rate multiresolution scalar quantizers and for mean squared error {{distortion}} measure, codecell convexity precludes optimality for certain discrete sources. However it was unknown whether the same phenomenon can occur for any continuous source. In this paper, examples of continuous sources (even with bounded continuous densities) are presented for which optimal fixed rate multiresolution scalar quantizers cannot have only convex codecells, proving that codecell convexity precludes optimality also for such regular sources. Index Termsâ€”Clustering methods, codecell convexity, continuous density function, mean squared error methods, multiresolution, optimization methods, <b>quantization,</b> rate <b>distortion</b> theory, source coding I. INTRODUCTION AND DEFINITION...|$|R
40|$|Conventional video {{compression}} algorithm such as variable length entropy encoding, Pyramid vector <b>quantization</b> provides more <b>distortion</b> during compression and transmission. In {{order to reduce}} distortion and also to improve compression performance, this paper introduces a modified rate distortion algorithm for {{video compression}} based on Adaptive Prediction. It removes spatial and temporal redundancies in a backward adaptive fashion. This proposed scheme provides significant improvements in compression performance and also reduces the distortion during compression and transmission...|$|R
40|$|We present {{techniques}} for processing MPEG-audio encoded signals during the decoding process, using efficient fixed-point arithmetic operations. A large signal-to-quantization-noise-ratio is achieved {{over a large}} range of input levels. By taking advantage of MPEG audio built-in properties, <b>quantization</b> <b>distortion</b> at the outputs of our systems is kept largely inaudible, even though only low-resolution fixed-point operations {{are used in the}} processing...|$|E
40|$|This paper {{studies the}} problem of reconstructing sparse or {{compressible}} signals from compressed sensing measurements that have undergone nonuniform quantization. Previous approaches to this Quantized Compressed Sensing (QCS) problem based on Gaussian models (bounded l 2 -norm) for the <b>quantization</b> <b>distortion</b> yield results that, while often acceptable, may not be fully consistent: re-measurement and quantization of the reconstructed signal do not necessarily match the initial observations. <b>Quantization</b> <b>distortion</b> instead more closely resembles heteroscedastic uniform noise, with variance depending on the observed quantization bin. Generalizing our previous work on uniform quantization, we show that for nonuniform quantizers described by the "compander" formalism, <b>quantization</b> <b>distortion</b> may be better characterized as having bounded weighted lp-norm (p >= 2), for a particular weighting. We develop a new reconstruction approach, termed Generalized Basis Pursuit DeNoise (GBPDN), which minimizes the sparsity of the reconstructed signal under this weighted lp-norm fidelity constraint. We prove that for B bits per measurement and under the oversampled QCS scenario (when the number of measurements is large compared to the signal sparsity) if the sensing matrix satisfies a proposed generalized Restricted Isometry Property, then, GBPDN provides a reconstruction error of sparse signals which decreases like O(2 ^{-B}/sqrt{p+ 1 }) : a reduction by a factor sqrt{p+ 1 } relative to that produced by using the l 2 -norm. Besides the QCS scenario, we also show that GBPDN applies straightforwardly to the related case of CS measurements corrupted by heteroscedastic Generalized Gaussian noise with provable reconstruction error reduction. Finally, we describe an efficient numerical procedure for computing GBPDN via a primal-dual convex optimization scheme, and demonstrate its effectiveness through simulations...|$|E
40|$|We {{investigate}} {{the upper and}} lower bounds on the quantization distortions for independent and identically distributed sources in the finite block-length regime. Based on the convex optimization framework of the rate-distortion theory, we derive a lower bound on the <b>quantization</b> <b>distortion</b> under finite block-length, which is shown to be greater than the asymptotic distortion given by the rate-distortion theory. We also derive two upper bounds on the <b>quantization</b> <b>distortion</b> based on random quantization codebooks, which can achieve any distortion above the asymptotic one. Moreover, we apply the new upper and lower bounds to two types of sources, the discrete binary symmetric source and the continuous Gaussian source. For the binary symmetric source, we obtain the closed-form expressions of {{the upper and lower}} bounds. For the Gaussian source, we propose a computational tractable method to numerically compute the upper and lower bounds, for both bounded and unbounded quantization codebooks. Numerical results show that the gap between the upper and lower bounds is small for reasonable block length and hence the bounds are tight...|$|E
40|$|The {{problem of}} {{communicating}} (unordered) sets, rather than (ordered) sequences is formulated. Elementary results in all major branches of source coding theory, including lossless coding, high-rate and low-rate <b>quantization,</b> and rate <b>distortion</b> theory are presented. In certain scenarios, rate savings of log n! bits for sets of size n are obtained. Asymptotically {{in the set}} size, the entropy rate is zero and for sources with an ordered parent alphabet, the (0, 0) point is the rate distortion function. ...|$|R
25|$|A digital {{computer}} simulation model of an analog system {{such as the}} brain is an approximation that introduces random <b>quantization</b> errors and <b>distortion.</b> However, the biological neurons also suffer from randomness and limited precision, for example due to background noise. The errors of the discrete model can be made smaller than the randomness of the biological brain by choosing a sufficiently high variable resolution and sample rate, and sufficiently accurate models of non-linearities. The computational power and computer memory must however be sufficient to run such large simulations, preferably in real time.|$|R
30|$|The subband or {{transform}} audio coding schemes primarily {{exploit the}} destination (human ear) model; the psychoacoustic model tells us where signal <b>distortions</b> (<b>quantization)</b> are allowed such {{that these are}} inaudible or least annoying. In speech coding, on the other hand, source models are primarily used. The incoming signal is matched to {{the characteristics of a}} source model (the vocal tract model), and the parameters of this source model are transmitted. In the decoder, the source model and its parameters are used to reconstruct the signal. For an overview on speech coding, please refer to [21].|$|R
