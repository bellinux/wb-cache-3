1028|118|Public
25|$|During encoding, 576 time-domain {{samples are}} taken and are {{transformed}} to 576 frequency-domain samples. If {{there is a}} transient, 192 samples are taken instead of 576. This is done to limit the temporal spread of <b>quantization</b> <b>noise</b> accompanying the transient. (See psychoacoustics.) Frequency resolution {{is limited by the}} small long block window size, which decreases coding efficiency. Time resolution can be too low for highly transient signals and may cause smearing of percussive sounds.|$|E
25|$|An analog {{computer}} or analogue computer {{is a form}} of computer that uses the continuously changeable aspects of physical phenomena such as electrical, mechanical, or hydraulic quantities to model the problem being solved. In contrast, digital computers represent varying quantities symbolically, as their numerical values change. As an {{analog computer}} does not use discrete values, but rather continuous values, processes cannot be reliably repeated with exact equivalence, as they can with Turing machines. Unlike digital signal processing, analog computers do not suffer from the <b>quantization</b> <b>noise,</b> but are limited by analog noise.|$|E
50|$|For complex {{signals in}} {{high-resolution}} ADCs {{this is an}} accurate model. For low-resolution ADCs, low-level signals in high-resolution ADCs, and for simple waveforms the <b>quantization</b> <b>noise</b> is not uniformly distributed, making this model inaccurate. In these cases the <b>quantization</b> <b>noise</b> distribution is strongly affected by the exact amplitude of the signal.|$|E
40|$|Abstract: Fixed-point {{conversion}} requires fast {{analytical methods}} {{to evaluate the}} accuracy degradation due to <b>quantization</b> <b>noises.</b> Usually, analytical methods do not consider the correlation between <b>quantization</b> <b>noises.</b> Correlation between <b>quantization</b> <b>noises</b> occurs when a data is quantized several times. This report explained, through an example, the methodology used in the ID. Fix tool to support correlation. To decrease the complexity, a method to group together several <b>quantization</b> <b>noises</b> inside a same noise source is described. The maximal relative estimation error obtained with the proposed approach is less than 2 %. Key-words: fixed-point, <b>quantization,</b> analytic approach, <b>noise,</b> correlation This is a note This is a second not...|$|R
3000|$|... variables. The noise {{model is}} based on [33] and second order effects are {{neglected}} by applying first order Taylor expansions. However, the paper {{seems to suggest that}} the contributions of the signal <b>quantization</b> <b>noises</b> at the output can be added, assuming that the noises are independent. In nonlinear systems, this is a strong assumption that leads to variance underestimation. The accuracy of the method is not supported with any empirical data.|$|R
3000|$|... is a uniformly {{distributed}} {{white noise}} that is uncorrelated with the signal, and independent {{from the other}} <b>quantization</b> <b>noises.</b> In this study, the round-off method is used rather than truncation. For convergent rounding, the quantization leads to an error with a zero mean. For classical rounding, the mean can be assumed to be null as soon as several bits (more than 3 bits) are eliminated in the quantization process. The expression of the statistical parameters of the noise sources {{can be found in}} [16]. If [...]...|$|R
5000|$|Equivalent {{pulse code}} {{modulation}} noise, measure of noise by comparing to PCM <b>quantization</b> <b>noise</b> ...|$|E
50|$|This {{theoretical}} maximum SNR {{assumes a}} perfect input signal. If the input signal is already noisy (as {{is usually the}} case), the signal's noise may be larger than the <b>quantization</b> <b>noise.</b> Real analog-to-digital converters also have other sources of noise that further decrease the SNR compared to the theoretical maximum from the idealized <b>quantization</b> <b>noise,</b> including the intentional addition of dither.|$|E
50|$|The {{range of}} {{possible}} values that can be represented numerically by a sample {{is defined by the}} number of binary digits used. This is called the resolution, and is usually referred to as the bit depth in the context of PCM audio. The <b>quantization</b> <b>noise</b> level is directly determined by this number, decreasing exponentially as the resolution increases (or linearly in dB units), and with an adequate number of true bits of quantization, random noise from other sources will dominate and completely mask the <b>quantization</b> <b>noise.</b> The Redbook CD standard uses 16 bits, which keep the <b>quantization</b> <b>noise</b> 96 dB below maximum amplitude, far below a discernible level with almost any source material.|$|E
40|$|International audienceTo satisfy cost constraints, {{application}} {{implementation in}} embedded systems requires fixed-point arithmetic. Thus, applications defined in floating-point arithmetic must {{be converted into}} a fixed-point specification. This conversion requires accuracy evaluation to ensure algorithm integrity. Indeed, fixed-point arithmetic generates <b>quantization</b> <b>noises</b> due to some bits elimination during a cast operation. These noises propagate through the system and modify computing accuracy. In this paper, an accuracy evaluation model based on an analytical approach is presented and valid for all systems including arithmetic operations. The LMS algorithm example is developed and its validity is verified through experimentations...|$|R
40|$|The aim of {{this paper}} is the {{description}} of an experiment carried out to verify the robustness of two different approaches for the reconstruction of convex polyominoes in discrete tomography. This is a new field of research, because it differs from classic computerized tomography, and several problems are still open. In particular, the stability problem is tackled by using both a modified version of a known algorithm and a new genetic approach. The effect of both, instrumental and <b>quantization</b> <b>noises</b> has been considered too. © 2007 Springer Science+Business Media, LLC...|$|R
40|$|International audienceTo satisfy cost constraints, {{application}} {{implementation in}} embed- ded systems requires fixed point arithmetic. Thus, the application defined in {{floating point arithmetic}} must be converted into a fixed- point specification. This conversion requires accuracy evaluation to ensure algorithm integrity. Indeed, fixed-point arithmetic generates <b>quantization</b> <b>noises</b> due to the elimination of some bits during a cast operation. These noises propagate through the system and degrade computing accuracy. In this paper, a method based on Generalized Gaussian PDF is presented to model and generate the output noise of the system. The accuracy of the proposed model is evaluated through different experiments...|$|R
50|$|When a {{measurement}} is digitized, {{the number of}} bits used to represent the measurement determines the maximum possible signal-to-noise ratio. This is because the minimum possible noise level is the error caused by the quantization of the signal, sometimes called <b>Quantization</b> <b>noise.</b> This noise level is non-linear and signal-dependent; different calculations exist for different signal models. <b>Quantization</b> <b>noise</b> is modeled as an analog error signal summed with the signal before quantization ("additive noise").|$|E
50|$|Although rarely used, {{there exists}} the {{capability}} for standardized emphasis in Red Book CD mastering. As CDs {{were intended to}} work on 14-bit audio, a specification for 'pre-emphasis' was included to compensate for <b>quantization</b> <b>noise.</b> After production spec was set at 16 bits, <b>quantization</b> <b>noise</b> became less of a concern, but emphasis remained an option through standards revisions. The pre-emphasis {{is described as a}} first-order filter with a gain of 10 dB (at 20 dB/decade) and time constants 50 μs and 15 μs.|$|E
50|$|Certain {{kinds of}} A/D {{converters}} known as delta-sigma converters produce disproportionately more <b>quantization</b> <b>noise</b> {{in the upper}} portion of their output spectrum. By running these converters at some multiple of the target sampling rate, and low-pass filtering the oversampled signal down to half the target sampling rate, a final result with less noise (over the entire band of the converter) can be obtained. Delta-sigma converters use a technique called noise shaping to move the <b>quantization</b> <b>noise</b> to the higher frequencies.|$|E
40|$|This work {{studies the}} joint design of {{precoding}} and backhaul compression {{strategies for the}} downlink of cloud radio access networks. In these systems, a central encoder is connected to multiple multi-antenna base stations (BSs) via finite-capacity backhaul links. At the central encoder, precoding is followed by compression {{in order to produce}} the rate-limited bit streams delivered to each BS over the corresponding backhaul link. In current state-of-the-art approaches, the signals intended for different BSs are compressed independently. In contrast, this work proposes to leverage joint compression, also referred to as multivariate compression, of the signals of different BSs in order to better control the effect of the additive <b>quantization</b> <b>noises</b> at the mobile stations (MSs). The problem of maximizing the weighted sum-rate with respect to both the precoding matrix and the joint correlation matrix of the <b>quantization</b> <b>noises</b> is formulated subject to power and backhaul capacity constraints. An iterative algorithm is proposed that achieves a stationary point of the problem. Moreover, in order to enable the practical implementation of multivariate compression across BSs, a novel architecture is proposed based on successive steps of minimum mean-squared error (MMSE) estimation and per-BS compression. Robust design with respect to imperfect channel state information is also discussed. From numerical results, it is confirmed that the proposed joint precoding and compression strategy outperforms conventional approaches based on the separate design of precoding and compression or independent compression across the BSs. Comment: Submitted to IEEE Transactions on Signal Processin...|$|R
40|$|Abstract—For {{digitally}} controlled {{switching power}} converters, on-line system identification {{can be used}} to assess the system dynamic responses and stability margins. This paper presents a modified correlation method for system identification of power converters with digital control. By injecting a multiperiod pseudo random binary signal (PRBS) to the control input of a power converter, the system frequency response can be derived by cross-correlation of the input signal and the sensed output signal. Compared to the conventional cross-correlation method, averaging the cross-correlation over multiple periods of the injected PRBS can significantly improve the identification results in the presence of PRBS-induced artifacts, switching and <b>quantization</b> <b>noises.</b> An experimental digitally controlled forward converter with an FPGA-based controller is used to demonstrate accurate and effective identification of the converter control-to-output response. Index Terms—Digital control, frequency response, parameter estimation, system identification. I...|$|R
30|$|To {{alleviate}} this problem, postprocessing {{is one of}} {{the most}} promising solutions as it can improve the video quality without the need of changing the encoder structure. Many postprocessing techniques have been proposed to reduce the quantization artifacts of block-based coding. These include block-boundary postfiltering techniques to smooth the discontinuous in either spatial [3 – 7] or transform domain [8 – 13] such as adaptive filtering and wavelet-based filtering. Also proposed are more sophisticated methods that enhance the reconstructed video by using image/video restoration techniques such as iterative methods based on the theory of projection onto convex sets (POCS) or constrained minimization [14 – 18], maximum a posterior probability estimation approach (MAP) [19 – 21], and regularized image/video restoration [22 – 27]. These methods consider the compressed images/videos to be distorted by a codec system and apply restoration techniques to reduce the <b>quantization</b> <b>noises</b> and coding artifacts.|$|R
50|$|Proper {{application}} of dither combats <b>quantization</b> <b>noise</b> effectively, and is commonly applied during mastering before final bit depth reduction, and also {{at various stages}} of DSP.|$|E
50|$|In reality, the <b>quantization</b> <b>noise</b> is {{of course}} not {{independent}} of the signal; this dependence {{is the source of}} idle tones and pattern noise in Sigma-Delta converters.|$|E
5000|$|Similarly for {{a second}} order delta sigma modulator, the noise is shaped by a filter with {{transfer}} function [...] The in-band <b>quantization</b> <b>noise</b> can be approximated as: ...|$|E
30|$|The best {{performance}} for QandPF, QandPFwithCS, and QandNC happens after {{a longer period}} of time than for QNC. As it can be seen, this is the best achievable quality (SNR value), which is limited only by <b>quantization</b> <b>noises</b> at the source nodes, for both QandPF and QandNC scenarios. As it is also expected, using compressed sensing decoding (as in QandPFwithCS scenario) provides a better estimation of the messages before all the packets are delivered. Furthermore, as opposed to QandPF which shows a progressive improvement in the quality, QandNC has an all or nothing characteristic, as mentioned earlier. It is also interesting to note that low-density adjacency matrices in networks with small degree of nodes result in having (finite field) measurement matrices that are not of full rank in the QandNC scenario. Hence, as shown in Figure  5 a, QandNC scheme fails to work properly.|$|R
30|$|Analog-to-digital {{converters}} {{have many}} applications in {{digital signal processing}} and communication systems. Conventional A/D converters consist of two steps: sampling operation followed by digital <b>quantization.</b> The <b>noise</b> introduced by signal distortion due to quantization decreases the A/D performance. In the literature, the performance of A/D converters {{is measured by the}} number of bits per sample and signal-to-noise ratio (SNR) [1, 2].|$|R
40|$|In this paper, {{we report}} on the new method of image compression. The method is based on LZ 77 {{dictionary}} algorithm. We introduce two modifications such as <b>quantization</b> and <b>noise</b> levels. Experimental results presented in this paper prove that the new method of image compression gives promising results as compared with original LZ 77 dictionary algorithm and JPEG 2000. Ó 2006 Elsevier B. V. All rights reserved...|$|R
5000|$|... irreversible: the CDF 9/7 wavelet transform. It {{is said to}} be [...] "irreversible" [...] {{because it}} {{introduces}} <b>quantization</b> <b>noise</b> that depends on the precision of the decoder.|$|E
5000|$|A {{symmetric}} source X can be modelled with , for [...] and 0 elsewhere.The {{step size}} [...] and {{the signal to}} <b>quantization</b> <b>noise</b> ratio (SQNR) of the quantizer is ...|$|E
50|$|However, the {{quantizer}} is not homogeneous, and so {{this explanation}} is flawed. It's true that ΔΣ is inspired by Δ-modulation, but the two are distinct in operation. From the first block diagram in Fig. 2, the integrator in the feedback path can be removed if the feedback is taken directly from the input of the low-pass filter. Hence, for delta modulation of input signal , the low-pass filter sees the signalHowever, sigma-delta modulation of the same input signal places at the low-pass filterIn other words, SDM and DM swap {{the position of the}} integrator and quantizer. The net effect is a simpler implementation that has the added benefit of shaping the <b>quantization</b> <b>noise</b> away from signals of interest (i.e., signals of interest are low-pass filtered while <b>quantization</b> <b>noise</b> is high-pass filtered). This effect becomes more dramatic with increased oversampling, which allows for <b>quantization</b> <b>noise</b> to be somewhat programmable. On the other hand, Δ-modulation shapes both noise and signal equally.|$|E
40|$|Signal {{acquisition}} is a noisy business. In photographic images, {{there is}} noise within the light intensity signal (e. g., photon noise), and additional noise can arise within the sensor (e. g., thermal noise in a CMOS chip), {{as well as}} in subsequent processing (e. g., <b>quantization).</b> Image <b>noise</b> can be quite noticeable, as in images captured by inexpensive cameras built into cellular telephones, or imperceptible, as in images captured b...|$|R
30|$|In {{the case}} of an MP 3 coder, the output noise power metric cannot be used {{directly}} as a compression quality criterion. The compression is indeed based on adding <b>quantization</b> <b>noises</b> where it is imperceptible, or at least barely audible. The compression quality has been tested using EAQUAL [26] which stands for evaluation of audio quality. It is an objective measurement tool very similar to the ITU-R recommendation BS. 1387 based on PEAQ technique. This has to be used because listening tests are impossible to formalize. In EAQUAL, the degradations due to compression are measured with the objective degradation grade (ODG) metric. This metric varies from 0 (no degradation) to − 4 (inaudible). The level of − 1 is the threshold beyond the degradation becomes annoying for ears. This ODG is used to measure the degradation due to fixed-point computation. Thus for the fixed-point design, the aim is to obtain the fixed-point specification of the coder which minimizes the implementation cost and maintains an ODG lower or equal to − 1 for the different audio tracks of the sample group.|$|R
40|$|A robust {{motion control}} system is {{essential}} for the linear motor (LM) -based direct drive to provide high speed and high-precision performance. This paper studies a systematic control design method using fast nonsingular terminal sliding mode (FNTSM) for an LM positioner. Compared with the conventional nonsingular terminal sliding mode control, the FNTSM control can guarantee a faster convergence rate of the tracking error in the presence of system uncertainties including payload variations, friction, external disturbances, and measurement noises. Moreover, its control input is inherently continuous, which accordingly avoids the undesired control chattering problem. We further discuss the selection criteria of the controller parameters for the LM to deal with the system dynamic constraints and performance tradeoffs. Finally, we present a robust model-free velocity estimator based on the only available position measurements with <b>quantization</b> <b>noises</b> such that the estimated velocity can be used for feedback signal to the FNTSM controller. Experimental results demonstrate the practical implementation of the FNTSM controller and verify its robustness of more accurate tracking and faster disturbance rejection compared with a conventional NTSM controller and a linear H infinty controller...|$|R
50|$|At offsets {{far removed}} from the carrier, the phase-noise floor of a DDS is {{determined}} by the power sum of the DAC <b>quantization</b> <b>noise</b> floor and the reference clock phase noise floor.|$|E
50|$|IIR digital {{filters are}} often more {{difficult}} to design, due to problems including dynamic range issues, <b>quantization</b> <b>noise</b> and instability.Typically digital IIR filters are designed {{as a series of}} digital biquad filters.|$|E
5000|$|Assuming {{a uniform}} {{distribution}} of input signal values, the <b>quantization</b> <b>noise</b> is a uniformly distributed random signal with a peak-to-peak amplitude of one quantization level, making the amplitude ratio 2n/1. The formula is then: ...|$|E
30|$|Iterative {{methods are}} quite robust against <b>quantization</b> and {{additive}} <b>noise.</b> In fact, we {{can prove that}} the iterative methods approach the pseudo-inverse (least squares) solution for a noisy environment; specially, when the matrix is ill-conditioned [50].|$|R
30|$|In this section, we {{will explore}} the {{performance}} of decoding using sparse recovery based on Equation 20 and the QNC design proposed in Theorem 1. It {{is well known that}} recovery of exactly sparse vectors from an under-determined set of linear measurements can be done with no error, using linear programming [39]. Specifically, theoretical works show that the NP-hard ℓ 0 minimization can be replaced with ℓ 1 minimization without any associated error, when dealing with noiseless measurements [37, 39]. However, when dealing with noisy measurements, ℓ 1 -min recovery does not necessarily offer a minimum mean squared error solution. There is still a lot of work being done to develop practical and near minimum mean squared error recovery algorithms for noisy cases. Sparse recovery from quantized measurements has been recently studied in a number of works [40 – 42]. For instance, the authors in [41] consider the estimation problem of sparse vectors from measurements that are quantized and corrupted by Gaussian noise. The main aspect that differentiates our model from that in [41] is that in our QNC scenario the resulting effective total measurement noises are non-linear functions of <b>quantization</b> <b>noises</b> at each edge.|$|R
40|$|Abstract—In {{embedded}} systems using fixed-point arithmetic, converting applications into fixed-point representations requires a fast and efficient accuracy evaluation. This paper presents a new analytical approach to determine an {{estimation of the}} numerical accuracy of a fixed-point system, which is accurate and valid for all systems formulated with smooth operations (e. g. additions, subtractions, multiplications and divisions). The mathematical expression of the system output noise power is determined using matrices to obtain more compact expressions. The proposed approach {{is based on the}} determination of the timevarying impulse-response of the system. To speedup computation of the expressions, the impulse response is modelled using a linear prediction approach. The approach is illustrated in the general case of time-varying recursive systems by the Least Mean Square (LMS) algorithm example. Experiments on various and representative applications show the fixedpoint accuracy estimation quality of the proposed approach. Moreover, the approach using the linear-prediction approximation is very fast even for recursive systems. A significant speed-up compared to the best known accuracy evaluation approaches is measured even for the most complex benchmarks. Index Terms—Fixed-point arithmetic, <b>quantization</b> <b>noises,</b> adaptive filters, accuracy evaluation I...|$|R
