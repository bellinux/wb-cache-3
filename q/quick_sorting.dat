11|132|Public
50|$|Grenade usage {{declined}} {{significantly in}} the early 18th century, a fact that {{can be attributed to}} the improved effectiveness of massive infantry line tactics and flintlock technology. However, the need for elite assault troops remained, and the existing grenadier companies were used for this purpose. As noted, above average physical size had been considered important for the original grenadiers and, in principle, height and strength remained the basis of selection for these picked companies. In the British regiments of foot during the 18th century the preference was, however, to draw on steady veterans for appointment to individual vacancies in a grenadier company (one of the eight companies comprising each regiment). The traditional criterion of size was only resorted to when newly raised regiments required a <b>quick</b> <b>sorting</b> of a mass of new recruits. Transferral to a grenadier company generally meant both enhanced status and an increase in subsistence pay.|$|E
40|$|Abstract. <b>Quick</b> <b>sorting</b> {{is one of}} the sorting {{algorithms}} {{with good}} performance. However, there is a bottleneck of its performance in dealing with massive data with high repetition rate. Therefore, a new effective <b>quick</b> <b>sorting</b> algorithm is proposed in this study. This approach possesses the advantage of conciseness of <b>quick</b> <b>sorting</b> algorithms while avoiding the disadvantages of recursive algorithms. The time complexity is O(n), and the space complexity is O(1). Theoretical analysis and experimental data have shown that its performance is superior to the original <b>quick</b> <b>sorting</b> algorithm, and it is applicable to the processing of massive data with high repetition rate...|$|E
30|$|Modification of sorting {{operation}}. Because {{the index}} operation and pattern string comparison operation of specific network data only use the first 6  bytes {{of the pattern}} string array, it is rather redundant for the <b>quick</b> <b>sorting</b> of the whole pattern string array, {{and in terms of}} later application, only the first 6  bytes need to be sorted.|$|E
5000|$|... {{implements}} <b>sort,</b> bubble <b>sort,</b> <b>quick</b> <b>sort,</b> heap sort {{and insert}} sort algorithm.|$|R
40|$|In {{computer}} science,especially for the algorithmic {{theory of}} information,the research of sorting algorithm {{is very important}} thema and from computational experiments,the <b>quick</b> <b>sort</b> algorithm,which is discoverd by Hoare[2]is known as fastest ones. However,the worst complexity of this sorting method is very large and {{for the sort of}} special data,which include much same data,its performance is less than other sorting method,for example merge sort. So,in this note,first,we analyse the complexity of <b>quick</b> <b>sort</b> and merge sort form theoritical aspect and show that <b>quick</b> <b>sort</b> algorithm is faster than merge sort algorithm,when each data are different. Secondly,we introduce the hash sort algorithm,which can sort numbers very fast. The complexity of sorting n numbers is O(n) and it is smaller than O(n log n),which is the complexity of <b>quick</b> <b>sort</b> or merge sort. Strictly saying,hash sort algorithm is not a sorting algorithm,since it treats only numbers. In this note,we apply the idea of hash sort to general data and propose an algorithm that dividing given general data into the sets of same data. Finally,we use this technic to <b>quick</b> <b>sort</b> algorithm and propose a new sorting method whose worst complexity is faster than that of <b>quick</b> <b>sort...</b>|$|R
40|$|Sorting is a {{commonly}} used operation in computer science. In {{addition to its}} main job of arranging lists or arrays in sequence, sorting is often also required to facilitate some other operation such as searching, merging and normalization or used as an intermediate operation in other operations. A sorting algorithm consists of comparison, swap, and assignment operations[1 - 3]. There are several simple and complex sorting algorithms that are being used in practical {{life as well as}} in computation such as <b>Quick</b> <b>sort,</b> Bubble sort, Merge sort, Bucket sort, Heap sort, Radix sort etc. But the application of these algorithms depends on the problem statement. This paper introduces MQ sort which combines the advantages of <b>quick</b> <b>sort</b> and Merge sort. The comparative analysis of performance and complexity of MQ sort is done against <b>Quick</b> <b>sort</b> and Merge sort. MQ sort significantly reduces complexity and provides better performance than <b>Quick</b> <b>sort,</b> Merge sort...|$|R
40|$|Turbine blade {{is one of}} the {{critical}} components of aircraft engine. The performance of the engine depends on the shape and dimensions of components, but superalloy blade material cannot be easily machined. Although investment casting is an ideal process for such net-shape components, it requires an accurate determination of the casting-die profile. In this paper, a reversing design methodology for investment casting die using ProCAST is proposed. By combining the methods of simplifying grid files and <b>quick</b> <b>sorting,</b> the efficiency of sorting and matching can be largely improved. Further, the mould/die cavity anti-deformation system can be easily built. With ProCAST, the optimized die profile for investment casting can be established...|$|E
40|$|This year a Fujitsu Laboratory team {{participated in}} web tracks. For TREC 9 we experimented passage {{retrieval}} {{which is expected}} to be effective for Web pages which contain more than one topic. To split document into passages, we used NLP based paragrah detecting program, not by fixed (variable) window size. But it did not produce better result for TREC 9 Web data. For indexing large web data faster, we developped two techiniques. One is multi-partional selective sorting for inversion which is about 10 - 30 % faster than normal <b>quick</b> <b>sorting</b> in sorting term-number, text-number pair. The other is compressed trie dictionary based stemming...|$|E
40|$|Recent {{molecular}} {{analyses revealed}} that many so-called “circum-Antarctic” benthic crustacean species {{appeared to be a}} complex of cryptic species with restricted distributions. In this study, we used DNA barcoding to detect possible cryptic diversity within the amphipod genus Orchomene s. lat. (superfamily Lysianassoidea) and to test the circumpolarity of some species belonging to this group. The analysis of mitochondrial cytochrome c oxidase I sequences indicated, in some species, a genetic homogeneity among specimens from remote sampling sites. Moreover, the eurybathy of some species was confirmed by a very low genetic diversity between specimens originating from shelf and abyssal depths. In other species, DNA barcoding revealed genetically divergent, cryptic taxa, occurring in sympatry. In addition, the DNA barcoding served as a tool for a <b>quick</b> <b>sorting</b> of the sample material into genetic units and enabled us to detect new species within the Orchomene complex. These results were confirmed by further analyses based on 28 S rRNA sequences...|$|E
50|$|If we {{limit the}} set of {{algorithms}} to a specific family (for instance, all deterministic choices for pivots in the <b>quick</b> <b>sort</b> algorithm), choosing an algorithm A from R is equivalent to running a randomized algorithm (for instance, running <b>quick</b> <b>sort</b> and randomly choosing the pivots at each step).|$|R
40|$|Sorting {{is among}} the first of algorithm, than any {{computer}} science student encounters during college and it is considered as a simple and well studied problem. With the advancement in parallel processing many parallel sorting algorithms have been investigated. These algorithms are designed {{for a variety of}} parallel computer architectures. In this paper, a comparative analysis of performance of three different types of sorting algorithms viz. sequential <b>quick</b> <b>sort,</b> parallel <b>quick</b> <b>sort</b> and hyperquicksort is presented. <b>Quick</b> <b>sort</b> is a divideand-conquer algorithm that sorts a sequence by recursively dividing it into smaller subsequences, and has Ө(nlogn) complexity for n data values. The comparative analysis is based on comparing average sorting times and speedup achieved in parallel <b>sorting</b> over sequential <b>quick</b> <b>sort</b> and comparing number of comparisons. The time complexity for each sorting algorithm will also be mentioned and analyzed...|$|R
40|$|<b>Quick</b> <b>sort</b> is a sorting {{algorithm}} whose {{worst case}} running time is &# 952;(n 2) on an input array of n numbers. It {{is the best}} practical for sorting because it {{has the advantage of}} sorting in place. Problem statement: Behavior of <b>quick</b> <b>sort</b> is complex, we proposed in-place 2 m threads parallel heap sort algorithm which had advantage in sorting in place and had better performance than classical sequential <b>quick</b> <b>sort</b> in running time. Approach: The algorithm consisted of several stages, in first stage; it splits input data into two partitions, next stages it did the same partitioning for prior stage which had been spitted until 2 m partitions was reached equal to the number of available processors, finally it used heap sort to sort respectively ordered of non internally sorted partitions in parallel. Results: Results showed the speed of algorithm about double speed of classical <b>Quick</b> <b>sort</b> for a large input size. The number of comparisons needed was reduced significantly. Conclusion: In this study we had been proposed a sorting algorithm that uses less number of comparisons with respect to original <b>quick</b> <b>sort</b> that in turn requires less running time to sort the same input data...|$|R
40|$|A novel {{algorithm}} is proposed for searching tolerant substrings from multiple DNA or protein sequences. To improve efficiencies in pattern searching, combination of hashing encoding, <b>quick</b> <b>sorting</b> and ladderlike stepping and/or interval jumping techniques are applied. Since multiple se-quence alignment of DNA or amino acid sequences from the giant genomic database is usually time consuming, we develop a three-phase methodology to search common substrings and reduce its time complexity for pattern matching. In the first coding phase, sequences are transformed into a nu-merical space set. Subsequently, the quick sort algorithms are employed in the second sorting stage to reorder the encoded data. In the last searching phase, ladderlike stepping and interval jumping rules are proposed to increase efficiencies of numerical comparison. In addition, two interval segmenta-tion techniques, uniform partition and bitwise partition are applied prior to interval jumping proce-dures. The segmenting methodologies are designed {{according to the length}} of searching pattern, and the proposed interval jumping searching algorithms provide robust and improved performance. Keyword?Hash coding?Ladderlike Stepping?Interval Jumping? Pattern Matching 1...|$|E
40|$|Although soft {{morphological}} filters give excellent {{performance in}} restoring corrupted images, they {{are hard to}} design. One successful approach is to use iterative search methods such as genetic algorithms. However, they can be computationally exhaustive taking weeks to converge. Parallel genetic algorithms give massive reductions in processing time, but their performance when implemented on multiprocessor systems or cluster of workstations has a heavy dependence {{on the number of}} servers. Also, they require some sort of synchronisation between the working processors. Because of their parallel capabilities, field programmable gate arrays (FPGAs) provide a cheap and efficient way to implement parallel algorithms. In this paper, a realisation of the genetic algorithm on an FPGA is introduced. The genetic algorithm is given the task of optimising a grey-scale soft morphological filter. The soft morphological filter is implemented using a parallel <b>quick</b> <b>sorting</b> algorithm. The results of synthesizing and simulating the design on SPARTAN II FPGA resulted in a good utilisation of the device resources. Also, the optimisation process was performed in a very short time. In contrast with the standard parallel genetic algorithms, the design does not require any synchronisation or management schemes...|$|E
40|$|A few {{aspects of}} {{aircraft}} noise were evaluated. These were (i) methods of subjective evaluations, (ii) effects of equalization and (iii) {{the effects of}} cognitive aspects. In the first paper, sorting algorithms were used instead of conventional paired comparison method {{in order to reduce}} the number of pairs in the evaluation of subjective judgments. The <b>quick</b> <b>sorting</b> algorithm method revealed more than 99 % correlation coefficient with paired comparison method although the method used N*log(N) evaluations instead of N(N- 1) / 2. In the second paper, equalization effects on perception were evaluated in two steps, first with stationary aircraft sounds and second with non-stationary aircraft sounds. The first experiment examined the effects of stationary sound segments respect to three different angle positions of the aircrafts relative to the observer (78. 7 °, 90 ° & 101. 3 °), two different SNR conditions (sounds having original broadband plus tonal components versus control broadband sounds having no tonal components) and two different flight conditions (arrival and take-off). Subjects were asked to scale five perceptual attributes (loudness, annoyance, hardness, power and pitch) using Borg CR 100 scale. The angle condition showed highly significant effects on annoyance and hardness. Maximal effects were found at an angle of 78. 7 °. The SNR revealed a significant impact on loudness, power and pitch. The second experiment analyzed the effects of tonal components and the problem of appropriate equalization. The spectrum of the signals was modified in two steps (buzz-saw, isolated BPF tone). Further EPNL-equalization, A-, B-, C-, D- and spectral broadband equalizations were applied to the synthesized sounds. Annoyance, loudness, hardness and pitch in the isolated tone conditions showed significantly stronger effects than the buzz-conditions on the perceived judgments. The EPNL-equalization led to a lower degree of differentiation between the spectral conditions compared to B- and C-level equalization. In the third paper, the effects of aircraft sounds on children’s cognitive performance were investigated. Impact of aircraft noise on children cognition was found significantly higher in reading comprehensions than in basic mathematics and problem solving tests. It seems children are very sensitive to the modifications in the aircraft noise but further studies are necessary to compliment such a finding...|$|E
2500|$|An {{implementation}} of an algorithm similar to <b>quick</b> <b>sort</b> over lists, {{where the first}} element is taken as the pivot: ...|$|R
40|$|This paper {{describes}} {{in detail the}} bitonic sort algorithm,and implements the bitonic sort algorithm based on cuda architecture. At the same time,we conduct two effective optimization of implementation details according to {{the characteristics of the}} GPU,which greatly improve the efficiency. Finally,we survey the optimized Bitonic sort algorithm on the GPU with the speedup of <b>quick</b> <b>sort</b> algorithm on the CPU. Since <b>Quick</b> <b>Sort</b> is not suitable to be implemented in parallel,but it is more efficient than other sorting algorithms on CPU to some extend. Hence,to see the speedup and performance,we compare bitonic sort on GPU with <b>quick</b> <b>Sort</b> on CPU. For a series of 32 -bit random integer,the experimental results show that the acceleration of our work is nearly 20 times. When array size is about 216,the speedup ratio is even up to 30...|$|R
5000|$|This is an {{implementation}} of an algorithm similar to <b>quick</b> <b>sort</b> over lists, {{in which the}} first element is taken as the pivot: ...|$|R
40|$|Purpose. The article {{highlights}} {{the question about}} creation the complex numerical models in order to calculate the ions concentration fields in premises of various purpose and in work areas. Developed complex should {{take into account the}} main physical factors influencing the formation of the concentration field of ions, that is, aerodynamics of air jets in the room, presence of furniture, equipment, placement of ventilation holes, ventilation mode, location of ionization sources, transfer of ions under the electric field effect, other factors, determining the intensity and shape of the field of concentration of ions. In addition, complex of numerical models has to ensure conducting of the express calculation of the ions concentration in the premises, allowing <b>quick</b> <b>sorting</b> of possible variants and enabling «enlarged» evaluation of air ions concentration in the premises. Methodology. The complex numerical models to calculate air ion regime in the premises is developed. CFD numerical model is based on the use of aerodynamics, electrostatics and mass transfer equations, and takes into account the effect of air flows caused by the ventilation operation, diffusion, electric field effects, as well as the interaction of different polarities ions {{with each other and with}} the dust particles. The proposed balance model for computation of air ion regime indoors allows operative calculating the ions concentration field considering pulsed operation of the ionizer. Findings. The calculated data are received, on the basis of which one can estimate the ions concentration anywhere in the premises with artificial air ionization. An example of calculating the negative ions concentration on the basis of the CFD numerical model in the premises with reengineering transformations is given. On the basis of the developed balance model the air ions concentration in the room volume was calculated. Originality. Results of the air ion regime computation in premise, which is based on numerical 2 D CFD model and balance model, are presented. Practical value. A numerical CFD model and balance model for the computation of air ion regime allow calculating the ions concentration in the premises in the conditions of artificial air ionization taking into account the main physical factors determining the formation of ions concentration fields...|$|E
40|$|The Lawrence Livermore National Laboratory uses neutron {{activation}} {{elements in a}} Panasonic TLD holder as a personnel nuclear accident dosimeter (PNAD). The LLNL PNAD has periodically been tested using a Cf- 252 neutron source, however until 2009, {{it was more than}} 25 years since the PNAD has been tested against a source of neutrons that arise from a reactor generated neutron spectrum that simulates a criticality. In October 2009, LLNL participated in an intercomparison of nuclear accident dosimeters at the CEA Valduc Silene reactor (Hickman, et. al. 2010). In September 2010, LLNL participated in a second intercomparison of nuclear accident dosimeters at CEA Valduc. The reactor generated neutron irradiations for the 2010 exercise were performed at the Caliban reactor. The Caliban results are described in this report. The procedure for measuring the nuclear accident dosimeters {{in the event of an}} accident has a solid foundation based on many experimental results and comparisons. The entire process, from receiving the activated NADs to collecting and storing them after counting was executed successfully in a field based operation. Under normal conditions at LLNL, detectors are ready and available 24 / 7 to perform the necessary measurement of nuclear accident components. Likewise LLNL maintains processing laboratories that are separated from the areas where measurements occur, but contained within the same facility for easy movement from processing area to measurement area. In the event of a loss of LLNL permanent facilities, the Caliban and previous Silene exercises have demonstrated that LLNL can establish field operations that will very good nuclear accident dosimetry results. There are still several aspects of LLNL's nuclear accident dosimetry program that have not been tested or confirmed. For instance, LLNL's method for using of biological samples (blood and hair) has not been verified since the method was first developed in the 1980 's. Because LLNL and the other DOE participants were limited in what they were allowed to do at the Caliban and Silene exercises and testing of various elements of the nuclear accident dosimetry programs cannot always be performed as guests at other sites, it has become evident that DOE needs its own capability to test nuclear accident dosimeters. Angular dependence determination and correction factors for NADs desperately need testing as well as more evaluation regarding the correct determination of gamma doses. It will be critical to properly design any testing facility so that the necessary experiments can be performed by DOE laboratories as well as guest laboratories. Alternate methods of dose assessment such as using various metals commonly found in pockets and clothing have yet to be evaluated. The DOE is planning to utilize the Godiva or Flattop reactor for testing nuclear accident dosimeters. LLNL has been assigned the primary operational authority for such testing. Proper testing of nuclear accident dosimeters will require highly specific characterization of the pulse fields. Just as important as the characterization of the pulsed fields will be the design of facilities used to process the NADs. Appropriate facilities will be needed to allow for early access to dosimeters to test and develop <b>quick</b> <b>sorting</b> techniques. These facilities will need appropriate laboratory preparation space and an area for measurements. Finally, such a facility will allow greater numbers of LLNL and DOE laboratory personnel to train on the processing and interpretation of nuclear accident dosimeters and results. Until this facility is fully operational for test purposes, DOE laboratories may need to continue periodic testing as guests of other reactor facilities such as Silene and Caliban...|$|E
40|$|<b>Quick</b> <b>sort</b> {{algorithm}} is studied thoroughly, a new incomplete <b>quick</b> <b>sort</b> {{algorithm is}} designed, {{and then a}} novel image median filtering algorithm based on incomplete <b>quick</b> <b>sort</b> algorithm is proposed to improve the filtering speed. The new algorithm considers in detail the characteristic of image median filtering (In median filtering algorithm, the sorting operation of all pixel values is not necessary, the median value can be given by many other methods), and can give the median value by only sorting part of the pixels value in the neighborhood, thus it can reduce many data move operations, and then greatly improve the speed of image median filtering. Algorithm analysis {{and a lot of}} experiment results show that, the new algorithm greatly improves the speed of image median filtering, and can keep the edge, outline, texture and much other information to a great extent...|$|R
40|$|There {{are several}} sorting algorithms. We can {{implement}} several sorting algorithms {{such as the}} insertion sort, the binary-insertion <b>sort,</b> <b>quick</b> <b>sort,</b> heap sort and merge sort. Moreover, we can compare the efficiency of each algorithm by measuring the running time and counting the numbers of comparisons...|$|R
40|$|We {{investigate}} {{the average number}} of moves made by Quick Select (a variant of <b>Quick</b> <b>Sort</b> for finding order statistics) to find an element with a randomly selected rank. This kind of grand average provides smoothing over all individual cases of a specific fixed order statistic. The variance of the number of moves involves intricate dependencies, and we only give reasonably tight bounds. 1 <b>Quick</b> Select. <b>Quick</b> <b>Sort</b> is a well known fast algorithm for data sorting. It was invented by Hoare (see [3]); see also [5] and [7]. <b>Quick</b> <b>Sort</b> is the default sorting scheme in some operating systems, such as UNIX. The algorithm is two-sided: It puts a pivot element in its correct position, and arranges the data in two groups relative to that pivot. Elements below the pivot go in one group, the rest are placed in the other group. The two groups are then handled recursively. The one-sided version (Quick Select) of the algorithm is popular for identifying order statistics. This one-sided version of <b>Quick</b> <b>Sort</b> to search for order statistics is also known as Hoare’s Find algorithm, which was first given in [2]. To find a certain order statistic, such as the first quartile, Quick Select proceeds with the partition stage, just as in <b>Quick</b> <b>Sort,</b> to place a pivot in its correct position, and creates the two groups on {{the two sides of the}} pivot. But then, the algorithm decides if the pivot is the sought order statistic or not. If it is, the algorithm terminates (announcing the pivot to be the sought order statistic), and if not it recursively pursues only a group on one side where the order statistic resides...|$|R
40|$|Sorting {{is one of}} {{the most}} {{important}} operations in database systems and its efficiency can influences drastically the overall system performance. To accelerate the performance of database systems, parallelism is applied to the execution of the data administration operations. We propose a new deterministic Parallel Sorting Algorithm (DPSA) that improves the performance of <b>Quick</b> <b>sort</b> in sorting an array of size n. where we use p Processor Elements (PE) that work in parallel to sort a matrix r*c where r is the number of rows r = 3 and c is the number of columns c = n/ 3. The simulation results show that the performance of the proposed algorithm DPSA out performs <b>Quick</b> <b>sort</b> when it works sequentially...|$|R
40|$|Today most {{computer}} have a {{multicore processor}} and are depending on parallel execution {{to be able}} to keep up with the demanding tasks that exist today, that forces developers to write software that can take advantage of multicore systems. There are multiple programming languages and frameworks that makes it possible to execute the code in parallel on different threads, this study looks at the performance and effort required to work with two of the frameworks that are available to the C programming language, POSIX Threads(Pthreads) and OpenMP. The performance is measured by paralleling three algorithms, Matrix multiplication, <b>Quick</b> <b>Sort</b> and calculation of the Mandelbrot set using both Pthreads and OpenMP, and comparing first against a sequential version and then the parallel version against each other. The effort required to modify the sequential program using OpenMP and Pthreads is measured in number of lines the final source code has. The results shows that OpenMP does perform better than Pthreads in Matrix Multiplication and Mandelbrot set calculation but not on <b>Quick</b> <b>Sort</b> because OpenMP has problem with recursion and Pthreads does not. OpenMP wins the effort required on all the tests but because there is a large performance difference between OpenMP and Pthreads on <b>Quick</b> <b>Sort</b> OpenMP cannot be recommended for paralleling <b>Quick</b> <b>Sort</b> or other recursive programs. ...|$|R
40|$|Sundararajan and Chakraborty (2007) {{introduced}} {{a new version of}} <b>Quick</b> <b>sort</b> removing the interchanges. Khreisat (2007) found this algorithm to be competing well with some other versions of <b>Quick</b> <b>sort.</b> However, it uses an auxiliary array thereby increasing the space complexity. Here, we provide a second version of our new sort where we have removed the auxiliary array. This second improved version of the algorithm, which we call K-sort, is found to sort elements faster than Heap sort for an appreciably large array size (n <= 70, 00, 000) for uniform U[0, 1] inputs. Comment: 9 pages, 9 figure...|$|R
40|$|Abstract- Sundararajan and Chakraborty [10] {{introduced}} {{a new version of}} <b>Quick</b> <b>sort</b> removing the interchanges. Khreisat [6] found this algorithm to be competing well with some other versions of <b>Quick</b> <b>sort.</b> However, it uses an auxiliary array thereby increasing the space complexity. Here, we provide a second version of our new sort where we have removed the auxiliary array. This second improved version of the algorithm, which we call K-sort, is found to sort elements faster than Heap sort for an appreciably large array size (n � � 70, 00, 000) for uniform U[0, 1] inputs. Index Terms- Internal sorting, uniform distribution, average time complexity, statistical analysis, statistical bound. I...|$|R
40|$|Recently {{the lower}} bound for integer sorting has {{considerably}} improved and achieved with comparison sorting to [1] for a deterministic algorithms or to for a radix sort algorithm in space that depends {{only on the}} number of input integers. Andersson et al. [2] presented signature sort in the expected linear time and space which gives very bad performance than randomized <b>quick</b> <b>sort.</b> We earlier presented in [14] that performance of signature sort can be enhanced using hashing and bitwise operators. This paper gives the implementation of that idea and later we have compared the performance of algorithm with existing randomized signature <b>sort</b> and randomized <b>quick</b> <b>Sort...</b>|$|R
40|$|Smart Sort {{algorithm}} is a "smart" fusion of heap construction procedures (of Heap sort algorithm) into the conventional "Partition" function (of <b>Quick</b> <b>sort</b> algorithm) {{resulting in a}} robust version of <b>Quick</b> <b>sort</b> algorithm. We have also performed empirical analysis of average case behavior of our proposed algorithm along with the necessary theoretical analysis for best and worst cases. Its performance was checked against some standard probability distributions, both uniform and non-uniform, like Binomial, Poisson, Discrete & Continuous Uniform, Exponential, and Standard Normal. The analysis exhibited the desired robustness coupled with excellent performance of our algorithm. Although this paper assumes the static partition ratios, its dynamic version is expected to yield still better results...|$|R
5000|$|Line 3: Sorts in {{increasing}} order of finish times {{the array of}} activities [...] by using the finish times stored in the array [...] This operation {{can be done in}} [...] time, using for example merge sort, heap <b>sort,</b> or <b>quick</b> <b>sort</b> algorithms.|$|R
5000|$|<b>Quick</b> <b>sort</b> : Partition {{the array}} into two segments. In the first segment, all {{elements}} {{are less than}} or equal to the pivot value. In the second segment, all elements are greater than or equal to the pivot value. Finally, sort the two segments recursively.|$|R
40|$|We {{show the}} {{importance}} of sequential sorting {{in the context of}} in memory parallel sorting of large data sets of 64 bit keys. First, we analyze several sequential strategies like Straight Insertion, <b>Quick</b> <b>sort,</b> Radix sort and CC-Radix sort. As a consequence of the analysis, we propose a new algorithm that we call Sequential Counting Split Radix sort, SCS-Radix sort. SCS-Radix sort is a combination of some of the algorithms analyzed and other new ideas. There are three important contributions in SCS-Radix sort. First, the work saved by detecting data skew dynamically. Second, the exploitation of the memory hierarchy done by the algorithm. Third, the execution time stability of SCS-Radix when sorting data sets with different characteristics. We evaluate the use of SCS-Radix sort {{in the context of a}} parallel sorting algorithm on an SGI Origin 2000. The parallel algorithm is from 1 : 2 to 45 times faster using SCS-Radix sort than using Radix <b>sort</b> or <b>Quick</b> <b>sort.</b> ...|$|R
40|$|The study {{presents}} a {{comparative study of}} some sorting algorithm with the aim {{to come up with}} the most efficient sorting algorithm. The methodology used was to evaluate the performance of median, heap, and <b>quick</b> <b>sort</b> techniques using CPU time and memory space as performance index. This was achieved by reviewing literatures of relevant works. We also formulated architectural model which serves as guideline for implementing and evaluating the sorting techniques. The techniques were implemented with C-language; while the profile of each technique was obtained with G-profiler. The results obtained show that in majority of the cases considered, heap sort technique is faster and requires less space than median and <b>quick</b> <b>sort</b> algorithms in sorting data of any input data size. Similarly, results also show that the slowest technique of the three is median sort; while <b>quick</b> <b>sort</b> technique is faster and requires less memory than median sort, but slower and requires more memory than heap sort. The number of sorting algorithms considered in this study for complexity measurement is limited to bubble, insertion, and selection sorting. Future effort will investigate complexities of other sorting techniques in the literature based on CPU time and memory space. The goal for this will be to adopt the most efficient sorting technique in the development of job scheduler for grid computing community...|$|R
40|$|Sorting is {{considered}} as a fundamental operation in computer science as it is used as an intermediate step in many operations. Sorting refers to the operation of arranging data in some given order such as increasing or decreasing, with numerical data, or alphabetically, with character data. An algorithm is any well-defined procedure or set of instructions, that takes some input {{in the form of}} some values, processes them and gives some values as output. Selection of best sorting algorithm for a particular problem depends upon problem definition. Merge Sort is the first that scales well to very large lists, because its worst-case running time is O(n log n). Together with its modest O(log n) space usage, <b>Quick</b> <b>Sort</b> {{is one of the most}} popular sorting algorithms and is available in many standard programming libraries. The most complex issue in <b>Quick</b> <b>Sort</b> is choosing a good pivot element; consistently poor choices of pivots can result in drastically slower O(n) performance, if at each step the median is chosen as the pivot then the algorithm works in O(n log n). Finding the median however, is an O(n) operation on unsorted lists and therefore exacts its own penalty with sorting. So, we are generating a new sorting algorithm while mixing the two sorting (Merge <b>Sort</b> and <b>Quick</b> <b>Sort)</b> to for best results...|$|R
40|$|We {{establish}} asymptotic expansions for factorial {{moments of}} following distributions: {{number of cycles}} in a random permutation, number of inversions in a random permutation, and number of comparisons used by the randomized <b>quick</b> <b>sort</b> algorithm. To achieve this we use singularity analysis of certain type of generating functions due to Flajolet and Odlyzko...|$|R
40|$|Sorting is an {{important}} and widely studied issue, where the execution time and the required resources for computation is of extreme importance, especially if it is dealing with real-time data processing. Therefore, {{it is important to}} study and to compare in details all the available sorting algorithms. In this project, an intensive investigation was conducted on five algorithms, namely, Bubble Sort, Insertion Sort, Selection Sort, Merge <b>Sort</b> and <b>Quick</b> <b>Sort</b> algorithms. Four groups of data elements were created for the purpose of comparison process among the different sorting algorithms. All the five sorting algorithms are applied to these groups. The worst time complexity for each sorting technique is then computed for each sorting algorithm. The sorting algorithms were classified into two groups of time complexity, O (n 2) group and O(nlog 2 n) group. The execution time for the five sorting algorithms of each group of data elements were computed. The fastest algorithm is then determined by the estimated value for each sorting algorithm, which is computed using linear least square regression. The results revealed that the Merge Sort was more efficient to sort data from the <b>Quick</b> <b>Sort</b> for O(nlog 2 n) time complexity group. The Insertion Sort had more efficiency to sort data from Selection Sort and Bubble Sort for O (n 2) group. Bubble Sort was the slowest or it was less efficient to sort the data. In conclusion, the efficiency of sorting algorithms can be ranked from highest to lowest as Merge <b>Sort,</b> <b>Quick</b> <b>Sort,</b> Insertion Sort, Selection Sort and Bubble Sort...|$|R
