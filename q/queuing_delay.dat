646|1204|Public
50|$|The {{algorithm}} is independently computed at each network hop. The algorithm operates over an interval, initially 100 milliseconds. Per-packet <b>queuing</b> <b>delay</b> is monitored through the hop. As each packet is dequeued for forwarding, the <b>queuing</b> <b>delay</b> (amount {{of time the}} packet spent waiting in the queue) is calculated. The lowest <b>queuing</b> <b>delay</b> for the interval is stored. When the last packet of the interval is dequeued, if the lowest <b>queuing</b> <b>delay</b> for the interval is greater than 5 milliseconds, this single packet is dropped and the interval used for the next group of packets is shortened. If the lowest <b>queuing</b> <b>delay</b> for the interval is less than 5 milliseconds, the packet is forwarded and the interval is reset to 100 milliseconds.|$|E
5000|$|In {{telecommunication}} {{and computer}} engineering, the <b>queuing</b> <b>delay</b> or queueing delay {{is the time}} a job waits in a queue until it can be executed. It is {{a key component of}} network delay. In a switched network, <b>queuing</b> <b>delay</b> is the time between the completion of signaling by the call originator and the arrival of a ringing signal at the call receiver. Queues may be caused by delays at the originating switch, intermediate switches, or the call receiver servicing switch. In a data network, <b>queuing</b> <b>delay</b> is the sum of the delays between the request for service and the establishment of a circuit to the called data terminal equipment (DTE). In a packet-switched network, <b>queuing</b> <b>delay</b> is the sum of the delays encountered by a packet between the time of insertion into the network and the time of delivery to the addressee.|$|E
50|$|In Kendall's notation, the M/M/1/K queuing model, where K is {{the size}} of the buffer, may be used to analyze the <b>queuing</b> <b>delay</b> in a {{specific}} system. Kendall's notation should be used to calculate the <b>queuing</b> <b>delay</b> when packets are dropped from the queue. The M/M/1/K queuing model is the most basic and important queuing model for network analysis.|$|E
30|$|For {{multimedia}} streaming applications, steady <b>queue</b> <b>delay</b> and jitter are {{as important}} as short <b>queue</b> <b>delay.</b> Steady <b>queue</b> <b>delay</b> and jitter let real-time data to be transmitted smoothly [17, 18]. Since the flow in the error-free channel have to compensate other flows if need be, this increases the <b>queue</b> <b>delay.</b> When there is no need of compensation, the <b>queue</b> <b>delay</b> of the flow will decrease immediately. Hence, whenever the state of flows switch between compensation and normal state, the <b>queue</b> <b>delay</b> varies dramatically in this period of time. This phenomena result in unsteady <b>queue</b> <b>delay</b> and jitter for both flows in the error-free and error channels. It is unavoidable for the flows in the error channel since those flows have to get additional slots to transmit data immediately to reduce the average delay time and maintain the fairness, but it should be avoided for the flows in the error-free channel {{to improve the quality of}} service for multimedia streaming applications.|$|R
30|$|The mean <b>queue</b> <b>delay</b> of SBFA is 0.272 s {{which is}} the highest among the three algorithms. There is an 81 % {{difference}} between the instantaneous and mean <b>queue</b> <b>delays</b> within 0.01 s. Because SBFA algorithm reserves the portion of shared bandwidth and uses a fractional bandwidth for compensation, the weights of flows perceiving error-free channel are beaten and the flows experience a longer <b>queue</b> <b>delay</b> than those in other algorithms when the system compensates lagging flows. The major reason for unsteady <b>queue</b> <b>delay</b> and jitter of SBFA is that lagging flows have higher priority than leading and sync flows to let the system serve for lagging flows first. The mean <b>queue</b> <b>delay</b> of CIFQ is 0.25 s {{which is the}} lowest among the three algorithms, but {{there is only a}} 77 % difference between the instantaneous and mean of <b>queue</b> <b>delays</b> within 0.01 s. CIFQ provides shorter <b>queue</b> <b>delay</b> while it compensates the lagging flow, but it provides longer delay when the flow experiences burst in the error channel to achieve a graceful degradation for the flows in the error-free channel. Flows preserving error-free channel can get additional slots from lagging flows to forward its packets. This feature allows the average <b>queue</b> <b>delay</b> of the CIFQ algorithm to be the shortest among the three algorithms for the flow in the error-free channel.|$|R
50|$|Note: we have {{neglected}} <b>queuing</b> <b>delays.</b>|$|R
50|$|Like FAST TCP and TCP Vegas, Compound TCP uses {{estimates}} of <b>queuing</b> <b>delay</b> {{as a measure}} of congestion; if the <b>queuing</b> <b>delay</b> is small, it assumes that no links on its path are congested, and rapidly increases its rate. However, unlike FAST and Vegas, it does not seek to maintain a constant number of packets queued.|$|E
5000|$|Limit the <b>queuing</b> <b>delay</b> {{it adds to}} that {{induced by}} other traffic, and ...|$|E
50|$|While {{throughput}} declines gradually as self-similarity increases, <b>queuing</b> <b>delay</b> increases more drastically. When {{traffic is}} self-similar, {{we find that}} <b>queuing</b> <b>delay</b> grows proportionally to the buffer capacity present in the system. Taken together, these two observations have potentially dire implications for QoS provisions in networks. To achieve a constant level of throughput or packet loss as self-similarity is increased, extremely large buffer capacity is needed. However, increased buffering leads to large queuing delays and thus self-similarity significantly steepens the trade-off curve between throughput/ packet loss and delay.|$|E
40|$|This thesis {{explores the}} effect of {{intersection}} skew angle on average <b>queue</b> <b>delay</b> through simulation. The simulation model used is the TEXAS (Traffic Experimental and Analytical Simulation) model. This microscopic simulation model uses a general nonlinear car-following model. It simulates individual intersections and was designed to capture the interaction of traffic operations and intersection geometry. Simulation models were developed for three stop-controlled, tee-intersections in Lincoln, Nebraska. Field data to develop and calibrate the simulation models were collected. All simulation models were calibrated by adjusting the car following parameters. An experimental design was developed to test {{the effect of}} skew angle on average <b>queue</b> <b>delay.</b> Skew angles from 1 degree to 30 degrees were evaluated. The average <b>queue</b> <b>delay</b> reported for each skew angle is based on 30 runs of the simulation model. The results indicate that skew angle does affect average <b>queue</b> <b>delay.</b> The results also suggest that the TEXAS model can capture the effect of skew on average <b>queue</b> <b>delay</b> for small skew angles of 1 degree from the base conditions. Advisor: Elizabeth G. Jone...|$|R
30|$|The mean <b>queue</b> <b>delay</b> of CSCPS is 0.262 s. There is a 99 % {{difference}} between the instantaneous and mean <b>queue</b> <b>delays</b> within 0.01 s which is the highest percentage among three algorithms. This shows that CSCPS controls the <b>queue</b> <b>delay</b> in a tiny region without a dramatic variation since CSCPS takes the channel condition into account of the compensation scheme. If the flow suffers from errors or a leading flow is selected to forward its packets, the lagging flow with maximum service tag is compensated first. If there are no backlogged lagging flows in the system, the system serves the flow with minimum service tag. In other words, leading flows can release their resource to lagging flows quickly if lagging flows exist. Therefore, flows preserving error-free channel have a constant rate to forward their packets to achieve steady <b>queue</b> <b>delay</b> and jitter.|$|R
40|$|International {{audience}} We {{propose a}} methodology {{to gauge the}} extent of <b>queueing</b> <b>delay</b> (aka bufferbloat) in the Internet, based on purely passive measurement of TCP traffic. We implement our methodology in Tstat and make it available as open source software. We leverage Deep Packet Inspection (DPI) and behavioral classification of Tstat to breakdown the <b>queueing</b> <b>delay</b> across different applications, in order to evaluate the impact of bufferbloat on user experience. We {{show that there is}} no correlation between the ISP traffic load and the <b>queueing</b> <b>delay,</b> thus confirming that bufferbloat is related only to the traffic of each single user (or household). Finally, we use frequent itemset mining techniques to associate the amount of <b>queueing</b> <b>delay</b> seen by each host with the set of its active applications, with the goal of investigating the root cause of bufferbloat. </p...|$|R
5000|$|TCP-Illinois is a loss-delay based algorithm, {{which uses}} packet loss {{as the primary}} {{congestion}} signal to determine the direction of window size change, and uses <b>queuing</b> <b>delay</b> as the secondary congestion signal to adjust the pace of window size change. Similarly to the standard TCP, TCP-Illinois increases the window size W by [...] for each acknowledgment, and decreases [...] by [...] for each loss event. Unlike the standard TCP, [...] and [...] are not constants. Instead, they are functions of average <b>queuing</b> <b>delay</b> : , where [...] is decreasing and [...] is increasing.|$|E
5000|$|TCP Vegas - {{estimates}} the <b>queuing</b> <b>delay,</b> and linearly increases or decreases the window {{so that a}} constant number of packets per flow are queued in the network. Vegas implements proportional fairness.|$|E
5000|$|We let [...] and [...] be {{continuous}} {{functions and}} thus , [...] and [...] Suppose [...] is the maximum average <b>queuing</b> <b>delay</b> and we denote , then {{we also have}} [...] From these conditions, we have ...|$|E
40|$|Abstract—We {{propose a}} {{methodology}} {{to gauge the}} extent of <b>queueing</b> <b>delay</b> (aka bufferbloat) in the Internet, based on purely passive measurement of TCP traffic. We implement our methodology in Tstat and make it available as open source software. We leverage Deep Packet Inspection (DPI) and behav-ioral classification of Tstat to breakdown the <b>queueing</b> <b>delay</b> across different applications, in order to evaluate the impact of bufferbloat on user experience. We {{show that there is}} no correlation between the ISP traffic load and the <b>queueing</b> <b>delay,</b> thus confirming that bufferbloat is related only to the traffic of each single user (or household). Finally, we use frequent itemset mining techniques to associate the amount of <b>queueing</b> <b>delay</b> seen by each host with the set of its active applications, with the goal of investigating the root cause of bufferbloat. Keywords—Bufferbloat; Queueing delay; Network monitoring; Passive traffic monitoring...|$|R
3000|$|Based on {{the above}} discussion, if the server is {{inactive}} in time slot x∈{ 1,…,N}, it adds one slot to the <b>queueing</b> <b>delay</b> (and the overall waiting time) of every packet arrived in slot x or afterward. In general, the packet which arrived in time slot t will experience a <b>queueing</b> <b>delay</b> of n [...]...|$|R
30|$|In this simulation, {{we present}} the {{long-term}} fairness of CSCPS algorithm by showing the average <b>queue</b> <b>delay</b> of flows {{which is only}} affected by its weight even in the error channel. Three 1.5 Mbps UDP/CBR traffic flows are generated, and they last for 100 s. The weight of CBR 1, CBR 2, and CBR 3 are 5, 3, and 2, respectively. A two-state channel model is applied. If a flow predicts that the slot suffers from errors, it will relinquish its slot to another flow with clean channel to transmit packets. Under this circumstance, the packet in queue of the flow with dirty channel will receive a longer <b>queue</b> <b>delay.</b> Therefore, the flow with clean channel receives additional service and its <b>queue</b> <b>delay</b> becomes shorter. Consequently, errors may cause unsteady <b>queue</b> <b>delay</b> for flows, but they still satisfy the delay guarantees for the flows in the error system.|$|R
5000|$|When the {{interval}} is shortened, {{it is done}} so {{in accordance with the}} inverse square root of the number of successive intervals in which packets were dropped due to excessive <b>queuing</b> <b>delay.</b> The sequence of intervals is , , , , ...|$|E
50|$|Both of {{the above}} {{implementations}} aim to limit the network <b>queuing</b> <b>delay</b> to 100ms. This is the maximum allowed for by the standardized protocol. If one used a lower value, {{then it would be}} starved when the other was in use.|$|E
5000|$|A traffic {{intensity}} greater than one erlang {{means that the}} rate at which bits arrive exceeds the rate bits can be transmitted and <b>queuing</b> <b>delay</b> will grow without bound (if the {{traffic intensity}} stays the same). If the traffic intensity is less than one erlang, then the router can handle more average traffic.|$|E
40|$|In {{this paper}} we show {{how to go}} beyond the study of the topological {{properties}} of the Internet, by measuring its dynamical state using special active probing techniques and the methods of network tomography. We demonstrate this approach by measuring the key state parameters of Internet paths, the characteristics of <b>queueing</b> <b>delay,</b> {{in a part of the}} European Internet. In the paper we describe in detail the ETOMIC measurement platform that was used to conduct the experiments, and the applied method of <b>queueing</b> <b>delay</b> tomography. The main results of the paper are maps showing various spatial structure in the characteristics of <b>queueing</b> <b>delay</b> corresponding to the resolved part of the European Internet. These maps reveal that the average <b>queueing</b> <b>delay</b> of network segments spans more than two orders of magnitude, and that the distribution of this quantity is very well fitted by the log-normal distribution...|$|R
30|$|Obviously, the {{bottleneck}} {{for this}} topology {{is located at}} wireless channels. Therefore, the packet scheduling algorithm on BS {{plays a key role}} for the network's performance. The measurement metrics include instantaneous and average <b>queue</b> <b>delay,</b> throughput, and jitter on BS since it directly reflects the properties and performance of fairness, delay bound, throughput guarantees, and steady <b>queue</b> <b>delay</b> and jitter.|$|R
40|$|With {{the rapid}} growth of data centers, {{minimizing}} the <b>queueing</b> <b>delay</b> at network switches {{has been one of the}} key challenges. In this work, we analyze the shortcomings of the current TCP algorithm when used in data center networks, and we propose to use latencybased congestion detection and rate-based transfer to achieve ultralow <b>queueing</b> <b>delay</b> in data centers. Categories and Subject Descriptor...|$|R
50|$|During {{processing}} of a packet, routers may check for bit-level {{errors in the}} packet that occurred during transmission as well as determining where the packet's next destination is. Processing delays in high-speed routers are typically {{on the order of}} microseconds or less. After this nodal processing, the router directs the packet to the queue where further delay can happen (<b>queuing</b> <b>delay).</b>|$|E
5000|$|A <b>queuing</b> <b>delay</b> {{induced by}} several such data packets might exceed {{the figure of}} 7.8 ms several times over, in {{addition}} to any packet generation delay in the shorter speech packet. This was clearly unacceptable for speech traffic, which needs to have low jitter in the data stream being fed into the codec {{if it is to}} produce good-quality sound. A packet voice system can produce this low jitter in a number of ways: ...|$|E
5000|$|For {{the mean}} queue length to be finite it is {{necessary}} that [...] as otherwise jobs arrive faster than they leave the queue. [...] "Traffic intensity," [...] ranges between 0 and 1, and is the mean fraction of time that the server is busy. If the arrival rate [...] is {{greater than or equal to}} the service rate , the <b>queuing</b> <b>delay</b> becomes infinite. The variance term enters the expression due to Feller's paradox.|$|E
40|$|We {{measure and}} analyze the single-hop packet delay through {{operational}} routers in a backbone IP network. First we present our delay measurements through a single router. Then we identify stepby-step the factors contributing to single-hop delay. In addition to packet processing, transmission, and <b>queueing</b> <b>delays,</b> we identify the presence of very large delays due to non-work-conserving router behavior. We use a simple output queue model to separate those delay components. Our step-by-step methodology used to obtain the pure <b>queueing</b> <b>delay</b> is easily applicable to any single-hop delay measurements. After obtaining the <b>queueing</b> <b>delay,</b> we analyze the tail of its distribution, and find that it is long tailed and fits a Weibull distribution with the scale parameter, � � ��, and the shape parameter, � � �� � to ��. The measured average <b>queueing</b> <b>delay</b> is larger than predicted by M/M/ 1, M/G/ 1, and FBM models when the link utilization is below 70 %, but its absolute value is quite small. I...|$|R
40|$|In this paper, {{we propose}} a {{framework}} for cross-layer optimization to ensure ultra-high reliability and ultra-low latency in radio access networks, where both transmission <b>delay</b> and <b>queueing</b> <b>delay</b> are considered. With short transmission time, the blocklength of channel codes is finite, and the Shannon Capacity cannot be used to characterize the maximal achievable rate with given transmission error probability. With randomly arrived packets, some packets may violate the <b>queueing</b> <b>delay.</b> Moreover, since the <b>queueing</b> <b>delay</b> is shorter than the channel coherence time in typical scenarios, the required transmit power to guarantee the <b>queueing</b> <b>delay</b> and transmission error probability will become unbounded even with spatial diversity. To ensure the required quality-of-service (QoS) with finite transmit power, a proactive packet dropping mechanism is introduced. Then, the overall packet loss probability includes transmission error probability, <b>queueing</b> <b>delay</b> violation probability, and packet dropping probability. We optimize the packet dropping policy, power allocation policy, and bandwidth allocation policy to minimize the transmit power under the QoS constraint. The optimal solution is obtained, which depends on both channel and queue state information. Simulation and numerical results validate our analysis, and show that setting packet loss probabilities equal is a near optimal solution. Comment: The manuscript has been accepted by IEEE transactions on wireless communication...|$|R
30|$|Based on {{the above}} discussion, {{we make the}} {{conclusion}} as follows. Although buffer-aided relaying results in <b>queueing</b> <b>delay</b> in the relay, it also facilitates data transfer from the BS to the user and leads to a large reduction in the <b>queueing</b> <b>delay</b> at the BS. Therefore, the overall effect is {{the improvement of the}} average end-to-end packet delay. In summary, we state this as follows.|$|R
50|$|The maximum <b>queuing</b> <b>delay</b> is {{proportional}} to buffer size. The longer the line of packets waiting to be transmitted, the longer the average waiting time is. The router queue of packets waiting to be sent also introduces a potential cause of packet loss. Since the router has a finite amount of buffer memory to hold the queue, a router which receives packets at too high a rate may experience a full queue. In this case, the router has no other option than to simply discard excess packets.|$|E
5000|$|This term is {{most often}} used in {{reference}} to routers. When packets arrive at a router, {{they have to be}} processed and transmitted. A router can only process one packet at a time. If packets arrive faster than the router can process them (such as in a burst transmission) the router puts them into the queue (also called the buffer) until it can get around to transmitting them. Delay can also vary from packet to packet so averages and statistics are usually generated when measuring and evaluating <b>queuing</b> <b>delay.</b>|$|E
50|$|However, in a non-trivial network, {{a typical}} packet will be {{forwarded}} over many links via many gateways, {{each of which}} will not begin to forward the packet until it has been completely received. In such a network, the minimal latency {{is the sum of}} the minimum latency of each link, plus the transmission delay of each link except the final one, plus the forwarding latency of each gateway. In practice, this minimal latency is further augmented by queuing and processing delays. <b>Queuing</b> <b>delay</b> occurs when a gateway receives multiple packets from different sources heading towards the same destination. Since typically only one packet can be transmitted at a time, some of the packets must queue for transmission, incurring additional delay. Processing delays are incurred while a gateway determines what to do with a newly received packet. A new and emergent behavior called bufferbloat can also cause increased latency that is an order of magnitude or more. The combination of propagation, serialization, queuing, and processing delays often produces a complex and variable network latency profile.|$|E
40|$|Abstract — This paper {{presents}} {{operational experience}} of largescale unicast network tomography, that samples {{a part of}} the European Internet. In the paper we describe in detail the ETOMIC measurement platform that was used to conduct the experiments, and its potential in future scaled-up measurements. The main results of the paper are maps showing various spatial and temporal structure in the characteristics of <b>queueing</b> <b>delay</b> corresponding to the resolved part of the European Internet. These maps reveal that the average <b>queueing</b> <b>delay</b> on different network segments spans more than two orders of magnitude. At the most loaded time of day we find that the distribution of average <b>queueing</b> <b>delays</b> among the different segments follows closely a log-normal distribution. I...|$|R
5000|$|Use all {{available}} bandwidth, {{and to maintain}} a low <b>queueing</b> <b>delay</b> when no other traffic is present, ...|$|R
40|$|Abstract. We {{study the}} delay {{performance}} of all-optical packet communication networks configured as ring and bus topologies employing cross-connect switches (or wavelength routers). Under a cross-connect network implementation, a packet experiences no (or minimal) internal <b>queueing</b> <b>delays.</b> Thus, the network {{can be implemented}} by high speed all-optical components. We further assume a packet-switched network operation, such as that using a slotted ring or bus access methods. In this case, a packet’s delay is known before it is fed into the network. This {{can be used to}} determine if a packet must be dropped (when its end-to-end delay requirement is not met) at the time it accesses the network. It also leads to better utilization of network capacity resources. We also derive the delay performance for networks under a store-and-forward network operation. We show these implementations to yield very close average end-to-end packet <b>queueing</b> <b>delay</b> performance. We note that a cross-connect network operation can yield a somewhat higher <b>queueing</b> <b>delay</b> variance levels. However, the mean <b>queueing</b> <b>delay</b> for all traffic flows are the same for a cross-connect network operation (under equal nodal traffic loading), while that in a store-and-forward network increases as the path length increases. For a ring network loaded by a uniform traffic matrix, the <b>queueing</b> <b>delay</b> incurred by 90 % of the packets in a cross-connect network may be lower than that experienced in a store-and-forward network. We also study a store-and-forward network operation under a nodal round robin (fair queueing) scheduling policy. We show the variance performance of the packet <b>queueing</b> <b>delay</b> for such a network to be close to that exhibited by a cross-connect (all-optical) network...|$|R
