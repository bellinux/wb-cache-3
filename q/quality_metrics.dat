2159|575|Public
2500|$|... {{reviewed}} several alternative <b>quality</b> <b>metrics</b> {{and compared}} their performance based on visual data obtained in 9 psychophysical experiments. It {{was found that}} a geometric mean of the GAI index and the CIE Ra correlated best with naturalness (r=0.85), while a color quality metric based on memory colors (MCRI) correlated best for preference (r=0.88). The differences in performance of these metrics with the other tested metrics (CIE Ra; CRI-CAM02UCS; CQS; RCRI; GAI; geomean(GAI, CIE Ra); CSA; Judd Flattery; Thornton CPI; MCRI) [...] {{were found to be}} statistically significant with p<0.0001.|$|E
2500|$|The Guardian {{reported}} in October 2016 that storage tests from [...] "Unbox Therapy" [...] and [...] "GSMArena" [...] showed the 32 GB iPhone 7 is [...] "significantly" [...] {{slower than the}} 128 and 256 GB versions, measuring data write speeds of 341 MBps on a 128 GB iPhone 7 model versus 42 MBps on a 32 GB model. October 2016 network tests by Cellular Insights showed that models A1660 and A1661 with Qualcomm modems had [...] "a significant performance edge" [...] over models A1778 and A1784 with Intel modems. Inspection of the modems {{also found that the}} Qualcomm version's ability to use Ultra HD Voice had been turned off, likely to [...] "level the playing field between the Qualcomm, and Intel variants". The report concluded with the statement that [...] "We are not sure what was the main reason behind Apple's decision to source two different modem suppliers for the newest iPhone." [...] Bloomberg {{reported in}} November 2016 that tests by researchers from Twin Prime and Cellular Insights had shown the two modems to perform similarly on some U.S. cellular networks despite one of the modems being technically capable of faster connectivity. Apple spokeswoman Trudy Muller told the publication that [...] "Every iPhone 7 and iPhone 7 Plus meets or exceeds all of Apple’s wireless performance standards, <b>quality</b> <b>metrics,</b> and reliability testing ... In all of our rigorous lab tests based on wireless industry standards, in thousands of hours of real-world field testing, and in extensive carrier partner testing, the data shows there is no discernible difference in the wireless performance of any of the models". Bloomberg quoted analysts and technology advisers who stated that [...] " [...] don’t want one version to get the reputation that it is better" [...] and that [...] "This may not impact the fanboys, but it may make other consumers think twice about buying an Apple phone, especially if they think they might be purchasing a sub-standard product".|$|E
50|$|<b>Quality</b> <b>metrics</b> such as Halstead {{complexity}} measures, Cyclomatic complexity, Knots metric {{are designed}} to verify that code is clear, maintainable and testable. The Quality Report in the LDRA tool suite presents both a summary and detailed breakdown of <b>quality</b> <b>metrics</b> which are deduced during static analysis.|$|E
30|$|After sequencing, {{quality control}} is {{performed}} on raw reads, aligned reads, {{and across the}} collection of cells to identify low quality cells. Relevant <b>quality</b> control <b>metrics,</b> {{similar to those used}} for bulk RNA-seq, include: per base sequence quality, sequence duplication levels, overrepresented sequences, sequence length distribution, and GC content, among others. <b>Quality</b> control <b>metrics</b> should be calculated for raw reads, as well as for aligned reads. Popular tools for assessing these metrics are FastQC, Kraken [47], and RNA-SeQC [44]. Additionally, parameters such as depth of coverage and library complexity should be addressed. Comparing <b>quality</b> control <b>metrics</b> across all cells is helpful in identifying outliers.|$|R
40|$|We {{present a}} new {{approach}} and tool (MRefactor) for model refactoring; we propose {{an extension of the}} UML metamodel for the assisted Model Driven Refactoring (MDR). Based on model <b>qualities</b> <b>metrics</b> and design flaws, we propose a new demarche allowing the automated detection of model refactoring opportunities and the assisted model restructuration. Precisely we focus on class and sequence diagrams...|$|R
30|$|The equal {{weightings}} of <b>quality</b> assessment <b>metrics</b> for payoff {{functions in}} Equations 8 and 9.|$|R
5000|$|... #Subtitle level 3: Classification of {{objective}} video <b>quality</b> <b>metrics</b> ...|$|E
5000|$|VMAF uses {{existing}} image <b>quality</b> <b>metrics</b> {{and other}} features to predict video quality: ...|$|E
50|$|The metric {{is based}} on initial work from the group of Prof. C.-C. J. Kuo at the University of Southern California. Here, the {{applicability}} of fusion of different video <b>quality</b> <b>metrics</b> using Support Vector Machines has been investigated, leading to a “FVQA (Fusion-based Video Quality Assessment) Index” that {{has been shown to}} outperform existing image <b>quality</b> <b>metrics</b> on a subjective video quality database.|$|E
40|$|Significant efforts {{from both}} the academia and {{industry}} have been devoted to advance three-dimensional (3 -D) imaging technologies. However, accurate and easy-to-use visual <b>quality</b> assessment <b>metrics</b> still lack for 3 -D images. In this paper, we propose three objective <b>quality</b> assessment <b>metrics</b> for 3 -D images taking into consideration a set of relevant visual characteristic factors, including contrast sensitivity, multichannel and binocular parallax characteristics. The human visual signal-to-noise ratio (HVSNR), parallax distortion ratio (PDR), and different peak signal-to-noise ratio (DPSNR) are the three independent objective <b>quality</b> assessment <b>metrics</b> proposed in this paper. Experimental {{results show that the}} quality assessment results based upon the proposed metrics are consistent with the quality grades obtained by subjective assessment. ...|$|R
40|$|This paper {{presents}} the system submit-ted by University of Wolverhampton for SemEval- 2014 task 1. We proposed a ma-chine learning approach {{which is based}} on features extracted using Typed Depen-dencies, Paraphrasing, Machine Transla-tion evaluation <b>metrics,</b> <b>Quality</b> Estima-tion <b>metrics</b> and Corpus Pattern Analysis. Our system performed satisfactorily and obtained 0. 711 Pearson correlation for the semantic relatedness task and 78. 52 % ac-curacy for the textual entailment task. ...|$|R
40|$|As mass-spectrometry-based {{proteomics}} {{has matured}} {{during the past}} decade, a growing emphasis {{has been placed on}} quality control. For this purpose, multiple computational quality control tools have been introduced. These tools generate a set of metrics {{that can be used to}} assess the quality of a mass spectrometry experiment. Here we review which types of <b>quality</b> control <b>metrics</b> can be generated, and how they can be used to monitor both intra-and inter-experiment performances. We discuss the principal computational tools for quality control and list their main characteristics and applicability. Asmost of these tools have specific use cases, it is not straightforward to compare their performances. For this survey, we used different sets of <b>quality</b> control <b>metrics</b> derived from information at various stages in a mass spectrometry process and evaluated their effectiveness at capturing qualitative information about an experiment using a supervised learning approach. Furthermore, we discuss currently available algorithmic solutions that enable the usage of these <b>quality</b> control <b>metrics</b> for decision-making...|$|R
50|$|Focusing on {{software}} <b>quality</b> <b>metrics</b> is another {{good way to}} maintain track of how well a project is performing and whether the governance models fit the project. If the resulting outcome is not on par with expectations, which will be rapidly obvious when looking at software <b>quality</b> <b>metrics,</b> then executives know they must improve governance (whether through implementation of a new model or improving {{one aspect of the}} existing process).|$|E
5000|$|Ahn S, Huff SM, Kim Y, Kalra D, <b>Quality</b> <b>metrics</b> for {{detailed}} clinical models, Int J Med Inform, 2012, http://www.ncbi.nlm.nih.gov/pubmed/23089521 ...|$|E
50|$|Tools-driven {{controlled}} language environments enable the automation of many editing tasks and provide objective <b>quality</b> <b>metrics</b> for the authoring process.|$|E
50|$|After Phred quality scores {{became the}} {{required}} standard in DNA sequencing, other manufacturers of DNA sequencing instruments, including Li-Cor and ABI, developed similar <b>quality</b> scoring <b>metrics</b> for their base calling software.|$|R
30|$|There {{are several}} studies about the {{convenience}} of using other image <b>quality</b> assessment <b>metrics</b> than PSNR that better fit to human perceptual quality assessment (i.e., subjective test results) [14, 17, 19, 20]. One of the best behaving objective <b>quality</b> assessment <b>metrics</b> is visual information fidelity (VIF) [7], which has been proven [17, 19] {{to have a better}} correlation with subjective perception than other metrics that are commonly used for encoder comparisons [14, 20]. The VIF metric uses statistic models of natural scenes in conjunction with distortion models in order to quantify the statistical information shared between the test and reference images.|$|R
2500|$|The [...] "MicroArray Quality Control (MAQC) Project" [...] {{is being}} {{conducted}} by the US Food and Drug Administration (FDA) to develop standards and <b>quality</b> control <b>metrics</b> which will eventually allow the use of MicroArray data in drug discovery, clinical practice and regulatory decision-making.|$|R
50|$|Video Quality Inspector is an {{automated}} Perceptual Video Quality Testing Tool. VQMg (Video <b>Quality</b> <b>Metrics</b> - General Model) and VQGD (Video Quality Glitch Detection) are two components of Video Quality Inspector.|$|E
50|$|HP ALM {{provides}} cross-project {{reporting and}} pre-configured business views for reports such as aggregated project status metrics, application <b>quality</b> <b>metrics,</b> requirements coverage, and defect trends for both an enterprise release and individual projects.|$|E
50|$|The two {{frameworks}} SEQUAL and GOM have {{a limitation}} of use in that they cannot be used {{by people who are}} not competent with modeling. They provide major <b>quality</b> <b>metrics</b> but are not easily applicable by non-experts.|$|E
40|$|Abstract — A Mobile Ad-Hoc Network (MANET) is a {{collection}} of wireless mobile nodes that communicate with each other without any centralized access points, infrastructure, or centralized administration. This paper presents an analysis of various <b>quality</b> of service <b>metrics</b> (QoS) of mobile ad hoc network nodes like Throughput, Delay, Packet Delivery Ratio (PDR), Hop Count (HC) and Control Overhead (CO) in varying terrain dimensions. The goal {{of this paper is to}} provide an insight on how node density affects <b>quality</b> of service <b>metrics</b> in mobile ad-hoc networks. A comparative study of the terrain dimensions is made to determine the dimension in which the mobile nodes produce optimum <b>quality</b> of service <b>metrics.</b> The analysis is performed in the simulation environment and graphical results are explained...|$|R
40|$|Face sketch {{synthesis}} has wide applications {{ranging from}} digital entertainments to law enforcements. Objective image quality assessment scores and face recognition accuracy are two mainly used tools {{to evaluate the}} synthesis performance. In this paper, we proposed a synthesized face sketch recognition framework based on full-reference image <b>quality</b> assessment <b>metrics.</b> Synthesized sketches generated from four state-of-the-art methods are utilized to test {{the performance of the}} proposed recognition framework. For the image <b>quality</b> assessment <b>metrics,</b> we employed the classical structured similarity index metric and other three prevalent metrics: visual information fidelity, feature similarity index metric and gradient magnitude similarity deviation. Extensive experiments compared with baseline methods illustrate the effectiveness of the proposed synthesized face sketch recognition framework. Data and implementation code in this paper are available online at www. ihitworld. com/WNN/IQA_Sketch. zip...|$|R
40|$|Research {{dealt with}} the general area of optimal flight control {{synthesis}} for manned flight vehicles. The work was generic; no specific vehicle {{was the focus of}} study. However, the class of vehicles generally considered were those for which high authority, multivariable control systems might be considered, for the purpose of stabilization and the achievement of optimal handling characteristics. Within this scope, the topics of study included several optimal control synthesis techniques, control-theoretic modeling of the human operator in flight control tasks, and the development of possible handling <b>qualities</b> <b>metrics</b> and/or measures of merit. Basic contributions were made in all these topics, including human operator (pilot) models for multi-loop tasks, optimal output feedback flight control synthesis techniques; experimental validations of the methods developed, and fundamental modeling studies of the air-to-air tracking and flared landing tasks...|$|R
50|$|The mobile {{receiving}} a variable-rate traffic frame {{does not know}} {{the rate at which}} the frame was transmitted. Typically, the frame is decoded at each possible rate, and using the <b>quality</b> <b>metrics</b> of the Viterbi decoder, the correct result is chosen.|$|E
5000|$|Biometric sample quality {{standards}} and technical reports: specify {{terms and definitions}} that are useful in the specification, use and testing of image <b>quality</b> <b>metrics</b> and define the purpose, intent, and interpretation of image quality scores for a particular biometric modality.|$|E
50|$|CAST code <b>quality</b> <b>metrics</b> are {{frequently}} used in application development service-level agreements. CAST research and experts are often consulted on issues {{having to do}} with development quality and security by medias such as Los Angeles Times, BBC, CNBC, The Wall Street Journal, and The Economist.|$|E
40|$|As mass spectrometry-based {{proteomics}} {{has matured}} {{during the past}} decade a growing emphasis has been placed on quality control. For this purpose multiple computational quality control tools have been introduced. These tools generate a set of metrics {{that can be used to}} assess the quality of a mass spectrometry experiment. Here we review which different types of <b>quality</b> control <b>metrics</b> can be generated, and how they can be used to monitor both intra- and inter-experiment performance. We discuss the principal computational tools for quality control and list their main characteristics and applicability. As most of these tools have specific use cases it is not straightforward to compare their performance. For this survey we used different sets of <b>quality</b> control <b>metrics</b> derived from information at various stages in a mass spectrometry process and evaluated their effectiveness at capturing qualitative information about an experiment using a supervised learning approach. Furthermore, we discuss currently available algorithmic solutions that enable the usage of these <b>quality</b> control <b>metrics</b> for decision-making. This is the peer reviewed version of the following article: "Bittremieux, W., Valkenborg, D., Martens, L. & Laukens, K. Computational quality control tools for mass spectrometry proteomics. PROTEOMICS 17, 1600159 (2017) ", which has been published in final form at [URL] This article may be used for non-commercial purposes in accordance with Wiley Terms and Conditions for Self-Archiving...|$|R
40|$|There are {{a variety}} of {{techniques}} that lecturers can use to get feedback on their teaching - for example, module feedback and coursework results. However, a question arises about how reliable and valid are the content that goes into these <b>quality</b> assurance <b>metrics.</b> The aim {{of this article is to}} present a new approach for collecting and analysing qualitative feedback from students that could be used as the first stage in developing more reliable <b>quality</b> assurance <b>metrics.</b> The approach, known as the multi-dimensional crystal view, is based on the belief that individuals have different views on the benefits that the embedded process in a system can have on the behaviour of the system. The results of this study indicate that in the context of evaluation and feedback methods, the multi-dimensional approach appears to provide the opportunity for developing more effective student feedback mechanisms...|$|R
40|$|The present {{analysis}} {{describes a}} series of experiments to quantify the effects of loss of contrast on highly-idealized, compensatory piloting tasks. The effects of spatial frequency are simultaneously studied via the Modulation Transfer Function (MTF). The MTF can quantify the contrast and spatial frequency (relative size) of the objects that provide visual cues necessary for closing pilot control loops. A brief analysis is also performed on different input devices and their effects on task performance. The results show compensatory task performance can be modeled with multiple MTFs, each representing a unique numeric characterization of the pilot response in a single task. The results also quantify the intuitively known fact that physiological limits of human vision directly correlate to piloting task performance. Therefore, the MTF may represent a key building block of quantitative, objective rotorcraft Handling <b>Qualities</b> <b>metrics</b> for Degraded Visual Environments (DVE) ...|$|R
50|$|Establishment by the Secretary of Health and Human Services of {{a quality}} cancer care {{demonstration}} {{project for the}} purpose of establishing <b>quality</b> <b>metrics</b> and aligning Medicare payment incentives in the areas of treatment planning and follow-up cancer care planning for Medicare beneficiaries with cancer.|$|E
50|$|GILT Metrics. GILT {{stands for}} (Globalization, Internationalization, Localization, and Translation). The GILT Metrics {{standard}} comprises three parts: GMX-V for volume metrics, GMX-C for complexity metrics and GMX-Q for <b>quality</b> <b>metrics.</b> The proposed GILT Metrics standard is tasked with quantifying the workload and quality requirements {{for any given}} GILT task.|$|E
50|$|Due to its popularity, SSIM {{is often}} {{compared}} to other metrics, including more simple metrics such as MSE and PSNR, and other perceptual image and video <b>quality</b> <b>metrics.</b> SSIM has been repeatedly shown to significantly outperform MSE and its derivates in accuracy, including research by its own authors and others.|$|E
40|$|Machine Learning (ML) is a {{powerful}} tool to support the development of objective visual <b>quality</b> assessment <b>metrics,</b> serving as a substitute model for the perceptual mechanisms acting in visual quality appreciation. Nevertheless, the reli- ability of ML-based techniques within objective <b>quality</b> as- sessment <b>metrics</b> is often questioned. In this study, the ro- bustness of ML in supporting objective quality assessment is investigated, specifically when the feature set adopted for prediction is suboptimal. A Principal Component Regres- sion based algorithm and a Feed Forward Neural Network are compared when pooling the Structural Similarity Index (SSIM) features perturbed with noise. The neural network adapts better with noise and intrinsically favours features ac- cording to their salient content...|$|R
30|$|Four popular image <b>quality</b> {{assessment}} <b>metrics</b> {{are used}} to compare the performance of OLVFOFR and other methods. These include the mean brightness, entropy, contrast, and gradient. Since there are multiple performances adopted, two additional summary indices formulated by the normalized sum and the normalized product of performance metrics are used. The assessment criteria are defined below.|$|R
40|$|Abstract: There are {{a variety}} of {{techniques}} that lecturers can use to get feedback on their teaching – for example, module feedback and coursework results. However, a question arises about how reliable and valid are the content that goes into these <b>quality</b> assurance <b>metrics.</b> The aim {{of this article is to}} present a new approach for collecting and analysing qualitative feedback from students that could be used as the first stage in developing more reliable <b>quality</b> assurance <b>metrics.</b> The approach, known as the multi-dimensional crystal view, is based on the belief that individuals have different views on the benefits that the embedded process in a system can have on the behaviour of the system. The results of this study indicate that in the context of evaluation and feedback methods, the multi-dimensional approach appears to provide the opportunity for developing more effective student feedback mechanisms...|$|R
