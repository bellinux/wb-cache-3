41|195|Public
2500|$|Performance/scalability: In 2008, {{performance}} tests between Drupal 6.1 and Joomla 1.5 demonstrated that Drupal's pages were delivered [...] "significantly faster" [...] {{than those of}} Joomla. Despite this, arguments over speed persist. Drupal {{is likely to be}} slower than a special-purpose application for a given task. For example, WordPress typically outperforms Drupal as a single-user blogging tool. Drupal positions itself for broader applications requirements that are outside the scope of more narrowly focused applications. Drupal offers caching to store various page elements, the use of which resulted in a 508% improvement in one benchmark. When using Drupal's default Page Cache mechanism, the cached pages are delivered only to anonymous users, so contributed modules must be installed to allow caching content for logged in users. Like performance, scalability (the ability to add servers to handle growing numbers of visitors with consistent response) can become a concern on large, interactive sites. MySQL's query caching can help reduce the load on the database server caused by Drupal's high <b>query</b> <b>rate.</b> Drupal caches database schema metadata as well as elements such as blocks, forms and menus. Drupal 7 increases performance in database queries and reduces PHP code usage.|$|E
3000|$|..., where q[*]is the <b>query</b> <b>rate</b> and R[*]is {{the number}} of object replicas in the system. Square-root {{replication}} does not consider the location of replicas. All strategies require global knowledge on {{the number of}} currently existing replicas and the current <b>query</b> <b>rate</b> for each of the replicas.|$|E
40|$|While Web {{search engines}} are built {{to cope with}} a large number of queries, query traffic can exceed the maximum <b>query</b> <b>rate</b> {{supported}} by the underlying computing infrastructure. We study how response times and results vary when, in presence of high loads, some queries are either interrupted after a fixed time threshold elapses or dropped completely. Moreover, we introduce a novel dropping strategy, based on machine learned performance predictors to select the queries to drop in order to sustain the largest possible <b>query</b> <b>rate</b> with a relative degradation in effectiveness...|$|E
40|$|Most object {{tracking}} {{trees are}} established using the predefined mobility profile. However, {{when the real}} object's movement behaviors and <b>query</b> <b>rates</b> {{are different from the}} predefined mobility profile and <b>query</b> <b>rates,</b> the update cost and query cost of object tracking tree may increase. To upgrade the object tracking tree, the sink needs to send very large messages to collect the real movement information from the network, introducing a very large message overhead, which is referred to as adaptation cost. The Sub Root Message-Tree Adaptive procedure was proposed to dynamically collect the real movement information under the sub-tree and reconstruct the sub-tree to provide good performance based on the collected information. The simulation results indicates that the Sub Root Message-Tree Adaptive procedure is sufficient to achieve good total cost and lower adaptation cost...|$|R
40|$|One {{problem with}} {{existing}} store-and-query sensor networks {{is that they}} fail to take data and <b>query</b> <b>rates</b> or network topology information into account. This leads to expensive (and avoidable) communication overhead that reduces the lifespan of battery-powered sensor networks. Scoop reduces this overhead (up to a factor of four in our experiments) by dynamically creating and adapting an in-network storage policy based on statistics it collects about data, queries, and network conditions. Whereas existing in-network storage techniques are often at the extreme ends of the storage policy spectrum (e. g., store all data externally on a basestation, or store all data locally), Scoop’s storage policy allows it to adapt between these extremes depending on the situation. The intuition behind our scheme is that data should be stored close to where it is needed, if it is needed. If <b>query</b> <b>rates</b> are high relative to the rate of data production, Scoop adapts its storage policy to store data closer {{to the source of}} <b>queries.</b> If <b>query</b> <b>rates</b> are low, the policy adapts to store data closer to its source. We have built a complete implementation of Scoop for TinyOS motes [3] and evaluated its performance on a 62 -node testbed and in the TinyOS simulator, TOSSIM. Our results show that Scoop not only provides substantial performance benefits over alternative approaches on a range of data sets, but is also able to efficiently adapt to changes in the distribution and rates of data and queries...|$|R
40|$|Abstract- Bots are {{compromised}} {{computers that}} communicate with a botnet {{command and control}} (C&C) server. Bots typically employ dynamic DNS (DDNS) to locate the respective C&C server. By injecting commands into such servers, botmasters can reuse bots {{for a variety of}} attacks. We evaluate two approaches for identifying botnet C&C servers based on anomalous DDNS traffic. The first approach consists in looking for domain names whose <b>query</b> <b>rates</b> are abnormally high or temporally concentrated. High DDNS <b>query</b> <b>rates</b> may be expected because botmasters frequently move C&C servers, and botnets with as many as 1. 5 million bots have been discovered. The second approach consists in looking for abnormally recurring DDNS replies indicating that the query is for an inexistent name (NXDOMAIN). Such queries may correspond to bots trying to locate C&C servers that have been taken down. In our experiments, the second approach automatically identified several domain names that were independently reported by others as being suspicious, while the first approach was not as effective. I...|$|R
40|$|We {{consider}} an archiving {{model for a}} database consisting of secondary and tertiary storage devices in which the <b>query</b> <b>rate</b> for a record declines as it ages. We propose a 'dynamic' archiving policy {{based on the number}} of records and the age of the records in the secondary device. We analyze the cases when the number of new records inserted in the system over time are either constant or follow a Poisson process. For both scenarios, we characterize the properties of the policy parameters and provide optimization results when the objective is to minimize the average record retrieval times. Furthermore, we propose a simple heuristic method for obtaining near-optimal policies in large databases when the record <b>query</b> <b>rate</b> declines exponentially with time. The effectiveness of the heuristic is tested via a numerical experiment. Finally, we examine the behavior of performance measures such as the average record retrieval time and the hit rate as system parameters are varied...|$|E
40|$|Cataloged from PDF {{version of}} article. We {{consider}} an archiving {{model for a}} database consisting of secondary and tertiary storage devices in which the <b>query</b> <b>rate</b> for a record declines as it ages. We propose a `dynamic' archiving policy {{based on the number}} of records and the age of the records in the secondary device. We analyze the cases when the number of new records inserted in the system over time are either constant or follow a Poisson process. For both scenarios, we characterize the properties of the policy parameters and provide optimization results when the objective is to minimize the average record retrieval times. Furthermore, we propose a simple heuristic method for obtaining near-optimal policies in large databases when the record <b>query</b> <b>rate</b> declines exponentially with time. The e ectiveness of the heuristic is tested via a numerical experiment. Finally, we examine the behavior of performance measures such as the average record retrieval time and the hit rate as system parameters are varied. Ó 2000 Elsevier Science B. V. All rights reserved...|$|E
40|$|The {{unstructured}} Peer-to-Peer (P 2 P) systems usually use a “blind search ” {{method to}} find the requested data object by propagating a query {{to a number of}} peers randomly. In order to increase the success rate of blind search, replication techniques are widely used in these systems. Most P 2 P systems replicate the most frequently accessed data objects to improve system performance. However, existing replication strategies cannot answer the question that how many replicas of an object should be kept in the P 2 P system. If an object is replicated excessively, it inevitably will affect the average efficiency of a replica, which will decrease the whole search performance. This paper addresses the issue of finding the proper number of replicas for an object according to its <b>query</b> <b>rate.</b> In this paper, we firstly investigate the precise relation among success rate, the allocation of replicas and <b>query</b> <b>rate.</b> Then we propose an approach of the allocation of copies to optimize the success rate. As a benchmark, our result offers a new understanding of replication. 1...|$|E
40|$|We {{introduce}} a <b>query</b> <b>rating</b> scheme that identifies the possible interpretations {{which can be}} assigned to a semantic query. The interpretations range from the traditional bag-of-words interpretation to more context- and semantic-aware interpretations. The aims of this scheme are to communicate the extent of semantics that is being interpreted for a query and to assign suitable query processing methods for each level of interpretation accordingly...|$|R
3000|$|... (2) Another notable {{side effect}} is that large {{transitive}} trust chains introduce more latency to resolution of records within domains depending on many other domains {{and increase the}} <b>queries</b> <b>rate</b> to name servers appearing in multiple transitive trust chains. Our study measured an increase of 50 ms for every transitive trust chain of three links, when measured with a cold (empty) cache. Resolutions of larger chains, e.g., 200, can often result in timeouts and unnecessary retransmissions, overloading the network and the name server, and increasing the latency for clients’ queries.|$|R
40|$|We study online {{algorithms}} for selective sampling {{that use}} regularized least squares (RLS) as base classifier. These algorithms typically perform well in practice, {{and some of}} them have formal guarantees on their mistake and <b>query</b> <b>rates.</b> We refine and extend these guarantees in various ways, proposing algorithmic variants that exhibit better empirical behavior while enjoying performance guarantees under much more general conditions. We also show a simple way of coupling a generic gradient-based classifier with a specific RLSbased selective sampler, obtaining hybrid algorithms with combined performance guarantees. 1...|$|R
40|$|Location-Based Social Network (LBSN) {{applications}} that support geo-location-based posting and queries to provide location-relevant information to mobile users are increasingly popular, but pose a location-privacy risk to posts. We investigated existing LBSNs and location privacy mechanisms, {{and found a}} powerful potential attack that can accurately locate users with relatively few queries, even when location data is well secured and location noise is applied. Our technique defeats previously proposed solutions including fake-location detection and <b>query</b> <b>rate</b> limits...|$|E
40|$|Collect Domain Name System (DNS) data, the DNS data {{generated}} by a DNS server and/or similar device, wherein the DNS data comprises DNS queries, wherein the collected DNS data comprises DNS <b>query</b> <b>rate</b> information. Examine the collected DNS data relative to DNS data from known compromised and/or uncompromised computers. Determine an existence of the collection of compromised networks and/or computers, and/or an identity of compromised networks and/or computers, based on the examination. Georgia Tech Research Corporatio...|$|E
3000|$|The goal is {{to place}} the right number of replicas at the right {{locations}} before they are requested. Researchers investigated the optimal number of replicas {{in the context of}} robustness. In (Cohen and Shenker 2002) and (Lv et al. 2002) the authors investigate random, proportional, and square root replication. When applying random replication a uniform number of replicas for each object are created. Proportional replication creates replicas proportional to their <b>query</b> <b>rate.</b> The authors showed that square root replication determines the optimal replication rate r [...]...|$|E
40|$|Abstract. In most {{wireless}} sensor networks, applications {{submit their}} requests as queries and {{wireless sensor network}} transmits the requested data to the applications. However, most existing {{work in this area}} focuses on data aggregation, not much {{attention has been paid to}} query aggregation. For many applications, especially ones with high <b>query</b> <b>rates,</b> <b>query</b> aggregation is very important. In this paper, we design an effective query aggregation algorithm SAQA to reduce the number of duplicate/overlapping queries and save overall energy consumption in the wireless sensor networks. This new aggregation algorithm focuses on the duplicate/overlapping spatial and attribute information in the original queries submitted by the applications. Our performance evaluations show that by applying our query aggregation algorithm, the overall energy consumption can be significantly reduced and the sensor network lifetime can be prolonged correspondingly. ...|$|R
40|$|Abstract — Every {{physical}} event {{results in}} a natural information gradient in {{the proximity of the}} phenomenon. Moreover, many physical phenomena follow the diffusion laws. This natural information gradient can be used to design efficient information-driven routing protocols for sensor networks. Information-driven routing protocols based on the natural information gradient, may be categorized into two major approaches: (i) the single-path approach and (ii) the multiple-path approach. In this paper, using a regular grid topology, we develop analytical models for the <b>query</b> success <b>rate</b> and the overhead of both approaches for ideal and lossy wireless link conditions. We validate our analytical models using simulations. Also, both the analytical and the simulation models are used to characterize each approach in terms of overhead, <b>query</b> success <b>rate</b> and increase in path length. From this study, we find that, surprisingly, the multiplepath approach is generally more energy efficient than the single-path approach when the source is relatively close to the sink. Also, the multiple-path approach yields shorter paths than the single-path approach. Further, as the number of malfunctioning nodes in the network increases, the <b>query</b> success <b>rate</b> of the single-path approach degrades significantly faster than that of the multiple-path approach. Finally, in the lossy link case, the <b>query</b> success <b>rate</b> of the single-path approach drops drastically while the multiplepath approach remains quite resilient. I...|$|R
40|$|Characteristics {{required}} for NASA scientific data base management application {{are listed as}} well as performance testing objectives. Results obtained for the ORACLE, SEED, and INGRES packages are presented in charts. It is concluded that vendor packages can manage 130 megabytes of data at acceptable load and <b>query</b> <b>rates.</b> Performance tests varying data base designs and various data base management system parameters are valuable to applications for choosing packages and critical to designing effective data bases. An applications productivity increases {{with the use of}} data base management system because of enhanced capabilities such as a screen formatter, a reporter writer, and a data dictionary...|$|R
40|$|Abstract-There is an {{increase}} in demand of high performance query services, with the emergence of high data rate applications. To meet this challenge we propose Dynamic Conflict-free Query Scheduling (DCQS), a novel scheduling technique for queries in wireless sensor networks. In contrast to earlier Time Division Multiple Access (TDMA) designed for query services in wireless sensor networks. DCQS has several unique features. First, it optimizes the query performance through conflict-free transmission scheduling based on the temporal properties of queries in wireless sensor networks. Second, it can adapt to workload changes without explicitly reconstructing the transmission schedule. Furthermore, DCQS also provides predictable performance in terms of the maximum achievable <b>query</b> <b>rate.</b> The nodes operate over the time-varying wireless channel whose quality significantly fluctuates over time due to fading and interference. Such time-varying nature of wireless channel imposes many constraints in designing an energy-efficient transmission scheme. In this work, we derive a tight bound on the maximum <b>query</b> <b>rate</b> achieved under DCQS. Such a bound is of practical importance since {{it can be used to}} prevent network overload. NS 2 simulations demonstrate that energy efficient DCQS significantly outperforms 802. 11 in terms of energy efficiency, over head, query latency, and throughput, thereby increasing the network life time...|$|E
40|$|Abstract — Efficient Resource {{discovery}} {{mechanism is}} one of the fundamental requirement for Grid computing systems, as it aids in resource management and scheduling of applications. Resource discovery involves searching for resources that match the user’s application requirements. Various kinds of solutions to Grid resource discovery have been developed, including the centralised and hierarchical information server approach. However, these approaches have serious limitations in regards to scalability, fault-tolerance and network congestion. To overcome such limitations, we propose a decentralised Grid resource discovery system based on a spatial publish/subscribe index. It utilises a Distributed Hash Table (DHT) routing substrate for delegation of d-dimensional service messages. Our approach has been validated using a simulated publish/subscribe index that assigns regions of a d-dimensional resource attribute space to the grid peers in the system. We generated the resource attribute distribution using the configurations obtained from the Top 500 Supercomputer list. The simulation study takes into account various parameters such as resource <b>query</b> <b>rate,</b> index load distribution, number of index messages generated, overlay routing hops and system size. Our results show that grid resource <b>query</b> <b>rate</b> directly affects the performance of the decentralised resource discovery system, and that at higher rates the queries can experience considerable latencies. Further, contrary to what one can expect, system size does not {{have a significant impact on}} the performance of the system, in particular the query latency. I...|$|E
40|$|International audienceSharing between {{commercial}} and Federal incumbent users, {{such as in}} the 3. 5 GHz band, is expected to increase the availability of spectrum for wireless broadband use. However, the spectrum coordination needed between incumbent and commercial users gives rise to several privacy concerns. This paper analyzes the vulnerability of the incumbent's operational center frequency to disclosure from inference attacks. We evaluate the inherent protection provided by two channel assignment schemes in terms of the time required for an attacker to infer the incumbent's frequency. We account for the activity of secondary users in a dynamically-shared environment. This analysis quantifies privacy for a given secondary load. It also provides an analytical framework to quantify the effectiveness of countermeasures such as limiting the <b>query</b> <b>rate</b> of secondaries. ...|$|E
40|$|The use of deep {{reinforcement}} learning allows for high-dimensional state descriptors, but {{little is known}} about how the choice of action representation impacts the learning difficulty and the resulting performance. We compare the impact of four different action parameterizations (torques, muscle-activations, target joint angles, and target joint-angle velocities) in terms of learning time, policy robustness, motion quality, and policy <b>query</b> <b>rates.</b> Our results are evaluated on a gait-cycle imitation task for multiple planar articulated figures and multiple gaits. We demonstrate that the local feedback provided by higher-level action parameterizations can significantly impact the learning, robustness, and quality of the resulting policies...|$|R
40|$|We {{consider}} {{the problem of}} optimizing the number of replicas for event information in wireless sensor networks, when queries are disseminated using expanding rings. We obtain closed-form approximations for the expected energy costs of search, as well as replication. Using these expressions we derive the replication strategies that minimize the expected total energy cost, both with and without storage constraints. In both cases, we find that events should be replicated with a frequency that {{is proportional to the}} square root of their <b>query</b> <b>rates.</b> We validate our analysis and optimization through a set of realistic simulations that incorporate non-idealities including deployment boundary effects and lossy wireless links. I...|$|R
40|$|Abstract — We {{consider}} {{the problem of}} optimizing the number of replicas for event information in wireless sensor networks, when queries are disseminated using expanding rings. We obtain closed-form approximations for the expected energy costs of search, as well as replication. Using these expressions we derive the replication strategies that minimize the expected total energy cost consisting of search and replication costs, both with and without storage constraints. In both cases, we find that events should be replicated with a frequency that {{is proportional to the}} square root of their <b>query</b> <b>rates.</b> We validate our analysis and optimization through a set of realistic simulations that incorporate non-idealities including deployment boundary effects and lossy wireless links. I...|$|R
40|$|Abstract: This paper {{addresses}} {{the issue that}} what is the optimal topology for the purely distributed peer-to-peer (P 2 P). It is usually assumed that unstructured network will form a Power-Law topology. However, a Power-Law structure {{is not always the}} best for all the applications. In this paper, the influence of the overlay topology on P 2 P search is firstly investigated and the precise relation among the distribution of the degree, the <b>query</b> <b>rate</b> and the success rate is attained. Then an optimal distribution model of degrees in terms of the popularity of data items is proposed to guide the overlay construction. Finally, the experimental results show that the proposed topology is effective to improve the success rate for the Random Walks. The fundamental result is significant for the optima...|$|E
40|$|Abstract. We {{investigate}} label efficient prediction, {{a variant}} of the problem of prediction with expert advice, proposed by Helmbold and Panizza, in which the forecaster does not have access to the outcomes of the sequence to be predicted unless he asks for it, which he can do for a limited number of times. We determine matching upper and lower bounds for the best possible excess error when the number of allowed queries is a constant. We also prove that a <b>query</b> <b>rate</b> of order (ln n) (ln ln n) 2 /n is sufficient for achieving Hannan consistency, a fundamental property in game-theoretic prediction models. Finally, we apply the label efficient framework to pattern classification and prove a label efficient mistake bound for a randomized variant of Littlestone’s zero-threshold Winnow algorithm. ...|$|E
40|$|Abstract — Failures of {{any type}} are common in current datacenters. As data scales up, its {{availability}} becomes more complex, while different availability levels per application or per data item may be required. In this paper, we propose a self-managed keyvalue store that dynamically allocates the resources of a data cloud to several applications in a cost-efficient and fair way. Our approach offers and dynamically maintains multiple differentiated availability guarantees to each different application despite failures. We employ a virtual economy, where each data partition acts as an individual optimizer and chooses whether to migrate, replicate or remove itself based on net benefit maximization regarding the utility offered by the partition and its storage and maintenance cost. Comprehensive experimental evaluations suggest that our solution is highly scalable and adaptive to <b>query</b> <b>rate</b> variations and to resource upgrades/failures. I...|$|E
40|$|Abstract Replication is {{a widely}} used {{technique}} in unstructured overlays to improve content availability or system performance. A fundamental question often addressed by previous work focused on: how many replicas ought to be allocated for each data item given the fixed <b>query</b> <b>rates</b> and limited storage capability? In this paper, we have put forth two optimal replica distributions to achieve the highest success rate and the lowest message consumption. Especially, we have investigated the influence of item size on replica distribution. Our results show that Square-Root Replication, which is traditionally considered to be optimal, {{is not always the}} best choice. Our study offers a new deep understanding of resource managment in self-organized unstructured overlays...|$|R
40|$|Abstract. With the {{exponential}} growth of moving objects {{data to the}} Gigabyte range, it has become critical to develop effective techniques for indexing, updating, and querying these massive data sets. To meet the high update rate as well as low query response time requirements of moving object applications, this paper takes a novel approach in moving object indexing. In our approach we do not require a sophisticated index structure {{that needs to be}} adjusted for each incoming update. Rather we construct conceptually simple short-lived throwaway indexes which we only keep for {{a very short period of}} time (sub-seconds) in main memory. As a consequence, the resulting technique MOVIES supports at the same time high <b>query</b> <b>rates</b> and high update rates and trades this for query result staleness. Moreover, MOVIES is the first main memory method supporting time-parameterized predictive queries. To support this feature we present two algorithms: non-predictive MOVIES and predictive MOVIES. We obtain the surprising result that a predictive indexing approach — considered state-of-the-art in an external-memory scenario — does not scale well in a main memory environment. In fact our results show that MOVIES outperforms state-of-the-art moving object indexes like a main-memory adapted B x-tree by orders of magnitude w. r. t. update <b>rates</b> and <b>query</b> <b>rates.</b> Finally, our experimental evaluation uses a workload unmatched by any previous work. We index the complete road network of Germany consisting of 40, 000, 000 road segments and 38, 000, 000 nodes. We scale our workload up to 100, 000, 000 moving objects, 58, 000, 000 updates per second and 10, 000 queries per second which is unmatched by any previous work. ...|$|R
40|$|Abstract — We {{present a}} {{comparative}} mathematical analysis of two important distinct approaches to hybrid push-pull querying in wireless sensor networks: structured hash-based datacentric storage (DCS) and the unstructured comb-needle (CN) rendezvous mechanism. Our analysis, {{which is based}} on a singlesink square-grid deployment, yields several interesting insights. For ALL-type queries pertaining to information about all events corresponding to a given attribute, we examine the conditions under which the two approaches outperform each other in terms of the average <b>query</b> and event <b>rates.</b> For the case of ANY-type queries where it is sufficient to obtain information from any one of the desired events for a given attribute, we propose and analyze a modified sequential comb-needle technique (SCN) to compare with DCS. We find that DCS generally performs better than CN/SCN for high <b>query</b> <b>rates</b> and low event rates, while CN/SCN perform better for high event rates. Surprisingly, for the cases of ALL-type aggregated queries and ANY-type queries, we identify the existence of “magic number ” event rate thresholds, independent of network size or query probability, which dictate the choice of querying protocol. I...|$|R
40|$|This paper {{presents}} a scheme using the virtual machine concept for creating: 1) An environment {{for increasing the}} effectiveness of researchers who must use analytical, modeling systems and have complex data management needs; 2) A mechanism for multi-user coordination of access and update to a central data base; 3) A mechanism for creating an environment where several different modeling facilities can access the same data base; 4) A mechanism for creating an environment where several different and potentially incompatible data management systems can all be accessed by the same user models or facilities. The paper investigates and formalizes the performance implications of this scheme specifically directed at the question of response time degradation {{as a function of}} number of virtual machines, of locked time of the data base machine, and of <b>query</b> <b>rate</b> of the modeling machine...|$|E
40|$|Abstract—With the {{emergence}} of high data rate sensor network applications, there is an increasing demand for high-performance query services. To meet this challenge, we propose Dynamic Conflict-free Query Scheduling (DCQS), a novel scheduling technique for queries in wireless sensor networks. In contrast to earlier TDMA protocols designed for general-purpose workloads, DCQS is specifically designed for query services in wireless sensor networks. DCQS has several unique features. First, it optimizes the query performance through conflict-free transmission scheduling based on the temporal properties of queries in wireless sensor networks. Second, it can adapt to workload changes without explicitly reconstructing the transmission schedule. Furthermore, DCQS also provides predictable performance {{in terms of the}} maximum achievable <b>query</b> <b>rate.</b> We provide an analytical capacity bound for DCQS that enables DCQS to handle overload through rate control. NS 2 simulations demonstrate that DCQS significantly outperforms a representative TDMA protocol (DRAND) and 802. 11 b in terms of query latency and throughput. Index Terms—Query scheduling, TDMA, sensor networks. ...|$|E
40|$|Similarity search {{consists}} on retrieving objects {{within a}} database that are similar or relevant to a particular query. It is a topic {{of great interest to}} scientific community because of its many fields of application, such as searching for words and images on the World Wide Web, pattern recognition, detection of plagiarism, multimedia databases, among others. It is modeled through metric spaces, in which objects are represented in a black-box that contains only the distance between objects; calculating the distance function is costly and search systems operate at a high <b>query</b> <b>rate.</b> Metrical structures have been developed to optimize this process; such structures work as indexes and preprocess data to decrease the distance evaluations during the search. Processing large volumes of data makes unfeasible the use of such structures without using parallel processing environments. Technologies based on multi- CPU and GPU architectures are among the most force due to its costs and performance. XV Workshop de Procesamiento Distribuido y Paralelo (WPDP...|$|E
40|$|Abstract—Providing {{efficient}} {{data services}} {{is one of}} the fundamental requirements for sensor networks. The data service paradigm requires that the application submit its requests as queries and the sensor network transmits the requested data to the application. While most existing work in this area focuses on data aggregation, not much {{attention has been paid to}} query aggregation. For many applications, especially ones with high <b>query</b> <b>rates,</b> <b>query</b> aggregation is very important. In this paper, we study a query aggregation-based approach for providing efficient data services. In particular: 1) we propose a multi-layered overlaybased framework consisting of a query manager and access points (nodes), where the former provides the query aggregation plan and the latter executes the plan; 2) we design an effective query aggregation algorithm to reduce the number of duplicate/overlapping queries and save overall energy consumption in the sensor network. Our performance evaluations show that by applying our query aggregation algorithm, the overall energy consumption can be significantly reduced and the sensor network lifetime can be prolonged correspondingly. Keywords—Sensor Network, Query Aggregation I...|$|R
40|$|Many web {{companies}} {{deal with}} enormous data sizes and request rates beyond {{the capabilities of}} traditional database systems. This {{has led to the}} development of modern Web Data Platforms (WDPs). WDPs handle large amounts of data and activity through massively distributed infrastructures. To achieve performance and availability at Internet scale, WDPs restrict querying capability, and provide weaker consistency guarantees than traditional ACID transactions. The reduced functionality is sufficient for many web applications. High data and <b>query</b> <b>rates</b> also appear in application performance management (APM). APM has similar requirements like current web based information systems such as weaker consistency needs, geographical distribution and asynchronous processing. At the same time, APM has some unique features and requirements that make previously published research and existing architectures inapplicable. 1...|$|R
40|$|Collection sizes, <b>query</b> <b>rates,</b> and {{the number}} of users of Web search engines are increasing. Therefore, there is {{continued}} demand for innovation in providing search services that meet user information needs. In this article, we propose new techniques to add additional terms to documents with the goal of providing more accurate searches. Our techniques are based on query association, where queries are stored with documents that are highly similar statistically. We show that adding query associations to documents improves the accuracy of Web topic finding searches by up to 7 %, and provides an excellent complement to existing supplement techniques for site finding. We conclude that using document surrogates derived from query association is a valuable new technique for accurate Web searching...|$|R
