28|39|Public
50|$|It is not {{a trivial}} problem to {{establish}} the criteria for {{whether or not a}} particular irreversible compression scheme applied with particular parameters to a particular individual image, or category of images, avoids the introduction of some <b>quantifiable</b> <b>risk</b> of a diagnostic error for any particular diagnostic task.|$|E
5000|$|Francis Cianfrocca, the CEO of Bayshore Networks, wrote, [...] "The administration’s {{management}} of the Chrysler bankruptcy {{has led to an}} astonishingly reckless abrogation of contract law that will introduce a new level of uncertainty into business transactions at all levels, and make wealth generation more difficult going forward... An extraordinary uncertainty has been created when {{the most powerful man in}} the world can rewrite contracts and choose winners and losers in private negotiations as he sees fit. Since this is an unquantifiable uncertainty, and not a <b>quantifiable</b> <b>risk,</b> its effect on business and investor confidence will be large and unpredictable. As in the 1930s, a time when government also cavalierly rewrote private contracts, the prudent approach for business will be to invest minimally and wait for another administration." ...|$|E
50|$|The first {{results from}} the PFR were {{reported}} in 1970 in the scientific journal Nature. The research underpinned the recommendations for more stringent airborne dust standards in British coalmines and the PFR was ultimately {{used as the basis}} for many national dust standards around the world. IOM's research in coal mining continued until about 1990, with many important scientific papers on respiratory diseases amongst miners having been published. In 1985, an important association between risk of pathological emphysema and dust exposure was demonstrated, leading ultimately to recognition of this disease as a <b>quantifiable</b> <b>risk</b> of coal mining. Recent analysis of the mortality of a subset of the miners originally studied has found an association between the risk of lung cancer and quartz exposure, and raised mortality from chronic lung disease and pneumoconiosis associated with increasing dust exposure.|$|E
40|$|All human endeavors are {{associated}} with <b>quantifiable</b> <b>risks.</b> Knowledge of the risk is essential for personal health maintenance. Nontherapeutic use of psychoactive drugs poses an important danger to individual persons and society. What are the quantitative estimates of these risks? Are they acceptable...|$|R
40|$|Invasive species {{eradication}} {{programs can}} fail by applying management {{strategies that are}} not robust to potentially large but non-quantiﬁed risks. A more robust strategy can succeed over a larger range of possible values for non-quantiﬁed risk. This form of robustness analysis is often not undertaken in eradication program evaluations. The main non-quantiﬁed risk initially facing Australia’s ﬁre ant eradication program was that the invasion had spread further than expected. Earlier consideration of this risk could {{have led to a}} more robust strategy involving a larger area managed in the program’s early stages. This strategy could potentially have achieved eradication at relatively low cost without signiﬁcantly increasing known and quantiﬁed risks. Our ﬁndings demonstrate that focusing on known and <b>quantiﬁable</b> <b>risks</b> can increase the vulnerability of eradication programs to known but non-quantiﬁed risks. This highlights the importance of including robustness to potentially large but non-quantiﬁed risks as a mandatory criterion in evaluations of invasive species eradication programs...|$|R
40|$|In the {{aftermath}} of the 2008 crisis, scholars have begun to revise their conceptions of how market participants interact. While the traditional “rationalist optic” posits market participants who are able to process decisionrelevant information and thereby transform uncertainty into <b>quantifiable</b> <b>risks,</b> the increasingly popular “sociological optic” stresses the role of uncertainty in expectation formation and social conventions for creating confidence in markets. Applications of the sociological optic to concrete regulatory problems are still limited. By subjecting both optics to the same regulatory problem—the role of credit rating agencies (CRAs) and their ratings in capital markets—this paper provides insights into whether the sociological optic offers advice to tackle concrete regulatory problems and discusses the potential of the sociological optic in complementing the rationalist optic. The empirical application suggests that the sociological optic is not only able to improve our understanding of the role of CRAs and their ratings, but also to provide solutions complementary to those posited by the rationalist optic...|$|R
5000|$|In motion pictures, gap/supergap {{financing}} {{is a form}} of {{mezzanine debt}} financing where the producer wishes to complete their film finance package by procuring a loan that is secured against the film's unsold territories and rights. Most gap financiers will only lend against the value of unsold foreign (non-North American) rights, as domestic (North American: USA & Canadian) rights are seen as a [...] "performance" [...] risk, as opposed to more <b>quantifiable</b> <b>risk</b> that is the foreign market. In short, this means that the foreign value of a film can be ascertained by a foreign sales company/agent by evaluating the blended value {{of the quality of the}} script, its genre, cast, director, producer, as well as whether it has theatrical distribution in the US from a major film studio; all of this is taken into consideration and applied against the historical and current market tastes, trends, and needs of each foreign territory of country. This is still an unpredictable practice. Domestic distribution is also unpredictable and far from ever a sure thing (e.g. just because a film has a big budget and a commercial genre and cast, it could still be unwatchable and thus never receive a theatrical or television release in the US, thus being relegated to being a big budget, direct-to-video film.) Any certainty in the entertainment business, lending against foreign value estimates is preferable to betting on strictly a domestic success (comedies and urban films being two notable exceptions: they are referred to as [...] "domestic pieces" [...] or [...] "domestic plays".) ...|$|E
40|$|We {{introduce}} {{a framework for}} strategic asset allocation with alternative investments. Our framework uses a <b>quantifiable</b> <b>risk</b> preference parameter, λ, instead of a utility function. We account for higher moments of the return distributions and approximate best-fit distributions. Thus, we replace the empirical return distributions with two normal distributions. We then use these in the strategic asset allocation. Our framework yields better results than Markowitz's framework. Furthermore, our framework better manages regime switches that occur during crises. To test the robustness of our results, we use a battery of robustness checks and find stable results...|$|E
40|$|This chapter {{introduces}} {{the technique of}} microbial risk assessment and outlines its development from a simple approach based upon a chemical risk model to an epidemiologically-based model that accounts for, among other things, secondary transmission and protective immunity. Two case studies are presented to highlight the different approaches. 8. 1 BACKGROUND <b>Quantifiable</b> <b>risk</b> assessment was initially developed, largely, to assess human health risks associated with exposure to chemicals (NAS 1983) and, in its simplest form, consists of four steps, namely: hazard assessment exposure assessment dose–response analysis risk characterisation...|$|E
25|$|Uncertainty in {{economics}} is an unknown prospect of gain or loss, whether <b>quantifiable</b> as <b>risk</b> or not. Without it, household behaviour would be unaffected by uncertain employment and income prospects, financial and capital markets would reduce to exchange {{of a single}} instrument in each market period, {{and there would be}} no communications industry. Given its different forms, there are various ways of representing uncertainty and modelling economic agents' responses to it.|$|R
40|$|The {{last several}} decades have {{witnessed}} a paradigm shift in environmental planning and watershed management from a top-down government-agency-driven process toward a more collaborative, grass-roots approach that includes stakeholder participation, problem solving, and consensus building. Rather than focusing on specific pollutants, pollution sources, or specific areas within the watershed landscape, the new community-based approach (a) fosters a coordinated and efficient implementation of management programs to reduce pollutant discharges, maintain and improve water quality, and restore valuable habitats; (b) establishes local priorities {{within the context of}} regional and national goals and coordinates private and public actions; (c) integrates the abiotic and biotic components as well as the human and economic factors into the planning and management decisions; (d) focuses on <b>quantifiable</b> <b>risks</b> and benefits and measurable outcomes; and (e) establishes a process for involving non-governmental organizations and citizens through formal and informal meetings. Under this approach, the watershed represents an appropriate unit or hydro-political boundary for unifying the planning and management process and for producing the desired outcomes (Thorud et al...|$|R
40|$|The {{attempt to}} provide a {{regulatory}} environment for xenotransplantation is giving rise to different scientific-normative models, aimed at balancing individual rights and public interests {{in dealing with the}} uncertainties deriving from the potential of xenogenetic infections. These models express different ways of constructing risks and of thinking about unknown, unprecedented and potentially irreversible events, reducing them to identifiable and <b>quantifiable</b> <b>risks,</b> identifiable but non-quantifiable uncertainties, or undetermined uncertainties. In all these constructs, scientific models and normative models interact with each others by prioritizing the different values involved –both from a scientific and from a legal point of view-, and so generating different orders of knowledge and rights-or-prescriptions. Particular assumptions about scientific issues are compatible and can be combined with some legal prescriptions or legal guarantees; particular assumptions about the legal context or rights are coherent with the adoption of determined visions of science. From time to time, a privileged vision of science will frame the legal tools involved in the regulatory activity; or fundamental legal guarantees will shape the vision of scientific issues accordingly. This contribution explores three different models of co-production between science and the law in the field of xenotransplantation, namely the US-PHS Guidelines (2001), the Canadian and the Australian regulatory approach (2002), and the Council of Europe Guidelines (2003) ...|$|R
40|$|This paper {{challenges}} {{the limitations of}} traditional <b>quantifiable</b> <b>risk</b> analysis demonstrating how {{this is a major}} contributor to significant software failures. The Software Development Impact Statement (SoDIS) process described is a new approach to risk analysis which is proven to reduce significantly the likelihood of software failure. The process has been applied in a range of projects, for example, electronic voting for Office of the Deputy Prime Minister, inventory system for Keane Inc. (USA) and personnel database for Commercial Information Systems (NZ) Ltd in New Zealand. SoDIS has attracted interest from professional bodies, e. g. IMIS...|$|E
40|$|The {{importance}} of understanding supply chain sustainability is being realized by increasingly more people, including corporate managers, investors, policy makers, customers and other stakeholders. A lot of practitioners and academic researchers have addressed {{this issue in}} past few years. However, most of their studies lack systematic thinking and are not quantifiable. Thus, a systematic and quantifiable model which incorporates economic, environmental and social factors is needed. In our study, a systematic and <b>quantifiable</b> <b>risk</b> assessment model based {{on the concept of}} “Triple Bottom Line” is developed in order to solve supply chain sustainability problem from risk assessment perspectiv...|$|E
30|$|In [21], the EU funded project SECCRIT {{enumerated}} {{very relevant}} cloud risk scenarios systematically, {{in a similar}} fashion to what ENISA [1] did a few years earlier, but this time they evaluated the risk perception from the users point of view. They used a survey-based methodology to ask respondents to rank risks in a standard way - assessing probability and impact. CARAM is innovative when compared with this approach in two ways: it is using real information disclosed by cloud providers to estimate the likelihood of threats affecting the cloud would become concrete. Second, it allows the user of the methodology to focus on what they know best, their assets: the methodology only requires to assign priorities to assets in order to provide <b>quantifiable</b> <b>risk</b> information.|$|E
40|$|Risks {{associated}} with {{the development and implementation}} of Renewable Energy (RE) technologies are a major concern both in terms of financing for manufacturers and for the financing viability of projects. Insurance is often perceived as a mandatory and costly cover to secure financing for a project and is rarely considered as a way of improving the overall financing conditions and perspectives. For certain RE projects, such as wind farm or biomass, the use of probabilistic models integrating <b>quantifiable</b> <b>risks</b> in the financing context provides a better overview of the risk potential impacts and the insurance merits. The purpose of this approach is to appraise the most efficient insurance package for the project. The probability distribution of the project cash flows, including risk assumptions, allows one to assess the merits of various insurance options and in particular to test their benefits on the rating of debt and financial covenants. This modelling approach provides a good indicator of the potential risk impacts for RE projects and is also a useful tool to determine the optimum allocation of risks, costs and reserves between the various parties involved in such a project. The Geneva Papers (2008) 33, 147 – 152. doi: 10. 1057 /palgrave. gpp. 2510156...|$|R
40|$|Abstract Background A {{new class}} of immuno-oncology agents has {{recently}} been shown to induce long-term survival in a proportion of treated patients. This phenomenon poses unique challenges for the prediction of analysis time in event-driven studies. If the phenomenon of long-term survival is not accounted for properly, {{the accuracy of the}} prediction based on the existing methods may be substantially compromised. Methods Parametric mixture cure rate models with the best fit to empirical clinical trial data were proposed to predict analysis times in immuno-oncology studies {{during the course of the}} study. The proposed prediction procedure also accounts for the mechanism of action introduced by cancer immunotherapies, such as delayed and long-term survival effects. Results The proposed methodology was retrospectively applied to a randomized phase III immuno-oncology clinical trial. Among various parametric mixture cure rate models, the Weibull cure rate model was found to be the best-fitting model for this study. The unique survival kinetics of cancer immunotherapy was captured in the longitudinal predictions of the final analysis times. Conclusions Parametric mixture cure rate models, along with estimated long-term survival rates, probabilities of study incompletion, and expected statistical powers over time, provide immuno-oncology clinical trial researchers with a useful tool for continuous event monitoring and prediction of analysis times, such that informed decisions with <b>quantifiable</b> <b>risks</b> can be made for better resource and logistic planning...|$|R
40|$|Predicting {{analysis}} {{times in}} randomized clinical trials with cancer immunotherapy Tai-Tsang Chen 1, 2 Background: A {{new class of}} immuno-oncology agents has recently been shown to induce long-term survival in a proportion of treated patients. This phenomenon poses unique challenges for the prediction of analysis time in event-driven studies. If the phenomenon of long-term survival is not accounted for properly, {{the accuracy of the}} prediction based on the existing methods may be substantially compromised. Methods: Parametric mixture cure rate models with the best fit to empirical clinical trial data were proposed to predict analysis times in immuno-oncology studies {{during the course of the}} study. The proposed prediction procedure also accounts for the mechanism of action introduced by cancer immunotherapies, such as delayed and long-term survival effects. Results: The proposed methodology was retrospectively applied to a randomized phase III immuno-oncology clinical trial. Among various parametric mixture cure rate models, the Weibull cure rate model was found to be the best-fitting model for this study. The unique survival kinetics of cancer immunotherapy was captured in the longitudinal predictions of the final analysis times. Conclusions: Parametric mixture cure rate models, along with estimated long-term survival rates, probabilities of study incompletion, and expected statistical powers over time, provide immuno-oncology clinical trial researchers with a useful tool for continuous event monitoring and prediction of analysis times, such that informed decisions with <b>quantifiable</b> <b>risks</b> can be made for better resource and logistic planning...|$|R
40|$|Annual Meeting, Chicago) for helpful {{comments}} and suggestions. All remaining errors are our own. Strategic Asset Allocation and the Role of Alternative Investments In this paper, we provide a realistic framework that investors can use for their strategic asset allocation with alternative investments (buyouts, commodities, hedge funds, REITs, and venture capital). Our {{approach is not}} based on a utility function, but on an easily <b>quantifiable</b> <b>risk</b> preference parameter, λ. We account for higher moments of the return distributions within our optimization framework and approximate best-fit distributions. Thus, we replace the empirical return distributions, which are often skewed and/or exhibit excess kurtosis, with two normal distributions. We then use the estimated return distributions in the strategic asset allocation. Our results show in out-of-sample analyses that our framework yields superior results compared to the Markowitz framework. It also features better abilities to manage regime switches, which tend to occur frequentl...|$|E
40|$|This paper {{addresses}} {{the expansion of}} risk practices through {{a case study of}} a government led project in Sweden purposed to develop a method to include social events in mandatory risk practices. Social heterogeneity was to be transformed into straightforward causality in order to turn the social into a manageable object. In this regard, the project was quite successful. By inviting social scientists into the process, otherwise often marginalized within risk practice, causality and <b>quantifiable</b> <b>risk</b> factors could be established and the model became a rigorous and legitimate scientific model. Although experts were granted significant autonomy and became experts far beyond their own area of expertise, the success of the model lies rather in allowing experts authority within confined boundaries. Grand narratives and critical perspectives are disregarded as too abstract and if social scientists are to infuse aspects of social critique they must adapt to these circumstances: they must become instrumentalists...|$|E
40|$|Reengineering of {{operational}} {{legacy system}} is a novel technique for software rejuvenation. Reengineering is used specifically to satisfy and even delight modern customers and market with the value of our software products and services to gain their loyalty and repeat business. However, it incurs some overhead in terms of risk. The basic necessity for the successful implementation of reengineering strategy is to measure the overall impact of different reengineering risk components that arises from system, managerial and technical domain of legacy system. <b>Quantifiable</b> <b>risk</b> measures are necessary for the measurement of reengineering risk to take decision about when the evolution of legacy system through reengineering is successful. We present a quantifiable measurement model to measure comprehensive impact of different reengineering risk arises from quality perspective of legacy system. The model consists of five reengineering risk component, including Maintainability risk, Project complexity risk, Software architecture risk, Training Risk and Security risk component. Proposed measurement model offers better performance in terms of risk measurement to support the decision-making process...|$|E
40|$|We {{propose a}} {{categorical}} data synthesizer with a <b>quantifiable</b> disclosure <b>risk.</b> Our algorithm, named Perturbed Gibbs Sampler, can handle high-dimensional categorical {{data that are}} often intractable to represent as contingency tables. The algorithm extends a multiple imputation strategy for fully synthetic data by utilizing feature hashing and non-parametric distribution approximations. California Patient Discharge data are used to demonstrate statistical properties of the proposed synthesizing methodology. Marginal and conditional distributions, {{as well as the}} coefficients of regression models built on the synthesized data are compared to those obtained from the original data. Intruder scenarios are simulated to evaluate disclosure risks of the synthesized data from multiple angles. Limitations and extensions of the proposed algorithm are also discussed. ...|$|R
40|$|Is it {{possible}} to obtain an objective and <b>quantifiable</b> measure of <b>risk</b> backed up by choices made by some specific groups of rational investors? To answer this question, {{in this paper we}} establish some behavior foundations for various types of VaR models, including VaR and conditional-VaR, as measures of downside risk. In this paper, we will establish some logical connections among VaRs, conditional-VaR, stochastic dominance, and utility maximization. Though supported to some extents with unanimous choices by some specific groups of expected or non-expected-utility investors, VaRs as profiles of risk measures at various levels of risk tolerance are not quantifiable - they can only provide partial and incomplete risk assessments for risky prospects. We also include in our discussion the relevant VaRs and several alternative risk measures for investors; these alternatives use somewhat weaker assumptions about risk-averse behavior by incorporating a mean-preserving-spread. For this latter group of investors, we provide arguments for and against the standard deviation versus VaR and conditional-VaRs as objective and <b>quantifiable</b> measures of <b>risk</b> in the context of portfolio choice. Decision analysis Risk analysis Risk attributes Utility Stochastic dominance...|$|R
40|$|In {{the wake}} of the sub-prime crisis of 2008, the European Insurance and Occupational Pensions Authority issued the Solvency II directive, aiming at {{replacing}} the obsolete Solvency I framework by 2016. Among the quantitative requirements of Solvency II, a measure for an insurance firms solvency risk, the solvency risk capital, is found. It aims at establishing the amount of equity the company needs to hold to be able to meet its insurance obligations with a probability of 0. 995 over the coming year. The SCR of a company is essentially built up by the SCR induced by a set of <b>quantifiable</b> <b>risks.</b> Among these, risks originating from the take up rate of contractual options, lapse risks, are included. In this thesis, the contractual options of a life insurer have been identified and risk factors aiming at capturing the risks arising are suggested. It has been concluded that a risk factor estimating the size of mass transfer events captures the risk arising through the resulting rescaling of the balance sheet. Further, a risk factor modeling the deviation of the Company's assumption for the yearly transfer rate is introduced to capture the risks induced by the characteristics of traditional life insurance and unit-linked insurance contracts upon transfer. The risk factors are modeled in a manner to introduce co-dependence with equity returns as well as interest rates of various durations and the model parameters are estimated using statistical methods for Norwegian transfer-frequency data obtained from Finans Norge. The univariate and multivariate properties of the models are investigated in a scenario setting and it is concluded the the suggested models provide predominantly plausible results for the mass-lapse risk factors. However, the performance of the models for the risk factors aiming at capturing deviations in the transfer assumptions are questionable, why two means of increasing its validity have been proposed...|$|R
40|$|Predicting the {{magnitude}} of enemy release in host–pathogen systems after introduction of novel disease resistance genes has become a central problem in ecology. Here, we develop a general quantitative framework for predicting changes in realized niche size and intrinsic population growth rate after introgression of disease resistance genes into wild host populations. We then apply this framework to a model host–pathogen system targeted by genetically modified and conventionally bred disease-resistant host lines (Trifolium repens lines expressing resistance to Clover yellow vein potyvirus) and show that, under a range of ecologically realistic conditions, the introduction of novel pathogen resistance genes into host populations can pose a <b>quantifiable</b> <b>risk</b> to associated nontarget native plant communities. In the host–pathogen system studied, we predict that pathogen release could result {{in an increase in}} the intrinsic rate of population growth of up to 15 % and the expansion of host populations into some marginal environments. This approach has general applicability to the ecological risk assessment of all novel disease-resistant plant genotypes that target coevolutionary host–pathogen systems for improvement of agricultural productivity...|$|E
30|$|Most formal models do not aim {{to measure}} risks themselves, {{but rather to}} {{integrate}} <b>quantifiable</b> <b>risk</b> factors into the formalized decision-making approach. Although the determination and definition of risk categories are elaborated by conceptual and empirical studies, {{the relative importance of}} risk factors can, for instance, be assessed using formal models like the AHP (see, e.g., Govindan et al. 2014 b). Out of 20 models that integrate SSCM risks, nine stochastic models (seven descriptive-stochastic, two normative-stochastic) are detected. In addition, 11 deterministic models (nine normative-deterministic, two descriptive-deterministic) include SSCM risks. The decision makers that are addressed by formal models are mostly corporate actors. For instance, in the context of supplier evaluation and selection, Bai and Sarkis (2010) use rough set methodology and include four company decision-makers from operations, finance, purchasing, and environmental management departments. These decision makers can also be seen as the internal stakeholders that mediate the pressures and incentives for SSCM from external stakeholders. Other model applications in focal companies concern offshoring decisions (Dou and Sarkis 2010), whereas some macroscopic models also target local authorities or regional governments as decision-makers (e.g., Georgopoulou et al. 1998).|$|E
40|$|This paper {{introduces}} a new computational framework {{to account for}} uncertainties in day-ahead electricity market clearing process {{in the presence of}} demand response providers. A central challenge when dealing with many demand response providers is the uncertainty of its realization. In this paper, a new economic dispatch framework that is based on the recent theoretical development of the scenario approach is introduced. By removing samples from a finite uncertainty set, this approach improves dispatch performance while guaranteeing a <b>quantifiable</b> <b>risk</b> level with respect to the probability of violating the constraints. The theoretical bound on the level of risk is shown to be a function of the number of scenarios removed. This is appealing to the system operator for the following reasons: (1) the improvement of performance comes at the cost of a quantifiable level of violation probability in the constraints; (2) the violation upper bound does not depend on the probability distribution assumption of the uncertainty in demand response. Numerical simulations on (1) 3 -bus and (2) IEEE 14 -bus system (3) IEEE 118 -bus system suggest that this approach could be a promising alternative in future electricity markets with multiple demand response providers...|$|E
40|$|Past {{research}} on “environmental justice ” has often failed to systematically link hazard proximity with <b>quantifiable</b> health <b>risks.</b> The authors employ {{recent advances in}} air emissions inventories and modeling techniques to consider {{a broad range of}} outdoor air toxics in Southern California and to calculate the potential lifetime cancer risks associated with these pollutants. They find that such risks are attributable mostly to transportation and small-area sources and not the usually tar-geted large-facility pollution emissions. Multivariate regression suggests that race plays an explanatory role in risk distribution even after controlling for other economic, land-use, and pop-ulation factors. This pattern suggests the need for innovative emissions reduction efforts as well as specific strategies to alter the spatial and racial character of the environmental “riskscape ” in urban centers. ENVIRONMENTAL INEQUITY AND AIR POLLUTION: THE LITERATURE AND METHODOLOGICAL CHALLENGE...|$|R
40|$|Purpose: The {{purpose of}} this paper is to present a generic {{framework}} to assess and simulate outsourcing risks in the supply chain. Design/methodology/ approach: This combination approach involves a qualitative risk analysis methodology termed as the supply chain risk-failure mode and effect analysis (SCR-FMEA) which integrates risk identification, analysis and mitigation actions together to evaluate supply chain outsourcing risk. The qualitative risk assessment will allow risk manager to provide a visual presentation of imminent risks using the risk map. Monte Carlo simulation (MCS) on the imminent risks of delivery outsourcing using the Milk-Run system is adopted. Findings: With basic statistical concepts, key performance variables and the risk of delivery outsourcing are analyzed. It is found that a newly implemented delivery outsourcing arrangement on the Milk-Run system reduces the average customer lead-time and total cost. However, a certain extent of risk or uncertainty can still be detected due to the presence of variation. Research limitations/implications: This paper reveals that company can manage the risk by adopting a systematic method for identifying the potential risks before outsourcing and MCS can be applied for examining the <b>quantifiable</b> <b>risks</b> such as lead time and cost. Practical implications - The paper provides a generic guideline for practitioners to assess logistics outsourcing, especially for logistics management consultants and professionals for evaluating the risk and impact of outsourcing. It is believed that the proposed risk assessment framework can help to analyze the operational cost uncertainty and ensure the stability of the supply chain. However, the limitation of this research is that the full spectrum of outsourcing risk, especially the non-quantifiable risk may not be analyzed by MCS. Originality/value: This paper proposed an integrated framework which combines qualitative and quantitative method together for managing outsourcing risk. This research provides a standardized metric to quantify risk in the supply chain so as to determine the effectiveness of outsourcing. Department of Industrial and Systems Engineerin...|$|R
40|$|The {{quantification}} of diversification benefits due to risk aggregation plays {{a prominent}} role in the (regulatory) capital management of large firms within the financial industry. However, the complexity of today's risk landscape makes a <b>quantifiable</b> reduction of <b>risk</b> concentration a challenging task. In the present paper we discuss {{some of the issues that}} may arise. The theory of second-order regular variation and second-order subexponentiality provides the ideal methodological framework to derive second-order approximations for the risk concentration and the diversification benefit. (C) 2010 Elsevier B. V. All rights reserved...|$|R
40|$|The {{well-known}} Knightian {{distinction between}} <b>quantifiable</b> <b>risk</b> and unquantifiable uncertainty {{is at odds}} with the dominant subjectivist conception of probability associated with de Finetti, Ramsey and Savage. Risk and uncertainty are rendered indistinguishable on the subjectivist approach insofar as an individual’s subjective estimate of the probability of any event can be elicited from the odds at which she would be prepared to bet for or against that event. The risk/uncertainty distinction has however never quite gone away and is currently under renewed theoretical scrutiny. The {{purpose of this article is}} to show that de Finetti’s understanding of the distinction is more nuanced than is usually admitted. Relying on usually overlooked excerpts of de Finetti’s works commenting on Keynes, Knight and interval valued probabilities, we argue that de Finetti suggested a relevant theoretical case for uncertainty to hold even when individuals are endowed with subjective probabilities. Indeed, de Finetti admitted that the distinction between risk and uncertainty is relevant when different individuals sensibly disagree about the probability of the occurrence of an event. We conclude that the received interpretation of de Finetti’s understanding of subjective probability needs to be qualified on this front...|$|E
40|$|Suicide {{rates are}} higher {{in later life}} {{than in any other}} age group. The design of {{effective}} suicide prevention strategies hinges on the identification of specific, <b>quantifiable</b> <b>risk</b> factors. Methodological challenges include the lack of systematically applied terminology in suicide and risk factor research, the low base rate of suicide, and its complex, multidetermined nature. Although variables in mental, physical, and social domains have been correlated with completed suicide in older adults, controlled studies are necessary to test hypothesized risk factors. Prospective cohort and retro-spective case control studies indicate that affective disor-der is a powerful independent risk factor for suicide in elders. Other mental illnesses play less of a role. Physical illness and functional impairment increase risk, but their influence appears to be mediated by depression. Social ties and their disruption are significantly and indepen-dently associated with risk for suicide in later life, rela-tionships between which may be moderated by a rigid, anxious, and obsessional personality style. Affective illness is a highly potent risk factor for suicide in later life with clear implications for the design of prevention strategies. Additional research is needed to define more precisely the interactions between emotional, physical, and social factors that determine risk for suicide in the older adult. Biol Psychiatry 2002; 52 : 193 – 20...|$|E
40|$|Objectives: This {{study used}} {{validated}} physical performance measures to examine function, risk of adverse health outcomes, {{and the relationship}} with allocated hours of weekly caregiving assistance among older adults receiving home and community-based services through a Medicaid waiver program. Methods: Older adults (n[*]=[*] 42) completed physical performance measures including grip strength, 30 -s chair rise, Timed Up and Go, and gait speed. Demographic information including age, gender, and allocated hours of weekly caregiving assistance were also collected. Results: A majority, 72 % of females and 86 % of males, had weak grip strength, 57 % met criteria for fall risk based on their Timed Up and Go score, 83 % had lower extremity strength impairments, and 98 % were unable to ambulate more than 1. 0 [*]m/s. Frailty was prevalent in the sample with 72 % of clients meeting Fried’s frailty criteria. The most significant predictors of allocated hours of weekly caregiving assistance approved for clients were race and gait speed. Conclusion: Based on scores on physical performance measures, clients {{are at risk of}} falls, hospitalization, and mortality, and scores indicate an urgent need to assess performance in addition to self-reported activities of daily living limitations for this population. Performance measures associated with <b>quantifiable</b> <b>risk</b> of adverse outcomes can be critical indicators for referrals and services needed to enhance the safety and improve care outcomes for homebound older adults...|$|E
30|$|This {{research}} examined timber investment returns, policy factors, {{and risks}} as of 2011. The objectives {{of this project}} were to: (1) estimate comparative timber investment returns, not including land costs, for important forest species and countries throughout the world; (2) synthesise <b>quantifiable</b> literature on <b>risks</b> among those countries; (3) estimate the effects of land costs, environmental regulations and forest productivity on the base returns for selected countries in the Americas; (4) examine the trends in these forest and plantation returns over time; and (5) compare those timber investment returns with traditional equity and debt investment returns of stocks and bonds.|$|R
40|$|This paper {{proposes a}} {{categorical}} data synthesizer algorithm that guarantees a <b>quantifiable</b> disclosure <b>risk.</b> Our algorithm, named Perturbed Gibbs Sampler (PeGS), can handle high-dimensional categorical {{data that are}} intractable if represented as contingency tables. PeGS involves three intuitive steps: 1) disintegration, 2) noise injection, and 3) synthesis. We first disintegrate the original data into building blocks that (approximately) capture essential statistical characteristics of the original data. This process is efficiently implemented using feature hashing and non-parametric distribution approximation. In the next step, an optimal amount of noise is injected into the estimated statistical building blocks to guarantee differential privacy or l-diversity. Finally, synthetic samples are drawn using a Gibbs sampler approach. California Patient Discharge data are used to demonstrate statistical properties of the proposed synthetic methodology. Marginal and conditional distributions as well as regression coefficients obtained from the synthesized data are compared to those obtained from the original data. Intruder scenarios are simulated to evaluate disclosure risks of the synthesized data from multiple angles. Limitations and extensions of the proposed algorithm are also discussed...|$|R
30|$|We {{define the}} margin of {{exposure}} as the hazard quotient (HQ), quantified by dividing the exposure potential of NMs concentration in air by the threshold value OEL. The resultant value represents the initial deterministic risk forecast generated by the BN risk assessment model. If the forecasted exposure level {{is greater than the}} threshold dose, i.e. HQ[*]>[*] 100 %, there exists potential risks of adverse human health implications for the particular NM. The HQ can be refined through a robust learning process in BN and offers a coherent and <b>quantifiable</b> human health <b>risk</b> assessment. It consolidates exposure, hazard and dose-response assessments into a single risk forecast.|$|R
