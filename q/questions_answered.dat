673|10000|Public
5|$|Medal of Honor {{recipients}} {{may apply}} in {{writing to the}} headquarters of the service branch of the medal awarded for a replacement or display Medal of Honor, ribbon, and appurtenance (Medal of Honor flag) without charge. Primary next of kin may also do the same and have any <b>questions</b> <b>answered</b> in regard to the Medal of Honor that was awarded.|$|E
5|$|Some {{participants}} {{tried to}} track Russian down {{to have their}} <b>questions</b> <b>answered.</b> One contestant, Louise Miles, discovered that Russian's production company, NRP, did not actually exist and that {{the woman who had}} been answering their phone calls was really Russian's mother, Margaret. Another participant, Daniel Pope, managed to track Russian down to an address in Richmond upon Thames and convinced him to be interviewed by Christmas TV and apologise on camera. As Russian had not actually taken money from his victims, he had not committed a crime, and a civil case was not pursued {{due to a lack of}} funds.|$|E
25|$|On May 19, 1991, the Croatian {{authorities}} {{held the}} Croatian referendum on independence. Serb local authorities {{called for a}} boycott of the vote, which was largely followed by Croatian Serbs. In the end, a majority of Croatians endorsed independence from Yugoslavia, with a turnout of 83.56% and the two referendum <b>questions</b> <b>answered</b> positively by 93.24% and 92.18% (resp.) {{of the total number}} of votes.|$|E
40|$|In today’s {{world of}} {{technological}} advancement, <b>Question</b> <b>Answering</b> {{has emerged as}} the key area for the researchers. In <b>Question</b> <b>Answering</b> user is provided with specific answers instead of large number of documents or passages. <b>Question</b> <b>Answering</b> has been carried out in many languages. This paper compares some already existing <b>Question</b> <b>Answering</b> Systems for Chinese and Japanese languages. Different Chinese and Japanese <b>Question</b> <b>Answering</b> Systems are compared {{on the basis of their}} performance in different workshops held for <b>Question</b> <b>Answering</b> Systems. We also discuss the best approach out of all and the reasons for which it is considered good and some methods for further improvement in those technique...|$|R
40|$|There {{are many}} <b>question</b> <b>answer</b> sites are {{available}} now a days. Community <b>question</b> <b>answering</b> sites are efficient when {{compare with the}} automate <b>question</b> <b>answering</b> sites. The drawback of available community <b>question</b> <b>answering</b> system {{is that it can}} only provide textual answer. In this paper, we propose a scheme that enriches the textual answer with multimedia data. Our scheme consists of four components: natural language processing, qa pair extraction, query generation, multimedia data selection and presentation. The <b>question</b> <b>answer</b> pair is extracted from the available community <b>question</b> <b>answering</b> sites database by online. Query is generated for the multimedia data. The resulting data is selected and present to the user...|$|R
40|$|<b>Question</b> <b>Answering</b> (QA) is {{a focused}} way of {{information}} retrieval. <b>Question</b> <b>Answering</b> system {{tries to get}} back the accurate <b>answers</b> to <b>questions</b> posed in natural language provided a set of documents. Basically <b>question</b> <b>answering</b> system (QA) has three elements i. e. question classification, information retrieval (IR), and answer extraction. These elements {{play a major role}} in <b>Question</b> <b>Answering.</b> In <b>Question</b> classification, the questions are classified depending upon the type of its entity. Information retrieval component is used to determine success by retrieving relevant <b>answer</b> for different <b>questions</b> posted by the intelligent <b>question</b> <b>answering</b> system. Answer extraction module is growing topics in the QA in which ranking and validating a candidate’s answer is the major job. This paper offers a concise discussion regarding different <b>Question</b> <b>Answering</b> types. In addition we describe different evaluation metrics used to evaluate the performance of different <b>question</b> <b>answering</b> systems. We also discuss the recent <b>question</b> <b>answering</b> systems developed and their corresponding techniques...|$|R
25|$|The quiz is {{the primary}} device of {{eliminating}} contestants across all series. Traditionally between ten and twenty questions, the quiz asks the players to identify the Mole and several pieces of information regarding the Mole, including the Mole's activity in challenges, biographical profile, their fashion and/or culinary choices during the show, etc. The player scoring lowest on the quiz (and taking the longest time to do so, {{in the event of}} tied scores) is eliminated from the game. The US version saw a format change from twenty questions to ten questions from Season 2 on, usually with all questions available to the public. Season 5 of the Australian series had contestants answer five questions about the Mole (not filmed), plus a further five questions for the live eliminations. On two separate occasions, the live elimination computer quiz system malfunctioned and didn't record some contestants' responses. The rules stated that if this were to happen the elimination would be based on the five <b>questions</b> <b>answered</b> before the live show.|$|E
500|$|With {{the rise}} of the Internet, debates about 0.999... have become commonplace on newsgroups and message boards, {{including}} many that nominally {{have little to do with}} mathematics. In the newsgroup , arguing over 0.999... is described as a [...] "popular sport", and it is one of the <b>questions</b> <b>answered</b> in its FAQ. The FAQ briefly covers , multiplication by 10, and limits, and it alludes to Cauchy sequences as well.|$|E
500|$|... "The Job" [...] {{was written}} by Paul Lieberstein and Michael Schur. It was {{directed}} by Ken Kwapis, who had, around the same time, directed The Office actor John Krasinski in the 2007 film License to Wed. Script reading for the episode took place on a beach during the filming of the season's twenty-third episode, [...] "Beach Games". Actress Kate Flannery remarked that [...] "we were so excited that we almost had another hot dog eating contest. Not. It's a great script. Lots of <b>questions</b> <b>answered.</b> Lots. The Office fans have been anxiously awaiting a one-hour episode, and guess what? You got it." [...] The original cut of the episode was an hour and twelve minutes long, {{and had to be}} edited down to forty-two minutes of screentime. It was the second Office episode to fill the entire hour timeslot; the first was the third-season episode [...] "A Benihana Christmas".|$|E
40|$|<b>Question</b> <b>Answering</b> (QA) is a {{specific}} type of information retrieval. Given a set of documents, a <b>Question</b> <b>Answering</b> system attempts to find out the correct <b>answer</b> to the <b>question</b> pose in natural language. <b>Question</b> <b>answering</b> is multidisciplinary. It involves information technology, artificial intelligence, natural language processing, knowledge and database management and cognitive science. From the technological perspective, <b>question</b> <b>answering</b> uses natural or statistical language processing, information retrieval, and knowledge representation and reasoning as potential building blocks. It involves text classification, information extraction and summarization technologies. In general, <b>question</b> <b>answering</b> system (QAS) has three components such as question classification, information retrieval, and answe...|$|R
40|$|Video <b>Question</b> <b>Answering</b> is a {{challenging}} problem in visual information retrieval, which provides {{the answer to}} the referenced video content according to the question. However, the existing visual <b>question</b> <b>answering</b> approaches mainly tackle the problem of static image question, which may be ineffectively for video <b>question</b> <b>answering</b> due to the insufficiency of modeling the temporal dynamics of video contents. In this paper, we study the problem of video <b>question</b> <b>answering</b> by modeling its temporal dynamics with frame-level attention mechanism. We propose the attribute-augmented attention network learning framework that enables the joint frame-level attribute detection and unified video representation learning for video <b>question</b> <b>answering.</b> We then incorporate the multi-step reasoning process for our proposed attention network to further improve the performance. We construct a large-scale video <b>question</b> <b>answering</b> dataset. We conduct the experiments on both multiple-choice and open-ended video <b>question</b> <b>answering</b> tasks to show the effectiveness of the proposed method. Comment: Accepted for SIGIR 201...|$|R
50|$|TeLQAS (Telecommunication Literature <b>Question</b> <b>Answering</b> System) is an {{experimental}} <b>question</b> <b>answering</b> system developed for <b>answering</b> English <b>questions</b> in the telecommunications domain.|$|R
2500|$|Other {{internal}} features include; the Microsoft Auditorium on {{the ground}} floor, the [...] "Living Room" [...] {{on the third floor}} (designed as a space for patrons to read), the Charles Simonyi Mixing Chamber (a version of a reference desk that provides interdisciplinary staff help for patrons who want to have <b>questions</b> <b>answered</b> or do research), and the Betty Jane Narver Reading Room on level 10 (with views of Elliott Bay).|$|E
2500|$|Fard wrote several lessons {{which are}} read and {{committed}} to memory {{by members of the}} Nation of Islam. Some of the lessons are in the form of questions asked by Fard to Elijah Muhammad. One such lesson concludes with the text: [...] "This Lesson No. 2 was given by our Prophet, W.D. Fard, which contains 40 <b>questions</b> <b>answered</b> by Elijah Muhammad, one of the lost found in the wilderness of North America February 20th, 1934." ...|$|E
2500|$|Ginzberg began {{teaching}} Talmud at the Jewish Theological Seminary from its reorganization in 1902 {{until his death}} in 1953. [...] For fifty years he trained two generations of future Conservative rabbis. During his era, Ginzberg influenced almost every rabbi of the Conservative Movement in a personal way. For some, Louis Ginzberg serves as a role model even today. Today’s leading Conservative posek in Israel, Rabbi David Golinkin, has written profusely on Louis Ginzberg. [...] Golinkin has recently published a collection of responsa containing 93 <b>questions</b> <b>answered</b> by Ginzberg.|$|E
40|$|Abstract — With {{technical}} advancement, <b>Question</b> <b>Answering</b> {{has emerged}} as the main area for the researchers. User is provided with specific answers instead of large number of documents or passages in <b>question</b> <b>answering.</b> <b>Question</b> <b>answering</b> proposes the solution to acquire efficient and exact <b>answers</b> to user <b>question</b> asked in natural language rather than language query. The major goal {{of this paper is to}} develop a hybrid algorithm for <b>question</b> <b>answering.</b> For this task different <b>question</b> <b>answering</b> systems for different languages were studied. After deep study, we are able to develop an algorithm that comprises the best features from excellent systems. An algorithm developed by us performs well...|$|R
40|$|This paper {{introduces}} {{a system for}} searching <b>question</b> <b>answer</b> pairs automatically extracted from the discussions in internet communities. The system, named Fora, aggregates discussions from multiple forums and newsgroups in the same domain, automatically extracts <b>question</b> <b>answer</b> pairs from the data, and provides searches of the <b>question</b> <b>answer</b> pairs. The system also offers expert search, query suggestion, page search, and other features. This paper explains the main features and technologies of Fora. It describes how the system extracts and ranks <b>question</b> <b>answer</b> pairs...|$|R
40|$|The paper studies <b>Question</b> <b>Answering</b> Systems {{that can}} be used in {{education}} as an engine to search for learning and/or other useful information. A wide set of different <b>Question</b> <b>Answering</b> Systems, such as START, Wolfram|Alpha, AllExperts, etc. is described and their main characteristics, benefits and drawbacks are given. The paper also presents four classifications of <b>Question</b> <b>Answering</b> Systems depending on: content, information source, language paradigm and information processing, as well as proposes recommendations for using <b>Question</b> <b>Answering</b> Systems in educatio...|$|R
2500|$|Each exam is scored {{based on}} the number of <b>questions</b> <b>answered</b> {{correctly}} and the number of questions left blank. [...] A student receives 6 points for each question answered correctly, 1.5 points for each question left blank, and no points for incorrect answers. [...] Thus, a student who answers 18 correctly, leaves 5 blank, and misses 2 gets 18×6 + 5×1.5 = 115.5 points. [...] The maximum possible score is 6×25 = 150 points; in 2006, the AMC 12 had a total of 17 perfect scores between its two administrations, while the AMC 10 had 89.|$|E
2500|$|The AMC 8 is scored {{based on}} the number of <b>questions</b> <b>answered</b> {{correctly}} only. [...] There is no penalty for getting a question wrong, and each question has equal value. [...] Thus, a student who answers 23 questions correctly and 2 questions incorrectly receives a score of 23. This is not a standardized test; i.e. no school has to take it, but some schools choose to, mainly to encourage growth in mathematics among their students. Full scorers on the AMC 8 (25/25) get recognition and their picture on the AMCs web site as a top scorer.|$|E
2500|$|In week two the housemates had {{to answer}} various {{questions}} {{in order to win}} sections of a luxury food budget they had ordered, in the game show-style [...] "Meal or No Meal", similarly named after Channel 4's game show [...] "Deal or No Deal". On Day 13 Pete was asked <b>questions</b> <b>answered</b> wrongly in previous rounds. Getting three out of four right, Pete won the House toilet rolls and miscellaneous shopping list items, and two new housemates, Sam and Aisleyne. On Day 14 George left voluntarily. Richard, Lea and Sezer faced the public vote, and on Day 16 Sezer was evicted with 91.6% of the public vote.|$|E
50|$|<b>Question</b> <b>answering</b> heavily {{relies on}} reasoning. There {{are a number}} of <b>question</b> <b>answering</b> systems {{designed}} in Prolog, a logic programming language associated with artificial intelligence.|$|R
40|$|We {{investigate}} {{the problem of}} complex <b>answers</b> in <b>question</b> <b>answering.</b> Complex answers consist of several simple answers. We describe the online <b>question</b> <b>answering</b> system shapaqa, and using data from this system we show {{that the problem of}} complex answers is quite common. We de ne nine types of complex questions, and suggest two approaches, based on answer frequencies, that allow <b>question</b> <b>answering</b> systems to tackle the problem. ...|$|R
40|$|Passage {{retrieval}} is {{an important}} component common to many <b>question</b> <b>answering</b> systems. Because most evaluations of <b>question</b> <b>answering</b> systems focus on end-to-end performance, comparison of common components becomes di#cult. To address this shortcoming, we present a quantitative evaluation of various passage retrieval algorithms for <b>question</b> <b>answering,</b> implemented in a framework called Pauchok. We present three important findings: Boolean querying schemes perform well in the <b>question</b> <b>answering</b> task. The performance di#erences between various passage retrieval algorithms vary with the choice of document retriever, which suggests significant interactions between document retrieval and passage retrieval. The best algorithms in our evaluation employ density-based measures for scoring query terms. Our results reveal future directions for passage retrieval and <b>question</b> <b>answering...</b>|$|R
2500|$|Titsingh {{was very}} keen on having his {{scholarly}} <b>questions</b> <b>answered</b> and showed an enormous inexhaustible thirst for knowledge. Looking at his private correspondence three mottos of his behaviour and values can be identified: {{the rejection of}} money, as it did not satisfy his enormous thirst of knowledge; an acknowledgment and consciousness of the brevity of life and wasting this precious time not with featureless activities; and his desire to die in calmness, as a [...] "forgotten citizen of the world". In this light he displayed the values of a European philosopher of the 18th century, who was as well interested in his fellow Japanese scholars. Therefore, he also acknowledged their intellectual competences and sophistication and contributed to an intense exchange of cultural knowledge between Japan and Europe in the 18th century.|$|E
2500|$|The SAT {{has four}} sections: Reading, Writing and Language, Math (no calculator), and Math (calculator allowed). The test taker may {{optionally}} {{write an essay}} which, in that case, is the fifth test section. The total time for the scored portion of the SAT is three hours (or three hours and fifty minutes if the optional essay section is taken). Some test takers who are not taking the essay may also have a fifth section which is used, at least in part, for the pretesting of questions that may appear on future administrations of the SAT. (These questions {{are not included in}} the computation of the SAT score.) Two section scores result from taking the SAT: Evidence-Based Reading and Writing, and Math. Section scores are reported on a scale of 200 to 800, and each section score is a multiple of ten. A total score for the SAT is calculated by adding the two section scores, resulting in total scores that range from 400 to 1600. There is no penalty for guessing on the SAT: scores are {{based on the number of}} <b>questions</b> <b>answered</b> correctly. In addition to the two section scores, three [...] "test" [...] scores on a scale of 10 to 40 are reported, one for each of Reading, Writing and Language, and Math. The essay, if taken, is scored separately from the two section scores.|$|E
2500|$|Originally {{opened in}} 1990, the Gorilla Research Center is a [...] habitat {{featuring}} a lush naturalistic landscape. The habitat was designed {{in a way}} that encourages the gorillas to roam freely in an environment that replicates, as closely as possible, their native equatorial forest habitat. The exhibit includes two areas, separated by a wall, which provide enough room for two gorilla troops. The exhibit closed in 2004 and reopened in 2006 after undergoing a $2.2 million renovation to raise the exhibit walls from 12 feet to 15 feet and add a visitor's center. The visitor's center is known as the Gorilla Research Station. It features high vantage points and floor-to-ceiling windows where visitors can interact with the gorillas and have their <b>questions</b> <b>answered</b> by on site gorilla guides. The habitat is currently home to two gorilla troops; a bachelor troop and a family troop. The bachelors are named Juba, B'wenzi, Shana, and Zola. Juba and B'wenzi came to the Dallas Zoo in 2011 and Shana and Zola, half-brothers, arrived in 2013. The family troop members are Subira, the silverback, Madge, and her daughter, Shanta, and Megan. The goal was for former silverback Patrick and one of the females to breed but in September 2013 the plan was abandoned due to Patrick's lack of interest in reproducing.|$|E
40|$|This paper {{gives an}} {{overview}} of the NTCIR- 5 Cross-Lingual <b>Question</b> <b>Answering</b> Task (CLQA 1), an evaluation campaign for Cross-Lingual <b>Question</b> <b>Answering</b> technology. This evaluation was carried out in June 2005. In CLQA 1, we aimed to promote research on cross-lingual <b>Question</b> <b>Answering</b> technology mainly for East Asian languages. As the first attempt, we conducted evaluations o...|$|R
40|$|This paper {{provides}} {{a brief summary}} of the ndings of the TREC <b>question</b> <b>answering</b> track to date and discusses the future directions of the track. The paper is extracted from a fuller description of the track given in TREC <b>Question</b> <b>Answering</b> Track" [8]. Complete details about the TREC <b>question</b> <b>answering</b> track {{can be found in}} the TREC proceeding...|$|R
40|$|The <b>Question</b> <b>Answering</b> Challenge (QAC) {{was carried}} out as the first {{evaluation}} task on <b>question</b> <b>answering</b> of the NTCIR Workshop 3 [Fukumoto 2002] [NTCIR]. <b>Question</b> <b>answering</b> in an open domain is a task for obtaining appropriate answers to given domain independent questions written in natural language from a large corpus. The purpose of the QAC was t...|$|R
60|$|His <b>questions</b> <b>answered,</b> {{he turned}} quickly and eyed the awakened sleeper with an {{ambiguous}} expression.|$|E
60|$|We {{began to}} talk {{ourselves}} into a realisation of what our divergent futures might be. I came back {{on the evening of}} that day with my <b>questions</b> <b>answered</b> by a solicitor.|$|E
60|$|These were hasty {{and hurried}} <b>questions,</b> <b>answered</b> as hastily and confusedly, and broken with ejaculations of {{surprise}} {{and thanks to}} Heaven, and to Our Lady, until the ecstasy of delight sobered down {{into a sort of}} tranquil wonder.|$|E
40|$|We {{describe}} {{our experience}} with two new, builtfrom -scratch, web-based <b>question</b> <b>answering</b> systems {{applied to the}} TREC 2005 Main <b>Question</b> <b>Answering</b> task, which use complementary models of <b>answering</b> <b>questions</b> over both structured and unstructured content on the Web. Our approaches depart from previous <b>question</b> <b>answering</b> (QA) work in several ways. For unstructured content, we used a web-based system with novel features such as web snippet pattern matching and generic answer type matching using web counts. We also experimented with a new, complementary <b>question</b> <b>answering</b> approach that uses information from the millions of tables and lists that abound on the web...|$|R
40|$|<b>Answering</b> <b>questions</b> that {{ask about}} {{temporal}} information involves several forms of inference. In {{order to develop}} <b>question</b> <b>answering</b> capabilities that benefit from temporal inference, we believe that a large corpus of <b>questions</b> and <b>answers</b> that are discovered based on temporal information should be available. This paper describes our methodology for creating AnswerTime-Bank, a large corpus of <b>questions</b> and <b>answers</b> on which <b>Question</b> <b>Answering</b> systems can operate using complex temporal inference. 1...|$|R
40|$|Growth in {{government}} investment, academic research, and commercial <b>question</b> <b>answering</b> (QA) systems is motivating {{a need for}} increased planning and coordination. The internationalization of QA research, {{and the need to}} move toward a common understanding of resources, tasks and evaluation methods provided motivate a need to facilitate more rapid and efficient progress. This paper characterizes a range of <b>question</b> <b>answering</b> systems and provides an initial roadmap for future research, including a list of existing resources and ones under development. This roadmap was initiated at the LREC 2002 Workshop on QA and we propose to update the roadmap during the workshop with moderated group brainstorming sessions. Characterizing <b>Question</b> <b>Answering</b> Systems Figure 1 characterizes a range of characteristics of <b>question</b> <b>answering</b> systems. The set of dimensions the distinguish various <b>question</b> <b>answering</b> systems which might range from systems for on-line help to access encyclopedic or technical manual information, to open web-based <b>question</b> <b>answering,</b> to very sophisticated QA in support of business o...|$|R
