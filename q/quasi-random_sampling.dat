32|28|Public
40|$|International audienceThe aim of {{this article}} is to compare {{experimentally}} the use of <b>quasi-random</b> <b>sampling</b> techniques for nonholonomic path planning. The experiments are evaluated in the context of the probabilistic roadmap methods (PRM). Two quasi-random variants of PRM-based planners are proposed: (1) a classical PRM with <b>quasi-random</b> <b>sampling,</b> and (2) a quasi-random lazy-PRM. Both have been implemented for car-like robots, and are shown through experimental results to offer some performance advantages in comparison to their randomized counterparts...|$|E
40|$|Within {{the popular}} {{probabilistic}} roadmap (PRM) framework for motion planning, we challenge {{the use of}} randomization. By applying <b>quasi-random</b> <b>sampling</b> techniques, we illustrate both experimental and theoretical advantages of using deterministic samples. Two quasi-random variants of PRM-based planners are proposed: 1) a classical PRM with <b>quasi-random</b> <b>sampling,</b> and 2) a quasi-random lattice-based Lazy roadmap. Both have been implemented, and are shown through experiments to oer some performance advantages in comparison to their randomized counterparts. Furthermore, our theoretical analysis shows that our approach leads to deterministic resolution completeness. We obtain the best possible asymptotic convergence rate, which is shown to be superior to that obtained by random sampling...|$|E
40|$|We {{propose the}} use of <b>quasi-random</b> <b>sampling</b> {{techniques}} for path planning in high-dimensional conguration spaces. Following similar trends from related numerical computation elds, we show several advantages oered by these techniques in comparison to random sampling. Our ideas are evaluated {{in the context of}} the probabilistic roadmap (PRM) framework. Two quasi-random variants of PRM-based planners are proposed: 1) a classical PRM with <b>quasi-random</b> <b>sampling,</b> and 2) a quasi-random Lazy-PRM. Both have been implemented, and are shown through experiments to oer some performance advantages in comparison to their randomized counterparts. 1 Introduction Over two decades of path planning research have led to two primary trends. In the 1980 s, deterministic approaches provided both elegant, complete algorithms for solving the problem, and also useful approximate or incomplete algorithms. The curse of dimensionality due to high-dimensional conguration spaces motivated researchers from the 199 [...] ...|$|E
40|$|We present {{algorithms}} for {{the generation}} of uniformly distributed Bayesian networks with constraints on induced width. The algorithms use ergodic Markov chains to generate samples, building upon previous algorithms by the authors. The introduction of constraints on induced width leads to more realistic results but requires new techniques. We discuss three applications of randomly generated networks: we study {{the average number of}} nodes d-connected to a query, the effectiveness of <b>quasi-random</b> <b>samples</b> in approximate inference, and the convergence of loopy propagation for non-extreme distributions...|$|R
40|$|AbstractA {{particle}} method {{adapted to}} the simulation of diffusion problems is presented. Time is discretized into increments of length Δt. During each time step, the particles are allowed to random walk to any point by taking steps sampled from a Gaussian distribution centered at the current particle position with variance related to the time discretization Δt. <b>Quasi-random</b> <b>samples</b> are used and the particles are relabeled according to their position at each time step. Convergence is proved for the pure initial-value problem in s space dimensions. For some simple demonstration problems, the numerical results indicate that an improvement is achieved over standard random walk simulation...|$|R
40|$|In a {{preliminary}} {{part of this}} paper, we analyze the necessity of randomness in evolution strategies. We conclude to the necessity of ”continuous”-randomness, but with a much more limited use of randomness than what is commonly used in evolution strategies. We then apply these results to CMA-ES, a famous evolution strategy already {{based on the idea}} of derandomization, which uses random independent Gaussian mutations. We here replace these random independent Gaussian mutations by a <b>quasi-random</b> <b>sample.</b> The modification is very easy to do, the modified algorithm is computationally more efficient and its convergence is faster {{in terms of the number}} of iterates for a given precision...|$|R
40|$|Mutual {{information}} {{is a widely}} used similarity measure for aligning multimodal medical images. At its core, it relies on the computation of a discrete joint histogram, which itself requires image samples for its estimation. In this paper, we study {{the influence of the}} sampling process. We show that <b>quasi-random</b> <b>sampling</b> based on Halton sequences outperforms methods based on regular sampling or on random sampling. Our results suggest that sampling itself—and not interpolation, as was previously believed— is the source of two major problems associated with mutual information: the grid effect, whereby grid-aligning transformations are favored, and the overlap problem, whereby the similarity measure exhibits discontinuities. Both defects tend to impede the accuracy of registration; they also result in reduced robustness {{because of the presence of}} local optima. By estimating the joint histogram by <b>quasi-random</b> <b>sampling,</b> we solve both issues at the same time...|$|E
40|$|The Sampling/Importance Resampling (SIR) al-gorithm can be {{used for}} {{generating}} represen-tative point sets from a distribution known up to a multiplicative constant. Moreover, the <b>Quasi-random</b> <b>Sampling</b> Importance Resampling (QSIR) scheme, based on quasi-Monte Carlo methods, is a recent modification of the SIR al-gorithm and was empirically shown to have bet-ter convergence. We present error convergence results for QSIR that we obtained using quasi-Monte Carlo theory. 1...|$|E
40|$|This article {{discusses}} the general problem of generating representative point sets from a distribution known up to a multiplicative constant. The Sampling/Importance Resampling (SIR) algorithm {{is known to}} be useful in this context. Moreover, the <b>Quasi-random</b> <b>Sampling</b> Importance Resampling (QSIR) scheme, based on quasi-Monte Carlo methods, is a more recent modification of the SIR algorithm and was empirically shown to have better convergence. By making use of quasi-Monte Carlo theory, we derive upper bounds for the error of the QSIR scheme. status: publishe...|$|E
40|$|International audienceIn a {{preliminary}} {{part of this}} paper, we analyze the neces- sity of randomness in evolution strategies. We conclude to the necessity of ”continuous”-randomness, but with a much more limited use of randomness than what is commonly used in evolution strategies. We then apply these results to CMA- ES, a famous evolution strategy already {{based on the idea}} of derandomization, which uses random independent Gaus- sian mutations. We here replace these random independent Gaussian mutations by a <b>quasi-random</b> <b>sample.</b> The mod- ification is very easy to do, the modified algorithm is com- putationally more efficient and its convergence is faster {{in terms of the number}} of iterates for a given precision...|$|R
40|$|Background Surgical site {{infection}} (SSI) is {{a serious}} potential complication of spinal surgery. SSI can impact significantly on in-patient hospitalisation and {{the costs associated with}} extra care. Aim To investigate the management of patients experiencing SSI following surgery for spinal metastatic tumours, and to estimate the costs associated with SSI in this context. Methods Patients experiencing SSI following spinal tumour surgery at a large spinal surgery centre between January 2009 and December 2012 were identified. Existing case notes were reviewed and patient and procedural data, details of the infection and treatment interventions were collected. A bottom-up approach to calculating costs associated with infection was used for patients experiencing SSI and compared with a <b>quasi-random</b> <b>sample</b> of similar patients without SSI. Findings The mean cost of treating patients with SSI was significantly greater than costs associated with those without SSI (p= 0. 019). Mean cost of in-patient hospital stay was 60...|$|R
40|$|This study aims to {{quantify}} the sale of inhalers without a prescription, describe the degree of difficulty for obtaining these inhalers without a prescription and assess if pharmacists educate patients about the correct use of the inhaler. A cross-sectional study of a <b>quasi-random</b> <b>sample</b> of 252 pharmacies stratified by the five regions of Riyadh. Each pharmacy was visited once by two investigators who used simulated patient methodology with governance of good research principles who simulated having a relative with difficulty of breathing due to airborne dust. Asthma inhalers were dispensed without prescription from 181 pharmacies (72 %). Of these, 12 % prescribed the inhaler after trying thelevel 1 of demand and 68 % after the third level. Only 31 pharmacists(17 %) explained {{how to use the}} inhalers. Asthma inhaler could be obtained in Riyadh without a medical prescription or an evidence-based indication with disobedience of pharmacists to educate on the appropriate use of inhalers...|$|R
40|$|A scalar {{model output}} Y {{is assumed to}} depend deterministically {{on a set of}} stochastically {{independent}} input vectors of different dimensions. The composition of the variance of Y is considered; variance components of particular relevance for uncertainty analysis are identified. Several analysis of variance designs for estimation of these variance components are discussed. Classical normal-model theory can suggest optimal designs. The designs can be implemented with various sampling methods: ordinary random sampling, latin hypercube sampling and scrambled <b>quasi-random</b> <b>sampling.</b> Some combinations of design and sampling method are compared in two small-scale numerical experiments...|$|E
40|$|We present, implement, {{and analyze}} a {{spectrum}} of closely-related planners, designed to gain insight into the relationship between classical grid search and probabilistic roadmaps (PRMs). Building on the <b>quasi-random</b> <b>sampling</b> literature, we have developed deterministic variants of the PRM that use Hammersley/Halton sequences and low-discrepancy lattices. Classical grid search is extended using subsampling for collision detection and also the optimal-dispersion Sukharev grid, which {{can be considered as}} a kind of lattice-based roadmap to complete the spectrum. Both multiple-query and lazy, single-query versions are considered for each planner. Our experimental results, [...] ...|$|E
30|$|A quasi-random {{sequence}} is less random than a pseudo-random sequence {{but is still}} useful for tasks such as numerical integration and optimisation because it tends to sample n-dimensional space “more uniformly” than random numbers. <b>Quasi-random</b> <b>sampling</b> also allows for additional sample points to be added, with continual improvement in accuracy if, say, a target precision has not been met after measuring an initial sample. It provides good spatial coverage with “random” locations, without {{the risk of the}} locations coinciding with some periodic variation in the population, a risk which grid-based sampling always carries.|$|E
40|$|An error {{bound for}} multidimensional {{quadrature}} is derived {{that includes the}} Koksma-Hlawka inequality as a special case. This error bound {{take the form of}} a product of two terms. One term, which depends only on the integrand, is defined as a generalized variation. The other term, which depends only on the quadrature rule, is defined as a generalized discrepancy. The generalized discrepancy is a figure of merit for quadrature rules and includes as special cases the L p -star discrepancy and P ff that arises in the study of lattice rules. 1. Introduction The multidimensional integral I(f) = Z C s f(x) dx can be approximated by the sample mean, Q(f) = 1 N X z 2 P f(z); (1. 1) where C s = [0; 1) s is the s-dimensional unit cube and P is some random or deterministic sample of N points in C s. (P may have multiple copies of the same point.) Many <b>quasi-random</b> <b>samples</b> designed for quadrature have been discussed in the literature. These include (t; m; s) -nets [Nie 92, Chapter 4 [...] ...|$|R
40|$|A {{cross-sectional}} study examines {{the relationship between}} participation in secular demonstrations, spiritual rituals, and communal coping, {{as well as the}} question whether these strategies might serve as triggers of post-traumatic growth, and enhance social well-being. A communal coping scale, showing satisfactory structural validity, was administered to a <b>quasi-random</b> <b>sample</b> (N = 517) of people affected by an earthquake in Chile in 2010. The results indicated that adaptive forms, such as communal reappraisal, regulated emotional expression, communal distraction, and communal searching for social support, were associated with social well-being (SWB) and post-traumatic growth (PTG). Participation in spiritual rituals was specifically related to communal reappraisal and contributed to post-traumatic growth. On the other hand, participation in secular collective gatherings also reinforced post-traumatic growth, as well as social well-being, but not through communal reappraisal. Overall, this study confirmed social functions of collective ritualized activities, which through the reinforcement of in-group interaction, foster individual post-traumatic growth and social well-being of people affected by a collective trauma, like an earthquake. Results are discussed in the framework of a collective positive psychology approach on micro- and macro-social processes of coping and their implications for social well-being...|$|R
40|$|We {{present the}} results of optical {{spectroscopy}} for 19 quasar candidates at photometric redshifts ≳ 3, of which enter into the Khorunzhev et al. (2016) catalog (K 16). This is a catalog of quasar candidates and known type 1 quasars selected among the X-ray sources of the 3 XMM-DR 4 catalog of the XMM-Newton serendipitous survey. We have performed spectroscopy for a <b>quasi-random</b> <b>sample</b> of new candidates at the 1. 6 -m telescope of the Sayan Solar Observatory and the 6 -m BTA telescope of the Special Astrophysical Observatory. The spectra at were taken with the new low- and medium-resolution ADAM spectrograph that was produced and installed on the telescope in 2015. Fourteen of the candidates actually {{have turned out to}} be quasars; 10 of them are at spectroscopic redshifts z > 3. The high purity of the sample of new candidates suggests that the purity of the entire K 16 catalog of quasars is probably 70 [...] 80 %. One of the most distant (= 5. 08) optically bright (i^'≲ 21) quasars ever detected in X-ray surveys has been discovered. Comment: russian text in Pisma v astronomicheskii Zhurnal (2017) 43 : 15...|$|R
40|$|In this talk, {{we discuss}} the general problem of {{generating}} representative point sets from a distribution known up to a multiplicative constant. The Sampling/Importance Resampling (SIR) algorithm {{is known to be}} useful in this context. The <b>Quasi-random</b> <b>Sampling</b> Importance Resampling (QSIR) scheme, based on quasi-Monte Carlo methods, is a more recent modification of the SIR algorithm and was empirically shown to have better convergence. We derive upper bounds for the error of the QSIR scheme using quasi-Monte Carlo theory. We restrict ourselves to distributions with independent and identically distributed marginals. status: publishe...|$|E
40|$|We {{compare the}} {{convergence}} properties {{of two different}} <b>quasi-random</b> <b>sampling</b> designs – Sobol’s quasi-Monte Carlo, and Latin supercube sampling in variance-based global sensitivity analysis. We use the non-monotonic V-function of Sobol’ as base case-study, and compare the performance of both sampling strategies at increasing sample size and dimensionality against analytical values. The results indicate that in general, the Sobol’ design performs better, however the Latin supercube sampling design appears to offer advantages in specific cases, such as smaller sample sizes and medium dimensionality. JRC. G. 3 -Econometrics and applied statistic...|$|E
40|$|The {{problem we}} {{consider}} {{is that of}} generating representative point sets from a distribution known up to a multiplicative constant. The Sampling/Importance Resampling (SIR) algorithm {{is known to be}} useful in this context. Moreover, the <b>Quasi-random</b> <b>Sampling</b> Importance Resampling (QSIR) scheme, based on quasi-Monte Carlo methods, is a more recent modification of the SIR algorithm and was empirically shown to have better convergence. By making use of quasi-Monte Carlo theory and restricting ourselves to distributions with independent and identically distributed marginals, we construct asymptotic error bounds for the QSIR scheme. status: publishe...|$|E
40|$|Particle filters {{have proven}} their value in many sensor-based {{robotics}} applications, and experience is gained about their computational properties such as cost, convergence robustness {{and the need}} for careful configuration of the resampling step. This paper investigates the appropriateness, in the robotics domain, of quasi-Monte Carlo techniques (QMC) which have shown a number of promising computational properties. Most notably: a better uniform distribution compared to pseudo-random samples and faster converging integral approximations. The following quasi-random sequences will be tested in this paper: Sobol, Halton, reverse Halton, Niederreiter and several lattice based generators, which posses the completely uniform distributed (CUD) property. Their performance in terms of the convergence of the particle filter is evaluated on the ''robot maze'' example, which was carefully chosen to be as simple as possible but still representative for real-world particle filter applications. No single quasi-random method is found to perform best and no clear advantage of the more uniform <b>quasi-random</b> <b>samples</b> is observed in the ''robot maze'' application. This is supporting the slumbering belief that, although QMC proved very efficient in many different areas, it might not be in a particle filter context. status: publishe...|$|R
40|$|AbstractA {{cross-sectional}} study examines {{the relationship between}} participation in secular demonstrations, spiritual rituals, and communal coping, {{as well as the}} question whether these strategies might serve as triggers of post-traumatic growth, and enhance social well-being. A communal coping scale, showing satisfactory structural validity, was administered to a <b>quasi-random</b> <b>sample</b> (N= 517) of people affected by an earthquake in Chile in 2010. The results indicated that adaptive forms, such as communal reappraisal, regulated emotional expression, communal distraction, and communal searching for social support, were associated with social well-being (SWB) and post-traumatic growth (PTG). Participation in spiritual rituals was specifically related to communal reappraisal and contributed to post-traumatic growth. On the other hand, participation in secular collective gatherings also reinforced post-traumatic growth, as well as social well-being, but not through communal reappraisal. Overall, this study confirmed social functions of collective ritualized activities, which through the reinforcement of in-group interaction, foster individual post-traumatic growth and social well-being of people affected by a collective trauma, like an earthquake. Results are discussed in the framework of a collective positive psychology approach on micro- and macro-social processes of coping and their implications for social well-being...|$|R
40|$|Cortisol {{is a key}} {{regulator}} of {{the immune}} system, energy metabolism, and stress, yet its relevance to fatigue experienced by people with relapsing-remitting multiple sclerosis (RRMS) remains uncertain. We examined cortisol secretory activity in RRMS and its association with fatigue severity between-individuals and within-individuals (day-to-day) using a case–control ecological momentary assessment design. While undergoing usual daily routines, 38 people with RRMS and 38 healthy control participants provided saliva samples at strategic time-points over 4 consecutive weekdays to measure the cortisol awakening response (CAR; 0, 30, and 45 min after awakening) and the diurnal cortisol slope (DCS; 6 <b>quasi-random</b> <b>samples</b> provided between 1000 h and 2000 h). Recalled fatigue was measured at baseline, and daily fatigue was measured as the mean average of momentary fatigue ratings provided alongside each DCS sample. Multilevel modeling found CAR output was greater in RRMS than controls, and recalled fatigue in RRMS was associated with both lower waking cortisol level and larger awakening response. Day-to-day, the CAR {{was not associated with}} same-day fatigue levels in RRMS. Cortisol appears to have a role in fatigue experienced in RRMS, but whether it is a causal factor remains unclear...|$|R
40|$|Abstract: Monte Carlo {{methods are}} used {{traditionally}} for {{forward and backward}} ray tracing which are numeric solutions of light transport equation in physically accurate computer graphics. Pseudo- Monte Carlo integration is not fast enough so recently the interest is moved towards quasi- Monte Carlo methods. Deterministic <b>quasi-random</b> <b>sampling</b> can provide better performance under some restrictions on the integrand. This work is devoted to basic approaches {{to speed up the}} global illumination solution by quasi-random sample points. The methods of overheads reduction due to high constructive dimension and lack of smoothness of rendering equation are considered. Different integration techniques are compared by the examples with analytic solution of rendering equation. Note: Publication language:russia...|$|E
40|$|We {{present a}} fast method for {{real-time}} computation of approximated global illumination for fully dynamic scenes under area light sources. To accelerate the computation, we use simplified models {{to calculate the}} indirect illumination, while render the direct illumination with original complex models. After direct illumination is computed with convolution soft shadow maps algorithm, color, position and normal textures are generated, from which a Halton <b>quasi-random</b> <b>sampling</b> method is used to produce the pixel lights for the second bounce. Our testing models show that thousands of passes can be rendered with the simplified scenes in one second for the indirect illumination, and it can dramatically improve the frame rate for relatively complex scenes. Final image is generated by blending direct image and all indirect images...|$|E
40|$|Traditionally, high {{computational}} {{costs have}} restricted high-fidelity interactive rendering to expensive shared-memory or dedicated distributed processors. Desktop grids offer a low-cost alternative by combining arbitrary computational resources {{connected to a}} network, such as the resources in a laboratory or an office. However, the prevalent interactive rendering algorithms can't seamlessly handle the variable computational power offered by a desktop grid's nondedicated resources. A proposed fault-tolerant algorithm renders high-fidelity images at an interactive rate that can handle variable resources. A conventional approach of rescheduling failed jobs in a volatile environment would inhibit performance when rendering at interactive rates because the time margins are small. Instead, this method uses <b>quasi-random</b> <b>sampling</b> along with image reconstruction. This video shows examples of scenes rendered on a desktop grid...|$|E
40|$|We {{present a}} new method for {{tracking}} the 3 D position, global orientation and full articulation of human hands. Inspired by {{recent advances in}} model-based, hypothesize-and-test methods, the high-dimensional parameter space of hand configurations is explored with a novel evolutionary optimization technique. The proposed method capitalizes {{on the fact that}} the <b>quasi-random</b> <b>samples</b> of the Sobol se-quence have low discrepancy and exhibit a more uniform coverage of the sampled space compared to random sam-ples obtained from the uniform distribution. The method has been tested for the problems of tracking the articula-tion of a single hand (27 D parameter space) and two hands (54 D space). Extensive experiments have been carried out with synthetic and real data, in comparison with state of the art methods. The quantitative evaluation shows that for cases of limited computational resources, the new approach achieves a speed-up of four (single hand tracking) and eight (two hands tracking) without compromising tracking accu-racy. Interestingly, the proposed method is preferable com-pared to the state of the art either in the case of limited com-putational resources or in the case of more complex (i. e., higher dimensional) problems, thus improving the applica-bility of the method in a number of application domains. 1...|$|R
40|$|Global {{uncertainty}} and sensitivity analysis {{is used to}} study the propagation of uncertainties in fundamental theoretical parameters through to uncertainties in the predicted temperature and pressure dependent phenomenological rate coefficients. Predictions are obtained from ab initio transition state theory based master equation calculations. The fundamental parameters for these rate predictions include barrier heights, well depths, vibrational frequencies, collision frequency, and energy transfer parameters. A random sampling high-dimensional model representation (HDMR) approach is used to perform the global sensitivity analysis. This approach determines the predicted distributions of the phenomenological rate coefficients based on a <b>quasi-random</b> <b>sample</b> of the fundamental parameters within their uncertainty range. Sensitivity analysis then identifies the main parameters which contribute to variance in the predicted distributions. Here the approach is applied to {{a study of the}} oxidation of the propyl radical, employing the parameters derived in our recent theoretical study. We find rates at 3 σ variances that typically differ from the most frequent values by factors of 4 – 6, with the uncertainties decreasing with increasing temperature. For the well skipping reactions there are more parameters that contribute significantly to the variance, the second-order sensitivities are greater, and the uncertainties increased with increasing pressure. For the other reactions, the uncertainties tend to decrease with increasing pressure...|$|R
40|$|Background: Antibiotics sales without medical {{prescriptions}} {{are increasingly}} recognized {{as sources of}} antimicrobial misuse that can exacerbate the global burden of antibiotic resistance. We aimed to determine the percentage of pharmacies who sell antibiotics without medical prescriptions, examining the potential associated risks of such practice in Riyadh, Saudi Arabia, by simulation of different clinical scenarios. Methods: A cross sectional study of a <b>quasi-random</b> <b>sample</b> of pharmacies stratified by the five regions of Riyadh. Each pharmacy was visited once by two investigators who simulated having a relative with a specific clinical illness (sore throat, acute bronchitis, otitis media, acute sinusitis, diarrhea, and urinary tract infection (UTI) in childbearing aged women). Results: A total of 327 pharmacies were visited. Antibiotics were dispensed without a medical prescription in 244 (77. 6 %) of 327, of which 231 (95 %) were dispensed without a patient request. Simulated cases of sore throat and diarrhea resulted in an antibiotic being dispensed in (90 %) of encounters, followed by UTI (75 %), acute bronchitis (73 %), otitis media (51 %) and acute sinusitis (40 %). Metronidazole (89 %) and ciprofloxacin (86 %) were commonly given for diarrhea and UTI, respectively, whereas amoxicillin/clavulanate was dispensed (51 %) for the other simulated cases. None of the pharmacists asked about antibiotic allergy history or provided information about dru...|$|R
40|$|Many {{approaches}} to analog performance parameter macro modeling {{have been investigated}} by the research community. These models are typically derived from discrete data obtained from circuit simulation using numerous input combinations of component sizes for a given circuit topology. The simulations are computationally intensive, therefore it is advantageous {{to reduce the number}} of simulations necessary to build an accurate macro model. We present a new algorithm for adaptively sampling multi-dimensional black box functions based on Duchon pseudo-cubic splines. The splines readily and accurately model high dimensional functions based on discrete unstructured data and require no tuning of parameters as seen in many other interpolation methods. The adaptive sampler, in conjunction with pseudo-cubic splines, is used to accurately model various analog performance parameters for an operational amplifier topology using fewer sample points than traditional gridded and <b>quasi-random</b> <b>sampling</b> methodologies...|$|E
40|$|We {{study the}} quasi-random choice method (QRCM) for the Liouville {{equation}} of ge-ometrical optics with discontinuous local wave speed. This equation arises in the phase space computation of high frequency waves through interfaces, where waves undergo partial transmissions and reflections. The numerical challenges include interface, contact discon-tinuities, and measure-valued solutions. The so-called QRCM is a random choice method based on <b>quasi-random</b> <b>sampling</b> (a deterministic alternative to random sampling). The method {{not only is}} viscosity-free but also provides faster convergence rate. Therefore, it is appealing for the problem under study which is indeed a Hamiltonian flow. Our analy-sis and computational {{results show that the}} QRCM 1) is almost first-order accurate even with the aforementioned discontinuities; 2) gives sharp resolutions for all discontinuities encountered in the problem; and 3) for measure-valued solutions, does not need the level set decomposition for finite difference/volume methods with numerical viscosities...|$|E
40|$|Detailed {{chemical}} kinetic investigations on dimethylether oxidation in one-dimensional premixed flat flames were performed. Local {{and global}} sensitivities {{of the reaction}} rate constants within selected chemical kinetic schemes were studied using maximum flame temperature, and peak methane and formaldehyde concentrations as predictive target quantities. The global sensitivity analysis {{was based on the}} application of high dimensional model representations using <b>quasi-random</b> <b>sampling.</b> First- and second-order sensitivity indices of important reaction steps were determined for fuel rich (Φ = 1. 49) and fuel lean (Φ = 0. 67) conditions. Differences in the importance ranking for key reactions were found to exist between the selected schemes, highlighting the influence of differences in the key rate constants. Whilst the peak flame temperature was predicted with fairly low uncertainty by both schemes, significant uncertainties were identified in the prediction of the target minor species. Key reaction rates requiring better quantification in order to improve the prediction of methane and formaldehyde concentrations are identified...|$|E
40|$|Seeking {{to fill a}} gap in the historiography, {{this study}} {{provides}} a closely-observed but contextualised social history of Scotland’s rural schools from the late nineteenth century through {{to the end of the}} twentieth century. Though particularly concerned with the period following the Education (Scotland) Act, 1872, consideration is given to earlier developments to ensure a depth of understanding and an appreciation of the subtleties of local experience. Adopting an interdisciplinary approach, and combining qualitative and quantitative analysis, the thesis draws together three layers of research: a detailed regional case study of the Highland Perthshire parishes of Fortingall, Kenmore and Killin; a <b>quasi-random</b> <b>sample</b> of sixty-six rural districts from across Scotland; and a national overview. In doing so, it challenges oft-made generalisations about rural life and provides a more nuanced picture of change and continuity in educational policy and practice across Scotland. Focusing in on the relationship between the small rural schools and their communities, the social dimensions of educational provision are explored in depth with special attention being paid to who taught, attended and supported the schools, and how they operated as educational and social spaces. To frame and guide discussion, three core themes – gender, culture and citizenship – are explored throughout and elements of social theory are drawn on to aid analysis and interpretation...|$|R
40|$|The {{location}} of tide gauges is not random. If their locations are positively (negatively) correlated with SLR, estimates of global SLR will be biased upwards (downwards). We {{show that the}} {{location of}} tide gauges in 2000 is independent of SLR as measured by satellite altimetry. Therefore PSMSL tide gauges constitute a <b>quasi-random</b> <b>sample</b> and inferences of SLR based on them are unbiased, {{and there is no}} need for data reconstructions. By contrast, tide gauges dating back to the 19 th century were located where sea levels happened to be rising. Data reconstructions based on these tide gauges are therefore likely to over-estimate sea level rise. We therefore study individual tide gauge data on sea levels from the Permanent Service for Mean Sea Level (PSMSL) during 1807 – 2010 without recourse to data reconstruction. Although mean sea levels are rising by 1 mm/year, sea level rise is local rather than global, and is concentrated in the Baltic and Adriatic seas, South East Asia and the Atlantic coast of the United States. In these locations, covering 35 percent of tide gauges, sea levels rose on average by 3. 8 mm/year. Sea levels were stable in locations covered by 61 percent of tide gauges, and sea levels fell in locations covered by 4 percent of tide gauges. In these locations sea levels fell on average by almost 6 mm/year. Acknowledgment; Thanks to Michal Lichter for assistance with Map 3. ...|$|R
40|$|Purpose: This paper {{examines}} {{the importance and}} concept of idol attachment, models its antecedents and moderators, and assesses its influence on human brand loyalty. Design/methodology/approach: This paper includes two studies. In study 1, survey questionnaires were distributed by mall intercept to <b>quasi-random</b> <b>samples</b> across Australia and Taiwan for completion and return. The return yielded 1135 and 736 usable questionnaires, respectively, the data from which were analysed using LISREL structural equation modelling software. In study 2, an experiment {{was employed to examine}} whether idol attractiveness is likely to positively moderate the relationship between vanity traits and attachment. Findings: The results suggest that achievement vanity, variety seeking, and peer norms {{have a positive impact on}} the phenomenon of idol attachment, which in turn positively affects human brand loyalty. Contradicting previous studies, the physical appearance of vanity was not found to be associated with idol attachment. However, the results of the experiment show idol attractiveness has a positive moderating effect on the relationship between vanity traits and human brand attachment. Research limitations/implications: The findings suggest that idol attachment is more complex than previously understood. The constructs chosen in this research represent an initial step but other variables such as liking, involvement, affective commitment, and brand love are not taken into account. Future research models should therefore include such variables. Practical implications: The findings contain many practical lessons for planners of marketing strategy for the music industry in an international context. Originality/value: Two existing theories of psychology are integrated with the concept of idol attachment to explain human brand loyalty in an international context...|$|R
