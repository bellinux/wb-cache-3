83|525|Public
50|$|The Worldwide Governance Indicators is {{a program}} funded by the World Bank to measure the quality of {{governance}} of over 200 countries. It uses six dimensions of governance for their measurements, Voice & Accountability, Political Stability and Lack of Violence, Government Effectiveness, Regulatory <b>Quality,</b> <b>Rule</b> of Law, and Control of Corruption. They have been studying countries since 1996.|$|E
50|$|While at the World Bank {{during the}} 1990s, Kaufmann and his {{colleague}} Aart Kraay pioneered {{the creation of a}} worldwide set of governance indicators, including voice and democratic accountability, political stability, government effectiveness, regulatory <b>quality,</b> <b>rule</b> of law, and control of corruption. The Worldwide Governance Indicators (WGI), which cover more than 200 countries, are issued annually and widely used by donor agencies, risk rating agencies, scholars, students and policy analysts.|$|E
50|$|Based on {{a long-standing}} {{research}} {{program of the}} World Bank, the Worldwide Governance Indicators capture six key dimensions of governance (Voice & Accountability, Political Stability and Lack of Violence, Government Effectiveness, Regulatory <b>Quality,</b> <b>Rule</b> of Law, and Control of Corruption) between 1996 and present. They measure the quality of governance in over 200 countries, based on close to 40 data sources produced by over 30 organizations worldwide and are updated annually since 2002.|$|E
5000|$|King's Lynn, demoted under stadium <b>quality</b> <b>rules</b> {{from the}} Conference North ...|$|R
5000|$|Cammell Laird, demoted under stadium <b>quality</b> <b>rules</b> {{from the}} NPL Premier Division ...|$|R
5000|$|Data {{standardization}} - {{a business}} rules engine that ensures that data conforms to <b>quality</b> <b>rules</b> ...|$|R
50|$|Kuscos is a {{software}} {{as a service}} (SaaS) platform for development teams, managers and executives, providing a range of analysis tools and reports that empower teams to develop with increasing productivity and quality. It delivers key information regarding source code modules and team members, from design documentation to dependencies, duplicate code and <b>quality</b> <b>rule</b> violations as well as custom pattern and metric analysis. It also provides oversight of team activities, from repository commits to issues resolved.|$|E
50|$|One {{of these}} efforts to create an internationally {{comparable}} measure of governance and {{an example of an}} external assessment is the Worldwide Governance Indicators project, developed by members of the World Bank and the World Bank Institute. The project reports aggregate and individual indicators for more than 200 countries for six dimensions of governance: voice and accountability, political stability and lack of violence, government effectiveness, regulatory <b>quality,</b> <b>rule</b> of law, control of corruption. To complement the macro-level cross-country Worldwide Governance Indicators, the World Bank Institute developed the World Bank Governance Surveys, which are country-level governance assessment tools that operate at the micro or sub-national level and use information gathered from a country’s own citizens, business people and public sector workers to diagnose governance vulnerabilities and suggest concrete approaches for fighting corruption.|$|E
30|$|However, the {{coefficients}} of regulatory <b>quality,</b> <b>rule</b> of law, and voice and accountability are insignificant {{in both groups}} of developing countries. This finding implies that these indicators are weak and do not impact FDI in these host countries. Therefore, other factors determine FDI in developing countries.|$|E
30|$|The {{research}} question was “How is quality management architecture constructed {{for evaluating the}} quality of social media data?” The data quality management architecture is comprised of a metadata management layer in the RA for big data systems. The metadata management layer consists of <b>quality</b> <b>rules,</b> metadata, and data (data stores). <b>Quality</b> <b>rules</b> provide means for the organization of the company to manage quality of social media data sets for decision making purposes. The main functional elements of the metadata management layer include metadata management, quality management, and quality evaluation. In the prototype system metadata management enabled creation of and access to metadata related to tweets. Quality management was responsible for managing quality aspects of metadata based on user-defined <b>quality</b> <b>rules.</b> <b>Quality</b> evaluation of tweets was performed in a Spark streaming cluster, which indicated that 150 – 800 tweets/s can be processed with two cloud nodes depending on the configuration.|$|R
40|$|<b>Quality</b> <b>rules</b> {{are used}} to capture {{important}} implementation and design decisions embedded in a software system's architecture. They can automatically analyze software and assign quality grades to its components. To provide a meaningful evaluation of <b>quality,</b> <b>rules</b> have to stay up-to-date with the continuously evolving system that they describe. However one would encounter unexpected anomalies during a historical overview because the notion of quality is always changing, while the qualitative evolution analysis requires it to remain constant. To understand the anomalies in a quality history of a real-world software system we use an immersive visualization that lays out the quality fluctuations in three dimensions based on two co-evolving properties: <b>quality</b> <b>rules</b> and source code. This helps us to identify and separate the impact caused by the changes of each property, and allows us to detect significant mistakes that happened during the development process...|$|R
3000|$|Quality metrics {{are read}} from the <b>quality</b> <b>rules</b> store. Metrics are decoded with {{appropriate}} decoders. Format-field of the metric (Fig.  14) indicates how the metric should be decoded [...]...|$|R
30|$|Although other {{governance}} indicators {{from the}} Worldwide Governance Indicators, such as control of corruption, government effectiveness, regulatory <b>quality,</b> <b>rule</b> of law, and voice and accountability, {{were employed in}} the estimation, political stability and absence of violence were the most consistently significant variable in the long-run growth estimation. Unfortunately, collinearity problems prevented the inclusion {{of more than one}} governance indicator in the regression.|$|E
40|$|This {{document}} {{was prepared to}} describe the air quality modeling performed by EPA {{in support of the}} proposed Interstate Air <b>Quality</b> <b>Rule</b> (IAQR). Included is information on (1) the air quality models and the development of model inputs, (2) the performance of the models as compared to measured data, (3) the procedures for projecting current air quality to future yea...|$|E
30|$|Rule of law {{indicates}} {{the stability of}} the country. Poor institutional <b>quality,</b> <b>rule</b> of law and corruption destabilize a country and may increase the chances of bankruptcy and/or decrease the efficiency of banks. However, the impact of bank loans on rule of law can be positive or negative. For instance, an unstable economy cannot enjoy foreign assistance and easy lending, and thus it relies on domestic funding, which increases output and bad loans.|$|E
40|$|Digitally {{collected}} data su ↵ ers from many data quality issues, such as duplicate, incorrect, or incomplete data. A common approach for counteracting these issues is {{to formulate a}} set of data cleaning rules to identify and repair incorrect, duplicate and missing data. Data cleaning systems {{must be able to}} treat data <b>quality</b> <b>rules</b> holistically, to incorporate heterogeneous constraints within a single routine, and to automate data curation. We propose an approach to data cleaning based on statistical relational learning (SRL). We argue that a formalism - Markov logic - is a natural fit for modeling data <b>quality</b> <b>rules.</b> Our approach allows for the usage of probabilistic joint inference over interleaved data cleaning rules to improve data quality. Furthermore, it obliterates the need to specify the order of rule execution. We describe how data <b>quality</b> <b>rules</b> expressed as formulas in first-order logic directly translate into the predictive model in our SRL framework...|$|R
40|$|The {{poor quality}} of data {{constitutes}} a major concern worldwide, and an obstacle to data integration and analysis efforts. Detecting errors and inconsistencies using application specific data <b>quality</b> <b>rules</b> {{play an important role}} in data <b>quality</b> assessment. These <b>rules</b> have different efficacy and cost under different circumstances. In our previous work, we have proposed a quantitative framework for measuring and comparing data <b>quality</b> <b>rules</b> in terms of their effectiveness. Effectiveness formulas are built from variables that represent probabilistic assumptions about the occurrence of errors in data values, and our earlier work gave examples of how to derive these formulas in an ad-hoc fashion. This paper lays the foundations of a workbench-approach for systematically deriving effectiveness formulas. The approach involves several steps, including building Bayesian network graphs, adding (symbolic) probabilities to the nodes in the graph, and deriving effectiveness formulas. The graphs are built algorithmically, for a large and useful class of data <b>quality</b> <b>rules.</b> We present this approach and its implementation in Python, and report its evaluation results, which show that the resulting formulas give reasonable estimates of effectiveness scores under various scenarios. 1...|$|R
50|$|The {{production}} chain of DiscoRobo {{is committed to}} use high quality plastic materials passing the strict safety and <b>quality</b> <b>rules</b> in many markets so that the toy should be perfectly safe products for kids.|$|R
40|$|Stock {{markets have}} become an {{important}} market for long run fund flows between savers and investors and also a determinant of economic growth {{with the emergence of}} endogenous growth theories. This study investigates the impact of institutional development on stock market development in 8 European Union transition economies during 2002 - 2013 period employing panel regression. We found that political stability, regulatory <b>quality,</b> <b>rule</b> of law and control of corruption had positive impact on stock market development...|$|E
40|$|Purpose – Following {{the demise}} of the Soviet Union in 1992, Russia {{undertook}} major institutional and market-oriented reforms to enhance the competitive advantage of domestic enterprises. Although Russia has experienced rapid growth over the last two decades, the extent to which institutions in Russia impact on firm innovation and performance remains poorly understood {{due to a lack of}} research on the subject. This paper seeks to contribute to the literature on the competitiveness of Russian firms by focussing specifically on the extent to which the state of the regulatory <b>quality,</b> <b>rule</b> of law, and corruption affect the innovation capacity and performance of firms in Russia. Design/methodology/approach – The study uses structural equation modelling and data from a large-scale firm level survey (n= 787) of firms in Russia undertaken by the World Bank in 2009. It investigates the direct and indirect perceptions of respondents of the effects the current institutional environment has on the innovation capacity and performance of their respective organisations. Findings – The results show that regulatory <b>quality,</b> <b>rule</b> of law and corruption have strong direct and negative impacts on both the innovation capacity and performance of firms, and that innovation capacity strongly mediates the effects of institutions on firm performance. The results suggest that the current state of the regulatory <b>quality,</b> <b>rule</b> of law and corruption in Russia inhibit firm innovation and their resulting performance. Research limitations/implications – The findings should be interpreted with caution to the extent that the study is limited to only three elements of the formal institutional environment and does not take into consideration the role of informal institutions. These two limitations present avenues for future research. Originality/value – The study is one of the first to provide empirical evidence based on a large-scale survey of the extent to which formal institutions inhibit innovation and firm performance in Russia, and provides valuable guidance to business policy-makers in Russia on possible avenues for enhancing the overall competitiveness of Russian firms. <br /...|$|E
40|$|New Zealand {{is ranked}} highly on the Worldwide Governance Indicators (WGI), which assess {{performance}} on six dimensions of governance: voice and accountability, political stability performance on six dimensions of governance: voice and accountability, political stability and absence of violence, government effectiveness, regulatory <b>quality,</b> <b>rule</b> of law, {{and control of}} corruption. However, these indicators {{should not be allowed}} to dominate or supplant valid social criticism in the form of political discourse and historical narrative, which are essential in assessing any country 2 ̆ 7 s quality of governance...|$|E
40|$|Data {{quality is}} one of the most {{important}} problems in data management, since dirty data often leads to inaccurate data analytics results and wrong business decisions. Poor data across businesses and the government cost the U. S. economy 3. 1 trillion a year, according to a report by InsightSquared in 2012. Data scientists reportedly spend 60 % of their time in cleaning and organizing the data according to a survey published in Forbes in 2016. Therefore, we need effective and efficient techniques to reduce the human efforts in data cleaning. Data cleaning activities usually consist of two phases: error detection and error repair. Error detection techniques can be generally classified as either quantitative or qualitative. Quantitative error detection techniques often involve statistical and machine learning methods to identify abnormal behaviors and errors. Quantitative error detection techniques have been mostly studied in the context of outlier detection. On the other hand, qualitative error detection techniques rely on descriptive approaches to specify patterns or constraints of a legal data instance. One common way of specifying those patterns or constraints is by using data <b>quality</b> <b>rules</b> expressed in some integrity constraint languages; and errors are captured by identifying violations of the specified rules. This dissertation focuses on tackling the challenges associated with detecting and repairing qualitative errors. To clean a dirty dataset using rule-based qualitative data cleaning techniques, we first need to design data <b>quality</b> <b>rules</b> that reflect the semantics of the data. Since obtaining data <b>quality</b> <b>rules</b> by consulting domain experts is usually a time-consuming processing, we need automatic techniques to discover them. We show how to mine data <b>quality</b> <b>rules</b> expressed in the formalism of denial constraints (DCs). We choose DCs as the formal integrity constraint language for capturing data <b>quality</b> <b>rules</b> because it is able to capture many real-life data <b>quality</b> <b>rules,</b> and at the same time, it allows for efficient discovery algorithm. Since error detection often requires a tuple pairwise comparison, a quadratic complexity that is expensive for a large dataset, we present a distribution strategy that distributes the error detection workload to a cluster of machines in a parallel shared-nothing computing environment. Our proposed distribution strategy aims at minimizing, across all machines, the maximum computation cost and the maximum communication cost, which are the two main types of cost one needs to consider in a shared-nothing environment. In repairing qualitative errors, we propose a holistic data cleaning technique, which accumulates evidences from a broad spectrum of data <b>quality</b> <b>rules,</b> and suggests possible data updates in a holistic manner. Compared with previous piece-meal data repairing approaches, the holistic approach produces data updates with higher accuracy because it realizes the interactions between different errors using one representation, and aims at generating data updates that can fix as many errors as possible...|$|R
40|$|Abstract — Erroneous and {{inconsistent}} data, {{often referred to}} as ‘dirty data’, is a major worry for businesses. Prevalent techniques to improve data quality consist of discovering data <b>quality</b> <b>rules,</b> identifying records that violate those rules, and then modifying the data to either remove those violations. Most of the work described in the literature deals with cases where both the data and the rules are visible to the party that is in charge of cleaning the data. However, consider the case where two parties with data and data <b>quality</b> <b>rules</b> wish to cooperate in data cleaning under two restrictions: (1) neither of the parties is willing to share their data due to its sensitive nature, and (2) the data <b>quality</b> <b>rules</b> may reveal information about the content of the data and may be considered as a private asset to the business. The question then is how to clean the data without having to share the data or the rules. While the data cleaning process involves several phases, our focus in this paper is on detecting inconsistent data. We propose a novel inconsistency detection protocol that preserves the privacy of both the data and the data <b>quality</b> <b>rules</b> without the use of a third party. Inconsistent data is defined as all records in a database that violate some conditional functional dependencies or CFDs. Our approach is based primarily on the secure multiparty computation framework. We present complexity analysis of our protocol and a series of experiments about its performance. I...|$|R
3000|$|HTTP POST is received. Processing {{configurations}} (Fig.  14) {{are read}} from the <b>quality</b> <b>rules</b> store. The information contains the data quality evaluation task to be started, and configuration/processing parameters {{to be used for}} starting of the analysis process [...]...|$|R
40|$|Economic {{growth is}} one of the {{indicators}} {{of the success of the}} performance of the economy of a country. The role of the Government in good governance are expected to encourage the achievement of increased economic growth. This research aims to analyze the effect of good governance towards economic growth in ASEAN. Through good governance as measured using the indicators, control of corruption, government effectiveness, political stability and absence violence, regulatory <b>quality,</b> <b>rule</b> of law and voice and accountability that is applied can encourage the stability of the economy. The data used in this research is the data panel with time series data from 2004 to 2014 and cross section data for as many as 10 countries in ASEAN with the regression model used is the data panel Fixed Effect Model (FEM). Economic growth data are used as the dependent variable and the data is good governance which includes the control of corruption, government effectiveness, political stability and absence violence, regulatory <b>quality,</b> <b>rule</b> of law and of voice and accountability as the independent variable. Based on the results of the regression data panel note that variables are not significant effect on economic growth in ASEAN is the control of corruption, government effectiveness, rule of law, and voice and accountability. While the variable political stability and absence of significant violence and regulatory quality influence on economic growth in ASEAN...|$|E
30|$|We use the (Kaufmann et al. 1999) data. They report six {{indicators}} of governance {{for a large}} set of developed and developing countries. A higher level of the indicator means better quality of institutions. The six indicators are voice and accountability, political stability, government effectiveness, regulatory <b>quality,</b> <b>rule</b> of law and control of corruption. The indicators are available over the 1994 – 2009 period. We use two data points for each indicator, i.e. the one related to 1994 and 2004. This allows us computing the change in governance quality over the 1994 – 2004 period that {{can be related to}} migration rates and country’s norms computed in 1990.|$|E
40|$|According to {{new growth}} theories, public {{governance}} {{is an important}} determinant for sustained economic growth. This study examines the impact of six public governance indicators, including voice and accountability, political stability {{and the absence of}} violence/terrorism, government effectiveness, regulatory <b>quality,</b> <b>rule</b> of law and control of corruption, on the economic growth in the transitional economies of the European Union during the 2002 - 2013 period. The results show that all governance indicators except regulatory quality had a statistically significant positive impact on economic growth. Our findings also indicate that control of corruption and rule of law had the largest impact on economic growth, while political stability had the lowest impact. </p...|$|E
30|$|Metadata {{management}} {{refers to}} creation of metadata, and {{providing access to}} it. MetadataCollectionEngine and MetadataSearchEngine components of the Front-end node (Fig.  5) encapsulated functionality of metadata management. Quality management refers to managing quality aspects of data sets with user-defined <b>quality</b> <b>rules.</b> In the prototype system MetadataQualityManagement, MetadataQualityEvaluator, and MetadataQualityPolicyManager of the Front-end node implemented quality management functionality (Fig.  5). Quality evaluation refers to analysing quality of social media data sets based on quality metrics, which have been selected to a context based on <b>quality</b> <b>rules.</b> In the prototype system quality evaluation was comprised of QualityEvaluator and TwitterAnalysis components of the Back-end node (Fig.  8), and Word 2 Vec-service of the relevancy-node (Fig.  4).|$|R
40|$|Association rule mining problem (ARM) is a struc-tured {{mechanism}} for unearthing hidden facts in large data sets and drawing inferences {{on how a}} subset of items influences the presence of another subset. ARM is computationally very expensive {{because the number of}} rules grow exponentially as the number of items in the database increase. This exponential growth is exacer-bated further when data dimensions increase. The asso-ciation rule mining problem is even made more com-plex when the need to take the different <b>rule</b> <b>quality</b> metrics into account arises. In this paper, we propose a genetic algorithm (GA) to generate high <b>quality</b> as-sociation <b>rules</b> with five <b>rule</b> <b>quality</b> metrics. We study the performance of the algorithm and the experimental results show that the algorithm produces high <b>quality</b> <b>rules</b> in good computational times...|$|R
40|$|Data {{cleaning}} techniques usually rely on some <b>quality</b> <b>rules</b> {{to identify}} violating tuples, and then fix these violations us-ing some repair algorithms. Oftentimes, the rules, which {{are related to}} the business logic, can only be defined on some tar-get report generated by transformations over multiple data sources. This creates a situation where the violations de-tected in the report are decoupled in space and time from the actual source of errors. In addition, applying the repair on the report would need to be repeated whenever the data sources change. Finally, even if repairing the report is possi-ble and affordable, this would be of little help towards iden-tifying and analyzing the actual sources of errors for future prevention of violations at the target. In this paper, we pro-pose a system to address this decoupling. The system takes <b>quality</b> <b>rules</b> defined over the output of a transformation and computes explanations of the errors seen on the output. This is performed both at the target level to describe these errors and at the source level to prescribe actions to solve them. We present scalable techniques to detect, propagate, and explain errors. We also study the effectiveness and ef-ficiency of our techniques using the TPC-H Benchmark for different scenarios and classes of <b>quality</b> <b>rules.</b> 1...|$|R
40|$|Recent work on Textual Entailment {{has shown}} a crucial role of {{knowledge}} to support entailment inferences. However, {{it has also been}} demonstrated that currently available entailment rules are still far from being optimal. We propose a methodology for the automatic acquisition of large scale context-rich entailment rules from Wikipedia revisions, taking advantage of the syntactic structure of entailment pairs to define the more appropriate linguistic constraints for the rule to be successfully applicable. We report on rule acquisition experiments on Wikipedia, showing that it enables the creation of an innovative (i. e. acquired rules are not present in other available resources) and good <b>quality</b> <b>rule</b> repository. ...|$|E
40|$|This paper {{proposes a}} {{sequential}} covering based algorithm that uses an ant colony optimization algorithm to directly extract classification rules {{from the data}} set. The proposed algorithm uses a Simulated Annealing algorithm to optimize terms selection, while growing a rule. The proposed algorithm minimizes the problem of a low quality discovered rule by an ant in a colony, where the rule discovered by an ant {{is not the best}} <b>quality</b> <b>rule,</b> by optimizing the terms selection in rule construction. Seventeen data sets which consist of discrete and continuous data from a UCI repository are used to evaluate the performance of the proposed algorithm. Promising results are obtained when compared to the Ant-Miner algorithm and PART algorithm in terms of average predictive accuracy of the discovered classification rules...|$|E
40|$|This {{paper is}} about a very {{complicated}} and multifaceted issue of corruption. The main {{purpose of this paper}} is to find the impact of development indicators which are political stability and violence, government effectiveness, regulatory <b>quality,</b> <b>rule</b> of law, voice and accountability on the control of corruption in Afghanistan. Furthermore we are interested to know the direction of causality among these variables. The data is analyzed through ordinary least square regression method. Johansen test is applied for cointegration and to find the causality we have applied the Granger causality test. Political Stability and violence, government effectiveness and voice and accountability have shown very strong results and can have higher impact. Policy recommendations are made at the end of the paper...|$|E
3000|$|End user/application {{creates a}} search {{filtering}} policy. Quality attributes and associated ranges are provided. It is also indicated, if social media data {{should be included}} to the response. Optionally, metadata identifier and time range may be provided. The policy is saved into the <b>quality</b> <b>rules</b> store [...]...|$|R
40|$|Abstract—In {{the context}} of data {{warehousing}} and business intelligence, data quality is of utmost importance. However, many mid-size data warehouse (DWH) projects do not implement a proper data quality process due to huge up-front investments. Nevertheless, assessing and monitoring data quality is necessary to establish confidence in the DWH data. In this paper, we describe a data quality monitoring system: The “Data Checking Engine ” (DCE). The goal {{of the system is}} to provide DWH projects with an easy and quickly deployable solution to as-sess data quality while still providing highest flexibility in the definition of the assessment rules. It allows to express complex <b>quality</b> <b>rules</b> and implements a two-staged template mechanism to facilitate the deployment of large numbers of similar rules. While the rules themselves are SQL statements the tool guides the data quality manager through the process of creating rule templates and rules so that it is rather easy for him to create large sets of <b>quality</b> <b>rules.</b> The rule definition language is illustrated in this paper and we also demonstrate the very flexible capabilities of the DCE by presenting examples of advanced data <b>quality</b> <b>rules</b> and how they can be implemented in the DCE. The usefulness of the DCE has been proven in practical implementations at different clients of SHS Viveon. An impression of the actual implementations of the system is given in terms of the system architecture and GUI screenshots in this paper...|$|R
40|$|We {{address the}} issue of expressing and {{evaluating}} <b>quality</b> <b>rules</b> on music notation. Since music engraving is a highly flexible process that can hardly be constrained by universal principles and rules, score production still heavily relies on the user expertise in order to make context-dependent decisions. We therefore propose a quality management approach based on a formal modeling of this expertise. We show how to use such a model to express context-aware rules that can be evaluated either a priori to prevent the production of faulty notations, or a posteriori to assess quality indicators regarding a score or a corpus of scores. The paper proposes a simple ontology for musical notation, shows how <b>quality</b> <b>rules</b> can be formally stated and evaluated, and illustrates the approach with examples drawn from a large digital library of scores...|$|R
