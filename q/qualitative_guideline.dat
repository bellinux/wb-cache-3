3|99|Public
40|$|The {{work of this}} {{dissertation}} {{was aimed}} at getting a better understanding about the way people evaluate visual similarity of skin lesions. Experiments testing the evaluation performance achieved following the ABCD rule were run at first. Results showed a substantial variability in the obtained evaluations which puts the usefulness of this <b>qualitative</b> <b>guideline</b> under questioning. According to additional analysis, {{the use of the}} ABCD rule in the development of automatic classifiers can be arguably discouraged. Experiments purely based on visual similarity, on the other hand, showed the emergence of homogeneous visual classes of Basal Cell Carcinomas. These classes delineate some visual criteria possibly followed by the observers during the assessment. A system is developed to learn these criteria from the experimental data and promising results are reported despite the limited availability of training and testing data. i Acknowledgements First of all I want to thank my supervisor, Dr. Lucia Ballerini, for her friendly help and constant guidance. A warm thank you goes to Prof. Fisher for the numerous suggestion...|$|E
40|$|As the {{manufacturing}} {{is becoming more}} and more globalization, {{the manufacturing}} jobs, especially low skilled jobs, have been moved to the developing countries for a lower production cost. For those jobs, such as skilled labor jobs, currently still kept in the developed countries, more and more cost pressure is experienced in these countries as those jobs will eventually be moved to the developing countries once their infrastructure, efficiency and skill level are improved. Therefore one of the challenges currently faced by the Canadian companies is how to improve the manufacturing productivity. Currently those companies are using their own way to improve the productivity. As a result, a company may repeat what another company has already done for the improvement of the productivity, and waste the effort. In some cases, a company even has no idea how to pursue the action to improve its productivity. In order to provide the Canadian companies some sort of systematic approach in the course of improving productivity, a systematic <b>qualitative</b> <b>guideline</b> for the best use of skilled labor, called as “Three-Step procedure ” (3 SA), is developed and described in detail in this paper...|$|E
40|$|Resource {{allocation}} schemes play {{an important}} role in large-scale smart infrastructures to ensure efficiency and fairness among users. However, designing good resource allocation schemes is challenging due to technical limitation, policy barriers, and cost of change. The goal of this thesis is to develop a methodology to design model-based, principled and practical resource allocation schemes. Given the diverse characteristics of infrastructures, it is difficult to design unified models and algorithms. Instead, we employ a case-study-based approach on two representative smart infrastructures: Internet video delivery and electric power networks. We further generalize the insights and develop a principled <b>qualitative</b> <b>guideline</b> to design resource allocation schemes in smart infrastructures. The Internet video delivery system employs a protocol-based resource allocation scheme: network bandwidth is implicitly allocated by transport layer protocol (TCP) while client- side video players adapt video quality based on application layer protocol (MPEG-DASH) to optimize users quality of experience (QoE). We study 1) how client-side video players improve users QoE by employing Model Predictive Control-based bitrate adaptation algorithms and 2) how to achieve multiplayer QoE fairness by router-side bandwidth allocation policies. We prototype and evaluate the algorithms in real video players. On the other hand, market-based schemes are adopted in real-time economic dispatch in electric power systems to satisfy demand by lowest-cost generation. However, such schemes can lead to power imbalances and market inefficiency when slow generators fail to follow system operators command. We study 1) how system operators can mitigate power imbalance by employing a centralized, two-stage robust dispatch and 2) how the market design can be improved by penalizing non-complying generators. Based on the lessons from the case studies, we develop a general methodology to design resource allocation schemes: First, develop a formal model capturing system objectives, dynamics, and constraints; Second, identify key practical constraints that have major impact on the choice of schemes; Finally, design model-based schemes that respect practical constraints for short-term and obtain insights to inform protocol or market improvement in the long run. We envision that a mathematical theory can significantly improve the future resource allocation ecosystems...|$|E
30|$|<b>Qualitative</b> <b>guidelines</b> – TPMs {{for which}} a {{qualitative}} description of policy impacts is adapted from the ASSIST project.|$|R
40|$|AbstractThe {{development}} of {{strategies for the}} integration of crystallographic database searching with first-principles density functional methods for the discovery and design of novel functional materials is discussed. <b>Qualitative</b> <b>guidelines</b> {{for the design of}} norm-conserving designed nonlocal pseudopotentials developed for high-throughput searches are also discussed, as are the preliminary results of four underdeveloped families of functional materials...|$|R
40|$|Novel shaped fibres are {{introduced}} {{as a means}} of improving the through thickness properties of composite materials. Twelve <b>qualitative</b> <b>guidelines</b> for the geometry of novel shaped reinforcing fibres are described and the bounds they place on fibre designs discussed. The paper concludes by proposing a fibre design that conforms to the majority of the guidelines. Manufacturing methods for experimental quantities of fibre and composite have been developed. Extensive development and testing are now underway to demonstrate the benefits of novel shaped fibres...|$|R
40|$|Phase-diverse {{coherent}} diffraction imaging {{provides a}} route to high sensitivity and resolution with low radiation dose. To {{take full advantage}} of this, the characteristics and tolerable limits of measurement noise for high quality images must be understood. In this work we show the artefacts that manifest in images recovered from simulated data with noise of various characteristics in the illumination and diffraction pattern. We explore the limits at which images of acceptable quality can be obtained and suggest <b>qualitative</b> <b>guidelines</b> that would allow for faster data acquisition and minimize radiation dose...|$|R
30|$|A second {{data set}} (the {{reference}} set), composed of self-made paints, was analyzed in this study. It was built {{with the aim}} to reach better spectral interpretation following <b>qualitative</b> <b>guidelines</b> specifically related to peak ratios, peak overlap and layering effects. The need for improved spectral interpretation {{is due to the}} large number of non-expert users in museums and was highlighted by a pXRF round robin in 2009. The round robin indicated that while 65 % of the laboratories identified the correct elements, only 40 % reached a correct interpretation [21].|$|R
40|$|This paper {{describes}} the research {{activities related to}} the study of LCA methodology applied to the sector of construction and related to possible improvements and developments in this context. The research has provided a first step of analysis {{of the state of the}} art and a second one of verification of the methodology through the application to a case of study represented by the CLT panels produced by a company of Friuli Venezia Giulia. Subsequently, a critical analysis of the work was conducted and were proposed <b>qualitative</b> <b>guidelines</b> for the definition of benchmarks for the interpretation of the LCA results...|$|R
40|$|A mathematically and {{physically}} sound three-degree-of-freedom dynamical model that emulates low- to high-confinement mode (L [...] H) transitions is elicited from a singularity theory critique of earlier fragile models. We construct a smooth {{map of the}} parameter space that is consistent both {{with the requirements of}} singularity theory and with the physics of the process. The model is found to contain two codimension 2 organizing centers and two Hopf bifurcations, which underlie dynamical behavior that has been observed around L-H transitions but not mirrored in previous models. The smooth traversal of parameter space provided by this analysis gives <b>qualitative</b> <b>guidelines</b> for controlling access to H-mode and oscillatory regimes...|$|R
40|$|The RFX {{experimental}} results {{at the highest}} plasma current (about 1 MA) and for relatively long pulses (150 ms) show the presence of high power flux densities on the first wall, up to 40 MW/m 2 averaged on a single tile. Detailed numerical analyses {{have been carried out}} to simulate the thermal response of a single tile. The results of sensitivity analyses give information on the radiative and convective power flux intensities and the related first wall temperature distributions. The experimental data and the results of thermal analyses give quantitative information and <b>qualitative</b> <b>guidelines</b> for the design of an optimized first wall for a RFP device...|$|R
40|$|AbstractAll product {{lifecycle}} {{processes are}} highly determined by product design. The concept of Lean Design focuses on maximizing customer value and minimizing waste throughout {{all stages of}} product lifecycle by an optimized product design. Design for X approaches are essential elements of Lean Design {{to make the right}} design decisions by help of concrete <b>qualitative</b> design <b>guidelines.</b> However, Design for X approaches focus on a specific stage of product lifecycle or specific aspect of products or processes, what makes a holistic optimization of product design highly complex. Therefore, the paper analyses the vast range of <b>qualitative</b> design <b>guidelines</b> given in Design for X approaches concerning their effects on product lifecycle and derives recommendations for a lifecycle optimized product design...|$|R
40|$|Behavioral {{end points}} for {{neurotoxicity}} risk assessment {{have been developed}} and examined {{over the past three}} decades. They are now ready to move from simple <b>qualitative</b> <b>guidelines,</b> such as exemplified by reference doses, to more quantitative models, such as benchmark doses, based on dose-response information. Risk assessors, confronted by a wider array of methodologies and data than in the past, should be offered guidance in interpretation because now {{they have to deal with}} unaccustomed questions and problems. These include reversibility, susceptible populations, multiple end points, and the details of dose-response and dose-effect distributions. Environ Health Perspect 104 (Suppl 2) : 217 - 226 (1996) Key words: benchmark dose, effect levels, identification of neurotoxicity, interindividua...|$|R
40|$|The {{oxidative}} coupling of nitride ligands (N 3 −) to dinitrogen and its microscopic reverse, N 2 -splitting to nitrides, {{are important}} elementary steps in chemical transformations, such as selective ammonia oxidation or nitrogen fixation. Here an experimental and computational evaluation is {{provided for the}} homo- and heterocoupling of our previously reported iridium(IV) and iridium(V) nitrides [IrN(PNP) ]n (n = 0, + 1; PNP = N(CHCHPtBu 2) 2). All three formal coupling products [(PNP) IrN 2 Ir(PNP) ]n (n = 0 –+ 2) were structurally characterized. While the three coupling reactions are all thermodynamically feasible, homocoupling of [IrN(PNP) ]+ is kinetically hindered. The contributing parameters to relative coupling rates are discussed providing <b>qualitative</b> <b>guidelines</b> for the stability of electron rich transition metal nitrides...|$|R
40|$|From the Proceedings of the 1976 Meetings of the Arizona Section - American Water Resources Assn. and the Hydrology Section - Arizona Academy of Science - April 29 -May 1, 1976, Tucson, ArizonaFlow {{frequency}} curves {{supported the}} hypothesis that channel-forming flows are exceptional events in ephemeral mountain streams. This was substantiated {{by the lack of}} a relationship between sediment production and sediment yield. Numerous bed nickpoints indicated channel instability, despite gravel bars and log steps {{that are part of the}} slope adjustment processes. Due to differences in structural density between bars and steps, size distribution of the sediment deposits above them differs. Although only <b>qualitative</b> <b>guidelines</b> are presented, the watershed or wildlife manager should be in a position to utilize the formation of gravel bars and log steps for his management goals...|$|R
40|$|Researchers are {{increasingly}} recognizing {{the importance of}} human aspects in software development and since qualitative methods are used to, in-depth, explore human behavior, we believe that studies using such techniques will become more common. Existing <b>qualitative</b> software engineering <b>guidelines</b> do not cover the full breadth of qualitative methods and knowledge on using them found in the social sciences. The {{aim of this study}} was thus to extend the software engineering research community's current body of knowledge regarding available qualitative methods and provide recommendations and guidelines for their use. With the support of a literature review, we suggest that future research would benefit from (1) utilizing a broader set of research methods, (2) more strongly emphasizing reflexivity, and (3) employing <b>qualitative</b> <b>guidelines</b> and quality criteria. We present an overview of three qualitative methods commonly used in social sciences but rarely seen in software engineering research, namely interpretative phenomenological analysis, narrative analysis, and discourse analysis. Furthermore, we discuss the meaning of reflexivity in relation to the software engineering context and suggest means of fostering it. Our paper will help software engineering researchers better select and then guide the application of a broader set of qualitative research methods. Comment: 29 page...|$|R
50|$|Since {{the onset}} of {{concerns}} regarding diamond origins, {{research has been conducted}} to determine if the mining location could be determined for an emerald already in circulation. Traditional research used <b>qualitative</b> <b>guidelines</b> such as an emerald’s color, style and quality of cutting, type of fracture filling, and/or the anthropological origins of the artifacts bearing the mineral to determine the emerald's mine location. More recent studies using energy dispersive X-ray spectroscopy methods have uncovered trace chemical element differences between emeralds; even emeralds mined within close proximity to one another. American gemologist David Cronin and his colleagues have extensively examined the chemical signatures of emeralds resulting from fluid dynamics and subtle precipitation mechanisms, and their research demonstrated the chemical homogeneity of emeralds from the same mining location and the statistical differences that exist between emeralds from different mining locations, including those between the three locations: Muzo, Coscuez, and Chivor, in Colombia, South America.|$|R
40|$|Curved beams {{are often}} found as {{connections}} of straight elements in structural networks, therefore {{there is an}} interest in assessing their integrity. The use of guided waves for damage characterization in structures is well established for straight elements but still requires {{a full understanding of}} the phenomena occurring when waves interact with discontinuities of different kinds, such as junctions between straight and curved elements and defects located in curved parts. In this paper, the semi analytical finite element method is used to describe wave propagation in curved beams. Firstly, the dispersion curves in toroidal beams are determined, then, the interaction of a longitudinal guided wave with a change of curvature and a defect is described. The effect of the curvature and damage parameters, that is position, extension and intensity on the propagation is studied using a one-dimensional model for the damaged structure. Notwithstanding its simplicity, this model can provide <b>qualitative</b> <b>guidelines</b> in view of the formulation of a damage characterization procedure...|$|R
40|$|Thin {{maintenance}} surfaces (TMS) {{extend the}} service life of bituminous and asphalt cement concrete roads—a task that has challenged road and highway agencies for years. Many of these agencies {{are aware of}} TMS as a maintenance treatment; however, selection of the proper TMS to use has been difficult. Guidelines for TMS selection were needed to improve the success of TMS. The first phase of this research project, funded by the Iowa Department of Transportation and completed in April 1999, developed <b>qualitative</b> selection <b>guidelines.</b> For example, “Slurry seal and micro-surfacing are not recommended for badly cracked pavements; however, those treatments {{can be used to}} address a small amount of light cracking. ” However, the definitions of “badly cracked ” and “light cracking ” can vary from one person to another. Therefore, quantitative standards for the selection of TMS were needed. The second phase of this research project refined the <b>qualitative</b> <b>guidelines</b> and developed quantitative guidelines for TMS selection. These new guidelines use the pavement condition index (PCI) rating developed by the U. S. Army Corps of Engineers. To avoid confusion with another index used in Iowa that is also referred to as the PCI, the index is called the surface condition index (SCI) herein. The allowable distress is chosen by considering an appropriate SCI value for given treatments, traffic levels, and distresses. Users are expected to use judgment and to interpolate or to extrapolate to select a TMS for a particular traffic count. Transportation maintenance managers may find these guidelines useful for pavement management systems. Key words: pavement management—thin maintenance surface...|$|R
40|$|Designing {{aircraft}} engines {{is a complex}} process in which requirements from multiple disciplines need to be considered. Decisions about product geometry and tolerances to achieve optimized aerodynamics, product life and weight can affect the manufacturing process. Therefore, providing information to designers about process capabilities is necessary to support design exploration and analysis. In this paper, the authors propose theWelding Capability Assessment Method (WCAM) {{as a tool to}} support the systematic identification and assessment of design issues related to product geometry critical to the welding process. Within this method, a list of potential failure modes during welding is connected to specific design parameters. Once the critical design parameters have been identified, quantitative methods are proposed to calculate tolerances to reduce the likelihood of welding failures. The application of this method is demonstrated through an industrial case study where a combination of interviews and welding simulations is used to study the welding capability of a number of product geometries. This method represents an advancement from traditional <b>qualitative</b> <b>guidelines</b> and expert judgments about welding difficulties towards a more quantitative approach, supporting virtual design...|$|R
40|$|The aim of {{the paper}} that treats the actuarial model of {{insurance}} in case of survival or early death is to show the actuarial methods and methodology for creating a model and an appropriate number of sub-models {{of the most popular}} form of life insurance in the world. The paper applies the scientific methodology of the deductive character based on scientific, theoretical knowledge and practical realities. Following the basic theoretical model’s determinants, which are {{at the beginning of the}} paper, the basic difference between models further in this paper was carried out according to the character of the premium to be paid. Finally, the financial repercussions of some models are presented at examples in insurance companies. The result of this paper is to show the spectrum of possible forms of capital endowment insurance which can be, without major problems, depending on the financial policy of the company, applied in actual practice. The conclusion of this paper shows the theoretical and the practical reality of this model, life insurance, and its quantitative and <b>qualitative</b> <b>guidelines...</b>|$|R
30|$|<b>Qualitative</b> DBMS {{selection}} <b>guidelines</b> need to {{be extended}} with respect to operational and adaptation features of current DBMS (i.e., support for orchestration frameworks to enable automated operation and adaptation and the integration support into Big Data frameworks).|$|R
40|$|Objective: To {{propose a}} {{framework}} for assessing the rigor of qualitative research that identifies and distinguishes between the diverse objectives of qualitative studies currently used in patient-centered outcomes and health services research (PCOR and HSR). Study Design: Narrative review of published literature discussing <b>qualitative</b> <b>guidelines</b> and standards in peer-reviewed journals and national funding organizations that support PCOR and HSR. Principal Findings: We identify and distinguish three objectives of current qualitative studies in PCOR and HSR: exploratory, descriptive, and comparative. For each objective, we propose methodological standards {{that can be used}} to assess and improve rigor across all study phases—from design to reporting. Similar to quantitative studies, we argue that standards for qualitative rigor differ, appropriately, for studies with different objectives and should be evaluated as such. Conclusions: Distinguishing between different objectives of qualitative HSR improves the ability to appreciate variation in qualitative studies as well as appropriately evaluate the rigor and success of studies in meeting their own objectives. Researchers, funders, and journal editors should consider how adopting the criteria for assessing qualitative rigor outlined here may advance the rigor and potential impact of qualitative research in patient-centered outcomes and health services research...|$|R
40|$|Measurement-based {{admission}} control (MBAC) {{is an attractive}} mechanism to concurrently offer quality of service (QoS) to users, without requiring a priori traffic specification and on-line policing. However, several aspects of such a system need to be clearly understood in order to devise robust MBAC schemes, i. e., schemes that can match a given QoS target despite the inherent measurement uncertainty, and without the tuning of external system parameters. We study the impact of measurement uncertainty, flow arrival, departure dynamics, and of estimation memory {{on the performance of}} a generic MBAC system in a common analytical framework. We show that a certainty equivalence assumption, i. e., assuming that the measured parameters are the real ones, can grossly compromise the target performance of the system. We quantify the improvement in performance {{as a function of the}} length of the estimation window and an adjustment of the target QoS. We demonstrate the existence of a critical time scale over which the impact of admissin decisions persists. Our results yield new insights into the performance of MBAC schemes, and represent quantitative and <b>qualitative</b> <b>guidelines</b> for the design of robust schemes...|$|R
40|$|Competition binding and UV melting {{studies of}} a DNA model system {{consisting}} of three, {{four or five}} mutually complementary oligonucleotides demonstrate that unpaired bases at the branch point stabilize three- and five-way junction loops but destabilize four-way junctions. The inclusion of unpaired nucleotides permits the assembly of five-way DNA junction complexes (5 WJ) having as few as seven basepairs per arm from five mutually complementary oligonucleotides. Previous work showed that 5 WJ, having eight basepairs per arm but lacking unpaired bases, could not be assembled [Wang, Y. L., Mueller, J. E., Kemper, B. and Seeman, N. C. (1991) Biochemistry, 30, 5667 - 5674]. Competition binding experiments demonstrate that four-way junctions (4 WJ) are more stable than three-way junctions (3 WJ), when no unpaired bases are included at the branch point, but less stable when unpaired bases are present at the junction. 5 WJ complexes are in all cases less stable than 4 WJ or 3 WJ complexes. UV melting curves confirm the relative stabilities of these junctions. These results provide <b>qualitative</b> <b>guidelines</b> for improving {{the way in which}} multi-helix junction loops are handled in secondary structure prediction programs, especially for single-stranded nucleic acids having primary sequences that can form alternative structures comprising different types of junctions...|$|R
40|$|AbstractFor {{transport}} {{planners and}} land use practitioners, many are the <b>qualitative</b> <b>guidelines</b> provided regarding the location and quality of separated bicycle facilities. When separated bicycle facilities are poorly designed or placed in less than optimum locations, their intended use is less than anticipated. An interesting element {{in the evaluation of}} bicycle facilities that have received less attention revolve around the disturbance due to the presence of other users on cyclists way. Other users consists in cyclists and pedestrians for off-street bicycle facilities, and motorized vehicles on the roadway. This study focuses on quantifying the role of disturbances encountered on separated cycling facilities, compared to disturbances from cycling mixed with traffic, assuming cyclists speed as a performance measure and analysing the cyclist speed reductions from different types of disturbances. Collecting data on three segments of Bologna's cycling network (Italy), we measured the frequency, type, and speed reduction attributed to different types of disturbances. The data collected shows that pedestrian disturbances on the separated facility are highly frequent but associated with moderate speed reductions, while disturbances in the mixed traffic environment can be relatively fewer but have more severe speed reductions. Moreover, our results suggest that design elements of separated facilities can play a role in affecting the frequency, type, and severity of disturbances. This work helps lay the foundation for outlining the existing relationship between bicycle travel speeds and non stationary disturbances...|$|R
40|$|We {{consider}} a multi-agent {{system for the}} logistics control of Automatic Guided Vehicles {{that are used in}} the dough making process at an industrial bakery. Here, logistics control refers to constructing robust schedules for transportation jobs. We discuss how alternative MAS designs can be developed and compared. <b>Qualitative</b> design <b>guidelines</b> turn out to be insufficient to select the best agent architecture. Therefore, we also use simulation to support decision making, where we use real-life data from the bakery to evaluate alternative designs. We show that depending on the degree of dynamism and objectives of the bakery, different architectures are preferred...|$|R
40|$|Date of {{latest version}} of Annex I against which the {{assessment}} will be made: 31 / 12 / 2011 Deliverable No. D 3. 1. 1 Deliverable Name Report on Social approach of Child Safety- including the data collection methodology, and analysis, and in depth analysis of results for quantitative and <b>qualitative</b> studies, giving <b>guideline</b> for solutions i...|$|R
40|$|Diffusion-tensor fiber {{tracking}} {{was used}} to identify the cores of several long-association fibers, including the anterior (ATR) and posterior (PTR) thalamic radiations, and the uncinate (UNC), su-perior longitudinal (SLF), inferior longitudinal (ILF), and inferior fronto-occipital (IFO) fasciculi. Tracking results were compared to existing anatomical knowledge, and showed good <b>qualitative</b> agreement. <b>Guidelines</b> were developed to reproducibly track these fibers in vivo. The interindividual variability of these recon-structions was assessed in a common spatial reference frame (Talairach space) using probabilistic mapping. As a first illustration of this technical capability, a reduction in brain connectivity in a patient with a childhood neurodegenerative disease (X-linked ad-renoleukodystrophy) was demonstrated. Magn Reson Med 47...|$|R
40|$|AbstractHabitat {{dynamics}} (habitat turnover due {{to natural}} perturbations or human activity) are commonplace, particularly in intensively used landscapes. Conservation planning {{requires an understanding}} of how spatio-temporal habitat dynamics and species characteristics interact with and relate to species persistence. We conducted a systematic literature review to determine how spatial and temporal properties of habitat networks can be changed to improve species viability in dynamic landscapes. We searched for both generalities that can be interpreted as spatial planning guidelines and gaps in knowledge that limit the application in spatial planning. Seventy studies matched our inclusion criteria. From these studies, we extracted knowledge regarding the role of four spatial and five temporal network properties (e. g. network area and habitat turnover rate) for species viability. We found that improving spatial network properties often effectively counterbalances the negative effects of habitat dynamics. Furthermore, changes in several temporal properties can alleviate the impact on species viability, for example, by reducing clustering in habitat turnover events. From these findings, we formulated a first set of general <b>qualitative</b> <b>guidelines</b> for planning practices. Moreover, we identified gaps between the available and required knowledge for planning ecological networks in dynamic landscapes, thereby leading to a research priority list containing the following recommendations: (1) provide guidance regarding the effective management of network properties; (2) compare alternate management regimes and their cost-effectiveness; (3) study management regimes {{for a wide range of}} species and habitat properties. Given the continuing climate change and economic development, guiding network design—including habitat dynamics—is urgently needed...|$|R
40|$|Who {{cares about}} who owns online courses? Nobody, {{because that is}} not what the issue is really about. Ownership is an {{emotional}} issue, but controlling the rights of a copyrightable work is tangible and logical. The important question to answer is not who owns online courses, but who controls the rights of any copyrightable work. For universities and faculty members, getting over the emotional issues and down to the foundation of what is truly at stake is of major concern. While it is nearly impossible to create <b>qualitative</b> <b>guidelines</b> for copyright policies and/or contracts, it is eminently possible to examine existing policies and contracts and relate how a handful of universities are handling copyright and intellectual property issues pertaining to online courses. The purpose of this thesis is to provide a starting point for this complex transaction {{in the form of a}} resource tool that includes some basic background about copyright law, relevant case law related to 2 ̆ 2 work-for-hire, 2 ̆ 2 and relevant academic freedom issues. The original work of this thesis is the creation of a tool, which reviews of a sampling of university policies pertaining to online copyright issues and ownership. Accordingly, the contribution this thesis makes to the understanding and clarification of universities policies related to online material copyright ownership will be important for faculty members and universities in two ways. First, it will help others develop better online copyright policies based on tangible issues rather than emotional ones. Second, this thesis can be a basis for others to build upon for future research on this important topic...|$|R
40|$|D. Phil. The aim of {{the study}} was to explore and {{understand}} the emerging leadership challenges in order to further enhance the development of leadership and management in the information technology industry in South Africa. A combined casing and life-story approach in this modernist qualitative research study was employed. Selective sampling, as described by Plummer (1983; 2001) as part of his critical humanistic approach, was employed to select the CEO, Mr X, from a particular South African information technology company. Data were obtained mainly from solicited sources, but also some that were unsolicited. This resulted in a life story containing rich descriptive data obtained at first hand from Mr X’s professional career and associated areas of his life. The life -story of this storyteller was organised manually as well as with the aid of ATLAS-Ti 5. 0, a computer-based software package suitable for this type of study. The leadership landscape model of Veldsman (2004), a local expert in the area of leadership, together with key theoretical concepts found in the literature, were used to construct an enhanced leadership model. Appropriate <b>qualitative</b> <b>guidelines</b> were used to ensure a study that attends to both academic rigour and aesthetics. The resultant constructed leadership model offers important, if not unique, insights and findings regarding the leadership environment, individual psychosocial dynamics, competencies and capabilities, as well as leadership roles, modes, styles and processes that contribute to personal leadership effectiveness. The thesis illuminates and offers recommendations for a number of methodological and theoretical implications regarding local and global leadership studies, as well as for practice and policy regarding leadership and managerial development...|$|R
40|$|Iterative {{flattening}} search (ifs) is a meta-heuristic {{strategy for}} solving multi-capacity scheduling problems. Given an initial solution, ifs iteratively applies: (1) a relaxation-step, {{in which a}} subset of scheduling decisions are randomly retracted from the current solution; and (2) a flattening-step, in which a new solution is incrementally recomputed from this partial schedule. Whenever a better solution is found, it is retained, and, upon termination, the best solution found during the search is returned. Prior research has shown ifs {{to be an effective}} and scalable heuristic procedure for minimizing schedule makespan in multi-capacity resource settings. In this paper we experimentally investigate the impact on ifs performance of algorithmic variants of the flattening step. The variants considered are distinguished by different computational requirements and correspondingly vary in the type and depth of search performed. The analysis is centered around the idea that given a time bound to the overall optimization procedure, the ifs optimization process is driven by two different and contrasting mechanisms: the random sampling performed by iteratively applying the “relaxation/flattening” cycle and the search conducted within the constituent flattening procedure. On one hand, one might expect that efficiency of the flattening process is key: the faster the flattening procedure, the greater the number of iterations (and number of sampled solutions) for a given time bound; and hence the greater the probability of finding better quality solutions. On the other hand, the use of more accurate (and more costly) flattening procedures can increase the probability of obtaining better quality solutions even if their greater computational cost reduces the number of ifs iterations. Comparative results on well-studied benchmark problems are presented that clarify this tradeoff with respect to previously proposed flattening strategies, identify <b>qualitative</b> <b>guidelines</b> for the design of effective ifs procedures, and suggest some new directions for future work in this area...|$|R
40|$|Quantifying species {{transport}} {{rates is}} a main concern in chemical and petrochemical industries. In particular, {{the design and}} operation of many large-scale industrial chemical processes is as much dependent on diffusion {{as it is on}} reaction rates. However, the existing diffusion models sometimes fail to predict experimentally observed behaviors and their accuracy is usually insufficient for process optimization purposes. Fractional diffusion models offer multiple possibilities for generalizing Fick’s law in a consistent manner in order to account for history dependence and nonlocal effects. These models have not been extensively applied to the study of real systems, mainly due to their computational cost and mathematical complexity. A least squares spectral formulation was developed for solving fractional differential equations. The proposed method was proven particularly well-suited for dealing with the numerical difficulties inherent to fractional differential operators. The practical implementation was explained in detail in order to enhance reproducibility, and directions were specified for extending it to multiple dimensions and arbitrarily shaped domains. A numerical framework based on the least-squares spectral element method was developed for studying and comparing anomalous diffusion models in pellets. This simulation tool is capable of solving arbitrary integro-differential equations and can be effortlessly adapted to various problems in any number of dimensions. Simulations of the flow around a cylindrical particle were achieved by extending the functionality of the developed framework. A test case was analyzed by coupling the boundary condition yielded by the fluid model with two families of anomalous diffusion models: hyperbolic diffusion and fractional diffusion. <b>Qualitative</b> <b>guidelines</b> for determining the suitability of diffusion models can be formulated by complementing experimental data with the results obtained from this approach. PhD i energi- og prosessteknikkPhD in Energy and Process Engineerin...|$|R
40|$|One of {{the primary}} {{challenges}} for decision makers during concept exploration in engineering system design is selecting designs that are valuable throughout the operational lifetime of the system. The problem is even more difficult when designing Systems of Systems (SoS), which are dynamic, complex, higher-order systems that may be composed of both legacy and new component systems. There are several heuristics and <b>qualitative</b> <b>guidelines</b> for designing SoS in the literature, {{but there is a}} lack of practical quantitative methods for SoS concept exploration. Development of quantitative methods for SoS conceptual design will greatly improve the ability of decision makers to select SoS designs that will be value robust over time. These quantitative methods will allow decision makers to consider a larger and more complete set of alternative SoS designs than is possible with qualitative methods alone. A SoS tradespace exploration method is being developed by augmenting the existing Dynamic Multi-Attribute Tradespace Exploration method with SoS-specific considerations, such as the existence of a multi-level stakeholder value function, the incorporation of both legacy and new component systems, and the potential time-varying composition of the SoS. In this paper, a case study of an operationally responsive multi-concept surveillance system for disaster relief is used to illustrate this developing SoS tradespace exploration method. This case study was partially completed as a collaborative project between MIT and the Charles Stark Draper Laboratory, and later extended by MIT. Several surveillance concepts- satellite, aircraft, unmanned air vehicle, and sensor swarms- were considered as possible concept solutions to achieve the surveillance mission objectives. The above system concepts are compared on the same tradespace, demonstrating the ability of the proposed method to allow decision makers to quantitatively compare disparate single system concepts on a common performanc...|$|R
40|$|One of the {{principal}} value propositions {{for the creation of}} Systems of Systems (SoS) is the ability to generate stakeholder value beyond that which can be delivered by a single system or even a collection of systems. The dynamic interactions among the component systems in a SoS make conceptual design decisions for SoS more complex compared to traditional system design. There are several heuristics and <b>qualitative</b> <b>guidelines</b> for designing SoS in the literature, but {{there is a lack of}} practical quantitative methods for SoS concept exploration. Development of quantitative methods for SoS conceptual design will greatly improve the ability of decision makers to select SoS designs in the concept design phase that will be value robust over time, by allowing them to consider a larger and more complete set of possible alternative SoS designs than is possible with qualitative methods alone. Multi-Attribute Tradespace Exploration has been used in the past to compare large numbers of system alternatives on a common cost-utility basis. In this method, the designer elicits the decision maker’s needs and formulates these as quantified attributes. The systems are then analyzed in terms of their ability to achieve the desired levels of attribute metrics. SoS-level attribute calculations must reflect component system interactions and emergent SoS-level value, as well as the added costs and benefits in the SoS as compared to that of the component systems operating alone. This paper introduces techniques for the SoS attribute combination modeling within the Multi-Attribute Tradespace Exploration method when considering SoS with heterogeneous component systems. Combining the attributes for the SoS-level must take into account the nature of the attributes provided by the component systems as well as the concept of operations for SoS. The techniques used for SoS attribute combination are classified according to the level of coordination between the component systems in the SoS. Using th...|$|R
