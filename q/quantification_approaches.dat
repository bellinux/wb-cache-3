77|257|Public
40|$|Quantification is {{the machine}} {{learning}} task of estimating test-data class proportions {{that are not}} necessarily {{similar to those in}} training. Apart from its intrinsic value as an aggregate statistic, quantification output {{can also be used to}} optimize classifier probabilities, thereby increasing classification accuracy. We unify major <b>quantification</b> <b>approaches</b> under a constrained multi-variate regression framework, and use mathematical programming to estimate class proportions for different loss functions. With this modeling approach, we extend existing binary-only <b>quantification</b> <b>approaches</b> to multi-class settings as well. We empirically verify our unified framework by experimenting with several multi-class datasets including the Stanford Sentiment Treebank and CIFAR- 10. Comment: 9 pages, 4 figure...|$|E
40|$|International audienceA {{collaborative}} {{study has been}} carried out according to internationally recognized guidelines in order to establish the performance characteristics of a liquid chromatography single quadrupole mass spectrometry analytical method for the determination of the feed additive semduramicin (SEM) in poultry feed at the level (20 - 25 mg kg- 1) authorized within the European Union. Fifteen laboratories participated in the validation study and all of them reported results. The content of SEM in the tested materials, provided as blind duplicates, ranged from 11. 5 mg kg- 1, which corresponds to half of the mean authorized level to 45. 0 mg kg- 1 which corresponds to twice the mean authorized level. All the materials have been analyzed by the participating laboratories using two different <b>quantification</b> <b>approaches,</b> namely standard addition and external standard calibration. The relative standard deviation of reproducibility (RSDR) for both <b>quantification</b> <b>approaches</b> varied from 8 to 18 %, corresponding to HORRAT values ranging from 0. 8 to 1. 5, which were therefore in all cases below the critical value of 2. 0. Consequently the proposed analytical method and both <b>quantification</b> <b>approaches</b> can be considered fully validated and transferable to the control laboratories to be applied for the determination of SEM in poultry compound feed at authorized level within the frame of official control. Further steps of the administrative procedure aimed to the adoption of the method as part of an ISO/CEN standard are currently ongoing...|$|E
40|$|A {{collaborative}} {{study has been}} carried out according to international recognized guidelines in order to establish the performance characteristics of a liquid chromatography single quadrupole mass spectrometry analytical method for the determination of the feed additive semduramicin (SEM) in poultry feed at the authorized level (20 - 25 mg kg- 1) valid within the European Union. Fifteen laboratories went through the validation phase of the study and all of them reported results. The content of SEM in the tested materials, provided as blind duplicates, ranged from 11. 5 mg kg- 1, which corresponds to half of the mean authorized level to 45. 0 mg kg- 1 which corresponds to twice the mean authorized level. All the materials have been analyzed by the participating laboratories using two different <b>quantification</b> <b>approaches,</b> namely standard addition and external standard calibration. The relative standard deviation of reproducibility (RSDR) for both <b>quantification</b> <b>approaches</b> varied from 8 to 18 %, corresponding to HORRAT values ranging from 0. 8 to 1. 5, which were therefore in all cases below the critical value of 2. 0. Consequently the proposed analytical method and both <b>quantification</b> <b>approaches</b> can be considered fully validated and transferable to the control laboratories to be applied for the determination of SEM in poultry compound feed at authorized level within the frame of official control. Further steps of the administrative procedure aimed to the adoption of the method as part of an ISO/CEN standard are currently ongoing. JRC. DG. D. 6 -Food Safety and Qualit...|$|E
30|$|The risk <b>quantification</b> <b>approach</b> {{determines the}} {{unprocessed}} units {{caused by the}} non-availability of an IT component based on the smart factory’s dependency structure. The resulting VaR values represent the central results of our model and enable {{the identification of the}} most critical IT components. The following section elaborates on the risk <b>quantification</b> <b>approach</b> and its assumptions in more detail.|$|R
30|$|Similar <b>quantification</b> <b>approach</b> for {{biodiesel}} preparation {{has been}} used by earlier workers [73, 74].|$|R
30|$|So far, {{dependency}} matrix D_C,P^*, as {{a central}} artifact of our algorithm and essential for the risk <b>quantification</b> <b>approach,</b> was derived considering the production network (A 1), the information network (A 2), and the functional dependencies between the two networks (A 3). These steps lay the ground for the risk <b>quantification</b> <b>approach,</b> which identifies and evaluates critical IT components regarding IT availability risks.|$|R
40|$|Metallomics (more specifically, metalloproteomics) is an {{emerging}} field that encompasses the role, uptake, {{transport and storage}} of trace metals, which are essential to preserve the functions of proteins within a biological system. The current strategies for metal-binding and metalloprotein analysis based on the combination of polyacrylamide gel electrophoresis (PAGE) and laser ablation inductively coupled plasma mass spectrometry (LA-ICP-MS) are discussed in this review. The advantages, limitations and the most recently developed and applied <b>quantification</b> <b>approaches</b> for this methodology are also described...|$|E
40|$|This is an {{internal}} project milestone report {{to document the}} CCSI Element 7 team's progress on developing Technology Readiness Level (TRL) metrics and risk measures. In this report, we provide {{a brief overview of}} the current technology readiness assessment research, document the development of technology readiness levels (TRLs) specific to carbon capture technologies, describe the risk measures and uncertainty <b>quantification</b> <b>approaches</b> used in our research, and conclude by discussing the next steps that the CCSI Task 7 team aims to accomplish...|$|E
40|$|International audienceQuantitative risk {{analysis}} (QRA) {{is a fundamental}} part of the decision-making process {{when it comes to}} the safety of people and the environment. However, due to the uncertainty involved, the credibility of risk assessment results is still a major issue. This paper aims to explore the most commonly used approaches to quantify uncertainty in {{risk analysis}}: interval analysis, fuzzy theory, probability theory, evidence theory, and the mixed probabilistic-fuzzy approach. These approaches are used to characterize uncertainty in model inputs obtained from different sources, such as statistical data and expert judgments, and to which different types of uncertainty can be attached. These uncertainty characterizations are then propagated through the model to obtain the corresponding representation of uncertainty for the model outputs. The paper presents the application of these <b>quantification</b> <b>approaches</b> to a loss of containment scenario (LOC), representing one of the most likely situations to occur in industry. The overall aim is to study the effects of uncertainty and compare the different approaches. Indeed, the uncertainty <b>quantification</b> <b>approaches</b> presented can lead to different representations of uncertainty in the outputs and hence to different decisions. The use of an inappropriate approach in an inappropriate place may lead to under or overestimation of risk and subsequently to a bad decision...|$|E
30|$|The {{introduced}} content {{has been}} published as part of Philip Wessely’s dissertation “Value Determination of Supply Chain Initiatives: A <b>Quantification</b> <b>Approach</b> Based on Fuzzy Logic and System Dynamics” available at Gabler-Verlag, Wiesbaden 2011.|$|R
40|$|The case-oriented <b>quantification</b> <b>approach</b> {{behind the}} {{software}} program winMAX is, accord-ing to its founder, Kuckartz, {{based on the}} methodological and theoretical work of Max Weber and Alfred Schutz. This claimed connection is not, however, explained in depth in the author’s available scientific literature. This article clarifies the methodological and theoretical back-grounds to winMAX, with special focus {{on the influence of}} Weber and Schutz. It became clear that—in spite of similarities—Weber and Schutz differ in several respects, which raises objec-tions to the claimed connection and puts practical application to the test. More in-depth infor-mation is therefore needed to apply Kuckartz’s case-oriented <b>quantification</b> <b>approach</b> in social research with respect to its theoretical background...|$|R
40|$|Paperboard {{is widely}} used in {{different}} applications, such as packaging and graphic printing, among others. Consumption of recycled paper is growing, which has led the paper-mill packaging industry to apply strict quality controls. This means {{that it is very}} important to develop methods to test the quality of recycled products. In this article, we focus on determining the recoveredfiber content of paperboard samples by applying Fourier transform mid-infrared (FT-MIR) spectroscopy in combination with multivariate statistical methods. To this end, two very fast, nondestructive approaches were applied: classification and <b>quantification.</b> The first <b>approach</b> is based on classifying unknown paperboard samples into two groups: high and low recovered-fiber content. Conversely, under the <b>quantification</b> <b>approach,</b> the content of recovered fiber in the incoming paperboard samples is determined. The experimental results presented in this article show that the classification approach, which classifies unknown incoming paperboard samples, is highly accurate and that the <b>quantification</b> <b>approach</b> has a root mean square error of prediction of about 4. 1 Peer ReviewedPostprint (author's final draft...|$|R
40|$|Millions {{of people}} are {{affected}} by respiratory diseases, leading to a significant health burden globally. Because of the current insufficient knowledge of the underlying mechanisms {{that lead to the}} development and progression of respiratory diseases, treatment options remain limited. To overcome this limitation and understand the associated molecular changes, noninvasive imaging techniques such as PET and SPECT have been explored for biomarker development, with (18) F-FDG PET imaging being the most studied. The quantification of pulmonary molecular imaging data remains challenging because of variations in tissue, air, blood, and water fractions within the lungs. The proportions of these components further differ depending on the lung disease. Therefore, different <b>quantification</b> <b>approaches</b> have been proposed to address these variabilities. However, no standardized approach has been developed to date. This article reviews the data evaluating (18) F-FDG PET <b>quantification</b> <b>approaches</b> in lung diseases, focusing on methods to account for variations in lung components and the interpretation of the derived parameters. The diseases reviewed include acute respiratory distress syndrome, chronic obstructive pulmonary disease, and interstitial lung diseases such as idiopathic pulmonary fibrosis. Based on review of prior literature, ongoing research, and discussions among the authors, suggested considerations are presented to assist with the interpretation of the derived parameters from these approaches and the design of future studies...|$|E
3000|$|... [...]) [20] was {{considered}} as the PET outcome measure that was calculated using three different quantification strategies, one- (1 TC) and two-tissue compartment (2 TC) models [21], and a graphical approach, the likelihood estimation in graphical analysis (LEGA) [22].It {{should be noted}} that the purpose here of considering three different <b>quantification</b> <b>approaches</b> is not to revisit the question of determining the “best” modeling approach for each tracer. This question has been adequately addressed in the original manuscripts for the respective tracers. Rather, multiple <b>quantification</b> <b>approaches</b> provide additional datasets to illustrate how the different test-retest metrics can be applied and what attributes of the data and quantification method can be measured. Ten ROIs were considered in common across all five data sets: anterior cingulate, amygdala, dorsal caudate, dorsolateral prefrontal cortex, gray matter cerebellum, hippocampus, insula, midbrain, parietal lobe, and ventral striatum. In the case of [11 C]WAY- 100635, an additional ROI, the white matter cerebellum, {{was considered}} [11], but not included in this analysis to maintain the same ROIs across all tracers. The test-retest variability is a result of noise in the ROI and in the arterial input function and is impacted by the size of the ROI. Analysis in this paper does not consider the ROI size as a factor, since ROI-size is the same for different tracers binding to the same target.|$|E
40|$|Abstract Traditional cultivation-based {{methods to}} {{quantify}} microbial abundance are {{not suitable for}} analyses of microbial communities in environmental or medical sam-ples, which consist mainly of uncultured microorganisms. Recently, different cultivation-independent <b>quantification</b> <b>approaches</b> {{have been developed to}} overcome this problem. Some of these techniques use specific fluorescence markers, for example ribosomal ribonucleic acid targeted oligonucleotide probes, to label the respective target organisms. Subsequently, the detected cells are visualized by fluorescence microscopy and are quantified by direct visual cell counting or by digital image analysis. This article provides an overview of these methods and some of their applications with emphasis on (semi-) automated image analysis solutions...|$|E
40|$|Conventional cultivation-based {{methods to}} measure {{microbial}} abundance are unsuitable for quanti-fying uncultured microorganisms {{that constitute the}} majority of microbial life in most environmental or medical samples. This problem is solved by the <b>quantification</b> <b>approach</b> described here, which com-bines {{fluorescence in situ hybridization}} (FISH) with rRNA-targeted probes and digital image analysis...|$|R
40|$|Abstract: An {{overview}} of {{nearly half a}} century of involvement with numerical simulations of wave and fluid phenomena has led me to the “visiometric ” or visualization and <b>quantification</b> <b>approach</b> that is illustrated in this paper. I believe that the visiometric approach to simulated scientific data sets will provide opportunities for artistic innovation and expression...|$|R
50|$|Friendlier {{sales and}} use tax audit - While state taxing {{authorities}} typically {{reserve the right}} to audit taxpayers who come forward pursuant to a voluntary disclosure agreement, the audit will typically be limited to the reduced look-back period, and it would generally focus more on understanding and confirming the reasonableness of the taxpayer's liability <b>quantification</b> <b>approach,</b> rather than on uncovering additional liabilities.|$|R
40|$|AbstractDimethyl {{sulfoxide}} (DMSO) {{has been}} advocated as a beneficial additive to electrospray solvents for peptide analysis {{due to the}} improved ionisation efficiency conferred. Previous reports {{have shown that the}} resultant improvements in peptide ion signal intensities are non-uniform. As a result, it was hypothesised that inclusion of DMSO in electrospray solvents could be detrimental to the outcome of intensity-based label-free absolute <b>quantification</b> <b>approaches,</b> specifically the top 3 method. The effect of DMSO as a mobile phase additive in top 3 label-free quantification was therefore evaluated. We show that inclusion of DMSO enhances data quality, improving the precision and number of proteins quantified, with no significant change to the quantification values observed in its absence...|$|E
40|$|Conventional {{automated}} segmentation {{techniques for}} {{magnetic resonance imaging}} (MRI) fail to perform in a robust and consistent manner when brain anatomy differs wildly from expectations – {{as is often the}} case in brain cancers. We propose a novel out-of-atlas technique to estimate the spatial extent of abnormal brain regions by combining multi-atlas based segmentation with semi-local non-parametric intensity analysis. In a study with 30 clinically-acquired MRI scans of patients with malignant gliomas and 29 atlases of normal anatomy from research acquisitions, we demonstrate that this technique robustly identifies cancerous regions. The resulting segmentations could be used to study cancer morphometrics or guide selection/application/refinement of tumor analysis models or regional image <b>quantification</b> <b>approaches...</b>|$|E
40|$|The goal of {{this study}} was to compare {{different}} <b>quantification</b> <b>approaches</b> and reconstruction methods to estimate the binding potential in [11 C]raclopride studies in rats. The final aim was to determine if the results obtained with short-acquisition scanning were comparable to the results obtained with long-acquistion (conventional) scanning. We analyzed two rat data sets: a baseline versus a pretreatment study (with cold raclopride) and a young versus an old animal group comparison. The study results support the contention that optimization of [11 C]raclopride positron emission tomographic studies in rats by shortening the acquisition time is feasible. In addition, filtered backprojection is recommended as a reconstruction algorithm, although iterative methods may be more sensitive to detect within-group differences...|$|E
40|$|We {{describe}} the systems {{we have used}} for participating in Subtasks D (binary quantification) and E (ordinal quantification) of SemEval- 2016 Task 4 "Sentiment Analysis in Twitter". The binary quantification system uses a "Probabilistic Classify and Count" (PCC) approach that leverages the calibrated probabilities obtained from the output of an SVM. The ordinal <b>quantification</b> <b>approach</b> uses an ordinal tree of PCC binary quantifiers, where the tree is generated via a splitting criterion that minimizes the ordinal quantification loss...|$|R
40|$|AbstractA {{stochastic}} uncertainty <b>quantification</b> <b>approach</b> {{has been}} introduced into heterogeneous slope stability evaluation {{with the aid of}} probabilistic collocation method. This method combines Karhunen-Loeve expansion and polynomial chaos expansion to estimate the stochastic properties by running the deterministic geomechanics simulation models independently. With this approach, yield approach index of a heterogeneous slope is computed. The results show that in the toe and middle region FAI have the largest value indicating more instability, and the standard deviation field demonstrates that uncertainty has the largest value in the toe...|$|R
40|$|Lactobacillus sobrius sp. nov., {{which was}} {{recently}} {{isolated from the}} intestine of weaning piglets, has potential probiotic properties. To follow the fate of L. sobrius strain 001 T in dietary interventions, a novel and strain-specific quantitative detection procedure was developed. This procedure {{was based on the}} isolation of specific genomic fragments from the type strain by representational difference analysis and their detection by real-time PCR. The described strain-specific <b>quantification</b> <b>approach</b> may be used in studies aimed at tracking bacterial strains added to specific environments...|$|R
40|$|Contractors’ {{claims for}} {{extension}} of time and/or cost reimbursements could result in disagreements {{that may not be}} amicably resolved by the parties concerned. Consequently significant additional costs are incurred in construction projects due to disagreements over these claims. A major criticism of the Sri Lankan construction industry is persistent delays in project delivery. A contributory factor to those delays is disagreements over certain percentage of business’ overhead expenses that are unrecoverable by the contractor. This unrecovered head office overheads (HOOH) is an actual loss to the contractor and the contractor could make a claim for the actual costs incurred during the delay. The selection and application of the most suitable recovery or calculation method is critical for both clients and contractors. As an aspect of a larger study which develops a HOOH claim process model, the current study focuses on the review of the methods currently being practiced to recover HOOH claims internationally as well as within the Sri Lankan construction industry. The preferred methods used within Sri Lankan construction industry to evaluate contractors’ claims are the formula approach and actual method by contractors and clients respectively. This study shows that salaries and wages of head office human resources and transporting and travelling costs contribute significantly to the contractors’ HOOH. There are a number of issues with the <b>quantification</b> <b>approaches</b> used during the HOOH claim stages that result in conflicts. The research therefore suggests {{that there needs to be}} pre-established claim-tracking processes for claim initiation, quantification and evaluation. The pre-established process would provide a clear understanding of HOOH claims and positively direct claimants to agreed claim records, HOOH cost data and <b>quantification</b> <b>approaches...</b>|$|E
40|$|Soil {{heterogeneity}} and {{the lack}} of detailed site characterization are two ubiquitous factors that render predictions of flow and transport in the vadose zone inherently uncertain. We employ the Green–Ampt model of infiltration and the Dagan– Bresler statistical parameterization of soil properties to compute probability density functions (PDFs) of infiltration rate and infiltration depth. By going beyond uncertainty <b>quantification</b> <b>approaches</b> based on mean and variance of system states, these PDF solutions enable one to evaluate probabilities of rare events that are required for probabilistic risk assessment. We investigate the temporal evolution of the PDFs of infiltration depth and corresponding infiltration rate, the relative importance of uncertainty in various hydraulic parameters and their cross-correlation, and the impact of the choice of a functional form of the hydraulic function...|$|E
40|$|AbstractDMNT {{biosynthesis}} {{was proposed}} to proceed via (E) -nerolidol in plants a decade ago. However, (E) -nerolidol function as airborne signal/substrate for in-vivo biosynthesis of DMNT {{remains to be}} investigated and the regulation of DMNT production and emission is largely unknown. We address both of these aspects using Achyranthes bidentata model plant in conjunction with deuterium-labeled d 5 -(E) -nerolidol, headspace, GC-FID, and GC/MS-based absolute <b>quantification</b> <b>approaches.</b> We demonstrate that airborne (E) -nerolidol is specifically metabolized in-vivo into DMNT emission, but requires airborne VOC MeJA or predator herbivore as additional environmental signal. In addition, we provide new insight into the complex regulation underlying DMNT emission, and highlight the importance of studying multiple environmental factors on emission patterns of plant VOCs and their mechanistic regulation...|$|E
40|$|To support {{effective}} decision making, engineers should comprehend {{and manage}} various uncertainties throughout the design process. In today's modern systems, quantifying uncertainty can become cumbersome and computationally intractable for one {{individual or group}} to manage. This is particularly true for systems comprised {{of a large number}} of components. In many cases, these components may be developed by different groups and even run on different computational platforms, making it challenging or even impossible to achieve tight integration of the various models. This thesis presents an approach for overcoming this challenge by establishing a divide-and-conquer methodology, inspired by the decomposition-based approaches used in multidisciplinary analysis and optimization. Specifically, this research focuses on uncertainty analysis, also known as forward propagation of uncertainties, and sensitivity analysis. We present an approach for decomposing the uncertainty analysis task amongst the various components comprising a feed-forward system and synthesizing the local uncertainty analyses into a system uncertainty analysis. Our proposed decomposition-based multicomponent uncertainty analysis approach is shown to converge in distribution to the traditional all-at-once Monte Carlo uncertainty analysis under certain conditions. Our decomposition-based sensitivity analysis approach, which is founded on our decomposition-based uncertainty analysis algorithm, apportions the system output variance among the system inputs. The proposed decomposition-based uncertainty <b>quantification</b> <b>approach</b> is demonstrated on a multidisciplinary gas turbine system and is compared to the traditional all-at-once Monte Carlo uncertainty <b>quantification</b> <b>approach.</b> To extend the decomposition-based uncertainty <b>quantification</b> <b>approach</b> to high dimensions, this thesis proposes a novel optimization formulation to estimate statistics from a target distribution using random samples generated from a (different) proposal distribution. The proposed approach employs the well-defined and determinable empirical distribution function associated with the available samples. The resulting optimization problem is shown to be a single linear equality and box-constrained quadratic program and can be solved efficiently using optimization algorithms that scale well to high dimensions. Under some conditions restricting the class of distribution functions, the solution of the optimization problem yields importance weights that are shown to result in convergence in the Ll-norm of the weighted proposal empirical distribution function to the target distribution function, as the number of samples tends to infinity. Results on a variety of test cases show that the proposed approach performs well in comparison with other well-known approaches. The proposed approaches presented herein are demonstrated on a realistic application; environmental impacts of aviation technologies and operations. The results demonstrate that the decomposition-based uncertainty <b>quantification</b> <b>approach</b> can effectively quantify the uncertainty of a multicomponent system for which the models are housed in different locations and owned by different groups. by Sergio Daniel Marques Amaral. Thesis: Ph. D., Massachusetts Institute of Technology, Department of Aeronautics and Astronautics, 2015. Cataloged from PDF version of thesis. Includes bibliographical references (pages 165 - 175) ...|$|R
5000|$|Recurrence <b>quantification</b> analysis, another <b>approach</b> to {{quantify}} recurrence properties.|$|R
5000|$|... #Subtitle level 4: <b>Quantification</b> and new <b>approaches</b> {{to history}} ...|$|R
40|$|Deep {{penetration}} of personal computers, data communication networks, and the Internet {{has created a}} massive platform for data collection, dissemination, storage, and retrieval. Large amounts of textual data are now available at a very low cost. Valuable information, such as consumer preferences, new product developments, trends, and opportunities, {{can be found in}} this large collection of textual data. Growing worldwide competition, new technology development, and the Internet contribute to an increasingly turbulent business environment. Conducting surveillance on this growing collection of textual data could help a business avoid surprises, identify threats and opportunities, and gain competitive advantages. Current text mining approaches, nonetheless, provide limited support for conducting surveillance using textual data. In this dissertation, I develop novel text <b>quantification</b> <b>approaches</b> to identify useful information in textual data, effective anomaly detection approaches to monitor time series data aggregated based on the text <b>quantification</b> <b>approaches,</b> and empirical evaluation approaches that verify the effectiveness of text mining approaches using external numerical data sources. In Chapter 2, I present free-text chief complaint classification studies that aim to classify incoming emergency department free-text chief complaints into syndromic categories, a higher level of representation that facilitates syndromic surveillance. Chapter 3 presents a novel detection algorithm based on Markov switching with jumps models. This surveillance model aims at detecting different types of disease outbreaks based on the time series generated from the chief complaint classification system. In Chapters 4 and 5, I studied the surveillance issue under the context of business decision making. Chapter 4 presents a novel text-based risk recognition design framework {{that can be used to}} monitor the changing business environment. Chapter 5 presents an empirical evaluation study that looks at the interaction between news sentiment and numerical accounting earnings information. Chapter 6 concludes this dissertation by highlighting major research contributions and the relevance to MIS research...|$|E
30|$|To make a trustful {{decision}} {{about the future}} field development plans of the cyclic CO_ 2 -assisted gravity drainage (GAGD) process, two different successive uncertainty <b>quantification</b> <b>approaches</b> were conducted to obtain the true optimal solution of oil recovery in the South Rumaila oil field. Given the base reservoir model of nominal production controls, the uncertainty was first quantified in terms of geological parameters. Therefore, nine realizations for each of permeability and anisotropy ratio were created and systematically simulated into the compositional reservoir model for geological uncertainty assessment. More specifically, DoE was adopted to create 81 distinct reservoir models honoring these nine realizations for each property to obtain the reservoir flow response of oil recovery. There was a significant impact of the geological uncertainties on the reservoir flow response as there was an important gap between the least-likely and most-likely field cumulative oil production.|$|E
40|$|A growing need {{exists for}} a {{quantitative}} measure of inspection effectiveness as an input to quantitative risk-informed in-service inspection (RI-ISI). A Probability of Detection (POD) curve {{could provide a}} suitable metric, but there can be significant problems associated with generating realistic POD curves by practical trials. The ENIQ inspection qualification methodology is used to provide high confidence that an inspection system will achieve its objectives, but is not {{designed to provide a}} quantitative measure of the type {{that can be used in}} RI-ISI analysis. This paper describes the results of a project set up to investigate approaches to quantifying the confidence associated with inspection qualification. The project applied <b>quantification</b> <b>approaches</b> in a pilot study, and produced guidelines on how to relate inspection qualification results, risk reduction and inspection interval. JRC. DG. F. 5 -Safety of present nuclear reactor...|$|E
30|$|This study {{aimed to}} provide a robust, {{clinically}} suitable <b>quantification</b> <b>approach</b> for the third-generation TSPO ligand [18 F]GE- 180 in MS patients. The investigated static 60 – 90  min imaging containing a PRR-based SUVR quantification correlated well with DVR from modelling by application of the Logan reference tissue model on dynamic 90  min and thus proved suitability for clinical TSPO PET application, when patient compliance and economic aspects have to be considered. The presence of non-saturated lesion TACs suggests that a prolongation of the scan duration, {{at the cost of}} a lower count statistic, might allow for an improved assessment of equilibrium and tracer wash-out.|$|R
30|$|In {{the current}} study, we {{acquired}} microSPECT/CT images at various time points {{to better understand}} the in vivo kinetic profile of 123 I-CMICE- 013 in healthy rats and quantified the activity in various organs. Time-activity curves were constructed to evaluate the uptake of tracers in the myocardium and other organs. We compared the data from imaging-based analysis to those acquired through tissue dissection and gamma counter measurement, and verified the accuracy of the image-based <b>quantification</b> <b>approach.</b> Radiation dosimetry is an important metric for evaluating the safety of a novel radioactive imaging agent. The internal radiation exposure of 123 I-CMICE- 013 was assessed using the resident activities data from the imaging biodistribution.|$|R
40|$|Abstract — Large-scale {{information}} systems (IS) investments of banks have {{direct impact on}} operational risk. After the New Basel Accord comes into effect 2006 operational risk may change the regulatory capital of banks. Considering long-term IS investments, one {{must take into account}} these effects even now. However, the possibility of integrating these effects in an investment decision depends on both applied investment measurement and Basel II risk <b>quantification</b> <b>approach.</b> The authors introduce a methodology for integrating these aspects into the decision process. Although there are several possible solutions, this paper emphasizes net present value as a controlling method and the loss distribution approach as a possible risk quantification instrument. I...|$|R
