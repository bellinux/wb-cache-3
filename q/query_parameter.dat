21|196|Public
5000|$|... binds {{the method}} {{parameter}} {{to the value}} of an HTTP <b>query</b> <b>parameter.</b>|$|E
5000|$|Sorting {{the query}} {{parameters}}. Some web pages use {{more than one}} <b>query</b> <b>parameter</b> in the URL. A normalizer can sort the parameters into alphabetical order (with their values), and reassemble the URL. Example: ...|$|E
5000|$|By convention, {{the server}} {{providing}} the JSON data offers the requesting website {{to name the}} JSONP function, typically using the name jsonp or callback [...] as the named <b>query</b> <b>parameter</b> field name, in its request to the server, e.g., ...|$|E
5000|$|Removing default <b>query</b> <b>parameters.</b> A {{default value}} in the query string may render identically whether it is there or not. Example: ...|$|R
5000|$|Webframeworks {{may provide}} methods for parsing {{multiple}} <b>parameters</b> in the <b>query</b> string, separated by some delimiter. In the example URL below, multiple <b>query</b> <b>parameters</b> {{are separated by}} the ampersand, : ...|$|R
50|$|The snippet of text in {{the search}} results {{matching}} the search query can be highlighted by specifying a set of field names {{as one of the}} <b>query</b> <b>parameters</b> for hit highlighting.|$|R
5000|$|Additionally, Mozilla {{supported}} the [...] "custom keyword" [...] feature. This feature allowed users to access their bookmarks from the location bar using keywords (and an optional <b>query</b> <b>parameter).</b> For example, using a custom keyword, a user could type [...] "google apple" [...] into the address bar and be redirected {{to the results}} of a Google search for [...] "apple".|$|E
30|$|In {{contrast}} to previous work, {{we focus on}} the processing of analytic queries for NoSQL databases where no data are stored in relational databases. We propose an index structure suited to answer drill-down and roll-up queries over large amount of data within fast response time. Similar to work in [22], we particularly focus on the temporal queries with the time-range as the <b>query</b> <b>parameter.</b>|$|E
30|$|As the {{preprocessing}} {{phase was}} done in certain extent {{it was possible to}} guarantee that analyzing these filtered tweets will give reliable results. Twitter does not provide the gender as a <b>query</b> <b>parameter</b> so {{it is not possible to}} obtain the gender of a user from his or her tweets. It turned out that twitter does not ask for user gender while opening an account so that information is seemingly unavailable. We used an AI tool which can be used to accurately deduce gender from a person’s user name called NamSor.|$|E
40|$|A central idea of Language Models is that {{documents}} (and perhaps queries) are random variables, {{generated by}} data-generating functions that {{are characterized by}} document (<b>query)</b> <b>parameters.</b> The key new idea {{of this paper is}} to model that a relevance judgment is also generated stochastically, and that its data generating function is also governed by those same document and <b>query</b> <b>parameters.</b> The result of this addition is that any available relevance judgments are easily incorporated as additional evidence about the true document and <b>query</b> model <b>parameters.</b> An additional aspect of this approach is that it also resolves the long-standing problem of document-oriented versus query-oriented probabilities. The general approach can be used with a wide variety of hypothesized distributions for documents, queries, and relevance. We test the approach on Reuters Corpus Volume 1, using one set of possible distributions. Experimental results show that the approach does succeed in incorporating relevance data to improve estimates of both document and <b>query</b> <b>parameters,</b> but on this data and for the specific distributions we hypothesized, performance was no better than two separate one-sided models. We conclude that the model's theoretical contribution is its integration of relevance models, document models, and query models, and that the potential for additional performance improvement over one-sided methods requires refinements...|$|R
5000|$|The SP crafts a {{proprietary}} authentication request that is {{passed through the}} browser using URL <b>query</b> <b>parameters</b> to supply the requester's SAML entityID, the assertion consumption location, and optionally the end page to return the user to.|$|R
50|$|There {{was only}} a URL builder module in this category. Pipes needed to have URLs of RSS to get the content. This URL builder enabled users to create URLs instead of typing the URL address. It used a base URL and <b>query</b> <b>parameters</b> to {{generate}} other URLs.|$|R
40|$|Abstract. We use PostgreSQL DBMS {{for storing}} XML metadata, {{described}} by the IVOA Characterisation Data Model. Initial XML type support in the PostgreSQL has recently been implemented. We make heavy use of this feature {{in order to provide}} comprehensive search over Characterisation metadata tree. We built a prototype of the Characterisation metadata query service, implementing two access methods: (1) HTTP-GET/POST based interface implements almost direct translation of the <b>query</b> <b>parameter</b> name into XPath of the data model element in the XML serialisation; (2) Web-Service based interface to receive XQuery which is also directly translated into XPath. This service will be used in the ASPID-SR archive, containing science-ready data obtained with the Russian 6 -m telescope. 1...|$|E
40|$|Context. Many {{applications}} today use databases {{to store}} user informationor other data for their applications. This information can beaccessed through various different languages {{depending on what}} typeof database it is. Databases that use SQL can maliciously be exploitedwith SQL injection attacks. This type of attack involves inserting SQLcode in the <b>query</b> <b>parameter.</b> The injected code sent from the clientwill then be executed on the database. This can lead to unauthorizedaccess to data or other modifications within the database. Objectives. In this study we investigate if a system can be builtwhich prevents SQL injection attacks from succeeding on web applicationsthat is connected with a MySQL database. In the intendedmodel, a proxy is placed between the web server and the database. The purpose of the proxy is to hash the SQL <b>query</b> <b>parameter</b> dataand remove any characters that the database will interpret as commentsyntax. By processing each query before it reaches its destination webelieve we can prevent vulnerable SQL injection points from being exploited. Methods. A literary study is conducted the gain the knowledgeneeded to accomplish the objectives for this thesis. A proxy is developedand tested within a system containing a web server and database. The tests are analyzed {{to arrive at a}} conclusion that answers ours researchquestions. Results. Six tests are conducted which includes detection of vulnerableSQL injection points and the delay difference on the system withand without the proxy. The result is presented and analyzed in thethesis. Conclusions. We conclude that the proxy prevents SQL injectionpoints to be vulnerable on the web application. Vulnerable SQL injectionpoints is still reported even with the proxy deployed in thesystem. The web server is able to process more http requests that requiresa database query when the proxy is not used within the system. More studies are required since there is still vulnerable SQL injectionspoints...|$|E
40|$|We use PostgreSQL DBMS {{for storing}} XML metadata, {{described}} by the IVOA Characterisation Data Model. Initial XML type support in the PostgreSQL has recently been implemented. We make heavy use of this feature {{in order to provide}} comprehensive search over Characterisation metadata tree. We built a prototype of the Characterisation metadata query service, implementing two access methods: (1) HTTP-GET/POST based interface implements almost direct translation of the <b>query</b> <b>parameter</b> name into XPath of the data model element in the XML serialisation; (2) Web-Service based interface to receive XQuery which is also directly translated into XPath. This service will be used in the ASPID-SR archive, containing science-ready data obtained with the Russian 6 -m telescope. Comment: 3 pages, 1 figure. Proceedings of ADASS-XVI held in Tucson, AZ, USA, Oct. 2006. To appear in the ASP Conference Serie...|$|E
40|$|In recent years, several {{methods have}} been {{proposed}} for implementing interactive similarity queries on multimedia databases. Common to all these methods is the idea to exploit user feedback in order to progressively adjust the <b>query</b> <b>parameters</b> and to eventually converge to an “optimal ” parameter setting. However, all these methods also share the drawback to “forget ” user preferences across multiple query sessions, thus requiring the feedback loop to be restarted for every new query, i. e. using default parameter values. Not only is this proceeding frustrating from the user’s point of view but it also constitutes a significant waste of system resources. In this paper we present FeedbackBypass, {{a new approach to}} interactive similarity query processing. It complements the role of relevance feedback engines by storing and maintaining the <b>query</b> <b>parameters</b> determined with feedback loops over time, using a wavelet-based data structure (the Simplex Tree). For each query, a favorable set of <b>query</b> <b>parameters</b> can be determined and used to either “bypass ” the feedback loop completely for already-seen queries, or to start the search process from a near-optimal configuration. FeedbackBypass can be combined well with all state-of-the-art relevance feedback techniques working in high-dimensional vector spaces. Its storage requirements scale linearly with the dimensionality of the query space, thus making even sophisticated query spaces amenable. Experimen...|$|R
25|$|The non-persistent (or reflected) cross-site {{scripting}} vulnerability {{is by far}} {{the most}} basic type of web vulnerability. These holes show up when the data provided by a web client, most commonly in HTTP <b>query</b> <b>parameters</b> (e.g. HTML form submission), is used immediately by server-side scripts to parse and display a page of results for and to that user, without properly sanitizing the request.|$|R
40|$|In recent years, several {{methods have}} been {{proposed}} for implementing interactive similarity queries on multimedia databases. Common to all these methods is the idea to exploit user feedback in order to progressively adjust the <b>query</b> <b>parameters</b> and to eventually converge to an "optimal" parameter setting. However, all these methods also share the drawback to "forget" user preferences across multiple query sessions, thus requiring the feedback loop to be restarted for every new query, i. e. using default parameter values. Not only is this proceeding frustrating from the user's point of view but it also constitutes a significant waste of system resources. In this paper we present FeedbackBypass, {{a new approach to}} interactive similarity query processing. It complements the role of relevance feedback engines by storing and maintaining the <b>query</b> <b>parameters</b> determined with feedback loops over time, using a wavelet-based data structure (the Simplex Tree). For each query, a favorable set of <b>query</b> <b>parameters</b> can be determined and used to either "bypass" the feedback loop completely for already-seen queries, or to start the search process from a near-optimal configuration. FeedbackBypass can be combined well with all state-of-the-art relevance feedback techniques working in high-dimensional vector spaces. Its storage requirements scale linearly with the dimensionality of the query space, thus making even sophisticated query spaces amenable. Experimen- Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large [...] ...|$|R
40|$|Abstract. Small {{watershed}} floods {{have a short}} confluence time. Aiming at {{the demand}} of flash floods forecast, we do this research mainly around two key components, telemetry terminal and central station software. First, we design the telemetry terminal technology index, telemetry terminal circuit and GXU- 1001 Hydrological Telemetry Terminal. They all based on equipment interface, communication channel, working mode, data sampling, solid-state storage, data <b>query,</b> <b>parameter</b> settings, power management, equipment, power consumption and other special technical requirements. Second, we developed central station software for GXU- 1001, realize the telemetry terminal set parameters, received and compiled data, managed database, remote diagnosis and calibration, forwarded data, leak proof message, etc. The application in medium reservoirs of Baise, Guangxi, China shows the water level, rainfall, and images can be automatically acquired, transfered and processed. It provides timely and accurate hydrological data for the flood control and disaster mitigation...|$|E
40|$|The aim of {{this study}} is a {{transparent}} tool for analysis of voice (sustained phonation /a/) and query data capable of providing support in screening for laryngeal disorders. In this work, screening is concerned with identification of potentially pathological cases by classifying subject’s data into ’healthy’ and ’pathological’ classes as well as visual exploration of data and automatic decisions. A set of association rules and a decision tree, techniques lending themselves for exploration, were generated for pathology detection. Data pairwise similarities, estimated in a novel way, were mapped onto a 2 D metric space for visual inspection and analysis. Accurate identification of pathological cases was observed on unseen subjects using the most discriminative <b>query</b> <b>parameter</b> and six audio parameters routinely used by otolaryngologists in a clinical practice: equal error rate (EER) of 11. 1 % was achieved using association rules and 10. 2 % using the decision tree. The EER was further reduced to 9. 5 % by combining results from these two classifiers. The developed solution can be a useful tool for Otolaryngology departments in diagnostics, education and exploratory tasks...|$|E
30|$|The {{product that}} {{we chose to}} analyze was the iPhone 6. Even though it is {{possible}} to analyze any product’s popularity using the defined method, the availability of data was an important issue. At the time of this research the only electronic product trending on twitter was the iPhone 6. Meaning that a reasonable amount of data about this device was available. But our method can be used to obtain results for any product given that a good amount of data about it is available on Twitter. So only the tweets which contained the term ‘iPhone 6 ’ in them were obtained. As we also decided to determine which feature of the iPhone 6 was most or least popular the query was enhanced using a few keywords to obtain feature specific tweets. An example would be ‘iPhone 6 battery’. This <b>query</b> <b>parameter</b> will cause the API to return only tweets which contain both iPhone 6 and battery terms together which results in tweets about the battery performance of the iPhone 6. Other keywords used were “camera”, “iOS”, “iTunes”, “screen”, “sound”, and “touch”. For each tweet, the user name, tweet text, location were extracted.|$|E
50|$|Intelligent code {{completion}} is a context-aware code completion {{feature in}} some programming environments that speeds {{up the process}} of coding applications by reducing typos and other common mistakes. Attempts to do this are usually done through auto completion popups when typing, <b>querying</b> <b>parameters</b> of functions, <b>query</b> hints related to syntax errors, etc. Intelligent code completion and related tools serve as documentation and disambiguation for variable names, functions and methods using reflection.|$|R
50|$|JWS {{is a way}} to {{authenticate}} (but not necessarily encrypt) information in a highly serializable, machine-readable format. That means that it is information, along with proof that the information hasn't changed since being signed. It can be used for sending information from one web site to another, and is especially aimed at communications on the web. It even contains a compact form optimized for applications like URI <b>query</b> <b>parameters.</b>|$|R
50|$|<b>Query</b> <b>parameters</b> can {{be bound}} to C++ {{variables}} either by value or by reference. If desired, native SQL queries can be executed {{in which case}} ODB still provides support for parameter binding. ODB also supports prepared queries which are a thin wrapper around the underlying database system's prepared statements functionality. Prepared queries provide a way to perform potentially expensive query preparation tasks only once and then execute the query multiple times.|$|R
40|$|I {{would like}} to thank my advisor Prof. Jayant Haritsa for helping me to take {{the first step in the}} world of {{scientific}} research. I {{would like to}} sincerely acknowledge his invaluable guidance and encouragement in all forms through out my stay in IISc. I do convey my gratitude to Atreyee and Harish for their valuable contributions and suggestions in structuring the Joint Paper. I {{would like to thank}} all the members of DSL who have made my stay at IISc memorable. I thank my family and friends for their continued support throughout my career. i Given a parameterized n-dimensional SQL query template and a choice of query optimizer, a plan diagram is a color-coded pictorial enumeration of the execution plan choices of the optimizer over the <b>query</b> <b>parameter</b> space. Similarly, we can define cost diagram and cardinality diagram as the pictorial enumerations of cost and cardinality estimations of the optimizer over the same space. These three diagrams are collectively called “optimizer diagrams”. These diagrams have proved to be very useful for the analysis and redesign of modern optimizers but their utility is adversel...|$|E
40|$|We {{consider}} encoding {{problems for}} range queries on arrays. In these problems {{the goal is}} to store a structure capable of recovering the answer to all queries that occupies the information theoretic minimum space possible, to within lower order terms. As input, we are given an array A[1 [...] n], and a fixed parameter k ∈ [1,n]. A range top-k query on an arbitrary range [i,j] ⊆ [1,n] asks us to return the ordered set of indices {ℓ_ 1, [...] ., ℓ_k} such that A[ℓ_m] is the m-th largest element in A[i [...] j], for 1 < m < k. A range selection query for an arbitrary range [i,j] ⊆ [1,n] and <b>query</b> <b>parameter</b> k' ∈ [1,k] asks us to return the index of the k'-th largest element in A[i [...] j]. We completely resolve the space complexity of both of these heavily studied problems [...] -to within lower order terms [...] -for all k = o(n). Previously, the constant factor in the space complexity was known only for k= 1. We also resolve the space complexity of another problem, that we call range min-max, in which {{the goal is to}} return the indices of both the minimum and maximum elements in a range. Comment: 24 pages: a short version of this paper will be presented at ICALP 201...|$|E
40|$|Abstract The {{database}} {{community has}} devoted extensive amount {{of efforts to}} indexing and querying temporal data in the past decades. However, insufficient amount of {{attention has been paid}} to temporal ranking queries. More precisely, given any time instance t, the query asks for the top-k objects at time t with respect to some score attribute. Some generic indexing structures based on R-trees do support ranking queries on temporal data, but as they are not tailored for such queries, the performance is far from satisfactory. We present the Seb-tree, a simple indexing scheme that supports temporal ranking queries much more efficiently. The Seb-tree answers a top-k query for any time instance t in the optimal number of I/Os in expectation, namely, N k O(logB B B) I/Os, where N is the size of the data set and B is the disk block size. The index has near-linear size (for constant and reasonable kmax values, where kmax is the maximum value for the possible values of the <b>query</b> <b>parameter</b> k), can be constructed in near-linear time, and also supports insertions and deletions without affecting its query performance guarantee. Most of all, the Seb-tree is especially appealing in practice due to its simplicity as it uses the B-tree as the only building block. Extensive experiments on a number of large data sets, show that the Seb-tree is more than an order of magnitude faster than the R-tree based indexes for temporal ranking queries...|$|E
40|$|Continuous top-k query over {{streaming}} data {{is a fundamental}} problem in database. In this paper, {{we focus on the}} sliding window scenario, where a continuous top-k query returns the top-k objects within each query window on the data stream. Existing algorithms support this type of queries via incrementally maintaining a subset of objects in the window and try to retrieve the answer from this subset as much as possible whenever the window slides. However, since all the existing algorithms are sensitive to <b>query</b> <b>parameters</b> and data distribution, they all suffer from expensive incremental maintenance cost. In this paper, we propose a self-adaptive partition framework to support continuous top-k query. It partitions the window into sub-windows and only maintains a small number of candidates with highest scores in each sub-window. Based on this framework, we have developed several partition algorithms to cater for different object distributions and <b>query</b> <b>parameters.</b> To our best knowledge, it is the first algorithm that achieves logarithmic complexity w. r. t. k for incrementally maintaining the candidate set even in the worstcase scenarios...|$|R
5000|$|<b>Query</b> string <b>parameters</b> in the URI are {{appropriate}} {{if they are}} inputs to a Resource which is an algorithm ...|$|R
40|$|There are {{increasing}} number of sites that proved search facility of their own. They {{are a kind of}} databases open to public with HTML interface, and are referred as Invisible Web. We are developing a system, which integrates these specialized search sites for user’s purpose. A solution is the automatic wrapper generation. In this paper, we show how we can extract the attributes of the <b>query</b> <b>parameters</b> to construct a query URL for each site...|$|R
40|$|Given a {{parametrized}} n-dimensional SQL {{query template}} and {{a choice of}} query optimizer, a plan diagram is a color-coded pictorial enumeration of the execution plan choices of the optimizer over the <b>query</b> <b>parameter</b> space. These diagrams {{have proved to be}} a powerful metaphor for the analysis and redesign of modern optimizers, and are gaining currency in diverse industrial and academic institutions. However, their utility is adversely impacted by the impractically large computational overheads incurred when standard bruteforce exhaustive approaches are used for producing fine-grained diagrams on high-dimensional query templates. In this paper, we investigate strategies for efficiently producing close approximations to complex plan diagrams. Our techniques are customized to the features available in the optimizer’s API, ranging from the generic optimizers that provide only the optimal plan for a query, to those that also support costing of sub-optimal plans and enumerating rank-ordered lists of plans. The techniques collectively feature both random and grid sampling, as well as inference techniques based on nearest-neighbor classifiers, parametric query optimization and plan cost monotonicity. Extensive experimentation with a representative set of TPC-H and TPC-DS-based query templates on industrial-strength optimizers indicates that our techniques are capable of delivering 90 % accurate diagrams while incurring less than 15 % of the computational overheads of the exhaustive approach. In fact, for full-featured optimizers, we can guarantee zero error with less than 10 % overheads. These approximation techniques have been implemented in the publicly available Picasso optimizer visualization tool. 1...|$|E
40|$|The {{database}} {{community has}} devoted extensive amount {{of efforts to}} indexing and querying temporal data in the past decades. However, insufficient amount of {{attention has been paid}} to temporal ranking queries. More precisely, given any time instance t, the query asks for the top-k objects at time t with respect to some score attribute. Some generic indexing structures based on R-trees do support ranking queries on temporal data, but as they are not tailored for such queries, the performance is far from satisfactory. We present the Seb-tree, a simple indexing scheme that supports temporal ranking queries much more efficiently. The Seb-tree answers a top-k query for any time instance t in the optimal number of I/Os in expectation, namely, O (log(B) N/B + k/B) I/Os, where N is the size of the data set and B is the disk block size. The index has near-linear size (for constant and reasonable k (max) values, where k (max) is the maximum value for the possible values of the <b>query</b> <b>parameter</b> k), can be constructed in near-linear time, and also supports insertions and deletions without affecting its query performance guarantee. Most of all, the Seb-tree is especially appealing in practice due to its simplicity as it uses the B-tree as the only building block. Extensive experiments on a number of large data sets, show that the Seb-tree is more than an order of magnitude faster than the R-tree based indexes for temporal ranking queries...|$|E
40|$|This poster {{proposes a}} post {{disaster}} {{evaluation of the}} damages produced by the tsunami in the Tohuku-oki region considering knowledge discovery from TerraSAR-X (TSX) products, by mapping extracted primitive features into semantic classes, thus assuring an interactive technique for productive information mining. Knowledge discovery from Earth Observation images implies mapping low level descriptors (primitive features) extracted from the image into semantic classes {{in order to provide}} an interactive method for effective image information mining. In the frame of information theory a communication channel is considered between remote sensing imagery and the user who receives existing information in the data sources, coded as image semantic content. This channel has three components - Data Source Model Generation, Query and Data Mining. Data Source Model Generation uses image content analysis to generate a set of scene’s content descriptors. Further, the Query component involves the user and performs an image retrieval based on image content as <b>query</b> <b>parameter.</b> The query component relies on the Support Vector Machine classifier which is able to group descriptors into relevant semantic classes. The classifier supports rapid mapping scenarios and interactive mapping. The envisaged data mining process includes three stages: data annotation, data query and quantitative analysis of the results. The Data annotations step considers dataset description, data preparation and data classification in order to perform user annotations. Some query examples considering several scenarios include: Assessment of the transportation infrastructures, highrisk of broken roads caused by damaged bridges, debris detection, assessment of aquaculture areas, and possible energy loss due to the damaged high voltages poles or assessment of agriculture areas, damaged crops and estimation of losses...|$|E
50|$|Faceted Navigation {{allows users}} to specify a field to facet in the <b>query</b> <b>parameters</b> passed to Azure Search. Users can drill down or filter search results by using {{criteria}} such as categories, prices and brand. There are several parameters providing customization of faceting capabilities such as sort and intervals. For example, if you specify facet=rating, sort:-valueThe returning results will contains all groups with a rating in descending order by value. Faceted navigation is common in most e-commerce sites such as Amazon.|$|R
30|$|The column headers shown {{online and}} in the.CSV files are {{described}} in Additional file 1 : Tables S 1 to S 6. Within the tables, the ordering of entries is governed sequentially, first by ‘Location Code’ (in alphabetical order), then ‘Core ID’ (in alphabetical order), ‘Core Depth (cm)’ or ‘Composite Depth (cm)’ (depending on which is given; however, if both exist, data will be arranged by core depth), and finally ‘Age (yr. BP)’. Above the results tables, summaries of the <b>query</b> <b>parameters</b> chosen by the user are displayed (Figure 8).|$|R
5000|$|A {{search string}} can be {{specified}} {{as one of the}} <b>query</b> <b>parameters</b> to retrieve matching documents. Azure Search supports search strings using simple query syntax. Supported features include logical operators, the suffix operator, and query with Lucene query syntax. (currently in preview) As an example, [...] white+house will search for documents containing both [...] "white" [...] and [...] "house". Lucene query syntax provides features similar to simple query syntax for logical operators and wildcard searches while also supporting more complicated functions such as proximity search and fuzzy search, ...|$|R
