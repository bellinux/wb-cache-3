2|27|Public
40|$|Quantification {{of quality}} {{parameters}} of inland and near shore waters {{by means of}} remote sensing has encountered {{varying degrees of success}} in spite of the high variability of the parameters under consideration and limitations of remote sensors themselves. This paper comprehensively evaluates the quantification of four types of water quality parameters: inorganic sediment particles, phytoplankton pigments, coloured dissolved organic material and Secchi disk depth. It concentrates on <b>quantification</b> <b>requirements,</b> as well as the options in selecting the most appropriate sensor data for the purpose. Relevant factors, such as quantification implementation and validation of the quantified results are also extensively discussed. This review reveals that the relationship between in situ samples and their corresponding remotely sensed data can be linear or nonlinear, but are nearly always site-specific. The quantification has been attempted from terrestrial satellite data largely for suspended sediments and chlorophyll concentrations. The quantification has been implemented through integration of remotely sensed imagery data, in situ water samples and ancillary data in a geographic information system (GIs). The introduction of GIS makes the quantification feasible for more variables at an increasingly higher accuracy. Affected by the number and quality of in situ samples, accuracy of quantification has been reported in different ways and varies widely...|$|E
40|$|Over {{the past}} decades, {{numerous}} practical applications of Digital Soil Mapping have emerged {{to respond to}} the need of land managers. One important contribution to this effort is the release of regional-scale soil maps from the GlobalSoilMap (GSM) project. While the GSM project aims at producing soil property predictions on a fine 90 × 90 m grid at the global scale, land managers often require aggregated information over larger areas of interest (e. g. farms, watersheds, municipalities). This study evaluated a geostatistical procedure aiming at aggregating GSM grids to a land management scale, thereby providing land suitability maps with associated uncertainty for the French region ‘Languedoc-Roussillon’ (27 236 km 2). Specifically, maps were derived from three GSM prediction grids (pH, organic carbon and clay content) by calculating the proportion of ‘suitable’ agricultural land within a municipality, where suitability was defined as having soil property values below or above a predefined threshold (pH 375 g/kg). Calculation of these nonlinear spatial aggregates and the associated uncertainty involved a three-step approach: (i) sampling from the conditional probability distributions of the soil properties at all grid cells by means of sequential Gaussian simulation applied to a regression kriging model, (ii) transformation of soil properties to suitability indicators for all grid cell samples generated in the first step and (iii) spatial aggregation of the suitability indicators from grid cells to municipalities. The maps produced show large differences between municipality areas for all three land suitability indicators. The uncertainties associated with the aggregated suitability indicators were moderate. This approach demonstrated that fine-scale GSM products may also fulfil user demands at coarser land management scales, without jeopardizing uncertainty <b>quantification</b> <b>requirements.</b> </p...|$|E
40|$|A {{nonlinear}} aircraft {{model is}} presented {{and used to}} develop an overall unified approach to online trim and maneuverability envelope estimation with uncertainty <b>quantification</b> without any <b>requirement</b> for active input excitation. The concept of time scale separation makes this method suitable for the adaptive characterization of altered safe maneuvering limitations based on aircraft performance after impairment. The results {{can be used to}} provide pilot feedback and/or be combined with flight planning, trajectory generation, and guidance algorithms to help maintain safe aircraft operations in both nominal and off-nominal scenarios...|$|R
40|$|Bayesian network are {{powerful}} probabilistic graphical models for modelling uncertainty. Among others, classification represents an important application: {{some of the}} most used classifiers are based on Bayesian networks. Bayesian networks are precise models: exact numeric values should be provided for <b>quantification.</b> This <b>requirement</b> is sometimes too narrow. Sets instead of single distributions can provide a more realistic description in these cases. Bayesian networks can be generalized to cope with sets of distributions. This leads to a novel class of imprecise probabilistic graphical models, called credal networks. In particular, classifiers based on Bayesian networks are generalized to so-called credal classifiers. Unlike Bayesian classifiers, which always detect a single class as the one maximizing the posterior class probability, a credal classifier may eventually be unable to discriminate a single class. In other words, if the available information is not sufficient, credal classifiers allow for indecision between two or more classes, this providing a less informative but more robust conclusion than Bayesian classifiers...|$|R
40|$|The {{main focus}} of this thesis is the {{analysis}} of channels used by Czech banks to increase their capital ratios. We identify the increase in capital as the main channel used. Further, within these channels we find retained earnings {{to be the main}} tool used. In addition, growth in the loans volume was the dominant tool within the channel of asset volume. Furthermore, we observe an increase in the use of more advanced capital <b>requirements</b> <b>quantification</b> methods, especially in larger banks. We also identify several factors, other than capital regulation, that might have contributed to the observed developments...|$|R
40|$|A {{quantitative}} study about eco cities, {{resulting in a}} vision of future sustainable urban development after a hypothetical energy revolution. The project regards remaining problems after the energy problem is solved. Sustainability means sustainable economy. A new assesment method is introduced based on a <b>quantification</b> of resource <b>requirements.</b> What kinds of resources does a city need? And how sustainable are the means of aquisition of those resources? Besides energy, attention is payed to; food, water, living space and materials. Throughout the research the motivating question was: Can we think of a model to accomodate a growing world population without negative environmental consequences?UrgenciesThe Why FactoryArchitectur...|$|R
40|$|Abstract—Agreement on Non-Functional Requirements be-tween {{customer}} and supplier {{is crucial to}} a successful IT solution delivery project. In an ideal world, stakeholders and architects cooperate to achieve their common goals in a win-win situation. In a commercial setting, however, one dominant feature often introduces powerful forces from outside the technical realm. That feature is the customer/supplier relationship, usually formalized in bidding rules or as a delivery contract. Formal customer/supplier relationships often place severe limitations on information exchange between stakehold-ers and architects. In this paper, we explore the effect of such limitations on the interaction of architectural design with the <b>quantification</b> of Non-Functional <b>Requirements,</b> and explore a number of avenues to deal with them. I...|$|R
40|$|Software quality models {{provide a}} {{framework}} to measure and evaluate software quality of software systems. They are the basis upon which classify requirements and may be eventually used to guide the <b>quantification</b> of these <b>requirements,</b> especially non-functional requirements. Lots of approaches for building quality models have been proposed in the last decades, but still their reuse along different projects is a challenge. In this paper we present several types of knowledge repositories and reuse processes to bridge this gap. The approach implements the idea of software factory and uses some well-known standards and notations like ISO/IEC 25010 as quality standard and the i* framework to codify knowledge patterns. We will illustrate how this reusebased approach helps in obtaining composite quality models for systems that integrate several software components with an individual quality model each. Peer ReviewedPostprint (published version...|$|R
40|$|Optimization of {{the total}} risk level by {{economically}} feasible risk mitigation can only be achieved through a well-grounded monetary evaluation of the current risk situation. In the field of material supply and procurement directly affecting production and distribution processes a lack of such quantitative simulation model has been identified. Efficient modeling and simulation of monetary supply risk quantification in manufacturing enterprises requires target oriented simplification of the <b>quantification</b> model. Thereby <b>requirements</b> from three different disciplines need to be considered: (1) procurement in supply chains, (2) supply risks, and (3) monetary risk quantification. In this {{paper we propose a}} target oriented approach to deviate and discuss requirements for an applicable monetary supply risk quantification model particularly focusing on supply risks providing meaningful results to derive reasonable risk mitigation measures. Hence, a requirement checklist will be provided to be considered in development of monetary supply risk quantification models...|$|R
40|$|ABSTRACT The {{adequacy}} {{range of}} dietary requirements of specific amino acids in disease states {{is difficult to}} determine. In health, several techniques are available allowing rather precise <b>quantification</b> of <b>requirements</b> based on growth of the organism, rises in plasma concentration, or increases in the oxidation of marker amino acids during incremental administration of the amino acid under study. Requirements may not be similar in disease with regard to protein synthesis or with regard to specific functions such as scavenging of reactive oxygen species by compounds including glutathione. Requirements for this purpose can be assessed only when such a function can be measured and related to clinical outcome. There is apparent consensus concerning normal sulfur amino acid (SAA) re-quirements. WHO recommendations amount to 13 mg/kg per 24 h in healthy adults. This amount is roughly doubled in artificial nutrition regimens. In disease or after trauma, requirements may be altered for methionine, cysteine, and taurine. Although in specific cases of congenital enzyme deficiency, prematurity, or diminished liver function, hy-permethionemia or hyperhomocysteinemia may occur, SAA supplementation can be considered safe in amounts exceeding 2 – 3 times the minimal recommended daily intake. Apart from some very specific indications (e. g., acetaminophen poisoning), the usefulness of SAA supplementation is not yet established. There is {{a growing body of}} data pointing out the potential importance of oxidative stress and resulting changes in redox state in numerous diseases including sepsis, chronic inflammation, cancer, AIDS/HIV, and aging. These observations warrant con...|$|R
40|$|Partiality abounds in {{specifications}} and programs. We {{present a}} threevalued typed logic for reasoning equationally about programming {{in the presence}} of partial functions. The logic in essence is a combination of the equational logic E and typed LPF. Of course, there are already many logics in which some classical theorems acquire the status of neither-true-nor-false. What is distinctive here is that we preserve the equational reasoning style of E, as well as most of its main theorems. The principal losses among the theorems are the law of the excluded middle, the anti-symmetry of implication, a small complication in the trading law for existential <b>quantification,</b> and the <b>requirement</b> to show definedness when using instantiation. The main loss among proof methods is proof by mutual implication; we present some new proof strategies that make up for this loss. Some proofs are longer than in E, but the heuristics commonly used in the proof methodology of E remain valid. We pres [...] ...|$|R
40|$|Abstract — The {{estimation}} of the expected traffic loss ratio (workload loss ratio, WLR) is a key issue in provisioning Quality of Service in packet based communication networks. Despite of its importance, the stationary (long run) loss ratio in queueing analysis is usually estimated through other assessable quantities, typically based on the approximates of the buffer overflow probability. These approaches have the drawback, that the relation between loss ratio and buffer overflow probability is often hardly quantifiable and it can in principle be arbitrary. In this paper we define a calculus for communication networks which is suitable for workload loss estimation based on the original definition of stationary loss ratio. Our novel calculus is a stochastic extension of the deterministic network calculus, which takes an envelope approach to describe arrivals and services for the <b>quantification</b> of resource <b>requirements</b> in the network. We introduce the effective w-arrival curve and the effective w-service curve for describing the inputs and the service and we show that the per-node results can be extended to a network of nodes with {{the definition of the}} effective network w-service curve...|$|R
40|$|The {{introduction}} of a new Building Act in New Zealand in 2004 prompted {{a comprehensive review of}} the Building Code to ensure that its provisions were both consistent with the new Act, and contained sufficient <b>quantification</b> of performance <b>requirements.</b> The review resulted in new Protection from Fire code clauses, Acceptable Solutions (prescriptive, non-mandatory deemed-to-satisfy provisions), and a Verification Method (based on the structural design process where design loads and performance criteria are specified) being introduced in 2012. This new generation of fire safety regulation is expected to substantially reduce the level of inconsistency and inefficiency that previously existed. The next paradigm shift in the New Zealand building regulatory environment is expected to introduce a risk-informed regime, where probabilistic provisions will be incorporated to address the inherent risk and uncertainty associated with performance-based fire safety engineering design. With this scenario in mind, a collaborative research project involving BRANZ Ltd and the University of Canterbury has recently developed a new fire safety engineering design tool called B-RISK, forming part of the essential underlying research necessary to underpin any such futur...|$|R
50|$|However, even by better {{understanding}} and modeling both the meteorological and power conversion processes, {{there will always}} be an inherent and irreducible uncertainty in every prediction. This epistemic uncertainty corresponds to the incomplete knowledge one has of the processes that influence future events. Therefore, in complement to point forecasts of wind generation for the coming hours or days, of major importance is to provide means for assessing online the accuracy of these predictions. In practice today, uncertainty is expressed in the form of probabilistic forecasts or with risk indices provided along with the traditional point predictions. It has been shown that some decisions related to wind power management and trading are more optimal when accounting for prediction uncertainty. For the example of the trading application, studies have shown that reliable estimation of prediction uncertainty allows wind power producer to significantly increase their income in comparison to the sole use of an advanced point forecasting method. Other studies of this type deal with optimal dynamic <b>quantification</b> of reserve <b>requirements,</b> optimal operation of combined systems including wind, or multi-area multi-stage regulation. More and more research efforts are expected on prediction uncertainty and related topics.|$|R
40|$|Medicines are {{important}} in curing and preventing diseases, and hence, {{the ultimate goal of}} `Health for All’ cannot be achieved if people do not have adequate access to essential drugs. Evidences show that substantial savings can be achieved by improving the selection and <b>quantification</b> of drug <b>requirements</b> through the use of essential drug lists and by purchasing drugs competitively. In India, though there exists a national drug list, different states have their own list of state formulary {{which may or may not}} necessarily be based on the list of essential drugs. A few state governments in India have their formulary based on the list of essential drugs. The state government of Tamil Nadu besides adopting a list of essential drugs has also streamlined the procurement and distribution of the same which is being looked upon as a model by other state governments. This paper details the procedures adopted by the Tamil Nadu Medical Services Corporation in procuring and supplying essential drugs to the government health care which is a positive measure in ensuring `health for all’. [GIDR WP NO 161]Essential drugs; WHO; Public health services;Tamil Nadu; inequity; globalisation; structural adjustment programmes; nutrition;education; Central medical stores...|$|R
40|$|Aim: The aim of {{this study}} was to develop a {{multiplex}} immunoassay for nine different drugs. Therefore each reagent undergoes a stringent quality control e. g. antibodies used has to be validated with at least 2 independent methods. Methods: For validation Western Blot analysis and ELISA were performed. A competitive ELISA was established allowing the quantification of the drugs in sera. Appropriate controls were included for background subtraction, determination of unspecific signals and assay control. Furthermore the assay was miniaturized on a microarray, which was produced with a non-contact spotter. Results: For 4 out of 9 selected drugs specific antibodies could be obtained and a competitive ELISA established for quantification. Validated antibodies were characterized by no cross-reactivity to serum and no unspecific binding to other compounds. Serum samples with spiked drugs or samples from the LKA Berlin were analyzed. Each sample was performed in triplicates and each experiment was done twice at least. Limits of <b>quantification</b> meet the <b>requirements</b> of the GTFCh. First multiplex and spotting experiments were done with a satisfying result. Conclusion: The presented approach enables a sensitive and reliable detection method for drug abuse in sera. The validation studies are continued for the remaining drugs...|$|R
40|$|International audienceRobust image {{analysis}} of spots in microarrays (quality control + spot segmentation + <b>quantification)</b> is a <b>requirement</b> for automated software {{which is of}} fundamental importance for a high-throughput analysis of genomics microarray-based data. This paper deals {{with the development of}} model-based image processing algorithms for qualifying/segmenting/quantifying adaptively each spot according to its morphology. A series of morphologicalmodels for spot intensities are introduced. The spot typologies representmost of the possible qualitative cases identified from a large database (different routines, techniques, etc.). Then, based on these spot models, a classification framework has been developed. The spot feature extraction and classification (without segmenting) is based on converting the spot image to polar coordinates and, after computing the radial/angular projections, the calculation of granulometric curves and derived parameters from these projections. Spot contour segmentation can also be solved by working in polar coordinates, calculating the up/downminimal path, which is easily obtained with the generalized distance function. With this model-based technique, the segmentation can be regularised by controlling different elements of the algorithm. According to the spot typology (e. g., doughnut-like or egg-like spots), several minimal paths can be computed to obtain a multi-region segmentation. Moreover, this segmentation is more robust and sensible to weak spots, improving the previous approaches...|$|R
40|$|Recent {{advances}} in satellite communication technologies {{in the tropical}} regions have led to {{significant increase in the}} demand for services and applications that require high channel quality for mobile satellite terminals. Determination and <b>quantification</b> of these <b>requirements</b> are important to optimize service quality, particularly in the Malaysian region. Moreover, the tests on current satellite propagation models were carried out at temperate regions whose environmental characteristics are much different from those in Malaysia. This difference renders these propagation models inapplicable and irrelevant to tropical regions in general. This paper presents the link characteristics observations and performance analysis with propagation measurements done in tropical region to provide an accurate database regarding rain and power arches supply (PAs) attenuations in the tropics for mobile scenarios. Hence, an extension for improving the performance assessment and analysis of satellite/transmission has been achieved. The Malaysia propagation measurement for mobile scenario (Malaysia-PMMS) enables first-hand coarse estimation and attenuation analysis, because the attenuation resulting from rain and PAs becomes easily amenable for measurement. Parallel to that, the measured attenuation has been compared with that of the simulated output at noise floor level. The underlying analytical tool is validated by measurements specific at tropical region, for dynamic model of mobile satellite links operating at higher than 10 [*]GHz...|$|R
40|$|AbstractPartiality abounds in {{specifications}} and programs. We {{present a}} three-valued typed logic for reasoning equationally about programming {{in the presence}} of partial functions. The logic in essence is a combination of the equational logic E and typed LPF. Of course, there are already many logics in which some classical theorems acquire the status of neither-true-nor-false. What is distinctive here is that we preserve the equational reasoning style of E, as well as most of its main theorems. The principal losses among the theorems are the law of the excluded middle, the anti-symmetry of implication, a small complication in the trading law for existential <b>quantification,</b> and the <b>requirement</b> to show definedness when using instantiation. The main loss among proof methods is proof by mutual implication; we present some new proof strategies that make up for this loss. Some proofs are longer than in E, but the heuristics commonly used in the proof methodology of E remain valid. We present a Hilbert-style axiomatisation of the logic in which modus ponens and generalisation are the only inference rules. The axiomatisation is easily modified to yield a classical axiomatisation of E itself. We suggest that the logic may be readily extended to a many-valued logic, and that this will have its uses...|$|R
40|$|Robust image {{analysis}} of spots in microarrays (quality control + spot segmentation + <b>quantification)</b> is a <b>requirement</b> for automated software {{which is of}} fundamental importance for a high-throughput analysis of genomics microarray-based data. This paper deals {{with the development of}} model-based image processing algorithms for qualifying/segmenting/quantifying adaptively each spot according its morphology. A series of morphological models for the spot intensities are introduced. The spot categories represent most of possible qualitative cases identified from a large database (different routines, techniques, etc.). Then based on these spots models, a classification framework has been developed. The spot feature extraction and classification (without segmenting) is based on converting the spot image to polar coordinates and, after computing the radial/angular projections, the calculation of granulometric curves and derived parameters from the projections. Spot contour segmentation can be solved by working in polar coordinates, and then calculating the up/down minimal path, easily obtained with the generalized distance function. With this model-based technique, the segmentation can be regularized by controlling different elements of the algorithm. According to the spot typology (e. g., doughnut-like or egg-like spots), several minimal paths can be computed to obtain a multi-region segmentation. Moreover this segmentation is more robust and sensible to weak spots, improving the previous approaches...|$|R
40|$|Current {{awareness}} {{of the seriousness of}} losses of salmonid fishes associated with hydroelectric developments and with water abstraction from river systems has stimulated renewed commitments on the part of fishery agencies to mitigation of damage, restoration of degraded habitat, protection and promotion of wild stocks, and increased artificial production of salmonids. To achieve these aims the fish system, the fluvial system, and the system of human values and intentions must be integrated; in addition today 2 ̆ 7 s knowledge must be equal to the challenge. Examination of the present status of six aspects of the task shows that (1) facilitation of fish passage at dams is a very high priority but requires greater commitment, (2) hatchery production has generated unreasonable expectations and may be laying the basis for the demise of wild populations, (3) the practice of stocking fry has run ahead of its evaluation, (4) determination of instream flow requirements is bedeviled by spurious <b>quantification,</b> (5) drawdown <b>requirements</b> of impoundments seem incompatible with fishery objectives, and (6) stream habitat improvements give mixed results and may be of restricted application in terms of scale. Yet another limitation of fisheries 2 ̆ 7 aspirations lies in political support. It is concluded that this is a time for stock-taking, improvement of current practices, and assessment of trends...|$|R
40|$|Current {{industrial}} processes often involve the collaboration {{of people at}} distant and remote locations. The technological media for such a tele-cooperation reach from simple email or text-based chatting systems to highly-sophisticated systems for an interactive video-conferencing. But with limited bandwidth the communication between persons at distant locations is often restricted to single modalities. Although this may still be suitable for some tasks, it may result into serious shortcomings and decreased performance with complex tasks like cooperative assembly or maintenance. This is because restricted communication reduces {{the availability of a}} common ground, i. e. sharing a common understanding of knowledge, opinions, and goals. The study presented in this paper examines the effect of different communication media on performance of a collaborative assembly task. The results show that tele-cooperation leads to additional verbal communication (AM(direct) = 71. 1 s; AM(vide o) = 145. 6 s; AM(audio) = 204. 7 s) and, thus, longer times to complete the task (AM (direct) = 45. 95 min; AM (video) = 50. 2 min; am AM(audio) = 56. 16 min). The percentage of relative speech duration also increases significantly. Workload measurement with NASA-TLX did not show any significant differences between cooperation modes. The results allow estimating the effect of reduced communication modalities on time to complete an assembly task. This facilitates a <b>quantification</b> of temporal <b>requirements</b> in time-critical maintenance and repair tasks...|$|R
40|$|The duck {{has a great}} {{potential}} in helping to meet the growing demand for high quality protein in human diets. In order for ducks to meet their potential, {{more research is needed}} to establish their dietary requirements. Feeding and excreta collection techniques developed at Purdue University that minimize loss of excreta during collection, the first step towards precise dietary <b>requirement</b> <b>quantification,</b> are described. Using the techniques that were developed, energy and amino acid utilization of White Pekin ducks were evaluated in several studies. Diets supplying 3, 000 kcal ME/kg, 0. 6 % Methionine, and 1. 2 % Lysine are adequate to meet duck`s requirement for optimum growth in the first week of life. Predicted ME of diets based on utilization values of individual ingredients was compared with the measured ME in the diets, the result showed that the energy in the feedstuffs had additive when compounded into a diet for ducks. In the case of amino acids, some essential amino acid showed significant associative effect. Enzymes likes xylanase and phytase improved performance and bone mineralization when used in ducks diet. The effect was demonstrated to be due to reduction in digesta viscosity and increased P availability as a result of the use of xylanase and phytase, respectively, the effects being more pronounced in younger ducks. There is need for more research to be done in the areas of amino acid nutrition in ducks and the evaluation of non-traditional feedstuffs...|$|R
40|$|A robust {{design process}} starts with {{modeling}} {{of the physical}} system and the uncertainty it faces. Robust design tools are then applied to achieve specified performance criteria. Verification of system properties is crucial as improvements on the modeling and design practices can be made based on results of such verification. In this thesis, we discuss three aspects of this closed-loop process. First {{and the most important}} aspect is the possibility of the feedback from verification to system modeling and design. When verification is hard, what does it tell us about our system? When the system is robust, would it be easy to verify so? We study the relation between robustness of a system property posed as a decision problem and the proof complexity of verifying such property. We examine this relation in two classes of problems: percolation lattices and linear programming problems, and show complexity is upper-bounded by the reciprocal of robustness, i. e. fragility. The second aspect we study is model validation. More precisely, when given a candidate model and experiment data, how do we rigorously refute the model or gain information about the consistent parameter set? Different methods for model invalidation and parameter inference are demonstrated with the G-protein signaling system in yeast to show the advantages and hurdles in their applications. While <b>quantification</b> of robustness <b>requirements</b> has been well-studied in engineering, it is just emerging in the field of finance. Robustness specification in finance is closely related to the availability of proper risk measures. We study the estimation of a coherent risk measure, Expected Shortfall (ES). A consistent and asymptotically normal estimator for ES based on empirical likelihood is proposed. Although empirical likelihood based estimators usually involve numerically solving optimization problems that are not necessarily convex, computation of our estimator can be carried out in a sequential manner, avoiding solving non-convex optimization problems...|$|R
40|$|Recent {{studies have}} {{revealed}} large unexplained variation in heat requirement-based phenology models, resulting in large uncertainty when predicting ecosystem carbon and water balance responses to climate variability. Improving {{our understanding of}} the heat requirement for spring phenology is thus urgently needed. In this study, we estimated the species-specific heat requirement for leaf flushing of 13 temperate woody species using long-term phenological observations from Europe and North America. The species were defined as early and late flushing species according to the mean date of leaf flushing across all sites. Partial correlation analyses were applied to determine the temporal correlations between heat requirement and chilling accumulation, precipitation and insolation sum during dormancy. We found that the heat requirement for leaf flushing increased by almost 50 % over the study period 1980 - 2012, with an average of 30 heat units per decade. This temporal increase in heat requirement was observed in all species, but was much larger for late than for early flushing species. Consistent with previous studies, we found that the heat requirement negatively correlates with chilling accumulation. Interestingly, after removing the variation induced by chilling accumulation, a predominantly positive partial correlation exists between heat requirement and precipitation sum, and a predominantly negative correlation between heat requirement and insolation sum. This suggests that besides the well-known effect of chilling, the heat requirement for leaf flushing is also influenced by precipitation and insolation sum during dormancy. However, we hypothesize that the observed precipitation and insolation effects might be artifacts attributable to the inappropriate use of air temperature in the heat <b>requirement</b> <b>quantification.</b> Rather than air temperature, meristem temperature is probably the prominent driver of the leaf flushing process, but these data are not available. Further experimental research is thus needed to verify whether insolation and precipitation sums directly affect the heat requirement for leaf flushing...|$|R
40|$|An {{emission}} inventory tool for estimating SO 2, NO 2, and PM 10 emissions from brick clamp kiln sites was developed from investigations performed on three representative South African clamp kiln sites {{in order to}} facilitate application for Atmospheric Emission Licenses (AELs) from these sources. The tool utilizes readily available site-specific parameters to generate emission factors for significant activities that emit the aforementioned pollutants. PM 10 emission factors for significant processes were developed using empirical expressions from the Compilation of Air Pollutant Emission Factors (AP- 42) documents. SO 2 emission factor for clamp kiln firing was obtained from “reverse-modelling”, a technique that integrates ambient monitoring and dispersion modelling (using Atmospheric Dispersion Modelling System software) to “standardize” actual emission rate from an assumed rate of 1 g/s. The use of multiple point sources proved to improve the simulation of the buoyancy-induced plume rise; therefore, a “bi-point” source configuration was adopted for the kiln. The “reverse-modelling” technique and “bi-point” source configuration produced SO 2 emission rates differing from - 9 % to + 22 % from mass balance results, indicating that the “reverse-modelling” calculations provide reliable emission estimates for SO 2. An NO 2 emission factor could not be obtained from the “reverse-modelling” technique due to experimental errors and the significant effect of NO 2 emissions from other onsite air emission sources such as internal combustion engines. The NO 2 emission factor was obtained from previous comprehensive study on a similar clamp kiln site. The emission factors obtained from this study were utilized in developing an “{{emission inventory}} tool” which is utilized by clay brick manufacturers in quantifying air emissions from their sites. Emissions <b>quantification</b> is a <b>requirement</b> for brick manufacturers to obtain an AEL which is regulated under South African environmental laws. It is suggested that the technique used here for SO 2 emission confirmation could be used to estimate emissions from a volume or area source where combustion occurs and where knowledge of the source parameters is limited. Dissertation (MSc) [...] University of Pretoria, 2014. gm 2014 Chemical Engineeringunrestricte...|$|R
40|$|This {{deliverable}} {{provides a}} general {{discussion of the}} international and Belgian legal framework applicable to operations of remotely piloted aircraft. Due to diverging use of many terms with different meaning {{in the field of}} unmanned aviation, {{the first part of the}} deliverable explains the different notions used and their meaning, establishing the use of common terminology in line with the international legal framework and the applicable Belgian legislation. It further discusses the applicable international legal framework and its impact on national and EU-level lawmaking in the field of unmanned aviation. The report considers the recent developments at EU level in the field of unmanned aviation. While it is mainly focused on the rules of the existing legal framework, it also touches upon some recent initiatives of EASA and the European Commission. It discusses the importance of common legal rules at EU level for the successful ‘take-off’ of unmanned aviation and how the existing barriers might impair the commercial success of these technologies. Following the analysis of the international safety legal framework, the report focuses on the specifics of the national legislation in Belgium. It situates the recently adopted Royal decree on the use of remotely piloted aircraft in Belgian airspace in the general aviation legal framework. It discusses the structure of the decree, focusing on the sections of importance to ensuring the safe operation of an unmanned aircraft. Finally, it will critically analyse the requirements of the royal decree in light of the recent policy and legislative initiatives at EU level. The report will particular focus on the level of autonomy allowed by the royal decree and will argue that a reasonable balance between safety and autonomy is critical to the success of commercial unmanned aviation. The final part of the report focuses on the elicitation and implementation of the non-functional legal requirements in the context of SafeDroneWare. It briefly discusses the strategies for extraction and <b>quantification</b> of legal <b>requirements,</b> the difficulties encountered in the context of SafeDroneWare and possible strategies for mitigating the lack of sufficient clarity in the legal provisions. The report provides a simple set of points that that could facilitate the elicitation of non-functional legal requirements in the development of software for remotely piloted aircraft in Belgium. status: publishe...|$|R
40|$|This is the Final Report of the Next-Generation Energy Efficient Fluorescent Lighting Products program, Department of Energy (DOE). The {{overall goal}} of this three-year program was to develop novel {{phosphors}} to improve the color rendition and efficiency of compact and linear fluorescent lamps. The prime technical approach was the development of quantum-splitting phosphor (QSP) to further increase the efficiency of conventional linear fluorescent lamps {{and the development of}} new high color rendering phosphor blends for compact fluorescent lamps (CFLs) as potential replacements for the energy-hungry and short-lived incandescent lamps in market segments that demand high color rendering light sources. We determined early in the project that the previously developed oxide QSP, SrAl{sub 12 }O{sub 19 }:Pr{sup 3 +}, did not exhibit an quantum efficiency higher than unity under excitation by 185 nm radiation, and we therefore worked to determine the physical reasons for this observation. From our investigations we concluded that the achievement of quantum efficiency exceeding unity in SrAl{sub 12 }O{sub 19 }:Pr{sup 3 +} was not possible due to interaction of the Pr{sup 3 +} 5 d level with the conduction band of the solid. The interaction which gives rise to an additional nonradiative decay path for the excitation energy is responsible for the low quantum efficiency of the phosphor. Our work has {{led to the development of}} a novel spectroscopic method for determining photoionzation threshold of luminescent centers in solids. This has resulted in further <b>quantification</b> of the <b>requirements</b> for host phosphor lattice materials to optimize quantum efficiency. Because of the low quantum efficiency of the QSP, we were unable to demonstrate a linear fluorescent lamp with overall performance exceeding that of existing mercury-based fluorescent lamps. Our work on the high color rendering CFLs has been very successful. We have demonstrated CFLs that satisfies the EnergyStar requirement with color rendering index (CRI) greater than 90; the CRI of current commercial CFLs are in the low 80 s. In this report we summarize the technical work completed under the Program, summarize our findings about the performance limits of the various technologies we investigated, and outline promising paths for future work...|$|R
40|$|The aim of {{this work}} was to analyse the effects of leaf removal on Touriga Nacional berry {{temperature}} and consequent thermal efficiency for anthocyanins biosynthesis. The field experiment was located at Dão Wine Research Station, Nelas, Portugal in an adult vineyard planted with North-South oriented rows, with the red grape variety Touriga Nacional grafted on 110 R rootstock. The vines were trained on a vertical shoot positioning, spur-pruned on a bilateral Royat cordon system and deficit irrigated (50 % ETc). The experimental design was a randomized complete block design with four replications of twelve vines per elemental plot, {{and the following two}} treatments: basal leaf removal (LR) and a control non-defoliated (ND). Berry temperature (Tb) was measured continuously during the second half (3 rd to 19 th September) of the 2009 ripening period using two-junction, fine-wires copper-constantan thermocouples manually inserted into the berries and connected to a data logger. A sample of clusters located in different canopy positions (exposed and internal; facing East and West) of 4 vines per treatment were used. To quantify the effect of Tb on anthocyanins biosynthesis, the berry hourly mean temperatures were converted into normal heat hours (NHH) and accumulated per day (NHHd) and per monitoring period (NHHc). For <b>quantification</b> of thermal <b>requirements</b> for anthocyanins synthesis and accumulation, a minimum of 10 °C, a maximum of 35 °C, and an optimum of 26 °C were used. Meteorological variables were measured at an automatic weather station installed within the experimental plot. For all days of the monitoring period, daily average berry temperature (dTb) of all monitored berries was lower in ND treatment than in LR, being the maximum differences between treatments registered on 11 th September. The highest dTb differences between treatments were registered on the clusters located at {{the west side of the}} canopy on 7 th September while dTb of the clusters located in the centre of the canopy was less affected by leaf removal. The control non-defoliated treatment (ND) presented a significantly higher NHHc than that of LR being the higher differences presented by the clusters located in the west side. The lowest differences in NHHc were obtained in the clusters located in the centre of the canopy. Our results show that the thermal efficiency for berry anthocyanins accumulation was significantly affected by leaf removal and that this effect was dependent of the meteorological conditions, time of the day and berry/cluster location into the vine canopy...|$|R

