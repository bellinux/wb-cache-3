142|1458|Public
2500|$|A {{histogram}} is {{an accurate}} graphical representation {{of the distribution of}} numerical data. It is an estimate of the probability distribution of a continuous variable (<b>quantitative</b> <b>variable)</b> and was first introduced by Karl Pearson. It is a kind of bar graph. To construct a histogram, {{the first step is to}} [...] "bin" [...] the range of values—that is, divide the entire range of values into a series of intervals—and then count how many values fall into each interval. [...] The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent, and are often (but are not required to be) of equal size.|$|E
5000|$|Ordinal {{data can}} be {{considered}} as a <b>quantitative</b> <b>variable.</b> In logistic regression, the equation ...|$|E
5000|$|... #Caption: The binary factor A and the <b>quantitative</b> <b>variable</b> X {{interact}} (are non-additive) when analyzed {{with respect}} to the outcome variable Y.|$|E
30|$|The used {{tests were}} Mann Whitney test (for {{abnormally}} distributed <b>quantitative</b> <b>variables,</b> to compare between two studied groups); Kruskal Wallis test (For abnormally distributed <b>quantitative</b> <b>variables,</b> to compare between {{more than two}} studied groups, and Post Hoc “Dunn’s multiple comparisons test” for pairwise comparisons); and Spearman coefficient (to correlate between two distributed abnormally <b>quantitative</b> <b>variables).</b>|$|R
5000|$|When {{there are}} few {{qualitative}} variables with respect to <b>quantitative</b> <b>variables</b> (one can be reluctant to discretize twenty <b>quantitative</b> <b>variables</b> {{to take into account}} a single qualitative variable).|$|R
30|$|Clinical and {{biological}} <b>quantitative</b> <b>variables</b> were expressed as median and 25 th– 75 th interquartiles. Intergroup comparisons {{were made by}} a Mann–Whitney test for <b>quantitative</b> <b>variables</b> and Chi-square test with Fischer correction for qualitative variables. A p <  0.05 was considered as statistically significant.|$|R
5000|$|Repeated {{measures}} {{analysis of}} variance (rANOVA) is a commonly used statistical approach to repeated measure designs. [...] With such designs, the repeated-measure factor (the qualitative independent variable) is the within-subjects factor, while the dependent <b>quantitative</b> <b>variable</b> on which each participant is measured is the dependent variable.|$|E
50|$|A table {{contains}} {{quantitative data}} organized into rows and columns with categorical labels. It is primarily {{used to look}} up specific values. In the example above, the table might have categorical column labels representing the name (a qualitative variable) and age (a <b>quantitative</b> <b>variable),</b> with each row of data representing one person (the sampled experimental unit or category subdivision).|$|E
5000|$|The {{representation}} of variables is called relationship square. The coordinate of qualitative variable [...] along axis [...] {{is equal to}} squared correlation ratio between the variable [...] and the factor of rank [...] (denoted [...] ). The coordinates of <b>quantitative</b> <b>variable</b> [...] along axis [...] {{is equal to the}} squared correlation coefficient between the variable [...] and the factor of rank [...] (denoted [...] ).|$|E
50|$|A {{generalized}} scatterplot matrix {{offers a}} range of displays of paired combinations of categorical and <b>quantitative</b> <b>variables.</b> A mosaic plot, fluctuation diagram, or faceted bar chart {{may be used to}} display two categorical variables. Other plots are used for one categorical and one <b>quantitative</b> <b>variables.</b>|$|R
30|$|Data was coded, entered to Excel 2010 (Microsoft Corporation, NY, USA). Data was {{analyzed}} using SPSS version 24 (IBM Corp. Released 2016. IBM SPSS Statistics for Windows, Version 24.0. Armonk, NY: IBM Corp.). Number and percentages {{were used to}} summarize qualitative variables while mean and standard deviations were used for <b>quantitative</b> <b>variables.</b> Comparison between groups was done using chi-squares test (qualitative variables) and independent sample t test (<b>quantitative</b> <b>variables).</b> Pearson correlation coefficient (r) {{was used to test}} the correlation between <b>quantitative</b> <b>variables.</b> P value {{less than or equal to}} 0.05 was considered as statistically significant.|$|R
5000|$|... 2.Representations of <b>quantitative</b> <b>variables</b> as in PCA (correlation circle).|$|R
5000|$|To {{illustrate}} how {{qualitative and quantitative}} regressors are included to form ANCOVA models, suppose we consider the same example used in the ANOVA model with one qualitative variable: average annual salary of public school teachers in three geographical regions of Country A. If we include a <b>quantitative</b> <b>variable,</b> State Government expenditure on public schools per pupil, in this regression, we get the following model: ...|$|E
5000|$|Scienceman then defined transformity as,"a <b>quantitative</b> <b>variable</b> {{describing}} the measurable property of {{a form of}} energy, its ability to amplify as feedback, relative to the source energy consumed in its formation, under maximum power conditions. As a <b>quantitative</b> <b>variable</b> analogous to thermodynamic temperature, transformity requires specification of units." [...] (1987, p. 261. My emphasis).In 1996 H.T.Odum defined transformity as,"the emergy of one type required to make a unit of energy of another type. For example, since 3 coal emjoules (cej) of coal and 1 cej of services are required to generate 1 J of electricity, the coal transformity of electricity is 4 cej/J"G.P.Genoni expanded on this definition and maintained that, [...] "the energy input of one kind required to sustain one unit of energy of another kind, is used to quantify hierarchical position" [...] (1997, p. 97). According to Scienceman, the concept of transformity introduces a new basic dimension into physics (1987, p. 261). However there is ambiguity in the dimensional analysis of transformity as Bastianoni et al. (2007) state that transformity is a dimensionless ratio.|$|E
50|$|In one {{published}} {{example of}} an application of binomial regression, the details were as follows. The observed outcome variable was {{whether or not a}} fault occurred in an industrial process. There were two explanatory variables: the first was a simple two-case factor representing whether or not {{a modified version of the}} process was used and the second was an ordinary <b>quantitative</b> <b>variable</b> measuring the purity of the material being supplied for the process.|$|E
30|$|With {{respect to}} the primary outcome measures, the Student t {{test was used to}} compare the <b>quantitative</b> <b>variables</b> that {{followed}} a normal distribution with the dichotomous variables (hybrid vs. cemented). The Kolmogorov–Smirnov test was applied to determine whether <b>quantitative</b> <b>variables</b> were normally distributed. The Pearson correlation test was used to verify the relationship between two <b>quantitative</b> <b>variables.</b> In all cases, 95 % confidence intervals were calculated. For the secondary outcome measures, the chi-square test was used to compare the qualitative variables. The survival study was performed by Kaplan–Meier analysis, with statistical significance assumed at p[*]<[*] 0.05.|$|R
5000|$|To {{describe}} {{the effect of}} categorical or <b>quantitative</b> <b>variables</b> on survival ...|$|R
50|$|The {{representation}} of <b>quantitative</b> <b>variables</b> is constructed as in PCA (correlation circle).|$|R
50|$|The {{managerial}} {{utility function}} includes {{variables such as}} salary, job security, power, status, dominance, prestige and professional excellence of managers. Of these, salary is the only <b>quantitative</b> <b>variable</b> and thus measurable. The other variables are non-pecuniary, which are non-quantifiable.The variables expenditure on staff salary, management slack, discretionary investments can be assigned nominal values. Thus these {{will be used as}} proxy variables to measure the real or unquantifiable concepts like job security, power, status, dominance, prestige and professional excellence of managers, appearing in the managerial utility function.|$|E
5000|$|A {{histogram}} is {{an accurate}} graphical representation {{of the distribution of}} numerical data. It is an estimate of the probability distribution of a continuous variable (<b>quantitative</b> <b>variable)</b> and was first introduced by Karl Pearson. It is a kind of bar graph. To construct a histogram, {{the first step is to}} [...] "bin" [...] the range of values—that is, divide the entire range of values into a series of intervals—and then count how many values fall into each interval. The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent, and are often (but are not required to be) of equal size.|$|E
5000|$|The F-test in one-way {{analysis}} of variance is used to assess whether the expected values of a <b>quantitative</b> <b>variable</b> within several pre-defined groups differ from each other. For example, suppose that a medical trial compares four treatments. The ANOVA F-test can be used to assess whether any of the treatments is on average superior, or inferior, to the others versus the null hypothesis that all four treatments yield the same mean response. This is an example of an [...] "omnibus" [...] test, meaning that a single test is performed to detect any of several possible differences. Alternatively, we could carry out pairwise tests among the treatments (for instance, in the medical trial example with four treatments we could carry out six tests among pairs of treatments). The advantage of the ANOVA F-test is that we do not need to pre-specify which treatments are to be compared, and we do not need to adjust for making multiple comparisons. The disadvantage of the ANOVA F-test is that if we reject the null hypothesis, we do not know which treatments can be said to be significantly different from the others, nor, if the F-test is performed at level α, can we state that the treatment pair with the greatest mean difference is significantly different at level α.|$|E
5000|$|The data include [...] <b>quantitative</b> <b>variables</b> [...] and [...] {{qualitative}} variables [...]|$|R
30|$|Implant {{survival}} probabilities {{were computed}} using Kaplan–Meier analysis, counting revision of any component {{for any reason}} as the terminating event or {{at the end of}} the follow-up period [20]. The ANOVA test was used for comparison of normally distributed <b>quantitative</b> <b>variables</b> and the Kruskal–Wallis test was used for comparison of <b>quantitative</b> <b>variables</b> across the three implant groups when distribution was not normal.|$|R
30|$|All {{data are}} {{presented}} as medians (25 th– 75 th percentiles) for <b>quantitative</b> <b>variables</b> and frequencies (percentage) for qualitative variables. Organ dysfunction was assessed by dichotomizing the LOD score at day 1 (LOD =  0 or LOD >  0). Baseline characteristics were compared between survival and dead patients using Wilcoxon rank-sum test for <b>quantitative</b> <b>variables</b> and Fisher’s exact test for qualitative variable.|$|R
40|$|A new {{stochastic}} {{randomized response}} model is introduced that {{is useful for}} estimating the population mean of a sensitive <b>quantitative</b> <b>variable.</b> The proposed stochastic randomized response model {{is an extension of}} the stochastic randomized response model from a qualitative sensitive variable to a <b>quantitative</b> <b>variable</b> found in Singh (2002). The stochastic nature of a randomized response device helps increase a respondent’s cooperation while collecting information on sensitive variables in a society. The Bar-Lev, Bobovitch, and Boukai (2004) model is shown to be a special case of the proposed model...|$|E
40|$|A common {{statistical}} {{problem is}} {{the assessment of the}} predictive power of a <b>quantitative</b> <b>variable</b> for some dependent variable. A maximally selected rank statistic regarding the <b>quantitative</b> <b>variable</b> provides a test and implicitly an estimate of a cutpoint as a simple classification rule. Restricting the selection to an arbitrary given inner part of the support of the <b>quantitative</b> <b>variable,</b> we show that the asymptotic null distribution of the maximally selected rank statistic is the distribution of the supremum of the absolute value of a standardized Gaussian process on an interval. The asymptotic argument holds also in the case of tied or censored observations. We compare Monte Carlo results with an approximation of the asymptotic distribution under the null hypothesis. In addition, we investigate the behaviour of the test procedure and of the familiar Spearman rank test for independence, under some alternatives. Moreover, we discuss some aspects of the problem of estimating an underlying cutpoint...|$|E
40|$|A {{major problem}} in {{reviewing}} the published results of different epidemiologic studies {{of the relation between}} a <b>quantitative</b> <b>variable</b> and the risk of disease is that the results are presented in many different ways. The {{purpose of this paper is}} to exemplify methods by which results expressed either as risks (or rates) according to quantile groups of the <b>quantitative</b> <b>variable</b> or as results derived from a logistic regression analysis can be reexpressed in a uniform manner, as a mean difference in the <b>quantitative</b> <b>variable</b> between the cases of disease and the other subjects in the study. An important assumption of the methods is that the <b>quantitative</b> <b>variable</b> has an approximately normal distribution, and a way of investigating the appropriateness of this assumption is given. The methods can be applied to both prospective and case-control studies and are exemplified by a number of studies of serum albumin concentrations and mortality. In some applications, these methods can be used as a precursor to formal meta-analysis, for example, when differential control of potential confounding factors is not a problem. At the least, the methods can be useful either in quantitatively reviewing published studies before undertaking new research or in putting the results of a new study into the context of previously published ones. Am J Epidemiol 1996; 144 : 610 - 21. epidemiologic methods; logistic models; meta-analysis; regression analysis; risk; serum albumin Summarizing the results of published studies is a...|$|E
30|$|All {{analyses}} were performed in an intention-to-treat manner. Distribution of <b>quantitative</b> <b>variables</b> was tested using the Shapiro-Wilk test. Normally and nonnormally distributed variables were expressed as mean[*]±[*]SD and median (25 th, 75 th interquartile), respectively. The {{difference between the two}} groups was considered as significant if p[*]<[*] 0.05. McNemar and Wilcoxon tests were used to compare qualitative and <b>quantitative</b> <b>variables</b> between the two 24 -h periods, respectively.|$|R
30|$|The {{results of}} the {{descriptive}} analysis were expressed as numbers and percentages for qualitative variables and as {{mean and standard deviation}} for <b>quantitative</b> <b>variables.</b> Two-tailed Fisher’s exact test for <b>quantitative</b> <b>variables</b> and Chi-square test for qualitative variable were used. Risk of infection was modeled using a multivariate logistical regression analysis. A stepwise selection was used, and the final model was adjusted on all variables associated with a p value below 0.2.|$|R
30|$|Several {{descriptive}} and regression analysis {{were conducted on}} the data. Data were collected and consolidated by the principal investigator and trained nursing educators. A statistician was involved as a co-investigator and assisted with data handling and analysis. Univariate analysis was undertaken to investigate participants’ knowledge and practice skills scores. Statistical comparison for qualitative and <b>quantitative</b> <b>variables</b> was carried out using analysis of variance for <b>quantitative</b> <b>variables.</b> Multivariate {{analysis was used to}} control for interaction effects.|$|R
40|$|Abstract: In {{the works}} [1, 2] we {{proposed}} an approach of forming a consensus of experts ’ statements {{for the case}} of forecasting of qualitative and <b>quantitative</b> <b>variable.</b> In this paper, we present a method of aggregating sets of individual statements into a collective one for the general case of forecasting of multidimensional heterogeneous variable...|$|E
40|$|In many simple designs, {{observed}} frequencies in subclasses {{defined by a}} qualitative variable are compared to the frequencies expected {{on the basis of}} population proportions, design parameters or models. Often there is a <b>quantitative</b> <b>variable</b> which may be affected {{in the same way as}} the frequencies. Its differences among the groups may also be analyzed. A simple test is described that combines the effects on the frequencies and on the <b>quantitative</b> <b>variable</b> based on comparing the sums of the values for the quantitative value within each group to the random expectation. The sampling variance of the difference is derived and is shown to combine the qualitative and quantitative aspects in a logical way. A version of the test based on enumeration of the possible values of the sum is described, an example is analyzed, factors affecting the test 2 ̆ 7 s power are discussed and extensions are suggested...|$|E
30|$|The Statistical Package for Social Sciences (SPSS) {{was used}} (version 13). Considering the {{frequencies}} obtained for each <b>quantitative</b> <b>variable,</b> Shapiro-Wilk normality tests were performed and all data showed p[*]≤[*] 0.002, therefore non-parametric tests were {{applied in the}} data analysis, including Spearman’s correlations, the Mann Whitney U test, and the Kruskal-Wallis test. Results in which p[*]≤[*] 0.05 were considered statistically significant.|$|E
50|$|In PCA, it {{is common}} {{that we want to}} {{introduce}} qualitative variables as supplementary elements. For example, many <b>quantitative</b> <b>variables</b> have been measured on plants. For these plants, some qualitative variables are available as, for example, the species to which the plant belongs. These data were subjected to PCA for <b>quantitative</b> <b>variables.</b> When analyzing the results, it is natural to connect the principal components to the qualitative variable species.For this, the following results are produced.|$|R
5000|$|... 7. Representations {{of factors}} of {{separate}} {{analyses of the}} different groups. These factors are represented as supplementary <b>quantitative</b> <b>variables</b> (correlation circle).|$|R
30|$|All {{statistical}} {{analyses were performed}} using JMP Pro software (version 10.0. 2; SAS, Cary, NC). <b>Quantitative</b> <b>variables</b> were expressed as mean ± standard deviation.|$|R
