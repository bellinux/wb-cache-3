15|3403|Public
5000|$|For each <b>quantization</b> <b>vector</b> {{centroid}} , let [...] {{denote the}} distance of [...] and ...|$|E
5000|$|Move {{the nearest}} <b>quantization</b> <b>vector</b> {{centroid}} towards this sample point, {{by a small}} fraction of the distance ...|$|E
50|$|Image {{analysis}} involves complex computer algorithms which {{identify and}} characterize cellular color, shape, {{and quantity of}} the tissue sample using image pattern recognition technology based on vector <b>quantization.</b> <b>Vector</b> representations of objects in the image, as opposed to bitmap representations, have superior zoom-in ability. Once the sample image has been acquired and resident in the computer's random access memory as a large array of 0's and 1's, a programmer knowledgeable in cellular architecture can develop deterministic algorithms applied to the entire memory space to detect cell patterns from previously defined cellular structures and formations known to be significant.|$|E
40|$|Abstract—Vector {{quantization}} is {{a powerful}} tool for speech coding applications. This paper deals with LPC Coding of speech signals which uses a new technique called Multi Switched Split <b>Vector</b> <b>Quantization,</b> This is a hybrid of two product code <b>vector</b> <b>quantization</b> techniques namely the Multi stage <b>vector</b> <b>quantization</b> technique, and Switched split <b>vector</b> <b>quantization</b> technique,. Multi Switched Split <b>Vector</b> <b>Quantization</b> technique quantizes the linear predictive coefficients in terms of line spectral frequencies. From results it is proved that Multi Switched Split <b>Vector</b> <b>Quantization</b> provides better trade off between bitrate and spectral distortion performance, computational complexity and memory requirements when compared to Switched Split <b>Vector</b> <b>Quantization,</b> Multi stage <b>vector</b> <b>quantization,</b> and Split <b>Vector</b> <b>Quantization</b> techniques. By employing the switching technique at each stage of the vector quantizer the spectral distortion, computational complexity and memory requirements were greatly reduced. Spectral distortion was measured in dB, Computational complexity was measured in floating point operations (flops), and memory requirements was measured in (floats). Keywords—Unconstrained <b>vector</b> <b>quantization,</b> Linear predictive Coding, Split <b>vector</b> <b>quantization,</b> Multi stage <b>vector</b> <b>quantization,</b> Switched Split <b>vector</b> <b>quantization,</b> Line Spectral Frequencies. I...|$|R
40|$|<b>Vector</b> <b>quantization</b> is a {{powerful}} tool for speech coding applications. This paper deals with LPC Coding of speech signals which uses a new technique called Multi Switched Split <b>Vector</b> <b>Quantization,</b> This is a hybrid of two product code <b>vector</b> <b>quantization</b> techniques namely the Multi stage <b>vector</b> <b>quantization</b> technique, and Switched split <b>vector</b> <b>quantization</b> technique,. Multi Switched Split <b>Vector</b> <b>Quantization</b> technique quantizes the linear predictive coefficients in terms of line spectral frequencies. From results it is proved that Multi Switched Split <b>Vector</b> <b>Quantization</b> provides better trade off between bitrate and spectral distortion performance, computational complexity and memory requirements when compared to Switched Split <b>Vector</b> <b>Quantization,</b> Multi stage <b>vector</b> <b>quantization,</b> and Split <b>Vector</b> <b>Quantization</b> techniques. By employing the switching technique at each stage of the vector quantizer the spectral distortion, computational complexity and memory requirements were greatly reduced. Spectral distortion was measured in dB, Computational complexity was measured in floating point operations (flops), and memory requirements was measured in (floats) ...|$|R
40|$|Abstract—Vector {{quantization}} is {{a powerful}} tool for speech coding applications. This paper deals with LPC Coding of speech signals which uses a new technique called Multi Switched Split <b>Vector</b> <b>Quantization</b> (MSSVQ), which is a hybrid of Multi, switched, split <b>vector</b> <b>quantization</b> techniques. The spectral distortion performance, computational complexity, and memory requirements of MSSVQ are compared to split <b>vector</b> <b>quantization</b> (SVQ), multi stage vector quantization(MSVQ) and switched split <b>vector</b> <b>quantization</b> (SSVQ) techniques. It has been proved from results that MSSVQ has better spectral distortion performance, lower computational complexity and lower memory requirements when compared to all the above mentioned product code <b>vector</b> <b>quantization</b> techniques. Computational complexity is measured in floating point operations (flops), and memory requirements is measured in (floats). Keywords—Linear predictive Coding, Multi stage <b>vector</b> <b>quantization,</b> Switched Split <b>vector</b> <b>quantization,</b> Split <b>vector</b> <b>quantization,</b> Line Spectral Frequencies (LSF) ...|$|R
40|$|Abstract—For vector {{quantization}} (VQ) of speech line spectrum frequency (LSF) parameters, we experimentally determine a mapping function between the {{mean square error}} (MSE) measure and the perceptually motivated average spectral distortion (SD) measure. Using the mapping function, we estimate the minimum bits/vector required for transparent quantization of telephone-band and wide-band speech LSF parameters, respectively, as 22 bits/vector and 36 bits/vector, where the distribution of LSF vector is modeled as a Gaussian mixture model (GMM). Index Terms—Gaussian mixture model, line spectrum frequency (LSF) <b>quantization,</b> <b>vector</b> quantization. I...|$|E
3000|$|RC scheme {{where there}} are two {{feedback}} modes. In one mode which achieves higher average sum rate, the SINRs of all codewords are {{sent back to the}} base station, and in the other mode only the largest SINR and the index of its corresponding codeword are sent back to the base station. In ZF-BF with CVQ each user sends back the index of a selected <b>quantization</b> <b>vector</b> along with its corresponding SINR lower bound [18, 37]. In the transmission scheme based on spatial multiplexing at the base station with linear receiver processing at each user terminal, each user sends back [...]...|$|E
3000|$|After quantization, MS i feeds back to {{the system}} the index k in binary form which {{corresponds}} to the <b>quantization</b> <b>vector</b> that best describes its channel direction. Therefore this piece of information is defined as Channel Direction Information (CDI). The more the feedback bits are, the larger the quantization codebook is, {{which leads to a}} better approximation of the MS's channel direction. Apart from CDI, the scheduling entity needs some information regarding the channel quality of each user {{in order to be able}} to make user selection decisions; this is defined as Channel Quality Information (CQI). In this paper we consider the unquantized channel norm [...]...|$|E
40|$|Voice banking is an {{excellent}} telephone banking service by which a user can access his account for any service {{at any time of}} a day, in a year. The speech techniques involved in voice banking are speech coding and speech recognition. This paper investigates the performance of a speech recognizer for a coded output at 20 bits/frame obtained by using various <b>vector</b> <b>quantization</b> techniques namely Split <b>Vector</b> <b>Quantization,</b> Multi Stage <b>Vector</b> <b>Quantization,</b> Split-Multi Stage <b>Vector</b> <b>Quantization,</b> Switched Split <b>Vector</b> <b>Quantization</b> using Hard decision scheme, Switched Multi Stage <b>Vector</b> <b>Quantization</b> using Soft decision scheme and Multi Switched Split <b>Vector</b> <b>Quantization</b> using Hard decision scheme techniques. The speech recognition technique used for recognition of the coded speech signal is the Hidden Markov Model technique and the speech enhancement technique used for enhancing the coded speech signal is the Spectral Subtraction technique. The performance of <b>vector</b> <b>quantization</b> is measured in terms of spectral distortion in decibels, computational complexity in Kflops/frame, and memory requirements in floats. The performance of the speech recognizer for coded outputs at 20 bits/frame has been examined and it is found that the speech recognizer has better percentage probability of recognition for the coded output obtained using Multi Switched Split <b>Vector</b> <b>Quantization</b> using Hard decision scheme. It is also found that the probability of recognition for various coding techniques has been varied from 80 % to 100 %...|$|R
40|$|In {{this paper}} we improve and extend our {{previous}} results in finite- state variable rate tree-structured <b>vector</b> <b>quantization</b> (FSVRTSVQ) 1 by developing two new algorithms {{to increase its}} coding efficiency. The two new algorithms are derived to utilize the inter-codeword dependency and the tree structure property of the codebook so as to achieve bit rate reduction. The evaluation of both algorithms on various synthesis and real sources has shown {{that as many as}} 32. 3 % of bit rate savings has been obtained over the pruned tree-structured <b>vector</b> <b>quantization</b> (PTSVQ) 3. Keywords: source compression, <b>vector</b> <b>quantization,</b> tree-structured <b>vector</b> <b>quantization,</b> finite state <b>vector</b> <b>quantization</b> 1 INTRODUCTION In <b>vector</b> <b>quantization</b> (VQ) of sources with memory, such as video and audio signals, there will exist correlation in the quantizer outputs. On the other hand, the structural design of the tree-structured codebooks determines that highly correlated source vectors will be encoded either [...] ...|$|R
40|$|Abstract—In {{this paper}} {{multistage}} trellis-coded <b>vector</b> <b>quantization</b> (MS-TCVQ) is {{developed as a}} constrained trellis sourcecoding technique. The performance of the two-stage TCVQ is studied for Gaussian sources. Issues of stage-by-stage design, output alphabet selection, and complexity are addressed with emphasis on selecting and partitioning the stage codebooks. For a given rate, MS-TCVQ achieves low encoding and storage complexity compared to TCVQ, and comparisons with samedimensional multistage <b>vector</b> <b>quantization</b> indicate a 0. 5 – 3 -dB improvement in signal-to-quantization-noise ratio. Index Terms — Data compression, residual quantizers, trelliscoded <b>vector</b> <b>quantization</b> (TCVQ), <b>vector</b> <b>quantization...</b>|$|R
40|$|An {{analysis}} of the rate-distorted performance of an entropy- constrained block transform quantization scheme operating on discrete-time stationary autoregressive process is presented. Uniform-threshold quantization is employed to quantize the transform coefficients. An algorithm for optimum stepsize (or, equivalently, entropy) assignment among the quantizers is developed. A simple asymptotic formula indicating the high rate performance of the block transform quantization schemes is presented. Finally, specific results determining the rate- distortion performance of the entropy-constrained block transform quantization scheme operating upon first-order Gauss-Markov and Laplace-Markov sources are presented and appropriate comparisons with the Haung and Schulthesis block transform <b>quantization,</b> <b>vector</b> quantization and predictive encoding are rendered...|$|E
40|$|This paper {{describes}} {{a method for}} image compression using a fusion technique: combining wavelet transform and curvelet transform. Both the transforms when used individually shows some disadvantages. Wavelets though optimal for point singularities have limitations with directional properties. Similarly curvelets are challenged with small features. By combining both the transforms, the number of bits used to represent the image is reduced. The coefficients obtained after applying fusion technique is then selected for quantization and encoding. Quantization chosen is vector quantization as it saves time compared to scalar <b>quantization.</b> <b>Vector</b> quantization, mapping of image pixel intensity vectors into binary vectors. Arithmetic encoding technique is employed. This method is effective to remove redundancy in encoding of data. This technique works fairly well for grayscale as well as colour image...|$|E
40|$|Multimedia {{retrieval}} {{systems are}} very important today with millions of content creators {{all over the world}} generating huge multimedia archives. Recent developments allows for content based image and video retrieval. These methods are often quite slow, especially if applied on a library of millions of media items. In this research a novel image retrieval method is proposed, which utilizes spatial metadata on images. By finding clusters of images based on their geographic location, the spatial metadata, and combining this information with existing content- based image retrieval algorithms, the proposed method enables efficient presentation of high quality image retrieval results to system users. Clustering methods considered include Vector <b>Quantization,</b> <b>Vector</b> Quantization LBG and DBSCAN. Clustering was performed on three different similarity measures; spatial metadata, histogram similarity or texture similarity. For histogram similarity there are many different distance metrics to use when comparing histograms. Euclidean, Quadratic Form and Earth Mover’s Distance was studied. As well as three different color spaces; RGB, HSV and CIE Lab. ...|$|E
40|$|Abstract—This paper evaluates a {{segmentation}} {{technique for}} magnetic resonance (MR) {{images of the}} brain based on fuzzy algorithms for learning <b>vector</b> <b>quantization</b> (FALVQ). These algorithms perform <b>vector</b> <b>quantization</b> by updating all prototypes of a competitive network through an unsupervised learning process. Segmentation of MR images is formulated as an unsupervised <b>vector</b> <b>quantization</b> process, where the local values of different relaxation parameters form the feature vectors which are represented by a relatively small set of prototypes. The experiments evaluate a variety of FALVQ algorithms {{in terms of their}} ability to identify different tissues and discriminate between normal tissues and abnormalities. Index Terms—Fuzzy algorithms for learning <b>vector</b> <b>quantization,</b> learning <b>vector</b> <b>quantization,</b> magnetic resonance imaging, segmentation. I...|$|R
3000|$|... rmVQ_fk_nps {{a variant}} of VQ_fk_nps is {{obtained}} by using random vectors in step 1.2 of the initialization phase as originally defined in <b>vector</b> <b>quantization</b> and using an entire update on the distance matrix after the cluster assignment of all the data vectors in step 2.1 of the cluster update phase as originally proposed in kernel-based <b>vector</b> <b>quantization</b> in [10]. We include this variant in order to check if our improvement on both <b>vector</b> <b>quantization</b> and kernel-based <b>vector</b> <b>quantization</b> is empirically appropriate.|$|R
50|$|Interpreted as <b>vector</b> <b>quantization,</b> {{three-dimensional}} <b>vectors</b> with components red, green, {{and blue}} are quantized using a forward adaptive codebook with between 1 and 8 entries.|$|R
40|$|Abstract—Limited {{feedback}} is a paradigm for the feedback of channel state information in wireless systems. In multiple antenna wireless systems, limited feedback usually entails quantizing a source that {{lives on the}} Grassmann manifold. Most work on limited feedback beamforming considered single-shot quantiza-tion. In wireless systems, however, the channel is temporally correlated, {{which can be used}} to reduce feedback require-ments. Unfortunately, conventional predictive quantization does not incorporate the non-Euclidean structure of the Grassmann manifold. In this paper, we propose a Grassmannian predictive coding algorithm where the differential geometric structure of the Grassmann manifold is used to formulate a predictive vector quantization encoder and decoder. We analyze the quantization error and derive bounds on the distortion attained by the proposed algorithm. We apply the algorithm to a multiuser multiple-input multiple-output wireless system and show that it improves the achievable sum rate as the temporal correlation of the channel increases. Index Terms—Prediction methods, correlation, feedback com-munication, MIMO systems, <b>quantization,</b> <b>vector</b> quantization. I...|$|E
40|$|This paper {{presents}} an ECG compressor based on optimized quantization of discrete cosine transform (DCT) coefficients. The ECG is partitioned in blocks and each DCT block is quantized using a <b>quantization</b> <b>vector</b> and a threshold vector. These vectors are defined for each signal {{so that the}} entropy is minimized for a target distortion or, alternatively, the distortion is minimized for a target entropy. After quantization, the coefficients are lossless encoded. In order to assess {{the performance of the}} proposed compressor, the first 2 minutes of all records of the MITBIH Arrhythmia Database were compressed at four different distortion levels, measured by the percent root-meansquare difference (PRD), and the compression ratios (CR) were computed. We present traces of original and decompressed signals. The method achieves good CR values with excellent reconstruction quality. An average CR of 9. 3 : 1 is achieved for PRD= 2. 5 %. Experiments with ECG signals used in results from the literature showed that the proposed method compares favorably with various classical and state-of-the-art ECG compressors...|$|E
40|$|Abstract—Data {{compression}} {{techniques for}} electrocardiographic and electroencephalographic exams {{have been widely}} {{reported in the literature}} over the last decades; but, there are no papers offering a unique solution for all biological signals typically present in polysomnographic records. Aiming to fill this gap, the present work proposes a method of lossy compression for polysomnographic signals based on optimal quantization of the coefficients obtained from the discrete cosine transform. The potentially grave distortions generated by the information loss are controlled by a compression parameter that may be config-ured to reach the desired Normalized Percent Root-mean-square Difference generating the optimum <b>quantization</b> <b>vector</b> with a minimization of the Lagrange parameter. The quantized signal is sent to a prediction by partial matching compressor, which works as the entropy coder of this compression strategy. The method was tested using the signals in the Polysomnographic database created by the Massachusetts Institute of Technology and Boston’s Beth Israel Hospital, achieving compression ratios between 2. 16 : 1 and 67. 48 : 1 with distortion values between 1. 0 % and 4. 0 %. Keywords–data compression; telemedicine; polysomnographic signals; lossy compression; discrete cosine transform. I...|$|E
40|$|A <b>vector</b> <b>quantization</b> fast search {{algorithm}} using hyperplane based k-dimensional multi-node {{search tree}} is presented. Misclassification problem associated with,hyperplane decision is eliminated by a multi-level back-tracing algorithm. The <b>vector</b> <b>quantization</b> complexity is further lowered by a novel relative distance quantization rule. Triangular inequality {{is applied to}} lower bound the search distance, thus eliminated all the sub-tree in the k-dimensional search tree during back-tracing. <b>Vector</b> <b>quantization</b> image coding results are presented which showed the proposed <b>vector</b> <b>quantization</b> algorithm outperform other <b>vector</b> <b>quantization</b> algorithms in literature both in PSNR and computation time...|$|R
40|$|<b>Vector</b> <b>quantization</b> is a {{powerful}} tool for speech coding applications. This paper deals with LPC Coding of speech signals which uses a new technique called Multi Switched Split <b>Vector</b> <b>Quantization</b> (MSSVQ), which is a hybrid of Multi, switched, split <b>vector</b> <b>quantization</b> techniques. The spectral distortion performance, computational complexity, and memory requirements of MSSVQ are compared to split <b>vector</b> <b>quantization</b> (SVQ), multi stage vector quantization(MSVQ) and switched split <b>vector</b> <b>quantization</b> (SSVQ) techniques. It has been proved from results that MSSVQ has better spectral distortion performance, lower computational complexity and lower memory requirements when compared to all the above mentioned product code <b>vector</b> <b>quantization</b> techniques. Computational complexity is measured in floating point operations (flops), and memory requirements is measured in (floats) ...|$|R
50|$|Twin <b>vector</b> <b>quantization</b> (VQF) {{is part of}} the MPEG-4 {{standard}} {{dealing with}} time domain weighted interleaved <b>vector</b> <b>quantization.</b>|$|R
40|$|Quantization, {{the process}} of {{approximating}} continuous-amplitude signals by digital (discreteamplitude) signals, is {{an important aspect of}} data compression or coding, the field concerned with the reduction of the number of bits necessary to transmit or store analog data, subject to a distortion or fidelity criterion. The independent quantization of each signal value or parameter is termed scalar quantization, while the joint quantization of a block of parameters is termed block or vector quantization. This tutorial review presents the basic concepts employed in vector quantization and gives a realistic assessment of its benefits and costs when compared to scalar <b>quantization.</b> <b>Vector</b> quantization is presented as a process of redundancy removal that makes effective use of four interrelated properties of vector parameters: linear dependency (correlation), nonlinear dependency, shape of the probability density function (pdf), and vector dimensionality itself. In contrast, scalar quantization can utilize effectively only linear dependency and pdf shape. The basic concepts are illustrated by means of simple examples and the theoretical limits of vector quantizer performance are reviewed, based on results from rate-distortion theory. Practical issues relating to quantizer design, implementa tion, and performance in actual applications are explored. While many of the methods presented are quite general and can be used for the coding of arbitrary signals, this paper focuses primarily on the coding of speech signals and parameters. I...|$|E
40|$|In this paper, {{we study}} {{feedback}} optimization problems that maximize the users' signal to interference plus noise ratio (SINR) in a two-cell MIMO broadcast channel. Assuming the users learn their direct and interfering channels perfectly, they can feed back {{this information to}} the base stations (BSs) over the uplink channels. The BSs then use the channel information to design their transmission scheme. Two types of feedback are considered: analog and digital. In the analog feedback case, the users send their unquantized and uncoded CSI over the uplink channels. In this context, given a user's fixed transmit power, we investigate how he/she should optimally allocate it to feed back the direct and interfering (or cross) CSI for two types of base station cooperation schemes, namely, Multi-Cell Processing (MCP) and Coordinated Beamforming (CBf). In the digital feedback case, the direct and cross link channel vectors of each user are quantized separately, each using RVQ, with different size codebooks. The users then send the index of the <b>quantization</b> <b>vector</b> in the corresponding codebook to the BSs. Similar to the feedback optimization problem in the analog feedback, we investigate the optimal bit partitioning for the direct and interfering link for both types of cooperation. We focus on regularized channel inversion precoding structures and perform our analysis in the large system limit in which the number of users per cell (K) {{and the number of}} antennas per BS (N) tend to infinity with their ratio β=K/N held fixed...|$|E
40|$|Abstract — In this paper, {{we study}} {{feedback}} optimization problems that maximize the users ’ signal to interference plus noise ratio (SINR) in a two-cell multiple-input multiple-output broadcast channel. Assuming the users learn their direct and interfering channels perfectly, they can feed back {{this information to}} the base stations (BSs) over the uplink channels. The BSs then use the channel information to design their transmission scheme. Two types of feedback are considered: 1) analog and 2) digital. In the analog feedback case, the users send their unquantized and uncoded channel state information (CSI) over the uplink channels. In this context, given a user’s fixed transmit power, we investigate how he/she should optimally allocate it to feed back the direct and interfering (or cross) CSI for two types of BS cooperation schemes, namely, multicell processing (MCP) and coordinated beamforming. In the digital feedback case, the direct and cross link channel vectors of each user are quantized separately, each using the random vector quantization scheme, with different size codebooks. The users then send the index of the <b>quantization</b> <b>vector</b> in the corresponding codebook to the BSs. Similar to the feedback optimization problem for the analog feedback, we investigate the optimal bit partitioning for the direct and interfering link for both types of cooperation. We focus on regularized channel inversion precoding structures and perform our analysis in the large system limit in which the number of users per cell (K) {{and the number of}} antennas per BS (N) tend to infinity with their ratio β = (K/N) held fixed. We show that for both types of cooperation, for some values of interfering channel gain, usually at low values, no cooperation between the BSs is preferred. This is because, for these values of cross channel gain, the channel estimates for the cross link are not accurate enough for their knowledge to contribute to improving the SINR and there is no benefit in doing BS cooperation under that condition. We also show that for the MCP scheme, unlike in the perfect CSI case, the SINR improves only when the interfering channel gain is above a certain threshold...|$|E
40|$|Abstract — Multi-site {{cooperative}} {{transmission is}} gaining industrial support in latest standard development. One key criteria to sucessfully implement a multi-site cooperative transmission {{system is the}} need to quantize an expanded set of channels. To maintain quantization precision, codebook size needs to scale roughly exponentially with the number of total antennas and renders quantizing the expanded channel set a computationally demanding task. In this paper, we identify a connection between noncoherent communication and the phase invariant property of beamforming vectors. We leverage techniques from the noncoherent communication literature to design an efficient beamforming <b>vector</b> <b>quantization</b> scheme. Our proposed scheme has a quantization complexity that grows linearly with number of antennas. Simulation results indicate this method has slightly more quantization distortion than RVQ while being significantly more efficient. Index Terms — beamforming <b>vector</b> <b>quantization,</b> efficient <b>vector</b> <b>quantization,</b> trellis coded <b>vector</b> <b>quantization</b> I...|$|R
50|$|Interpreted as <b>vector</b> <b>quantization,</b> a {{three-dimensional}} <b>vector</b> with the components red, green, and blue is quantized using a codebook with four entries.|$|R
50|$|In {{computer}} science, learning <b>vector</b> <b>quantization</b> (LVQ), is a prototype-based {{supervised classification}} algorithm. LVQ is the supervised counterpart of <b>vector</b> <b>quantization</b> systems.|$|R
40|$|Supervised and {{unsupervised}} <b>vector</b> <b>quantization</b> {{methods for}} classification and clustering traditionally use dissimilarities, frequently taken as Euclidean distances. In {{this article we}} investigate the applicability of divergences instead. We deduce the mathematical fundamentals for its utilization in derivative based <b>vector</b> <b>quantization</b> algorithms. It bears on the generalized derivatives known as Fréchet-derivatives. We exemplary show the application of this methodology for widely applied su-pervised and unsupervised <b>vector</b> <b>quantization</b> schemes including self-organizing maps, neural gas, and learning <b>vector</b> <b>quantization.</b> Further we show principles for hyperparameter optimization for parametrized divergences {{in the case of}} supervised <b>vector</b> <b>quantization</b> to achieve an improved classification accuracy. Machine Learning Reports,Research group on Computational Intelligence...|$|R
50|$|In data compression, twin <b>vector</b> <b>{{quantization}}</b> {{is related}} to <b>vector</b> <b>quantization,</b> but {{the speed of the}} quantization is doubled by the secondary vector analyzer.|$|R
40|$|<b>Vector</b> <b>quantization</b> is an {{essential}} tool for tasks involving large scale data, for example, large scale similarity search, which is crucial for content-based information retrieval and analysis. In this paper, we propose a novel <b>vector</b> <b>quantization</b> framework that iteratively minimizes quantization error. First, we provide a detailed review on a relevant <b>vector</b> <b>quantization</b> method named residual <b>vector</b> <b>quantization</b> (RVQ). Next, we propose generalized residual <b>vector</b> <b>quantization</b> (GRVQ) to further improve over RVQ. Many <b>vector</b> <b>quantization</b> methods {{can be viewed as}} the special cases of our proposed framework. We evaluate GRVQ on several large scale benchmark datasets for large scale search, classification and object retrieval. We compared GRVQ with existing methods in detail. Extensive experiments demonstrate our GRVQ framework substantially outperforms existing methods in term of quantization accuracy and computation efficiency. Comment: published on International Conference on Multimedia and Expo 201...|$|R
40|$|In this paper, we {{introduce}} a soft <b>vector</b> <b>quantization</b> scheme with inverse power-function distribution, and analytically derive an upper bound {{of the resulting}} quantization noise energy in comparison to that of typical (hard-deciding) <b>vector</b> <b>quantization.</b> We also discuss the positive impact {{of this kind of}} soft <b>vector</b> <b>quantization</b> on the performance of machine-learning systems that include one or more <b>vector</b> <b>quantization</b> modules. Moreover, we provide experimental evidence on the advantage of avoiding over-fitting and boosting the robustness of such systems in the presence of considerable parasitic variance; e. g. noise, in the runtime inputs. The experiments have been conducted with two versions of one of the best reported discrete HMM-based Arabic OCR systems; one version deploying hard <b>vector</b> <b>quantization</b> and the other deploying our herein presented soft <b>vector</b> <b>quantization.</b> Test samples of real-life scanned pages are used to challenge both versions; hence the recognition error margins are compare...|$|R
40|$|Abstract-In this paper, we {{introduce}} a soft <b>vector</b> <b>quantization</b> scheme with inverse power-function distribution, and analytically derive an upper bound {{of the resulting}} quantization noise energy in comparison to that of typical (hard-deciding) <b>vector</b> <b>quantization.</b> We also discuss the positive impact {{of this kind of}} soft <b>vector</b> <b>quantization</b> on the performance of machine-learning systems that include one or more <b>vector</b> <b>quantization</b> modules. Moreover, we provide experimental evidence on the advantage of avoiding over-fitting and boosting the robustness of such systems in the presence of considerable parasitic variance; e. g. noise, in the runtime inputs. The experiments have been conducted with two versions of one of the best reported discrete HMM-based Arabic OCR systems; one version deploying hard <b>vector</b> <b>quantization</b> and the other deploying our herei...|$|R
40|$|We propose {{relevance}} {{learning for}} unsupervised online <b>vector</b> <b>quantization</b> algorithm based on stochastic gradient descent learning {{according to the}} given <b>vector</b> <b>quantization</b> cost function. We consider several widely used models including the neural gas algorithm, the Heskes variant of self-organizing maps and the fuzzy c-means. We apply the relevance learning scheme for divergence based similarity measures between prototypes and data vectors in the <b>vector</b> <b>quantization</b> schemes...|$|R
