3|1805|Public
5000|$|Context-adaptive variable-length coding (CAVLC), {{which is}} a lower-complexity {{alternative}} to CABAC for the coding of <b>quantized</b> <b>transform</b> <b>coefficient</b> values. Although lower complexity than CABAC, CAVLC is more elaborate and more efficient than the methods typically used to code coefficients in other prior designs.|$|E
40|$|Context-adaptive binary {{arithmetic}} coding (CABAC) {{is one of}} the most time-consuming modules in H. 264 /AVC decoder. A potential way to accelerate CABAC is by parallelization. However, the context modeling process for level information in CABAC is highly serial in nature and can not be parallelized in the coefficient level. In order to improve the throughput of CABAC, in this paper we present a parallel context model for level information. The key feature of the model is to use the total number of the significant coefficients and the scanned position of the current significant coefficient in the <b>quantized</b> <b>transform</b> <b>coefficient</b> block as the context information. Since the context information is independent of the previously decoded significant coefficients, parallel decoding in coefficient level is achieved. In experiments, the proposed context model achieves the similar compression efficiency as the CABAC. © 2011 IEEE...|$|E
40|$|Transform coding is a {{technique}} used worldwide for image coding, and JPEG {{has become the most}} common tool for image compression. In a JPEG decoder, the <b>quantized</b> <b>transform</b> <b>coefficient</b> blocks are usually processed using the inverse discrete cosine transform (DCT) in order to reconstruct an approximation of the original image. The direct and inverse DCT pair can be arranged {{in the form of a}} perfect reconstruction filter bank, and it can be shown that, in the presence of quantization of the transform coefficients, the perfect reconstruction synthesis is not the best choice. In this paper, we propose a procedure for the design of separable 2 -D synthesis filters that minimize the reconstruction error power for transform coders. The procedure is used to design a family of filters which are used in the decoder instead of the inverse DCT. The appropriate reconstruction filters are selected on the basis of the standard quantization information provided in the JPEG bit stream, We show that the proposed decoding method gives some gain with respect to the usual decoder in most cases, Moreover, it only makes use of the standard information provided by a SPEG bit stream...|$|E
40|$|To {{acquire the}} optimal coding mode of each macroblock, the H. 264 /AVC encoder exhaustively calculates the {{rate-distortion}} cost for all available modes and chooses the minimum one {{as the best}} mode. Therefore, the mode decision process is very computationally demanding. To reduce the computation complexity of the rate-distortion cost, in this paper, we propose a novel rate estimation model for the mode decision in H. 264 /AVC. By modeling the <b>transform</b> <b>coefficients</b> with Generalized Gaussian distributions (GGD), a direct relationship between the magnitude and the information bits of the <b>quantized</b> <b>transform</b> <b>coefficients</b> is deduced. Based on this deduction, the weighted sum of <b>quantized</b> <b>transform</b> <b>coefficients</b> is proposed as an efficient bit-rate estimator of the residual blocks. Extensive experiments show that the proposed algorithm can save up to 30 % of total encoding time with ignorable degradation in coding performance for both inter- and intra-mode decision...|$|R
40|$|Abstract [...] The paper {{presents}} a robust rate control algorithm for JPEG compression, mainly used in Digital Cameras. The overall control {{is based on}} statistical properties of JPEG compressed images, {{the relationship between the}} coding bit rate and the percentage of zeros among the <b>quantized</b> <b>transform</b> <b>coefficients.</b> It allows obtaining a good trade off between resources and precision. I...|$|R
3000|$|... blocks, {{where the}} motion {{characteristics}} of the block can be effectively predicted from the motion of its neighboring blocks, and the <b>quantized</b> <b>transform</b> <b>coefficients</b> of the block are all zeros. When a block is skipped, the <b>transformed</b> <b>coefficients</b> and the motion data are not transmitted, since {{the motion of the}} block is equivalent to the median of the motion vectors of the surrounding blocks. This median is known as the predicted motion vector ([...] [...]...|$|R
40|$|This paper {{describes}} an algorithm {{that can be}} used to determine the image quality of video sequences coded according to the MPEG- 4 part 10 standard (H. 264 /AVC). The PSNR is used as an objective gauge of image quality, based on the statistics of the <b>quantized</b> <b>transform</b> <b>coefficients.</b> The algorithm needs no information beyond the H. 264 /AVC data stream, a useful advantage where no reference signal is available...|$|R
40|$|International audienceIn this paper, {{we present}} a novel {{approach}} for active finger-printing of {{state of the art}} video codec H. 264 /AVC. Tardos probabilistic fingerprinting code is embedded in H. 264 /AVC video signals using spread spectrum watermarking technique. Different linear and non-linear collusion attacks have been performed in the pixel domain to show the robustness of the proposed approach. The embedding has been performed in the non-zero <b>quantized</b> <b>transformed</b> <b>coefficients</b> (QTCs) while taking into account the reconstruction loop...|$|R
40|$|In this paper, {{we present}} a novel {{approach}} for active finger-printing of {{state of the art}} video codec H. 264 /AVC. Tardos probabilistic fingerprinting code is embedded in H. 264 /AVC video signals using spread spectrum watermarking technique. Different linear and non-linear collusion attacks have been performed in the pixel domain to show the robustness of the proposed approach. The embedding has been performed in the non-zero <b>quantized</b> <b>transformed</b> <b>coefficients</b> (QTCs) while taking into account the reconstruction loop. Index Terms — Tardos fingerprinting code, active video fingerprinting, H. 264 /AVC, spread spectrum watermarking. 1...|$|R
40|$|In hybrid video coding, the {{difference}} between the intra or inter prediction signal and the original signal is transmitted using block-based transform coding. The state-of-the-art in coding the <b>quantized</b> <b>transform</b> <b>coefficients</b> is the approach specified in H. 264 /AVC for context-adaptive binary arithmetic coding. It has, however, been shown that the number of binary symbols that have to be arithmetically coded for the <b>transform</b> <b>coefficients</b> can become very large, making the concept less attractive for high rate applications. To overcome this issue, we propose a combination of simple variable-length codes and context-adaptive binary coding, which yields the same coding efficiency as the H. 264 /AVC <b>transform</b> <b>coefficient</b> coding at a lower complexity level and which has been adopted into the HEVC test model (HM) ...|$|R
40|$|Similar to {{previous}} video coding standards, transform and quantization {{are used in}} the most recent video coding standard High Efficiency Video Coding (HEVC) and a large number of <b>transform</b> <b>coefficients</b> are <b>quantized</b> to zeros. In order to re-duce the computations involved in transform and quantization in HEVC, a prediction approach based on Gaussian model is proposed to predict zero <b>quantized</b> <b>transform</b> <b>coefficients</b> within the 4 × 4, 8 × 8, 16 × 16 and 32 × 32 blocks. Ex-tensive experiments demonstrate that the proposed algorithm is able to effectively predict zero coefficients and thus reduce redundant computations while keeping the video quality and compression efficiency almost intact. Index Terms — High efficiency video coding, zero coef-ficients, transform, quantization, Gaussian model 1...|$|R
5000|$|The inter-picture {{prediction}} reduces temporal redundancy, with motion vectors used {{to compensate}} for motion. Whilst only integer-valued motion vectors are supported in H.261, a blurring filter {{can be applied to}} the prediction signal - partially mitigating the lack of fractional-sample motion vector precision. Transform coding using an 8×8 discrete cosine transform (DCT) reduces the spatial redundancy. The DCT that is widely used in this regard was introduced by N. Ahmed, T. Natarajan and K. R. Rao in 1974. Scalar quantization is then applied to round the <b>transform</b> <b>coefficients</b> to the appropriate precision determined by a step size control parameter, and the <b>quantized</b> <b>transform</b> <b>coefficients</b> are zig-zag scanned and entropy-coded (using a [...] "run-level" [...] variable-length code) to remove statistical redundancy.|$|R
40|$|In this paper, {{we present}} a novel steganographic method for {{embedding}} of secret data in still grayscale JPEG image. In order to provide large capacity of the proposed method while maintaining good visual quality of stego-image, the embedding process is performed in <b>quantized</b> <b>transform</b> <b>coefficients</b> of Discrete Cosine transform (DCT) by modifying coefficients according to modulo function, what gives to the steganography system blind extraction predisposition. After-embedding histogram of proposed Modulo Histogram Fitting (MHF) method is analyzed to secure steganography system against steganalysis attacks. In addition, AES ciphering was implemented to increase security and improve histogram after-embedding characteristics of proposed steganography system as experimental results show...|$|R
40|$|We {{consider}} {{the use of}} <b>quantized</b> block <b>transform</b> <b>coefficients</b> to the image database retrieval problem. Based on the <b>transform</b> <b>coefficients</b> feature vectors are constructed. These feature vectors are used in histograms and combination of histograms with similarity measure for database retrieval. Experiments on public face image database show good performance of the approach in comparison with other methods. 1...|$|R
40|$|This paper {{discusses}} distributed coding of two correlated video signals. The video {{signals are}} captured from a dynamic scene where each signal is temporally decorrelated by a motion-compensated Haar wavelet. The two cameras operate independently, however, the central decoder {{is able to}} exploit the coded information from all cameras to achieve the best reconstruction of the correlated video signals. The coding system utilizes nested lattice codes for the <b>transform</b> <b>coefficients</b> and exploits side information at the decoder. The efficiency of the decoder is improved by disparity compensation of one video signal. When compared to decoding without side information, decoding of the <b>quantized</b> <b>transform</b> <b>coefficients</b> with side information reduces the bit-rate of our test sequence by up to 5 %. Further, we observe bit-rate savings of up to 8 % with disparity compensation at the decoder and decoding of <b>transform</b> <b>coefficients</b> with side information...|$|R
30|$|This paper {{presents}} a fast context-based adaptive variable-length decoding (CAVLD) method of H. 264 /AVC {{with a very}} long instruction word-based bitstream processing unit (BsPU) designed for entropy decoding of multiple video formats. A new table mapping algorithm for the coeff_token, level, and run_before syntax elements of the <b>quantized</b> <b>transform</b> <b>coefficients</b> is proposed, and many branch operations are removed by utilizing several designated instructions in the BsPU. By applying designated instructions and the proposed table mapping algorithm to CAVLD, {{we found that the}} proposed fast CAVLD method achieves an increase of approximately 47 % in the decoding speed and a reduction of approximately 59 % in memory requirements for the table mapping.|$|R
40|$|This paper {{presents}} a high payload watermarking scheme for High Efficiency Video Coding (HEVC). HEVC is an emerging video compression standard that provides better compression performance {{as compared to}} its predecessor, i. e. H. 264 /AVC. Considering that HEVC may {{will be used in}} a variety of applications in the future, the proposed algorithm has a high potential of utilization in applications involving broadcast and hiding of metadata. The watermark is embedded into the <b>Quantized</b> <b>Transform</b> <b>Coefficients</b> (QTCs) during the encoding process. Later, during the decoding process, the embedded message can be detected and extracted completely. The experimental results show that the proposed algorithm does not significantly affect the video quality, nor does it escalate the bitrate...|$|R
40|$|Runlength coding is the {{standard}} coding technique for block transform-based image/video compression. A block of <b>quantized</b> <b>transform</b> <b>coefficients</b> is first represented as a sequence of RUN/LEVEL pairs that are then entropy coded [...] -RUN being the number of consecutive zeros and LEVEL being {{the value of the}} following nonzero coefficient. We point out in this letter the inefficiency of conventional runlength coding and introduce a novel adaptive runlength (ARL) coding scheme that encodes RUN and LEVEL separately using adaptive binary arithmetic coding and simple context modeling. We aim to maximize compression efficiency by adaptively exploiting the characteristics of block <b>transform</b> <b>coefficients</b> and the dependency between RUN and LEVEL. Coding results show that with the same level of complexity, the proposed ARL coding algorithm outperforms the conventional runlength coding scheme by a large margin in the rate [...] distortion sense...|$|R
40|$|International audienceIn this paper, a novel {{data hiding}} {{strategy}} is proposed to integrate disparity data, needed for 3 D visualization based on depth image based rendering, {{into a single}} H. 264 format video bitstream. The proposed method {{has the potential of}} being imperceptible and efficient in terms of rate distortion trade-off. Depth information has been embedded in some of <b>quantized</b> <b>transformed</b> <b>coefficients</b> (QTCs) while taking into account the reconstruction loop. This provides us with a high payload for embedding of depth information in the texture data, with a negligible decrease in PSNR. To maintain synchronization, the embedding is carried out while taking into account the correspondence of video frames. Three different benchmark video sequences containing different combinations of motion, texture and objects are used for experimental evaluation of the proposed algorithm...|$|R
40|$|Many {{advanced}} video applications require {{manipulations of}} compressed video signals. Popular video manipulation functions include overlap (opaque or semi-transparent), translation, scaling, linear filtering, rotation, and pixel multiplication. In this paper, we propose algorithms to manipulate compressed video in the compressed domain. Specifically, {{we focus on}} compression algorithms using the Discrete Cosine Transform (DCT) with or without Motion Compensation (MC). Compression systems of such kind include JPEG, Motion JPEG, MPEG, and H. 261. We derive {{a complete set of}} algorithms for all aforementioned manipulation functions in the transform domain, in which video signals are represented by <b>quantized</b> <b>transform</b> <b>coefficients.</b> Due to a much lower data rate and the elimination of decompression/compression conversion, the transform-domain approach has great potential in reducing the computational complexity. The actual computational speedup depends on the specific manipulation functions and [...] ...|$|R
40|$|In this work, {{we study}} {{and analyze the}} contourlet {{transform}} for low bit-rate image coding. This image-based geometrical transform has been recently introduced to efficiently represent images with a spars set of coefficients. In order to explore the potentiality of this new transform {{as a tool for}} image coding, we developed a direct coding scheme that is based on using non-linear approximation of images. We code the <b>quantized</b> <b>transform</b> <b>coefficients</b> as well as the significance map of an image in the contourlet transform domain. Based on the proposed approach, we analyzed the rate-distortion curves for a set of images and concluded that this coding approach, despite its redundancy, is visually competitive with a direct wavelet transform coder, and in particular, it is visually superior to wavelet coding for images with textures and oscillatory patterns...|$|R
40|$|In hybrid video coding, {{an entropy}} coding scheme transmits the <b>quantized</b> <b>transform</b> <b>coefficients,</b> {{resulting}} from block-based transformation and quantization {{of the difference}} between the prediction signal and the original signal, and additional side information. The state-of-the-art hybrid video coding standard H. 264 /AVC defines two different entropy coding schemes with different complexity-performance trade-off. As a result, the support for two different entropy coding schemes has to be maintained and introduces several problems. To overcome these issues, a unified solution is proposed, which is based on the PIPE/V 2 V coding concept. It achieves the same complexity-performance trade-offs as the existing entropy coding schemes by scalability. The advantage of the proposed scheme over the existing concept is the usage of the same set of tools for all configurations. Simulation results and complexity analysis...|$|R
40|$|In a {{standard}} transform coding scheme of images or video, the decoder {{can be implemented}} by a table-lookup technique without the explicit use of an inverse transformation. In this new decoding method, each received code index of a <b>transform</b> <b>coefficient</b> addresses a particular codebook to fetch a component code vector that resembles the basis vector of the linear transformation. The output image is then reconstructed by summing {{a small number of}} non-zero component code vectors. With a set of well-designed codebooks, this new decoder can exploit the correlation among the <b>quantized</b> <b>transform</b> <b>coefficients</b> to achieve better rate-distortion performance than the conventional decoding method. An iterative algorithm for designing a set of locally optimal codebooks from a training set of images is presented. We demonstrate that this new idea can be applied to decode improved quality pictures from the bit stream generated from {{a standard}} encoding scheme of still images or video, while the complexi [...] ...|$|R
40|$|International audienceThis paper {{presents}} {{design and}} analysis of watermarking of intra and inter frames in H. 264 /AVC video codec. Most of video watermarking algorithms take into account only intra for watermark embedding. In this paper, we analyze watermark embedding in intra {{as well as in}} inter and we note that watermark embedding capability of inter is comparable to that of intra. Watermark embedding, in only those non-zero <b>quantized</b> <b>transform</b> <b>coefficients</b> (QTCs) which are above a specific threshold, enables us to detect and extract the watermark on the decoding side. There is not significant compromise on quality and bitrate of the video bitstream because we have taken into account the reconstruction loop during the watermarking step. The proposed scheme does not target robustness. Rather the main contribution of our scheme is higher payload as compared to payloads obtained in previous works...|$|R
40|$|In {{this paper}} {{we present a}} rate {{distortion}} analysis and a statistical model in order to select coding parameters for memoryless coset codes, for a spatial scalability based mixed resolution Wyner-Ziv framework. The mixed resolution framework, used in this work, is based on full resolution coding of the key frames and spatial 2 -layer coding of the intermediate non-reference frames where the spatial enhancement layer is Wyner-Ziv coded. The framework enables reduced encoding complexity through reduced spatial-resolution encoding of the non-reference frames. The <b>quantized</b> <b>transform</b> <b>coefficients</b> of the Laplacian residual frame are mapped to cosets {{and sent to the}} decoder. A correlation estimation mechanism that guides the parameter choice process is proposed based on extracting edge information and residual error rate in co-located blocks from the low resolution base layer. Index Terms — Wyner-Ziv, reversed-complexity coding, spatial scalabilit...|$|R
40|$|In this paper, a novel {{data hiding}} {{strategy}} is proposed to integrate disparity data, needed for 3 D visualization based on depth image based rendering, {{into a single}} H. 264 format video bitstream. The proposed method {{has the potential of}} being imperceptible and efficient in terms of rate distortion trade-off. Depth information has been embedded in some of <b>quantized</b> <b>transformed</b> <b>coefficients</b> (QTCs) while taking into account the reconstruction loop. This provides us with a high payload for embedding of depth information in the texture data, with a negligible decrease in PSNR. To maintain synchronization, the embedding is carried out while taking into account the correspondence of video frames. Three different benchmark video sequences containing different combinations of motion, texture and objects are used for experimental evaluation of the proposed algorithm. Index Terms — 3 D TV, DIBR, H. 264 /AVC, data hiding 1...|$|R
40|$|This paper {{presents}} {{design and}} analysis of watermarking of intra and inter frames in H. 264 /AVC video codec. Most of video watermarking algorithms take into account only intra for watermark embedding. In this paper, we analyze watermark embedding in intra {{as well as in}} inter and we note that watermark embedding capability of inter is comparable to that of intra. Watermark embedding, in only those non-zero <b>quantized</b> <b>transform</b> <b>coefficients</b> (QTCs) which are above a specific threshold, enables us to detect and extract the watermark on the decoding side. There is not significant compromise on quality and bitrate of the video bitstream because we have taken into account the reconstruction loop during the watermarking step. The proposed scheme does not target robustness. Rather the main contribution of our scheme is higher payload as compared to payloads obtained in previous works. 1...|$|R
40|$|This paper {{describes}} a fast, low-complexity, entropy efficient video coder for wavelet pyramids. This coder approaches the entropy-limited coding rate of video wavelet pyramids, is fast in both {{hardware and software}} implementations, and has low complexity (no multiplies) for use in ASICs. It consists of a modified Z-coder used to code the zero/non-zero significance function and Huffman coding for the non-zero coefficients themselves. Adaptation is not required. There is a strong speed-memory trade-off for the Huffman tables allowing the coder to be customized {{to a variety of}} platform parameters. 1. Introduction An image transform codec consists of three steps: 1) a reversible transform, often linear, of the pixels for the purpose of decorrelation, 2) quantization of the transform values, and 3) entropy coding of the <b>quantized</b> <b>transform</b> <b>coefficients.</b> This paper presents an entropy codec which is fast, efficient in silicon area, coding-wise efficient, and practical when the transform i [...] ...|$|R
40|$|The {{standard}} H. 264 /AVC defines {{an efficient}} coding architecture both for coding applications where bandwidth or storage capacity is limited (e. g., video telephony or video conferencing over mobile channels and devices) and for applications that require high reconstruction quality and bit rate (e. g., HDTV). Since its main applications concern video communication over time-varying bandwidth channels, the bit rate {{has to be}} controlled with scalable algorithms that can be implemented on low resource devices. The paper describes a rate control algorithm that needs reduced memory area and complexity compared to other ones. The number of coded bits for each frame is accurately predicted through the percentage of null <b>quantized</b> <b>transform</b> <b>coefficients,</b> which {{is related to the}} quantization step via the energy of the quantized signal. It is possible to design a rate control algorithm based on this model that provides a good compression performance at a low computational cost...|$|R
40|$|WZD (Wavelet Z-coDec) is a {{component}} or composite video codec engineered and optimized to greatly reduce the ASIC silicon implementation area {{in return for}} a modest degradation of the R-D (rate/distortion) tradeoff. It consists of wavelet transforms organized into a carefully designed pyramid, dyadic quantization, and a novel application of Zcoding and Huffman coding to entropy code the wavelet coefficients. The resulting design can be completely implemented, including RAM, in less than 10 % of the silicon area of and MPEG 2 encoder excluding RAM. 1. Introduction An image transform codec consists of three steps: 1) a reversible transform, often linear, of the pixels for the purpose of decorrelation, 2) quantization of the transform values, and 3) entropy coding of the <b>quantized</b> <b>transform</b> <b>coefficients.</b> This paper presents an entropy codec WZD which is fast, efficient in silicon area, coding-wise efficient, and practical when the transform is a wavelet pyramid [3]. We will focus on natur [...] ...|$|R
40|$|High Efficiency Video Coding (HEVC) is {{the most}} recent jointly {{developed}} video coding standard of ITU-T Visual Coding Experts Group (VCEG) and ISO/IEC Moving Picture Experts Group (MPEG). Although its basic architecture is built along the conventional hybrid block-based approach of combining prediction with transform coding, HEVC includes a number of coding tools with greatly enhanced coding-efficiency capabilities relative to those of prior video coding standards. Among these tools are new transform coding techniques that include the support for dyadically increasing transform block sizes ranging from 4 x 4 to 32 x 32, the partitioning of residual blocks into variable block-size transforms by using a quadtree-based partitioning dubbed as residual quadtree (RQT) {{as well as some}} properly designed entropy coding techniques for <b>quantized</b> <b>transform</b> <b>coefficients</b> of variable <b>transform</b> block sizes. In this paper, we describe these HEVC techniques for transform coding with a particular focus on the RQT structure and the entropy coding stage and demonstrate their benefit in terms of improved coding efficiency by experimental results...|$|R
40|$|Scanning of <b>quantized</b> <b>transform</b> <b>coefficients</b> {{is a very}} {{significant}} procedure in video coding. In H. 264, it affects the coding efficiency of the following CABAC or CAVLC entropy coder directly. In this paper, we propose a novel edge-based predictive scanning scheme to improve the coding efficiency for inter-frame coding. This scheme includes three scanning pattern candidates. Besides zigzag pattern as defined in H. 264, two alternative patterns are obtained by on-line training on frame level. Specifically, reference block is utilized to predict edge information in current 88 block. Based on predictive edge information, a suitable scanning pattern will be selected to scan the quantized coefficients of current 88 block. Since a similar prediction process {{can be done in}} the decoder side as well, no overhead is needed to be transmitted in the bit-stream. Experimental results show that the proposed edge-based predictive scanning scheme yields an average of 0. 42 % BD-bitrate reduction over the H. 264 high profile. © 2011 IEEE...|$|R
40|$|We {{present a}} variant of the JPEG {{baseline}} image compression algorithm optimized for images that were generated by a JPEG decompressor. It inverts the computational steps of one particular JPEG decompressor implementation (Independent JPEG Group, IJG), and uses interval arithmetic and an iterative process to infer the possible values of intermediate results during the decompression, which are not directly evident from the decompressor output due to rounding. We applied our exact recompressor on a large database of images, each compressed at ten different quality factors. At the default IJG quality factor 75, our implementation reconstructed the exact <b>quantized</b> <b>transform</b> <b>coefficients</b> in 96 % of the 64 -pixel image blocks. For blocks where exact reconstruction is not feasible, our implementation can output transform-coefficient intervals, each guaranteed to contain the respective original value. Where different JPEG images decompress to the same result, we can output all possible bit-streams. At quality factors 90 and above, exact recompression becomes infeasible due to combinatorial explosion; but 68 % of blocks still recompressed exactly. 1...|$|R
40|$|We use the interpolating {{formulation}} of the lifting scheme to construct optimized second [...] generation wavelets for a given segment of data. We optimize with respect to an energy compaction cost function - the Shannon entropy of the uniformly <b>quantized</b> <b>transform</b> <b>coefficients.</b> We determine the predict operator adaptively at each level (and thus there is no dilation relation between wavelets on different scales) and calculate an associated update operator to produce vanishing moments in the synthesis wavelet. Our initial adaptive algorithm is an inefficient search over the space of 4 tap linear predictor operators sampled at a finite resolution. This search provides a good estimate of {{the best we can}} hope to achieve using any efficient algorithm for the same task. We present results for three typical segments of PCM audio. A further 0 - 5. 6 % reduction in Shannon entropy (an improvement of 0 - 22 %) is seen when compared to the standard polynomial interpolation predictors in our examples. Finally, we discuss possible techniques for determining an optimal, or near optimal linear predict operator efficiently for a given signal...|$|R
40|$|International audienceThis paper {{presents}} a new selective and scalable encryption (SSE) method for intra dyadic scalable coding framework based on wavelet/subband (DWTSB) for H. 264 /AVC. It {{has been achieved}} through the scrambling of <b>quantized</b> <b>transform</b> <b>coefficients</b> (QTCs) in all the subbands of DWTSB. To make the encryption scalable, it {{takes advantage of the}} prior knowledge of the frequencies which are dominant in different high frequency (HF) subbands, as traditional zigzag scan is not that efficient for them. Thus, by scrambling the scan order of QTCs in the intra scalable coding framework of H. 264 /AVC, {{we were able to get}} encryption and compression for enhancement layers (ELs) simultaneously. Watermarking has been integrated in the proposed architecture to avoid the requirement of separate keys for each spatial layer. The algorithm is better suited for multimedia streaming as the bitrate of the encrypted bitstream is lower than the original bitrate. Besides offering SSE, the proposed algorithm, when applied to different benchmark video sequences, outperformed the standard zigzag scan in terms of bitrate...|$|R
40|$|One way to {{save the}} power {{consumption}} in the H. 264 decoder is for the H. 264 encoder to generate decoderfriendly bit streams. By following this idea, a decoding complexity model of context-based adaptive binary arithmetic coding (CABAC) for H. 264 /AVC is investigated in this research. Since different coding modes {{will have an impact}} on the number of <b>quantized</b> <b>transformed</b> <b>coefficients</b> (QTCs) and motion vectors (MVs) and, consequently, the complexity of entropy decoding, the encoder with a complexity model can estimate the complexity of entropy decoding and choose the best coding mode to yield the best tradeoff between the rate, distortion and decoding complexity performance. The complexity model consists of two parts: one for source data (i. e. QTCs) and the other for header data (i. e. the macro-block (MB) type and MVs). Thus, the proposed CABAC decoding complexity model of a MB is a function of QTCs and associated MVs, which is verified experimentally. The proposed CABAC decoding complexity model can provide good estimation results for variant bit streams. Practical applications of this complexity model will also be discussed. 1...|$|R
