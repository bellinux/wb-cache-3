16|77|Public
50|$|A {{prosodic}} {{stress system}} in which moraically heavy syllables are assigned stress {{is said to have}} the property of <b>quantity</b> <b>sensitivity.</b>|$|E
50|$|Another type of {{prosodic}} {{stress pattern}} is <b>quantity</b> <b>sensitivity</b> - in some languages additional stress {{tends to be}} placed on syllables that are longer (moraically heavy).|$|E
50|$|A {{term used}} in government-binding theory for a {{specification}} {{of the types}} of variation that a principle of grammar manifests among different languages. It is suggested that there are no rules of grammar in the traditional sense, but only principles which can take a slightly different form in different languages. For example, a head parameter specifies the positions of heads within phrases (e.g. head-first in English, head-last in Japanese). The adjacency parameter of case theory specifies whether case assigners must be adjacent to their noun phrases (e.g. to the left in English, to the right in Chinese). The pro-drop (or ‘null subject’) parameter determines whether the subject of a clause can be suppressed. Determining the parametric values for given languages is known as parameter-setting. The overall approach has been called the principles and parameters theory (PPT) of universal grammar, and has since come to be applied outside of syntactic contexts, notably in characterizing phonological relations. Later versions of metrical phonology, for example, recognize a series of parameters governing the way metrical feet should be represented, such as <b>quantity</b> <b>sensitivity</b> and directionality.|$|E
5000|$|... {{with the}} two {{right-hand}} <b>quantities</b> being <b>sensitivity</b> and specificity. Thus the expanded formula is: ...|$|R
40|$|In {{this article}} we compare two {{different}} techniques to measure the concentration of saline solutions for {{the identification of the}} apparent mass diffusion coefficient in soils saturated with distilled water. They are the radiation measurement technique and the electrical conductivity measurement technique. These techniques are compared in terms of measured <b>quantities,</b> <b>sensitivity</b> coefficients with respect to unknown parameters and the determinant of the information matrix. The apparent mass diffusion coefficient is estimated by utilizing simulated measurements containing random errors. The Levenberg-Marquardt method of minimization of the least-squares norm is used as the parameter estimation procedure. The effects of the volume of saline solution injected into the column devised for the experiments on the accuracy of the estimated parameters are also addressed in this article...|$|R
40|$|Various {{measures}} {{can be used}} to estimate bias or unfairness in a predictor. Previous work has already established that some of these measures are incompatible with each other. Here we show that, when groups differ in prevalence of the predicted event, several intuitive, reasonable measures of fairness (probability of positive prediction given occurrence or non-occurrence; probability of occurrence given prediction or non-prediction; and ratio of predictions over occurrences for each group) are all mutually exclusive: if one of them is equal among groups, the other two must differ. The only exceptions are for perfect, or trivial (always-positive or always-negative) predictors. As a consequence, any non-perfect, non-trivial predictor must necessarily be "unfair" under two out of three reasonable sets of criteria. This result readily generalizes {{to a wide range of}} well-known statistical <b>quantities</b> (<b>sensitivity,</b> specificity, false positive rate, precision, etc.), all of which can be divided into three mutually exclusive groups. Importantly, The results applies to all predictors, whether algorithmic or human. We conclude with possible ways to handle this effect when assessing and designing prediction methods...|$|R
40|$|Abstract — Trees are a {{powerful}} structure for representing hierarchical relations {{in a natural}} way. Comparison of trees is a recurrent task in various computer science related fields. The widely used Robinson-Foulds distance for comparing leaf labeled trees is overly sensitive to very small changes in the tree. The measure of bipartition dissimilarity refines Robinson-Foulds metric by comparing {{the quality of the}} tree bipartitions instead of their <b>quantity.</b> <b>Sensitivity</b> analysis is used in this paper which shows that bipartition dissimilarity has smaller sensitivity to small modifications in the tree. Keywords- leaf labeled trees; Robinson-Foulds distance; bipartition dissimilarity; sensitivity analysis; tree rearrangement operations I...|$|E
40|$|We {{present a}} {{cognitive}} model of moral decision-making, MoralDM, which models psychological findings about utilitarian and deontological modes of reasoning. Current theories of moral decision-making extend beyond pure utilitarian models by relying strongly on contextual factors that vary with culture. In MoralDM, {{the impacts of}} secular versus sacred values are modeled via qualitative reasoning, using {{an order of magnitude}} representation. We present a simplified version of ROM(R) (Dague, 1993) and discuss how {{it can be used to}} capture people’s degree of <b>quantity</b> <b>sensitivity.</b> MoralDM uses a combination of first-principles reasoning and analogical reasoning to determine consequences and utilities of moral judgments. A natural language system is used to produce formal representations for the system from psychological stimuli, to reduce tailorability. We compare MoralDM against psychological results in moral decision-making tasks and show that its performance improves with experience...|$|E
40|$|In {{this study}} {{it is argued}} that the {{omission}} of closed class morphemes and of unstressed syllables within words is related to their common characteristic, viz. that they are unstressed, rhythmically weak parts of utterances. Several strands of evidence indicate that it is unlikely that children are unable to perceive these elements in the input speech. The pattern of (non) realization of unstressed syllables within content words and the class of determiners, was analysed in two Dutch children from 1; 6 to 2; 11. It appeared that polysyllabic words were quite generally truncated {{in such a way that}} they fitted a trochaic (strong-weak) pattern, particularly in the early samples. Some observations with respect to the (non) realization of determiners are suggestive of an influence of an SW-constraint on the realization of noun phrases. These findings support the hypothesis that in the course of utterance preparation, words and phrases are mapped onto S(W) templates. Anecdotal evidence suggests that the dissolution of the SW-constraint coincides with the acquisition of specific aspects of stress assignment in Dutch, such as <b>quantity</b> <b>sensitivity...</b>|$|E
30|$|With the {{increased}} usage frequency and ubiquitous usage of OSNs, the <b>quantity</b> and <b>sensitivity</b> of user data that is stored on OSNs has grown tremendously as well. This is fostered by {{the availability of}} social networking services on mobile devices that provide location-based features and camera functions, for instance, allowing users to publish their current activity and location. For service providers, {{it is possible to}} derive rich profiles of their users [4], leading to social footprints[5].|$|R
40|$|Most neuropsychologists {{are aware}} that, given the {{specificity}} and sensitivity {{of a test}} and {{an estimate of the}} base rate of a disorder, Bayes' theorem can be used to provide a post-test probability for the presence of the disorder given a positive test result (and a post-test probability for the absence of a disorder given a negative result). However, in the standard application of Bayes' theorem the three <b>quantities</b> (<b>sensitivity,</b> specificity, and the base rate) are all treated as fixed, known quantities. This is very unrealistic as there may be considerable uncertainty over these quantities and therefore even greater uncertainty over the post-test probability. Methods of obtaining interval estimates on the specificity and sensitivity of a test are set out. In addition, drawing and extending upon work by Mossman and Berger (2001), a Monte Carlo method is used to obtain interval estimates for post-test probabilities. All the methods have been implemented in a computer program, which is described and made available (www. abdn. ac. uk/~psy 086 /dept/BayesPTP. htm). When objective data on the base rate are lacking (or have limited relevance to the case at hand) the program elicits opinion for the pre-test probability...|$|R
40|$|A {{method is}} {{presented}} {{for the analysis}} of damped structural systems in which the structural components are represented by impedance models and analyzed in the frequency domain. Methods are presented to assemble and condense system impedance matrices, and then to identify approximate mass, stiffness, and damping matrices for systems whose impedances are complicated functions of frequency. Formulas are derived for determination of approximate values for system natural frequencies and damping using frequency domain <b>quantities.</b> The <b>sensitivities</b> of these approximate values to system parameter changes are analyzed. The implementation of these analysis tools is discussed and applied to a simple mechanical system...|$|R
40|$|Traditional {{approaches}} have regarded German stress to be predictable by localizing the initial stem syllable. Later, Metrical Phonology localized German stress {{close to the}} right edge of the stem, depending on the syllable weight. It will be shown that an algorithm based on a polytomous scale of syllable weight rather than a dichotomous one (heavy- light) is well able to predict German lexical stress. However, within the word class of proper names the algorithm fails. Here speakers of German appear to place stress on the left rather than the right edge of the stem. A {{closer look at the}} phenomena shows that this is not due to a preference of initial stress in historically older proper names but rather a rhythmic preference for trochees and dactyls. This appears to be evidence for a diachronic process where the influence of syllable weight has increased and become more important than a specific rhythmic pattern. This <b>quantity</b> <b>sensitivity</b> has not yet reached the peak of its influence in the diachronically older proper names. 1...|$|E
40|$|Morally-motivated {{decision}} making has recently {{emerged as an}} especially relevant domain of study for understanding cultural conflict. Baron and Spranca (1997) have proposed that people hold protected values that are protected against being traded off for other goods. One of the most characteristic properties of protected values is quantity insensitivity, meaning that holders of these values are relatively insensitive to {{the consequences of their}} actions. However, recent research suggests that quantity insensitivity may be a highly context-dependent property of protected values (Bartels & Medin, 2007). To examine the context-dependency of <b>quantity</b> <b>sensitivity,</b> we asked participants if one harmful act was less wrong than five harmful acts and if five harmful acts were more wrong than one harmful act. Participants were relatively insensitive to the magnitude of the consequences in response to the first question, but highly quantity sensitive in response to the second question. This effect was amplified for people who held protected values – they were both more and less quantity sensitive than people without protected values in the respective conditions. We propose that this effect is due to the different frames of reference elicited by each question...|$|E
40|$|Extrametricality {{has played}} an {{important}} role in metrical theory since its beginnings (cf. Liberman and Prince 1977), but its formal representation has been quite varied. Examples of its application to syllables range from a simple diacritic on the syllable (Hayes 1979, 1981) to exclusion of that syllable and its segmental content from the domain of rule application (Inkelas 1989). In the extended bracketed grids theory of Idsardi (1992), extrametricality results from the insertion of a foot edge that leads to exclusion of a syllable from foot structure. While this approach is appealing in its elegance, I argue that it cannot account adequately for the interaction of extrametricality with <b>quantity</b> <b>sensitivity.</b> The argument is based on two languages with the same foot structure — moraic trochees constructed at the right edge of the word — and similar, but importantly distinct, roles for extrametricality. Section 1 outlines Latin stress and extrametricality and its theoretical analysis, while section 2 demonstrates problems that this analysis encounters in treating the similar facts in Manam. (The discussion is restricted to primary stress.) Section 3 shows that in Optimality Theory a unified and principled treatment of the two languages is easily available...|$|E
40|$|We {{report the}} {{microstructure}} and gas-sensing properties of a nonequilibrium TiO 2 -SnO 2 solid solution {{prepared by the}} sol-gel method. In particular, {{we focus on the}} effect of Cd doping on the sensing behavior of the TiO 2 -SnO 2 sensor. Of all volatile organic compound gases examined, the sensor with Cd doping exhibits exclusive selectivity as well as high sensitivity to formaldehyde, a main harmful indoor gas. The key gas-sensing <b>quantities,</b> maximum <b>sensitivity,</b> optimal working temperature, and response and recovery time, are found to meet the basic industrial needs. This makes the Cd-doped TiO 2 -SnO 2 composite a promising sensor material for detecting the formaldehyde gas...|$|R
40|$|<b>Quantities</b> {{measuring}} <b>sensitivities</b> and {{correlations of}} parameters {{involved in an}} engineering system are systematically defined by means of lattice theory. Consideration of lattice duality leads to recognition of polymatroid structure of the sensitivity and correlation measures. Analogous algebraic structures are shown {{to exist in the}} formulation of mutual information in information theory and multivariate analysis in statistics. Then, the sensitivity and correlation measures thus obtained are transformed into more tractable analytic expressions by means of expansion in series of orthogonal functions. Finally, a scheme of the so-called number-theoretic or quasi-Monte Carlo method is given for numerical evaluation of these expressions. This formulation generalizes the existing method of nonlinear sensitivity analysis...|$|R
40|$|The {{injection}} of diluted iodine into the supersonic flow of helium was studied {{by means of}} CFD software Fluent. The computational grid was subsequently adapted, i. e. refined in the selected regions with large gradients of different defined <b>quantities.</b> A <b>sensitivity</b> of the refinement on the local and far-field solution was obtained. A sensitivity of the flowfield on {{the replacement of the}} first order interpolation scheme to the second order one in several combinations of solved equations was done. An influence of the increasing the interpolation scheme order on the far-field was found, dependent on the way of combination of the 1 st and 2 nd orders in selected equations interpolation...|$|R
40|$|A {{critical}} {{aspect of}} international safeguards activities {{performed by the}} International Atomic Energy Agency (IAEA) is the verification that facility design and construction (including upgrades and modifications) do not create opportunities for nuclear proliferation. These Design Information Verification activities require that IAEA inspectors compare current and past information about the facility to verify the operator’s declaration of proper use. The actual practice of DIV presents challenges to the inspectors due to {{the large amount of}} data generated, concerns about sensitive or proprietary data, the overall complexity of the facility, and the effort required to extract just the safeguards relevant information. Planned and anticipated facilities will (especially in the case of reprocessing plants) be ever larger and increasingly complex, thus exacerbating the challenges. This paper reports the results of a workshop held at the Idaho National Laboratory in March 2009, which considered technologies and methods to address these challenges. The use of 3 D Laser Range Finding, Outdoor Visualization System, Gamma-LIDAR, and virtual facility modeling, as well as methods to handle the facility data issues (<b>quantity,</b> <b>sensitivity,</b> and accessibility and portability for the inspector) were presented. The workshop attendees drew conclusions about the use of these techniques with respect to successfully employing them in an operating environment, using a Fuel Conditioning Facility walk-through as a baseline for discussion...|$|E
40|$|Is morally {{motivated}} {{decision making}} different from others kinds of decision making? There {{is evidence that}} when people have sacred or protected values (PVs) they reject tradeoffs for secular values (e. g. ” You can’t {{put a price on}} a human life. ”) and tend to employ deontological rather than consequentialist decision principles. People motivated by PVs appear to show “quantity insensitivity. ” That is, in tradeoffs situations they are less sensitive to the consequences of their choices than people without PVs. The current study shows that the relationship between PVs and quantity insensitivity varies across contexts: in one design previous results are replicated; in a second, PVs are related to increased <b>quantity</b> <b>sensitivity.</b> These and other findings call into question important properties of PVs. Are Morally-Motivated 3 If we want to comprehend people’s commonplace and extraordinary actions, we must understand the values that inspire them. “Extreme ” actions (e. g., selfless heroism, suicide terrorism) show that strong values may motivate behavior and some researchers suggest that “all attitudinal and behavioral decisions should be traceable to personal value priorities ” (Rohan, 2000, p. 270). Recently, researchers have begun to examine morally motivated decision making, and it appears to have a number of distinctive properties. Our focus is on decisions involving protected values (PVs). The PV framework developed b...|$|E
40|$|ABSTRACT—Is morally {{motivated}} {{decision making}} {{different from other}} kinds of decision making? There is evidence that when people have sacred or protected values (PVs), they reject trade-offs for secular values (e. g., ‘‘You can’t {{put a price on}} a human life’’) and tend to employ deontological rather than consequentialist decision principles. People motivated by PVs appear to show quantity insensitivity. That is, in trade-off situations, they are less sensitive to the consequences of their choices than are people without PVs. The current study examined the relation between PVs and quantity insensitivity using two methods of preference assessment: In one design, previous results were replicated; in a second, PVs were related to increased <b>quantity</b> <b>sensitivity.</b> These and other findings call into question important presumed properties of PVs, suggesting that how PVs affect willingness to make tradeoffs depends on where attention is focused, a factor that varies substantially across contexts. If one wants to comprehend people’s commonplace and extraordinary actions, one must understand the values that inspire them. ‘‘Extreme’ ’ actions (e. g., selfless heroism, suicide terrorism) show that strong values may motivate behavior, and some researchers suggest that ‘‘all attitudinal and behavioral decisions should be traceable to personal value priorities’ ’ (Rohan, 2000, p. 270). Recently, researchers have begun to examine morally motivated decision making, and it appears to have a number of distinctive properties. Our focus is on decisions involving protected values (PVs). According to the PV framework developed by Baron and hi...|$|E
40|$|This paper explores how a {{coherent}} risk measure {{could be used}} to determine risk-sensitive capital requirements for reinsurance treaties. The need for a risk-sensitive capital calculation arises in the context of estimating the return on equity (ROE) for several treaties or different options on one treaty. Looking at the loss random variable alone is insufficient for a complete risk analysis since this would fail to incorporate the impact of adjustable premium and ceding commission provisions on the final net risk. The paper presents a framework for systematically reflecting treaty features by viewing capital {{as a function of the}} distribution of the final net underwriting loss. To avoid negative values for indicated capital, the concept of a risk quantity variable is introduced as a non-negative monotonically increasing function of the net underwriting loss variable. Two risk quantities are discussed: one obtained by capping the net underwriting loss from below at zero, and the other by taking the excess of the net underwriting loss above its expectation. A coherent risk measure is then applied to a risk quantity to obtain indicated capital. The approach is demonstrated in simple discrete distribution examples by applying {{a coherent}} measure, the Tail Value at Risk, to the two risk <b>quantities.</b> <b>Sensitivity</b> testing on the examples is presented showing how the different measures respond to changes in premium adequacy, swing rating, sliding scale commission plans, and layering. In summary, this paper is one attempt to bridge the gap between the theoretical results of coherence theory and the practical need for methods to set risk-sensitive capital in treaty ROE analysis...|$|R
50|$|THOR is an {{advanced}} 50th percentile male dummy. The successor of Hybrid III, THOR {{has a more}} human-like spine and pelvis, and its face contains a number of sensors which allow analysis of facial impacts to an accuracy currently unobtainable with other dummies. THOR's range of sensors is also greater in <b>quantity</b> and <b>sensitivity</b> than those of Hybrid III. THOR's original manufacturer, GESAC Inc., ceased production after the slowdown of the auto industry in the late 2000s. THOR was being further developed, and two other companies were working on similar dummies; NHTSA's ultimate goal for this government-funded project was {{the development of a}} single THOR dummy, but THOR dummy development stopped. FTSS, bought by Humanetics, and DentonATD both continued to produce the THOR LX and THOR FLX.|$|R
50|$|In {{mathematical}} finance, the Greeks are the <b>quantities</b> {{representing the}} <b>sensitivity</b> {{of the price}} of derivatives such as options to a change in underlying parameters on which the value of an instrument or portfolio of financial instruments is dependent. The name is used because the most common of these sensitivities are denoted by Greek letters (as are some other finance measures). Collectively these have also been called the risk sensitivities, risk measures or hedge parameters.|$|R
40|$|Due {{to strict}} {{environmental}} legislations and competitive economics worldwide, closed-loop supply chain (CLSC) management has been receiving increasing attention. In {{order to improve}} the return rate of used products, in this paper, a recovery framework is proposed by employing buy-back offer at retailer level. The proposed recovery framework is integrated with an optimization model for a multi-period CLSC under demand and capacity uncertainty to determine optimal buy-back price {{that needs to be}} offered to consumers so that the minimum collection limit set by the legislators is fulfilled as well as overall cost of the integrated system is minimized. The developed model addresses the possibility of three way recovery options, namely; product remanufacturing, component remanufacturing, and raw material recovery to determine the optimal manufacturing, remanufacturing and recycling <b>quantity.</b> <b>Sensitivity</b> analysis suggests that there is a trade-off between the extra benefit generated through remanufacturing and the cost of acquisition of used products due to the employment of buy-back offer. A condition for the buy-back offer to be simultaneously advantageous for both manufacturer and retailer under revenue sharing contract is derived. A comparative study is also undertaken to examine the performance of the CLSC when collection is made through buy-back offer vs. third-party collection. The proposed CLSC model is further extended by adding a separate demand for remanufactured products and the performance of the initial model with the extended model is compared. (C) 2016 Elsevier Ltd. All rights reserved...|$|E
40|$|Traditionally, it {{has been}} held that that the sound systems of {{languages}} are consistent regarding sensitivity to syllable weight (among others, see Hayes ’ Metrical Stress Theory). That is, in quantity sensitive languages the content of syllables {{is an important factor}} in their phonology; by contrast, in quantity insensitive languages, the content of syllables is not relevant. Furthermore, Hayes treats <b>quantity</b> <b>sensitivity</b> as a parameter, which means languages are either sensitive or insensitive to quantity, but they cannot be both at the same time. This paper shows that this all-or-nothing conception of weight (in) sensitivity is too rigid and does not account for languages, such as Valencian Catalan, that do manifest contradicting quantity patterns in their phonology. Section 2 of this study analyzes the role of syllable weight in stress assignment and truncation, a process of prosodic morphology (McCarthy and Prince 1986, 1990, 1991, 1995), in Catalan and claims that one of its major regional varieties, Valencian Catalan, shows a conflicting weight pattern, that is a quantity sensitive (QS) non-verbal main-stress system and a quantity insensitive (QI) prosodic morphology, mainly involving stress and truncation. On the other hand, the rest of Catalan dialects are consistent in the use of weight in their main nonverbal stress assignment and prosodic morphology. This study also argues in section 3 that a constraint-based model (Optimality Theory, henceforth OT, Prince and Smolensky) is able to accommodate contradictory weight patterns such as the ones found in Catalan. Finally, section 4 offers a summary of this paper...|$|E
40|$|The {{sensitive}} explosives used in initiating devices like primers and detonators {{are called}} primary explosives. Successful detonations of secondary explosives are accomplished by suitable sources of initiation energy that is transmitted {{directly from the}} primaries or through secondary explosive boosters. Reliable initiating mechanisms are available in numerous forms of primers and detonators depending upon {{the nature of the}} secondary explosives. The technology of initiation devices used for military and civilian purposes continues to expand owing to variations in initiating method, chemical composition, <b>quantity,</b> <b>sensitivity,</b> explosive performance, and other necessary built-in mechanisms. Although the most widely used primaries contain toxic lead azide and lead styphnate, mixtures of thermally unstable primaries, like diazodinitrophenol and tetracene, or poisonous agents, like antimony sulfide and barium nitrate, are also used. Novel environmentally friendly primary explosives are expanded here to include cat[FeII(NT) 3 (H 2 O) 3], cat 2 [FeII(NT) 4 (H 2 O) 2], cat 3 [FeII(NT) 5 (H 2 O) ], and cat 4 [FeII(NT) 6] with cat = cation and NT− = 5 -nitrotetrazolato-N 2. With available alkaline, alkaline earth, and organic cations as partners, four series of 5 -nitrotetrazolato-N 2 -ferrate hierarchies have been prepared that provide a plethora of green primaries with diverse initiating sensitivity and explosive performance. They hold great promise for replacing not only toxic lead primaries but also thermally unstable primaries and poisonous agents. Strategies are also described for the systematic preparation of coordination complex green primaries based on appropriate selection of ligands, metals, and synthetic procedures. These strategies allow for maximum versatility in initiating sensitivity and explosive performance while retaining properties required for green primaries...|$|E
2500|$|THOR is an {{advanced}} 50th percentile male dummy. The successor of Hybrid III, THOR {{has a more}} human-like spine and pelvis, and its face contains a number of sensors which allow analysis of facial impacts to an accuracy currently unobtainable with other dummies. THOR's range of sensors is also greater in <b>quantity</b> and <b>sensitivity</b> than those of Hybrid III. [...] THOR's original manufacturer, GESAC Inc., ceased production after the slowdown of the auto industry in the late 2000s. THOR was being further developed, and two other companies were working on similar dummies; NHTSA's ultimate goal for this government-funded project was {{the development of a}} single THOR dummy, but THOR dummy development stopped. FTSS, bought by Humanetics, and DentonATD both continued to produce the THOR LX and THOR FLX.|$|R
40|$|A {{major goal}} of this {{research}} was to quantify the interactions between UVR, ozone and aerosols. One method of quantification was to calculate sensitivity coefficients. A novel aspect of this work was the use of Automatic Differentiation software to calculate the sensitivities. The authors demonstrated the use of ADIFOR {{for the first time in}} a dimensional framework. Automatic Differentiation was used to calculate such <b>quantities</b> as: <b>sensitivities</b> of UV-B fluxes to changes in ozone and aerosols in the stratosphere and the troposphere; changes in ozone production/destruction rates to changes in UV-B flux; aerosol properties including loading, scattering properties (including relative humidity effects), and composition (mineral dust, soot, and sulfate aerosol, etc.). The combined radiation/chemistry model offers an important test of the utility of Automatic Differentiation as a tool in atmospheric modeling...|$|R
40|$|Temperature {{sensitivity}} and pleasure-displeasure sensitivity of materials found indoors were evaluated numerically through tactile sensation measurements at 65 %RH and 10, 20, 30 degreees C. 1) Materials {{were found to}} be arranged in increasing order of warmth (magnitude of normalized sensory quantity) as follows : cloth・tatami, wood, wood-based material, paper, plaster, plastic, cement, ceramic, stone and metal, with respect to temperature sensitivity at eath temperature level. For pleasure-displeasure sensitivity, the arrangement of materials in increasing order of pleasure (magnitude of normalized sensory quantity) was the same as for temperature sensitivity at 10 degrees C, but tended to be arranged in the reverse order at 20 and 30 degrees C. Furthermore, there were positive correlations at 10 and 20 degrees, and a negative correlation at 30 degrees between the normalized sensory <b>quantities</b> of temperature <b>sensitivity</b> and pleasure-displeasure sensitivity. 2) A slight change in the normalized sensory quantity could be observed with increasing temperature without reference to the types of materials for temperature sensitivity, but had three features with respect to materials : steep decreases, slight variations in the range of sensory quantity of about 0 to 1, and steep increases of sensory <b>quantity</b> in pleasure-displeasure <b>sensitivity...</b>|$|R
40|$|New Markers of Alcohol Consumption Development and Evaluation of the Clinical Use of Carbohydrate-Deficient Transferrin in Serum and S-Hydroxytryptophol in Urine Dissertation from Karolinska Institutet, Department of Clinical Neuroscience, Psychiatry Section, S- 171 76 Stockholm, Sweden Treatment {{of alcohol}} {{dependence}} {{and evaluation of}} treatment and research programs are often based on self-reports of the patients. Since the alcohol consumption pattern does not always relate to the reported behaviour {{there is a need}} for validation by reliable biochemical markers. In this study, urinary 5 -hydroxytryptophol levels have been shown to increase in a dose dependent way after alcohol intake. Increased levels can be seen several hours after blood ethanol levels reach zero, and the 5 -hydroxytryptophol/ 5 -hydroxyindole- 3 -acetic acid (5 -HTOL/ 5 -HIAA) ratio has showed a satisfactory degree of individual stability, when it was followed daily in a group of teetotallers for one month. The distribution of the 5 -HTOL/ 5 -HIAA ratio differed statistically between a group of teetotallers and regular consumers. A 5 -HTOL/ 5 -HIAA ratio of 20 (pmoles/nmoles) is proposed as a cut-offlimit. Carbohydrate-deficient transferrin (CDT) in serum has been shown to be a specific marker for regular alcohol consumption. Detection of relapse in alcohol consumption in alcohol-dependent patients were examined using CDT and 5 -HTOL as validation of self-reports. Clinical ratings and self-reports about alcohol consumption were performed three times a week during six months. The frequency of alcohol consumption as measured by self-reports differed significantly compared to the biochemical markers. None of the alcohol-dependent patients totally abstained when 5 -HTOL values were used for detecting alcohol intake. Apart from high CDT values at the start of the treatment only few showed elevated values during treatment due to more sporadic alcohol intake. The reference levels of CDT (20 U/l for males and 27 U/l for females) is used for a normal population. In monitoring alcohol-dependent patients individualized levels, based on the lowest values in each individual were used. By using individualized levels the sensitivity of CDT increased. Most episodes of elevated levels of CDT could be validated clinically and/or by 5 -HTOL 14 days preceeding each serum sampling. 5 -HTOL and CDT seems to be complemantary markers since they reflect different aspects of alcohol intake with respect to time, <b>quantity,</b> <b>sensitivity,</b> and specificity, and therefore have different properties in treatment monitoring. The utility of CDT and gamma-glutamyl transferase (GGT) as complementory markers of excessive alcohol consumption was examined. CDT appeared to be the most sensitive marker in the out-patient group. In 114 alcoholics sampled at random at a detoxification unit there was a higher degree of elevated GGT levels, but no significant correlation between CDT and GGT. Hence, measurement of both CDT and GGT may increase the possibility to identify excessive alcohol consumption. By following changes in CDT and GGT values during a period of treatment monitoring after withdrawal, the most useful individual marker can be determined. Finally, the effects of long-term abstinence on psychological functioning in alcohol-dependent patients and healthy controls were compared through a prospective longitudinal study design. For the alcohol- dependent patients the self-reports were compared with biochemical markers. Patients and controls differed in terms of mood stability, craving, cognitive, and vegetative disturbances. A gradual normalization back to baseline levels was observed for some symptoms. The unstable affective state for alcohol-dependent patients may be related to the protracted withdrawal or may represent residual symptomatology. Among the alcohol-dependent patients differences in overall psychological functioning and craving for alcohol were found in relation to low- or high-frequency alcohol intake during long-term treatment. The findings have important implications in that reliable identification of patients' alcohol consumption pattern using biochemical markers would allow for an individually treatment. Key Words: Alcohol drinking, 5 -hydrogxytryptophol, 5 -hydroxyindole- 3 -acetic acid, urine, human, carbohydrate-deficient transferrin, biochemical marker, gamma-glutamyl transferase, alcohol-dependence protracted withdrawal, relapse, craving, psychological functionin...|$|E
40|$|A variational {{technique}} {{is used to}} derive analytical expressions for the sensitivity of several geometric indicators of flow separation to steady actuation. Considering the boundary layer flow above a wall-mounted bump, the six following representative quantities are considered: {{the locations of the}} separation point and reattachment point connected by the separatrix, the separation angles at these stagnation points, the backflow area and the recirculation area. For each geometric <b>quantity,</b> linear <b>sensitivity</b> analysis allows us to identify regions which are the most sensitive to volume forcing and wall blowing/suction. Validations against full non-linear Navier-Stokes calculations show excellent agreement for small-amplitude control for all considered indicators. With very resemblant sensitivity maps, the reattachment point, the backflow and recirculation areas are seen to be easily manipulated. In contrast, the upstream separation point and the separatrix angles are seen to remain extremely robust with respect to external steady actuation. Comment: 20 pages, 12 figure...|$|R
40|$|The Thouless energy, characterizes {{numerous}} <b>quantities</b> {{associated with}} <b>sensitivity</b> to boundary conditions in diffusive mesoscopic conductors. What happens to these quantities if the disorder strength is decreased and {{a transition to}} the ballistic regime takes place? In the present analysis we refute the intuitively plausible assumption that loses its meaning as an inverse diffusion time through the system at hand, and generally disorder independent scales take over. Instead we find {{that a variety of}} (thermodynamic) observables are still characterized by the Thouless energy. Comment: 4 pages REVTEX, uuencoded file. To appear in Physical Review Letter...|$|R
40|$|Abstract:Sensitivity {{coefficients}} {{are essentially}} conversion factors that allow one {{to convert the}} units of an input quantity into the units of the measurand. Sensitivity coefficients are also, and more importantly, measures of how much change is produced in the measurand by changes in an input <b>quantity.</b> Mathematically, <b>Sensitivity</b> coefficients are obtained from partial derivatives of the model function {{with respect to the}} input quantities [1]. Many laboratories use the value of the sensitivity coefficient equals one for every input quantity without performing the partial derivatives. They use the value one just to express the uncertainty of the input quantity as a percentage which is not true. The aim of this work is to demonstrate the difference in the uncertainty values calculated by using the sensitivity coefficient by the partial differential equations and by using percentage values through examples in hardness and tension tests. The examples show high difference between the two methods...|$|R
40|$|Because experiment/model {{comparisons}} in {{magnetic confinement}} fusion {{have not yet}} satisfied the requirements for validation as understood broadly, a set of approaches to validating mathematical models and numerical algorithms are recommended as good practices. Previously identified procedures, such as verification, qualification, and analysis of error and uncertainty, remain important. However, particular challenges intrinsic to fusion plasmas and physical measurement therein lead to identification of new or less familiar concepts that are also critical in validation. These include the primacy hierarchy, which tracks the integration of measurable <b>quantities,</b> and <b>sensitivity</b> analysis, which assesses how model output is apportioned to different sources of variation. The use of validation metrics for individual measurements is extended to multiple measurements, with provisions for the primacy hierarchy and sensitivity. This composite validation metric is essential for quantitatively evaluating comparisons with experiments. To mount successful and credible validation in magnetic fusion, a new culture of validation is envisaged. Comment: 27 pages, 1 table, 6 figure...|$|R
40|$|A {{study has}} been {{performed}} focusing on the calculation of sensitivities of displacements, velocities, accelerations, and stresses in linear, structural, transient response problems. One significant goal was to develop and evaluate sensitivity calculation techniques suitable for large-order finite element analyses. Accordingly, approximation vectors such as vibration mode shapes are used to reduce the dimensionality of the finite element model. Much of the research focused on the accuracy of both response <b>quantities</b> and <b>sensitivities</b> {{as a function of}} number of vectors used. Two types of sensitivity calculation techniques were developed and evaluated. The first type of technique is an overall finite difference method where the analysis is repeated for perturbed designs. The second type of technique is termed semianalytical because it involves direct, analytical differentiation of the equations of motion with finite difference approximation of the coefficient matrices. To be computationally practical in large-order problems, the overall finite difference methods must use the approximation vectors from the original design in the analyses of the perturbed models...|$|R
