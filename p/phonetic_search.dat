24|12|Public
50|$|<b>Phonetic</b> <b>Search</b> Technology (PST) is {{a method}} of speech recognition. An audio signal of speech is broken down into series of phonemes, {{which can be used}} to {{identify}} words.A string of six phonemes for example, “_B _IY _T _UW _B _IY,” represent the acronym “B2B”.|$|E
50|$|Some {{search engines}} can search {{recorded}} speech such as podcasts, though {{this can be}} difficult if there is background noise. Around 40 phonemes exist in every language with about 400 in all spoken languages. Rather than applying a text search algorithm after speech-to-text processing is completed, some engines use a <b>phonetic</b> <b>search</b> algorithm to find results within the spoken word. Others work by listening to the entire podcast and creating a text transcription.|$|E
50|$|Around 40 phonemes {{exist in}} every {{language}} with about 400 in all spoken languages. Rather than applying a text search algorithm after speech-to-text processing is completed, some engines use a <b>phonetic</b> <b>search</b> algorithm to find results within the spoken word. Others work by literally {{listening to the}} entire podcast and creating a text transcription using a sophisticated speech-to-text process. Once the text file is created, the file can be searched {{for any number of}} search words and phrases.|$|E
50|$|PMB {{has its own}} <b>search</b> engine, {{supporting}} <b>phonetic</b> <b>searches</b> without needing any complementary search engine.|$|R
3000|$|This {{technique}} of best <b>phonetic</b> substring <b>search</b> {{is used for}} both GMM-based and MLP-based system. Nevertheless, the [...]...|$|R
40|$|We review {{various issues}} related to {{techniques}} for searching through collections of trademarks for phonetic and other verbal similarities. Based on recent case studies, we survey judicial arbitrations that have established whether trademarks are similar or not and comment on popular algorithms for performing approximate searches, both phonetically and based on letter sequences. The particularities of international registrations and multilingual trademarks are also discussed. Overall, fuzzy searching for verbal similarities is a difficult task that calls for a cautious attitude. No solution can guarantee a fully automatic procedure. A manual validation of results will always need to be performed. Trademarks <b>Phonetic</b> <b>searches</b> Verbal similarity WIPO...|$|R
40|$|We are {{interested}} in retrieving information from speech data using <b>phonetic</b> <b>search.</b> We show improvement by expanding the query phonetically using a joint maximum entropy N-gram model. The value {{of this approach is}} demonstrated on Broadcast News data from NIST 2006 Spoken Term Detection evaluation. Index Terms: spoken information retrieval, <b>phonetic</b> <b>search,</b> query expansion. ...|$|E
40|$|<b>Phonetic</b> <b>Search</b> Methods for Large Databases” {{focuses on}} Keyword Spotting (KWS) within large speech databases. The brief will begin by {{outlining}} the challenges associated with Keyword Spotting within large speech databases using dynamic keyword vocabularies. It will then continue by highlighting the various market segments {{in need of}} KWS solutions, as well as, the specific requirements of each market segment. The work also includes {{a detailed description of}} the complexity of the task and the different methods that are used, including the advantages and disadvantages of each method and an in-depth comparison. The main focus will be on the <b>Phonetic</b> <b>Search</b> method and its efficient implementation. This will include a literature review of the various methods used for the efficient implementation of <b>Phonetic</b> <b>Search</b> Keyword Spotting, with an emphasis on the authors’ own research which entails a comparative analysis of the <b>Phonetic</b> <b>Search</b> method which includes algorithmic details. This brief is useful for researchers and developers in academia and industry from the fields of speech processing and speech recognition, specifically Keyword Spotting...|$|E
40|$|Abstract. In <b>phonetic</b> <b>search,</b> {{the goal}} is to find in a text all words with the same {{pronunciation}} as the search phrase. The user writes the word down using a different alphabet and transcription rules. Mrázová et al. proposed a new method for <b>phonetic</b> <b>search</b> based on searching for all possible transcriptions with Aho-Corasick automata [8]. Their algorithm offers better precision than the previous existing algorithms for <b>phonetic</b> <b>search,</b> but the size of the search automata grows exponentially with the length of the search phrase. In this paper, we examine the utilisation of the states of the automata created by this approach, and we discuss a memory-efficient implementation of the automata exploiting the fact that many states of the automaton are never visited during the search phase. We demonstrate that this implementation is comparable in search speed to the original automata, but it leads to significant memory savings...|$|E
5000|$|Built-in Functions [...] - [...] table <b>search,</b> <b>phonetic</b> conversion, field verify, field edit, bit checking, input formatting, {{weighted}} retrieval.|$|R
5000|$|PhraseFind - {{analyzes}} {{clips and}} indexes all dialog phonetically allowing text search of spoken words. (with Nexidia <b>phonetic</b> indexing and <b>search)</b> (discontinued in v8) ...|$|R
50|$|Recognizing {{that there}} were many {{spelling}} variants of Eastern European Jewish surnames, even though they sounded similar, Mokotoff collaborated with Randy Daitch to create the Daitch-Mokotoff Soundex, system which provides a <b>phonetic</b> alternative to <b>searching</b> databases of names.|$|R
40|$|In {{this paper}} we examine an {{alternative}} interface for <b>phonetic</b> <b>search,</b> namely query-by-example, that avoids OOV {{issues associated with}} both standard word-based and <b>phonetic</b> <b>search</b> methods. We develop three methods that compare query lattices derived from example audio against a standard ngrambased phonetic index and we analyze factors affecting the performance of these systems. We show that the best systems under this paradigm are able to achieve 77 % precision when retrieving utterances from conversational telephone speech and returning 10 results from a single query (performance that {{is better than a}} similar dictionary-based approach) suggesting significant utility for applications requiring high precision. We also show that these systems can be further improved using relevance feedback: By incorporating four additional queries the precision of the best system can be improved by 13. 7 % relative. Our systems perform well despite high phone recognition error rates (> 40 %) and make use of no pronunciation or letter-to-sound resources...|$|E
30|$|Phonetics-based {{approaches}} {{allow for}} a high speed spotting and OOV detection, but the system's performance suffers {{from a lack of}} linguistic information that help distinguish targeted terms from phonetically close utterances [19], especially on short phonetic sequences [11]. Therefore, many authors proposed hybrid approaches that combine <b>phonetic</b> <b>search</b> and ASR-based detection in off-line detection systems, in both spotting and STD contexts [20 – 23].|$|E
40|$|This paper {{presents}} {{a strategy for}} enabling speech recognition to be performed in the cloud whilst preserving the privacy of users. The approach advocates a demarcation of responsibilities between the client and server-side components for performing the speech recognition task. On the client-side resides the acoustic model, which symbolically encodes the audio and encrypts the data before uploading to the server. The server-side then employs searchable encryption to enable the <b>phonetic</b> <b>search</b> of the speech content. Some preliminary results for speech encoding and searchable encryption are presented...|$|E
40|$|The article {{describes}} the computerised trademark search system developed in 1984 - 1985 as {{the first stage of}} the USPTO's automation plan, and now fully operational. The system, called 'T-Search' includes not only automatic <b>phonetic</b> <b>searching,</b> but also capabilities of full text search with indexing features and couples them with the capability to search the figurative elements of trademarks. The database contains records of pending applications and registered trademarks, both those in force and those abandoned, cancelled or expired after 5 March, 1984, and the electronic data includes a 'HELP' facility explaining how the system is not to be used. There are records in the system of almost three quarters of a million trademarks, of which {{about a quarter of a}} million include figurative elements, and the database is updated daily. The system currently supports 70 workstations and 33 graphic printers. Retrieval is on the basis of registration or serial numbers, design mark codes, wordmarks, phonetics or syllables, and search terms may be related by Boolean operators and may be truncated on either left or right for root word search. A typical example of a search by a trademark examiner is described. ...|$|R
50|$|The {{dictionary}} function allows {{the user to}} input Chinese characters or pinyin (including the <b>phonetic</b> marking) to <b>search.</b> The user can also highlight words in a text and search {{for them in the}} dictionary. Wenlin has its own built-in dictionaries, but user-created dictionaries can be uploaded into the software.|$|R
50|$|Christian Engström {{was born}} in Högalid, Stockholm. He {{graduated}} from Stockholm University in 1983 {{with a degree in}} mathematics and computer science. While studying, Engström worked as a tutor at the university, teaching object-oriented programming in Simula. From 1978 he also worked part-time as a programmer at Skriptor, a small company which specialized in <b>phonetic</b> similarity <b>searches</b> for trademark names. After finishing his studies he started working full-time at the company. He became a partner in the firm in 1987 and in 1991 he became vice president. In 1997 the company was sold to the leading European trademark search house CompuMark. Engström stayed on in a similar capacity as before until 2001, when he left the company to set up his own consultancy firm Glindra AB.|$|R
40|$|This paper {{details the}} {{submission}} from the Speech and Audio Research Lab of Queensland University of Technology (QUT) to the inaugural 2006 NIST Spoken Term Detection Evaluation. The task involved accurately locating the occurrences of a specified list of English terms {{in a given}} corpus of broadcast news and conversational telephone speech. The QUT system uses phonetic decoding and Dynamic Match Lattice Spotting to rapidly locate search terms, combined with a neural network-based verification stage. The use of <b>phonetic</b> <b>search</b> means the system is open vocabulary and performs usefully (Actual Term-Weighted Value of 0. 23) whilst avoiding {{the cost of a}} large vocabulary speech recognition engine...|$|E
40|$|The {{original}} WAIS implementation by Thinking Machines et al. treats documents as uniform bags of erms. Since most documents exhibit some internal structure, it {{is desirable}} to provide the user means to exploit this structure in his queries. In this paper, we present extensions to the [freeWAIS][1] indexer and server, which allow access to documents structure using the original WAIS protocol. Major extensions include: arbitrary document formats, search in individual structure elements, stemming and <b>phonetic</b> <b>search,</b> support of 8 -bit character sets, numeric concepts and operators, combination of Boolean and linear retrieval. We also present an WWW-WAIS gateway specially tailored for usage with [freeWAIS-sf], which transforms filled out HTML forms to the new query syntax...|$|E
40|$|International audienceRecognition of Proper Names (PNs) {{in speech}} is {{important}} for content based indexing and browsing of audio-video data. However, many PNs are Out-Of-Vocabulary (OOV) words nfor LVCSR systems used in these applications due to the diachronicnature of data. By exploiting semantic context of the audio, relevant OOV PNs can be retrieved and then the target PNs can be recovered. To retrieve OOV PNs, we propose to represent their context with document level semantic vectors; and show that this approach is able to handle less frequent OOV PNs in the training data. We study different representations, including Random Projections, LSA, LDA, Skip-gram, CBOW and GloVe. A further evaluation of recovery of target OOV PNs using a <b>phonetic</b> <b>search</b> shows that document level semantic context is reliable for recovery of OOV PNs...|$|E
40|$|Searching for {{relevant}} {{material in}} documents containing transcription errors presents new challenges for Information Retrieval. This paper examines information retrieval effectiveness on a corpus of spoken broadcast news documents. For documents transcribed using speech recognition, {{a substantial number}} of retrieval errors are due to query terms that occur in the spoken document, but are not transcribed because they are not within the speech recognition system’s lexicon, even if that lexicon contains twenty thousand words. It has been shown that a <b>phonetic</b> lattice <b>search</b> in conjunction with full word search regains some of the information lost due to out-of-vocabulary words. In this paper an efficient alternative to this search is proposed that does not require a complete search of the phoneme lattices for all documents at run-time. By using fixed length strings of phonemes instead o...|$|R
40|$|This project {{utilizes}} {{speech recognition}} Application Program Interface (API) together with <b>phonetic</b> algorithms to <b>search</b> Stockholm's restaurant names via Google Glass with higher precision. This project considers {{the ability of}} phonetic algorithms and N-gram analyzer to retrieve the word {{and how it can}} be combined with automatic speech recognition to find the correct match. Significantly, the combination of these algorithms and the Google Glass limitation, e. g. its smallscreen, makes using a phonnetic filtering algorithm very helpful in getting better results...|$|R
40|$|In this paper, {{we report}} our recent {{research}} {{aimed at improving}} the pronunciation-modeling component of a speech recognition system designed for mobile voice search. Our new discriminative learning technique overcomes the limitation of the traditional ways of introducing alternative pronunciations that often enlarge confusability across different lexical items. Instead, we make use of a phonetic recognizer to generate pronunciation candidates, which are then evaluated and selected using the global minimum-classificationerror measure, guaranteeing a reduction of the training-set error rate after introducing alternative pronunciations. A maximum entropy approach is subsequently used to learn the weight parameters of the selected pronunciation candidates. Our experimental results demonstrate {{the effectiveness of the}} discriminative pronunciation learning technique in a real-world speech recognition task where pronunciation of business names presents special difficulty for high-accuracy speech recognition. Index Terms — Pronunciation modeling, discriminative learning, MCE objective function, <b>phonetic</b> decoding, greedy <b>search</b> 1...|$|R
40|$|The point {{process model}} (PPM) for keyword search is a whole-word {{parametric}} modeling framework {{based on the}} timing of phonetic events rather than the evolution of frame-level pho-netic likelihoods. Recent progress in PPM training and decod-ing algorithms has yielded state-of-the-art <b>phonetic</b> <b>search</b> per-formance in high-resource settings, {{both in terms of}} accuracy and computational efficiency. In this paper, we consider PPM application to low-resource settings where the amount of tran-scribed speech is severely limited and the pronunciation dic-tionary is incomplete. By using (i) state-of-the-art deep neu-ral network acoustic models to generate phonetic events and (ii) grapheme-to-phoneme conversion to generate pronuncia-tions for out-of-vocabulary (OOV) keywords, we find the PPM system reaches state-of-the-art OOV search performance at a small computational cost. Moreover, due to their complemen-tary methodologies, combining PPM outputs with the LVCSR baseline produces average relative ATWV improvements of 7...|$|E
40|$|We {{describe}} a robust, N [...] best formant tracker. The 2 stage algorithm initially finds single formants or parts thereof. In {{the second stage}} a robust dynamic programming search with a wild card mechanism is employed to find the N best consistent interpretation of the initial formant information. The selection of the correct formant tracks is delayed until after the <b>phonetic</b> <b>search,</b> thus overcoming the lack of robustness of traditional formant trackers by delaying the final decision until after phonemic classification. 1. INTRODUCTION We are building a knowledge [...] based, segmental speech recognition system. Such systems have traditionally used cepstral, spectral or related features {{as a basis for}} segmentation and classification [1, 2]. However, from our experience with spectrogram reading, we know that formants are the single most important source of evidence for the classification of phonetic segments. Formants (especially their relative positioning) are the primary indicator for the cl [...] ...|$|E
30|$|Larger {{datasets}} {{might not}} be presentable in a single table row. Possible approaches would be folding out entries on demand or displaying detailed data of a marked entry in an overlay panel. Search options have to be available and should support <b>phonetic</b> <b>search.</b> By doing this, search results would be returned that sound similar to the given term. This might be important because incident commanders might not know correct spelling of some search terms, e.g. last names, or make typing errors while having serious time pressure. Moreover, in favor of searching by entering data freely, filter mechanisms should be implemented, if the range of values is limited. They would only offer proper values. Activated filters must be clearly visible, e.g. by changed background colors or other visual hints. Otherwise, the subset of displayed datasets could be perceived as the total set. Temporarily marking favorites, e.g. in terms of patients to remember, can be a feature to relieve incident commanders’ working memory {{and allow them to}} continue tasks later on more easily.|$|E
3000|$|Generally, subword-level {{decoding}} {{consists in}} a fast acoustic matching between the signal and the phonetic or syllabic transcription {{of the term}} [13, 14]. Various developments of this idea were evaluated in the past, {{with the purpose of}} being able to detect OOV and of improving system robustness [12]. In [5], the <b>phonetic</b> <b>search</b> integrates the phoneme confusion matrix in order to limit the impact of recognition errors. Other authors combine complementary acoustic scoring methods, for example, Gaussian mixture models (GMMs) and multilayer perceptrons- (MLP-) based estimators [15, 16]. In [6], the authors propose, {{in the context of the}} STD task, to estimate the scores of the OOV by combining the posterior probabilities of their phonetic substrings. Therefore, many of the fast wordspotters are based on off-line phonetic matching. They generally use two models representing, respectively, the targeted word (or term) and the [...] "garbage,'' the latter aiming to [...] "absorb'' all nontargeted utterances [16 – 18]. These models are built from Hidden Markov Models (HMMs) representing sublexical units, typically phonemes or triphones.|$|E
40|$|Today, most systems use large {{vocabulary}} {{continuous speech}} recognition tools to produce word transcripts which have indexed transcripts and query terms retrieved from the index. However, query {{terms that are}} not part of the recognizer’s vocabulary cannot be retrieved, thereby affecting the recall of the search. Such terms can be retrieved using <b>phonetic</b> <b>search</b> methods. Phonetic transcripts can be generated by expanding the word transcripts into phones using the baseforms in the dictionary. In addition, advanced systems can provide phonetic transcripts using sub-word based language models. However, these phonetic transcripts suffer from inaccuracy and do not provide a good alternative to word transcripts. We demonstrate how to retrieve information from speech data by presenting a novel approach for vocabulary independent retrieval combining search on transcripts that are produced according to different word and sub-word decoding methods. We present two different algorithms: the first is based on the Threshold Algorithm (TA); the second uses a Boolean retrieval model on inverted indices. The value of this combination is demonstrated on data from NIST 2006 Spoken Term Detection evaluation...|$|E
30|$|The main {{objective}} of the work {{presented in this paper}} was to develop a complete system that would accomplish the original visions of the MALACH project. Those goals were to employ automatic speech recognition and information retrieval techniques to provide improved access to the large video archive containing recorded testimonies of the Holocaust survivors. The system has been so far developed for the Czech part of the archive only. It takes advantage of the state-of-the-art speech recognition system tailored to the challenging properties of the recordings in the archive (elderly speakers, spontaneous speech and emotionally loaded content) and its close coupling with the actual search engine. The design of the algorithm adopting the spoken term detection approach is focused on the speed of the retrieval. The resulting system is able to search through the 1, 000 h of video constituting the Czech portion of the archive and find query word occurrences in the matter of seconds. The <b>phonetic</b> <b>search</b> implemented alongside the search based on the lexicon words allows to find even the words outside the ASR system lexicon such as names, geographic locations or Jewish slang.|$|E
40|$|By {{the late}} 90 ?s Brazil {{experienced}} the ?boom? of Internet although its use {{was restricted to}} the universities and large companies. In this context the Parliament {{of the state of}} Rio Grande do Sul (Southern Brazil) has sponsored the development of a tool for publishing its contents. That tool should have some characteristics like being portable, ?plug-and-play?. Responding to the demand has emerged EVM. net (acronym for modular virtual structure on the Internet), a GPL integrated solution for publishing and managing web contents. EVM. net follows a distributed information architecture, each user is responsible for maintaining his contents. The system has been conceived to be scaleable: its architecture has been planned following the metaphor of a building. The foundations contain the security tools and the persistence model, while the other stages are customized for each demand. EVM. net also contains algorithms for query optimization and <b>phonetic</b> <b>search</b> for Brazilian Portuguese. Recently the tool had been used to implement the Extensive Communication Model, which is based on interactivity, hypertextuality, and hypermediation. Currently, EVM. net is being used at many institutions like the University of Brasilia, the Federal Senate of Brazil and the Brasilia?s Federation of Industries...|$|E
40|$|The main {{objective}} of the work {{presented in this paper}} was to develop a complete system that would accomplish the original visions of the MALACH project. Those goals were to employ automatic speech recognition and information retrieval techniques to provide improved access to the large video archive containing recorded testimonies of the Holocaust survivors. The system has been so far developed for the Czech part of the archive only. It takes advantage of the state-of-the art speech recognition system tailored to the challenging properties of the recordings in the archive (elderly speakers, spontaneous speech, emotionally loaded content) and its close coupling with the actual search engine. The design of the algorithm adopting the spoken term detection approach is focused on the speed of the retrieval. The resulting system is able to search through the 1, 000 hours of video constituting the Czech portion of the archive and find query word occurrences in the matter of seconds. The <b>phonetic</b> <b>search</b> implemented alongside the search based on the lexicon words allows to find even the words outside the ASR system lexicon such as names, geographic locations or Jewish slang...|$|E
3000|$|The plots {{reveal that}} the number of false alarms is {{essentially}} the same for search in 1 -best ASR output (i.e., the situation when only the best path through the lattice is retained) and search in the word lattice, up {{to the point where the}} 1 -best system is not able to provide any more correct hits and the lattice search becomes the clear winner. Searching in the phoneme lattice, on the other hand, produces substantially more false alarms than both the word-based algorithms, yet its big advantage lies in the ability to search for the words that are missing from the ASR lexicon. Those missing words are often some rare personal and place names and just as they are underrepresented in the language model training data, they are also very important to the searchers of the collection [28] (in fact, two-thirds of the requests specified named entities in the preliminary MALACH user studies). The <b>phonetic</b> <b>search</b> mode also allows users to type only an [...] "approximate" [...] spelling of the searched query which is extremely helpful especially in the case of foreign words or even words that are transliterated from different alphabets and it is not clear what spelling variant (if any) appears in the ASR lexicon.|$|E
40|$|For {{efficient}} {{organization of}} speech recordings – meetings, interviews, voice mails, and lectures – {{being able to}} search for spoken keywords is essential. Today, most spoken document retrieval systems use large-vocabulary recognition. For the above scenarios, such systems suffer from the unpredictable domain, out-ofvocabulary queries, and generally high word-error rate (WER). In [1], we presented a system for phonetic indexing and searching of spontaneous speech. It is vocabulary-independent and based on phoneme lattices. In the present paper, we propose to combine it with word-based search into a hybrid approach. We explore two methods of combination: posterior combination (merging search results of a word-based and a phoneme-based system) and prior combination (combining word and phoneme language models and vocabularies to form a hybrid recognizer). The search accuracy of our best purely phonetic baseline is 64 % (Figure of Merit), and our purely word-based baselines are below 50 %. The new hybrid approach achieves 73 %, if the recognizer uses a language model that matches the test-set domain. With a mismatched language model, 71 % is achieved. Our {{results show that the}} proposed hybrid model benefits from the best of two worlds: Word-level language context and robustness of <b>phonetic</b> <b>search</b> to unknown words and domain mismatch. 1...|$|E
40|$|Finding {{the right}} {{information}} from the increasing amount of data on the Internet is not easy. This is why most people use search engines because they make searching less difficult {{with a variety of}} techniques. In this thesis, we address one of them called phonetic matching. The idea is to look for documents in a document set based on not only the spellings but their pronunciations as well. It is useful when a query contains spelling mistakes or a correctly spelled one does not return enough results. In these cases, phonetic matching can fix or tune up the original query by replacing some or all query words with the new ones that are phonetically similar, and hopefully achieve more hits. We propose the design of such a search system for short text documents. It allows for single- and multiple-word queries to be matched to sound-like words or phrases contained in a document set and sort the results in terms of their relevance to the original queries. Our design differs from many existing systems in that, instead of relying heavily on a set of extensive prior user query logs, our system makes search decisions mostly based on a relatively small dictionary consisting of organized metadata. Our goal is to make it suitable for start-up document sets to have the comparable <b>phonetic</b> <b>search</b> ability as those of bigger databases, without having to wait till enough historical user queries are accumulated. ii Table of Contents Abstract [...] . i...|$|E
40|$|This thesis {{investigates the}} use of {{explicit}} speech knowledge in computer speech-recognition. Speech knowledge is generally {{expressed in terms of}} acoustic events occurring near phonetic segment boundaries and the location, shape and dynamics of formant trajectories. This suggests the creation of a segment-based recognition framework and {{the use of}} explicit formant features in a flexible integration scheme to ultimately improve the phonetic recognition accuracy. We describe a segmentation algorithm that produces a lattice of segment hypotheses, each with an associated broad phonetic identity. We build a single phonetic segment classifier along with separate vowel/semi-vowel and consonant classifiers based on traditional cepstral features paying attention to reducing the mismatch between training and deployment conditions. We develop a robust, N-best formant tracking algorithm that generates a list of up to N consistent formant interpretations. The use of the N best feature paradigum is based on the observation that there are generally only a handful of reasonable interpretation of the given formant information. Instead of finding the best formant interpretation {{through the use of a}} global cost function that includes energy maximization and smoothness terms, we delay the selection of the correct formant interpretation until after the segment classification and <b>phonetic</b> <b>search.</b> We use the formant interpretations to extract features for a vowel/semi-vowel segment classifier. The formant trajectories are approximated either by three line segments or by a third-order Legendre polynomial. We show that together with formant amplitude, formant bandwidth, pitch, and segment durations we can produce a classifier of comparable performance to a cepstral-based classifier. We further demonstrate the potential of the N best classification paradigm and show that a combination of formant and cepstral features further improves the classification accuracy. Finally, the validity of the entire approach of using a segment-based approach, separate classifiers for vowels and consontans, and explicit formant features is verified by phonetic recognition experiments...|$|E
