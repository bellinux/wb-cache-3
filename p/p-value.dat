10000|4342|Public
5|$|The {{problem of}} {{induction}} discussed above {{is seen in}} another form in debates over the foundations of statistics. The standard approach to statistical hypothesis testing avoids claims about whether evidence supports a hypothesis or makes it more probable. Instead, the typical test yields a <b>p-value,</b> which is the probability of the evidence being such as it is, {{under the assumption that}} the hypothesis being tested is true. If the <b>p-value</b> is too low, the hypothesis is rejected, in a way analogous to falsification. In contrast, Bayesian inference seeks to assign probabilities to hypotheses. Related topics in philosophy of statistics include probability interpretations, overfitting, and the difference between correlation and causation.|$|E
5|$|The {{most common}} {{approach}} of GWA studies is the case-control setup, which compares two {{large groups of}} individuals, one healthy control group and one case group affected by a disease. All individuals in each group are genotyped {{for the majority of}} common known SNPs. The exact number of SNPs depends on the genotyping technology, but are typically one million or more. For each of these SNPs it is then investigated if the allele frequency is significantly altered between the case and the control group. In such setups, the fundamental unit for reporting effect sizes is the odds ratio. The odds ratio is the ratio of two odds, which in the context of GWA studies are the odds of disease for individuals having a specific allele and the odds of disease for individuals who do not have that same allele. When the allele frequency in the case group is much higher than in the control group, the odds ratio is higher than 1, and vice versa for lower allele frequency. Additionally, a <b>P-value</b> for the significance of the odds ratio is typically calculated using a simple chi-squared test. Finding odds ratios that are significantly different from 1 is the objective of the GWA study because this shows that a SNP is associated with disease.|$|E
25|$|This {{probability}} is the <b>p-value,</b> considering only extreme {{results that}} favor heads. This {{is called a}} one-tailed test. However, the deviation can be in either direction, favoring either heads or tails. The two-tailed <b>p-value,</b> which considers deviations favoring either heads or tails, may instead be calculated. As the binomial distribution is symmetrical for a fair coin, the two-sided <b>p-value</b> is simply twice the above calculated single-sided p-value: the two-sided <b>p-value</b> is 0.115.|$|E
40|$|The {{customary}} use of <b>P-values</b> {{in scientific}} {{research has been}} attacked as being ill-conceived, and the utility of <b>P-values</b> has been derided. This paper reviews common misconceptions about <b>P-values</b> and their alleged deficits as indices of experimental evidence and, using an empirical exploration of the properties of <b>P-values,</b> documents the intimate relationship between <b>P-values</b> and likelihood functions. It is shown that <b>P-values</b> quantify experimental evidence not by their numerical value, but through the likelihood functions that they index. Many arguments against the utility of <b>P-values</b> are refuted and the conclusion is drawn that <b>P-values</b> are useful indices of experimental evidence. The widespread use of <b>P-values</b> in scientific research is well justified by the actual properties of <b>P-values,</b> but those properties {{need to be more}} widely understood. Comment: 31 pages, 9 figures and R cod...|$|R
40|$|Cochran’s Q assesses {{treatment}} {{differences in}} randomized block designs with binary data. We suggest using bootstrap <b>p-values</b> rather than <b>p-values</b> {{based on the}} chi-squared distribution for tests based on Q. These chi-squared <b>p-values</b> for Q {{are the only ones}} usually given in statistical software and can be inaccurate. The same approach allows improved <b>p-values</b> to be given for sparse two-way cross-classification data...|$|R
30|$|The <b>P-values</b> were two-tailed and {{considered}} significant when < 0.05. Nominal <b>P-values</b> were presented without adjustment for multiple testing.|$|R
25|$|While in {{principle}} the acceptable level of statistical significance {{may be subject}} to debate, the <b>p-value</b> is the smallest significance level that allows the test to reject the null hypothesis. This is logically equivalent to saying that the <b>p-value</b> is the probability, assuming the null hypothesis is true, of observing a result at least as extreme as the test statistic. Therefore, the smaller the <b>p-value,</b> the lower the probability of committing type I error.|$|E
25|$|In {{statistic}}s, when a <b>p-value</b> {{is used as}} a {{test statistic}} for a simple null hypothesis, and the distribution of the test statistic is continuous, then the <b>p-value</b> is uniformly distributed between 0 and 1 if the null hypothesis is true.|$|E
25|$|Suppose a {{researcher}} flips a coin some arbitrary {{number of times}} (n) and assumes a null hypothesis that the coin is fair. The test statistic is {{the total number of}} heads and is a two-tailed test. Suppose the researcher observes heads for each flip, yielding a test statistic of n and a <b>p-value</b> of 2/2n. If the coin was flipped only 5 times, the <b>p-value</b> would be 2/32 = 0.0625, which is not significant at the 0.05 level. But if the coin was flipped 10 times, the <b>p-value</b> would be 2/1024 ≈ 0.002, which is significant at the 0.05 level.|$|E
40|$|A QQ {{plot was}} {{generated}} for the pooled association results by plotting the pooling-based <b>p-values</b> versus <b>p-values</b> for a null distribution on a log-log scale. Typically, one expects higher genomic inflation values under a pooling-based study design since <b>p-values</b> include both population based stratification and assay-based stratification...|$|R
40|$|Statistical {{practice}} {{often requires}} combining evidence from independent sources. A popular {{approach is to}} combine <b>p-values.</b> Motivated by the observation that <b>p-values</b> under the alternative are stochastically smaller than <b>p-values</b> under the null, we develop new combination rules that explicitly account for this ordering. The new combination rules are broadly applicable, optimal against some alternatives, and highly efficient against optimal procedures. Combining functions Optimality <b>p-values</b> Restricted inference Stochastic order relations Testing uniformity...|$|R
40|$|A/B {{tests are}} {{typically}} analyzed via <b>p-values</b> and confidence intervals; but these inferences are wholly unreliable if users make decisions while continuously monitoring their tests. We define always valid <b>p-values</b> that let users {{try to take}} advantage of data as fast as it becomes available, providing valid statistical inference whenever they make their decision. Always valid <b>p-values</b> can be interpreted as the natural <b>p-values</b> corresponding to a sequential hypothesis test. Through this connection we derive always valid <b>p-values</b> with good detection properties. Notably, we also extend our approach to address multiple hypothesis testing in the sequential setting. Our methodology has been implemented in a large scale commercial A/B testing platform, from which we present empirical results...|$|R
25|$|For {{statistical}} hypothesis testing this function {{is used to}} construct the <b>p-value.</b>|$|E
25|$|The test {{statistic}} is approximately 1.959, which gives a 2-tailed test <b>p-value</b> of 0.09077.|$|E
25|$|The test {{statistic}} is approximately equal to 1.959, which gives 2-sided <b>p-value</b> of 0.07857.|$|E
40|$|LMGene calls {{function}} genediff {{to calculate}} the unadjusted gene-specific and posterior <b>p-values</b> of all genes and then calculates the FDR-adjusted <b>p-values</b> of all genes. Significant genes for each factor in model (based on either the gene-specific or posterior FDR-adjusted <b>p-values)</b> are output. LMGene(eS, model = NULL, level = 0. 05, posterior = FALSE, method = c("MLE", "MOM", "MOMlog") ...|$|R
3000|$|The <b>p-values</b> of {{this test}} are {{represented}} on Fig.  1. Surprisingly, {{the distribution of}} the <b>p-values</b> shows that the resulting test is not conservative enough. Indeed, the test will reject [...]...|$|R
40|$|The higher {{criticism}} {{of a family}} of tests starts with the individual uncorrected <b>p-values</b> of each test. It then requires a procedure for deciding whether the collection of <b>p-values</b> indicates the presence of a real effect and if possible selects the ones that deserve closer scrutiny. This paper investigates procedures in which the ordered <b>p-values</b> are compared to an arbitrary positive and non-decreasing threshold sequence...|$|R
25|$|In {{both cases}} the {{data suggest that the}} null {{hypothesis}} is false (that is, the coin is not fair somehow), but changing the sample size changes the <b>p-value.</b> In the first case, the sample size is not large enough to allow the null hypothesis to be rejected at the 0.05 level (in fact, the <b>p-value</b> can never be below 0.05 for the coin example).|$|E
25|$|If the <b>p-value</b> is {{not less}} than the {{required}} significance level (equivalently, if the observed test statistic is outside the critical region), then the test has no result. The evidence is insufficient to support a conclusion. (This is like a jury that fails to reach a verdict.) The researcher typically gives extra consideration to those cases where the <b>p-value</b> is close to the significance level.|$|E
25|$|The <b>p-value</b> was devised as an informal, but objective, index {{meant to}} help a {{researcher}} determine (based on other knowledge) whether to modify future experiments or strengthen one's faith in the null hypothesis. Hypothesis testing (and Type I/II errors) was devised by Neyman and Pearson as a more objective alternative to Fisher's <b>p-value,</b> also meant to determine researcher behaviour, but without requiring any inductive inference by the researcher.|$|E
40|$|We {{introduce}} {{a method for}} computation of exact conditional efficiency robust enumeration <b>p-values</b> for detection of genotype [...] phenotype associations at a single bi-allelic genetic locus. Our method can be based on any arbitrary ranking test statistics, such as efficiency robust test statistics or asymptotic <b>p-values.</b> The resulting <b>p-values</b> are exact conditional enumeration <b>p-values</b> and satisfy the basic statistical validity property. Practically, the method allows performing statistically valid significance testing in genomic analyses with unknown modes of inheritance at individual bi-allelic genetic loci [...] the situation typical in genome-wide association studies. We provide an open-source R code implementing the method...|$|R
40|$|Abstract. We {{present a}} novel dynamic {{programming}} framework that {{allows one to}} compute tight upper bounds for the <b>p-values</b> of gapped local alignments in pseudo–polynomial time. Our algorithms are fast and simple and unlike most earlier solutions, require no curve fitting by sampling. Moreover, our new methods do not suffer from the so–called edge effects, a by–product of the common practice used to compute <b>p-values.</b> These new methods also provide {{a way to get}} into very small <b>p-values,</b> that are needed when comparing sequences against large databases. Based on our experiments, accurate estimates of small <b>p-values</b> are difficult to get by curve fitting. ...|$|R
50|$|For small samples the chi-squared {{approximation}} is overly sensitive, often {{rejecting the}} null hypothesis when it is true. Furthermore, the distribution of <b>p-values</b> departs from a uniform distribution and becomes a right-skewed uni-modal distribution, especially for small <b>p-values.</b> This leads to a large Type I error rate. The table below shows some <b>p-values</b> approximated by a chi-squared distribution that differ from their true alpha levels for small samples.|$|R
25|$|Two-tailed <b>p-value</b> of {{observation}} O given H0 = 2*min(Prob(no. of heads ≥14heads), Prob(no. of heads ≤14heads))= 2*min(0.058, 0.978) = 2*0.058 = 0.115.|$|E
25|$|Calculate the <b>p-value.</b> This is the probability, {{under the}} null hypothesis, of {{sampling}} a test statistic {{at least as}} extreme as that which was observed.|$|E
25|$|However, had {{one more}} head been obtained, the {{resulting}} <b>p-value</b> (two-tailed) would have been0.0414(4.14%). The null hypothesis is rejected when a 5% cut-off is used.|$|E
40|$|Higher-order {{asymptotic}} {{theory is}} used to derive <b>p-values</b> that achieve superior accuracy compared to the <b>p-values</b> obtained from traditional tests for inference about parameters of the multinomial logit model. Simulations are provided to assess the finite sample behavior of the test statistics considered and to demonstrate {{the superiority of the}} higher-order method. Stata code that outputs these <b>p-values</b> is available to facilitate the implementation of these methods for the end-user. ...|$|R
40|$|Two {{procedures}} for checking Bayesian models are compared using a simple test problem {{based on the}} local Hubble expansion. Over four orders of magnitude, <b>p-values</b> derived from a global goodness-of-fit criterion for posterior probability density functions (Lucy 2017) agree closely with posterior predictive <b>p-values.</b> The former can therefore serve as an effective proxy for the difficult-to-calculate posterior predictive <b>p-values.</b> Comment: 4 pages, 3 figures. Submitted to Astronomy & Astrophysic...|$|R
40|$|Recent {{likelihood}} theory produces <b>p-values</b> {{that have}} remarkable accuracy and wide applicability. The calculations use familiar {{tools such as}} maximum likelihood values (MLEs), observed informations, and parameter rescaling. The usual evaluation of such <b>p-values</b> is by simulations, and such simulations do verify that the global distribution of the <b>p-values</b> is uniform(0, 1), to high accuracy in repeated sampling. The derivation of the <b>p-values</b> however asserts a stronger statement, {{that they have a}} uniform(0, 1) distribution conditionally, given identified precision information provided by the data. We take a simple regression example that involves exact precision information and use large sample techniques to extract highly accurate information as to the statistical position of the data point with respect to the parameter: specifically, we examine various <b>p-values</b> and Bayesian posterior survivor s-values for validity. With observed data we numerically evaluate the various <b>p-values</b> and s-values, and we also record the related general formulas. We then assess the numerical values for accuracy using Markov chain Monte Carlo (McMC) methods. We also propose some third-order likelihood-based procedure...|$|R
25|$|<b>P-value.</b> Defined as the {{probability}} {{measure of the}} complement of the ball with the hypothesized value as center point and chi distance as radius.|$|E
25|$|Reject {{the null}} hypothesis, {{in favor of}} the {{alternative}} hypothesis, if and only if the <b>p-value</b> is less than the significance level (the selected probability) threshold.|$|E
25|$|The <b>p-value</b> was {{introduced}} by Karl Pearson in the Pearson's chi-squared test, where he defined P (original notation) as {{the probability that the}} statistic would be at or above a given level. This is a one-tailed definition, and the chi-squared distribution is asymmetric, only assuming positive or zero values, and has only one tail, the upper one. It measures goodness of fit of data with a theoretical distribution, with zero corresponding to exact agreement with the theoretical distribution; the <b>p-value</b> thus measures how likely the fit would be this bad or worse.|$|E
40|$|Abstract. Recent {{likelihood}} theory produces <b>p-values</b> {{that have}} remarkable accuracy and wide applicability. The calculations use familiar {{tools such as}} maximum likelihood values (MLEs), observed information and parameter rescaling. The usual evaluation of such <b>p-values</b> is by simulations, and such simulations do verify that the global distribution of the <b>p-values</b> is uniform(0, 1), to high accuracy in repeated sampling. The derivation of the <b>p-values,</b> however, asserts a stronger statement, {{that they have a}} uniform(0, 1) distribution conditionally, given identified precision information provided by the data. We take a simple regression example that involves exact precision information and use large sample techniques to extract highly accurate information as to the statistical position of the data point with respect to the parameter: specifically, we examine various <b>p-values</b> and Bayesian posterior survivor s-values for validity. With observed data we numerically evaluate the various <b>p-values</b> and s-values, and we also record the related general formulas. We then assess the numerical values for accuracy using Markov chain Monte Carlo (McMC) methods. We also propose some third-order likelihood-based procedures for obtaining means and variances of Bayesian posterio...|$|R
40|$|A {{construction}} of <b>p-values</b> for hypothesis tests based on subsampling {{and the related}} m out of n bootstrap is introduced. The <b>p-values</b> {{are based on a}} modification of the usual subsampling hypothesis tests that involves an appropriate centering of the subsampled or bootstrapped test statistics as in the {{construction of}} confidence intervals. This modification makes the hypothesis tests more powerful, and as a consequence provides meaningful <b>p-values.</b> The procedure is shown to apply for both i. i. d. and stationary data satisfying mixing conditions. The new <b>p-values</b> have the desired properties of being asymptotically uniform under the null hypothesis and converging to zero under the alternative. ...|$|R
40|$|Higher order {{asymptotic}} {{theory is}} used to derive <b>p-values</b> that achieve superior accuracy compared to the <b>p-values</b> obtained from traditional tests for inference about parameters of the multinomial logit model. Simulations are provided to assess the finite sample behavior of the test statistics considered and to demonstrate {{the superiority of the}} higher order method. Stata code that outputs these <b>p-values</b> is available to facilitate the implementation of these methods for the end-user...|$|R
