2|10000|Public
40|$|Abstract — Fitting an ellipse to {{the iris}} {{boundaries}} {{accounts for the}} projective distortions present in off-axis images of the eye and <b>provides</b> <b>the</b> <b>contour</b> fitting necessary for the dimensionless mapping used in leading iris recognition algorithms. Previous iris segmentation efforts have either focused on fitting circles to pupillary and limbic boundaries or assigning labels to image pixels. This paper approaches the iris segmentation problem by adapting the Starburst algorithm to locate pupillary and limbic feature pixels used to fit a pair of ellipses. The approach is evaluated by comparing the fits to ground truth. Two metrics {{are used in the}} evaluation, the first based on the algebraic distance between ellipses, the second based on ellipse chamfer images. Results are compared to segmentations produced by ND IRIS over randomly selected images from the Iris Challenge Evaluation database. Statistical evidence shows significant improvement of Starburst’s elliptical fits over the circular fits on which ND IRIS relies. I...|$|E
40|$|Cardiovascular {{diseases}} {{are the major}} {{cause of death in}} the developed world. About half of these are due to ischemia heart diseases. The high death rate caused by coronary artery diseases increases the need for preliminary detection. Perfusion magnetic resonance imaging {{has turned out to be}} very promising for this purpose. A contrast agent is injected intravenously to visualize the perfusion. Due to the extremely time-consuming manual analysis of these relatively large datasets, efforts for automatic approaches have been introduced. Most of these proposed methods focus on parts of the analysis process. The present thesis identifies four steps for an automatic analysis approach: localization of the heart, suppression of motion artifacts, segmentation of the myocardium, and perfusion analysis. This thesis presents a method covering all these subtasks in an automatic manner with no need for any user interaction. First the acquired MR images are analyzed to roughly detect the heart. A registration step compensates motion artifacts based on the breathing of the patient. The segmentation step <b>provides</b> <b>the</b> <b>contour</b> of the myocardium at every time step. Based on these segmentations the perfusion i...|$|E
40|$|Abstract. As is well known, the {{stability}} of a dynamical system in two dimensions may be demonstrated in a very intuitive fashion from {{the existence of a}} suitable positive-definite Liapunov function, <b>providing</b> <b>the</b> <b>contours</b> of this function in a neighborhood of the stable point are Jordan curves. It is shown that the Liapunov function will certainly have this property if the stable point is an isolated stationary point {{in the sense of the}} Clarke calculus, but a counterexample is given if this assumption is weakened to the stable point being an isolated local extremum...|$|R
40|$|The {{last twenty}} years has seen {{the rise of a}} series of {{intellectual}} and practical responses to environmental degradation. Many socialists and critical theorists have sought to develop sophisticated analyses of ecological despoiling and have aimed to <b>provide</b> <b>the</b> <b>contours</b> of various ‘eco-socialist’ alternatives. These range from visions of small-scale communal autarky through ‘green’ or ‘eco-city’ concepts to global perspectives. Crucially, for most of these ecologically minded socialists, the social relations of capitalism rather than simply the ‘industrial mode of production’ {{has been the focus of}} critique. Where many liberal or reactionary environmentalists see the industrial processes of production and the wasteful activities of consumption as driving the planet towards ecological doom, most socialists seek to analyze the ways in which the relational social processes of capital augment, enlarge and exaggerate the ecological harm that results from industrial production and mass consumption...|$|R
40|$|This article {{presents}} {{a distance and}} angle similarity measure. The integrated similarity measure takes the strengths of both the distance and direction of mea-sured documents into account. This article analyzes {{the features of the}} similarity measure by comparing it with the traditional distance-based similarity measure and <b>the</b> cosine measure, <b>providing</b> <b>the</b> iso-similarity <b>contour,</b> investigating <b>the</b> impacts of the parameters and vari-ables on the new similarity measure. It also gives the further research issues on the topic...|$|R
30|$|Our {{motivation}} is to validate our predictive multiscale model of cosmetic outcome {{in a clinical}} study. This model <b>provides</b> <b>the</b> breast <b>contour</b> surface based {{on a combination of}} soft tissue mechanical deformation and a numerical simulation of healing. We look for a comparison of this prediction with the patient outcome at the millimeter scale. It is essential for the clinical study that our surface reconstruction system can be easily used by the nurse and/or medical doctor during the standard follow-up examinations of the patient and does not generate any additional cost.|$|R
50|$|A full {{thickness}} flap {{is usually}} elevated {{to a point}} apical to the desired area to be contoured, {{and according to the}} amount of bone needed to be removed, a bone file, or a bone rongeur, or a burr under copious irrigation can be used to <b>provide</b> <b>the</b> desired <b>contour.</b> Taking in consideration that lack of irrigation can lead to bone necrosis. When finished, the flap is repositioned and sutured. The alveolar mucosa covering bone should have uniform thickness, density and compressibility to evenly distribute the masticatory forces to the underlying bone.|$|R
40|$|We {{present a}} class of spherically {{symmetric}} random variables defined by the property that as dimension increases to infinity the mass becomes concentrated in a hyperspherical shell, the width of which is negligible compared to its radius. We provide a sufficient condition for this property {{in terms of the}} functional form of the density and then show that the property carries through to equivalent elliptically symmetric distributions, <b>provided</b> that <b>the</b> <b>contours</b> are not too eccentric, in a sense which we make precise. Individual components of such distributions possess a number of appealing Gaussian-like limit properties, in particular that the limiting one-dimensional marginal distribution along any component is Gaussian...|$|R
40|$|Available (free) at: [URL] audienceHigh {{water level}} at the coast {{may be the}} result of {{different}} combinations of offshore hydrodynamic conditions (e. g. wave characteristics, offshore water level, etc.). <b>Providing</b> <b>the</b> <b>contour</b> of <b>the</b> "critical" set of offshore conditions leading to high water level is of primary importance either to constrain early warning networks based on hydro-meteorological forecast or observations, or for the assessment of the coastal flood hazard return period. The challenge arises from the use of computationally intensive simulators, which prevent the application of a grid approach consisting in extracting <b>the</b> <b>contour</b> through <b>the</b> systematic evaluation of the simulator on a fine design grid defined in the offshore conditions domain. To overcome such a computational difficulty, we propose a strategy based on the kriging meta-modelling technique combined with an adaptive sampling procedure aiming at improving the local accuracy in the regions of the offshore conditions that contribute the most to the estimate of <b>the</b> targeted <b>contour.</b> This methodology is applied to two cases using an idealized site on the Mediterranean coast (southern France) : (1) a two-dimensional case to ease the visual analysis and aiming at identifying the combination of offshore water level and of significant wave height; (2) a more complex case aiming at identifying four offshore conditions (offshore water level and offshore wave characteristics: height, direction and period). By using a simulator of moderate computation time cost (a few tens of minutes), <b>the</b> targeted <b>contour</b> can be estimated using a cluster composed of a moderate number of computer units. This reference contour is then compared with the results of the meta-model-based strategy. In both cases, we show that the critical offshore conditions can be estimated with a good level of accuracy using a very limited number (of a few tens) of computationally intensive hydrodynamic simulations...|$|R
40|$|We {{present a}} novel method for shape {{modeling}} using an extended class of semiparametric skew-symmetric (SSS) distributions. Given several realizations {{of a simple}} shape, the proposed method models it as a joint distribution of angle and distance from the centroid of all points on the boundary. The model, called “Semiparametric Skew-Symmetric Shape Model ” (SSSM), is capable of capturing inherent variability of shapes <b>provided</b> <b>the</b> realization <b>contours</b> remain within a certain neighborhood range around a “mean ” with high probability. In this paper, we will discuss SSSM learning, classification through SSSM and sampling shapes from the learned models. 1...|$|R
40|$|This article <b>provides</b> <b>the</b> initial <b>contours</b> of an {{argument}} that uses International Law to challenge the validity of Israeli apartheid. It challenges the conventional discourse of legal debates on Israel’s actions and borders and seeks to link the illegalities of these actions to the validity of an inbuilt Israeli apartheid. The argument also connects the deontological doctrine of peremptory norms of International Law (jus cogens), the right of self-determination and the International Crime of Apartheid to the doctrine of state recognition. It applies these to the State of Israel and {{the vision of a}} single democratic state in historic Palestine...|$|R
40|$|International audienceThe {{combination}} of various residual stress measurement methods {{is a common}} practice to complete knowledge that a single measurement method cannot provide. In this study, incremental X-Ray diffraction is combined with <b>the</b> <b>contour</b> method to measure a bent notched specimen to study the methods robustness. A finite element analysis model is built and validated with strain measurement of the bending process thus providing prior knowledge of the residual stress field. Three-dimensional neutron diffraction residual stress measurements are also performed to obtain a reference measurement with a non-destructive method and to validate the simulated stress field. In-depth stress gradient measured by X-ray diffraction is corrected with four different methods that all show good correlation with neutron diffraction measurements. Correction methods, assumptions and uncertainties are discussed and differences are observed on the robustness of <b>the</b> methods. <b>Contour</b> method measurements are performed and results are also in agreement with neutron measurements. <b>The</b> results <b>provided</b> by <b>the</b> <b>contour</b> method are complementary {{to those of the}} X-Ray diffraction since, despite a lower accuracy on the edges where X-ray diffraction is performed, <b>the</b> <b>contour</b> method offers <b>the</b> complete cartography of longitudinal stress in a symmetry plane of the bent specimen. Uncertainty of <b>the</b> <b>contour</b> method due to the post-processing procedure is discussed...|$|R
40|$|Abstract: Constructing a {{three-dimensional}} blood vessel {{model for a}} numerical blood flow analysis requires <b>the</b> extraction of <b>contour</b> coordinate data for on the blood vessel region from a two-dimensional medical image obtained by such methods as MRI, CT and ultrasonic-echo cardiography by a manual image analysis that is labour-intensive and time-consuming. This constitutes a bottleneck to the practical use of a numerical analysis to predict blood flow for application to medical examination and treatment. A method for automatically extracting contour coordinate data by using pattern recognition by the genetic algorithm (GA) and Snake is proposed. This method <b>provides</b> <b>the</b> required <b>contour</b> data from a medical image so that a blood vessel model can be created for a numerical blood flow analysis. 1...|$|R
40|$|We {{address the}} problem of {{following}} an unknown planar contour with a nonholonomic vehicle based on visual feedback. The control task is to keep a point of the vehicle {{as close as possible to}} <b>the</b> <b>contour</b> for a choice of norm. A camera mounted on-board <b>the</b> vehicle <b>provides</b> measurements of <b>the</b> <b>contour.</b> We formulate <b>the</b> problem and compute the control law in a moving reference frame modeling the evolution of <b>the</b> <b>contour</b> as seen by an observer sitting on the vehicle. The result is an on-line path planning strategy and a predictive control law which leads the vehicle to land softly on the unknown curve. Depending on the choice of the tracking criterion, the controller can exhibit non-trivial behaviors including automatic manouvering. 1 Introduction In this paper we consider the problem of tracking an unknown contour by a nonholonomic vehicle, using visual feedback. This is a fundamental problem in autonomous navigation. <b>The</b> <b>contour</b> to be followed may be the boundary of some unknown obstacle [...] ...|$|R
40|$|Abstract. Detection of Frame {{difference}} fails {{when the}} human target is stationary in course of moving, {{this paper presents}} a method based on combination of adaptive difference and GVF-snake algorithm to solve it. Adaptive differential detection algorithm can accurately extract <b>the</b> target <b>contour,</b> {{and use it as}} <b>the</b> initial <b>contour</b> of GVF-snake model which cannot automatically extract it after we got the target. In the process of detection and tracking, calculating GVF field of the whole picture consume too much time, so we use the method of sub-region to improve the real-time. The experimental results show that, <b>the</b> algorithm can <b>provide</b> <b>the</b> actual body <b>contour</b> for GVF-snake model, and effectively track whether the target is stationary or moving...|$|R
40|$|The QCD {{corrections}} to the R_τ^ 12 {{moment of}} the invariant mass distribution in hadronic decays of the tau lepton are discussed. The next-to-next-to-leading order prediction is shown to be stable with respect to change of <b>the</b> renormalization scheme, <b>provided</b> that <b>the</b> <b>contour</b> integral expression is used. The optimized predictions are obtained using the principle of minimal sensitivity to select the preferred renormalization scheme. The optimized predictions for R_τ^ 12 and R_τ are used in a simplified fit to the experimental data to determine the strong coupling constant and the parameter characterizing the nonperturbative contribution. Comment: 4 pages LateX, 2 figures in PostScript, uses stwol. sty and epsfig. sty, {{to appear in the}} Proceedings of the 28 th International Conference on High Energy Physics, Warsaw, Poland, 25 - 31 July, 199...|$|R
40|$|Some {{significant}} {{features of}} the approach adopted for the combustor aerothermal modeling program are described. The individual computerized models utilized in the aero design approach are characterized. The preliminary design module <b>provides</b> <b>the</b> overall envelope definition of the burner. <b>The</b> diffuser module <b>provides</b> <b>the</b> detailed <b>contours</b> of <b>the</b> diffuser and combustor cowl region, {{as well as the}} pressure loss characteristics into each of the individual flow passages into the dome and around the combustor. The flow distribution module <b>provides</b> <b>the</b> air entry quantities through each of the aperatures and the overall pressure drop. The heat transfer module provides detailed metal temperature distribution throughout the metal structure as input to stress and life analysis that {{are not part of the}} aerothermo design effort. Finally, the internal flow module, INTFLOW, is described and the approach for model evaluation using laboratory data is discussed...|$|R
40|$|This opening chapter <b>provides</b> <b>the</b> <b>contours</b> of <b>the</b> {{debate about}} whether art and {{pornography}} are mutually exclusive and is meant as {{an introduction to the}} main themes of the book. It begins by looking at some of the classic ways of explaining the difference between art and pornography. Pornography, some have said, is sexually explicit and focuses exclusively on certain body parts, while art possesses emotional and psychological depth and is essentially suggestive. Others have stressed that pornography, unlike art, is inherently formulaic, or that pornography is exploitative in a way that art is not, or that pornography aims for a particular response, sexual arousal, that is incompatible with artistic contemplation or aesthetic experience. Such dichotomies, it is argued, are illuminating insofar as they help us to clarify how typical examples of art differ from typical examples of pornography, yet {{it would be wrong to}} see them as absolute distinctions. Whenever one attempts to draw a strict line between the two domains, whether it is on the basis of representational content, moral status, artistic quality, or prescribed response, one can always find examples of art or pornography that would fall on the ‘wrong side’ of the divide. Furthermore, it is argued that a value-neutral definition of pornography is to be preferred over any normative characterisation that stipulates that pornographic works, by definition, lack any significant artistic or aesthetic aspect. Finally, after providing a critical assessment of Christy Mag Uidhir’s incompatibilist account, which contrasts the ‘manner specificity’ of art with the ‘manner inspecificity’ of pornography, Maes highlights some of the practical implications of this philosophical discussion...|$|R
40|$|The {{purpose of}} this study was the {{hardware}} implementation of a real time moving object contour extraction. Segmentation of image frames to isolate moving objects followed by contour extraction using digitalmorphology was carried out in this work. Segmentation using temporal difference with median thresholdingapproach was implemented, experimental methods were used to determine the suitable morphological operatorsalong with their structuring elements dimensions to <b>provide</b> <b>the</b> optimum <b>contour</b> extraction. <b>The</b> detector with image resolution of 1280 x 1024 pixels and frame rate of 60 Hz was successfully implemented,the results indicate the effect of proper use of morphological operators for post processing and contourextraction on the overall efficiency of the system. An alternative segmentation method based on Stauffer & Grimson algorithm was investigated and proposed which promises better system performance at the expense ofimage resolution and frame rat...|$|R
40|$|This {{dissertation}} supplies {{a number}} of research findings that add to a theory of news framing effects, and also {{to the understanding of}} the role media effects play in political communication. We show that researchers must think more about what actually constitutes a framing effect, and that a dissociation of framing from other media effects concepts is not necessarily the ideal way to go in creating a more advanced framing theory. We also <b>provide</b> <b>the</b> first <b>contours</b> of a theory of news framing effects over time. Our results initially support the notion of a long-term influence of news frames on political attitudes, but also prompt questions about how stable these effects really are. Future research must continue to determine in what ways and to what extent our daily life is affected by how the news frame politics...|$|R
40|$|Deference in {{international}} human rights law has provoked animated discussion, particularly the margin of appreciation doctrine of the European Court of Human Rights. Many commentators describe the practice of deference but do not explain how it affects judicial reasoning. Some approve characteristics of deference but do not provide a justification to defend the practice against criticism. Others regard deference as a danger to human rights because it betrays the universality of human rights or involves tribunals either failing to consider a case properly or missing an opportunity to set human rights standards. This thesis employs a different approach by focussing on deference as the practice of assigning weight to reasons for a decision {{on the basis of}} external factors. This approach draws on theories of second-order reasoning from the philosophy of practical reasoning. The thesis offers a conceptual account of deference that accords with the practice not only of the European Court of Human Rights, but also the Inter-American Court of Human Rights and the UN Human Rights Committee. Additionally the thesis presents a normative account of deference, that the role of these tribunals entails permitting a measure of diversity as states implement {{international human rights}} standards. Deference {{in international}} human rights law then is the judicial practice of assigning weight to the respondent states’ reasoning in a case on the basis of three factors: democratic legitimacy, the common practice of states and expertise. This affects judicial reasoning by impacting the balance of reasons in the proportionality assessment. The account defended in this thesis dispels concerns that deference is a danger to human rights, whilst providing a theory that justifies the practice of the tribunals. <b>The</b> thesis thus <b>provides</b> <b>the</b> <b>contours</b> of a doctrine of deference {{in each of the three}} international human rights systems. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R
40|$|In Punitive Damages, Due Process, and Employment Discrimination, Joseph Seiner tackles {{the growing}} {{complexity}} of employment discrimination punitive damages claims {{and provides a}} workable solution to a difficult problem. Given the importance of punitive damages in shaping incentives to bring discrimination suits, his contribution is valuable, especially in trying to align recent constitutional punitive damages cases with the underlying discrimination law. This Essay begins by emphasizing the fundamental idea on which Professor Seiner and I agree-that there should be little room for courts to reduce punitive damages in federal employment discrimination cases based on constitutional concerns about excessiveness. Title VII contains express damages limitations that alleviate such concerns. The Essay continues by discussing whether the Supreme Court 2 ̆ 7 s decision in Exxon Shipping Co. v. Baker applies to federal discrimination claims. In arguing {{that there is little}} need for courts to use due process limits on punitive damages in federal discrimination cases, Professor Seiner draws on the underlying rationales of Exxon. Part II demonstrates how invoking Exxon is problematic because courts might use its reasoning to impose unnecessary and inappropriate limits on punitive damages, even those that fall within Title VII 2 ̆ 7 s modest damages cap. Finally, this Essay supplements the multi-part test suggested by Professor Seiner. Professor Seiner 2 ̆ 7 s model involves situations in which the employer is being held vicariously liable for punitive damages. Part III argues that employers also should face direct liability for punitive damages and <b>provides</b> <b>the</b> <b>contours</b> for such analysis. Any punitive damages test that is centered on expressed Supreme Court norms related to agency principles will necessarily be underinclusive as to possible punitive damages awards against employers. To date, the Supreme Court has failed to fully explore how employers are liable for their own intentional conduct rather than just derivatively liable for the acts of their agents...|$|R
40|$|Multi-modal image {{sequence}} {{registration is}} a challenging problem that consists in aligning two image sequences {{of the same}} scene acquired with a different sensor, hence containing different characteristics. We focus in this paper on the registration of optical and infra-red image sequences acquired during the flight of a helicopter. Both cameras are located at different positions and they provide complementary informations. We propose a fast registration method based on the edge information: a new criterion is defined {{in order to take}} into account both the magnitude and the orientation of the edges of the images to register. We derive a robust technique based on a gradient ascent and combined with a reliability test in order to quickly determine the optimal transformation that matches the two image sequences. We show on real multi-modal data that our method outperforms classical registration methods, thanks to <b>the</b> shape information <b>provided</b> by <b>the</b> <b>contours.</b> Besides, results on synthetic images and real experimental conditions show that the proposed algorithm manages to find the optimal transformation in few iterations, achieving a rate of about 8 frames per second...|$|R
40|$|International audienceOver {{the past}} 20 years {{researchers}} focusing on poverty have acknowledged {{the need for}} more comprehensive approaches to poverty combining the rigour of quantitative methods and reflexivity of qualitative approaches. The present book, which offers a portrait of poverty in the city of Mumbai through a wide spectrum of social sciences – from economics to population studies and feminist epistemology – is aimed at getting a better grasp on some of the actions that could be taken to tackle urban poverty. Besides <b>providing</b> insights into <b>the</b> <b>contours</b> of poverty, this is a good illustration of how to build comprehensive knowledge on poverty and exclusion. It will be of interest to students and scholars concerned with poverty issues and urbanization...|$|R
40|$|The aim of {{the paper}} is to propose {{effective}} technique for tumor extraction from T 1 -weighted magnetic resonance brain images with combination of co-clustering and level set methods. The co-clustering is the effective region based segmentation technique for the brain tumor extraction but have a drawback at the boundary of tumors. While, the level set without re-initialization which is good edge based segmentation technique but have some drawbacks in providing initial contour. Therefore, in this paper the region based co-clustering and edge-based level set method are combined through initially extracting tumor using co-clustering and then <b>providing</b> <b>the</b> initial <b>contour</b> to level set method, which help in cancelling the drawbacks of co-clustering and level set method. The data set of five patients, where one slice is selected from each data set is {{used to analyze the}} performance of the proposed method. The quality metrics analysis of the proposed method is proved much better as compared to level set without re-initialization method...|$|R
40|$|The delocalization {{aspects of}} {{globalization}} are {{often cited as}} the most socially and economically destructive of its forces. In this paper, I want to discuss the possibilities for ethnography to relocalize the global. In particular, I argue that the assessment of risks attached to numerous aspects of global enterprises pleads {{for the kind of}} recontextualization that ethnography can <b>provide</b> through tracing <b>the</b> <b>contours</b> of local and translocal suffering and illness {{on the one hand and}} mobilization of agency and resistance on the other. My discussion will touch down on the SARS epidemic and the health of transnational immigrant workers in the California agricultural industry. They provide two different windows on the value of risk as a lens for examining the role of transnational forces in the social production of health inequality...|$|R
30|$|All but {{the second}} radius values and spread values are just {{slightly}} overestimated, and Figure 7 b,c exhibits satisfactory visual results. Figure 7 d shows that the Chan and Vese method <b>provides</b> <b>the</b> inner and outer frontiers of <b>the</b> blurred <b>contours,</b> but also unexpected pixels due to the convergence onto noise pixels. On the contrary, <b>the</b> proposed method <b>provides</b> only <b>the</b> expected <b>contours.</b> <b>The</b> computational loads required by the proposed methods and comparative Chan and Vese method are as follows: center estimation requires 4.25 × 10 - 2 s. Then for each circle, TLS-ESPRIT method requires 8.8 × 10 - 3 s to estimate the radius. To estimate the spread parameter σ, Newton algorithm requires 4.8 × 10 - 3 s. In this case, the ALS loop is not run because {{there is only one}} circle for each center, and so one spread value to retrieve for each center. The total running time for the proposed method is then 8.3 × 10 - 2 s. For the image as a whole, the Chan and Vese method requires 2.64 s, so 31 times more than the proposed method.|$|R
40|$|A new {{approach}} for power system event recognition and classification using HS-transform and RBFNN {{is presented in}} this paper. Different power system events (disturbances) like sag, swell, notch, spike, transient, and chirp are generated and processed through Hyperbolic S-transform (HS-Transform). The excellent time-frequency resolution property of HS-Transform is used to extract useful information (features) from the non-stationary signals for pattern recognition. Here HS-transform generates the S-matrix and S-matrix <b>provides</b> <b>the</b> time-frequency <b>contours,</b> phase contours and absolute phase of the corresponding signal. From the above extracted information, various numerical indices like standard deviation, variance, norm, energy are found out. Further these indices are used as inputs to the Radial Basis Function Neural Network (RBFNN) for classifying different power system events accordingly. <b>The</b> RBFNN <b>provides</b> accurate results even with inputs (indices) found out under high noise conditions (SNR 20 dB). Thus <b>the</b> proposed method <b>provides</b> a robust and accurate method for power system events classification. KEYWORDS: power system events, HS-transform, time-frequency contours, phase contours, RBFNN ∗ Thank {{you very much for}} considering the paper for publication in IJPEES...|$|R
40|$|Graphite is {{commonly}} used to protect the nozzle metallic housing and to <b>provide</b> <b>the</b> internal <b>contour</b> to expand <b>the</b> exhaust gases in solid rocket motors. Because of the extremely harsh environment in which these materials operate, they are chemically eroded during motor firing, with a resulting performance reduction. The objective of the present work is to study the thermochemical erosion of graphite nozzles under {{a wide range of}} pressure conditions for both metallized and nonmetallized propellants. Numerical simulations are performed to reproduce recent literature experimental tests for nominal chamber pressures up to 250 bar. The adopted approach relies on a validated full Navier–Stokes flow solver coupled with a thermochemical ablation model that takes into account finite-rate heterogeneous chemical reactions at the nozzle surface, rate of diffusion of the species through the boundary layer, ablation species injection in the boundary layer, heat conduction inside the nozzle material, and variable multispecies thermophysical properties. Results show that numerical simulations are able to correctly evaluate <b>the</b> erosion rate, <b>provided</b> that modeling aspects such as entrance length and combustion efficiency are suitably accounted for...|$|R
40|$|Ablative {{materials}} {{are commonly used}} to protect the nozzle housing and to <b>provide</b> <b>the</b> internal <b>contour</b> to expand <b>the</b> exhaust gases in solid rocket motors (SRM). Due to the extremely harsh environment in which these materials operate, they are chemically eroded during motor firing with a resulting performance reduction. The objective of the present work is to study the erosion rate of graphite SRM nozzles under {{a wide range of}} pressure conditions for both metallized and non-metallized propellants. Recently, Evans et al. have performed extensive nozzle erosion rate measurements for G- 90 graphite using an instrumented sub-scale solid rocket motor (ISPM) capable of performing tests at elevated pressure levels. Numerical simulations have been performed to reproduce some of the experimental tests for nominal chamber pressures up to 250 bar. The adopted approach relies on a validated full Navier-Stokes flow solver coupled with a thermochemical ablation model which takes into account finite-rate heterogeneous chemical reactions at the nozzle surface, rate of diffusion of the species through the boundary-layer, ablation species injection in the boundary layer, heat conduction inside the nozzle material, and variable multispecies thermophysical properties...|$|R
40|$|Ablative {{materials}} {{are commonly used}} to protect the nozzle metallic housing and to <b>provide</b> <b>the</b> internal <b>contour</b> to expand <b>the</b> exhaust gases in both solid and hybrid rockets. Because of interaction with hot gas, these {{materials are}} chemically eroded during rocket firing, with a resulting nominal performance reduction. The objective of the present work is to study the erosion behavior of graphite nozzles in hybrid engines at different operating conditions and compare results with those obtained for solid motors. A main distinctive feature of hybrid engine operating conditions is, in fact, a greater concentration of oxygen-containing combustion products than solid motors. The adopted approach relies on a validated full Navier-Stokes flow solver coupled with a therm chemical ablation model {{that takes into account}} heterogeneous chemical reactions at the nozzle surface, rate of diffusion of the species through the boundary layer, ablation species injection in the boundary layer, heat conduction inside the nozzle material, and variable multispecies thermophysical properties. The parametric analysis performed in this study allows one {{to assess the impact of}} various parameters that affect the nozzle erosion rate, taking into account various combinations of fuels and oxidizers operating at different conditions...|$|R
40|$|Ablative {{materials}} {{are commonly used}} to protect the nozzle metallic housing and to <b>provide</b> <b>the</b> internal <b>contour</b> to expand <b>the</b> exhaust gases in solid rocket motors (SRM). Due to the extremely harsh environment in which these materials operate, they are chemically eroded during motor firing with a resulting nominal performance reduction. The objective of the present work is to study the erosion behavior of carbon-phenolic solid rocket nozzles. The adopted approach relies on a validated full Navier-Stokes flow solver coupled with a thermochemical ablation model which takes into account finite-rate heterogeneous chemical reactions at the nozzle surface, rate of diffusion of the species through the boundary-layer, pyrolysis gas and ablation species injection in the boundary layer, heat conduction inside the nozzle material, and variable multispecies thermophysical properties. The proposed approach is validated against two sets of experimental data: sub-scale motor tests carried out for the Space Shuttle Reusable Solid Rocket Motor and the static firing tests of {{the second and third}} stage solid rocket motors of the European VEGA launcher which use carbon-carbon for the throat insert and carbon-phenolic for the region downstream of the throat. © 2011 by D. Bianchi, A. Turchi and F. Nasuti...|$|R
40|$|Landing on {{the moon}} {{requires}} the selection and identification of a location that is level and free of hazards, along with a stable, controlled descent to the lunar surface {{through the use of}} automated systems and manual control. Spatial disorientation may occur upon reentering a gravitational field after vestibular adaptation to microgravity during lunar transit. The workload associated with selecting a suitable landing point based on the remaining fuel and current vehicle states is a concern. In Apollo, visual out-the-window information was heavily relied upon to support the selection of a landing point, and there was little support information available to indicate whether the desired site was achievable. A novel achievability contour display element showing the dynamic achievable landing area was developed based on a Goal-Directed Task Analysis and usability testing. A subject experiment was conducted in a lunar landing simulation environment to test the effects of <b>the</b> achievability <b>contour</b> on pilot performance, situation awareness, and workload in simulated approach and terminal descent scenarios as compared to an Apollo-style auditory display. Two control modes were used: supervisory control and roll, pitch, and yaw rate-control/attitude-hold (RCAH) manual control. The experiment also investigated differences in display effect with and without a required redesignation. Results of the subject experiment (N = 10) indicate that <b>the</b> achievability <b>contour</b> display showed significant improvement in subjective situation awareness and workload ratings. The results also indicate a change in decision-making behavior with the use of <b>the</b> achievability <b>contour</b> display. There was no measurable difference in flight and landing performance measures between the two display conditions. The results of the experiment suggest that <b>providing</b> <b>the</b> achievability <b>contour</b> display may have beneficial effects on pilot situation awareness and workload during the final approach and terminal descent maneuvers. Additional {{research is needed to determine}} the optimal implementation and pilot interaction methods in the use of this display. by Alexander J. Stimpson. Thesis (S. M.) [...] Massachusetts Institute of Technology, Dept. of Aeronautics and Astronautics, 2011. Cataloged from PDF version of thesis. Includes bibliographical references (p. 81 - 83) ...|$|R
40|$|We {{investigate}} the future {{evolution of the}} universe using the Buchert framework for averaged backreaction {{in the context of}} a two-domain partition of the universe. We show that this approach allows for the possibility of the global acceleration vanishing at a finite future time, provided that none of the subdomains accelerate individually. The model at large scales is analogously described in terms of a homogeneous scalar field emerging with a potential that is fixed and free from phenomenological parametrization. The dynamics of this scalar field is explored in the analogous FLRW cosmology. We use observational data from Type Ia Supernovae, Baryon Acoustic Oscillations, and Cosmic Microwave Background to constrain the parameters of the model for a viable cosmology, <b>providing</b> <b>the</b> corresponding likelihood <b>contours.</b> Comment: 12 pages, 8 figure...|$|R
40|$|We {{investigate}} cosmological {{scenarios of}} generalized Chaplygin gas {{in a universe}} governed by Horava-Lifshitz gravity. We consider both the detailed and non-detailed balance versions of the gravitational background, and we include the baryonic matter and radiation sectors. We use observational data from Type Ia Supernovae (SNIa), Baryon Acoustic Oscillations (BAO), and Cosmic Microwave Background (CMB), along with requirements of Big Bang Nucleosynthesis (BBN), to constrain {{the parameters of the}} model, and we <b>provide</b> <b>the</b> corresponding likelihood <b>contours.</b> We deduce that the present scenario is compatible with observations. Additionally, examining the evolution of the total equation-of-state parameter, we find in a unified way the succession of the radiation, matter, and dark energy epochs, consistently with the thermal history of the universe. Comment: 16 pages, 14 figures, version published in Gen. Rel. Gra...|$|R
40|$|In object {{segmentation}} by active <b>contours,</b> <b>the</b> initial <b>contour</b> {{is often}} required. Conventionally, <b>the</b> initial <b>contour</b> is <b>provided</b> by <b>the</b> user. This paper extends <b>the</b> conventional active <b>contour</b> model by incorporating feature matching in the formulation, which {{gives rise to}} a novel matching-constrained active <b>contour.</b> <b>The</b> numerical solution to the new optimization model provides an automated framework of object segmentation without user intervention. The main idea is to incorporate feature point matching as a constraint in active contour models. To this effect, we obtain a mathematical model of interior points to boundary contour such that matching of interior feature points gives contour alignment, and we formulate the matching score as a constraint to active contour model such that the feature matching of maximum score that gives <b>the</b> <b>contour</b> alignment <b>provides</b> <b>the</b> initial feasible solution to the constrained optimization model of segmentation. The constraint also ensures that <b>the</b> optimal <b>contour</b> does not deviate too much from <b>the</b> initial <b>contour.</b> Projected-gradient descent equations are derived to solve the constrained optimization. In the experiments, we show that our method is capable of achieving the automatic object segmentation, and it outperforms the related methods...|$|R
