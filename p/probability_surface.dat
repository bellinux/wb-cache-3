46|221|Public
50|$|If Yi are {{independent}} observations with corresponding values xi of the predictor variables, then θ {{can be estimated}} by maximum likelihood. The maximum-likelihood estimates lack a closed-form expression and must be found by numerical methods. The <b>probability</b> <b>surface</b> for maximum-likelihood Poisson regression is always concave, making Newton-Raphson or other gradient-based methods appropriate estimation techniques.|$|E
50|$|Geographic Profilers often employ {{tools such}} as Rigel, CrimeStat or Gemini to perform {{geographic}} analysis. System inputs are crime location addresses or coordinates, often entered through a geographic information system (GIS). Output is a jeopardy surface (three-dimensional <b>probability</b> <b>surface)</b> or color geoprofile, which depicts the most likely areas of offender residence or search base. These programs assist crime analysts and investigators to focus their resources more effectively by highlighting the crucial geographic areas.|$|E
40|$|In a single-agent dose-finding Phase I trial, the key {{underlying}} {{assumption is}} that toxicity probability increases monotonically with the dose level. However, in multi-agent trial, this assumption may not hold because the drug-drug interaction effect can either decrease or increase the joint toxicity as compared to either one used alone, which may lead to an unforeseen toxicity <b>probability</b> <b>surface.</b> In {{the first part of}} the dissertation, we develop a novel adaptive dose-finding approach which can be applied to these kinds of multi-drug combination trials under the situation of non-monotonic toxicity <b>probability</b> <b>surface.</b> In the second part of the dissertation, we extend our investigation on the drug combination dose-finding trials with late-onset toxicity outcomes and have proposed a Bayesian adaptive dose-finding design under a nonignorable missing data mechanism, and where surrogate data are available. We evaluate the operating characteristics of the aforementioned methods and also compare them with existing methods through extensive simulation studies under various scenarios. The proposed methods demonstrate satisfactory performance in general...|$|E
40|$|This paper {{considers}} matched-field {{tracking and}} track prediction for a moving ocean acoustic source when {{properties of the}} environment (water column and seabed) are poorly known. The goal is not simply to estimate source locations, but to determine track uncertainty distributions, thereby quantifying the information content of the tracking process. The algorithm involves two stages. The first stage (referred to as the tracking stage) consists of probabilistic tracking by inverting acoustic recordings of the source at a sequence of past times. For this problem, a Bayesian formulation is applied in which the posterior probability density (PPD) is integrated over unknown environmental parameters to obtain a time-ordered sequence of joint marginal <b>probability</b> <b>surfaces</b> over source range and depth, referred to as <b>probability</b> ambiguity <b>surfaces</b> (PASs). Due t...|$|R
30|$|RO, <b>probability</b> {{index for}} <b>surface</b> runoff.|$|R
5000|$|RPDF {{stands for}} [...] "Rectangular Probability Density Function," [...] {{equivalent}} to a roll of a die. Any number has the same random <b>probability</b> of <b>surfacing.</b>|$|R
40|$|Abstracts del XXIV Annual Scientific Meeting, celebrado del 8 - 10 de noviembre 2007, en Tafí del Valle, Tucumán, Argentina. Currently, GIS {{are being}} used in {{different}} research fields such as plant pathology as key tools {{for the study of}} diseases. The {{purpose of this study was}} the temporal and spatial analysis of the Bemisia tabaci -geminivirus complex, using a geographical information system. Data were used on 54 location points where B. tabaci and/ or geminivirus were found in soybean and bean in 2007. The data were incorporated into the GIS Floramap 1. 02. A map with the <b>probability</b> <b>surface</b> was thus obtained of the potential locations where B. tabaci-geminivirus might be found, bearing in mind climate features, and using 2007 data. This layer of information was superposed on the <b>probability</b> <b>surface</b> obtained from the data of the previous 2004 - 2006 period, showing an increase of potential locations, mainly in the provinces of Córdoba, Santa Fe, Entre Ríos, San Luis and Buenos Aires. It may be concluded that the implementation of technology such as GIS in spatial and temporal studies of plant diseases produced by vector-transported viruses is highly useful. Peer reviewe...|$|E
40|$|We {{present a}} method to create {{empirical}}ly informed, and thus realistic, random trajectories between two endpoints. The method used relies on empirical distribution functions, which define a dynamic drift expressed in a stepwise joint <b>probability</b> <b>surface.</b> We create random discrete time-step trajectories that connect spatiotemporal points while maintaining a predefined geometry, often based on real observed trajectories. The resulting trajectories have multiple uses, such as to generate null models for hypotheses testing, {{to serve as a}} basis for resource selection models, through the integration of spatial context and to quantify space use intensity...|$|E
40|$|The {{concept of}} a <b>probability</b> <b>surface</b> metric (PSM) within a {{modified}} Maximum A Posteriori (MAP) decoder is used to develop an efficient criteria to establish the near optimum number of iterations required for a given SNR whilst decoding. The proposed scheme achieves a similar BER and average number of iterations performance as other well known schemes, but with a much lower complexity that is independent of the frame size {{and the number of}} code states. Simulation results are presented for the standard rate 1 / 2 (7, 5) turbo code. ...|$|E
40|$|The {{horizontal}} and vertical distributions of light transmittance were evaluated {{as a function of}} foliage distribution using lidar (light detection and ranging) observations for a sugar maple (Acer saccharum) stand in the Turkey Lakes Watershed. Along the vertical profile of vegetation, horizontal slices of probability of light transmittance were derived from an Optech ALTM 1225 instrument's return pulses (two discrete, 15 -cm diameter returns) using indicator kriging. These predictions were compared with (i) below canopy (1 -cm spatial resolution) transect measurements of the fraction of photosynthetically active radiation (FPAR) and (ii) measurements of tree height. A first-order trend was initally removed from the lidar returns. The vertical distribution of vegetation height was then sliced into nine percentiles and indicator variograms were fitted to them. Variogram parameters were found to vary as a function of foliage height above ground. In this paper, we show that the relationship between ground measurements of FPAR and kriged estimates of vegetation cover becomes stronger and tighter at coarser spatial resolutions. Three-dimensional maps of foliage distribution were computed as stacks of the percentile <b>probability</b> <b>surfaces.</b> These <b>probability</b> <b>surfaces</b> showed correspondence with individual tree-based observations and provided a much more detailed characterization of quasi-continuous foliage distribution. These results suggest that discrete-return lidar provides a promising technology to capture variations of foliage characteristics in forests to support the development of functional linkages between biophysical and ecological studies...|$|R
40|$|Due to {{the rapidly}} {{increasing}} need for methods of data compression, quantization {{has become a}} flourishing field in signal and image processing and information theory. The same techniques are also used in statistics (cluster analysis), pattern recognition, and operations research (optimal location of service centers). The book gives the first mathematically rigorous account of the fundamental theory underlying these applications. The {{emphasis is on the}} asymptotics of quantization errors for absolutely continuous and special classes of singular <b>probabilities</b> (<b>surface</b> measures, self-similar measures) presenting some new results for the first time. Written for researchers and graduate students in probability theory the monograph is of potential interest to all people working in the disciplines mentioned above...|$|R
50|$|A greater <b>surface</b> <b>probability</b> {{means that}} an antigen {{is more likely}} to be {{immunogenic}} (i.e. induce the formation of antibodies).|$|R
40|$|Aim: We {{examined}} three potential enhancements of {{the stable}} isotope tech- nique for elucidating migratory connectivity in birds inhabiting poorly studied areas, illustrated for Eurasian cranes (Grus grus) that overwinter in and migrate through Israel. First, {{we examined the}} use of oxygen stable isotopes (d 18 O), sel- dom applied for this purpose. Second, we {{examined the relationship between}} ambient water d 18 O and hydrogen stable isotope (d 2 H) values derived from various models, to determine the geographical origins of migrants. Third, we introduced the use of probabilistic distribution modelling to refine the assign- ment to origin of migrants lacking detailed distribution maps. Location: Feather samples were collected in the Hula Valley (northern Israel) and across the species breeding range in north Eurasia. Methods: We analysed d 18 O and d 2 H in primary and secondary flight feathers using standard mass spectrometry. The maximum entropy (MAXENT) model was used to map the <b>probability</b> <b>surface</b> of potential breeding areas, as a Bayesian prior for assigning Hula Valley cranes to potential breeding grounds. Results: We found that d 18 O was suitable and informative. The soil water iso- scape performed better for d 18 O while precipitation isoscape was preferable for d 2 H. The MAXENT-based <b>probability</b> <b>surface</b> largely refined assignments. Overall, most (> 85...|$|E
40|$|FloraMap is a {{specialized}} GIS program which {{was developed to}} map the predicted distribution, or areas of possible climatic adaptation, of organisms in the wild. The climate at the locations where individuals of a given taxon were observed {{is assumed to be}} representative of the environmental range of the organism. The climate at these locations is extracted from a set of interpolated climate grids, and used as a calibration set to calculate a <b>probability</b> <b>surface.</b> The system can be used for a number of different plant genetic resources applications besides predicting species distributions. Some of these are described, along with future plans for development of the software...|$|E
30|$|We used a machine-learning algorithm, Maxent version 3.3. 3 e (Phillips et al. 2006), {{to model}} species {{potential}} habitats. Maxent computes {{the suitability of}} a pixel (corresponds to the grid of a given size in the real world) in a defined landscape by contrasting random background pixels against the ones with actual species presence (Merow et al. 2013). The landscape {{is characterized by a}} set of rasterized environmental variables such as temperature, precipitation, vegetation index, and the species real presence in the raster grids are indicated by geographic coordinates. Thus, it estimates a <b>probability</b> <b>surface</b> representing the distribution of pixels with a suitability range from 0 to 1 (Elith et al. 2011).|$|E
50|$|In immunology, <b>surface</b> <b>probability</b> is {{the amount}} of {{reflection}} of an antigen's secondary or tertiary structure to the outside of the molecule.|$|R
40|$|We {{introduce}} a <b>probability</b> model of <b>surface</b> visibilities in densely cluttered 3 D scenes. The model assumes the scene {{consists of a}} large number of small surfaces distributed randomly in a 3 D view volume. An example is the leaves and/or branches on a tree. We derive <b>probabilities</b> for <b>surface</b> visibility, instantaneous image velocity under egomotion, and binocular half–occlusions in these scenes. The probabilies depend on parameters such as scene depth, object size, 3 D density, observer speed, and binocular baseline. We verify the correctness of our models using computer graphics simulations and discuss possible applications of this model to various problems in computer vision...|$|R
30|$|More {{complicated}} descriptors {{originate from}} random field theory and are introduced with the functions applied over the random sets of data. Among {{the most commonly}} applied statistical descriptor functions are n-point <b>probability</b> functions, <b>surface</b> correlation functions, pore-size functions, cluster functions, nearest neighbor functions, linear path functions and others (Matheron 1975; Torquato 2002).|$|R
40|$|The PresenceAbsence {{package for}} R {{provides}} {{a set of}} functions useful when evaluating the results of presence-absence analysis, for example, models of species distribution or the analysis of diagnostic tests. The package provides a toolkit for selecting the optimal threshold for translating a <b>probability</b> <b>surface</b> into presence-absence maps specifically tailored to their intended use. The package includes functions for calculating threshold dependent measures such as confusion matrices, percent correctly classified (PCC), sensitivity, specificity, and Kappa, and produces plots of each measure as the threshold is varied. It also includes functions to plot the Receiver Operator Characteristic (ROC) curve and calculates the associated area under the curve (AUC), a threshold independent measure of model quality. Finally, the package computes optimal thresholds by multiple criteria, and plots these optimized thresholds on the graphs. ...|$|E
30|$|Figure  4 a {{shows the}} {{combination}} of sampled uncertainty values for hourly wind direction and wind speed. For example, when uncertainty had a positive signal, the sampled value {{was added to the}} reference value, and vice versa, i.e. when a 10  km/h wind speed uncertainty value was sampled, this value was added to the input hourly wind streams. Since uncertainty for both variables was defined considering normal distributions centred on 0, the combination is a bell-shaped <b>probability</b> <b>surface.</b> Figure  4 b shows an example of the combination between sampled ROS adjustment factor (for fuel model 6) and sampled uncertainty of daily relative humidity. The surface {{is quite different from the}} one shown in Fig.  4 a, since the adjustment factor PDF presents a bimodal configuration with two distinct peaks centred in values close to 1 and between 2 and 3.|$|E
40|$|The {{self-organization}} behavior {{exhibited by}} ants may be modeled to solve real world clustering problems. The general idea of artificial ants {{walking around in}} search space to pick up, or drop an item based upon some probability measure has been examined to cluster {{a large number of}} World Wide Web (WWW) documents. However, this idea is extended with the direct application of template matching with a Gaussian <b>Probability</b> <b>Surface</b> (GPS) to constrain the formation of the clusters in pre-defined areas of workspace with these multi-agents in this paper. Some comparisons between the clustering performance of supervised ants using GPS against the typical ants clustering algorithm are shown. Their performance are evaluated on the same dataset consisting of a collection of multi-class web documents. Finally, the paper concludes with some recommendations for further investigation. Povzetek: Tehnike kolonij mravelj so bile uporabljene za kategorizacijo internetnih dokumentov. ...|$|E
40|$|In {{this paper}} we present an {{algorithm}} for adaptive resolution integration of 3 D {{data collected from}} multiple distributed sensors. The input to the algorithm {{is a set of}} 3 D surface points and associated sensor models. Using a probabilistic rule, a <b>surface</b> <b>probability</b> function is generated that represents the probability that a particular volume of space contains the <b>surface.</b> The <b>surface</b> <b>probability</b> function is represented using an octree data structure; regions of space with samples of large conariance are stored at a coarser level than regions of space containing samples with smaller covariance. The algorithm outputs an adaptive resolution surface generated by connecting points that lie on the ridge of <b>surface</b> <b>probability</b> with triangles scaled to match the local discretization of space given by the algorithm, we present results from 3 D data generated by scanning lidar and structure from motion...|$|R
40|$|Abstract. Many {{methods for}} 3 D {{reconstruction}} in computer vision rely on probability models, for example, Bayesian reasoning. Here we introduce a <b>probability</b> model of <b>surface</b> visibilities in densely cluttered 3 D scenes. The scenes {{consist of a}} large number of small surfaces distributed randomly in a 3 D view volume. An example is the leaves or branches on a tree. We derive <b>probabilities</b> for <b>surface</b> visibility, instantaneous image velocity under egomotion, and binocular half–occlusions in these scenes. The probabilities depend on parameters such as scene depth, object size, 3 D density, observer speed, and binocular baseline. We verify the correctness of our models using computer graphics simulations, and briefly discuss applications of the model to stereo and motion. ...|$|R
40|$|AbstractThe {{convex hull}} {{of a set}} of {{independent}} random points sampled from three types of spherically symmetric distributions in Rd is investigated. Asymptotic behavior of the expected number of vertices, number of facets, <b>probability</b> content, <b>surface</b> area, and volume is estimated as sample size grows without bound. The estimates are applied to analyzing algorithms for constructing convex hulls...|$|R
40|$|Climate-space {{models were}} {{constructed}} for 241 plant species from {{a sample of}} 86 nature reserve communities in Great Britain. Convex Hull climate envelopes were used to compile Mutual Climatic Range diagrams for selected species at each site. Present-day and potential future climatic values over the next 100 years were compared against the climatic ranges of the species. A new Combined Envelope (a quadratic logistic regression <b>probability</b> <b>surface</b> constrained by a Convex Hull envelope) was used in a Climate Change Trend Analysis to determine future climatic suitability for species at each site (defined as a change in probability of species' presence). Results indicate that the warming climate could favour {{a large proportion of}} plants on Scottish reserves (excepting montane species) and be less favourable for many plants on reserves in the south of England. The situation appears to be one of 'no change' for the majority of species on Welsh reserves and those further north in England...|$|E
40|$|Distance {{constraint}} is a {{major concern}} in many spatial analyses. Buffering is one of the proximity techniques in GIS most commonly used to address this constraint. I introduce shape-based point buffering, an anisotropic and variable-distance buffer generation method conformal to the original polygons. In contrast with isotropic fixed-distance buffering, shape-based buffering is defined using a relative distance (percentage) instead of a real unit (for example, meters), and it allows all buffered boundaries to be formed at the same time. The construction and implementation of the buffering method are described below. Three emergency-response scenarios are designed to demonstrate potential applications of this buffering method, including a shape-based fixed-percentage buffer calculation and space distribution, a shape-based variable-percentage buffer region, and a reverse calculation of the task schedule from a <b>probability</b> <b>surface</b> constructed from shape-based buffers. Limitations of the method are discussed. The method has potential applications in emergency preparedness and planning to better address fairness issues when a geographic area must be zoned arbitrarily. ...|$|E
40|$|We {{constructed}} maps {{of probability}} of lava inundation using computer simulations considering the past eruptive behaviour of the Mt. Etna volcano and data deriving from monitoring networks. The basic a priori {{assumption is that}} new volcanoes will not form far from existing ones and that such a distribution can be performed using a Cauchy kernel. Geophysical data are useful to update or fine tune the initial Cauchy kernel to better reflect the distribution of future volcanism. In order to obtain a final susceptibility map, a statistical analysis permits a classification of Etna’s flank eruptions into twelve types. The simulation method consists of creating a <b>probability</b> <b>surface</b> of the location of future eruption vents and segmenting the region {{according to the most}} likely historical eruption on which to base the simulation. The paths of lava flows were calculated using the MAGFLOW Cellular Automata (CA) model, allowing us to simulate the discharge rate dependent spread of lava as a function of time...|$|E
40|$|The {{research}} objective is {{the analysis of}} the appropriation of space in urban public parks. For this purpose, extensive field observations were conducted in several parks in Zurich, Switzerland, over the span of three years, with records made of the location, assumed age, gender and activity of park visitors. Based on research in environmental psychology and anthropology, a model was developed building on the two concepts of «personal space» and «activity footprints» to represent space appropriation. In line with the view that quantitative spatial analysis methods remain a valid tool for critical, non-positivist research, the model was implemented using kernel density estimations for the spatio-temporal analysis of the observed park use. It is argued that the <b>probability</b> <b>surfaces</b> generated by kernel density estimations are an adequate representation of the specific vagueness of human space appropriation as they remain sensitive to the presence of individual park visitors...|$|R
3000|$|... [*]=[*] 200 pA, and an {{excitation}} <b>probability</b> of the <b>surface</b> plasmons per electron tunneling {{event is}} considered to be in the range of 10 - 2 to 1.|$|R
3000|$|... {{exhibited}} {{relatively high}} values below 390  nm {{while it was}} gradually decreased above 400  nm, which exhibits similar tendency with typical absorption spectra of ZnO NRs. Though the passivation on ZnO NRs with dielectric or organic materials enables enhanced photoconduction and efficiency by reducing the <b>probability</b> of <b>surface</b> recombination [29 – 31], for the ZnO NR-based NUV PDs, the R [...]...|$|R
40|$|This study {{utilized}} {{remote sensing}} and spatial-statistical geostatistics to model future land degradation in the mud-beach coast of southwest Nigeria. Current landcover {{was derived from}} Landsat ETM+ data. Model input data consists of 12 predictor variables. Attribution of weights to variables was done through multi-criteria evaluation. These weights were used to develop logistic regression function for simulating <b>probability</b> <b>surface</b> maps. Degraded lands accounted for about 30. 2 % of the total landcover with permanently inundated lands and bare surfaces contributing 22. 4 %. The results suggest soil, geology, elevation, distance to ocean, and location of old bitumen wells {{as the most important}} predictor variables. Simulated composite probabilities for transiting into degraded lands range between 0. 4184 and 0. 4871 in the next 20 years (from 2001) to between 0. 4284 and 0. 4973 in the next 100 years. Mangrove, scrub/coastal grassland, farmland/fallow and built-up areas appear to have higher probabilities, while the palm swamp ecosystems have the least...|$|E
40|$|Normal 0 false false false EN-ZA X-NONE X-NONE This study {{utilized}} {{remote sensing}} and spatial-statistical geostatistics to model future land degradation in the mud-beach coast of southwest Nigeria. Current landcover {{was derived from}} Landsat ETM + data. Model input data consists of 12 predictor variables. Attribution of weights to variables was done through multi-criteria evaluation. These weights were used to develop logistic regression function for simulating <b>probability</b> <b>surface</b> maps. Degraded lands accounted for about 30. 2 % of the total landcover with permanently inundated lands and bare surfaces contributing 22. 4 %. The results suggest soil, geology, elevation, distance to ocean, and location of old bitumen wells {{as the most important}} predictor variables. Simulated composite probabilities for transiting into degraded lands range between 0. 4184 and 0. 4871 in the next 20 years (from 2001) to between 0. 4284 and 0. 4973 in the next 100 years. Mangrove, scrub/coastal grassland, farmland/fallow and built-up areas appear to have higher probabilities, while the palm swamp ecosystems have the least. </p...|$|E
40|$|Abstract. Distance {{constraint}} is a {{major concern}} in many spatial analyses. Buffering is one of the proximity techniques in GIS most commonly used to address this constraint. I introduce shape-based point buffering, an anisotropic and variable-distance buffer generation method conformal to the original polygons. In contrast with isotropic fixed-distance buffering, shape-based buffering is defined using a relative distance (percentage) instead of a real unit (for example, meters), and it allows all buffered boundaries to be formed at the same time. The construction and implementation of the buffering method are described below. Three emergency-response scenarios are designed to demonstrate potential applications of this buffering method, including a shape-based fixedpercentage buffer calculation and space distribution, a shape-based variable-percentage buffer region, and a reverse calculation of the task schedule from a <b>probability</b> <b>surface</b> constructed from shape-based buffers. Limitations of the method are discussed. The method has potential applications in emergency preparedness and planning to better address fairness issues when a geographic area must be zoned arbitrarily. Introductionöbuffer analysis in GIS Buffering {{is one of the most}} commonly used proximity techniques in GIS, becaus...|$|E
40|$|Available {{interferometric}} {{data from}} recent earthquakes clearly show that major earthquakes produce relatively large crustal deformation over wide {{areas around the}} fault. As a consequence, these large areas undergo significant strain and stress {{that need to be}} considered in the analysis of seismic and fault displacement hazards for an NPP. Here, we consider <b>probability</b> of <b>surface</b> displacement due to faulting...|$|R
40|$|International audienceDue to {{the lack}} of {{reliable}} market information, building financial term-structures may be associated with a significant degree of uncertainty. In this paper, we propose a new term-structure interpolation method that extends classical spline techniques by additionally allowing for quantification of uncertainty. The proposed method is based on a generalization of kriging models with linear equality constraints (market-fit conditions) and shape-preserving conditions such as monotonicity or positivity (no-arbitrage conditions). We define the most likely curve and show how to build confidence bands. The Gaussian process covariance hyper-parameters under the construction constraints are estimated using cross-validation techniques. Based on observed market quotes at different dates, we demonstrate the efficiency of the method by building curves together with confidence intervals for term-structures of OIS discount rates, of zero-coupon swaps rates and of CDS implied default probabilities. We also show how to construct interest-rate <b>surfaces</b> or default <b>probability</b> <b>surfaces</b> by considering time (quotation dates) as an additional dimension...|$|R
40|$|Abstract—Particle filters {{have been}} found to be {{effective}} in tracking mobile targets in indoor environments. One frequently encountered problem in these settings occurs when the target’s movement pattern changes unexpectedly; such as when the target turns around, enters a room from a corridor or turns left or right at an intersection. If the particle filter makes an incorrect prediction, it might not be able to recover using the normal techniques of prediction, weight update and resampling. We propose an approach to automatically restart the particle filter by sampling the latest trusted observation when the particle cloud diverges too much from the observations. The restart decision is based on Kullback-Leibler divergence between the <b>probability</b> <b>surfaces</b> associated with the current observation and the particle cloud. Through an experimental study we show that the restart algorithm allows the successful early recovery of stranded particle filters, in our scenarios providing a 36 % average improvement in localization accuracy...|$|R
