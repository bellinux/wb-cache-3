3|59|Public
2500|$|First, in {{the mapping}} of virtual memory addresses, instead of needing an MMU, the MCP systems are descriptor-based. [...] Each {{allocated}} memory block {{is given a}} master descriptor with {{the properties of the}} block (i.e., the size, address, and whether present in memory). [...] When a request is made to access the block for reading or writing, the hardware checks its presence via the <b>presence</b> <b>bit</b> (pbit) in the descriptor.|$|E
50|$|Another {{significant}} advantage was realized for virtual memory. In the B5000 design, if a {{data block}} were rolled out, all descriptors referencing that block {{needed to be}} found in order to update the <b>presence</b> <b>bit</b> and address. With the master descriptor, only the <b>presence</b> <b>bit</b> in the master descriptor needs changing. Also the MCP can move blocks around in memory for compaction and only needs to change the address in the master descriptor.|$|E
50|$|First, in {{the mapping}} of virtual memory addresses, instead of needing an MMU, the MCP systems are descriptor-based. Each {{allocated}} memory block {{is given a}} master descriptor with {{the properties of the}} block (i.e., the size, address, and whether present in memory). When a request is made to access the block for reading or writing, the hardware checks its presence via the <b>presence</b> <b>bit</b> (pbit) in the descriptor.|$|E
5000|$|... {{power mode}} changes are {{executed}} reliably (even in the <b>presence</b> of <b>bit</b> errors) ...|$|R
40|$|The next {{generation}} of Global Navigation Satellite Systems (GNSS), such as Galileo [1] and GPS modernization [2], will use signals with equal code and bit periods, which will introduce a potential bit sign transition in each segment of the received signal processed in the acquisition block. The <b>presence</b> of <b>bit</b> sign transitions in the data record is a critical aspect in all the acquisition methods where the data are processed in blocks, like the fast acquisition method based on Fast Fourier Transformation (FFT) where the bit sign transition could occur in any position within the data block. A bit sign transition occurring within an integration time causes a splitting of the main peak of the Cross Ambiguity Function (CAF) into two smaller lobes along the Doppler shift axis, which causes a great performance degradation of the acquisition system [3]. In this paper an innovative two steps acquisition algorithm is proposed to mitigate the CAF peak splitting effect caused by the <b>presence</b> of <b>bit</b> sign transitions in the signal segments, which fits {{the requirements of the}} new generation of GNSS signals. The bit sign transition problem is described in detail in this paper and the CAF peak splitting effect dependent on the bit sign transition position in the signal segment is also clearly explained. It is able to prove that the <b>presence</b> of <b>bit</b> sign transition does not destroy the information on the presence of the satellite in view, but it introduces an erroneous Doppler frequency shift estimation. When the proposed two steps acquisition technique is adopted in the signal acquisition in <b>presence</b> of <b>bit</b> sign transitions, it could overcome the CAF peak splitting effect due to the <b>presence</b> of <b>bit</b> sign transitions and provide improved performance in comparison with the classical fast acquisition scheme. In order to validate the proposed technique, simulation campaigns have been performed on the simulated Galileo Open Service (OS) BOC 1, 1 signals to evaluate the performances in terms of histograms of the estimated Doppler shift and code phase delay, Receiver Operative Characteristics (ROC) and Signal to Noise Ratio (SNR) curves. The simulation results show that the proposed two steps acquisition method provides superior performance over the classical fast acquisition approach, which prove the advantages of the proposed technique and consolidate its validity and effectiveness to solve the CAF peak splitting problem in <b>presence</b> of <b>bit</b> sign transition...|$|R
50|$|In {{order to}} prevent any {{unnecessary}} or spurious protection switching in the <b>presence</b> of <b>bit</b> errors on both paths, a switch will typically occur when {{the quality of the}} alternate path exceeds that of the current working path by some threshold (e.g., an order of magnitude better BER). Consecutively, any case of failure drops in SNCP’s decision mechanism.|$|R
40|$|International audienceIn global {{navigation}} satellite system (GNSS) receivers, the acquisition {{process is the}} first stage of the signal processing module. It consists of assessing the presence of GNSS signals and providing a rough estimation of the incoming signal parameters: the Doppler frequency and the code delay. However, the <b>presence</b> of <b>bit</b> sign transitions affects receiver performance in signal acquisition detection. This article focuses on the bit transition {{and its impact on the}} acquisition performance by providing a general mathematical study and an illustration for two GNSS signals: the global positioning system legacy civil signal (GPS L 1 C/A) and Galileo E 1 open service (OS). This study is applicable to a terrestrial user in a constraint environment. Furthermore, the presented results are mathematical models of the probability of detection in the <b>presence</b> of <b>bit</b> sign transitions (only one potential bit sign transition per integration interval), and potential uncertainties on the Doppler frequency and code delay. These do not result from empirical acquisition of real signals...|$|R
40|$|In {{this paper}} {{we present a}} model of coarse grain {{dataflow}} execution. We present a top down method for generating machine independent multithreaded code, called MIDC. We define MIDC. We discuss the relevant phases in the Sisal to MIDC compilation process, and present some example compilations. We quantify the number of threads, number of inputs per thread, and average thread size for Livermore and Purdue benchmarks. Keywords: Hybrid von Neumann/Dataflow, threads, code generation algorithm. 1 Introduction Hybrid dataflow machines execute threads of von Neumann RISC code, where the threads are enabled by the availability of data. Thread enabling is either implemented by efficient matching using explicit token storage and <b>presence</b> <b>bits,</b> or by pools of "waiting " and "ready" threads with hardware support to move threads from and to these pools. A strict firing rule allows a thread to execute only when all its inputs are available, avoiding threads to block but potentially increasing laten [...] ...|$|R
40|$|International audienceThis paper {{presents}} {{a prototype of}} a hardered version of the 8051 micro-controller, able to assure reliable operation in the <b>presence</b> of <b>bit</b> flips caused by radiation. Aiming at avoiding such faults in the 8051 micro-controller, Hamming code protection was used in its SRAM memory and registers. This paper shows implementation details of this technique in the micro-controller VHDL description and area overhead result...|$|R
40|$|This paper {{presents}} an analytical model for estimating the network convergence time of IPv 6 Routing Protocol for Low-power and Lossy Networks (RPL) in a static chain topology network of IEEE 802. 15. 4 nodes, in the <b>presence</b> of <b>bit</b> errors. The model presented is validated by extensive simulation results. We show that performance degradation may be noticed by a user for BER values around {{or greater than}} 8 · 10 Postprint (published version...|$|R
40|$|OBEX is {{a higher}} layer {{protocol}} adopted as the framework for wireless object exchange for wireless transports including IrDA and Bluetooth. In this paper, we develop a model which leads to derivation of the OBEX throughput over the IrDA protocol stack for various data rates in the <b>presence</b> of <b>bit</b> errors. We then optimize the OBEX packet size and turnaround for maximum OBEX throughput. The results show significant improvement on OBEX performance using optimized parameters. 1...|$|R
40|$|ABSTRACT. TinyTP, {{developed}} by Infrared Data Association (IrDA), is a transport protocol dedicated for the indoor {{point to point}} infrared applications. This article presents a comprehensive mathematical model for TinyTP over the IrDA protocol stacks accounting for the <b>presence</b> of <b>bit</b> errors. By implementing different TinyTP buffer sizes and various numbers of TinyTP connections, the performance of TinyTP is examined. The results show that TinyTP buffer size has {{a significant effect on}} the overall throughput. 1...|$|R
25|$|The <b>presence</b> of <b>bit</b> wear {{suggest that}} a horse was ridden or driven, and the {{earliest}} of such evidence from a site in Kazakhstan dates to 3500 BCE. Because horses can be ridden and controlled without bits by using a noseband or a hackamore, and such tools are used even today, the absence of bit wear on horse teeth is not conclusive evidence against domestication, but such materials do not produce significant physiological changes nor are they apt to be preserved for millennia.|$|R
40|$|We {{consider}} {{the security of}} the Bennett-Brassard 1984 (BB 84) protocol for Quantum Key Distribution (QKD), in the <b>presence</b> of <b>bit</b> and basis dependent detector flaws. We suggest a powerful attack {{that can be used in}} systems with detector efficiency mismatch, even if the detector assignments are chosen randomly by Bob. A security proof is provided, valid for any basis dependent, possibly lossy, linear optical imperfections in the channel/receiver/detectors. The proof does not assume the so-called squashing detector model. Comment: 10 pages, 4 figures; v 4 : rewritten for clarit...|$|R
40|$|International Telemetering Conference Proceedings / October 25 - 28, 1993 / Riviera Hotel and Convention Center, Las Vegas, NevadaThis paper {{describes}} algorithms {{implemented by}} the Magellan High Rate Processor to recover radar data corrupted {{by the failure of}} an onboard tape recorder that dropped bits. For data with error correction coding, an algorithm was developed that decodes data in the <b>presence</b> of <b>bit</b> errors and missing bits. For the SAR data, the algorithm takes advantage of properties in SAR data to locate corrupted bits and reduce there effects on downstream processing. The algorithms rely on communication approaches, including an efficient tree search and the Viterbi algorithm to maintain the required throughput rate...|$|R
40|$|We {{present a}} {{statistical}} model of erratic behaviors in Flash memory arrays {{based on a}} Markov chain model. The model parameters are experimentally evaluated through short program/erase cycling characterizations. The model is suitable for Monte Carlo simulations of Flash memory arrays through which the <b>presence</b> of tail <b>bits</b> in the threshold voltage distribution can be correctly predicted. Finally, an application to NAND architectures is discussed in relation to overprogramming issu...|$|R
40|$|International Telemetering Conference Proceedings / October 14 - 16, 1975 / Sheraton Inn, Silver Spring, MarylandConventional {{burst error}} {{correction}} techniques for (n,k) cyclic block codes cannot {{cope with the}} <b>presence</b> of <b>bit</b> slippages that frequently occur in conjunction with burst errors of the bit inversion variety. A technique is described to enable the correction of an (n,k) cyclic code subjected to a noise disturbance consisting of an arbitrary number of both bit deletions and bit inversions contained within a single error burst. An efficient implementation of a Burst/Deletion Correction Decoder is presented. Although bit insertion correction is conceptually {{similar to that of}} bit deletions, the decoder implementation for combined insertion and inversion correction within a burst is much more cumbersome. The probabilities of false correction are analyzed...|$|R
40|$|International Telemetering Conference Proceedings / October 28 - 31, 1985 / Riviera Hotel, Las Vegas, NevadaThis {{paper is}} {{concerned}} with the concept and development of cost effective methods for determining sensor-to-user system performance under simulated real world conditions. Many system elements involved in the digital pre- and post- processing functions found in telemetry systems are designed to operate primarily on error free data. The <b>presence</b> of <b>bit</b> errors drastically alters the performance of these elements: data compressors pass noise, E. U. converters produce subtle and wild data variations, and embedded voice channels become overly noisy. This paper identifies the digital areas affected by noise in modern systems, categorizes their susceptability, and suggests laboratory simulation techniques that may identify problems prior to mission data flow...|$|R
30|$|TinyTP is the IrDA {{transport}} layer protocol for indoor infrared communications. For the first time, {{this paper presents}} a mathematical model for TinyTP over the IrDA protocol stacks {{taking into account the}} <b>presence</b> of <b>bit</b> errors. Based on this model, we carry out a comprehensive optimisation study to improve system performance at the {{transport layer}}. Four major parameters are optimised for maximum throughput including TinyTP receiver window, IrLAP window and frame size, as well as IrLAP turnaround time. Equations are derived for the optimum IrLAP window and frame sizes. Numerical results show that the system throughput is significantly improved by implementing the optimised parameters. The major contribution of this work is the modelling of TinyTP including the low-layer protocols and optimisation of the overall throughput by appropriate parameter selection.|$|R
40|$|Abstract—In this paper, we {{introduce}} a 1 -bit compressive sensing reconstruction algorithm {{that is not}} only robust against bit flips in the binary measurement vector, but also does not require a priori knowledge of the sparsity level of the signal to be reconstructed. Through numerical experiments, we show that our algorithm outperforms state-of-the-art reconstruction algorithms for the 1 -bit compressive sensing problem in the <b>presence</b> of random <b>bit</b> flips and when the sparsity level of the signal deviates from its estimated value. I...|$|R
5000|$|His son John II (1432-1458), was {{described}} as [...] "effeminate, but not unattractive" [...] and was reviled by Pope Pius II as a vile evil sloth. He was dominated by two women in his life, both Greek; Helena Palaiologina his wife and Marietta de Patras, his mistress. Supposedly in a fight between these two in the king's <b>presence,</b> the queen <b>bit</b> off her adversary's nose. As the queen was Greek, she was well loved by the Cypriots and the orthodox church.|$|R
40|$|This paper {{presents}} a symmetric key establishment protocol, called Event-Based Reconciliation (ERS), {{that does not}} require sensor nodes to store pairwise keys prior to deployment. The novel idea behind ERS is to establish symmetric keys by exchanging the secret key information via distributed source coding that uses some event data as side information. Any two neighboring sensor nodes {{that are able to}} detect the same event first exchange some key information using distributed source coding and data of the event detected in the overlapping sensing region, and reconcile a symmetric key in two iterations. A key expansion algorithm is also presented to enhance the recovery of key <b>bits</b> in the <b>presence</b> of <b>bit</b> errors and to improve the secrecy of the key establishment. Performance evaluation results show that ERS has low memory and communication overhead compared to previous symmetric key distribution schemes. ...|$|R
40|$|We {{present an}} {{analytical}} {{model for the}} maximum throughput of Bluetooth Low Energy (BLE), considering the <b>presence</b> of uncorrelated <b>bit</b> errors {{and the impact of}} a key BLE parameter that defines the time between the start of two consecutive connection events. The derived analysis models the generic application of master-to-slave unidirectional data transmission, which also forms an upper bound for bidirectional data transmission throughput. Simulation results show that our model accurately predicts the maximum BLE throughput for all bit error rates and BLE parameter settings evaluated. Postprint (published version...|$|R
40|$|In this paper, {{we propose}} a new {{analytical}} model of {{different versions of}} the TCP, viz., Tahoe, Reno and New Reno, where TCP mechanisms such as slow start, fast retransmit, fast recovery and timeout are modeled as a Markov chain. In our proposed model, we consider the exponential increase of the congestion window and the exponential increase of the timeout back-off. Finally, we focus on the bulk throughput performance analytically, and compare it of different ver-sions of the TCP in the <b>presence</b> of random <b>bit</b> errors on a wireless link. ...|$|R
40|$|The erase {{operation}} in NOR-Flash memories intrinsically {{gives rise to}} a wide threshold voltage distribution causing various reliability issues: read margin reduction; increase of total bitline leakage current and electrical stress during reading and programming. This paper will address and review the erasing operation by analyzing the causes, the reliability issues and the possible solutions of the erased threshold voltage distribution width, the <b>presence</b> of ultrafast <b>bits,</b> the erratic erase phenomenon, the presence of a significant tail (extrinsic behavior) in the erased distribution and the intrinsic oxide degradation during cycling (oxide aging) ...|$|R
40|$|Research work is {{described}} that {{was carried out}} to develop adaptive delta modulators (ADM) capable of encoding black-and-white and color video signals in the <b>presence</b> of high <b>bit</b> error rates. These were a one-dimensional ADM capable of encoding black-and-white pictures and transmitting digitally encoded voice without increase in bandwidth; an ADM capable of encoding color video signals; and an ADM capable of encoding field-sequential color. For {{each of these three}} units, the bit rate could be adjusted within the range from 6 to 24 Mbits/sec. The effects of the bit rate and channel errors on the response of the picture are examined...|$|R
40|$|The International Telecommunications Union (ITU) {{recently}} adopted reversible {{variable length}} codes (RVLCs) {{for use in}} the emerging H. 263 + video compression standard. As the name suggests, these codes can be decoded in two directions and can therefore be used by a decoder to enhance robustness in the <b>presence</b> of transmission <b>bit</b> errors. In addition, these RVLCs involve little or no efficiency loss relative to the corresponding non-reversible variable length codes. We present here the ideas behind two general classes of RVLCs and discuss the results of applying these codes in the framework of the H. 263 + and MPEG- 4 video coding standards...|$|R
40|$|AbstractA string S∈Σm can {{be viewed}} as a set of pairs {(si,i) ∣si∈S,i∈{ 0,…,m− 1 }}. We follow the recent work on pattern {{matching}} with address errors and consider approximate pattern matching problems arising from the setting where errors are introduced to the location component (i), rather than the more traditional setting, where errors are introduced to the content itself (si). Specifically, we continue the work on string matching in the <b>presence</b> of address <b>bit</b> errors. In this paper, we consider the case where bits of i may be stuck, either in a consistent or transient manner. We formally define the corresponding approximate pattern matching problems, and provide efficient algorithms for their resolution...|$|R
40|$|Abstract This paper {{reports on}} perceptual {{quality of service}} (PQoS) {{investigation}} for the Motion-JPEG 2000 video compression. In all today’s networks (Internet, mobile or Wireless Local Area Networks), video quality is measured mostly using peak signal-to-noise ratio (PSNR). PSNR, however, {{does not take into}} account human vision and thus cannot be a reliable predictor of perceived visual quality. Human observers will perceive different kinds of distortions in digital video, like jerkiness, blockiness, and blurriness. We consider a more fundamental definition of quality, PQoS, and report on a method which maximizes the perceived video quality in the <b>presence</b> of random <b>bit</b> errors, as these conditions are expected in the context of wireless transmission...|$|R
40|$|A {{major issue}} of indoor GPS signals is the {{extremely}} low signal-to-noise ratio (e. g. C/N 0 = 5 dB Hz ÷ 30 dB Hz) {{and the consequent}} difficulty, for the acquisition stage, of identifying "reliable" autocorrelation peaks. Acquisition sensitivity can be increased by extending the coherent in-tegration time, but the maximum achievable performance is bounded, for instance, by the <b>presence</b> of navigation <b>bits</b> that introduce sign reversals within the coherent integration window. This {{may result in a}} partial or even total cancella-tion of "true" peaks. The "sensitivity assistance" approach enables High-Sensitivity (HS) acquisition by providing the acquisition engine with approximate code-phase/Doppler-frequency estimates together with fragments of the data bit-stream, to allow for wiping off the bit transitions and extending the coherent integratio...|$|R
40|$|International audienceUnderwater {{acoustic}} networks (UANs) are attracting {{interest in}} recent decades. The unique {{characteristics of the}} underwater acoustic channel, such as long propagationdelay, delay variance, and high bit error rate, present challenges for themedium access control (MAC) protocol design in UANs. Most existing mediumaccess control protocols ignore the delay variance which prevents the accurateestimation of round trip time (RTT). The expected RTT value {{can be used to}} computethe Retransmission Time-Out (RTO) or the waiting time in MAC. The estimationof RTT is also meaningful for Automatic Repeat re-Quest (ARQ) schemebecause the system should ensure reliable data transmissions in the <b>presence</b> ofhigh <b>bit</b> error rate in the underwater acoustic channel. By analyzing the impact ofRTO on throughput under the effect of delay variance, we conclude that the fixedRTO is inefficient and RTO should be adaptively set to improve the throughput. We present a novel approach of predicting the RTT using a Bayesian dynamiclinear model, and then adjust RTO adaptively according to the predicted values. Simulation results show that the predicted values can adapt quickly to the sample RTT values. Under the effect of RTT fluctuations, the Bayesian algorithm offersperformance gains in terms of throughput and prediction performance, comparingwith Karn’s algorithm. Our study highlights the value of predicting the RTT usingBayesian approach in underwater acoustic networks...|$|R
30|$|Several {{studies have}} been {{published}} regarding the question {{if it is a}} good idea to deliver erroneous packets to the application. One possibility to implement such functionality is to use a protocol like UDP Lite [11], employing a partial checksum for bit error detection. Partial checksums cover only the most vulnerable parts of the packet, such as protocol headers. It has widely been accepted that in the <b>presence</b> of <b>bit</b> errors, UDP Lite can significantly improve the throughput [1, 11 – 14]. However, the improved usage of channel capacity comes at the cost of bit errors appearing in the coded content. Therefore, the benefits of using UDP Lite depend highly on how well the bit errors are handled at the application layer. A wide range of different partial error protection schemes have been proposed for error prone transmission. According to Singh et al. [12], some video quality improvements can be gained with UDP Lite together with an error resilient codec, but the results are highly dependent on the bit error characteristics. Other researchers, such as Khayam et al. [13] and Masala et al. [14], have proposed different FEC and partitioning strategies in adjunction with UDP Lite to obtain better performance. In our earlier study, we have observed relatively large improvements in terms of PSNR, when a UDP Lite approach is compared to conventional rate control in a congested radio channel [1].|$|R
6000|$|... "We {{are going}} to have a very {{considerable}} amount of trouble explaining ourselves," [...] I said in conclusion. [...] "We are here by an act of the imagination, and that is just one of those metaphysical operations that are so difficult to make credible. We are, by the standard of bearing and clothing I remark about us, unattractive in dress and deportment. We have nothing to produce to explain our <b>presence</b> here, no <b>bit</b> of a flying machine or a space travelling sphere or any of the apparatus customary on these occasions. We have no means beyond a dwindling amount of small change out of a gold coin, upon which I suppose in ethics and the law some native Utopian had a better claim. We may already have got ourselves into trouble with the authorities with that confounded number of yours!" ...|$|R
40|$|A vector {{quantizer}} {{based on}} artificial neural networks is developed {{for use in}} digital video data compression applications. A series of experiments investigating the edge performance of various distortion measures and experiments exploring various vector sizes are presented. The paper then describes a differential vector quantizer which preserves edge features and an adaptive algorithm, Frequency-Sensitive Competitive Learning, {{which is used to}} develop equiprobable vector quantizer codebooks. By using codebooks comprised of equiprobable codevectors, variable length coding is unnecessary which results in robust performance in the <b>presence</b> of channel <b>bit</b> errors. The resulting coder is efficient, robust, and permits real-time hardware realizations. The DVQ coder currently under construction is also described. 1. INTRODUCTION Data compression algorithms reduce transmission bandwidth and/or storage space. Currently there is particular interest in the low bit rate coding of images. In this [...] ...|$|R
40|$|Abstract- In modern world, {{cryptography}} hackers try {{to break}} a cryptographic algorithm or try to retrieve the key, which is needed to encrypt a message (data), by analyzing the insertion or <b>presence</b> of repetitive <b>bits</b> / characters (bytes) in the message and encrypted message {{to find out the}} encryption algorithm or the key used for it. So it is must for a good encryption method to exclude the repetitive terms such that no trace of repetitions can be tracked down., Somdip Dey, Joyshree Nath and Ashoke Nath (SJA) is an amalgamation of Modified Caesar cipher(MCC), Bit Rotation Reversal(BRR), Neeraj Khanna, Joel James, Joyshree Nath, Sayantyan Chakraborty, Amlan Chakrabarti, Asoke nath (NJJSAA), Function Encryption (FE) methods. By using MCC, BRR, NJJSAA and FE methods we can reduce time complexity by exluding repetitive characters in the cipher text. The proposed method is robust in terms of efficiency and computational cost...|$|R
40|$|Abstract [...] Steganography is covert communication, {{which means}} {{to hide the}} very {{existence}} of a message from a third party. Due to growing need for security of data, image steganography is gaining popularity. The traditional image steganography algorithm is Least Significant Bit embedding, but it can be easily detected by the attackers as it embeds data sequentially in all pixels. Instead of sequentially embedding data, data can be embedded in random pixels, but it causes speckles in the image. A better approach is to hide the data in the regions like edges. An attacker has less suspicion of the <b>presence</b> of data <b>bits</b> in edges, because pixels in edges appear to be either much brighter or dimmer than their neighbors. So we present a novel technique to hide data in the edges of the image by extending the Least Significant Bit embedding algorithm. This algorithm hides data in the edge pixels and thus ensures better security against attackers...|$|R
