873|14|Public
25|$|Java {{bytecode}} {{can either}} be interpreted at run time by a virtual machine, {{or it can be}} compiled at load time or runtime into native code which runs directly on the computer's hardware. Interpretation is slower than native execution, and compilation at load time or runtime has an initial performance penalty for the compilation. Modern <b>performant</b> JVM implementations all use the compilation approach, so after the initial startup time the performance is equivalent to native code.|$|E
2500|$|VMware Workstation 12 was {{reviewed}} by Infoworld. The review mainly concerns differences with earlier versions, concluding that this version has moderate improvements, mainly to support newer hardware and operating systems, {{that may not}} be worth the cost of upgrading. The review concludes that Workstation 12 is [...] "the most <b>performant,</b> polished, and feature-rich desktop virtualization product available", but points out that it is the most expensive.|$|E
5000|$|<b>Performant</b> Inc. was a {{software}} vendor of J2EE diagnostic tools. Mercury Interactive acquired <b>Performant</b> in 2003 for $22.5M.|$|E
5000|$|Honoured as a Grand Entrepreneur at the Gala Les Nouveaux <b>Performants</b> in February 2004.|$|R
40|$|L'utilisation d'une méthode appropriée de suivi-évaluation est {{indispensable}} aux processus de gestion adaptative. Pour cela, des outils plus <b>performants</b> de suivi-évaluation des moyens de subsistance sont requis. Un outil basé sur des indicateurs simples de moyens de subsistance a été développé et testé à l'échelle du village. Des méthodes participatives intègrant les perceptions et aspirations locales en Effective monitoring is {{a fundamental}} requirement for adaptive management. The need for better tools for monitoring livelihoods is particularly acute. A simple indicators-based livelihoods monitoring tool to be used at the village scale was developed and tested. Participatory methods were used to elicit local perceptions and aspirations about development and conservation. That information was used to develop a prototype set of indicators, organized using the Sustainable Livelihoods Framework. Data were collected for the indicators in six villages to calibrate and test the tool. Indicators of financial and physical capital worked well. Social capital and natural capital were more difficult to capture with low-cost indicators. Most of the indicators are applicable in rural areas of the district and beyond. Despite some shortcomings, the tool shows good potential {{as a way to}} focus attention on key aspects of rural development and as a way to inform and facilitate discussion and negotiation between communities and other stakeholders. (Résumé d'auteur...|$|R
40|$|Dans le but de contribuer aux réflexions {{relatives}} à l’élargissement de la notion de performance, nous entendons traiter la question de la performance d’une filière économique dans une perspective située, à la fois dans l’espace et dans le temps, en mettant en lumière la variété des éléments constitutifs de la performance d’une part et l’existence de tensions, voire de contradictions entre ces éléments d’autre part. Dès lors, nous soutenons l’idée d’une diversité de modèles de production <b>performants</b> et insistons sur la nécessité d’identifier des indicateurs adaptés à cette diversité. Nous nous appuyons pour cela sur les outils conceptuels développés {{dans le cadre de}} l’économie patrimoniale. Nous illustrons notre propos en nous appuyant sur le cas de la filière ostréicole dans le Bassin d’Arcachon. This article {{deals with}} the question of the performance of an industry, viewed in its relations with the space and with the time. The purpose, which contributes to the literature relating to the broadening of the performance, is to insist on the variety of constituent elements of the performance {{on the one hand and}} on the existence of contradictions between these elements on the other hand. Thus, we assume the diversity of performing production models and we stress the importance to define indicators which take into account this diversity. To this end, the tools developed within the framework of the heritage economy are used. We refer to the case study of oyster-farming industry in the Arcachon Bay (France) to illustrate these points...|$|R
5000|$|To {{succeed and}} be <b>performant</b> in its tasks a (semi)autonomous team needs: ...|$|E
50|$|In 2013, John McCutchan {{announced}} that he had created a <b>performant</b> interface to single instruction, multiple data (SIMD) instruction sets for Dart.|$|E
50|$|Apache Apex is a YARN-native {{platform}} that unifies stream and batch processing. It processes big data-in-motion {{in a way}} that is scalable, <b>performant,</b> fault-tolerant, stateful, secure, distributed, and easily operable.|$|E
40|$|The Tunisian banks {{currently}} {{operate in}} a very competitive environment. Long-term viability of this sector depends on its degree of efficiency. Therefore a study relating to the determinants of X-Efficiency in Tunisian banks is of major interest. For that purpose, we made recourse to {{an extension of the}} stochastic frontier approach called " Improved SFA " which assumes a parameter of truncation specific to each bank. The empirical results reveal differences in efficiency pronounced according to the size and the structure of property of the banks. The average efficiency of the small and average sizes banks is significantly more significant than that of large banks. Moreover, the public banks are relatively more efficient than the private banks. Thereafter, we analyze the internal determinants of the level of the efficiency of the Tunisian banks. Within this framework, three results deserve to be underlined. Firstly, the improvement of the level of the efficiency of the Tunisian banks is related to the managerial capacity rather than with the size of the banks. Moreover, preponderance of the activity of credit, compared to other outputs represents a source of efficiency. Secondly, there is a negative relation between the ratio equity on total asset and the efficiency of banks, which seems to indicate that those are too committed in risk activities. Thirdly, the share of the non <b>performants</b> loans represents a source of inefficiency, insofar as it cost of a bank increases with these types of loans, especially for the banks of large sizes. ...|$|R
40|$|Born about {{a century}} ago, Quantum Mechanics has {{revolutionized}} the description and {{the interpretation of}} Physics at sub-microscopic level. In the last decades, due {{to the influence of}} mathematical and engineering research fields, Quantum Mechanics has given birth to related research areas like Quantum Computation, Quantum Information and Quantum Communication. With the discovery of the laser, and later the development of fiber optics and satellite networks, Quantum Communication and Quantum Optics seems to have a natural field of application in Communication Systems. Despite this, the interest in this technology and studies for communication purpose has been overshadowed by the great results in communication networks achieved in the last decades with classical paradigms. However, due to the increasing demand of communication data rate, system designers are now looking at Quantum Mechanics for new and more <b>performanting</b> solutions in communication purposes. Early theoretical studies on Quantum Discrimination Theory and Quantum Information predict better performance for Communication Systems that take advantage of the quantum laws. In addition, Quantum Mechanics provides the deepest description of the physical phenomena, and there are scenarios where a quantum model fits best, as in in deep space communications, where the received signal is really weak, or in a satellite networks, where we are interested in strongly reducing the power of transmitted signals, possibly without sacrificing performance significantly. However, if on one side Quantum Communication Theory promises great gains in the performance of communication systems, on the other hand it fails to describe how to implement physical devices that reach these ultimate limits. So far, only a few architectures achieving these performances are known, and only for simple modulation formats. We are interested in the scenario of optical communications, where the message transmitted is encoded in a sequence of coherent states. Transmitter devices for coherent modulation are well known and consist in laser pulse generators. Instead, receiver implementations working at the quantum limit performance limit are yet to be found. In this Thesis I deal with different topics in the quantum transmission scenario. First, I review existing classical (suboptimal) and quantum (suboptimal and optimal) receiver schemes for the binary coherent modulation. I present a new formulation of the optimal scheme known as Dolinar Receiver with the multiple copies problem, focusing on the information gained during the measurement. Second, I analyze the binary communication in a noisy environment, studying the error probability and the capacity of the binary channel induced. Given the description of the quantum channel, I optimize both the transmitted quantum states and the measurement operators employed in the communication. Third, I consider the Pulse Position Modulation, that is particularly suitable for space and satellite communication due to its simplicity of implementation and high capacity. I review some known suboptimal receivers, and I propose a receiver scheme which approaches the limit performance predicted with quantum theory outperforming the existing schemes. To sum up the results of this Thesis, in order to approach the limit performance predicted by Quantum Mechanics, an optimization is always necessary to exceed the classical effects and trigger the quantum phenomena. In particular, the information gained during the measurement plays an important role, for example in the definition of adaptive receivers. In this Thesis both these aspects have been deeply investigated...|$|R
40|$|L'accroissement des contraintes subies par les matériaux utilisés dans les {{techniques}} pétrolières et parapétrolières et liées aux températures et pressions élevées ainsi qu'aux agressions chimiques, fait que l'on exige de ces matériaux qu'ils soient de plus en plus <b>performants.</b> Ceci implique, pour les concepteurs et utilisateurs de matériel comportant des élastomères en contact avec les hydrocarbures, une connaissance actualisée des produits existants, lesquels sont en constante évolution. On commence donc par décrire les principales familles d'élastomères résistant en milieu pétrolier actuellement disponibles sur le marché. Puis, après avoir évoqué les problèmes que rencontrent les industries pétrolière et automobile dans l'utilisation des élastomères et présenté sommairement les résultats de travaux réalisés à l'Institut Français du Pétrole (IFP), on définira quelques axes de recherches dont la finalité est de participer à l'amélioration des techniques pétrolières dans le sens d'une meilleure fiabilité et d'une réduction des coûts. The {{materials used}} in the oil and automotive industries are being subjected to higher and higher mechanical, thermal and chemical stresses, so these materials, and especially elastomers, {{have to be more}} and more resistant. For designers and users of equipment containing elastomers in contact with hydrocarbons, this requires an updated understanding of existing products. The aim of this article is thus to describe, first of all, the leading families of resistant elastomers now available for use in a petroleum environment, with particular emphasis on chlorinated, fluorinated, acrylic and nitrile rubbers and their derivatives, which have evolved greatly in recent years with the appearance of new varieties and new types of elastomers, such as Atlas, Vamac, Eypel, etc. For each family of elastomer, the structure is described along with the vulcanization method, its properties and its uses. Then the problems with which elastomers are confronted in the petroleum industry are described. They are mainly linked to the high pressures and temperatures encoutered in boreholes and to chemical attacks due to acid gases, corrosion inhibitors and completion and treatment fluids. Elastomers must also be resistant to blistering, extrusion and abrasion. The proper choice of a material can lead to a reduction in operating costs and to increased safety. Concerted action among elastomer producers and users seems indispensable. Problems in the automotive industry are linked to the increase in thermal stresses due to the raising of the temperature underneath the hood, to the use of new fuels, unleaded gasoline oxygenated fuel, and to the evolution of lubricant additives. This is leading to the progressive forsaking of nitrile rubber and polychloroprene to the benefit of higher-performance but more expensive elastomers such as fluorinated rubbers or hydrogenated nitrile rubbers. Fluorosilicones and acrylics are also being called upon for development in the automotive industry. Then several research findings made at Institut Français du Pétrole (IFP) are described, concerning the comparative analysis of the behavior of nitrile, hydrogenated nitrile and fluorinated rubbers in complex petroleum environments, together with a few examples of applications in the field of pumps, composite-material tubes with an elastomeric liner and gaskets. To conclude some suggestions are made on possibilities of improving elastomers for petroleum applications, based on {{a better understanding of the}} phenomenon of blistering, an improvement of vulcanization systems and the optimizing of some elastomers such as HNBR. Progress is also possible in the behavior of elastomers with regard to lubricants and fuels and in the predicting of the lifetime of seals used in boreholes...|$|R
50|$|Tips and {{recommendations}} on deploying highly <b>performant,</b> production grade clusters {{can be found}} in the MySQL Cluster Evaluation Guide and the Guide to Optimizing Performance of the MySQL Cluster Database.|$|E
50|$|During {{the season}} 2008/09, as Xamax's trainer by then Nestor Clausen had {{difficulties}} finding a <b>performant</b> right back, he moved Nuzzolo to this position, which he kept {{for most of}} the season.|$|E
50|$|OneLogin {{remained}} {{available and}} <b>performant</b> during the October 2016 attack on Dyn, a major provider of DNS services, which brought down many websites, including Spotify, Twitter, Reddit, and The New York Times, {{in part by}} using redundant DNS providers.|$|E
40|$|The growing {{needs in}} {{computing}} performance imply more complex computer architectures. The lack of good programming environments for these machines must be filled. The goal {{to be reached}} {{is to find a}} compromise solution between portability and performance. The subject of this thesis is studying the problem of static allocation of task graphs onto distributed memory parallel computers. This work takes part of the project INRIA-IMAG APACHE and of the european one SEPP-COPERNICUS (Software Engineering for Parallel Processing). The undirected task graph is the chosen programming model. A survey of the existing solutions for scheduling and for mapping problems is given. The possibility of using directed task graphs after a clustering phase is underlined. An original solution is designed and implemented; this solution is implemented within a working programming environment. Three kinds of mapping algorithms are used: greedy, iterative and exact ones. Most developments have been done for tabu search and simulated annealing. These algorithms improve various objective functions (from most simple and portable to the most complex and architecturaly dependant). The weigths of the task graphs can be tuned using a post-mortem analysis of traces. The use of tracing tools leads to a validation of the cost function and of the mapping algorithms. A benchmark protocol is defined and used. The tests are runned on the Meganode (a 128 transputer machine) using VCR from the university of Southampton as a router, synthetic task graphs generation with ANDES of the ALPES project (developped by the performance evaluation team of the LGI-IMAG) and the Dominant Sequence Clustering of PYRROS (developped by Tao Yang and Apostolos Gerasoulis). La demande croissante de puissance de calcul est telle que des ordinateurs de plus en plus <b>performants</b> sont fabriques. Afin que ces machines puissent etre facilement exploitees, les lacunes actuelles en terme d'environnements de programmation doivent etre comblees. Le but a atteindre est de trouver un compromis entre recherche de performances et portabilite. Cette these s'interesse plus particulierement au placement statique de graphes de taches sur architectures paralleles a memoire distribuee. Ce travail s'inscrit dans le cadre du projet INRIA-IMAG APACHE et du projet europeen SEPP-COPERNICUS (Software Engineering for Parallel Processing). Le graphe de taches sans precedence est le modele de representation de programmes paralleles utilise dans cette these. Un tour d'horizon des solutions apportees dans la litterature au probleme de l'ordonnancement et du placement est fourni. La possibilite d'utilisation des algorithmes de placement sur des graphes de precedence, apres une phase de regroupement, est soulignee. Une solution originale est proposee, cette solution est interfacee avec un environnement de programmation complet. Trois types d'algorithmes (gloutons, iteratifs et exacts) ont ete concus et implementes. Parmi ceux-ci, on retrouve plus particulierement un recuit simule et une recherche tabu. Ces algorithmes optimisent differentes fonctions objectives (des plus simples et universelles aux plus complexes et ciblees). Les differents parametres caracterisant le graphe de taches peuvent etre affines suite a un releve de traces. Des outils de prise de traces permettent de valider les differentes fonctions de cout et les differents algorithmes d'optimisation. Un jeu de tests est defini et utilise. Les tests sont effectue sur le Meganode (machine a 128 transputers), en utilisant comme routeur VCR de l'universite de Southampton, les outils de generation de graphes synthetiques ANDES du projet ALPES (developpe par l'equipe d'evaluation de performances du LGI-IMAG) et l'algorithme de regroupement DSC (Dominant Sequence Clustering) de PYRROS (developpe par Tao Yang et Apostolos Gerasoulis). Mapping task graphs on distributed memory parallel computer...|$|R
40|$|La {{production}} des circulations ferroviaires a la sncf repose actuellement sur un processus essentiellement sequentiel dans lequel la conception des grilles horaires de circulation (reservation de l'infrastructure pour la circulation des trains de l'offre de transport de la sncf) conditionne largement la conception des planifications des engins ferroviaires (les roulements engins), puis celle des agents de conduite (adc) (les grilles de service des adc). cette strategie de planification sequentielle des ressources ferroviaires a ete massivement adoptee pour des raisons pratiques et scientifiques (historique, savoir-faire, complexite du systeme ferroviaire, etc.). toutefois, cette strategie de planification sequentielle genere des solutions qui peuvent etre de cout eleve et moins robustes aux aleas, car les decisions prises a une etape donnee peuvent reduire considerablement l'ensemble des solutions realisables aux etapes suivantes. face a ce constat et a la forte interaction entre ces trois ressources heterogenes et tres couteuses, la sncf a souhaite investiguer la praticabilite et les apports d'une demarche d'optimisation du plan de transport par planification integree de ces ressources critiques. dans cette optique, les travaux de these ont porte sur l'etude de faisabilite, le prototypage et la validation d'une demarche de planification integree des ressources permettant d'ameliorer l'efficacite globale du plan de transport, d'accroitre la competitivite de la sncf et d'ameliorer la qualite de ses services. nous avons propose une formalisation du probleme de planification integree engins/adc et des algorithmes <b>performants</b> qui s'appuient sur une approche par relaxation lagrangienne pour resoudre de maniere efficace la problematique etudiee. cette approche repose sur l'exploitation de deux briques logicielles developpees a la sncf pour resoudre chacun des sous-problemes de planification des engins et des adc. les algorithmes ont ete testes experimentalement avec des donnees reelles de la region ter bretagne. differentes evolutions des modeles et des algorithmes ont ete etudiees pour rendre ces derniers plus efficaces. les tests de validation sur des jeux de donnees reelles a une echelle industrielle sont encourageants et montrent des gains potentiels allant jusqu'a 4 % des adc exploites {{par rapport}} a une approche traditionnelle (sequentielle). The planning of railway {{production at the}} french national railways (sncf) is currently based on a mainly sequential {{process in which the}} design of railway timetabling widely conditioning design planning of railway equipment (rolling stock), then one of the train drivers (driver rosters). this strategy of sequential planning of railway resources massively adopted for practical and scientific reasons (expertise, complexity of the railway system, etc.). however, this strategy generates solutions which can be more expensive and less robust to uncertainties, because decisions taken at any given stage can significantly reduce the overall feasible solutions of the following steps. given this situation and the strong interaction between these heterogeneous and very expensive resources, the thesis deals with the feasibility and inputs of a process where these critical resources could be planned and optimized in an integrated way. the thesis focuses on the feasibility study, prototyping and validation of an integrated approach for planning rolling stocks and drivers, so as to improve the efficiency of the overall transportation plan, increase sncf competitiveness and enhance the quality of its services. we propose a mixed integer linear programming formulation of the rolling stock/ train drivers integrated planning problem. in this mathematical model, each planning sub-problem is formalized and coupling constraints are further introduced to model the interdependencies of these two resources when they are simultaneously used for train production. in this heuristic, the solution of the lagrangian dual and the calculation of feasible solutions are performed by calling two proprietary software modules available at sncf for planning rolling stocks and train drivers. the heuristic is tested experimentally with real data from the ter bretagne region, and several evolutions are introduced in the models and algorithms so as to improve their performances. validation tests on of real data sets at an industrial scale are encouraging and, when compared to a traditional (sequential) approach, show gain of up to 4 % for train drivers used. ST ETIENNE-ENS des Mines (422182304) / SudocSudocFranceF...|$|R
40|$|The {{planning}} of railway {{production at the}} french national railways (sncf) is currently based on a mainly sequential {{process in which the}} design of railway timetabling widely conditioning design {{planning of}} railway equipment (rolling stock), then one of the train drivers (driver rosters). this strategy of sequential planning of railway resources massively adopted for practical and scientific reasons (expertise, complexity of the railway system, etc.). however, this strategy generates solutions which can be more expensive and less robust to uncertainties, because decisions taken at any given stage can significantly reduce the overall feasible solutions of the following steps. given this situation and the strong interaction between these heterogeneous and very expensive resources, the thesis deals with the feasibility and inputs of a process where these critical resources could be planned and optimized in an integrated way. the thesis focuses on the feasibility study, prototyping and validation of an integrated approach for planning rolling stocks and drivers, so as to improve the efficiency of the overall transportation plan, increase sncf competitiveness and enhance the quality of its services. we propose a mixed integer linear programming formulation of the rolling stock/ train drivers integrated planning problem. in this mathematical model, each planning sub-problem is formalized and coupling constraints are further introduced to model the interdependencies of these two resources when they are simultaneously used for train production. in this heuristic, the solution of the lagrangian dual and the calculation of feasible solutions are performed by calling two proprietary software modules available at sncf for planning rolling stocks and train drivers. the heuristic is tested experimentally with real data from the ter bretagne region, and several evolutions are introduced in the models and algorithms so as to improve their performances. validation tests on of real data sets at an industrial scale are encouraging and, when compared to a traditional (sequential) approach, show gain of up to 4 % for train drivers used. La production des circulations ferroviaires a la sncf repose actuellement sur un processus essentiellement sequentiel dans lequel la conception des grilles horaires de circulation (reservation de l'infrastructure pour la circulation des trains de l'offre de transport de la sncf) conditionne largement la conception des planifications des engins ferroviaires (les roulements engins), puis celle des agents de conduite (adc) (les grilles de service des adc). cette strategie de planification sequentielle des ressources ferroviaires a ete massivement adoptee pour des raisons pratiques et scientifiques (historique, savoir-faire, complexite du systeme ferroviaire, etc.). toutefois, cette strategie de planification sequentielle genere des solutions qui peuvent etre de cout eleve et moins robustes aux aleas, car les decisions prises a une etape donnee peuvent reduire considerablement l'ensemble des solutions realisables aux etapes suivantes. face a ce constat et a la forte interaction entre ces trois ressources heterogenes et tres couteuses, la sncf a souhaite investiguer la praticabilite et les apports d'une demarche d'optimisation du plan de transport par planification integree de ces ressources critiques. dans cette optique, les travaux de these ont porte sur l'etude de faisabilite, le prototypage et la validation d'une demarche de planification integree des ressources permettant d'ameliorer l'efficacite globale du plan de transport, d'accroitre la competitivite de la sncf et d'ameliorer la qualite de ses services. nous avons propose une formalisation du probleme de planification integree engins/adc et des algorithmes <b>performants</b> qui s'appuient sur une approche par relaxation lagrangienne pour resoudre de maniere efficace la problematique etudiee. cette approche repose sur l'exploitation de deux briques logicielles developpees a la sncf pour resoudre chacun des sous-problemes de planification des engins et des adc. les algorithmes ont ete testes experimentalement avec des donnees reelles de la region ter bretagne. differentes evolutions des modeles et des algorithmes ont ete etudiees pour rendre ces derniers plus efficaces. les tests de validation sur des jeux de donnees reelles a une echelle industrielle sont encourageants et montrent des gains potentiels allant jusqu'a 4 % des adc exploites par rapport a une approche traditionnelle (sequentielle) ...|$|R
5000|$|In 2013 John McCutchan {{announced}} that he had created a <b>performant</b> interface to SIMD instruction sets for the Dart programming language, bringing the benefits of SIMD to web programs for the first time. The interface consists of two types: ...|$|E
50|$|Velocity was {{developed}} by Julian Shapiro to address a lack of <b>performant</b> and designer-oriented JavaScript animation libraries. Stripe, a popular web developer-focused Internet technology company sponsored Shapiro on a grant to help provide the financial resources necessary to continue full-time development on Velocity.|$|E
50|$|GCM {{has been}} criticized for example by Silicon Labs in the {{embedded}} world as the parallel processing is not suited to <b>performant</b> use of cryptographic hardware engines and therefore reduces the performance of encryption for some of the most performance sensitive devices.|$|E
40|$|Le Québec a consacré des efforts {{techniques}} et financiers substantiels à l'assainissement des eaux usées municipales et à l'entreposage des déjections animales afin de satisfaire les demandes des citoyens en matière de restauration des usages des cours d'eau. L'assainissement de l'eau a, dans l'ensemble, par ses choix technologiques et administratifs, engendré des investissements publics dépassant 7, 2 milliards de dollars et plus de 400 millions de dollars annuellement au chapitre de l'exploitation. Ces choix ont-ils permis d'atteindre un niveau de qualité de l'eau correspondant à un optimum social? À l'aide d'une étude de cas portant sur le bassin versant de la rivière Chaudière (Québec, Canada), cet article met en évidence les facteurs qui ont nuit à l'efficacité des politiques de contrôle de la pollution de l'eau au Québec. Sur ce bassin, 125 M$ ont été consacrés entre 1981 et 1992 à l'érection d'usines d'épuration utilisant différents types de traitement, 8, 6 M$ ont été alloués pour la construction de structures d'entreposage de fumiers et le service de la dette pour l'assainissement des eaux usées municipales atteindrait près de 527 M$ selon une hypothèse de financement de 25 ans. La performance des usines d'épuration a permis de réduire significativement les apports au cours d'eau, notamment en DBO 5 et en phosphore. Enfin, cette performance et le coût total de l'assainissement municipal sur le bassin de la Rivière Chaudière permettent d'évaluer, sur la base d'une relation coût-efficacité, qu'il y aurait un niveau optimal de qualité de l'eau pouvant résulté de l'établissement d'infrastructures d'assainissement des eaux usées municipales. Ainsi, dans l'optique d'une prise {{en charge}} sociale du problème collectif de la pollution de l'eau sur la base du bassin versant, il serait approprié que les gestionnaires et les usagers-contribuables de la ressource-eau, prennent en compte, non pas uniquement les objectifs de rejets, mais également les coûts et les performances de l'ensemble des usines d'épuration sur le bassin afin de retirer le maximum de charges polluantes là où les équipements sont les plus <b>performants.</b> Considerable technical and financial {{effort has been}} invested by Québec in the cleaning up of municipal wastewaters and storage of animal manure to meet demands by citizens to restore the province's rivers to their former state. Water pollution control has required technological and management choices that have resulted overall in public investments in excess of $ 7. 2 billion, with over $ 400 million going to operation costs annually. Have these choices enabled Québec to attain a water quality level consistent with a social optimum?Based on a case study taken from the Chaudière river watershed, Québec, Canada, this article posits two conditions for achieving a social optimum and underscores the factors that have offset the efficiency of water pollution control policies in Québec. According to the data collected on this watershed, between 1981 and 1992, $ 125 M was invested {{in the construction of}} sewage water treatment plants using various treatment methods, while $ 8. 6 M went towards manure storage facilities. On the whole, $ 527 M is expected to be spent over 25 years to service the debt for municipal wastewater treatment within the watershed. While inputs of pollutants, especially BOD 5 and phosphorus, have dropped significantly with the construction of the wastewater treatment plants, levels of residual pollution in the watershed remain high. It is suspected that total residual loads of phosphorus from municipal and agricultural sources are still well above the loads eliminated through wastewater treatment. If they are to achieve an efficient watershed-based approach to water management, decision-makers are faced with two conditions: the first addresses intersectoral efficiency in controlling pollution in a watershed and the second involves minimizing intrasectoral costs of pollution control. The condition explains the administrative and technical choices made as well as the importance of the political market in allocating resources to water pollution control among the socioeconomic sectors responsible for water quality deterioration. The condition explains how to minimize the costs in a specific socioeconomic sector among the available water treatment solutions. Using performance data from wastewater treatment plants and the total cost of wastewater treatment in the Chaudière river watershed, it can be assumed, based on a cost efficiency ratio, that an optimal level of water quality should occur {{as a result of the}} establishment of municipal wastewater treatment infrastructures. However, it would appear from the results obtained that Québec's water treatment program has deviated from a social optimum, i. e., restoration costs have not been shared equitably among users/polluters within the watershed, and measures to ensure maximum removal of pollution at minimum cost have not been secured. The play of political forces is central to the allocation of resources among pollution sources. Without a proper hard core concept, a water pollution control policy will not be able to elaborate the best solutions oriented towards attaining a social optimum. In the context of the high residual pollution loads within the watershed, there remains the issue of what water quality level is desirable at what cost, particularly with respect to the community's contribution to date and the efficiency of the response strategies that have been implemented. Now that wastewater treatment infrastructures have been set up, and a watershed-based approach to water management becomes effective, water resource managers and users/taxpayers should turn their attention away from discharge objectives only to focus also on the costs and performance of the watershed's treatment plants as a whole, so that removal of pollutant loads at high-performance facilities may be maximized...|$|R
40|$|L'Indice de Polluosensibilité Spécifique (IPS) est considéré comme l'un des indices diatomiques les plus <b>performants</b> pour l'évaluation de la qualité des cours d'eau. Son {{utilisation}} en réseau de surveillance reste cependant limitée {{en raison de}} la nécessité de travailler {{au niveau}} spécifique voire infraspécifique et de la systématique en perpétuelle évolution. A l'opposé, l'Indice Diatomique Générique (IDG) est plus accessible dans sa mise en oeuvre mais ne permet pas d'obtenir des résultats très fiables. Un nouvel Indice Diatomique Pratique (IDP) a donc été mis au point sur un bassin versant expérimental à partir d'un jeu de 86 relevés. Dans un premier temps, les inventaires ont été classés en fonction des écarts observés entre IPS et IDG. Dans un second temps, ont été identifiées les espèces responsables de ces écarts en prenant en compte celles présentant une abondance relative supérieure à 5 % et une différence de polluosensibilité avec le genre correspondant supérieure ou égale à 0, 4. Plusieurs IDP ont été mis au point et leurs performances, par rapport à l'IPS, étudiées. Il apparaît que la prise en compte des espèces responsables des écarts supérieurs ou égaux à 2 constitue le meilleur compromis entre fiabilité et applicabilité en réseau. Cette méthodologie a été appliquée aux 480 relevés effectués dans le bassin Artois - Picardie et aux 550 espèces inventoriées. Elle permet de proposer un indice diatomique pratique basé sur l'identification de 45 genres et 91 espèces. Macroinvertebrates constitute the main biological support for {{an evaluation of the}} quality of water courses and are, therefore, widely put to use in monitoring networks. However, for major water courses and canalized waterways the use of other methodologies is imperative. Diatoms and diatom indices are well adapted to the study of these environments. Among these, the Specific Polluosensitivity Index (SPI) established by CEMAGREF seems {{to be one of the}} better performing diatom indices. Calculation of this index relies on the Zelinka & Marvan formula derived from the saprobic system: SPI=[Epsilon]A[inf]j v[inf]j i[inf]j / [Epsilon] A[inf]j v[inf]j where A[inf]j is the relative abundance of the species j, v j is its indicative value (1 [smaller or equal] v[inf]j [smaller or equal] 3) and i[inf]j its pollution sensitivity (1 [smaller or equal] i[inf]j [smaller or equal] 5). The values initially falling in the range between 1 and 5 are transformed into values comprised between 1 and 20, in order to make comparisons between the various existing indices easier. Five categories of water quality can be distinguished according to the value of the index: SPI [Bigger or equal] 16 : zero pollution or low eutrophication; 13. 5 [smaller or equal] SPI < 16 : moderate eutrophication; 11 [smaller or equal] SPI < 13. 5 : moderate pollution or heavy eutrophication; 7 [smaller or equal] SPI < 11 : high pollution; SPI < 7  : very heavy pollution. However, the SPI index is rarely used because of two main obstacles: it requires data at a specific or even infraspecific level, and it is based on constantly changing systematics. Progress towards increased accessibility and, therefore, larger application was made with the elaboration of the Generic Diatomic Index (IDG) based on the same principle as the SPI. However, this GDI does not yield reliable results, in so far as certain genera, such as Navicula and Nitzschia, contain species with a widely differing ecologies. In order to provide a methodology that can be used as a matter of routine, a protocol for the elaboration of a Practical Diatomic Index (PDI) was established and tested on 86 inventories from the water basin of the river Aa (North of France). These were first classified into four categories according to the variations observed between SPI and GDI: category 1 : |SPI-GDI| [bigger or equal] 3; category 2 : 2 [smaller or equal] |SPI-GDI|; category 3 : 1 [smaller or equal] |SPI-DGI| < 2; category 4 : |SPI-DGI| < 1. For each of the first three categories, the species responsible for the variations were identified, taking into consideration those with a relative abundance of more than 5 %, the pollution sensitivity of which showed, compared to the corresponding genus, a variation higher than or equal to 0. 4. Thus, three indices corresponding respectively to category 1 (PDI 1), 2 (PDI 2), and 3 (PDI 3) were proposed and tested against the SPI taken as reference index. The results of this comparative study can be summarized as follows:- GDI= 0. 57 SPI + 5. 47 r= 0. 801 (242 species), - PDI 1 = 0. 86 SPI + 1. 12 r= 0. 972 (27 species), - PDI 2 = 0. 95 SPI + 0. 55 r= 0. 991 (39 species), - PDI 3 = 0. 96 SPI + 0. 45 r= 0. 994 (42 species). To test the implications of replacing the presently used SPI by this practical index, a comparative study of the classification of inventories in four categories of hydrobiological quality was also carried out. This study shows that the mean, at - 1. 76 ± 2. 25 for the GDI, is reduced to 0. 14 ± 0. 94 for PDI 1, to - 0. 07 ± 0. 51 for PDI 2, and to - 0. 07 ± 0. 45 for PDI 3. Given the variability of the index at one and the same site and in one sampling, PDI 2 considered to be the best compromise between reliability and network applicability. The methodology corresponding to PDI 2 was applied to the 480 samplings carried out in the Artois-Picardie basin and a new Practical Diatom Index is thus proposed for the monitoring of the 200 sites making up the monitoring network of the Artois-Picardie water basin. This PDI, built on a base of more than 550 species and varieties, rests on the joint determination of 45 genera and 91 species of which the pollution sensitivity coefficients and the indicative values are given...|$|R
40|$|Background Since the {{introduction}} of laparoscopic surgery, experts {{have been interested in}} its effectiveness, virtues and drawbacks. The amount of operations performed using conventional laparoscopy and laparoendoscopic single-site surgery has progressively increased over the past years. Both approaches to laparoscopy were shown to be very effective, nevertheless, limitations still persist. Despite the increased number of clinical applications of single-site approaches to laparoscopic surgery, limited research in a simulated environment has been done to evaluate its performance and <b>performants.</b> The impact of surgical consequences {{led to the development of}} laparoscopic simulators with the goal of training novice laparoscopic surgeons. Developing technical skills is essential to surgical training and thus the objective of the present thesis is to compare the performance and learning progress of experienced surgeons and students of conventional and single-site laparoscopy. Physical Setup For the purpose of the present study, a laparoscopic physical simulator was developed from a basic shell. Participants interact with the simulator through conventional straight or double curved 5 mm hand instruments. Different configurations of the hand instruments with 5 mm trocars or a single-incision port were analysed. A force platform, inserted inside the simulator, measured parameters related with tissue manipulation, including different types of forces applied. Furthermore, handle sensors that measure grasp forces between the pincers of the conventional hand instruments were developed. A trocar sensor was built to obtain data about the forces transferred from the hand instruments to the abdominal wall. On the top of the force platform, a custom-made training platform was placed, on which participants had to perform a task. This manipulation task and platform were developed for the purposes of this thesis with the goal of mimicking some basic actions performed during laparoscopic surgery. During the task, a silicone piece of artificial tissue had to be navigated through a ring and attached to a pin. Experimental Setup Twenty-four medical students without surgical experience were randomly assigned into two groups. The first group performed trials on the simulator using the conventional laparoscopic approach (hereby abbreviated as Conv) and later switched into single-site laparoscopic approach with conventional straight instruments (from now on abreviated as LESS (C)). The second group performed the same training in reverse order. Each approach was executed six times by each group, totalling 288 trials. The performance of the participants on the tissue manipulation task was assessed. For comparison, a control group of five single-site laparoscopy expert surgeons were asked to execute the same task as the two groups of novices. In random order, three trials were performed with Conv. approach, three trials with LESS (C). Additionally, experts performed three trials with single-site approach with double-curved hand instruments (hereby LESS (D)), not performed by novices. Experimental Study Two main studies were carried out. The first study compares tissue manipulation performance of novices and surgeons. The second study concerns only the control group of expert surgeons. The latter focus on the amount of grasp force applied on the tissues and the amount of forces applied on the trocar (forces that in a real operation would be applied to the incision on the abdominal wall). After the conclusion of the trials, novices and experts filled in a questionnaire that evaluated their performance. This questionnaire was used to understand the connection between self-evaluation and quantitative results from the simulator. First Study Results Time, maximum applied force (pushing and pulling) and force-volume (a parameter based on the variation of the force applied) are fundamental for the determination of the performance level of the participants. As a summary of the results, the conclusion reached was that the duration of a single trial, the amount of maximum interaction force and force-volume applied improve as the task is repeated. This is a positive result as it demonstrates that these are valid parameters to assess the skill of a surgeon or medical student. Experts demonstrated their proficiency with a short duration of the trials (48 s std 20. 8 for Conv; 60 s std 29. 9 for LESS (C); 88 s std 26. 3 for LESS (D)). In comparison, novices performance on their final trial (group 1 : 101 s std 68. 1 for Conv; 193 s std 151. 4 for LESS (C); group 2 : 115 s std 40. 4 for Conv; 199 s std 99. 1 for LESS (C)) did not reach the same proficiency as experts. Novices exhibited a progressive improvement in the time taken to perform the task from trial to trial. Another important parameter that evidenced progressive improvement during the conventional approach, was the maximum interaction force. Nevertheless, the maximum interaction force during LESS (C) approach was characterised by fluctuating values in the first group and increasing values in the second group. The results obtained from the first group of novices (6. 6 N std 2. 3 for Conv and 9. 0 N std 2. 7 for LESS (C)) were similar to those of experts (7. 5 N std 3. 2 for Conv and 9. 0 N std 2. 2 for LESS (C)). Slightly higher results, in what concerns maximum force applied, were obtained in group 2 (8. 0 N std 2. 2 for Conv and 10. 9 N std 2. 5 for LESS (C)). Data about maximum interaction force according to the LESS (D) trials was obtained only from the control group (8. 9 N std 2. 6). The third important parameter for the evaluation of participants performance was the force-volume. Force-volume is calculated using the standard deviations of the exerted forces in a 3 -dimensional referential. This parameter is related to the smoothness of a movement and dexterity of using the laparoscopic hand instruments. If the silicone artificial tissue is pushed with a constant force in one direction, a small force-volume is obtained. During the conventional approach, the first group of novices presented the lowest value of force-volume (4. 9 N^ 3 std 6. 2) in comparison with the second group (9. 4 N^ 3 std 7. 1) and almost twice lower than the experts (9. 6 N^ 3 std 13. 1). The force-volume during LESS (C) approach, exerted by the second group of novices (11. 8 N^ 3 std 15. 4), presented the lowest value, in this approach, among the novices (18. 8 N^ 3 std 17. 9) and even among the experts (30. 2 N^ 3 std 53. 8). As novices performed a larger number of trials, the fact that novices used less force-volume than the experts was interpreted as tangible proof of evidence to the importance of practice. Second Study Results Grasp forces (between the pincers) applied by experts were obtained and analysed. The mean grasp force applied by the left (non-dominant) hand (8. 4 N std 2. 7 during Conv and 9. 5 N std 2. 9 during LESS (C)) was weaker in comparison with the grasp forces by the right hand (7. 7 N std 4. 7 during Conv and 8. 7 N std 4. 1 during LESS (C)). No significant difference in the maximum grasp force was observed between the left hand (19. 6 N std 1. 1 during Conv and 19. 4 N std 1. 1 during LESS (C)) and the right hand (18. 0 N std 6. 0 for Conv and 19. 7 N std 3. 6 for LESS (C)). The mean force detected by the trocar sensor did not show excessive values (2. 7 N std 0. 5 for LESS (C) and 2. 7 N std 0. 3 for LESS (D)). The maximum force detected by the trocar sensor (11. 4 N std 3. 7 for LESS (C) and 9. 3 N std 2. 9 for LESS (D)) requires definitely further investigation as the value which causes damage in patients is not known. Additional Results The questionnaire showed that the first group of novices preferred the conventional approach, while only 36 % of the participants from the second group expressed preference about LESS (C) approach. This difference in preference can be justified by the order in which the approaches were learnt. In general, there was a consensus about the self-evaluation of the participants performance. Both groups of novices rated the conventional approach with 8 out of 10, while the LESS (C) approach was rated with 6 in group 1 and 7 in group 2. Further results were obtained from direct observation of tissue handling and posture of the participants. Different handling strategies of the artificial tissue, as well as the correct posture of the participants influenced the overall performance. Conclusion The sensor-based laparoscopic simulator built for the present study, can be considered a reliable, valid and innovative device for acquiring skills in the performance of laparoscopic tasks. Useful results about interaction forces, grasp forces, force-volume and trocar forces were obtained. The order of the approaches, the handling strategies and the correct posture of the participants, showed to influence the overall performance on the laparoscopic simulator. Further research is required to determine a reference threshold that indicates a limit to the range of safe forces applied by novice surgeons. This would allow the use of the simulator as a training device that gives feedback on the performance of the trainee. Additionally, it is relevant to understand if strong forces, applied to the incision of the abdominal wall, during manoeuvring of the hand instruments are a significant reason for post-operative complications...|$|R
50|$|CORDIC was {{conceived}} in 1956 by Jack E. Volder at the aeroelectronics department of Convair {{out of necessity}} to replace the analog resolver in the B-58 bomber's navigation computer by a more accurate and <b>performant</b> real-time digital solution. Therefore, CORDIC is {{sometimes referred to as}} digital resolver.|$|E
50|$|Extempore is a live coding {{environment}} {{focused on}} real-time audiovisual software development. It {{is designed to}} accommodate the demands of cyber-physical computing. Extempore consists of two integrated languages, Scheme (with extensions) and Extempore Language. It uses the LLVM cross-language compiler to achieve <b>performant</b> digital signal processing and related low-level features, on-the-fly.|$|E
50|$|The {{subsystems}} already mainlined {{and available}} in the Linux kernel are most probably <b>performant</b> enough so to not impede the gaming experience in any way, however additional software is available, such as e.g. the Brain Fuck Scheduler (a process scheduler) or the Budget Fair Queueing (BFQ) scheduler (an I/O scheduler).|$|E
40|$|International audienceConstructing {{the united}} Nation's Norm in recent years, {{important}} political events have brought researchers in political sciences and com munication {{to work more}} explicitly with each other. the last Un General assembly, in September 2011, has confirmed the relevance of such a convergent perspective. Until now, academic research has not questioned the communicational dimension of inter national organizations taken as a highly complex phe nomenon. and yet, the Un is a perfect foundation for examining bordering topics such as authority and new mechanisms of political deliberation, decision and legitimization. Guillaume Devin defines interna tional organizations as meeting points between coo perative behaviour systems and constraining rules. this institutionalization process depends on three kinds of resources - legal, functional and also symbolic. their interdependence {{is the subject of}} this contribution, and more precisely the extent to which communi cational processes have become crucial in building and perpetuating what we would call the Un norm. our intellectual purpose is modest and questions the conditions of communication analysis in the field of international organizations. Taking inspiration from liberal institutionalism and constructivism schools in in ternational relations, this research examines the voca tion of these schools of thought to reveal their analytic potential by maintaining a dialogue with information and communication sciences, especially the theory of communicative action, rhetoric and discursive analysis. a multilateral construction of universalism that rests on procedural resources and values the consecration of multilateralism just after the Second World War was based on two presuppositions. First, it would be a specific discourse about values presumed as universal and indivisible, all of them concerning humanity as a whole. thus, this crucial contribution to the international relations civilization process has always been based on a highly normative judge ment. Second, multilateralism is based on a reiterated game, in other words on repeated and more or less codified procedures and rituals. Embodied in judi cially consecrated organs, multilateralism conceived as a method, tends to introduce so much reciprocity and foreseeable choices among the actors that they encourage the implementation of a certain kind of di plomacy. Most certainly, procedural formalism often prevails over the rationality or efficiency of the deci sions taken at the Un. But taken as a place for learning by experience and a source for diffusion of norms, the organization and Its bureaucracy generate spaces of common sense and conceptions in which member states' advantage lies in enrolling. this "communica tive action", spontaneously settled in language and discourse, coordinates their interactions so as to eva luate the situation in the same way and tends to har monize their strategies of action. Once seized by the information and communication sciences, the buil ding of this collective intentionality challenges one fa mous realist paradigm in international relations. inter national organizations can be conceived as agents of influence, reducing political violence, and not only as strategic agents of interests. the Word of the Un: the legitimacy for acting and ruling the Un legitima ting process depends on concrete modes of settle ment, among its external public, of a shared belief in what is fair and, thus, to be wished. in other words, the Un depends on its ability to produce a meanin gful and expressive discourse towards them. This first Un's Word, the San Francisco Charter of april 1945, fur nishes a specific rhetoric style in the sense that it mixes pragmatic and rhetorical criteria with a precise institu tional function - establishing the Un itself. Furthermore, considering that this charter intended to found a new world order, it gained another function for its "spea kers" - not only a rhetorical one, but also becoming a matrix capable of establishing norms of reference upon which a highly specific kind of debates could take place in the international arena. Study of the Un's communication strategy also furnishes evidence concerning the building of its identity. Since this com munication function has to reinforce the organization as a representative team, the organization frequently uses a meta-discursive posture. even if the Un's pro mise fluctuates between knowledge production and operational involvement on the ground, these textual forms can be given as rhetorical, interactional and communicational gestures. that is why the Un's poli tical action cannot be disconnected from its rheto rical and communicational matrix. thus, the ethic of consensus that determines the organization's dyna mic appears as a paradoxical motive for its legitimacy in acting and ruling. the Un as subject and instigator of a specific norm and order Last, but not least, the Un can be conceived as the subject and instigator of a specific norm and order in the sense that the pu blic sphere appropriates its normative and cognitive productions. in fact, it nourishes the circulation and promotion of key words ("human security", "common public goods", "peacekeeping operations", etc.) that engage specific convictions and mental representa tions of humanity. these norms of reference not only turn diplomatic debates towards specific bias, but also impregnate their media coverage. the refe rence to Human rights is a good example of this inter national relations' civilization process. Since the text of the San Francisco Charter and the Universal Declara tion of Human rights voted in December 1948, these rights are consecrated in both their moral and legal dimensions. This UN's contribution to "legalize" inter national relations is well known and studied by politi cal scientists. However, its ability to communicate this activity can only be questioned by the information and communication sciences. this focus on indivi duals establishes the fact that states' sovereignty now has strong limitations. Human rights are raised as fun damental "coordinates" of peace and inspire mental representations that are increasingly legitimate. thus, they nourish a normative activity which places the Un in keeping with the long time scope. the Un's values inspire the rhetorical expression of a vision of humanity and stabilize in some specific "texts" that also work as guarantees for their moral authority. Finally, each of these discursive practices also poses the problematic of its uses. in the beginning of the 21 st Century, this questions more fundamentally the function of univer sality itself for the institutions that promotes it and for contemporary society as a whole. Ces derniers temps, un certain nombre de phéno mènes politiques convergents invitent les recherches en SiC et celles en science politique à interagir plus explicitement. après une assemblée générale de l'ONU particulièrement médiatisée, en septembre 2011, une telle approche est pertinente. a ce jour, on ne trouve nulle trace, dans la recherche acadé mique, d'une approche communicationnelle véri table révélant toute ses dimensions au phénomène complexe des organisations internationales. or, une organisation telle que l'onU fournit un socle permet tant d'interroger des objets frontières comme l'auto rité et les nouveaux dispositifs de discussion, de déci sion et de légitimation politiques. Guillaume Devin y voit le point de rencontre entre des conduites coopé ratives plus ou moins complexes, et disposant d'une autorité et d'un pouvoir de contrainte de plus en plus indéniables. le processus d'institutionnalisation dont il s'agit s'appuie sur trois types de ressources: non seu lement juridiques et fonctionnelles, mais aussi symbo liques. C'est l'interdépendance dans laquelle fonc tionnent ces trois dimensions qui nous intéresse ici, et plus spécifiquement la part des processus communi cationnels dans la consolidation et la perpétuation de la norme onusienne. l'objectif de cette contribution est modeste et interroge les conditions d'analyse de la communication des organisations internationales. Prenant appui sur les analyses de l'école de l'interna tionalisme libéral et sur l'approche constructiviste en relations internationales, elle interroge leur vocation à révéler leur potentiel analytique en dialoguant avec les SIC, notamment la théorie de l'agir communica tionnel et les phénomènes rhétoriques et discursifs. Une construction multilatérale de l'universalisme ap puyée sur des ressources procédurales et des valeurs la consécration du multilatéralisme au lendemain du drame de la Seconde Guerre mondiale a reposé sur deux présupposés. Premièrement, il constitue un discours spécifique sur des valeurs présumées univer selles et indivisibles, et toutes liées à l'humanité. Cet apport essentiel au processus de civilisation des rela tions internationales s'appuie sur un jugement qui est donc très normatif. ensuite, il repose sur un jeu itératif, c'est-à-dire continu et répété selon des procédures de décision, implicites ou explicites, et des rituels. Concrétisé dans des organes juridiquement habili tés, le multilatéralisme, comme méthode, introduit entre les participants une norme de comportement faite de réciprocité diffuse qui est un facteur possible de réassurance, mais surtout contribue à la mise en œuvre d'une certaine diplomatie. Certes, le forma lisme procédural l'emporte souvent sur la rationalité substantielle, voire l'efficacité des décisions prises à l'onU. Mais en tant que diffuseurs de normes et lieux d'apprentissage, l'organisation et sa bureaucratie établissent des espaces de sens au sein desquels les etats ont intérêt à s'inscrire. Cet " agir communica tionnel ", ancré spontanément dans le langage et le discours, coordonne leurs interactions de façon à ce qu'ils interprètent ensemble la situation et tendent à s'accorder sur la conduite à tenir. appréhendée via les SiC, la construction de cette intentionnalité collec tive onusienne défie donc les paradigmes de l'école réaliste en relations internationales : les organisations internationales se donnent bien comme des agents d'euphémisation de la violence très sous estimés par la recherche, et pas seulement comme de simples agents d'intérêts. la parole onusienne : vers une figure de la légitimité à agir et à régir La légitimation de l'onU dépend à terme des modalités concrètes d'établissement, parmi ses publics externes, d'une croyance partagée dans ce qui est juste et donc sou haitable. C'est-à-dire de sa capacité à produire une parole et un discours <b>performants</b> et signifiants à leur attention. Première " parole " onusienne, la Charte de San Francisco d'avril 1945 fournit un genre rhétorique à part entière, en ce qu'elle associe des critères prag matiques et rhétoriques à une fonction institutionnelle cruciale (instituer l'organisation). en outre, dans la mesure où cette charte entendait fonder un nouveau monde commun, elle a acquis une seconde fonction pour les locuteurs, non plus rhétorique mais cette fois en se donnant comme une matrice susceptible de fournir un ensemble de référentiels à partir desquels s'est construit un ensemble spécifiques de débats dans l'arène internationale. l'étude de la commu nication de l'onU fournit également quelques indices solides concernant les modalités de sa construction identitaire. Cette fonction communication ayant pour mission de consolider l'organisation en tant qu'équipe de représentation, la tentation de la posture méta-dis cursive est y fréquente. Même si la parole onusienne oscille en permanence entre production de savoirs et opérationnalité sur le terrain, ces textes, dans leur visée morale et politique, s'appréhendent en tant que gestes rhétoriques, interactionnels et commu nicationnels. l'action politique de l'onU ne peut donc être déconnectée de sa matrice rhétorique, et donc communicationnelle. l'éthique du consensus qui l'anime se donne paradoxalement comme un moteur de sa légitimité non seulement à agir, mais aussi à régir. l'onU comme sujet et instigateur d'un ordre et d'une norme spécifiques L'ONU se conçoit enfin comme l'instigateur d'un ordre et d'une norme spécifiques par l'appropriation de ses productions normatives et cognitives dans l'espace public. elle nourrit en effet la circulation et la promotion de mots clefs (" sécurité humaine", " biens publics mondiaux ", " maintien de la paix " [...] .) qui engagent certaines représentations du monde. autant de normes qui tout en orientant les débats diplomatiques, imprègnent de plus en plus leur médiatisation. la référence aux droits de l'Homme fournit un exemple éclairant de ce processus de civilisation des relations internationales. Dès le texte de la Charte de San Francisco et dans la Déclaration Universelle des Droits de l'Homme votée en décembre 1948, ces droits sont consacrés dans leur dimension à la fois morale et juridique. Si la contri bution de l'onU à " juridiciser " les relations internatio nales est bien connue des politistes, la communicabi lité de cette dynamique est questionnée par les SiC. Ce recentrage sur les individus consacre le fait que la souveraineté des etats n'est plus sans limites. erigés en coordonnée de la paix à part entière et source de représentations perçues de plus en plus comme légitimes, les droits de l'Homme nourrissent une acti vité normative qui entend inscrire l'onU dans le temps long. ici, les valeurs onusiennes nourrissent donc l'ex pression rhétorique d'une vision de l'humanité et se stabilisent dans certains " textes " qui ont aussi pour fonction d'en garantir l'autorité morale. Enfin, cha cune de ces pratiques discursives pose aussi la ques tion de l'usage dont elles font l'objet. Ce qui, en ce début de XXième siècle, soulève la question du sta tut et de la fonction de l'universalité elle-même, non seulement pour les institutions qui la promeuvent, mais dans la société contemporaine toute entière...|$|R
40|$|International audienceIn video games, {{the body}} {{is central to the}} experience. Even when it is not represented, as in First Person Shooters for instance, it remains a place of vertigo. This {{impossible}} disembodiment assuredly expresses a feeling of loss of control and imbalance. Should the player have the «hand of God" or should he be a third person avatar, he moves in the here and now, through digital devices in a virtual space or an augmented reality. As a real, intimate and alien place, our body is central to our perception of the world. It is through it, with it, that we watch, touch, smell, listen. The body is the familiar and intimate space of our lives. It sometimes is the only witness to certain actions or thoughts otherwise condonable. As Michel Foucault said in «Le Corps utopique", "I can’t move without him, I can’t leave it where it is and go, myself, elsewhere. I could go {{to the end of the}} world, I could cringe in the morning under my blankets, make myself as small as I could, let myself burn in the sun on a beach, there it will always be, where I am. It is here, and nowhere else. «And yet, one of the peculiarities of the body with which we are permanently attached is that it often works without our conscious input. As a matter of fact, only pain and dysfunction allows us to recall its presence. When our eyes accidentally capture our reflection in a window or when we discover our expressions and poses on a photograph, we often experience difficulty recognizing ourselves physiologically. Thus we feel intimately connected to our bodies, but its social and aesthetic representation almost entirely escapes us. Hence perhaps our time spent dressing up, being other, carving ourselves to control this reflection. A game's avatar becomes a reflection of this elsewhere, stranger to his own image, yet embedded in everyday cultural practices. In video games, the player plays with his identity and progresses masked to achieve the ultimate dream of a ubiquitous place, where he will finally be another one. Today's media and video games act as symbolic avatars where are built the models of socialization and our individual identities. As such the individual incorporates, both symbolically and unconsciously, attitudes issued from the prevailing spirit of its inhabited time through singular practices and uses. Henceforth, the stereotypical representations associated with the body configure, discipline and dominate the individual, allowing him new experiences, discoveries and other cognitive and communicational skills. For Michel Foucault, the definition of identity as a set of relationships with others (of varied class, gender, and culture) is crossed and shaped by various hegemonic forms. In cultural studies, these representations are the expression of what is called a soft power. Indeed, «by incorporating these values, from game to game, in all of our lives, these products actively generate consent[1] ". Which invites, methodologically speaking, to consider immediately the links between symbolic forces, where hegemonic values, especially technological ones, are confronted to countercultural values until these one are in turn getting back by the process of speculative, polymorphic, opportunistic capitalism – this hegemonic figure of the contemporary world. In this way some games like Call of Duty (while they require payment for access) provide possibilities for customizing objects, weapons, accessible spaces, unusual venues for agile players with their credit card. At the heart of player-experience, marketing solicitations are coloring the structure of game mechanics, and, according to financial players, players can acquire a specific eye retina or get access to unpublished areas (Call Of Duty on Xbox 360 using the Kinect device). This hyper rationality increased of the body, is also expressed in the transhuman and posthuman ultimate dream, trying to overcome nature by incorporating the technique in order to achieve immortality (Bioshock, Deus Ex Human Revolution, Minority Report). In recent years, augmented reality has colonized the familiar gestures and video game action with the arrival of touch and gestural interfaces, video game consoles (Wii and Nintendo DS, Kinect, Microsoft Xbox and Sony Move, PlayStation) and various other technical features such as Leap Motion, tablets and cell phones. Actions unfold and dancing fingers on the keyboard becomes involuntary choreographies in the living room, on the street or becomes sensual caress of singular objects. Copenhagen Game Collective gave us B. U. T. T. O. N (Brutally Unfair Tactics Totally OK Now) and Magnetize me: these games disguise the social space with meetings and conviviality, and invite players in body contact. At the same time as the body become interfaces, devices leave the screen and grow to be seized. In this way Gigantomachia by One Life Remains or Giant Joystick by Tiltfactory depicts situations of gigantism, wondering the miniaturization of electronic components and the hyperindividualisation of computers-player relationships. In Giant Joystick, Mary Flanagan depicts a giant Joystick such as a phallus, highlighting ironically male hegemony of these practices: players find themselves caressing and embracing the joystick to interact. These intuitive interfaces allow different categories of players to have fun, regardless of their practices. Also parents, grandparents and teenagers can share a play time in which a pleasure of “playing together” seems necessary. The video game is no longer reserved to stereotype hardcore players – adolescent, geek and asocial. On the Wii Fit, we see women doing fitness activities; we see pensioners in nursing homes enjoying the Wii bowling, or also patients with the physiotherapist rehabilitating themselves with a video game. Everyone seems happy, dynamic and powerful in this gamefull society! However by carrying bodies, these interfaces create psychological discomfort. They force a performative staging, which borrows from smiles with disarray, but which also shows the difference between the canonical and ideal body presents in virtual worlds and visible and material reality. This gap is so important that it is impossible to transfer mechanisms, rhythms and sequences of actions feasible on the keyboard, of video games ‘standards to gestural devices. So if the avatar and the keyboard disappear, the player finds himself in a destabilizing primitive nakedness, where he has to face a virtual and technological space with his own body. A new kind asserts oneself with these new interfaces: the "slow gaming" close to the concept of "calm computing" by Mark Weiser probably making echo movements such as "slow food" interfaces. In this way the work of Flower That Game Company, offers PlayStation 3 games where the player is either the wind or a flower petal in a poetic walk. Child Of Eden - adaptation of the game Rez - uses the Kinect peripheral for the Xbox 360 console in a sweet musical performance that respects the rhythm of human body. Dans les jeux vidéo, le corps est central. Quand bien même celui-ci ne serait pas représenté comme dans les jeux de tir à la première personne (First Person Shooter), il reste le lieu d’un vertige. L’impossible désincorporation s’exprime assurément dans un sentiment de perte de contrôle et de déséquilibre. Que le joueur ait « la main de Dieu » ou qu’il soit l’avatar à la troisième personne, il se déplace, ici et maintenant, par l’intermédiaire de périphériques dans un espace virtuel numérique ou une réalité augmentée. Comme lieu réel, intime et étranger, le corps est au centre de notre perception du monde, c’est de lui et avec lui que nous regardons, touchons, sentons, écoutons. Le corps est l’espace familier et intime de notre vie, il est même parfois le seul témoin de certaines actions ou pensées inavouables. Comme le dit Michel Foucault dans « le corps utopique », « je ne peux pas me déplacer sans lui; je ne peux pas le laisser là où il est pour m’en aller, moi, ailleurs. Je peux bien aller au bout du monde, je peux bien me tapir, le matin, sous mes couvertures, me faire aussi petit que je pourrais, je peux bien me laisser fondre au soleil sur la plage, il sera toujours là où je suis. Il est ici irréparablement, jamais ailleurs. » Et pourtant, une des particularités de ce corps auquel nous sommes irrémédiablement attachés est qu’il fonctionne sans que nous en ayons toujours conscience. D’ailleurs, seule la douleur et ses dysfonctionnements rappellent sa présence. Lorsque notre regard capte par hasard notre reflet dans une vitrine ou que nous découvrons nos expressions et allures sur une photographie, nous éprouvons souvent des difficultés à nous reconnaître physiologiquement. Ainsi nous nous sentons intimement liés à notre corps, mais sa représentation sociale et esthétique nous échappe presque entièrement. D’où sans doute le temps passé à s’habiller, à se faire autre, à se sculpter, pour tenter de maîtriser ce reflet, cette image projetée vers les autres. L’avatar vidéoludique devient le reflet de cet ailleurs, l’étranger de sa propre image, incorporé pourtant dans des pratiques culturelles quotidiennes. Dans les jeux vidéo, le joueur joue avec son identité et avance masqué pour trouver avec et dans l’avatar la possibilité d’atteindre l’ultime rêve d’un lieu ubiquitaire où il serait enfin autre. Les médias et les jeux vidéo jouent aujourd’hui le rôle d’avatars symboliques où se construisent les modèles de socialisation et l’identité des individus. Ainsi l’individu incorpore symboliquement et inconsciemment des comportements, acquiert des mentalités issues de l’esprit dominant du temps par le biais de pratiques et d’usages singuliers. Dès lors les représentations stéréotypées attachées aux corps configurent, disciplinent et dominent l’individu, tout en lui permettant de faire de nouvelles expériences et de découvrir d’autres capacités cognitives et communicationnelles. Pour Michel Foucault, la définition de l’identité comme des relations avec autrui (de classe, de genres, de culture) sont traversées et façonnées par diverses formes hégémoniques. En études culturelles, ces représentations sont l’expression d’un pouvoir dit doux (softpower). En effet, « en incorporant ces valeurs, de jeu en jeu, dans notre vie à tous, ces produits ludiques fabriquent du consentement[1] »[i]. Lequel invite, méthodologiquement parlant, à considérer immédiatement les rapports de forces symboliques en présence, où les valeurs hégémoniques, et notamment technologiques, viennent se confronter aux valeurs en résistance, jusqu’à ce que ces dernières soient récupérées par le processus d’un capitalisme spéculatif, polymorphe et opportuniste, cette figure hégémonique du monde contemporain. Ainsi certains jeux comme Call of Duty (alors qu’ils sont payants d’accès), offrent des possibilités de personnalisation d’objets et d’armes, des espaces accessibles, des lieux inédits pour les joueurs agiles de leur carte bancaire. Au cœur même de l’expérience-joueur, des sollicitations marketing viennent colorer la structure des mécanismes de jeu, et selon les moyens financiers des joueurs, ceux-ci peuvent acquérir une rétine oculaire spécifique ou accéder à des zones inédites (Call Of Duty sur Xbox 360 utilisant le périphérique Kinect). Cette hyperrationalité augmentée du corps, s’exprime également dans l’ultime rêve transhumain et posthumain en cherchant à dépasser la nature par incorporation de la technique afin d’atteindre l’immortalité (Bioschok, Deus Ex Human Revolution, Minority Report). Ces dernières années, la réalité augmentée a colonisé les gestes familiers et les actions vidéoludiques avec l’arrivée d’interfaces tactiles et gestuelles, de consoles de jeux vidéo (Wii et la DS chez Nintendo, Kinect chez Microsoft Xbox et Move chez Sony, Playstation), ainsi que d’autres dispositifs techniques différents tels que la Leap Motion, les tablettes tactiles et les téléphones cellulaires. Les gestes se déploient et la danse des doigts sur le clavier se transforme en chorégraphies involontaires dans le salon, dans la rue ou devient caresse sensuelle d’objets singuliers. Copenhague Game Collective propose B. U. T. T. O. N (Brutally Unfair Tactics Totally OK Now) et Magnetize me, où les jeux viennent habiller l’espace social de rencontre et de convivialité en invitant les joueurs au contact corporel. En même temps que les corps deviennent interfaces, les périphériques quittent l’écran et grandissent afin d’être saisis, ainsi Gigantomachie de One Life Remains ou Giant Joystick de Tiltfactory mettent en scène des situations de gigantisme, interrogeant la miniaturisation des composants électroniques et l’hyperindividualisation des relations ordinateurs-joueurs. Dans Giant Joystick Mary Flanagan met en scène un Joystick géant tel un phallus mettant en évidence ironiquement l’hégémonie masculine de ces pratiques : les joueurs se retrouvent à caresser et embrasser ce joystick pour interagir. Ces interfaces intuitives permettent à différentes catégories de joueurs de prendre du plaisir, quelles que soient leurs pratiques. Ainsi parents, grands-parents et adolescents peuvent partager un temps de jeu où s’impose un plaisir de « jouer ensemble », le jeu vidéo n’étant plus réservé à des joueurs inconditionnels dont le stéréotype est l’adolescent, geek et asocial. On voit alors des femmes sur la Wii Fit faire leurs activités de fitness, des retraités dans les maisons de retraite s’amuser au Wii bowling, des patients chez le kinésithérapeute se rééduquer devant un jeu vidéo. Tous semblent heureux, dynamiques et <b>performants</b> dans une société de plus en plus gamifiée ! Pourtant en convoquant les corps, ces interfaces créent aussi un malaise psychologique, elles forcent une mise en scène performative, qui, si elle fait rire de désarrois, montre l’écart entre les canons d’un corps idéal dans les univers virtuels et sa réalité matérielle visible. Cet écart apparaît alors si important qu’il est impossible de transférer les mécanismes de jeu vidéo standards vers ces dispositifs gestuels, les rythmes et les séquences d’actions devenant irréalisables hors du clavier. Ainsi si l’avatar et le clavier disparaissent, le joueur se retrouve dans une nudité primitive déstabilisante, où il doit avec son seul corps se confronter à un espace virtuel et technologique. Un nouveau genre s’affirme alors avec ces interfaces le « slow gaming » proche du concept de « calm computing » de Mark Weiser faisant sans doute écho aux mouvements tels que « slow food ». Ainsi l’œuvre Flower de That Game Compagny, propose un jeu sur PlayStation 3 où le joueur incarne soit le vent soit un pétale de fleur dans une promenade poétique. Child Of Eden adaptation du jeu Rez, utilise le périphérique Kinect de la console Xbox 360 dans une performance musicale douce qui respecte le rythme corporel humain...|$|R
50|$|By {{leveraging}} the Erlang language, CSCM can {{target a}} concise, precise specification of the Scheme language. Consequently, by leveraging the Erlang VM, CSCM can target a <b>performant,</b> robust {{implementation of the}} Scheme language. The default language is Scheme R7RS and the default virtual machine is Erlang/OTP 17 or higher. Scheme R5RS is available as a Scheme library.|$|E
50|$|DRM {{has been}} {{paramount}} {{for the development}} and implementations of well-defined and <b>performant</b> free and open-source graphics device drivers without which no rendering acceleration would be available at all, or even worse, only the 2D drivers would {{be available in the}} X.Org Server. DRM was developed for Linux, and since has been ported to other operating systems as well.|$|E
50|$|BCX {{contains}} verbs that {{simplify the}} creation of Windows UI desktop applications. Unlike many BASIC implementations that rely on run-time engines, the combination of BCX and most C/C++ compilers produce efficient and <b>performant</b> native code applications. BCX {{can be used to}} create GUI, DLL, console mode, and web server applications. BCX can use the Standard C Library.|$|E
50|$|The Gender Equality Index {{shows that}} in 2010, despite more than 50 years of gender {{equality}} policy, the EU is only just over half way towards {{a society that is}} both gender equal and <b>performant</b> with a score of only 54 out of 100.A breakdown of the scores by Member States and domains is provided in the table below.|$|E
50|$|In {{concurrent}} systems, particularly distributed systems, when a {{group of}} processes must interact in order to progress, if the processes are scheduled at separate times or on separate machines (fragmented across time or machines), the time spent waiting for each other or in communicating with each other can severely degrade performance. Instead, <b>performant</b> systems require coscheduling of the group.|$|E
50|$|The Gender Equality Index is a {{synthetic}} indicator that measures how far (or close) the EU and its Member States are from achieving {{a society that}} is both gender equal and <b>performant.</b> The measurement used produces a number that ranges between 1 and 100, where 100 stands for the best situation, where there are no gender gaps combined with the highest level of achievement.|$|E
5000|$|VMware Workstation 12 was {{reviewed}} by Infoworld. The review mainly concerns differences with earlier versions, concluding that this version has moderate improvements, mainly to support newer hardware and operating systems, {{that may not}} be worth the cost of upgrading. The review concludes that Workstation 12 is [...] "the most <b>performant,</b> polished, and feature-rich desktop virtualization product available", but points out that it is the most expensive.|$|E
