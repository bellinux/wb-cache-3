30|20|Public
50|$|He also computated 41 digits of , {{based on}} <b>polygon</b> <b>approximation</b> and Richardson extrapolation.|$|E
50|$|The tetracontaoctagon {{appeared}} in Archimedes' <b>polygon</b> <b>approximation</b> of pi, {{along with the}} hexagon (6-gon), dodecagon (12-gon), icositetragon (24-gon), and enneacontahexagon (96-gon).|$|E
40|$|The paper {{presents}} a parallel algorithm for <b>polygon</b> <b>approximation</b> targeted at reconfigurable multi-ring hardware. The proposed algorithm grows {{the edges of}} <b>polygon</b> <b>approximation</b> {{that is based on}} principle of merging. The edge/s are grown simultaneously at point/s where the minimum merging error is produced. The merging process is made faster by carrying it out in two stages: i) During first stage it uses templates, generated during an off line process, to carry out fast initial <b>polygon</b> <b>approximation,</b> ii) The segments of the initial <b>polygon</b> <b>approximation</b> are further merged during the second stage. The simultaneous growing of edges makes the algorithm suitable for a parallel processing hardware. The paper outlines a parallel algorithm for <b>polygon</b> <b>approximation.</b> It discusses three broadcasting mechanisms for utilizing the multi-ring hardware. The mapping of the <b>polygon</b> <b>approximation</b> algorithm on the multi-ring topology using various broadcasting mechanisms is discussed. Index Terms- Parallel algorithm for <b>polygon</b> <b>approximation,</b> Local and global error for <b>polygon</b> <b>approximation,</b> Reconfigurable multi-ring network, broadcasting mechanisms for multi-ring network. ...|$|E
40|$|AbstractIn this paper, {{we present}} {{approximation}} algorithms for minimum vertex and edge guard problems for polygons {{with or without}} holes {{with a total of}} n vertices. For simple <b>polygons,</b> <b>approximation</b> algorithms for both problems run in O(n 4) time and yield solutions that can be at most O(logn) times the optimal solution. For <b>polygons</b> with holes, <b>approximation</b> algorithms for both problems give the same approximation ratio of O(logn), but the running time of the algorithms increases by a factor of n to O(n 5) ...|$|R
40|$|Abstract — We propose {{algorithms}} that compute <b>polygon</b> <b>approximations</b> for convex contours. This geometric {{problem is}} relevant in interpolation theory, data compression, and has potential applications in robotic sensor networks. The algorithms {{are based on}} simple feedback ideas, on limited nearest-neighbor information, and amount to gradient descent laws for appropriate cost functions. The approximations are based on intuitive performance metrics, such as {{the area of the}} inner, outer, and “outer minus inner ” approximating polygons. I...|$|R
5000|$|The maximum allowed {{size of the}} <b>approximation</b> <b>polygon</b> (for triangulations it can be maximum allowed {{length of}} {{triangle}} sides). This parameter ensures enough detail for further analysis.|$|R
40|$|Polygon/spline {{approximation}} of arbitrary image region shapes is submitted for tool evaluation. First, <b>polygon</b> <b>approximation</b> is performed. Starting {{with an initial}} 4 [...] vertices polygon which represents the farest elongations into different directions, the <b>polygon</b> <b>approximation</b> is iteratively refined {{taking into account the}} local requirements until a given tolerance criterion is satisfied everywhere. Then, spline approximation is calculated using the polygon vertices. Whenever spline approximation also satisfies the tolerance criterion for a certain part of the shape, spline approximation substitutes <b>polygon</b> <b>approximation</b> for that part thus giving the approximated shape a more natural look. By this combined approach, the advantages of both techniques are utilized and their disadvantages avoided. Thus, the combined approach is efficient in terms of reduction of the number of shape points for which coordinates must be transmitted and with this also in terms of bit rate reduction. It has no [...] ...|$|E
40|$|This report {{deals with}} edge and line {{detection}} in pictures with complicated and/or blurred objects. It explores the alternatives available, in edge detection, edge linking and object recognition. Choice of methods are the Canny edge detection and Local edge search processing combined with regional edge search processing {{in the form}} of <b>polygon</b> <b>approximation.</b> </p...|$|E
40|$|The authors {{present a}} {{graduate}} iterative merging algorithm for <b>polygon</b> <b>approximation.</b> The algorithm always gives a minimum area {{difference between the}} curve and the polygon with the minimum number of breakpoints. Realisation of the algorithm is compared with other algorithms as tested on good and noisy images and the performance {{is very close to}} that of a human viewer. link_to_subscribed_fulltex...|$|E
40|$|This paper {{presents}} a new error bound simplification algorithm for complex geometric models. A lower <b>polygon</b> count <b>approximation</b> of the input model {{is generated by}} performing edge collapse operations. The collapse vertex is constrained to lie within a localised tolerance volume built around the edge collapse neighbourhood. This constraint ensures that all points on the simplified surface are within a user specified distance from the surface after the previous edge collapse...|$|R
5000|$|The maximum allowed angle {{between two}} {{adjacent}} <b>approximation</b> <b>polygons</b> (on the same face). This parameter ensures that even very small humps or hollows {{that can have}} significant effect to analysis will not disappear in mesh.|$|R
5000|$|The maximum allowed {{distance}} between the planar <b>approximation</b> <b>polygon</b> and the surface (known as [...] "sag"). This parameter ensures that mesh is similar enough to the original analytical surface (or the polyline {{is similar to the}} original curve).|$|R
40|$|Abstract. In {{different}} types of information systems, such as multimedia information systems and geographic information systems, object-based information is represented via polygons corresponding to the boundaries of object regions. In many applications, the polygons have large number of vertices and edges, thus a way of representing the polygons with less number of vertices and edges is developed. This approach, called <b>polygon</b> <b>approximation,</b> or polygon simplification, is basically motivated with the difficulties faced in processing polygons with large number of vertices. Besides, large memory usage and disk requirements, {{and the possibility of}} having relatively more noise can also be considered as the reasons for polygon simplification. In this paper, a kinematics-based method for <b>polygon</b> <b>approximation</b> is proposed. The vertices of polygons are simplified according to the velocities and accelerations of the vertices with respect to the centroid of the polygon. Another property of the proposed method is that the user may set the number of vertices to be in the approximated polygon, and may hierarchically simplify the output. The approximation method is demonstrated through the experiments based on a set of polygonal objects. ...|$|E
40|$|The thesis {{presents}} a visual position measurement {{system that allows}} an Automated Guided Vehicle (AGV) to recover its absolute position anywhere on a pseudo-random encoded guide-path. It {{is based on the}} "window property" of pseudo-random binary sequences. The guide-path consists of an (2 n- 1) bit long encoding pseudo-random binary sequence marked on the floor as a sequence of geometric binary symbols ('E' for the binary value 1, and 'H' for the binary value 0). A long portion of the encoded guide-path has to be visible to the camera mounted on the AGV, in order to provide enough information to recover the n-bit pseudo-random window attached to any point in the field of view. The binary contents of this window uniquely identifies the absolute position of the encoded point. Image analysis and pattern recognition algorithms have been developed for the visual recovery of the pseudo-random window. The image processing essentially consists of the following steps: thresholding of the original image, boundary tracing each object in the image, <b>polygon</b> <b>approximation</b> of each boundary, extracting vertices and corners from each polygonal contour, and finally binary-symbol recognition by tree search. An original implementation of the split and merge <b>polygon</b> <b>approximation</b> results in an improvement of the run-time performance of this algorithm. An experimental vision system was implemented to test its performance for AGV absolute position recovery applications...|$|E
40|$|<b>Polygon</b> <b>approximation</b> is an {{important}} step in the recognition of planar shapes. Traditional polygonal approximation algorithms handle only images that are related by a similarity transformation. The transformation of a planar shape as the viewpoint changes with a perspective camera is a general projective one. In this paper, we present a novel method for polygonal approximation of closed curves that is invariant to projective transformation. The polygons generated by our algorithm from two images, related by a projective homography, are isomorphic. We also describe an application of this in the form of numeral recognition. We demonstrate the importance of this algorithm for real-life applications like number plate recognition and aircraft recognition...|$|E
40|$|We {{present a}} new {{approximate}} occlusion-culling algorithm that {{in contrast to}} other algorithms, manages the objects of the scene in a 3 D-sectorgraph. For generating a frame, {{as far as possible}} only the visible objects are rendered that can be found quickly by an edge of the graph. The algorithm allows a real-time navigation with over 20 frames per second in complex scenes consisting of over 10 millions of <b>polygons.</b> Moreover, <b>approximation</b> errors are very low...|$|R
40|$|The use of {{topological data}} {{structures}} for <b>polygon</b> based <b>approximation</b> of 3 D objects leads to great advantages in many computer graphics applications. Especially in algorithms for rendering or global illumination their use can yield better performance and more accurate solutions. We will show, how the Hierarchical Radiosity algorithm benefits from using topological data structures and how typical drawbacks {{of the classical}} quad-tree based approach can be avoided. Advantages of this method are better performance for complex scenes and an accurate Gouraud shading even for curved surfaces...|$|R
40|$|Abstract — This paper {{presents}} a special hardware implementation {{developed for the}} computation of the specular term {{which is the most}} time consuming part in the Phong's illumination. In the Phong shading, the exponentiation operation of two floating-point numbers is necessary for each point inside a <b>polygon.</b> An <b>approximation</b> algorithm is developed to speed up the exponentiation operation, and it is supported by simple hardware that can be easily merged into a floating-point multiplier. The exponentiation operation takes just 4 cycles in the proposed hardware while it takes about 100 - 200 cycles in conventional floating-point units. Although an approximation algorithm is employed for the exponentiation operation, the amount of error is so minimal that the difference is virtually indistinguishable. I...|$|R
40|$|Abstract. The {{use of an}} {{alphabet}} of {{line segments}} to compose a curve is a possible approach for curve data compression. Curves having more sophisticated topology with self-intersections can be handled by methods considering recursive decomposition of the canvas containing the curve. We propose a graph theory based algorithm for tracing the curve directly to eliminate the decomposition needs. This approach obviously improves the compression performance, as longer line segments can be used. We tune our method further by selecting optimal turns at junctions during tracing the curve. We assign a <b>polygon</b> <b>approximation</b> to the curve which consists of letters coming from an alphabet of line segments. ...|$|E
40|$|Recognition of {{discrete}} planar contours under similarity transformations {{has received a}} lot of attention but little work has been reported on recognizing them under more general transformations. Planar object boundaries undergo projective and affine transformations across multiple views when the cameras used are perspective and affine respectively. We present two methods to recognize discrete curves in this paper. The first method computes a piecewise parametric approximation of the discrete curve that is projectively invariant. A <b>polygon</b> <b>approximation</b> scheme and a piecewise conic approximation scheme are presented here. The second method computes an invariant sequence directly from the sequence {{of discrete}} points on the curve in a Fourier transform space. The sequence is shown to be identical upto a scale factor in all affine related views of the curve. We present the theory and demonstrate its applications to several problems including numera...|$|E
40|$|This {{dissertation}} {{is concerned}} with the development of an automatic handling and assembling system combining the use of a fixed base manipulator with an image processing and recognition system. To this end, the recognition of objects in the working environment of the manipulator is of paramount importance and guides the development of the work in this thesis. The recognition system uses a two-dimensional image representation of the objects and extracts appropriate object features, while in a second level a classification scheme is used in order to recognize the objects. New object feature vectors are proposed, which are based on the <b>polygon</b> <b>approximation</b> of the object shape and a new neural multi-classifier scheme is proposed aiming at the enhancement of the recognition accuracy. Basic components of the recognition system are image preprocessing, feature vector extraction and object classification. During the image preprocessing phase the various objects are initially detected and in the sequel transformed so that useful parameters (centres of holes, area, center of area, perimeter, rotation angle) can be calculated. These parameters are in the sequel used in order to derive useful feature vectors, or are used as supplemental information for the guidance of the manipulator. Feature extraction is based on descriptors that use one dimensional representation of the surrounding line of a 2 -dimensional figure, aiming at the production of reliable features with low computational cost. Descriptors are based on geometrical features, on statistical moments and on <b>polygon</b> <b>approximation</b> of object shapes. This way, three independent feature vectors are formed, which are invariant to the position, the size and the orientation of image objects. The first vector is composed of three complementary geometrical features based on the area of the figure its perimeter and the distances of the contour points to the center of area. The second vector is formed by using scaled normalized central moments, which arrive from statistical moments and do not suffer from the exponential growth presented by high order moments. The last vector is composed of features produced by a stepwise <b>polygon</b> <b>approximation</b> method, which was derived in this dissertation aiming at the real time extraction of reliable features. Stepwise <b>polygon</b> <b>approximation</b> offers a satisfactory {{solution to the problem of}} optimally determining a polygon with predefined number of edges able to approximate a given shape contour. The proposed methodology is particularly useful in situations that time of computation is critical. It uses a technique of repeating approximations with successively reducing input data points. Following this procedure two feature vectors are extracted, which use the angles of the approximating polygon and the successive approximation errors. These errors are defined to be the ratio of the initial contour length to the length of an edge determined by two successive vertexes of the polygon. The two features are invariant to space, magnitude and orientation of the shape they describe. Therefore, are mighty features and can be used either independently or in combination with other features in 2 -dimensional shape recognition applications. In the last stage of the recognition procedure, a multi-classifier scheme is proposed, which is based on neural classifiers and comprises two levels. In the first level, neural classifiers are trained to perform independently of each other, based on different feature vectors. In the second level the final classification is performed, combining the outputs of the first level classifiers and using a simplified layout of small neural networks. More specifically, a low order neural network is assigned to all similar output classes of the independent first level classifiers and is trained to perform the final classification. This way, the ability of each first level classifier to differentiate its results based on the appearance of non-trained data is better taken into consideration in the final judgment. The proposed architecture is of low complexity presenting at the same time high recognition rates. Moreover, it requires small number of training cycles, making thus the recognition scheme appropriate for demanding classification applications, where in a changing environment the system needs retraining during its operation. The performance of the proposed multi-classifier architecture is tested and compared to other existing techniques based on an adequate number of digitally modified patterns. Finally, the proposed methodologies are tested by building a robotic vision application experiment using the existing industrial manipulator of the laboratory of automatic control systems and robotics of Democritus University of Thrace. ...|$|E
40|$|Marching triangles is {{a method}} for {{producing}} a <b>polygon</b> mesh surface <b>approximation</b> composed of triangular facets which are approximately equilateral. This paper improves the Marching Triangles algorithm where the inputs are multiple range images of scenes. C 1 discontinuities (fold edges) are detected and used to constrain the final triangulation, thus increasing {{the accuracy of the}} mesh at sharp edges and corners and decreasing the number of triangles with a poor aspect ratio...|$|R
40|$|This thesis {{describes}} {{and develops}} {{procedures for the}} generation of theoretical lightcurves {{that can be used}} to model gravitational microlensing events that involve multiple lenses. Of particular interest are the cases involving a single lens star with one or more orbiting planets, as this has proven to be an effective way of detecting extrasolar planets. Although there is an analytical expression for microlensing lightcurves produced by single lensing body, the generation of model lightcurves for more than one lensing body requires the use of numerical techniques. The method developed here, known as the semi-analytic method, involves the analytical rearrangement of the relatively simple ‘lens equation’ to produce a high-order complex lens polynomial. Root-finding algorithms are then used to obtain the roots of this ‘lens polynomial’ in order to locate the positions of the images and calculate their magnifications. By running example microlensing events through the root-finding algorithms, both the speed and accuracy of the Laguerre and Jenkins-Traub algorithms were investigated. It was discovered that, in order to correctly identify the image positions, a method involving solutions of several ‘lens polynomials’ corresponding to different coordinate origins needed to be invoked. Multipole and <b>polygon</b> <b>approximations</b> were also developed to include finite source and limb darkening effects. The semi-analytical method and the appropriate numerical techniques were incorporated into a C++ modelling code at VUW (Victoria University of Wellington) known as mlens 2. The effectiveness of the semi-analytic method was demonstrated using mlens 2 to generate theoretical lightcurves for the microlensing events MOA- 2009 -BLG- 319 and OGLE- 2006 -BLG- 109. By comparing these theoretical lightcurves with the observed photometric data and the published models, it was demonstrated that the semi-analytic method described in this thesis is a robust and efficient method for discovering extrasolar planets...|$|R
40|$|Abstract. In {{this paper}} we propose and analyse a hybrid numerical-asymptotic {{boundary}} element method for the solution of problems of high frequency acoustic scattering by a class of soundsoft nonconvex <b>polygons.</b> The <b>approximation</b> space is enriched with carefully chosen oscillatory basis functions; these are selected via {{a study of the}} high frequency asymptotic behaviour of the solution. We demonstrate via a rigorous error analysis, supported by numerical examples, that to achieve any desired accuracy it is sufficient for the number of degrees of freedom to grow only in proportion to the logarithm of the frequency as the frequency increases, in contrast to the at least linear growth required by conventional methods. This appears to be the first such numerical analysis result for any problem of scattering by a nonconvex obstacle. 1. Introduction. Ther...|$|R
30|$|The set of unordered points was {{produced}} by simple edge detection. Then, zero mean Gaussian noise with varying standard deviation was added {{in order to get}} several configurations of signal-to-noise ratio (SNR). A representative result of a degraded contour is given in Figure 5 d. Note that no ordering of points may be established in that case and thus <b>polygon</b> <b>approximation</b> may not be performed. To make the experiment independent from the noise configuration, each experiment was repeated 20 times. The algorithm assumes that a form of binary data (e.g., an edge map) is provided. Degradation by noise is performed after the edge extraction in order to examine the behavior of the algorithm to the detection of line segments. If the noise was added to the original image, the edges would be erroneous and we would not have a standard baseline for evaluating the algorithm.|$|E
40|$|The {{processing}} of spatial joins can be greatly improved {{by the use}} of filters that reduce the need for examining the exact geometry of polygons in order to find the intersecting ones. Approximations of candidate pairs of polygons are examined using such filters. As a result, three possible sets of answers are identified: the positive one, composed of intersecting polygon pairs; the negative one, composed of non-intersecting polygon pairs; and the inconclusive one, composed of the remaining pairs of candidates. To identify all the intersecting pairs of polygons with inconclusive answers, it is necessary to have access to the representation of polygons so that an exact geometry test can take place. Generally, an approximation can identify only positive and inconclusive answers, or only negative and inconclusive answers. In that way, two complementary approximations are required for filter construction. This article presents a <b>polygon</b> <b>approximation</b> for spatial join processing [...] ...|$|E
40|$|One of {{the most}} {{difficult}} tasks of ship detection is detecting the ship which is docking at the port in remote sensing image. Traditional methods of automatic detection cannot be used to detect the land/waterboundaries, because both the gray values and textural features of a port {{are similar to those of}} the ships which are docking at the port. Therefore, ships cannot be accurately detected in this case. In this study, a novel method of land/water-boundaries detection is proposed, which is based on a <b>polygon</b> <b>approximation</b> method by incorporating two techniques, i. e., Fuzzy Randomized Hough Transform (FRHT) and Multi-Points Curvature (MPC). The method considered the feature of human vision that the straight-line of the land/water-boundaries can be detected more accurately and rapidly. With the detection result of land/water-boundaries, ships docking at the port can be accurately detected. The experiment results demonstrate that this method can achieve good result of ship detection...|$|E
40|$|Lixian Han Hongjun Zhu Jianwen Su University of California at Santa Barbara ABSTRACT Evaluation of {{a spatial}} join {{typically}} has two steps: a filter step that applies the join on approximations of spatial objects and a refinement step that determines the actual intersection of two spatial objects. In this paper, we study the e#ectiveness of filters with di#erent approximations including minimum bounding rectangles, rectilinear approximations, minimum 5 -corner bounding convex polygons, and convex hulls. We compare the e#ectiveness of filters {{by comparing the}} number of "false hits" in the filter result using these approximations. We analyze the impact of parameters such as aspect ratios of input <b>polygons</b> and <b>approximation</b> quality on the filter e#ectiveness. 1. INTRODUCTION The conventional approach to evaluate a spatial join uses two steps: (1) a filter step that checks intersections of approximations of spatial objects; and (2) a refinement step that check intersection of objects [...] ...|$|R
40|$|In {{this paper}} we propose and analyse a hybrid numerical-asymptotic {{boundary}} element method for the solution of problems of high frequency acoustic scattering by a class of sound-soft nonconvex <b>polygons.</b> The <b>approximation</b> space is enriched with carefully chosen oscillatory basis functions; these are selected via {{a study of the}} high frequency asymptotic behaviour of the solution. We demonstrate via a rigorous error analysis, supported by numerical examples, that to achieve any desired accuracy it is sufficient for the number of degrees of freedom to grow only in proportion to the logarithm of the frequency as the frequency increases, in contrast to the at least linear growth required by conventional methods. This appears to be the first such numerical analysis result for any problem of scattering by a nonconvex obstacle. Our analysis is based on new frequency-explicit bounds on the normal derivative of the solution on the boundary and on its analytic continuation into the complex plane...|$|R
40|$|Abstract: An {{approach}} to polygonal approximation of regular digital curves based on PSO algorithm is presented. In this paper, each particle {{corresponds to a}} candidate solution to the polygonal approximation problem, which is represented as a binary vector. The offset error of centroid between the original curve and the <b>approximation</b> <b>polygon,</b> and the variance of distance error for each approximation segment are adopted in the fitness function to evaluate the feasibility degree of the candidate solution. The sigmoid function of iteration times is used as the acceleration factors instead of the constant factors to improve the global searching characteristics. Experimental {{results show that the}} proposed approach can get suitable approximation results for preserving the features of original curves...|$|R
40|$|Piecewise linear digital curve {{representation}} and compression using graph theory {{and a line}} segment alphabet Abstract — The use of an alphabet of line segments to compose a curve is a possible approach for curve data compression. Many approaches are developed with the drawback that they can process simple curves only. Curves having more sophisticated topology with self-intersections can be handled by methods considering recursive decomposition of the canvas containing the curve. In this paper, we propose a graph theory based algorithm for tracing the curve directly to eliminate the decomposition needs. This approach obviously improves the compression performance, as longer line segments can be used. We tune our method further by selecting optimal turns at junctions during tracing the curve. We assign a <b>polygon</b> <b>approximation</b> to the curve which consists of letters coming from an alphabet of line segments. We also discuss how other application fields {{can take advantage of}} the provided curve description scheme...|$|E
40|$|In {{this paper}} {{we present a}} novel {{approach}} to shape similarity estimation. The target application is content-based indexing and retrieval over large image databases. The technique is based on wavelet decomposition and uses <b>polygon</b> <b>approximation</b> over several scales. This technique uses simple features (aspect ratio, angles, distances from the centroid, distance ratios and relative positions) extracted at high curvature points. These points are detected at each level of the wavelet decomposition as wavelet transform modulus maxima. The experimental results and comparisons show {{the performance of the}} proposed technique. This technique is also suitable to be extended to the retrieval of 3 -D objects. Introduction Since the early 1990 s, content-based indexing and retrieval (CBIR) of digital images became a very active area of research. Both industrial and academic systems for image retrieval have been built. Most of these systems (e. g. QBIC [9] from IBM, Netra from UCSB, Virage from Virage [...] ...|$|E
40|$|In {{this paper}} we {{consider}} the following problem in computational geometry which has applications in VLSI floorplan design and image processing. Given an orthogonal polygon P (i. e. edges are either vertical or horizontal) with n vertices and a positive integer m ! n, determine an orthogonal polygon Q with {{less than or equal}} to m vertices such that ERROR(P;Q) is minimized, where ERROR(P;Q) is defined as the area of the symmetric difference of the interiors of the two polygons. We present a polynomial time optimal algorithm to solve this problem for convex orthogonal polygons. Our algorithm is based on a transformation of the <b>polygon</b> <b>approximation</b> problem into a constrained shortest path problem. 1 Introduction An orthogonal polygon is a polygon in which every edge is either vertical or horizontal. In this paper, we only consider orthogonal polygons, hence the terms polygon and orthogonal polygon will be used interchangeably. Given a polygon P with n vertices, and an integer m ! n, o [...] ...|$|E
40|$|Many {{acoustic}} and {{electromagnetic wave}} scattering {{problems can be}} formulated as the Helmholtz equation. Standard boundary element method (BEM) solution of these problems becomes expensive as frequency increases. Recently Chandler-Wilde and Langdon [1] have proposed a novel Galerkin BEM {{to solve the problem}} of acoustic scattering by a sound soft convex <b>polygon,</b> with the <b>approximation</b> space consisting of piecewise polynomials supported on a graded mesh, with smaller elements adjacent to the corners of the polygon, multiplied by plane wave basis functions. They proved via rigorous error analysis, supported by numerical experiments, that the number of degrees of freedom required to achieve a prescribed level of accuracy need only grow logarithmically as frequency increases. In this paper we extend their scheme to problems of scattering by impedance convex polygons. Here we describe the implementation of the numerical method. A rigorous error analysis of this problem will appear in [2]...|$|R
40|$|Many {{classes of}} scenes contain {{objects that are}} (approximately) {{two-dimensional}} polygons—for example, buildings in an aerial photograph, or flat mechanical parts on a tabletop. This paper deals {{with the problem of}} recovering (an approximation to) an unknown polygon from noisy digital data, obtained by digitizing either an image of the (solid) polygon or a sequence of points on its boundary. Note that our goal is to obtain an approximation to the original <b>polygon,</b> not an <b>approximation</b> to the noisy data. We derive constraints on the polygon and on the noisy digitization process under which (approximate) recovery of the polygon is possible. We show that if these constraints are satisfied, the desired approximation can be recovered by selecting a subset of the data points as vertices. We define a vertex elimination process that accomplishes this recovery and give examples of successful recovery of both synthetic and real noisy polygons. c ○ 2002 Elsevier Science (USA) 1...|$|R
40|$|Abstract—Low-power compact sensor nodes {{are being}} increas-ingly {{used to collect}} {{trajectory}} data from moving objects such as wildlife. The size of this data can easily overwhelm the data storage available on these nodes. Moreover, the transmission of this extensive data over the wireless channel {{may prove to be}} difficult. The memory and energy constraints of these platforms underscores the need for lightweight online trajectory compres-sion albeit without seriously affecting the accuracy of the mobility data. In this paper, we present a novel online <b>Polygon</b> Based <b>Approximation</b> (PBA) algorithm that uses regular polygons, the size of which is determined by the allowed spatial error, as the smallest spatial unit for approximating the raw GPS samples. PBA only stores the first GPS sample as a reference. Each subsequent point is approximated to the centre of the polygon containing the point. Furthermore, a coding scheme is proposed that encodes the relative position (distance and direction) of each polygon with respect to the preceding polygon in the trajectory. The resulting trajectory is thus a series of bit codes, that have pair-wise dependencies at the reference point. It is thus possible to easily reconstruct an approximation of the original trajectory by decoding the chain of codes starting with the first reference point. Encoding a single GPS sample is an O(1) operation, with an overall complexity of O(n). Moreover, PBA only requires the storage of two raw GPS samples in memory at any given time. The low complexity and small memory footprint of PBA make it particularly attractive for low-power sensor nodes. PBA is evaluated using GPS traces that capture the actual mobility of flying foxes in the wild. Our results demonstrate that PBA can achieve up to nine-fold memory savings as compared to Douglas-Peucker line simplification heuristic. While we present PBA in the context of low-power devices, it can be equally useful for other GPS-enabled devices such smartphones and car navigation units. I...|$|R
