5|17|Public
5000|$|Average <b>Page</b> <b>Depth</b> / Page Views per Average Session - <b>Page</b> <b>Depth</b> is the {{approximate}} [...] "size" [...] {{of an average}} visit, calculated by dividing total number of page views by total number of visits.|$|E
5000|$|While {{the article}} count increased, extreme care {{was taken to}} {{maintain}} the quality of articles. The <b>page</b> <b>depth</b> of the wiki remains high at 301 (...) [...] When the Wikipedia crossed 10,000 articles on June 1, a number of print and online newspapers covered the story.Recently, a Malayalam newspaper 'Madhyamam' spent an editorial for the contributors of Malayalam Wikipedia. The mobile version of the Malayalam Wikipedia was launched on February 2010.|$|E
30|$|Constrained by the {{crawling}} algorithms for automatic web content download, it {{is difficult}} for crawlers to perfectly mimic human beings’ visiting patterns. Therefore, path related features can effectively differentiate crawlers and normal users. Stevanovic et al. (2013) uses standard deviation of requested <b>page</b> <b>depth</b> as one feature to describe the visiting path. However, it cannot accurately reflect the difference between crawlers and normal users since the <b>page</b> <b>depth</b> is simply extracted from parsing the URL. Tan and Kumor (2004) learn session depth and width from the referrer field of HTTP request headers to more accurately describe the path information. However, it is easy for intelligent crawlers to fake the referrer field of HTTP headers. Similarly, PathMarker also largely rely on path-related features to identify crawlers. Differently, PathMarker relies on the URL marker appended to each URL to learn the referring relationship between two requests. The URL marker and path of a URL are encrypted so the crawler cannot fake visiting path through forging URL markers.|$|E
5000|$|To {{give its}} <b>pages</b> <b>depth</b> and breadth, Canadian Literature {{alternates}} general and special issues. The general issues {{deal with a}} range of periods and topics, while the special issues focus on more specific topics, including travel, ethnicity, womens writing, and multiculturalism. Canadian Literature is not aligned with any single theoretical approach; rather, it is interested in exploring articles on all subjects relating to writers and writing in Canada. Each issue contains both English and French content {{from a wide range of}} contributors and has been described as [...] "critically eclectic".|$|R
40|$|Cache-oblivious {{algorithms}} {{are well}} understood when the cache size remains constant. Recently variable cache sizes have been considered. We {{are motivated by}} programs running in pseudo-parallel and competing for a single cache. This thesis studies the underlying cache model and gives a generalization of two models considered in the literature. We give a new cache model called the "depth model" where pages are accessed by <b>page</b> <b>depths</b> in an LRU cache instead of their ad- dresses. This model allows us to construct cache-oblivious algorithms that cause {{a certain number of}} cache misses prescribed by an arbitrary function computable without causing a cache miss. Finally we prove that two algorithms satisfying the regularity property running in pseudo-parallel cause asymptotically the same number of cache misses as their serial computations provided that the cache is satisfying the tall-cache assumption...|$|R
40|$|Abstract. The {{localization}} of Underwater Sensor Networks(UWSNs) faces great challenges {{with the}} special underwater environment. In this <b>page,</b> the <b>depth</b> information is considered to use for localization. A novel localization scheme named U-iTPS is designed for UWSNs. Nodes do not rely on special hardware or time synchronization and depend on time difference of arrival (TDoA) signals measured locally to detect range differences from themselves to four anchors and then locate themselves in U-iTPS. U-iTPS is energy conservation {{because it does not}} rely on the node degree and the number of anchors, in which the node does not need communication each other and only listen to the beacon packets. After intensive experiment and theoretical analysis, U-iTPS show better performance such as low storage, computation and communication with other schemes, and also show good robustness in the underwater environment...|$|R
40|$|The MTS Manual is {{a series}} of volumes that {{describe}} in detail the facilities provided by the Michigan Terminal System. Volume 15 contains the descriptions for the FORMAT and TEXT 360 text-processing programs. FORMAT is a text-processing language that takes the text of a document in free format, and reformats it into lines, paragraphs, and pages according to control information supplied with the text. FORMAT numbers pages, generates titles and footers, justifies text (aligns right and left margins), indents sections of text, prints text in multiple columns, etc. An index and table of contents may be generated for the document. TEXT 360 is a text-processing system that can expedite the production of publishable documents. The TEXT 360 user specifies the desired format by embedding two types of instructions in the text flow: "edit codes," which usually affect the format of a relatively small area of text, and "alter codes," which are more general instructions dealing with format specifications of a larger scope, such as <b>page</b> <b>depth,</b> column width, etc...|$|E
40|$|Web Usage Mining (WUM) is {{the process}} of extracting {{knowledge}} from Web user’s access data by exploiting Data Mining technologies. It can be used for different purposes such as personalization, system improvement and site modification. Study of interested web users, provides valuable information for web designer to quickly respond to their individual needs. The main objective {{of this paper is to}} study the behavior of the interested users instead of spending time in overall behavior. The existing model used enhanced version of decision tree algorithm C 4. 5. In this paper, we propose to use the Naive Bayesian Classification algorithm for classifying the interested users and also we present a comparison study of using enhanced version of decision tree algorithm C 4. 5 and Naive Bayesian Classification algorithm for identifying interested users. The performance of this algorithm is measured for web log data with session based timing, page visits, repeated user profiling, and <b>page</b> <b>depth</b> to the site length. Experimental results conducted shows that the performance metric i. e., time taken and memory to classify the web log files are more efficient when compared to existing C 4. 5 algorithm...|$|E
40|$|One of {{the most}} common modes of {{accessing}} information in the World Wide Web (WWW) is surfing from one document to another along hyperlinks. Several large empirical studies have revealed common patterns of surfing behavior. A model which assumes that users make a sequence of decisions to proceed to another page, continuing as long as the value of the current page exceeds some threshold, yields the probability distribution for the number of <b>pages,</b> or <b>depth,</b> that a user visits within a Web site. This model was verified by comparing its predictions with detailed measurements of surfing patterns. It also explains the observed Zipf-like distributions in page hits observed at WWW sites. Huberman et al 1 The exponential growth of World Wide Web (WWW) is making it the standard information system for an increasing segment of the world's population. From electronic commerce and information resource to entertainment, the Web allows inexpensive and fast access to unique and novel services provided by individuals and institutions scattered throughout the world (1) ...|$|R
40|$|The thought-provoking {{review of}} Denham by Dr. Robert Banks points out some very {{important}} parts of cultural analysis, unfortunately seldom discussed. Some of the questions posed by Banks are in part answered by two citations in Denham’s original paper: those of Hirshleifer (1977) and Gammage (2011). While Denham discusses Gammage in a bit more <b>depth</b> (<b>pages</b> 82 and 83), he cites Hirshleifer for more narrow reasons. Hirshleifer, a micro-economist, {{was one of the}} original modern thinkers on how biological and cultural evolution can be treated as one subject. Had Hrdy not treated the subject, Denham could have proposed his study showing that Hirshleifer predicted much of what Denham found. ...|$|R
40|$|When {{representing}} a solid object there are {{alternatives to the}} use of traditional explicit (surface meshes) or implicit (zero crossing of implicit functions) methods. Skeletal representations encode shape information in a mixed fashion: they are composed of a set of explicit primitives, yet they are able to efficiently encode the shape's volume as well as its topology. I will discuss, in two dimensions, how symmetry can be used to reduce the dimensionality of the data (from a 2 D solid to a 1 D curve), and how this relates to the classical definition of skeletons by Medial Axis Transform. While the medial axis of a 2 D shape is composed of a set of curves, in 3 D it results in a set of sheets connected in a complex fashion. Because of this complexity, medial skeletons are difficult to use in practical applications. Curve skeletons address this problem by strictly requiring their geometry to be one dimensional, resulting in an intuitive yet powerful shape representation. In this report I will define both medial and curve skeletons and discuss their mutual relationship. I will also present several algorithms for their computation and a variety of scenarios where skeletons are employed, with a special focus on geometry processing and shape analysis. Comment: 42 <b>pages,</b> SFU <b>Depth</b> Exa...|$|R
5000|$|Also {{writing in}} The Guardian, Kate Pullinger found The Kills to be [...] "a gripping, hallucinogenic - and {{enormous}} - novel", and that, [...] "The digital edition is {{far and away}} the better way to read this novel; the first two books in particular are augmented by a series of short films embedded on the page, often with text overlaid, as well as animations and audio clips. For example, listening to the phone messages left by one character's mother as she tries to cajole him into contacting her, before she understands that he is in danger, adds an emotional jolt to the text. Throughout, the simple yet elegant enhancements work to take us beyond the <b>page,</b> adding <b>depth</b> and texture to the story. This is the first time I've read a digital edition of a primarily text-based novel where I've thought: yes, this works". Pullinger judged that the first two parts, Sutler and The Massive, [...] "provide a wholly original view of our involvement in the Iraq conflict", but was less convinced by the third, The Kill, saying, [...] "House has a great ability to create vivid characters, and the novel teems with them. But the writing in this third book is too abundant - there is simply too much story".|$|R
40|$|We {{present a}} novel {{automated}} technique for parallelizing quantum circuits via {{forward and backward}} translation to measurement-based quantum computing patterns and analyze the trade off in terms of depth and space complexity. As a result we distinguish a class of polynomial depth circuits that can be parallelized to logarithmic depth while adding only polynomial many auxiliary qubits. In particular, we provide {{for the first time}} a full characterization of patterns with flow of arbitrary depth, based on the notion of influencing paths and a simple rewriting system on the angles of the measurement. Our method leads to insightful knowledge for constructing parallel circuits and as applications, we demonstrate several constant and logarithmic depth circuits. Furthermore, we prove a logarithmic separation in terms of quantum depth between the quantum circuit model and the measurement-based model. Comment: 34 <b>pages,</b> 14 figures; <b>depth</b> complexity, measurement-based quantum computing and parallel computin...|$|R
40|$|We {{propose a}} new web page {{transformation}} method to facilitate web browsing on handheld {{devices such as}} Personal Digital Assistants (PDAs). In our approach, an original web page that does not fit on the screen {{is transformed into a}} set of sub-pages, each of which fits on the screen. This transformation is done through slicing the original page into page blocks iteratively with several factors considered. These factors include the size of the screen, the size of each page block, the number of blocks in each transformed <b>page,</b> the <b>depth</b> of the tree hierarchy that the transformed pages form, as well as the semantic coherence between blocks. We call the tree hierarchy of the transformed pages an SP-tree. In an SP-tree, an internal node consists of a textually-enhanced thumbnail image with hyperlinks, and a leaf node is a block extracted from a sub-page of the original web page. We adaptively adjust the fanout and the height of the SP-tree so that each thumbnail image is clear enough for users to read, while at the same time, the number of clicks needed to reach a leaf page is few. Through this transformation algorithm, we preserve the contextual information in the original web page and reduce scrolling. We have implemented this transformation module on a proxy server and have conducted usability studies on its performance. Our system achieved a shorter task completion time compared with that of transformations from the Opera browser in nine of ten tasks. The average improvement on familiar pages was 44 %. The average improvement on unfamiliar pages was 37 %. Subjective responses were positive...|$|R
40|$|This {{document}} contains {{descriptions of}} the work items that have been pursued within the Trilogy project in the reachability domain {{during the first year}} of the project. The work comprises three main strands of work: 1) the analysis of key characteristics of today’s routing system that are crucial to understand for our future design work; 2) early exploration and evaluation of mechanisms to guide our later protocol work and to ensure a close integration of the project’s resource control work; 3) implementation and modelling to prepare for our experimentation work scheduled to start in the second year of the project. This document is not intended to describe the detailed technicalities of the project’s work but to reflect the kind, scope and extend of the work of the project in the reachability domain. The project does produce various documents that explain those technical details in great depth including standards contributions and scientific publications. The project website at www. trilogy-project. org is maintained up-to-date and contains references for the interested readers to learn about the work described on the following <b>pages</b> in greater <b>depth.</b> Disclaime...|$|R
5000|$|The {{first edition}} of this work, {{containing}} more than 700 <b>pages</b> of in <b>depth</b> teachings into the major themes of the occult science, was dedicated to a knowledgeable lecturer of the occult field called Rudolf Steiner, to whom Max Heindel felt greatly indebted. It had the subtitle [...] "Occult Science" [...] instead of [...] "Mystic Christianity" [...] and just above the message and mission ("A Sane Mind, a Soft Heart and a Sound Body", [...] "Service") there was a quotation from Paul of Tarsus: [...] "PROVE ALL THINGS".In the subsequent edition, Heindel removed the initial dedication and changed the mentioned elements. The first dedication became a controversial issue among some students of both teachers down to the present. However, as both Heindel and Steiner {{appear to have been}} influenced by the same Elder Brother of the Rose Cross, to some extent and at some stage of their lives, it becomes an accessory issue that may only be unveiled through the discernment of the student along her/his path of spiritual unfoldment.|$|R
40|$|We {{propose a}} new Web page {{transformation}} method to facilitate Web brows-ing on handheld {{devices such as}} Personal Digital Assistants (PDAs). These de-vices have limited capabilities, in particular, small display and low bandwidth. In our approach, an original Web page that does not fit into the screen is trans-formed into a set of pages, each of which fits into the screen. This transformation is done through slicing the original page into page blocks iteratively with several factors considered, including {{the size of the}} screen, the size of each page block, the number of blocks in each transformed <b>page,</b> the <b>depth</b> of the tree hierarchy that the transformed pages form, as well as the semantic coherence between blocks. We call the tree hierarchy of the transformed pages a slicing*-tree, in which an internal node consists of a textually-enhanced thumbnail image with hyperlinks and a leaf node is a block from the original Web page. The textually-enhanced thumbnails take advantage of both graphical and textual summaries and provide better overviews of transformed pages. Moreover, we adaptively adjust the degree of a node and the height of the slicing*-tree so that each thumbnail image is clear enough for users to read while the number of clicks needed to reach a leaf page only a few. Through this transformation, we preserve the contextual information in the original Web page and save the effort on page scrolling. We have implemented this transformation system in a proxy server. On a page request, the proxy fetches the original Web page as well as embedded im-ages, dynamically transforms the Web page for small displays, and delivers the transformed pages to users progressively. We discuss the architecture, the trans-formation algorithm, and the empirical studies on our system. The experimental results show that our approach improves Web browsing on small displays in terms of task completion time, input effort, and network bandwidth consumption...|$|R
40|$|Web {{archives}} {{preserve the}} history of born-digital content and offer great potential for sociologists, business analysts, and legal experts on intellectual property and compliance issues. Data quality is crucial for these purposes. Ideally, crawlers should gather sharp captures of entire web sites, but the politeness etiquette and completeness requirement mandate very slow, long-duration crawling while web sites undergo changes. This paper presents the SHARC framework for assessing the data quality in Web archives and for tuning capturing strategies towards better quality with given resources. We define quality measures, characterize their properties, and derive a suite of quality-conscious scheduling strategies for archive crawling. It is assumed that change rates of Web pages can be statistically predicted based on <b>page</b> types, directory <b>depths,</b> and url names. We develop a stochastically optimal crawl algorithm for the offline case where all change rates are known. We generalize the approach into an online algorithm that detect information on a web site while it is crawled. For dating a site capture and for assessing its quality, we propose several strategies that revisit pages after their initial downloads in a judiciously chosen order. All strategies are fully implemented in a testbed, and shown to be effective by experiments with both synthetically generated sites and a daily crawl series for a medium-sized site...|$|R
40|$|General comment. This {{paper is}} a useful review. I {{recommend}} that it be published after revisions. The ratio of words to figures seems unbalanced; there are only 10 figures in a long paper. Major comments. Regarding the UV absorption coefficient of ice (page 5964 line 19). Perovich and Govoni 1991 (PG) has been superseded by Ackermann et al. 2006 and Warren et al. 2006. Figure 5 of Warren et al. shows that the UV absorption coefficients are a factor of 100 smaller than reported by PG. The consequence is that extinction coefficients in snow computed using PG’s values will be a factor of 10 too large (square-root of 100). S 2607 The section on remote sensing (Section 3) seems {{out of place in}} this paper, given the title of the paper. That section could be removed. Specific comments. page 5943 line 6. For comparison, it would be interesting also to give the SAI for the dry-snow zone of Greenland (i. e., integrated down to the close-off <b>depth).</b> <b>page</b> 5950 eq. 1. 4 - 4. What are the units of C: g per cubic cm of snow, or g per cubic cm of interstitial air? Should phi be in the numerator instead of the denominator...|$|R
40|$|The primary {{objectives}} {{of the present study}} were to gain insight into website use and to predict the surfing depth on a behaviour change website and its effect on behaviour. Two hundred eight highly educated adults from the intervention condition of a randomised trial received access to a medical intervention, individual coaching (by e-mail, post, telephone or face-to-face) and a behaviour change website. Website use (e. g. surfing <b>depth,</b> <b>page</b> view duration) was registered. Online questionnaires for physical activity and fat intake were filled out at baseline and after 6 months. Hierarchical linear regression was used to predict surfing depth and its effect on behaviour. Seventy-five per cent of the participants visited the website. Fifty-one and fifty-six per cent consulted the physical activity and fat intake feedback, respectively. The median surfing depth was 2. The total duration of interventions by e-mail predicted deeper surfing (beta= 0. 36; p< 0. 001). Surfing depth did not predict changes in fat intake (beta=- 0. 07; p= 0. 45) or physical activity (beta=- 0. 03; p= 0. 72). Consulting the physical activity feedback led to more physical activity (beta= 0. 23; p= 0. 01). The findings from the present study can be used to guide future website development and improve the information architecture of behaviour change websites...|$|R
40|$|Most of {{the topics}} which have been {{associated}} with "artificial intelligence " are mentioned in this book. The author includes descriptions of general computer hardware and software, lots of illustrations, chapter summaries, lists of suggested readings and an index in 322 smallish <b>pages.</b> Thus, the <b>depth</b> of treatment of individual topics is generally limited. Roughly 40 Yo of the material is related to problem-solving in the narrow, mathematical, game, theorem sense and 40 ~o to the broader, perception, robots sense. Searching (trees, graphs) and theorem proving have relatively large coverage, very similar to that of Nilsson's book (1971). The stated aim of the book is to attempt to explain why workers in A. I. are trying to make computers intelligent; and to indicate progress so far, directions of research, obstacles to further progress, as well as the importance of the field. It is also intended as an attempt to clear up confusion, to stimulate the reader, to fill the gap between superficiality and technicality, to answer various questions, and to clear away some misleading myths. The author believes that any reader with a good high school education should be able to enjoy the book; that it {{could be used as a}} text in a new elementary college course, as supplementary reading for some standard courses and perhaps be of interest to the general public. There is probably some truth in al...|$|R
40|$|Building {{upon the}} success and {{relevance}} of the 2014 Magnetism Roadmap, this 2017 Magnetism Roadmap edition follows a similar general layout, even if its focus is naturally shifted, and a different group of experts and, thus, viewpoints are being collected and presented. More importantly, key developments have changed the research landscape in very relevant ways, so that a novel view onto {{some of the most}} crucial developments is warranted, and thus, this 2017 Magnetism Roadmap article is a timely endeavour. The change in landscape is hereby not exclusively scientific, but also reflects the magnetism related industrial application portfolio. Specifically, Hard Disk Drive technology, which still dominates digital storage {{and will continue to do}} so for many years, if not decades, has now limited its footprint in the scientific and research community, whereas significantly growing interest in magnetism and magnetic materials in relation to energy applications is noticeable, and other technological fields are emerging as well. Also, more and more work is occurring in which complex topologies of magnetically ordered states are being explored, hereby aiming at a technological utilization of the very theoretical concepts that were recognised by the 2016 Nobel Prize in Physics. Given this somewhat shifted scenario, it seemed appropriate to select topics for this Roadmap article that represent the three core pillars of magnetism, namely magnetic materials, magnetic phenomena and associated characterization techniques, as well as applications of magnetism. While many of the contributions in this Roadmap have clearly overlapping relevance in all three fields, their relative focus is mostly associated to one of the three pillars. In this way, the interconnecting roles of having suitable magnetic materials, understanding (and being able to characterize) the underlying physics of their behaviour and utilizing them for applications and devices is well illustrated, thus giving an accurate snapshot of the world of magnetism in 2017. The article consists of 14 sections, each written by an expert in the field and addressing a specific subject on two <b>pages.</b> Evidently, the <b>depth</b> at which each contribution can describe the subject matter is limited and a full review of their statuses, advances, challenges and perspectives cannot be fully accomplished. Also, magnetism, as a vibrant research field, is too diverse, so that a number of areas will not be adequately represented here, leaving space for further Roadmap editions in the future. However, this 2017 Magnetism Roadmap article can provide a frame that will enable the reader to judge where each subject and magnetism research field stands overall today and which directions it might take in the foreseeable future. The first material focused pillar of the 2017 Magnetism Roadmap contains five articles, which address the questions of atomic scale confinement, 2 D, curved and topological magnetic materials, as well as materials exhibiting unconventional magnetic phase transitions. The second pillar also has five contributions, which are devoted to advances in magnetic characterization, magneto-optics and magneto-plasmonics, ultrafast magnetization dynamics and magnonic transport. The final and application focused pillar has four contributions, which present non-volatile memory technology, antiferromagnetic spintronics, as well as magnet technology for energy and bio-related applications. As a whole, the 2017 Magnetism Roadmap article, just as with its 2014 predecessor, is intended to act as a reference point and guideline for emerging research directions in modern magnetism...|$|R

