42|45|Public
25|$|Such {{rules are}} another {{standard}} device in traditional linguistics; e.g. passivization in English. Much of generative grammar {{has been devoted}} to finding ways of refining the descriptive mechanisms of <b>phrase-structure</b> <b>grammar</b> and transformation rules such that exactly the kinds of things can be expressed that natural language actually allows. Allowing arbitrary transformations doesn't meet that goal: they are much too powerful, being Turing complete unless significant restrictions are added (e.g. no transformations that introduce and then rewrite symbols in a context-free fashion).|$|E
50|$|Many {{grammatical}} theories, such as <b>phrase-structure</b> <b>grammar,</b> involve hierarchy.|$|E
50|$|Within {{the theory}} of {{generative}} grammar, and within <b>phrase-structure</b> <b>grammar,</b> binding theory explains how anaphors share a relationship with their referents.|$|E
25|$|In linguistics, {{some authors}} {{use the term}} phrase {{structure}} grammar to refer to context-free <b>grammars,</b> whereby <b>phrase-structure</b> <b>grammars</b> are distinct from dependency grammars. In computer science, a popular notation for context-free grammars is Backus–Naur form, or BNF.|$|R
40|$|<b>Phrase-structure</b> <b>grammars</b> are {{effective}} models for important syntactic and semantic aspects of natural languages, {{but can be}} computationally too demanding for use as language models in real-time speech recognition. Therefore, finite-state models are used instead, even though they lack expressive power. To reconcile those two alternatives, we designed an algorithm to compute finite-state approximations of context-free grammars and context-free-equivalent augmented <b>phrase-structure</b> <b>grammars.</b> The approximation is exact for certain context-free grammars generating regular languages, including all left-linear and right-linear context-free grammars. The algorithm {{has been used to}} build finite-state language models for limited-domain speech recognition tasks. Comment: 24 pages, uses psfig. sty; revised and extended version of the 1991 ACL meeting paper with the same titl...|$|R
40|$|<b>Phrase-structure</b> <b>grammars</b> {{assign a}} unique phrase struc-ture (constituency) to an {{unambiguous}} sentence. Thus, for example, John likes apples will be bracketed as follows (ig-noring the phrase labels and ignoring some brackets not essential for our present purpose) : (1) (John (likes apples)) There are systems, however, for example, Combinator...|$|R
50|$|That is, each {{production}} rule {{is a simple}} substitution rule, often written in the form g &rarr; h. It has been proved that any Post canonical system is reducible to such a substitution system, which, as a formal grammar, is also called a <b>phrase-structure</b> <b>grammar,</b> or a type-0 grammar in the Chomsky hierarchy.|$|E
50|$|Such {{rules are}} another {{standard}} device in traditional linguistics; e.g. passivization in English. Much of generative grammar {{has been devoted}} to finding ways of refining the descriptive mechanisms of <b>phrase-structure</b> <b>grammar</b> and transformation rules such that exactly the kinds of things can be expressed that natural language actually allows. Allowing arbitrary transformations doesn't meet that goal: they are much too powerful, being Turing complete unless significant restrictions are added (e.g. no transformations that introduce and then rewrite symbols in a context-free fashion).|$|E
5000|$|TAG {{originated}} in investigations by Joshi {{and his students}} into the family of adjunction grammars (AG), the [...] "string grammar" [...] of Zellig Harris. AGs handle endocentric properties of language in a natural and effective way, but {{do not have a}} good characterization of exocentric constructions; the converse is true of rewrite grammars, or <b>phrase-structure</b> <b>grammar</b> (PSG). In 1969, Joshi introduced a family of grammars that exploits this complementarity by mixing the two types of rules. A few very simple rewrite rules suffice to generate the vocabulary of strings for adjunction rules. This family is distinct from the Chomsky-Schützenberger hierarchy but intersects it in interesting and linguistically relevant ways. The center strings and adjunct strings can also be generated by a dependency grammar, avoiding the limitations of rewrite systems entirely.|$|E
30|$|In a sense, flowcharts {{represent}} “parsed” programs, {{stored in}} the form that directly reflects their syntactic and semantic structure. Natural language sentences also can be represented in a parsed tree-like form with <b>phrase-structure</b> <b>grammars</b> or dependency grammars[10]. Our idea is to let the students compose parsed sentences directly instead of traditional writing.|$|R
40|$|This paper {{proposes a}} new model for DNA {{computation}} termed YAC based on self-assembly principle. The model has three advantages: (i) It has the universal computability of Turing machines. (ii) It requires only simple and basic molecular biological operations. Besides annealing and melting in a one-pot reaction, only the detection of a completely hybridized double stranded molecule is used. (iii) Therefore, a molecular biological implementation of the model seems highly feasible. In order to make YAC computation more resource efficient, we introduce an incremental computation method. 1. Introduction In recent intensive study of normal form theorems for the generative grammars in formal language theory, quite a few number of normal form theorems {{for a variety of}} types of grammars have been presented, ranging from regular <b>grammars</b> to <b>phrase-structure</b> <b>grammars</b> in Chomsky's hierarchy. Among others, some of the normal forms for <b>phrase-structure</b> <b>grammars</b> can be of crucially importance wh [...] ...|$|R
40|$|Also called <b>phrase-structure</b> <b>grammars</b> • The most {{commonly}} used mathematical system for modeling the constituent structure in natural languages – Ordering • What are the rules that govern the ordering of words and bigger units in the language – Constituency • How do words group into units and what we say about how the various kinds of units behave 3 Major Characteristics of CFGs • CFG example...|$|R
40|$|This paper {{discusses}} {{the consequences of}} allowing discontinuous constituents in syntactic representions and phrase-structure rules, and the resulting complications for a standard parser of <b>phrase-structure</b> <b>grammar.</b> It is argued, first, that discontinuous constituents seem inevitable in a <b>phrase-structure</b> <b>grammar</b> which is acceptable from a semantic point of view. It is shown that tree-like constituent structures with discontinuities can be given a precise definition which makes them just as acceptable for syntactic representation as ordinary trees. However, the formulation of phrase-structure rules that generate such structures entails quite intricate problems. The notions. of linear precedence and adjacency are reexamined, {{and the concept of}} &quot;n-place adjacency sequence &quot; is introduced. Finally, the resulting form of <b>phrase-structure</b> <b>grammar,</b> called &quot;Discontinuous Phrase-Structure Grammar&quot; is shown to be parsable by an algorithm for context-free parsing with relatively minor adaptations. The paper describes the adaptations in the chart parser which was implemented as part of the TENDUM dialogue system...|$|E
40|$|AbstractA {{derivation}} in a <b>phrase-structure</b> <b>grammar</b> {{is said to}} be k-bounded if {{each word}} in the derivation contains at most k occurrences of nonterminals. A set L {{is said to be}} derivation bounded if there exists a <b>phrase-structure</b> <b>grammar</b> G and a positive integer k such that L is the set of words in the language generated by G which have some k-bounded derivation. The main result is that every derivation-bounded set is a context-free language. Various characterizations of the derivation-bounded languages are then given. For example, the derivation-bounded languages coincide with the standard matching-choice sets discussed by Yntema. They also coincide with the smallest family of sets containing the linear context-free languages and closed under arbitrary substitution, a family discussed by Nivat...|$|E
40|$|We {{describe}} {{a method for}} automatically learning a parser from labeled, bracketed corpora that results in a fast, robust, lightweight parser that is suitable for real-time dialog systems and similar applications. Unlike ordinary parsers, all grammatical knowledge is captured in the learned decision trees, so no explicit <b>phrase-structure</b> <b>grammar</b> is needed...|$|E
40|$|We study several {{extensions}} {{of the notion of}} alternation from context-free grammars to context-sensitive and arbitrary <b>phrase-structure</b> <b>grammars.</b> Thereby new grammatical characterizations are obtained for the class of languages that are accepted by alternating pushdown automata. Die Arbeit von Herrn Prof. Dr. Etsuro Moriya wurde durch einen Grant der Waseda University, Tokio, gefördert (Waseda University Grant for Special Research Projects # 2006 B- 073) ...|$|R
40|$|The {{notion of}} {{interactive}} languages generated by interactions between two <b>phrase-structure</b> <b>grammars</b> are proposed and discussed. It is {{shown that a}} family of context-free languages does not include a family of interactive languages between two regular grammars and vice versa. The family of interactive languages, however, is not closed under any of the ordinary operations. The paper also includes discussions about n-cyclic interactive languages among n grammars...|$|R
25|$|The {{formalism}} of context-free grammars {{was developed}} in the mid-1950s by Noam Chomsky, and also their classification as a special type of formal grammar (which he called <b>phrase-structure</b> <b>grammars).</b> What Chomsky called a phrase structure grammar is also known now as a constituency grammar, whereby constituency grammars stand in contrast to dependency grammars. In Chomsky's generative grammar framework, the syntax of natural language was described by context-free rules combined with transformation rules.|$|R
40|$|The left-associative grammar model (LAG) {{has been}} applied {{successfully}} to the morphologic and syntactic analysis of various european and asian languages. The algebraic denition of the LAG is very well suited for the application to natural language processing as it inherently obeys de Saussure's second law (de Saussure, 1913, p. 103) on the linear nature of language, which <b>phrase-structure</b> <b>grammar</b> (PSG) and categorial grammar (CG) do not...|$|E
40|$|Phrase-structure grammars are an {{effective}} rep-resentation for important syntactic and semantic aspects of natural languages, but are computa-tionally too demanding {{for use as}} language mod-els in real-time speech recognition. An algorithm is described that computes finite-state approxi-mations for context-free grammars and equivalent augmented <b>phrase-structure</b> <b>grammar</b> formalisms. The approximation is exact for certain context-free grammars generating regular languages, in-cluding all left-linear and right-linear context-free grammars. The algorithm {{has been used to}} con-struct finite-state language models for limited-domain speech recognition tasks. ...|$|E
40|$|In {{numerous}} domains {{in cognitive}} science {{it is often}} useful to have a source for randomly generated corpora. These corpora {{may serve as a}} foundation for artificial stimuli in a learning experiment (e. g., Ellefson & Christiansen, 2000), or as input into computational models (e. g., Christiansen & Dale, 2001). The following compact and general C program interprets a <b>phrase-structure</b> <b>grammar</b> specified in a text file. It follows parameters set at a Unix or Unix-based command-line and generates a corpus of random sentences from that grammar. Comment: Brief paper with source code and example...|$|E
40|$|This paper {{studies the}} {{representation}} of the derivations in <b>phrase-structure</b> <b>grammars</b> by the use of “derivation languages. ” The words in the derivation language of a grammar correspond exactly to the “syntactical graphs” of Loeckx (1970). Furthermore, the derivation languages generalize {{the representation of}} context-free derivation trees in prefix form. “Graph automata” are developed as acceptors of the derivation languages. The graph automata generalize the theory of tree automata as studied in connection with contextfree grammars...|$|R
40|$|In gb, the {{importance}} of phrase-structure rules has dwindled in favour of nearness conditions. Today, nearness conditions {{play a major role}} in defining the correct linguistic representations. They are expressed in terms of special binary relations on trees called command relations. Yet, while the formal theory of <b>phrase-structure</b> <b>grammars</b> is quite advanced, no formal investigation into the properties of command relations has been done. We will try to close this gap. In particular, we will study the intrinsic properties of command relations as relations on trees as well as the possibility to reduce nearness conditions expressed by command relations to phrase-structure rules...|$|R
40|$|This paper {{describes}} a technique for parsing dependency grammars using a bottom-up chart parser originally designed for <b>phrase-structure</b> <b>grammars,</b> using typed feature structures {{as the only}} data structure. Each lexical item is represented as a tree where nodes indicate lexical elements (the anchor, its dependents and governor) and edges (branches) indicate dependency relations between these elements. Nodes may carry additional features, including one for node saturation. Trees combine into derived trees provided that node and edge features unify. The ALE system is used to implement an active chart parser where a chart edge represents a tree, and two adjacent edges are combined into a more saturated tree. status: publishe...|$|R
40|$|A {{computer}} grammar {{is described}} which includes {{most of the}} English relative-clause constructions. It is written {{in the form of}} a left-to-right <b>phrase-structure</b> <b>grammar</b> with discontinuous constituents and subscripts, which carry such syntactic restrictions as number and verb government category. The motivation for the hierarchy of syntactic choices and for the use of discontinuous constituents is discussed. Many examples are given, and special attention is given to complement constructions and to the relation of the relative pronoun to complex prenominal and postnominal determiner constructions. Written in COMIT, the program runs as part of a larger grammar of English. I...|$|E
40|$|An {{algorithm}} is presented for learning a <b>phrase-structure</b> <b>grammar</b> from tagged text. It clusters sequences of tags together based on local distributional information, and selects clusters that satisfy a novel mutual information criterion. This criterion {{is shown to}} be related to the entropy of a random variable associated with the tree structures, and it is demonstrated that it selects linguistically plausible constituents. This is incorporated in a Minimum Description Length algorithm. The evaluation of unsupervised models is discussed, and results are presented when the algorithm has been trained on 12 million words of the British National Corpus. ...|$|E
40|$|AbstractConceptual graphs are a {{semantic}} representation {{that has a}} direct mapping to natural language. This article presents a universal algorithm for scanning the graphs, together with a version of augmented <b>phrase-structure</b> <b>grammar</b> for specifying the syntax of particular languages. When combined with a specific grammar, the universal algorithm determines a mapping from graphs to language with several important properties: multiple surface structures may be generated from a common underlying structure, constraints on the mapping result from the connectivity of the graphs rather than ad hoc assumptions, and the graphs combined with phrase-structure rules enforce context-sensitive conditions...|$|E
40|$|The {{concepts}} of “right parse” and “left parse” {{to represent the}} outputs of bottom-up and top-down parsers (respectively) of context-free grammars are extended in a natural way to cover all <b>phrase-structure</b> <b>grammars.</b> The duality between left and right parses is demonstrated. Algorithms are presented for converting between parses and the “derivation languages. ” The derivation languages give the most efficient representation of the syntactical structure of a word in a grammar. This work gives a unified approach {{to the problem of}} grammatical parsing and to the problems of organizing the output of parsers. The general theory of parses then leads naturally to the notion of label languages and control sets induced by canonical derivations...|$|R
40|$|A {{recent study}} by Fitch and Hauser {{reported}} that finite-state grammars can be learned by non-human primates, whereas <b>phrase-structure</b> <b>grammars</b> cannot. Humans, by contrast, learn both grammars easily. This species difference is taken as the critical juncture in the evol-ution of the human language faculty. Given the far-reaching relevance of this conclusion, the question arises {{as to whether the}} distinction between these two types of grammars finds its reflection in different neural systems within the human brain. For centuries people have sought the critical parameters surrounding the evolution of language. With the advent of genetic research methods, the possible role of a part gene, FOXP 2, in speech and language abilities has been brought to light [1]. Moreover, {{it has been shown that}} this gene wa...|$|R
40|$|We {{present a}} new family of models for {{unsupervised}} parsing, Dependency and Boundary models, that use cues at constituent boundaries to inform head-outward dependency tree generation. We build on three intuitions that are explicit in <b>phrase-structure</b> <b>grammars</b> but only implicit in standard dependency formulations: (i) Distributions of words that occur at sentence boundaries — such as English determiners — resemble constituent edges. (ii) Punctuation at sentence boundaries further helps distinguish full sentences from fragments like headlines and titles, allowing us to model grammatical differences between complete and incomplete sentences. (iii) Sentence-internal punctuation boundaries help with longer-distance dependencies, since punctuation correlates with constituent edges. Our models induce state-of-the-art dependency grammars for many languages without special knowledge of optimal input sentence lengths or biased, manually-tuned initializers. ...|$|R
40|$|We survey a {{sequence}} of results relating model-theoretic and language-theoretic denability over an innite hierarchy of multi-dimensional tree-like structures and explore their applications to a corresponding range of theories of syntax. We discuss, in particular, results for Government and Binding Theory (GB), Tree-Adjoining Grammar (TAG) and Generalized <b>Phrase-Structure</b> <b>Grammar</b> (GPSG) along with a generalized version of TAG extending TAG {{in much the same}} way that GPSG extends CFLs. In addition, we look at a hierarchy of language classes, Weir’s version of the Control Language Hierarchy, which is characterized by denability in our hierarchy and speculate on possible linguistic signicance of higher levels of these hierarchies...|$|E
40|$|Currently several {{grammatical}} formalisms converge towards being declarative {{and towards}} utilizing context-free <b>phrase-structure</b> <b>grammar</b> as a backbone, e. g. LFG and PATR-II. Typically {{the processing of}} these formalisms is organized within a chart-parsing framework. The declarative character of the formalisms makes it important to decide upon an overall optimal control strategy {{on the part of}} the processor. In particular, this brings the ruleinvocation strategy into critical focus: to gain maximal processing efficiency, one has to determine the best way of putting the rules to use. The aim of this paper is to provide a survey and a practical comparison of fundamental rule-invocation strategies within context-free chart parsing...|$|E
40|$|Abstract — This paper aims {{to give a}} hierarchical, {{generative}} {{account of}} diatonic harmony progressions and proposes a generative <b>phrase-structure</b> <b>grammar.</b> The formalism accounts for structural properties of key, functional, scale and surface level. Being related to linguistic approaches in generative syntax and to the hierarchical account of tonality in the generative theory of tonal music (GTTM) [1], cadence-based harmony contexts and its elaborations are formalised. This approach covers cases of modulation, tonicisation and some aspects of large-scale harmonic form, and may be applied to large sets of diatonic compositions. Potential applications may rise in computational harmonic and corpus analysis, {{as well as in}} the music psychological investigation of tonal cognition. 1 I...|$|E
40|$|Hierarchical phrase {{structures}} are considered fundamental to any {{description of the}} syntax of human languages because {{of their ability to}} handle nonadjacent, hierarchical dependencies between the words of a sentence (Chomsky, 1957; Hauser, Chomsky, & Fitch, 2002). Nevertheless, it is still unclear what role these structures play in the cognitive process of sentence comprehension. Although models based on <b>phrase-structure</b> <b>grammars</b> (PSGs) have appeared in explanations of several psycholinguistic findings (e. g., see Levy, 2008), it has also been argued that people’s sensitivity to hierarchical struc-ture is limited, as evidenced by, for instance, the ease with which some ungrammatical structures can be processed (Christiansen & MacDonald, 2009). In the experiment reported here, we approached the issue differently: Rather than arguing that a particular linguistic (o...|$|R
40|$|In {{this paper}} we study automata which work on {{directed}} ordered acyclic graphs, in particular those graphs, called derivation dags (d-dags), which model derivations of <b>phrase-structure</b> <b>grammars.</b> A rather complete {{characterization of the}} relative power of the following features of automata on d-dags is obtained: parallel versus sequential, deterministic versus nondeterministic and finite state versus a (restricted type of) pushdown store. New results concerning trees follows as special cases. Closure properties of classes of d-dag languages definable by various automata are studied for some basic operations. Characterization of general directed ordered acyclic graphs by these automata is also given. Contents. Abstract. 1. Introduction. 2. Definitions of the graphs. 3. Parallel dag automata. 4. Finite state relabeling. 5. Two-way dag-walking automata. 6. Comparison. 7. Closure properties of d-dag languages. 8. Recognition of doags. Acknowledgment. References...|$|R
40|$|This work {{is based}} on my PhD thesis, which continueswith {{studying}} of grammar and automata systems. Firstof all, it deals with regularly controlled CD grammarsystems with <b>phrase-structure</b> <b>grammars</b> as components. Into these systems, three new derivation restrictions areplaced and their eect on the generative power of thesesystems are investigated. Thereafter, the thesis denestwo automata counterparts of canonical multi-generativenonterminal and rule synchronized grammar systems, gen-erating vectors of strings, and it shows that these inves-tigated systems are equivalent. Furthermore, the thesisgeneralizes denitions of these systems and establishesfundamental hierarchy of n-languages (sets of n-tuplesof strings). In relation with these mentioned systems,automaton-grammar translating systems based upon -nite automaton and context-free grammar are introducedand investigated as a mechanism for direct translating. At the end, in the thesis introduced automata systemsare used as the core of parse-method based uponrestricted tree-controlled grammars...|$|R
