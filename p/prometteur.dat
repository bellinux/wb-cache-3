15|16|Public
50|$|Il présageait bien les séquelles tristes et pénibles de cet exil: la séparation de ses ouailles, de ses parents et amis, et de sa patrie bien-aimée, l'incertitude d'un pays étranger, le problème de survie dans un pays où l'exercice du sacerdoce catholique et en langue étrangère était peu <b>prometteur,</b> et quoi encore. Bien que ce pays ait été à {{plusieurs}} reprises l'antagoniste obstiné de la France en temps de guerre, il témoigne pourtant d'une hospitalité remarquable envers ces malheureux émigrés. Il fait preuve d'autant de commisération humanitaire qu'il lui était possible dans les circonstances, allant même jusqu'à payer une obole de deux à quatre guinées aux exilés dans le besoin. Dans l'ensemble cela était, cependant, grandement insuffisant pour faire vivre cette masse de réfugiés français qui ne pouvait réellement pas vivre de leur sacerdoce en un pays étranger de religion protestante. En résumé, Sigogne a vécu sept ans en Angleterre d'où il est parti le 16 avril 1799 dans l'espoir de reprendre la pratique de son ministère sacerdotal, cette fois en Acadie (Nouvelle-Écosse). Durant son exil {{en terre}} britannique, il s'est adonné à des besognes journalières, à des oeuvres de charité et d'éducation dans la mesure où le lui permettaient les circonstances de l'époque.|$|E
40|$|Funding NN-T is {{supported}} by the IHU <b>prometteur</b> OPERA. PM is a senior member of and supported by the Institut Universitaire de France. Competing interests NN-T and PM hold a patent on the determination of bioactive IL- 17. Patient consent Obtained. Ethics approval Ethics committee of the hospitals of Lyon. Provenance and peer review Not commissioned; externally peer reviewed...|$|E
40|$|Debates {{over the}} penal {{system of the}} revolutionary Constituent Assembly between 1789 and 1791 have not given rise to great {{historical}} works, except for those which concern the "intermediate legislation" contained in well-known university manuals of historical synthesis (Esmein, Laingui, Lebigre, Royer, Carbasse). The stakes of this penal debate shape the modernity {{of the right to}} punish. The important thing is to re-discover the specifics of the subject, which lie less in the "intermediate legislation" than in the constitutional nature of the penal question during the period of the Constituant Assembly. Martucci Roberto. L'enjeu pénal à l'assemblée constituante : un chantier <b>prometteur</b> (1789 - 1791). In: Dix-huitième Siècle, n° 37, 2005. Politiques et cultures des Lumières, sous la direction de Michel Porret. pp. 283 - 303...|$|E
40|$|National audienceSerious {{games are}} {{pedagogical}} multimedia products made to help learners develop specific skills. Their use {{has proven to}} be promising in many domains, but is at present restricted by the time consuming and costly nature of the developing process. When developing Serious Games (SGs) for academic purposes, not only is there a budgetary challenge, but there is also the challenge of integrating enough educational value without sacrificing the fun characteristics. In this article, we detail the designing process of a SG and enumerate the various tools we can offer to the SG conception teem to help them design better and faster. On assiste à un essor impressionnant des Serious Games (SG) dans beaucoup de domaines. Ces environnements éducatifs sont en effet très <b>prometteurs</b> mais ils posent aussi des problèmes en termes de coût et de temps pour les étapes de conception et de production. Nous proposons un ensemble d'outils informatiques et de démarches principalement organisées autour de composants interopérables et génériques qui visent à surmonter ces barrières...|$|R
40|$|International audienceThis paper reports an {{experience}} feedback about a Master- 2 and Engineering-School level training in HCI {{based on the}} transfer of knowledge and technology from research and industry. We {{emphasize the importance of}} training students to promising research areas such as user interface plasticity and to recent technologies for developing mobile media and MS surface table applications. The whole training enhances user-centered methodsto encourage students to focus on the use of applications. Cet article présente un retour d'expérience sur une formation en IHM de niveau Master 2 et École d'ingénieurs reposant sur le transfert de savoirs et de technologies issus de la recherche et de l'industrie. Nous mettons l'accent sur l'importance de former les étudiants à des domaines de recherche <b>prometteurs</b> tels que la plasticité des IHM et à des technologies récentes permettant le développement d'applications sur supports mobiles et table MS Surface. L'ensemble de la formation valorise les méthodes centrées utilisateurs afin d'inciter les étudiants à privilégier l'usage des applications. ABSTRACT This paper reports {{an experience}} feedback about a Master- 2 and Engineering-School level training in HCI based on the transfer of knowledge and technology from research and industry. We emphasize the importance of training students to promising research areas such as user interface plasticity and to recent technologies for developing mobile media and MS surface table applications. The whole training enhances user-centered methods to encourage students to focus on the use of applications...|$|R
40|$|Huit méthodes de {{transfert}} d'étalonnage basées sur l'élimination ses signaux orthogonaux ont été comparées dans l'objectif de transférer les modèles d'étalonnage de teneur en huile et en protéine de soja: Les méthodes sont: Dynamic {{orthogonal projection}} (DOP), transfer by orthogonal projection (TOP), error removal by orthogonal subtraction (EROS), orthogonal signal correction (OSC), orthogonal projection to latent structures (O-PLS) ainsi que des modifcations et extensions de ces méthodes. Toutes ont été comparées sur le transfert inter-marques ou intra-marques, à partir de spectres obtenus sur 2 Foss Infratecs et deux Bruins OmegAnalyzerGs. La méthode donnant les résultats les plus <b>prometteurs</b> est la méthode DOP modifiée par la matrice des différences, alors que les méthodes TOP et EROS modifiées n'ont pas apporté d'amélioration. / Eight calibration transfer methods {{based on the}} removal of orthogonal signal were compared for the standardization of whole soybean protein and oil models. Dynamic orthogonal projection (DOP), transfer by orthogonal projection (TOP), error removal by orthogonal subtraction (EROS), orthogonal signal correction (OSC), and orthogonal projections to latent structures (O-PLS) {{as well as the}} modification and extension of some of these methods were compared in the transfer of models in intra and inter-brand situations using two Foss Infratecs and two Bruins OmegAnalyzerGs. For each brand, a master was designated and its models transferred onto the second unit of its network and the two units of the second brand. Calibration models were transferable from brand to brand with similar or better precision than when each instrument was calibrated on its own calibration set (for Infratec 1229, the relative predictive determinant (RPD) increased in intra and inter-brand calibration transfer situations from 10. 42 to 11. 45 and 10. 57, with DOP and EROS respectively). Performance of each method was different across parameters, instruments, and validation sets. DOP modifications on the determination of the difference matrix showed promising results while TOP and EROS extensions to include variability specifically present in certain crop years did not bring any beneficial effects...|$|R
40|$|RQsum 6. - AprGs avoir r&sumQ les principales propriQt 6 s exp 6 rimentales des verres de spin (chaleur spdcifique, tempdrature critique, ph 6 nomSnes d 1 hyst&r 6 sis et de mdtastabilitd), nous montrons connnent les thgories existantes phQnomQnologiques et de champ moyen se sont dQveloppQes. Le concept de frus-tration, associs P une thQorie de gauge s'avsre <b>prometteur,</b> mais il reste beaucoup P faire pour com-prendre cette {{nouvelle}} transition de phase. Abstract. - After a rapid {{description of}} the main experimental properties of spin glasses (specific heat, critical temperature, hysterisis and metastability), we show how the actual theories (phenome-nology, mean field theory) are being developed. The concept of frustration, and its link to gauge theories seems promising, {{but a lot of}} work has to be done in order to understand this new phase transition. 1. INTRODUCTION. - The expression "spin glass " appea-red about ten years ago, in order to describe the properties of dilute magnetic impurities in normal metals, canonical examples being CuMn or AuMn. In fact, it was an old problem: the first experiment...|$|E
40|$|This work is {{presenting}} a semiotic {{point of view}} of a promising but problematic tool of the communication field -eye-tracking- which is used in various fields to track and exploit the eye-path on an object. Drawing form a list of eye-paths on various packagings, the first step of the analysis was the identification without theoretical a priori- of fixed patterns in these paths. Several models can then be inferred on how the order of acquisition of the elements of an image or a pack might have an influence on the construction of their overall meaning. Ce travail propose un point de vue sémiotique sur un outil du champ de la communication <b>prometteur</b> mais problématique, l'eye-tracking, ou traçage oculaire, utilisé dans différents secteurs pour suivre et exploiter le parcours de l'œil sur un objet. À partir d'un corpus permettant de recenser les parcours de l'œil sur différents packagings, la première phase de l'analyse consiste à repérer des invariants dans ces parcours, sans a priori théorique. Dans une seconde phase sont proposés plusieurs modèles permettant d'imaginer comment l'ordre d'acquisition des éléments d'une image, ou d'un pack, peut avoir une influence sur la construction du sens global de ce pack ou de cette image...|$|E
40|$|LE MAGNESIUM EST UN CANDIDAT <b>PROMETTEUR</b> POUR LE STOCKAGE DE L'HYDROGENE (7, 6 % MASSIQUE). CEPENDANT, SON HYDRURE MGH 2 EST STABLE ET SES CINETIQUES D'HYDRURATION ET DE DESHYDRURATION SONT FAIBLES. AFIN DE PALIER A CES INCONVENIENTS, NOUS AVONS PREPARE PAR BROYAGE ENERGETIQUE DES MELANGES COMPOSITES A BASE DE MAGNESIUM MG + X% YNI ET MG + X%Y 1 XNI 2. L'AJOUT DE CES INTERMETALLIQUES (YNI ET/OU Y 1 XNI 2) AMELIORE CONSIDERABLEMENT LES CINETIQUES D'HYDRURATION ET DE DESHYDRURATION DU MG. LES MEILLEURES PROPRIETES DE SORPTION D'HYDROGENE SONT OBTENUES POUR LA COMPOSITION MASSIQUE MG + 25 % YNI ET UN TEMPS DE BROYAGE DE 3 HEURES. LA MECANOSYNTHESE A AUSSI ETE UTILISEE AVEC SUCCES POUR ELABORER LES COMPOSES YNI ET Y 1 XNI 2. YNI A ETE OBTENU AVEC UNE STRUCTURE DIFFERENTE DE CELUI OBTENU PAR FUSION. Y 1 XNI 2 OBTENU PAR MECANOSYNTHESE (Y 0, 9 7 NI 2) EST INSTABLE THERMIQUEMENT ET SE DECOMPOSE EN DEUX PHASES Y 0, 9 2 NI 2 ET YNI 1, 8 4. L'EFFET DU BROYAGE SUR LES PROPRIETES D'HYDRURATION DE YNI ET Y 1 XNI 2 OBTENUS APRES FUSION A ETE ETUDIE. D'AUTRE PART, L'EFFET DU BROYAGE SUR LES PROPRIETES STRUCTURALES ET MAGNETIQUES DES COMPOSES A BASE DE GADOLINIUM GDX 2 ET GDXAL (X = MN, NI, CU) A ETE DETERMINE. CET EFFET SUR LES PROPRIETES MAGNETIQUES DE GDMN 2 EST COMPARABLE A CELUI D'UNE PRESSION ISOSTATIQUE APPLIQUEE IN-SITU SUR LE MATERIAU, BIEN QUE LES CAUSES SOIENT PROBABLEMENT DIFFERENTES. BORDEAUX 1 -BU Sciences-Talence (335222101) / SudocBELFORT-UTBM-SEVENANS (900942101) / SudocSudocFranceF...|$|E
40|$|National audienceLearning Games (LGs) are {{promising}} pedagogical tools but their design still remains experimental. Inspired by design-pattern based methods, recommended in educational domains, we propose a methodology {{and a model}} to analyze the scenario of LGs, which {{have proven to be}} effective, in order to extract design patterns. The proposed methodology and model allowed us to extract nine design patterns from the analysis of two LGs, which have been actively used in schools for nearly ten years. These design patterns proved to be very useful because half of them where adopted by teams of designers, in the process of creating LGs, for similar contexts to the ones of the existing LGs. Les Learning Games (LGs) sont des outils pédagogiques très <b>prometteurs,</b> mais leur conception reste encore expérimentale. Inspirés par les méthodes de conception à base de patrons, préconisées dans le domaine de l'éducation, nous proposons une méthodologie et un modèle capable d'analyser le scénario de LGs éprouvés sur le terrain, afin d'en extraire des patrons de conception. La méthodologie et le modèle proposés ont permis d'extraire neuf patrons de conception à partir de l'analyse de deux LGs utilisés depuis une dizaine d'années. Ces patrons se sont avérés très utiles puisque plus de la moitié ont été adoptés par des équipes de concepteurs, lors de la création de LGs pour des contextes d'utilisation similaires aux deux LGs existants. Mots-clés. Jeux sérieux, learning games, patrons de conception, modèle de scénario Abstract. Learning Games (LGs) {{are promising}} pedagogical tools but their design still remains experimental. Inspired by design-pattern based methods, recommended in educational domains, we propose a methodology and a model to analyze the scenario of LGs, which have proven to be effective, in order to extract design patterns. The proposed methodology and model allowed us to extract nine design patterns from the analysis of two LGs, which have been actively used in schools for nearly ten years. These design patterns proved to be very useful because half of them where adopted by teams of designers, in the process of creating LGs, for similar contexts to the ones of the existing LGs...|$|R
40|$|This thesis {{presents}} {{a new study}} aiming at constraining the gluon contribution DeltaG to the 1 / 2 nucleon spin. The collinear pQCD theoretical framework, on which it is based, deals with asymmetries calculated from cross-sections for single inclusive hadron in the regime of quasi-real photoproduction (Q^ 2 1 GeV/c). These calculations are done up to Next-to-Leading order with a foreseen inclusion of Next-to-Leading logarithm threshold gluon resummation, only performed for the unpolarised cross-sections yet. This makes the asymmetries sensitive to the gluon polarisation not only through Photon Gluon Fusion ("gamma^* g") but also through resolved photon processes such as "q g" or "g g". The measurement of the asymmetries is performed for all the COMPASS data available from 2002 to 2011 with a polarised muon beam at 160 - 200 GeV scattered off a longitudinally polarised target of deuteron (LiD for 2002 - 2006) or proton (NH 3 for 2007 and 2011). The asymmetries are presented in bins of pT and of pseudorapidity eta (p_T in [1, 4]$ with = 3 (GeV/c) ^ 2, and eta in [- 0. 1, 2. 4]). Since the resummation calculations are not completed yet for the polarised case, the measurements are only compared with theoretical calculations using different parametrisation sets of polarised Parton Distribution Functions with a large range of different DeltaG. This comparison is then {{used to evaluate the}} DeltaG of these measurements. Complementary to this analysis, a study of new tracking detectors, the pixelised Micromegas, is performed. After calibration, it shows promising efficiencies and time and spatial resolutions. Cette thèse présente une nouvelle étude ayant pour but de contraindre la contribution du gluon au spin 1 / 2 du nucléon. Cette analyse se place dans le cadre théorique de la pQCD colinéaire pour calculer des asymétries de section efficaces pour des hadrons inclusifs à grande impulsion transverse (pT> 1 GeV/c) dans le régime de photoproduction quasi-réel (Q^ 2 = 3 (GeV/c) ^ 2, et eta dans [- 0. 1, 2. 4]). Les calculs de resommation n'étant pas terminés pour le cas polarisé, les mesures sont seulement comparés aux calculs théoriques en utilisant différents jeux de parametrisations de fonctions de distribution de parton polarisées ayant des valeurs de DeltaG assez étendues. Ces comparaisons sont ensuite utilisées pour évaluer le DeltaG des mesures. De façon complémentaire à cette analyse, une étude sur de nouveaux détecteurs, les pixel Micromegas, servant à pister le passage des particules, a été réalisée. Après certaines calibrations, ces détecteurs montrent des résultats <b>prometteurs</b> aussi bien en terme d'éfficacité qu'en terme de résolution temporelle et spatiale...|$|R
40|$|When using Brain-Computer Interfaces (BCI) {{based on}} ElectroEncephaloGraphy (EEG), the {{identification}} of mental tasks relies on two main points: feature extraction and classification [MAM+ 06, BFWB 07, LCL+ 07]. Feature extraction aims at describing EEG signals by a few relevant values called “features ”, whereas classification aims at automatically assigning a class to these features. In this paper we focus on feature extraction, as the BCI community has stressed the need to explore new feature extraction algorithms [MAM+ 06]. Recently, inverse models have been revealed as promising feature extraction algorithms for BCI [QDH 04, GGP+ 05, WGWB 05, CLL 06]. Such models aims at computing the activity in the whole brain volume, by using only scalp EEG signals and a head model representing the brain {{as a set of}} voxels (volume elements). The activity computed in a few brain regions has been used as features for BCI systems. Despite good results, some limitations remain. Indeed, it seems that current methods cannot conciliate genericity, i. e., the capability to deal with any kind of mental task, and the fact of generating few features. On one hand, methods that are generic and automatic tend to generate a large number of features, as they extract several features for each voxel [GGP+ 05]. The activity in neighboring voxels can be correlated and, as such, it would be more appropriate to gather these voxels in brain regions. On the other hand, methods that generate few features have been proposed, but they are not generic anymore as they need a priori knowledge on the mental tasks used, and are currently limited to motor imagerybased BCI [QDH 04, WGWB 05]. Recently, we have proposed a method which is generic and which generates few features, as voxels whose activity is correlated are gathered into Regions Of Interest (ROI) [CLL 06]. However, this method is not completely automatic and is limited to the use of two ROI whose spatial extension is hard to define [CLL 06]. In this paper, we propose a generic feature extraction algorithm which can automatically identify any number of relevant ROI and can properly define their spatial extension thanks to the new concept of fuzzy ROI. This algorithm is known as FuRIA, which stands for “Fuzzy Region of Interest Activity” [...] . Cet article propose un nouvel algorithme d’extraction de caractéristiques pour les Interfaces Cerveau- Ordinateur (ICO) basées sur l’électroencéphalographie. Cet algorithme utilise les modèles inverses ainsi que le nouveau concept de Région d’Intérêt (RI) floue. Il peut automatiquement identifier les RI pertinentes pour la discrimination ainsi que les bandes de fréquences dans lesquelles ces RI sont les plus discriminantes. Les activités calculées dans ces RI peuvent ensuite être utilisées comme caractéristiques d’entrée pour n’importe quel classifieur. Une première évaluation de l’algorithme, utilisant une Machine à Vecteurs Supports (SVM) comme classifieur, est présentée sur le jeu de données IV de la « BCI competition 2003 ». Les résultats s’avèrent <b>prometteurs</b> avec une précision sur l’ensemble de test allant de 85...|$|R
40|$|L'ELABORATION DE MATERIAUX A PROPRIETES SPECIFIQUES RESTE UN OBJECTIF MAJEUR POUR L'INDUSTRIE, EN PARTICULIER DANS LE DOMAINE DE LA CABLERIE OU L'ON CHERCHE CONSTAMMENT A AMELIORER LES QUALITES DES MATERIAUX ISOLANTS UTILISES. SOUMIS A UN CAHIER DES CHARGES PRECIS, LE PRODUIT FINI DOIT PRINCIPALEMENT PRESENTER UNE TENUE A HAUTE TEMPERATURE TOUT EN RESTANT SOUPLE. CES PROPRIETES ANTAGONISTES SONT TRES DIFFICILES A TROUVER SUR UN SEUL POLYMERE. LE MELANGE DE POLYMERES PRESENTANT CHACUN UNE DE CES PROPRIETES EST UNE SOLUTION A CE PROBLEME. L'ELABORATION PAR EXTRUSION SIMULE LES CONDITIONS FINALES DE MISE EN OEUVRE DE LA GAINE POUR UN MELANGE SATISFAISANT LE CAHIER DES CHARGES. UNE POLYSULFONE EST RETENUE POUR SES QUALITES DE RESISTANCE THERMIQUE, ET UN CAOUTCHOUC ACRYLATE POUR SA SOUPLESSE. DEUX SERIES DE MELANGES SONT REALISEES. DIFFERENTS MELANGES POLYSULFONE/CAOUTCHOUC ACRYLATE, A DIVERS POURCENTAGES SONT ELABORES DANS UNE EXTRUDEUSE BIVIS CO-ROTATIVE INDUSTRIELLE. LA SOUPLESSE DU MATERIAU AUGMENTE AVEC LA PROPORTION EN CAOUTCHOUC, MAIS SA TENUE THERMIQUE DIMINUE. TOUTEFOIS, LE MELANGE CONTENANT 60 % DE PHASE SOUPLE PRESENTE UN COMPROMIS INTERESSANT ENTRE TOUTES LES PROPRIETES REQUISES. UNE SECONDE SERIE ETUDIEE CONCERNE DES MELANGES POLYPHENYLSULFONE/CAOUTCHOUC ACRYLATE EXTRUDES EN LABORATOIRE. LES DIFFERENTS POURCENTAGES REALISES MONTRENT LA MEME EVOLUTION DES PROPRIETES QUE LA PREMIERE SERIE AVEC UN MELANGE 60 % ENCORE PLUS <b>PROMETTEUR.</b> POUR COMPLETER CETTE ETUDE, UN SUIVI SOMMAIRE DE L'EVOLUTION DES MELANGES PAR VIEILLISSEMENT THERMIQUE EST EFFECTUE. IL REVELE UNE MEILLEURE RESISTANCE DES MELANGES A MATRICE POLYPHENYLSULFONE, MEME S'IL EXISTE UNE RIGIDIFICATION DES MELANGES AINSI QU'UNE MODIFICATION DE LEURS PROPRIETES THERMIQUES. ENFIN, NOUS AVONS VERIFIE LA FAISABILITE DES GAINES ELECTRIQUES, APPLICATION INDUSTRIELLE DE CETTE ETUDE, AFIN D'EVALUER L'EVOLUTION DES PROPRIETES LORS D'UNE SECONDE EXTRUSION. TOULON-BU Centrale (830622101) / SudocSudocFranceF...|$|E
40|$|LE VIRUS DE L'IMMUNODEFICIENCE FELINE (FIV) EST UN LENTIVIRUS LYMPHOTROPE D'HOTE NON-PRIMATE CONSIDERE COMME UN MODELE ANIMAL TRES <b>PROMETTEUR</b> DE L'INFECTION PAR LE VIRUS DE L'IMMUNODEFICIENCE HUMAINE (HIV). DE NOMBREUSES ETAPES DE LA REPLICATION DU FIV SONT ENCORE MAL CONNUES. PAR NOTRE TRAVAIL DE THESE, NOUS AVONS TENTE D'ELUCIDER LA REGULATION TRANSCRIPTIONNELLE ET LE TROPISME CELLULAIRE DE CE VIRUS. NOUS AVONS CARACTERISE LA PROTEINE TRANSACTIVATRICE (FTAT) DU FIV. FTAT EST SIMILAIRE A LA PROTEINE TAT DES LENTIVIRUS DES ONGULES, ELLE POSSEDE UNE ACTIVITE TAR-INDEPENDANTE, ET TRANSACTIVE FAIBLEMENT LE PROMOTEUR DU FIV. LES SITES IMPLIQUES DANS LA TRANSACTIVATION PAR FTAT ONT DES SEQUENCES CONSENSUS POUR AP- 1, C/EBP ET ATF. LA GLYCOPROTEINE CD 9 A ETE PROPOSEE COMME UN RECEPTEUR CELLULAIRE PUTATIF DU FIV SUR LA BASE D'ETUDES D'INHIBITION DE L'INFECTION PAR UN ANTICORPS ANTI-CD 9. NOUS AVONS MONTRE QUE CET ANTICORPS AGIT A UNE ETAPE POST-ENTREE DU CYCLE DE REPLICATION SANS INHIBER L'INTERACTION DU VIRUS AVEC LA CELLULE HOTE. NOUS AVONS CIBLER L'ACTIVITE ANTI-VIRALE DE CET ANTICORPS A L'ETAPE DE L'ASSEMBLAGE ET/OU DU BOURGEONNEMENT DES PARTICULES VIRALES. LE CHIMIORECEPTEUR CXCR 4 EST UN (CO) RECEPTEUR POUR LEFIV. TOUTEFOIS, CERTAINES LIGNEES CELLULAIRES CXCR 4 + SONT REFRACTAIRES A L'INFECTION PAR DES SOUCHES PRIMAIRES DE FIV. POUR MIEUX COMPRENDRE LE ROLE DE CXCR 4 COMME RECEPTEUR PRINCIPAL OU CORECEPTEUR, NOUS AVONS PRODUIT LA GLYCOPROTEINE DE SURFACE (GP 95) DU FIV SOUS LA FORME D'UNE IMMUNOADHESINE (GP 95 -FC). L'ATTACHEMENT CELLULAIRE DE LA GP 95 -FC A ETE ETUDIE PAR CYTOMETRIE DE FLUX EN UTILISANT DIFFERENTES LIGNEES CELLULAIRES. NOS TRAVAUX ONT MONTRE QUE L'INTERACTION DE LA GP 95 -FC AVEC LA CELLULE HOTE FAIT INTERVENIR LE CHIMIORECEPTEUR CXCR 4, LES HEPARANES SULFATES ET UNE PROTEINE NON-IDENTIFEE DE 40 KDA, ET DEPEND DE L'ORIGINE VIRALE DE LA GP 95 (SOUCHE PRIMAIRE VERSUS SOUCHE ADAPTEE EN LABORATOIRE) MAIS EGALEMENT DE LA LIGNEE CELLULAIRE UTILISEE (LYMPHOIDE VERSUS NON-LYMPHOIDE). PARIS 7 -Bibliothèque {{centrale}} (751132105) / SudocSudocFranceF...|$|E
40|$|LES TRAVAUX PRESENTES DANS CE MEMOIRE PORTENT SUR LA SYNTHESE DE COMPOSES ORIGINAUX SUSCEPTIBLES D'INTERAGIR AVEC LE PROCESSUS DE LA REPLICATION DU VIRUS DU SIDA. DEUX ETAPES STRATEGIQUES DE LA REPLICATION DU RETROVIRUS ONT ETE CIBLEES : LA REVERSE TRANSCRIPTASE (RT), QUI CONTROLE LA TRANSCRIPTION DU GENOME VIRAL, ET LA PROTEINE TAT, QUI INDUIT LA TRANSACTIVATION DE SON EXPRESSION. L'INHIBITION DE LA RT A ETE ENVISAGEE PAR L'ACTION D'ANALOGUES DE NUCLEOSIDES DONT LES MODIFICATIONS CONCERNENT LA PARTIE HETEROCYCLIQUE (BASE NUCLEIQUE) OU LA POSITION 3 -OH. LES MODIFICATIONS DE LA PARTIE AGLYCONE DES NUCLEOSIDES ONT ETE REALISEES EN SERIE RIBOSE ET 2 -DESOXYRIBOSE PAR COUPLAGE N-OSIDIQUE ENTRE UN HALOGENOSUCRE ET DIFFERENTS AZAHETEROCYCLES SOUFRES. LES THIOAZAHETEROCYCLES INTRODUITS EN POSITION ANOMERIQUE DU SUCRE, DE TYPE THIAZINIQUES (6 CHAINONS) ET THIAZOLIQUES (5 CHAINONS), ONT ETE SYNTHETISES A PARTIR DE PRECURSEURS N-THIOACYLAMIDINES (1 -THIA- 3 -AZA- 1, 3 BUTADIENES) PAR REACTION DE CYCLOADDITION. LA FONCTIONALISATION DE L'HYDROXYLE EN 3 DE LA THYMIDINE, L'UN DES ACIDES NUCLEIQUES NATURELS ESSENTIELS, QUI A CONSTITUE LE DEUXIEME OBJECTIF DE CE TRAVAIL, A CONSISTE A INTRODUIRE SUR CETTE POSITION STRATEGIQUE UN SYSTEME THIAAZAHETERODIENIQUE (N-THIOACYLAMIDINES) DONT LA REACTIVITE A ENSUITE ETE EXPLOITEE POUR ACCEDER A UNE VARIETE D'ANALOGUES HETEROCYCLIQUES ORIGINALE. L'ACTIVITE DE CES PREMIERS COMPOSES A ETE EVALUEE VIS A VIS DU VIH. PAR LA SUITE, UNE EXTENSION DE CES TRAVAUX A CONCERNE UNE ETUDE SUR L'ACCES A DES DIMERES NUCLEOSIDIQUES DONT LE LIEN INTERNUCLEOSIDIQUE, ORIGINELLEMENT DE TYPE PHOSPHATE, DEVAIT ETRE SUBSTITUE PAR UN ENCHAINEMENT THIAAZAHETEROCYCLIQUE OU THIAAZAHETERODIENIQUE. LES PREMIERES INVESTIGATIONS INFRUCTUEUSES QUI ONT ETE MENEES ONT POSE LES BASES D'UNE STRATEGIE DE SYNTHESE QUI CONSTITUERA LE DEVELOPPEMENT DE CES RECHERCHES VERS DES OLIGONUCLEOTIDES ORIGINAUX. DANS LA DERNIERE PARTIE DU MEMOIRE L'INHIBITION DE LA PROTEINE TAT A ETE ETUDIEE PAR L'ACTION DE MOLECULES POLYAROMATIQUES FONCTIONNALISEES AVEC DES GROUPEMENTS DONNEURS DE LIAISON HYDROGENE TDS. CES ANALOGUES SYNTHETIQUES ONT MONTRES UNE EFFICACITE VIS-A-VIS DE LA PROTEINE TAT A LAQUELLE ILS SE LIENT SPECIFIQUEMENT ET CEC QUELQUE SOIT SON ORIGINE. CETTE NOUVELLE CLASSE D'INHIBITEUR DE LA PROTEINE TAT POURRAIT S'AVERER, COMPTE TENU DES PREMIERS RESULTATS BIOLOGIQUES TRES <b>PROMETTEUR,</b> UN COMPLEMENT EFFICACE AUX AGENTS BLOQUANTS DU CYCLE VIRAL DEJA UTILISES CONTRE LE SIDA. NANTES-BU Sciences (441092104) / SudocSudocFranceF...|$|E
40|$|Light is {{incredibly}} versatile for measuring {{all kinds of}} physical quantities :temperature, electric field (E-field), displacement and strain etc. Photonic sensors are promising candidates for {{the new generation of}} sensors developments due to their virtues of high sensitivity, large dynamic range and compact size etc. Integrated and on-fiber end photonic sensors on thin film lithium niobate (TFLN) exploring the electro-optic (EO) and pyro-electric effects are studied in this thesis in order to design E-field sensors and temperature sensors (T-sensors). These studies aim to develop sensors with high sensitivity and compact size. To achieve that aim, sensors that are made of photonic crystals (PhC) cavities are studied by sensing the measurand through the resonance wavelength interrogation method. In integrated sensor studies, intensive numerical calculations by PWE method, mode solving technique and FDTD methods are carried out for the design of high light confinement waveguiding structures on TFLN and suitable PhC configurations. Four types of waveguide (WG) structures (ridge WG, strip loaded WG, slot WG and double slot WG) are studied with a large range of geometrical parameters. Among them, slot WG yields the highest confinement factor while strip loaded WG is an easier option for realizations. Bragg grating is designed in slot WG with an ultra compact size (about 0. 5 µm× 0. 7 µm × 6 µm) and is employed to design PhC cavity. A moderate resonance Q of about 300 in F-P like cavity where the mirrors are made of PhC is achieved with ER of about 70 % of the transmission. Theoretical minimum E-field sensitivity of this slot Bragg grating structure can be as low as 200 µV/m. On the other hand, Si 3 N 4 strip loaded WG is designed with 2 D PhC structure and a low resonance Q of about 100 is achieved. Fabrications of nano-metrical WG such as ridge WG Si 3 N 4 strip loaded are demonstrated. However, the realization of nanometric components on LN presents a big challenge. In the on-fiber end sensor studies, guided resonance, oftentimes referred to as Fano resonance due to its asymmetric lineshape, is studied with different PhC lattice types. A Suzuki phase lattice (SPL) PhC presenting a Fano resonance at the vicinity of 1500 nm has been studied and demonstrated as temperature sensor with sensitivity of 0. 77 nm/oC with a size of only 25 µm × 24 µm. In addition, guided resonances on rectangular lattice PhC have been systematically studied through band diagram calculations, 2 D-FDTD and 3 D- FDTD simulations. La lumière est incroyable polyvalente pour mesurer toutes sortes de grandeurs physiques : température, champ électrique, déplacement et déformation, etc. Les capteurs photoniques sont des candidats <b>prometteurs</b> pour les développements de nouvelles générations de capteurs en raison de leurs vertus de sensibilité élevée, une grande gamme dynamique, etc. Les capteurs intégrés et ceux placés en bout de fibre sur une couche mince de niobate de lithium seront ici étudiés en explorant l’électro-optique ainsi que les pyro-électronique afin de concevoir des capteurs de champs et de capteurs de température...|$|R
40|$|Novel {{advanced}} {{metal-oxide semiconductor}} (MOS) {{devices such as}} fully-depleted-silicon-on-insulator (FD-SOI) Tri-gate transistor, junctionless transistor, and amorphous-oxide-semiconductor thin film transistor were developed for continuing down-scaling trend and extending the functionality of CMOS technology, for example, the transparency and the flexibility. In this dissertation, the electrical characteristics and modeling of these advanced MOS devices are presented and they are analyzed. The sidewall mobility trends with temperature in multi-channel tri-gate MOSFET showed that the sidewall conduction is dominantly governed by surface roughness scattering. The degree of surface roughness scattering was evaluated with modified mobility degradation factor. With these extracted parameters, {{it was noted that}} the effect of surface roughness scattering can be higher in inversion-mode nanowire-like transistor than that of FinFET. The series resistance of multi-channel tri-gate MOSFET was also compared to planar device having same channel length and channel width of multi-channel device. The higher series resistance was observed in multi-channel tri-gate MOSFET. It was identified, through low temperature measurement and 2 -D numerical simulation, that it could be attributed to the variation of doping concentration in the source/drain extension region in the device. The impact of channel width on back biasing effect in n-type tri-gate MOSFET on SOI material was also investigated. The suppressed back bias effects was shown in narrow device (Wtop_eff = 20 nm) due to higher control of front gate on overall channel, compared to the planar device (Wtop_eff = 170 nm). The variation of effective mobility in both devices was analyzed with different channel interface of the front channel and the back channel. In addition, 2 -D numerical simulation of the the gate-to-channel capacitance and the effective mobility successfully reconstructed the experimental observation. The model for the effective mobility was inherited from two kinds of mobility degradations, i. e. different mobility attenuation along lateral and vertical directions of channel and additional mobility degradation in narrow device due to the effect of sidewall mobility. With comparison to inversion-mode (IM) transistors, the back bias effect on tri-gate junctionless transistors (JLTs) also has been investigated using experimental results and 2 -D numerical simulations. Owing to the different conduction mechanisms, the planar JLT shows more sensitive variation on the performance by back biasing than that of planar IM transistors. However, the back biasing effect is significantly suppressed in nanowire-like JLTs, like in extremely narrow IM transistors, due to the small portion of bulk neutral channel and strong sidewall gate controls. Finally, the characterization method was comprehensively applied to a-InHfZnO (IHZO) thin film transistor (TFT). The series resistance and the variation of channel length were extracted from the transfer curve. And mobility values extracted with different methods such as split C-V method and modified Y-function were compared. The static characteristic evaluated as a function of temperature shows the degenerate behavior of a-IHZO TFT inversion layer. Using subthreshold slope and noise characteristics, the trap information in a-IHZO TFT was also obtained. Based on experimental results, a numerical model for a-IHZO TFT was proposed, including band-tail states conduction and interface traps. The simulated electrical characteristics were well-consistent to the experimental observations. For the practical applications of novel devices, the electrical characterization and proper modeling are essential. These attempts shown in the dissertation will provides physical understanding for conduction of these novel devices. Selon la feuille de route des industriels de la microélectronique (ITRS), la dimension critiqueminimum des MOSFET en 2026 ne devrait être que de 6 nm [1]. La miniaturisation du CMOS reposeessentiellement sur deux approches, à savoir la réduction des dimensions géométriques physiques etdes dimensions équivalentes. La réduction géométrique des dimensions conduit à la diminution desdimensions critiques selon la « loi » de Moore, qui définit les tendances de l’industrie dessemiconducteurs. Comme la taille des dispositifs est réduite de façon importante, davantage d’effortssont consentis pour maintenir les performances des composants en dépit des effets de canaux courts,des fluctuations induites par le nombre de dopants…. [2 - 4]. D’autre part, la réduction des dimensionséquivalentes devient de plus en plus importante de nos jours et de nouvelles solutions pour laminiaturisation reposant sur la conception et les procédés technologiques sont nécessaires. Pour cela,des solutions nouvelles sont nécessaires, en termes de matériaux, d’architectures de composants et detechnologies, afin d’atteindre les critères requis pour la faible consommation et les nouvellesfonctionnalités pour les composants futurs (“More than Moore” et “Beyond CMOS”). A titred’exemple, les transistors à film mince (TFT) sont des dispositifs <b>prometteurs</b> pour les circuitsélectroniques flexibles et transparents...|$|R
40|$|Selon la feuille de route des industriels de la microélectronique (ITRS), la {{dimension}} critiqueminimum des MOSFET en 2026 ne devrait être que de 6 nm [1]. La miniaturisation du CMOS reposeessentiellement sur deux approches, à savoir la réduction des dimensions géométriques physiques etdes dimensions équivalentes. La réduction géométrique des dimensions conduit à la diminution desdimensions critiques selon la loi de Moore, qui définit les tendances de l industrie dessemiconducteurs. Comme la taille des dispositifs est réduite de façon importante, davantage d effortssont consentis pour maintenir les performances des composants en dépit des effets de canaux courts,des fluctuations induites par le nombre de dopants. [2 - 4]. D autre part, la réduction des dimensionséquivalentes devient de plus en plus importante {{de nos jours}} et de nouvelles solutions pour laminiaturisation reposant sur la conception et les procédés technologiques sont nécessaires. Pour cela,des solutions nouvelles sont nécessaires, en termes de matériaux, d architectures de composants et detechnologies, afin d atteindre les critères requis pour la faible consommation et les nouvellesfonctionnalités pour les composants futurs (More than Moore et Beyond CMOS). A titred exemple, les transistors à film mince (TFT) sont des dispositifs <b>prometteurs</b> pour les circuitsélectroniques flexibles et transparents. Novel advanced {{metal-oxide semiconductor}} (MOS) devices such as fully-depleted-silicon-on-insulator (FD-SOI) Tri-gate transistor, junctionless transistor, and amorphous-oxide-semiconductor thin film transistor were developed for continuing down-scaling trend and extending the functionality of CMOS technology, for example, the transparency and the flexibility. In this dissertation, the electrical characteristics and modeling of these advanced MOS devices are presented and they are analyzed. The sidewall mobility trends with temperature in multi-channel tri-gate MOSFET showed that the sidewall conduction is dominantly governed by surface roughness scattering. The degree of surface roughness scattering was evaluated with modified mobility degradation factor. With these extracted parameters, {{it was noted that}} the effect of surface roughness scattering can be higher in inversion-mode nanowire-like transistor than that of FinFET. The series resistance of multi-channel tri-gate MOSFET was also compared to planar device having same channel length and channel width of multi-channel device. The higher series resistance was observed in multi-channel tri-gate MOSFET. It was identified, through low temperature measurement and 2 -D numerical simulation, that it could be attributed to the variation of doping concentration in the source/drain extension region in the device. The impact of channel width on back biasing effect in n-type tri-gate MOSFET on SOI material was also investigated. The suppressed back bias effects was shown in narrow device (Wtop_eff = 20 nm) due to higher control of front gate on overall channel, compared to the planar device (Wtop_eff = 170 nm). The variation of effective mobility in both devices was analyzed with different channel interface of the front channel and the back channel. In addition, 2 -D numerical simulation of the the gate-to-channel capacitance and the effective mobility successfully reconstructed the experimental observation. The model for the effective mobility was inherited from two kinds of mobility degradations, i. e. different mobility attenuation along lateral and vertical directions of channel and additional mobility degradation in narrow device due to the effect of sidewall mobility. With comparison to inversion-mode (IM) transistors, the back bias effect on tri-gate junctionless transistors (JLTs) also has been investigated using experimental results and 2 -D numerical simulations. Owing to the different conduction mechanisms, the planar JLT shows more sensitive variation on the performance by back biasing than that of planar IM transistors. However, the back biasing effect is significantly suppressed in nanowire-like JLTs, like in extremely narrow IM transistors, due to the small portion of bulk neutral channel and strong sidewall gate controls. Finally, the characterization method was comprehensively applied to a-InHfZnO (IHZO) thin film transistor (TFT). The series resistance and the variation of channel length were extracted from the transfer curve. And mobility values extracted with different methods such as split C-V method and modified Y-function were compared. The static characteristic evaluated as a function of temperature shows the degenerate behavior of a-IHZO TFT inversion layer. Using subthreshold slope and noise characteristics, the trap information in a-IHZO TFT was also obtained. Based on experimental results, a numerical model for a-IHZO TFT was proposed, including band-tail states conduction and interface traps. The simulated electrical characteristics were well-consistent to the experimental observations. For the practical applications of novel devices, the electrical characterization and proper modeling are essential. These attempts shown in the dissertation will provides physical understanding for conduction of these novel devices. SAVOIE-SCD - Bib. électronique (730659901) / SudocGRENOBLE 1 /INP-Bib. électronique (384210012) / SudocGRENOBLE 2 / 3 -Bib. électronique (384219901) / SudocSudocFranceF...|$|R
40|$|Workflow-based {{computing}} {{has become}} a dominant paradigm to design and execute scientific applications. After the initial breakthrough of now standard workflow management sys- tems, several approaches have recently proposed to decentralise the coordination of the execution. In particular, shared space-based coordination {{has been shown to}} provide appropriate building blocks for such a decentralised execution. Uncertainty is also still a major concern in scientific workflows. The ability to adapt the workflow, change its shape and switch for alternate scenarios on-the-fly is still missing in workflow management systems. In this paper, based on the shared space model, we firstly devise a programmatic way to specify such adaptive workflows. We use a reactive, rule-based programming model to modify the workflow description by changing its asso- ciated direct acyclic graph on-the-fly without needing to stop and restart the execution from the beginning. Secondly, we present the GinFlow middleware, a resilient decentralised workflow execu- tion manager implementing these concepts. Through a set of deployments of adaptive workflows of different characteristics, we discuss the GinFlow performance and resilience and show the limited overhead of the adaptiveness mechanism, making it a promising decentralised adaptive workflow execution manager. Les workflows sont devenus une manière dominante de mettre au point et d’exécuter des applications scientifiques. Après l’apparition des premiers gestionnaires de workflows, différentes approches ont eu pour but de decentraliser leur exécution. En particulier la coordination basée sur un espace partagé est connue comme pouvant constituer une brique intéressante en ce sens. D’autre part il n’est pas rare que l’exécution d’un workflow scientifique soit soumis à quelque incertitude. La possibilité de s’adapter et de changer de scénario d’exécution à la volée est une caractéristique manquante dans les gestionnaires de workflows. Dans ce papier, nous introduisons tout d’abord une manière programma- tique de spécifier un workflow adaptatif. Pour cela nous nous appuyons sur un espace partagé. Nous utilisons un modèle de programmation à base de règles de réécriture afin de modifier la description du workflow en changeant le graph sous-jacent. Ces modifications se font à la volée sans nécessité ni d’arrêter ni de redémarrer l’exécution du workflow. Nous présentons ensuite GinFlow, un moteur décentralisé d’exécution de workflows qui implémente ces concepts. Nous concluons enfin par une série d’expérimentations démontrant les perfor- mances, la tolérance aux pannes et l’impact limité des macanismes d’adaptation. Ces expériences nous font penser que GinFlow est un moteur <b>prometteur</b> pour l’exécution distribué de workflows adaptatifs...|$|E
40|$|RESUMEN: There is {{no doubt}} among most {{geography}} educators that GIS is an important tool for teaching and learning, but its use has been slowed by {{issues such as the}} cost of the software and the management of large spatial data files. The move to cloud computing is one trend that is promising for GIS in education. The "cloud" refers to a virtual network that provides many users with access to files, services, and applications. In this article I argue that cloud computing and WebGIS have the potential to transform geography education. I will describe three case studies that make use of these emerging tools in classrooms in the US, and discuss the lessons that we can learn from these cases. PALABRAS CLAVE WEBSIG; SIG; enseñanza de la geografía; la nube de Internet; ArcGIS Online; ArcGIS Explorer Desktop (AGX). ABSTRACT There {{is no doubt}} among most geography educators that GIS is an important tool for teaching and learning, but its use has been slowed by issues such as the cost of the software and the management of large spatial data files. The move to cloud computingis one trend that is promising for GIS in education. The "cloud" refers to a virtual network that provides many users with access to files, services, and applications. In this article I argue that cloud computing and WebGIS have the potential to transform geography education. I will describe three case studies that make use of these emerging tools in classrooms in the US, and discuss the lessons that we can learn from these cases. KEY WORDS WEBGIS; GIS; cloud computing; ArcGIS Online; ArcGIS Explorer Desktop (AGX). RÉSUMÉ Il n'ya aucun doute parmi les éducateurs les plus géographie que le SIG est un outil important pour l'enseignement et l'apprentissage, mais son utilisation a été ralentie par des problèmes tels que le coût du logiciel et la gestion des grands fichiers de données spatiales. Le passage au nuage de l'internet est une tendance qui est <b>prometteur</b> pour les SIG dans l'éducation. Le «nuage» se réfère à un réseau virtuel qui fournit de nombreux utilisateurs avec un accès aux fichiers, services et applications. Dans cet article, je soutiens que le nuage de l'internet et WebSIG ont le potentiel pour transformer l'éducation géographie. Je vais décrire trois études de cas qui font usage de ces outils émergents dans les salles de classe aux États-Unis, et de discuter des leçons que nous pouvons apprendre de ces cas. MOTS-CLÉS WEBSIG; SIG; le nuage de l'internet; ArcGIS Online; ArcGIS Explorer Desktop (AGX). </p...|$|E
40|$|Prenant appui sur une méthodologie d'enquête, de {{conceptualisation}} et de développement itérative - la Théorie Ancrée - notre recherche sur l'instrumentation informatique des annotations et des pratiques d'écriture professionnelles nous a permis de développer des concepts, un modèle et un prototype informatique originaux pour le support de l'organisation des soins en cancérologie. Pour mener à bien ce travail itératif de modélisation, nous nous sommes inspirés des pratiques d'écriture des soignants que nous avons observées durant plusieurs années dans un hôpital en cancérologie. Cette étude qualitative a été complétée par un état de l'art pluridisciplinaire. Ce travail d'articulation entre différents domaines scientifiques, enquête et confrontation aux acteurs du terrain nous a permis de développer les concepts de pratique annotative et d'écriture heuristique considérés comme constitutifs du travail d'organisation des soins. Ces pratiques permettent aux soignants d'appréhender leurs environnements de travail complexes, et de gérer efficacement les dynamiques et les variétés de situations et de prise {{en charge}} des patients. La caractérisation de ces pratiques nous a permis de développer un modèle informatique simple, robuste et ouvert, mais surtout ancré dans le terrain de recherche et dans les pratiques soignantes. Ce prototype grâce notamment aux contextes d'affichage diversifiés des réseaux d'annotation qu'il autorise, grâce à sa flexibilité et son évolutivité, permet effectivement d'adresser la question du support du travail collectif d'organisation des soins en cancérologie, autant qu'il est <b>prometteur</b> pour d'autres contextes d'exploitation. Our research about computerized equipment of annotations and professional writing practices has articulated Computer Sciences and Communication and Information Sciences {{in order for}} us to develop a software prototype for the support of organisation of care in oncology thanks to an empirical and longitudinal study. This articulation has relied on 4 years long of multimodal ethnography, a conceptualization based upon Grounded Theory rules, a model and a software development that we iteratively conduct simultaneously with this qualitative inquiry. This original combination of methodologies from different domains has been remarkably rich to help us build a specific point of view about oncology hospital organizations, and about the accomplishment of care work and about patients' management in these complex professional organizations. The modern medical and hospital rationalizations, including ICT and e-health tools, are in the confluence of various movements that impact upon several elements of hospital organizations, and upon medical and care practices. We focused our study on the situated writing practices of caregivers and specifically on the richness of the materiality of writings that led us to question notions such as information systems, collective and distributed production of knowledge, and the documentary production cycles for the organization of activity. We are making the hypothesis that annotations can be opportunely considered as constitutive elements in the production of "organizational texts" that are in the core of the support of the organization and the realization of collective and individual work. Caregivers rely on these "organizational texts" that they build, actualize and stabilize thanks to what we call the "annotative practice" that enables them to apprehend their complex environments of work, and to handle the dynamics and the variety of situations and to manage patients. We will show how the characterization of this "annotative practice" helped us to develop a simple, robust and open software model. We will detail the iterative process between inquiry, analysis, conceptualizations and developments that led us to stabilize our model and our web prototype that implement this annotative practice. We will conclude our work by showing that this prototype, thanks to the numerous context displays of annotations networks, thanks to its flexibility and evolutivity achieves the support of the collective organization of patients' care in oncology, as much as it seems to be relevant in other exploitation context of collective organization of work...|$|E
40|$|L'estimation de l'intensité de précipitations extrêmes est un sujet de {{recherche}} en pleine expansion. Nous présentons ici une synthèse des travaux de recherche sur l'analyse régionale des précipitations. Les principales étapes de l'analyse régionale revues sont les méthodes d'établissement de régions homogènes, la sélection de fonctions de distributions régionales et l'ajustement des paramètres de ces fonctions. De nombreux travaux sur l'analyse régionale des précipitations s'inspirent de l'approche développée en régionalisation des crues. Les méthodes de types indice de crues ont été utilisées par plusieurs auteurs. Les régions homogènes établies peuvent être contiguës ou non-contiguës. L'analyse multivariée a été utilisée pour déterminer plusieurs régions homogènes au Canada. L'adéquation des sites à l'intérieur d'une région homogène a souvent été validée par une application des L-moments, bien que d'autres tests d'homogénéité aient aussi été utilisés. La loi générale des valeurs extrêmes (GEV) est celle qui a le plus souvent été utilisée dans l'analyse régionale des précipitations. D'autres travaux ont porté sur la loi des valeurs extrêmes à deux composantes (TCEV), de même que sur des applications des séries partielles. Peu de travaux ont porté sur les relations intensité durée dans un contexte régional, ni sur les variations saisonnières des paramètres régionaux. Finalement, les recherches ont débuté sur l'application des concepts d'invariance d'échelle et de loi d'échelle. Ces travaux sont jugés <b>prometteurs.</b> Research on {{the estimation}} of extreme precipitation events is currently expanding. This field of research is of great importance in hydraulic engineering {{not only for the}} design of dams and dikes, but also for municipal engineering designs. In many cases, local data are scarce. In this context, regionalization methods are very useful tools. This paper summarizes the most recent work on the regionalization of precipitation. Steps normally included in any regionalization work are the delineation of homogenous regions, selection a regional probability distribution function and fitting the parameters. Methods to determine homogenous regions are first reviewed. A great deal of work on precipitation was inspired by methods developed for regional flow analysis, especially the index flood approach. Homogenous regions can be contiguous, but in many cases they are not. The region of influence approach, commonly used in hydrological studies, has not been often applied to precipitation data. Homogenous regions can be established using multivariate statistical approaches such as Principal Component Analysis or Factorial Analysis. These approaches have been used in a number of regions in Canada. Sites within a homogenous region may be tested for their appropriateness by calculating local statistics such as the coefficient of variation, coefficient of skewness and kurtosis, and by comparing these statistics to the regional statistics. Another common approach is the use of L-moments. L-moments are linear combinations of ordered statistics and hence are not as sensitive to outliers as conventional moments. Other homogeneity tests have also been used. They include a Chi-squared test on all regional quantiles associated with a given non-exceedance probability, and a Smirnoff test used to validate the inclusion of a station in the homogenous region. Secondly, we review the distributions and fitting methods used in regionalization of precipitation. The most popular distribution function used is the General Extreme Value (GEV) distribution. This distribution has been recommended for precipitation frequency analysis in the United Kingdom. For regional analysis, the GEV is preferred to the Gumbel distribution, which is often used for site-specific frequency analysis of precipitation extremes. L-moments are also often used to calculate the parameters of the GEV distribution. Some applications of the Two-Component Extreme Value (TCEV) distribution also exist. The TCEV has mostly been used to alleviate the concerns over some of the theoretical and practical restrictions of the GEV. Applications of the Partial Duration Series or Peak-Over-Threshold (POT) approach are also described. In the POT approach, events with a magnitude exceeding a certain threshold are considered in the analysis. The occurrence of such exceedances is modelled as a Poisson process. One of the drawbacks of this method is that it is sometimes necessary to select a relatively high threshold in order to comply with the assumption that observations are independent and identically distributed (i. i. d.). The use of a re-parameterised Generalised Pareto distribution has also been suggested by some researchers. Research on depth-duration relations on a regional scale is also discussed. Empirical approaches used in Canada and elsewhere are described. In most cases, the method consists of establishing a non-linear relationship between a quantile associated with a given duration and its return period to a reference quantile, such as a 1 -hour rainfall with a 10 -year return period. Depth duration relationships cannot be applied uniformly across Canada for events with durations exceeding two hours. Seasonal variability studies in regionalization are relatively scarce, but are required because of the obvious seasonality of precipitation. In many cases, seasonal regimes may lead to different regionalization approaches for the wet and the dry season. Some research has focused on the use of periodic functions to model regional parameters. Another approach consists of converting the occurrence data of a given event in an angular measurement and developing seasonal indices based on this angular measurement. Other promising avenues of research include the scaling approach. The debate over the possibility of scale invariance for precipitation is ongoing. Simple scaling was studied on a number of precipitation data, but the fact that intermittence is common in precipitation regimes and the presence of numerous zero values in the series does not readily lead to proper application of this approach. Recent research has shown that multiple scaling is likely a more promising avenue...|$|R
40|$|This work {{is related}} to Quantum Electrodynamics in {{semiconductor}} systems, which is a currently a very active field. I present {{one of the first}} experimental demonstrations of the strong coupling regime (SCR) for a single quantum dot inserted in a microcavity. The manuscript shows the progression toward the SCR observation with in-depth studies of the electronic and optical properties of natural GaAs quantum dots. It is organized in 6 chapters. The first chapter recalls the necessary conditions for SCR observation, highlighting the most promising systems for the realisation of the SCR at the beginning of 2003. The candidate system was determined to be a combination of : (i) a GaAs/AlGaAs interface quantum dot exciton, because of its great oscillator strength and; (ii) a microdisk for the microcavity, because of the high quality factor of the whispering gallery modes. As well as the above criteria, there are some requirements on the linewidth of the exciton emission and of the cavity mode. Indeed, the transition between weak coupling and strong coupling occurs when the Rabi splitting becomes greater than the mean linewidth of the emitter and the cavity mode. These severe constraints led to the detailed studies on the electronic and optical properties of the quantum dot emitter, described in chapters II, III and IV. Chapter II presents the GaAs quantum dot samples. Structural analysis and optical characterisations in photoluminescence (PL) allow to distinguish two types of samples : a « small » quantum dot sample and a « large » quantum dot sample, as compared to the 2 D exciton Bohr radius. Chapter III concerns the coupling strength between the GaAs quantum dot excitons and the electromagnetic field. This work was essential as although considerable work has been previously published on InAs/AlGaAs quantum dot exciton characteristics, little is known regarding GaAs natural quantum dots. The main result of this chapter is the possibility of “engineering” the excitonic oscillator strength using the lateral quantum dot size. As the characteristic lifetimes of capture and recombination in these quantum dots are similar, the interpretation of the PL kinetics required modelling of the emission dynamics. Simple models, coupled to complementary experiments such as measurements of the photon emission statistics, lead to a good description of the light/matter coupling. I show that « large » quantum dots present a greater oscillator strength compared to smaller ones, and are for this reason a better candidate for the observation of the SCR if inserted in a microdisk. In addition to large oscillator strengths, GaAs quantum dots must exhibit an excitonic linewidth narrow enough to observe the SCR. I show theoretically and experimentally in chapter IV the dependence of the exciton-phonon coupling with the confinement of the exciton, which governs the spectral linewidths and their evolution with temperature. It is shown that large quantum dots present a reduced spectral broadening with temperature as compared to small quantum dots, further enforcing their advantages as a candidate for SCR observation. In chapter V, I present the fabrication process of GaAs/AlGaAs microdisks. I also present a new technique for the realisation of AlOx microdisks, which show some advantages over traditional microdisks. At the end of this chapter, I show that in both types of microdisks, whispering gallery modes exhibit high quality factors (Q> 10000) and small effective volumes, which satisfy the criteria for SCR observation. Chapter VI is the outcome of the extensive studies presented in the previous chapters. The exciton-photon SCR in a GaAs quantum dot within a microdisk is demonstrated for the first time. Spectral tuning of the exciton and the cavity mode with temperature showed the characteristic SCR anticrossing. A Rabi splitting twice as large as each individual linewidth is demonstrating, corresponding to a clear achievement of the SCR. In conclusion, this work has lead to the observation of strong quantum optical coupling effects in 0 D semiconductor nanostructures. It opens up a rich research path in Cavity Quantum Electrodynamics, with implications for Quantum Information Science as well as for fundamental optics. Lorsqu'un émetteur est placé dans une cavité, il existe deux régimes de couplage lumière-matière : dans le régime dit de couplage faible, la cavité a pour effet de modifier le taux d'émission spontanée de l'émetteur. Cet effet perturbateur de la cavité est connu sous le nom d'effet Purcell. Dans le régime dit de couplage fort, l'interaction dipolaire électrique n'est plus perturbative; les états-propres du système couplé sont des états mixtes lumière-matière. Dans le domaine temporel, ce couplage se traduit par le fait que l'émission spontanée devient réversible : le photon émis spontanément par l'émetteur dans le mode de cavité peut à nouveau être ré-absorbé par l'émetteur, puis ré-émis, [...] . donnant ainsi lieu à un cycle d'oscillations de Rabi. Dans le domaine spectral, le couplage se manifeste par une levée de dégénerescence (ou doublet de Rabi) lorsqu'émetteur et mode de cavité sont mis en résonance. L'objet de cette thèse est la démonstration expérimentale du couplage fort entre un exciton confiné par une boîte quantique naturelle de GaAs et un mode de galerie d'un microdisque semi-conducteur. Les paramètres-clefs pour atteindre ce régime sont, pour ce qui est de l'émetteur, sa force d'oscillateur ainsi que sa largeur spectrale, gouvernée par l'interaction avec l'environnement. Un chapitre est consacré à chacune de ces 2 notions-clefs. Concernant la cavité, les 2 figures de mérite pertinentes pour le renforcement de l'interaction lumière-matière sont le facteur de qualité et le volume modal. Nous présentons la réalisation technologique et la caractérisation des microdisques de GaAs (sur air et sur AlOx) les plus <b>prometteurs</b> en terme de facteur de qualité et volume modal. Enfin, nous présentons la première démonstration expérimentale du régime de couplage fort pour une boîte quantique naturelle de GaAs en microdisque...|$|R
40|$|Thrombin is {{the central}} enzyme {{in the process of}} hemostasis, being at the cross-roads of {{coagulation}} and platelet reactions. Normally, in vivo concentration of thrombin is rigorously regulated; however, clinically impaired or unregulated thrombin generation predisposes patients either to hemorrhagic or thromboembolic complications, the major causes of mortally in the US and Europe. Monitoring thrombin in real-time is therefore needed to enable rapid and accurate determination of drug administration strategy for patients under vital threat. Aptamers, short single-stranded oligonucleotide ligands, selected for their high affinity and specificity, represent promising candidates as biorecognition elements for new-generation biosensors. Several aptamers have been developed for thrombin sensing. These aptamers are known to bind distinct exosites of thrombin that gives an advantage of building different assays involving simultaneous targeting two different binding sites. The aim of this PhD work therefore is to investigate different solutions for the integration of thrombin-binding aptamers in point-of-care devices for continuous monitoring of thrombin in plasma. First the kinetics of aptamer interaction with thrombin and specificity towards prothrombin and thrombin – inhibitor complexes was rigorously investigated using Surface Plasmon Resonance. These experiments unveiled the complex character of interaction of the HD 1 a 15 -bp aptamer with thrombin, confirming nonspecific interactions with prothrombin, natural inhibitors of thrombin, serum albumin and some of other plasma proteins, whereas another 29 -bp aptamer HD 22 proved to be highly affine and specific towards thrombin. On the other hand we explored aptamer integration options. In order to find optimal surface functionalization conditions for aptamers sensing, different supports such as gold, polystyrene, polycarbonate and appropriate protocols were used. We found that the easiest and the most efficient functionalization strategy is grafting thiol-modified aptamers on the gold surface. Furthermore to achieve nanoscale control of functional aptamer density, we showed advantage of ordered immobilization of already DNA-capped nanoparticles and well-characterized on the desired solid support through convective assembly. Aptamer-modified Gold NP-s were also used to validate thrombin detection principle through hydrodynamic assay involving aggregation of NPs {{in the presence of the}} thrombin. With this assay, we validated the principle and at the same managed to detect different concentrations of thrombin (5 - 500 nM) with good precision, study the thermal stability of the aptamers and their complex with thrombin and specificity of aptamers towards thrombin in buffer. In diluted plasma, however aggregation of nanoparticles occurred even in the absence of thrombin, suggesting the presence of nonspecific interactions with aptamers that cannot be controlled or neglected in aggregation assay. We finally proposed a novel approach to increase sensitivity and specificity for thrombin detection based on the engineering of aptadimer structures bearing aptamers HD 1 and HD 22 interconnected with a nucleic acid spacer. This spacer forms a hairpin of 4 to 9 bp, and fluorophore and quencher couple is embedded within hairpin. In the absence of target, the fluorophore remains quenched, whereas upon capturing a target, fluorescent signal is triggered. Since this strategy requires simultaneous targeting of both binding-sites, it provides an efficient solution for improved thrombin detection, by increasing specificity, selectivity and affinity, and decreasing non-specific interactions. Preliminary tests gave promising results in specific detection for thrombin and opened new perspectives for development specific aptamer-based fluosensor both for the surface and the volume measurements. La thrombine est l’enzyme principale dans le processus d’hémostase, étant impliquée dans la coagulation et les réactions plaquettaires. Les dérèglements de la concentration de thrombine en raison d’un trouble clinique prédisposent les patients à des complications hémorragiques ou thromboemboliques qui sont une cause de mortalité majeure en Europe et aux Etats-Unis. Le suivi en temps réel de la thrombine dans le sang est donc nécessaire pour améliorer le traitement de patients en état critique. Les aptamères, qui sont de courts nucléotides monobrins, sélectionnés pour leur grande affinité et spécificité vers des cibles déterminées, semblent constituer des candidats <b>prometteurs</b> pour la reconnaissance moléculaire dans des biocapteurs de nouvelle génération. Plusieurs aptamères ont été développés pour la détection de la thrombine. L’objectif de ces travaux est l’étude de différentes solutions d’intégration des aptamères dans des dispositifs de diagnostic de type «point of care» (au chevet du patient) pour le suivi en continu de la thrombine dans le plasma. Dans un premier temps, la cinétique d’interaction des aptamères avec la thrombine et leur spécificité vis-à-vis de la prothrombine et des inhibiteurs de la thrombine ont été étudiés rigoureusement par résonance par plasmons de surface (SPR). Ces travaux ont démontré la faible spécificité de l’aptamère HD 1, qui un nucléotide de 15 bases, vis-à-vis de la thrombine, et ont confirmé le présence d’interactions non-spécifiques avec la prothrombine, les inhibiteurs naturels de la thrombine, l’albumine de boeuf et d’autres protéines plasmatiques. Inversement, nous avons observé une bonne affinité de l’aptamère HD 22 avec la même liste de cible. Parallèlement, nous avons évalué des stratégies d’intégration d’aptamères dans des dispositifs d’analyse. Nous montrons en particulier que la technique le plus simple et le plus efficace de fonctionnalisations de surface par des aptamères est fondée sur le couplage de fonctions thiol sur des surfaces d’or. Le principe de reconnaissance a ensuite été validé par des expériences d’agrégation en régime hydrodynamique. Celles-ci ont montré la possibilité de détecter la thrombine dans des gammes de concentration de 5 à 500 nM. Toutefois, dans du plasma, l’agrégation des nanoparticules a été observée en absence de thrombine suggérant la présence d’interactions non spécifiques. Enfin, afin d’augmenter la spécificité de la détection de la thrombine, nous avons proposé une nouvelle approche basée sur l’ingénierie de structures dimères interconnectant HD 1 et HD 22. Le lien entre ces deux aptamères peut se replier suivant une structure en tête d’épingle, ce qui met à proximité un fluorophore et son absorbeur de fluorescence. En l’absence de cible, le fluorophore est éteint en raison de la proximité l’absorbeur, alors qu’après capture de la cible, le signal de fluorescence est augmenté. Des tests préliminaires ont donné des résultats encourageants pour la détection spécifique de la thrombine, même dans le plasma...|$|R
40|$|In {{a context}} of growing needs of {{communication}} means to increase flight safety and meet the expectations of companies and passengers, the world of civil aviation seeks new communication systems that can meet these objectives. The Aeronautical Ad-Hoc Networks, AANETs represent an innovative approach to address this problem. It is self-configured networks, using no fixed infrastructure where the nodes are commercial aircraft. The AANETs {{can be seen as}} a subset of the VANET (Vehicular Ad-Hoc Networks) since they share many features as the constraints imposed on the trajectories. In order to use these mobile networks more efficiently while meeting the needs of new applications, such as the transmission of weather information in real time, requiring air to air communications., we propose in this thesis to use the paradigm of content based routing above AANET. In this kind of routing, it is not a destination address that is used to identify the recipients, but the message content itself. In this paradigm, a transmitter sends a message having attributes and the message is then transmitted by the network to nodes interested by the content of the message. Applied to weather information update, this approach allows an aircraft detecting a dangerous phenomenon such as a thunderstorm to only prevent interested nodes, ie those whose the trajectory come close to the storm during the lifetime of the event. In this thesis, we have chosen to rely on the popular Publish / Subscribe (P / S) paradigm to provide a content based routing service. In this approach, publishers publish events. On the other side, nodes send subscriptions to declare their interest and the system is then in charge of forward events to nodes that match their needs [...] After a state of the art about existing P / S systems, particularly those adapted to VANETs, we choose to test the solutions seemed interesting in a AANET context. To accomplish this, we have developed as a Omnet ++ mobility model using real position reports to replay a full day of traffic of aircraft and several aeronautical applications based on a P / S system to generate realistic data. The results show that these solutions are not completely suitable for AANET context. Therefore, in a second step, we proposed a new P / S system which is more efficient on a AANET. This solution is based on an overlay network built thanks to a new of 1 -hopping clustering algorithm suitable for AANET. In order to increase the stability of the overlay architecture, this algorithm is based on the number of neighbors and the relative mobility between the nodes to define groups. The tests show that the P iii / S system based on this overlay provides better results than the previously tested solutions, whether in terms of network load or percentage of transmitted events. Les réseaux Ad Hoc mobiles (MANET : Mobile Ad hoc NETworks), sont des réseaux auto-configurés, n'utilisant pas d'infrastructure fixe. Les AANET (Aeronautical Ad hoc NETworks) sont un sous-ensemble de ces réseaux dont la spécificité réside dans le fait que les nœuds composant le réseau sont des avions commerciaux avec des schémas de mouvements caractéristiques. Les AANETs peuvent apparaître comme un moyen de communication complémentaire aux systèmes existants entre les avions et le sol ou entre les avions eux mêmes. Afin de répondre aux besoins de nouvelles applications, telle que l'information météorologique sur des phénomènes dangereux en temps réel, qui nécessitent des communications d'avion à avion, le paradigme du routage basé sur le contenu semble <b>prometteur.</b> Dans ce type de routage, ce n'est plus une adresse de destination qui est utilisée pour joindre le ou les correspondants, mais le contenu du message qui permet de décider des destinataires. Dans ce paradigme, un émetteur envoi un message possédant des attributs et le message est alors transmis par le réseau uniquement aux terminaux intéressés par le contenu du message. Dans cette thèse, nous avons conçu un système de publication/souscription basé sur le contenu et adapté aux AANETs. Ce système s'appuie sur une architecture recouvrante ("overlay network") construite à l'aide d'un algorithme original de regroupement à 1 -saut (1 -hop clusterisation) adapté aux AANETs. Ce système a été validé en simulation avec un rejeu de trajectoires avion réelles...|$|E
40|$|International audienceMiami, a latin {{american}} music Hub. Between diversity and standardization of transnational cultural products Referring {{to the topic of}} Latino-American music under the perspective of singularity in the title is not without meaning. What at first seems to evict the diversity and heterogeneity of South American practices enables on the contrary to enhance the very specific role played by Miami in the market of " Latino " music. By becoming the centre of exchanges between the two Americas, the city has gradually imposed itself as the entertainment capital for Latin America. The majors of the record industry have managed to create a hybrid cultural project with their " Latino " subsidiaries by using influences of the " periphery " referred to as the " Miami Sound " by specialists and professionals. By integrating music elements from various Latin American, North American and European influences in a single mass media content, music labels, supported by powerful broadcasting means such as MTV Latino or Univisión, the first Hispanic channel in the US, have turned Miami into the cultural window of a hybrid Latin American identity which is able to transcend different regional particularities of the continent. We first need to understand how and why Miami became this " cultural capital of Latin America " (Yudice, 2003). As a " hub " between the two Americas (Girault, 1998), it is thanks to the influx of big fashion, entertainment, communication and media companies that it got this central position, promoted by an important presence of capitals and banks from around the world, attracted by favourable tax policies {{at the end of the}} Seventies. This economic success, mostly the result of the first wave of migration after the Cuban Revolution of 1958 - 59, which brought people with a good knowledge of the business world and a great ability to develop commercial relationships with the South American continent, largely contributed to the booming of the city on an international level and also helped attract capital, investors and population. Executives and professionals of the entertainment industry find in Miami a welcoming place for the development of their activity, supported by the manpower of poor populations from Cuba and other Latin American countries brought there by the successive immigration waves of the 1970 's and 1980 's. These phenomena give a solid basis to a very promising " Latino " market for the music industry, which ended up installing its Latin American subsidiaries at the beginning of the 1990 's. We will then show what exactly this Miami Sound is and what were the strategies of record labels to promote it and transform it into what is today one of the most profitable markets of the sector. To illustrate this, we will look at the career of the Cuban-American singer and producer Emilio Estefan, since he is one of the pioneers of this " U. S. Latin American " music fusion that is so typical of Miami. He also became one of the most active producers of the city, stepping aside as a singer, following the success of his wife Gloria Estefan and their " Miami Sound Machine " band. This enabled his development and appearance on the international music scene. Miami then turned into the very place where "Latino music" is recorded, produced, promoted and broadcast. However, this was only made possible thanks to the establishment of record labels that rapidly organised the infant industry. Sony was one of the companies that perceived this Latin American potential at a very early stage and kept developing the market, particularly thanks to the talent of Emilio and Gloria Estefan. Thus, we will show in what way the association of Emilio Estefan with Discos CBS International, a Miami-based division of CBS Inc. in 1980 was a key moment in the expansion of the singer, the label, and Floridian Latin pop. A transnational strategy with many levels then developed to become recurrent and systematic within the major for the next 20 years, as we will explain with the example of the Cuban couple. To analyse these phenomena, we will base our study on Emilio Estefan's recently-published autobiography, as well as on some of the key songs of this " Miami Sound ". We will also use articles of the specialised press (from Billboard magazine, in particular with articles by Leila Cobo, a Latin American music specialist) and general-interest press from Miami (El Nuevo Herald, Miami Today). Annual reports of the IFPI and Sony Corporation will also be used in our research. In addition, a dozen interviews with producers, musicians and professionals of the industry, conducted between September and December 2010 in Miami, will enable to back up our argumentation. With all these elements, we will conduct a crosswise analysis of the socio-economical processes that helped turn Miami into a privileged place of transnational Latino pop production, as well as the socio-cultural processes that enabled the development of a hybrid culture specific of Miami and essential to understand the position that this city has today on the market of Latin(o) popular music. To conclude, we could ask ourselves whether what is made in Miami, in a strong context of industrialisation, tends to standardize Latin American expression forms, or if on the contrary the multiplicity of creative forms from Latin America does not enrich the diversity of the music offering produced by the oligopoly of the four big transnational firms implanted in Miami, in particular Sony music latin. It seems interesting to build a bridge between a socio-economical approach of the subject, in order to be aware of the economical and industrial environment in which the object of study developed (Bouquillion, 2011; Burnett 1996), and a more socio-anthropologic vision, under the influence of the works of George Yúdice, who studied the Latino-American music industry and the important position played in it by the city of Miami. Finally, in the same vein, the publications of Nestor García Canclini will shed light on these hybrid cultural phenomena at stake between Latin America and the United States. Aborder dans le titre l'objet de la musique latino-américaine sous l'angle de la singularité n'est pas neutre. Ce qui semble au premier abord évincer diversité et hétérogénéité des pratiques sud-américaines permet au contraire de mettre en exergue le rôle tout à fait singulier de Miami dans le marché de la musique " latino ". C'est en devenant le centre des échanges entre les deux Amériques que la ville s'est progressivement imposée comme capitale de l'entertainment pour l'Amérique latine. Les majors de l'industrie du disque, présentes avec leurs filiales " latino ", ont su créer, en se nourrissant des influences de la " périphérie ", un produit culturel hybride que les spécialistes et professionnels nomment le "Miami Sound". En intégrant des éléments musicaux issus de diverses influences latinoaméricaines, nord-américaines et européennes dans un seul et même contenu médiatique de masse, les labels de musique, relayés par de puissants moyens de diffusion tels MTV latino ou Univisión, la première chaîne hispanophone des États-Unis, ont fait de Miami la vitrine culturelle d'une identité latino-américaine hybride se présentant dès lors comme capable de transcender les différentes spécificités régionales du continent. Il s'agit d'abord de comprendre pourquoi et comment Miami est devenue cette " capitale culturelle de l'Amérique latine " (Yudice, 2003). En tant que " point de contact et d'échange (hub) " entre les Amériques (Girault, 1998), c'est grâce à l'afflux des grandes entreprises de la mode, de l'entertainment, de la communication et des médias qu'elle a obtenu cette position centrale, encouragée par une présence importante des capitaux et des banques du monde entier, très fortement attirés dès la fin des années 1970 par des politiques fiscales avantageuses. Cette réussite économique, que l'on doit en grande partie à la première vague d'émigrés de la révolution cubaine de 1958 - 59, débarqués avec une connaissance du monde des affaires et une capacité à nouer des liens commerciaux avec le continent sudaméricain, a largement contribué à l'éclosion de la ville sur le plan international et ainsi à attirer à la fois capitaux, investisseurs et populations. Cadres et professionnels de l'entertainment trouvent alors à Miami une terre d'accueil propice au développement de leur activité, soutenue par une main d'œuvre issue des vagues d'immigration des années 1970 et 1980 des populations pauvres de Cuba et du reste de l'Amérique latine. Ces phénomènes ancrant une base solide à un marché " latino " très <b>prometteur</b> pour l'industrie musicale, qui finit par y installer ses filiales latino-américaines au tout début des années 1990. Nous montrerons ensuite à quoi correspond ce "Miami Sound" et quelles ont été les stratégies des maisons de disques pour le promouvoir et en faire aujourd'hui l'un des marchés les plus rentables du secteur. Pour cela, nous nous pencherons sur le parcours du chanteur producteur cubano-américain Emilio Estefan, car il est l'un des précurseurs de cette fusion musicale " pop latino-étasunienne " caractéristique de Miami. C'est également lui qui, en s'effaçant derrière le succès de sa femme Gloria Estefan et de leur groupe le " Miami Sound Machine ", est devenu l'un des producteurs les plus actifs de la ville et a ainsi permis son développement et son éclosion sur la scène musicale internationale. Miami s'est alors érigée en lieu d'enregistrement, de production, de promotion et de diffusion incontournable pour la musique " latino ". Mais cela n'a pu se faire qu'avec l'installation des maisons de disques qui ont rapidement organisé la filière naissante. Sony est une de celles qui a perçu le plus tôt ce potentiel latino-américain et n'a cessé de développer le marché, grâce notamment aux talents d'Emilio et Gloria Estefan. Ainsi, nous montrerons en quoi la signature du chanteur en 1980 par Discos CBS International, une division de CBS Inc. basée à Miami, est un moment clé dans l'essor du chanteur, du label, et partant de la latin pop floridienne. Une stratégie transnationale à plusieurs échelles, que nous expliquerons à travers l'exemple du couple cubain, se développera alors pour devenir récurrente et se systématiser au sein de la major pendant les vingt années suivantes. Pour analyser ces phénomènes, nous nous appuierons sur la biographie qu'Emilio Estefan a récemment publié, ainsi que sur certaines des chansons clés de ce " Miami Sound ". Nous utiliserons également des articles de la presse professionnelle (du magazine Billboard, avec en particulier les articles de Leila Cobo, spécialiste de la musique latinoaméricaine) et généraliste de Miami (El Nuevo Herald, Miami Today). Les rapports annuels de l'IFPI et de Sony Corporation seront aussi mobilisés pour notre recherche. Enfin une dizaine d'entretiens que nous avons réalisé avec des producteurs, des musiciens et des professionnels du secteur entre septembre et décembre 2010 à Miami permettront d'étayer notre propos. Donc à travers tous ces éléments, nous nous intéresserons à la fois aux processus socio-économiques qui ont contribué à ériger Miami comme lieu privilégié de production de cette pop latino transnationale, ainsi qu'aux processus socio-culturels ayant permis le développement d'une culture hybride spécifique à Miami et essentiels pour comprendre aujourd'hui la place de la ville dans le marché de la musique latino-américaine. Pour conclure, nous nous demanderons si ce qui est créé depuis Miami, dans un fort contexte d'industrialisation, tend à uniformiser les formes d'expression latino-américaines, ou si au contraire la multiplicité des formes créatrices en provenance d'Amérique latine ne vient pas nourrir la diversité de l'offre musicale produite par l'oligopole des quatre grandes firmes transnationales installées à Miami, et en particulier par Sony music latin. Il semble intéressant de faire le pont entre une approche socio-économique du sujet, afin d'avoir conscience de l'environnement économique et industriel dans lequel s'est développé notre objet d'étude (Bouquillion, 2011; Burnett, 1996), et une vision plus socio-anthropologique, sous l'influence des travaux de George Yúdice qui a étudié l'industrie musicale latinoaméricaine et la place importante qu'occupe en son sein la ville de Miami. Enfin, dans cette même perspective, les écrits de Nestor García Canclini nous éclaireront sur ces phénomènes culturels et identitaires hybrides en cours entre l'Amérique latine et les États-Unis...|$|E
40|$|Strontium Bismuth Titanate {{is a very}} {{promising}} material for high temperature piezoelectric applications, its elevated ferroelectric phase transition (530 °C), linear piezoelectric properties under low field and relatively low room temperature conductivity (compared to others Bismuth Titanates) make it very attractive for precision sensors. However, under severe conditions (low frequency, high field, high temperature or low oxygen partial pressure) some of those advantages disappear. Piezoelectric response is dominated by charge drift in general becomes unstable. Above all, at high temperature and low oxygen partial pressure, a large conductivity increase reduces the piezoelectric efficiency of the material, in this work, electrical conductivity, piezoelectric properties and dielectric permittivity of SrBIT ceramic have been investigated in conditions of high temperature, low oxygen partial pressure, low to moderate driving field and frequency. As {{a result of this}} research, a better understanding of SrBIT properties was achieved. Thanks to a careful study of SrBIT processing as a bulk ceramic, a reproducible route was established. Many basic mechanisms leading to both SrBIT crystallization and sintering have been identified. It was demonstrated that a detailed knowledge of the exact processing conditions is required in order to achieve high quality material. DC conductivity measurements were carried out as a function of temperature, oxygen partial pressure and dopant concentration. It was found that the apparent activation energy for conduction for undoped SrBIT was 1 eV between 140 and 220 °C and 1. 5 eV between 450 and 700 °C. Decrease of the activation energy in the lower temperature range has been discussed considering grain boundary conductivity, as a transition from electronic to ionic conduction or as consequence of small polarons conduction. It was shown that lower activation energy resulted from Manganese doping (0. 5 eV), this was interpreted as either growing influence of grain boundary conductivity as dopant concentration increases or as shallow hole trap formation. DC conductivity measurements and acceptor/donor doping experiments demonstrated p-type conductivity in the low temperature range (up to 220 °C) as donor doping decreases conductivity, while oxygen partial pressure controlled measurements indicated n-type conductivity at higher temperature (above 700 °C). An electrical impedance analysis was performed with several equivalent circuits. The aim of these models was to simulate the impedance of SrBIT. The best approximation was found with a distributed element of Havriliak-Negami type. However, as the physical justification for this circuit was not clear, the investigation of the grain, grain boundary and electrode impedances was performed with multiple discrete parallel RC elements. With temperature, grain size and oxygen activity variations, the identification of three separate contributions as grain, grain boundary and electrode was realized. The anisotropy of conductivity and permittivity was demonstrated with textured material and both DC and AC analysis. With the master curves built for the electrical modulus, {{it was found that the}} impedance probably related to Bismuth oxide layers produces an additional high frequency are. From this finding and by comparing characteristic relaxation frequencies of undoped, 2 mol. % Mn and 4 mol. % Nb SrBIT, it was determined that conductivity is higher in the ab plane direction than in the c direction within both perovskite units and Bismuth oxide layers. With conductivity measurements under controlled oxygen partial pressure, it was found that an acceptor-based (intrinsic or extrinsic) model could be used to describe the electrical conductivity under controlled oxygen partial pressure of both undoped and 2 mol. % Mn doped SrBIT. However, as neither a pO 2 - 1 / 6 region (intrinsic oxygen vacancies compensated by electrons) for undoped SrBIT nor a pO 2 - 1 / 4 region (oxygen vacancies compensated by singly ionized acceptors) for Mn doped SrBIT were seen, it was concluded that the acceptorcontrolled model is not sufficient for a complete description of SrBIT. For this reason and in order to include the low oxygen partial pressure behavior of undoped SrBIT, a donor-based (intrinsic) model was also considered. The source of intrinsic donors would be in that case Bi 3 + cations sitting on Sr 2 + sites in the perovskite sublattice. Considering Bismuth vacancies as the negative compensating species, both pO 2 - 1 / 4 and pO 2 -independent regions could be predicted with the model. However, even if the donor-controlled model seems to better match conductivity measurements in the full PO 2) range, rejecting the acceptor-based model would be an error. It is actually not demonstrated that in SrBIT the concentration of exchanged Bismuth cations is always (all temperature, pressure) larger than the natural acceptor impurity concentration. It is very likely that the cation exchange is dependent of the oxygen partial pressure. It is also not proved as suggested in the literature that direct compensation between exchanged Strontium and Bismuth exists, reducing the net donor-excess. With the acceptor-controlled model, the mass-action constants for reduction and for ionization of intrinsic carriers across the band gap were determined. The band gap of SrBIT was estimated to be 3. 5 eV. The ionic conductivity of SrBIT was determined at high temperature with measurements under controlled oxygen partial pressure. It was found that the electrical conductivity of SrBIT is probably mixed (electronic and ionic) as the estimation of the transference number provided quite large values (t= 0. 8 at 800 °C). From electronic and ionic conductivity data, mixed conduction can actually be predicted in a large temperature range (above room temperature). The piezoelectric measurements using direct effect demonstrated that it is actually possible to unlock piezoelectrically active ferroelectric domain walls and create non-linear piezoelectric properties in undoped SrBIT. This occurs above a threshold elastic field, which is thermally activated. With a piezoelectric composite, it was demonstrated that the electromechanical coupling between two different phases creates a piezoelectric relaxation. This one could be positive or negative depending on the respective properties of each composite's component. It was shown experimentally that a small temperature change is sufficient to transform a positive relaxation into a negative one. While these experiments did not provide a detailed microstructural explanation for the piezoelectric relaxation observed in 2 mol. % Mn doped SrBIT, they gave a first insight into an original phenomenological approach. Microstructure and piezoelectric properties were related with the calculation of a piezoelectric relaxation composite made of two textured samples. This demonstrated that a piezoelectric relaxation may occur just because of anisotropy. Microstructural reproduction of this coupling is actually an important feature of Aurivillius phases. Le Titanate de Strontium Bismuth (abrégé SrBIT) est un matériau très <b>prometteur</b> pour des applications piézo-électriques à haute température. Grâce à une température de transition de phase ferro-électrique élevée (530 °C), à des propriétés piézo-électriques très stables sous faible champ et à une conductivité électrique relativement basse à température ambiante (comparée à d'autres Titanates de Bismuth), ce matériau est très intéressant pour réaliser des capteurs de précision. Cependant, sous conditions extrêmes (basse fréquence, champ élevé, haute température ou basse pression d'oxygène), ces avantages disparaissent et la réponse piézo-électrique, dominée par les dérives de charge, devient instable. Mais avant tout, à haute température et basse pression d'oxygène, l'accroissement très fort de la conductivité réduit considérablement l'effet piézo-électrique de SrBIT. Dans ce travail, la conductivité électrique, les propriétés piézo-électriques et la permittivité diélectrique de SrBIT ont été étudiées à haute température, basse pression partielle d'oxygène, faible champ et fréquence. Cette recherche a permis une meilleure compréhension des propriétés de SrBIT. Par une étude détaillée de la préparation de SrBIT sous forme de céramique massive, une méthode de préparation reproductible a été établie. Différents mécanismes élémentaires qui interviennent lors de la cristallisation ou du frittage ont été identifiés. Mais, il a été démontré également qu'une connaissance pointue des paramètres du procédé est requise pour garantir la préparation de matériaux de bonne qualité. Des mesures de conductivité électrique de type DC ont été effectuées en variant la température, la pression partielle d'oxygène et la teneur en dopant. L'énergie d'activation pour la conductivité déterminée lors de ce travail de thèse est de 1 eV entre 140 et 220 °C et de 1. 5 eV entre 450 et 700 °C. La diminution de l'énergie d'activation dans la gamme inférieure de température a été interprétée comme la conséquence d'une conductivité aux joints de grain prépondérante à basse température. Les hypothèses de la transition d'un régime électronique à un régime ionique ou la conduction par polarons ont également été considérées. La diminution de l'énergie d'activation (0. 5 eV) par le dopage de type accepteur de SrBIT a été interprétée comme soit l'influence grandissante de la conductivité des joints de grains soit le piégeage des trous dans des niveaux peu profonds. Les mesures de conductivité DC et les dopages de type donneur et accepteur ont démontré une conductivité de type p dans la gamme inférieure de température (jusqu'à 220 °C), car l'ajout d'une certaine quantité de Niobium (donneur) diminue la conductivité de SrBIT. Par contre, les mesures effectuées à plus haute température sous pression partielle d'oxygène contrôlée indiquent une conductivité de type n. L'analyse dc l'impédance électrique de SrBIT a été réalisée à l'aide de différents circuits équivalents. A l'aide de ces modèles, l'impédance de SrBIT a pu être simulées. La meilleure approximation a été trouvée avec un élément distribué de type Havriliak-Negami. Cependant, comme la justification physique de ce circuit n'est pas claire l'étude de l'impédance des grains, des joints de grains et de l'interface céramique-électrode a été effectuée avec plusieurs circuits équivalents de type discret. À l'aide de l'influence de la température, de la taille de grain et de la pression partielle d'oxygène, l'identification de l'impédance des grains, des joints de grains et de l'électrode a pu être établie. L'anisotropie de la conductivité et de la permittivité a été démontrée par l'analyse AC et DC de matériau texturé. Grâce à des courbes maîtresses construites pour le module électrique, l'impédance probablement liée aux couches d'oxyde de Bismuth a été observée à haute fréquence. Avec ce résultat et en comparant les fréquences de relaxation de SrBIT non-dopé, dopé accepteur (2 % mol Mn) et dopé donneur (4 % mol.), il a pu être démontré que la conductivité est plus grande dans la direction dite "plan ab" que dans la direction "c" pour les unités de structure pérovskite et pour les couches d'oxyde de Bismuth. Avec des mesures de conductivité DC à haute température (> 700 °C) sous pression partielle d'oxygène contrôlée, il a pu être démontré qu'un modèle basé sur un excès d'accepteurs pouvait être avantageusement utilisé pour décrire la conductivité de SrBIT non-dopé et dopé accepteur. Cependant, comme aucune région en pO 2 - 1 / 6 (compensation des lacunes d'oxygènes intrinsèques par des électrons) pour SrBIT non-dopé ou en O 2 - 1 / 4 (compensation des lacunes d'oxygènes par des accepteurs simplement ionisés) pour SrBIT dopé Mn n'ont été observée, il a été conclu qu'un modèle basé uniquement sur un excès d'accepteurs était insuffisant. C'est pourquoi, afin aussi d'inclure le comportement de SrBIT à très basse pression d'oxygène, un autre modèle, basé sur un excès intrinsèque de donneurs a également été considéré. Dans ce modèle, la source intrinsèque de donneur proviendrait d'un échange de site entre Bi 3 + des couches d'oxydes de Bismuth et Sr 2 + des unités pérovskite. En assumant la compensation des défauts positifs par des lacunes de Bismuth, une région de conductivité en pO 2 - 1 / 4 et une autre indépendante de la pression peuvent être prédites. Cependant, même si ce modèle semble mieux correspondre aux mesures de conductivité, rejeter le modèle accepteur serait une erreur. En effet, il n'est pas prouvé que la concentration de donneurs intrinsèque soit toujours supérieure (à n'importe quelle température ou pression) que la concentration d'impuretés. Il est par exemple très probable que l'échange de cations soit dépendant de la pression partielle d'oxygène. De plus, la compensation directe des deux cations échangés ne peut pas être exclue, ce qui réduirait l'excès net de donneur. Avec le modèle accepteur, les constantes d'équilibre pour la création de lacunes d'oxygène et pour l'ionisation de porteurs au travers de la bande interdite ont été déterminées. L'énergie de la bande interdite de SrBIT a été estimée à 3. 5 eV. Avec les mesures de conductivité sous pression partielle d'oxygène contrôlée, la conductivité ionique de SrBIT a été estimée à haute température. Ainsi, il est proposé que la conductivité de SrBIT est de type mixte (électronique et ionique), par exemple le nombre de transférence à 800 °C est de 0. 8. A partir des mesures de conductivité ionique et électronique, une conduction mixte sur une large gamme de température peut être prédite. Des mesures piézo-électriques ont démontré que le désancrage de parois de domaines ferroélectriques actives pour la piézo-électricité de SrBIT était possible, ce qui induit des propriétés piézo-électriques non linéaires. Ceci se produit au-dessus d'un seuil de champ élastique qui est activé thermiquement. Avec un composite piézo-électrique, la création d'une relaxation piézoélectrique par le couplage électro-mécanique de deux différentes phases a été démontré. Cette relaxation peut être positive ou négative selon les propriétés des matériaux constituant le composite. Par exemple, un petit écart de température transforme une relaxation positive en relaxation négative. Bien que ces expériences ne permettent pas d'expliquer les relaxations piézo-électriques observées pour SrBIT dopé Manganèse, elles donnent un premier aperçu d'une approche phénoménologique originale. Le lien entre la microstructure et la relaxation piézoélectrique a été établi par le calcul du coefficient piézo-électrique d'un composite constitué de deux échantillons texturés. Il a ainsi été démontré que les différences de propriétés dues à l'anisotropie de SrBIT suffisent à créer une relaxation. La reproduction interne à la microstructure de ce couplage est certainement une caractéristique importante des phases d'AuriviIlius...|$|E
40|$|Sous le climat à faible pluviosité du Tchad, les altérites sont dénoyées et seul le socle fracturé est aquifère. Le taux d'échec des forages atteint 60 % car les {{fractures}} ont une répartition très discontinue comme le montre leur organisation fractale. Cela entraîne la coexistence de secteurs productifs et stériles. A l'échelle kilométrique, on peut ajuster le nombre de sondages de reconnaissance par village {{en fonction}} des caractéristiques climatiques, topographiques et géologiques. On définit ainsi des zones de productivité forte, où le taux de succès atteint 79 %, moyenne et faible. On peut alors affecter chaque village d'un " potentiel d'investigation " qui est le produit du nombre de sondages par leur profondeur prévisionnelle. A l'échelle locale, une analyse en composantes principales des paramètres de forage montre que la présence d'eau souterraine est liée aux caractéristiques du socle fracturé et non altéré. Une analyse discriminante fournit une " équation de productivité " {{qui permet de}} prévoir 90 % des résultats en cours de foration: dès que le forage a traversé une dizaine de mètres de socle non altéré, elle permet de définir une profondeur limite d'investigation dépendant des caractéristiques intrinsèques de chaque site. Elle est surtout applicable dans les zones les moins productives où l'on observe systématiquement un surcreusement inutile des forages négatifs. On dispose ainsi d'une stratégie de prospection alliant le nombre et la profondeur des forages. Elle permet de limiter la profondeur des forages implantés sur des sites peu productifs et de reporter le métré ainsi récupéré sur des sites plus <b>prometteurs.</b> The {{aim of this study}} is to define a new strategy for groundwater prospection in sahelian basement aquifers. At present, the number and the depth of boreholes are fixed a priori in the project document: these parameters are the same for all the villages, regardless of their environmental context. In fact, during the drilling campaign, we systematically observe a useless overdrilling of negative boreholes that affects the cumulative drilled length of the project (Table 1). This is particularly important in granitic basement areas under the low rainfall sahelian climate: water is difficult to find because of low success rates, and the driller needs to ensure no groundwater indication appears a few meters under the fatal 60 m depth. An illustration of this methodology is proposed for the Guéra, Ouaddaï and Biltine provinces of eastern Chad (Figure 1). This 150 000 km 2 area is situated at the border of the Chadian basin from 10 to 15 ° north latitude at elevations of 400 - 700 m, with an annual rainfall between 200 and 600 mm. The geology is represented by precambrian granitoïds. Tectonics are well developed with many fractures, faults and photolineations from metric to multi-kilometric scales. In Chad, weak recharge rates imply that the weathered rock reservoir is unsaturated and the aquifer is constituted by the fractured granitic basement. Thus, the overall success rate of 500 boreholes is only 42 %. The unequal distribution of fractures leads to the presence of productive and barren adjacent areas with significantly different success rates. A statistical analysis of photolineations shows their fractal distribution with a dimension around 1. 57, similar to the 1. 59 dimension obtained in fractal fracture models (Figure 2). Fracturation is a main component of hydrogeological knowledge in basement areas and its variations between the villages can explain the different potentials of basement productivity: we must consequently adjust an "investigation potential" depending of the characteristics of each area. The proposed strategy of prospection determines the number of boreholes to drill and their specific depth. At the kilometer scale, the total number of boreholes can be adjusted according to climatic, topographical and geological characteristics (Table 2). We show that only four parameters can explain a range of success rate from 0 to 79 % in different villages (Table 3) : altitude, average rainfall, petrography and fracturation intensity (measured in situ). Thus we can define the investigation potential which is the previous depth divided by the theoretical success rate of the area including the village. It is interesting to notice that the success rate in the high productivity class is similar to the average value obtained in more rainy basement countries of West Africa: for example 79 % in south-west of Burkina Faso or 73 % in Togo. At the local scale, a principal components analysis on 12 drilling parameters was performed. It shows that the appearance of groundwater is mainly correlated to parameters describing the unweathered fractured rocks (Figure 3). A discriminant analysis was then performed on four of these parameters: thickness of unweathered drilled basement, depth of first water arrival, number of water arrivals and hammer velocity in the unweathered basement. This yields a "productivity equation" which allows one to anticipate 90 % of the borehole results (Table 4). According to this equation, we can define a maximum investigation depth based on the geological characteristics of each borehole site. The last section presents the complete strategy of groundwater basement prospection and two examples applied to Chad. For an average aquifer depth of 60 m, the investigation potential of each village depends on its productivity class: it varies theoretically from 10 boreholes (i. e. 600 m) in low productivity area to 1. 3 boreholes (i. e. 76 m) in high productivity zones (Table 5). This potential must then be distributed among the different sites according to the result of their productivity equations. The village of Getgéré is situated in a particularly unproductive zone (see Table 3) where about ten boreholes are statistically needed to obtain a positive result: its investigation potential is supposed to be 300 m. Four negative boreholes were drilled from 62 to 75 m with a total depth of 261 m. In fact, the productivity equation showed all these sites were unproductive from drilled depths of 28 to 40 m deep (Table 6) : the same result could have been obtained with only 130 m drilled; 131 m were uselessly consumed. With this excess drilled length, we could have drilled eight extra shallower boreholes and increased the probability of success in obtaining a productive well. The village of Eroua is situated in a productive area where the success rate is 79 %: its investigation potential is 60 / 0. 79 = 76 m. The first borehole was negative at depth of 74 m, but the productivity equation already indicated this result after only 38 m of drilling. At the second site, a positive borehole was obtained at 41 m depth where the equation foresaw 43 m. Finally, the cumulative drilled length was 115 m and the investigation strategy would have permitted the transfer of 34 m to another more promising site...|$|R
40|$|N° d'ordre: DU 2240 EDSF : 714 Ultra-relativistic heavy-ion {{collisions}} aim at {{investigating the}} properties ofstrongly-interacting matter at extreme conditions of temperature and energy density. According to quantum chromodynamics (QCD) calculations, under such conditions, {{the formation of}} a deconfined medium, the Quark-Gluon Plasma (QGP), is expected. Amongst the most important probes of the properties of the QGP, heavy quarks are of particular interest since they are expected to be produced in hard scattering processes during the early stage of the collision and subsequently interact with the hot and dense medium. Therefore, the measurement of quarkonium states and open heavy flavours should provide essential information on the properties of the system formed at the early stage of heavy-ion collisions. Indeed, open heavy flavours are expected {{to be sensitive to the}} energy density through the mechanism of in-medium energy loss of heavy quarks, while quarkonium production should be sensitive to the initial temperature of the system through their dissociation due to color screening. The measurement of the collective flow of heavy flavours provides additional insights on the possible thermalization of heavy quarks in the medium. Furthermore, one of the important medium characteristic, viscosity over entropy (eta/s), can be extracted by combining the information from measured nuclear modification factor (related to in-medium energy loss) and the magnitude of the heavy quark flow. In this regard, both quarkonium and open heavy flavour production are a field of intense experimental and theoretical researches. Despite the work devoted to these studies at SPS and RHIC, several questions are left open. With a nucleus-nucleus center of mass energy nearly 15 times larger than the one reached RHIC, the LHC which started operating in November 2009, provides a new era for studies of interacting matter at high temperature and energy density. One of the most important aspects of this new energy range is the abundant production rate of heavy quarks which are used, for the first time, as high statistics probes of the medium. The LHC delivered the first proton-proton collisions at √s = 0. 9 TeV in October 2009 and reached its current maximum energy of 7 TeV in March 2010. A short proton-proton run at √s = 2. 76 TeV, at the same energy than the Pb-Pb run, was performed in March 2011. The first heavy-ion run (Pb-Pb collisions at √sNN = 2. 76 TeV) took place in November 2010 and the second one end of 2011. ALICE (A Large Ion Collider Experiment) is the experiment dedicated to the study of heavy-ion collisions at the LHC. ALICE also takes part in the LHC proton- proton program which is of great interest for testing perturbative QCD calculations at unprecedented low Bjorken-x values and for providing the necessary baseline for nucleus-nucleus and proton-nucleus collisions. ALICE will also collect, in the beginning of 2013, p-Pb/Pb-p collisions in order to investigate cold nuclear matter effects. ALICE measures quarkonia and open heavy flavours with (di) -electrons, (di) -muons and through the hadronic channels. This thesis work is devoted to the study of open heavy flavours in proton-proton and Pb-Pb collisions via single muons with the ALICE forward muon spectrometer. The document is organized as follows. The first chapter consists in a general introduction on heavy-ion collisions and QCD phase transitions. Chapter 2 summarizes the motivations for the study of open heavy flavours in nucleon-nucleon, nucleon-nucleus and nucleus-nucleus collisions. A particular emphasis is placed on the novelties at the LHC. Chapter 3 gives an overview of the ALICE experiment with a detailed description of the forward muon spectrometer. Chapter 4 gives a short summary of the ALICE online and offline systems. Then the analysis framework (for data and simulations) and in particular the software developed for the study of open heavy flavours is detailed. Chapter 5 summarizes the performance of the ALICE muon spectrometer for the study of the production of open heavy flavours in pp collisions via single muons and dimuons. Chapters 6 to 9 are dedicated to data analysis. Chapter 6 deals with the analysis of first pp collisions at 900 GeV. The main aim was the understanding of the response of the apparatus. These data allowed also to determine the analysis strategy for heavy flavour measurement in the single muon channel: selection of events, optimization of cuts, understanding of the background components in data. Chapter 7 presents the measurement of the production of heavy flavour decay muons in pp collisions at √s = 7 TeV. The analysis strategy is described: event and track selection, background subtraction (mainly the contribution of muons from primary pion and kaon decays), corrections, normalization and investigation of the systematic uncertainties. The experimental results are discussed and compared to perturbative QCD calculations (Fixed Order Next-to-Leading Log calculations). That concerns the transverse momentum and rapidity differential production cross sections of muons from heavy flavours decays at forward rapidity (2. 5 < y < 4). Chapter 8 addresses the measurement of heavy flavour decay muon production in Pb-Pb collisions at √sNN = 2. 76 TeV collected in 2010. The analysis strategy is presented. In-medium effects are investigated by means of the nuclear modification factor (RAA) of muons from heavy flavour decays. The proton-proton reference is obtained from the measurement of the differential production cross section of heavy flavour decay muons at the same center of mass energy. The nuclear modification factor is studied as a function transverse momentum (pt) and collision centrality. For comparison, results obtained with the central-to-peripheral nuclear modification factor RCP are also discussed. Chapter 9 gives an overview of the different methods investigated in ALICE for the study of the elliptic flow. In particular, the recent methods which allow to remove non-flow effects like the Q-Cumulants and Lee-Yang Zeroes are detailed. Promising results concerning the inclusive muon elliptic flow as a function of pt and centrality obtained with different flow analysis methods are compared. Finally, summary and outlooks are given. Les collisions d'ions lourds ultra-relativistes ont pour objectif principal l'étude des propriétés de la matière nucléaire soumise à des conditions extrêmes et de température de densité d'énergie. Les calculs de la ChromoDynamique Quantique (QCD) prédisent dans ces conditions une nouvelle phase de la matière dans laquelle on assisterait au déconfinement des constituants des hadrons en un plasma de quarks et gluons (QGP). Les saveurs lourdes (charme et beauté) sont produites lors de processus durs aux premieres instants de la collision puis traversent le milieu produit durant la collision. Par conséquent, la mesure des quarkonia et des saveurs lourdes ouvertes devrait être particulièrement intéressante pour l'étude des propriétés du système créé aux premiers instants de la collision. On s'attend à ce que les saveurs lourdes ouvertes présentent des sensibilités à la densité d'énergie via les mécanismes de perte d'énergie des quarks lourds dans le milieu et que les quarkonia soient sensibles à la température initiale du système via leur dissociation par écrantage de couleur. La mesure du flot des saveurs lourdes devrait apporter des informations concernant le degré de thermalisation des quarks lourds dans le milieu nucléaire. De plus, l'observable viscosité/entropie pourrait être obtenue en combinant les mesures du facteur de modification nucléaire et de flot. En conséquence, l'étude de la production des quqrkonia et saveurs lourdes ouvertes est un domaine de recherche intensément étudié au niveau experimental et théorique. Les mesures effectuées au SPS et RHIC ont permis de mettre en évidence plusieurs caractéristiques du milieu produit mais ont aussi laissé plusieurs questions sans réponse. Avec une énergie par paire de nucléon de 15 fois supérieure à celle du RHIC, le LHC entré en fonctionnement fin 2009, a ouvert une nouvelle ère pour l'étude des propriétés du QGP. Un des plus importants aspects de ce domaine en énergie est l'abondante production de quarks lourds utilisés pour la première fois comme sonde de haute statistique du milieu. Le LHC délivra les premières collisions pp à √s = 0. 9 TeV en octobre 2009 et a atteint l'énergie de √s = 7 TeV en mars 2010. Un run pp à √s = 2. 76 TeV a eu lieu en mars 2011 pendant une durée limitée. Les runs Pb-Pb à √sNN = 2. 76 TeV ont eu lieu fin 2010 et 2011. ALICE (A Large Ion Collider Experiment) est l'expérience dédiée à l'étude des collisions d'ions lourds au LHC. ALICE enregiste aussi des collisions pp afin de tester les calculs perturbatifs de QCD dans la région des faibles valeurs de x-Bjorken et de fournir la référence indispensable pour l'étude des collisions noyau-noyau et p-noyau. ALICE enregistrera aussi, début 2013, des collisions p-Pb/Pb-p afin d'étudier les effets nucléaires froids. Les quarkonia et saveurs lourdes ouvertes sont mesurés dans ALICE suivant leur mode de désintégration (di) -muonique, (di) -electronique et hadronique. Cette thèse concerne l'étude des saveurs lourdes ouvertes dans les collisions pp et Pb-Pb avec les muons simples mesurés aux rapidités avant avec le spectromètre à muons d'ALICE. Le document est structuré comme suit. Le premier chapitre est une introduction à la physique des collisions d'ions lourds et du diagramme de phase de la matière nucléaire. Le deuxième chapitre présente les objectifs de l'étude des saveurs lourdes ouvertes dans les collisions proton-proton, proton-noyau et noyau-noyau. Un intérêt particulier est porté au domaine en énergie du LHC. Le troisième chapitre est une description du détecteur ALICE et du spectromètre à muons. Le quatrième chapitre présente les systèmes "online" et "offline". Le cinquième chapitre est un résumé des performances du spectromètre à muons pour la mesure des saveurs lourdes ouvertes dans les collisions pp au moyen des muons simples et dimuons. Les chapitres 6 à 9 concernent l'analyse de données. Le sixième chapitre décrit l'analyse des premières collisions pp à √s = 0. 9 TeV collectées avec ALICE. L'objectif principal était la compréhension de la réponse du détecteur. Ces données ont permis aussi fixer la stratégie d'analyse des saveurs lourdes ouvertes : sélection des événements, optimisation des coupures, différentes sources de bruit de fond à soustraire. Le septième chapitre présente la mesure de la section de production des saveurs lourdes ouvertes dans les collisions pp à √s = 7 TeV. La méthode d'analyse est décrite. Cela concerne la sélection des collisions et traces reconstruites dans le spectromètre à muons, la soustraction du bruit de fond (composé principalement de muons issus de la désintégration de pions et kaons primaires), les corrections, la normalisation et la détermination des incertitudes systématiques. Les résultats expérimentaux sont discutés et comparés aux calculs perturbatifs QCD (calculs "Fixed Order Next-to-Leading Log"). Cela concerne les sections efficaces de production des muons issus de la désintégration des saveurs lourdes ouvertes aux rapidités avant (2. 5 < y < 4) en fonction de la rapidité et de l'impulsion transverse (pt). Le huitième chapitre aborde la mesure des muons issus de la désintégration des saveurs lourdes ouvertes dans les collisions Pb-Pb à √sNN = 2. 76 TeV collectées en 2010. Les effets de milieu nucléaire sont étudiés à partir du facteur de modification nucléaire RAA. La référence pp est déterminée à partir de l'analyse des collisions pp à √s = 2. 76 TeV. Le facteur de facteur modification nucléaire est étudié en fonction de pt et de la centralité de la collision. Pour comparaison, les résultats obtenus à partir de la mesure du facteur de modification nucléaire central sur périphérique (RCP) sont aussi présentés. Le neuvième chapitre commence par une revue des différentes méthodes utilisées pour la mesure de la composante de flot elliptique. Les méthodes telles que les cumulants et Lee-Yang Zeroes, permettant de supprimer les effets non-flot, sont détaillées. Des premiers résultats <b>prometteurs</b> concernant la mesure de la composante de flot elliptique des muons sont discutés. Ils sont obtenus avec différentes méthodes et présentés en fonction de pt et de la centralité de la collision. Le manuscrit se termine par une conclusion et des perspectives...|$|R
40|$|Research in {{multimedia}} {{for consumer}} electronics {{is dominated by}} the problem of incredibly short times-to-market, that means fast complexity estimations and fast design of new architectures. On one side more and more sophisticated and flexible applications are rapidly developed, on the other side the exponential growth in IC computational power seems to be hardly capable to keep pace with requirements for real-time applications, since their complexity is exponentially growing as well. The processor's performance, often slowed down by bottlenecks in memories and buses, is further reduced by the time wasted in communication among the several application layers. From this problem comes the conception of integrated development frameworks for simulation and design. Tools for simulation and analysis of architectures have appeared from academic and industrial research laboratories above all in the last decade. Many of them are conceived to provide low-level exact simulation of the supported devices, at the price of heavy slowdowns in simulation times and huge sizes of traced data reports. Some others, in the last years, introduced some degrees of approximation in simulations, in order to speed up execution time and to increase the flexibility of the tools to support multi-processors. The resulting more or less abstract models are anyway not suitable to analyze real multimedia-oriented applications, where programs are usually available in some languages including dedicated libraries and meaningful results are only those measured in function of time. On another level, tools for hardware/software codesign or for block-based system design provide more useful results, but to be effective they must rely on rigid cores that may allow only a few degrees of reconfigurability; some very recent tools are conceived to design complex systems by modeling blocks through a high level description language. Conversely, the virtual model and related tool proposed in this dissertation have their roots in an approach to the analysis of complexity that aims to be, as far as possible, platform independent. The method is based on the concepts of abstract classes of operations and simulation in function of the performance time. The work described in this dissertation finds its application field in the world of multimedia, more precisely in multimedia-oriented Audio applications. Media applications are commonly programmed by imperative or object-oriented languages, which are composed by many different statements, operators and above all standard libraries. A careful profiling of typical applications permits to detect fundamental operations and functions and to define a virtual instruction set, grouping more or less similar operators and breaking functions into basic building blocks. The resulting virtual instruction set does not correspond to any actual one but it has as property to be easily mapped on a large number of existing ones. The simulation of an architecture requires then the availability of measures, benchmarks or estimations of at least one member of each abstract class. The number of classes, i. e. the complexity vector, can be adapted in length and detail to the needed degree of precision and to the available set of actual measures and/or benchmarks. The input to the simulation is described by a high-level standardized programming language, the new MPEG- 4 Structured Audio Orchestra Language (SAOL); in principle, it may also be the case that the application does not need any translation, if it is already available in this format. Moreover, simulation through a high-level language permits to trace the behavior of the target architecture in function of the internal time of the application itself, result that is fundamental when the related workload is highly variable as in downloadable and/or interactive scenarios. In such cases complexity has always been considered a guess. The new virtual model for analyses of complexity led to two main practical results: the proposed method of simulation has been used to define complexity levels for Structured Audio in the MPEG- 4 Standard, and consequently the platform independent analyzer has become the MPEG- 4 reference software for MPEG- 4 SA Conformance test. a virtual instruction set has been conceived that has permitted the implementation of an efficient Structured Audio decoder, SAINT, based on a virtual interpreted DSP core. The flexibility of the SAINT virtual DSP approach has permitted a fast porting of its execution engine on a superscalar VLIW DSP, {{making it one of the}} building blocks of the ThreeDSPACE system, a framework for advanced rendering of 3 -D Audio scenes based on MPEG- 4 descriptions. The complete tool for simulation of architectures, including a cache simulator, has shown promising results, achieving estimations of the execution time of SAINT with an approximation of the 10 % in the mean for a general purpose processor, and of the 20 % for a very complex superscalar VLIW processor. Estimated programs have sometimes dynamic excursions of a factor 10 in their complexity along the time axis. In general, the experimental results can be considered in line with those of the most recent, state-of-the-art simulators presented in literature. A limitation of the language adopted to model the applications is that its generality is limited to onedimensional floating-point computation; the most relevant advantage is that simulation in function of the performance time is straightforward and provides the time-dependent results that are fundamental for the optimization of real-time applications. With the tool developed in this PhD work, complex programs can be quickly modeled thanks to SA specific libraries and also quickly analyzed; moreover, the tool can be easily extended to become a tool for automatic generation and configuration of the main building blocks of a system running the application, or the class of applications, under consideration. The proposed model is finally intended as an alternative approach to coordinate two sides; a principal goal of this work was to conceive and specify a systematic analysis method that can be useful to both the software programmer and to the hardware system engineer. The former can benefit of a reconfigurable and dedicated highlevel software tool able to profile programs in a simple and platform independent manner and to easily simulate, with some margins of error, the behavior of a specific platform, existing or virtual; the latter can exploit complexity estimations in an abstract format, with the possibility to study the target application in its several aspects (operations, memory usage, data flows among the different program blocks) and the potentiality to extend these results to an automatic generation of high-level system architectures. La recherche dans le multimédia pour l'électronique de consommation est dominée par le problème du temps de marché extrêmement court, ce qui signifie des estimations rapides de complexité et des conceptions rapides d'architectures. D'une part des applications de plus en plus sophistiquées et flexibles sont rapidement développées; d'autre part la croissance exponentielle dans la puissance de calcul des circuits intégrés semble être à peine capable de suivre les conditions requises par les applications temps réel, car leur complexité croît aussi de façon exponentielle. La performance d'un processeur, souvent réduite par la vitesse limitée des mémoires et des bus, est ultérieurement diminuée par le temps perdu dans la communication entre les différents niveaux de l'application. De ce problème, naît la conception de cadres de développement intégrés pour l'intégration et la simulation. Les outils pour la simulation et l'analyse d'architectures sont diffusés par des laboratoires de recherche académiques et industriels surtout dans cette dernière décennie. La plus part d'entre eux sont conçus pour obtenir une simulation exacte de bas niveau des dispositifs supportés, au prix de forts ralentissements dans le temps de simulation et d'énormes tailles des fichiers de données à la sortie. Quelques autres, dans les dernières années, ont introduit des degrés d'approximation dans les simulations, pour accélérer le temps d'exécution et pour augmenter la flexibilité des outils afin de supporter les multiprocesseurs. Les modèles résultants, plus ou moins abstraits, ne sont de toute façon pas aptes à l'analyse des vraies applications orientées au multimédia, où les programmes sont habituellement disponibles dans un langage qui inclut des librairies dédiées et les résultats significatifs sont seulement ceux mesurés en fonction du temps. A un autre niveau, les outils pour le hardware/software codesign ou pour la conception de systèmes basé sur des blocs donnent des résultats plus utiles, mais pour être efficaces ils doivent s'appuyer sur des noyaux rigides qui ne permettent que peu de reconfiguration; des outils très récents sont conçus pour projeter des systèmes complexes en manipulant des blocs à travers des langages de description de haut niveau. Le modèle virtuel et son outil relatif proposés dans ce rapport s'inspirent au contraire d'une approche à l'analyse de la complexité qui veut être le plus possible indépendant de la plate forme. La méthode est basée sur les concepts de classes d'opérations abstraites et de simulation en fonction du temps de performance. Le travail décrit dans ce rapport trouve son champ d'application dans le monde du multimédia, plus précisément dans les applications audio orientées multimédia. Les applications multimédia sont communément programmées par des langages impératifs ou orientés objets qui sont composés par plusieurs différents instructions, opérateurs et surtout librairies standard. Une analyse de profil attentive des applications typiques permet de détecter les opérations et les fonctions fondamentales et de définir un jeu d'instructions virtuel, en regroupant les opérateurs plus ou moins similaires et en subdivisant les fonctions en blocs de base qui les constituent. Le résultant jeu d'instruction virtuel ne correspond à aucun autre réel; au contraire il a comme propriété d'être facilement converti en un grand nombre de jeux existants. La simulation d'une architecture requiert donc la disponibilité de mesures ou d'estimations d'au moins un membre de chaque classe abstraite. Le nombre de classes, c'est-à-dire le vecteur de complexité, peut être adapté en longueur et en détail au degré nécessaire de précision et à la quantité disponible de mesures. L'entrée de la simulation est décrite par un langage de programmation standard de haut niveau : le nouveau MPEG- 4 Structured Audio Orchestra Language (SAOL); en principe, il n'est pas exclu qu'aucune traduction soit nécessaire pour l'application, si celle-ci est déjà disponible dans ce format. En plus, la simulation à travers un langage de haut niveau permet de tracer le comportement de l'architecture simulée en fonction du temps de l'application même, résultat qui est fondamental quand sa charge de travail est fortement variable comme dans des scénarios interactifs. Dans ces cas la complexité a toujours été considérée comme une chose à deviner. Le nouveau modèle virtuel pour l'analyse de la complexité a amené deux résultats pratiques principaux : la méthode de simulation proposée a été utilisée pour définir les niveaux de complexité pour l'outil « Structured Audio » de la norme MPEG- 4, et par conséquent l'analyseur, indépendant de la plate forme, est devenu le logiciel de référence pour le test de Conformance de MPEG- 4 SA. un jeu d'instruction virtuel a été conçu; il a permis l'implémentation d'un décodeur efficace pour SA : SAINT, basé sur un noyau virtuel de DSP interprété. La flexibilité du DSP virtuel de SAINT a permis un transfert rapide de son moteur d'exécution vers un DSP superscalaire VLIW, ce qui a fait de lui un des blocs composants de ThreeDSPACE, système pour la diffusion avancée de scènes Audio 3 -D basées sur des descriptions MPEG- 4. L'outil complet pour la simulation d'architecture, qui contient un simulateur de mémoire cache, a montré des résultats <b>prometteurs,</b> obtenant des estimations du temps d'exécution de SAINT avec une approximation de 10 % en moyenne pour des processeurs general purpose et de 20 % pour un processeur superscalaire VLIW très complexe. Les programmes estimés ont parfois des variations d'un facteur 10 dans leur complexité le long de l'axe du temps. En général, les résultats expérimentaux peuvent être considérés comparables à ceux des plus récents simulateurs décrits dans la littérature. Une limite du langage utilisé pour la modélisation des applications est que sa généralité est limitée à des calculs à une dimension en virgule flottante; le plus grand avantage est que la simulation en fonction du temps de la performance est très facile et donne des résultats dépendants du temps qui sont fondamentaux pour l'optimisation des applications en temps-réel. Avec l'outil développé dans ce travail de doctorat des programmes complexes peuvent être modélisés rapidement grâce aux librairies spécifiques de SA et aussi analysés rapidement; de plus, l'outil peut être facilement étendu pour devenir un outil de génération automatique et de configuration des blocs de base principaux du système qui réalise les applications considérées. En conclusion, le modèle proposé est pensé comme une approche alternative pour coordonner deux côtés : un des objectifs principaux de ce travail à été de concevoir une méthode d'analyse systématique qui puisse être utile en même temps au programmateur de logiciels et à l'ingénieur des systèmes hardware. Le premier peut bénéficier d'un outil de haut niveau reconfigurable et spécifique capable de analyser le profil des programmes de façon simple et indépendante de la plate forme et de simuler facilement, avec des marges d'erreur, le comportement d'une plate forme existante ou virtuelle; le deuxième peut exploiter des estimations de complexité en forme abstraite, avec la possibilité d'étudier les applications envisagées dans ses divers aspects (opérations, utilisation de la mémoire, flux de données entre les différents blocs du programme) et avec la potentialité d'étendre ces résultats vers la génération automatique d'architectures de système...|$|R
40|$| efficacité des {{patients}} dans la gestion de leurs symptômes, l'intensité des symptômes et les besoins en soins de support au cours du traitement de première ligne. L'étape de développement et implantation du rôle de l'IPACaP s'est appuyée sur les sept premières phases décrites dans le cadre conceptuel « Participatory, Evidence-informed, Patient- centred Process for Advanced practice nursing rôle development, {{implementation and}} évaluation » (PEPPA). Deux focus groups auprès de l'équipe multidisciplinaire (n = 5 infirmières, n = 6 médecins) ainsi que des entretiens semi-structurés auprès de patients atteints d'un cancer du poumon (n = 4) et auprès de l'IPACaP, ont été menés dans le but d'explorer l'acceptabilité du rôle. Concernant les autres objectifs, les patients atteints d'un cancer du poumon recevant un traitement de première ligne, avec ou sans radiothérapie, ont été recrutés selon la méthode d'échantillonnage non probabiliste accidentelle. La faisabilité des consultations menées par l'IPACaP et la capacité à recueillir les PROMs a été évaluée par une étude de phase II à bras unique : l'étude répondait au critère de faisabilité si au moins 55 % des patients participaient aux quatre consultations menées par l'IPACaP et complétaient les PROMs aux trois étapes de la récolte des données, en baseline, à T 1 (jours : 4 à 50) et à T 2 (jours 71 à 95). Finalement, des statistiques descriptives et des modèles à effets mixtes ont été utilisés pour explorer la contribution l'IPACaP envers le sentiment d'auto-efficacité des patients dans la gestion de leurs symptômes, l'intensité des symptômes et les besoins en soins de support insatisfaits. En suivant les sept premières phases du cadre PEPPA, le rôle de l'IPACaP a été conçu sur la base d'un consensus d'opinions avec les acteurs clés du Centre des tumeurs thoracique du CHUV. En considérant les failles du modèle de soins précédemment établi, le rôle de l'IPACaP a été développé avec l'objectif d'améliorer la satisfaction des patients en regard de leur besoins en soins de support. L'intervention de l'IPACaP au sein du Centre des tumeurs thoraciques du CHUV incluait deux consultations face à face alternées avec deux consultations téléphoniques auprès des patients atteints d'un cancer du poumon, au cours de leur première ligne de traitement. Le rôle de l'IPACaP comprenait : i) le soutien psychologique, ii) l'éducation thérapeutique à l'autogestion des symptômes, ainsi que iii) l'information liée à la maladie et aux traitements. Le rôle de l'IPACaP impliquait le partage d'informations concernant les patients avec les autres acteurs de santé, afin de renforcer la collaboration et assurer la continuité des soins. L'analyse des contenus d'entretiens a mis en évidence trois thèmes décrivant l'acceptabilité de l'IPACaP : «l'identification du rôle », « la contribution spécifique du rôle » et « la flexibilité du rôle ». Si les médecins et les patients identifiaient clairement l'étendue de pratique de l'IPACaP, les infirmières d'oncologie percevaient un chevauchement avec leur propre rôle et craignaient de perdre une partie de leur rôle initial. Autant les professionnels de la santé que les patients ont mis en évidence que l'IPACaP jouait un rôle fondamental dans le soutien psychosocial et dans la continuité des soins et permettait aux patients d'acquérir des capacités dans la gestion des symptômes. La flexibilité du rôle de l'IPACaP était perçue comme une force par les patients et les professionnels de la santé, mais répondre à cette flexibilité était exprimé par l'IPACaP comme un défi organisationnel. Parmi les 46 patients recrutés dans l'étude de faisabilité, 35 ont rempli les critères de faisabilité en participant aux quatre consultations programmées avec l'IPACaP (76 %; IC 95 % : 0, 61 - 0, 87) et 26 patients (56 %; IC 95 % : 0, 41 - 0, 71) ont complété les PROMs aux trois temporalités de la collecte des données. Ces résultats indiquent une perspective prometteuse concernant cet objectif. Toutefois, le recrutement des patients à l'étude a dû être interrompu avant d'atteindre la taille d'échantillon théorique fixée préalablement à n = 71, en regard de ressources et d'un effectif limité au moment de l'étude. Les analyses longitudinales ont montré que malgré les patients décrivaient une tendance à l'amélioration du sentiment d'auto- efficacité dans la gestion de leurs symptômes entre la baseline et T 1, avec une stabilisation entre T 1 et T 2, l'intensité des symptômes augmentait dans le temps. Comparé à la baseline, les besoins en soins de support spécifique à l'information étaient significativement diminués à T 2 (RC = 0, 15; IC 95 % : 0, 03 - 0, 68, p< 0, 01), alors que les besoins reliés au domaine de la sexualité étaient significativement augmentés à T 1 (RC = 4, 04; IC 95 % : 1, 13 - 14, 53, p< 0, 03). Pour la première fois en Suisse, cette thèse de doctorat décrit le processus de développement du rôle d'IPACaP au sein d'une équipe multidisciplinaire d'un centre des tumeurs thoraciques. Les résultats de cette thèse soulignent l'applicabilité et l'utilité du cadre conceptuel PEPPA en tant que guide structuré pour développer et implémenter un nouveau rôle d'IPACaP dans le contexte du système de santé Suisse, alors que l'IPA est à une phase initiale de développement. L'IPACaP apparaît bien acceptée par les patients et par les médecins. Des barrières à l'implantation de ce rôle pourraient être davantage liées à des facteurs intra-professionnels plutôt qu'interprofessionnels. La tension intra-professionnelle décrite dans cette thèse, soulignent la nécessité de développer un cadre national au niveau Suisse qui vise à décrire, clarifier et réguler les rôles d'IPA afin de soutenir leur progression dans le pays et à favoriser leur acceptation. Bien que le recrutement ait dû être interrompu avant d'atteindre la taille d'échantillon définie, des résultats <b>prometteurs</b> concernant la faisabilité des consultations de l'IPACaP suggèrent que cette intervention serait appropriée et bénéfique pour les patients. L'implantation du rôle de l'IPACaP au sein d'une équipe multidisciplinaire pourrait contribuer à améliorer et maintenir le sentiment d'auto-efficacité des patients dans la gestion de leurs symptômes et diminuer les besoins en soins de support. Afin de soutenir les patients atteints d'un cancer du poumon dans la gestion de leurs symptômes et favoriser la diminution de leur intensité, les résultats de cette recherche suggèrent deux ajustements au rôle de l'IPACaP : l'augmentation de la fréquence et l'intensité des consultations par l'IPACaP et le renforcement de l'approche interdisciplinaire qui comprendrait notamment des consultations en binôme (IPACaP - médecin) auprès des patients complexes. Dans le contexte actuel où le système de santé est contraint par des ressources limitées et face à l'augmentation croissante des rôles d'IPA en Suisse, les résultats de cette thèse de doctorat recommandent de mener d'autres recherches qui visent à mesurer l'impact clinique des rôles d'IPA en oncologie et auprès d'autres populations atteintes de maladies chroniques...|$|R

