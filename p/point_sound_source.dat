13|5962|Public
25|$|Bending wave {{transducers}} use {{a diaphragm}} that is intentionally flexible. The rigidity {{of the material}} increases from the center to the outside. Short wavelengths radiate primarily from the inner area, while longer waves reach {{the edge of the}} speaker. To prevent reflections from the outside back into the center, long waves are absorbed by a surrounding damper. Such transducers can cover a wide frequency range (80Hz to 35,000Hz) and have been promoted as being close to an ideal <b>point</b> <b>sound</b> <b>source.</b> This uncommon approach is being taken by only a very few manufacturers, in very different arrangements.|$|E
40|$|This diploma thesis solves {{creation}} of a computing model of sound propagation for determination of acoustic pressure level around multiple <b>point</b> <b>sound</b> <b>source</b> with consideration of sound directivity of each source (compresor and fan). Thesis contains neccesary theoretical knowledge for {{creation of}} computing model and basics of sound measuring, theoretical computing model, measurement data of real unit and model configuration for real measured values. Computing code of the model in Visual Basic for MS Excel {{is a part of}} the thesis...|$|E
40|$|AbstractElastic wave on {{seafloor}} {{caused by}} low frequency noise radiated from ship is called ship seismic wave {{which can be}} used to identify ship target. In order to analyze the wave components and the propagating properties of ship seismic wave, the numerical calculation of synthetic seismograms on seafloor aroused by a low frequency <b>point</b> <b>sound</b> <b>source</b> is carried out using a wave number integration technique combined with inverse Fourier transform. According to the numerical example of hard seafloor, the time series of seismic wave on seafloor are mostly composed of interface waves and normal mode waves. Each normal mode wave has a well-defined low cut-off frequency, while the interface wave doesn't have. The frequency dispersion of normal mode wave is obvious when frequency is lower than 100  Hz, while the interface wave is dispersive only in the infra-sound frequency range. The time series of seismic wave is dominated by the interface wave when the source frequency is less than the minimal cut-off frequency of normal mode wave...|$|E
2500|$|In physics, an {{isotropic}} radiator is a <b>point</b> radiation or <b>sound</b> <b>source.</b> [...] At a distance, {{the sun is}} an {{isotropic radiator}} of electromagnetic radiation.|$|R
40|$|Some of the {{problems}} associated with the applications of acoustical devices for the determination of airstream characteristics are considered in this study. The velocity and pressure fields for both <b>point</b> <b>sound</b> <b>sources</b> and finite <b>sound</b> <b>sources</b> in both subsonic and supersonic flow are discussed, with a view toward using sound waves for the determination of velocity, Mach number, temperature, and other properties of a flow. The experimental investigations included the measurement of the spectra of ultra-audio-pressure pulsations, both static and total, in the small C. I. T. supersonic tunnel and also in the C. I. T. hypersonic tunnel. A broad range of Mach numbers and a variety of operating conditions were covered. The development of a modified Hartmann sound generator is described, and measurements of the sound field from this device in supersonic flow are included...|$|R
50|$|In physics, an {{isotropic}} radiator is a <b>point</b> radiation or <b>sound</b> <b>source.</b> At a distance, {{the sun is}} an {{isotropic radiator}} of electromagnetic radiation. The Big Bang {{is another example of}} an isotropic radiator - the Cosmic Microwave Background.|$|R
40|$|In this paper, we {{employ the}} {{addition}} theorem and superposition technique {{to solve the}} scattering problem with multiple circular cylinders arising from a <b>point</b> <b>sound</b> <b>source.</b> Using the superposition technique, the problem can be decomposed into two individual parts. One is the free-space fundamental solution. The other is a typical boundary value problem (BVP) with boundary conditions derived from the addition theorem by translating the fundamental solution. Following the success of null-field boundary integral formulation to solve the typical BVP of the Helmholtz equation with Fourier boundary densities, the second-part solution can be easily obtained after collocating the observation point exactly on the real boundary and matching the boundary condition. The total solution is obtained by superimposing the two parts which are the fundamental solution and the semi-analytical solution of the Helmholtz problem. An example was demonstrated to validate of the present approach. The parameters of size and spacing between cylinders are considered. The results are well compared with the available theoretical solutions and experimental data...|$|E
40|$|In the Environmental Quality Standards for Noise in Japan (EQS), {{the problem}} of {{environmental}} noise at areas facing roads is evaluated by obtaining the number and the rates of buildings at which noise levels exceed the standard value. The Standards allow for the estimation of noise levels, instead of requiring actual measurements, in cases where taking actual measurements would be difficult. In order to estimate noise levels，to grasp insertion loss of buildings in an evaluated area is needed. As the above background, the authors proposed in the previous study a method to predict insertion loss of detached houses against road traffic noise based on a <b>point</b> <b>sound</b> <b>source</b> model named as F 2012 when the heights of houses and receiving point are 10 m and 1. 2 m, respectively. This paper aims to expand it applicable to different heights of houses and receiving point to apply F 2012 to the evaluation of EQS in residential areas...|$|E
40|$|This paper {{describes}} {{new equipment}} for measuring head-related transfer functions (HRTFs) near a listener’s head. 3 D sounds in headphones are {{generated by the}} convolution of sound sig-nals and an HRTF, which {{is defined as the}} acoustical trans-fer function between a <b>point</b> <b>sound</b> <b>source</b> and the entrance to the ear canal. A loudspeaker is usually used for HRTF measurements, and a distance of more than 1 m separates the loudspeaker and the subject. The region within 1 m of the head is called the ’proximal region, ’ where a small loud-speaker is needed for accurately measuring HRTF, that is, a conventional loudspeaker cannot be used. In our study, a micro-dodecahedral loudspeaker with twelve piezoelectric ceramic devices is used for HRTF measurements at a diame-ter is 38 mm. Our experiments examined the characteristics of this loudspeaker. From the results, our developed loudspeaker provides similar performance to a point source, and it is very effective for measuring the HRTFs in the proximal region. 1...|$|E
40|$|Results of {{experimental}} {{measurements of the}} sound fields of 1 / 4 -scale general aviation propellers are presented and experimental wake surveys and pressure signatures obtained are compared with theoretical predictions. Experiments were performed primarily on a 1 C 160 propeller model mounted {{in front of a}} symmetric body in an anechoic wind tunnel, and measured the thrust and torque produced by propeller at different rotation speeds and tunnel velocities, wakes at three axial distances, and sound pressure at various azimuths and tip speeds with advance ratio or tunnel velocity constant. Aerodynamic calculations of blade loading were performed using airfoil section characteristics and a modified strip analysis procedure. The propeller was then modeled as an array of <b>point</b> <b>sound</b> <b>sources</b> with each <b>point</b> characterized by the force and volume of the corresponding propeller section in order to obtain the acoustic characteristics. Measurements are found to agree with predictions over a wide range of operating conditions, tip speeds and propeller nacelle combinations, without the use of adjustable constants...|$|R
3000|$|Preis et al. have <b>pointed</b> {{out that}} <b>sound</b> <b>source</b> {{recognition}} influences differences in annoyance ratings between bus recordings and [...] "bus-like" [...] noises, which were generated from white noise to have spectral and temporal characteristics {{similar to those}} of original bus sounds [21]. Similarly, {{in the case of the}} present paper, good recognition of <b>sound</b> <b>sources</b> may be the reason why the emotional impressions of the major onomatopoeic stimuli were similar to those for the real sound stimuli.|$|R
40|$|We present {{experimental}} data comparing the accuracy obtained for 2 D and 3 D scanning surfaces using CLEAN-SC deconvolution of beamformed acoustic maps. A spherical array {{is used to}} obtain recordings from a dense <b>point</b> cloud of <b>sound</b> <b>source</b> locations. Beamform-ing and CLEAN-SC acoustic maps are generated using traditional 2 D scanning surfaces and 3 D scanning surfaces corresponding to the surface geometry of an object being acous-tically imaged. Results for the 3 D method show improved accuracy of measured positions and magnitudes of <b>sound</b> <b>sources</b> under a range of circumstances. The most benefit, in regard to position error, is for frequencies above 5 kHz and <b>sound</b> <b>sources</b> located less than a metre from the array. In these circumstances, the three-dimensionality is more dominant. ...|$|R
40|$|In this paper, {{we present}} a system for exhibiting a Chinese land-scape {{painting}} about 900 years old. There are three parts in our system: (1) we allocate a voice dubbing or background music, which is treated as a <b>point</b> <b>sound</b> <b>source,</b> onto the 2 D painting and obtain {{its position in the}} 2 D space. All of the audio data are then located in a 3 D hidden space, by projecting their 2 D positions to the 3 D space through a projection model. (2) A two-layer directed graph structure is proposed to well organize the audio data in a 4 D space (with 1 D temporal and 3 D spatial). (3) The exhibition is defined as an active exploration in a view-point space, which faces both the image and the 3 D world where the sound sources reside. The 3 D space and the two-layer graph structure generate a natural and meaningful stereo audio field. Meanwhile, compared to videos with guided walk through, the active exploration makes the exhibition more attractive...|$|E
40|$|Whether a solid {{trailing}} edge {{is required to}} produce efficient coupling between sound and instability waves in a shear layer was investigated. The differences {{found in the literature}} on the theoretical notions about receptivity, and a need to resolve them by way of well-planned experiments are discussed. Instability waves in the shear layer of a subsonic jet, excited by a <b>point</b> <b>sound</b> <b>source</b> located external to the jet, were first visualized using an ensemble averaging technique. Various means were adopted to shield the sound reaching the nozzle lip. It was found that the low frequency sound couples more efficiently at distances downstream of the nozzle. To substantiate the findings further, a supersonic screeching jet was tested such that it passed through a small opening in a baffle placed parallel to the exit plane. The measured feedback or screech frequencies and also the excited flow disturbances changed drastically on traversing the baffle axially thus providing a strong indication that a {{trailing edge}} is not necessary for efficient coupling between sound and flow...|$|E
40|$|When a {{free jet}} (or open jet) {{is used as}} a wind tunnel to {{simulate}} the effects of flight on model noise sources, it is necessary to calibrate out the effects of the free jet shear layer on the transmitted sound, since the shear layer is absent in the real flight case. In this paper, a theoretical calibration procedure for this purpose is first summarized; following this, the results of an experimental program, designed to test the validity of the various components of the calibration procedure, are described. The experiments are conducted by using a <b>point</b> <b>sound</b> <b>source</b> located at various axial positions within the free jet potential core. By using broadband excitation and cross-correlation methods, the angle changes associated with ray paths across the shear layer are first established. Measurements are then made simultaneously inside and outside the free jet along the proper ray paths to determine the amplitude changes across the shear layer. It is shown that both the angle and amplitude changes can be predicted accurately by theory. It is also found that internal reflection at the shear layer is significant only for large ray angles in the forward quadrant where total internal reflection occurs. Finally, the effects of sound absorption and scattering by the shear layer turbulence are also examined experimentally...|$|E
30|$|The {{direction}} of the <b>sound</b> <b>source</b> can be estimated using the sound localization method described above. With one stationary microphone array, {{it is hard to}} estimate the <b>sound</b> <b>source</b> position. However, the home service robot can move around, which makes it possible to use triangulation to localize the <b>sound</b> <b>source.</b> Figure  4 shows an example of using triangulation to estimate the positions of two <b>sound</b> <b>sources.</b> If the robot can measure the sound direction at two different positions on the 2 D map, the sound position can be estimated by calculating the intersection of two lines <b>pointing</b> to the <b>sound</b> <b>sources</b> from the robot positions. This method may create a undesired intersection point like point P as shown in Fig.  4. However, this point moves when the robot measures at another position. Therefore, it can be eliminated given the assumption that the <b>sound</b> <b>sources</b> are stationary. With multiple steps, the robot can improve the accuracy of position estimation using the RANdom SAmple Consensus (RANSAC) algorithm [29]. The <b>sound</b> <b>source</b> position estimation algorithm is shown in Algorithm 1.|$|R
30|$|To capture platform-top acoustics, we then covered points {{between the}} {{centrally}} located source point (S 1) and receivers within the plaza top, including a point {{halfway to the}} northern wall (R 1), a point at the northern wall (R 2), and {{a point in the}} northeast corner (R 3). We then moved the sound receiver off the platform top, to central points {{near the edge of the}} lower surrounding platform, on the southern side, facing the middle of the staircase (R 4), and on the eastern side (R 5). Our final receiver <b>point</b> from the central <b>sound</b> <b>source</b> was on the plaza floor, several meters beyond the low-platform receiver point (R 6).|$|R
40|$|This thesis first {{presents}} a novel object-oriented scheme which provides for ex- tensive description of time-varying 3 D audio scenes using XML. The scheme, named XML 3 DAUDIO, provides a new format for encoding and describing 3 D audio scenes in an object oriented manner. Its creation {{was motivated by}} the fact that other 3 D audio scene description formats are either too simplistic (VRML) and lacking in realism, or are too complex (MPEG- 4 Advanced AudioBIFS) and, as a result, have not yet been fully implemented in available decoders and scene authoring tools. This thesis shows that the scene graph model, used by VRML and MPEG- 4 AudioBIFS, leads to complex and ine±cient 3 D audio scene descriptions. This complexity {{is a result of the}} aggregation, in the scene graph model, of the scene content data and the scene temporal data. The resulting 3 D audio scene descriptions, are in turn, difficult to re-author and signifcantly increase the complexity of 3 D audio scene renderers. In contrast, XML 3 DAUDIO follows a new scene orchestra and score approach which allows the separation of the scene content data from the scene temporal data; this simplifies 3 D audio scene descriptions and allows simpler 3 D audio scene renderer implementations. In addition, the separation of the temporal and content data permits easier modification and re-authoring of 3 D audio scenes. It is shown that XML 3 DAUDIO can be used as a new format for 3 D audio scene rendering or can alternatively be used as a meta-data scheme for annotating 3 D audio content. Rendering and perception of the apparent extent of <b>sound</b> <b>sources</b> in 3 D audio displays is then considered. Although perceptually important, the extent of <b>sound</b> <b>sources</b> is one the least studied auditory percepts and is often neglected in 3 D audio displays. This research aims to improve the realism of rendered 3 D audio scenes by reproducing the multidimensional extent exhibited by some natural <b>sound</b> <b>sources</b> (eg a beach front, a swarm of insects, wind blowing in trees etc). Usually, such broad <b>sound</b> <b>sources</b> are treated as <b>point</b> <b>sound</b> <b>sources</b> in 3 D audio displays, resulting in unrealistic rendered 3 D audio scenes. A technique is introduced whereby, using several uncorrelated <b>sound</b> <b>sources,</b> the apparent extent of a <b>sound</b> <b>source</b> can be controlled in arbitrary ways. A new hypothesis is presented suggesting that, by placing uncorrelated <b>sound</b> <b>sources</b> in particular patterns, <b>sound</b> <b>sources</b> with apparent shapes can be obtained. This hypothesis and the perception of vertical and horizontal <b>sound</b> <b>source</b> extent are then evaluated in several psychoacoustic experiments. Results showed that, using this technique, subjects could perceive the horizontal extent of <b>sound</b> <b>sources</b> with high precision, differentiate horizontally from vertically extended <b>sound</b> <b>sources</b> and could identify the apparent shapes of <b>sound</b> <b>sources</b> above statistical chance. In the latter case, however, the results show identification less than 50 % of the time, and then only when noise signals were used. Some of these psychoacoustic experiments were carried out for the MPEG standardisation body with a view to adding <b>sound</b> <b>source</b> extent description capabilities to the MPEG- 4 AudioBIFS standard; the resulting modifications have become part of the new capabilities in version 3 of AudioBIFS. Lastly, this thesis presents the implementation of a novel real-time 3 D audio rendering system known as CHESS (Configurable Hemispheric Environment for Spatialised Sound). Using a new signal processing architecture and a novel 16 -speaker array, CHESS demonstrates the viability of rendering 3 D audio scenes described with the XML 3 DAUDIO scheme. CHESS implements all 3 D audio signal processing tasks required to render a 3 D audio scene from its textual description; the definition of these techniques and the architecture of CHESS is extensible and can thus be used as a basis model for the implementation of future object oriented 3 D audio rendering systems. Thus, overall, this thesis presents ontributions in three interwoven domains of 3 D audio: 3 D audio scene description, spatial psychoacoustics and 3 D audio scene rendering...|$|R
40|$|The {{objective}} {{of this study is}} to determine whether or not a solid trailing edge is required to produce efficient coupling between sound and instability waves in a shear layer. Instability waves in the shear layer of a subsonic jet, excited externally by a <b>point</b> <b>sound</b> <b>source,</b> were first visualized by using a phase-locked flow visualization technique. Various means were adopted to shield the sound reaching the nozzle lip. It was found that the low frequency sound couples more efficiently at distances downstream of the nozzle exit. To substantiate the findings further, a supersonic screeching jet was tested such that it passed through a small opening in a baffle placed parellel to the exit plane. The measured feedback or screech frequencies, and also the excited flow disturbances, were found to change drastically on traversing the baffle axially. Similar conclusions were reached when a second baffle was placed downstream of the first baffle, and the effects of moving the first baffle on the feedback frequency of sound generated by flow impingement on the second baffle were examined. It is argued that sound waves passing through the mixing layer of a jet can excite flow instability without the benefit of assistance and coupling at the trailing edge...|$|E
40|$|Measuring the {{farfield}} {{noise levels}} of full-scale rotor systems is not trivial {{and can be}} costly. Researchers prefer to perform small-scale experiments in the laboratory {{so that they can}} extrapolate the model scaled results to the larger scale. Typically Inverse Square Law (ISL) is used to extrapolate the sound pressure levels (SPL), obtained from model-scale experiments at relatively small distances to predict noise at much larger distances for larger scale systems. The assumption underlying this extrapolation is that the source itself can be treated as a <b>point</b> <b>sound</b> <b>source.</b> At what distance from a rotor system it can be treated as a point source has never been established. Likewise, many theoretical models of shielding by hard surfaces assume the source to be a point monopole source. If one is interested in shielding the noise of a rotor system by interposing a hard surface between the rotor and the observer, can the rotor system really be considered to be a monopole? If rotating noise sources are under consideration what is the effect of configuration and design parameters? Exploring the validity of point source assumption alluded to above for a rotor for farfield acoustic measurements with and without shielding form the backbone of the present work. Ph. D. Committee Chair: Ahuja, Krishan K.; Committee Member: Gaeta, Richard J.; Committee Member: Jagoda, Jechiel; Committee Member: Sankar, Lakshmi; Committee Member: Whitfield, Charlotte E...|$|E
40|$|Abstract The {{atmospheric}} {{influence on}} sound propagation {{along the ground}} sur-face is a critical issue for estimating the noise impact of industrial plants or road networks. Indeed, sound refraction in the surface layer has a dramatic impact on the geographical acoustic exposure. Many analytical and numerical models and stud-ies based on {{the laws of physics}} are available in scientific papers whereas very few works in statistical analysis have been attempted. However several important practical issues need to be considered. Among these, time and space represen-tativity of “in situ ” measurements, sampling design, influence of meteorological and ground parameters on acoustic exposure show to be a few challenges. They need to be investigated with statistical tools taking into account space and time autocorrelation. A new protocol which includes ground impedance monitoring, spatial micro-meteorological and acoustical characterization has been applied to an experimental campaign from June to August 2005 in a case of sound propagation from a <b>point</b> <b>sound</b> <b>source</b> on a grassy flat ground. The first geostatistical study on such a multi-variable experimental database is presented. It adresses both the issue of modelling space varying impedance properties of an homogeneous meadow and the issue of modelling the acoustic field itself. This latter includes an analysis of the spatial variogram of the acoustic field residual calculated from a basic physical model as an external drift...|$|E
50|$|The pair {{consists}} of an array {{of two of the}} same microphones that have a bi-directional (figure 8) pickup pattern. They are positioned 90° from each other. Ideally, the transducers should occupy the same physical space; since this cannot be achieved, the microphone capsules are placed as close to each other as physically possible, generally with one centered directly above the other. The array is oriented so that the line bisecting the angle between the two microphones <b>points</b> towards the <b>sound</b> <b>source</b> to be recorded (see diagram). The pickup patterns of the pair, combined with their positioning, delivers a high degree of stereo separation in the source signal as well as the room ambiance.|$|R
40|$|In {{order to}} {{reproduce}} nearby <b>sound</b> <b>sources</b> with distant loudspeakers {{to a single}} listener, the near field compensated (NFC) method for higher-order Ambisonics (HOA) has been previously proposed. In practical realization, this method {{requires the use of}} regularization functions. This study analyzes the impact of two existing and a new proposed regularization function on the reproduced sound fields and on the main auditory cue for nearby <b>sound</b> <b>sources</b> outside the median plane, i. e, low-frequencies interaural level differences (ILDs). The proposed regularization function led to a better reproduction of <b>point</b> <b>source</b> <b>sound</b> fields compared to existing regularization functions for NFC-HOA. Measurements in realistic playback environments showed that, for very close sources, significant ILDs for frequencies above about 250 Hz can be reproduced with NFC-HOA and the proposed regularization function whereas the existing regularization functions failed to provide ILDs below 500 Hz. A listening test showed that these lower-frequency ILDs provided by the proposed regularization function lead to a significantly improved distance perception performance. This test also showed that the distance of virtual sources are perceived less accurately than corresponding physical sources when amplitude cues are not available. 13 page(s...|$|R
40|$|Accurate {{localization}} {{of multiple}} <b>sound</b> <b>sources</b> is indispensable for the microphone array-based high quality sound capture. For single <b>sound</b> <b>source</b> localization, the CSP (Cross-power Spectrum Phase analysis) method has been proposed. The CSP method localizes a <b>sound</b> <b>source</b> as a crossing <b>point</b> of <b>sound</b> directions estimated using di erent microphone pairs. However, when localizing multiple <b>sound</b> <b>sources,</b> the CSP method {{has a problem}} that the localization accuracy is degraded due to cross-correlation among di erent <b>sound</b> <b>sources.</b> To solve this problem, this paper proposes a new method which suppresses the undesired cross-correlation by synchronous addition of CSP coe cients derived from multiple microphone pairs. Experiment results in a real room showed that the proposed method improves the localization accuracy when {{increasing the number of}} the synchronous addition. 1...|$|R
40|$|ICASSP 2000 : IEEE International Conference on Acoustics, Speech, and Signal Processing, June 5 - 9, 2000, Istanbul, Turkey. Accurate {{localization}} {{of multiple}} <b>sound</b> <b>sources</b> is indispensable for the microphone array-based high quality sound capture. For single <b>sound</b> <b>source</b> localization, the CSP (cross-power spectrum phase analysis) method has been proposed. The CSP method localizes a <b>sound</b> <b>source</b> as a crossing <b>point</b> of <b>sound</b> directions estimated using different microphone pairs. However, when localizing multiple <b>sound</b> <b>sources,</b> the CSP method {{has a problem}} that the localization accuracy is degraded due to cross-correlation among different <b>sound</b> <b>sources.</b> To solve this problem, this paper proposes a new method which suppresses the undesired cross-correlation by synchronous addition of CSP coefficients derived from multiple microphone pairs. Experiment results in a real room showed that the proposed method improves the localization accuracy when {{increasing the number of}} the synchronous additio...|$|R
40|$|In this paper, {{beamforming}} {{and normal}} modes method are combined {{to calculate the}} underwater acoustical field. Firstly, beamforming technology is used to form directional sources by controlling the phase and relative amplitude. Secondly, considering the ocean speed, surface and bottom boundary, the <b>point</b> <b>source</b> <b>sound</b> field {{is calculated based on}} the normal modes model. Then underwater acoustical field with directional sources is achieved by adding all point source field. Taken the 11 elements vertical linear array as an example, the special directional source is formed by chebyshev amplitude weighting, and then underwater acoustical field is calculated in the Munk profile of different grazing angles. The results indicate that beam grazing angle is very essential to its sound field, and changing the grazing angle can enhance the sound intensity of the shadow zone correspondingly...|$|R
5000|$|In general, as sound propagates {{underwater}} {{there is}} {{a reduction in the}} sound intensity over increasing ranges, though in some circumstances a gain can be obtained due to focusing. Propagation loss (sometimes referred to as transmission loss) is a quantitative measure of the reduction in sound intensity between two <b>points,</b> normally the <b>sound</b> <b>source</b> and a distant receiver. If [...] is the far field intensity of the source referred to a point 1 m from its acoustic centre and [...] is the intensity at the receiver, then the propagation loss is given by [...]In this equation [...] is not the true acoustic intensity at the receiver, which is a vector quantity, but a scalar equal to the equivalent plane wave intensity (EPWI) of the sound field. The EPWI is defined as the magnitude of the intensity of a plane wave of the same RMS pressure as the true acoustic field. At short range the propagation loss is dominated by spreading while at long range it is dominated by absorption and/or scattering losses.|$|R
40|$|A {{method is}} {{presented}} for generating a sound field that is significantly attenuated {{over half of}} the reproduction region, which has application to the generation of two independent sound fields for two listeners. The half-space sound field is produced by attenuating the negative or positive modes in the cylindrical or spherical expansion of a plane wave or <b>point</b> <b>source</b> <b>sound</b> field. It is shown that this is equivalent to adding to the original sound field, in quadrature, a second field which is the Hilbert transform of the original field. The resulting analytic field has a small magnitude in one half of the plane. Methods are presented for controlling the attenuation in the unwanted half-space. Finally, a simulation is presented showing the generation of a wideband pulse that propagates across half of the area within a circular array of sources...|$|R
40|$|An audio {{localization}} test comparing {{accuracy of}} static and moving <b>sound</b> <b>sources</b> {{was carried out}} in a spatially immersive virtual environment, using loudspeaker array with vector based amplitude panning for virtual <b>sound</b> <b>sources.</b> Azimuth and elevation error in localization were measured with different sound signals. As was expected errors in azimuth localization accuracy were smaller than errors in elevation accuracy. There were more localization blur with virtual <b>sound</b> <b>sources</b> than <b>sound</b> <b>sources</b> reproduced directly from a single loudspeaker. Localization blur with moving <b>sound</b> <b>sources</b> were in the same level as with static panorated <b>sound</b> <b>sources.</b> Although the <b>sound</b> <b>sources</b> moved steadily, the measurements indicated that subjects perceived the changes in <b>sound</b> <b>source</b> location stepwise due to applied amplitude panning...|$|R
50|$|In {{the free}} field, sound {{which has its}} origin at a point (a point source) will be {{propagated}} equally in all directions as a sphere. Since the surface area of a sphere = 4π r² where r is the radius, every doubling of the radius results in a four-fold increase in the sphere's surface area. The result {{of this is that}} the sound intensity quarters for every doubling of distance from the <b>point</b> <b>source.</b> <b>Sound</b> intensity is the acoustic power per unit area, and it decreases as the surface area increases since the acoustic power is spread over a greater area. The ratio between two acoustic pressures in deciBels is expressed by the equation dB = 20log(p1/p2), so for every doubling of distance from the point source p1 = 1 and p2 = 2, thus there is a sound pressure decrease of approximately 6 dB.|$|R
50|$|The {{binaural}} {{aspect of}} the cocktail party effect {{is related to the}} localization of <b>sound</b> <b>sources.</b> The auditory system is able to localize at least two <b>sound</b> <b>sources</b> and assign the correct characteristics to these sources simultaneously. As soon as the auditory system has localized a <b>sound</b> <b>source,</b> it can extract the signals of this <b>sound</b> <b>source</b> out of a mixture of interfering <b>sound</b> <b>sources.</b>|$|R
25|$|Acousticians, in {{studying}} the radiation of <b>sound</b> <b>sources</b> have developed some concepts important to understanding how loudspeakers are perceived. The simplest possible radiating source is a point source, sometimes called a simple source. An ideal point source is an infinitesimally small <b>point</b> radiating <b>sound.</b> It may be easier to imagine a tiny pulsating sphere, uniformly increasing and decreasing in diameter, sending out sound waves in all directions equally, independent of frequency.|$|R
40|$|Sound power {{describes}} a <b>sound</b> <b>source</b> {{regardless of its}} environment and is useful in noise control applications, but can be cumbersome and time consuming to measure. Sound power levels can rank different <b>sound</b> <b>sources</b> and is often restricted in noise control legislation. An acoustic camera records a sound field with a microphone array. Due to properties of the array, and by using beamforming algorithms, an acoustic camera can separate sound from different directions. The acoustic camera measures sound pressure from a <b>sound</b> <b>source.</b> By assuming directivity properties the sound power of a <b>sound</b> <b>source</b> {{can be derived from}} the sound pressure. In this thesis an acoustic camera has been evaluated in order to determine sound power estimation performance and <b>sound</b> <b>source</b> separation ability. This is tested by six different measurement set-ups in an anechoic chamber. Two different <b>sound</b> <b>sources</b> are used in the trials: one reference <b>sound</b> <b>source</b> and one disturbing <b>sound</b> <b>source.</b> The reference <b>sound</b> <b>source</b> has a calibrated and documented sound power level to which the measurement results are compared. Measurements were performed at 1 to 5 m distance from the acoustic camera with both <b>sound</b> <b>sources.</b> The influence of a disturbing <b>sound</b> <b>source</b> on the reference <b>sound</b> <b>source</b> <b>sound</b> power level was measured with the <b>sound</b> <b>sources</b> separated 0. 65 m to 2. 6 m. The measurements show that the sound power level could at best be determined within 1 dB. The acoustic camera can separate different <b>sound</b> <b>sources</b> well. Influence from a disturbing <b>sound</b> <b>source,</b> 10 dB SPL stronger and distanced 1 m from a reference <b>sound</b> <b>source</b> was 2 to 3 dB for mid-frequency one-third octave bands at 5 m measurement distance. Measuring sound power with an acoustic camera is fast and mobile compared to room interaction methods and sound intensity measurements. The results of this thesis are useful when measuring sound power levels, especially for <b>sound</b> <b>sources</b> such as chimney outlets, wind power stations and big objects that can not be moved or do not fit in a room. Validerat; 20101217 (root...|$|R
5000|$|... #Caption: A Numark DM2002X Pro Master DJ mixer. This three channel mixer {{can have}} up to three input <b>sound</b> <b>sources.</b> The gain control knobs and {{equalization}} control knobs allow the volume and tone of each <b>sound</b> <b>source</b> to be adjusted. The vertical faders allow for further adjustment {{of the volume of}} each <b>sound</b> <b>source.</b> The horizontally-mounted crossfader enables the DJ to smoothly transition from a song on one <b>sound</b> <b>source</b> to a song from a different <b>sound</b> <b>source.</b>|$|R
40|$|Monopole <b>sound</b> <b>sources</b> (i. e. omni {{directional}} <b>sound</b> <b>sources</b> with a known volume velocity) {{are essential}} for reciprocal measurements used in vehicle interior panel noise contribution analysis. Until recently, these monopole <b>sound</b> <b>sources</b> use a <b>sound</b> pressure transducer sensor as a reference sensor. A novel monopole <b>sound</b> <b>source</b> principle is demonstrated that uses a Microflown (acoustic particle velocity) sensor as a reference sensor. As compared to a <b>sound</b> <b>source</b> that uses a sound pressure transducer as a reference sensor, the new <b>sound</b> <b>sources</b> demonstrated are relatively easy to calibrate, not sensitive to changes in ambient temperatures, and suitable to use {{in all sorts of}} acoustic environments. © 2008 SAE International...|$|R
40|$|Measuring the {{contribution}} of a particular <b>sound</b> <b>source</b> to the ambient sound level at an arbitrary location is impossible without some form of <b>sound</b> <b>source</b> separation. This made it difficult, if not impossible, to design automated systems that measure {{the contribution}} of a target sound to the ambient sound level. This paper introduces <b>sound</b> <b>source</b> separation technology {{that can be used}} to measure {{the contribution of}} a <b>sound</b> <b>source,</b> a passing plane, in environments where planes are not the dominant <b>sound</b> <b>source.</b> This <b>sound</b> <b>source</b> separation and classification technology, developed by Sound Intelligence makes it, in principle, possible to monitor the temporal development of any soundscape...|$|R
