0|1236|Public
40|$|Judgments and {{decisions}} under uncertainty are frequently {{linked to a}} prior sequential search for relevant information. In such cases, the subject has to decide when to stop the search for information. Evidence accumulation models from social and cognitive psychology assume an active and sequential information search until enough evidence has been accumulated to <b>pass</b> a <b>decision</b> <b>threshold.</b> In line with such theories, we conceptualize the evidence threshold as the ``desired level of confidence'' (DLC) of a person. This model is tested against a fixed stopping rule (one-reason decision making) and against the class of multi-attribute information integrating models. A series of experiments using an information board for horse race betting demonstrates an advantage of the proposed model by measuring the individual DLC of each subject and confirming its correctness in two separate stages. In addition {{to a better understanding}} of the stopping rule (within the narrow framework of simple heuristics), the results indicate that individual aspiration levels might be a relevant factor when modelling decision making by task analysis of statistical environments. evidence accumulation, sequential information search, information acquisition, threshold models, stopping rule, level of confidence, probabilistic cue, validity, one-reason decision making. ...|$|R
3000|$|... where η is the <b>decision</b> <b>threshold.</b> For a given {{false alarm}} probability, {{determining}} the <b>decision</b> <b>threshold</b> {{needs to know}} the conditional probability density function p(ξ|H [...]...|$|R
40|$|If {{humans are}} faced with {{difficult}} choices when making decisions, the ability to slow down responses becomes critical {{in order to avoid}} suboptimal choices. Current models of decision making assume that the subthalamic nucleus (STN) mediates this function by elevating <b>decision</b> <b>thresholds,</b> thereby requiring more evidence to be accumulated before responding [1, 2, 3, 4, 5, 6, 7, 8 and 9]. However, direct electrophysiological evidence for the exact role of STN during adjustment of <b>decision</b> <b>thresholds</b> is lacking. Here, we show that trial-by-trial variations in STN low-frequency oscillatory activity predict adjustments of <b>decision</b> <b>thresholds</b> before subjects make a response. The relationship between STN activity and <b>decision</b> <b>thresholds</b> critically depends on the subjects’ level of cautiousness. While increased oscillatory activity of the STN predicts elevated <b>decision</b> <b>thresholds</b> during high levels of cautiousness, it predicts decreased <b>decision</b> <b>thresholds</b> during low levels of cautiousness. This context-dependent relationship may be mediated by increased influence of the medial prefrontal cortex (mPFC) -STN pathway on <b>decision</b> <b>thresholds</b> during high cautiousness. Subjects who exhibit a stronger increase in phase alignment of low-frequency oscillatory activity in mPFC and STN before making a response have higher <b>decision</b> <b>thresholds</b> and commit fewer erroneous responses. Together, our results demonstrate that STN low-frequency oscillatory activity and corresponding mPFC-STN coupling are involved in determining how much evidence subjects accumulate before making a decision. This finding might explain why deep-brain stimulation of the STN can impair subjects’ ability to slow down responses and can induce impulsive suboptimal decisions...|$|R
40|$|SummaryIf {{humans are}} faced with {{difficult}} choices when making decisions, the ability to slow down responses becomes critical {{in order to avoid}} suboptimal choices. Current models of decision making assume that the subthalamic nucleus (STN) mediates this function by elevating <b>decision</b> <b>thresholds,</b> thereby requiring more evidence to be accumulated before responding [1 – 9]. However, direct electrophysiological evidence for the exact role of STN during adjustment of <b>decision</b> <b>thresholds</b> is lacking. Here, we show that trial-by-trial variations in STN low-frequency oscillatory activity predict adjustments of <b>decision</b> <b>thresholds</b> before subjects make a response. The relationship between STN activity and <b>decision</b> <b>thresholds</b> critically depends on the subjects’ level of cautiousness. While increased oscillatory activity of the STN predicts elevated <b>decision</b> <b>thresholds</b> during high levels of cautiousness, it predicts decreased <b>decision</b> <b>thresholds</b> during low levels of cautiousness. This context-dependent relationship may be mediated by increased influence of the medial prefrontal cortex (mPFC) -STN pathway on <b>decision</b> <b>thresholds</b> during high cautiousness. Subjects who exhibit a stronger increase in phase alignment of low-frequency oscillatory activity in mPFC and STN before making a response have higher <b>decision</b> <b>thresholds</b> and commit fewer erroneous responses. Together, our results demonstrate that STN low-frequency oscillatory activity and corresponding mPFC-STN coupling are involved in determining how much evidence subjects accumulate before making a decision. This finding might explain why deep-brain stimulation of the STN can impair subjects’ ability to slow down responses and can induce impulsive suboptimal decisions...|$|R
40|$|Bit {{error rate}} (BER) {{monitoring}} {{is the ultimate}} goal of performance monitoring in all digital transmission systems as well as optical fiber transmission systems. To achieve this goal, optimization of the <b>decision</b> <b>threshold</b> must also be considered because BER is dependent on the level of <b>decision</b> <b>threshold.</b> In this paper, we analyze a pseudo-error counting scheme and propose an algorithm to achieve both BER monitoring and adaptive <b>decision</b> <b>threshold</b> optimization in optical fiber transmission systems. To verify the effectiveness of the proposed algorithm, we conduct computer simulations in both Gaussian and non-Gaussian distribution cases. According to the simulation results, BER and the optimum <b>decision</b> <b>threshold</b> can be estimated with the errors of 40 -Gb/s transmission systems...|$|R
40|$|We {{consider}} a multiple classifier system which combines the hard decisions of experts by voting. We {{argue that the}} individual experts should not set their own <b>decision</b> <b>thresholds.</b> The respective thresholds should be selected jointly as this will allow compensation of the weaknesses of some experts by the relative strengths of the others. We perform the joint optimization of <b>decision</b> <b>thresholds</b> for a multiple expert system by a systematic sampling of the multidimensional <b>decision</b> <b>threshold</b> space. We show {{the effectiveness of this}} approach on the important practical application of video shot cut detection...|$|R
40|$|International audienceParallel {{interference}} cancellation (PIC) is {{a family}} of low complexity multi-user detection methods for DS/CDMA systems. The performance of the multistage PIC depends on the decision function used in the interference cancellation. In this paper, we propose to provide the performance of PIC receiver using Xha function ‘clipping' with respect to <b>decision</b> <b>threshold</b> value. The <b>decision</b> <b>threshold</b> determines how much MAI is to be cancelled with hard decision and high probability. Otherwise, we used a soft bit decision to cancel only the interference of unreliable users form the received signal. This approach is original compared to PIC receiver with <b>decision</b> <b>thresholds</b> using null zone hard decision. It is shown that the proposed detector performs well under realistic conditions, {{particularly in the case}} of error optimisation of <b>decision</b> <b>threshold</b> value...|$|R
40|$|Abstract—We {{consider}} a multiple classifier system which combines the hard decisions of experts by voting. We {{argue that the}} individual experts should not set their own <b>decision</b> <b>thresholds.</b> The respective thresholds should be selected jointly as this will allow for compensation of the weaknesses of some experts by the relative strengths of the others. We perform the joint optimization of <b>decision</b> <b>thresholds</b> for a multiple expert system by a systematic sampling of the multidimensional <b>decision</b> <b>threshold</b> space. We show {{the effectiveness of this}} approach on the important practical application of video shot cut detection. 1 1...|$|R
40|$|This thesis {{investigates the}} {{performance}} of free-space optical (FSO) communication systems in a turbulent atmosphere employing optical amplifiers (OAs) to extend transmission reach and wavelength-division multiplexing (WDM) to improve capacity. This system performance is considered {{in the presence of}} amplified spontaneous emission (ASE) noise, scintillation, beam spreading, atmospheric attenuation and interchannel crosstalk. In this work, the modulation scheme used is the on-off keying non-return-to-zero and the main performance metric employed is the average bit error rate (BER). Various performance evaluation methods are used to estimate system performance. Analysis of single link, cascaded OA and WDM FSO communication systems are given and the implications of using both adaptive (to channel state) and non-adaptive <b>decision</b> <b>threshold</b> schemes are analysed. The benefits of amplifier saturation, for example in the form of effective scintillation reduction when a non-adaptive <b>decision</b> <b>threshold</b> scheme is utilised at the receiver for different atmospheric turbulence regimes, are presented. Monte Carlo simulation techniques are used to model the probability distributions of system parameters such as the optical signal power, amplified spontaneous emission noise, optical signal to noise ratio and the average bit error rate due to scintillation. It is found that {{the performance of}} an adaptive <b>decision</b> <b>threshold</b> is superior to a non-adaptive <b>decision</b> <b>threshold</b> for both saturated and fixed gain preamplified receivers and the ability of a saturated gain OA to suppress scintillation is only meaningful for system performance when a non-adaptive <b>decision</b> <b>threshold</b> is used at the receiver. In a saturated gain preamplified system, the optimum non-adaptive <b>decision</b> <b>threshold</b> is investigated. An OA cascade can be successfully used to extend reach in FSO communication systems and specific system implementations are presented. The optimal cascade scheme with a non-adaptive receiver would use frequent low gain saturated amplification although this has a cost implication. Furthermore, a saturated gain amplified WDM FSO system with a non-adaptive <b>decision</b> <b>threshold</b> is superior to a non-amplified WDM FSO system with an adaptive <b>decision</b> <b>threshold...</b>|$|R
40|$|The {{performance}} of a free-space optical (FSO) communication system in a turbulent atmosphere employing an optical amplifier (OA) cascade to extend reach is investigated. Analysis of both single and cascaded OA FSO communication links is given {{and the implications of}} using both adaptive (to channel state) and non-adaptive <b>decision</b> <b>threshold</b> schemes are analysed. The benefits of amplifier saturation, for example in the form of effective scintillation reduction when a non-adaptive <b>decision</b> <b>threshold</b> scheme is utilised at the receiver for different atmospheric turbulence regimes, are presented. Monte Carlo simulation techniques are used to model the probability distributions of the optical signal power, noise and the average bit error rate due to scintillation for the cascade. The {{performance of}} an adaptive <b>decision</b> <b>threshold</b> is superior to a non-adaptive <b>decision</b> <b>threshold</b> for both saturated and fixed gain preamplified receivers and the ability of a saturated gain OA to suppress scintillation is only meaningful for system performance when a non-adaptive <b>decision</b> <b>threshold</b> is used at the receiver. An OA cascade can be successfully used to extend reach in FSO communication systems and specific system implementations are presented. The optimal cascade scheme with a non-adaptive receiver would use frequent low gain saturated amplification...|$|R
30|$|It {{should be}} {{also noted that}} p(FA) and p(miss) in Tables  6 and 7 do not relate to ATWV {{performance}} but to MTWV performance (i.e., with the a posteriori best <b>decision</b> <b>threshold).</b> In this way, systems with MTWV = 0.0 (i.e., {{those that do not}} generate detections at best <b>decision</b> <b>threshold)</b> obtain p(FA)= 0.0 and p(miss)= 1.0.|$|R
40|$|When {{animals have}} to make a number of {{decisions}} during a limited time interval, they face a fundamental problem: how much time they should spend on each decision in order to achieve the maximum possible total outcome. Deliberating more on one decision usually leads to more outcome but less time will remain for other decisions. In the framework of sequential sampling models, the question is how animals learn to set their <b>decision</b> <b>threshold</b> such that the total expected outcome achieved during a limited time is maximized. The aim {{of this paper is to}} provide a theoretical framework for answering this question. To this end, we consider an experimental design in which each trial can come from one of the several possible ``conditions. A condition specifies the difficulty of the trial, the reward, the penalty and so on. We show that to maximize the expected reward during a limited time, the subject should set a separate value of <b>decision</b> <b>threshold</b> for each condition. We propose a model of learning the optimal value of <b>decision</b> <b>thresholds</b> based on the theory of semi-Markov decision processes (SMDP). In our model, the experimental environment is modeled as an SMDP with each ``condition being a ``state and the value of <b>decision</b> <b>thresholds</b> being the ``actions taken in those states. The problem of finding the optimal <b>decision</b> <b>thresholds</b> then is cast as the stochastic optimal control problem of taking actions in each state in the corresponding SMDP such that the average reward rate is maximized. Our model utilizes a biologically plausible learning algorithm to solve this problem. The simulation results show that at the beginning of learning the model choses high values of <b>decision</b> <b>threshold</b> which lead to sub-optimal performance. With experience, however, the model learns to lower the value of <b>decision</b> <b>thresholds</b> till finally it finds the optimal values...|$|R
30|$|Properly {{choosing}} the <b>decision</b> <b>threshold</b> {{to mitigate the}} higher energy due to pilots.|$|R
3000|$|... [...]. In other words, the {{proposed}} simplified detection strategy is robust against local <b>decision</b> <b>threshold</b> mismatches.|$|R
30|$|It {{is worth}} {{mentioning}} that whether the theoretical lower bound {{mentioned in the}} Appendix given with (23) exists. As Proposition 1 implies, lower bound could be reached in case {{the behavior of the}} decision device could be characterized statistically, since (23) {{is a function of the}} <b>decision</b> <b>threshold,</b> ζ. For the energy detector, it is known that optimal <b>decision</b> <b>threshold</b> theoretically exists as expressed in ([44] § III.B).|$|R
3000|$|... [...]), {{we assume}} that each sensor makes use of an actual <b>decision</b> <b>threshold</b> which is uniformly {{distributed}} in [...]...|$|R
3000|$|... and the <b>decision</b> <b>threshold</b> τ {{are adopted}} in the simulations, respectively. The user number is set to be 8.|$|R
3000|$|... where L is the <b>decision</b> <b>threshold,</b> {{chosen as}} {{to achieve a}} given {{operating}} point on the sensor's ROC curve.|$|R
3000|$|Within one stage, no {{threshold}} for intermediate weak classifiers is required. We need only determine each <b>decision</b> <b>threshold</b> θ [...]...|$|R
30|$|Train/dev {{data were}} used to train the <b>decision</b> <b>{{threshold}}.</b> Next, this threshold was used to hypothesize query detections of the test data.|$|R
3000|$|... min(·) {{denote the}} largest and {{smallest}} eigenvalue of a matrix respectively and γ stands for the predefined <b>decision</b> <b>threshold.</b> If Γ [...]...|$|R
40|$|Accuracy is a {{fundamental}} dimension for the effectiveness of recommender systems. Several accuracy metrics have been investigated in the literature. However, we argue, these metrics are not sufficiently user-specific. In previous work, we proposed accuracy metrics that take into account a userspecific pointwise <b>decision</b> <b>threshold.</b> In this paper, we present even more user-specific accuracy metrics that rely on the user utility function on the rating scale {{as well as on}} a user-specific sigmoid functional <b>decision</b> <b>threshold...</b>|$|R
30|$|Finally, the {{obtained}} {{scores are}} subject to ZT-normalization[21], and the <b>decision</b> <b>threshold</b> minimizing equal error rate (EER) is chosen (separately for each condition).|$|R
40|$|Abstract. This paper {{presents}} {{equipment and}} procedures for on-card (in-situ) performance testing of biometric on-card comparison implemen-tations using pre-existing databases of biometric samples. A DTW-based on-line signature on-card comparison implementation serves as an exam-ple test object. The test results presented are false match rates and false non-match rates over a range of <b>decision</b> <b>thresholds</b> on a per-test-subject basis. The results reveal considerable differences in the comparison-score frequency distribution among test subjects, which necessitates the setting of user-dependent <b>decision</b> <b>thresholds</b> or comparison-score normalization...|$|R
3000|$|... {{when the}} PU exists actually. These {{detection}} errors, respectively, degrade {{the performances of}} CR system and PU and {{are very sensitive to}} the <b>decision</b> <b>threshold.</b>|$|R
3000|$|In the {{observation}} phase, {{we assume that}} there could be an error in the <b>decision</b> <b>threshold</b> used at each sensor. More precisely, denoting by [...]...|$|R
30|$|The Yogarajah’s method {{consists}} {{in analyzing the}} distribution of the error signal in a facial region to determine the <b>decision</b> <b>thresholds</b> from the obtained Gaussian parameters.|$|R
40|$|This paper {{presents}} {{equipment and}} procedures for on-card (in-situ) performance t esting of biometric on-card comparison implementations using pre-existing databa ses of biometric samples. A DTW-based on-line signature on-card comparison imple mentation serves as an example test object. The test results presented are false match rates and false non-match rates over a range of <b>decision</b> <b>thresholds</b> on a per-test-subject basis. The results reveal considerable differences in the comp arison-score frequency distribution among test subjects, which necessitates the setting of user-dependent <b>decision</b> <b>thresholds</b> or comparison-score normalization...|$|R
30|$|With {{the rapid}} {{development}} of information geometry theory, {{the concept of}} statistical manifolds is used to transform signal detection problems into geometric problems on manifolds, and then geometric tools {{can be used to}} visually analyze detection problems. Liu et al. used information geometry theory to detect radar signals. At the same time, a matrix constant false alarm rata (CFAR) and a distance detector based on geodesic were proposed [15]. Chen applied the information geometry method to spectrum sensing, increased the measurement of manifolds, and obtained the <b>decision</b> <b>threshold</b> through simulation [16]. Lu et al. used the matching method to obtain the closed expression of the <b>decision</b> <b>threshold,</b> which has higher computational complexity [17]. However, in spectrum sensing, the derivation of the threshold is not only complicated, but there is always some deviation in using the fixed <b>decision</b> <b>threshold</b> to determine whether the PU exists.|$|R
40|$|Common Law distinguishes two {{standards}} of proof applicable in {{civil and criminal}} matters, respectively. The criminal standard of “beyond reasonable doubt” {{is much higher than}} the “preponderance of the evidence” standard used in civil cases. Continental European Civil Law, on the other hand, recognizes just one standard of “full conviction” applicable in both criminal and civil cases. This study is the first to look at the standard of proof actually used by judges and judicial clerks in a Civil Law country (Switzerland). It is shown that, when asked directly, the members of court express a high <b>decision</b> <b>threshold</b> in line with legal doctrine and case law. But when Swiss judges are asked to estimate the error costs associated with each outcome and the error-cost-minimizing <b>decision</b> <b>threshold</b> is calculated based on the responses, the resulting standard is no different from the Common Law’s “preponderance of the evidence” standard. When using the stated degree of belief in the truth of the plaintiff’s allegations as a predictor for the grant of the plaintiff’s request in a civil action, the probability of grant is 50 % at a stated conviction of only 63 %. It is further shown that the <b>decision</b> <b>threshold</b> is influenced by the individual’s loss aversion, with individuals with a higher loss aversion having a higher <b>decision</b> <b>threshold.</b> No difference between the estimated <b>decision</b> <b>threshold</b> for members of the courts and members of the general population is found. The results suggest that the standard of proof actually employed by Swiss judges is not much different from the Common Law’s “preponderance of the evidence” standard, despite the doctrinal insistence to the contrary...|$|R
40|$|The goal of {{this article}} is to {{investigate}} how human participants allocate their limited time to decisions with different properties. We report the results of two behavioral experiments. In each trial of the experiments, the participant must accumulate noisy information to make a decision. The participants received positive and negative rewards for their correct and incorrect decisions, respectively. The stimulus was designed such that decisions based on more accumulated information were more accurate but took longer. Therefore, the total outcome that a participant could achieve during the limited experiments' time depended on her "decision threshold", the amount of information she needed to make a decision. In the first experiment, two types of trials were intermixed randomly: hard and easy. Crucially, the hard trials were associated with smaller positive and negative rewards than the easy trials. A cue presented at the beginning of each trial would indicate the type of the upcoming trial. The optimal strategy was to adopt a small <b>decision</b> <b>threshold</b> for hard trials. The results showed that several of the participants did not learn this simple strategy. We then investigated how the participants adjusted their <b>decision</b> <b>threshold</b> based on the feedback they received in each trial. To this end, we developed and compared 10 computational models for adjusting the <b>decision</b> <b>threshold.</b> The models differ in their assumptions on the shape of the <b>decision</b> <b>thresholds</b> and the way the feedback is used to adjust the <b>decision</b> <b>thresholds.</b> The results of Bayesian model comparison showed that a model with time-varying thresholds whose parameters are updated by a reinforcement learning algorithm is the most likely model...|$|R
3000|$|... 2 q+ 2), where A and B (B≤ 0 ≤A) are the <b>decision</b> <b>thresholds</b> for {{the desired}} miss {{detection}} and false alarm probabilities, which correspond to H [...]...|$|R
3000|$|... fa {{by finding}} the {{corresponding}} <b>decision</b> <b>threshold</b> in the χ _ 2 n_ν^ 2 tables. We cannot design a test {{based on a}} desired detection rate P [...]...|$|R
30|$|In the following, we {{will analyze}} the {{deterministic}} {{relation between the}} probability of correct recognition and the <b>decision</b> <b>threshold</b> value, {{and then try to}} derive the optimal threshold value.|$|R
40|$|Keyword {{spotting}} {{is a very}} forward-looking {{and promising}} branch of speech recognition. This paper presents a HMM-based keyword spotting system, which works with a new algorithm. The first discussion topic is {{the description of the}} search algorithm, that needs no representation of the non-keyword parts of the speech signal. For this purpose, the computation of the HMM scores and the Viterbi algorithm had to be modified. The keyword HMMs are not concatenated with other HMMs, so that there is no necessity for filler or garbage models. As a further advantage, this algorithm needs only low computional expense and storage requirement. The second discussion topic is the determination of a optimal <b>decision</b> <b>threshold</b> for each keyword. In order two decide between the two possibilities "keyword was spoken" and "keyword was not spoken", the scores of the keywords are compared with keyword specific <b>decision</b> <b>thresholds.</b> This paper introduces a method to fix <b>decision</b> <b>thresholds</b> in advance. Starting [...] ...|$|R
3000|$|... 1 > 0 is a preset <b>decision</b> <b>{{threshold}}.</b> Empirically, {{the threshold}} is set as λ _ 1 = ρ _ 1 + m ·σ _ 1 /t_ 1, where ρ [...]...|$|R
