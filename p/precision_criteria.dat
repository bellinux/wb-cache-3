39|83|Public
5000|$|Testing {{criteria}} {{are based on}} ISO 3159 which defines a wrist chronometer with spring-balance oscillator. Only movements which meet the <b>precision</b> <b>criteria</b> established under ISO 3159 are granted an official chronometer certificate. (Compare ISO 3158.) ...|$|E
50|$|The {{train set}} {{was built by}} a joint venture of Siemens and ThyssenKrupp from Kassel, Germany and based on years of tests and {{improvements}} of their Transrapid maglev monorail. The Shanghai Maglev track (guideway) was built by local Chinese companies who, {{as a result of}} the alluvial soil conditions of the Pudong area, had to deviate from the original track design of one supporting column every 50 metres to one column every 25 metres, to ensure that the guideway meets the stability and <b>precision</b> <b>criteria.</b> Several thousand concrete piles were driven to depths up to 70 metres to attain stability for the support column foundations. A mile-long, climate controlled facility was built alongside the line's right of way to manufacture the guideways.|$|E
5000|$|Glass {{pipettes}} {{are classified}} as genre, class, and dimension at a specific graduated pipettes, which considered as the excellent specification standard. There are two genres called genre 1 and genre 2 characterized as “Standard taper tip” and “long taper tip” respectively. The class of the specification standard consists of two class called class A and class B. The signification of the class A in the graduated pipettes approves for the applicable designation and <b>precision</b> <b>criteria.</b> The marking of class B specify for the general contemplate as class A. Volumetric capacities for Class B pipettes necessitate for at least twice of the identification scope allowed for Class A pipets. The class specification or serial number in class B is not marked.|$|E
30|$|Identify subsets between {{rows and}} columns where the {{interpolated}} data between {{the last two}} iterations differ more than by a requested z <b>precision</b> <b>criterion.</b>|$|R
30|$|Usual SVD {{algorithms}} [3] compute SVD modes one-by-one “incrementally” until {{having a}} basis satisfying {{a certain level}} of accuracy. These algorithms iterate until finding a mode. Once the <b>precision</b> <b>criterion</b> is reached, the basis is ensured to be optimal because each found mode is the most representative one.|$|R
3000|$|... 2 -distribution {{or when a}} {{bootstrap}} is required. This question {{shall be}} tackled in a simulation study. Moreover, if we switch to the bootstrap, precision depends {{on the number of}} bootstrap samples. A concrete guideline will be given, how many bootstrap replications are required to fulfill a desired <b>precision</b> <b>criterion.</b>|$|R
40|$|Establishment of <b>precision</b> <b>criteria</b> from an EU interlaboratory {{comparison}} organised by the EURL- Food Contact Materials for the quantification from and migration into poly(2, 6 -diphenyl phenylene oxide) Development of a harmonised method for specific migration {{into the new}} simulant for dry foods established i...|$|E
40|$|This paper {{reports on}} a simple pure {{numerical}} method developed for computing Hansen coefficients by using recursive harmonic analysis technique. The <b>precision</b> <b>criteria</b> of the computations are very satisfactory and provide materials for computing Hansen's and Hansen's like expansions, also to check the accuracy of some existing algorithms. Comment: 10 pages, no figure...|$|E
30|$|In addition, we {{adopt the}} {{precision}} and success rate {{for evaluating the}} tracking performance. The <b>precision</b> <b>criteria</b> is the percentage of frames which estimated location is within a given threshold distance of the ground truth and the success criteria is the ratios of successful frames at a given threshold ranged from 0 to 1.|$|E
30|$|In our investigation, we {{calculated}} the conservation value per hectare of myrtle in the Dooreh forest area in Lorestan Province. Using the Contingent Valuation (CV) and Double Bounded Dichotomous Choice (DBDC) methods, we determined {{the willingness to}} pay (WTP) for myrtle conservation. The WTP was estimated with a logit model for which indices were obtained based on a maximum <b>precision</b> <b>criterion.</b>|$|R
30|$|Where the z <b>precision</b> <b>criterion</b> failed, {{measure a}} net of rows and columns with twice as fine {{resolution}} on those rectangles. Note {{that in order}} to save the measurement time, this process needs to be optimized so that the movement between different refinement areas is minimized. An optimum path for the SPM probe is therefore planned merging all the necessary movements (including movements between different areas) with the criterion of the shortest total distance. This is in principle a traveling salesman problem, and here, it is solved using the nearest neighbor algorithm.|$|R
30|$|Generally, the {{approximation}} {{of the test}} statistic under the null-hypothesis shows sufficient approximation to the theoretical distribution if samples comprise at least 500 respondents and an instrument with more than ten items is considered. For studies considering smaller samples or fewer items we recommend the more expensive bootstrap method. However, this is little a drawback as bootstrapping small samples takes only {{a reasonable amount of}} time. In order to further control the required time, Eq. (11) provides an easily applicable rule of thumb allowing {{to limit the number of}} bootstrap samples warranting a <b>precision</b> <b>criterion</b> of interest.|$|R
40|$|Abstract We {{report on}} a simple pure {{numerical}} method developed for computing Hansen coefficients by using a recursive harmonic analysis technique. The <b>precision</b> <b>criteria</b> of the computations are very satisfactory and provide materials for comput-ing Hansen’s and Hansen’s like expansions, and also to check the accuracy of some existing algorithms. Key words: celestial mechanics — methods: numerical...|$|E
40|$|In this paper, an {{intrinsic}} relation {{was developed for}} Hyades stars between {{a function of the}} right ascensions α and declination δ with the angular distance λ from the vertex. The <b>precision</b> <b>criteria</b> of this relation are very satisfactory and a correlation coefficient of value ≈ 1 was found which proves that the attributes are completely related linearly...|$|E
40|$|Adjustment {{strategies}} {{associated to}} the methodology applied {{used to the}} implantation of a gravity network of high precision in Paraná are presented. A network was implanted with stations in 21 places in the State of Paraná {{and one in the}} state of São Paulo To reduce the risk of the losing of points of that gravity network, they were established on the points of the GPS High Precision Network of Paraná, which possess a relatively homogeneous geographical distribution. For each one of the gravity lines belonging to the loops implanted for the network, it was possible to obtain three or six observations. In the first strategy, of adjustment investigated, for the net, it was considered, as observation, the medium value of the observations obtained for each gravity line. In the second strategy, of the adjustment, the observations were considered independent. The comparison of those strategies revealed that only the <b>precision</b> <b>criteria</b> is not enough to indicate the great solution of a gravity network. It was verified that there is need to use an additional criterion for analysis of the adjusted solution of the network, besides the <b>precision</b> <b>criteria.</b> The reliability criterion for geodesic networks, which becomes separated in reliability internal and external reliability it was used. The reliability internal it was used to verify the rigidity with which the network reacts in the detection and quantification of existent gross errors in the observations, and the reliability external in the quantification of the influence on the adjusted parameters of the errors non located. They are presented the aspects that differentiate the obtained solutions, when they combine the <b>precision</b> <b>criteria</b> and reliability criteria in the analysis of the quality of a gravity network...|$|E
40|$|International audienceParameter {{estimation}} mainly {{consists in}} characterising a parameter set consistent with measurements, {{the model and}} the equation error description. The problem to be solved is that of finding the set of admissible parameter values corresponding to an admissible error. The uncertainties must be treated by a global analysis of the problem: both the equation error and the parameter set are considered unknown. Then, a solution is given as a domain of time-variant parameters and a bounded set of the error. This procedure consists in explaining the measurements performed at all time by optimising a <b>precision</b> <b>criterion</b> based on the polytope theory...|$|R
40|$|Abstract—A near-lossless, {{adaptive}} watermarking algorithm {{based on}} DCT {{is presented to}} protect DEM from theft and illegal reproduction. Due to the high accuracy of the DEM, {{the focus of this}} paper is put on ensuring that the watermarked DEM should meet the precision requirement which also means the watermarked DEM should be near-lossless. The contribution of this work is that not only the DEM precision, but also the precision of slope and aspect are considered. In order to improve the robustness, the watermark should be embedded in the terrain lines. The preliminary results show that the error both of the aspect and the slope are very small and the watermarked DEM meets the medium error and maximum error proposed in the national DEM <b>precision</b> <b>criterion.</b> The watermarking can resist the compression and cropping attack...|$|R
40|$|International Telemetering Conference Proceedings / October 10 - 12, 1972 / International Hotel, Los Angeles, CaliforniaThe {{reconstruction}} of the form of analog signals by samples by means of linear filtration is considered. <b>Precision</b> <b>criterion</b> - mean square reconstruction error and the maximum reconstruction error probability. Linear filters (reconstructing functions) of increasing difficulty are considered together with an optimal filter. A signal model is introduced by means of large set of correlation functions. The calculations of reconstruction errors are carried out for different reconstructing functions. The tables and the figures allow to determine reconstruction errors, to estimate the necessity of decreasing complexity of the filter and to choose the type of filter. The results of calculations and the recommendations of choosing the sampling rates of the sensors {{are made in the}} paper can be used by the experimenters and specialists in telemetry...|$|R
30|$|The {{evaluation}} {{results are}} showed in Table 2. The sensitivity and <b>precision</b> <b>criteria</b> {{are used for}} evaluation the method. The sensitivity [True positive rate (TPR)] is computed by #TPR = #True positive/ (# True positive + # False negative). The precision is computed by # Precision=#true positive / (# true positive + # false positive). The true positive rate and precision {{are affected by the}} zoom level, that mean under the condition of the same size of images, the result at higher zoom level is better that of lower. The road detection is perfect at the 17 th zoom level and higher.|$|E
40|$|By using general {{information}} structures and <b>precision</b> <b>criteria</b> {{based on the}} dispersion of con-ditional expectations, we study how oligopolists ’ information acquisition decisions may change the effects of information sharing on the consumer surplus. Sharing information about individual cost parameters gives the following trade-off in Cournot oligopoly. On the one hand, it decreases the expected consumer surplus for a given information precision, as the literature shows. On the other hand, information sharing increases the firms ’ incentives to acquire information, and the consumer surplus increases in the precision of the firms ’ information. Interestingly, the latte...|$|E
40|$|This paper {{describes}} Bayesian {{methods for}} life test planning with Type II censored {{data from a}} Weibull distribution, when the Weibull shape parameter is given. We use conjugate prior distributions and criteria based on estimating a quantile of interest of the lifetime distribution. One criterion {{is based on a}} precision factor for a credibility interval for a distribution quantile and the other is based on the length of the credibility interval. We provide simple closed form expressions for the relationship between the needed number of failures and the <b>precision</b> <b>criteria.</b> Examples are used to illustrate the results...|$|E
40|$|This article {{describes}} {{a new approach}} to estimate F 0 curves using a B-Spline model characterized by a knot sequence and associated control points. The free parameters of the model are the number of knots and their location. The free-knot placement, which is a NP-hard problem, is done using a global MLE within a simulated-annealing strategy. The optimal knot number estimation is provided by MDL methodology. Three criteria are proposed: control points are first considered as integer values, next as real coefficients with fixed precision and then with variable precision. Experiments are conducted in a speech processing context on a 7000 syllables french corpus. We show that a variable <b>precision</b> <b>criterion</b> gives better results in terms of RMS error (0. 42 Hz) as well as in terms of reduction of the number of B-spline degrees of freedom (63 % of the full model). 1...|$|R
40|$|We {{present in}} this paper a {{comparison}} of the dispersion properties for several finitedifference approximations of the acoustic wave equation. We investigate the compact and staggered schemes of fourth order accuracy in space and of second order or fourth order accuracy in time. We derive the computational cost of the simulation implied by a <b>precision</b> <b>criterion</b> on the numerical simulation (maximum allowed error in phase or group velocity). We conclude that for moderate accuracy the staggered scheme of second order in time is more efficient, whereas for very precise simulation the compact scheme of fourth order in time is a better choice. The comparison increasingly favors the lower order staggered scheme as the dimension increases. In three dimensional simulation, the cost of extremely precise simulation with any of the schemes is very large, whereas for simulation of moderate precision the staggered scheme is the least expensive. 1 Introduction Asymptotic expansions and formulas like [...] ...|$|R
40|$|International audienceA {{method for}} {{image-based}} contact detection and modeling, with guaranteed precision on the intersection volume, is presented. Unlike previous image-based methods, our method optimizes a non-uniform ray sampling resolution and allows precise {{control of the}} volume error. By cumulatively projecting all mesh edges into a generalized 2 D texture, we construct a novel data structure, the Error Bound Polynomial Image (EBPI), which allows efficient computation of the maximum volume error {{as a function of}} ray density. Based on a <b>precision</b> <b>criterion,</b> EBPI pixels are subdivided or clustered. The rays are then cast in the projection direction according to the non-uniform resolution. The EBPI data, combined with ray-surface intersection points and normals, is also used to detect transient edges at surface intersections. This allows us to model intersection volumes at arbitrary resolution, while avoiding the geometric computation of mesh intersections. Moreover, the ray casting acceleration data structures can be reused for the generation of high quality images...|$|R
40|$|Three grab samples (TX- 95 - 1, TX- 95 - 2, and TX- 95 - 3) {{were taken}} from tank 241 - TX- 244 riser 8 on November 7, 1995 and {{received}} by the 222 -S Laboratory on that same day. Samples TX- 95 - 1 and TX- 95 - 2 were designated as supernate liquids, and sample TX- 95 - 3 was designated as a supernate/sludge. These samples were analyzed to support the waste compatibility safety program. Accuracy and <b>precision</b> <b>criteria</b> were met for all analyses. No notifications were required based on sample results. This document provides the analysis to support the waste compatibility safety program...|$|E
40|$|Abstract. In this paper, {{relation}} {{was developed}} for Hyades stars between {{a function of the}} right ascensions and the angular distances from the vertex. The <b>precision</b> <b>criteria</b> of this relation are very satisfactory and a correlation coefficient value of 1 was found which proves that the attributes are completely related linearly. The importance of this relation was illustrated through its usages as: • a criterion for membership of the cluster, • a generating function for evaluating some parameters of the cluster, • a generating function for the initial values of the vertex equatorial coor-dinates which could then be improved iteratively using the procedure of differential corrections. Key words. Moving clusters—Hyades cluster—open clusters—Hyades astrometric and kinematic. 1...|$|E
40|$|<b>Precision</b> <b>criteria</b> for the {{quantification}} {{of metals}} in acetic acid 3 % and acetic acid 4 % migration solutions for different analytical techniques were calculated and {{are presented in}} this report. They were derived from two interlaboratory comparisons (ILC 03 / 04 2014) data, including repeatability and reproducibility standard deviations. Three groups of analytical techniques were identified from the ILCs: Inductively coupled plasma mass spectrometry (ICP-MS), inductively coupled plasma Optical emission spectrometry (ICP-OES) and atomic absorption spectrometry (AAS). AAS included graphite furnace (GF-AAS) and flame (F-AAS). <b>Precision</b> <b>criteria</b> were calculated applying robust statistic methods with algorithms A and S (ISO 5725 - 5 and ISO 13528) and DIN 38402 A 45 (Q-method/Hampel-estimator) The robust means calculated according to both algorithms were checked for significant difference (student's t-test) and equivalence test. Relative repeatability standard deviations were very low generally less than 2 % and reached 4. 4 % only for Pb in acetic acid 4 %. Relative reproducibility standard deviations for Pb were around 10 % for ICP-MS and ICP-OES techniques and 16 % for AAS. Relative reproducibility standard deviations for Cd were around 5 %, 10 % and 20 % using respectively ICP-MS, AAS or ICP-OES techniques. For other elements the relative reproducibility standard deviations obtained were {{for most of the}} cases less than 10 %. From the comparison study it was possible to establish with the exception of Fe, for all elements of the two ILCs in both solutions (corresponding to elements from ceramics and elements from plastics) the analytical techniques were generally equivalent and not significantly different at the concentrations investigated. JRC. I. 1 -Chemical Assessment and Testin...|$|E
40|$|International audienceThe aim of {{this work}} {{is to find a}} tube which {{encloses}} a sequence of points representing a shape that uses a set of linear static models by set-membership approach. The sequence of the points is a two-dimensional and can represent any undefined shape. The model parameters are characterized {{in such a way that}} they minimize a <b>precision</b> <b>criterion.</b> Generally, one linear static model is not able to describe an undefined non-linear shape with a desired precision. To achieve a more precise model, the points are divided into m parts, each part is called a mode and each mode is described by a different static linear model. Multi-mode modelling means to define the switching conditions of departing from a mode to another, the validity domain and parameter vector of each mode. The model can be used in different domains such as process safety analysis and fault detection in control engineering where it describes all allowed and normal behaviors of a static system...|$|R
40|$|Information {{retrieval}} systems {{depend on}} Boolean queries. Proposed evolution of Boolean queries should increase {{the performance of}} the information retrieval system. Information retrieval systems quality are measured in terms of two different <b>criteria,</b> <b>precision</b> and recall. Evolutionary techniques are widely applied for optimization tasks in different areas including the area of information retrieval systems. In information retrieval applications both criteria have been combined in a single scalar fitness function by means of a weighting scheme 'harmonic mean'. Usage of genetic algorithms in the Information retrieval, especially in optimizing a Boolean query, is presented in this paper. Influence of both <b>criteria,</b> <b>precision</b> and recall, on quality improvement are discussed as well...|$|R
40|$|Purpose. Development of <b>precision</b> {{selection}} <b>criteria</b> {{of options}} of technical realization of effective active and adaptive system of expeditious service of {{elements of a}} power supply system in the conditions of network-centric management. Methodology. In development of power supply systems their evolution from the elementary forms using elementary network technologies and models of interactions in power to more irregular shapes within the concept of Smart Grid with elements of network-centric character is observed. This direction is based on Internet-technologies of the last generation, and realize models of power activity which couldn't be realized before. Results. The number of possible options of active and adaptive system of expeditious service of elements of a power supply system is usually rather big {{and it is difficult}} to choose the acceptable option by direct search. Elimination of admissible options of the technical realization constructed on the principles of a network centrism means application of the theory of multicriteria optimization from a position of discrete programming. The basis of procedure of elimination is made by algorithm of an assessment of system by criterion of accuracy. Originality. The case of an assessment of the precision characteristic of system at restrictions for the set accuracy is connected with need of decomposition of requirements of all system in general and on separate subsystems. For such decomposition the ratios connecting the accuracy of functioning of a separate subsystem with variations of parameters of all system, and also with precision characteristics of subsystems of the lower levels influencing this subsystem are received. Practical value. In the conditions of the network-centric organization of management of expeditious service of elements of a power supply system elimination of options of subsystems when using <b>precision</b> <b>criterion</b> allows to receive the maximum number of essentially possible options of system of service taking into account the accepted service strategy...|$|R
40|$|This paper {{presents}} {{a study on}} spiral bevel gear (SBGs) manufacturing method based on subdivision surface modeling scheme. The explicit nominal surface descriptions for both gears and pinions have been first developed based on gear generative motions. Then a group of key points on the tooth surface are selected as seed points (starting points) for a subdivision surface. From a given manufacturing <b>precision</b> <b>criteria,</b> a subdivision surface can be generated with {{a limited number of}} iterations during a subdivision surface modeling. This new surface modeling technique can be used for manufacturing (SBGs) with a general numerical control machine and a tool cutter in a flexible manufacturing environment. The experimental evaluation has been carried out and indicated that the subdivision surface-based SBG manufacturing method has advantages in high precise SBG manufacturing and precision control...|$|E
40|$|The recent common {{feeling about}} a skyrocketing {{economic}} risk has drawn increasing {{attention to its}} role and consequences on individuals' welfare. In literature one of the concepts that aims to measure it is vulnerability to poverty, that is the probability, today, of being in poverty or to fall into deeper poverty in the future (The World Bank, 2011). This paper compares empirically the several measures of individual vulnerability proposed in the literature, {{in order to understand}} which is the best signal of poverty {{that can be used for}} policies purposes. To this aim the Receiver Operating Characteristic (ROC) curve, the Pearson and Spearman correlation coefficients are used as <b>precision</b> <b>criteria.</b> The results show that two groups of indexes can be identified, high- and low-performers, and, among the former, that proposed by Dutta et al. (2011) is the most precise. ...|$|E
40|$|How much {{information}} does an auctioneer want bidders {{to have in}} a private value environment? We address this question using a novel approach to ordering information structures based on the property that in private value settings more information leads to a more disperse distribution of buyers ’ updated expected valuations. We define the class of <b>precision</b> <b>criteria</b> following this approach and different notions of dispersion, and relate them to existing criteria of informativeness. Using supermodular precision, we obtain three results: (1) a more precise information structure yields a more efficient allocation; (2) the auctioneer provides less than the efficient level of information since more information increases bidder informational rents; (3) there is a strategic complementarity between information and competition, so that both the socially efficient and the auctioneer’s optimal choice of precision increase {{with the number of}} bidders, and both converge as the number of bidders goes to infinity...|$|E
40|$|Precision, {{reliability}} and cost {{are the major}} criteria applied in optimization and design of geodetic networks. The terrestrial networks are being replaced quickly by permanent and campaign Global Positioning System (GPS) networks. These networks must be optimized using the same three criteria. In this article the optimization of the observational plan of local GPS networks (Second Order Design (SOD)) is considered using the <b>precision</b> <b>criterion.</b> This study {{is limited to the}} selection of optimal numbers and the best distribution of the non-trivial baselines throughout the network. This objective is accomplished based on the SOD solution through the analytical method in operational research by the means of quadratic programming algorithm. This presented method is tested on a real GPS network and appears to be a useful technique in terms of cost reduction in the field work by the provided observational plan and optimal distribution of the baselines throughout the network. Results indicate that weights of almost 36 % of the baselines are negligiblewhen compared to the weights {{of the rest of the}} baselines; therefore, they could be eliminated fromthe observational plan, resulting in a 36 % saving in the fieldwork cost...|$|R
40|$|Key words: {{standard}} error, {{coefficient of}} variation, dissemination In {{the case of}} sample surveys, the standard error could still be considered {{as the most important}} quality indicator from the user’s point of view. Therefore it is an important obligation and responsibility of the producer of statistics to provide at least some basic information about the level of the standard error together with the disseminated statistics. At the Statistical Office of the Republic of Slovenia the system for marking the statistical results with lower level of precision has been used for a long time. In this system the marks were designated exclusively {{on the basis of the}} coefficient of variation. Values in single or double brackets were used as a mark for the result with lower level of precision. The results with the coefficient of variation higher than the critical value (usually 30 %) were not disseminated at all. In 2006 the system was subject of the substantial revision and the new system was fully implemented in 2007. The paper describes the new strategy together with the plans for its implementation. An essential part of the paper is devoted to the discussion about the adequacy of the coefficient of variation as a <b>precision</b> <b>criterion</b> for the different types of statistics...|$|R
40|$|One of {{existing}} challenges in personalization {{of the web}} is increasing the efficiency of a web in meeting the users' requirements for the contents they require in an optimal state. All the information associated with the current user behavior following in web and data obtained from pervious users’ interaction in web can provide some necessary keys to recommend presentation of services, productions, and the required information of the users. This study aims at presenting a formal model based on colored Petri nets to identify the present user's interest, which is utilized to recommend the most appropriate pages ahead. In the proposed design, recommendation of the pages is considered with respect to information obtained from pervious users' profile {{as well as the}} current session of the present user. This model offers the updated proposed pages to the user by clicking on the web pages. Moreover, an example of web is modeled using CPN Tools. The results of the simulation show that this design improves the precision factor. We explain, through evaluation where the results of this method are more objective and the dynamic recommendations demonstrate that the results of the recommended method improve the <b>precision</b> <b>criterion</b> 15 % more than the static method...|$|R
