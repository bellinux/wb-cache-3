1728|1127|Public
25|$|Positive {{results are}} much more likely to be false if the <b>prior</b> <b>probability</b> of the claim under test is low.|$|E
25|$|The PCFG is used {{to predict}} the <b>prior</b> <b>probability</b> {{distribution}} of the structure whereas posterior probabilities are estimated by the inside-outside algorithm and the most likely structure is found by the CYK algorithm.|$|E
25|$|To sum up this formula: the {{posterior}} probability of the hypothesis {{is equal to}} the <b>prior</b> <b>probability</b> of the hypothesis multiplied by the conditional probability of the evidence given the hypothesis, divided by the probability of the new evidence.|$|E
40|$|Classification of {{multispectral}} data {{by the use}} of a maximum likelihood classifier is dependent upon knowing in advance a set of <b>prior</b> <b>probabilities.</b> Therefore, the selection of an optimal set of <b>prior</b> <b>probabilities</b> is critical to the estimation of proportions for each class. In the proposed procedure, a function is minimized to yield a set of optimal <b>prior</b> <b>probabilities</b> for a specific data set. Classification results using optimal, actual, and default (equal <b>prior</b> <b>probabilities</b> for each class) values are compared...|$|R
50|$|Estimates of <b>prior</b> <b>probabilities</b> are {{particularly}} suspect. Estimates will be constructed {{that do not}} follow any consistent frequency distribution. For this reason <b>prior</b> <b>probabilities</b> are considered as estimates of probabilities rather than probabilities.|$|R
30|$|There are {{two ways}} of {{assessing}} the prior and conditional probabilities: objective-based <b>prior</b> <b>probabilities</b> and subjective-based <b>prior</b> <b>probabilities,</b> which should be used {{depending on whether the}} probability distribution of the occurrence of the factors can be obtained from the data. The objective-based <b>prior</b> <b>probabilities</b> magnify the uncertainty of the occurrence of the events. Therefore, as the RCI cases being analysed are collected from the Legislation Council in Hong Kong, the <b>prior</b> and conditional <b>probability</b> analyses are conducted based on the subjective method.|$|R
25|$|This {{contrasts}} with the Bayesian approach, which requires that the hypothesis be assigned a <b>prior</b> <b>probability,</b> which is revised {{in the light of}} the observed data to obtain the final probability of the hypothesis. Within the Bayesian framework there is no risk of error since hypotheses are not accepted or rejected; instead they are assigned probabilities.|$|E
25|$|The Bayesian Kepler {{periodogram}} is {{a mathematical}} algorithm, {{used to detect}} single or multiple extrasolar planets from successive radial-velocity measurements of the star they are orbiting. It involves a Bayesian statistical analysis of the radial-velocity data, using a <b>prior</b> <b>probability</b> distribution over the space determined {{by one or more}} sets of Keplerian orbital parameters. This analysis may be implemented using the Markov chain Monte Carlo (MCMC) method.|$|E
25|$|Fallacy of the {{transposed}} conditional, aka prosecutor's fallacy: criticisms arise {{because the}} hypothesis testing approach forces one hypothesis (the null hypothesis) to be favored, since {{what is being}} evaluated is probability of the observed result given the null hypothesis and not probability of the null hypothesis given the observed result. An alternative to this approach is offered by Bayesian inference, although it requires establishing a <b>prior</b> <b>probability.</b>|$|E
5000|$|... {{controls}} {{the density of}} [...] Values significantly above 1 cause a dense vector where all states will have similar <b>prior</b> <b>probabilities.</b> Values significantly below 1 cause a sparse vector where only a few states are inherently likely (have <b>prior</b> <b>probabilities</b> significantly above 0).|$|R
5000|$|Generate {{attribute}} values {{based on}} user-supplied <b>prior</b> <b>probabilities.</b>|$|R
30|$|For {{both the}} {{implicit}} and explicit shape models, <b>prior</b> <b>probabilities</b> {{were considered to}} be the same. In the case of three class classification, we can observe that sedans are misclassified. We can alleviate this problem and improve the results by using higher <b>prior</b> <b>probabilities</b> for the sedan class.|$|R
25|$|In Bayesian inference, {{the beta}} {{distribution}} is the conjugate <b>prior</b> <b>probability</b> {{distribution for the}} Bernoulli, binomial, negative binomial and geometric distributions. For example, the beta distribution {{can be used in}} Bayesian analysis to describe initial knowledge concerning probability of success such as the probability that a space vehicle will successfully complete a specified mission. The beta distribution is a suitable model for the random behavior of percentages and proportions.|$|E
25|$|The {{difference}} between two models is that hierarchical model can explicitly make causal inference to predict certain stimulus while non-hierarchical model can only predict joint probability of stimuli. However, hierarchical model {{is actually a}} special case of non-hierarchical model by setting joint prior as a weighted average of the prior to common and independent causes, each weighted by their <b>prior</b> <b>probability.</b> Based on the correspondence of these two models, we can also say that hierarchical is a mixture modal of non-hierarchical model.|$|E
25|$|Random {{sampling}} {{by using}} lots {{is an old}} idea, mentioned {{several times in the}} Bible. In 1786 Pierre Simon Laplace estimated the population of France by using a sample, along with ratio estimator. He also computed probabilistic estimates of the error. These were not expressed as modern confidence intervals but as the sample size that would be needed to achieve a particular upper bound on the sampling error with probability 1000/1001. His estimates used Bayes' theorem with a uniform <b>prior</b> <b>probability</b> and assumed that his sample was random. Alexander Ivanovich Chuprov introduced sample surveys to Imperial Russia in the 1870s.|$|E
50|$|Bayes's theorem {{is named}} after Rev. Thomas Bayes 1701-1761. Bayesian {{inference}} broadened the application of probability to many situations where a population was not well defined. But Bayes' theorem always depended on <b>prior</b> <b>probabilities,</b> to generate new probabilities. It was unclear where these <b>prior</b> <b>probabilities</b> should come from.|$|R
40|$|A mixed-integer {{programming}} model (MIP) incorporating <b>prior</b> <b>probabilities</b> for the two-group discriminant {{problem is}} presented. Its classificatory performance is compared against that of Fisher's linear discrimininant function (LDF) and Smith's quadradic discriminant function (QDF) for simulated data from normal and nonnormal populations for different settings of the <b>prior</b> <b>probabilities</b> of group membership. The proposed model {{is shown to}} outperform both LDF and QDF for most settings of the <b>prior</b> <b>probabilities</b> when the data are generated from nonnormal populations but underperforms the parametric models for data generated from normal populations...|$|R
5000|$|Controversy {{of using}} <b>prior</b> <b>probabilities.</b> Using <b>prior</b> <b>probabilities</b> for Bayesian {{analysis}} {{has been seen}} by many as an advantage as it will provide a hypothesis a more realistic view of the real world. However some biologists argue about the subjectivity of Bayesian posterior probabilities after the incorporation of these priors.|$|R
25|$|Reverse {{inference}} {{demonstrates the}} logical fallacy of affirming {{what you just}} found, although this logic could be supported by instances where a certain outcome is generated solely by a specific occurrence. With regard to the brain and brain function it is seldom that a particular brain region is activated solely by one cognitive process. Some suggestions to improve the legitimacy of reverse inference have included both increasing the selectivity of response in the brain region of interest and increasing the <b>prior</b> <b>probability</b> of the cognitive process in question. However, Poldrack suggests that reverse inference should be used merely {{as a guide to}} direct further inquiry rather than a direct means to interpret results.|$|E
25|$|Applications whose goal is {{to create}} a system that generalizes well to unseen examples, face the {{possibility}} of over-training. This arises in convoluted or over-specified systems when the capacity of the network significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and optimally select hyperparameters to minimize the generalization error. The second is to use some form of regularization. This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger <b>prior</b> <b>probability</b> over simpler models; but also in statistical learning theory, where the {{goal is to}} minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.|$|E
25|$|One approach, {{suggested}} by {{writers such as}} Stephen D. Unwin, is to treat (particular versions of) theism and naturalism {{as though they were}} two hypotheses in the Bayesian sense, to list certain data (or alleged data), about the world, and to suggest that the likelihoods of these data are significantly higher under one hypothesis than the other. Most of the arguments for, or against, the existence of God can be seen as pointing to particular aspects of the universe in this way. In almost all cases it is not seriously {{suggested by}} proponents of the arguments that they are irrefutable, merely that they make one worldview seem significantly more likely than the other. However, since an assessment of the weight of evidence depends on the <b>prior</b> <b>probability</b> that is assigned to each worldview, arguments that a theist finds convincing may seem thin to an atheist and vice versa.|$|E
30|$|Problem dependency—Initial CPTs and <b>prior</b> <b>probabilities</b> {{defined as}} uniform distributions.|$|R
30|$|Evolution dependency—BEL(p_i^t) used as <b>prior</b> <b>probabilities</b> for {{the next}} generation.|$|R
5000|$|If the <b>prior</b> <b>probabilities</b> are all {{the same}} the probabilities are, ...|$|R
2500|$|Bayesian {{probability}} specifies {{that there}} is some <b>prior</b> <b>probability.</b> Bayesian statisticians can use both an objective and a subjective approach when interpreting the <b>prior</b> <b>probability,</b> which is then updated in light of new relevant information. The concept is a manipulation of conditional probabilities: ...|$|E
2500|$|... is the <b>prior</b> <b>probability,</b> is {{the initial}} {{degree of belief}} in A.|$|E
2500|$|... {{that the}} item {{selected}} was defective {{enables us to}} replace the <b>prior</b> <b>probability</b> ...|$|E
40|$|An {{adaptive}} detection {{scheme for}} M hypotheses was analyzed. It {{was assumed that}} the probability density function under each hypothesis was known, and that the <b>prior</b> <b>probabilities</b> of the M hypotheses were unknown and sequentially estimated. Each observation vector was classified using the current estimate of the <b>prior</b> <b>probabilities.</b> Using a set of nonlinear transformations, and applying stochastic approximation theory, an optimally converging adaptive detection and estimation scheme was designed. The optimality of the scheme {{lies in the fact}} that convergence to the true <b>prior</b> <b>probabilities</b> is ensured, and that the asymptotic error variance is minimum, for the class of nonlinear transformations considered. An expression for the asymptotic mean square error variance of the scheme was also obtained...|$|R
5000|$|In two {{problems}} {{where we have}} the same prior information we should assign the same <b>prior</b> <b>probabilities</b> ...|$|R
50|$|Each {{probability}} {{is always}} {{associated with the}} state of knowledge at a particular point in the argument. Probabilities before an inference are known as <b>prior</b> <b>probabilities,</b> and probabilities after are known as posterior probabilities.|$|R
2500|$|Beta {{distributions}} [...] {{provide a}} family of <b>prior</b> <b>probability</b> distributions for binomial distributions in Bayesian inference: ...|$|E
2500|$|The use of Bayesian {{probability}} involves specifying a <b>prior</b> <b>probability.</b> This may {{be obtained}} from consideration of whether the required <b>prior</b> <b>probability</b> is greater or lesser than a reference probability associated with an urn model or a thought experiment. [...] The issue is that for a given problem, multiple thought experiments could apply, and choosing one {{is a matter of}} judgement: different people may assign different prior probabilities, known as the reference class problem.|$|E
2500|$|Given data [...] and {{parameter}} , {{a simple}} Bayesian analysis {{starts with a}} <b>prior</b> <b>probability</b> (prior) [...] and likelihood [...] to compute a posterior probability [...]|$|E
40|$|In many signal {{detection}} and classification problems, we have {{knowledge of the}} distribution under each hypothesis, but not the <b>prior</b> <b>probabilities.</b> This paper is aimed at providing theory to quantify the performance of detection via estimating <b>prior</b> <b>probabilities</b> from either labeled or unlabeled training data. The error or risk is considered {{as a function of}} the <b>prior</b> <b>probabilities.</b> We show that the risk function is locally Lipschitz in the vicinity of the true <b>prior</b> <b>probabilities,</b> and the error of detectors based on estimated <b>prior</b> <b>probabilities</b> depends on the behavior of the risk function in this locality. In general, we show that the error of detectors based on the Maximum Likelihood Estimate (MLE) of the <b>prior</b> <b>probabilities</b> converges to the Bayes error at a rate of n^- 1 / 2, where n is the number of training data. If the behavior of the risk function is more favorable, then detectors based on the MLE have errors converging to the corresponding Bayes errors at optimal rates of the form n^-(1 +α) / 2, where α> 0 is a parameter governing the behavior of the risk function with a typical value α = 1. The limit α→∞ corresponds to a situation where the risk function is flat near the true probabilities, and thus insensitive to small errors in the MLE; in this case the error of the detector based on the MLE converges to the Bayes error exponentially fast with n. We show the bounds are achievable no matter given labeled or unlabeled training data and are minimax-optimal in labeled case. Comment: Submitted to IEEE Transactions on Information Theor...|$|R
40|$|The {{widely used}} Bayesian {{classifier}} {{is based on}} the assumption of equal <b>prior</b> <b>probabilities</b> for all the classes. However, inclusion of equal <b>prior</b> <b>probabilities</b> may not guarantee high classification accuracy for the individual classes. Here, we propose a novel technique-Hybrid Bayesian Classifier (HBC) -where the class <b>prior</b> <b>probabilities</b> are determined by unmixing a supplemental low spatial-high spectral resolution multispectral (MS) data that are assigned to every pixel in a high spatial-low spectral resolution MS data in Bayesian classification. This is demonstrated with two separate experiments-first, class abundances are estimated per pixel by unmixing Moderate Resolution Imaging Spectroradiometer data to be used as <b>prior</b> <b>probabilities,</b> while posterior probabilities are determined from the training data obtained from ground. These have been used for classifying the Indian Remote Sensing Satellite LISS-III MS data through Bayesian classifier. In the second experiment, abundances obtained by unmixing Landsat Enhanced Thematic Mapper Plus are used as <b>priors,</b> and posterior <b>probabilities</b> are determined from the ground data to classify IKONOS MS images through Bayesian classifier. The results indicated that HBC systematically exploited the information from two image sources, improving the overall accuracy of LISS-III MS classification by 6 % and IKONOS MS classification by 9 %. Inclusion of <b>prior</b> <b>probabilities</b> increased the average producer's and user's accuracies by 5. 5 % and 6. 5 % in case of LISS-III MS with six classes and 12. 5 % and 5. 4 % in IKONOS MS for five classes considered...|$|R
50|$|The {{normative}} {{method for}} integrating base rates (<b>prior</b> <b>probabilities)</b> and featural evidence (likelihoods) {{is given by}} Bayes' rule.|$|R
