6|17|Public
40|$|Since multi-view {{video is}} a {{collection}} of videos captured by a multiple camera array, the volume of data is huge. Various algorithms have been developed for multi-view video coding. In this paper, we propose two methods: efficient view interpolation and ‘VIP <b>P-picture</b> ’ coding. The view interpolation method includes initial disparity estimation, variable block-based matching, and pixel-level disparity estimation operations. The ‘VIP P-picture’ coding method is an additional motion estimation process at the encoder. The proposed view interpolation method improves objective quality of generated images up to about 1 ~ 4 dB, and the ‘VIP <b>P-picture</b> ’ coding method increases the average coding gain about 0. 66 dB in well synthesized sequences. Index Terms — multi-view video coding, disparity, view interpolation 1...|$|E
40|$|Only {{a limited}} number of methods have been {{proposed}} to realize heterogeneous transcoding, for example from MPEG- 2 to H. 263, or from H. 264 to H. 263. The major difficulties of transcoding a B-picture to a <b>P-picture</b> are that the incoming discrete cosine transform (DCT) coefficients of the B-frame are prediction errors arising from both forward and backward predictions, whilst the prediction errors in the DCT domain arising from the prediction using the previous frame alone are not available. The required new prediction errors need to be re-estimated in the pixel domain. This process involves highly complex computation and introduces re-encoding errors. We propose a new approach to convert a B-picture into a <b>P-picture</b> by making use of some properties of motion compensation in the DCT domain and the direct addition of DCT coefficients. We derive a set of equations and formulate {{the problem of how to}} obtain the DCT coefficients. One difficulty is that the last P-frame inside a GOP with an IBBP structure, for example, needs to be transcoded to become the last P-frame in the IPPP structure, and it has to be linked to the previous reconstructed P-frame instead of to the I-frame. We increased the speed of the transcoding process by making use of the motion activity which is expressed in terms of the correlation between pictures. The whole transcoding process is done in the transform domain, hence re-encoding errors are completely avoided. Results from our experimental work show that the proposed video transcoder not only achieves a speed-up of two to six times that of the conventional video transcoder, but it also substantially improves the quality of the video. Department of Electronic and Information Engineerin...|$|E
40|$|The {{forthcoming}} multimedia {{telecommunication services}} {{are expected to}} use pre-encoded videos for storage and transmission. The heterogeneities of the present communication networks and the clients ’ devices demand matching the encoding format of the video source to {{the constraints of the}} networks and clients ’ devices. In order to provide quality support services to heterogeneous clients or transmission channels, the video server should have the capability of performing heterogeneous transcoding, which is regarded as a process of converting a previously compressed video bitstream into another bitstream with a different format. However, much investigation has been conducted to focus on homogeneous transcoding. Only a limited number of methods have been proposed to realize the heterogeneous transcoding from MPEG- 2 to H. 263. The major difficulties for transcoding B-picture to <b>P-picture</b> is that the incoming DCT coefficients are predicted from the forward and backward prediction. In other words, the DCT coefficients predicted from the previous frame is not available. Therefore, the required new prediction errors need to be re-estimated in the pixel domain. This process involves high computational complexity as well as introduces re-encoding errors. Motivated by this, we propose a new transcoder architecture to convert a B-picture into a <b>P-picture</b> by making use of the techniques of motion compensation in the DCT domain and indirect addition of DCT coefficients. In this paper, we derive a set of equations and formulate {{the problem of how to}} obtain the DCT coefficients. Experimental results show that the proposed video transcoder achieve a better performance as compared to the conventional video transcoder in terms of both quality and complexity. Furthermore, we propose a fast algorithm to speed up the transcoding process making use of the correlation of motion activities between pictures...|$|E
30|$|The {{effect on}} the coding {{efficiency}} depends on the characteristic of the sequence. At sequences with relative low motion, for example, Mobile orTempete, the coding efficiency in terms of PSNR increases with a higher number of nonreferenced <b>p-pictures</b> whereas at sequences with relative high motion, for example, Football, the coding efficiency decreases {{with the number of}} <b>p-pictures.</b> The Foreman or Paris sequences show a similar coding efficiency at different numbers of <b>p-pictures.</b>|$|R
40|$|A motion compression/reconstruction method {{based on}} max t-norm {{composite}} fuzzy relational equations (MCF) is proposed, where a motion sequence {{is divided into}} intra-pictures (I-pictures) and predictive-pictures (<b>P-pictures).</b> The I-pictures and the <b>P-pictures</b> are compressed by using uniform coders and non-uniform coders, respectively. A design method of non-uniform coders is proposed to perform eective compression/reconstruction of the <b>P-pictures</b> based on an overlap level of fuzzy sets and a fuzzy equalization. An experiment using 10 <b>P-pictures</b> conrms that the root means square error of the proposed method is decreased to 89 : 4 % of {{that one of the}} uniform coders under the condition that compression rate is 0. 0057. Two test motions ('Tennis' and 'Woman', 100 frames) are compressed and reconstructed by the proposed MCF...|$|R
3000|$|Let us {{consider}} a video stream of a GOP structure with {{a period of}} nine video pictures (one leading I-picture followed by eight <b>P-pictures)</b> as a typical example. Since every I-picture is referenced for decoding its subsequent <b>P-pictures,</b> none of I-packets should be dropped until the congestion level exceeds the limit of physical queue (q [...]...|$|R
3000|$|SVA-based MVC {{experiments}} are implemented on the JMVM 7.0 reference software with seven multiview video sequences and their ROI masks, Ballet, Breakdancers, Doorflowers, Alt Moabit, Pantomime, Champagne tower, and Dog, {{to evaluate the}} effectiveness of the proposed SVA-based bit allocation. The MVC-HBP prediction structure is adopted for MVC simulation. Eight views and GOP Length are 15, fast motion/disparity estimation is enabled, and search range is 64. There are three kinds of picture in the MVC-HBP prediction structure: intracoded picture (I-picture), interpredicted picture (<b>P-picture),</b> and hierarchical bidirectional predicted picture (B-picture). In the coding experiment, all B- and P-pictures are coded with regional bit allocation optimization and I-pictures are coded with original MVC scheme without bit allocation optimization. The bQP is set as 12, 17, 22, 27, or 32, and the QPs of background and ROI are set according to (16) and obtain optimal [...]...|$|E
40|$|This paper {{proposes a}} {{flexible}} <b>P-picture</b> (FlexP) coding technique, where P-pictures are selectively used as references for other pictures. For a given reference bit-rate, the proposed FlexP technique {{can improve the}} quality of references by both allocating more bits to the selected P-pictures and reducing the number of non-selected Ppictures. In case that the reference bits for each selected Ppicture are fixed, the proposed technique is able to reduce the reference bit-rate by selecting fewer P-pictures as references, meanwhile limit drifting error at low bit-rates. Therefore, the proposed FlexP technique provides more flexibility on bandwidth adaptation, as well as achieves a good trade-off between high coding efficiency and low drifting error at the enhancement layer especially when the R-D rate allocation is enabled. The experimental results show that the fine-granular scalable video coding with the proposed technique can gain more than 1. 0 dB...|$|E
40|$|This paper {{presents}} a simple bitrate control method {{to prevent the}} abrupt quality degradation after scene change. After scene change, the quality degradation occurs due to the poor temporal prediction between pictures before and after scene change. We predict the coding complexity of picture using the spatial variance before DCT and spectral flatness measure. From the predicted coding complexity, we show that the rate-distortion relation of image can be approximated to exponential function. When scene changes, picture target bit is adjusted in the direction to minimize the distortion in a GOP using the rate-distortion relations for each <b>P-picture.</b> Since the bit shortage could be occurred, proposed method extends the current GOP to the next. The algorithm {{can be applied to}} the existing MPEG codecs and real-time applications easily. Compared with the MPEG- 2 TM 5 rate control algorithm, proposed algorithm shows 0. 52. 5 dB gain in PSNR and a small fluctuation in quality after scene change. [...] ...|$|E
3000|$|With the {{assumption}} that the loss of nonreferenced <b>p-pictures</b> is tolerable (cf. Section 5), we can compare the med quality coverage of the ML scheme with the max quality coverage of the SL scheme. That is, the [...] "UEP 0.7 / 0.9 @ [...]...|$|R
30|$|As {{already shown}} in the {{preceding}} section, some parts of a media stream can be considered more important than other parts; for example, referenced pictures (I- and P-) can be considered more important than nonreferenced pictures (<b>p-pictures).</b> Hence, {{it is reasonable to}} subdivide the media stream into two or more layers accordingly and increase the protection of the more important layers, for example, the layer containing I- and <b>P-pictures,</b> as compared to the less important layers. Such schemes are denoted as ML transmission schemes. To increase the reliability of the more important layer (base layer) while keeping the transmission cost constant, such an increase in reliability requires a decrease in reliability of the less important layer(s). In order to support streaming with different quality layers per service stream over 3 GPP MBMS, the following methods could be used to differentiate the protection for the different layers.|$|R
30|$|We use an {{objective}} quality measure, which reflects in a comprehensible way {{the behavior of}} the introduced temporal scalability with a very simple and intuitive measure. We define four quality categories: maximum (max), medium (med), minimum (min), and unacceptable. The thresholds of the quality categories were selected in such a manner, that each of category reflects a certain user experience. A media stream sorted in the max quality has hardly any errors. Received media streams of the med quality can have a reduced frame rate, but hardly any prediction errors or audio outages. That is, in this category we assume that the loss of nonreferenced <b>p-pictures</b> is tolerable for the user because it only results in a reduced frame rate but does not affect any prediction errors. The min quality tolerates the loss of all <b>p-pictures</b> and some prediction losses and audio outages. The rest of the simulated media streams, which do not fit in one of the aforementioned categories, are sorted in the unacceptable category.|$|R
40|$|Motion-compensated {{prediction}} {{based on}} multiple previous or future frames {{can enhance the}} compression efficiency of video coding. Multi-frame prediction can be applied as an extension to <b>P-Pictures,</b> but also to B-Pictures {{in the form of}} multi-hypothesis prediction. We review recent advances, several of which have been embraced by the ITU-T Rec. H. 263 and the emerging JVT/H. 26 L standard, jointly developed by ITU-T and ISO/IEC MPEG. Already, JVT/H. 26 L video outperforms MPEG- 2 by a factor greater than 2 X. These recent developments could {{play a significant role in}} future digital set-top boxes...|$|R
40|$|Adapting the {{resolution}} of motion compensated prediction in a video codec is considered in this paper. A new motion search and signaling scheme for increasing the accuracy of prediction without affecting the complexity of encoding is proposed. It involves using common information available to both encoder and decoder, e. g. current slice type, texture in reference pictures, etc as a cue to control motion vector accuracy and an efficient reuse of motion information for predicting subsequent blocks. An average bit rate reduction of around 2. 5 % for <b>P-pictures</b> and 0. 5 % for B-pictures is observed with no extra search compared to a fixed quarter-sample resolution...|$|R
3000|$|The Peak-Signal-to Noise Ratio (PSNR) measure, {{which is}} derived from mean square {{distortion}} between original video sequence and the reconstructed video sequence, {{is the most popular}} objective quality measure used in the area of video coding. Although we use PSNR for evaluation of the encoding results in Section 3, PSNR cannot show properly the effect of prediction errors (lost I- and <b>P-pictures),</b> which might be highly visible but create small distortion in terms of mean square error, or even of reduced frame rate by the loss of nonreferenced <b>p-pictures.</b> And it is obviously not suitable to represent outages in the audio play-out. Much work has been done in the area of objective video quality assessment (VQA) to find an appropriate measure. A survey of this research area is given by Engelke and Zepernick [20]. This work distinguishes between full-reference method reduced-reference method and nonreference method, where [...] "reference" [...] indicates the original, unimpaired video sequence. The full referenced methods use the original sequence as reference to predict the quality degradation of the distorted medium. The reduced-reference methods send additional information of the original reference along with the video sequence which can be used for the quality assessment. The nonreference methods or [...] "blind" [...] methods rate the video quality without any information of the original reference. In [21], Lotfallah et al. present a reduced-reference method, where they try to estimate the experienced video quality using information of the coding structures, mainly of the prediction range, and the position of the lost frame in the Group Of Picture (GOP). Similar approach is presented in [22], where Da Silva et al. present a [...] "Pseudo subjective Quality Assessment" [...] approach [23]. In this work, they calculate an objective measure based on the loss rate of different frame types, respectively I-, P-, and B- pictures. However, both approaches do not take the audio loss rate into account and are not sufficient to rate the behavior of the temporal scalability. In [24], a nonreference method for audio/video streaming is presented, which estimates the audio-visual quality based on information about the used audio and video codecs, encoding bit rates, packet loss rates, and duration of potential rebuffering events. Its targeted application is quality monitoring for multimedia services in 3 G networks; thus both video and audio are considered. However the approach is not tuned to consider the effects of temporal scalability or loss of different frame types. For our quality assessment, we use a measurement that is related to [21, 22, 24], picking up the ideas of weighting the loss rate of different frame types (I-, P-, and <b>p-pictures</b> and audio-frames), the prediction length of the video coding structure, and the PSNR measure.|$|R
30|$|Increasing {{the number}} of <b>p-pictures</b> also {{increases}} the scalability ratio in terms of bit rate and frame rate. Depending on the sequence the IpP coding allows for up to 33 % bit rate reduction, which means that 33 % of the total bit rate lies in the temporal enhancement layer. The frame rate can be reduced by half. Using IppP allows for bit rate reduction of slightly over 50 %, whereas the frame rate {{can be reduced to}} one-third of the full frame rate. The IpppP coding structure can offer two temporal enhancement layers. Dropping the first temporal layer results in similar scalability features as observed with IpP coding. Additional dropping the second temporal layer allows a bit rate reduction up to 60 % with the football sequence and a frame rate reduction down to one-fourth of the full sequence frame rate.|$|R
40|$|The {{objective}} of rate control in video coding is {{to utilize the}} available communication bandwidth to transmit the highest quality video. This paper presents an innovative rate control algorithm for MPEG- 1 / 2 encoders to improve the video quality at sudden and gradual scene transitions. The algorithm works in three steps. The fixed encoding structure of the MPEG data stream is disturbed by forcing an extension or termination of the last GOP of the first scene such that the first anchor picture of the second scene becomes an I-coded picture. <b>P-pictures</b> are provided with additional bits {{to compensate for the}} inefficient forward prediction during gradual scene transitions. The complexity parameters of each picture type are corrected with an iterative algorithm, Experimental results show that with the proposed algorithm, both objective and subjective quality of MPEG video can be improved significantly during scene transitions...|$|R
40|$|Mobile digital TV environments demand {{flexible}} {{video compression}} like {{scalable video coding}} (SVC) because of varying bandwidths and devices. Since existing infrastructures highly rely on H. 264 /AVC video compression, network providers could adapt the current H. 264 /AVC encoded video to SVC. This adaptation {{needs to be done}} efficiently to reduce processing power and operational cost. This paper proposes two techniques to convert an H. 264 /AVC bitstream in Baseline (<b>P-pictures</b> based) and Main Profile (B-pictures based) without scalability to a scalable bitstream with temporal scalability as part of a framework for low-complexity video adaptation for digital TV broadcasting. Our approaches are based on accelerating the interprediction, focusing on reducing the coding complexity of mode decision and motion estimation tasks of the encoder stage by using information available after the H. 264 /AVC decoding stage. The results show that when our techniques are applied, the complexity is reduced by 98 % while maintaining coding efficiency...|$|R
40|$|Since its {{emerging}} in mid 90 ’s, MPEG- 2 [1] {{has been widely}} accepted by the digital video industry. There have been huge amount of video content stored in MPEG- 2 format. MPEG- 4 [2] is the latest video coding standard out of MEPG targeted at network video applications. The Simple Profile (SP) of MPEG- 4 is often used in practical applications. Thus, it is often necessary to perform MPEG- 2 to MPEG- 4 SP transcoding for the transmission of MPEG- 2 coded video content over networks. Such transcoding usually requires hybrid spatial/temporal resolution down-sampling. Since MPEG- 4 SP does not support bi-directional prediction, the B-pictures in the MPEG- 2 bit-stream {{need to be changed}} into <b>P-pictures.</b> In addition, the transcoder {{needs to be able to}} deal with interlaced input video supported by MPEG- 2 standard. In this paper, we propose techniques to perform motion vector re-estimation that effectively handles the hybrid spatial/temporal resolution reduction with picture type conversion, as well as the interlaced MEPG- 2 input video. I...|$|R
30|$|In this work, {{we propose}} {{transmission}} schemes for 3 GPP MBMS Rel. 6 {{allowing for a}} graceful degradation with H. 264 /AVC Baseline Profile Temporal Scalability using unequal error protection, radio transmission power spreading, {{or a combination of}} both. We implemented a test system simulating a 3 GPP MBMS Rel. 6 compliant transmission including application layer FEC using the systematic Raptor code. We introduce quality categories for video and audio picking up the ideas of weighting the loss rate of different frame types (I-, P-, <b>p-pictures</b> and audio-frame), the prediction length of the video coding structure, and the PSNR measure. The simulation results show that in principle graceful degradation can be applied to 3 GPP MBMS. Using graceful degradation a higher number of users are provided with an acceptable media quality. As expected the number of users receiving perfect media quality is reduced. The results are strongly dependent on the bit rate distribution between the layers. In case of an even bit rate distribution between base and enhancement layer the most significant gains are observed. In case of uneven bit rate distribution gains are lower or not present. Future work could involve the use of temporal scalability together with a hierarchical prediction coding structure.|$|R
30|$|Recall {{from the}} PWD design {{that the concept}} of EPL-based {{priority}} weighting has been demonstrated to be successful, and the above inequality was slightly modified to be ωAC_VI ≧ ωAAC_VI, where the equality holds only when non-real-time I-packets (from AAC_VI) encounter real-time P-packets of least importance (from AC_VI), e.g. P 8 -packets in the case of Fig.  2 a. Also recall that the importance levelling of EPL is linearly descending from the leading I-picture to its subsequent <b>P-pictures</b> within a given GOP, and the values of ωAC_VI are proportional to the corresponding EPL values and equally spaced among the range [0.5, 1]. Meanwhile, the normalisation condition ωAAC_VI = 1 − ωAC_VI constraints the values of ωAAC_VI to be within [0, 0.5]. Hence, it is difficult in general for non-real-time I-packets to compete with any type of real-time packets, and the best chance is to get an equal weighting factor when a non-real-time I-packet encounters a real-time P-packet of least importance at the head-of-line. Obviously, this could be the weakness of PWD because I-packets with long delays can induce much more serious error propagation than any subsequent P-packets within the same GOP in terms of the decoded video quality at the video receiver when exceeding the playback deadline.|$|R
30|$|Finally, for {{temporal}} scalability, in 2008, a transcoding method from an H. 264 /AVC P-picture-based bitstream to an SVC bitstream {{was presented}} in [23] by Dziri et al. In this approach, the H. 264 /AVC bitstream was transcoded to two layers of <b>P-pictures</b> (one with reference pictures {{and the other with}} non-reference ones). Then, this bitstream was transformed to an SVC bitstream by syntax adaptation. In 2010, Al-Muscati and Labeau [24] proposed another technique for transcoding that provided temporal scalability. The method presented was applied in baseline profile and reused information from the mode decision and ME processes from the H. 264 /AVC stream. During that year, we presented an H. 264 /AVC to SVC video transcoder that efficiently reuses some motion information of the H. 264 /AVC decoding process {{in order to reduce the}} time consumption of the SVC encoding algorithm by reducing the motion estimation process time. The approach was developed for main profile [25] and dynamically adapted for several temporal layers. Later, in 2011, the previous algorithm was adjusted for the baseline profile and P frames [26]. In 2012, Yeh et al. proposed another technique [27] for transcoding from H. 264 /AVC to SVC using probability models and Markov chains, and we presented another work [28, 29] focusing on accelerating the mode decision algorithm, while our previous approaches focused only on motion estimation process. The present work is an extension of these last ones.|$|R
40|$|Abstract—Since {{variable}} block-size {{motion compensation}} (MC) and rate-distortion optimization (RDO) techniques are adopted in H. 264 /MPEG- 4 AVC, modes and motion vectors (MVs) in input stream {{can no longer}} be reused equivalently efficient over a wide range of bit rate in transcoded streams. This paper proposes a new RDO model to maintain good coding efficiency and greatly reduce computation of the H. 264 /MPEG- 4 AVC transcoding, in which the distortion caused by motion and mode changes is not calculated directly from the sum of absolute difference (SAD) or the sum of square difference (SSD) between source signals and interpolated prediction signals. Instead, distortion is directly estimated from MV variation and the power spectrum (PS) of the prediction signal generated from input stream. The proposed RDO model can be applied to both the pixel-domain transcoding and the transform-domain transcoding even when coded signals are not reconstructed at all. Furthermore, the techniques as to derive the Lagrangian multiplier in the proposed model are developed in respective pixel- and transform-domains. Additionally, we propose an H. 264 /MPEG- 4 transcoding scheme that demonstrates the advantage of the proposed RDO model in terms of peak signal-to-noise ratio and transcoding speed, in which <b>P-pictures</b> are transcoded in the pixel domain for achieving reconstructed high quality and B-pictures are transcoded in the transform domain for high-transcoding speed. Index Terms—H. 264 /MPEG- 4 AVC, picture power spectrum (PS), rate-distortion optimization (RDO), video coding, video transcoding. I...|$|R

