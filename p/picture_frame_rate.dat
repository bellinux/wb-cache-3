2|9329|Public
50|$|The Panavision HD-900F is {{the first}} digital high {{definition}} camera, able to record using the standard motion <b>picture</b> <b>frame</b> <b>rate</b> of 24 frames per second. It {{is the result of}} a collaboration of Panavision and Sony In 1997.|$|E
40|$|Abstract — In {{the first}} part of this paper, we derive a source model {{describing}} the relationship between bits, distortion, and quantization step size for transform coders. Based on this source model, a variable frame rate coding algorithm is developed. The basic idea is to select a proper <b>picture</b> <b>frame</b> <b>rate</b> to ensure a minimum picture quality for every frame. Because our source model can predict approximately the number of coded bits when a certain quantization step size is used, we could predict the quality and bits of coded images without going through the entire realcoding process. Therefore, we could skip the right number of picture frames to accomplish the goal of constant image quality. Our proposed variable frame rate coding schemes are simple but quite effective as demonstrated by simulation results. The results of using another variable frame rate scheme, Test Model for H. 263 (TMN- 5), and the results of using a fixed frame rate coding scheme, Reference Model 8 for H. 261 (RM 8), are also provided for comparison. Index Terms — Image coding, rate distortion theory, source coding. I...|$|E
50|$|A {{frame is}} the total <b>picture.</b> The <b>frame</b> <b>rate</b> {{is the number of}} {{pictures}} displayed in one second. But each frame is actually scanned twice interleaving odd and even lines. Each scan is known as a field (odd and even fields.) So field rate is twice the <b>frame</b> <b>rate.</b> In each <b>frame</b> there are 405 lines (or 202.5 lines in a field.) So line rate (line frequency) is 405 times the frame frequency or 405•25=10125 Hz.|$|R
50|$|Some of the {{important}} specs are listed below.A frame is the total <b>picture.</b> The <b>frame</b> <b>rate</b> {{is the number of}} pictures displayed in one second. But each frame is actually scanned twice interleaving odd and even lines. Each scan is known as a field (odd and even fields.) So field rate is twice the <b>frame</b> <b>rate.</b> In each <b>frame</b> there are 625 lines (or 312.5 lines in a field.) So line rate (line frequency) is 625 times the frame frequency or 625•25=15625 Hz.|$|R
5000|$|The 9000-W-M was, for {{all intents}} and purposes, a custom pre-HDTV video system. The 655 line system was also used for 24 frame {{playback}} on TVs and monitors used on movie studio sets. Thus the TVs had no flicker when shot on film, due to the different (and thus compatible to motion <b>picture</b> film's) <b>frame</b> <b>rate.</b> The 9000-W-M was used for some [...] "JAWS 3D's" [...] composite special effects.|$|R
5000|$|As {{the term}} {{is used in the}} standard, a [...] "level" [...] is a {{specified}} set of constraints that indicate a degree of required decoder performance for a profile. For example, a level of support within a profile specifies the maximum <b>picture</b> resolution, <b>frame</b> <b>rate,</b> and bit rate that a decoder may use. A decoder that conforms to a given level must be able to decode all bitstreams encoded for that level and all lower levels.|$|R
50|$|The {{system was}} {{developed}} for VHF band (part of RF band lower than 300 MHz.) Some of the important specs are listed below.A frame is the total <b>picture.</b> The <b>frame</b> <b>rate</b> {{is the number of}} pictures displayed in one second. But each frame is actually scanned twice interleaving odd and even lines. Each scan is known as a field (odd and even fields.) So field rate is twice the <b>frame</b> <b>rate.</b> In each <b>frame</b> there are 625 lines (or 312.5 lines in a field.) So line rate (line frequency) is 625 times the frame frequency or 625•25=15625 Hz.|$|R
50|$|ITU-R Recommendation BT.2020, more {{commonly}} {{known by the}} abbreviations Rec. 2020 or BT.2020, defines various aspects of ultra-high-definition television (UHDTV) with standard dynamic range (SDR) and wide color gamut (WCG), including <b>picture</b> resolutions, <b>frame</b> <b>rates</b> with progressive scan, bit depths, color primaries, RGB and luma-chroma color representations, chroma subsamplings, and an opto-electronic transfer function. The first version of Rec. 2020 was posted on the International Telecommunication Union (ITU) website on August 23, 2012, and two further editions have been published since then.|$|R
40|$|In this demonstration, {{we show an}} energy-aware video {{streaming}} system which allows users to play back video for the specified duration within the remaining battery amount. In the system, we execute a proxy server on an intermediate node in the network. It receives the video stream from a content server, transcodes it to the videos with appropriate quality, and forwards it to a PDA or a laptop PC. Here, suitable parameter values of the video (such as <b>picture</b> size, <b>frame</b> <b>rate</b> and bitrate) which enable playback for the specified duration are automatically calculated on the proxy using our battery consumption model. The system also allows users to play back video segments with different qualities based on the importance specified to each video segment. ...|$|R
50|$|Caption blocks are {{inserted}} {{after the}} sequence and GOP headers, so each block is {{for one second}} of video which would end up being one or two long lines or three to four short lines of text. Also that means if the caption_block_count is greater than 30 then the block contains both interleaved caption fields and one could devise the <b>framing</b> <b>rate</b> from the caption_block_count. However since the data is grouped together the <b>framing</b> <b>rate</b> will almost always be 30/1.001, unlike the ATSC method that inserts one byte pair for each field after the <b>picture</b> header making <b>framing</b> <b>rates</b> of 24/1.001 possible for HD content. Since when a decoder does a 3:2 pull-down for NTSC output the captions will remain in sync.|$|R
40|$|MDM' 06 : 7 th International Conference on Mobile Data Management, May 9 - 12, 2006, Nara, JapanIn this demonstration, {{we show an}} energy-aware video {{streaming}} system which allows users to play back video for the specified duration within the remaining battery amount. In the system, we execute a proxy server on an intermediate node in the network. It receives the video stream from a content server, transcodes it to the videos with appropriate quality, and forwards it to a PDA or a laptop PC. Here, suitable parameter values of the video (such as <b>picture</b> size, <b>frame</b> <b>rate</b> and bitrate) which enable playback for the specified duration are automatically calculated on the proxy using our battery consumption model. The system also allows users to play back video segments with different qualities based on the importance specified to each video segment...|$|R
5000|$|At its {{introduction}} in October 2000, {{features of the}} software included [...] "proprietary media delivery technology", a contact list to monitor online and offline status and to enable a one-click video chat session, control of <b>picture</b> size, <b>frame</b> <b>rate,</b> and audio performance, and other [...] "media quality attributes". Services available for a paid subscription were to include [...] "multi-party video conferencing -- up to six people, video messaging -- record, send, and play video messages, and web-based access -- initiate video chat session and view video messages from any browser." [...] At the time, Eyeball Networks stated that it intended [...] "to charge fees for certain types of use of the Eyeball Chat System, and {{for the use of}} some of the features of the system" [...] but in 2009, this was replaced by a requirement that large institutional users should buy one of the company's [...] "enterprise products and/or solutions." ...|$|R
40|$|In this paper, {{we propose}} a new video {{delivery}} method called MTcast (Multiple Transcode based video multicast) which achieves efficient simultaneous video delivery to multiple users with different quality requirements {{by relying on}} user nodes to transcode and forward video to other user nodes. In MTcast, each user specifies a quality requirement for a video consisting of bitrate, <b>picture</b> size and <b>frame</b> <b>rate</b> based on the user's environmental resource limitation. All users can receive video with the specified quality (or near this quality) along a single delivery tree. The main characteristics of MTcast are in its scalability, high user satisfaction degree in received video quality, short startup latency and robustness against node failure. Through simulations, we have confirmed that MTcast can achieve much higher user satisfaction degree and robustness against node failure than the layered multicast method...|$|R
40|$|Abstract. In this paper, {{we propose}} a new video {{delivery}} method called MTcast (Multiple Transcode based video multicast) which achieves efficient simultaneous video delivery to multiple users with different quality requirements {{by relying on}} user nodes to transcode and forward video to other user nodes. In MTcast, each user specifies a quality requirement for a video consisting of bitrate, <b>picture</b> size and <b>frame</b> <b>rate</b> based on the user’s environmental resource limitation. All users can receive video with the specified quality (or near this quality) along a single delivery tree. The main characteristics of MTcast are in its scalability, high user satisfaction degree in received video quality, short startup latency and robustness against node failure. Through simulations, we have confirmed that MTcast can achieve much higher user satisfaction degree and robustness against node failure than the layered multicast method. Keyword:video multicast, transcode, QoS, service overlay networks...|$|R
40|$|OPODIS 2005 : International Conference On Principles Of Distributed Systems, Dec 12 - 14, 2005, Pisa, ItalyIn this paper, {{we propose}} a new video {{delivery}} method called MTcast (Multiple Transcode based video multicast) which achieves efficient simultaneous video delivery to multiple users with different quality requirements {{by relying on}} user nodes to transcode and forward video to other user nodes. In MTcast, each user specifies a quality requirement for a video consisting of bitrate, <b>picture</b> size and <b>frame</b> <b>rate</b> based on the user’s environmental resource limitation. All users can receive video with the specified quality (or near this quality) along a single delivery tree. The main characteristics of MTcast are in its scalability, high user satisfaction degree in received video quality, short startup latency and robustness against node failure. Through simulations, we have confirmed that MTcast can achieve much higher user satisfaction degree and robustness against node failure than the layered multicast method...|$|R
50|$|The maximum <b>frame</b> <b>rate</b> {{depends on}} the {{bandwidth}} of the electronics and the transmission system, {{and the number of}} horizontal scan lines in the image. A <b>frame</b> <b>rate</b> of 25 or 30 hertz is a satisfactory compromise, while the process of interlacing two video fields of the <b>picture</b> per <b>frame</b> is used to build the image. This process doubles the apparent number of video frames per second and further reduces flicker and other defects in transmission.|$|R
40|$|Nowadays, video {{streaming}} application {{is widely used}} in wired and wireless environment. Extending this application into Wireless sensor network (WSN) applications featuring low data rate transmission, low energy consumption, ease of deployment and low cost has attracted lots of attention in the research community. However, video transmission over such network is more challenging {{because of the large}} amount of bandwidth required. To cater this problem, video compression is of utmost importance to decrease the amount of bandwidth required over WSN. MPEG- 4 video codec is one of the compression scheme that was identified to be suitable for WSN environment. In this paper, a simulation study for MPEG- 4 video encoding scheme based on an experimental model was carried out to determine conformance with IEEE 802. 15. 4 requirements. The results obtained from this paper would be used as a benchmark for the configuration of the video encoding scheme for WSN applications. There are three parameters that we are concerned with in this experiment, which are quantization scale, group of <b>picture</b> (GOP) and <b>frame</b> <b>rate</b> (fps). The results from this simulation study shows that an optimal selection of the parameters value that enhances the video transmission over WSN...|$|R
40|$|Scalable Video Coding (SVC) differs form {{traditional}} {{single point}} approaches mainly {{because it allows}} to encode in a unique bit stream several working points corresponding to different quality, <b>picture</b> size and <b>frame</b> <b>rate.</b> This work describes the current state-of-the-art in SVC, focusing on wavelet based motion-compensated approaches (WSVC). It reviews individual components that {{have been designed to}} address the problem over the years and how such components are typically combined to achieve meaningful WSVC architectures. Coding schemes which mainly differ from the space-time order in which the wavelet transforms operate are here compared, discussing strengths and weaknesses of the resulting implementations. An evaluation of the achievable coding performances is provided considering the reference architectures studied and developed by ISO/MPEG in its exploration on WSVC. The paper also attempts to draw a list of major differences between wavelet based solutions and the SVC standard jointly targeted by ITU and ISO/MPEG. A major emphasis is devoted to a promising WSVC solution, named STP-tool, which presents architectural similarities with respect to the SVC standard. The paper ends drawing some evolution trends for WSVC systems and giving insights on video coding applications which could benefit by a wavelet based approach...|$|R
50|$|The <b>picture</b> <b>frame</b> {{may contain}} a pane of <b>picture</b> <b>framing</b> glass or an acrylic glass {{substitute}} such as acrylite or plexiglas {{to protect the}} picture. In some instances where the art in the frame is dispensable or durable, no protection may be necessary. Glass is common over watercolors and other artwork on paper, but rare over oil paintings, except very valuable ones in some museums. <b>Picture</b> <b>framing</b> glass may be treated with anti-reflective coatings to make the glass virtually invisible under certain lighting conditions. When a <b>picture</b> <b>frame</b> {{is expected to be}} exposed to direct sunlight, or harsh lighting conditions such as fluorescent lights, UV filtering may be added to slow down the photocatalytic degradation of organic materials behind <b>picture</b> <b>framing</b> glass.|$|R
2500|$|Series 5 saw the <b>picture</b> <b>frame</b> {{being used}} {{less than in}} {{previous}} series and in addition, there were attempts to implement numerous tricks with the <b>picture</b> <b>frame,</b> including firing gunge and pushing out small objects like bouquets of flowers. e.g. when Huw Edwards was in the <b>picture</b> <b>frame</b> he said [...] "News just in, this just out" [...] before the person operating the mouth hosed gunge out of his mouth.|$|R
25|$|Diasec is {{a process}} which uses acrylic glass as a {{substitute}} for normal glass in <b>picture</b> <b>frames.</b> This is done for its relatively low cost, light weight, shatter-resistance, aesthetics and because it can be ordered in larger sizes than standard <b>picture</b> <b>framing</b> glass.|$|R
5000|$|In the <b>picture</b> <b>framing</b> industry, {{a fillet}} (also {{referred}} to as a slip) is a small piece of moulding which fits inside a larger frame or, typically, underneath or in between matting, used for decorative purposes. The <b>picture</b> <b>framing</b> term is probably related to, though not necessarily derived from, the engineering term, which it is frequently pronounced similarly to; however, unlike the use of fillets in mechanical engineering, the use of [...] "fillets" [...] in <b>picture</b> <b>frames</b> is wholly decorative.|$|R
50|$|Common {{applications}} include <b>picture</b> <b>frames,</b> pipes, and molding.|$|R
5000|$|... #Caption: Reproductions {{of antique}} gilded <b>picture</b> <b>frame</b> {{mouldings}} ...|$|R
5000|$|<b>Picture</b> <b>frames</b> have {{traditionally}} been made of wood, which {{is still the most}} common material, although other materials are used including silver, bronze, aluminum, and plastics such as polystyrene. A <b>picture</b> <b>frame</b> may be of any color or texture, but gilding is common, especially on older wooden <b>frames.</b> Some <b>picture</b> <b>frames</b> have elaborate molding which may relate to the subject matter. Complicated older frames are often made of moulded and gilded plaster over a plain wood base. <b>Picture</b> <b>frames</b> {{come in a variety of}} profiles, but generally the lengths of moulding feature a [...] "lip" [...] and rabbet, the function of which is to allow a space to hold in the materials in the frame. The lip extends usually about a quarter of an inch past the edge of the rabbet.|$|R
50|$|Present day products, include crosses, earrings, {{brooches}} and <b>picture</b> <b>frames.</b>|$|R
5000|$|Larson-Juhl, {{manufacturer}} of <b>picture</b> <b>frames,</b> a subsidy of Berkshire Hathaway ...|$|R
50|$|Lux Corporation {{was founded}} in Japan on June 1925, by T. Hayakawa and his brother K. Yoshikawa. The company began as the radio {{equipment}} department of Kinsuido <b>Picture</b> <b>Frame</b> Store in Osaka, until then only an importer of <b>picture</b> <b>frames,</b> and was founded {{just ahead of the}} first radio broadcast that year.|$|R
40|$|International Symposium on Intelligent Signal Processing and Communication Systems, ISPACS 2003 This paper {{presents}} an architectural enhancement for reducing memory requirements of MPEG video coding based on incremental memory sharing between the reconstructed <b>picture</b> <b>frames.</b> The method exploits the temporal locality of block-based hybrid coding by dynamically replacing the processed macroblocks {{of the previously}} reconstructed <b>picture</b> <b>frame</b> with macroblocks of a newly reconstructed <b>picture</b> <b>frame.</b> Simulation results show that using this method we can reduce the total memory size by a 13 %- 32 % for bidirectional prediction and almost half for unidirectional prediction schemes without any impact on image quality and throughput...|$|R
50|$|Davis, Deborah, “Julius Lowy Turns 100,” <b>Picture</b> <b>Framing</b> Magazine, January 2007.|$|R
50|$|The Guild sets {{standards}} and guidelines for prints and <b>picture</b> <b>framing.</b>|$|R
5000|$|... #Caption: The Girl in a <b>Picture</b> <b>Frame,</b> 1641, Royal Castle, Warsaw ...|$|R
5000|$|... #Caption: A {{band clamp}} used to clamp a mitered <b>picture</b> <b>frame.</b>|$|R
5000|$|Mirror and Picture FactoryC. V. Thornley & Co. <b>Picture</b> <b>Framing</b> Supplies ...|$|R
5000|$|Linear Animation Generator {{is a form}} of {{animation}} by using static <b>picture</b> <b>frames</b> installed in a tunnel or a shaft. The animation illusion is created by putting the viewer in a linear motion, parallel to the installed <b>picture</b> <b>frames.</b> The concept and the technical solution were invented in 2007 by Mihai Girlovan in Romania.|$|R
5000|$|... #Caption: Detail of gilt papier-mâché {{as applied}} to an English <b>picture</b> <b>frame</b> ...|$|R
