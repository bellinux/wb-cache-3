358|2079|Public
25|$|Council Administration Block, 1966. Tibor K.Donner architect. Considered a <b>positive</b> <b>example</b> of 1950s modernism, which {{contrasts}} with the 1911 Town Hall {{on the far side}} of Aotea Square. Is considered to be Auckland's first 'skyscraper'.|$|E
25|$|A <b>positive</b> <b>example</b> {{may be a}} {{clinician}} {{who uses}} an interview technique {{as well as a}} specific questionnaire to determine if a patient has mental illness and has better success at determining mental illness than a clinician who uses the interview technique alone. Thus, the specific questionnaire would be considered incrementally valid. Because the questionnaire in conjunction with the interview produced more accurate determinations, and added information for the clinician, the questionnaire is incrementally valid.|$|E
25|$|Sun Quan mourned Chen Wu's {{death and}} {{attended}} the latter's funeral. Sun Quan also had Chen Wu's favourite concubine sacrificed to join Chen in death, and he awarded Chen's family 200 taxable households in their estate. The historian Sun Sheng criticised Sun Quan's act of forcing Chen Wu's concubine to join Chen in death, citing an earlier negative example of Duke Mu of Qin and a <b>positive</b> <b>example</b> of Wei Ke (魏顆).|$|E
30|$|Recall: Recall {{indicates}} {{the ratio of}} the true examples found from the predicted <b>positive</b> <b>examples,</b> that is out of n <b>positive</b> <b>examples</b> how many <b>positive</b> <b>examples</b> were found. Unlike precision, the recall does not have that big of a difference when all the feature sets are added, only a difference of 0.0058 is observed.|$|R
40|$|Inference {{of formal}} {{languages}} from <b>positive</b> <b>examples</b> {{is more difficult}} than inference from <b>positive</b> and negative <b>examples.</b> The entire class of regular languages can be identified in the limit from <b>positive</b> and negative <b>examples,</b> but not from <b>positive</b> <b>examples</b> alone (Gold, 1967). Angluin introduced an algorithm that efficiently identifies k-reversible regular languages for k _ 0, {{a subset of the}} class of regular languages, from <b>positive</b> <b>examples</b> (Angluin, 1982). Sakakibara defined a subset of context-free grammars that can be characterized as zero-reversible and extended Angluin's algorithm to learn them from <b>positive</b> structural <b>examples</b> (Sakakibara, 1992). We define...|$|R
5000|$|Operationalize(Literal, <b>Positive</b> <b>examples,</b> Negative examples) ...|$|R
25|$|The Mark Messier Leadership Award is a National Hockey League (NHL) award {{that recognizes}} an {{individual}} as a superior leader within their sport, {{and as a}} contributing member of society. The award is given to a player selected by Hockey Hall of Fame center Mark Messier to honor an individual who leads by <b>positive</b> <b>example</b> through on-ice performance, motivation of team members and a dedication to community activities and charitable causes. It was first awarded during 2006–07 NHL season and sponsored by Cold-fX.|$|E
25|$|The machine {{learning}} community most often uses the ROC AUC statistic for model comparison. However, this practice {{has recently been}} questioned based upon new {{machine learning}} research that shows that the AUC is quite noisy as a classification measure and has some other significant problems in model comparison. A reliable and valid AUC estimate {{can be interpreted as}} the probability that the classifier will assign a higher score to a randomly chosen <b>positive</b> <b>example</b> than to a randomly chosen negative example. However, the critical research suggests frequent failures in obtaining reliable and valid AUC estimates. Thus, the practical value of the AUC measure has been called into question, raising the possibility that the AUC may actually introduce more uncertainty into machine learning classification accuracy comparisons than resolution. Nonetheless, the coherence of AUC as a measure of aggregated classification performance has been vindicated, in terms of a uniform rate distribution, and AUC has been linked to a number of other performance metrics such as the Brier score.|$|E
25|$|A {{review meeting}} {{was held in}} May 2008, {{approximately}} 8 months after the restrictions began, which was attended by the Director of Liquor Licensing {{and members of the}} Aboriginal community. Oscar said the meeting was ‘the most important mins of our lives’. The views of people who attended were that women now felt more empowered, confident and able to participate in community-level discussions, the Valley was a much more quiet and safe place to live, and other Aboriginal communities had noticed the <b>positive</b> <b>example</b> set in the Valley. Alcohol restrictions had encouraged government and non-government agencies to become more involved, a strong desire to not return to the chaos of pre-restriction times prevailed, and change needed to be substantial and long lasting. The people felt priority needed to be given to children’s health and welfare, and they wanted the next generation of children to be raised without alcohol affecting their lives, families were stronger and sober, old people were being cared for, young people were thinking about buying homes, and children were learning new skills. They also believed sly grogging had become an issue, communities with people suffering from FASD needed help, and if the restrictions were lifted, all of the confidence that now existed in the community would be ‘stripped away’.|$|E
40|$|Context-free grammars {{cannot be}} {{identified}} in the limit from <b>positive</b> <b>examples</b> (Gold, 1967), yet natural language grammars are more powerful than context-free grammars and humans learn them with remarkable ease from <b>positive</b> <b>examples</b> (Marcus, 1993). Identifiability results for formal languages ignore a potentially powerful source of information available to learners of natural languages, namely, meanings. This paper explores the learnability of context-free grammars given <b>positive</b> <b>examples</b> and lexical semantics. That is, the learner has {{a representation of the}} meaning of each lexical item...|$|R
5000|$|Add Operationalize(L, <b>Positive</b> <b>examples,</b> Negative examples) to OperationalLiterals ...|$|R
30|$|We {{address the}} problem of finding a general pattern that covers <b>positive</b> <b>examples</b> as much as {{possible}} and does not cover negative examples as much as possible. Therefore, it is desirable to diversify patterns to cover all the <b>positive</b> <b>examples</b> as much as possible, in the course of evolutionary computation. We introduce a consistency-based feature selection algorithm, super-CWC [19] to ensure this diversity with a small number of individuals. This algorithm constructs a minimal set of features by which <b>positive</b> <b>examples</b> are consistently discriminated from negative examples as much as possible.|$|R
500|$|Iaukea died in Honolulu, on March 5, 1940, {{at the age}} of 84. He {{was buried}} at the Oahu Cemetery. As the last {{surviving}} court member of the defunct monarchy, Iaukea was regarded as an important authority on the past during his lifetime but also as a <b>positive</b> <b>example</b> of those who adapted to the changing politics of the islands. His great-great-granddaughter Sydney Lehua Iaukea noted: ...|$|E
500|$|Several of Donna's pivotal episodes were {{selected}} as recommended viewing by critics. [...] "Myself, Coming Back", the series ten episode {{in which she}} goes on a road-trip and discovers the existence of her niece, Mia, was named a televisual highlight by the Daily Mirror, Birmingham Post, and Liverpool Daily Echo, with the latter publication deeming it [...] "refreshingly different" [...] following the [...] "doom and gloom" [...] of preceding episodes. Many regional newspapers named [...] "Promises", in which she decides to adopt Mia, a [...] "pick of the day", and Sarah Morgan of the Daily Record cited Jacobs' appearance as Donna in the premiere episode of Casualty twenty-fifth series as a <b>positive</b> <b>example</b> of the BBC [...] "pull {{out all the stops}} to make sure the first edition in this latest run is something of a cracker." ...|$|E
500|$|The {{character}} Miles Morales {{was first}} reported by USA Today on August 2, 2011, {{shortly before the}} character officially debuted in Ultimate Fallout #4. The announcement received international coverage {{in the mainstream media}} and was met with mixed reactions by audiences. Chris Huntington of The New York Times lauded the creation of Morales, relating that it gave his adopted Ethiopian son Dagim a superhero who looks like him. Some fans and commentators felt the decision was an attempt by Marvel Comics to be politically correct and that the introduction of a minority Spider-Man was simply a publicity stunt to attract more readers, while others felt that a person of color as Spider-Man would set a <b>positive</b> <b>example</b> for minority readers, particularly children. Many Spider-Man fans were disappointed that Peter Parker was killed, regardless of who replaced him. The wide-ranging critical reception prompted The Washington Post to run an article called, [...] "Sorry, Peter Parker. The response to the black Spider-Man shows why we need one", in which writer Alexandra Petri wrote that the character should be judged on the quality of its stories rather than on his appearance or ethnicity.|$|E
40|$|In this paper, {{we study}} inferability of Prolog {{programs}} from <b>positive</b> <b>examples</b> alone. We define {{a class of}} Prolog programs called recursion bounded programs that can capture non-linear relationships between inputs and outputs and yet inferable from <b>positive</b> <b>examples.</b> This class is rich enough to include many programs like append...|$|R
40|$|AbstractWe {{consider}} {{the problem of}} learning deterministic even linear languages from <b>positive</b> <b>examples.</b> We show that, for any nonnegative integer k, the class of LR(k) even linear languages is not learnable from <b>positive</b> <b>examples</b> {{while there is a}} subclass called LRS(k), which is a natural subclass of LR(k) in the strong sense, learnable from <b>positive</b> <b>examples.</b> Our learning algorithm identifies this subclass in the limit with almost linear time in updating conjectures. As a corollary, in terms of even linear grammars, we have a learning algorithm for k-reversible languages that is more efficient than the one proposed by Angluin...|$|R
40|$|Recently, several wrapper {{induction}} algorithms for structured documents {{have been}} introduced. They {{are based on}} contextual tree languages and learn from <b>positive</b> <b>examples</b> only but have the disadvantage that they need parameters. To obtain the optimal parameter setting, they use precision and recall. This goes in fact beyond learning from <b>positive</b> <b>examples</b> only. In this paper, a parameter estimation method for a wrapper based on (k, l) contextual tree languages is introduced that is solely based on a few <b>positive</b> <b>examples.</b> Experiments show {{that the quality of}} the wrappers is very close to that of wrappers with the optimal parameter setting. ...|$|R
500|$|Holby City {{was praised}} by campaigners for the Royal National Institute for Deaf People (RNID) in October 2003, when an episode which coincided with [...] "Learn To Sign Week" [...] used deaf actors, and {{featured}} characters communicating through British Sign Language. RNID chief executive John Low stated: [...] "Too often individuals {{have to rely}} on family members or friends to communicate complicated personal information to professionals. This is the reason the RNID is calling on the government to channel funding into the training of British Sign Language interpreters who could then be available to NHS staff treating deaf patients." [...] Stokes commented: [...] "The writer had a great story he wanted to tell – for us, that's what matters first and foremost." [...] A 2008 report into ethnic diversity on television, commissioned by Channel 4, cited Holby City as a <b>positive</b> <b>example</b> of [...] "diverse British programm". Five years previously in 2003, former BBC host Sir Ludovic Kennedy complained that ethnic minorities were over-represented on television, prompting a BBC spokeswoman to explain that Holby City has more ethnic characters as it is set in an area where minorities account for up to 30% of the population. According to the 2001 census, the population of Bristol – which the city of Holby is loosely based upon – is 88% white and 12% ethnic minorities.|$|E
500|$|In the 2009–10 NHL season, Crosby tied Tampa Bay Lightning centre Steven Stamkos for {{the lead}} in goals scored, with 51 goals, earning the Rocket Richard Trophy. He also {{garnered}} 58 assists {{for a total of}} 109 points, enough to tie with Alex Ovechkin for second in league points, trailing only the Vancouver Canucks' Henrik Sedin's 112. Crosby was also named a finalist for the Hart Memorial Trophy and Ted Lindsay Award. Crosby won the Mark Messier Leadership Award, getting recognized as a [...] "superior leader within the sport, setting a <b>positive</b> <b>example</b> through on-ice performance, motivation of team members and a dedication to the community". This was the second time he had received this honour, the other being in January 2007, during the award's first year when it was presented monthly. Crosby's Penguins were defeated {{in the second round of}} the 2010 Stanley Cup playoffs, losing to the Montreal Canadiens in seven games. Crosby had 19 points in 13 games in the playoffs, though through seven games against the Canadiens he had only 1 goal and 4 assists for a total of 5 points. Game seven was also the last game to be played at Mellon Arena, the Penguins' home rink since the start of the franchise. On July 27, 2010, Crosby joined his mentor Mario Lemieux to be the first to skate on the new ice at the Consol Energy Center. The two skated for about five minutes before being joined on the ice by a group of young hockey fans all wearing Lemieux's 66 or Crosby's 87 jerseys.|$|E
2500|$|In July 2004, the United States Senate Select Committee on Intelligence {{issued a}} scathing report on prewar {{intelligence}} on Iraq. INR was spared the poor performance review that most other intelligence agencies received, and the panel specifically endorsed the dissent that INR {{inserted into the}} National Intelligence Estimate of 2002. [...] The bureau is being studied as a <b>positive</b> <b>example,</b> as Congress debates how to best reform U.S. intelligence agencies {{in the wake of}} the 2003 invasion of Iraq.|$|E
5000|$|Schema: <b>positive</b> <b>examples</b> + {{negative}} examples + {{background knowledge}} &rArr; hypothesis.|$|R
5000|$|Compute {{information}} gain of the clause over <b>Positive</b> <b>examples</b> and Negative examples ...|$|R
40|$|In {{the problem}} of {{learning}} with <b>positive</b> and unlabeled <b>examples,</b> existing research all assumes that <b>positive</b> <b>examples</b> P and the hidden <b>positive</b> <b>examples</b> in the unlabeled set U are generated from the same distribution. This assumption may be violated in practice. In such cases, existing methods perform poorly. This paper proposes a novel technique A-EM {{to deal with the}} problem. Experimental results with product page classification demonstrate the effectiveness of the proposed technique...|$|R
2500|$|On February 7, 2012, Arava Power {{announced}} that it had received a license for the Tarabin Solar Field, the first solar field for the Bedouin community. Financing for the $30 million Tarabin installation is to be provided by OPIC – the Overseas Private Investment Corporation of the United States Government. [...] Arava Power President Yosef Abramowitz sees solar power for the Bedouin as a <b>positive</b> <b>example</b> for Native Americans, First Nations, Aboriginals and others with historic land claims.|$|E
2500|$|Răutu's {{contribution}} as a propagandist {{was entirely}} absent from official reference {{works such as}} the 1978 biographical dictionary of Romanian historiography. Although highly decorated and commended as a <b>positive</b> <b>example,</b> the Agitprop Section founder was generally introduced as a dedicated [...] "party activist", a communist powerhouse rather than a national instructor: while honoring him with the Star of the Socialist Republic, Ceaușescu made sure to remind the audience that Răutu's history {{had its share of}} [...] "minuses and unfulfilled chapters".|$|E
2500|$|In Genesis 39, a <b>positive</b> <b>example</b> is {{presented}} in Joseph, one of Jacob’s twelve sons. He is sold into slavery in Egypt and quickly rises to a prominent and successful position managing the household of Potiphar, a military captain. [...] He resists sexual advances from Potiphar’s wife “day after day,” protesting {{that he does not}} wish to betray Potiphar’s trust. [...] One day her advances become physical, and in his effort to escape, Joseph leaves his cloak behind. Potiphar’s wife uses this ‘evidence’ to falsely accuse Joseph of attempted rape and he is imprisoned, losing all but his life. [...] More than two years later Joseph is restored to an even higher position serving Pharaoh himself.|$|E
5000|$|Inputs Literal to be operationalized, List of <b>positive</b> <b>examples,</b> List of {{negative}} examples ...|$|R
40|$|Abstract. Many {{real-world}} classification applications {{fall into}} the class of positive and unlabeled (PU) learning problems. In many such applications, not only could the negative training examples be missing, the number of <b>positive</b> <b>examples</b> available for learning may also be fairly limited due to the impracticality of hand-labeling {{a large number of}} training examples. Current PU learning techniques have focused mostly on identifying reliable negative instances from the unlabeled set U. In this paper, we address the oft-overlooked PU learning problem when the number of training <b>examples</b> in the <b>positive</b> set P is small. We propose a novel technique LPLP (Learning from Probabilistically Labeled <b>Positive</b> <b>examples)</b> and apply the approach to classify product pages from commercial websites. The experimental results demonstrate that our approach outperforms existing methods significantly, even in the challenging cases where the <b>positive</b> <b>examples</b> in P and the hidden <b>positive</b> <b>examples</b> in U were not drawn from the same distribution. ...|$|R
40|$|Learning with <b>positive</b> <b>examples</b> only occurs very {{frequently}} in natural learning. But learning theories have always encountered {{a lot of}} difficulties with these situations. While learning with <b>positive</b> <b>examples</b> has been extensively studied in Gold framework, it does not exist satisfactory generalization of PAC learning model to the general case of positive-only examples. We make two proposals: the first one assumes that the learner has available an oracle with "two buttons" which respectively provide positive and unlabeled examples; the second one supposes that the class of possible distributions is restricted (to the uniform distribution or to the Solomonoff-Levin distribution). These two models are tested with the class of k-DNF. We show that k-DNF are learnable when the learner has the possibility to draw <b>positive</b> <b>examples</b> and unlabeled examples. We show also that k-DNF are learnable when the learner has uniquely the possibility to draw <b>positive</b> <b>examples</b> according to the uniform dis [...] ...|$|R
2500|$|Paul advises Timothy that {{he should}} not drink water only, but should use a little wine {{for the sake of his}} stomach and {{frequent}} infirmities. Some have suggested this advice is particularly in reference to purifying low quality drinking water, while others suggest it was simply intended to help his digestion and general sickliness. Abstentionists generally regard this passage as a <b>positive</b> <b>example</b> of abstention from wine and see Paul's instructions as exceptional and purely for the sake of health, while other interpreters suggest that Timothy was [...] "upright in his aims" [...] but here guilty of an [...] "excess of severity" [...] or that he felt inappropriately bound by a Hellenistic custom that younger men should not drink.|$|E
2500|$|Despite these {{successes of}} the public company, the International Monetary Fund and the World Bank insisted about {{establishing}} a public-private partnership for water supply in Burkina Faso during the late 1990s. A 10-year lease contract with a private French company signed previously in Senegal was cited as one <b>positive</b> <b>example.</b> Nevertheless, the government rejected the option of a lease contract, which was considered too far-reaching without completely rejecting {{all the elements of}} the sector reforms initiated in some of the neighboring West African countries. The government thus adopted only those aspects that it considered a good fit for the conditions of Burkina Faso. In that vein the government initiated sector reforms in 2001 to achieve what it called “a genuine public-public partnership between the Government and ONEA.” ...|$|E
2500|$|Willow's {{religion}} and sexuality {{have made her}} {{a role model for}} audiences. Whedon, however, has compared her Jewish identity to her sexuality, stating that they are rarely made a significant focus of the show. Willow at times reminds the other characters of her religion, wondering what her father might think of the crucifixes she must apply to her bedroom wall to keep out vampires, and commenting that Santa Claus misses her house every Christmas because of the [...] "big honkin' menorah". Buffy essayist Matthew Pateman criticizes the show for presenting Willow's Jewish identity only when it opposes Christian declarations of holidays and other traditions. The New York Times, however, named her as a <b>positive</b> <b>example</b> of a depiction of a Jewish woman, who stood out among portrayals of Jews as harsh, unfeminine, and shallow. Producer Gail Berman states that as a Jew, Willow [...] "handles herself just fine, thank you".|$|E
40|$|Concepts {{that can}} be {{expressed}} as solutions to multilinear pseudo boolean equations with a bounded degree are shown to be learnable in polynomial time from <b>positive</b> <b>examples.</b> This implies the leamability from <b>positive</b> <b>examples</b> of many families of boolean formulae by a unified algorithm. Some of these formulae were not previously known to be learnable, and some {{were known to be}} learn-able by different algorithms...|$|R
40|$|Abstract—In this paper, we conduct two word {{learning}} ex-periments {{to study}} the human word learning and generalization behaviour. The participants are shown abstract figures that form the <b>positive</b> <b>examples</b> of a word concept. All the <b>positive</b> <b>examples</b> have a common defining feature. The participants are then asked whether the word applies to various test examples. We vary several independent variables across our two word learning experiments. Our {{results show that the}} word generalization behaviour is based on the similarity of a test <b>example</b> with the <b>positive</b> <b>examples</b> of a word. The generalization behaviour is not based on the defining features. This is true even when enough examples exist from which the defining features can be inferred...|$|R
40|$|Marchand and Shawe-Taylor (2002) have {{proposed}} a loss bound for the set covering machine that has the property {{to depend on the}} observed fraction of <b>positive</b> <b>examples</b> and on what the classifier achieves on the <b>positive</b> training <b>examples.</b> We show that this loss bound is incorrect. We then propose a loss bound, valid for any sample-compression learning algorithm (including the set covering machine), that depends on the observed fraction of <b>positive</b> <b>examples</b> and on what the classifier achieves on them. We also compare numerically the loss bound proposed in this paper with the incorrect bound, the original SCM bound and a recently proposed loss bound of Marchand and Sokolova (2005) (which does not depend on the observed fraction of <b>positive</b> <b>examples)</b> and show that the latter loss bounds can be substantially larger than the new bound in the presence of imbalanced misclassifications...|$|R
