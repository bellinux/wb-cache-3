25|4|Public
40|$|Abstract. Dynamic {{composition}} of web services requires an automated step of contracting, i. e., the computation of a <b>possibly</b> <b>fruitful</b> interaction between two (or more) services, {{based on their}} policies and goals. In previous work, the SCIFF abductive logic language was used to represent the services ’ policies, and the associated proof procedure to perform the contracting. In this paper, we build on that work in order to exploit {{the results of the}} Description Logics research area to represent domain specific knowledge, either by importing the knowledge encoded in an ontology into a SCIFF knowledge base, or by interfacing the SCIFF proof procedure to an existing ontological reasoner. ...|$|E
40|$|This paper proposes an {{evolutionary}} approach for searching information collections in a distributed environment. This approach, which {{is given a}} set of information collections and user queries, finds the best possible matching between the information collections and user queries by {{taking into account the}} factors from both user queries perspective and collections’ perspective. The contribution of each factor in a searching model is indicated by a weighted parameter derived by a genetic algorithm. The values of these parameters are used to determine the contribution of the corresponding factors to the effectiveness of selection. Some preliminary results are demonstrated. Although inconclusive, they provide directions for <b>possibly</b> <b>fruitful</b> research and applications...|$|E
40|$|This paper {{provides}} {{a review of}} empirical research on the link between financial intermediation by banks and economic growth. Special {{attention is paid to}} the issues of causality, non-linearity, time perspective, financial intermediation proxies, and interaction terms. The review shows that there are still quite a few unresolved issues in empirical research, which causes scepticism towards prioritizing financial sector policies in order to cause economic growth. Progress in the finance and growth literature is slow and researchers seem to go round in circles. A <b>possibly</b> <b>fruitful</b> direction for future empirical research is the relationship between government and banks, especially from the standpoint of political economy. financial intermediation, banks, economic growth...|$|E
5000|$|Since its inception, {{it has now}} {{sponsored}} a [...] "Crowdmath" [...] project in collaboration with MIT PRIMES program and the Art of Problem Solving. This project is built upon the same idea of the Polymath project that massive collaboration in mathematics is possible and <b>possibly</b> quite <b>fruitful.</b> However, this is specifically aimed at only {{high school and college}} students with a goal of creating [...] "a specific opportunity for the upcoming generation of math and science researchers." [...] The problems are original research and unsolved problems in mathematics. All high school and college students from around the world with advanced background of mathematics are encouraged to participate. Older participants are welcomed to participate as mentors and encouraged not to post solutions to the problems. The first Crowdmath project began on March 1, 2016.|$|R
40|$|Left-ventricular assist {{devices are}} being {{increasingly}} {{used as a}} bridge to heart transplantation in patients with end-stage heart failure. However, current evidence does not support their widespread application for this indication. Other indications such as cardiogenic shock in acute myocardial infarction, or severe myocarditis may <b>possibly</b> be more <b>fruitful.</b> The use of these devices in the chronic support of patients with end-stage heart failure has recently been evaluated in patients excluded for heart transplantation. Patients treated with an assist device had {{a better quality of}} life than patients on drug treatment, and a greater number of them were still alive after two years (23 % versus 8 %). In the future, novel options such as skeletal muscle cell transplantation, may offer a feasible alternative to heart transplantation...|$|R
40|$|Elizabeth Grosz {{tells us}} that bodily pain forms the {{identity}} of both {{the individual and the}} society and culture. She credits Nietzsche with “the insight that pain is the key in instituting memory”(Grosz 1994 : 131). Law, she claims, is branded on to the body through “a memory fashioned out of the suffering and pain of the body, ” and she quotes Nietzsche: One might even say that wherever on earth solemnity, seriousness, mys-tery and gloomy coloring still distinguish the life {{of a man and a}} people, something of the terror that formerly attended all promises, pledges and vows on earth is still effective (Grosz 1994 : 132). I believe that J. M. Coetzee never flinches from examining the employment of this tech-nique by those fashioning our political, including legal, relations in his brilliant novels of apartheid South Africa. But in the novel Disgrace he entwines sexual politics, racial poli-tics and species politics in equal measure. Set in the new South Africa, the novel shows the struggle of the characters to reconcile their differences, set in concrete by the legal, politi-cal and social order of apartheid. In this paper, I will discuss only the relationship of fear between the canine species and the human species in the novel. In the contemporary world, we are searching for another way of situating humanity in nature to discover the reality of our relations with other species. <b>Possibly</b> a <b>fruitful</b> mov...|$|R
40|$|A <b>possibly</b> <b>fruitful</b> {{extension}} of conventional random matrix ensembles is proposed by imposing symmetry constraints on conventional Hermitian matrices or parity-time- (PT-) symmetric matrices. To illustrate the main idea, we first study 2 * 2 complex Hermitian matrix ensembles with O(2) invariant constraints, yielding novel level-spacing statistics such as singular distributions, half-Gaussian distribution, distributions interpolating between GOE (Gaussian Orthogonal Ensemble) distribution and half Gaussian distributions, {{as well as}} gapped-GOE distribution. Such a symmetry-reduction strategy is then used to explore 2 * 2 PT-symmetric matrix ensembles with real eigenvalues. In particular, PT-symmetric random matrix ensembles with U(2) invariance can be constructed, with the conventional complex Hermitian random matrix ensemble being a special case. In two examples of PT-symmetric random matrix ensembles, the level-spacing distributions {{are found to be}} the standard GUE (Gaussian Unitary Ensemble) statistics or "truncated-GUE" statistics...|$|E
40|$|Part 3 : Big and Open DataInternational audienceFacebook, Twitter, Instagram, {{and other}} {{players in the}} social media world {{have been on the}} rise during the last couple of years. In {{contrast}} to their popularity, their underlying business models are vague and often only linked to advertising. In this explorative study we identify new revenue sources for social media service providers besides advertising. Based on three use cases with Facebook, Tencent, and LinkedIn, we identify three <b>possibly</b> <b>fruitful</b> ways to extend existing social media business models. Subsequently, a survey with 301 respondents changes perspectives on the user’s willingness to pay in order to identify usage-related differences evoked by cultural and external circumstances. Four derived hypotheses lead the way to avenues of further research especially in terms of Big Data analytics with new e-commerce trends like Facebook’s Buy Button...|$|E
40|$|One of the {{discernible}} {{trends in}} the historiography of recent decades especially in that which concerns the early modern centuries has been {{the emergence of a}} literature that describes itself as Atlantic History. This paper seeks to identify positive and negative reasons why the once-popular history of exploration and discovery has given way to this new subject, it identifies some fresh meanings that may be drawn from some well-known sources when they are reappraised in an Atlantic context, and it suggests some <b>possibly</b> <b>fruitful</b> lines of enquiry that would lead {{to a better understanding of}} how an Atlantic world was fashioned and functioned during the sixteenth, seventeenth and eighteenth centuries. Finally, the paper draws a distinction between Atlantic history and Global history and suggests that the latter is a subject that belongs more properly to the nineteenth and subsequent centuries. ...|$|E
5000|$|In the 1960s {{her work}} became more varied {{and we see}} {{a vast array of}} {{different}} artistic media being used, such as plaster and ceramic sculptures, textile designs and hundreds of Neon-Sign advertising panels, in pastel-chalks on black board,( [...] in order to show fluorescence). She also taught and lectured for many years at the Sint Maria Institute, where she herself used to study during the 1930s. In the 1950s and 60's she was also an Award-winning member of the SCM Small Film Club in Mortsel, where she produced many 8 and 16 mm. films, for which she received several medals. <b>Possibly</b> her most <b>fruitful</b> output during the later days, {{came in the form of}} some unique and wonderful Stained-Glass windows, which she produced in varying sizes. Colourful and of high quality, most of these were created after an extensive study at The Academy of Fine Arts in Berchem-Antwerp. All of her stained glass windows were warmly accepted not only locally, but they also became very much appreciated, both in the USA and in Canada.|$|R
40|$|This {{study is}} {{concerned}} with the description of the semantic and pragmatic characteristics of the attributive adjective self-proclaimed, employing corpus-linguistic methodology to explore its meaning from user-based data. The initial query provided the material from which a lexical pro-file of the target word was constructed, systematically describing collocational data, semantic preferences, semantic associations and discourse prosodies. Qualitative analysis of sample con-cordances illustrated the role of the target word in expressing different kinds of meaning-bearing stances. The results demonstrate the importance of context and communicative functionality as constraints determining meaning, determining the discourse prosodies of self-proclaimed as one of either negation; accepted-positive and accepted-negative. Further, the analysis of self-proclaimed as a stance marker indicates the linking evaluative meanings of extended lexical units to the project of linguistic description of intersubjective stancetaking as a <b>possibly</b> <b>fruitful</b> venue for researc...|$|E
40|$|Abstract. There {{are many}} {{improvements}} needed in concrete, especially {{for use in}} renewal and expansion of the world’s infrastructure. Nanomodification can help solve many of these problems. However, concrete {{has been slow to}} catch on to the nanotechnology revolution. There are several reasons for this lag in the nanos-cience and nanotechnology of concrete (NNC). First is the lack of a complete ba-sic understanding of chemical and physical mechanisms and structure at the nanometer length scale. Another reason is the lack of a broad understanding of what nanomodification means to concrete, which is a liquid-solid composite. NNC ideas need to profit from, but not be bound by, experience with other materials. As an illustration of these ideas, a specific application will be given of using nano-size molecules in solution to affect the viscosity of the concrete pore solution so that ionic diffusion is slowed. A molecular-based understanding would help move this project towards true nanotechnology. A final section of this paper lists some <b>possibly</b> <b>fruitful</b> focus areas for the nanoscience and nanotechnology of concrete. 1 Introduction- The Need for Researc...|$|E
40|$|The local {{purity of}} large many-body quantum {{systems can be}} studied by {{following}} a statistical mechanical approach based on a random matrix model. Restricting the analysis {{to the case of}} global pure states, this method proved to be successful, and a full characterization of the statistical properties of the local purity was obtained by computing the partition function of the problem. Here we generalize these techniques to the case of global mixed states. In this context, by uniformly sampling the phase space of states with assigned global mixedness, we determine the exact expression of the first two moments of the local purity and a general expression for the moments of higher order. This generalizes previous results obtained for globally pure configurations. Furthermore, through the introduction of a partition function for a suitable canonical ensemble, we compute the approximate expression of the first moment of the marginal purity in the high-temperature regime. In the process, we establish a formal connection with the theory of quantum twirling maps that provides an alternative, <b>possibly</b> <b>fruitful,</b> way of performing the calculation...|$|E
40|$|We {{historically}} trace various non-conventional {{explanations for}} {{the origin of the}} cosmic microwave background and discuss their merit, while analyzing the dynamics of their rejection, as well as the relevant physical and methodological reasons for it. It turns out that there have been many such unorthodox interpretations; not only those developed in the context of theories rejecting the relativistic ("Big Bang") paradigm entirely (e. g., by Alfven, Hoyle and Narlikar) but also those coming from the camp of original thinkers firmly entrenched in the relativistic milieu (e. g., by Rees, Ellis, Rowan-Robinson, Layzer and Hively). In fact, the orthodox interpretation has only incrementally won out against the alternatives {{over the course of the}} three decades of its multi-stage development. While on the whole, none of the alternatives to the hot Big Bang scenario is persuasive today, we discuss the epistemic ramifications of establishing orthodoxy and eliminating alternatives in science, an issue recently discussed by philosophers and historians of science for other areas of physics. Finally, we single out some plausible and <b>possibly</b> <b>fruitful</b> ideas offered by the alternatives. Comment: 53 pages, accepted in "The Studies in History and Philosophy of Modern Physics...|$|E
40|$|Financialization, it is argued, has agency at {{a number}} of scales, ranging from higher levels of {{instability}} within the economy as a whole, through pressure exerted on corporations by capital markets, to the equity effects of the financial system on individuals and households. The paper develops a sympathetic critique of the concept, arguing that the purchase of financialization on the nature of change within contemporary society has been relatively underplayed when compared to similar concepts such as neoliberalization. While the concept of financialization has the potential to unite researchers across cognate social science fields and so build badly needed critical mass and bring recognition to the social significance of money and finance, we argue that to date research has been insufficiently attentive to the role of space and place, both in terms of its processes and its effects. Research on financialization also tends to be characterized by an overly pessimistic view of the nature and future of financial markets. The paper explores a number of <b>possibly</b> <b>fruitful</b> directions for work on financialization to pursue, focusing in particular on the concepts of the financial ecology and financial citizenship...|$|E
40|$|The paper {{develops}} a sympathetic {{critique of the}} concept of financialization. This concept has been developed to account for the empowering of financial markets and their influence over the unfolding of economy, polity and society. Processes of financialization are claimed to manifest at a number of scales, ranging from higher levels of instability within the economy as a whole, through pressure exerted on corporations by capital markets, to the equity effects of the financial system on individuals and households [...] In seeking to explain the change within contemporary society financialization has to date been relatively underplayed, particularly when compared to similar and related concepts such as neoliberalization. While the concept of financialization has the potential to unite researchers across cognate social science fields, thereby building critical mass and recognition within social studies of money and finance, we argue that to date research has been insufficiently attentive to the role of space and place, both in terms of its processes and its effects. The paper explores a number of <b>possibly</b> <b>fruitful</b> directions for work on financialization to pursue, focusing in particular on the concepts of the financial ecology and financial citizenship...|$|E
40|$|Years ago, a {{professional}} parapsychological conference included {{a panel discussion}} devoted {{to the future of}} parapsychology. Many of us in the audience were amused to hear one of our esteemed colleagues begin his presentation by saying, “Of course, we cannot predict the future. ” This was a delightfully curious comment from someone who had dedicated much of his life to a field that has collected considerable anecdotal and laboratory evidence for the reality of precognition (future-knowing or future-telling) —that, under certain conditions, persons are indeed able to “predict the future. ” However, predicting the future of parapsychology is not something I would wish to attempt in this essay. Rather, I will present some personal views on the functions, meanings, and implications of psi events and experiences—as we now know them—and suggest some <b>possibly</b> <b>fruitful</b> directions for future psi research. Functions, Meanings, and Implications Suggested by Psi Research Findings Psi researchers typically think of psi as an information or communication process (e. g., see Shapin & Coly, 1980). Certainly, information—especially imagery-related and emotion-related information—can be “communicated ” in many psi interactions, and considerable research has been devoted to exploring this aspect. I have been intrigued, however, by other roles psi might play and by other functions it might serv...|$|E
40|$|I {{begin the}} {{analysis}} of understanding by considering the initially plausible claim that understanding is a species of knowledge. In order to do this, I investigate {{a variety of ways}} in which the two epistemic states might come apart, and see whether the notion that they often do so is plausible. I progress to examine a number of the most common and plausible hallmark features of understanding discussed in the current literature, and go on to try and clarify the different sorts of understanding that are available to agents whilst trying to discover which of these is epistemically significant (and why). I then explore the value problem for knowledge in more depth, explaining that there are in fact three interrelated value problems and looking at what degree of success the most promising attempt to solve these problems has had. Following that, I look at the properties which I believe are primarily responsible for the value of understanding, and investigate whether its possessing such properties allows us to better explain why understanding might be finally valuable even if knowledge is not. To conclude, I summarise what import my discussion has for the practice of contemporary epistemological theorising in general, and briefly review <b>possibly</b> <b>fruitful</b> avenues for further research...|$|E
40|$|Most of {{parameters}} used {{to describe}} states and dynamics of financial market depend on proportions of the appropriate variables rather than on their actual values. Therefore, projective geometry {{seems to be the}} correct language to describe the theater of financial activities. We suppose that the object of interest of agents, called here baskets, form a vector space over the reals. A portfolio is defined as an equivalence class of baskets containing assets in the same proportions. Therefore portfolios form a projective space. Cross ratios, being invariants of projective maps, form key structures in the proposed model. Quotation with respect to an asset X (i. e. in units of X) are given by linear maps. Among various types of metrics that have financial interpretation, the min-max metrics on the space of quotations can be introduced. This metrics has an interesting interpretation in terms of rates of return. It can be generalized so that to incorporate a new numerical parameter (called temperature) that describes agent's lack of knowledge {{about the state of the}} market. In a dual way, a metrics on the space of market quotation is defined. In addition, one can define an interesting metric structure on the space of portfolios/quotation that is invariant with respect to hyperbolic (Lorentz) symmetries of the space of portfolios. The introduced formalism opens new interesting and <b>possibly</b> <b>fruitful</b> fields of research. ...|$|E
40|$|Abstract. Subsequence {{matching}} {{has appeared}} to be an ideal approach for solving many problems related to the fields of data mining and sim-ilarity retrieval. It has been shown that almost any data class (audio, image, biometrics, signals) is or can be represented by some kind of time series or string of symbols, which can be seen as an input for various subsequence matching approaches. The variety of data types, specific tasks and their partial or full solutions is so wide that the choice, im-plementation and parametrization of a suitable solution for a given task might be complicated and time-consuming; a <b>possibly</b> <b>fruitful</b> combina-tion of fragments from different research areas may not be obvious nor easy to realize. The leading authors of this field also mention the imple-mentation bias that makes difficult a proper comparison of competing approaches. Therefore we present a new generic Subsequence Matching Framework (SMF) that tries to overcome the aforementioned problems by a uniform frame that simplifies and speeds up the design, development and evaluation of subsequence matching related systems. We identify several relatively separate subtasks solved differently over the literature and SMF enables to combine them in straightforward manner achieving new quality and efficiency. This framework can be used in many appli-cation domains and its components can be reused effectively. Its strictly modular architecture and openness enables also involvement of efficient solutions from different fields, for instance efficient metric-based indexes. This is an extended version of a paper published on DEXA 2012. ...|$|E
40|$|In a 1962 {{manifesto}} {{published in}} New Worlds called ‘Which Way to Inner Space?’ Ballard wrote: ‘The biggest {{developments of the}} immediate future will take place, not on the Moon or Mars, but on Earth, and it is inner space, not outer, {{that needs to be}} explored. The only truly alien planet is Earth. ’ This chapter will explore Ballard’s own ‘space age’, the deep implication of time, space and psychology in his early fiction, from concepts of the ‘time zone’, ‘deep time’ and ‘archaeopsychic time’ to the fugue states of Ballard’s later short stories, in which time solidifies or crystalizes into material space. Ballard’s ‘space age’ is one in which alienation is produced by the conditions of modernity, by technology and what he has called the ‘mediascape’, resulting both in a kind of self-alienation (which provides access to <b>possibly</b> <b>fruitful</b> ‘other’ states of mind or being) and a physical dislocation to Ballard’s typical chronotopes: the deserted resort, evacuated launch site, the disaster area. In Ballard’s early novels, The Drowned World, The Drought, and The Crystal World, he works with science fiction conventions to produce a different kind of ‘space fiction’ that explores what he would understand to be the hidden imperatives of modernity. This chapter will explore the connections between Ballard’s early novels and the development of his short fiction, and as a whole will attempt to map out the different phases of Ballard’s imaginative exploration of the inner space age...|$|E
40|$|In "Skating on Thin Ice", Frohlich and Oppenheimer (2006) {{describe}} a phenomenon they observed in public goods experiments that is rarely {{discussed in the}} literature: individual contributions to the public good are often inconsistent over time, appearing to fluctuate between two distinct contribution levels. Although they conjecture that individuals have complex context-dependent preferences, they did not develop a full specification of the theory. Using an agent-based simulation model, we explore the likelihood of these psychological conjectures, and thereby provide a possible specification of a theory of complex context-dependent preferences. We consider two main theories: first, that inconsistent contributions arise from a deterministic avoidance of exploitation and second, that inconsistent contributions arise from a probabilistic response to exploitation. We show the former theory clearly fails and the latter theory, under specifiable conditions, does produce the observed pattern of contributions. Two simple alternative theories are also considered, that of a highly-stylized "probabilistic guilt" and of goal-oriented but non-utility maximizing behavior (with stable preferences). Both alternatives, under certain conditions, are also able to generate the observed pattern. We develop an analysis of situations in which the predictions of these theories diverge and suggest that one could discriminate between them in laboratory settings. Finally, we consider a <b>possibly</b> <b>fruitful</b> relationship between simulation and experimentation to consider the implications of one's models and conjectures: this research {{can be seen as}} one step in an iterative process of theory development, vetting and testing, generating an empirically grounded theory of individual behavior in VCM games. Social dilemmas Altruism Experimental economics Decision-making Computer simulation...|$|E
40|$|This work {{presents}} a brief and non-technical {{description of the}} main results and concepts of the modern scientific cosmology, viewing it from an epistemological perspective which allows a dialog with other modes of thinking like e. g. history, philosophy, sociology and religion. This epistemological viewpoint {{is based on the}} philosophical theses advanced by Ludwig Boltzmann (1844 - 1906) which states that scientific theories are nothing more than representations, or images, of nature (arXiv:physics/ 0701308 v 1). By being representations one cannot know how nature really is because the intrinsic and indispensable properties that characterize nature are unreachable by science. In other words, the true essences that constitute nature are unknowable. Therefore, all answers proposed by science are partial, simplified and replaceable. Another way of putting forward this viewpoint is to state that all scientific truths are provisional, a result which naturally leads {{to the conclusion that the}} same set of phenomena, or scientific questions, may have various answers, or representations. This conclusion is generally known as theoretical pluralism (arXiv:physics/ 9806011). It is exactly such a plurality for conceiving, or representing, nature that opens the way for a <b>possibly</b> <b>fruitful</b> dialog among the various forms of thinking, since this dialog can take place in the realm of the representations. A few examples taken from cosmology, sociology and theology are discussed in the context of this epistemological framework. Comment: 27 pages. Invited talk presented at the XII Symposium of the Brazilian Association for the History of Religions. Text written in Brazilian Portugues...|$|E
40|$|Subsequence {{matching}} {{has appeared}} to be an ideal approach for solving many problems related to the fields of data mining and similarity retrieval. It has been shown that almost any data class (audio, image, biometrics, signals) is or can be represented by some kind of time series or string of symbols, which can be seen as an input for various subsequence matching approaches. The variety of data types, specific tasks and their partial or full solutions is so wide that the choice, implementation and parametrization of a suitable solution for a given task might be complicated and time-consuming; a <b>possibly</b> <b>fruitful</b> combination of fragments from different research areas may not be obvious nor easy to realize. The leading authors of this field also mention the implementation bias that makes difficult a proper comparison of competing approaches. Therefore we present a new generic Subsequence Matching Framework (SMF) that tries to overcome the aforementioned problems by a uniform frame that simplifies and speeds up the design, development and evaluation of subsequence matching related systems. We identify several relatively separate subtasks solved differently over the literature and SMF enables to combine them in straightforward manner achieving new quality and efficiency. This framework can be used in many application domains and its components can be reused effectively. Its strictly modular architecture and openness enables also involvement of efficient solutions from different fields, for instance efficient metric-based indexes. This is an extended version of a paper published on DEXA 2012. Comment: This is an extended version of a paper published on DEXA 201...|$|E
40|$|We {{identify}} {{three major}} areas of ignorance which limit predictability in current ocean GCMs. One {{is the very}} crude representation of subgrid-scale mixing processes. These processes are parameterized with coefficients whose values and variations {{in space and time}} are poorly known. A second problem derives from the fact that ocean models generally contain multiple equilibria and bifurcations, but there is no agreement as to where the current ocean sits with respect to the bifurcations. A third problem arises from the fact that ocean circulations are highly nonlinear, but only weakly dissipative, and therefore are potentially chaotic. The few studies that have looked at this kind of behavior have not answered fundamental questions, such as what are the major sources of error growth in model projections, and how large is the chaotic behavior relative to realistic changes in climate forcings. Advances in computers will help alleviate some of these problems, for example by making it more practical to explore to what extent the evolution of the oceans is chaotic. However models will have to rely on parameterizations of key small-scale processes such as diapycnal mixing for a long time. To make more immediate progress here requires the development of physically based prognostic parameterizations and coupling the mixing to its energy sources. Another <b>possibly</b> <b>fruitful</b> area of investigation is the use of paleoclimate data on changes in the ocean circulation to constrain more tightly the stability characteristics of the ocean circulation. Abstract in HTML and technical report in PDF available on the Massachusetts Institute of Technology Joint Program on the Science and Policy of Global Change website ([URL]...|$|E
40|$|The three novels {{studied in}} this thesis are La familia de Pascual Duarte, Navas andanzas y desventuras de Lazarillo de Tormes and La colmena. Although they are {{basically}} different in content and structure, each does, however, manifest a preoccupation with the reactions {{of the individual to}} his fellow man. In general, intimate human relationships are not seen but rather the opposite with violence, hostility, selfishness and apathy playing a dominant role in varying degrees in each of these novels. Thus, for a variety of reasons, some beyond their control, Pascual Duarte, Lazaro and Martin Marco come to represent the ‘lower’ or outsider' figures within their respective societies. The factor that, is largely beyond the control of each of these men is the attitude of their fellows; the hostility, suspicion and lack of compassion that, apart from brief interludes, pervade the three novels in question. Duarte and Lazaro increasingly come to profess a belief in an inclement destiny, although the former may well have an ulterior motive for this. The 'Final' of La colmena may, however, permit a feeling of cautious optimism as various individuals share a common goal in an attempt to help Martin Marco. Whatever fate awaits him it is ironical that he, too, may be frustrated by circumstances or destiny when he at last appears to have decided to conduct his life in a more positive and <b>possibly</b> <b>fruitful</b> manner. Yet at least some people are showing a genuine concern for an individual's plight, acting upon this and thereby showing a responsive and compassionate attitude rarely seen in La colmena or for that matter, in either of the other two novels. Also discussed at length, and with particular reference to thematic exigencies, is the structure of each of the novels under consideration...|$|E
40|$|This {{article focuses}} on an esoteric but {{practical}} use of automated reasoning that may indeed be new to many, especially those concerned primarily with verification of both hardware and software. Specifically, featured are a discussion and some methodology for taking an existing design [...] of a circuit, a chip, a program, or the like [...] and refining and improving it in various ways. Although the methodology is general and {{does not require the}} use of a specific program, McCune's program OTTER does offer what is needed. OTTER has played and continues to play the key role in my research, and an interested person can gain access to this program in various ways, {{not the least of which}} is through the included CD-ROM in [3]. When success occurs, the result is a new design that may require fewer components, avoid the use of certain costly components, offer more reliability and ease of verification, and, perhaps most important, be more efficient in the contexts of speed and heat generation. Although the author has minimal experience in circuit design, circuit validation, program synthesis, program verification, and similar concerns, (at the encouragement of colleagues based on successes to be cited) he presents materials that might indeed be of substantial interest to manufacturers and programmers. He writes this article in part prompted by the recent activities of chip designers that include Intel and AMD, activities heavily emphasizing the proving of theorems. As for his research that appears to the author to be relevant, he has made an intense and most profitable study of finding proofs that are shorter [2, 3], some that avoid the use of various types of term, some that are far less complex than previously known, and the like. Those results suggest to me a strong possible connection between more appealing proofs (in mathematics and in logic) and enhanced and improved design of both hardware and software. Here the author explores diverse conjectures that elucidate some of the <b>possibly</b> <b>fruitful</b> connections...|$|E
40|$|One of the {{key point}} of the Particle Swarm Optimization (PSO) {{algorithm}} {{is the absence of}} use of local derivatives of the objective function. PSO is usually able to rapidly converge toward the basin of attraction of a global optimum, and its derivative-free nature is reflected in the small number of objective function evaluations. The side effects of the absence of local information are essentially twofold. Firstly, once the basin of attraction has been identified, the exploitation of the successive local exploration is pretty slow. Secondly, there is not any guarantee about the qualities of the converging point, whose qualification cannot be performed without an analysis of the first and second order local information. To tackle this problem, the application of a local algorithm as soon as a partial convergence of PSO is detected has been successfully investigated in cite{OPTE}. Unfortunately, the computational cost of the local algorithm is added to the overall computational cost, and this may become unsustainable in case of expensive objectives to be considered. Furthermore, the use of a local algorithm simply {{at the end of the}} exploration by PSO is only exploring in deep the selected basin of attraction, and this is not helping PSO in detecting the global optimum: in this sense, the use of local information during the course of PSO is much more interesting and <b>possibly</b> <b>fruitful,</b> but is too expensive to be performed by central differences or other schemes. To this aim, the use of surrogate models for the computation of first and second order local information may represents a good option in order to better address the search of the global optimum and also to add some value to the converging point, preserving also the small computational cost of PSO. By the way, the distribution and evolution of a swarm into the design space can provide similar data as a DOE for the derivation of approximated models of the objective function: as a consequence, no further evaluations are needed to this aim. In this paper, the classical PSO algorithm cite{Clerc} is coupled with a Newton method. In order to produce a monotonically descendent sequence, the PSO iteration scheme is adopted, while the Newton step is substituting the classical PSO iteration when this is not able to detect a descent direction. Gradient and Hessian of the objective function are approximated by means of a surrogate model: the training of the surrogate model is obtained by using all the previously computed values of the objective function as from the PSO algorithm. The use of a trust-region approach is helping in maintaining some global convergence properties...|$|E

