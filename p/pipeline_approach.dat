77|270|Public
50|$|For added {{flexibility}} each {{core part}} of the system has been developed using a pluggable <b>pipeline</b> <b>approach.</b> This means that authentication, identity and SSO events must each traverse a pipeline of plugins which all perform different tasks. This allows organizations to add functionality specific to their deployment while still taking advantage of the core logic.|$|E
50|$|The <b>pipeline</b> <b>approach</b> (stepped-wedge design) uses {{beneficiaries}} already {{chosen to}} participate in a project at a later stage as the comparison group. The assumption is that as they have been selected to receive the intervention in the future they are similar to the treatment group, and therefore comparable in terms of outcome variables of interest. However, in practice, it cannot be guaranteed that treatment and comparison groups are comparable and some method of matching will need to be applied to verify comparability.|$|E
50|$|The Research Program {{primarily}} {{started in}} 1976 to help {{graduate students in}} their agricultural researches. Having momentum and acknowledging the importance of research in an academic community, its thrust expanded to cover several technical researches on selected agricultural commodities. In 1978, the Research and Extension Programs were merged which {{gave birth to the}} Research and Development Center (R & DC). The R & DC adopted the <b>pipeline</b> <b>approach</b> as its strategy to spur countryside group for information and technology dissemination and contribute to the realization of the university’s development goals. It relives the maxim “development is research utilized”. Research was, therefore, envisioned to establish a foundation that would accomplish one of the trilogies of functions of the University.|$|E
40|$|In today’s digital world, where {{portable}} computers have become {{as small as}} the size of palm limitation on processing speed has increased. Thus there’s a need for modification in the traditional approach to overcome this limitation. An implementation using parallel and <b>pipelined</b> <b>approach</b> could work at higher speed while occupying limited number of slices. Paper deals with analyzing and reviewing different multiplication algorithms viz. Vedic, Chinese, Wallace, Booth, Karatsuba and Toom-Cook by performing 11 * 8 bit multiplication using parallel and <b>pipelined</b> <b>approach...</b>|$|R
40|$|Abstract — In today’s digital world, where {{portable}} computers have become {{as small as}} the size of palm limitation on processing speed has increased. Thus there’s a need for modification in the traditional approach to overcome this limitation. An implementation using parallel and <b>pipelined</b> <b>approach</b> could work at higher speed while occupying limited number of slices. Paper deals with analyzing and reviewing different multiplication algorithms viz. Vedic, Chinese, Wallace, Booth, Karatsuba and Toom-Cook by performing 11 * 8 bit multiplication using parallel and <b>pipelined</b> <b>approach.</b> Keywords- Multiplication Algorithms, Pipelining. I...|$|R
40|$|We {{present a}} {{cognitive}} model of early lexical acquisition which jointly performs word segmentation and learns an explicit model of phonetic variation. We define the model as a Bayesian noisy channel; we sample segmentations and word forms simultaneously from the posterior, using beam sampling {{to control the}} size of the search space. Compared to a <b>pipelined</b> <b>approach</b> in which segmentation is performed first, our model is qualitatively more similar to human learners. On data with variable pronunciations, the <b>pipelined</b> <b>approach</b> learns to treat syllables or morphemes as words. In contrast, our joint model, like infant learners, tends to learn multiword collocations. We also conduct analyses of the phonetic variations that the model learns to accept and its patterns of word recognition errors, and relate these to developmental evidence. ...|$|R
40|$|Mining {{is nothing}} but {{retrieving}} the information from various resources. We have different approaches to retrieve these information {{one of them is}} traditional <b>pipeline</b> <b>approach.</b> As of increasing technologies it became more complicated to workout with these traditional approach the main drawback in these <b>pipeline</b> <b>approach</b> is if any modifications are done or any module is developed newly then we have to reapply the extraction. So we are developing the different approach for data mining in this paper is through database queries. These are optimized by databases that make this as efficient approach...|$|E
40|$|This paper {{describes}} the system submit-ted for the Sentiment Analysis in Twitter Task of SEMEVAL 2014 and specifically the Message Polarity Classification sub-task. We used a 2 –stage <b>pipeline</b> <b>approach</b> employing a linear SVM classifier {{at each stage}} and several features including mor-phological features, POS tags based fea-tures and lexicon based features. ...|$|E
40|$|In this work, {{we present}} a short review of shape from stereo {{reconstruction}} techniques and propose a stereo matching algorithm based on a <b>pipeline</b> <b>approach.</b> At each stage, current positions of match are estimated from previous computed disparities. Determining the best matching only requires the calculus of correlation values of pixels in a small window centered at candidate positions. The algorithm was succesfully applied to stereo pairs of images, {{but it can be}} more advantageously applied to sequences of stereo image frames, due to the <b>pipeline</b> <b>approach.</b> We also present some visual and numeric results obtained using the algorithm and compare them with those obtained by some usual methods, also implemented. Keywords: Stereo images, Laplacian of gaussian, Binary images, Matching, SSD, Disparity map. 1 INTRODUTION In this work, we discuss some basic background about stereo reconstruction techniques and present a stereo matching algorithm with some interesting aspects. In thi [...] ...|$|E
40|$|Abstract. How to {{deal with}} part of speech (POS) tagging is a very {{important}} problem when we build a syntactic parsing system. We could preprocess the text with a POS tagger before perform parsing in a <b>pipelined</b> <b>approach.</b> Alterna-tively, we could perform POS tagging and parsing simultaneously in an inte-grated approach. Few, if any, comparisons have been made on such architecture issues for Chinese parsing. This paper presents an in-depth study on this prob-lem. According to comparison experiments, we find that integrated approach can make significantly better performance both on Chinese parsing and un-known words POS tagging than the <b>pipelined</b> <b>approach.</b> As for known words POS tagging, we find that the two approaches get similar tagging accuracy, but the tagging results of integrated approach do lead to much better parsing per-formance. We also analyze the reasons account for the performance difference. ...|$|R
30|$|Using the <b>pipelining</b> <b>approach</b> {{mentioned}} in[64] {{the code}} has been optimized for N = 8 taps. The inner loop was completely unrolled {{to reduce the}} loop overhead, the dependency graph was created and the instructions were pipelined {{to reduce the number}} of cycles. The optimized code consists of 3 parts. The prolog, the mainloop and epilog.|$|R
30|$|The main {{disadvantage}} of the FS-FBMC receiver {{is related to}} the higher rate at which the FFT has to be performed when M/K subcarriers are employed in the frequency-spreading structure in comparison with the classical OFDM transceiver employing M carriers. Here, however, a <b>pipelining</b> <b>approach</b> to the FFT implementation may significantly reduce the importance of such a disadvantage.|$|R
40|$|This paper {{describes}} the systems {{with which we}} participated in the task Sentiment Analysis in Twitter of SEMEVAL 2013 and specifically the Message Polarity Classification. We used a 2 -stage <b>pipeline</b> <b>approach</b> employing a linear SVM classifier at each stage and several features including BOW features, POS based features and lexicon based features. We have also experimented with Naive Bayes classifiers trained with BOW features. ...|$|E
40|$|This paper {{describes}} our {{submission to}} the ANLP- 2014 shared task on auto-matic Arabic error correction. We present a <b>pipeline</b> <b>approach</b> integrating an er-ror detection model, {{a combination of}} character- and word-level translation mod-els, a reranking model and a punctuation insertion model. We achieve an F 1 score of 62. 8 % on the development set of the QALB corpus, and 58. 6 % on the official test set. ...|$|E
40|$|This paper {{proposes a}} history-based struc-tured {{learning}} approach that jointly ex-tracts entities and relations in a sentence. We introduce a novel simple and flexible table representation of entities and rela-tions. We investigate several feature set-tings, search orders, and learning meth-ods with inexact search on the table. The experimental results demonstrate that a joint learning approach significantly out-performs a <b>pipeline</b> <b>approach</b> by incorpo-rating global features and by selecting ap-propriate learning methods and search or-ders. ...|$|E
40|$|Data {{encryption}} {{process can}} easily be quite complicated and usually requires significant computation time and power despite significant simplifications. This paper discusses about pipelined and non-pipelined implementation {{of one of the}} most commonly used symmetric encryption algorithm, Data Encryption Standard (DES). The platform used for this matter is, Xilinx new high performance silicon foundation, Virtex- 6 Field Programmable Gate Array technology. Finite state machine is used only in non-pipelined implementation, and it is not implemented for the <b>pipelined</b> <b>approach.</b> The testing of the implemented design shows that it is possible to generate data in 16 clock cycles when non-pipelined approach is employed. When <b>pipelined</b> <b>approach</b> is employed on the other hand, 17 clock signals are required for the initial phase only, and one clock signal is sufficient afterwards for each data generation cycle. The Very High Speed Integrated Circuit Hardware Description Language (VHDL) is used to program the design...|$|R
40|$|We present RelationFactory, {{a highly}} ef-fective open source {{relation}} extraction sys-tem based on shallow modeling tech-niques. RelationFactory emphasizes mod-ularity, is easily configurable {{and uses a}} transparent <b>pipelined</b> <b>approach.</b> The interactive demo allows the user to pose queries for which RelationFactory re-trieves and analyses contexts that contain relational information about the query en-tity. Additionally, a recall error analy-sis component categorizes and illustrates {{cases in which the}} system missed a correct answer. ...|$|R
40|$|We {{present a}} global joint model for lemmatization and part-of-speech prediction. Using only {{morphological}} lexicons and unlabeled data, we learn a partiallysupervised part-of-speech tagger and a lemmatizer which are combined using features on a dynamically linked dependency structure of words. We evaluate our model on English, Bulgarian, Czech, and Slovene, and demonstrate substantial improvements over both a direct transduction approach to lemmatization and a <b>pipelined</b> <b>approach,</b> which predicts part-of-speech tags before lemmatization. ...|$|R
40|$|Wageningen Economic Research {{conducts a}} study to {{evaluate}} the socioeconomic impact on sugarcane farmers of the Solidaridad programme. A <b>pipeline</b> <b>approach</b> was used, which clusters the farmers in cohorts based on the year they receive support and training: 2016, 2017 and 2018. A baseline survey was conducted on 1, 008 farmers from the command areas in April 2016. This report gives a representative and {{detailed description of the}} target group in 2016...|$|E
40|$|This paper {{presents}} the coreference resolution system Poly-co {{submitted to the}} closed track of the CoNLL- 2011 Shared Task. Our sys-tem integrates a multilayer perceptron classi-fier in a <b>pipeline</b> <b>approach.</b> We describe the heuristic used to select the pairs of corefer-ence candidates that are feeded to the network for training, and our feature selection method. The features used in our approach are based on similarity and identity measures, filtering in-formations, like gender and number, and other syntactic information. ...|$|E
40|$|Abstract. Component-Based Software Development {{is widely}} used in the {{software}} development, which focus on component composition and reutilization, but some problems such as compatibility and consistency always impact on component composition. Thus, we proposed a <b>pipeline</b> <b>approach</b> for component composition, which establish pipeline management mechanisms included different workshop section, workshop, monitoring and coordination, then ontology is interposed different workshop to implement component composition. Finally, it is analyzed by application example {{to show that the}} approach is feasible...|$|E
40|$|International audienceIn this paper, we {{introduce}} {{a new kind of}} constraints, called core precedence constraints, in a cyclic resource constrained scheduling problem with temporal constraints. We use them to model energy saving in a network scheduling problem. Then we study the impact of these new constraints on an effcient approach for cyclic RCPSP: the decomposed software <b>pipelining</b> <b>approach.</b> After discussion, we show that this approach can be used to prove that even without resource constraint, the existence of a periodic schedule is an NP-complete problem...|$|R
40|$|Presented {{here is an}} {{architecture}} for implenting a sub-pixel resolution edge detector based upon the second difference of a Gaussian filtering. A <b>pipelining</b> <b>approach</b> allows many simple operations to be performed in parallel across the video data stream so allowing a throughput at video rate, 10 MHz. The output is intended describe edges {{in terms of an}} 8 -bit strength and orientation as well as 8 -bit Cartesian sub-pixel offsets giving a possible resolution of l/ 5 Oth of a pixel edge position. I...|$|R
40|$|This paper {{presents}} a software system supporting analysts {{in the domain}} of open source intelligence. The desktop application contains functions to extract information acquired from open sources. The text mining engine combines finite state machines, rule based entity guessing and machine learning in a lightweight <b>pipelined</b> <b>approach.</b> The text mining functions and the overall system design have been developed to be applicable in law enforcement settings. This paper focuses on the information extraction functions of the system. JRC. DG. G. 2 -Global security and crisis managemen...|$|R
40|$|Abstract. The idea of linked programs, or {{procedural}} RDF metadata, has {{not been}} deeply explored. This paper introduces a dedicated scripting language for linked data, called Ripple, whose programs both operate upon and reside in RDF graphs. Ripple is {{a variation on the}} concatenative theme of functional, stack-oriented languages such as Joy and Factor, and takes a multivalued, <b>pipeline</b> <b>approach</b> to query composition. The Java implementation includes a query API, an extensible library of primitive functions, and an interactive command-line interpreter. ...|$|E
40|$|In this paper, we {{describe}} our system for Chinese personal name disambiguation {{task in the}} first CIPS-SIGHAN joint conference on Chinese Language Processing(CLP 2010). We use a <b>pipeline</b> <b>approach,</b> in which preprocessing, unrelated documents discarding, Chinese personal name extension and document clustering are performed separately. Chinese personal name extension {{is the most important}} part of the system. It uses two additional dictionaries to extract full personal names in Chinese text. And then document clustering is performed under different personal names. Experimental results show that our system can achieve good performances. ...|$|E
40|$|Turkish is an agglutinative {{language}} with rich morphology-syntax interactions. As {{an extension of}} this property, the Turkish Treebank is designed to represent sublexical dependencies, which brings extra challenges to parsing raw text. In this work, we use a joint POS tagging and parsing approach to parse Turkish raw text, and we show it outperforms a <b>pipeline</b> <b>approach.</b> Then we experiment with incorporating morphological feature prediction into the joint system. Our results show statistically significant improvements with the joint systems and achieve the state-ofthe-art accuracy for Turkish dependency parsing. ...|$|E
40|$|This paper {{introduces}} speech-based visual {{question answering}} (VQA), {{the task of}} generating an answer given an image and a spoken question. Two methods are studied: an end-to-end, deep neural network that directly uses audio waveforms as input versus a <b>pipelined</b> <b>approach</b> that performs ASR (Automatic Speech Recognition) on the question, followed by text-based visual question answering. Furthermore, we investigate the robustness of both methods by injecting various levels of noise into the spoken question and find both methods to be tolerate noise at similar levels...|$|R
40|$|Both entity and {{relation}} extraction {{can benefit}} from being performed jointly, allowing each task to correct the errors of the other. We present a new method for joint entity and relation extraction using a graph we call a “card-pyramid. ” This graph compactly encodes all possible entities and relations in a sentence, reducing the task of their joint extraction to jointly labeling its nodes. We give an efficient labeling algorithm that is analogous to parsing using dynamic programming. Experimental results show improved results for our joint extraction method compared to a <b>pipelined</b> <b>approach.</b> ...|$|R
40|$|In this paper, {{we propose}} a new frame-work that unifies {{the output of}} three infor-mation {{extraction}} (IE) tasks- entity men-tions, relations and events as an informa-tion network representation, and extracts all of them using one single joint model based on structured prediction. This novel formulation allows {{different parts of the}} information network fully interact with each other. For example, many rela-tions can now be considered as the re-sultant states of events. Our approach achieves substantial improvements over traditional <b>pipelined</b> <b>approaches,</b> and sig-nificantly advances state-of-the-art end-to-end event argument extraction. ...|$|R
40|$|Abstract—Fine-grained {{real-time}} metering is {{a fundamental}} service of wireless energy auditing networks, where metering data is transmitted from embedded power meters to gateways for centralized processing, storage, and forwarding. Due to limited meter capability and wireless bandwidth, the increasing sampling rates and network scales needed to support new energy auditing applications pose significant challenges to metering data fidelity and secrecy. This paper exploits the compression and encryption properties of compressive sensing (CS) to design a joint data compression and encryption (JICE) approach that addresses these two challenges simultaneously. Compared with a conventional signal processing pipeline that compresses and encrypts data sequentially, JICE reduces computation and storage complexities due to its simple design. It thus leaves more processor time and available buffer space for handling lossy wireless transmissions. Moreover, JICE features a machine-learning-based reconfigu-ration mechanism that adapts its signal representation basis to changing power patterns autonomously. On a smart plug platform, we implemented JICE and several baseline approaches including downsampling, lossless compression, and the <b>pipeline</b> <b>approach.</b> Extensive testbed experiments show that JICE achieves higher data delivery ratios and lower recovery distortions under a range of realistic settings. In particular, JICE increases the number of meters supported by a gateway by 50 %, compared with the <b>pipeline</b> <b>approach,</b> while keeping a distortion rate lower than 5 %. I...|$|E
40|$|We {{address the}} {{challenge}} of interpreting spoken input in a conversational dialogue system with an approach that aims to exploit the close relationship between the tasks of speech recognition and language understanding through joint modeling of these two tasks. Instead of using a standard <b>pipeline</b> <b>approach</b> where the output of a speech recognizer is the input of a language understanding module, we merge multiple speech recognition and utterance classification hypotheses into one list to be processed by a joint reranking model. We obtain substantially improved performance in language understanding in experiments with thousands of user utterances collected from a deployed spoken dialogue system...|$|E
40|$|This paper {{develops}} a general framework for machine learning based dependency parsing {{based on a}} <b>pipeline</b> <b>approach,</b> where a task is decomposed into several sequential stages. To overcome the error accumulation problem of pipeline models, we propose two natural principles for pipeline frameworks: (i) make local decisions as reliable as possible, and (ii) {{reduce the number of}} sequential decisions made. We develop an algorithm that provably satisfies these principles and show that the proposed principles support several algorithmic choices that improve the dependency parsing accuracy significantly. We present state of the art experimental results for English and several other languages. 1...|$|E
40|$|In {{this paper}} a novel <b>pipelining</b> <b>approach</b> {{applicable}} to Winograd Fourier transforms is presented. The novel approach {{makes use of}} reconfigurable multiplier blocks to implement the real multipliers required for the transform as well as sharing the hardware resources among additions. The additions are realized using modified forms of butterfly circuits. The novel approach is tested on a 5 -point Winograd Fourier transform and the circuit area and power dissipation of the design are estimated using an in-house power estimation tool and compared to the state-of-the- art approaches...|$|R
30|$|But how the {{pipelining}} {{and parallel}} implementations {{are useful for}} reducing the power consumption? In research paper [20] illustrate that the leakage current is the dominant source of energy consumption in scaled transistors. Because, sub-threshold and leakage current both depend on the total gate count, transistors and gate width, a <b>pipelined</b> <b>approach</b> makes substantial contribution in reducing the leakage current. As noted, pipelining gives the low-power processor solution because it always runs at low voltage [21]. With this insight, we propose an alternate solution for power-efficient processor design using pipelining concept called variable stage pipelining (VSP).|$|R
40|$|Standard) is presented. The {{design is}} {{implemented}} using RTL design techniques by employing Verilog HDL for both design and verification. Logic synthesis is completed by applying several different ASIC synthesis techniques {{to achieve the}} lowest possible area and the highest possible throughput. The design {{is based on a}} bit parallel approach in the submodules and a <b>pipelined</b> <b>approach</b> in the main encryption module. Basic building blocks such as adders and multipliers were designed for the lowest area/highest throughput principle. With a system clock frequency of 10 MhZ the designed device permits a data conversion rate of 193. 9 Mbits/sec...|$|R
