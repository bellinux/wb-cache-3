11|63|Public
40|$|AbstractIn {{this paper}} we present several {{algorithms}} to reorder unknowns in a finite-element mesh {{so that we}} can use the multicolour SOR method to solve the corresponding linear system on a <b>pipelined</b> <b>computer</b> or on a parallel computer. We also discuss the assembling process by reordering elements with our algorithms. Numerical tests on a <b>pipelined</b> <b>computer</b> indicate the efficiency of the multicolour SOR method...|$|E
40|$|The {{performance}} of a <b>pipelined</b> <b>computer</b> system depends heavily {{on the degree of}} Instruction Level Parallelism (ILP) obtained. Pipeline stalling or flushing due to unpredicted or mispredicted branch instructions has become a significant bottleneck to achieving high ILP. A good branch predictor can reduce the number of such stalls and flushes and is therefore critical to the {{performance of}} modern day computers...|$|E
40|$|In {{this paper}} we present several {{algorithms}} to reorder unknowns in a {{finite element mesh}} {{so that we can}} use the multicolour SOR method to solve the corresponding linear system on a <b>pipelined</b> <b>computer</b> or on a parallel computer. We also discuss the assembling process by reordering elements with our algorithms. Numerical tests on a <b>pipelined</b> <b>computer</b> indicate the efficiency of the multicolour SOR method. 1. Introduction We consider the solution of a sparse n Î˜ n linear system of equations Ax = b (1 : 1) arising from the finite element discretisation of partial differential equations, in particular we discuss the method of Successive Overrelaxation (SOR) on vector or parallel computers. Several authors e. g. Adams (1982), Hayes (1974), Lambiotte (1975), have observed that for the finite difference discretisation the classical Red/Black ordering of the grid points may increase the speed of the SOR method on vector or parallel computers. In the paper of Kincaid et al (1986), ITPACK i [...] ...|$|E
50|$|These {{were the}} first general purpose {{industry}} standard computer benchmarks. They do not necessarily obtain high scores on modern <b>pipelined</b> <b>computers.</b>|$|R
50|$|The ILLIAC II {{was one of}} {{the first}} <b>pipelined</b> <b>computers,</b> along with IBM's Stretch <b>Computer.</b> The <b>pipelined</b> control was {{designed}} by faculty member Donald B. Gillies. The pipeline stages were named Advanced Control, Delayed Control, and Interplay.|$|R
50|$|The center {{provides}} 24-hour {{study area}} with computer workstations, printers, scanners, study tables, and help desk support. The computer support center is also located within {{this section of}} the library. Staff at the center help students who are experiencing problems with their Wayne State email address, blackboard program, <b>pipeline,</b> <b>computer</b> applications, and other technologies.|$|R
40|$|An {{analytical}} {{model for a}} synchronous pipeline sub ject to stochastic input and output is presented. We find a nonlinear discrete-time equation that describes the time evolution of the occupancy probabilities in the pipeline and then solve a special case of the equation where the input and output are stationary Bernoulli processes. For this case, the two commonly used steady-state performance measures [...] latency and throughput [...] are obtained. The results obtained in this paper are applicable to <b>pipelined</b> <b>computer</b> systems such as network processors and media processors...|$|E
40|$|Since {{pipelining}} is a {{very important}} implementation technique for processors, students in Computer Science need to achieve a good understanding of it. UCO. MIPSIM simulator has been developed to support teaching such concepts. This paper introduces the basics of pipelining and describes UCO. MIPSIM, its main components and its functions. The learning effectiveness of the simulator has been tested by means of the comparison with two learning tools, the traditional paper and pencil and another <b>pipelined</b> <b>computer</b> simulator. In this way, a tool evaluation methodology is also introduce...|$|E
40|$|Decoupled {{computer}} architectures provide {{an effective means}} of exploiting instruction level parallelism. Selftimed micropipeline systems are inherently decoupled due to the elastic nature of the basic FIFO structure, and may be ideally suited for constructing decoupled {{computer architectures}}. Fred is a self-timed decoupled, <b>pipelined</b> <b>computer</b> architecture based on micropipelines. We present the architecture of Fred, with specific details on a micropipelined implementation that includes support for multiple functional units and out-of-order instruction completion due to the self-timed decoupling. 1. Introduction As computer systems have grown in size and complexity, the difficulty in synchronizing the system components has also grown. For example, simply distributing the clock signal throughout a large synchronous system can be a major source of complication. Clock skew is a serious concern in a large system, and is becoming significant even within a single chip. At the chip level, mor [...] ...|$|E
40|$|Abstract-An all-pairs {{problem is}} a {{computation}} on every possible subset consisting of two elements chosen from a set of n elements. N-body simulation and Householder reduction are all-pairs problems. The paper defines the all-pairs problem concisely by means of precedence ma-trices and derives a parallel algorithm. The algorithm is presented in both coarse-grain and medium-grain form. The all-pairs paradigm is illustrated by a pipeline for Householder reduction of a matrix to triangular form. Indez Terms-All-pairs paradigm, Householder reduction, Precedence matrices, <b>Pipelined</b> <b>computers...</b>|$|R
40|$|AbstractThe paper {{deals with}} the {{performance}} analysis of three Monte Carlo algorithms for some models of computer architectures. To estimate the performance and the speedup of these algorithms, we introduce a special modification of the criterion for {{the time required to}} achieve a preset probable error and consider a serial (von Neumann) architecture, a pipeline architecture, and two MIMD (Multiple Instruction stream, Multiple Data stream) parallel architectures. An approach to constructing Monte Carlo vector algorithms to be efficiently run on <b>pipeline</b> <b>computers</b> has also been considered...|$|R
40|$|The {{latest and}} {{possibly}} fastest {{of the general}} factoringmethods for large compositenumbers is the quadratic sieve of Carl Pomerance. A variation of the algorithm is described and an implementationis suggestedwhich combines the forces of a fast <b>pipeline</b> <b>computer</b> such as the Cray I, and a high speed highly parallel array processor such as the Goodyear MPP. A runnin ~ time analysis,which is based on empirical data rather than asymptoticestimatas, suggests that this method co~ld be capable of factoring a 60 digit number {{in as little as}} 10 minutes and a 100 digit number is as little as 60 days of continuous computer time...|$|R
40|$|Self-timed systems {{structured}} as multiple {{concurrent processes}} and communicating through self-timed queues are a convenient way to implement decoupled computer architectures. Machines {{of this type}} can exploit instruction level parallelism in a natural way, and can be easily modified and extended. However, providing a precise exception model for a self-timed micropipelined processor can be difficult, since the processor state does not change at uniformly discrete intervals. We present a precise exception method implemented for Fred, a self-timed, decoupled, <b>pipelined</b> <b>computer</b> architecture with out-of-order instruction completion. 1. Introduction Fred 1 is an architecture for a self-timed processor structured {{as a set of}} communicating micropipelines [9]. The basic Fred architecture is based roughly on the NSR (Non-Synchronous RISC) architecture developed at the University of Utah [1, 5]. The NSR is a simple 16 -bit machine designed to explore the potential of self-timed organization for [...] ...|$|E
40|$|VHDL, ????? ???????????? ??????????? ?????????? ? ??????????? ?????????, ????? ??????? ???????? ????. ??? ????? ?????????? ??????? ?? ??????? ????? VHDL ? ??????????? ????? ??????, ???? ??????? ?? ???????????? ? ???? ??? ?????????? ????. ?? ??????? ???????? ??????? ???????? ????? ?????????, ??????? ???? ?? ??????????? ??????????? ?????????? ? ??? ????? ??????????????. ?????????? ????? ????? ??????????????? ????, ???? ? ??????????? ?? ??? ????? ?????????? ? ?????????? ? ??????????? ???????. ?????? ???????????? ??????? ????????? ??? ?? ???????????? ??????????. ???????? ?????? ?????????? ??? ???????????? ?? ??? ???????? ??????? ???????? ? ????????? ????? ???????? ???????, ??????????? ? ????, ?????????? ???? ??????????? ??? ?? ????? ?????????? ?????? ?????????? ???????. ???????????, ?? ???????? ????`????? ?? ?????? ?????????????? ????`?????? ????? ?? ??????? ????? ???????????????? ??? ?????????? ????????? ? ?????????? ? ????????????????? ????????? ? ?????? ???? ?????????????? ??????? ???????????? ?????????????? ???????. ????????????? ????? ?????????? ?????????? ? ??????? ????????? ?????????? ??? ?????????? ?? ??????? ? ?????????????? ?????????? ???? ?????????, ???? ??????? ??????? ????? ?????????? ?? ????? ??? ????? ?????? ?????????? ???? ?? ???????? ??????. ?????????? ??? ??????????? ?????????????? ??????? ??????????????? ?????????? ?????????, ????? ?? ????????? ????????? ?????? Ethernet, ??????? ????-????????, ?????????????? ?????? ? ??????? GZIP. ?????????? ???? ??????????????? ? 8051, ??? ??? ????????? ????????? ?? 100 ???. ?????? ?? ???. ?????????? ? ??????????? ????????????????? ?????? ?????????? ?????????? ?? ?????? ???? Xilinx XCV- 4 SX 35. A set {{of methods}} for <b>pipelined</b> <b>computer</b> system {{synthesis}} is developed. The methods {{are based on}} mapping the spaced synchronous dataflow graphs (SSDFG) into the system structure and its schedule. The input data of the methods are initial SSDFG, given period of the algorithm implementation, and optimization criterium. The developed methods provide minimising the clock period as well as processor unit, register, multiplexor number, interprocessor communications, memory volume, energy consumption minimizations. A set of methods for synthesis of the <b>pipelined</b> <b>computer</b> systems for the field programable gate array (FPGA) is developed. It includes method based on the VHDL language, method of mapping the iterative algorithms with the control operators. The resulting computer structure is described by VHDL and is a project which {{is ready to be}} configured in FPGA or implemented in ASIC. Due to the fact that this project is built without the very structure synthesis, the project optimization process is straight and has less complexity. The method of SSDFG retiming is proposed which is directed and has less complexity comparing to the usual retiming. The method description level provides its implementation in some computer aided design frameworks. The methods are checked by the design of a set of application specific processors for the digital signal processing and the linear algebra problem solving, which are implemented in FPGA. The parameters of the resulting processes are equal to or supersede the parameters of the best known processors. It was found out that the network processors based on the configurable computers have much less energy consumption and increased speed comparing to the microprocessor systems. Theu can be configured by the means of proposed methods. A method for the static schedule of a set of tasks in the network processor system with the resource constraints is proposed which minimizes the common computation time due do the data dependence graph processing. The method provides the effective schedule finding in real time. A set of intelectual property cores (IP cores) for the configurable network processor building is developed. It contains the media acces controller core, Reed-Solomon decoder, GZIP file decompressor. i 8051 microcontroller core is developed which has increased speed up to 100 mln. instructions per second. An experimental network processor based on Xilinx XCV- 4 SX 35 FPGA was designed and probed. ?????????? ??????????? ???????? ??????? ??????? ??????????? ?????????????? ?????? (??) ????? ??????????? ????????????????? ????? ?????????? ??????? ?????? (????) ? ????????? ?? ? ?? ??????????. ????????????? ?????? ???????????? ??? ???????? ??????? ?????????? ????????? ??????????? ???????????? ????????? ?????????, ?????????? ???????????? ?????????, ?? ?????????, ???????????????, ?????????????, ?????? ??????, ? ????? ?????????????????. ??????????? ?????? ??????? ??????????? ?? ??? ??????????????? ?????????? ???????????? ???? (????), ??????? ????? ?????????????? ? ?????????????? ????? VHDL, ????? ??????????? ????????????? ?????????? ? ??????????? ??????????, ????? ??????? ???????? ????. ??? ???? ??????????? ??????? ?? ??????? ?????? VHDL ? ???????????? ????? ??????, ??????? ? ????????????? ? ???? ??? ???????? ????. ??? ??????? ?????????? ????? ?????????? ????? ?????????, ????????? ???? ??? ??????????? ??????????? ??????????? ? ????? ??????? ????????????. ?????????? ????? ????? ??????????????? ????, ??????? ???????? ???????????? ? ????? ??????? ????????? ?? ????????? ? ???????????? ???????. ??????? ???????????? ??????? ??????????? ??? ?? ?????????????? ??????????. ?????????? ?????? ????????? ??? ?????????????? ?? ??? ???????? ????????? ???????? ? ??????? ????? ???????? ???????, ????????????? ? ????, ????????? ??????? ????????? ??? ?? ???? ?????????? ?????? ?????????? ????????. ???????????, ??? ??????? ?????????? ?? ?????? ???????????????? ??????????? ????? ?? ??????? ??????? ????????????????? ??? ?????????? ?????????????? ? ????????? ? ?????????????????? ????????? ? ????? ???? ????????????????? ????????? ?????????? ???????????? ???????. ????????? ????? ???????????? ???????????? ? ??????? ??????? ??????????? ??? ???????????? ?? ??????? ? ???????????? ?????? ??????? ??????????, ??????? ????????? ????????? ????? ??????????? ?? ??????, ????????? ????????? ????? ???????????? ????? ? ?????????? ????. ?????????? ??? ??????????? ?????????????? ??????? ???????????????? ???????? ??????????, ????? ??? ?????????? ????????? ???? Ethernet, ??????? ????? ????-????????, ??????????? ?????? ? ??????? GZIP. ??????????? ???? ???????????????? ? 8051, ??????? ????? ?????????? ?????????????? ?? 100 ???. ?????? ?? ???. ?????????? ? ??????? ????????????????? ??????? ???????? ?????????? ?? ?????? ???? Xilinx XCV- 4 SX 35...|$|E
40|$|This paper {{presents}} a parallel <b>pipelined</b> <b>computer</b> architecture and its six network configurations targeted {{for the implementation}} {{of a wide range of}} digital signal processing (DSP) algorithms described by both atomic and large grain data flow graphs. The proposed architecture is considered together with programmability, yielding a system solution that combines extensive concurrency with simple programming. It is an SSIMD (Skewed Single Instruction Multiple Data) or MIMD (Multiple Instruction Multiple Data) machine depending on the algorithms implemented and the programming methodologies. The concurrency that can be exploited by the algorithms using this Parallel pipelined architecture is both temporal and spatial concurrency. The third level of concurrency (second spatial concurrency) can be also achieved by using input and output synchronized circular buses. An experimental parallel pipelined AdEPar (Advanced Educational Parallel) DSP system architecture, and its network configurations using printed circuit boards (as processing elements-PEs) based on DSP processors were designed and implemented. The hardware debugging of parallel programs and development of other high level programming tools such as automatic task schedulers and code generators) are relatively easy for the AdEPar architecture compared with other architectures having complicated interprocessor communications. Key words: Digital signal processing, parallel processing, parallel pipelined architecture. 1...|$|E
40|$|We {{are rapidly}} {{entering}} {{the era of}} genomics. The dramatic cost reduction of DNA sequencing due {{to the introduction of}} Next Generation Sequencing (NGS) techniques has resulted in an exponential growth of genetics data. The amount of data generated, and its associated processing into useful information, poses serious computational challenges. Here, we give a brief introduction of NGS, show a typical NGS processing pipeline, and show the associated challenges from a computational perspective. A case study is presented where one component of the NGS processing pipeline is accelerated: BWA-MEM, the de-facto industry-standard for the mapping stage. This is a first step in achieving a fully heterogeneously accelerated NGS <b>pipeline.</b> <b>Computer</b> EngineeringQuantum EngineeringFTQC/Bertels La...|$|R
5000|$|Designers {{looked for}} alternatives.A {{successful}} alternative is the Earle latch. It requires {{only a single}} data input, and its output takes a constant two gate delays. In addition, the two gate levels of the Earle latch can, in some cases, be merged with the last two gate levels of the circuits driving the latch because many common computational circuits have an OR layer followed by an AND layer as their last two levels. Merging the latch function can implement the latch with no additional gate delays. [...] The merge is commonly exploited {{in the design of}} <b>pipelined</b> <b>computers,</b> and, in fact, was originally developed by J. G. Earle {{to be used in the}} IBM System/360 Model 91 for that purpose.|$|R
5000|$|... openPipeline is an {{open-source}} plug-in for Autodesk Maya that {{is designed}} to assist in a Production <b>Pipeline</b> structure and <b>Computer</b> animation.|$|R
40|$|Predicting future {{outcomes}} {{based on}} past observational data {{is a common}} application in data mining. While the primary goal is usually to achieve the highest possible prediction accuracy, {{the interpretation of the}} resulting prediction model is important to understand its shortcomings for further improvements. Throughout this paper we focus on branch prediction, where the (binary) outcome of a test is needed for enhancing the performance of <b>pipelined</b> <b>computer</b> architectures. Many research has been done in this domain and different branch prediction solutions are described in the literature. The quality of a prediction model is highly dependent {{on the quality of the}} available data. Especially the choice of the related variables or features to base the prediction on is important. In this paper we evaluate the predictive power of different branch prediction features using the metric Gini-index, which is used as feature selection measure in the construction of decision trees. We observe that through this Gini-metric an explanation can be provided for the performance of existing branch predictors. We show that the Gini-index is a good metric for comparing branch prediction features. Further, we found that a feature can have good discriminative capacities, although this does not result in very good accuracies because of shortcomings in the predictor implementation...|$|E
40|$|Abstract. Predicting future {{outcomes}} {{based on}} past observational data {{is a common}} application in data mining. While the primary goal is usually to achieve the highest possible prediction accuracy, {{the interpretation of the}} resulting prediction model is important to understand its shortcomings for further improvements. Throughout this paper we focus on branch prediction, where the (binary) outcome of a test is needed for enhancing the performance of <b>pipelined</b> <b>computer</b> architectures. Many research has been done in this domain and different branch prediction solutions are described in the literature. The quality of a prediction model is highly dependent {{on the quality of the}} available data. Especially the choice of the related variables or features to base the prediction on is important. In this paper we evaluate the predictive power of different branch prediction features using the metric Gini-index, which is used as feature selection measure in the construction of decision trees. We observe that through this Gini-metric an explanation can be provided for the performance of existing branch predictors. We show that the Gini-index is a good metric for comparing branch prediction features. Further, we found that a feature can have good discriminative capacities, although this does not result in very good accuracies because of shortcomings in the predictor implementation. Keywords: computer architecture, branch prediction, data mining, decision tree, Gini-index 1...|$|E
40|$|Librarians {{will assist}} {{you with your}} {{research}} and information needs. Stop by, call (808) 544 - 1133, {{or e-mail us at}} reference @hpu. edu. Chat with us online on the Libraries Tab, Campus <b>Pipeline.</b> <b>Computers</b> & Internet Access: Access the Internet, library databases, e-mail, and Microsoft Office products at these computer workstations. Print to the HPU UniCard printer . In addition, iMac computers and other software applications are available. Research Databases: Access more than 75 + full-text, research databases covering multiple subject areas from any of these computer workstations. Print to the HPU UniCard printer. Library Catalog: This workstation is specifically designated for the HPU Library Catalog. The catalog allows you to find books available in the HPU Libraries including AV and reserve materials. Access the catalog online a...|$|R
40|$|Three {{parallel}} algorithms {{were compared}} for the direct solution of tridiagonal linear systems of equations. The algorithms {{are suitable for}} computers such as ILLIAC 4 and CDC STAR. For array computers similar to ILLIAC 4, cyclic odd-even reduction has the least operation count for highly structured sets of equations, and recursive doubling has the least count for relatively unstructured sets of equations. Since the difference in operation counts for these two algorithms is not substantial, their relative running times may be more related to overhead operations, which are not measured in this paper. The third algorithm, based on Buneman's Poisson solver, has more arithmetic operations than the others, {{and appears to be}} the least favorable. For <b>pipeline</b> <b>computers</b> similar to CDC STAR, cyclic odd-even reduction appears to be the most preferable algorithm for all cases...|$|R
40|$|The {{performance}} degradation caused by branch instructions in <b>pipelined</b> <b>computers</b> is well known. The degradation is even greater on <b>computers</b> with multiple <b>pipelines</b> processing a single instruction stream, such as superscalar and scalable compound instruction-set machines (SCISM). Several branch prediction schemes {{have been proposed}} that attempt to reduce this performance penalty. One of these [...] dynamic prediction of branch outcomes by tagging instructions in an instruction cache with prediction information [...] is adapted to an IBM ESA/ 370 SCISM implementation with several important additions. The adaptation may be extended to other architectures with similar characteristics. More significantly, a scheme is developed that allows the predominant IBM ESA/ 370 branch instructions {{to be removed from}} the instruction stream. These instructions, in effect, execute in zero time when the prediction is correct, thereby significantly increasing the performance achieved by the base SCISM machine org [...] ...|$|R
40|$|The {{performance}} of a mobile robot crucially depends on the accuracy, duration and reliability of its sensor interpretation. A major source of information are CCD-cameras which provide a detailed view of the robot's environment. This paper presents a real-time stereo algorithm implemented on the mobile robot RHINO of the University of Bonn. The algorithm exploit the phases of wavelet-filtered image pairs to localize edges and to estimate their disparities with subpixel accuracy. The disparities are computed by an initial search for corresponding points within a given interval and a subsequent measurement of phasedif ferences. The real-time constraints of autonomous object detection and navigation are fulf illed by partially implementing the stereo algorithm on a <b>pipeline</b> <b>computer</b> Datacube. Experimental results on real world scenes under real world conditions demonstrate the stereo algorithm's robustness and suitability for autonomous robot applications...|$|R
40|$|Abstmct-The {{performance}} of pipelined processors is lim-ited by data dependencies and branch instructions. In {{order to achieve}} high performance, mechanisms must exist to alleviate the effects of data dependencies and branch instructions. Further-more, in many cases, for example the support of virtual memory, it is essential interrupts be precise. In multiple functional unit pipelined processors where the instructions can complete and update {{the state of the}} machine out of program order, hard-ware support must be provided to implement precise interrupts. In this paper, we combine the problems of data dependency resolution and precise interrupt implementation. We present a design for a hardware mechanism that resolves dependencies dynamically and, at the same time, guarantees precise inter-rupts. Simulation studies show that, by resolving dependencies, the proposed mechanism is able to obtain a significant speedup over a simple instruction issue mechanism as well as implement precise interrupts. Index Terms- Dependency resolution, multiple functional units, out-of-order execution, <b>pipelined</b> <b>computers,</b> precise in-terrupts, register update unit, Tomasuloâ€™s algorithm. I...|$|R
40|$|In {{the last}} decade many models for {{parallel}} computation have been proposed and many parallel algorithms have been developed. However, few of these models have been realized {{and most of these}} algorithms are supposed to run on idealized, unrealistic parallel machines. The parallel machines constructed so far all use a simple model of parallel computation. Therefore, not every existing parallel machine is equally well suited for each type of algorithm. The adaptation of a certain algorithm to a specific parallel archi- tecture may severely increase the complexity of the algorithm or severely obscure its essence. Little is known about the performance of some standard combinatorial algorithms on existing parallel machines. In this paper we present computational results concerning the solution of knapsack, shortest paths and change-making problems by branch and bound, dynamic programming, and divide and conquer algorithms on the ICL-DAP (an SIMD computer), the Manchester dataflow machine and the CDC-CYBER- 205 (a <b>pipeline</b> <b>computer)</b> ...|$|R
40|$|Improving leak {{detection}} capability to eliminate undetected releases {{is an area}} of focus for the energy pipeline industry, and the pipeline companies are working to improve existing methods for monitoring their <b>pipelines.</b> <b>Computer</b> model-based {{leak detection}} methods that detect leaks by analyzing the pipeline hydraulic state have been widely employed in the industry, but their effectiveness in practical applications is often challenged by real-world uncertainties. This study quantitatively assessed the effects of uncertainties on leak detectability of a commonly used real-time transient model-based leak detection system. Uncertainties in fluid properties, field sensors, and the data acquisition system were evaluated. Errors were introduced into the input variables of the leak detection system individually and collectively, and the changes in leak detectability caused by the uncertainties were quantified using simulated leaks. This study provides valuable quantitative results contributing towards {{a better understanding of how}} real-world uncertainties affect leak detection. A general ranking of the importance of the uncertainty sources was obtained: from high to low it is time skew, bulk modulus error, viscosity error, and polling time. It was also shown that inertia-dominated pipeline systems were less sensitive to uncertainties compared to friction-dominated systems...|$|R
40|$|A {{model for}} digital {{parallel}} systems composed by mutually asynchronous modules, {{each of them}} with processing speed in general variable, is described, by means {{of which it is}} possible to realize parallel and/or <b>pipeline</b> <b>computers</b> in a systematic and correct way. Each module of a system corresponding to the proposed model has its own control part and its own operation part, and the behaviour of the entire system, called PMC system, depends exclusively on the exchange of messages among the control parts of the modules belonging to the system. Moreover the modules exchange each other data by means of queues: data held in queues and messages are the only resources shared by modules. Three rules of message exchange among modules are defined, obtaining three types of systems called PMC systems with weakly linked modules, with strongly linked modules and with strongly linked modules looking-ahead. Some ways of transmitting messages are introduced which give rise to several schemes of the control structure of the entire system. The activity of a PMC system is described by a graph; by some constraints Lmposed on this graph it is possible to define a class of systems, called PMC linear systems, into which any PMC system can be transformed. For PMC linear systems, with regard to the rules of message exchange, general conditions are given under which such systems are determinate and deadlock-free...|$|R
40|$|Microorganisms such as {{protozoa}} {{and bacteria}} play very {{important roles in}} many practical domains, like agriculture, industry and medicine. To explore functions of different categories of microorganisms is a fundamental work in biological studies, which can assist biologists and related scientists {{to get to know}} more properties, habits and characteristics of these tiny but obbligato living beings. However, taxonomy of microorganisms (microorganism classification) is traditionally investigated through morphological, chemical or physical analysis, which is time and money consuming. In order to overcome this, since the 1970 s innovative content-based microscopic image analysis (CBMIA) approaches are introduced to microbiological fields. CBMIA methods classify microorganisms into different categories using multiple artificial intelligence approaches, such as machine vision, pattern recognition and machine learning algorithms. Furthermore, because CBMIA approaches are semi- or full-automatic computer-based methods, they are very efficient and labour cost saving, supporting a technical feasibility for microorganism classification in our current big data age. In this article, we review the development history of microorganism classification using CBMIA approaches with two crossed pipelines. In the first pipeline, all related works are grouped by their corresponding microorganism application domains. By this pipeline, it is easy for microbiologists to have an insight into each special application domain and find their interested applied CBMIA techniques. In the second pipeline, the related works in each application domain are reviewed by time periods. Using this <b>pipeline,</b> <b>computer</b> scientists can see the dynamic of technological development clearly and keep up with the future development trend in this interdisciplinary field. In addition, the frequently-used CBMIA methods are further analysed to find technological common points and potential reasons...|$|R
5000|$|With {{the advent}} of {{instruction}} <b>pipelining</b> in modern <b>computers</b> and multi-core processors facilitating parallel computing, two or more operations can be performed at once. This can speed up the lowly temporary-variable swap algorithm and give it an edge over other algorithms. For example, the XOR swap algorithm requires sequential execution of three instructions. However, using two temporary registers, two processors executing in parallel can swap two variables in two clock cycles: ...|$|R
40|$|This paper {{presents}} {{recent work}} on {{the improvement of the}} robotics vision based control strategy for underwater pipeline tracking system. The study focuses on developing image processing algorithms and a fuzzy inference system for the analysis of the terrain. The main goal is to implement the supervisory fuzzy learning control technique to reduce the errors on navigation decision due to the pipeline occlusion problem. The system developed is capable of interpreting underwater images containing occluded pipeline, seabed and other unwanted noise. The algorithm proposed in previous work does not explore the cooperation between fuzzy controllers, knowledge and learnt data to improve the outputs for underwater <b>pipeline</b> tracking. <b>Computer</b> simulations and prototype simulations demonstrate the effectiveness of this approach. The system accuracy level has also been discussed...|$|R
50|$|In computing, MISD (multiple instruction, single data) {{is a type}} of {{parallel}} computing architecture where many functional units perform different operations on the same data. Pipeline architectures belong to this type, though a purist might say that the data is different after processing by each stage in the <b>pipeline.</b> Fault-tolerant <b>computers</b> executing the same instructions redundantly in order to detect and mask errors, in a manner known as task replication, may be considered to belong to this type. Not many instances of this architecture exist, as MIMD and SIMD are often more appropriate for common data parallel techniques. Specifically, they allow better scaling and use of computational resources than MISD does. However, one prominent example of MISD in computing are the Space Shuttle flight control computers.|$|R
40|$|A {{continuous}} model and a combined event-process discrete model are developed by employing the Slam II general purpose simulation language. The {{continuous model}} addressed the on-going {{progress of the}} construction process on a higher level; and the combined event-process discrete model describes the pipeline construction on the operational level. Resource sharing and related issues also are investigated in the two models through an actual gas line project adopted {{for the purpose of}} this research. A comparison of the two models provides insights into the modelling approaches. It is concluded that the combined event-process model is more flexible and more powerful for modelling complex construction operations than the continuous model, but at the price of requiring a better understanding of the actual operations and more detailed information. Linear Construction, <b>Pipeline</b> Construction, <b>Computer</b> Simulation, Modelling, Planning, Slam Ii, Slamsystem,...|$|R
40|$|Abstract â€” <b>Pipelining</b> {{improves}} <b>computer</b> {{performance by}} overlapping {{the execution of}} several different instructions. If no interactions exist in the pipeline, then several instructions can be in different stages of execution simultaneously. However, dependencies between instructions â€“ particularly branch instructions â€“ prevent the processor from realizing its maximum performance. Branch instructions form a significant fraction of executed instructions and cause the maximum bottlenecks in any processorâ€™s performance. Their design is thus a crucial component of any architecture. In this paper, we have proposed a novel architectural technique {{which can be used}} to boost performance of modern day processors. It is especially useful in certain code constructs like small loops and try-catch blocks. The technique was implemented in a RISC microprocessor, named MP and it aims at improving performance by reducing the number of instructions that need to enter the pipeline itself...|$|R
40|$|This paper {{presents}} {{the idea of}} naturally encoding three-dimensional (3 D) range data into regular two-dimensional (2 D) images utilizing <b>computer</b> graphics rendering <b>pipeline.</b> The <b>computer</b> graphics <b>pipeline</b> provides a means to sample 3 D geometry data into regular 2 D images, and also to retrieve the depth information for each sampled pixel. The depth information for each pixel is further encoded into red, green, and blue color channels of regular 2 D images. The 2 D images can further be compressed with existing 2 D image compression techniques. By this novel means, 3 D geometry data obtained by 3 D range scanners can be instantaneously compressed into 2 D images, providing a novel way of storing 3 D range data into its 2 D counterparts. We will present experimental results to verify the performance of this proposed technique...|$|R
40|$|Abstractâ€”This paper {{presents}} {{recent work}} on {{the improvement of the}} robotics vision based control strategy for underwater pipeline tracking system. The study focuses on developing image processing algorithms and a fuzzy inference system for the analysis of the terrain. The main goal is to implement the supervisory fuzzy learning control technique to reduce the errors on navigation decision due to the pipeline occlusion problem. The system developed is capable of interpreting underwater images containing occluded pipeline, seabed and other unwanted noise. The algorithm proposed in previous work does not explore the cooperation between fuzzy controllers, knowledge and learnt data to improve the outputs for underwater <b>pipeline</b> tracking. <b>Computer</b> simulations and prototype simulations demonstrate the effectiveness of this approach. The system accuracy level has also been discussed. Keywordsâ€”Fuzzy logic, Underwater target tracking, Autonomous underwater vehicles, Artificial intelligence, Simulations, Robot navigation, Vision system. I...|$|R
