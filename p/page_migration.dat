86|46|Public
50|$|Key {{enhancements}} to {{the original}} CP/CMS system included changes in the dispatching algorithm and the paging system. Virtual memory was of course a new concept at the time, and the IBM System/360-67 address translation technology enabled various technical approaches. Ultimately, the VP/CSS <b>page</b> <b>migration</b> algorithm and three-queue dispatcher became well-known, and some NCSS personnel eventually joined IBM's Thomas J. Watson Research Center to work on VM technologies.|$|E
50|$|Early NCSS {{enhancements}} involved {{such areas}} as <b>page</b> <b>migration,</b> dispatching, file system, device support, and efficient fast-path hypervisor functions accessed via the diagnose (DIAG) instruction. Later features included a packet-switched network, FILEDEF-level (pipe) interprocess/intermachine communication, and database integration. Similar features also appeared in the VM implementation. Ultimately, the NCSS development team rivaled the size of IBM's, implementing {{a wide array of}} features. The VP/CSS platform remained in use through at least the mid-80s. NCSS was acquired by Dun & Bradstreet in 1979; renamed DBCS (Dun & Bradstreet Computer Services); increased its focus on the NOMAD product; changed its business strategy to embrace VM and other platforms; and in the process discontinued support and development of VP/CSS, probably the last non-VM fork of CP/CMS.|$|E
40|$|Abstract. Costly <b>page</b> <b>migration</b> {{is a major}} {{obstacle}} to integrating OpenMP and page-based software distributed shared memory (SDSM) to realize the easy-touse programming paradigm for SMP clusters. To reduce {{the impact of the}} <b>page</b> <b>migration</b> overhead on the execution time of an application, the previous researches have mainly focused on reducing the number of page migrations and hiding the <b>page</b> <b>migration</b> overhead by overlapping computation and communication. We propose the ‘collective-prefetch ’ technique, which overlaps page migrations themselves even when the prior approach cannot be effectively applied. Experiments with a communication-intensive application show that our technique reduces the <b>page</b> <b>migration</b> overhead significantly, and the overall execution time was reduced to 57 %~ 79 %. ...|$|E
30|$|Ma et al. (2012) {{presents}} a compression based approach, in which authors use a compression technique, called as Run Length Encoding (RLE), to reduce number of <b>pages</b> transferred during <b>migration.</b> Reduction in total <b>pages</b> transferred during <b>migration</b> leads to reduction in total migration time and downtime. The RLE technique compresses only allocated pages instead of all pages. For this, it uses Linux memory management mechanism buddy system which scans whole virtual memory and maintains {{a list of}} unallocated <b>pages.</b> During <b>migration,</b> in place of sending unallocated page, only one byte NODATA is sent. In this way, it tries to reduce total <b>pages</b> transferred during <b>migration</b> by sending only allocated pages in compressed form, and one byte NODATA for each unallocated page. Memory exploration module used in this work is guest dependent. For different guest operating systems, it requires to write different exploration module. Compression algorithm RLE reduces total data transferred as well as migration time but increases overhead of compression/decompression.|$|R
40|$|Modern DRAM {{architectures}} allow {{a number}} of low-power states on individual memory ranks for advanced power management. Many previous studies {{have taken advantage of}} demotions on low-power states for energy saving. However, most of the demotion schemes are statically performed on a limited number of pre-selected low-power states, and are suboptimal for different workloads and memory architectures. Even worse, the idle periods are often too short for effective power state transitions, especially for memory intensive applications. Wrong decisions on power state transition incur significant energy and delay penalties. In this paper, we propose a novel memory system design named RAMZzz with rank-aware energy saving optimizations including dynamic <b>page</b> <b>migrations</b> and adaptive demotions. Specifically, we group the pages with similar access locality into the same rank with dynamic <b>page</b> <b>migrations.</b> Ranks have their hotness: hot ranks are kept busy for high utilization and cold ranks can have more lengthy idle periods for power state transitions. We further develop adaptive state demotions by considering all low-power states for each rank and a prediction model to estimate the power-down timeout among states. We experimentally compare our algorithm with other energy saving policies with cycle-accurate simulation. Experiments with benchmark workloads show that RAMZzz achieves significant improvement on energy-delay 2 and energy consumption over other energy saving techniques. Comment: 19 page...|$|R
40|$|Shared memory {{programs}} {{running on}} Non-Uniform Memory Access (NUMA) machines usually face inherent performance problems stemming from excessive remote memory accesses. A solution, called the Adaptive Runtime System (ARS), {{is presented in}} this paper. ARS is designed to adjust the data distribution at runtime through automatic <b>page</b> <b>migrations.</b> It uses memory access histograms gathered by hardware monitors to find access hot spots and, based on this detection, to dynamically and transparently modify the data layout. In this way, incorrectly allocated data can be moved to the most appropriate node and hence data locality can be improved. Simulations show that this allows to achieve a performance gain of as high as 40 %. 2002 Elsevier Science B. V. All rights reserved...|$|R
40|$|Abstract. This paper {{describes}} transparent {{mechanisms for}} emulating {{some of the}} data distribution facilities offered by traditional data-parallel programming models, such as High Performance Fortran, in OpenMP. The vehicle for implementing these facilities in OpenMP without modifying the programming model or exporting data distribution details to the programmer is user-level dynamic <b>page</b> <b>migration</b> [9, 10]. We have implemented a runtime system called UPMlib, which allows the compiler to inject into the application a smart user-level <b>page</b> <b>migration</b> engine. The <b>page</b> <b>migration</b> engine improves transparently the locality of memory references at the page level {{on behalf of the}} application. This engine can accurately and timely establish effective initial page placement schemes for OpenMP programs. Furthermore, it incorporates mechanisms for tuning page placement across phase changes in the application communication pattern. The effectiveness of <b>page</b> <b>migration</b> in these cases depends heavily on the overhead of page movements, the duration of phases in the application code and architectural characteristics. In general, dynamic <b>page</b> <b>migration</b> between phases is effective if the duration of a phase is long enough to amortize the cost of page movements. ...|$|E
40|$|We {{present an}} {{extension}} of a classical data management subproblem, the <b>page</b> <b>migration.</b> The problem is investigated in dynamic networks, where costs of communication between different nodes may change with time. We construct asymptotically optimal online algorithms for this problem, both in deterministic and randomized scenarios. Key words: online algorithms, randomized algorithms, <b>page</b> <b>migration,</b> data management, dynamic networks...|$|E
40|$|This paper {{describes}} the mechanisms offered by UPMLIB for emulating data distribution and redistribution, features included in data parallel programming paradigms (for instance, High Performance Fortran). UPMLIB allows the compiler to insert in the application, without neither programmer intervention nor modification of OpenMP, a smart user-level <b>page</b> <b>migration</b> engine {{that makes it}} immune to performance flaws related to page placement. This engine can accurately and timely fix poor initial page placement schemes. However, our dynamic <b>page</b> <b>migration</b> engine is unable to tune page placement at a fine-grain time scale, such as in {{cases in which the}} program exhibits phase changes in the page reference pattern. The effectiveness of <b>page</b> <b>migration</b> in these cases is limited by the overhead of coherent page movements and the low remote-to-local memory access latency ratio of contemporary ccNUMA systems...|$|E
40|$|We {{present the}} design and {{implementation}} of UPMLIB, a runtime system that provides transparent facilities for dynamically tuning the memory performance of OpenMP programs on scalable shared-memory multiprocessors with hardware cache-coherence. UPMLIB integrates information from the compiler and the operating system, to implement algorithms that perform accurate and timely <b>page</b> <b>migrations.</b> The algorithms and the associated mechanisms correlate memory reference information with the semantics of parallel programs and scheduling events that break the association between threads and data for which threads have memory affinity at runtime. Our experimental evidence shows that UPMLIB makes OpenMP programs immune to the page placement strategy of the operating system, thus obviating the need for introducing data placement directives in OpenMP. Furthermore, UPMlib provides solid improvements of throughput in multiprogrammed execution environments. Keywords: OpenMP, scalable shared [...] ...|$|R
40|$|Abstract. We {{present the}} design and {{implementation}} of UPMLIB, a runtime system that provides transparent facilities for dynamically tuning the memory performance of OpenMP programs on scalable shared-memory multiprocessors with hardware cache-coherence. UPMLIB integrates information from the compiler and the operating system, to implement algorithms that perform accurate and timely <b>page</b> <b>migrations.</b> The algorithms and the associated mechanisms correlate memory reference information with the semantics of parallel programs and scheduling events that break the association between threads and data for which threads have memory affinity at runtime. Our experimental evidence shows that UPMLIB makes OpenMP programs immune to the page placement strategy of the operating system, thus obviating the need for introducing data placement directives in OpenMP. Furthermore, UPMlib provides solid improvements of throughput in multiprogrammed execution environments...|$|R
40|$|Abstract—Main {{memory is}} a {{significant}} energy consumer which may contribute to over 40 % of the total system power, and will become more significant for server machines with more main memory. In this paper, we propose a novel memory system design named RAMZzz with rank-aware energy saving optimizations. Specifically, we rely on a memory controller to monitor the memory access locality, and group the pages with similar access locality into the same rank. We further develop dynamic <b>page</b> <b>migrations</b> to adapt to data access patterns, and a prediction model to estimate the demotion time for accurate control on pow-er state transitions. We experimentally compare our algorithm with other energy saving policies with cycle-accurate simulation. Experiments with benchmark workloads show that RAMZzz achieves significant improvement on energy-delay 2 and energy consumption over other power saving techniques. I...|$|R
40|$|The {{performance}} of multiprogrammed shared-memory multiprocessors suffers often from scheduler interventions that neglect data locality. On cachecoherent distributed shared-memory (DSM) multiprocessors, such scheduler interventions tend {{to increase the}} rate of remote memory accesses. This paper presents a novel dynamic <b>page</b> <b>migration</b> algorithm that remedies this problem in iterative parallel programs. The purpose of the algorithm is the early detection of pages that {{will most likely be}} accessed remotely by threads associated with them via a thread-to-memory affinity relation. The key mechanism that enables timely identification of these pages is a communication interface between the <b>page</b> <b>migration</b> engine and the operating system scheduler. The algorithm improves previously proposed competitive <b>page</b> <b>migration</b> algorithms in many aspects, including accuracy, timeliness and cost amortization. Most notably, the algorithm is not biased by obsolete memor...|$|E
40|$|In this paper, we {{claim that}} memory {{migration}} mechanism {{is a useful}} approach to improve the execution of parallel applications in dynamic execution environments, but that their performance depends on related system components such as the processor scheduling. To show that, we evaluate the automatic memory migration mechanism provided by IRIX in Origin systems, under different dynamic processor allocation policies when executing OpenMP parallel multiprogrammed workloads. We have focused the evaluation {{on the effects of}} the <b>page</b> <b>migration</b> mechanism on the CPU time consumed by each application, the processor allocation received, and the speedup. Results demonstrate that, if the processor scheduler is memory conscious, that is, it maintains as much as possible the system stable, the automatic memory <b>page</b> <b>migration</b> mechanism provided by IRIX improves the CPU time consumed by OpenMP applications. Keywords: CC-NUMA. Memory <b>page</b> <b>migration.</b> Dynamic processor allocation policy. OpenMP. Multiprogrammed workload. 1...|$|E
40|$|This paper {{presents}} algorithms {{for improving}} the performance of parallel programs on multiprogrammed sharedmemory NUMA multiprocessors, via the use of user-level dynamic <b>page</b> <b>migration.</b> The idea that drives the algorithms is that a <b>page</b> <b>migration</b> engine can perform accurate and timely page migrations in a multiprogrammed system if it can correlate page reference information with scheduling information obtained from the operating system. The necessary page migrations can be performed {{as a response to}} scheduling events that break the implicit association between threads and their memory affinity sets. We present two algorithms that use feedback from the kernel scheduler to aggressively migrate pages upon thread migrations. The first algorithm exploits the iterative nature of parallel programs, while the second targets generic codes without making assumptions on their structure. Performance evaluation on an SGI Origin 2000 shows that our <b>page</b> <b>migration</b> algorithms provide substantial improv [...] ...|$|E
40|$|The Flash-based {{storage device}} is {{becoming}} a viable storage solution for mobile and desktop systems. In this paper, we propose a novel garbage collection technique, called buffer-aware garbage collection (BAGC), for flash-based storage devices. The BAGC improves the efficiency of two main steps of garbage collection, a block merge step and a victim block selection step, by taking account {{of the contents of}} a buffer cache, which is typically used to enhance I/O performance. The buffer-aware block merge (BABM) scheme eliminates unnecessary <b>page</b> <b>migrations</b> by evicting dirty data from a buffer cache during a block merge step. The bufferaware victim block selection (BAVBS) scheme, on the other hand, selects a victim block so that the benefit of the buffer-aware block merge is maximized. To identify the invalid pages and to delete the invalid pages from memory. The gated clock method is used to implemented in flash storage device...|$|R
30|$|Algorithm 1 {{shows the}} {{performance}} calculation of virtual machine live migration on different parameters like total <b>pages</b> transferred, total <b>migration</b> time and downtime, also given by (2011 b).|$|R
30|$|Method {{presented}} in this paper works in all three phases and tries to reduce extra <b>pages</b> transferred during <b>migration</b> from all three phases. It effectively improves migration performance with less overhead.|$|R
40|$|This paper {{presents}} user-level dynamic <b>page</b> <b>migration,</b> a runtime technique which transparently enables parallel {{programs to}} tune their memory performance on distributed shared memory multiprocessors, with feedback obtained from dynamic monitoring of memory activity. Our technique exploits the iterative nature of parallel programs and {{information available to}} the program both at compile time and at runtime {{in order to improve}} the accuracy and the timeliness of page migrations, as well as amortize better the overhead, compared to <b>page</b> <b>migration</b> engines implemented in the operating system. We present an adaptive <b>page</b> <b>migration</b> algorithm based on a competitive and a predictive criterion. The competitive criterion is used to correct poor page placement decisions of the operating system, while the predictive criterion makes the algorithm responsive to scheduling events that necessitate immediate page migrations, such as preemptions and migrations of threads. We also present a new technique f [...] ...|$|E
40|$|Scientific {{computing}} is used {{frequently in}} {{an increasing number}} of disciplines to accelerate scientific discovery. Many such computing problems involve the numerical solution of partial differential equations (PDE). In this thesis we explore and develop methodology for high-performance implementations of PDE solvers for shared-memory multiprocessor architectures. We consider three realistic PDE settings: solution of the Maxwell equations in 3 D using an unstructured grid and the method of conjugate gradients, solution of the Poisson equation in 3 D using a geometric multigrid method, and solution of an advection equation in 2 D using structured adaptive mesh refinement. We apply software optimization techniques to increase both parallel efficiency and the degree of data locality. In our evaluation we use several different shared-memory architectures ranging from symmetric multiprocessors and distributed shared-memory architectures to chip-multiprocessors. For distributed shared-memory systems we explore methods of data distribution {{to increase the amount of}} geographical locality. We evaluate automatic and transparent <b>page</b> <b>migration</b> based on runtime sampling, user-initiated <b>page</b> <b>migration</b> using a directive with an affinity-on-next-touch semantic, and algorithmic optimizations for page-placement policies. Our results show that <b>page</b> <b>migration</b> increases the amount of geographical locality and that the parallel overhead related to <b>page</b> <b>migration</b> can be amortized over the iterations needed to reach convergence. This is especially true for the affinity-on-next-touch methodology whereby <b>page</b> <b>migration</b> can be initiated at an early stage in the algorithms. We also develop and explore methodology for other forms of data locality and conclude that the effect on performance is significant and that this effect will increase for future shared-memory architectures. Our overall conclusion is that, if the involved locality issues are addressed, the shared-memory programming model provides an efficient and productive environment for solving many important PDE problems...|$|E
40|$|This {{paper is}} {{concerned}} with the <b>page</b> <b>migration</b> (or file migration) problem [BS 89] as part of a large class of on-line problems. The <b>page</b> <b>migration</b> problem deals with the management of pages residing in a network of processors. In the classical problem there is only one copy of each page which is accessed by different processors over time. The page is allowed to be migrated between processors. However a migration incurs higher communication cost than an access (proportionally to the page size). The problem is that of deciding when and where to migrate the page in order to lower access costs. A more general setting is the k-page migration where we wish to maintain k copies of the page. The <b>page</b> <b>migration</b> problems are concerned with a dilemma common to many on-line problems: determining when is it beneficial to make configuration changes. We deal with the relaxed task systems model which captures a large class of problems of this type, that can be described as the generalizati [...] ...|$|E
30|$|The page re-send {{problem was}} first {{discussed}} by Clark et al. [33] {{and can lead}} to excessive resource consumption, as only the final version of a page is used and re-sending <b>pages</b> during <b>migration</b> consume both network and CPU resources. Furthermore, the page re-send problem is a challenge to the predictability criteria as it is not known beforehand the total number of pages that are to be re-transferred, making it difficult to estimate how long migration takes to complete.|$|R
30|$|Other than this, for the {{execution}} of precopy method, Xen uses some data structure for effective transfer of virtual machine memory pages during virtual machine live <b>migration.</b> Guest <b>page</b> table are managed by guest itself and pointed by CR 3 register. Initially, Xen makes guest page tables read-only and when the guest tries to change or update its <b>page</b> table during <b>migration,</b> a <b>page</b> fault occurs. Xen employs shadow page table under the running virtual machine to log information of updated pages. It uses log of updated <b>pages</b> during <b>migration.</b> Xen also uses another bitmap named dirty log bitmap, which also contains the log of updated or dirty pages. When pages are updated during migration, the changes are propagated to both shadow page table and dirty log bitmap. Both are used to manage transferring of virtual machine pages {{at the time of}} migration and for each iteration bitmap is scanned to locate updated pages for transferring.|$|R
30|$|Svärd et al. (2011) {{also used}} {{compression}} approach with dynamic page transfer reordering. This approach orders the page transfer {{in such a}} way that retransfer of frequently updated pages is reduced. Based on the number of times a page is being updated during <b>migration,</b> <b>page</b> weight for each page is calculated and pages are reordered accordingly. This results into reduced number of <b>pages</b> transferred during <b>migration.</b> For further improvement, authors combine this idea with delta compression technique which leads to reduction in both migration time as well as downtime. Delta compression reduces amount of data transferred by sending XOR deltas between previous and current page versions instead of full page. It is highly dependent on fast, efficient page privatization schemes and compression. Here, again overhead is high.|$|R
40|$|The <b>page</b> <b>migration</b> problem {{occurs in}} {{managing}} a globally addressed shared memory in a multiprocessor system. Each physical page of memory {{is located at}} a given processor, and memory references to that page by other processors are charged a cost equal to the network distance. At times the page may migrate between processors, at a cost equal to the distance times a page size factor, D. The problem is to schedule movements on-line so as to minimize {{the total cost of}} memory references. <b>Page</b> <b>migration</b> can also be viewed as a restriction of the 1 -server with excursions problem. This paper presents a collection of algorithms and lower bounds for the <b>page</b> <b>migration</b> problem in various settings. Competitive analysis is used. The competitiveness of an on-line algorithm is the worst-case ratio of its cost to the optimum cost on any sequence of requests. Randomized (2 + 1 2 D) -competitive on-line algorithms are given for trees and products of trees, including the mesh and the hypercube, and for un [...] ...|$|E
40|$|This paper explores {{previously}} {{established and}} novel methods for scaling {{the performance of}} OpenMP on NUMA architectures. The spectrum of methods under investigation includes OS-level automatic page placement algorithms, dynamic <b>page</b> <b>migration,</b> and manual data distribution. The trade-off that these methods face lies between performance and programming effort. Automatic page placement algorithms are transparent to the programmer, but may compromise memory access locality. Dynamic <b>page</b> <b>migration</b> is also transparent, but requires careful engineering of online algorithms to be effective. Manual data distribution on the other requires substantial programming effort and architecture-specific extensions to OpenMP, but may localize memory accesses in a nearly optimal manner. The main contribution [...] ...|$|E
40|$|In this paper, we {{introduce}} a profile-driven online <b>page</b> <b>migration</b> scheme and investigate {{its impact on}} the performance of multithreaded applications. We use lightweight, inexpensive plug-in hardware counters to profile the memory access behavior of an application, and then migrate pages to memory local to the most frequently accessing processor. Using the Dyninst runtime instrumentation combined with hardware counters, we were able to add <b>page</b> <b>migration</b> capabilities to the system without having to modify the operating system kernel, or to re-compile application programs. This approach reduced the total number of non-local memory accesses of applications by up to 90 %. Even on a system with small remote to local memory access latency rations, this resulted in up to 16 % improvement in execution time. 1...|$|E
30|$|Migration must be {{seamless}} {{to provide}} the continuous services. Live migration moves the VM without disconnecting with the client. Performance of live VM migration must be very high for continuous services. Current techniques face many challenges while migrating memory and data intensive applications, like - network faults, consumption of bandwidth and cloud resources, overloaded VM’s. Common challenges that hamper live migration are: transfer rate problem, page re-send problem, missing <b>page</b> problem, <b>migration</b> over WAN network, migration of VM with the larger application, resources availability problem, and address-wrapping problem.|$|R
30|$|TPO is also {{compared}} with other works for analyzing it more noticeably. Table 8 gives a comparative analysis for the behavior of TPO and other methods as compared to precopy corresponding to the parameters such as total <b>page</b> transferred, total <b>migration</b> time, downtime, overhead, experimental workloads used and their migration techniques used.|$|R
40|$|Past {{military}} operations {{have resulted in}} the contamination of soils by chemical munitions such as RDX, HMX and TNT (contaminant description, <b>page</b> 2). The <b>migration</b> of water through the soils can transport the contaminants into groundwater and contaminate drinking water sources. Many military munitions are known or suspected to be carcinogenic and, therefore, must be removed from drinking water...|$|R
40|$|AbstractWe {{present an}} {{extension}} of a classical data management subproblem, the <b>page</b> <b>migration.</b> The problem is investigated in dynamic networks, where costs of communication between different nodes may change with time. We construct asymptotically optimal online algorithms for this problem, both in deterministic and randomized scenarios...|$|E
40|$|Dynamic <b>page</b> <b>migration,</b> when {{employed}} in Distributed Shared Memory (DSM) systems offers several advantages: (i) reduces the latency of memory accesses, (ii) improves resource utilization {{by considering the}} computational and communicational needs of the applications and adapting to the changing resource availability, and (iii) achieves the above with lower overhead than traditional approaches that rely on thread migration. We propose a simple and efficient <b>page</b> <b>migration</b> mechanism [1], that dynamically allocates shared memory pages to home nodes. Each page has a designated home node and nodes that heavily modify the pages can become their new homes. In our protocol, to avoid redundant page transfers, we perform migration only {{when the number of}} modifications of a page becomes larger than a threshold. The migration information is piggybacked on the existing synchronization messages to minimize the communication overhead. The migration decision is taken locally, at the hom...|$|E
40|$|This paper {{presents}} performance {{results from}} work done on Sun's WildFire system. WildFire is a codename for a prototype shared memory multiprocessor developed by Sun Microsystems TM consisting {{of up to}} four unmodified Sun Enterprise TM x 000 series symmetric multiprocessors (SMPs). A goal of the WildFire system is {{to evaluate the effectiveness}} of leveraging large SMPs in the construction of even larger systems. We have conducted several performance experiments with a shared memory parallelized finite difference solver. Our work demonstrates the key features of the WildFire system, including automatic <b>page</b> <b>migration</b> and read/write replication. Our results show that the dynamic <b>page</b> <b>migration</b> algorithms used by the WildFire system are effective in automatically optimizing data placement at runtime. Performance comparisons between the WildFire system and currently available SMPs show that the system exhibits good scalability characteristics, and actually outperforms SMPs on this particular application. ...|$|E
30|$|Clark et al. (2005) {{presents}} one of {{the first}} works which uses precopy method for virtual machine live migration and uses Xen as a virtualization platform. In this it traces writable working set (WWS), which is a set of frequently updated pages and it will be sent at the end, instead of each time they are updated. In this way, it reduces transfer of redundant pages. The paper also introduces another method named ‘dynamic rate limiting’, to further enhance the performance of the virtual machine live migration. This method adapts dynamic bandwidth limit with respect to dirty rate for transferring of <b>pages</b> during <b>migration.</b> Which results into decrease in downtime by increasing bandwidth has been shown.|$|R
40|$|This paper {{presents}} {{a proposal for}} a flexible agent mobility architecture based on IEEE-FIPA standards and intended to be one of them. This proposal is a first step towards interoperable mobility mechanisms, which are needed for future agent migration between different kinds of platforms. Our proposal is presented as a flexible and robust architecture that has been successfully implemented in the JADE and AgentScape platforms. It is based on an open set of protocols, allowing new protocols and future improvements to be accommodated in the architecture. With this proposal we demonstrate that a standard architecture for agent mobility capable of supporting several agent platforms can be defined and implemented. Comment: 10 <b>pages,</b> agent <b>migration</b> architecture proposa...|$|R
40|$|Part 1 : Systems, Networks and ArchitecturesInternational audienceRecently, the {{development}} of phase change memory (PCM) motivates new hybrid memory architectures that consist of PCM and DRAM. An important issue in such hybrid memory architectures is how to manage the pages resisting in heterogeneous memories. For example, when a requested page is missing in the hybrid memory and the memory has no free spaces, what pages in which type of memory (PCM or DRAM) should be replaced? This problem is much different from traditional buffer replacement management, where they do not consider the special properties {{of different types of}} memories. In particular, differing from DRAM, PCM is non-volatile but it has lower access speeds than DRAM. Further, PCM has a limited write endurance which implies that it cannot be written endlessly. Therefore, we have to design a new page replacement algorithm that can not only maintain a high hit ratio as traditional algorithms do but also can avoid frequent writes to PCM. In this paper, aiming to provide a new solution to the page replacement problem in PCM/DRAM-based hybrid memories, we propose a new algorithm called MHR-LRU (Maintain-hit-ratio LRU). The objective of our algorithm is to reduce PCM writes while maintaining a high hit ratio. Specially, it keeps recently updated pages in DRAM and performs <b>page</b> <b>migrations</b> between PCM and DRAM. The migrations take into account both page access patterns and the influences of page faults. We conduct trace-driven experiments and compared our proposal with some existing algorithms including LRU, LRU-WPAM, and CLOCK-DWF. The results show that our proposal is able to efficiently reduce PCM writes without degrading the hit ratio. Thus, our study offers a better solution for the page replacement issue in PCM/DRAM-based hybrid memory systems than previous approaches...|$|R
