53|314|Public
50|$|VPIM {{defines a}} subset of the Internet {{multimedia}} messaging protocols (MIME) for use between voice <b>processing</b> <b>server</b> platforms.|$|E
5000|$|May 11, 2010 Patent 7,716,153 for [...] "Memory {{assistance}} system [...] of {{a signal}} <b>processing</b> <b>server</b> receiving a media signal and associated data relating to {{information to be}} remembered and processing the input signal to identify media characteristics relevant to aiding user memory". Vemuri, S.|$|E
5000|$|It {{consisted}} {{of up to}} three customized DSP boards that could be plugged into the expansion bus on a NeXT Computer (a [...] "cube"). The ISPW could then run a customized real-time audio <b>processing</b> <b>server</b> on the hardware boards controlled by a client application on the NeXT.|$|E
40|$|This paper {{addresses}} architectures of concurrent request <b>processing</b> <b>servers,</b> {{which are}} typically implemented using multitasking capabilities of an underlying operating system. Request <b>processing</b> <b>servers</b> should respond quickly to concurrent requests {{of an open}} number of clients without wasting server resources. This paper describes a small system of patterns for request <b>processing</b> <b>servers,</b> covering a relative wide range of architectures. Certain types of dependencies between the patterns are identified which are important for understanding and selecting the patterns. As the patterns deal with the conceptual architecture, they are mostly independent from programming languages and paradigms. The examples {{presented in this paper}} show applications of typical pattern combinations which can be found in productive servers. The pattern language is completed by a simple pattern selection guideline. The systematical compilation of server patterns, together with the guideline, can help both choosing and evaluating a server architecture...|$|R
50|$|On March 17, 2008, The Boston Globe {{reported}} that the company's credit-card <b>processing</b> <b>servers</b> had been compromised for three months. Some 4.2 million credit card numbers were stolen, at least 1,800 {{of which had been}} used fraudulently. In August 2009, criminal computer hacker Albert Gonzalez was indicted for the crime.|$|R
50|$|The Earth Rangers 9.3 {{square metre}} data centre was {{completed}} in December 2008. The data room uses storage servers from Pillar, Dell’s <b>processing</b> <b>servers,</b> and virtual personal machines using VMware View software. Nortel’s Power over Ethernet (PoE) switching facilitates the transfer of electrical power and data across the infrastructure. Earth Rangers also participates in Compugen’s Green4Good program which works to combat {{the environmental impact of}} end-of-life technology products and support charities in need.|$|R
5000|$|The Ken Lowe Judge {{system was}} the first judge system to be created. [...] The Ken Lowe Judge system allows users to send press {{to one another and}} send orders to the <b>processing</b> <b>server</b> through a variety of text commands. Today, many online {{adjudication}} systems run in the same, or similar, fashion to the Ken Lowe Judges.|$|E
50|$|In short, server {{provisioning}} configures servers {{based on}} resource requirements. The {{use of a}} hardware or software component (e.g. single/dual processor, RAM, HDD, RAID controller, a number of LAN cards, applications, OS, etc.) depends on the functionality of the server, such as ISP, virtualization, NOS, or voice <b>processing.</b> <b>Server</b> redundancy depends {{on the availability of}} servers in the organization. Critical applications have less downtime when using cluster servers, RAID, or a mirroring system.|$|E
40|$|This master's thesis {{describes}} Grand Central Dispatch technology. It is {{technology for}} parallel computing and task processing. It also describes history and techniques of parallel computing. As an example stand simple HTTP server with 4 methods of request parallel <b>processing.</b> <b>Server</b> {{is connected to}} AVG antivirus software and check each request for viruses...|$|E
30|$|We {{decided to}} go with a {{multicore}} system in order to enable more streamlined parallel data processing of multiple cameras per computer. Also, with our server architecture we have 3 levels of processing. If later on this amount of processing power is insufficient, each computer should have a second vacant CPU socket for another processor to allow doubling the processing power of the server farm if necessary without increasing the physical footprint of the system. For uniformity and to facilitate maintenance, all <b>processing</b> <b>servers</b> have the same hardware.|$|R
30|$|We {{make the}} case for IP cameras and {{server-side}} processing by designing and implementing a system utilizing network cameras running on a softwarereconfigurable server and network architecture. While we use conventional IP cameras without on-board camera processing, the configuration of the server-side processing is user-configurable and allows on-the-fly changes such as going from tiered-processing (e.g., low-level <b>processing</b> <b>servers</b> do object detection and send silhouettes to mid-level processors which generate object signatures and broadcast to high-level servers) to 1 -to- 1 camera-to-server processing which simulates the behavior of on-camera processing networks.|$|R
40|$|For an {{improved}} QoS provisioning, operators are strongly concerned {{both with the}} availability and the reliability of their performance critical servers. Among these entities, we mention the firewalls used to filter the offered network traffic to the operator's information system {{as well as the}} end <b>processing</b> <b>servers</b> which service each incoming client request. In this work, we advocate and evaluate a full architecture for highly available services. Performance evaluations show that our proposed framework incurs a minimal overhead to the end-to-end communications during failsafe periods and performs well during failures...|$|R
30|$|Remote health {{monitoring}} {{has been proposed}} and investigated for a few decades [18]. Sensors and sensor networks are the fundamental technologies, which are deployed to collect a number of health-related signals, such as ECG and heartbeat. A typical solution for remote {{health monitoring}} is utilising a smart phone as a networking gateway, which gathers the sensor signals, with or without further processing, and then transmits to an associated medical centre or data <b>processing</b> <b>server.</b> Another key concept is the Body Area Network (BAN), which is composed {{of a number of}} interconnected wearable sensing devices to collect a series of biomedical parameters, such as ECG, EEG, blood pressure, and body temperature. Such sensor nodes are interconnected via various communication protocols such as ZigBee, Bluetooth, and WiFi. However, sensor nodes are often designed with limited memory and computing capacity; thus, sensor data are typically transmitted to another <b>processing</b> <b>server,</b> where mobile phone is commonly adopted.|$|E
40|$|A full-frame {{compression}} technique for video sequences of faces, based on principal component analysis, is presented. Entire video frames are encoded to very low bitrates and reconstructed with good accuracy. Principal component analysis is utilized {{to model the}} facial mimic in an Eigenspace. By using a camera construction where the user’s face is almost fixed in all frames, a heavy normalization is circumvented. Encoding and decoding can be performed in real-time. The creation of Eigenspaces can be performed by a central <b>processing</b> <b>server</b> to avoid this memory and time-consuming process to be performed on mobile devices. Video frames can be reconstructed from a bitstream of 2. 4 kbps with an average peak {{signal to noise ratio}} above 31 dB. The effect of using nonhomogenous background compared to a white background is examined and the effect is moderate for a slightly shifting background. KEY WORDS Very low bitrate, video coding, video compression, central <b>processing</b> <b>server...</b>|$|E
40|$|With the {{introduction}} of Microsoft SSPI authentication in SAS ® version 8, using SAS/CONNECT ® to establish a server session on a remote Windows server and accessing data on the <b>processing</b> <b>server</b> or on the network has evolved. This paper explores different ways available {{to sign on to}} a remote Windows server with SAS/CONNECT ® and describes the implications of accessing data. Methods of channeling data through the client SAS ® session will also be discussed...|$|E
50|$|On shared computers, whether time-sharing, batch <b>processing,</b> or <b>server</b> systems, core dumps allow {{off-line}} debugging of {{the operating}} system, so that the system can go back into operation immediately.|$|R
50|$|The {{following}} tables compare {{general and}} technical information {{for a number}} of online analytical <b>processing</b> (OLAP) <b>servers</b> supporting MDX language. Please see the individual products articles for further information.|$|R
5000|$|Correlation {{of network}} {{transfer}} time and <b>server</b> <b>processing</b> time ...|$|R
40|$|This paper {{describes}} a new monitoring and event management concept. eEMU is a clientserver system that provides for rapid development of monitoring agents. This is {{thanks to its}} messaging language that takes advantage of heuristic algorithms implemented in the eEMU server. As opposed to SNMP and other static monitoring methodologies used in system and application monitoring, eEMU takes a whole new approach by incorporating the dynamic aspect of application and system events into {{the very heart of}} the dynamic message <b>processing</b> <b>server...</b>|$|E
40|$|The {{principal}} {{aim of this}} thesis is to create and expound a distributed-telemanufacturing model (“DTMF” model, for short) {{that can be used}} for final-product realization, with “telemanufacturing” being the remote application of a layered-manufacturing machine and its software to create a model or product. It is envisaged (at no distant date) that machines will be made to be exponentially more accurate and that they will even be able to “create” models or products from a host of different materials and elements. The model to be expounded in the present thesis will, therefore, serve to address problems and issues relevant to such service, which service will, ultimately, be rendered mutually by businesses and − in the e-commerce scenario − by businesses to their customers. The DTMF model comprises an interface, a <b>processing</b> <b>server,</b> a locating agent and a manufacturing resource, with the interface being the overview that the user will have of the telemanufacturing process and the extent to which he/she will be able interactively to submit and glean information on and from a submission. The <b>processing</b> <b>server,</b> in its turn, prepares the submission in such a way as to allow its direct submission to a manufacturing resource, which resource consists of either a layered-manufacturing machine or any digital-input machine. The <b>processing</b> <b>server,</b> therefore, ensures that the submitted design be in the correct format to be interpreted by the manufacturing machine. The locating agent then takes the processed design and locates an appropriate manufacturing resource that matches the user’s specifications and meets the requirements of the processed design. On having received the processed design, the locating agent submits it to a queue at the manufacturing resource. The manufacturing resource is, therefore, controlled by the locating agent in that the locating agent calls up the available manufacturing methods through a web service at each machine. Next, the DTMF model is extended also to allow the use of a design repository, where a design can be searched for and retrieved. This enables a user to produce products on demand by retrieving a stored design and by applying customization, if necessary. The DTMF model, therefore, makes possible not only on-demand manufacturing for current machines but also music of the future, such as final-product realization. Ehlers, E. M., Prof...|$|E
30|$|To {{collect the}} records of visited webpages (i.e., clickstreams), we {{delegated}} this responsibility to the first visited web server in the session. We made use of the HTTP Referrer header field along with time stamps to construct the sequence of visited pages, and used a message broker to have the second, third, and any subsequent web server send the click information to the first server. The first visited web server stores the records for the particular session in a special database, where they stay there briefly before they get shipped to a central <b>processing</b> <b>server,</b> which we name Base Server.|$|E
30|$|The server {{architecture}} is {{designed as a}} 3 -level tree hierarchy: a master high-level interface server communicates {{with a set of}} mid-level <b>processing</b> <b>servers,</b> which in turn process data received from a number of low-level servers. A server architecture physically connected in this fashion would entail cameras forwarding data to one set of servers which forward low-level data to another set of servers, and once more to the high-level server. To minimize network overhead, we use a central network switch to connect the servers (as opposed to physically tiering the servers with direct connections) and implement the server hierarchy in the network's DNS configuration and in the communication strategy of our software.|$|R
40|$|Civil {{engineers}} {{create and}} employ {{a very large}} number of design standards, especially in the United States. Designing using such a large number of design standards is a tedious, laborious, and difficult task. One major research task in Computer-Aided Engineering (CAE) is the development of software tools that assist in the usage of design standards during the design process. This dissertation, a standards processing framework is presented. It is an agent-based approach to providing computer-aided support for using design standards. In this framework, modules, such as standards <b>processing</b> <b>servers,</b> are treated as agents communicating using a defined communication language. One immediate advantage of this architecture is that it allows the incorporation of a broad, powerful set of representation for use in modelling design standards. ...|$|R
50|$|The 9020As and 9020Ds were {{in service}} in North America until 1989 {{when they were}} finally {{replaced}} by IBM 3083 BX1 mainframes {{as part of the}} FAA's HOST Computer System (HCS) upgrade. The 3083s in turn were replaced with IBM 9672 RA4 parallel <b>processing</b> <b>servers</b> during the FAA's Host and Oceanic Computer System Replacement (HOCSR) completed in 1999. One reason for the 1999 upgrade was concern, probably unfounded, that the IBM 3083's microcode would not operate properly in the year 2000. At least during {{the first phase of the}} upgrade, the 9672s were running the FAA's original assembly language code in System/360 emulation mode. Because of the failure of the FAA's Advanced Automation System (AAS) project, the 9020E Display Channel Complexes lasted well into the 1990s.|$|R
40|$|We {{describe}} our knowledge-based service architecture for multi-risk environmental decision-support, {{capable of}} handling geo-distributed heterogeneous real-time data sources. Data sources include tide gauges, buoys, seismic sensors, satellites, earthquake alerts, Web 2. 0 feeds to crowd source 'unconventional ' measurements, and simulations of Tsunami wave propagation. Our system of systems multi-bus architecture provides a scalable and high performance messaging backbone. We are overcoming semantic interoperability between heterogeneous datasets by using a self-describing 'plug-in ' data source approach. As crises develop we can agilely steer the <b>processing</b> <b>server</b> and adapt data fusion and mining algorithm configurations in real-time...|$|E
40|$|We will {{demonstrate}} the MIT Spoken Lecture <b>Processing</b> <b>Server</b> and an accompanying lecture browser {{that students can}} use to quickly locate and browse lecture segments that apply to their query. We will show how lecturers can upload recorded lectures and companion text material to our server for automatic processing. The server automatically generates a time-aligned word transcript of the lecture which can be downloaded for use within a browser. We will also demonstrate a browser we have created which allows students to quickly locate and browse audio segments {{that are relevant to}} their query. These tools can provide students with easier access to audio (or audio/visual) lectures, hopefully improving their educational experience...|$|E
40|$|The thesis {{presents}} {{an overview of}} third generation of IP telephony. The architecture of 3 G IP Telephony and its components are described. The main goal of the thesis is to investigate the interface between the Call <b>Processing</b> <b>Server</b> and Multimedia IP Networks. The interface functionality, proposed protocol stack and a general description are presented in the thesis. To provide useful services, 3 G IP Telephony requires a set of control protocols for connection establishment, capabilities exchange and conference control. The Session Initiation Protocol (SIP) and the H. 323 are two protocols that meet these needs. In the thesis these two protocols are investigated and compared in terms of Complexity, Extensibility, Scalability, Services, Resource Utilization and Management...|$|E
40|$|With {{the rising}} {{penetration}} of smartphones {{in the consumer}} market, mobile multimedia content is becoming the dominant form of information that people produce and consume on a daily basis. In this paper we present a wireless multi-hop video streaming application for mobile phones with the Android operating system. This application allows to share live information captured by mobile phone sensors (e. g., camera, microphone) with persons that might be multiple wireless hops away. The video streaming is based on peer-to-peer communication between mobile phones, i. e. {{without the use of}} video <b>processing</b> <b>servers</b> or network infrastructure. We show the feasibility of such peer-to-peer video streaming application for Android phones in a variety of experiments that evaluate various video streaming scenarios, including various video codecs and various generations of Android phones...|$|R
40|$|This paper {{presents}} the SWFL workflow engine, a general workflow framework {{that meets the}} needs of business processes as well as scientific computing processes with fine multi-level parallelism supports. The workflow description language, SWFL, follows a graph-oriented model to specify workflow processes composed of services. The workflow engine provides an efficient enactment environment for SWFL flow model composed of services. It provides multi-level parallelism supports: a server-level parallelism support using flexible server-level schedule algorithms, a flow-level parallelism support by partitioning a workflow into sub-flows and running the sub-flows in multiple job <b>processing</b> <b>servers</b> in parallel, and a message-passing parallelism support by using the MPFL, an extension language to SWFL and its associated tools. The architecture of the workflow engine and some other related implementation issues are also presented in the paper. 1...|$|R
50|$|The NetInsight Extract, transform, load {{process can}} read log files in {{virtually}} any format, including logs from web servers, proxy servers, streaming media servers and FTP servers. As well as <b>processing</b> normal <b>server</b> log files NetInsight can use log files derived from page tags to replace or augment log file data.|$|R
40|$|In {{this paper}} a new {{wireless}} decision-support system for haemodialysis patients using {{heart rate variability}} (HRV) is presented. The telemedicine system provides connectivity to three participant sites: the general practitioner or nurse {{at the point of}} care in the dialysis unit, the remote information and <b>processing</b> <b>server</b> and the cardiologist. At the clinical point of care, the nurse acquires the electrocardiogram (ECG) by using a tailored mobile telecardiology system as well as other relevant physiological information during the clinical procedure, and sends it to the information server. The received information is stored in a secure file server, linked to the patient database and the ECG signal is automatically analyzed by using advanced signal processing tools in the <b>processing</b> <b>server,</b> where a complete clinical results report is generated. The cardiologist can then be linked by means of a web browser to the information server to analyze these results for further clinical diagnosis support. The system has been applied to study HRV in patients undergoing haemodialysis. The clinical report consisted of trends for time- and frequency-domain HRV indexes and other supplementary information automatically calculated, which show the response of the electrical activity of the heart to the dialysis process and that can be helpful for the follow-up of these patients. The telecardiology framework has been successfully evaluated both by the patients and the hospital personnel showing a high compliance with the system. The design and implementation of the telecardiology system have followed the most recent advances in web technologies, biomedical information and storage standards and signal processing techniques. The presented system {{can be used as a}} telemedicine tool for clinical diagnosis support and could also be used in other clinical settings...|$|E
40|$|Abstract — Modeling and conserving {{energy is}} one of the {{fundamental}} research problems in sensor networks. We analyze the energy consumption for sensor networks in which each sensor periodically reports its measured data from the field to a <b>processing</b> <b>server.</b> We study the trade offs between single-hop and multi-hop transmissions in terms of energy efficiency. In our analysis, we employ a realistic energy model that captures various aspects of energy consumption in sensors. We compute the optimal transmission distance for each sensor to minimize the total energy consumption. Our analysis shows that our optimal solution not only minimizes the total energy consumption, but also balances the average per-node energy consumption across all nodes, and hence prolongs the sensor network lifetime. I...|$|E
40|$|Essbase is multidimensional {{database}} software that is optimized for planning, analysis, and management-reporting applications. Essbase uniquely blends an innovative technical design with an open, client-server architecture. The product {{enables you to}} extend decision support system beyond ad hoc queries and reports on historical performance to dynamic, operational systems that combine historical analysis and future planning. Hyperion Essbase is the industry-leading multi-dimensional online analytical <b>processing</b> <b>server,</b> providing a rich environment for effectively developing custom analytic and enterprise performance management applications. By leveraging its self-managed, rapid application development capabilities, business users can quickly model complex business scenarios. In this paper we see how Hyperion Essbase supports extremely fast query response times for vast numbers of users, large data sets, and complex business models...|$|E
25|$|Analytics Platform System (APS): Formerly Parallel Data Warehouse (PDW) A massively {{parallel}} <b>processing</b> (MPP) SQL <b>Server</b> appliance optimized for large-scale data warehousing such {{as hundreds of}} terabytes.|$|R
30|$|Huge {{amounts of}} data is {{collected}} by smart city systems and the corresponding IoT devices that are spread out over a considerably larger geographic area. Analyzing and extracting useful information from this data can provide considerable advantages for businesses and government institutions. In addition, communication and collection {{of a very large}} number of messages in a timely fashion according to their priority, delay-tolerance, and size is vital to the efficient operation of smart city systems. In order to reduce the amount of exchanged traffic, local processing, compression, and aggregation of the generated messages need to be done at the lower and intermediate levels of the node hierarchy and geographic areas. Consequently, More research is needed to provide proper convergence and mapping of the networking parameters between the various layers of the networking stack at the data-generating nodes (e.g. sensors, IoT devices, etc.), the intermediate routers, <b>processing</b> <b>servers</b> (typically in the cloud), and actor nodes at the other end of the communication cycle.|$|R
40|$|Software {{today is}} often {{developed}} for deployment on varying architectures. In order to model {{and analyze the}} consequences of such deployment choices {{at an early stage}} in software development, it seems desirable to capture aspects of low-level deployment concerns in high-level models. In this paper, we propose an integration of a generic cost model for resource consumption with deployment components in Timed ABS, an abstract behavioral specification language for executable object-oriented models. The actual cost model may be user-defined and specified by means of annotations in the executable Timed ABS model, and can be used to capture specific resource requirements such as processing capacity or memory usage. Architectural variations are specified by resource-restricted deployment scenarios with different capacities. For this purpose, the models have deployment components which are parametric in their assigned resources. The approach is demonstrated on an example of multimedia <b>processing</b> <b>servers</b> with a user-defined cost model for memory usage. We use our simulation tool to analyze deadline misses for given usage and deployment scenarios...|$|R
