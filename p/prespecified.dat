3327|48|Public
25|$|When using pulsed fluoroscopy, {{radiation}} dose is only applied in <b>prespecified</b> intervals of time, thus less dose {{is used to}} produce the same image sequence. For the time in between, the last image stored is displayed.|$|E
2500|$|The {{authors of}} the study, {{including}} the non-Merck authors, responded by claiming that the three additional heart attacks had occurred after the <b>prespecified</b> cutoff date for data collection and thus were appropriately not included. (Utilizing the <b>prespecified</b> cutoff date also meant that an additional stroke in the naproxen population was not reported.) Furthermore, {{they said that the}} additional data did not qualitatively change any of the conclusions of the study, and the results of the full analyses were disclosed to the FDA and reflected on the Vioxx warning label. They further noted that all of the data in the [...] "omitted" [...] table were printed in the text of the article. The authors stood by the original article.|$|E
2500|$|Although the Polchinski ERGE and the {{effective}} average action ERGE look similar, {{they are based}} upon very different philosophies. In {{the effective}} average action ERGE, the bare action is left unchanged (and the UV cutoff scale—if there is one—is also left unchanged) but the IR contributions to the effective action are suppressed whereas in the Polchinski ERGE, the QFT is fixed {{once and for all}} but the [...] "bare action" [...] is varied at different energy scales to reproduce the <b>prespecified</b> model. Polchinski's version is certainly much closer to Wilson's idea in spirit. Note that one uses [...] "bare actions" [...] whereas the other uses effective (average) actions.|$|E
40|$|When {{firm-specific}} {{human capital}} is involved, both the worker and the employer have {{the incentive to}} <b>prespecify</b> future wages. This incentive arises from transaction costs associated with spot contracts and from opportunistic bargaining which may occur during the postinvestment period. In <b>prespecifying</b> wages the parties may use economic indicators to estimate productivities. To the extent that such indicators are less than perfect measures of true productivities, some wage rigidity will occur. Wage rigidity causes resource loss of various types. This article analyzes various contractual arrangements designed to minimize such a loss, and discusses potentially testable implications. ...|$|R
40|$|Mathematical {{programming}} {{models for}} nutrition planning {{in developing nations}} typically involve the optimization of a single criterion function subject to resource and nutritional constraints. The nutritional constraints specify that the amount available of each nutrient for human consumption should meet or exceed some prechosen nutrient requirement level. However, fixed nutrient requirement levels are difficult to <b>prespecify.</b> In this paper, a bicriteria mathematical programming model is proposed which does not require planners to <b>prespecify</b> nutrient requirement levels for several key nutrients. Instead, the model generates an entire set of efficient nutrition plans which supply various amounts of these nutrients. To demonstrate the use and potential benefits of this model, an illustrative application to Colombia, South America is included. bicriteria mathematical programming, nutrition planning, multiple criteria decision making...|$|R
40|$|This paper {{examines}} the firm's optimal wage and layoff policies {{in situations where}} only general training is provided and the firm providing training has an advantage in information over other firms. The asymmetric information among firms creates adverse selection that seriously impairs the mobility of labor and consequently affects the firm's optimal policies. General human capital is shown {{to have the same}} layoff and cost-sharing implications as firm-specific human capital. Furthermore, the contracts <b>prespecifying</b> rigid wages are proven to be optimal. ...|$|R
2500|$|In July 2013 Jureidini {{announced}} {{his intention to}} produce a new write-up of study 329 {{in accordance with the}} RIAT initiative (restoring invisible and abandoned trials). The RIAT researchers—Joanna Le Noury, John M. Nardo, David Healy, Jon Jureidini, Melissa Raven, Catalin Tufanaru, and Elia Abi-Jaoude—published their re-analysis in the BMJ in September 2015. They concluded that [...] "he efficacy of paroxetine and imipramine was not statistically or clinically significantly different from placebo for any <b>prespecified</b> primary or secondary efficacy outcome," [...] and that there were [...] "clinically significant increases in... suicidal ideation and behaviour and other serious adverse events in the paroxetine group and cardiovascular problems in the imipramine group." ...|$|E
50|$|In the VALUE trial, the {{angiotensin}} II receptor blocker valsartan {{produced a}} statistically significant 19% (p=0.02) relative increase in the <b>prespecified</b> secondary end point of myocardial infarction (fatal and non-fatal) compared with amlodipine.|$|E
50|$|When using pulsed fluoroscopy, {{radiation}} dose is only applied in <b>prespecified</b> intervals of time, thus less dose {{is used to}} produce the same image sequence. For the time in between, the last image stored is displayed.|$|E
40|$|Abstract- In this paper, {{we apply}} the information-theoretic {{learning}} (ITL) technique to the extended Luenberger observer. Instead of <b>prespecifying</b> the globally stable observer gains for nonlinear dynamic systems, we propose minimizing {{the entropy of}} the error between the measurement and the estimated output to update the observer gains. A stochastic gradient-based algorithm is presented {{and the performance of}} the entropy observer is demonstrated on linear and nonlinear dynamic systems. We also point out that this approach leads to the introduction of kernel methods into state estimation. I...|$|R
40|$|In {{this paper}} {{a model of}} urban land use, which <b>prespecifies</b> neither the number nor the {{location}} of employment centers, is developed {{within the context of}} office location. The number and location of employment centers are determined endogenously, based on the values of the model parameters. Offices and households compete for land and urban land serves as the mediator of balance. It is shown that traditional location theory, which is based upon physical distances and transportation costs, cannot explain the spatial structure of the contemporary urban form. ...|$|R
40|$|A {{difficult}} {{aspect of}} system optimization {{is the choice}} of a single cost function relating system parameters {{to the quality of the}} design; there are generally conflicting criteria by which the system can be evaluated. This paper discusses the problem of optimization with respect to multiple criteria, i. e., with a vector-valued cost function, without <b>prespecifying</b> constraints or the weighting of the criteria. The paper discusses theoretical results concerning the set of non-inferior solutions so generated and a computational algorithm for obtaining a characteristic set of non-inferior solutions...|$|R
5000|$|Here [...] is a <b>prespecified</b> free {{parameter}} {{that determines}} {{the amount of}} regularisation. Letting [...] be the covariate matrix, so that [...] and [...] is the ith row of , the expression can be written more compactly as ...|$|E
50|$|Logic optimization, a part {{of logic}} {{synthesis}} in electronics, {{is the process of}} finding an equivalent representation of the specified logic circuit under one or more specified constraints. Generally the circuit is constrained to minimum chip area meeting a <b>prespecified</b> delay.|$|E
5000|$|The {{authors of}} the study, {{including}} the non-Merck authors, responded by claiming that the three additional heart attacks had occurred after the <b>prespecified</b> cutoff date for data collection and thus were appropriately not included. (Utilizing the <b>prespecified</b> cutoff date also meant that an additional stroke in the naproxen population was not reported.) Furthermore, {{they said that the}} additional data did not qualitatively change any of the conclusions of the study, and the results of the full analyses were disclosed to the FDA and reflected on the Vioxx warning label. They further noted that all of the data in the [...] "omitted" [...] table were printed in the text of the article. The authors stood by the original article.|$|E
40|$|A {{model is}} {{produced}} in which labor contracts that <b>prespecify</b> (unindexed) nominal wage payments arise endogenously. These contracts function as a self-selection mechanism. Under appropriately different attitudes toward price-level risk (which can either arise directly from preferences or be induced by different patterns of asset holdings), nominal contracts allow high-productivity workers to signal their type by their willingness to accept unindexed contracts. This explanation of nominal contracts does not require that money be used in any particular set of transactions, and nominal contracts enhance the risk faced by all parties accepting them. Copyright 1989 by University of Chicago Press. ...|$|R
40|$|Population {{studies of}} the {{pharmacokinetics}} or pharmacodynamics of drugs help us learn about the variability in drug disposition and effects, {{information that can be}} used to treat future patients at safe and effective doses. We present a new approach to population modeling based on a weighted mixture of normal distributions having random weights and means. This method allows estimation of underlying continous population distributions without <b>prespecifying</b> the parametric form or shape of these probability distributions. Additionally, this method can carry out nonparametric regression of pharmacokinetic or dynamic parameters on patient covariates can be carried out while estimating the underlying distributions. Two examples illustrate the method and its flexibility...|$|R
40|$|In {{this paper}} we develop and solve {{a model of}} credit lines. Credit lines are long term {{relations}} between banks and households that <b>prespecify</b> a credit limit and have a fixed interest rate. Households can unilaterally default according to U. S. Bankruptcy law, and can switch credit lines at will, albeit at a (utility) cost. Banks issue costly credit lines to households and they can either have to commit to them or not (we look at both cases). We solve and characterize the equilibria. We find that this model replicates the main properties of typical lending contracts and that it {{holds a lot of}} promise for quantitative work...|$|R
50|$|Street skating is the {{practice}} of roller skating (commonly on inline skates or quad skates) in groups on public roads. Street skates can be formal affairs, with <b>prespecified</b> routes, marshals and, at times, police escorts or ad hoc gatherings of like minded individuals.|$|E
5000|$|... #Caption: Gene {{clusters}} from Rosenberg (2006) for K=7 clusters. (Cluster analysis divides a dataset {{into any}} <b>prespecified</b> number of clusters.) Individuals have genes from multiple clusters. The cluster prevalent only among the Kalash people (yellow) only splits off at K=7 and greater.|$|E
5000|$|Objectives-based {{approaches}} relate outcomes to <b>prespecified</b> objectives, allowing judgments to be {{made about}} their level of attainment. Unfortunately, the objectives are often not proven to be important or they focus on outcomes too narrow to {{provide the basis for}} determining the value of an object.|$|E
40|$|We {{extend the}} frequency-specific Granger-causality test of Breitung and Candelon (2006) {{to a more}} general null {{hypothesis}} that allows non-causality at unknown frequencies within an interval, {{instead of having to}} <b>prespecify</b> a single frequency. This setup corresponds better to most hypotheses that are typically analyzed in applied research and is easy to implement. We also discuss a test approach that departs from strict non-causality, given the impossibility of (non-trivial) non-causality over a continuum of frequencies. In an empirical application dealing with the dynamics of US temperatures and CO 2 emissions we find that emissions cause temperature changes only at very low frequencies with more than 30 years of oscillation...|$|R
40|$|We {{analyze the}} problem of {{minimizing}} average inventory costs subject to fill-rate type of service-level constraints in serial and assembly production/distribution systems. We propose optimal and heuristic procedures to solve this problem. Our model and solution procedures {{can be used to}} manage the fill rate or fill rate within a "time window" service measures. We also relate our service-constrained model to the traditional model with back-order costs and show {{that it is possible to}} <b>prespecify</b> backorder cost rates to achieve desired service levels. We explore the inventory cost impact of such a practice, and we find that the cost penalty can be very high. Inventory/Production, Multistage, Serial, Fill Rate, Base-Stock Policy, Solution and Heuristics...|$|R
40|$|Clinical {{responder}} {{studies should}} {{contribute to the}} trans-lation of effective treatments and interventions to the clinic. Since ultimately this translation will involve regula-tory approval, we recommend that clinical trials <b>prespecify</b> a responder definition that can be assessed against the requirements and suggestions of regulatory agencies. In this article, we propose a clinical responder definition to specifically assist researchers and regulatory agencies in interpreting the clinical importance of statistically signifi-cant findings for studies of interventions intended to preserve b-cell function in newly diagnosed type 1 diabe-tes. We focus on studies of 6 -month b-cell preservation in type 1 diabetes as measured by 2 -h–stimulated C-peptide. We introduce criteria (bias, reliability, and external validity) {{for the assessment of}} responder definitions to ensure the...|$|R
5000|$|The annualised {{realised}} variance {{is calculated}} {{based on a}} <b>prespecified</b> set of sampling points over the period. It does not always coincide with the classic statistical definition of variance as the contract terms may not subtract the mean. For example, suppose that there are n+1 sample points ...|$|E
50|$|Hash(m) = xm mod n where n is hard {{to factor}} {{composite}} number, and x is some <b>prespecified</b> base value. A collision xm1 congruent to xm2 reveals a multiple m1 - m2 {{of the order of}} x. Such information can be used to factor n in polynomial time assuming certain properties of x.|$|E
5000|$|The {{trial was}} stopped early after a <b>prespecified</b> interim {{analysis}} revealed {{a reduction in}} the primary endpoint of cardiovascular death or heart failure in the sacubitril/valsartan group relative to those treated with enalapril. Taken individually, the reductions in cardiovascular death and heart failure hospitalizations retained statistical significance. [...] Relative to enalapril, sacubitril/valsartan provided reductions in ...|$|E
40|$|Ideally, {{forecasting}} methods {{should be}} evaluated in the situations for which they will be used. Underlying the evaluation procedure {{is the need to}} test methods against reasonable alternatives. Evaluation consists of four steps: testing assumptions, testing data and methods, replicating outputs, and assessing outputs. Most principles for testing forecasting methods are based on commonly accepted methodological procedures, such as to <b>prespecify</b> criteria or to obtain a large sample of forecast errors. However, forecasters often violate such principles, even in academic studies. Some principles might be surprising, such as do not use R-square, do not use Mean Square Error, and do not use the within-sample fit of the model to select the most accurate time-series model. A checklist of 32 principles is provided to help in systematically evaluating forecasting methods...|$|R
40|$|We have {{recently}} proposed {{an alternative approach}} to economic analysis, which we call Imperfect Knowledge Economics (IKE). Although IKE builds on the methodology of contemporary macroeconomics by modeling aggregate outcomes {{on the basis of}} mathematical representations of individual decision making, it jettisons models that generate sharp predictions. In this paper, we elaborate on and extend the arguments that led us to propose IKE. We show analytically that {{in order to avoid the}} fundamental epistemological flaws inherent in extant models, economists must stop short of fully <b>prespecifying</b> change. We also show how acknowledging the limits of their knowledge may enable economists to shed new light on the basic features of observed time-series of market outcomes, such as fluctuations and risk in asset markets, which have confounded extant approaches for decades. ...|$|R
40|$|The author {{argues that}} {{researchers}} should do replications using preanalysis plans. These plans should specify {{at least three}} characteristics: (1) how much flowtime the researchers will spend, (2) how much money and effort (working hours) the researchers will spend, and (3) the intended results and the precision of the replication necessary for "success". A researcher's replication will be "successful" according to context-specific criteria in the preanalysis plan. The author also argues that the two biggest drawbacks of preanalysis plans-(1) that they discount unexpected but extraordinary findings and (2) that they {{make it difficult for}} researchers to <b>prespecify</b> all possible actions in their decision trees-are less relevant for replications compared with new research. The author concludes with describing a preanalysis plan for replicating a paper on housing demand and household formation...|$|R
50|$|When {{compared}} with other primates, whose communication system is restricted to a highly stereotypic repertoire of hoots and calls, humans have very few <b>prespecified</b> vocalizations, extant examples being laughter and sobbing. Moreover, these remaining innate vocalizations are generated by restricted neuronal pathways, whereas language is generated by a highly distributed system involving numerous regions of the human brain.|$|E
50|$|Some Torah {{commentators}} and poskim {{advocate the}} keeping of <b>prespecified</b> nuances of tumah and taharah {{even in the}} absence of the temple in Jerusalem and even in the diaspora. The advocated sub-divisions of tumah and taharah include tumath ochlin v'mashkin (consuming food and drink that did not become tamei) and abstaining from the midras of a niddah.|$|E
50|$|Integrated Services with RSVP (which reserve {{resources}} for the flow of packets through the network) using controlled-load service ensures that a call cannot be set up if it cannot be supported. CAC rejects calls when either there is insufficient CPU processing power, the upstream and downstream traffic exceeds <b>prespecified</b> thresholds, {{or the number of}} calls being handled exceeds a specified limit.|$|E
40|$|We {{describe}} {{a framework for}} exploring the evolution of adaptive behaviors in response to different physical environment structures. We focus here on the evolving behavior-generating mechanisms of individual creatures, and briefly mention some approaches to characterizing different environments in which various behaviors may prove adaptive. The environments are described initially as simple two-dimensional grids containing food arranged in some layout. The creatures in these worlds can have evolved sensors, internal states, and actions and action-triggering conditions. By allowing {{all three of these}} components to evolve, rather than <b>prespecifying</b> any of them, we can explore a wide range of behavior types, including "blind" and memoryless behaviors. Our system is simple and well-defined enough to allow complete specification of the range of possible actiontypes (including moving, eating, and reproducing) and their effects on the energy levels of the creature and the env [...] ...|$|R
40|$|Automated {{methods of}} machine {{learning}} {{may prove to}} be useful in discovering biologically meaningful information hidden in the rapidly growing databases of DNA sequences and protein sequences. Genetic programming is an extension of the genetic algorithm in which a population of computer programs is bred, over a series of generations, in order to solve a problem. Genetic programming is capable of evolving complicated problem-solving expressions of unspecified size and shape. Moreover, when automatically defined functions are added to genetic programming, genetic programming becomes capable of efficiently capturing and exploiting recurring sub-patterns. This chapter describes how genetic programming with automatically defined functions successfully evolved motifs for detecting the D-E-A-D box family of 2 proteins and for detecting the manganese superoxide dismutase family. Both motifs were evolved without <b>prespecifying</b> their length. Both evolved motifs employed automatically defined fun [...] ...|$|R
40|$|International audienceBrain imaging {{technology}} has boosted the quantification of neurobiological phenomena underlying human mental operations and their disturbances. Since its inception, drawing inference on neurophysiological effects hinged on classical statistical methods, especially, the general linear model. The {{tens of thousands}} variables per brain scan were routinely tackled by independent statistical tests on each voxel. This circumvented the curse of dimensionality in exchange for neurobiologically imperfect observation units, a challenging multiple comparisons problem, and limited scaling to currently growing data repositories. Yet, the always-bigger information granularity of neuroimaging data repositories has lunched a rapidly increasing adoption of statistical learning algorithms. These scale naturally to high-dimensional data, extract models from data rather than <b>prespecifying</b> them, and are empirically evaluated for extrapolation to unseen data. The present paper portrays commonalities and differences between long-standing classical inference and upcoming generalization inference relevant for conducting neuroimaging research...|$|R
