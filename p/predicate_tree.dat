0|24|Public
40|$|International audienceNeg-Raising (NR) verbs form a {{class of}} verbs with a clausal {{complement}} that show the following behavior: when a negation syntactically attaches to the matrix predicate, it can semantically attach to the embedded predicate. This paper presents an account of NR <b>predicates</b> within <b>Tree</b> Adjoining Grammar (TAG). We propose a lexical semantic interpretation that heavily relies on a Montague-like semantics for TAG and on higher-order types...|$|R
40|$|A {{computer}} program implements two extensions of ANTLR (Another Tool for Language Recognition), {{which is a}} set of software tools for translating source codes between different computing languages. ANTLR supports predicated- LL(k) lexer and parser grammars, a notation for annotating parser grammars to direct <b>tree</b> construction, and <b>predicated</b> <b>tree</b> grammars. [LL(k) signifies left-right, leftmost derivation with k tokens of look-ahead, referring to certain characteristics of a grammar. ] One of the extensions is a syntax for tree transformations. The other extension is the generation of tree grammars from annotated parser or input tree grammars. These extensions can simplify the process of generating source-to-source language translators and they make possible an approach, called "polyphase parsing," to translation between computing languages. The typical approach to translator development is to identify high-level semantic constructs such as "expressions," "declarations," and "definitions" as fundamental building blocks in the grammar specification used for language recognition. The polyphase approach is to lump ambiguous syntactic constructs during parsing and then disambiguate the alternatives in subsequent tree transformation passes. Polyphase parsing is believed to be useful for generating efficient recognizers for C++ and other languages that, like C++, have significant ambiguities...|$|R
40|$|Recent work on Semantic Role Labeling (SRL) {{has shown}} that to achieve high {{accuracy}} a joint inference on the whole predicate argument structure should be applied. In this paper, we used syntactic subtrees that span potential argument structures of the target <b>predicate</b> in <b>tree</b> kernel functions. This allows Support Vector Machines to discern between correct and incorrect predicate structures and to re-rank them based on the joint probability of their arguments. Experiments on the PropBank data show that both classification and re-ranking based on tree kernels can improve SRL systems...|$|R
40|$|Abstract. Recent work by Hermanns et al. and Kattenbelt et al. has {{extended}} counterexample-guided abstraction refinement (CEGAR) to probabilistic programs. In these approaches, programs are abstracted into Markov Decision Processes (MDPs). Analysis of the MDPs allows to compute lower and upper bounds for {{the probability of}} reaching an error state. The bounds can be improved by refining the abstraction. The approaches of Hermanns et al. and Kattenbelt et al. are limited to <b>predicate</b> reachability <b>tree</b> recently introduced by Gulavani et al, that can use arbitrary interpretation). We show how suitable widening operators can deduce loop invariants difficult to find for predicate abstraction, and propose several refinement techniques. ...|$|R
40|$|Abstract XML query {{rewriting}} in the {{distributed computing}} environment receives high attention recently. Different from existing work, {{we focus on}} the query rewriting in the cases that there are multiple views at the client side. We design two data structures to manage the multiple XML views. MPTree is constructed from the main path of the XML views to generate the candidate query rewriting plan. PPLattice is constructed from the <b>predicate</b> sub <b>trees</b> of the XPath views to validate the candidate query rewriting plan. Based on MPtree and PPLattice, the query rewriting plans search space over multiple views is pruned significantly and the high cost of the query containment can be reduced...|$|R
5000|$|The {{fact that}} the raised {{constituent}} behaves as {{though it is a}} dependent of the higher predicate is generally reflected in the syntax trees that are employed to represent raising structures. The following trees are illustrative of the type of structures assumed for raising-to-object <b>predicates.</b> Both constituency-based <b>trees</b> of phrase structure grammar and dependency-based trees of dependency grammar are employed here: ...|$|R
25|$|The two {{arguments}} Sam and Sally in tree (a) {{are dependent}} on the predicate likes, whereby these arguments are also syntactically dependent on likes. What {{this means is that}} the semantic and syntactic dependencies overlap and point in the same direction (down the tree). Attributive adjectives, however, are predicates that take their head noun as their argument, hence big is a <b>predicate</b> in <b>tree</b> (b) that takes bones as its one argument; the semantic dependency points up the tree and therefore runs counter to the syntactic dependency. A similar situation obtains in (c), where the preposition predicate on takes the two arguments the picture and the wall; one of these semantic dependencies points up the syntactic hierarchy, whereas the other points down it. Finally, the predicate to help in (d) takes the one argument Jim but is not directly connected to Jim in the syntactic hierarchy, which means that that semantic dependency is entirely independent of the syntactic dependencies.|$|R
40|$|This paper {{introduces}} a new algorithm called SIAO 1 for learning {{first order logic}} rules with genetic algorithms. SIAO 1 uses the covering principle developed in AQ where seed examples are generalized into rules using however a genetic search, as initially introduced in the SIA algorithm for attribute-based representation. The genetic algorithm uses a high level representation for learning rules in first order logic and may deal with numerical data as well as background knowledge such as hierarchies over the <b>predicates</b> or <b>tree</b> structured values. The genetic operators may for instance change a predicate into a more general one according to background knowledge, or change a constant into a variable. The evaluation function may take into account user preference biases. Introduction The so-called supervised learning framework is the main way to apply Machine Learning (ML) techniques to rule discovery. In such a framework, a set of examples is collected, where each example is of the form "de [...] ...|$|R
50|$|The two {{arguments}} Sam and Sally in tree (a) {{are dependent}} on the predicate likes, whereby these arguments are also syntactically dependent on likes. What {{this means is that}} the semantic and syntactic dependencies overlap and point in the same direction (down the tree). Attributive adjectives, however, are predicates that take their head noun as their argument, hence big is a <b>predicate</b> in <b>tree</b> (b) that takes bones as its one argument; the semantic dependency points up the tree and therefore runs counter to the syntactic dependency. A similar situation obtains in (c), where the preposition predicate on takes the two arguments the picture and the wall; one of these semantic dependencies points up the syntactic hierarchy, whereas the other points down it. Finally, the predicate to help in (d) takes the one argument Jim but is not directly connected to Jim in the syntactic hierarchy, which means that that semantic dependency is entirely independent of the syntactic dependencies.|$|R
40|$|Abstract- Many XML {{applications}} {{over the}} Internet favor high-performance single-pass streaming XPath evaluation. Finite automata-based algorithms suffer from potentially combinatorial explosion of dynamic states for matching descendant axes. We present QuickXScan for streaming evaluation of XPath queries containing child and descendant axes with complex <b>predicates.</b> Using a <b>tree</b> representation for an XPath query, it employs a matching grid, a compact tree of interrelated stacks {{as in the}} holistic twig join algorithms, to represent the matches and their relationships. QuickXScan fully utilizes transitivity for matching, thus reduces the number of active states to the query size in the worst case. It also evaluates expressions incrementally using propagation and iterative rules, and produces result sequences {{without the need for}} duplicate removal. QuickXScan is practical and highly efficient...|$|R
40|$|An {{attribute}} grammar AG generates the language consisting of all strings {{that have a}} legal parse tree in AG. Because of the <b>predicates,</b> a parse <b>tree</b> of the original context-free grammar {{may no longer be}} a legal parse tree of the {{attribute grammar}}, and thus the language generated by an attribute grammar is in general a subset of the corresponding context-free language. It is clear that any context-free language can be generated by an attribute grammar with trivial attributes, functions, and predicates, but the converse is not valid. It is easy to construct simple attribute grammars that accept languages that are not context-free. Two questions are investigated: what exactly is the expressive power of attribute grammar; and which superset of the context-free languages do they exactly define...|$|R
40|$|We study {{abstract}} {{local reasoning}} for concurrent libraries. There {{are two main}} approaches: provide a specification of a library by abstracting from concrete reasoning about an implementation; or provide a direct abstract library specification, justified by refining to an implementation. Both approaches have a data structures and the concrete connectivity of the concrete heap representations. We demonstrate this predicates (CAP) for reasoning about a concrete tree implementation. The gap between the abstract and concrete connectivity emerges as a mismatch between the SSL <b>tree</b> <b>predicates</b> and CAP heap predicates. This gap is closed by an interface function I which links the abstract and concrete connectivity. In the accompanying technical report, we generalise our SSL reasoning and results to arbitrary concurrent data libraries...|$|R
40|$|As a {{consequence}} of the rapid, government-led and globally fuelled urban development that is occurring within China, an unplanned form of urbanization is emerging, whereby landless farmers and economic migrants are resettling and occupying both public space and housing in ways that deviate from the community development plan. The paper will use both historical and contemporary urban theory, together with a case study of Zhangjiing in Suzhou Industrial Park, China as means of critiquing and learning from these consequences and the planning and policy instruments in place. The case of Zhangjing can be critically reviewed in the context of Christopher Alexander’s argument that when a new urban development is created which is modelled or <b>predicated</b> on a <b>tree</b> structure to replace the semi-lattice that was there before, the city takes a step towards dissociating itself from its geographical and cultural context...|$|R
40|$|We {{present a}} {{class-based}} approach {{to building a}} verb lexicon that makes explicit the close relation between syntax and semantics for Levin classes. We have used a Lexicalized Tree Adjoining Grammar to capture the syntax associated with each verb class and have added semantic <b>predicates</b> to each <b>tree,</b> which allow for a compositional interpretation. 1. Introduction We describe a computational verb lexicon called VerbNet which utilizes Levin verb classes (Levin, 1993) to systematically construct lexical entries. We have used Lexicalized Tree Adjoining Grammar (LTAG) (Joshi, 1985; Schabes, 1990) to capture the syntax associated with each verb class, and have added semantic predicates. We also show how regular extensions of verb meaning can be achieved through the adjunction of particular syntactic phrases. We base these regular extensions on intersective Levin classes, a fine-grained variation on Levin classes, {{as a source of}} semantic components associated with specific adjuncts (Dang et [...] ...|$|R
40|$|Abstract: AI planner {{is one of}} the {{important}} representations of AI planning study, the performances of planner, the efficiency and quality of plan, represent the researches of AI planning directly. This paper introduces the architectures of AI planner and StepByStep planner in brief, then describes the methods and strategies adopted by SteByStep in detail, defines the knowledge <b>tree</b> of <b>predicate</b> for extracting domain knowledge. Based on knowledge <b>tree</b> of <b>predicate,</b> the planning <b>tree</b> of <b>predicate</b> is defined and some strategies are applied for constructing the planning tree fast. StepByStep adopts some planning strategies according to the planning trees. Finally, some experiments are taken for eight planners with three representative benchmark domains and their problems, the performances of StepByStep, the efficiency and quality of plan, are analyzed in detail. Experiments show that the strategies of StepByStep controls the process of planning the problems of the three domains well, and validate th...|$|R
40|$|ProB {{is being}} used for {{teaching}} the B-method. In this paper, we present two new features of ProB that we have introduced while teaching B. One feature allows a student (or an expert user) to graphically visualise any <b>predicate</b> as a <b>tree.</b> The underlying algorithm can deal with undefined subformulas and tries to provide useful feedback even for existentially quantified formulas which are false. This feature is especially useful to inspect unexpected invariant violations or operations which are unexpectedly enabled or disabled. The other feature enables a student or lecturer to easily and quickly write custom graphical state representations, to provide {{a better understanding of}} the model. With this method, one simply has to assemble a series of pictures and to write an animation function in B itself, which stipulates which pictures should be shown where depending on the current state of the model. As an additional side-benefit, writing the animation function in B itself is a good exercise for students...|$|R
40|$|Abstract. The ease of {{compiling}} {{malicious code}} from source code in higher programming languages {{has increased the}} volatility of malicious programs: The first appearance of a new worm in the wild is usually followed by modified versions in quick succession. As demonstrated by Christodorescu and Jha, however, classical detection software relies on static patterns, and is easily outsmarted. In this paper, we present a flexible method to detect malicious code patterns in executables by model checking. While model checking was originally developed to verify the correctness of systems against specifications, we argue that it lends itself equally well to the specification of malicious code patterns. To this end, we introduce the specification language CTPL (Computation <b>Tree</b> <b>Predicate</b> Logic) which extends the well-known logic CTL, and describe an efficient model checking algorithm. Our practical experiments demonstrate {{that we are able}} to detect a large number of worm variants with a single specification. Key words: Model Checking, Malware Detection. ...|$|R
40|$|AbstractWe study {{abstract}} {{local reasoning}} for concurrent libraries. There {{are two main}} approaches: provide a specification of a library by abstracting from concrete reasoning about an implementation; or provide a direct abstract library specification, justified by refining to an implementation. Both approaches have a significant gap in their reasoning, due to a mismatch between the abstract connectivity of the abstract data structures and the concrete connectivity of the concrete heap representations. We demonstrate this gap using structural separation logic (SSL) for specifying a concurrent tree library and concurrent abstract predicates (CAP) for reasoning about a concrete tree implementation. The gap between the abstract and concrete connectivity emerges as a mismatch between the SSL <b>tree</b> <b>predicates</b> and CAP heap predicates. This gap is closed by an interface function I which links the abstract and concrete connectivity. In the accompanying technical report, we generalise our SSL reasoning and results to arbitrary concurrent data libraries...|$|R
40|$|In {{this paper}} we develop mathematically strong systems of {{analysis}} in higher types which, nevertheless, are proof-theoretically weak, i. e. conservative over elementary resp. primitive recursive arithmetic. These systems {{are based on}} non-collapsing hierarchies (n -WKL+; 	 n -WKL+) of principles which generalize (and for n = 0 coincide with) the so-called `weak' König's lemma WKL (which has been studied extensively {{in the context of}} second order arithmetic) to logically more complex <b>tree</b> <b>predicates.</b> Whereas the second order context used in the program of reverse mathematics requires an encoding of higher analytical concepts like continuous functions F : X ! Y between Polish spaces X;Y, the more exible language of our systems allows to treat such objects directly. This is of relevance as the encoding of F used in reverse mathematics tacitly yields a constructively enriched notion of continuous functions which e. g. for F : IN ! IN can be seen (in our higher order context...|$|R
40|$|The ease of {{compiling}} {{malicious code}} from source code in higher programming languages {{has increased the}} volatility of malicious programs: The first appearance of a new worm in the wild is usually followed by modified versions in quick succession. As demonstrated by Christodorescu and Jha, however, classical detection software relies on static patterns, and is easily outsmarted. In this paper, we present a flexible method to detect malicious code patterns in executables by model checking. While model checking was originally developed to verify the correctness of systems against specifications, we argue that it lends itself equally well to the specification of malicious code patterns. To this end, we introduce the specification language CTPL (Computation <b>Tree</b> <b>Predicate</b> Logic) which extends the well-known logic CTL, and describe an efficient model checking algorithm. Our practical experiments demonstrate {{that we are able}} to detect a large number of worm variants with a single specification. © Springer-Verlag Berlin Heidelberg 2005...|$|R
40|$|Many query {{languages}} {{are currently being}} proposed for specifying XML document retrievals. The expressive power and usefulness of these query languages is really based on their embedded formalisms and intended XML document applications. The emerging MPEG- 7 multimedia standard uses XML Schema:Datatypes for multimedia content descriptions and has posed an interesting challenge to XML query language design for XML document retrievals. Most XML query language proposals have limitations in specifying queries {{for this type of}} XML documents. In this paper, we have identified some critical specification issues in MPEG- 7 XML queries and propose an XML query language, MMDOC-QL with multimedia query constructs. MMDOC-QL is based on a logic formalism, called path predicate calculus. In this path predicate calculus, the atomic logic formulas are element predicates rather than relation predicates in relational calculus. In this path calculus query language, queries in this calculus are equivalent to finding all proofs to existential closure of logical assertions in the form of path <b>predicates</b> that the <b>tree</b> document elements must satisfy. Spatial, temporal and visual datatypes and relationships can also be described in this formalism for content retrieval. A Logic Approach for MPEG- 7 XML Document Querie...|$|R
40|$|The Tamil IT! (Interactive Translation) speech {{translation}} system {{is intended to}} allow unsophisticated users to communicate across the Tamil ↔ English language barrier, without strong domain restrictions, despite the error prone nature of current speech and translation technologies. Achieving this ambitious goal depends {{in large part on}} allowing the users to interactively correct recognition and translation errors. We briefly present the Multi Engine Machine Translation (MEMT) architecture, describing how it is well suited for such an application. We then describe our incorporation of interactive error correction throughout the system design. We are currently {{in the process of developing}} a Tamil ↔ English system based on this architecture. A Brief Overview of Tamil Language Analysis Like any other language analysis process, Tamil language analysis also involves morphological analysis, syntax analysis and semantic analysis. Tamil is a Morphologically rich language. Most of the grammatical functions are embedded into the word in the form of inflections. Morphological Analysis Here is an example of Tamil morphological analysis. eeRineen eeRu in een (climbed) (Verb) (Past tense) (I Person+Singular+Neuter) In the above example the word eeRineen (climbed) has three morphemes viz. (1) eeRu [Verb for climb], (2) in [Past tense marker] and (3) een [GNP marker] Syntax Analysis Syntactically, Tamil is a head final language. Information like the tense, gender, number and person can be found embedded within the inflected verb (<b>predicate).</b> The parse <b>tree</b> for the __________________________________________________________________________...|$|R
40|$|Deposited with {{permission}} of the author. © 2006 Catherine LaiThe analysis of human communication, in all its forms, increasingly depends on large collections of texts and transcribed recordings. These collections, or corpora, are often richly annotated with structural information. These datasets are extremely large so manual analysis is only successful up to a point. As such, significant effort has recently been invested in automatic techniques for extracting and analyzing these massive data sets. However, further progress on analytical tools is confronted by three major challenges. First, we need the right data model. Second, {{we need to understand}} the theoretical foundations of query languages on that data model. Finally, we need to know the expressive requirements for general purpose query language with respect to linguistics. This thesis has addressed all three of these issues. Specifically, this thesis studies formalisms used by linguists and database theorists to describe tree structured data. Specifically, Propositional dynamic logic and monadic second-order logic. These formalisms have been used to reason about a number of tree querying languages and their applicability to the linguistic tree query problem. We identify a comprehensive set of linguistic tree query requirements and the level of expressiveness needed to implement them. The main result {{of this study is that}} the required level of expressiveness of linguistic tree query is that of the first-order <b>predicate</b> calculus over <b>trees.</b> This formal approach has resulted in a convergence between two seemingly disparate fields of study. Further work in the intersection of linguistics and database theory should also pave the way for theoretically well-founded future work in this area. This, in turn, will lead to better tools for linguistic analysis and data management, and more comprehensive theories of human language. Open Acces...|$|R
40|$|Sorting is, {{together}} with partitioning and indexing, {{one of the}} core paradigms on which current Database Management System implementations base their query processing. It {{can be applied to}} efficiently compute joins, anti-joins, nearest neighbour joins (NNJs), aggregations, etc. It is efficient since, after the sorting, it makes one sequential scan of both inputs, and does not fetch redundantly tuples that do not appear in the result. However, sort-based approaches loose their efficiency in the presence of temporal data: i) when dealing with time intervals, backtracking to previously scanned tuples that are still valid refetches in vain also tuples that are not anymore valid and will not appear in the result; ii) when dealing with timestamps, in computing NNJs with grouping attributes, blocks storing tuples of different groups are refetched multiple times. The goal of this thesis is to provide support to database systems for performing efficient sort-merge computations in the above cases. We first introduce a new operator for computing NNJ queries with integrated support of grouping attributes and selection <b>predicates.</b> Its evaluation <b>tree</b> avoids false hits and redundant fetches, which are major performance bottlenecks in current NNJ solutions. We then show that, in contrast to current solutions that are not group- and selection-enabled, our approach does not constrain the scope of the query optimizer: query trees using our solution can take advantage of any optimization based on the groups, and any optimization on the selection predicates. For example, with our approach the Database Management System can use a sorted index scan for fetching at once all the blocks of the fact table storing tuples with the groups of the outer relation and, thus, reducing the tuples to sort. With Lateral NNJs, instead, groups are processed individually, and blocks storing tuples of different groups are fetched multiple times. With our approach the selection can be pushed down before the join if it is selective, or evaluated on the fly while computing the join if it’s not. With an indexed NNJ, instead, selection push down causes a nested loop which makes the NNJ inefficient due to the quadratic number of pairs checked. We applied our findings and implemented our approach into the kernel of the open source database system PostgreSQL. We then introduce a novel partitioning technique, namely Disjoint Interval Partitioning (DIP), for efficiently computing sort-merge computations on interval data. While current partitioning techniques try to place tuples with similar intervals into the same partitions, DIP does exactly the opposite: it puts tuples that do not overlap into the same partitions. This yields more merges between partitions but each of those no longer requires a nested-loop but can be performed more efficiently using sort-merge. Since DIP outputs the partitions with their elements already sorted, applying a temporal operator to two DIP partitions is performed in linear time, in contrast to the quadratic time {{of the state of the}} art solutions. We illustrate the generality of our approach by describing the implementation of three basic database operators: join, anti-join, and aggregation. Extensive analytical evaluations confirm the efficiency of the solutions presented in this thesis. We experimentally compare our solutions to the state of the art approaches using real-world and synthetic temporal data...|$|R

