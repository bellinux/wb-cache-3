32|10000|Public
50|$|This {{field of}} study has its {{historical}} roots in numerous disciplines including machine learning, experimental psychology and Bayesian statistics. As early as the 1860s, {{with the work of}} Hermann Helmholtz in experimental psychology the brain's ability to extract <b>perceptual</b> <b>information</b> <b>from</b> sensory data was modeled in terms of probabilistic estimation. The basic idea is that the nervous system needs to organize sensory data into an accurate internal model of the outside world.|$|E
5000|$|Geometric feature {{learning}} is a technique combining machine learning and computer vision to solve visual tasks. The main goal of this method {{is to find a}} set of representative features of geometric form to represent an object by collecting geometric features from images and learning them using efficient machine learning methods. Humans solve visual tasks and can give fast response to the environment by extracting <b>perceptual</b> <b>information</b> <b>from</b> what they see. Researchers simulate humans' ability of recognizing objects to solve computer vision problems. For example, M. Mata et al.(2002) [...] applied {{feature learning}} techniques to the mobile robot navigation tasks in order to avoid obstacles. They used genetic algorithms for learning features and recognizing objects (figures). Geometric feature learning methods can not only solve recognition problems but also predict subsequent actions by analyzing a set of sequential input sensory images, usually some extracting features of images. Through learning, some hypothesis of the next action are given and according the probability of each hypothesis give a most probable action. This technique is widely used in the area of artificial intelligence.|$|E
40|$|Abstract Tensor {{voting is}} a {{well-known}} robust technique for extracting <b>perceptual</b> <b>information</b> <b>from</b> clouds of points. This chapter proposes a general methodology to adapt tensor voting to different types of images {{in the specific context}} of image structure estimation. This methodology is based on the structural relationships between tensor voting and the so-called structure tensor, which is the most popu...|$|E
40|$|ABSTRACT—For many of {{the complex}} motor actions we perform, <b>perceptual</b> <b>information</b> is {{available}} <b>from</b> several different senses including vision, touch, hearing, and the vestibular system. Here I discuss the use of multisensory information for the control of motor action in three par-ticular domains: aviation, sports, and driving. It is shown that performers in these domains use <b>information</b> <b>from</b> multiple senses—frequently with beneficial effects on per-formance but sometimes with dangerous consequences. Applied psychologists {{have taken advantage of}} our natural tendency to integrate sensory information by designing multimodal displays that compensate for situations in which <b>information</b> <b>from</b> one or more of our senses is un-reliable or is unattended due to distraction. KEYWORDS—perception; action; sensory integration; vision...|$|R
40|$|My {{research}} investigates why nouns are learned disproportionately {{more frequently}} than other kinds of words during early language acquisition (Gentner, 1982; Gleitman, et al., 2004). This question {{must be considered in}} the context of cognitive development in general. Infants have two major streams of environmental information to make meaningful: perceptual and linguistic. <b>Perceptual</b> <b>information</b> flows in <b>from</b> the senses and is processed into symbolic representations by the primitive language of thought (Fodor, 1975). These symbolic representations are then linked to linguistic input to enable language comprehension and ultimately production. Yet, how exactly does <b>perceptual</b> <b>information</b> become conceptualized? Although this question is difficult, there has been progress. One way that children might have an easier job is if the...|$|R
40|$|The {{purpose of}} this study is to present a test-based ap-proach for {{recognizing}} from the spectral structure of un-pitched short sounds (in the 50 - 100 milliseconds range) whether several time-stretching response functions are per-ceptually similar. For example, in a spectral composition the problem may be to determine whether the harmonic-ity/inharmonicity percentage defined in a short temporal scale by mathematical ratios, apply to each different time-stretching factor, preserving the object identity. This inves-tigation proposes to evaluate the <b>perceptual</b> <b>information</b> obtained <b>from</b> the audible similarity comparison between the time-stretching models and their reference patterns. Let (a 1, [...] ., am) be p-dimensional parameters associated with m response models of the same type. This study is con-cerned with the morphological comparison of a 1, [...] ., am to individuate the timbral properties that are salient for simi-larity or identity judgments...|$|R
40|$|Perceptual {{simulations}} are {{unconscious and}} automatic, whereas perceptual imagery is conscious and deliberate, {{but it is}} unclear how easily one can transfer <b>perceptual</b> <b>information</b> <b>from</b> unconscious to conscious awareness. We investigated whether it is possible to be aware of what one is mentally representing; that is, whether it is possible to consciously examine the contents of a perceptual simulation without information being lost. Studies 1 and 2 found that people cannot accurately evaluate the perceptual content of a representation unless attention is explicitly drawn to each modality individually. In particular, when asked to consider sensory experience as a whole, modality-specific auditory, gustatory, and haptic information is neglected, and olfactory and visual information distorted. Moreover, information loss is greatest for perceptually complex, multimodal simulations. Study 3 examined if such information loss leads to behavioral consequences by examining performance during lexical decision, a task whose semantic effects emerge from automatic access to the full potential of unconscious perceptual simulation. Results showed that modality-specific perceptual strength consistently outperformed modality-general sensory experience ratings in predicting latency and accuracy, which confirms that the effects of Studies 1 and 2 are indeed due to information being lost in the transfer to conscious awareness. These findings suggest that people indeed have difficulty in transferring <b>perceptual</b> <b>information</b> <b>from</b> unconscious simulation to conscious imagery. People cannot be aware of the full contents of a perceptual simulation because the act of bringing it to awareness leads to systematic loss of information...|$|E
40|$|Spatial {{information}} processing {{with respect to}} an egocentric reference frame {{has been shown to}} recruit a fronto-parietal network along the dorsal stream. The present study investigates how brain lesions in the relevant areas affect the ability to navigate through computer-simulated tunnels shown from a first person perspective. Our results suggest that parietal, but not frontal, patients are impaired in this task. They confused the direction of tunnel turns more frequently and made less accurate judgments about the location of the end position. Errors in map drawing suggest that the impairment may be linked to deficits in updating cognitive heading in the absence of corresponding <b>perceptual</b> <b>information</b> <b>from</b> the virtual environment...|$|E
40|$|DMAPS (Distributed Multi-Agent Planning System) is a {{planning}} system developed for distributed multi-robot teams based on MAPS (Multi-Agent Planning System). MAPS assumes that each agent {{has the same}} global view of the environment {{in order to determine}} the most suitable actions. This assumption fails when perception is local to the agents: each agent has only a partial and unique view of the environment. DMAPS addresses this problem by creating a probabilistic global view on each agent by fusing the <b>perceptual</b> <b>information</b> <b>from</b> each robot. The experimental results on consuming tasks show that while the probabilistic global view is not identical on each robot, the shared view is still effective in increasing performance of the team...|$|E
40|$|The {{original}} {{version of the}} counter model for perceptual identification (Ratcliff & McKoon, 1997) assumed that word frequency and prior study act solely to bias the identification process (i. e., subjects {{have a tendency to}} prefer high-frequency and studied low-frequency words, irrespective of the presented word). In a recent study, using a two-alternative forced-choice paradigm, we showed an enhanced discriminability effect for high-frequency and studied low-frequency words (Wagenmakers, Zeelenberg, & Raaijmakers, 2000). These results have led to a fundamental modification of the counter model: Prior study and high frequency not only result in bias, but presumably also result in a higher rate of feature extraction (i. e., better perception). We demonstrate that a criterion-shift model, assuming limited <b>perceptual</b> <b>information</b> extracted <b>from</b> the flash as well as a reduced distance to an identification threshold for high-frequency and studied low-frequency words, can also account for enhanced discriminability...|$|R
40|$|Spatial mental {{representations}} can {{be derived}} from linguistic and non-linguistic sources of information. This study tested whether these representations could be formed from statistical linguistic frequencies of city names, and to what extent participants differed in their performance when they estimated spatial locations from language or maps. In a computational linguistic study, we demonstrated that co-occurrences of cities in Tolkien’s Lord of the Rings trilogy and The Hobbit predicted the authentic longitude and latitude of those cities in Middle Earth. In a human study, we showed that human spatial estimates of the location of cities were very similar regardless of whether participants read Tolkien’s texts or memorized a map of Middle Earth. However, text-based location estimates obtained from statistical linguistic frequencies better predicted the human text-based estimates than the human map-based estimates. These findings suggest that language encodes spatial structure of cities, and that human cognitive map representations can come from implicit statistical linguistic patterns, <b>from</b> explicit non-linguistic <b>perceptual</b> <b>information,</b> or <b>from</b> both...|$|R
40|$|The <b>perceptual</b> <b>information</b> {{transmitted}} <b>from</b> {{a damaged}} cochlea {{to the brain}} is more poorly specified than <b>information</b> <b>from</b> an intact cochlea and requires more processing in working memory before language content can be decoded. In addition to making sounds audible, current hearing aids include several technologies that are intended to facilitate language understanding for persons with hearing impairment in challenging listening situations. These include directional microphones, noise reduction, and fast-acting amplitude compression systems. However, the processed signal itself may challenge listening {{to the extent that}} with specific types of technology, and in certain listening situations, individual differences in cognitive processing resources may determine listening success. Here, current and developing digital hearing aid signal processing schemes are reviewed in the light of individual working memory (WM) differences. It is argued that signal processing designed to improve speech understanding may have both positive and negative consequences, and that these may depend on individual WM capacity. The definitive version is available at www. blackwell-synergy. com: Thomas Lunner, Mary Rudner and Jerker Rönnberg, Cognition and hearing aids., 2009, Scandinavian journal of psychology, (50), 5, 395 - 403. [URL] Copyright: Blackwell Publishing</p...|$|R
40|$|Knowledge {{construction}} requires both <b>perceptual</b> <b>information</b> <b>from</b> external {{sources and}} our active interpretation of that information. Thirty one 5 th grade {{elementary school students}} were asked to move plates in a Tower of Hanoi (TOH) task, which was displayed on a screen monitor. When the {{students were asked to}} respond to the weight of the plates, both imagination and non-imagination groups reported that they felt weight of the plates which actually had no weight. Also, students in imagination group reported that they felt the plates heavier than did those in non-imagination group. The result shows that haptic information can be created without providing any information through haptic channel. In addition, the result implies how knowledge is constructed based on previous knowledge and suggests that children create their own imaginary world even at a perceptual level. Also, it was discussed why imagination experience can maximize and help learning in embodiment activities...|$|E
40|$|International audienceWe {{present an}} {{original}} integration of high level planning and execution with incoming <b>perceptual</b> <b>information</b> <b>from</b> vision, SLAM, topological map segmentation and dialogue. The {{task of the}} robot system, implementing the integrated model, is to explore unknown areas and report detected objects to an operator, by speaking loudly. The knowledge base of the planner maintains a graph-based representation of the metric map that is dynamically constructed via an unsupervised topological segmentation method, and augmented with information about the type and position of detected objects, within the map, such as cars or containers. According to this knowledge the cognitive robot can infer strategies in so generating parametric plans that are instantiated from the perceptual processes. Finally, a model-based approach for the execution {{and control of the}} robot system is proposed to monitor, concurrently, the low level status of the system and the execution of the activities, in order to achieve the goal, instructed by the operator...|$|E
40|$|In this paper, we {{introduce}} a near set approach to image analysis. Near sets result from generalization of rough set theory. One set X is near another set Y {{to the extent}} that the description of at least one of the objects in X matches the description of at least one of the objects in Y. Near set Evaluation And Recognition (NEAR) system is used to measure the degree of resemblance between facial images. The goal of the NEAR system is to extract <b>perceptual</b> <b>information</b> <b>from</b> images using near set theory, which provides a framework for measuring the perceptual nearness of objects. In this work, we have used images from Japanese Female Facial Expression (JAFFE) database. The images were first converted into Local Binary Patterns (LBP) images and then divided into non-overlapping blocks. The degree of nearness of histograms of all the blocks of one image is measured with the corresponding blocks of another image by using NEAR system...|$|E
40|$|How do we {{integrate}} modality-specific <b>perceptual</b> <b>information</b> arising <b>from</b> {{the same}} physical event {{into a coherent}} percept? One possibility is that observers rely on <b>information</b> across <b>perceptual</b> modalities that shares temporal structure and/or semantic associations. To explore the contributions of these two factors in multisensory integration, we manipulated the temporal and semantic relationships between auditory and visual information produced by real-world events, such as paper tearing or cards being shuffled. We identified distinct neural substrates for integration based on temporal structure as compared to integration based on event semantics. Semantically incongruent events recruited left frontal regions, while temporally asynchronous events recruited right frontal cortices. At the same time, both forms of incongruence recruited subregions in the temporal, occipital, and lingual cortices. Finally, events that were both temporally and semantically congruent modulated activity in the parahippocampus and anterior temporal lobe. Taken together, {{these results indicate that}} low-level perceptual properties such as temporal synchrony and high-level knowledge such as semantics play a role in our coherent perceptual experience of physical events. Comment: in revision from Jo...|$|R
40|$|Episodic {{memories}} are typically composed of <b>perceptual</b> <b>information</b> derived <b>from</b> {{the external environment}} and representations of internal states (e. g., one’s thoughts during prior episodes). To date, however, research has mostly focused on the remembrance of external stimuli, such that {{little is known about}} how internal mentation is represented within episodic memory. In the present fMRI study, we examined the neural correlates of these two components of episodic memories using a novel method of cuing memories from photographs taken during real-life events. We found that, compared to corresponding semantic memory tasks, memories for internal thoughts and external elements were associated with activity in brain areas supporting episodic recollection. Most importantly, however, the two kinds of memories also showed differential activation in large-scale brain networks: the remembrance of external elements was associated with greater activity in the dorsal attention network, whereas memories of internal thoughts mainly recruited default network areas. These findings shed new light on the representation of internal and external aspects of prior experience within episodic memory. The default network may contribute to the reinstatement of thoughts experienced during past events, whereas the dorsal attention network may support the allocation of attention to visuo-spatial features within episodic memory representations. Peer reviewe...|$|R
40|$|Learning and {{representing}} semantics {{is one of}} {{the most}} important tasks that significantly contribute to some growing areas, as successful stories in the recent survey of Turney and Pantel (2010). In this thesis, we present an in- novative (and first) framework for creating a multimodal distributional semantic model from state of the art text-and image-based semantic models. We evaluate this multimodal semantic model on simulating similarity judgements, concept clustering and the newly introduced BLESS benchmark. We also propose an effective algorithm, namely Parameter Estimation, to integrate text- and image- based features in order to have a robust multimodal system. By experiments, we show that our technique is very promising. Across all experiments, our best multimodal model claims the first position. By relatively comparing with other text-based models, we are justified to affirm that our model can stay in the top line with other state of the art models. We explore various types of visual features including SIFT and other color SIFT channels in order to have prelim- inary insights about how computer-vision techniques should be applied in the natural language processing domain. Importantly, in this thesis, we show evi- dences that adding visual features (as the <b>perceptual</b> <b>information</b> coming <b>from</b> [...] ...|$|R
40|$|This thesis {{presents}} {{the design of}} a vision framework integrated into a robotics research and development platform. The vision system was implemented as part of the software platform developed by the Personal Robots Group at the MIT Media Lab. Featuring representations for images and camera sensors, this system provides a structure that is used to build robot vision applications. One application shows how to merge the representations of two different cameras {{in order to create a}} camera entity that provides images with fused depth-color data. The system also allows the integration of computer vision algorithms that can be used to extract <b>perceptual</b> <b>information</b> <b>from</b> the robot's surroundings. Two more applications show detection and tracking of human face and body pose using depth-color images. by Julián Hernández Muñoz. Thesis (M. Eng.) [...] Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2011. Cataloged from PDF version of thesis. Includes bibliographical references (p. 69 - 70) ...|$|E
40|$|There is {{evidence}} to suggest that sports experts are able to extract more <b>perceptual</b> <b>information</b> <b>from</b> a single fixation than novices when exposed to meaningful tasks that are specific to their field of expertise. In particular, Reingold et al. (2001) showed that chess experts use a larger visual span including fewer fixations when compared to their less skilled counterparts. The aim {{of the present study was}} to examine whether also in a more complex environment, namely soccer, skilled players use a larger visual span and fewer fixations than less skilled players when attempting to recognise players' positions. To this end, we combined the gaze-contingent window technique with the change detection paradigm. Results seem to suggest that skilled soccer players do not use a larger visual span than less skilled players. However, skilled soccer players showed significantly fewer fixations of longer duration than their less skilled counterparts, supporting the notion that experts may extract more information from a single glance. © 2011 Psychology Press...|$|E
40|$|This paper {{describes}} {{an application of}} decoupled probabilistic world modeling to achieve team planning. The research {{is based on the}} principle that tbe action selection mechanism of a member in a robot team cm select am effective action if a global world model is available to all team members. In the real world, the sensors are imprecise, and are individual to each robot, hence providing each robot a partial and unique view about the environment. We address this problem by creating a probabilistic global view on each agent by combining the <b>perceptual</b> <b>information</b> <b>from</b> each robot. This probsbilistie view forms the basis for selecting actions to achieve the team goal in a dynamic environment. Experiments have been carried ont to investigate the effectiveness of this principle using custom-built robots for real world performance, in addition, to extensive simulation results. The results show an improvement in team effectiveness when using probabilistic world modeling based on perception sharing for team planning...|$|E
40|$|In {{settings}} where heterogenous robotic systems interact with humans, <b>information</b> <b>from</b> the environment must be systematically captured, organized and maintained in time. In this work, we propose {{a model for}} connecting <b>perceptual</b> <b>information</b> to semantic information in a multi-agent setting. In particular, we present semantic cooperative perceptual anchoring, that captures collectively acquired <b>perceptual</b> <b>information</b> and connects it to semantically expressed commonsense knowledge. We describe how we implemented the proposed model in a smart environment, using different modern perceptual and knowledge representation techniques. We present {{the results of the}} systemand investigate different scenarios in which we use the common sense together with perceptual knowledge, for communication, reasoning and exchange of information...|$|R
50|$|<b>Perceptual</b> <b>information</b> is {{processed}} in both hemispheres, but is laterally partitioned: <b>information</b> <b>from</b> {{each side of}} the body is sent to the opposite hemisphere (visual information is partitioned somewhat differently, but still lateralized). Similarly, motor control signals sent out to the body also come from the hemisphere on the opposite side. Thus, hand preference (which hand someone prefers to use) is also related to hemisphere lateralization.|$|R
40|$|This paper {{proposes a}} <b>Perceptual</b> <b>Information</b> Infrastructure {{supporting}} robots {{in a real}} world and shows a distributed vision system for navigating mobile robots {{as an example of}} the infrastructure. The <b>perceptual</b> <b>information</b> infrastructure enables to develop a new research direction of robots which are integrated with environments...|$|R
40|$|In {{this paper}} a new perceptual {{grouping}} method is proposed, {{and the quality}} of its expected grouping result is studied. The proposed method is fairly general; it may be used for the grouping of various types of data features, and to incorporate different grouping cues of first order and of higher order statistics. The grouping method is divided into two main stages: first it uses some perceptual grouping cues to extract <b>perceptual</b> <b>information</b> <b>from</b> the image. It uses a cue enhancement procedure, which takes grouping cues of high order statistics and provides a very reliable bi-feature grouping cue. The extracted information, represented by a graph, is then used to find the "best" partition of the graph into groups. Both stages are implemented using known statistical tools such as Wald's SPRT algorithm and the Maximum Likelihood criterion. The quantitative analysis of this method shows how the quality of the resulted grouping depends on the reliability of the grouping cues, an [...] ...|$|E
40|$|We {{present an}} {{original}} integration of high level planning and execution with incoming <b>perceptual</b> <b>information</b> <b>from</b> vision, SLAM, topological map segmentation and dialogue. The {{task of the}} robot system, implementing the integrated model, is to explore unknown areas and report detected objects to an operator, by speaking loudly. The knowledge base of the planner maintains a graph-based representation of the metric map that is dynamically constructed via an unsupervised topological segmentation method, and augmented with information about the type and position of detected objects, within the map, such as cars or containers. According to this knowledge the cognitive robot can infer strategies in so generating parametric plans that are instantiated from the perceptual processes. Finally, a model-based approach for the execution {{and control of the}} robot system is proposed to monitor, concurrently, the low level status of the system and the execution of the activities, in order to achieve the goal, instructed by the operator. Copyright © 2011, Association for the Advancement of Artificial Intelligence. All rights reserved...|$|E
40|$|Processing spatial {{information}} {{with respect to}} an egocentric reference frame {{has been shown to}} recruit a cortical network along the dorsal stream. However, how brain lesions affect this ability remains controversial. The present study investigated spatial navigation in parietal and frontal patients as well as healthy controls in computer-simulated tunnels. Two different answer formats were employed: the setting of a virtual three-dimensional arrow to indicate the location of the endpoint relative to the starting point, and map drawing. Also, mental rotation skills and visuospatial working memory were assessed. The results suggest that parietal, but not frontal, patients are impaired in building up an adequate representation of virtual space relative to controls, and that this impairment covaries with spatial working memory performance. Patients with damage to the parietal lobes confused the direction of turn more frequently and made less accurate angular judgements of tunnel turns than frontal-lobe patients. Analysis of the map drawings suggests that the impairment may be linked to deficits in the updating of cognitive heading in absence of corresponding <b>perceptual</b> <b>information</b> <b>from</b> the virtual environment. 2 1...|$|E
40|$|Auditory-verbal {{hallucinations}} (AVHs) {{are frequently}} associated with {{activation of the}} left superior temporal gyrus (including Wernickes area), left inferior frontal gyrus (including Brocas area), and the right hemisphere homologs of both areas. It has been hypothesized that disconnectivity of both interhemispheric transfer and frontal and temporal areas may underlie hallucinations in schizophrenia. We investigated reduced information flow in this circuit {{for the first time}} using dynamic causal modeling, which allows for directional inference. A group of healthy subjects and 2 groups of schizophrenia patientsuwith and without AVHuperformed a task requiring inner speech processing during functional brain scanning. We employed connectivity models between left hemispheric speech-processing areas and their right hemispheric homologs. Bayesian model averaging was used to estimate the connectivity strengths and evaluate group differences. Patients with AVH showed significantly reduced connectivity from Wernickes to Brocas area (97 % certainty) and a trend toward a reduction in connectivity from homologs of Brocas and Wernickes areas to Brocas area (93 % and 94 % certainty). The connectivity magnitude in patients without hallucinations was found to be intermediate. Our results point toward a reduced input from temporal to frontal language areas in schizophrenia patients with AVH, suggesting that Brocas activity may be less constrained by <b>perceptual</b> <b>information</b> received <b>from</b> the temporal cortex. In addition, a lack of synchronization between Broca and its homolog may lead to the erroneous interpretation of emotional speech activity from the right hemisphere as coming from an external source. ...|$|R
40|$|Research {{in speech}} {{perception}} {{has been dominated}} by a search for invariant properties of the signal that correlate with lexical and sublexical categories. We argue that this search for invariance has led researchers to ignore the perceptual consequences of systematic variation within such categories and that sensitivity to this variation may provide an important source of information for integrating information over time in speech perception. Data from a study manipulating VOT continua in words using an eye-movement paradigm indicate that lexical access shows graded sensitivity to within-category variation in VOT and that this sensitivity has a duration sufficient to be useful for information integration. These data support a model in which the <b>perceptual</b> system integrates <b>information</b> <b>from</b> multiple sources and from the surrounding temporal context using probabilistic cue-weighting mechanisms. KEY WORDS: speech perception; spoken word recognition; invariance; lexical access; eye movements...|$|R
50|$|Most model based {{strategies}} of motor control rely on <b>perceptual</b> <b>information,</b> but {{assume that this}} information is not always useful, veridical or constant. Optical information is interrupted by eye blinks, motion is obstructed by objects in the environment, distortions can change the appearance of object shape. Model based and representational control strategies are those that rely on accurate internal models of the environment, constructed {{from a combination of}} <b>perceptual</b> <b>information</b> and prior knowledge, as the primary source information for planning and executing actions, {{even in the absence of}} <b>perceptual</b> <b>information.</b>|$|R
40|$|The 'body schema' has {{traditionally}} been defined as a passively updated, proprioceptive representation of the body. However, recent work has suggested that body representations are more complex and flexible than previously thought. They may integrate current <b>perceptual</b> <b>information</b> <b>from</b> all sensory modalities, and can be extended to incorporate indirect representations {{of the body and}} functional portions of tools. In the present study, we investigate the source of a facilitatory effect of viewing the body on speeded visual discrimination reaction times. Participants responded to identical visual stimuli that varied only in their context: being presented on the participant's own body, on the experimenter's body, or in a neutral context. The stimuli were filmed and viewed in real-time on a projector screen. Careful controls for attention, biological saliency, and attribution confirmed that the facilitatory effect depends critically on participants attributing the context to a real body. An intermediate effect was observed when the stimuli were presented on another person's body, suggesting that the effect of viewing one's own body might represent a conjunction of an interpersonal body effect and an egocentric effect...|$|E
40|$|Because of {{detailed}} geometrical components based description, 3 D complex building contains the most elaborated perceptual and comprehensive semantic information. However, since {{the lack of}} optimal simplification method, the automatic LOD generation of such kind of model becomes a bottle neck which prohibited the high-fidelity 3 D city applications. This paper proposed a perceptually guided geometrical primitive location method for the optimal simplification of 3 D complex buildings. Firstly, the rendered image is snapped and a 2 D discrete wavelet transform based human vision system filtering approach is adopted to extract the imperceptible details in the image, and then a kind of visual difference image is generated with sufficient perceptual information. Secondly, a ray-casting like method is proposed to precisely map the <b>perceptual</b> <b>information</b> <b>from</b> the image onto the geometric primitives. The statistics is carried out {{to determine whether a}} traced primitive is to be preserved or simplified. The results show that this method is able to efficiently locate the perceptible primitives and leave the imperceptible and undisplayable primitives to be further handled by simplification operations which enable a strong perceptual feature preserved simplification of 3 D complex building models. 1...|$|E
40|$|We have an {{abundance}} of <b>perceptual</b> <b>information</b> <b>from</b> multiple modalities specifying our body proportions. Consequently, it seems reasonable for researchers to assume {{that we have an}} accurate perception of our body proportions. In contrast to this intuition, recent research has shown large, striking distortions in people’s perceptions of their the relative proportions of their own bodies. Specifically, individuals show large distortions when estimating the length of their body parts with a corporal metric, such as the hand, but not with a non-corporal object of the same length (Linkenauger et al., 2015). However, it remains unclear whether these distortions are specific to the perception of the relative proportions of one’s own body or whether they generalize to the perception of the relative proportions of all human bodies. To assess this, individuals judged the relative lengths of either their own body parts or the body parts of another individual. We found that people have distorted perceptions of relative body proportions even when viewing the bodies of others. These distortions were greater when estimating the relative body parts of someone of the same gender. These results suggest our implicit full body representation is distorted and influences our perceptions of other people’s bodies, especially if the other person’s body is similar to our own...|$|E
40|$|This paper {{concerns}} a problem which is basic to perception: {{the integration of}} <b>perceptual</b> <b>information</b> into a coherent description of the world. In this paper we present perception {{as a process of}} dynamically maintaining a model of the local external environment. Fusion of <b>perceptual</b> <b>information</b> {{is at the heart of}} this process...|$|R
40|$|Perceptual {{grouping}} {{is traditionally}} {{concerned with the}} use of grouping information (or cues) for image partitioning tasks such as figure ground discrimination, edge completion, segmentation and partition. Here, on the other hand, such grouping cues are not used to partition the image but to confirm a hypothesized segmentation. Hypothesis verification is the last stage in most object recognition methods. It is carried out by evaluating a score for each hypothesis, and choosing the hypotheses associated with the highest score. This paper suggests a grouping-based verification paradigm, relying on the observation that a set of data features belonging to an hypothesized object instance should be a "good group". Therefore it should support <b>perceptual</b> grouping <b>information,</b> available <b>from</b> the image by grouping relations. The proposed score, which is the joint likelihood of these grouping cues, quantifies this observation in a probabilistic framework. Experiments with synthetic and [...] ...|$|R
30|$|Using PE to {{evaluate}} the <b>perceptual</b> <b>information,</b> only intrachannel redundancy and irrelevancy are exploited; the overall PE is simply the sum of PE of {{the left and right}} channels. Using SPE based on BCPPM, interchannel redundancy and irrelevancy are also exploited; the overall <b>perceptual</b> <b>information</b> is about one normal audio channel plus some spatial parameters, which has significantly lower bitrate.|$|R
