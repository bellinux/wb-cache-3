0|90|Public
40|$|This paper {{develops}} two coordination {{models of}} {{a supply chain}} consisting of one manufacturer, one dominant retailer and multiple fringe retailers to investigate how to coordinate the supply chain after demand disruption. We consider two coordination schedules, <b>linear</b> <b>quantity</b> discount schedule and Groves wholesale price schedule. We find that, under the <b>linear</b> <b>quantity</b> discount schedule, the manufacturer only needs to adjust the maximum variable wholesale price after demand disruption. For each case of the disrupted amount of demand, the higher the market share of the dominant retailer, the lower its average wholesale price and the subsidy will be under the <b>linear</b> <b>quantity</b> discount schedule, while the higher its fraction of the supply chain's profit will be under Groves wholesale price schedule. When the increased amount of demand is very large and production cost is sufficiently low, <b>linear</b> <b>quantity</b> discount schedule is better for the manufacturer. However, when the production cost is sufficiently large, Groves wholesale price schedule is always better. We also find that the disrupted amount of demand largely affects the allocation of the supply chain's profit. Coordination mechanism Disruption management Supply chain management Game theory...|$|R
50|$|Significant {{volcanic}} activity {{took place in}} the Lower Paleozoic in the Malé Karpaty Mts., where the relicts are seen in the rock of the Pernek Group with typical basic volcanism. Large volumes of volcanic rock, considered a product of stratovolcanos, significantly changed by metamorphism, are present in the Gemeric. Basic volcanism is recognized in the Carboniferous and Permian rock. Among the Permian rock the Ipoltica Group of the Hronic nappe is best known. The lower part of the group is called the Malužiná Formation. It is characteristic of synsedimentary dacite to andesite volcanism in the lower part and andesitic-basalts close to the Tholeitic type in the upper part. Nodules of hydrothermal agate are common in the cavities of these rocks, widely known as the melaphyres. According to some authors, Permian volcanism in Hronic has <b>polyphase</b> <b>linear</b> character.|$|R
2500|$|The voltage {{standing}} wave ratio (VSWR) at a port, represented by the lower case 's', is a similar measure of port match to return loss but is a scalar <b>linear</b> <b>quantity,</b> {{the ratio of the}} {{standing wave}} maximum voltage to the standing wave minimum voltage. It therefore relates to the magnitude of the voltage reflection coefficient and hence to the magnitude of either [...] for the input port or [...] for the output port.|$|R
40|$|We {{consider}} the frequency response problem and derive a posteriori error {{estimates for the}} discrete error in a reduced finite element model obtained using the component mode synthesis (CMS) method. We provide estimates in a <b>linear</b> <b>quantity</b> of interest and the energy norm. The estimates reflect to what degree each CMS subspace influence the overall error in the reduced solution. This enables automatic error control through adaptive algorithms that determine suitable dimensions of each subspace. We illustrate the theoretical results by including several numerical examples...|$|R
40|$|We {{consider}} a <b>linear</b> <b>quantity</b> setting duopoly game and analyze {{which of the}} players will commit when both players have the possibility to do so. To that end, we study a 2 -stage game in which each player can either commit to a quantity in stage 1 or wait till stage 2. We show that committing is more risky for the high cost firm and that, consequently, risk dominance considerations, as in Harsanyi and Selten (1988), allow the conclusion that only the low cost firm will choose to commit. Hence, the low cost firm will emerge as the endogenous Stackelberg leader...|$|R
40|$|Over {{the last}} ten years there has been an {{increase}} on the use of goal-oriented error estimates aimed to quantify the local error on a (non) <b>linear</b> <b>quantity</b> of interest (QoI) that might result relevant for design purposes. Residual-based error estimators have been used recursively to obtain upper and lower bounds of the error in quantities of interest for finite element approximations. In this paper, we present a recovery technique for 2 D linear elasticity problems, based on the superconvergent patch recovery (SPR), which provides recovered displacement and stress fields that are then utilised to evaluate practical upper and lower error bounds in QoI...|$|R
40|$|We {{consider}} a <b>linear</b> <b>quantity</b> setting duopoly game and analyze {{which of the}} players will commit when both players have the possibility to do so. To that end, we study a 2 -stage game in which each player can either commit to a quantity in stage 1 or wait till stage 2. We show that committing is more risky for the high cost firm and that, consequently, risk dominance considerations, as in Harsanyi and Selten (1988), allow the conclusion that only the low cost firm will choose to commit. Hence, the low cost firm will emerge as the endogenous Stackelberg leader. Duopoly, Stackelberg, equilibrium selection...|$|R
5000|$|<b>Linear</b> in the <b>quantity</b> {{of light}} received, up {{to a single}} unknown multiplicative {{constant}} for the entire image; ...|$|R
40|$|This {{article is}} closed access. When a small amplitude, water-wave tram is {{incident}} upon a fixed body, a second-order analysis {{predicts that the}} body experiences a steady force and a force at twice {{the frequency of the}} incident wave The double-frequency force is comprised of integrals of products of <b>linear</b> <b>quantities</b> over the surface of the body and the mean waterline and a term due to the second-order potential An application of Green's theorem to the first-order potential and its horizontal derivative shows that the integral of the first order terms over the body is related in a simple way to the waterline integral and the far-field representation of the linear, diffraction potential A minor modification of the analysis yields the farfield formulae for the drift force...|$|R
40|$|A group {{purchasing}} organization (GPO) is {{an entity}} that utilizes collective buying power to obtain significant discounts from vendors, {{which can be}} suppliers, distributors and manufacturers. In the healthcare sector, it is reported that about 72 % of hospital purchases are settled through GPO contracts. This paper seeks to examine two critical questions that vendors face: (1) business strategy: does partnering with a GPO to offer quantity discounts make strategic sense? And, if so, (2) pricing policy: what price will yield optimal, maximum profits in such a relationship? Using a <b>linear</b> <b>quantity</b> discount scheme, {{we find that the}} size of GPO members strongly influences the vendor's decision to contract (or not) with the GPO. Furthermore, we show that vendors should price their products close to the reservation prices of the GPO members if the vendors indeed wish to pursue such partnerships...|$|R
40|$|At {{the time}} this study was undertaken, linear {{electric}} machines (LEMs) were relatively new. Compared with rotating machines, relatively little work had been done on them. Although LEMs are finding ready applications, they concern the polyphase LEMs only. In some applications, they concern the polyphase LEMs only. In some applications LEMs have advantages over rotating machines because of an absence of gears or rotary-to-linear converters, a high reliability {{and the possibility of}} very high speed of the travelling field. However, problems arise from the undesired characteristics of big air gaps, lossy end-effects, low efficiency, and a low power factor. 	This thesis is concerned with a device called a single phase travelling wave linear induction motor (STLIM). Unlike most electrical machines, STLIMs utilise propagating magnetic waves, waves that are obtained by arranging the windings of the linear primary coils and shunt capacitors to simulate a transmission line. The waves drive linear conducting sheet secondaries. The STLIMs have some interesting characteristics: (1) most importantly, the speed of STLIM can be varied by the value of shunt capacitance or series coil inductance; (2) due to the attenuated travelling wave nature, the exit end-effect is self-reduced; (3) the device operates at nearly unity power factor for all ranges of slip; (4) it is a constant current device. Although the experimental efficiency demonstrated in the present work has not been high, it is still compatible with either small single phase induction motors or <b>polyphase</b> <b>linear</b> induction motors of the same physical dimensions. Original mathematical theories of STLIMs have been presented that have been based on one-dimensional current sheet analysis and adapted transmission line distributed parameter theory. The developed experimental models and the tests that have been carried out substantiate the theories...|$|R
40|$|A lesson {{experiment}} {{was used to}} investigate how instruction impacted prospective elementary teachers' conceptual understandings of area and volume. Data sources included ten prospective teachers‟ work on a measurement pre-assessment, lesson activities, and three post-assessments as well as audio recordings of lesson activities. The qualitative analysis consisted of two steps: a „real-time‟ analysis within {{the constraints of the}} week surrounding the lesson and an „intensive-delayed‟ constant-comparative analysis over the next several months. Findings revealed the prospective teachers enhanced their understandings of area and volume. The lesson experiment led to instructional recommendations for improving the lesson in the future, such as the need to address prospective teachers‟ formulaic tendencies, interpretations about basic units, and perceptions of area and volume measurements as <b>linear</b> <b>quantities.</b> Implications include the value of lesson experiments as an iterative process to contribute to the shared knowledge base of mathematics teacher educators...|$|R
40|$|Road {{transportation}} of dangerous materials {{as well as}} pollution are considered as being catastrophic risks. In case the normal insurance models are applied to catastrophic risks, {{one of the basic}} assumptions, namely that the average surplus is a <b>linear</b> <b>quantity</b> in time, is certainly not satisfied. In more realistic situations, an average movement of the surplus is linear up to a certain time, where a drastic decrease in the surplus occurs, namely at the moment when the claims are paid out, then afterwards the surplus will show a linear trend again. In the present contribution we describe the stochastic approach to this kind of situations, based on the same ideas as used in ‘A stochastic approach to insurance cycles’ by Goovaerts et al. (1992). The consequences for the probability of ruin at a specific point in time are also investigated as well as their relation to solvency margins...|$|R
5000|$|... so 's {{component}} {{in the direction of}} [...] is zero. Thus, helicity is just the projection of the spin onto the direction of <b>linear</b> momentum. This <b>quantity</b> is conserved.|$|R
40|$|In this work, we {{investigate}} adaptive approaches to control errors in response surface approximations computed from numerical approximations of differential equations with uncertain or random data and coefficients. The adaptivity {{of the response}} surface approximation {{is based on a}} posteriori error estimation, and the approach relies on the ability to decompose the a posteriori error estimate into contributions from the physical discretization and the approximation in parameter space. Errors are evaluated in terms of <b>linear</b> <b>quantities</b> of interest using adjoint-based methodologies. We demonstrate that a significant reduction in the computational cost required to reach a given error tolerance can be achieved by refining the dominant error contributions rather than uniformly refining both the physical and stochastic discretization. Error decomposition is demonstrated for a two-dimensional flow problem, and adaptive procedures are tested on a convection-diffusion problem with discontinuous parameter dependence and a diffusion problem, where the diffusion coefficient is characterized by a 10 -dimensional parameter space...|$|R
40|$|The nested error {{regression}} {{model is a}} useful tool for analyzing clustered (grouped) data, and is especially used in small area estimation. The classical nested error {{regression model}} assumes normality of random effects and error terms, and homoscedastic variances. However, these assumptions are often violated in real applications and more flexible models are required. This article proposes a nested error regression model with heteroscedastic variances, where the normality for the underlying distributions is not assumed. We propose the structure of heteroscedastic variances by using some specified variance functions and some covariates with unknown parameters. Under the setting, we construct the moment-type estimators of model parameters and some asymptotic properties including asymptotic biases and variances are derived. For predicting <b>linear</b> <b>quantities</b> including random effects, we suggest the empirical best linear unbiased predictors and the second-order unbiased estimators of mean squared errors are derived in the closed form. We investigate the proposed method with simulation and empirical studies...|$|R
40|$|We {{consider}} a <b>linear</b> <b>quantity</b> setting duopoly game and analyze {{which of the}} players will commit when both players have the possibility to do so. To that end, we study a 2 -stage game in which each player can either commit to a quantity in stage 1 or wait till stage 2. We show that committing is more risky for the high cost firm and that, consequently, risk dominance considerations, as in Harsanyi and Selten (1988), allow the conclusion that only the low cost firm will choose to commit. Hence, the low cost firm will emerge as the endogenous Stackelberg leader. Journal of Economic Literature Classification Numbers: C 72, D 43. Copyright c fl 1998 by Academic Press. This material has been accepted for publication in Games and Economic Behavior, the only definitive repository of the content that has been certified and accepted after peer review. Copyright and all rights therein are retained by Academic Press. This material may not be copied or reposted without explicit permission. Hurkens gr [...] ...|$|R
40|$|The skull posture on {{the spine}} is an {{important}} factor for the diagnosis of craniomandibulars functional disorders in children, as well as in adults. Alterations in biomechanics may be evidenced in common x-rays. The aim {{of this study was to}} relate findings of craniocervical analysis with skeletal pattern Class I, II, III in individuals from 7 to 12 years old. X-rays of 92 individuals were taken by NHP method and digitalized for computerized analysis of skeletic patterns and craniocervical analysis. Descriptive analysis was used to relate craniocervical analysis values with the type of skeletal pattern presented by individuals. For skeletal pattern Class I, II and III, nasopharyngeal soft tissue percentage and oropharyngeal soft tissue <b>linear</b> <b>quantity</b> were above normality value; nasopharyngeal cervical plot was normal and hyoid triangle was positive for the majority of the individuals. The angular relationship between skull and cervical spine show values lower than those considered normal for skeletal pattern class I and II. It was concluded that in the majority of the individuals with skeletal pattern Class I, II and III had high values of airway blockage, head posterior rotation, nasopharyngeal cervical plot with normal values and positive hyoid triangle. The skull posture on the spine {{is an important}} factor for the diagnosis of craniomandibulars functional disorders in children, as well as in adults. Alterations in biomechanics may be evidenced in common x-rays. The aim of this study was to relate findings of craniocervical analysis with skeletal pattern Class I, II, III in individuals from 7 to 12 years old. X-rays of 92 individuals were taken by NHP method and digitalized for computerized analysis of skeletic patterns and craniocervical analysis. Descriptive analysis was used to relate craniocervical analysis values with the type of skeletal pattern presented by individuals. For skeletal pattern Class I, II and III, nasopharyngeal soft tissue percentage and oropharyngeal soft tissue <b>linear</b> <b>quantity</b> were above normality value; nasopharyngeal cervical plot was normal and hyoid triangle was positive for the majority of the individuals. The angular relationship between skull and cervical spine show values lower than those considered normal for skeletal pattern class I and II. It was concluded that in the majority of the individuals with skeletal pattern Class I, II and III had high values of airway blockage, head posterior rotation, nasopharyngeal cervical plot with normal values and positive hyoid triangle...|$|R
40|$|Much {{attention}} has been placed into the design of digital filter banks with pairwise mirror-image symmetry in the frequency domain because such filter bank structure requires fewer multipliers and less time to design than most other structures. Furthermore, the designed subband filters have better attenuation in most cases. In this paper, the <b>polyphase</b> matrix of <b>linear</b> phase paraunitary filter banks with an odd number of channels and pairwise mirror-image symmetry frequency responses are completely characterized and parameterized by a lattice structure. The parameterization is shown to be complete and minimal. Design examples are presented...|$|R
40|$|We {{provide the}} first {{quantitative}} {{calculation of the}} dominant contribution to the bispectrum for general multiple-field inflation models that give large non-Gaussianity. Our bispectrum expression captures the nonlinear superhorizon influence of the isocurvature modes on the adiabatic mode during inflation. It involves only background <b>quantities</b> and <b>linear</b> perturbation <b>quantities</b> at horizon crossing. We also derive a simple analytic estimate and demonstrate {{that it is possible}} to get large non-Gaussianity even with the simplest quadratic two-field potential...|$|R
5000|$|Consider now the {{following}} form of v(r-R):Then by direct differentiation it follows thatDefine a monopole, dipole, and (traceless) quadrupole by, respectively,and we obtain finally {{the first few}} terms of the multipole expansion of the total potential, which {{is the sum of}} the Coulomb potentials of the separate charges:This expansion of the potential of a discrete charge distribution is very similar to the one in real solid harmonics given below. The main difference is that the present one is in terms of <b>linear</b> dependent <b>quantities,</b> for ...|$|R
40|$|The role of {{simulation}} {{has kept}} increasing for the sensitivity analysis and the uncertainty quantification of complex systems. Such numerical procedures are generally {{based on the}} processing of {{a huge amount of}} code evaluations. When the computational cost associated with one particular evaluation of the code is high, such direct approaches based on the computer code only can be not affordable. Surrogate models have therefore to be introduced to interpolate the information given by a fixed set of code evaluations to the whole input space. When confronted to deterministic mappings, the Gaussian process-based regression (GPR), or kriging, presents a good compromise between complexity, efficiency and error control. Such a method considers the quantity of interest of the system as a particular realization of a Gaussian stochastic process, which mean and covariance functions have to be identified from the available code evaluations. In this context, this work proposes an innovative parameterization of this mean function, which is based on the composition of two polynomials. This approach is particularly relevant for the approximation of strongly non <b>linear</b> <b>quantities</b> of interest from very little information. After presenting the theoretical basis of this method, this work compares its efficiency to alternative approaches on a series of examples...|$|R
40|$|Abstract—Much {{attention}} has been placed into the design of digital filter banks with pairwise mirror-image symmetry in the frequency domain be-cause such filter bank structure requires fewer multipliers and less time to design than most other structures. Furthermore, the designed subband fil-ters have better attenuation in most cases. In this paper, the <b>polyphase</b> ma-trix of <b>linear</b> phase paraunitary filter banks with an odd number of channels and pairwise mirror-image symmetry frequency responses are completely characterized and parameterized by a lattice structure. The parameteriza-tion is shown to be complete and minimal. Design examples are presented. Index Terms—Filter banks, frequency-domain pairwise mirror-image symmetry, lattice structure, linear phase, pararunitary, PMI, polynomial matrix...|$|R
40|$|We {{analyze the}} effects of {{competition}} with quantity discounts in a duopoly model with asymmetric firms. Consumers are privately informed about demand, so firms use quantity discounts as a price discrimination device. However, a dominant firm may also use quantity discounts to weaken or eliminate its competitor. We analyze {{the effects of}} quantity discounts on firms' profits and consumer surplus. Our main finding is that quantity discounts can decrease social welfare (i. e., the sum of producers' and consumers' surplus) for a small set of parameter values. Dominant firm; Exclusion; Non <b>linear</b> pricing; <b>Quantity</b> discounts...|$|R
40|$|Digital Signal Processing (DSP) {{has become}} one of the most {{powerful}} techniques in reshaping science and engineering in the areas of communication, medical imaging, radar, hi-fi music reproduction, oil prospecting etc. In this paper, Multirate DSP where the signal at a given sampling rate needs to be converted into another signal with a different sampling rate are investigated. Noble identities and <b>polyphase</b> decomposition of <b>linear</b> filters which are computationally more efficient approaches are illustrated. The results showed that the number of filter operations as well as the number of memory required were reduced by a factor of M and L (where M and L are decimation and interpolation factors respectively). The use of this technique promises cheaper DSP hardware that dissipates less heat...|$|R
40|$|We {{investigate}} non-Gaussianity {{in general}} multiple-field inflation using the formalism we developed in earlier papers. We use a perturbative {{expansion of the}} non-linear equations to calculate the three-point correlator of the curvature perturbation analytically. We derive a general expression that involves only a time integral over background and <b>linear</b> perturbation <b>quantities.</b> We work out this expression explicitly for the two-field slow-roll case, and find that non-Gaussianity can be orders of magnitude larger than in the single-field case. In particular, the bispectrum divided by the power spectrum squared can easily be of O(1 - 10), depending on the model. Our result shows the explicit momentum dependence of the bispectrum, which is significant...|$|R
40|$|Explicit {{formulas}} are proved for the 5 -torsion {{points on}} the Tate normal form E_ 5 of an elliptic curve having (X,Y) =(0, 0) {{as a point of}} order 5. These formulas express the coordinates of points in E_ 5 [5] - 〈(0, 0) 〉 as products of <b>linear</b> fractional <b>quantities</b> in terms of 5 -th roots of unity and a parameter u, where the parameter b which defines the curve E_ 5 is given as b=(ε^ 5 u^ 5 - ε^- 5) /(u^ 5 + 1) and ε = (- 1 +√(5)) / 2. Comment: 14 pages; the details of Watson's method {{have been added to the}} pape...|$|R
40|$|International audienceWe {{consider}} {{linear elasticity}} problems approximated by the Finite Element (FE) method with the resulting linear systems being solved by non-overlapping domain decomposition methods like FETI(DP) or BDD(C). This presentation {{deals with the}} computation of guaranteed {{upper and lower bounds}} of the error during the iterations of the solver. The bounds we consider are based on the error in constitutive equation and on the residual equation. These methods imply to recover certain displacement and stress fields with high regularity on the whole structure. We first show {{that it is possible to}} intercept, during the iterations of FETI(DP) or BDD(C), the information necessary for the parallel construction of such fields. Then by a little modification of the classical bounds, we manage to obtain new bounds which separate the algebraic error (due to the use of a DD iterative solver) from the discretization error (due to the FE). These bounds provides an unbiased criterion to stop the iterations when the solver error is lesser than the discretization error. They can also be used for the estimation of the error on <b>linear</b> <b>quantities</b> of interest. Assessments on 2 D static linear mechanic problems illustrate the relevance of the separation of sources of error and the independence of the bounds with respect to the substructuring. Finally, we steer the iterative solver by an objective of precision on a quantity of interest. The strategy consists in a sequence of solving, adaptive local remeshing and recycling of search directions, in order to reach the desired quality for the quantity of interest with minimal computations...|$|R
2500|$|The {{symmetric}} Nash equilibrium is at [...] (See Holt (2005, Chapter 13) for asymmetric examples.) [...] Making suitable assumptions for {{the partial}} derivatives (for example, assuming each firm's cost is a <b>linear</b> function of <b>quantity</b> and thus using {{the slope of}} that function in the calculation), the equilibrium quantities can be substituted in the assumed industry price structure [...] to obtain the equilibrium market price.|$|R
40|$|Based on a {{recently}} developed quantum dissipation formulation [R. X. Xu and Y. J. Yan, J. Chem. Phys. 116, 9196 (2002) ], {{we present a}} reduced Liouville-space approach to evaluate the response and correlation functions of dissipative systems. The weak system-bath interaction is treated properly for its effects on the initial state, the evolution, and the correlation between coherent driving and non-Markovian dissipation. Numerical demonstration shows this correlated effect cannot be neglected even in the calculation of <b>linear</b> response <b>quantities</b> that do not explicitly depend on external fields. Highlighted in this paper is also the proper choice of theory among various formulations in the weak system-bath interaction regime. (C) 2005 American Institute of Physics...|$|R
40|$|We {{study the}} social welfare optimum for a {{vertically}} differentiated duopoly under partial market coverage and variable costs. Costs are specified as <b>linear</b> in <b>quantity</b> and quadratic in quality. We {{demonstrate that the}} spread of product quality observed under a profit maximizing duopoly is too high, relative to the social welfare optimum. We also find that high quality firms in a duopoly will have higher profits but lower market shares than low quality firms, after quality and price competition is resolved. Finally, we demonstrate that the social welfare optimum results in greater total output than the duopoly and has both firms producing equal amounts. Keywords: Product Differentiation, Partial Market Coverage, Social Welfare JEL Classification: L 13, D 60...|$|R
40|$|The {{purpose of}} this paper is to study the {{structure}} of large sparse graphs such as the graph of the web - the graph consisting of one node per document in the web and a directed edge from i to j whenever document i has a hypertext link to document j. This particular graph is of course of interest as being by far the largest 'human-made' graph (with millions of vertices) arising from 'natural setting'. We develop theoretical tools and algorithms to analyze the density structure of such graphs by relating density to easy to find <b>Linear</b> Algebra <b>quantities.</b> (orig.) SIGLEAvailable from TIB Hannover: RN 4052 (99886) / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekDEGerman...|$|R
40|$|We {{report a}} reanalysis of a near-pristine {{absorption}} system, located at a redshift z_abs= 2. 52564 towards the quasar Q 1243 + 307, {{based on the}} combination of archival and new data obtained with the HIRES echelle spectrograph on the Keck telescope. This absorption system, which has an oxygen abundance [O/H]=- 2. 769 +/- 0. 028 (~ 1 / 600 of the Solar abundance), is among the lowest metallicity systems currently known where a precise measurement of the deuterium abundance is afforded. Our detailed analysis of this system concludes, {{on the basis of}} eight D I absorption lines, that the deuterium abundance of this gas cloud is log_ 10 (D/H) = - 4. 622 +/- 0. 015, which is in very good agreement with the results previously reported by Kirkman et al. (2003), but with an improvement on the precision of this single measurement by a factor of ~ 3. 5. Combining this new estimate with our previous sample of six high precision and homogeneously analyzed D/H measurements, we deduce that the primordial deuterium abundance is log_ 10 (D/H) _P = - 4. 5974 +/- 0. 0052 or, expressed as a <b>linear</b> <b>quantity,</b> (D/H) _P = (2. 527 +/- 0. 030) x 10 ^- 5; this value corresponds to a one percent determination of the primordial deuterium abundance. Combining our result with a BBN calculation that uses the latest nuclear physics input, we find that the baryon density derived from BBN agrees to within 2 sigma of the latest results from the Planck CMB data. Comment: 18 pages, 10 figures, Submitted to The Astrophysical Journa...|$|R
40|$|Abstract — The linear, binary, block codes with no equally likely probabilities for {{the binary}} symbols are analyzed. The {{encoding}} graph for systematic linear block codes is proposed. These codes {{are seen as}} sources with memory and the information quantities H(S,X), H(S), H(X), H(X|S), H(S|X), I(S,X) are derived. On the base of these quantities, the code performances are analyzed. Index Terms — information <b>quantities,</b> <b>linear,</b> block codes, sources with memory. I...|$|R
40|$|A uniform {{description}} is given of {{a method of}} measurement using a Michelson interferometer for measuring the <b>linear</b> motion <b>quantities</b> acceleration, velocity and displacement, and a diffraction grating interferometer for measuring the circular motion quantities angular acceleration, angular velocity and rotation angle. The paper focusses on {{an analysis of the}} dynamic behaviour of an interferometric measurement system based on the counting technique with regard to the measurement errors due to deterministic and stochastic disturbing quantities. The error analysis and description presented are aimed at giving some rules, mathematical expressions and graphical presentations that have proved to be helpful in recognizing the errors in interferometric measurements of motion quantities, optimizing the measurement conditions (e. g., filter settings), obtaining corrections and estimating the uncertainty of measurement...|$|R
40|$|Abstract — The paper {{presents}} {{the design and}} performance evaluation of a novel navigation solution that merges low-rate delayed GPS measurements with high-rate linear acceleration, attitude, and angular velocity measurements to estimate, in three dimensions, <b>linear</b> motion <b>quantities</b> (position, <b>linear</b> velocity, an acceleration of gravity) of unmanned aerial vehicles (UAVs). The design {{is based on the}} continuous-discrete Kalman filter solution for an equivalent LTI realization and allows for the natural use of frequency weights to explicitly achieve ade-quate disturbance rejection and measurement noise attenuation on the state estimates. The proposed solution is optimal with respect to all quantities assuming exact angular measurements and, in the presence of noisy angular quantities, it outperforms classic solutions developed in inertial coordinates. Simulation results illustrate the achievable performance in the presence of realistic measurements, including noise and delays. I...|$|R
