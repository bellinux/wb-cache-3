118|598|Public
5000|$|Any rule {{may have}} a <b>probability</b> <b>interval</b> (called a support pair) {{associated}} with it by appending [...] to it, where min and max are the minimum and maximum probabilities. Fril includes predicates that calculate the support for a given query.|$|E
5000|$|From {{the mass}} assignments, {{the upper and}} lower bounds of a <b>probability</b> <b>{{interval}}</b> can be defined. This interval contains the precise probability of a set of interest (in the classical sense), and is bounded by two non-additive continuous measures called belief (or support) and plausibility: ...|$|E
5000|$|When [...] and , the Jeffreys {{interval}} {{is taken}} to be the [...] equal-tailed posterior <b>probability</b> <b>interval,</b> i.e., the [...] and [...] quantiles of a Beta distribution with parameters [...] These quantiles need to be computed numerically, although this is reasonably simple with modern statistical software.|$|E
40|$|Handling {{uncertainty}} by <b>interval</b> <b>probabilities</b> is recently receiving {{considerable attention}} by researchers. <b>Interval</b> <b>probabilities</b> are used {{when it is}} difficult to characterize the uncertainty by point-valued probabilities due to partially known information. Most of researches related to <b>interval</b> <b>probabilities,</b> such as combination, marginalization, condition, Bayesian inferences and decision, assume that <b>interval</b> <b>probabilities</b> are known. How to elicit <b>interval</b> <b>probabilities</b> from subjective judgment is a basic and important problem for the applications of <b>interval</b> <b>probability</b> theory and till now a computational challenge. In this work, the models for estimating and combining <b>interval</b> <b>probabilities</b> are proposed as linear and quadratic programming problems, which can be easily solved. The concepts including <b>interval</b> <b>probabilities,</b> <b>interval</b> entropy, interval expectation, interval variance, interval moment, and the decision criteria with <b>interval</b> <b>probabilities</b> are addressed. A numerical example of newsvendor problem is employed to illustrate our approach. The analysis results show that the proposed methods provide a novel and effective alternative for decision making when point-valued subjective probabilities are inapplicable due to partially known information. Uncertainty modeling Decision making <b>Interval</b> <b>probabilities</b> Linear programming Quadratic programming Newsvendor problem...|$|R
40|$|<b>Probability</b> <b>intervals</b> are imprecise <b>probability</b> {{assignments}} over elementary events. They {{constitute a}} very convenient tool to model uncertain information: two common cases are confidence intervals on parameters of multinomial distributions built from sample data and expert opinions provided {{in terms of}} such intervals. In this paper, we study how <b>probability</b> <b>intervals</b> can be transformed into other uncertainty models such as possibility distributions, Ferson’s p-boxes, random sets and Neumaier’s clouds...|$|R
40|$|AbstractAn {{essential}} component in Machine Learning processes is to estimate any uncertainty measure reflecting {{the strength of}} the relationships between variables in a dataset. In this paper we focus on those particular situations where the dataset has incomplete entries, as most real-life datasets have. We present a new approach to tackle this problem. The basic idea is to initially estimate a set of <b>probability</b> <b>intervals</b> that will be used to complete the missing values. Then, these values are used to obtain new bounds of the expected number of entries in the dataset. The <b>probability</b> <b>intervals</b> are narrowed iteratively until convergence. We have shown that the same processes can be used to estimate both, <b>probability</b> <b>intervals</b> and <b>probability</b> distributions, and give conditions that guarantee that the estimator is the correct one...|$|R
50|$|The last {{stage of}} the process is to {{determine}} the <b>probability</b> <b>interval</b> for the expected control mode, the opportunistic control mode. Referring to table 1, for this example, the general action failure probability is within the range of 1.0 E-2 < p < 0.5 E-0. As this is not regarded as an acceptable there is no clear and justified reason to continue with the analysis being undertaken.|$|E
5000|$|A {{tolerance}} interval {{can be seen}} as a statistical version of a <b>probability</b> <b>interval.</b> [...] "In the parameters-known case, a 95% {{tolerance interval}} and a 95% prediction interval are the same." [...] If we knew a population's exact parameters, we would be able to compute a range within which a certain proportion of the population falls. For example, if we know a population is normally distributed with mean [...] and standard deviation , then the interval [...] includes 95% of the population (1.96 is the z-score for 95% coverage of a normally distributed population).|$|E
50|$|The {{tolerance}} interval differs from a confidence interval {{in that the}} confidence interval bounds a single-valued population parameter (the mean or the variance, for example) with some confidence, while the {{tolerance interval}} bounds the range of data values that includes a specific proportion of the population. Whereas a confidence interval's size is entirely due to sampling error, and will approach a zero-width interval at the true population parameter as sample size increases, a tolerance interval's size is due partly to sampling error and partly to actual variance in the population, and will approach the population's <b>probability</b> <b>interval</b> as sample size increases.|$|E
40|$|The adversaries of {{probability}} theory {{for dealing with}} uncertainty in AI systems often {{argue that it is}} not expressive enough to distinguish between uncertainty and ignorance due to incompleteness of information. <b>Probability</b> <b>intervals,</b> however, have proven to be suitable for expressing incompleteness of information. In this paper, we present a new method for computing such intervals from a partial specification of a joint probability distribution. We will show that our method allows for the successive updating of <b>probability</b> <b>intervals</b> as evidence becomes available...|$|R
40|$|When {{we use a}} {{mathematical}} model to represent information, we can obtain a closed and convex set of probability distributions, also called a credal set. This type of representation involves two types of uncertainty called conflict (or randomness) and non-specificity, respectively. The imprecise Dirichlet model (IDM) allows us to carry out inference about the probability distribution of a categorical variable obtaining a set of a special type of credal set (<b>probability</b> <b>intervals).</b> In this paper, we shall present tools for obtaining the uncertainty functions on <b>probability</b> <b>intervals</b> obtained with the IDM, which can enable these functions in any application of this model to be calculated...|$|R
40|$|Many AI {{researchers}} {{argue that}} probability theory is only capable {{of dealing with}} uncertainty in situations where a full specification of a joint probability distribution is available, and conclude {{that it is not}} suitable for application in knowledge-based systems. <b>Probability</b> <b>intervals,</b> however, constitute a means for expressing incompleteness of information. We present a method for computing such <b>probability</b> <b>intervals</b> for <b>probabilities</b> of interest from a partial specification of a joint probability distribution. Our method improves on earlier approaches by allowing for independency relationships between statistical variables to be exploited. Comment: Appears in Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence (UAI 1990...|$|R
50|$|A fishery for Antarctic toothfish, {{managed by}} the Convention for the Conservation of Antarctic Marine Living Resources (CCAMLR), has existed since 1997. The {{existence}} of this fishery in the Ross Sea, the area where most Antarctic toothfish are caught, is very contentious - the main argument proposed for this being the lack of accurate population parameters, such as original stock size, fecundity, and recruitment. Moreover, the main fishing grounds is presumed by some researchers to cover the area through which the entire stock of Antarctic toothfish pass. Typically the fishing season has finished in the area {{by the end of}} February and {{for the remainder of the}} year much of the area is covered by sea-ice providing a natural impediment to fishing. This fishery is characterised by opponents as being a challenge to manage owing to the nature of benthic longline fishing. The bycatch of other fish can also be significant, with the ratio to tonnes of toothfish caught ranging from 4.5% to 17.9% and averaging 9.3% from the 1999/2000 fishing season to 2013/14 in CCAMLR Subarea 88.1 when the toothfish catch first exceeded 50 tonnes and from 2.3% to 24.5% averaging 12.4% in CCAMLR Subarea 88.2 up to the latest publicly available figure from 2013/14. The bycatch of other fish species is also regulated to a maximum amount annually by CCAMLR. CCAMLR decision rules are based on determining the catch level that will ensure that the median estimated spawning stock biomass (not total biomass) is greater than or equal to 50% of the average pre-exploitation spawning biomass after a further 35 years of fishing (i.e. 35 years from each year of assessment), with the additional condition that there is less than a 10% probability that the spawning biomass will decline below 20% of the pre-exploitation level at any time during this period. Current spawning stock biomass for Antarctic toothfish in the Ross Sea Region is estimated to be at 75% of the pre-exploitation level (95% Bayesian <b>probability</b> <b>interval</b> 71-78%), well above the 50% target reference point.|$|E
3000|$|... to the <b>probability</b> <b>interval</b> in each {{iterative}} coding {{step and}} controls {{the locations of}} the forbidden symbol by a PRNG with a seed, [...]...|$|E
30|$|For the {{computation}} {{of uncertainty}} intervals for our model predictions we propose to follow an approach, {{based in the}} method described in Section  2, consisting of three stages: clustering, parameter estimation via maximum likelihood, and <b>probability</b> <b>interval</b> computation.|$|E
40|$|In today's {{uncertain}} world, imprecision in probabilistic {{information is}} often specified by <b>probability</b> <b>intervals.</b> We present here a new database {{framework for the}} efficient storage and manipulation of <b>interval</b> <b>probability</b> distribution functions and their associated contextual information. While work on <b>interval</b> <b>probabilities</b> and on probabilistic databases, has appeared before, ours {{is the first to}} combine these into a coherent and mathematically sound framework including both standard relational queries and queries based on probability theory. In particular, our query algebra allows the user not only to query existing <b>interval</b> <b>probability</b> distributions, but also to construct new ones by means of conditionalization and marginalization, as well as other more common database operations...|$|R
40|$|The {{likelihood}} {{approach to}} statistics {{can be interpreted}} as a theory of fuzzy probability. This paper presents a generalization of credal networks obtained by generalizing imprecise probabilities to fuzzy probabilities; that is, by additionally considering the relative plausibility of different values in the <b>probability</b> <b>intervals...</b>|$|R
40|$|Probabilistic {{inference}} forms {{lead from}} point probabilities of the premises to <b>interval</b> <b>probabilities</b> of the conclusion. The probabilistic version of Modus Ponens, for example, licenses the inference from P(A) =α and P(B|A) =β to P(B) ∈[αβ,αβ+ 1 −α]. We study generalized inference forms {{with three or}} more premises. The generalized Modus Ponens, for example, leads from P(A 1) =α 1,…,P(An) =αn and P(B|A 1 ∧⋯∧An) =β to an according interval for P(B). We present the <b>probability</b> <b>intervals</b> for the conclusions of the generalized versions of Cut, Cautious Monotonicity, Modus Tollens, Bayes’ Theorem, and some SYSTEM O rules. Recently, Gilio has shown that generalized inference forms “degrade”—more premises lead to less precise conclusions, i. e., to wider <b>probability</b> <b>intervals</b> of the conclusion. We also study Adam’s probability preservation properties in generalized inference forms. Special attention is devoted to zero probabilities of the conditioning events. These zero probabilities often lead to different intervals in the coherence and the Kolmogorov approach...|$|R
40|$|Implementation of a Bayesian {{analysis}} of a selection experiment is illustrated using litter size [total number of piglets born (TNB) ] in Danish Yorkshire pigs. Other traits studied include average litter weight at birth (WTAB) and proportion of piglets born dead (PRBD). Response to selection for TNB was analyzed {{with a number of}} models, which differed in their level of hierarchy, in their prior distributions, and in the parametric form of the likelihoods. A model assessment study favored a particular form of an additive genetic model. With this model, the Monte Carlo estimate of the 95 % <b>probability</b> <b>interval</b> of response to selection was (0. 23; 0. 60), with a posterior mean of 0. 43 piglets. WTAB showed a correlated response of - 7. 2 g, with a 95 % <b>probability</b> <b>interval</b> equal to (- 33. 1; 18. 9). The posterior mean of the genetic correlation between TNB and WTAB was - 0. 23 with a 95 % <b>probability</b> <b>interval</b> equal to (- 0. 46; - 0. 01). PRBD was studied informally; it increases with larger litters, when litter size is > 7 piglets born. A number of methodological issues related to the Bayesian model assessment study are discussed, as well as the genetic consequences of inferring response to selection using additive genetic models...|$|E
40|$|This chapter proposes an {{approach}} to the combination of forecasts from a new perspective and uses a new estimation methodology. Concepts from optimization and statistics are combined in order to cast the problem as one of estimating the minimizer of a loss function, which {{is assumed to be}} not explicitly known. Using weak assumptions on the unknown loss function, and on the performance distribution of the experts, a density for the expert estimates may be derived. Sufficient conditions are then obtained for reducing the problem to that of statistical mode estimation. By regarding individual forecasts as expert estimates of the minimizer of the decision maker's loss function, the mode of these forecasts becomes a combination forecast. Using the Bayesi{{an approach}} and the Markov chain Monte Carlo method, an empirical distribution corresponding to the predictive density of the expert estimates can be constructed. Hence the mode can be estimated and a <b>probability</b> <b>interval</b> for the forecast value can also be obtained. The <b>probability</b> <b>interval</b> associated with a combination forecast has not previously been studied in the literature. Thus, the construction of the <b>probability</b> <b>interval</b> is a new contribution. Simulation studies as well as an empirical example are presented to illustrate our method. Department of Applied Mathematic...|$|E
30|$|What is the <b>probability</b> <b>interval</b> (minimum and maximum) of {{a failure}} to occur in a variant? The answer to this {{question}} can guide the architect in choosing and discarding variants based on monitoring data. Besides, this information may also interfere with the choice of the adjudicator, as discussed in Section 5.|$|E
40|$|It {{is pointed}} out that the Chebychev {{confidence}} intervals and maximum p-values advocated by Davis and Espinoza for sensitivity analysis of equilibrium displacement models are unnecessary. Desired <b>probability</b> <b>intervals</b> and <b>probabilities</b> can be accurately estimated without resorting to gross approximations. simulation, probability distributions, empirical quantiles, Research Methods/ Statistical Methods,...|$|R
40|$|There exist many simple {{tools for}} jointly {{capturing}} variability and incomplete information {{by means of}} uncertainty representations. Among them are random sets, possibility distributions, <b>probability</b> <b>intervals,</b> and the more recent Ferson's p-boxes and Neumaier's clouds, both defined by pairs of possibility distributions. In the companion paper, we have extensively studied a generalized form of p-box and situated it with respect to other models. This paper focuses on the links between clouds and other representations. Generalized p-boxes are shown to be clouds with comonotonic distributions. In general, clouds cannot always be represented by random sets, in fact not even by 2 -monotone (convex) capacities. Comment: 30 pages, 7 figures, Pre-print of journal paper {{to be published in}} International Journal of Approximate Reasoning (with expanded section concerning clouds and <b>probability</b> <b>intervals...</b>|$|R
40|$|The method {{proposed}} by Bernardo and Smith [2000] to approximate reference priors by simulation was analyzed {{with the objective}} of improving the procedure in order to obtain consistent estimators and to allow the estimation of asymptotic <b>probability</b> <b>intervals.</b> In this sense, the variance of Bernardo's estimator was derived and was used to construct <b>probability</b> <b>intervals</b> that permitted the expression of the estimation error as a function of sample size. Additionally a variance reduction technique (common random numbers) were explored as a means to obtain more precise estimations with smaller sample sizes. These technique was found to considerably reduce estimation error for some of the examples explored. In other cases the use of the technique resulted in zero estimation error given that the estimator does not depend on the sample. Comment: in Spanis...|$|R
40|$|We {{present a}} novel {{approach}} to entropy coding, which provides the coding efficiency and simple probability modeling capability of arithmetic coding at the complexity level of Huffman coding. The key element of the proposed approach is a partitioning of the unit interval into a small set of probability intervals. An input sequence of discrete source symbols is mapped to a sequence of binary symbols {{and each of the}} binary symbols is assigned to one of the probability intervals. The binary symbols that are assigned to a particular <b>probability</b> <b>interval</b> are coded at a fixed probability using a simple code that maps a variable number of binary symbols to variable length codewords. The probability modeling is decoupled from the actual binary entropy coding. The coding efficiency of the <b>probability</b> <b>interval</b> partitioning entropy (PIPE) coding is comparable to that of arithmetic coding...|$|E
40|$|International audienceIn {{previous}} works, it was {{proved that}} General Single-layer Sequential Weightless Neural Networks (GSSWNNs) are equivalent to probabilistic automata. The class of GSSWNNs {{is an important}} representative {{of the research on}} temporal pattern processing in Weightless Neural Networks or RAM-based neural networks. Some of the proofs provide an algorithm to map any probabilistic automaton into a GSSWNN. They not only allows the construction of any probabilistic automaton, but also increases the class of functions that can be computed by the GSSWNNs. For instance, these networks are not restricted to finite-state languages and can now deal with some context-free languages. In this paper, based on such algorithms, we employ the <b>probability</b> <b>interval</b> method and Java to develop a tool to transform any PA into a GSSWNNs (including the probabilistic recognition algorithm). The <b>probability</b> <b>interval</b> method minimizes the round-off errors that occur while computing the probabilities...|$|E
40|$|We used {{photographic}} mark-recapture {{methods to}} estimate the number of mammal-eating ‘‘transient’’ killer whales using the coastal waters from the central Gulf of Alaska to the central Aleutian Islands, around breeding rookeries of endangered Steller sea lions. We identified 154 individual killer whales from 6, 489 photographs collected between July 2001 and August 2003. A Bayesian mixture model estimated seven distinct clusters (95 % <b>probability</b> <b>interval</b> = 7 – 10) of individuals that were differentially covered by 14 boat-based surveys exhibiting varying degrees of association in space and time. Markov Chain Monte Carlo methods were used to sample identification probabilities across the distribution of clusters to estimate a total of 345 identified and undetected whales (95 % <b>probability</b> <b>interval</b> = 255 – 487). Estimates of covariance between surveys, {{in terms of their}} coverage of these clusters, indicated spatial population structure and seasonal movements from these near-shore waters, suggesting spatial and temporal variation in the predation pressure on coastal marine mammals...|$|E
40|$|Representation and {{modeling}} of economic uncertainty is addressed by different modeling methods, namely stochastic variables and <b>probabilities,</b> <b>interval</b> analysis, and fuzzy numbers, in particular triple estimates. Fo-cusing on {{discounted cash flow}} analysis numerical results are presented, comparisons are made between alter-native modeling methods, and characteristics of the methods are discussed...|$|R
40|$|AbstractTreatment of imprecise probabilities {{within the}} {{probabilistic}} satisfiability approach to uncertainty in knowledge-based systems is surveyed and discussed. Both <b>probability</b> <b>intervals</b> and qualitative <b>probabilities</b> are considered. Analytical and numerical methods to test coherence and bound {{the probability of}} a conclusion are reviewed. They use polyhedral combinatorics and advanced methods of linear programming...|$|R
3000|$|LC, {{which is}} able to save {{enormous}} computational power in hardware equipments of SA. Secondly, the three weighting coefficients are all related to the <b>probability</b> <b>intervals</b> of the decoding overhead γ. If the three weighting coefficients are set properly {{in accordance with the}} random distribution of lifespan L and the Markov transition probabilities λ [...]...|$|R
40|$|Propagating {{possibilistic}} and probabilistic variables {{through a}} mapping yields a fuzzy random variable. We propose {{a method to}} attach probability intervals to events pertaining to the output variable. We show that this method is consistent with classical approaches to fuzzy random variables and that the obtained <b>probability</b> <b>interval</b> is the mean value of the fuzzy probability defined by viewing a fuzzy random variable as higher order possibilistic uncertainty...|$|E
40|$|This paper {{provides}} behavioral {{foundations for}} parametric weighting functions under rankdependent utility. This {{is achieved by}} decomposing the independence axiom of expected utility into separate meaningful properties. These conditions allow us to characterize rank-dependent utility with power and exponential weighting functions. Moreover, by restricting the conditions to subsets of the <b>probability</b> <b>interval,</b> foundations of rank-dependent utility with parametric inverse-S shaped weighting functions are obtained. [...] Comonotonic independence,probability weighting function,preference foundation,rank-dependent utility...|$|E
40|$|This paper {{provides}} preference {{foundations for}} parametric weighting functions under rank-dependent utility. This {{is achieved by}} decomposing the independence axiom of expected utility into separate meaningful properties. These conditions allow us to characterize rank-dependent utility with power and exponential weighting functions. Moreover, by allowing probabilistic risk attitudes to vary within the <b>probability</b> <b>interval,</b> a preference foundation for rank-dependent utility with parametric inverse-S shaped weighting function is obtained. Comonotonic independence Probability weighting function Preference foundation Rank-dependent utility...|$|E
40|$|The groundswell for the ` 00 s is imprecise probabilities. Whether {{the numbers}} {{represent}} the probable {{location of a}} GPS device at its next sounding, the inherent uncertainty of an individual expert's probability prediction, or the range of values derived from the fusion of sensor data, <b>probability</b> <b>intervals</b> became an important way of representing uncertainty...|$|R
30|$|Sharpness {{is another}} {{important}} performance indicator for evaluating the forecasted PDF. Obviously, the sharper the forecasted distribution is, the better the probabilistic forecast approach will be, since a sharper distribution means less volatility of the forecast result. In this paper, the sharpness of the forecasted PDF is assessed by {{the coverage of the}} central <b>probability</b> <b>intervals.</b>|$|R
40|$|Following {{the concept}} of {{intervals}} of measures, some general results are derived for the maximum imprecision related to imprecise densities. A model is proposed for Bayesian inference with imprecise prior densities, where members of a conjugate family can be chosen to enable updating in a simple way. Key Words: Bayesian inference, imprecise <b>probabilities,</b> <b>intervals</b> of measures, conjugate prior densities...|$|R
