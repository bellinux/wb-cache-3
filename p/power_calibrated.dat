4|62|Public
30|$|With the {{sensitivity}} ΔS of 9.8 [*]±[*] 0.5  μV/K, the corresponding temperature difference ΔT of each heating power {{is determined by}} ΔT[*]=[*]ΔV/ΔS. The results are plotted in Fig.  3 c. We also obtain the curve of ΔT versus heating power for the single-layer Pd sensors from Fig.  2 b in the same way. Surprisingly, the curve of ΔT versus heating <b>power</b> <b>calibrated</b> with 4 -cm-long Cr/Pd thermocouples matches well {{with the results of}} the dual-beam Pd sensors of 800 – 80 and 500 – 80  nm, as shown in Fig.  3 c. This suggests that our dual-beam Pd sensors have the same reliability as the conventional Cr/Pd thin film thermocouples.|$|E
40|$|We {{report a}} new {{measurement}} of A{sub b} using {{data obtained by}} SLD in 1997 - 98. This measurement uses a vertex tag technique, where {{the selection of a}} b hemisphere is based on the reconstructed mass of the bottom hadron decay vertex. The method uses the 3 D vertexing capabilities of SLD's CCD vertex detector and the small and stable SLC beams to obtain a high b-event tagging efficiency and purity of 78 % and 97 %, respectively. Charged kaons identified by the CRID detector provide an efficient quark-antiquark tag, with the analyzing <b>power</b> <b>calibrated</b> from the data. We obtain a preliminary result of A{sub b} = 0. 997 {+-} 0. 044 {+-} 0. 067...|$|E
40|$|We {{report a}} new {{preliminary}} measurement of A b using 350 k hadronic Z decays obtained by SLD in 1997 - 98. This measurement uses a vertex tag technique, where {{the selection of}} a b hemisphere is based on the reconstructed mass of the bottom hadron decay vertex. The method uses the 3 D vertexing capabilities of SLD's CCD vertex detector and the small and stable SLC beams to obtain a high b-event tagging eÆciency and purity of 54 % and 96 %, respectively. The charge of the reconstructed vertex provides a quark-antiquark tag, with the analyzing <b>power</b> <b>calibrated</b> from the data. Tracks reconstructed in the vertex detector alone are used to enhance the charge purity of the vertices. The preliminary result is A...|$|E
40|$|Abstract—Monte Carlo {{simulation}} {{is widely}} used for predicting ion implantation profiles in amorphous targets. Here, we compared Monte Carlo simulation results with a vast database of ion implantation secondary ion mass spectrometry (SIMS), and showed that the Monte Carlo data sometimes deviated from the experimental data. We modified the electron stopping <b>power</b> model, <b>calibrated</b> its parameters, and reproduced most of the database. We also demon-strated that Monte Carlo simulation can accurately predict profiles in a low energy range of around 1 keV once it is calibrated in the higher energy region. Index Terms—Ion implantation, database, amorphous, monte carlo, electron stopping powe...|$|R
50|$|A typical optical {{power meter}} {{consists}} of a calibrated sensor, measuring amplifier and display.The sensor primarily {{consists of a}} photodiode selected for the appropriate range of wavelengths and power levels.On the display unit, the measured optical power and set wavelength is displayed. <b>Power</b> meters are <b>calibrated</b> using a traceable calibration standard such as a NIST standard.|$|R
30|$|I–V {{characteristics}} are measured using a Keithley 238 source meter. The samples {{are placed on}} a copper base from backside; a metallic sharp tip (D = 0.5 mm) is used as top contact to the NWs/PMMA array. The energy conversion efficiency is determined by illuminating the structures using a halogen arc-lamp with the <b>calibrated</b> <b>power</b> density of P = 100 mW/cm 2.|$|R
40|$|Near-threshold {{operation}} {{can increase}} the number of simultaneously active cores, at the expense of much lower operating frequency (“dim silicon”), but dim cores suffer from diminishing returns as the number of cores increases. At this point, hardware accelerators become more efficient alternatives. To explore such a broad design space, we present an analytical model to quantify the performance limits of many-core, heterogeneous systems operating at near-threshold voltage. The model augments Amdahl’s Law with detailed scaling of frequency and <b>power,</b> <b>calibrated</b> by circuit-level simulations using a modified Predictive Technology Model (PTM), and factors in effects of process variations. While our results show that dim cores do indeed boost throughput, even in the presence of process variations, significant benefits are only achieved in applications with very high parallelism or with novel architectures to mitigate variation. Reconfigurable logic that supports a variety of accelerators is more beneficial than “dim cores ” or dedicated, fixed-logic accelerators, unless 1) the kernel targeted by fixed logic has overwhelming coverages across applications, or 2) the speedup of the dedicated accelerator over the reconfigurable equivalent is significant...|$|E
40|$|Abstract: The paper mainly {{discusses}} how {{to calculate}} composition error of low-voltage metering device. The composition error of metering device, the error of power meter and the ratio error of CT were calculated also by comparing the standard power meter with the <b>calibrated</b> <b>power</b> meter. The formula of error calculation was put forward, that will has an important guiding significance {{for the development of}} on-site calibrating instrumen...|$|R
40|$|We {{present a}} formal {{comparison}} {{of the performance of}} algorithms used for synthesis imaging with optical/infrared long-baseline interferometers. Five different algorithms are evaluated based on their performance with simulated test data. Each set of test data is formatted in the OI-FITS format. The data are <b>calibrated</b> <b>power</b> spectra and bispectra measured with an array intended to be typical of existing imaging interferometers. The strengths and limitations of each algorithm are discussed...|$|R
40|$|This {{thesis is}} {{composed}} by two articles. In the first paper, co-authored with Roberto Pancrazi, we study {{the relationship between}} sovereign debt and political frictions. We model political frictions as a disagreement among parties about distribution of resources. When analyzing a small-open economy framework we find two important results. First, when considering standard utility function (CRRA with risk aversion parameter greater or equal to one) political frictions induce saving (not borrowing) incentives. Second, when introducing retrospective voting, for which electoral outcomes are affected by recent economic performance, we find that more severe political frictions indeed lead to stronger borrowing incentives. Then, we use the theoretical predictions of our model to structurally estimate the country-specific degree of retrospective voting using data on debt, quality of institutions, and election probability in 56 emerging and transition economies. We find that retrospective voting is strongly related to corruption indices. In the second paper I study the effect of political frictions in a model where repayment of sovereign debt is not enforceable. Sovereign default models that study how income fluctuations {{and the level of}} debt affect default risk when sovereign debt is non contingent, are successful in explaining business cycle in emerging economies by matching the stylized facts of main economic aggregates in normal and default periods but they fail in reproducing jointly the large levels of debt and spread observed in the data. I introduce political uncertainty in the standard default model of Arellano (2008) : the incumbent has an exogenous probability of not being reappointed in the next period, but in the case she decides to default, there is a larger probability of losing <b>power.</b> <b>Calibrating</b> political uncertainty on Argentinian polls data, the model generates realistic levels of debt to gdp and spread without affecting the performance on the other business cycle statistics. This thesis is composed by two articles. In the first paper, co-authored with Roberto Pancrazi, we study the relationship between sovereign debt and political frictions. We model political frictions as a disagreement among parties about distribution of resources. When analyzing a small-open economy framework we find two important results. First, when considering standard utility function (CRRA with risk aversion parameter greater or equal to one) political frictions induce saving (not borrowing) incentives. Second, when introducing retrospective voting, for which electoral outcomes are affected by recent economic performance, we find that more severe political frictions indeed lead to stronger borrowing incentives. Then, we use the theoretical predictions of our model to structurally estimate the country-specific degree of retrospective voting using data on debt, quality of institutions, and election probability in 56 emerging and transition economies. We find that retrospective voting is strongly related to corruption indices. In the second paper I study the effect of political frictions in a model where repayment of sovereign debt is not enforceable. Sovereign default models that study how income fluctuations and the level of debt affect default risk when sovereign debt is non contingent, are successful in explaining business cycle in emerging economies by matching the stylized facts of main economic aggregates in normal and default periods but they fail in reproducing jointly the large levels of debt and spread observed in the data. I introduce political uncertainty in the standard default model of Arellano (2008) : the incumbent has an exogenous probability of not being reappointed in the next period, but in the case she decides to default, there is a larger probability of losing <b>power.</b> <b>Calibrating</b> political uncertainty on Argentinian polls data, the model generates realistic levels of debt to gdp and spread without affecting the performance on the other business cycle statistics. LUISS PhD Thesi...|$|R
40|$|Abstract—As {{research}} on improving energy efficiency becomes preva-lent, {{the necessity of}} a tool to accurately estimate power is increasing. Among various tools proposed, McPAT has gained some popularity due to its easy-to-use analytical power models. However, McPAT’s prediction has several limitations. Although under- or over-estimated power from unmodeled and mis-modeled parts offset each other, it still incorporates errors in each block. Moreover, the lack of awareness to the implementation details exacerbates the prediction inaccuracies. To alleviate this problem, we propose a new methodology to train McPAT towards precise processor power prediction using power measurements from real hardware. This calibration enables McPAT’s power to fit to the target processor power. Once we adjusted the power consumption of each block to best match those in the target processor, our trained McPAT delivered more precise <b>power</b> estimation. We <b>calibrated</b> the outputs of McPAT against a Cortex-A 15 within a Samsung Exynos 5422 SoC. We observe that our methodology successfully reduces the errors, particularly for workloads with fluctuating power behaviors. The results show that the mean percentage error and the mean percentage absolute error of the <b>calibrated</b> <b>power</b> against real hardware are 2. 04 percent and 4. 37 percent, respectively. I...|$|R
30|$|Considering the {{responses}} of array antenna and RF cable do not vary so much over time and space, total responses including those of receivers are measured in a radio anechoic chamber. Strictly speaking, the radiation pattern of each antenna element, mutual coupling and deviation among RF cables should be properly calibrated. As far as this system employs a linear antenna array, the deviation of the radiation pattern of each antenna element cannot be eliminated by calibration processing since there exists cone ambiguity in the beamforming. Therefore we measured the calibration data only on the plane determined by the directional vector toward the maximum radiation pattern and the array axis. Moreover, the receiver is greatly affected by temperature, thus on-site calibration is necessary. For receiver calibration, we used the one-to-eight <b>power</b> divider <b>calibrated</b> by a vector network analyzer in advance.|$|R
40|$|A new s-tetrazine-based low-bandgap {{semiconducting}} polymer, PCPDTTTz, {{was designed}} and synthesized. This is the first solution-processable conjugated polymer with tetrazine in the main chain. This polymer shows good thermal stability and broad absorption covering 4502 ̆ 212700 nm. The HOMO and LUMO energy levels were estimated to be 2 ̆ 2125. 34 and 2 ̆ 2123. 48 eV, with an electrochemical bandgap of 1. 86 eV. Simple polymer solar cells based on PCPDTTTz and PC 71 BM exhibit a <b>calibrated</b> <b>power</b> conversion efficiency of 5. 4...|$|R
40|$|A parental {{environment}} placing schizophrenic {{patients at}} high risk to relapse has been defined by two measures. One assessing expressed emotion (EE) is ‘objective’, but takes a considerable time to administer and requires a highly trained rater. The other involves use of a self-report meahre (the Parental Bonding Instrument or PBI) which is acceptable in clinical groups and requires only a brief period for completion. The {{degree to which the}} two measures predict the course of schizophrenia in patients discharged to their families is examined in representative studies. Each measure is assessed in terms of its sensitivity, specificity, odds ratio and overall diagnostic <b>power,</b> after <b>calibrating</b> modified PBI cut-off scores against rehospitalisation data. The PBI is a potentially useful clinical aid to predicting relapse in schizophrenic patients who return to the home environment. While the relevance of family style to the onset of schizophrenia still remains to be clarified, on...|$|R
40|$|We have {{measured}} {{the intensity of}} the cosmic microwave background (CMB) at a frequency of 7. 5 GHz (wavelength 4. 0 cm) using a ground-based, total <b>power</b> radiometer <b>calibrated</b> at the horn aperture by an external cryogenic reference target. The radiometer {{measured the}} difference in antenna temperature between the reference target and the zenith sky from a dry, high-altitude site. Subtraction of foreground signals (primarily atmospheric and galactic emission) measured with the same instrument leaves the CMB as the residual. The radiometer measured the atmospheric antenna temperature by correlating the signal change with the airmass in the beam during tip scans. The small galactic signal was subtracted based on extrapolation from lower frequencies, and was checked by differential drift scans. The limiting uncertainty in the CMB measurement was the effect of ground radiation in the antenna sidelobes during atmospheric measurements. The thermodynamic temperature of the CMB at 7. 5 GHz is 2. 59 {+-} 0. 07 K (68 % confidence level) ...|$|R
40|$|Abstract- To <b>calibrate</b> <b>power</b> {{and power}} quality {{measurement}} instruments such as flicker meters or harmonic measurement devices, several alternative techniques are generally used or proposed: • Using a reference device to make measurements {{which can be}} compared against those made by the UUT. • Applying variable loads to a static, impedance loaded power line to induce required conditions. • Generating the required signals with a precision signal source. This paper discusses the third option. There {{are a number of}} benefits to this approach, {{as well as a number}} of difficulties to overcome and decisions to be made to implement it effectively...|$|R
40|$|An {{improved}} standard {{derived from}} a current <b>power</b> bridge for <b>calibrating</b> active/reactive <b>power</b> and energy meters under sinusoidal conditions is described. Measurements can be made at any power factor from zero lag through unity to zero lead, positive or negative power, at 120 V, 5 A, and 50 or 60 Hz. The improved power standard has an estimated uncertainty of not more than 2. 5 uW/VA at k= 1. Special high accuracy current and voltage range extenders have been incorporated to extend the current and voltage ranges up to 200 A and 1200 V, respectively. Peer reviewed: YesNRC publication: Ye...|$|R
30|$|One {{limitation}} {{of our study}} {{is the use of}} surface coils for MRI acquisitions, resulting in a decreasing sensitivity from the cranial vertex to the skull base. This detection bias can be observed by comparing 31 P-ZTE (Fig.  3 c) and CT (Fig.  3 d): part of the skull base seen on CT is missing on 31 P-ZTE (white arrow). This bias is even more pronounced for the rat jaw which is not detected. However, the 31 P coil is optimized for brain detection so that cortical bone is properly taken into account when located in close contact to the brain. It must be noted that the excitation profile may also vary with the flip angle. However, the transmission <b>power</b> was <b>calibrated</b> in order to maximize the broad peak of the 31 P spectrum so that the effective flip angle was close to the Ernst angle of the bone, i.e., ~ 2 °. At a low angle such as this one, the excitation profile is rather insensitive to the flip angle.|$|R
40|$|We use a {{physically}} plausible four parameter linear response equation to relate 2, 000 years of global temperatures and sea level. We estimate likelihood distributions of equation parameters using Monte Carlo inversion, which then allows visualization {{of past and}} future sea level scenarios. The model has good predictive <b>power</b> when <b>calibrated</b> on the pre- 1990 period and validated against the high rates of sea level rise from the satellite altimetry. Future sea level is projected from {{intergovernmental panel on climate}} change (IPCC) temperature scenarios and past sea level from established multi-proxy reconstructions assuming that the established relationship between temperature and sea level holds from 200 to 2100 ad. Over the last 2, 000 years minimum sea level (− 19 to − 26 cm) occurred around 1730 ad, maximum sea level (12 – 21 cm) around 1150 ad. Sea level 2090 – 2099 is projected to be 0. 9 to 1. 3 m for the A 1 B scenario, with low probability of the rise being within IPCC confidence limits...|$|R
40|$|An {{improved}} {{power standard}} {{derived from a}} current comparator <b>power</b> bridge for <b>calibrating</b> active/reactive <b>power</b> and energy meters under sinusoidal conditions is described. Measurements can be made at any power factor from zero lag through unity to zero lead, at positive or negative power, at 120 V, 5 A, and 50 or 60 Hz. The improved power standard has an estimated uncertainty of not more than 2. 5 3 ̆bcW/VA at k = 1. Special high-accuracy current and voltage range extenders have been incorporated to extend the current and voltage ranges up to 200 A and 1200 V, respectively. 9 2013 IEEE. Peer reviewed: YesNRC publication: Ye...|$|R
40|$|The {{process of}} high {{harmonic}} generation allows for coherent transfer of infrared laser {{light to the}} extreme ultraviolet spectral range opening a variety of applications. The low conversion efficiency of this process calls for optimization or higher repetition rate intense ultrashort pulse lasers. Here we present state-of-the-art fiber laser systems for the generation of high harmonics up to 1 MHz repetition rate. We perform measurements of the average <b>power</b> with a <b>calibrated</b> spectrometer and achieved µW harmonics between 45 nm and 61 nm (H 23 -H 17) at a repetition rate of 50 kHz. Additionally, we show the potential for few-cycle pulses at high average power and repetition rate that may enable water-window harmonics at unprecedented repetition rate...|$|R
40|$|International Telemetering Conference Proceedings / October 13 - 15, 1970 / International Hotel, Los Angeles, CaliforniaThis paper {{describes}} a stellar calibration technique, using the absolute flux density from Cassiopeia A or Cygnus A, to determine effective antenna gain, or system noise temperature, at the IRIG L- and S-band frequencies. Paraboloidal dish antennas, ranging from 20 feet to 85 feet in diameter, can be calibrated using a total-power conventional RF receiver. Previous investigators utilized a Dicke radiometer {{to perform the}} same function. It is recommended that the Cass. A and Cyg. A flux densities, known within several tenths of a decibel, be utilized to calibrate IRIG antennas located on the North American Continent. It is demonstrated that Cass. A and Cyg. A provide sufficient signal <b>power</b> to <b>calibrate</b> a 20 -foot diameter dish antenna; dish antennas up to 85 -feet in diameter may be calibrated without applying a beam correction factor. Precision values of absolute flux density for Cass. A and Cyg. A are given for the 1700 - 1710 MHz space research, and IRIG 1435 - 1540 MHz and 2200 - 2300 MHz bands. An accurate radio sky map is also provided that may be scaled in frequency for the various bands...|$|R
40|$|Imports car, leaps, foreign, MBESS Description Each {{function}} accomplishes {{the work}} of several or more standard R functions. For example, two function calls, Read() and CountAll(), read the data and generate descriptive statistics for all variables in the data frame, plus histograms and bar charts as appropriate. Other functions provide for descriptive statistics, a comprehensive regression analysis, ANOVA and t-test, plotting, bar chart, histogram, box plot, density curves, <b>calibrated</b> <b>power</b> curve, the reading and display of csv and other formatted data and color themes. The function Help provides a help system that suggests specific analyses and functions. Variable labels are available. A confirmatory factor analysis of multiple indicator measurement models is also available as well as pedagogical routines for data simulation such as for the Central Limit Theorem...|$|R
40|$|In {{recent years}} {{there has been}} {{considerable}} growth in ‘Fair Trade’ markets for several commodities, but most notably for coffee. We argue that coffee is grown under conditions that might well subject growers to the market power of intermediaries. Using an approach designed to evaluate the impact of state trading enterprises, we develop an oligopsony model of intermediaries. In this model, ‘Fair Trade’ firms optimize a welfare function that includes the producer surplus of growers. This concern for growers’ welfare among some intermediary firms helps to alleviate the market <b>power</b> distortion. We <b>calibrate</b> the model to price data reported by a fair trade organization, and consider the counterfactual removal of fair trade behavior by intermediaries and customers downstream. As expected, the income of coffee growers (in aggregate) is reduced, though the effects are quite small. coffee, fair trade, oligopsony...|$|R
40|$|Abstract-The {{usage of}} {{proximity}} sensor for marker detections such as color line detection {{is the easiest}} technique in robotic competition. Most of the contestants use simple photo-reflective circuit as proximity sensor with constant light source and voltage-based threshold to deduce the presence of marker line. By using this technique, noises such as various changes in indoor lighting exposure and camera flash may cause negative effect against system performance. To overcome this situation, a proximity sensor model is presented utilizing photo-reflective circuit with modulated light source. Goertzel algorithm is then {{used to calculate the}} power spectrum value of the received signal from photo-reflective circuit. The presence of marker line is deduced by simple thresholding technique based on the previous <b>calibrated</b> <b>power</b> spectrum value. Using this model, marker line detection method based on proximity sensor becomes more robust against noises and disturbances. Keywords- detection; proximity sensor; robust; goertzel algorithm; I...|$|R
40|$|International audienceAtmospheric NO 2 columns {{retrieved}} from satellites {{can provide a}} useful top-down assessment of bottom-up NOx emissions inventories. We present three case studies of an approach to evaluate NOx emissions at a sector level by comparing satellite retrievals to regional chemical-transport model calculations of NO 2 columns. In the first example, the atmospheric impact of implementing NOx controls at eastern US power plants is demonstrated. In the second study, we use NOx monitors at western US <b>power</b> plants to <b>calibrate</b> our satellite-model comparisons. We then apply our approach to evaluate bottom-up estimates of NOx emissions from western US cities. In the third example, we validate our satellite-model approach using in-situ aircraft measurements and assess NOx emissions from power plants, cities, industrial facilities, and ports in eastern Texas. We conclude with some general insights on the usefulness of this approach and suggestions for future areas of research...|$|R
40|$|This chapter {{analyses}} {{the extent}} to which the notion of social responsibility through leadership has been embraced by Bristol and Liverpool mayors, as the only elected mayors in England’s core cities, operationalised through the broad framing principles of accountability, integrity, dependability, and authenticity. At the macro level, whilst the Localism Act 2011 enables the mayors to deliver authentic solutions during times of austerity, there is no sense of real devolution of <b>powers</b> from Whitehall, <b>calibrated</b> by the risk-averse provisions under the Public Sector (Social Value) Act 2012. At the meso level, the discussion pivots around the transformation of mayoral relationships with non-state actors, from co-production of the public services to co-determination of the local decision-making. At the micro level, socially responsible leadership has been theorised from the citizens’ expectation viewpoint, such as paving the way, inspiring a shared vision, and challenging the status quo...|$|R
40|$|The full {{aperture}} backscatter system provides {{a measure of}} the spectral power, and integrated energy scattered by stimulated Brillouin (348 - 354 nm) and Raman (400 - 800 nm) scattering into the final focusing lens of the first four beams of the NIF laser. The system was designed to provide measurements at the highest expected fluences with: (1) spectral and temporal resolution, (2) beam aperture averaging, and (3) near-field imaging. This is accomplished with a strongly attenuating diffusive fiber coupler and streaked spectrometer and separate calibrated time integrated spectrometers, and imaging cameras. A new technique determines the wavelength dependent sensitivity of the complete system with a calibrated Xe lamp. Data from the calibration system are combined with scattering data from targets to produce the <b>calibrated</b> <b>power</b> and energy measurements that show significant corrections due to the broad band calibrations. (C) 2004 American Institute of Physics...|$|R
40|$|International audienceIn {{this paper}} a {{two-stage}} 2 -GHz GaN HEMT amplifier with 15 -W output power, 28 -dB power gain, and 70 % power-added efficiency (PAE) is presented. The power stage {{is designed to}} operate under class F conditions. The driver stage operates under class F- 1 conditions and feeds the power stage with both fundamental and second harmonic components. The inter stage matching is designed to target a quasi-half sine voltage shape at the intrinsic gate port of the power stage. The goal is to reduce aperture angle of the power stage and get PAE improvements over a wide frequency bandwidth. In addition to the amplifier design description, this paper reports original time-domain waveform measurements at internal nodes of the designed two-stage <b>power</b> amplifier using <b>calibrated</b> high-impedance probes and large signal network analyzer. Furthermore, waveform measurements recorded at different frequencies show that aperture angle remains reduced over large frequency bandwidth. In this study, a PAE greater than 60 % is reached over 20 % frequency bandwidth...|$|R
40|$|Abstract. In {{the present}} study, we {{investigated}} the technical qualities of power consumption meters. A selection of nine meters available at general electric and discounts stores {{was selected for}} an evaluation along with two more expensive products and one prototype meter. The majority of the affordable devices fell into the price range of 10 ‐ 20 Euros. In the test, variable load combinations were used to asses the meters in reference to a known, <b>calibrated</b> <b>power</b> quality measurement device. The loads included small and reactive loads estimated to {{be difficult for the}} meters. The accuracy of the measured devices was very variable, some meters qualifying with small resistive loads, the majority of meters with high resistive loads. The found inaccuracies were predominantly negative. The devices underestimated the consumption and some devices had more than 25 % error. In this report we document these measurements and reveal the relative qualities of the tested products, highlighting two products...|$|R
40|$|AbstractBackgroundTo {{develop and}} {{validate}} sex specific prediction algorithms for 4 -year risk of {{major depressive episode}} (MDE) using data from a population-based longitudinal cohort. MethodsHousehold residents from 10 provinces were randomly recruited and interviewed by Statistics Canada. 10, 601 participants who were aged 18 years and older and who {{did not meet the}} criteria for MDE in the 12 months prior to a baseline interview in 2000 / 01 were included in algorithm development; data from 7902 participants who were aged 18 and older and who were free of MDE in 2004 / 05 were used for validation. Validation was also conducted in sub-populations that are of practice and policy importance. MDE was assessed using the World Health Organization's Composite International Diagnostic Interview(CIDI) —Short Form for Major Depression (CIDI-SFMD). ResultsIn the training data, the C statistics for algorithms in men was 0. 7953 and was 0. 7667 for algorithm in women. The algorithms had good predictive <b>power</b> and <b>calibrated</b> well in the development and validation data. LimitationsThe data relied on self-report. MDE was assessed with CIDI-SFMD. It was not feasible to validate the algorithms in different populations from different countries. ConclusionsMore studies are needed to further validate and refine these algorithms. However, the ability of a small number of easily assessed variables to predict MDE risk indicates that algorithms are a promising strategy for identifying individuals in need of enhanced monitoring and preventive interventions. Ultimately, application of algorithms may lead to increased personalization of treatment, and better clinical outcomes...|$|R
40|$|Structural health {{monitoring}} (SHM) {{is a research}} field that targets detecting and locating damage in structures. The main objective of SHM is to detect damage at its onset and inform authorities about the type, nature and location of the damage in the structure. Successful SHM requires deploying optimal sensor networks. We present a probabilistic approach to identify optimal location of sensors based on a priori knowledge on damage locations while considering the need for redundancy in sensor networks. The optimal number of sensors is identified using a multi-objective optimization approach incorporating information entropy and cost of the sensor network. As {{the size of the}} structure grows, the advantage of the optimal sensor network in damage detection becomes obvious. We also present an innovative field application of SHM using Field Programmable Gate Array (FPGA) and wireless communication technologies. The new SHM system was installed to monitor a reinforced concrete (RC) bridge on interstate I- 40 in Tucumcari, New Mexico. The new monitoring system is powered with renewable solar energy. The integration of FPGA and photovoltaic technologies make it possible to remotely monitor infrastructure with limited access to <b>power.</b> Using <b>calibrated</b> finite element (FE) model of the bridge with real data collected from the sensors installed on the bridge, we establish fuzzy sets describing different damage states of the bridge. Unknown states of the bridge performance are then identified using degree of similarity between these fuzzy sets. The proposed SHM system will reduce human intervention significantly and can save millions of dollars currently spent on prescheduled inspection by enabling performance based monitoring. Civil EngineeringDoctoralUniversity of New Mexico. Dept. of Civil EngineeringTaha, Mahmoud RedaRoss, TimothyEl- Osery, AlyAl-Haik, MarwanAnsari, Farha...|$|R
40|$|Owning to {{increasing}} power consumption and the corre-sponding heat dissipated on die, efficient on-chip temper-ature regulation becomes imperative for today’s high per-formance microprocessors. Temperature tracking {{based on the}} on-chip thermal sensors is not sufficient as the temper-ature hot spots keep changing with the load. One way to mitigate this problem is by means of software sensors, where temperature of any location is computed based on realtime <b>power</b> information and <b>calibrated</b> with the physi-cal sensors. In this paper, we present a very efficient nu-merical thermal analyzer, which is suitable for fast tem-perature tracking and online thermal regulation. The pro-posed method, called FEKIS, combines two existing numer-ical techniques: extended Krylov subspace reduction tech-nique to reduce the thermal circuit complexity and large-step integration method to exploits the piecewise constant power input traces, which is typical in the power traces at the architecture level. Experimental results show that FEKIS runs 10 X faster than the precise time-step integra-tion method only and 1000 X faster than the traditional nu-merical integration method with high accuracy...|$|R
40|$|Nitrogen- 15 Carr-Purcell-Meiboom-Gill (CPMG) {{transverse}} relaxation experiment {{are widely}} used to characterize protein backbone dynamics and chemical exchange parameters. Although an accurate value of the transverse relaxation rate, R(2), is needed for accurate characterization of dynamics, the uncertainty in the R(2) value depends on the experimental settings and {{the details of the}} data analysis itself. Here, we present an analysis of the impact of CPMG pulse phase alternation on the accuracy of the (15) N CPMG R(2). Our simulations show that R(2) can be obtained accurately for a relatively wide spectral width, either using the conventional phase cycle or using phase alternation when the r. f. pulse <b>power</b> is accurately <b>calibrated.</b> However, when the r. f. pulse is miscalibrated, the conventional CPMG experiment exhibits more significant uncertainties in R(2) caused by the off-resonance effect than does the phase alternation experiment. Our experiments show that this effect becomes manifest under the circumstance that the systematic error exceeds that arising from experimental noise. Furthermore, our results provide the means to estimate practical parameter settings that yield accurate values of (15) N transverse relaxation rates in the both CPMG experiments...|$|R
40|$|One {{atmospheric}} variable {{which can}} be deduced from stratosphere-troposphere (ST) radar data other than wind speed and direction is C sub n sup 2, related to the eddy dissipation rate. The computation of C sub n sup 2 makes use of the transmitted power (average, or peak plus duty cycle), {{the range of the}} echoes, and the returned power. The returned <b>power</b> can be <b>calibrated</b> only if a noise source of known strength is imposed; e. g., in the absence of absolute calibration, one can compare the diurnal noise signal with the galactic sky temperature. Thus to compute C sub n sup 2 one needs the transmitter power, the returned signal as a function of height, and the returned noise at an altitude so high that it is not contaminated by any signal. Now C sub n sup 2 relates with the amount of energy within the inertial subrange, and for many research studies it may be desirable to relate this with background flow as well as shears or irregularities {{on the size of the}} sample volume. The latter are quantified by the spectral width...|$|R
40|$|Photovoltaic (PV) power {{production}} increased drastically in Europe throughout the last years. About the 6 % of electricity in Italy comes from PV {{and for an}} efficient management of the power grid an accurate and reliable forecasting of production would be needed. Starting from a dataset of electricity production of 65 Italian solar plants for the years 2011 - 2012 we investigate the possibility to forecast daily production from one to ten days of lead time without using on site measurements. Our study is divided in two parts: {{an assessment of the}} predictability of meteorological variables using weather forecasts and an analysis on the application of data-driven modelling in predicting solar <b>power</b> production. We <b>calibrate</b> a SVM model using available observations and then we force the same model with the predicted variables from weather forecasts with a lead time from one to ten days. As expected, solar {{power production}} is strongly influenced by cloudiness and clear sky, in fact we observe that while during summer we obtain a general error under the 10 % (slightly lower in south Italy), during winter the error is abundantly above the 20 %. Comment: Submitted to Renewable Energ...|$|R
