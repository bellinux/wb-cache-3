4|13|Public
40|$|Recent work on {{the design}} of robust {{proportional}} plus integral non-adaptive process controllers for unknown minimum-phase (but possibly unstable) multivariable systems is compared and contrasted with recent work, extended to predict <b>permissible</b> <b>data</b> inaccuracies and illustrated by application to an open-loop unstable batch process...|$|E
40|$|International Telemetering Conference Proceedings / October 18 - 20, 1977 / Hyatt House Hotel, Los Angeles, CaliforniaA {{method has}} been {{proposed}} for improving the performance of automatic gain control (agc) weighted diversity combiners {{in the presence of}} fast fading radiofrequency (rf) signals by use of the amplitude modulation (a-m) (detected linear intermediate-frequency (i-f) envelope) in addition to the agc voltage to weight the combiner. Also suggested was a method for selecting the channel with the best signal-tonoise ratio (SNR) by use of the a-m and agc voltages. Experimental hardware has been constructed for evaluation of three configurations: an a-m/agc weighted combiner; an a-m/agc based selector; and an a-m/agc combiner/selector where the criterion for combine or select is determined by the phase error between the two channels. An experimental study was also conducted of the phase-locked loop (PLL) to determine the best configuration and parameter values for the combiner application (where relatively large phase errors are <b>permissible).</b> <b>Data</b> were taken under laboratory and operational (Vandenberg Air Force Base) conditions and are compared with data taken with a commercial agc weighted combiner...|$|E
40|$|Variable {{spreading}} gain (VSG) CDMA allows data terminals {{to operate}} at dissimilar transmission rates. With this technique, the chip rate is common to all users, but the spreading gains vary. Our work is relevant to the uplink of a single-cell VSG CDMA system, in which each terminal’s data throughput is weighted differently in calculating the network throughput. We seek for each active user a power level and transmission rate which will maximize the network’s aggregate weighted throughput. This paper focuses on the two-terminal, interference-limited scenario. The development is entirely analytical, based on optimization theory. One of the principal results is that the favorite terminal should always operate at the highest data rate. The optimal bit rate of the other terminal depends on {{the ratio of the}} minimum allowable spreading gain to the square root of the priority coefficient. Only when this ratio is large enough, is it optimal for both terminals {{to operate at}} the highest <b>permissible</b> <b>data</b> rate. In either case, we provide equations whose solutions lead to the optimal power ratios. ...|$|E
5000|$|Easy {{connectivity}} {{either directly}} or indirectly to all <b>permissible</b> sources of <b>data</b> ...|$|R
40|$|Data for {{surfactant}} diffusion are reproted for sodium dodecylsulfate at 25 ° and tetradecyltrimethylammonium bromide at 25 °, 90 °, and 135 °C, {{as measured}} by Taylor tube dispersion. These data are analyzed in terms of two limiting forms of theory, one appropriate to "slow" reaction rates, the other to "fast" rates. It is shown that the usual extrapolation to the critical micelle concentration to infer intrinsic diffusion constants is not <b>permissible.</b> The <b>data</b> is explicable if transport occurs by a process wherein ionic micelles disassociate, diffuse as monomers and reassemble into micelles. This is directly contrary to current ideas on diffusion of surfactants...|$|R
40|$|The aim of {{this study}} is the assessmentof the quality control of raw milk and {{traditional}} burduf cheese obtained fromcow milk mixed with 10 % sheep milk. Appreciation of the integrity and freshness assessmentof milk (cow and sheep) was tested by physico-chemical analysis. On theshelf-live were determined the physico-chemical parameters in cheese samples. Theantibiotics residues were tested of the milk samples with portable analyser,model Rosa Charm Reader. Theresults of physico-chemical determinations for the milk and cheese samples werewithin the maximum <b>permissible</b> by <b>data</b> legislation. Regardingthe content of antibiotics, the results were negative both for cow milk and forsheep milk. The sensorycharacteristics of burduf cheese are influenced by the different types of milk...|$|R
40|$|I {{will give}} a broad {{overview}} of what has become the standard paradigm in cosmology. I will describe the relational (à la Leibnitz) notion of time that is often used in cosmological calculations and discuss how the local nature of Einstein's equations allows us to translate this notion into statements about `initial' data. Classically this relates our local definition of time to a quasi-local region of a particular spatial slice, however incorporating quantum theory comes {{at the expense of}} losing this (quasi-) locality entirely. This occurs due to the presence of two, apparently distinct, issues: (1) Seemingly classical issues to do with the infinite spatial volume of the universe and (2) Quantum field theory issues, which revolve around trying to apply renormalization in cosmology. 	Following the ‘cosmological principle’ - an extension of the ‘Copernicus principle’ - that physics at every point in our universe should look the same, we are lead to the modern view of cosmology. This procedure is reasonably well understood for an exactly homogeneous universe, however the inclusions of small perturbations over this homogeneity leads to many interpretational/ conceptual difficulties. For example, in an (spatially) infinite universe perturbations can be arbitrarily close to homogeneous. To any observer, such a perturbation would appear to be a simple rescaling of the homogenous background and hence, physically, would not be considered an inhomogeneous perturbation at all. However, any attempt to choose the physically relevant scale at which perturbations should be considered homogeneous will break the cosmological principle i. e. it will make the resulting physics observer dependent. It amounts to `putting the perturbations in a box' and a delicate practical issue is that the universe is not static, hence the scale of the box will be time dependent. Thus what appears ‘physically homogeneous’ to an observer at one time will not appear so at another. 	This issue is brought to the forefront by considering the canonical (space and time rather space-time) version of the theory. The phase space formulation of General Relativity, just as for any other theory, contains the shadow of the underlying quantum theory. This means that, although the formulation is still classical, many of the subtleties that are present in the quantum theory are already apparent. In the cosmological context the infinite spatial volume renders almost all expressions formal or ill-defined. In order to proceed, we must restrict our attention to a cosmology that has some finite spatial extent, on which our relational notion of time is everywhere definable. In particular, this would constrain the <b>permissible</b> <b>data</b> outside our `observable universe'. 	This difficulty is an IR or large (spatial) scale issues in cosmology, however in addition there are UV or short (spatial) scale problems that need to be tackled. There are the usual problems of renormalization, which are further {{complicated by the fact that}} the universe is not static. In the cosmological setting this leads to new IR problems which again prevent one from taking the spatial extent of the universe to infinity. The physical relevance of this problem, the consequence for defining a time variable, and the distinction of homogeneous and inhomogeneous IR issues will be discussed...|$|E
40|$|Groundwater from {{sites in}} the Chamarajnagar District, Karnataka, India, were {{collected}} to evaluate its suitability for domestic and irrigation purposes. Results showed district groundwater was well within <b>permissible</b> limits. <b>Data</b> were subjected to hydrochem. interpretations. According to US Salinity Lab. (1954) classification, this groundwater is mainly 2 types: C 2 S 1 and C 3 S 1; at 1 or 2 sites, groundwater was C 3 S 2 type with moderately high salinity. The sodium absorption ratio was excellent. According to the B. K. Handa (1964) classification, study area groundwater has temporary and permanent hardness. Based on the H. Schoellers (1965, 1967) water type classification, most samples were types I and III; only one sample was type II. [on SciFinder(R) ...|$|R
40|$|An attempt {{has been}} made in this work to {{evaluate}} the environmental chemistry of groundwater in Kodagu district. Karnataka, India. The results show that, the ground water of this district are well within the <b>permissible</b> limits. The <b>data</b> are subjected to various hydrochemical interpretations. According to USSL Classification, the ground water fall under mostly two types as C 2 S 1 and C 3 S 1. In one location it is found to be of C 3 S 2 type with still moderately high salinity. The values of SAR are well within the permissible limit s. According to Handa's classification, the study area is characterized by water having both temporary and permanent hardness. However, most of the ground water samples have shown temporary hardness (65...|$|R
40|$|In this paper, we ask if XML {{access control}} can be {{supported}} when underlying (XML or relational) storage {{system does not}} provide adequate security features, and propose three alternative solutions – primitive, pre-processing, and post-processing. Towards that scenario, in particular, we advocate a scalable and effective pre-processing approach, called QFilter. QFilter is based on Non-deterministic Finite Automata (NFA), and rewrites user’s queries such that parts violating access control rules are pre-pruned. Through analysis and experimental validation, we show that (1) QFilter guarantees that only <b>permissible</b> portion of <b>data</b> is returned to the authorized users, (2) such access controls can be efficiently enforced without relying on security features of underlying storage system, and (3) such independency makes QFilter capable of many emerging applications, such as in-network access control and access control outsourcing...|$|R
30|$|To {{control the}} {{distortion}} {{induced by the}} embedding process, most audio steganography methods based on transform domain use a perceptual model to determine the <b>permissible</b> amount of <b>data</b> embedding without distorting the audio signal. Previous investigations evaluated frequency domain method are reported in Figure 10. Related results are reported in Additional file 1 : Table S 1. In a more challenging environment, such as real time applications, encoded domain methods ensure robustness against compression. A similar performance investigation reports the results shown in Additional file 1 : Table S 1 and in Figures (10 g), (10 h) and (10 i). Our results show that while using the same embedding capacity in temporal and frequency domains, stego signals generated in the frequency domain are less distinguishable than the ones produced by hiding data in the temporal domain.|$|R
40|$|The {{development}} of the divertor for the forthcoming DEMO fusion reactor stipu-lates heat flux loads larger than 10 MW/m 2. Successful design should withstand such high loads {{for a number of}} load cycles. One of the possible divertor con-figurations is also a divertor concept where the cooling is provided by multiple helium cooling jets (HEMJ) (1). In this work we propose a combined com-putational fluid dynamics and structural model for evaluating the structural response of the HEMJ design under the EFREMOV test experimental condi-tions (2), designed to be close to reactor operation conditions. Heat transfer coefficients between the helium and inner surface of the thimble are first cal-culated using a steady state response solution. Calculated coefficients are then used as a boundary condition in a thermo-mechanical analysis of the divertor. The analysis is performed for a number of load cycles under different surface heat flux levels. Good agreement of the highest temperatures on the tile’s top surface with the experimental data is obtained. The calculations suggest that there are three areas in the design where failure could initialize: a) the thim-ble’s inner surface with the highest thermal gradients, b) tiles outer surface and c) the layer between the tile and the thimble where the temperature is higher that <b>permissible.</b> Post-examination <b>data</b> of the performed tests confirm these conclusions as cracks were observed at the above mentioned areas a) and b), while melting of the layer c) was also observed...|$|R
40|$|M. Com. (Computer Auditing) During {{the last}} few decades the {{hardware}} of a computer system has undergone repeated revolutions. On the other hand the software development process has remained largely unchanged. The advent of the Information Age has, however, necessitated major improvements in the software development process. Object-Orientated Programming is seen as the vehicle by which this can be achieved. The use of object-orientation involves the auditor in two major areas. Firstly, the auditor may be involved in advising as to which systems engineering process to use and secondly, to assess the influence of the systems engineering process on the control environment of the client's computer system. In this dissertation, both the use of object orientation as a systems engineering methodology and the implications of this methodology on the control environment are discussed. Object-Orientated Programming can be broken down into the three main features, encapsulation, inheritance and interfaces. Encapsulation implies that both the data and processes that are <b>permissible</b> on that <b>data,</b> should be encapsulated as a single entity, known as an object. Inheritance on the other hand {{can be thought of as}} a specialisation of objects, to form a hierarchy of objects. Inheritance is, therefore, a way of sharing information between objects but with additional features to change or add certain attributes or methods of other objects. The external powers of an object are completely circumscribed by message passing. The only way in which an object can be addressed is to send a message to the object. This is done by the specific interfaces between the objects...|$|R
40|$|Abstract. We {{consider}} the classic problem {{of establishing a}} statistical ranking {{of a set of}} n items given a set of inconsistent and incomplete pairwise comparisons between such items. Instantiations of this problem occur in numerous applications in data analysis (e. g., ranking teams in sports data), computer vision, and machine learning. We formulate the above problem of ranking with incomplete noisy information as an instance of the group synchronization problem over the group SO(2) of planar rotations, whose usefulness has been demonstrated in numerous applications in recent years in computer vision and graphics, sensor network localization and structural biology. Its least squares solution can be approximated by either a spectral or a semidefinite programming (SDP) relaxation, followed by a rounding procedure. We show extensive numerical simulations on both synthetic and real-world data sets (Premier League soccer games, a Halo 2 game tournament and NCAA College Basketball games), which show that our proposed method compares favorably to other ranking methods from the recent literature. Existing theoretical guarantees on the group synchronization problem imply lower bounds on the largest amount of noise <b>permissible</b> in the <b>data</b> while still achieving an approximate recovery of the ground truth ranking. We propose a similar synchronization-based algorithm for the rank-aggregation problem, which integrates in a globally consistent ranking many pairwise rank-offsets or partial rankings, given by different rating systems on the same set of items, an approach which yields significantly more accurate results than other aggregation methods, including Rank-Centrality, a recent state-of-the-art algorithm. Furthermore, we discuss the problem of semi-supervised ranking when there is available information on the ground truth rank of a subset of players, and propose an algorithm based on SD...|$|R
40|$|Concentrations of cadmium, lead, {{chromium}} {{and copper}} {{in the blood}} samples collected from different groups of the population, living {{in and around the}} city of Lahore were determined in order to assess the exposure levels of these metals in the industrial workers and general population exposed to environmental pollutants. The groups were comprised of normal males and females; exposed industrial workers, laboratory workers and cancer patients. The male industrial worker and exposed female groups were found to contain higher lead-blood levels than I the normal groups. Cancer patient groups were found to contain the highest lead-blood concentrations than all the groups studied. The results showed higher blood-lead levels for all male groups compared to the female groups. Cadmium-blood concentrations were found to be higher for male industrial worker s and exposed female groups compared to the normal groups. Significantly low cadmium-blood levels were observed for cancer patients of both sexes. There appears to be a possible 1 ink between lead add cadmium blood levels and cancer. Chromium and copper-blood levels were generally higher f or females of all groups compared to the male groups. No significant variations were observed for copper-blood levels for exposed and cancer patients groups of both sexes. Almost similar chromium-blood levels were observed for industrial worker and cancer patient groups. The levels of heavy metals in blood for normal groups were generally higher than those for the similar groups {{in different parts of the}} world In order to investigate the major possible sources metallic pollutants, such as cadmium, lead, chromium nickel and copper, the contents of these metals were determined in water and commonly used food commodities. Tap water samples were found to contain heavy medals below the permissible limits except lead, whereas city sewage water contained elevated levels of heavy metals. Concentrations of cadmium, lead, chromium, nickel and copper were determined in urban (polluted) and rural (polluted) and rural vegetables, rice, wheat, pulses, spices, refined and raw s gar, meat (beef, mutton and chicken), vegetable oil and hyrogenated vegetable oil (vegetable ghee). Elevated levels of these toxic metals were found in urban vegetables, spices and beef Nickel was found to be 4. 4 times higher in vegetable ghee than the maximum <b>permissible</b> limits. Dietary <b>data</b> showed that the prime source of heavy metals intake by city population were vegetables and cereals. These sources contribute 79 - 93...|$|R
40|$|In {{the last}} twenty years, {{ecotoxicology}} has been increasingly concerned {{with the use of}} biomarkers to evaluate the biological hazard of toxic chemicals and, as an integrated approach, in the assessment of environmental health. The concept of biomarkers in the evaluation of environmental risk has captured the attention of regulatory agencies and is currently being assessed by several research commissions. This interest is confirmed by the increasing number of specialist manuals (see other publications by McCarthy and Shugart, Huggett et al., PeakaIl, and Shugart and Peakal). The central feature of this methodological approach is to "quantify exposure and its potential impact by monitoring biological end-points (biomarkers) in feral animals and plants as indicators of exposure to and the effects of environmental contaminants". Sometimes, however, in environmental contamination problems, the terms of investigation may shift from evaluation of environmental health, using sentinel species as bioindicators, to a more specific investigation of the "health" of a population or an endangered species in a situation of already-ascertained environmental poIlution. This inversion of terms inevitably leads to a demand for analytical and sampling methods that are compatible with the protection and conservation of the organism to be studied. In light of this increasingly important requirement, this book focuses on the use of nondestructive biomarkers (NDB) in the hazard assessment of vertebrate populations. The choice of nondestructive biomarkers over destructive biomarkers is not only an ethical one. The editors do not whoIly agree with the ideology of certain radical environmental movements in which the animal organism, as an individual, must be saved at all costs. From the ecological point of view, the value of a population or a community is greater than that of an individual. With this in mind, the loss of a few individuals for research purposes is <b>permissible</b> if the <b>data</b> obtained contribute to the conservation of the population or community studied. On {{the other side of the}} scale, there is the problem of the "ethic of the researcher". One may often ask whether the researcher is more harmful to the population than the contaminants studied. Several examples exist of "case studies" in which populations of protected species, already heavily stressed by anthropogenic disturbance and contaminants, have been further reduced in number by "wildcat" sampling on the part of shortsighted ecotoxicologists. Apart from ethical considerations, destructive testing in vertebrates may be unacceptable under many conditions, for example, in the hazard assessment of protected or threatened species, or when the number of animals available at a site is limited, or when sequential samples from the same individual are required for time-course studies. The use of noninvasive methods of monitoring the health of species and populations at risk has rarely been the subject of investigation by the "biomarker scientific community”. In this book we present an alternative approach for hazard assessment in high vertebrates based on nondestructive, rather than destructive, methods. World experts in the biomarker field have been co-opted in this "editorial adventure" in which we attempt to review the state of the art and to define the development and validation procedures of this new strategy. In November 1991, after a stimulating discussion with John McCarthy and Lee Shugart at Oak Ridge National Laboratory (U. S.), we conceived the idea of organizing an international workshop to discuss the current state of the nondestructive biomarker approach with the main experts in the sector, many of whom are authors of chapters in this book. In the winter of 1992, an application was made to the U. S. Department of Energy (USDOE) to hold an International Workshop on "Nondestructive Biomarkers in Vertebrates". The Organizing Committee consisted of M. Cristina Fossi and Claudio Leonzio, as directors, together with Lee Shugart, John McCarthy, David Peakall, Colin Walker, Silvano Focardi, and Aristeo Renzoni. The application was approved in the spring of 1992 and additional financial support to supplement the USDOE award was obtained from the Consiglio Nazionale delle Ricerche (Italy) and the Italian bank, Monte dei Paschi di Siena (MPS). On May 25 through 27, 19 scientists from six countries - United States, 6; United Kingdom, 6; Italy, 4; Spain, 2; Denmark, 1 - met in a medieval monastery, the Certosa di Pontignano, now owned by the University of Siena, for high-level scientific discussions on a new strategy for hazard assessment in vertebrates based on nondestructive biomarkers. This workshop provided a forum for the comprehensive review of the state of the art and for establishing an international consensus on the most useful and sensitive nondestructive biomarkers. Research priorities for the development and validation of this promising new method were also defined. This book makes the results of the workshop available to the international scientific community. The chapters in this volume describe different types of nondestructive biomarkers for hazard assessment in vertebrate species. The biomarkers are classified according to the nature of the toxic endpoint being probed. Particular attention is paid to the study of endangered species such as marine mammals. Each chapter contains an introduction in which the scientific basis and rationale for the endpoint being used as biomarker is explained, followed by a brief history of its application to environmental problems, together with available analytical techniques and possible destructive and nondestructive uses. The book is organized in eight sections: Overview (Chapter 1), Enzymatic Biomarkers (Chapters 2 and 3), Metabolic Products as Biomarkers (Chapter 4), Genotoxic Responses (Chapters 5, 6, and 7), Cellular Biomarkers (Chapter 8), Biomarkers in Eggs (Chapter 9), Biomarkers in Studies of Endangered Species (Chapters 10 and 1]), and Remarks on Nondestructive Biomarker Strategy (Chapters 12, 13, and 14) ...|$|R
40|$|The Extended Single Window (ESW) project aims {{to support}} goods flows by Information and Communication Technology (ICT). Specifically, the project takes {{the concept of}} Single Windows (often used {{in the sense that}} governments offer a single portal or {{interface}} to which businesses can submit information, supporting re-ruse by multiple agencies and coordination of government activities) and includes the business side; creating an Extended Single Window. An Extended Single Window includes business information systems and platforms and supports the re-use of business data, both for supporting new business applications and for making it easier to connect to government single windows. The project is not alone in this ambition. For example, in the FP 7 project CASSANDRA, funded by the European Commission, the concept of a data pipeline was developed and put to practice in various international trade lanes comprising four continents in total. Within the Netherlands, the national initiative to support innovations in logistics (Topsector Logistics) yielded the development of a Neutral Logistics Information Platform (NLIP, see www. nlip. org). This platform aimed to support information exchange in international supply chains. Similar to ESW, the starting point in the NLIP were the Port Community Systems (PCSs) in main ports in the Netherlands, and build from there. Given the similarities, ESW has been heavily impacted by the development of NLIP and much of the material in this report is {{in the context of the}} NLIP concept and programme. However, be it a data pipeline, an Extended single window, or a logistics information platform, one of the pressing issues of these ICTs for information exchange in the international supply chain is the issue of governance. Governance primarily concerns what kind of decision structures are needed, for example on the process of agreeing on data ownership, the selection of standards, and the funding structures. What incentives can be created to have parties adopt it, and who should provide these incentives? Is value added functionality an option? If so, what kind of functionality; only for parties that agree to it and have a role in it, or can it actually be part of the funding structure? That makes the question for data ownership, and cost- and benefit distribution even greater. There are several configuration options for global information sharing ’system-of-systems’. For example, commercial platform providers could each offer commercial solutions, the adoption of which would benefit the supply chains using it because all of the platforms adhere to a similar standard for supporting compliance (Bharosa, Klievink, Janssen, & Tan, n. d.). Apart from the commercial platforms of global IT solution providers, one of the most realistic developments paths is to have national platforms as main hubs, or ‘landing places’, connecting the complex logistical processes and stakeholder setting of port environments to the international trade flows, information-wise that is. In this report, we analyse the route towards a national information platform. To ensure our analysis is rooted in empirical material, as a case study we picked a specific Port Community System (PCS) as one of the building blocks of the national information platform. The case study comprises three parts, of which the key findings are:  As the NLIP/ESW is all about value-added functionalities for the sector as a whole by making smart combinations of data, we study three value-added services of the PCS. These three services (cargo information, inland manifest and discrepancy list) illustrate the role of a community system in bringing together a multitude of parties that are all independent but come together in specific trade lanes where the actions and information of one affect those of others.  We analyse the role of the system in an export process. We find that this is largely community functionality that is needed for (the parties in) a port to efficiently operate in a competitive international environment. Our analysis shows how this kind of core functionality generates a steady stream of key data, both public and private, that is necessary to make the above-mentioned value-added service possible. These are often public-private combinations, with often one of more stakeholders that (more) directly benefit from these functionalities, but do require others to contribute (that benefit less or not). A major area for decision making that follows from this analysis is that for a NLIP/ESW, decisions need to be made on which functionalities are <b>permissible</b> and which <b>data</b> may be used for them (i. e. can data that have been provided for community functionality be re-used for value-added services?). This is also related to the issue of data ownership and any rights or permissions a custodian of data may have.  Third, we analyse the role of the system in an import process. Again, this is core functionality, needed by parties involved in importing goods, whether they are involved in the logistics (handling in the port and hinterland transport), the trade lane (e. g. as a buyer or re-seller of the goods), or as an inspection agency (e. g. Customs, food and product safety). In this situation, the ‘cargo information’ service (described as part of step one) offers functionality for various parties involved. However, our analysis shows that this also yields a debate on the pricing of such a service, as well as the cost distribution. A major area for decision making that follows from this, is that of decisions on the finance structure of the system as a whole (e. g. which services are considered community functionality and how to fund that) and of individual services that need to be decided on at the community-level (e. g. how are costs and/or benefits distributed among parties that are involved in the service). From the interviews we learn that stakeholders have multiple perspectives regarding the decisions at the community system level. Though NLIP/ESW is basically a federated system, the abovementioned aspects and areas for decision making transcend the level that individual actors can make decisions on. As NLIP/ESW brings multiple communities (e. g. the community in a specific port or in a specific sector) together, these areas also transcend the level of communities that have existing collaborations at the community level. Dealing with these aspects requires processes or structures for collaboration among stakeholders for agenda setting and decision-making. We argue that this situation can be dealt with by developing a structure (e. g. an institutionalised process, potentially with stakeholder participation) for deciding on these issues in a way that makes the decisions and the process transparent to the stakeholders. Also, the structure needs to accommodate that stakeholders can raise issues, are heard, and committed to the outcomes. This actor-related complexity is the area of governance of NLIP/ESW, the topic of work package 3 in the ESW project, of which this is the final report. When assessing the current governance related to NLIP/ESW, it becomes clear that currently much of the actor-complexity is funnelled on the technical complexity. In other words; the technical arrangement have to accommodate not only the technical complexity but also the positions and interests of the stakeholders that were involved in the development phase. Further adding to the complexity is that the types of operations and information exchanges that the NLIP/ESW should support are highly diverse, if it were to act as a national platform and pipeline ‘landing place’. This complexity cannot only be dealt with by technical solutions, further emphasising the need for solutions in the area of governance. Currently, many governance-related issues are discussed and decided on in a temporary collaboration structure (ESW is a project and NLIP could also be considered a project, or a programme covering multiple projects). Some issues that stakeholders encounter may seem operational or technical problems, but at the core these revolve around deciding what NLIP/ESW may do, aims to do, how it does that, and who pays for what. Our study finds that for the next step in the development of NLIP/ESW, a long-term basis for proper decision-making needs to be developed, also internationally. This basis entails that there be a structure and decision making processes that are able to ensure effective and efficient decision making regarding those aspects that transcend the level of individual actors (Veeneman, Ten Heuvelhof, De Bruijn, & Saanen, 2011). Parts of this structure are already in place in the existing NLIP/ESW programme, but its temporary ‘project’ basis is likely to be too permissive to be able to make decisions without risking a long, dragging process of strategic behaviour and negotiations. Also, the governance structures of the existing NLIP/ESW components (i. e. the PCSs) do work with representation of various stakeholder communities, but our research suggests that parties that are not involved directly (including parties that have representation) do not have a clear understanding of how decisions were made...|$|R

