31|10|Public
50|$|While much of {{the early}} {{academic}} {{work in this area}} was focused on the English language (with significant use of the <b>Porter</b> <b>Stemmer</b> algorithm), many other languages have been investigated.|$|E
5000|$|An {{example of}} understemming in the <b>Porter</b> <b>stemmer</b> is [...] "alumnus" [...] → [...] "alumnu", [...] "alumni" [...] → [...] "alumni", [...] "alumna"/"alumnae" [...] → [...] "alumna". This English word keeps Latin morphology, and so these near-synonyms are not conflated.|$|E
5000|$|Martin F. Porter is the {{inventor}} of the <b>Porter</b> <b>Stemmer,</b> {{one of the most common}} algorithms for stemming English, and the Snowball programming framework. His 1980 paper [...] "An algorithm for suffix stripping", proposing the stemming algorithm, has been cited over 8000 times (Google Scholar).|$|E
40|$|A {{language}} independent stemmer {{has always}} been looked for. Single N-gram tokenization technique works well, however, it often generates stems that start with intermediate characters, rather than initial ones. We present a novel technique that takes the concept of N gram stemming one step ahead and compare our method with an established algorithm in the field, <b>Porter's</b> <b>Stemmer.</b> Results indicate that our N gram stemmer is not inferior to <b>Porter's</b> linguistic <b>stemmer.</b> Comment: 10 page...|$|R
40|$|There is {{a growing}} {{interest}} {{in the use of}} context-awareness as a technique for developing pervasive computing applications that are flexible and adaptable for users. In this context, however, information retrieval (IR) is often defined in terms of location and delivery of documents to a user to satisfy their information need. In most cases, morphological variants of words have similar semantic interpretations and can be considered as equivalent for the purpose of IR applications. Consequently, document indexing will also be more meaningful if semantically related root words are used instead of stems. The popular <b>Porter’s</b> <b>stemmer</b> was studied with the aim to produce intelligible stems. In this paper, we propose Context-Aware Stemming (CAS) algorithm, which is {{a modified version of the}} extensively used <b>Porter’s</b> <b>stemmer.</b> Considering only generated meaningful stemming words as the stemmer output, the results show that the modified algorithm significantly reduces the error rate of Porter’s algorithm from 76. 7 % to 6. 7 % without compromising the efficacy of Porter’s algorithm. Keywords- Context-awareness; information retrieval; stemming, precision; recal. 1...|$|R
30|$|All our {{experiments}} are {{conducted on the}} standard datasets used in the TREC CDS tasks of 2014, 2015 and 2016. The target document collection used is an open access subset 1 of PubMed Central 2 (PMC). In 2014 and 2015, the same 733, 138 articles were extracted, and in 2016, a larger and newer set of 1.25 million articles were extracted. We extract the title, abstract, keywords and body fields from each article {{as the source of}} the index. We use the open source Terrier toolkit version 4.1 [20] to index the collection with the recommended settings of the toolkit. Standard English stopwords are removed and the collection is stemmed using <b>Porter’s</b> English <b>stemmer.</b> Using <b>Porter’s</b> <b>stemmer,</b> inflected or derived words are reduced to their word stem, base or root forms.|$|R
5000|$|For example, {{the widely}} used <b>Porter</b> <b>stemmer</b> stems [...] "universal", [...] "university", and [...] "universe" [...] to [...] "univers". This {{is a case}} of overstemming: though these three words are {{etymologically}} related, their modern meanings are in widely different domains, so treating them as synonyms in a search engine will likely reduce the relevance of the search results.|$|E
30|$|<b>Porter</b> <b>stemmer</b> [36] {{has been}} applied to both {{collections}} and stop words [37] and words with less than three characters are also removed.|$|E
40|$|This paper {{investigates the}} use of {{stemming}} for classification of Dutch (email) texts. We introduce a stemmer, which combines dictionary lookup (implemented efficiently as a finite state automaton) with a rule-based backup strategy and show that it outperforms the Dutch <b>Porter</b> <b>stemmer</b> in terms of accuracy, while not being substantially slower...|$|E
40|$|This paper {{describes}} {{our approach}} to the 2006 Adhoc Monolingual Information Re-trieval run for French. The goal of our experiment was to compare {{the performance of a}} proposed statistical stemmer with that of a rule-based stemmer, specically the French version of <b>Porter's</b> <b>stemmer.</b> The statistical stemming approach is based on lexicon clustering, using a novel string distance measure. We submitted three ocial runs, besides a baseline run that uses no stemming. The results show that stem-ming signicantly improves retrieval performance (as expected) by about 9 - 10 %, and the performance of the statistical stemmer is comparable with that of the rule-based stemmer...|$|R
40|$|Text {{summarization}} is {{an important}} activity {{in the analysis of}} a high volume text documents. Text summarization has number of applications; recently number of applications uses text summarization for the betterment of the text analysis and knowledge representation. In this paper a frequent term based text summarization algorithm is designed and implemented in java. The designed algorithm works in three steps. In the first step the document which is required to be summarized is processed by eliminating the stop word and by applying the stemmers. In the second step term-frequent data is calculated from the document and frequent terms are selected, for these selected words the semantic equivalent terms are also generated. Finally in the third step all the sentences in the document, which are containing the frequent and semantic equivalent terms, are filtered for summarization. The designed algorithm is implemented using open source technologies like java, DISCO, <b>Porters</b> <b>stemmer</b> etc. and verified over the standard text mining corpus...|$|R
30|$|We use five {{standard}} TREC test {{collections in}} our experiments, {{and the basic}} statistics about the test collections and topics are given in Table  10. Documents are preprocessed by removing all HTML tags, standard English stopwords are removed and the test collections are stemmed using <b>Porter’s</b> English <b>stemmer.</b> Each topic contains three fields, i.e., title, description and narrative, and we only use the title field. The title-only queries are very short which is usually regarded as a realistic snapshot of real user queries.|$|R
40|$|A {{stemming}} is {{a technique}} used to reduce words to their root form, by removing derivational andinflectional affixes. The stemming is widely used in information retrieval tasks. Many researchersdemonstrate that stemming improves the performance of information retrieval systems. <b>Porter</b> <b>stemmer</b> isthe most common algorithm for English stemming. However, this stemming algorithm has severaldrawbacks, since its simple rules cannot fully describe English morphology. Errors made by this stemmermay affect the information retrieval performance. The present paper proposes an improved version of the original Porter stemming algorithm for the Englishlanguage. The proposed stemmer is evaluated using the error counting method. With this method, theperformance of a stemmer is computed by calculating the number of understemming and overstemmingerrors. The obtained results show an improvement in stemming accuracy, compared with the originalstemmer, but also compared to other stemmers such as Paice and Lovins stemmers. We prove, in addition,that the new version of <b>porter</b> <b>stemmer</b> affects the information retrieval performance...|$|E
40|$|Likey is an {{unsupervised}} statistical {{approach for}} keyphrase extraction. The method is language-independent {{and the only}} language-dependent component is the reference corpus with which the documents to be analyzed are compared. In this study, we have also used another language-dependent component: an English-specific <b>Porter</b> <b>stemmer</b> as a preprocessing step. In our experiments of keyphrase extraction from scientific articles, the Likey method outperforms both supervised and unsupervised baseline methods. ...|$|E
30|$|Initially, text {{documents}} {{which have}} been collected from various sources were accumulated in a database. Then, pre-processing {{was carried out by}} considering the various stages like: tagging by means of Stanford POS tagger tool, stop word removal and stemming, based on <b>Porter</b> <b>Stemmer</b> algorithm and morphological capabilities of WordNet. The above preprocessing is common for both existing and proposed algorithms considered in this study. Then the documents are represented as VSM. These documents are clustered using Bisecting K-means algorithm which generates K number of clusters.|$|E
40|$|Stemming is an {{operation}} that conflates morphologically similar terms into a single term without doing complete morphological analysis. Stemming is used in information retrieval systems to improve performance. We describe a method to get the stem from the given word. Stemming is a technique which is required in information retrieval system and {{it is used to}} increase the performance of the retrieval result. All natural language processing systems must require a stemmer for it. The common goal of stemming is to standardize words by reducing a word to its base. <b>Porter's</b> <b>stemmers</b> have been used as a standard for English language. In this paper we have used Brute force technique with suffix stripping approach. Here we are using two approaches to get the maximum accuracy from the stemmer. For a language like Punjabi {{it is not easy to}} create a stemmer for it. Well known techniques for stemming are suffix removal, brute force technique, rule based technique and hybrid approaches. The need for good stemming algorithms for these languages has increased in the wake of search and retrieval system. In this we have created a huge database and a list of suffixes. With the help of big database we are getting higher accuracy than the other stemmers. We also reduce the over-stemming and understemming errors by finding number of words which causes these errors. We have already added these words in our database so to avoid the errors...|$|R
40|$|Hummingbird {{participated in}} the {{monolingual}} information retrieval tasks of the Cross-Language Evaluation Forum (CLEF) 2003 : for natural language queries in 9 European languages (German, French, Italian, Spanish, Dutch, Finnish, Swedish, Russian and English) find all the relevant documents (with high precision) in the CLEF 2003 document sets. For each language, SearchServer scored higher than the median average precision on more topics than it scored lower. In a comparison of experimental SearchServer lexical <b>stemmers</b> with <b>Porter’s</b> algorithmic <b>stemmers,</b> the biggest differences were for the languages in which compound words are frequent (German, Dutch, Finnish and Swedish). SearchServer scored significantly higher in average precision for German and Finnish, apparently from its ability to split compound words and find terms when they are parts of compounds in these languages. Most of the differences for the other languages appeared to be from SearchServer’s lexical stemmers performing inflectional stemming while the algorithmic stemmers often additionally performed derivational stemming; these differences did not pass a significance test. ...|$|R
40|$|Open source Python modules, {{linguistic}} data and documentation {{for research and}} development in natural language processing, supporting dozens of NLP tasks. NLTK includes the following software modules (~ 120 k lines of Python code) : Corpus readers interfaces to many corpora Tokenizers whitespace, newline, blankline, word, treebank, sexpr, regexp, Punkt sentence segmenter <b>Stemmers</b> <b>Porter,</b> Lancaster, regexp Taggers regexp, n-gram, backoff, Brill, HMM, TnT Chunkers regexp, n-gram, named-entity Parsers recursive descent, shift-reduce, chart, feature-based, probabilistic, dependency, [...] . Semantic interpretation untyped lambda calculus, first-order models, DRT, glue semantics, hole semantics, parser interface WordNet WordNet interface, lexical relations, similarity, interactive browser Classifiers decision tree, maximum entropy, naive Bayes, Weka interface, megam Clusterers expectation maximization, agglomerative, k-means Metrics accuracy, precision, recall, windowdiff, distance metrics, inter-annotator agreement coefficients, word association measures, rank correlation Estimation uniform, maximum likelihood, Lidstone, Laplace, expected likelihood, heldout, cross-validation, Good-Turing, Witten-Bell Miscellaneous unification, chatbots, many utilities NLTK-Contrib (less mature) categorial grammar (Lambek, CCG), finite-state automata, hadoop (MapReduce), kimmo, readability, textual entailment, timex, TnT interface, inter-annotator agreemen...|$|R
40|$|Unsolicited or spam emails {{are on the}} rise, where one’s email storage inbox is {{bombarded with}} emails that make no sense at all. This creates excess usage of traffic {{bandwidth}} and results in unnecessary wastage of network resources. We wanted to test the Bayesian spam detection scheme with context matching that we had developed by implementing the keyword stripping using the <b>Porter</b> <b>Stemmer</b> algorithm. This could make the keyword search more efficient, as the root or stem word is only considered. Experimental results on two public spam corpuses are also discussed at the end...|$|E
40|$|This paper {{investigates the}} use of {{stemming}} for classification of Dutch (email) texts. We introduce a stemmer, which combines dictionary lookup (implemented efficiently as a finite state automaton) with a rule-based backup strategy and show that it outperforms the Dutch <b>Porter</b> <b>stemmer</b> in terms of accuracy, while not being substantially slower. For text classification, the most important property of a stemmer {{is the number of}} words it (correctly) reduces to the same stem. Here the dictionary-based system also outperforms Porter. However, evaluation of a Bayesian text classification system with either no stemming or the Porter or dictionary-based stemmer on an email classification and a newspaper topic classification task does not lead to significant differences in accuracy. We conclude with an analysis of why this is the case. ...|$|E
30|$|Twitter users often mention {{one or more}} {{users in}} their own tweets with @user to include the other users in their conversation. Although {{mentions}} appear {{many times in the}} data set, we removed these user handles from the data set because they are generally used to show interest in the mentioned user or the relationship, but not in the user itself. Additionally, we follow common information retrieval preprocessing steps by: (1) removing punctuation and non-alphanumeric symbols; (2) removing common stop-words; (3) transforming all text to lowercase; (4) stemming (we employed the <b>Porter</b> <b>Stemmer</b> from the open source Python library NLTK [14]). Some non-English languages that do not use word dividers (e.g., a blank space between words) such as Japanese would require an extra pre-processing step to identify the parts of speech before performing the above steps.|$|E
40|$|Stemmers {{attempt to}} reduce a word to its stem or root form and are used widely in {{information}} retrieval tasks to increase the recall rate. Most popular stemmers encode {{a large number of}} language specific rules built over a length of time. Such stemmers with comprehensive rules are available only for a few languages. In the absence of extensive linguistic resources for certain languages, statistical language processing tools have been successfully used to improve the performance of IR systems. In this paper, we describe a clustering-based approach to discover equivalence classes of root words and their morphological variants. A set of string distance measures are defined, and the lexicon for a given text collection is clustered using the distance measures to identify these equivalence classes. The proposed approach is compared with <b>Porter’s</b> and Lovin’s <b>stemmers</b> on the AP and WSJ sub-collections of the TIPSTER data set using 200 queries. Its performance is comparable to that of Porter’s and Lovin’s 1 stemmers, both in terms of average precision and the total number of relevant documents retrieved. The proposed stemming algorithm also provides consistent improvements in retrieval performance for French and Bengali, which is currently resource-poor. ...|$|R
40|$|Application that {{discussed}} in this paper is able to perform the process of finding web pages that have similar content to the url of the desired web page. Also developed an automated process for crawling web pages. This crawling process will continue since the process is activated. The search process begins by entering a url and web page url is obtained from the extract to get the key words that represent the web page. The keywords will be processed into a basic form using the <b>Porter</b> <b>Stemmer</b> algorithm. TF-IDF method used to obtain {{the importance of a}} keyword. Furthermore Jaccard Coefficient formula used to find similarity between web pages. Applications are limited to Web Page in English. Based on test results concluded that this application has worked well and can be utilized...|$|E
40|$|For {{our first}} {{participation}} to the TREC evaluation campaign, our efforts {{concentrated on the}} genomic track. Because we joined the competition {{at the end of}} June, {{we were not able to}} submit runs for the ad hoc retrieval task (task I), and therefore this report mostly focuses on the information extraction task (task II). Task I. Our approach uses thesaural resources (from the UMLS) together with a variant of the <b>Porter</b> <b>stemmer</b> for string normalization. Gene and Protein Entities (GPE) of the collection (525, 938 MedLine citations) were simply marked up by dictionary look up during the indexing in order to avoid erroneous conflation: strings not found in the UMLS Specialist lexicon (augmented with various English lexical resources) were considered as GPE and were moderately overweighted. In the same spirit like other TREC competitor...|$|E
40|$|This paper {{investigates the}} use of {{stemming}} for classification of Dutch (email) texts. We introduce a stemmer, which combines dictionary lookup (implemented efficiently as a finite state automaton) with a rule-based backup strategy and,how, that it outperforms the Dutch <b>Porter</b> <b>stemmer</b> in terms of accuracy. while not being substantially slower. For text classification, the most important property of a stemmer {{is the number of}} words it (correctly) reduces to the same stem. Here the dictionary - based system also outperforms Porter. However, evaluation of a Bayesian text classification system with either no stemming or the Porter or dictionary-based stemmer on an email classification and a newspaper topic classification task does not lead to significant differences in accuracy. We conclude with an analysis of why this is the case...|$|E
40|$|We {{describe}} {{our approach}} to the construction and evaluation of a large-scale database called "CatVar" which contains categorial variations of English lexemes. Due to the prevalence of cross-language categorial variation in multilingual applications, our categorial-variation resource may serve {{as an integral part}} of a diverse range of natural language applications. Thus, the research reported herein overlaps heavily with that of the machine-translation, lexicon-construction, and information-retrieval communities. We apply the information-retrieval metrics of precision and recall to evaluate the accuracy and coverage of our database with respect to a human-produced gold standard. This evaluation reveals that the categorial database achieves a high degree of precision and recall. Additionally, we demonstrate that the database improves on the linkability of <b>Porter</b> <b>Stemmer</b> by over 30 %. UMIACS-TR- 2003 - 13 LAMP-TR- 09...|$|E
40|$|Effects {{of three}} {{different}} morphological methods- lemmatization, stemming and inflectional stem generation- for Finnish are compared in a probabilistic IR environment (INQUERY). Evaluation is done using a four point relevance scale which is partitioned differently in different test settings. Results show that inflectional stem generation {{which has not been}} used much in IR, compares well with lemmatization in a best-match IR environment. Differences in performance between inflectional stem generation and lemmatization are small and they are not statistically significant in most of the tested settings. It is also shown that hitherto a rather neglected method of morphological processing for Finnish, stemming, performs reasonably well although the stemmer used – a <b>Porter</b> <b>stemmer</b> implementation – is far from optimal for a morphologically complex language like Finnish. In another series of tests, the effects of 1 compound splitting and derivational expansion of queries are tested...|$|E
40|$|Personal {{recommendation}} system is one which gives better and preferential recommendation to the users {{to satisfy their}} personalized requirements such as practical applications like Webpage Preferences, Sport Videos preferences, Stock selection based on price, TV preferences, Hotel preferences, books, Mobile phones, CDs and various other products now use recommender systems. The existing Pearson Correlation Coefficient (PCC) and item-based algorithm using PCC, are called as UPCC and IPCC respectively. These systems are mainly based on only the rating services and does not consider the user personal preferences, they simply just give the result based on the ratings. As the size of data increases it will give the recommendations based on the top rated services and it will miss out most of user preferences. These are main drawbacks in the existing system which will give same results to the users based on some evaluations and rankings or rating service, they will neglect the user preferences and necessities. To address this problem we propose a new approach called, Personnel Recommendation System (PRS) for huge data analysis using <b>Porter</b> <b>Stemmer</b> to solve the above challenges. In the proposed system it provides a personalized service recommendation list to the users and recommends the most useful services to the users which will increase the accuracy and efficiency in searching better services. Particularly, a set of suggestions or keywords are provided to indicate user preferences and we used Collaborative Filtering and <b>Porter</b> <b>Stemmer</b> algorithm which gives a suitable recommendations to the users. In real, the broad experiments are conducted on the huge database which is available in real world, and outcome shows that our proposed personal recommender method extensively improves the precision and efficiency of service recommender system over the KASR method. In our approach mainly consider the user preferences so it will not miss out the any of the data, based on the ranking system and gives better preferential recommendations...|$|E
30|$|The medical domain {{has been}} an early adopter of NLP. Unlike many domains which use general NLP techniques, {{clinical}} NLP systems attempt to incorporate domain knowledge {{in order to increase}} the quality of extracted information. A common practice in NLP is stemming, which attempts to find the root of a word. This technique can potentially increase the quality of extracted data, but general techniques such as the <b>Porter</b> <b>stemmer</b> have limited use in the clinical domain [10]. Variants of common English terms often contain no more than an additional prefix or suffix. Clinical term variants may be medication brand names of the same generic drug or diseases which may have different common names internationally. Additional concerns exist such as extracting numeric values and units of laboratory results, identification of family histories vs patient history, and negation of findings. These concerns have {{led to the development of}} domain specific clinical NLP systems spanning back many decades.|$|E
40|$|The aim of {{document}} clustering is {{to produce}} coherent clusters of similar documents. Clustering algorithms rely on text normalisation techniques to represent and cluster documents. Although most document clustering algorithms perform well in specific knowledge domains, processing cross-domain document repositories is still a challenge. This paper attempts to address this challenge. It investigates {{the performance of the}} sk-means clustering algorithm across domains, by comparing the cluster coherence produced with semantic-based and traditional (TF-IDF-based) document representations. The evaluation is conducted on 20 different generic sub-domains of a thousand documents, each randomly selected from the Reuters 21578 corpus. The experimental results obtained from the evaluation demonstrate improved coherence of clusters produced by using a semantically enhanced text stemmer (SETS), when compared to the text normalisation obtained with the <b>Porter</b> <b>stemmer.</b> In addition, semantic-based text normalisation is shown to be resistant to noise, which is often introduced in the index aggregation stage, a stage that acquires features to represent documents...|$|E
40|$|The Forum for Information Retrieval Evaluation (FIRE) {{provides}} document collections, topics, {{and relevance}} assessments for information retrieval (IR) experiments on Indian languages. Several research questions are explored in this paper: 1. {{how to create}} create a simple, languageindependent corpus-based stemmer, 2. how to identify sub-words and which types of sub-words are suitable as indexing units, and 3. how to apply blind relevance feedback on sub-words and how feedback term selection {{is affected by the}} type of the indexing unit. More than 140 IR experiments are conducted using the BM 25 retrieval model on the topic titles and descriptions (TD) for the FIRE 2008 English, Bengali, Hindi, and Marathi document collections. The major findings are: The corpus-based stemming approach is effective as a knowledge-light term conation step and useful in case of few language-specific resources. For English, the corpusbased stemmer performs nearly as well as the <b>Porter</b> <b>stemmer</b> and significantly better than the baseline of indexing words when combined with query expansion. In combination with blind relevance feedback, it also performs significantly better than the baseline for Bengali and Marathi IR. Sub-words such as consonant-vowel sequences and word prefixes can yield similar or better performance in comparison to word indexing. There is no best performing method for all languages. For English, indexing using the <b>Porter</b> <b>stemmer</b> performs best, for Bengali and Marathi, overlapping 3 -grams obtain the best result, and for Hindi, 4 -prefixes yield the highest MAP. However, in combination with blind relevance feedback using 10 documents and 20 terms, 6 -prefixes for English and 4 -prefixes for Bengali, Hindi, and Marathi IR yield the highest MAP. Sub-word identification is a general case of decompounding. It results in one or more index terms for a single word form and increases the number of index terms but decreases their average length. The corresponding retrieval experiments show that relevance feedback on sub-words benefits from selecting a larger number of index terms in comparison with retrieval on word forms. Similarly, selecting the number of relevance feedback terms depending on the ratio of word vocabulary size to sub-word vocabulary size almost always slightly increases information retrieval effectiveness compared to using a fixed number of terms for different languages...|$|E
40|$|Motivation: Attribute {{selection}} {{is a critical}} step in development of document classification systems. As a standard practice, words are stemmed and the most informative ones are used as attributes in classification. Owing to high complexity of biomedical terminology, general-purpose stemming algorithms are often conservative and could also remove informative stems. This can lead to accuracy reduction, especially {{when the number of}} labeled documents is small. To address this issue, we propose an algorithm that omits stemming and, instead, uses the most discriminative substrings as attributes. Results: The approach was tested on five annotated sets of abstracts from iProLINK that report on the experimental evidence about five types of protein post-translational modifications. The experiments showed that Naive Bayes and support vector machine classifiers perform consistently better [with area under the ROC curve (AUC) accuracy in range 0. 92 – 0. 97] when using the proposed attribute selection than when using attributes obtained by the <b>Porter</b> <b>stemmer</b> algorithm (AUC in 0. 86 – 0. 93 range). The proposed approach is particularly useful when labeled datasets are small. Contact...|$|E
40|$|Abstract. In this paper, {{we study}} text {{classification}} algorithms by uti-lizing two concepts from Information Extraction discipline; dependency patterns and stemmer analysis. To {{the best of}} our knowledge, this is the first study to fully explore all possible dependency patterns during the formation of the solution vector in the Text Categorization problem. The benchmark of the classical approach in text classification is improved by the proposed method of pattern utilization. The test results show that support of four patterns achieves the highest ranks, namely, participle modifier, adverbal clause modifier, conjunctive and possession modifier. For the stemming process, we benefit from both morphological and syn-tactic stemming tools, <b>Porter</b> <b>stemmer</b> and Stanford Stemmer, respec-tively. One of the main contributions of this paper is its approach in stemmer utilization. Stemming is performed not only for the words but also for all the extracted pattern couples in the texts. Porter stemming is observed to be the optimal stemmer for all words while the raw form without stemming slightly outperforms the other approaches in pattern stemming. For the implementation of our algorithm, two formal datasets...|$|E
40|$|Wenormally view text as a {{sequence}} of words, or we can impose additional structure on it like syntax. Or, we can dissolve some structure, namely, {{the order of the}} words. This gives a bag of words, which simply counts howmany times each word occurs in a sentence (or document). the cat sat on the mat → {cat,mat,on,sat, the,the} Why? First, this representation is computationally extremely easy to work with. Second, this represen-tation melts a text down into bits of meaning and serves as a crude way of capturing what the text is “about. ” Crude, but often effective. Tokenization Weassume that all the sentences/documents have been tokenized so that thewordbound-aries are unambiguous. A commonly used English tokenizer is part of Stanford CoreNLP, 1 and LDC has a simple rule-based tokenizer. 2 Both are implemented/wrapped in Python by NLTK. 3 Stemming It is also common in bag-of-word approaches to do morphological stemming, that is, re-moving affixes like-ed,-ing, etc. A classic stemmer for English is the <b>Porter</b> <b>stemmer,</b> 4 and another is the Lancaster (Paice/Husk) stemmer. 5 Both are implemented/wrapped in Python by NLTK. 6 StopWords Finally, it’s also common to remove high-frequency but low-content words, like (Manning...|$|E
40|$|Abstract. The article {{presents}} the experiments carried out {{as part of}} the participation in the Tweet Contextualization (TC) track of INEX 2012. We have submitted three runs. The INEX TC task has two main sub tasks, Focused IR and Automatic Summarization. In the Focused IR system, we first preprocess the Wikipedia documents and then index them using Nutch with NE field. Stop words are removed and all NEs are tagged from each query tweet and all the remaining tweet words are stemmed using <b>Porter</b> <b>stemmer.</b> The stemmed tweet words form the query for retrieving the most relevant document using the index. The automatic summarization system takes as input the query tweet along with the title from the most relevant text document. Most relevant sentences are retrieved from the associated document based on the TF-IDF of the matching query tweet, NEs text and title words. Each retrieved sentence is assigned a ranking score in the Automatic Summarization system. The answer passage includes the top ranked retrieved sentences with a limit of 500 words. The three unique runs differ {{in the way in which}} the relevant sentences are retrieved from the associated document...|$|E
40|$|Now a days, {{the text}} {{document}} is spontaneously increasing over the internet, e-mail and web pages {{and they are}} stored in the electronic database format. To arrange and browse the document it becomes difficult. To overcome such problem the document preprocessing, term selection, attribute reduction and maintaining {{the relationship between the}} important terms using background knowledge, WordNet, becomes an important parameters in data mining. In these paper the different stages are formed, firstly the document preprocessing is done by removing stop words, stemming is performed using <b>porter</b> <b>stemmer</b> algorithm, word net thesaurus is applied for maintaining relationship between the important terms, global unique words, and frequent word sets get generated, Secondly, data matrix is formed, and thirdly terms are extracted from the documents by using term selection approaches tf-idf, tf-df, and tf 2 based on their minimum threshold value. Further each and every document terms gets preprocessed, where the frequency of each term within the document is counted for representation. The purpose of this approach is to reduce the attributes and find the effective term selection method using WordNet for better clustering accuracy. Experiments are evaluated on Reuters Transcription Subsets, wheat, trade, money grain, and ship, Reuters 21578, Classic 30, 20 News group (atheism), 20 News group (Hardware), 20 News group (Computer Graphics) etc...|$|E
