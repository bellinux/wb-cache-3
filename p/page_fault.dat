247|232|Public
25|$|When the kernel detects a <b>page</b> <b>fault</b> it {{generally}} adjusts the virtual memory {{range of the}} program which triggered it, granting it access to the memory requested. This gives the kernel discretionary power over where a particular application's memory is stored, or even {{whether or not it}} has actually been allocated yet.|$|E
25|$|If {{a program}} tries to access memory that isn't {{in its current}} range of {{accessible}} memory, but nonetheless has been allocated to it, the kernel is interrupted {{in the same way}} as it would if the program were to exceed its allocated memory. (See section on memory management.) Under UNIX this kind of interrupt is referred to as a <b>page</b> <b>fault.</b>|$|E
25|$|In some cases, a <b>page</b> <b>fault</b> may {{indicate}} a software bug, {{which can be}} prevented by using memory protection as one of key benefits of an MMU: an operating system {{can use it to}} protect against errant programs by disallowing access to memory that a particular program should not have access to. Typically, an operating system assigns each program its own virtual address space.|$|E
50|$|<b>Page</b> <b>faults,</b> {{by their}} very nature, degrade the {{performance}} of a program or operating system and in the degenerate case can cause thrashing. Optimization of programs and the operating system that reduce the number of <b>page</b> <b>faults</b> improve the performance of the program or even the entire system. The two primary focuses of the optimization effort are reducing overall memory usage and improving memory locality. To reduce the <b>page</b> <b>faults</b> in the system, programmers must make use of an appropriate page replacement algorithm that suits the current requirements and maximizes the page hits. Many have been proposed, such as implementing heuristic algorithms to reduce the incidence of <b>page</b> <b>faults.</b> Generally, making more physical memory available also reduces <b>page</b> <b>faults.</b>|$|R
40|$|A {{key issue}} for Cluster-enabled OpenMP implemen-tations based on {{software}} Distributed Shared Memory (sDSM) systems, is maintaining {{the consistency of}} the shared memory space. This forms the major source of over-head for these systems, and is driven by the detection and servicing of <b>page</b> <b>faults.</b> This paper investigates how appli-cation performance can be modelled {{based on the number of}} <b>page</b> <b>faults.</b> Two simple models are proposed, one based on the number of <b>page</b> <b>faults</b> along the critical path of the computation, and one based on the aggregated numbers of <b>page</b> <b>faults.</b> Two different sDSM systems are considered. The models are evaluated using the OpenMP NAS Paral-lel Benchmarks on an 8 -node AMD-based Gigabit Ether-net cluster. Both models gave estimates accurate to within 10 % in most cases, with the critical path model showing slightly better accuracy; accuracy is lost if the underly-ing <b>page</b> <b>faults</b> cannot be overlapped, or if the application makes extensive use of the OpenMP flush directive. 1...|$|R
50|$|Thrashing {{which may}} occur due to {{repeated}} <b>page</b> <b>faults.</b>|$|R
25|$|One more {{issue is}} that some complex {{instructions}} are difficult to restart, e.g. following a <b>page</b> <b>fault.</b> In some cases, restarting from the beginning will work (although wasteful), {{but in many cases}} this would give incorrect results. Therefore, the machine needs to have some hidden state to remember which parts went through and what remains to be done. With a load/store machine, the program counter is sufficient to describe the state of the machine.|$|E
2500|$|Sometimes, a PTE prohibits {{access to}} a virtual page, perhaps because no {{physical}} random access memory has been allocated to that virtual page. [...] In this case, the MMU signals a <b>page</b> <b>fault</b> to the CPU. The operating system (OS) then handles the situation, perhaps by {{trying to find a}} spare frame of RAM and set up a new PTE to map it to the requested virtual address. [...] If no RAM is free, {{it may be necessary to}} choose an existing page (known as a [...] "victim"), using some replacement algorithm, and save it to disk (a process called [...] "paging"). With some MMUs, there can also be a shortage of PTEs, in which case the OS will have to free one for the new mapping.|$|E
50|$|Contains a value called <b>Page</b> <b>Fault</b> Linear Address (PFLA). When a <b>page</b> <b>fault</b> occurs, {{the address}} the program {{attempted}} to access {{is stored in}} the CR2 register.|$|E
5000|$|When {{the working}} set {{is a small}} {{percentage}} of the system's total number of pages, virtual memory systems work most efficiently and an insignificant amount of computing is spent resolving <b>page</b> <b>faults.</b> As the working set grows, resolving <b>page</b> <b>faults</b> remains manageable until the growth reaches a critical point. Then faults go up dramatically and the time spent resolving them overwhelms time spent on the computing the program was written to do. This condition is referred to as thrashing. Thrashing occurs on a program that works with huge data structures, as its large working set causes continual <b>page</b> <b>faults</b> that drastically slow down the system. Satisfying <b>page</b> <b>faults</b> may require freeing pages that will soon have to be re-read from disk. [...] "Thrashing" [...] is also used in contexts other than virtual memory systems; for example, to describe cache issues in computing or silly window syndrome in networking.|$|R
5000|$|GPU sends {{interrupt}} {{requests to}} CPU on various events (such as <b>page</b> <b>faults)</b> ...|$|R
40|$|Protected module architectures, such as Intel SGX, enable strong trusted {{computing}} {{guarantees for}} hardware-enforced enclaves on top a potentially malicious operating system. However, such enclaved execution environments {{are known to}} be vulnerable to a powerful class of controlled-channel attacks. Recent research convincingly demonstrated that adversarial system software can extract sensitive data from enclaved applications by carefully revoking access rights on enclave pages, and recording the associated <b>page</b> <b>faults.</b> As a response, a number of state-of-the-art defense techniques has been proposed that suppress <b>page</b> <b>faults</b> during enclave execution. This paper shows, however, that page table-based threats go beyond <b>page</b> <b>faults.</b> We demonstrate that an untrusted operating system can observe enclave page accesses without resorting to <b>page</b> <b>faults,</b> by exploiting other side-effects of the address translation process. We contribute two novel attack vectors that infer enclaved memory accesses from page table attributes, {{as well as from the}} caching behavior of unprotected page table memory. We demonstrate the effectiveness of our attacks by recovering EdDSA session keys with little to no noise from the popular Libgcrypt cryptographic software suite. status: accepte...|$|R
50|$|The dual 68000 {{processor}} {{design was}} to provide automatic <b>page</b> <b>fault</b> switching, with one processor acting as a watchdog, while the other executed the OS and program instructions. When a <b>page</b> <b>fault</b> was raised, the main CPU was halted in mid (memory) cycle while the watchdog CPU would bring the page into memory and then allow the main CPU to continue, unaware of the <b>page</b> <b>fault.</b> Later improvements in the Motorola 68010 processor obviated {{the need for the}} dual processor design.|$|E
50|$|In a load-store architecture, {{instructions}} that might possibly cause a <b>page</b> <b>fault</b> are idempotent. So if a <b>page</b> <b>fault</b> occurs, the OS can load the page from disk and then simply re-execute the faulted instruction. In a processor where such instructions are not idempotent, dealing with page faults {{is much more}} complex.|$|E
50|$|If a <b>page</b> <b>fault</b> occurs for a {{reference}} to an address that {{is not part of}} the virtual address space, meaning there cannot be a page in memory corresponding to it, then it is called an invalid <b>page</b> <b>fault.</b> The <b>page</b> <b>fault</b> handler in the operating system will then generally pass a segmentation fault to the offending process, indicating that the access was invalid; this usually results in abnormal termination of the code that made the invalid reference. A null pointer is usually represented as a pointer to address 0 in the address space; many operating systems set up the memory management unit to indicate that the page that contains that address is not in memory, and do not include that page in the virtual address space, so that attempts to read or write the memory referenced by a null pointer get an invalid <b>page</b> <b>fault.</b>|$|E
40|$|In this paper, we {{analyze the}} {{performance}} of parallel multithreaded algorithms that use dag-consistent distributed shared memory. Specifically, we analyze execution time, <b>page</b> <b>faults,</b> and space requirements for multithreaded algorithms executed by a FP(C) workstealing thread scheduler and the BACKER algorithm for maintaining dag consistency. We prove that if the accesses to the backing store are random and independent (the BACKER algorithm actually uses hashing), the expected execution time TP(C) of a “fully strict” multithreaded computation on P processors, each with a LRU cache of C pages, is O(T 1 (C) =P+mCT∞), where T 1 (C) is the total work of the computation including <b>page</b> <b>faults,</b> T ∞ is its critical-path length excluding <b>page</b> <b>faults,</b> and m is the minimum page transfer time. A...|$|R
5000|$|Abort (ABORTB) {{input and}} {{associated}} vector supports processor repairs of bus error conditions, such as <b>page</b> <b>faults</b> and memory access violations.|$|R
40|$|Virtual {{memory of}} {{computers}} is usually implemented by demand paging. For some page replacement algorithms {{the number of}} <b>page</b> <b>faults</b> may increase {{as the number of}} page frames increases. Belady, Nelson and Shedler constructed reference strings for which page replacement algorithm FIFO produces near twice more <b>page</b> <b>faults</b> in a larger memory than in a smaller one. They formulated the conjecture that 2 is a general bound. We prove that this ratio can be arbitrarily large...|$|R
50|$|A <b>page</b> <b>fault</b> may not {{necessarily}} indicate an error. Page faults are not only used for memory protection. The operating system may manage the page table {{in such a way}} that a reference to a page that has been previously swapped out to disk causes a <b>page</b> <b>fault.</b> The operating system intercepts the <b>page</b> <b>fault,</b> loads the required memory page, and the application continues as if no fault had occurred. This scheme, known as virtual memory, allows in-memory data not currently in use to be moved to disk storage and back in a way which is transparent to applications, to increase overall memory capacity.|$|E
50|$|The {{flowchart}} provided {{explains the}} working of a TLB. If it is a TLB miss, then the CPU checks the page {{table for the}} page table entry. If the ‘present bit’ is set, then the page is in main memory, and the processor can retrieve the frame number from the page table entry to form the physical address. The processor also updates the TLB to include the new page table entry. Finally, if the present bit is not set, then the desired page {{is not in the}} main memory and a <b>page</b> <b>fault</b> is issued. Then a <b>page</b> <b>fault</b> interrupt is called which executes the <b>page</b> <b>fault</b> handling routine.|$|E
5000|$|Return {{control to}} the program, transparently {{retrying}} the instruction that caused the <b>page</b> <b>fault.</b>|$|E
40|$|Cross {{correlation}} {{is widely}} used for image matching. The Fast Fourier Transform(FFT) and the Fast Hartley Transform(FHT) are used to speed up cross correlation computation. Hartley transform[1] is specifically designed for real-valued data. FHT is especially useful on small-memory machines as it is its own inverse. However, cross correlation based on FHT suffers heavily as compared to FFT, in Paged Operating Systems(OS), as it requires more page frames {{to be present in}} the physical memory at any time [1]. Thus chances for <b>page</b> <b>faults</b> in a high memory demand environment increase, thereby worsening the computation times. In this paper,we use the concept of page locking [4] to minimize the number of <b>page</b> <b>faults</b> in both FFT and FHT based cross correlation computation. We also combine several consecutive steps in FHT based method to further reduce the number of <b>page</b> <b>faults</b> by half...|$|R
50|$|An {{algorithm}} is conservative, if on any consecutive request sequence containing k or fewer distinct page references, the algorithm will incur k or fewer <b>page</b> <b>faults.</b>|$|R
50|$|The not {{frequently}} used page-replacement algorithm generates fewer <b>page</b> <b>faults</b> than the least recently used page replacement algorithm when the page table contains null pointer values.|$|R
50|$|In {{searching for}} a mapping, the hash anchor table is used. If no entry exists, a <b>page</b> <b>fault</b> occurs. Otherwise, the entry is found. Depending on the architecture, the entry may {{be placed in the}} TLB again and the memory {{reference}} is restarted, or the collision chain may be followed until it has been exhausted and a <b>page</b> <b>fault</b> occurs.|$|E
50|$|This {{part of the}} {{operating}} system creates and manages page tables. If the hardware raises a <b>page</b> <b>fault</b> exception, the paging supervisor accesses secondary storage, returns the page that has the virtual address {{that resulted in the}} <b>page</b> <b>fault,</b> updates the page tables to reflect the physical location of the virtual address and tells the translation mechanism to restart the request.|$|E
50|$|On some systems, the <b>page</b> <b>fault</b> {{mechanism}} {{is also used}} for executable space protection such as W^X.|$|E
40|$|Software {{developers}} commonly exploit multicore processors {{by building}} multithreaded software {{in which all}} threads of an application share a single address space. This shared address space has a cost: kernel virtual memory operations such as handling soft <b>page</b> <b>faults,</b> growing the address space, mapping files, etc. can limit the scalability of these applications. In widely-used operating systems, all of these operations are synchronized by a single per-process lock. This paper contributes a new design for increasing the concurrency of kernel operations on a shared address space by exploiting read-copy-update (RCU) so that soft <b>page</b> <b>faults</b> can both run in parallel with operations that mutate the same address space and avoid contending with other <b>page</b> <b>faults</b> on shared cache lines. To enable such parallelism, this paper also introduces an RCU-based binary balanced tree for storing memory mappings. An experimental evaluation using three multithreaded applications shows performance improvements on 80 cores ranging from 1. 7 × to 3. 4 × for an implementation of this design in the Linux 2. 6. 37 kernel. The RCU-based binary tree enables soft <b>page</b> <b>faults</b> to run at a constant cost with {{an increasing number of}} cores, suggesting that the design will scale well beyond 80 cores...|$|R
50|$|Another {{example is}} used by the Linux kernel on ARM. The lack of {{hardware}} functionality is made up for by providing two page tables - the processor-native page tables, with neither referenced bits nor dirty bits, and software-maintained page tables with the required bits present. The emulated bits in the software-maintained table are set by <b>page</b> <b>faults.</b> In order to get the <b>page</b> <b>faults,</b> clearing emulated bits in the second table revokes some of the access rights to the corresponding page, which is implemented by altering the native table.|$|R
40|$|In this paper, {{we present}} the {{run-time}} behavior of contour tracing algorithms for processing large documents, in a contemporary workstation environment. The performance criteria examined are processing time, memory usage, {{and the number}} of <b>page</b> <b>faults.</b> A new contour tracing algorithm based on Capson's algorithm is proposed and it is compared with other algorithms. It is observed that for the set of document images under test, the new algorithm consumes the least amount of memory, has the smallest number of <b>page</b> <b>faults</b> and is slightly faster than Capson's algorithm. ...|$|R
50|$|The {{first model}} was the DN416 workstation, later {{referred}} to as the DN100 after the green screen was substituted with a black and white screen. This system used two 68000 processors and implemented virtual memory (which the 68000 wasn't theoretically capable of) by stopping one processor when there was a <b>page</b> <b>fault</b> and having the other processor handle the fault, then release the primary processor when the <b>page</b> <b>fault</b> was handled.|$|E
50|$|When the SMAP bit in CR4 is set, {{explicit}} memory reads and writes to user-mode pages performed by code running with a privilege level less than 3 will always {{result in a}} <b>page</b> <b>fault</b> if the EFLAGS.AC flag is not set. Implicit reads and writes (such as those made to descriptor tables) to user-mode pages will always trigger a <b>page</b> <b>fault</b> if SMAP is enabled, regardless {{of the value of}} EFLAGS.AC.|$|E
5000|$|... #Caption: Flowchart [...] {{shows the}} working of a Translation Lookaside Buffer. For simplicity, the <b>page</b> <b>fault</b> routine is not mentioned.|$|E
40|$|Modern {{operating}} systems use virtual memory concept {{because of its}} advantages but they use different page replacement techniques. An efficient page replacement technique is required so as to produce minimum number of <b>page</b> <b>faults.</b> Some of the page replacement techniques are FIFO, LRU, OPTIMAL etc. Optimal has been proven to be best producing minimum number of <b>page</b> <b>faults.</b> LRU approximates optimal. Considerable {{research has been done}} to evaluate theses policies and to develop new ones based on recency, frequency, token, and locality model parameters etc. This paper uses a histogram based approach to compare FIFO, LRU, LRU 2, OPTIMAL policies. Simulation results show that histograms for all policies equalize as the number of frames increases. Also histogram for optimal policy equalizes more rapidly then other policy’s histograms. Also pages of large frequency of occurrences contribute much to the total number of <b>page</b> <b>faults</b> in both LRU and optimal page replacement algorithms. General Terms Operating system, virtual replacement techniques. memory, simulation of pag...|$|R
50|$|These prefetches are {{non-blocking}} memory operations, i.e. these {{memory accesses}} do {{not interfere with}} actual memory accesses. They do not change {{the state of the}} processor or cause <b>page</b> <b>faults.</b>|$|R
30|$|The post-copy {{technique}} is effective when {{the majority of}} pages are transferred to target server before page faulty occur at destination VM and minor <b>page</b> <b>faults</b> occur due to network faults.|$|R
