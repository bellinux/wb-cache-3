4|9|Public
50|$|The data {{should be}} {{continually}} kept logically segregated across all MDB files, with file naming convention used to allow an ODBC-enabled application program to perform random {{access to a}} single MDB file to locate a particular record or group of records. Segregation of data in this way also makes sequential access more efficient. It may even make the restoring of a <b>partial</b> <b>backup</b> faster if the problem can be isolated to 1 or a few MDB files.|$|E
30|$|The {{supplier}} {{data interface}} (c_ 5) ranks third, {{and before the}} SCADA modules (fourth and fifth, respectively), although the supplier data interface has a lower interference degree than the SCADA modules. This {{can be explained by}} the impact location of the failing IT components. The supplier data interface influences the first process step, in contrast to the SCADA modules, which influence later process steps. Therefore, an interesting insight is that the impact location in the production network is an important factor because the supplier data interface’s restriction causes production failures in all subsequent process steps of our smart factory example. Further, the SCADA module for the sewing machines has a <b>partial</b> <b>backup,</b> which reduces its threat potential.|$|E
40|$|In this study, {{we present}} a series of {{well-known}} optimization methods to address two related decisions associated with the design of large-scale ambulance operations on highways: (1) The question of location, and (2) the issue of districting. As a result of computer storage and runtime constraints, previous approaches have only considered small-to-moderate scale problem scenarios, generally employing exact hypercube queuing models integrated into optimization procedures. We overcome these limitations here by embedding a fast and accurate hypercube approximation algorithm adapted for <b>partial</b> <b>backup</b> dispatch policies in single- and multi-start greedy heuristics. The proposed methods are tested on small-to-large-scale problems involving up to 100 ambulances. The results suggest that our approach is a viable alternative for the analysis and configuration of large-scale highway emergency medical systems, providing reasonable accuracy and affordable run times. Emergency medical systems Ambulance deployment Approximate hypercube queuing model Multi-start greedy heuristic Probabilistic location and districting problems...|$|E
50|$|This {{procedure}} involves taking complete backups of {{all data}} {{at regular intervals}} daily, weekly, monthly, or whatever is appropriate. Multiple generations of backup are retained, often three which {{gives rise to the}} name. The most recent backup is the son, the previous the father, and the oldest backup is the grandfather. This method is commonly used for a batch transaction processing system with a magnetic tape. If the system fails during a batch run, the master file is recreated by restoring the son backup and then restarting the batch. However, if the son backup fails, is corrupted or destroyed, then the previous generation of backup (the father) is used. Likewise, if that fails, then the generation of backup previous to the father (i.e. the grandfather) is required. Of course the older the generation, the more the data may be out of date. Organize only of records that have changed. For example, a full backup could be performed weekly, and <b>partial</b> <b>backups</b> taken nightly. Recovery using this scheme involves restoring the last full backup and then restoring all <b>partial</b> <b>backups</b> in order to produce an up-to-date database. This process is quicker than taking only complete backups, at the expense of longer recovery time.|$|R
5000|$|Files are {{regularly}} backed up to tape {{unless they have}} been marked as NOSAVE. The file save process includes full and <b>partial</b> <b>backups.</b> Full saves are typically done once a week with no users {{signed on to the}} system. Partial saves save just the files that have changed since the last full or partial save and are typically done once each day in the late evening or early morning during normal operation with users signed on to the system. At the University of Michigan two copies of the full save tapes were made and one copy was stored [...] "off-site". Save tapes were kept for six weeks and then reused. The tapes from every sixth full save were kept [...] "forever". Files are saved to allow recovery from [...] "disk disasters" [...] in which the file system becomes damaged or corrupt, usually due to a hardware failure. But users could restore individual files as well using the program *RESTORE.|$|R
50|$|In 2004, {{after two}} years away from the game, the Chicago Bears became the seventh NFL team to employ George, signing him to a one-year {{contract}} in November for a <b>partial</b> season <b>backup</b> role, but he never took the field during a game, and was not retained by the Bears for the 2005 season, and was not signed by any team. The Detroit Lions worked him out during their bye week in the event they needed another quarterback; however, George was not offered a contract.|$|R
3000|$|... (iii) After {{defining}} the setup, the actual orbit parameter estimation will be done. For Swarm, a reduced-dynamic orbit determination approach will be adopted using the GHOST software for providing high-precision time series of positions and velocities (Montenbruck et al., 2005). The GHOST software is developed under {{auspices of the}} DLR German Space Operations Centre together with the TU Delft. The kinematic orbit solution will also be provided as an additional product. This kinematic orbit solution {{can be considered as}} a condensed set of the original GPS SST observations and it will serve as pseudo observations for a dynamic orbit determination that is done with the purpose of providing accelerometer calibration parameters (needed by the TDW chain, Subsection 3.2). In this dynamic orbit determination, the non-gravitational accelerations are represented by the accelerometer observations. Typically, use is made of daily arcs where the estimated parameters are the begin position and velocity of the satellite, and one bias and scale factor for each accelerometer axis (Visser and van den IJssel, 2003). Especially for the flight direction, very precise estimates can be obtained for accelerometer bias and scale factor (Helleputte and Visser, 2009). In fact, only the X-axis calibration parameters are retained for further use (Subsection 3.2). This dynamic orbit determination is done with the GEODYN parameter estimation software, kindly provided by the NASA Goddard Space Flight Center (Pavlis et al., 2006). A highly reduced-dynamic orbit determination will be done as well for obtaining time series of non-gravitational accelerations (van den IJssel and Visser, 2004 a, b). These time series are used for validation of the accelerometer observations and also serve as a (<b>partial)</b> <b>backup</b> for the accelerometers in case of failure/problems. Thus, several orbit estimation runs will be carried out for each Swarm satellite. It has to be noted that the estimated time series of non-gravitational accelerations from the highly reduced-dynamic orbit solutions cover only the longer wavelengths or low frequencies of, for example, atmospheric drag perturbations. As such, they cannot replace the accelerometer observations for {{an important part of the}} frequency range.|$|E
40|$|Dynamic {{programming}} {{is a well-known}} approach for solving MDPs. In large state spaces, asynchronous versions like Real-Time Dynamic Programming (RTDP) have been applied successfully. If unfolded into equivalent trees, Monte-Carlo Tree Search algorithms are a valid alternative. UCT, the most popular representative, obtains good anytime behavior by guiding the search towards promising areas of the search tree and supporting non-admissible heuristics. The global Heuristic Search algorithm AO ∗ finds optimal solutions for MDPs that can be represented as acyclic AND/OR graphs. Despite the differences, these approaches actually have much in common. We present the Trial-based Heuristic Tree Search (THTS) framework that subsumes these approaches and distinguishes them based on only five ingredients: heuristic function, backup function, action selection, outcome selection, and trial length. We describe the ingredients that model RTDP, AO ∗ and UCT within this framework, and use THTS to combine attributes of these algorithms step by step in order to derive novel algorithms with superior theoretical properties. We merge Full Bellman and Monte-Carlo <b>backup</b> functions to <b>Partial</b> Bellman <b>backups,</b> and gain a function that both allows partial updates and a procedure that labels states when they are solved. DP-UCT combines attributes and theoretical properties from RTDP and UCT even though it differs from the latter only in the used <b>Partial</b> Bellman <b>backups.</b> Our main algorithm, UCT ∗ adds a limited trial length to DP-UCT to inherit the global search behavior of AO ∗, which ensures that {{parts of the state}} space that are closer to the root are investigated more thoroughly. The experimental evaluation shows that both DP-UCT and UCT ∗ are not only superior to UCT, but also outperform PROST, the winner of the International Probabilistic Planning Competition (IPPC) 2011 on the benchmarks of IPPC 2011. Keywords...|$|R
40|$|Abstract — The {{issue of}} {{handling}} network failures {{is becoming increasingly}} important. In this paper, we {{address the problem of}} constrained routing by treating reliability as one of the QoS requirements. The problem is to create a feasible path from a node to the destination such that the bandwidth and reliability requirements of the node are satisfied and the cost of the path is minimized (Reliability Constrained Least Cost Routing Problem). To solve the problem, we propose an approach which employs a novel concept, called <b>partial</b> protection, wherein <b>backup</b> paths are created for a selected set of domains in the network so as to meet the reliabilit...|$|R
40|$|Currents in {{the region}} of {{actively}} breaking waves (i. e., the surf zone) present hazards to recreation, transport sediment, and advect nearshore pollutants. While extensively studied with both Langrangian and Eulerian techniques during the last 60 years, many aspects of surfzone currents are still poorly understood, especially over irregular bathymetry. This work describes the design, development, and deployment of a new device for Lagrangian surfzone observations, a GPS-tracked surfzone drifter. The drifters are designed to observe the mean flow in presence of breaking waves, and to withstand harsh surfzone conditions. GPS positions are logged internally and transmitted to shore every 10 s, allowing real-time tracking and <b>partial</b> data <b>backup.</b> Tracking range is approximately 10 km, and battery life is 24 hr. Observations of flows in a frequently-occuring rip current and adjacent waters, made in July 2000, 2001, and 2002, suggest that the location of rip current cross-shore velocity maxima and extent are correlated to the surfzone width. Eddies located entirely withing the surfzone were observed to be bathymetrically-controlled products of the mean flow. Inter-annual flow variations were attributed to subtle variations in the bathymetry, and intra-day variations of two particular days appeared to result from tidal variations of 0. 5 m. Numerical solutions of the fully nonlinear 2 D Navier-Stokes equations successfully charaterized the flows. Simulated drifter releases also compared favorably to observations, although velocities differ by a factor of 2. Momentum balances near a rip current are discussed...|$|R
40|$|Remote backup systems provide {{database}} availability even in case {{of disasters}} that cause complete database failure. In a remote backup system a backup tracks the transaction processing at the primary and {{in the event of}} the primary failure takes over transaction processing without causing users to observe a breach in service. Remote backup algorithms have been developed for a variety of system architectures including the case in which primary and backup consist of multiple processors connected via multiple communication lines. A limitation of existing algorithms for such environments is that they ignore the issue of <b>partial</b> primary and <b>backup</b> failures in which {{one or more of the}} primary#backup processors fail but the system as a whole survives. This paper describes the design of a primary backup system which uses the backup to provide continued availability not only during disasters but also during partial failures. The key to the approach is a non-blocking two-phase commit protocol ad [...] ...|$|R
40|$|The {{issue of}} {{providing}} reliable QoS multicasting {{to cope with}} node/link failures is becoming increasingly important. In this paper, we {{address the problem of}} dynamic multicast routing by treating reliability as one of the QoS requirements. The problem is to create a feasible path from a receiver (who wishes to join the multicast group) to the Core of the multicast group such that the bandwidth and reliability requirements of the receiver are satisfied and the cost of the path is minimized (Reliability Constrained Least Cost Dynamic Multicast Routing Problem). To solve the problem, we propose an approach which employs a novel concept, called <b>partial</b> protection, wherein <b>backup</b> paths are created for a selected set of domains in the network so as to meet the reliability constraints. The Partial Protection Approach (PPA) has two steps: (i) Primary Path Creation and (ii) Backup Path Creation if necessary. To implement PPA, we propose three scalable two-pass resource reservation schemes, viz., Conservative, Optimistic and Hybrid schemes. These schemes differ depending on whether the backup paths are created in forward pass, reverse pass, or both. We evaluate the performance of the proposed schemes for dynamic groups with different bandwidth and reliability requirements using average call acceptance rate and average tree cost as performance metrics. Our studies show that group dynamics and reliability requirements of the receivers have significant impact on the performance of the schemes...|$|R
40|$|Finding {{an optimal}} {{solution}} to the problem of fast and efficient provisioning of reliable connections and failure recovery in future intelligent optical networks is an ongoing challenge. In this dissertation, we investigate and compare the performance of an adapted shared-path protection algorithm with a more conventional approach; both designed for survivable optical Wavelength Division Multiplexing (WDM) mesh networks. The effect of different classes of service on performance is also investigated. Dedicated path protection is a proactive scheme which reserves spare resources to combat single link failures. Conventional Shared-path Protection (CSP) is desirable due to the efficient utilization of resources which results from the sharing of backup paths. Availability is an important performance assessment factor which measures the probability that a connection is in an operational state at some point in time. It is the instantaneous counterpart of reliability. Therefore, connections that do not meet their availability requirements are considered to be unreliable. Reliability Aware Shared-path Protection (RASP) adopts the advantages of CSP by provisioning reliable connections efficiently, but provides protection for unreliable connections only. With the use of a link disjoint parameter, RASP also permits the routing of <b>partial</b> link disjoint <b>backup</b> paths. A simulation study, which evaluates four performance parameters, is undertaken using a South African mesh network. The parameters that are investigated are: 1. Blocking Probability (BP), which considers the percentage of connection requests that are blocked, 2. Backup Success Ratio (BSR), which considers the number of connections that are successfully provisioned with a backup protection path, 3. Backup Primary Resource Ratio (BPR), which considers the ratio of resources utilized to cater for working traffic to the resources reserved for protection paths and lastly 4. Reliability Satisfaction Ratio (RSR), which evaluates the ratio of provisioned connections that meet their availability requirements to the total number of provisioned connections. Under dynamic traffic conditions with varying network load, simulation results show that RASP can provision reliable connections and satisfy Service Level Agreement (SLA) requirements. A competitive Blocking Probability (BP) and lower Backup Primary Resource Ratio (BPR) signify an improvement in resource utilization efficiency. A higher Backup Success Ratio (BSR) was also achieved under high Quality of Service (QoS) constraints. The significance of different availability requirements is evaluated by creating three categories, high availability, medium availability and low availability. These three categories represent three classes of service, with availability used as the QoS parameter. Within each class, the performance of RASP and CSP is observed and analyzed, using the parameters described above. Results show that both the BP and BPR increase with an increase in the availability requirements. The RSR decreases as the reliability requirements increase and a variation in BSR is also indicated. Dissertation (MEng) [...] University of Pretoria, 2009. Electrical, Electronic and Computer Engineeringunrestricte...|$|R

