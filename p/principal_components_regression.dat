165|10000|Public
40|$|In {{this paper}} we will {{classify}} patterns using an algorithm {{analogous to the}} k-means algorithm and the <b>principal</b> <b>components</b> <b>regression</b> (PCR). We will also present a financial application in which we apply PCR if the points represent the interests for accounts with different terms. <b>Principal</b> <b>components</b> <b>regression,</b> pattern classification, k-means...|$|E
40|$|Batah et al. (2009) {{combined}} the unbiased ridge estimator and <b>principal</b> <b>components</b> <b>regression</b> estimator and introduced the modified r-k class estimator. They {{also showed that}} the modified r-k class estimator is superior to the ordinary least squares estimator and <b>principal</b> <b>components</b> <b>regression</b> estimator in the mean squared error matrix. In this paper, firstly, we will give a new method to obtain the modified r-k class estimator; secondly, we will discuss its properties in some detail, comparing the modified r-k class estimator to the ordinary least squares estimator and <b>principal</b> <b>components</b> <b>regression</b> estimator under the Pitman closeness criterion. A numerical example and a simulation study are given to illustrate our findings...|$|E
40|$|Assuming {{violation}} multicollinearity {{in classical}} regression analysis can cause estimator resulting from classical model regression inefficient. <b>Principal</b> <b>components</b> <b>regression</b> and ridge regression are the methods {{that can be}} used to overcome the problem of multicollinearity. This research aimed to compare between the <b>principal</b> <b>components</b> <b>regression</b> with ridge regression to tackle the problem of multicollinearity in the analysis of the factors that affect revenue (PAD) of the Central Java province. The data used in this research are data revenue (PAD), and factors that affect the region, such as local tax, retribution, Gross Regional Domestic Products (GRDP) at current prices, Gross Regional Domestic Products (GRDP at constant prices, population, regional spending. Based on the coefficient of determination value and test on individual regression coefficients, the value of variance inflation factor and correlations sufficiently high among some independent variables so we can conclude the existence of a violation of multicollinearity on analysis factors PAD. Based on standard error resulting from <b>principal</b> <b>components</b> <b>regression</b> and ridge regression show that <b>principal</b> <b>components</b> <b>regression</b> results in a standard smaller error. This shows that principal component regression is better than ridge regression in solving the problem multicollinearity on analysis of factors that affects pad province of central java. Keywords: Multicolinearity, revenue (PAD), Principal Component Regression, Ridge Regression, standard error...|$|E
5000|$|<b>Principal</b> <b>component</b> {{analysis}} (PCA) and <b>principal</b> <b>component</b> <b>regression</b> ...|$|R
40|$|Advances in data {{collection}} and storage have tremendously increased the presence of functional data, whose graphical representations are curves, images or shapes. As a new area of Statistics, functional data analysis extends existing methodologies and theories from the fields of functional analysis, generalized linear models, multivariate data analysis, nonparametric statistics and many others. This paper provides a review into functional data analysis with main emphasis on functional <b>principal</b> <b>component</b> analysis, functional <b>principal</b> <b>component</b> <b>regression,</b> and bootstrap in functional <b>principal</b> <b>component</b> <b>regression.</b> Recent trends as well as open problems {{in the area are}} discussed. Bootstrap, functional <b>principal</b> <b>component</b> <b>regression,</b> functional time series, Stiefel manifold, Von Mise-Fisher distribution. ...|$|R
5000|$|Ridge <b>{{regression}}</b> or <b>principal</b> <b>component</b> <b>regression</b> {{or partial}} least squares regression can be used.|$|R
40|$|One of the {{problems}} that has plagued researchers in their estimation of reduced-form price equations for specific housing markets has been multicollinearity-the lack of statistical independence of the explanatory variables. This paper evaluates the suitability for structural analysis and prediction of stepwise regression and <b>principal</b> <b>components</b> <b>regression</b> as alternatives to the standard regression model in the estimation of equations with interdependent data. In general, {{the results of this study}} indicate that under conditions of multicollinearity <b>principal</b> <b>components</b> <b>regression</b> is the superior estimation technique. Copyright American Real Estate and Urban Economics Association. ...|$|E
40|$|The {{ordinary}} least squares, the <b>principal</b> <b>components</b> <b>regression</b> and {{the ordinary}} ridge regression estimators are special {{cases of the}} r - k class estimator proposed by Baye and Parker (1984) for regression models with multicollinearity. We obtain necessary and sufficient conditions for {{the superiority of the}} r - k class estimator over each of these three estimators by the criterion of mean square error matrix. We also suggest tests to verify if these conditions are indeed satisfied. Mean square error matrix Multicollinearity Ordinary ridge regression estimator <b>Principal</b> <b>components</b> <b>regression</b> estimator r - k class estimator...|$|E
40|$|The {{existing}} {{theory of}} subset regression is examined, {{taking into account}} optimality criteria, small experiments, nonlinear models, colinearities, and special techniques. Approaches based on chain pooling coupled with <b>principal</b> <b>components</b> <b>regression</b> are discussed, giving attention to a comparison of half-normal plotting with chain pooling, a procedure based on prior ordering, deletion under the F-test, the largest {{of a set of}} chi-square variates, and <b>principal</b> <b>components</b> <b>regression</b> and model deletion. The choice of a true (population) model for simulations is considered along with the evaluation of the decision procedure and suitable computer programs...|$|E
50|$|Eslamian, S. S., Ghasemizadeh, M., Biabanaki, M. and M. Talebizadeh, 2010, A <b>principal</b> <b>component</b> <b>regression</b> {{method for}} {{estimating}} low flow index, Water Resources Management, Vol. 24, No. 11, 2553-2566.|$|R
5000|$|Heij, Christiaan, Patrick JF Groenen, and Dick van Dijk. [...] "Forecast {{comparison}} of <b>principal</b> <b>component</b> <b>regression</b> and <b>principal</b> covariate regression." [...] Computational statistics & data analysis 51.7 (2007): 3612-3625.|$|R
40|$|Abstract: This paper uses a {{predictive}} regression {{framework to}} examine the out-of-sample predictability of South Africa’s equity premium, using a host of financial and macroeconomic variables. We employ various methods of forecast combination, bootstrap aggregation (bagging), diffusion index (<b>principal</b> <b>component)</b> and Bayesian <b>regressions</b> {{to allow for a}} simultaneous role of the variables under consideration, besides individual predictive regressions. We assess both the statistical and economic significance of the individual predictive regressions, combination methods, bagging, <b>principal</b> <b>components</b> and Bayesian <b>regressions.</b> Our results show that forecast combination methods and <b>principal</b> <b>component</b> <b>regressions</b> improve the predictability of the equity premium relative to the benchmark autoregressive model of order one (AR(1)). However, the Bayesian predictive regressions are found to be the standout performers with the models outperforming the individual regressions, forecast combination methods, bagging and <b>principal</b> <b>component</b> <b>regressions,</b> both in terms of statistical (forecasting) and economic (utility) gains...|$|R
40|$|KaçIranlar, and SakallIoglu, [2001. Combining the Liu {{estimator}} and {{the principal}} component regression estimator. Comm. Statist. Theory Methods 30, 2699 - 2705] introduced the r-d class estimator which is a general estimator of the ordinary least squares (OLS), the <b>principal</b> <b>components</b> <b>regression</b> (PCR) and the Liu estimators. In this paper, we derive conditions for {{the superiority of the}} r-d class estimator over each of these estimators and the r-k class estimator by the matrix mean square error (MMSE) criterion. Also, we suggest tests to verify if these conditions are indeed satisfied. r-d class estimator Ordinary least squares estimator <b>Principal</b> <b>components</b> <b>regression</b> estimator Liu estimator r-k class estimator Multicollinearity...|$|E
40|$|This report {{includes}} {{the details of}} the model building procedure and prediction of seismic field data. <b>Principal</b> <b>Components</b> <b>Regression,</b> a multivariate analysis technique, was used to model seismic data collected as two pieces of equipment were cycled on and off. Models built that included only the two pieces of equipment of interest had trouble predicting data containing signals not included in the model. Evidence for poor predictions came from the prediction curves as well as spectral F-ratio plots. Once the extraneous signals were included in the model, predictions improved dramatically. While <b>Principal</b> <b>Components</b> <b>Regression</b> performed well for the present data sets, the present data analysis suggests further work will be needed to develop more robust modeling methods as the data become more complex...|$|E
40|$|An {{optimization}} problem {{which provides a}} new characterization for ridge regression is discussed. A variant of this {{optimization problem}} leads to a new family of biased estimators that includes the Stein estimation method and <b>principal</b> <b>components</b> <b>regression</b> as particular cases. The whole approach is illustrated {{on the basis of}} real data sets. ...|$|E
40|$|<b>Principal</b> <b>component</b> <b>regression</b> (PCR) is a {{two-stage}} {{procedure that}} selects some <b>principal</b> <b>components</b> and then constructs a regression model regarding them as new explanatory variables. Note that the <b>principal</b> <b>components</b> are obtained from only explanatory variables and not considered with the response variable. To address this problem, we propose the sparse <b>principal</b> <b>component</b> <b>regression</b> (SPCR) {{that is a}} one-stage procedure for PCR. SPCR enables us to adaptively obtain sparse <b>principal</b> <b>component</b> loadings {{that are related to}} the response variable and select the number of <b>principal</b> <b>components</b> simultaneously. SPCR can be obtained by the convex optimization problem for each of parameters with the coordinate descent algorithm. Monte Carlo simulations and real data analyses are performed to illustrate the effectiveness of SPCR. Comment: 24 page...|$|R
40|$|In recent years, many {{algorithms}} {{based on}} kernel <b>principal</b> <b>component</b> analysis (KPCA) {{have been proposed}} including kernel <b>principal</b> <b>component</b> <b>regression</b> (KPCR). KPCR {{can be viewed as}} a non-linearization of <b>principal</b> <b>component</b> <b>regression</b> (PCR) which uses the ordinary least squares (OLS) for estimating its regression coefficients. We use PCR to dispose the negative effects of multicollinearity in regression models. However, it is well known that the main disadvantage of OLS is its sensitiveness to the presence of outliers. Therefore, KPCR can be inappropriate to be used for data set containing outliers. In this paper, we propose a novel nonlinear robust technique using hybridization of KPCA and R-estimators. The proposed technique is compared to KPCR and gives better results than KPCR...|$|R
40|$|The aim of {{this thesis}} is to examine what drives {{the changes in the}} price of carbon credits in the European Union’s Emission Trading Scheme (EU ETS) and to make {{predictions}} based on these relationships. The study, which is based on the British energy market and global equity indices, starts with a large dataset which is reduced in dimension using correlation and <b>principal</b> <b>component</b> analysis. Predictions are then made by multiple linear <b>regression,</b> <b>principal</b> <b>component</b> <b>regression</b> and latent root regression. Correlation is the preferred dimension reduction technique to be followed by <b>principal</b> <b>component</b> <b>regression.</b> Certified emission reduction units (CERs) are shown to be the only same-day market relationship which provides useful predictions of European Union Allowance prices (EUAs) ...|$|R
40|$|When {{combining}} a set {{of learned}} models to form an improved estimator, the issue of redundancy in the set of models must be addressed. Existing methods for addressing this problem have failed to perform robustly, especially as the redundancy in the set of learned models increases. Recently, a variant of <b>principal</b> <b>components</b> <b>regression,</b> PCR*, demonstrated that these limitations could be overcome by mapping the original learned models to {{a set of}} principal components and then choosing which components to include in the final regression. Weights for the original learned models are then be derived from the weights of the <b>principal</b> <b>components</b> <b>regression.</b> The focus {{of this paper is to}} compare PCR*'s cross-validation-based stopping criteria for choosing the number of principal components to existing methods. Experimental results show that existing stopping criteria are often too conservative and discard useful components leading to poor performance. Introduction Combining a set of learned mo [...] ...|$|E
40|$|Abstract: Partial Least Squares {{regression}} and <b>Principal</b> <b>Components</b> <b>Regression</b> {{make possible}} to relate {{a set of}} dependant variables Y {{to a set of}} independent variables X, when there is multicollinearity. This paper suggests a new approach for analyzing the net interest margin. After using the PCR method, the determinants of the net interest margin have been viewed through a PLS model...|$|E
40|$|This paper {{suggests}} {{a new approach}} for analyzing operating costs. After using a <b>principal</b> <b>components</b> <b>regression</b> method, the criteria expressing the costs structuring have been viewed through a partial least squares regression model. We still confirm the impact of performance functions as the existence of unreducible overheads. bank;banking operating costs;input;output; productive performance;multiple linear regression; regression in principal components;partial least squares regression...|$|E
40|$|In this paper, we derive a <b>principal</b> <b>component</b> <b>regression</b> (PCR) {{method for}} {{estimating}} the optical flow between frames of video sequences {{according to a}} pel-recursive manner. This is an easy alternative to dealing with mixtures of motion vectors {{due to the lack}} of too much prior information on their statistics (although they are supposed to be normal). The 2 D motion vector estimation takes into consideration local image properties. The main advantage of the developed procedure is that no knowledge of the noise distribution is necessary. Preliminary experiments indicate that this approach provides robust estimates of the optical flow. KEY WORDS Motion estimation, <b>principal</b> <b>component</b> <b>regression,</b> and surveillance. 1...|$|R
40|$|<b>Principal</b> <b>Component</b> <b>Regression</b> (PCR) is {{one method}} to handle multicollinear problems. PCR {{produces}} <b>principal</b> <b>components</b> {{that have a}} VIF less than ten. The purpose for this research is to obtained PCR model using R software. The result {{is a model of}} PCR with two <b>principal</b> <b>components</b> and determination coefficients R(square) = 97, 27 %. </p...|$|R
40|$|<b>Principal</b> <b>Component</b> <b>Regression</b> is {{a method}} to {{overcome}} multicollinearity techniques by combining <b>principal</b> <b>component</b> analysis with <b>regression</b> analysis. The calculation of classical <b>principal</b> <b>component</b> analysis {{is based on the}} regular covariance matrix. The covariance matrix is optimal if the data originated from a multivariate normal distribution, but is very sensitive to the presence of outliers. Alternatives are used to overcome this problem the method of Least Median Square-Minimum Covariance Determinant (LMS-MCD). The purpose of this research is to conduct a comparison between <b>Principal</b> <b>Component</b> <b>Regression</b> (RKU) and Method of Least Median Square - Minimum Covariance Determinant (LMS-MCD) in dealing with outliers. In this study, Method of Least Median Square - Minimum Covariance Determinant (LMS-MCD) has a bias and mean square error (MSE) is smaller than the parameter RKU. Based on the difference of parameter estimators, still have a test that has a difference of parameter estimators method LMS-MCD greater than RKU method. </p...|$|R
40|$|Description This package {{can be used}} to {{classify}} microarray data using one of three penalized regression methods; partial least squares, <b>principal</b> <b>components</b> <b>regression,</b> or ridge regression. License Artistic- 2. 0 Depends Biobase (> = 1. 4. 22), R (> = 1. 9. 0), fibroEset, mda biocViews Classification R topics documented: pdmClass [...] 2 pdmClass. cv [...] . 3 pdmGenes [...] 4 predict. pls [...] 6 Index 8 1 2 pdmClas...|$|E
40|$|Motivation: Survival {{prediction}} from {{gene expression}} data and other high-dimensional genomic data has {{been subject to}} much research during the last years. These kinds of data {{are associated with the}} methodological problem of having many more gene expression values than individuals. In addition, the responses are censored survival times. Most of the proposed methods handle this by using Cox’s proportional hazards model and obtain parameter estimates by some dimension reduction or parameter shrinkage estimation technique. Using three well-known microarray gene expression data sets, we compare the prediction performance of seven such methods: univariate selection, forward stepwise selection, <b>principal</b> <b>components</b> <b>regression</b> (PCR), supervised <b>principal</b> <b>components</b> <b>regression,</b> partial least squares regression (PLS), ridge regression and the lasso. Results: Statistical learning from subsets should be repeated several times {{in order to get a}} fair comparison between methods. Methods using coefficient shrinkage or linear combinations of the gene expression values have much better performance than the simple variable selection methods. For our data sets, ridge regression has the overall best performance. Availability: Matlab and R code for the prediction methods are available a...|$|E
40|$|In imaging genomics, {{there have}} been rapid {{advances}} in genome-wide, image-wide searches for genes that influence brain structure. Most efforts focus on univariate tests that treat each genetic variation independently, ignoring the joint effects of multiple variants. Instead, we present a genebased method to detect the joint effect of multiple single nucleotide polymorphisms (SNPs) in 18, 044 genes across 31, 662 voxels of the whole brain in a tensor-based morphometry analysis of baseline MRI scans from 731 subjects from the Alzheimer’s Disease Neuroimaging Initiative (ADNI). Our gene-based multivariate statistics use <b>principal</b> <b>components</b> <b>regression</b> to test the combined effect of multiple genetic variants on an image, using a single test statistic. In some situations, which we describe, this can boost power by encoding population variations within each gene, reducing the effective number of statistical tests, and reducing the effect dimension of the search space. Multivariate gene-based methods may discover gene effects undetectable with standard, univariate methods, accelerating ongoing imaging genomics efforts worldwide. Index Terms — <b>principal</b> <b>components</b> <b>regression,</b> multivariate, voxelwise, imaging genomics, GWAS 1...|$|E
40|$|The {{so-called}} Functional Linear Regression model {{consists in}} explaining a scalar response by a regressor {{which is a}} random function observed on a compact subset of R: in this context, the " of linear model {{is a function of}} the weights. In order to estimate this functional coecient some estimators such as Functional <b>Principal</b> <b>Component</b> <b>Regression</b> Estimator, Smooth <b>Principal</b> Com-ponent Regression Estimator, Penalized B-Splines Estimator, have been in-troduced in literature. We focus our attention on the Functional <b>Principal</b> <b>Component</b> <b>Regression</b> Estimator and in particular on the connected dimen-sionality problem. Our aim is to apply and compare some dierent selection methods, which have been proposed in the classical regression eld. These methods are illus-trated and compared by the means of simulations. ...|$|R
40|$|This paper {{analyzes}} {{the ability of}} <b>principal</b> <b>component</b> <b>regressions</b> and Bayesian regression methods under Gaussian and double-exponential prior in forecasting the real house price of the United States (US), based on a monthly dataset of 112 macroeconomic variables. Using an in-sample period of 1992 : 01 to 2000 : 12, Bayesian regressions are used to forecast real US house prices at the twelve-months-ahead forecast horizon over the out-of-sample period of 2001 : 01 to 2004 : 10. In terms of the Mean Square Forecast Errors (MSFEs), our results indicate that a <b>principal</b> <b>component</b> <b>regression</b> with only one factor is best-suited for forecasting the real US house price. Amongst the Bayesian models, the regression based on the double exponential prior outperforms the model with Gaussian assumptions...|$|R
40|$|Generally, {{non-linear}} predictive models {{should be}} superior to linear predictive models. The objective {{of this study is}} to compare the performance of soluble solid content (SSC) prediction via Artificial Neural Network with <b>Principal</b> <b>Components</b> (PCs-ANN) and <b>Principal</b> <b>Component</b> <b>Regression</b> (PCR) in Visible and Shortwave Near Infrared (VIS-SWNIR) (400 - 1000 nm) spectrum. The spectra of 116 Fuji Apple samples were separated into calibration set of 84 apple samples and testing set of 32 apple samples randomly. Firstly, multiplicative scattering correction (MSC) was used to pre-process the spectra. Secondly, <b>Principal</b> <b>Component</b> <b>Regression</b> (PCR) was used to obtain the optimal number of <b>principal</b> <b>components</b> (PCs). Thirdly, the optimal PCs were used as the inputs of both multiple linear regression (MLR) and Artificial Neural Network (ANN) models. The results from this study showed that the predictive performance was improved significantly when PCs-ANN with two neurons was used compared to the PCR...|$|R
40|$|PEM – Polymorphic {{epithelial}} mucin PyMT – Polyoma middle T antigen SPE – Solid phase extraction PCA – Principal components analysis PCR – <b>Principal</b> <b>components</b> <b>regression</b> GCC – Graphatized carbon cartridge Hex – Hexose HexNAc – N-acetylhexosamine MMTV – Mouse mammary {{tumor virus}} CM – Condition media FBS – Fetal bovine serum DCIS – Ductal in situ carcinoma 2 SUMMARY Since the glycosylation of proteins {{is known to}} change in tumor cells during th...|$|E
40|$|In {{this paper}} we apply the Monte Carlo method {{to find the}} {{eigenvalues}} and the eigenvectors of a k-symmetric matrix A. At first we add to the main diagonal of A a real number large enough to obtain a covariance matrix B and we {{take into account that}} the minimum sum of the squares in the <b>principal</b> <b>components</b> <b>regression</b> (PCR) is given by the corresponding eigenvector of the minimum eigenvalue of B. ...|$|E
40|$|Multiple linear {{regression}} is considered and the partial {{least squares method}} (PLS) for computing a projection onto a lower-dimensional subspace is analyzed. In the analysis we use the equivalence to Lanczos bidiagonalization. In particular we illustrate, using singular value analysis and Krylov subspaces, why, in many cases, PLS gives a faster reduction of the residual than standard <b>principal</b> <b>components</b> <b>regression.</b> Our analysis also shows why {{in some cases the}} dimension of the subspace, given by PLS, is not as small as desired...|$|E
40|$|A new biased {{estimator}} obtained {{by combining the}} <b>Principal</b> <b>Component</b> <b>Regression</b> Estimator and the special case of Liu-type estimator is proposed. The properties of the new estimator are derived and comparisons between the new estimator and other estimators in terms of mean squared error are presented...|$|R
40|$|A <b>principal</b> <b>component</b> <b>regression</b> (PCR) based {{approach}} {{for studying the}} functional connectivity of blood oxygenation level dependent (BOLD) responses of {{functional magnetic resonance imaging}} (fMRI) is proposed. The temporal dependency of BOLD responses from different spatial areas is determined from the eigenvectors of the correlation matrix. 1...|$|R
40|$|We ran <b>principal</b> <b>component</b> <b>regressions</b> {{of growth}} and income on {{existing}} measures of institutions to assess which {{are the most important}} for economic performance. We find that broadly defined institutions of checks and balances as well as a democratic and anti-authoritarian culture are the most robust institutional determinants of long-run growth in income. Institutions Development <b>Principal</b> <b>component</b> analysis Separation of powers Culture...|$|R
