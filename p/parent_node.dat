385|333|Public
25|$|A {{degenerate}} (or pathological) tree {{is where}} each <b>parent</b> <b>node</b> {{has only one}} associated child node. This means that performance-wise, the tree will behave like a linked list data structure.|$|E
25|$|The {{algorithm}} maintains two values, alpha and beta, {{which represent}} the minimum score that the maximizing player is assured of and the maximum score that the minimizing player is assured of respectively. Initially alpha is negative infinity and beta is positive infinity, i.e. both players start with their worst possible score. It can happen that {{when choosing a}} certain branch of a certain node the maximum score that the minimizing player is assured of becomes less than the minimum score that the maximizing player is assured of (beta <= alpha). If this is the case, the <b>parent</b> <b>node</b> should not choose this node, because it will make the score for the <b>parent</b> <b>node</b> worse. Therefore, the other branches of the node {{do not have to}} be explored.|$|E
25|$|Searching {{is similar}} to searching a binary search tree. Starting at the root, the tree is recursively traversed from top to bottom. At each level, the search reduces its field of view to the child pointer (subtree) whose range {{includes}} the search value. A subtree's range {{is defined by the}} values, or keys, contained in its <b>parent</b> <b>node.</b> These limiting values are also known as separation values.|$|E
30|$|The {{next step}} is to {{determine}} a collision-free transmission schedule for <b>parent</b> <b>nodes.</b> This is carried out as follows. First, ILBS collects the <b>parents</b> of <b>nodes</b> in U and Y∖U into two corresponding subsets W 1 and W 2 according to the said selection order. Then to schedule interfering <b>parent</b> <b>nodes,</b> it uses two D 2 -coloring methods: (1) front-to-back ordering, whereby the coloring proceeds from the first to the last node and (2) smallest-degree-last ordering, with the rule being that two <b>parent</b> <b>nodes</b> must not share the same color if a subset of a parent’s children is adjacent to another, i.e., a <b>parent</b> <b>node’s</b> transmission interferes with the reception of another parent’s children.|$|R
30|$|Intermediate node: <b>nodes</b> {{that relate}} <b>parent</b> <b>nodes</b> to end nodes.|$|R
50|$|Let P be {{the set of}} <b>parent</b> <b>nodes</b> for the nodes in S.|$|R
25|$|A rooted {{phylogenetic tree}} (see two {{graphics}} at top) is a directed tree {{with a unique}} node — the root — corresponding to the (usually imputed) most recent common ancestor of all the entities at {{the leaves of the}} tree. The root node does not have a <b>parent</b> <b>node,</b> but serves as the parent of all other nodes in the tree. The root is therefore a node of degree 2, whereas other internal nodes of the tree do have a minimum degree of 3.|$|E
2500|$|Behavior {{generation}} {{is responsible for}} executing tasks received from the superior, <b>parent</b> <b>node.</b> [...] It also plans for, and issues tasks to, the subordinate nodes.|$|E
2500|$|Attack {{trees are}} multi-leveled {{diagrams}} consisting of one root, leaves, and children. [...] From the bottom up, child nodes are conditions {{which must be}} satisfied to make the direct <b>parent</b> <b>node</b> true; when the root is satisfied, the attack is complete. [...] Each node may be satisfied only by its direct child nodes.|$|E
3000|$|... as {{the set of}} <b>parent</b> <b>nodes</b> in the ℓth cluster {{including}} the cluster-head, i.e., those generating a cluster; [...]...|$|R
5000|$|Traverse {{the forest}} [...] in {{preorder}} and number the <b>nodes.</b> <b>Parent</b> <b>nodes</b> {{in the forest}} now have lower numbers than child nodes.|$|R
5000|$|Network structure: network {{structures}} also organizes data using {{nodes and}} branches. But, unlike hierarchical, each child node {{can be linked}} to multiple, higher <b>parent</b> <b>nodes.</b>|$|R
2500|$|The {{domain name}} space {{consists}} of a tree data structure. Each node or leaf in the tree has a label and zero or more resource records (RR), which hold information associated with the domain name. [...] The domain name itself consists of the label, possibly concatenated {{with the name of}} its <b>parent</b> <b>node</b> on the right, separated by a dot.|$|E
2500|$|The heapify {{procedure}} can {{be thought}} of as building a heap from the bottom up by successively sifting downward to establish the heap property. An alternative version (shown below) that builds the heap top-down and sifts upward may be simpler to understand. This siftUp version can be visualized as starting with an empty heap and successively inserting elements, whereas the siftDown version given above treats the entire input array as a full but [...] "broken" [...] heap and [...] "repairs" [...] it starting from the last non-trivial sub-heap (that is, the last <b>parent</b> <b>node).</b>|$|E
2500|$|The {{figure to}} the right shows a concept tree with five concepts. [...] is the root concept, which {{contains}} all ten objects in the data set. [...] Concepts [...] and [...] are the children of , the former containing four objects, and the later containing six objects. [...] Concept [...] is also the parent of concepts , , and , which contain three, two, and one object, respectively. [...] Note that each <b>parent</b> <b>node</b> (relative superordinate concept) contains all the objects contained by its child nodes (relative subordinate concepts). [...] In Fisher's (1987) description of COBWEB, he indicates that only the total attribute counts (not conditional probabilities, and not object lists) are stored at the nodes. [...] Any probabilities are computed from the attribute counts as needed.|$|E
30|$|In {{practice}} {{it might be}} useful to add two terms, δ _ 0 and δ _ 1 to the above equation to allow respectively a non-zero probability for the worst-case scenario for Z (when all <b>parent</b> <b>nodes</b> are in their worst state) and a probability less than 1 for the best-case scenario for Z (when all <b>parent</b> <b>nodes</b> are in their best state). The values of δ _ 0 and δ _ 1 could again be elicited from experts, the literature or other studies where experts may need to validate the values.|$|R
5000|$|S-Attributed Grammars are a {{class of}} {{attribute}} grammars characterized by having no [...] inherited attributes, but only [...] synthesized attributes. Inherited attributes, which must be passed down from <b>parent</b> <b>nodes</b> to children nodes of the abstract syntax tree during the semantic analysis of the parsing process, are a problem for bottom-up parsing because in bottom-up parsing, the <b>parent</b> <b>nodes</b> of the abstract syntax tree are created after creation of all of their children. Attribute evaluation in S-attributed grammars can be incorporated conveniently in both top-down parsing and bottom-up parsing.|$|R
30|$|Left-hand side of {{rewriting}} rule contains node non-terminal symbol with the context on a left defined by traversing <b>parent</b> <b>nodes</b> {{up to the}} root inclusively and concatenating their labels.|$|R
2500|$|We {{assume that}} we find every [...] of all nodes [...] whose {{distance}} from the root node is {{less than or equal}} k, and now we are seeking the [...] of the node [...] whose distance from the root node is k +1. Its <b>parent</b> <b>node</b> is , and the letter represented by the node [...] and , is x. (1): If the next letter of the node [...] is x, we set the other node of this edge as , and =. (2): If all letters is not x by searching all edges between [...] and its child nodes, [...] is a suffix of [...] plus x. Because this suffix matches the STRING begin with the root node (similar to prefix), we can detect if there is x after [...] or not. And if not, continue this process until find x or find the root node.|$|E
2500|$|Usually, {{the number}} of keys is chosen to vary between [...] and , where [...] is the minimum number of keys, and [...] is the minimum degree or {{branching}} factor of the tree. In practice, the keys take up the most space in a node. The factor of 2 will guarantee that nodes can be split or combined. If an internal node has [...] keys, then adding a key to that node {{can be accomplished by}} splitting the hypothetical [...] key node into two [...] key nodes and moving the key that {{would have been in the}} middle to the <b>parent</b> <b>node.</b> Each split node has the required minimum number of keys. Similarly, if an internal node and its neighbor each have [...] keys, then a key may be deleted from the internal node by combining it with its neighbor. Deleting the key would make the internal node have [...] keys; joining the neighbor would add [...] keys plus one more key brought down from the neighbor's parent. The result is an entirely full node of [...] keys.|$|E
2500|$|Maximum parsimony can be {{implemented}} by one of several algorithms. [...] One of the earliest examples is Fitch's method, which assigns ancestral character states by parsimony via two traversals of a rooted binary tree. The first stage is a post-order traversal that proceeds from the tips toward the root of a tree by visiting descendant (child) nodes before their parents. [...] Initially, we are determining the set of possible character states Si for the i-th ancestor based on the observed character states of its descendants. [...] Each assignment is the set intersection of the character states of the ancestor's descendants; if the intersection is the empty set, then it is the set union. [...] In the latter case, it is implied that a character state change has occurred between the ancestor {{and one of its}} two immediate descendants. Each such event counts towards the algorithm's cost function, which may be used to discriminate among alternative trees on the basis of maximum parsimony. [...] Next, a pre-order traversal of the tree is performed, proceeding from the root towards the tips. Character states are then assigned to each descendant based on which character states it shares with its parent. Since the root has no <b>parent</b> <b>node,</b> one may be required to select a character state arbitrarily, specifically when more than one possible state has been reconstructed at the root.|$|E
50|$|In general after {{execution}} of this function the proof won't be a legal proof anymore.The following algorithm takes the root node of a proof and constructs a legal proof out of it.The computation begins with recursively {{calls to the}} children nodes. In order to minimize the algorithm calls, it is beingt kept track of which nodes were already visited. Note that a resolution proof {{can be seen as}} a general directed acyclic graph as opposed to a tree.After the recursive call the clause of the present node is updated. While doing so four different cases can occur.The present pivot variable can occur in both, the left, the right or in none of the <b>parent</b> <b>nodes.</b> If it occurs in both <b>parent</b> <b>nodes</b> the clause is calculated as resolvent of the parent clauses.If it is not present in one of the <b>parent</b> <b>nodes</b> the clause of this parent can be copied. If it misses in both parents one has to choose heuristically.|$|R
5000|$|While it {{may seem}} as though the {{operators}} have to be invertible for the reverse search, it is only necessary {{to be able to}} find, given any node , the set of <b>parent</b> <b>nodes</b> of [...] such that there exists some valid operator from each of the <b>parent</b> <b>nodes</b> to [...] This has often been likened to a one-way street in the route-finding domain: {{it is not necessary to}} be able to travel down both directions, but it is necessary when standing at the end of the street to determine the beginning of the street as a possible route.|$|R
3000|$|... ‘Expansion task firstly computes K × W {{values of}} Dn and xn (i.e., child <b>nodes)</b> from K <b>parent</b> <b>nodes</b> {{selected}} from stage n+ 1. It then calculates PEDn = PEDn+ 1 + Dn.|$|R
5000|$|Otherwise, {{push the}} middle value {{up into the}} <b>parent</b> <b>node.</b> Ascend into the <b>parent</b> <b>node.</b>|$|E
50|$|Let Np be the <b>parent</b> <b>node</b> of N.|$|E
5000|$|Those {{reserved}} {{words can}} only be written lowercase {{and will not be}} recognized otherwise. The parent reserved word gives a reference to the <b>parent</b> <b>node</b> in the syntax tree of the code where the word is placed. In the following code, the <b>parent</b> <b>node</b> is the operator [...]|$|E
40|$|In a Bayesian network, for any node its {{conditional}} probabilities {{given all}} possible com-binations of values for its <b>parent</b> <b>nodes</b> are specified. In this paper a new notion, the parental synergy, is introduced {{which can be}} computed from these probabilities. This paper also conjectures a general expression for the error which {{is found in the}} marginal prior probabilities computed for a <b>node</b> when the <b>parents</b> of this <b>node</b> are assumed to be independent. The parental synergy is an important factor of this expression; it determines to what extend the actual dependency between the <b>parent</b> <b>nodes</b> can affect the computed probabilities. This role in the expression of the prior convergence error indicates that the parental synergy is a fundamental feature of a Bayesian network. ...|$|R
30|$|Stage II(a): {{identify}} {{key factors}} Identify the key {{factors that affect}} the principal objective(s) defined in Stage I. Key factors will become <b>parent</b> <b>nodes</b> of the top level node and will have further sub factors influencing them.|$|R
3000|$|... ‘Sorting task sorts K × W {{values of}} PEDn {{to find the}} K {{smallest}} values of PEDn and the corresponding {xN,…,xn}. The selected data will become the <b>parent</b> <b>nodes</b> of the next stage (i.e., stage n- 1).|$|R
50|$|Sibling nodes: {{these are}} nodes {{connected}} to the same <b>parent</b> <b>node.</b>|$|E
50|$|Child: A {{child node}} is a node {{extending}} from another node. For example, a computer with internet access {{could be considered}} a child node of a node representing the internet. The inverse relationship is that of a <b>parent</b> <b>node.</b> If node C is a child of node A, then A is the <b>parent</b> <b>node</b> of C.|$|E
5000|$|... "Sibling" [...] ("brother" [...] or [...] "sister") nodes {{share the}} same <b>parent</b> <b>node.</b>|$|E
50|$|In {{the second}} phase, the {{multiple}} unconnected trees are {{turned into a}} single tree by merging {{the roots of the}} initial trees, but this time starting from the right and adding new <b>parent</b> <b>nodes</b> as needed (red nodes).|$|R
30|$|Stage V(c): {{influence}} analysis The magnitudes of {{impacts of}} <b>parent</b> <b>nodes</b> on their respective child nodes are identified through an influence analysis. The analysis results {{can provide a}} better understanding of the most significant factors in a decision scenario.|$|R
50|$|The {{attributes}} {{are divided}} into two groups: synthesized attributes and inherited attributes. The synthesized attributes {{are the result of}} the attribute evaluation rules, and may also use the values of the inherited attributes. The inherited attributes are passed down from <b>parent</b> <b>nodes.</b>|$|R
