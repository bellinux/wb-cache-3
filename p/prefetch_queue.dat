13|17|Public
50|$|The {{speed of}} the {{execution}} unit (EU) and the bus of the 8086 CPU was well balanced; with a typical instruction mix, an 8086 could execute instructions out of the <b>prefetch</b> <b>queue</b> {{a good bit of}} the time. Cutting down the bus to 8 bits made it a serious bottleneck in the 8088. With the speed of instruction fetch reduced by 50% in the 8088 as compared to the 8086, a sequence of fast instructions can quickly drain the 4-byte <b>prefetch</b> <b>queue.</b> When the queue is empty, instructions take as long to complete as they take to fetch. Both the 8086 and 8088 take 4 clock cycles to complete a bus cycle; whereas for the 8086 this means 4 clocks to transfer 2 bytes, on the 8088 it is 4 clocks per byte. Therefore, for example, a 2-byte shift or rotate instruction, which takes the EU only 2 clock cycles to execute, actually takes 8 clocks to complete if it is not in the <b>prefetch</b> <b>queue.</b> A sequence of such fast instructions prevents the queue from being filled as fast as it is drained, and in general, because so many basic instructions execute in fewer than 4 clocks per instruction byte - including almost all the ALU and data-movement instructions on register operands and some of these on memory operands - it is practically impossible to avoid idling the EU in the 8088 at least 1/4 of the time while executing useful real-world programs, and {{it is not hard to}} idle it half the time. In short, an 8088 typically runs about half as fast as 8086 clocked at the same rate, because of the bus bottleneck (the only major difference).|$|E
50|$|A {{side effect}} of the 8088 design, with the slow bus and the small <b>prefetch</b> <b>queue,</b> is that the speed of code {{execution}} can be very dependent on instruction order. When programming the 8088, for CPU efficiency, {{it is vital to}} interleave long-running instructions with short ones whenever possible. For example, a repeated string operation or a shift by three or more will take long enough to allow time for the 4-byte <b>prefetch</b> <b>queue</b> to completely fill. If short instructions (i.e. ones totaling few bytes) are placed between slower instructions like these, the short ones can execute at full speed out of the queue. If, on the other hand, the slow instructions are executed sequentially, back to back, then after the first of them the bus unit will be forced to idle because the queue will already be full, with the consequence that later more of the faster instructions will suffer fetch delays that might have been avoidable. As some instructions, such as single-bit-position shifts and rotates, take literally 4 times as long to fetch as to execute, the overall effect can be a slowdown by a factor of two or more. If those code segments are the bodies of loops, the difference in execution time may be very noticeable on the human timescale.|$|E
5000|$|The 8088 was {{designed}} in Israel, at Intel's Haifa laboratory, as were {{a large number}} of Intel's processors. [...] The 8088 was targeted at economical systems by allowing the use of an 8-bit data path and 8-bit support and peripheral chips; complex circuit boards were still fairly cumbersome and expensive when it was released. The <b>prefetch</b> <b>queue</b> of the 8088 was shortened to four bytes, from the 8086's six bytes, and the prefetch algorithm was slightly modified to adapt to the narrower bus. These modifications of the basic 8086 design were one of the first jobs assigned to Intel's then new design office and laboratory in Haifa, Israel.|$|E
50|$|As the {{complexity}} of these chips increases, the cost also increases. These processors are relatively costlier than their counterparts without the <b>prefetch</b> input <b>queue.</b>|$|R
50|$|However, these {{disadvantages}} {{are greatly}} {{offset by the}} improvement in processor execution time. After the introduction of <b>prefetch</b> instruction <b>queue</b> in the 8086 processor, all successive processors have incorporated this feature.|$|R
5000|$|When a {{page fault}} occurs, [...] "anticipatory paging" [...] systems {{will not only}} bring in the {{referenced}} page, but also the next few consecutive pages (analogous to a <b>prefetch</b> input <b>queue</b> in a CPU).|$|R
5000|$|In 1985, National Semi {{introduced}} the NS32332, a much improved {{version of the}} 32032. From the datasheet, the enhancements include [...] "the addition of new dedicated addressing hardware (consisting of a high speed ALU, a barrel shifter and an address register), a very efficient increased (20 bytes) instruction <b>prefetch</b> <b>queue,</b> a new system/memory bus interface/protocol, increased efficiency slave processor protocol and finally enhancements of microcode." [...] There was also a new NS32382 MMU, NS32381 FPU and the (very rare) NS32310 interface to a Weitek FPA.The aggregate performance boost of the NS32332 from these enhancements only made it 50 percent faster than the original NS32032, and therefore {{less than that of}} the main competitor, the MC68020.|$|E
50|$|Finally, because calls, jumps, and {{interrupts}} {{reset the}} <b>prefetch</b> <b>queue,</b> and because loading the IP register requires {{communication between the}} EU and the BIU (since the IP register is in the BIU, not in the EU, where the general registers are), these operations are costly. All jumps and calls take at least 15 clock cycles. Any conditional jump requires 4 clock cycles if not taken, but if taken, it requires 16 cycles in addition to resetting the prefetch queue; therefore, conditional jumps should be arranged to be not taken most of the time, especially inside loops. In some cases, a sequence of logic and movement operations is faster than a conditional jump that skips over one or two instructions {{to achieve the same}} result.|$|E
50|$|The key {{difference}} between DDR2 and DDR SDRAM is {{the increase in}} prefetch length. In DDR SDRAM, the prefetch length was two bits for every bit in a word; whereas it is four bits in DDR2 SDRAM. During an access, four bits were read or written to or from a four-bit-deep <b>prefetch</b> <b>queue.</b> This queue received or transmitted its data over the data bus in two data bus clock cycles (each clock cycle transferred two bits of data. Increasing the prefetch length allowed DDR2 SDRAM to double {{the rate at which}} data could be transferred over the data bus without a corresponding doubling in {{the rate at which the}} DRAM array could be accessed. DDR2 SDRAM was designed with such a scheme to avoid an excessive increase in power consumption.|$|E
40|$|The speed {{disparity}} between processor and memory subsystems has been bridged in many existing large-scale scientific computers and microprocessors {{with the help}} of instruction buffers or instruction caches. In this paper we classify these buffers into traditional instruction buffers, conventional instruction caches and <b>prefetch</b> <b>queues,</b> detail their prominent features, and evaluate the performance of buffers in several existing systems, using trace driven simulation. We compare these schemes with a recently proposed queue-based instruction cache memory. An implementation independent performance metric is proposed for the various organizations and used for the evaluations. We analyze the simulation results and discuss the effect of various parameters such as prefetch threshold, bus width and buffer size on performance...|$|R
50|$|Generally in {{applications}} like <b>prefetch</b> input <b>queue,</b> M/M/1 Model is popularly {{used because}} of limited use of queue features. In this model {{in accordance with}} microprocessors, the user takes {{the role of the}} execution unit and server is the bus interface unit.|$|R
50|$|Pipelining {{was brought}} to the {{forefront}} of computing architecture design during the 1960s due to the need for faster and more efficient computing. Pipelining is the broader concept and most modern processors load their instructions some clock cycles before they execute them. This is achieved by pre-loading machine code from memory into a <b>prefetch</b> input <b>queue.</b>|$|R
50|$|Although partly shadowed {{by other}} design choices in this {{particular}} chip, the multiplexed address and data buses limit performance slightly; transfers of 16-bit or 8-bit quantities are done in a four-clock memory access cycle, which is faster on 16-bit, although slower on 8-bit quantities, compared to many contemporary 8-bit based CPUs. As instructions vary from one to six bytes, fetch and execution are made concurrent and decoupled into separate units (as it remains in today's x86 processors): The bus interface unit feeds the instruction stream to the execution unit through a 6-byte <b>prefetch</b> <b>queue</b> (a form of loosely coupled pipelining), speeding up operations on registers and immediates, while memory operations unfortunately became slower (four years later, this performance problem was fixed with the 80186 and 80286). However, the full (instead of partial) 16-bit architecture with a full width ALU meant that 16-bit arithmetic instructions could now be performed with a single ALU cycle (instead of two, via internal carry, as in the 8080 and 8085), speeding up such instructions considerably. Combined with orthogonalizations of operations versus operand types and addressing modes, {{as well as other}} enhancements, this made the performance gain over the 8080 or 8085 fairly significant, despite cases where the older chips may be faster (see below).|$|E
30|$|In [33], {{the authors}} model the 4 -byte-long <b>prefetch</b> <b>queue</b> of an Intel 80188. Even for this simple <b>prefetch</b> <b>queue,</b> {{the authors have}} to perform some {{simplifications}} in their approach to handle the resulting complexity due to {{the interaction between the}} instruction execution and the instruction prefetch (the consuming and the producing ends of the queue).|$|E
3000|$|... [...]. This feature {{compensates}} for the inefficiency of a stack machine. However, the folding unit {{depends on a}} 16 -byte instruction buffer with all the resulting unbounded timing effects of a <b>prefetch</b> <b>queue.</b>|$|E
5000|$|The {{technique}} of self-modifying code can be problematic on a pipelined processor. In this technique, {{one of the}} effects of a program is to modify its own upcoming instructions. If the processor has an instruction cache, the original instruction may already have been copied into a <b>prefetch</b> input <b>queue</b> and the modification will not take effect.|$|R
50|$|Most modern {{processors}} {{load the}} machine code before they execute it, {{which means that}} if an instruction that is too near the instruction pointer is modified, the processor will not notice, but instead execute the code {{as it was before}} it was modified. See <b>prefetch</b> input <b>queue</b> (PIQ). PC processors must handle self-modifying code correctly for backwards compatibility reasons but they are far from efficient at doing so.|$|R
50|$|Processors {{implementing}} the instruction <b>queue</b> <b>prefetch</b> algorithm are rather technically advanced. The CPU design level {{complexity of the}} such processors {{is much higher than}} for regular processors. This is primarily because of the need to implement two separate units, the BIU and EU, operating separately.|$|R
30|$|The {{instruction}} fetching {{is often}} decoupled {{from the main}} memory or the instruction cache by a prefetch unit. This unit fills the <b>prefetch</b> <b>queue</b> with instructions independently of the main pipeline. This form of prefetching {{is especially important for}} a variable length instruction set as the x 86 ISA or the bytecode instructions of the Java virtual machine (JVM). The fill status of the <b>prefetch</b> <b>queue</b> depends {{on the history of the}} instruction stream. The possible length of this history is unbounded. To model this queue for a WCET tool, we need to cut the maximum history and assume an empty queue at such a cut point.|$|E
30|$|To avoid a <b>prefetch</b> <b>queue,</b> with {{probably}} unbounded execution-time dependencies over {{a stream}} of instructions, a fixed-length instruction set is recommended. Variable length instructions can complicate instruction cache analysis because an instruction can cross a block boundary. The method cache, as proposed in the following section, avoids this issue. Either all instructions of a function, independent of their length, are in the cache, or none of them.|$|E
40|$|This {{research}} was conducted in the District of Batang Cenaku Indragiri Hulu. This is because the District of Batang Cenaku an area that makes rubber commodity as one commodity and in meeting their needs. The {{purpose of this study}} was to determine how the income of farmers in the district rubber rod upstream and Indragiri district Cenaku To know that used descriptive analysis, the data obtained are tabulated and then described by existing theories. The survey results revealed that the rubber plantation is vital to the income of the people in the District of Batang Cenaku Thus it can be seen that the average income of farmers quality seeds rubber plant samples with an area of 1 hectare of Rp. 8, 241, 724, - per year, while the average income of farmers 2 ̆ 7 traditional seed samples rubber plant with an area of 2 hectares of Rp. 8, 463, 739, - per year. A comparison of income between farmers using traditional quality seeds and that happened 2 times where income 1 hectare equals income <b>prefetch</b> <b>queue</b> seeds 2 acres of farmers using traditional seeds. Businesses manage rubber plantation is a tradition for the people of District of Batang Cenaku Indragiri Hulu. Many efforts have been done by the government in the development of rubber plantations, such as by providing quality seeds and adequate capital for rubber farmers...|$|E
50|$|Fetching the {{instruction}} opcodes from program memory {{well in advance}} is known as prefetching and it is served by using <b>prefetch</b> input <b>queue</b> (PIQ).The pre-fetched instructions are stored in data structure - namely a queue. The fetching of opcodes well in advance, prior to their need for execution increases the overall efficiency of the processor boosting its speed. The processor no longer has {{to wait for the}} memory access operations for the subsequent instruction opcode to complete. This architecture was prominently used in the Intel 8086 microprocessor.|$|R
50|$|To enter {{protected}} mode, the Global Descriptor Table (GDT) {{must first}} be created {{with a minimum of}} three entries: a null descriptor, a code segment descriptor and data segment descriptor. In an IBM-compatible machine, the A20 line (21st address line) also must be enabled to allow the use of all the address lines so that the CPU can access beyond 1 megabyte of memory (Only the first 20 are allowed to be used after power-up, to guarantee compatibility with older software written for the Intel 8088-based IBM PC and PC/XT models). After performing those two steps, the PE bit must be set in the CR0 register and a far jump must be made to clear the <b>prefetch</b> input <b>queue.</b>|$|R
40|$|Current {{techniques}} for prefetching linked data structures (LDS) exploit the work available in one loop iteration or recursive call to overlap pointer chasing latency. Jumppointers, which provide {{direct access to}} non-adjacent nodes, {{can be used for}} prefetching when loop and recursive procedure bodies are small and do not have sufficient work to overlap a long latency. This paper describes a framework for jump-pointer prefetching (JPP) that supports four <b>prefetching</b> idioms: <b>queue,</b> full, chain, and root jumping and three implementations: software-only, hardware-only, and a cooperative software/hardware technique. On a suite of pointer intensive programs, jumppointer prefetching reduces memory stall time by 72 % for software, 83 % for cooperative and 55 % for hardware, producing speedups of 15 %, 20 % and 22 % respectively. ...|$|R
40|$|Deviana Diah Probowati. S 641308002. Analysis of {{consumer}} preferences and marketing {{strategies in the}} district tangerine fruit bojonegoro. Thesis Program Pascasarjana Program Studi Agribisnis Universitas Sebelas Maret Surakarta. Supervisor (1) Prof. Dr. Ir. Suprapti Supardi, MP, (2) Dr. Ir. Sri Marwanti, MS. This study aims to determine the factors that differentiate consumer preferences in purchasing fruit of tangerine, citrus fruit attributes that are preferred by consumers in choosing fruits tangerines and marketing strategies {{that can be applied}} in marketing fruit tangerine. Factors consumer preferences tangerine fruit analyzed in this study were sex, education level, occupation, age, income level and number of family members or dependents in the family. Based on this research, different income factors to consumer preferences tangerine fruit in Bojonegoro. While the factors gender, education, age, occupation and number of family members are no different to consumer preferences tangerine fruit in Bojonegoro. Based on the analysis Fishbein of research shows that consumer attitudes to attribute more importance tangerine fruit freshness and fruit flavors tangerine which has a value of 1. 72 and 0. 85 respectively -masing. While, he price factor is not a major consideration in selecting fruit consumers tangerines. Based on these factors and attributes into consumer preferences in choosing tangerine fruit can be formulated strategic alternatives in marketing the fruit tangerine. Strategies that can be done is to optimize the use of seeds <b>prefetch</b> <b>queue,</b> optimizing the mutually beneficial relationship between farmers and traders, increased knowledge of farmers in the cultivation of fruits tangerines and optimize the role of the government to farmer...|$|E
40|$|The {{increasing}} {{availability and}} power of mobile computing devices are creating a demand for network applications which can accommodate slow or intermittently available network connections. A prototype system for browsing the World Wide Web on such a network was implemented, using caching, <b>prefetching,</b> and <b>queued</b> communications to hide communication latencies. Electronic mail {{was used as the}} underlying transport mechanism because of its ubiquitousness and fundamentally queued operation. A modified user interface based on NCSA Mosaic was designed to accommodate queued communications. The experimental system was found to be well-suited to the task, as well as to have significant advantages in the areas of reliability and user interface functionality. Thesis Supervisor: David K. Gifford Title: Professor of Electrical Engineering and Computer Science Contents 1 Introduction 5 1. 1 The Rover project : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 1. 2 Other related work : : : : : : [...] ...|$|R
40|$|The {{memory system}} remains a major {{performance}} bottleneck in modern and future architectures. In this dissertation, we propose a hardware/software cooperative approach and demonstrate its effectiveness. This approach combines the global yet imperfect {{view of the}} compiler with the timely yet narrow-scope context of the hardware. It relies on a light-weight extension to the instruction set architecture to convey compile-time knowledge (hints) to the hardware. The hardware then uses these hints to make better decisions. ^ Our work shows that a cooperative hardware/software approach to (1) cache replacement, (2) prefetching, and (3) their combination eliminates or tolerates much of the memory performance bottleneck. (1) Our work enhances cache replacement decisions using compiler hints. The compiler detects which data will or will not be reused and annotates loads accordingly. The compiler sets one bit (the evict-me bit) to denote a preferred eviction candidate. On a miss, the cache replacement algorithm preferentially replaces a cache line with its evict-me bit set. Otherwise, it follows the LRU policy. The evict-me replacement scheme improves cache replacement decisions and is effective in both L 1 and L 2 caches. (2) We also use compiler hints to direct aggressive hardware region prefetching and content-aware pointer prefetching. The original SRP (scheduled region <b>prefetching)</b> engine <b>queues</b> <b>prefetching</b> requests on every outstanding L 2 miss and tolerates latencies {{at the cost of}} dramatically increasing the memory traffic. GRP (guided region prefetching) enhances SRP by restricting prefetching to compiler-marked loads. Our compiler algorithms effectively mark spatial reuses across the SPEC CPU 2000 benchmarks, and thus GRP achieves the performance of SRP with only one eighth of the additional traffic. (3) The evict-me cache replacement scheme helps alleviate the side effects of cache pollution introduced by useless region prefetches. The combination of evict-me caching and region prefetching further improves cache performance. These results demonstrate significant promise for overcoming the memory bottleneck with cooperative hardware/software techniques. ...|$|R
40|$|Abstract—Graph-based {{applications}} {{have become}} increasingly important in many application domains. The large graph sizes offer data level parallelism at a scale that makes it attractive to run such applications on distributed shared memory (DSM) based modern clusters composed of multicore machines. Our analysis of several graph applications that rely on speculative parallelism or asynchronous parallelism shows that the balance between computation and communication differs between ap-plications. In this paper, we study this balance {{in the context of}} DSMs and exploit the multiple cores present in modern multicore machines by creating three kinds of threads which allows us to dynamically balance computation and communication: compute threads to exploit data level parallelism in the computation; fetch threads that replicate data into object-stores before it is accessed by compute threads; and update threads that make results computed by compute threads visible to all compute threads by writing them to DSM. We observe that the best configuration for above mechanisms varies across different inputs in addition to the variation across different applications. To this end, we design ABC 2 : a runtime algorithm that automatically configures the DSM using simple runtime information such as: observed object <b>prefetch</b> and update <b>queue</b> lengths. This runtime algorithm achieves speedups close to that of the best hand-optimized configurations...|$|R
40|$|Ubiquitous {{availability}} of growing troves of interesting datasets warrants a rewrite of existing programs, for clusters or for out-of-core versions, to handle larger datasets. While DSM clusters deliver programmability and performance via shared-memory programming and tolerating latencies by prefetching and caching, copious disk space {{is far more}} readily available than managing clusters. Irregular applications, however, are challenging to parallelize because the input related data dependences that manifest at runtime require use of speculation for correct parallel execution. By speculating {{that there are no}} input related cross iteration dependences, it can be doall parallelized; the absence of dependences is validated before committing the computed results. Latency tolerating mechanisms like prefetching and caching which have traditionally benefited data-parallel applications, can hurt performance of speculation from reading stale values. This thesis seeks to address the task of size oblivious programming of irregular applications for large inputs on DSM clusters. We first simplify the task of programming very large and irregular data, which may sometimes not fit in the DSM. To this end, we introduce a language, the associated runtime and a compiler for efficient distributed software speculation to parallelize irregular applications. The programming model consists of an easy to use, templated C++ library for writing new vertex-centric programs or the simple large and speculate constructs as extensions to the C/C++ language for existing programs. In addition we introduce the InfiniMem random-access efficient object data format on disk to enable I/O of arbitrary data objects from disk in at most two logical disk seeks. We also present a simple API for I/O into this data format and the accompanying object-centric library framework to program size oblivious programs to support scale-up on individual machines as a first step. We then leverage InfiniMem on individual machines in the cluster to support distributed size oblivious programs. As a final stage to ease programming, we built an LLVM/Clang-based source-to-source compiler, SpeClang, which instruments the annotated source code with invocations into our libraries and runtime. The runtime comprises of an object-based caching DSM with explicit support for software speculation and transparently performs out-of-core computations using InfiniMem to handle large inputs. Next, we address the inefficiencies that result from employing traditional latency tolerance mechanisms like prefetching and caching on DSM systems to support speculation: (a) we demonstrate the need for balancing computation and communication for efficient distributed software speculation and provide an adaptive, dynamic solution that leverages <b>prefetch</b> and commit <b>queue</b> lengths to drive this balance. (b) we demonstrate that aggressive caching can hurt speculation performance and present three techniques to decrease communication and cost of misspeculation check and speed up misspeculated recomputations by leveraging the DSM caching and speculation protocols to keep misspeculation rates low...|$|R
40|$|A {{well known}} {{performance}} bottleneck in computer architecture {{is the so-called}} memory wall. This term refers to the huge disparity between on-chip and off-chip access latencies. Historically speaking, the operating frequency of processors has increased at a steady pace, while most past advances in memory technology have been in density, not speed. Nowadays, the trend for ever increasing processor operating frequencies {{has been replaced by}} an increasing number of CPU cores per chip. This will continue to exacerbate the memory wall problem, as several cores now have to compete for off-chip data access. As multi-core systems pack more and more cores, it is expected that the access latency as observed by each core will continue to increase. Although the causes of the memory wall have changed, it is, {{and will continue to be}} in the near future, a very significant challenge in terms of computer architecture design. Prefetching has been an important technique to amortize the effect of the memory wall. With prefetching, data or instructions that are expected to be used in the near future are speculatively moved up in the memory hierarchy, were the access latency is smaller. This dissertation focuses on hardware data prefetching at the last cache level before memory (last level cache, LLC). Prefetching at the LLC usually offers the best performance increase, as this is where the disparity between hit and miss latencies is the largest. Hardware prefetchers operate by examining the miss address stream generated by the cache and identifying patterns and correlations between the misses. Most prefetchers divide the global miss stream in several sub-streams, according to some pre-specified criteria. This process is known as localization. The benefits of localization are well established: it increases the accuracy of the predictions and helps filtering out spurious, non-predictable misses. However localization has one important drawback: since the misses are classified into different sub-streams, important chronological information is lost. A consequence of this is that most localizing prefetchers issue prefetches in an untimely manner, fetching data too far in advance. This behavior promotes data pollution in the cache. The first part of this thesis proposes a new class of prefetchers based on the novel concept of Stream Chaining. With Stream Chaining, the prefetcher tries to reconstruct the chronological information lost in the process of localization, while at the same time keeping its benefits. We describe two novel Stream Chaining prefetching algorithms based on two state of the art localizing prefetchers: PC/DC and C/DC. We show how both prefetchers issue prefetches in a more timely manner than their nonchaining counterparts, increasing performance by as much as 55 % (10 % on average) on a suite of sequential benchmarks, while consuming roughly the same amount of memory bandwidth. In order to hide the effects of the memory wall, hardware prefetchers are usually configured to aggressively prefetch as much data as possible. However, a highly aggressive prefetcher can have negative effects on performance. Factors such as prefetching accuracy, cache pollution and memory bandwidth consumption have to be taken into account. This is specially important in the context of multi-core systems, where typically each core has its own prefetching engine and there is high competition for accessing memory. Several prefetch throttling and filtering mechanisms have been proposed to maximize the effect of prefetching in multi-core systems. The general strategy behind these heuristics is to promote prefetches that are more likely to be used and cause less interference. Traditionally these methods operate at the source level, i. e., directly into the prefetch engine they are assigned to control. In multi-core systems all prefetches are aggregated in a FIFO-like data structure called the <b>Prefetch</b> Request <b>Queue</b> (PRQ), where they wait to be dispatched to memory. The second part of this thesis shows that a traditional FIFO PRQ does not promote a timely prefetching behavior and usually hinders part of the performance benefits achieved by throttling heuristics. We propose a novel approach to prefetch aggressiveness control in multi-cores that performs throttling at the PRQ (i. e., global) level, using global knowledge of the metrics of all prefetchers and information about the global state of the PRQ. To do this, we introduce the Resizable Prefetching Heap (RPH), a data structure modeled after a binary heap that promotes timely dispatch of prefetches as well as fairness in the distribution of prefetching bandwidth. The RPH is designed as a drop-in replacement of traditional FIFO PRQs. We compare our proposal against a state-of-the-art source-level throttling algorithm (HPAC) in a 8 -core system. Unlike previous research, we evaluate both multiprogrammed and multithreaded (parallel) workloads, using a modern prefetching algorithm (C/DC). Our experimental results show that RPH-based throttling increases the throttling performance benefits obtained by HPAC by as much as 148 % (53. 8 % average) in multiprogrammed workloads and as much as 237 % (22. 5 % average) in parallel benchmarks, while consuming roughly the same amount of memory bandwidth. When comparing the speedup over fixed degree prefetching, RPH increased the average speedup of HPAC from 7. 1 % to 10. 9 % in multiprogrammed workloads, and from 5. 1 % to 7. 9 % in parallel benchmarks. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|R

