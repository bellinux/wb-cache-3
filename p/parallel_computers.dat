3459|3027|Public
5|$|Parallel {{programming}} {{languages and}} <b>parallel</b> <b>computers</b> {{must have a}} consistency model (also known as a memory model). The consistency model defines rules for how operations on computer memory occur and how results are produced.|$|E
5|$|<b>Parallel</b> <b>computers</b> {{based on}} {{interconnected}} networks {{need to have}} some kind of routing to enable the passing of messages between nodes that are not directly connected. The medium used for communication between the processors is likely to be hierarchical in large multiprocessor machines.|$|E
5|$|<b>Parallel</b> <b>computers</b> can {{be roughly}} {{classified}} {{according to the}} level at which the hardware supports parallelism, with multi-core and multi-processor computers having multiple processing elements within a single machine, while clusters, MPPs, and grids use multiple computers to work on the same task. Specialized parallel computer architectures are sometimes used alongside traditional processors, for accelerating specific tasks.|$|E
40|$|This report {{describes}} an exercise {{carried out to}} port a FORTRAN program operating on one <b>parallel</b> <b>computer</b> system to a different <b>parallel</b> <b>computer</b> system. Transonic Small perturbation Code developed on the FLOSOLVER system at the National Aeronautica 1 Laboratory was ported to the SUPRENUM <b>parallel</b> <b>computer.</b> A limited analysis was also made of computational performanc...|$|R
40|$|Abstract — In a {{heterogeneous}} <b>parallel</b> <b>computer</b> system, the computational power {{of each of}} the processors differs from one another. Furthermore, with distributed memory, the capacity of the memory, which is distributed to each of the processors, differs from one another. Using queuing system to describe a distributed memory heterogeneous <b>parallel</b> <b>computer</b> system, each of the heterogeneous processors will have its own heterogeneous queue. The variation of waiting time of heterogeneous <b>parallel</b> <b>computer</b> system with distributed memory needs to be modeled because it will help designers of <b>parallel</b> <b>computer</b> system {{to determine the extent of}} variation of the waiting time. It will also help users to know when to realize minimum variation of the waiting time. This paper models the variation of the waiting time of distributed memory heterogeneous <b>parallel</b> <b>computer</b> system using recursive models. It also uses the statistical method of Z-Transform to verify and validate the recursive model...|$|R
40|$|In this paper, a {{parallel}} computing system for image intelligent processing is described. This parallel computing system includes two main parts: A <b>parallel</b> <b>computer</b> {{and a set}} of software tools. The <b>parallel</b> <b>computer</b> is constructed by a host processor, a SIMD coprocessor, a stream memory and a controller of this memory. Furthermore, the <b>parallel</b> <b>computer</b> can be extended by different kinds of organization with the host processors and the SIMD coprocessors. Programmers write image intelligent processing programs in a C-like language {{and a set of}} software tools maps these programs to code that runs on the <b>parallel</b> <b>computer.</b> These two parts work together to provide a high performance for image intelligent processing...|$|R
5|$|<b>Parallel</b> <b>computers</b> can {{be roughly}} {{classified}} {{according to the}} level at which the hardware supports parallelism. This classification is broadly analogous to the distance between basic computing nodes. These are not mutually exclusive; for example, clusters of symmetric multiprocessors are relatively common.|$|E
5|$|Concurrent {{programming}} languages, libraries, APIs, {{and parallel}} programming models (such as algorithmic skeletons) {{have been created}} for programming <b>parallel</b> <b>computers.</b> These can generally be divided into classes based on the assumptions they make about the underlying memory architecture—shared memory, distributed memory, or shared distributed memory. Shared memory programming languages communicate by manipulating shared memory variables. Distributed memory uses message passing. POSIX Threads and OpenMP {{are two of the}} most widely used shared memory APIs, whereas Message Passing Interface (MPI) is the most widely used message-passing system API. One concept used in programming parallel programs is the future concept, where one part of a program promises to deliver a required datum to another part of a program at some future time.|$|E
25|$|The Intel Scientific Computers {{division}} {{was founded in}} 1984 by Justin Rattner, in order to design and produce <b>parallel</b> <b>computers</b> based on Intel microprocessors connected in hypercube topologies. In 1992, the name was changed to the Intel Supercomputing Systems Division, {{and development of the}} iWarp architecture was also subsumed. The division designed several supercomputer systems, including the Intel iPSC/1, iPSC/2, iPSC/860, Paragon and ASCI Red. In November 2014, Intel revealed {{that it is going to}} use light beams to speed up supercomputers. The renowned chip maker has also disclosed that all its Supercomputer forms will use optical technology for data transfer from 2015.|$|E
40|$|In a {{heterogeneous}} <b>parallel</b> <b>computer</b> system, thecomputational power {{of each of}} the processors differs from oneanother. Furthermore, with distributed memory, the capacity ofthe memory, which is distributed to each of the processors, differsfrom one another. Using queuing system to describe a distributedmemory heterogeneous <b>parallel</b> <b>computer</b> system, each of theheterogeneous processors will have its own heterogeneous queue. The variation of waiting time of heterogeneous parallel computersystem with distributed memory needs to be modeled because itwill help designers of <b>parallel</b> <b>computer</b> system to determine theextent of variation of the waiting time. It will also help users toknow when to realize minimum variation of the waiting time. Thispaper models the variation of the waiting time of distributedmemory heterogeneous <b>parallel</b> <b>computer</b> system using recursivemodels. It also uses the statistical method of Z-Transform to verifyand validate the recursive model...|$|R
5000|$|... #Subtitle level 2: Biological {{brain as}} massively <b>parallel</b> <b>computer</b> ...|$|R
5000|$|... #Caption: Intel iPSC/2 16-node <b>parallel</b> <b>computer.</b> August 22, 1995.|$|R
500|$|As <b>parallel</b> <b>computers</b> become {{larger and}} faster, it becomes {{feasible}} {{to solve problems}} that previously took too long to run. Parallel computing is used {{in a wide range}} of fields, from bioinformatics (protein folding and sequence analysis) to economics (mathematical finance). Common types of problems found in parallel computing applications are: ...|$|E
500|$|SIMD <b>parallel</b> <b>computers</b> can {{be traced}} back to the 1970s. The {{motivation}} behind early SIMD computers was to amortize the gate delay of the processor's control unit over multiple instructions. In 1964, Slotnick had proposed building a massively parallel computer for the Lawrence Livermore National Laboratory. His design was funded by the US Air Force, which was the earliest SIMD parallel-computing effort, ILLIAC IV. The key to its design was a fairly high parallelism, with up to 256processors, which allowed the machine to work on large datasets in what would later be known as vector processing. However, ILLIAC IV was called [...] "the most infamous of supercomputers", because the project was only one fourth completed, but took 11years and cost almost four times the original estimate. When it was finally ready to run its first real application in 1976, it was outperformed by existing commercial supercomputers such as the Cray-1.|$|E
2500|$|Sorting {{algorithms}} {{are also}} given for <b>parallel</b> <b>computers.</b> [...] These algorithms {{can all be}} run on a single instruction stream multiple data stream computer. Habermann's parallel neighbor-sort (or {{the glory of the}} induction principle) sorts k elements using k processors in k steps. This article introduces Optimal Algorithms for Paraller Computers where rk elements can be sorted using k processors in k steps.|$|E
40|$|We {{propose a}} simple method to {{construct}} staples in lattice gauge theory with Wilson action on a <b>parallel</b> <b>computer.</b> This method can {{be applicable to}} any dimensional system and to any dimensional division without difficulty. Furthermore this requires rather small working area to realize gauge simulation on a <b>parallel</b> <b>computer.</b> 1...|$|R
5000|$|Leonard Uhr (Ed.) <b>Parallel</b> <b>Computer</b> Vision. Boston: Academic Press. 1987.|$|R
5000|$|World’s First <b>Parallel</b> <b>Computer</b> Based on Biomolecular Motors (DOI: 10.1073/pnas.1510825113) ...|$|R
2500|$|GEM {{uses the}} delta-f particle-in-cell (PIC) plasma {{simulation}} method. An expansion about an adiabatic response {{is made for}} electrons to overcome the limit of small time step, which {{is caused by the}} fast motion of electrons. [...] GEM uses a novel electromagnetic algorithm allowing direct numerical simulation of the electromagnetic problem at high plasma pressures. [...] GEM uses a two-dimensional domain decomposition (see domain decomposition method) of the grid and particles to obtain good performance on massively <b>parallel</b> <b>computers.</b> [...] A Monte Carlo method is used to model small angle Coulomb collisions.|$|E
2500|$|At {{the age of}} 18, Plateau {{had wanted}} to become a medical doctor, but her father advised against it. After {{studying}} at the École Normale Supérieure and , Brigitte Plateau submitted in 1980 a postgraduate thesis (DEA) in computer science at the University of Paris XI and in 1984, a state computer thesis. She obtained a (fr) at the Centre national de la recherche scientifique (CNRS), then taught as a visiting scholar at the University of Maryland in the United States. In 1988, she was appointed full professor by Grenoble Polytechnic Institute, assigned to Ensimag and to the laboratory of Computer Engineering. In 1999, she created the IT and distribution Laboratory (Grenoble INP-UJF-CNRS) that she headed until 2004. In January 2007 Plateau created the Grenoble Informatics Laboratory associated with the French Institute for Research in Computer Science and Automation (INRIA). [...] As of 2014, Plateau was in charge of 500 computer scientists at the laboratory. Her research work is on the performance of computer systems, in particular distributed and parallel systems. She is studying queueing models, distributed algorithms and massively <b>parallel</b> <b>computers</b> (by simulation and observation). She is an expert in high speed calculations using massive parallelism.|$|E
50|$|Neural & Massively <b>Parallel</b> <b>Computers,</b> 1988.|$|E
40|$|This report {{describes}} {{a collection of}} software utilities that together form a "parallel processing gateway" for the computer aided control system design software package,MATLAB. These utilities allow control engineers to configure and boot processes on the <b>parallel</b> <b>computer</b> and concurrent and parallel routines transparently from the MATLAB command line. Here, the requirements of such a gateway, its design, implementation and use and features that enable the control engineer to readily exploit a <b>parallel</b> <b>computer</b> are described and discussed. In addition, {{the performance of the}} gateway is assessed in terms of the communications achievable between MATLAB and the <b>parallel</b> <b>computer...</b>|$|R
40|$|A CMOS VLSI {{layout and}} {{verification}} of a 3 x 3 processor <b>parallel</b> <b>computer</b> has been completed. The layout was done using the MAGIC tool and the verification using HSPICE. Suggestions for expanding the computer into a million processor network are presented. Many problems {{that might be}} encountered when implementing a massively <b>parallel</b> <b>computer</b> are discussed...|$|R
5000|$|IEEE Fellow, 2008: Awarded for his {{contributions}} in <b>Parallel</b> <b>Computer</b> Architectures and Compilers.|$|R
50|$|Most <b>parallel</b> <b>computers,</b> as of 2013, are MIMD systems.|$|E
5000|$|The Royal Institute of Technologies - Center for <b>Parallel</b> <b>Computers,</b> (KTH), Stockholm, Sweden ...|$|E
50|$|Some system {{designers}} building <b>parallel</b> <b>computers</b> pick CPUs {{based on}} the speed per dollar.|$|E
40|$|Two {{algorithms}} for LU-decomposition on a transputer based reconfigurable MIMD <b>parallel</b> <b>computer</b> with {{distributed memory}} have been analyzed {{in view of}} the interdependence of granularity and execution time. In order to investigate this experimentally, LU-decomposition algorithms have been implemented on a <b>parallel</b> <b>computer,</b> the Parsytec SuperCluster 128. The results of this investigation may be summarized as follows. The LU-decomposition algorithms are very efficient on the <b>parallel</b> <b>computer,</b> if the ratio between problem size and number of processors is not too small. No loss of efficiency is to be expected, if the number of processors is increased only proportionally to the number of elements in the matrix being decomposed...|$|R
5000|$|... #Caption: Intel iPSC/860 32-node <b>parallel</b> <b>computer</b> {{running a}} Tachyon {{performance}} test. August 22, 1995.|$|R
40|$|Parallel {{discrete}} event simulation (PDES), {{sometimes called}} distributed simulation, {{refers to the}} execution of a single discrete event simulation program on a <b>parallel</b> <b>computer.</b> This article deals with the execution of a simulation program on a <b>parallel</b> <b>computer</b> by decomposing the simulation application into a set of concurrently executing process. It Surveys existing approaches, analyzed the merits and drawbacks of various technique...|$|R
5000|$|... 2003 David Kent IV, New Quantum Monte Carlo Algorithms to Efficiently Utilize Massively <b>Parallel</b> <b>Computers</b> ...|$|E
5000|$|... "VR-CUBE" [...] at Center for <b>Parallel</b> <b>Computers</b> (PDC) at the Royal Institute of Technology (KTH) in Stockholm, Sweden.|$|E
5000|$|From October 2000 until 2004, {{he was the}} {{director}} of CIRI, the CEPBA-IBM Research Institute on <b>parallel</b> <b>computers.</b>|$|E
50|$|Hypertrees are {{a choice}} for <b>parallel</b> <b>computer</b> architecture, used, e.g., in the {{connection}} machine CM-5.|$|R
50|$|The first {{computer}} {{that was not}} serial (the first <b>parallel</b> <b>computer)</b> was the Whirlwind — 1951.|$|R
40|$|Abstract. We derive cost {{formulae}} {{for three}} di erent parallelisation techniques for training supervised networks. These formulae are parameterised by {{properties of the}} target computer architecture. It is therefore possible to decide the best match between <b>parallel</b> <b>computer</b> and training technique. One technique, exemplar parallelism, is far superior for almost all <b>parallel</b> <b>computer</b> architectures. Formulae also take into account optimal batch learning as the overall training approach. ...|$|R
