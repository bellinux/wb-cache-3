0|1737|Public
40|$|The {{design and}} {{characteristics}} of the scientific instrument package for the Large Space Telescope are discussed. The subjects include: (1) general scientific objectives, (2) package system analysis, (3) scientific instrumentation, (4) <b>imaging</b> <b>photoelectric</b> <b>sensors,</b> (5) environmental considerations, and (6) reliability and maintainability...|$|R
40|$|Sampling of <b>photoelectric</b> <b>imaging</b> {{system could}} {{infinitely}} replicate the scene spectrum in spectrum space. If the sampling intervals were {{too large to}} satisfy the Nyquist criterion, the replicas would be overlapped and there was aliasing. The optical transfer function could demonstrate that how the optical imaging, integral sampling and reconstruction affected the overall performance. The optical transfer function was used for analysis of the <b>photoelectric</b> <b>imaging</b> system with OLPF. The results show that optic low-pass filter could effectively limit the scene spectrum width and critically satisfy Nyquist sampling condition, so the aliasing could be eliminated {{and quality of the}} image reconstructed is improved. link_to_subscribed_fulltex...|$|R
40|$|A {{laser scanner}} computes a {{range from a}} laser line to an <b>imaging</b> <b>sensor.</b> The laser line {{illuminates}} a detail within an area covered by the <b>imaging</b> <b>sensor,</b> the area having a first dimension and a second dimension. The detail has a dimension perpendicular to the area. A traverse moves a laser emitter coupled to the <b>imaging</b> <b>sensor,</b> at a height above the area. The laser emitter is positioned at an offset along the scan direction {{with respect to the}} <b>imaging</b> <b>sensor,</b> and is oriented at a depression angle with respect to the area. The laser emitter projects the laser line along the second dimension of the area at a position where a image frame is acquired. The <b>imaging</b> <b>sensor</b> is sensitive to laser reflections from the detail produced by the laser line. The <b>imaging</b> <b>sensor</b> images the laser reflections from the detail to generate the image frame. A computer having a pipeline structure is connected to the <b>imaging</b> <b>sensor</b> for reception of the image frame, and for computing the range to the detail using height, depression angle and/or offset. The computer displays the range to the area and detail thereon covered by the image frame...|$|R
25|$|Since {{then the}} number of megapixels in <b>imaging</b> <b>sensors</b> have {{increased}} steadily, with most companies focusing on high ISO performance, speed of focus, higher frame rates, the elimination of digital 'noise' produced by the <b>imaging</b> <b>sensor,</b> and price reductions to lure new customers.|$|R
40|$|Proposed {{subsystem}} {{of small}} auxiliary <b>imaging</b> <b>sensors</b> yields additional data on motion of linear <b>imaging</b> <b>sensor</b> scanned across scene perpendicular to its length. Sensors yield data on components of motion to which inertial, radar, and air-speed detectors insensitive. Additional data on motion enhances geometric fidelity...|$|R
5000|$|... #Subtitle level 3: Interferometric {{reflectance}} <b>imaging</b> <b>sensor</b> ...|$|R
30|$|Traditional <b>imaging</b> <b>sensors,</b> such as {{visible and}} {{infrared}} cameras or laser radar systems, {{may have a}} reduced performance in adverse weather conditions, like fog [1 – 3]. Furthermore, in defense and security scenarios, smoke screens [4] may literally blind these <b>imaging</b> <b>sensors</b> based on very short wavelengths.|$|R
40|$|Advanced <b>imaging</b> <b>sensor</b> {{technologies}} that are being developed for future NASA earth observation missions are discussed. These include the multilinear array, the Shuttle imaging spectrometer, and the Shuttle imaging radar. The principal specifications and functional descriptions of the instruments are presented, and it is shown that the advanced technologies will enable a synergistic approach {{to the use of}} VIS/IR and microwave <b>imaging</b> <b>sensors</b> for remote sensing research and applications. The key problems posed by these future <b>imaging</b> <b>sensor</b> technologies are discussed, with particular attention given to data rates, power consumption, and data processing...|$|R
40|$|Imaging sensors {{are being}} used as data {{acquisition}} systems in new biomedical applications. These applications require wide dynamic range (WDR), high linearity and high signal-to-noise ratio (SNR), which cannot be met simultaneously by existing CMOS <b>imaging</b> <b>sensors.</b> This paper introduces a new activity-triggered WDR CMOS <b>imaging</b> <b>sensor</b> with very low distortion. The new WDR pixel includes self-resetting circuits to partially quantize the photocurrent in the pixel. The pixel residual analog voltage is further quantized by a low-resolution column-wise ADC. The ADC code and the partially quantized pixel codes are processed by column-wise digital circuits to form WDR images. Calibration circuits {{are included in the}} pixel to improve the pixel linearity by a digital calibration method, which requires low calibration overhead. Current-mode difference circuits are included in the pixel to detect activities within the scene so that the <b>imaging</b> <b>sensor</b> captures high quality images only for scenes with intense activity. A proof-of-concept 32 x 32 <b>imaging</b> <b>sensor</b> is fabricated in a 0. 35 mu m CMOS process. The fill factor of the new pixel is 27 %. Silicon measurements show that the new <b>imaging</b> <b>sensor</b> can achieve 95. 3 dB dynamic range with low distortion of - 75. 6 after calibration. The maximum SNR of the sensor is 74. 5 dB. The <b>imaging</b> <b>sensor</b> runs at frame rate up to 15 Hz...|$|R
40|$|A novel {{focal plane}} <b>imaging</b> <b>sensor</b> capable of real time {{extraction}} of polarization information is presented. The imaging system {{consists of a}} photo array of 256 by 256 linear current mode active pixel sensors (APS). Analog processing circuitry is included at the focal plane for noise suppression and computation of the Stokes parameters. The <b>imaging</b> <b>sensor</b> was fabricated in 0. 18 μm process with 10 μm pixel pitch and 75 % fill factor. An array of micro polarizer is designed and fabricated separately and will be mounted {{on top of the}} imaging array. Simulation results of the <b>imaging</b> <b>sensor</b> are presented...|$|R
5000|$|... 3D {{modeling}} and tracking of clouds from satellite and terrestrial <b>imaging</b> <b>sensors</b> ...|$|R
40|$|Modern {{service robots}} {{need to be}} able to "see" the world. In order to design a {{perception}} system for robots modern range <b>imaging</b> <b>sensors</b> are used in conjunction with traditional colour <b>imaging</b> <b>sensors.</b> Two recognition systems build on this: a trainable object detector and a human kinematics reconstruction algorithm that can be used e. g. for gesture recognition...|$|R
40|$|The {{chemical}} <b>imaging</b> <b>sensor</b> is a semiconductor-based {{chemical sensor}} that can visualize the two-dimensional distribution of chemical {{species in the}} specimen. One of the prospective applications of the chemical <b>imaging</b> <b>sensor</b> is the visualization of biochemical activities in biological systems. The chemical <b>imaging</b> <b>sensor</b> was successfully applied {{for the detection of}} Escherichia coli colonies. For further applications to biological systems, enhanced adhesion of the sample to the sensing surface seems to be indispensable. As a candidate material for biocompatible sensors, porous Si was tested and was found to have a high PH sensitivity of 57. 3 mV/pH. (C) 2001 Elsevier Science Ltd. All rights reserved...|$|R
40|$|DE 102009007868 B 3 UPAB: 20100527 NOVELTY - The system (22) has an <b>imaging</b> <b>sensor</b> (7) i. e. {{ultrasonic}} sensor, {{for receiving}} of imaging data {{of an object}} (13) to be detected. An optical sensor (9) receives image data of the <b>imaging</b> <b>sensor</b> and the object. The <b>imaging</b> <b>sensor</b> is arranged in a receiving region (14) of the optical sensor. An evaluating unit (16) evaluates the imaging data and an image data. The evaluating unit is designed such that relative movement between the <b>imaging</b> <b>sensor</b> and the object is detected from the image data. A three dimensional object image is reconstructed based on the detected relative movement. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for detecting an object. USE - Sensor system for use in an endoscope system (claimed) to detect an object i. e. organ, in medial application. Can {{also be used for}} testing a material e. g. textile, foaming and insulation materials, and a component such as air and space technology components and rotor blade of a wind power plant. ADVANTAGE - The design of the system enables simple detection of relative movement between the <b>imaging</b> <b>sensor</b> and the object. The system is provided with high tolerance with respect to input movement of the object...|$|R
40|$|The light-addressable {{potentiometric}} sensor (LAPS) is an electrochemical sensor with a field-effect structure {{to detect the}} variation of the Nernst potential at its sensor surface, the measured area on which is defined by illumination. Thanks to this light-addressability, the LAPS {{can be applied to}} chemical <b>imaging</b> <b>sensor</b> systems, which can visualize the two-dimensional distribution of a particular target ion on the <b>sensor</b> surface. Chemical <b>imaging</b> <b>sensor</b> systems are expected to be useful for analysis of reaction and diffusion in various electrochemical and biological samples. Recent developments of LAPS-based chemical <b>imaging</b> <b>sensor</b> systems, in terms of the spatial resolution, measurement speed, image quality, miniaturization and integration with microfluidic devices, are summarized and discussed...|$|R
3000|$|... is the {{illumination}} component, {{which can}} be regarded as the light directly entered into an <b>imaging</b> <b>sensor,</b> and f [...]...|$|R
30|$|Being less {{sensitive}} to the <b>imaging</b> <b>sensor</b> and geometric distortion, graph entropy can help to locate potential target area correctly.|$|R
50|$|NICMOS is {{also the}} name of the devices's 256×256-pixel <b>imaging</b> <b>sensor</b> built by Rockwell International Electro-Optical Center (now DRS Technologies).|$|R
5000|$|In 2007 this {{camcorder}} had {{the largest}} <b>imaging</b> <b>sensor</b> for a consumer camcorder, 1/2.5". Other distinguishing features of this camcorder include: ...|$|R
50|$|Some common {{applications}} of foveated <b>imaging</b> include <b>imaging</b> <b>sensor</b> hardwareand image compression.For descriptions {{of these and}} other applications, see the list below.|$|R
40|$|Abstract- The {{so-called}} {{triangulation method}} of contact-less dimension measurement determines the measured object’s dimension from its shadow projected on an <b>imaging</b> <b>sensor</b> without lens. A point light source {{is used to}} illuminate the measured object. The so far used linear CCD sensors introduced a considerable uncertainty to the measurement. Application of area CMOS <b>imaging</b> <b>sensors</b> improves the measurement uncertainty and enables to use some new measurement methods. A special CMOS measuring camera was designed for this purpose. I...|$|R
40|$|Electro-optical <b>imaging</b> <b>sensors</b> {{form the}} {{principal}} devices that are typically {{used to obtain}} images of earth from satellite platforms, and substantial improvements in optics, electro-optical detection devices, cooling subsystems and composite structures have enabled the high quality performance exhibited by the current instrumentation. This paper reviews the fundamentals of system design for <b>imaging</b> <b>sensors</b> such as the Thematic Mapper on board Landsat, points out the key areas of current R&D, and briefly alludes to some important applications...|$|R
50|$|Georgian army {{upgraded}} T-72SIM-1 {{tanks are}} using Drawa-T fire control system, a {{development of the}} fire control system on PT-91. The FCS is equipped with laser range finder and thermal <b>imaging</b> <b>sensor.</b> The system is slightly {{different from the one}} used on Polish PT-91s, commander is using an LCD screen instead of the eyepiece. The Thermal Elbow Sight thermal <b>imaging</b> <b>sensor</b> used in Georgian tanks is of the same (Israeli) origin as the one used on PT-91 but the external housing is different.|$|R
40|$|AbstractThe {{chemical}} <b>imaging</b> <b>sensor</b> is a field-effect sensor {{which is}} able to visualize both the distribution of ions (in LAPS mode) {{and the distribution of}} impedance (in SPIM mode) inthe sample. In this study, a novel wound-healing assay is proposed, in which the chemical <b>imaging</b> <b>sensor</b> operated in SPIM mode is applied to monitor the defect of a cell layer brought into proximity of the sensing surface. A reduced impedance inside the defect, which was artificially formed ina cell layer, was successfully visualized in a photocurrent image...|$|R
40|$|This {{thesis is}} based on {{modifications}} performed on the U. S. Army TACOM (Tank Automotive Command, Warren, Michigan) Thermal Imaging Model (TTIM). It discusses the TTIM computer model of a staring thermal <b>imaging</b> <b>sensor</b> with respect to spatial nonuniformities. The spatial nonuniformities in a staring sensor is caused by fixed pattern noise or responsivity variations across the sensor. The objective of the thesis is to present the correction schemes for spatial nonuniformities present on a staring thermal <b>imaging</b> <b>sensor</b> and the data analysis of the corrections using flat field and bar chart targets of known temperatures. The signal-to-noise ratios (S/Ns) of the images will be calculated and measured {{before and after the}} correction. A simulated image after a one-point correction will be evaluated by comparison with an image from a real system using a platinum silicide thermal <b>imaging</b> <b>sensor.</b> The limits and assumptions of the simulation also will be discussed...|$|R
5000|$|TTL {{metering}} {{systems have}} been incorporated into other types of cameras as well. Most digital [...] "point-and-shoot cameras" [...] use TTL metering, performed by the <b>imaging</b> <b>sensor</b> itself.|$|R
40|$|Investigations {{into the}} {{feasibility}} of sensing ocean color from high altitude for determination of chlorophyll and sediment distributions {{have been carried out}} using sensors on NASA aircraft, coordinated with surface measurements carried out by oceanographic vessels. Spectrometer measurements in 1971 and 1972 led to development of an <b>imaging</b> <b>sensor</b> now flying on a NASA U- 2 and the Coastal Zone Color Scanner to fly on Nimbus G in 1978. Results of the U- 2 effort have shown the <b>imaging</b> <b>sensor</b> to also be of great value in sensing pollutants in the ocean...|$|R
40|$|This work proposes two {{particle}} filter-based visual trackers — one using output {{images from}} a color camera {{and the other}} using images from a time-of-flight range <b>imaging</b> <b>sensor.</b> These proposed trackers were compared {{in order to identify}} the advantages and drawbacks of utilizing output images from the color camera as opposed to output from the time-of-flight range <b>imaging</b> <b>sensor</b> for the most efficient visual tracking. This paper is also unique in its novel mixture of efficient methods to produce two stable and reliable human trackers using the two cameras. </p...|$|R
5000|$|With {{the advance}} of the digital <b>imaging</b> <b>sensors,</b> the same ETTR {{technique}} may be applicable to scenes with relatively high dynamic range (DR) (high-contrast, containing both very bright highlights and deep shadows, harshly-lit scenes), previously {{in the domain of}} HDR (High Dynamic Range) techniques involving multiple exposures. The better of the recent photographic <b>imaging</b> <b>sensors</b> of the 35 mm [...] "full-frame" [...] format, as of 2015, can accommodate up to about 14 stops of engineering dynamic range or 11.5 stops of useful photographic dynamic range in the raw shooting mode.|$|R
30|$|Multi-sensor—Image {{collection}} {{from multiple}} <b>imaging</b> <b>sensors</b> could {{be appropriate for}} mitigating degrading factors from uncontrollable and personalized attributes. Fusion could {{be done on the}} image features for age estimation.|$|R
50|$|The SkySat {{satellites}} {{are based}} on the CubeSat concept with optimized design using inexpensive automotive grade electronics, as well as fast commercially available processors. The cameras use two-dimensional <b>imaging</b> <b>sensors.</b>|$|R
5000|$|... 2009: Willard S. Boyle, George E. Smith {{shared the}} Nobel Prize in Physics with Charles K. Kao. Boyle and Smith were cited for inventing {{charge-coupled}} device (CCD) semiconductor <b>imaging</b> <b>sensors.</b>|$|R
50|$|His {{research}} interests include molecular <b>imaging</b> <b>sensors</b> {{for the study of}} redox biology and metals, especially as applied to neuroscience and immunology, metal catalysts for renewable energy cycles, and green chemistry.|$|R
50|$|The AN/AAQ-26 is {{a second}} {{generation}} infrared detection set manufactured by Raytheon. The infrared detecting set is a high-performance multipurpose thermal <b>imaging</b> <b>sensor,</b> providing long-range navigation, surveillance, and fire control capabilities.|$|R
30|$|In smart {{lighting}} systems, {{the sensors}} being used can be generally {{divided into two}} categories: <b>imaging</b> <b>sensors</b> and non-imaging sensors. The computer vision community usually employs <b>imaging</b> <b>sensors</b> such as cameras and depth sensors to capture images, videos, and depth maps of the scene. An image, whether gray-level, RGB, or depth, has a 2 D structure, which describes the spatial distribution of objects or people in the space. A great deal of high-level information can be inferred from such data with computer vision and pattern recognition methods, which enable various applications such as object detection and tracking, event detection, and traffic surveillance.|$|R
50|$|Infrared image sensors include {{active and}} passive systems. For active sensing, IR <b>imaging</b> <b>sensors</b> {{typically}} scan a laser {{across the field}} of view of a scene and look for backscattered light at the absorption line wavelength of a specific target gas. Passive IR <b>imaging</b> <b>sensors</b> measure spectral changes at each pixel in an image and look for specific spectral signatures that indicate the presence of target gases. The types of compounds that can be imaged are the same as those that can be detected with infrared point detectors, but the images may be helpful in identifying the source of a gas.|$|R
