31|0|Public
50|$|Light {{post-editing}} implies {{minimal intervention}} by the <b>post-editor,</b> as strictly required {{to help the}} end user make {{some sense of the}} text; the expectation is that the client will use it for inbound purposes only, often when the text is needed urgently, or has a short time span.|$|E
50|$|After {{some thirty}} years, {{post-editing}} is still “a nascent profession”. What the right {{profile of the}} <b>post-editor</b> is {{has not yet been}} fully studied. Post-editing overlaps with translating and editing, but only partially. Most think the ideal <b>post-editor</b> will be a translator keen to be trained on the specific skills required, but there are some who think a bilingual without a background in translation may be easier to train. Not much is known either on who the actual post-editors are, whether they tend to be professional translators, whether they work mostly as in-house employees or self-employed, and on which conditions. Many professional translators dislike post-editing, among other reasons because it tends to be paid at lower rates than conventional translations, with the International Association of Professional Translators and Interpreters (IAPTI) having been particularly vocal about it.|$|E
5000|$|Post-editing {{involves}} the correction of machine translation output {{to ensure that}} it meets a level of quality negotiated in advance between the client and the <b>post-editor.</b> Light post-editing aims at making the output simply understandable; full post-editing at making it also stylistically appropriate. With advances in machine translation full post-editing is becoming an alternative to manual translation. Practically all computer assisted translation (CAT) tools now support post-editing of machine translated output.|$|E
50|$|Post-editing (or postediting) is {{the process}} whereby humans amend machine-generated {{translation}} to achieve an acceptable final product. A person who post-edits is called a <b>post-editor.</b> The concept of post-editing is linked to that of pre-editing. In the process of translating a text via machine translation, best results may be gained by pre-editing the source textfor example by applying the principles of controlled languageand then post-editing the machine output. It is distinct from editing, which refers {{to the process of}} improving human generated text (a process which is often known as revision in the field of translation). Post-edited text may afterwards be revised to ensure the quality of the language choices or proofread to correct simple mistakes.|$|E
40|$|The {{pause to}} word ratio, {{the number of}} pauses per word in a post-edited MT segment, is an {{indicator}} of cognitive effort in post-editing (Lacruz and Shreve, 2014). We investigate how low the pause threshold can reasonably be taken, and we propose that 300 ms is a good choice, as pioneered by Schilperoord (1996). We then seek to identify a good measure of the cognitive demand imposed by MT output on the <b>post-editor,</b> {{as opposed to the}} cognitive effort actually exerted by the <b>post-editor</b> during post-editing. Measuring cognitive demand is closely related to measuring MT utility, the MT quality as perceived by the <b>post-editor.</b> HTER, an extrinsic edit to word ratio that does not necessarily correspond to actual edits per word performed by the <b>post-editor,</b> is a well-established measure of MT quality, but it does not comprehensively capture cognitive demand (Koponen, 2012). We investigate intrinsic measures of MT quality, and so of cognitive demand, through edited-error to word metrics. We find that the transfer-error to word ratio predicts cognitive effort better than mechanical-error to word ratio (Koby and Champe, 2013). We identify specific categories of cognitively challenging MT errors whose error to word ratios correlate well with cognitive effort...|$|E
40|$|The {{present study}} has {{surveyed}} <b>post-editor</b> trainees ’ views and attitudes {{before and after}} the introduction of speech technology as a front end to a computer-aided translation workbench. The aim of the survey was (i) to identify attitudes and perceptions among <b>post-editor</b> trainees before performing a post-editing task using automatic speech recognition (ASR); and (ii) to assess the degree to which post-editors ’ attitudes and expectations to the use of speech technology changed after actually using it. The survey was based on two questionnaires: the first one administered before the participants performed with the ASR system and the second one {{at the end of the}} session, once they have actually used ASR while post-editing machine translation outputs. Overall, the results suggest that the surveyed <b>post-editor</b> trainees tended to report a positive view of ASR in the context of post-editing and they would consider adopting ASR as an input method for future post-editing tasks. ...|$|E
40|$|This paper {{describes}} the <b>Post-Editor</b> Z sys-tem {{submitted to the}} L 2 writing assis-tant task in SemEval- 2014. The aim of task {{is to build a}} translation assistance system to translate untranslated sentence fragments. This is not unlike the task of post-editing where human translators improve machine-generated translations. <b>Post-Editor</b> Z emulates the manual pro-cess of post-editing by (i) crawling and ex-tracting parallel sentences that contain the untranslated fragments from a Web-based translation memory, (ii) extracting the pos-sible translations of the fragments indexed by the translation memory and (iii) apply-ing simple cosine-based sentence similar-ity to rank possible translations for the un-translated fragment...|$|E
40|$|We {{investigate}} {{the effect of}} four different competitive machine translation systems on <b>post-editor</b> productivity and behaviour. The study involves four volunteers post-editing automatic translations of news sto-ries from English to German. We see sig-nificant difference in productivity due to the systems (about 20 %), and even bigger variance between post-editors. ...|$|E
40|$|Abstract. This paper {{focuses on}} domain {{specific}} use of MT {{with a special}} focus on SMT in the workflow of a Language Service Provider (LSP). We report on the feedback of post-editors using fluency/adequacy evaluation and the evaluation metric ’Usability’, understood in this context as where users on a three point scale evaluate the sentence {{from the point of}} view of the <b>post-editor.</b> The <b>post-editor</b> profile defined by the LSP is based on the experiences of introducing MT in the LSP workflow. The relation between the Translation Edit Rate (TER) scores and ‘Usability ’ scores is tested. We find TER a candidate for an automatic metric simulating the post-editors ’ usability judgements. LSP tests show 67 % saved time in post-editing for the tested domain. Finally, the use of weighted sub-domain phrase tables in a SMT system is shown to improve translation quality...|$|E
40|$|In {{the design}} of {{effective}} machine translation systems the direction of translation is fundamental. "Import" translation from foreign languages into the user's language should be contrasted with "export" translation from the user's language into foreign languages. The structural transfer approach used in most current MT systems is very effective for import translation. But it is less effective for export translation, which is extremely dependent on the human <b>post-editor.</b> An alternative approach to export translation ([Wilcock 91]) uses a source language functional interface to a target language systemic grammar for generation. By helping the source language user to make choices in target language systems, the extreme dependence on the <b>post-editor</b> is reduced. Further developments of this approach are being investigated, including incremental refinement of the systemic choices for "forward generation" of the target text. Other desirable developments would be confirmation of the choices by "back generation" into the user language, and checking for ambiguity in the target text by "back translation"...|$|E
40|$|In {{order to}} improve the {{symbiosis}} between machine translation (MT) system and <b>post-editor,</b> {{it is not enough}} to know that the output of one system is better than the output of another system. A fine-grained error analysis is needed to provide information on the type and location of errors occurring in MT and the corresponding errors occurring after post-editing (PE). This article reports on a fine-grained translation quality assessment approach which was applied to machine translated-texts and the post-edited versions of these texts, made by student post-editors. By linking each error to the corresponding source text-passage, it is possible to identify passages that were problematic in MT, but not after PE, or passages that were problematic even after PE. This method provides rich data on the origin and impact of errors, which can be used to improve <b>post-editor</b> training as well as machine translation systems. We present the results of a pilot experiment on the post-editing of newspaper articles and highlight the advantages of our approach...|$|E
40|$|Translators {{who work}} by {{post-editing}} ma-chine translation output often find them-selves repeatedly correcting the same er-rors. We propose {{a method for}} Post-edit Propagation (PEPr), which learns <b>post-editor</b> corrections and applies them on-the-fly to further MT output. Our proposal {{is based on a}} phrase-based SMT system, used in an automatic post-editing (APE) setting with online learning. Simulated experi-ments on a variety of data sets show that for documents with high levels of internal repetition, the proposed mechanism could substantially reduce the post-editing effort. ...|$|E
40|$|We {{present a}} {{supervised}} learning pilot ap-plication for estimating Machine Transla-tion (MT) output reusability, {{in view of}} supporting a human <b>post-editor</b> of MT content. We train our model on typed dependencies (labeled grammar relation-ships) extracted from human reference and raw MT data, to then predict gram-mar relationship correctness values that we aggregate to provide a binary segment-level evaluation. In view of scaling up to larger data, we provide implemented Naı̈ve Bayes and Stochastic Gradient De-scent with Support Vector Machine loss function approaches and their evaluation, and verify the correlation of predicted val-ues with human judgement. ...|$|E
40|$|Machine {{translation}} systems {{should improve}} with feedback from post-editors, but none do beyond the very localized benefit of adding the corrected translation to parallel training data (for statistical and example-base MTS) or a memory data base. Rule based systems to date improve only via manual debugging. In contrast, we introduce a largely automated method for capturing more {{information from the}} human <b>post-editor,</b> so that corrections may be performed automatically to translation grammar rules and lexical entries. This paper focuses on the information capture phase and reports on an experiment with English-Spanish translation. 1...|$|E
40|$|Using machine {{translation}} output {{as a starting}} point for human translation has recently gained traction in the transla-tion community. This paper describes cdec Realtime, a framework for build-ing adaptive MT systems that learn from <b>post-editor</b> feedback, and TransCenter, a web-based translation interface that con-nects users to Realtime systems and logs post-editing activity. This combination allows the straightforward deployment of MT systems specifically for post-editing and analysis of human translator produc-tivity when working with these systems. All tools, as well as actual post-editing data collected as part of a validation exper-iment, are freely available under an open source license. ...|$|E
40|$|For {{this work}} we have {{carried out a}} number of anal-ysis {{experiments}} comparing raw MT output pro-duced by Microsoft’s Treelet MT engine (Quirk et al., 2005) with its human post-edited counterpart, for English–German and English–French. Through these experiments we identify a number of interest-ing post-editing patterns, both textual (string-based) and constituent-based. In this paper we discuss our analysis methodologies, present some of our results and provide information on how this type of analy-sis can be of benefit to translation systems and post-editors, with a view to improving initial MT output and consequently <b>post-editor</b> productivity. In addi-tion, we also discuss the MT and post-editing work-flow at Microsoft and results from MT post-editing pilots for a number of different language pairs. ...|$|E
40|$|Null {{subjects}} are non overtly expressed subject pronouns found in pro-drop languages such as Italian and Spanish. In {{this study we}} quantify and compare the occurrence of this phenomenon in these two languages. Next, we evaluate null subjects’ translation into French, a “non prodrop” language. We use the Europarl corpus to evaluate two MT systems on their performance regarding null subject translation: Its- 2, a rule-based system developed at LATL, and a statistical system built using the Moses toolkit. Then we add a rule-based preprocessor and a statistical <b>post-editor</b> to the Its- 2 translation pipeline. A second evaluation of the improved Its- 2 system shows an average increase of 15. 46 % in correct pro-drop translations for Italian-French and 12. 80 % for Spanish-French. ...|$|E
40|$|This paper {{explores the}} skills and profile of the new role of the {{translator}} as MT <b>post-editor</b> {{in view of the}} rising interest and use of MT in the translation industry. After a brief review of the relevant literature declaring post-editing (PE) as a profession on its own, the paper goes on to identify the different tasks involved in PE processes, following the work of Krings (Krings, 2001). Then, a series of competences are defined and grouped into three main categories: core competences, linguistic skills and instrumental competences. Finally, a description of the controlled translation scenario of MT PE is advanced taking into account the overall scenario of any translation project, including client description, text domain, text description, use of glossaries, MT engine, MT output quality and purpose of the translated text. No data (2012...|$|E
40|$|Supervised {{approaches}} to NLP tasks rely on high-quality data annotations, which typically result from expensive manual labelling procedures. For some tasks, however, the subjectivity of human judgements might reduce {{the usefulness of}} the annotation for real-world applications. In Machine Translation (MT) Quality Estimation (QE), for instance, using humanannotated data to train a binary classifier that discriminates between good (useful for a <b>post-editor)</b> and bad translations is not trivial. Focusing on this binary task, we show that subjective human judgements can be effectively replaced with an automatic annotation procedure. To this aim, we compare binary classifiers trained on different data: the human-annotated dataset from the 7 th Workshop on Statistical Machine Translation (WMT- 12), and an automatically labelled version of the same corpus. Our results show that human labels are less suitable for the task. ...|$|E
40|$|Abstract. This paper {{addresses}} the practical challenge of improving existing, op-erational translation systems with relatively weak, black-box MT engines when higher quality MT engines {{are not available}} and only a limited quantity of online re-sources is available. Recent research results show impressive performance gains in translating between Indo-European languages when chaining mature, existing rule-based MT engines and post-MT editors built automatically with limited amounts of parallel data. We show that this hybrid approach of serially composing or “chaining” an MT engine and automated post-MT editor [...] -when applied to much weaker lexi-con-based and rule-based MT engines, translating across the more widely divergent languages of Urdu and English, and given limited amounts of document-parallel only training data [...] -will yield statistically significant boosts in translation quality up to the 50 K of parallel segments in training the <b>post-editor,</b> but not necessarily be-yond that...|$|E
40|$|State-of-the-art {{statistical}} {{machine translation}} (MT) systems have made significant progress towards producing user-acceptable translation output. However, {{there is still no}} efficient way for MT systems to inform users which words are likely translated correctly and how confident it is about the whole sentence. We propose a novel framework to predict wordlevel and sentence-level MT errors with a large number of novel features. Experimental results show that the MT error prediction accuracy is increased from 69. 1 to 72. 2 in F-score. The Pearson correlation between the proposed confidence measure and the human-targeted translation edit rate (HTER) is 0. 6. Improvements between 0. 4 and 0. 9 TER reduction are obtained with the n-best list reranking task using the proposed confidence measure. Also, we present a visualization prototype of MT errors at the word and sentence levels with the objective to improve <b>post-editor</b> productivity. ...|$|E
40|$|The {{advantages}} of {{neural machine translation}} (NMT) have been extensively validated for offline translation of several language pairs for different domains of spoken and written language. However, research on interactive learning of NMT by adaptation to human post-edits {{has so far been}} confined to simulation experiments. We present the first user study on online adaptation of NMT to user post-edits. Our study involves 29 human subjects whose post-editing effort and translation quality were measured on about 4, 500 interactions of a human <b>post-editor</b> and a machine translation system integrating an online adaptive learning algorithm. Our experimental results show a significant reduction of human post-editing effort due to online adaptation in NMT according to several evaluation metrics, including hTER, hBLEU, and KSMR. Furthermore, we found significant improvements in BLEU/TER between NMT outputs and human references, and a strong correlation of these improvements with quality improvements of post-edits...|$|E
40|$|Translation {{agencies}} are introducing sta- tistical machine translation (SMT) {{into the work}} flow of human translators. Typ- ically, SMT produces a first-draft transla- tion, which is then post-edited by a per- son. SMT has met much resistance from translators, partly because of professional conservatism, but partly because the SMT community has often neglected some practical aspects of translation. Our paper discusses one of these: transferring formatting tags such as bold or italic from the source to the target document with a low error rate, thus freeing the <b>post-editor</b> from having to reformat SMT-generated text. In our 2 ̆ 01 ctwo-stream 2 ̆ 01 d approach, tags are stripped from the input to the decoder, then reinserted into the resulting target-language text. Tag trans- fer has been tackled by other SMT teams, {{but only a few}} have published descrip- tions of their work. This paper contrib- utes to understanding tag transfer by ex- plaining our approach in detail. Peer reviewed: YesNRC publication: Ye...|$|E
40|$|For a {{professional}} user of MT, quality, performance and cost efficiency are critical. It is therefore surprising that only little attention – both {{in theory and}} in practice- {{has been given to}} the task of post-editing machine translated texts. This paper will focus on this important user aspect and demonstrate that substantial savings in time and effort can be achieved by implementing intelligent automatic tools. Our point of departure is the PaTrans MT-system, developed by CST and used by the Danish translation company Lingtech. An intelligent post-editing facility, Ape, has been developed and added to the system. We will outline and discuss this mechanism and its positive effects on the output. The underlying idea of the intelligent post-editing facility is to exploit the lexical and grammatical knowledge already present in the MT-system’s linguistic components. Conceptually, our approach is general, although its implementation remains system specific. Surveys of <b>post-editor</b> satisfaction and cost-efficiency improvements, as well as a quantitative, benchmark-based evaluation of the effect of Ape demonstrate the success of the approach and encourage further development...|$|E
40|$|Post-editing (PE) is a {{necessary}} process in ev ery MT deployment environment. The compe tences needed for PE are traditionally seen as a subset of a human translator's competence. Meanwhile, some companies are accepting that the PE process involves self-standing lin guistic tasks, which need their own training efforts and appropriate software tool support. To date, we still lack recorded qualitatively and quantitatively PE user-activity data that adequately describe the tasks {{and in particular the}} human cognitive processes accomplished. This data is needed to effectively model, de sign and implement supportive software sys tems which, on the one hand, efficiently guide the human <b>post-editor</b> and enhance her cogni tive capabilities, and on the other hand, have a certain influence on the translation perfor mance and competence of the employed MT system. In this paper we argue for a frame work of practices to describe the PE process by correlating data obtained in laboratory ex periments and augmented by additional data from different resources such as interviews and mathematical prediction models with the tasks fulfilled, and to model the identified pro cess in a multi-facetted fashion as a basis for the implementation of a human PE-aware in teractive software system. ...|$|E
40|$|Abstract. Statistical Machine Translation (SMT) is {{currently}} used {{in real-time and}} commercial settings to quickly produce initial translations for a document which can later be edited by a human. The SMT models specialized for one domain often perform poorly when applied to other domains. The typical assumption that both training and testing data are drawn from the same distribution no longer applies. This paper evaluates domain adaptation techniques for SMT systems {{in the context of}} end-user feedback in a real world application. We present our experiments using two adaptive techniques, one relying on log-linear models and the other using mixture models. We describe our experimental results on legal and government data, and present the human evaluation effort for post-editing in addition to traditional automated scoring techniques (BLEU scores). The human effort is based primarily on the amount of time and number of edits required by a professional <b>post-editor</b> {{to improve the quality of}} machine-generated translations to meet industry standards. The experimental results in this paper show that the domain adaptation techniques can yield a significant increase in BLEU score (up to four points) and a significant reduction in post-editing time of about one second per word...|$|E
40|$|There is {{a growing}} demand for {{translation}}. To meet this demand, many translation companies are introducing a hybrid technology solution combining translation memory and machine translation. However, few trainee translators receive training in machine translation post-editing. This paper asks the question: Why should translator training programmes teach post-editing skills? Is post-editing the same as translation and traditional revision? The skill-sets required of a <b>post-editor</b> are listed and the usual list of skills is extended. An outline for a course in post-editing, divided into theoretical and practical components, is proposed. Finally, the question of when such a course {{should be given to}} trainee translators is addressed. 1. Why teach post-editing? 1. 1 Growing Demand The global market for translation was valued at around $ 13 billion in 2000, and a growth to around $ 22. 7 billion by the end of 2005 has been predicted. 1 This increasing demand has led to an increase in the use of translation aids, including terminology management tools, translation memory (TM) and machine translation (MT) technology. At a recent conference on multilingual communication, leading translation companies reported that they are now testing and implementing a hybrid TM-MT technology solution to meet the growing demand for translation. ...|$|E
40|$|While machine {{translation}} is sometimes sufficient for conveying information across language barriers, many scenarios still require precise human-quality translation that MT is currently unable to deliver. Governments {{and international organizations}} such as the United Nations require accurate translations of content dealing with complex geopolitical issues. Community-driven projects such as Wikipedia rely on volunteer trans-lators to bring accurate information to diverse language communities. As the amount of data requiring translation has continued to increase, the idea of using {{machine translation}} to improve the speed of human translation has gained interest. In the frequently employed practice of post-editing, a machine translation system outputs an initial translation and a human translator edits it for correctness, ideally saving time over translating from scratch. While general improvements in MT quality have led to productivity gains with this technique, there has been little work on designing translation systems specifically for post-editing. In this work, we propose improvements to key components of statistical machine translation systems aimed at directly reducing the amount of work required from human translators. We propose casting MT for post-editing as an online learning task where new training instances are created as humans edit system output, introducing an online translation model that immediately learns from <b>post-editor</b> feedback. W...|$|E
40|$|Post-editing machine {{translation}} (MT) {{is an important}} step towards high quality translations. In order to better understand the post-editing process, {{a better understanding of the}} relationship between MT output and the post-editing of this output is necessary. While automatic evaluation metrics such as the widely used BLEU can be used to compare the overall quality of different MT systems, a more detailed error analysis is necessary to identify the typical errors that appear in MT output and the subsequent post-editing. By using a fine-grained Translation Quality Assessment approach and grouping translation errors into source text-related error sets, we link MT errors to errors after post-editing to examine their relationship. We are mainly interested in answering the following questions: What (and how many) MT errors are solved by post-editors, what (and how many) problems occur in post-edited MT and which (and how many) of these originate from MT? We present the results of a pilot study in which student translators post-edited newspaper articles and user documentation from English into Dutch. We found that the MT errors that student post-editors most easily correct are grammatical errors, whereas e. g. wrong collocations, word sense disambiguation errors and the misspelling of compounds prove to be more challenging. As such, we can identify the types of errors that <b>post-editor</b> training should pay more attention to...|$|E
40|$|We {{design and}} {{evaluate}} several models for integrating Machine Translation (MT) output into a Translation Memory (TM) environment {{to facilitate the}} adoption of MT technology in the localization industry. We begin with the integration on the segment level via translation recommendation and translation reranking. Given an input to be translated, our translation recommendation model compares the output from the MT and the TMsystems, and presents the better one to the <b>post-editor.</b> Our translation reranking model combines k-best lists from both systems, and generates a new list according to estimated post-editing effort. We perform both automatic and human evaluation on these models. When measured against the consensus of human judgement, the recommendation model obtains 0. 91 precision at 0. 93 recall, and the reranking model obtains 0. 86 precision at 0. 59 recall. The high precision of these models indicates {{that they can be}} integrated into TM environments without the risk of deteriorating the quality of the post-editing candidate, and can thereby preserve TM assets and established cost estimation methods associated with TMs. We then explore methods for a deeper integration of translation memory and machine translation on the sub-segment level. We predict whether phrase pairs derived from fuzzy matches could be used to constrain the translation of an input segment. Using a series of novel linguistically-motivated features, our constraints lead both to more consistent translation output, and to improved translation quality, reflected by a 1. 2 improvement in BLEU score and a 0. 72 reduction in TER score, both of statistical significance (p < 0. 01). In sum, we present our work in three aspects: 1) translation recommendation and translation reranking models that can access high quality MT outputs in the TMenvironment, 2) a sub-segment translation memory and machine translation integration model that improves both translation consistency and translation quality, and 3) a human evaluation pipeline to validate the effectiveness of our models with human judgements...|$|E
40|$|Vandeghinste V., Vanallemeersch T., Van Eynde F., Macken L., Lefever E., Hoste V., Moens M. -F., Pelemans J., Wambacq P., Haesen M., Coninx K., De Wachter K., ''Smart {{computer}} aided translation environment'', 17 th {{annual conference}} of the European Association for Machine Translation - EAMT 2014, June 16 - 18, 2014, Dubrovnik, Croatia. In the SCATE project we aim at improving the translators' efficiency. Commercial translation tools are faced with ever higher productivity requirements imposed by the globalisation of business activities and the increasing information flow. The SCATE project intends to improve translators' efficiency along the following axes: • Exploitation of already translated data – We will exploit data more exhaustively through the use of syntactic models for fuzzy matching, and detect syntactically similar constructions in the translation memory. We will investigate complex types of translation grammar induction and tree alignment that allow to transduce source syntax trees into target trees (i. e. accepting one tree and producing another). We will investigate how to seamlessly integrate MT into a translation memory, by automatically resolving the syntactic fuzziness of the match through MT techniques. • Translation evaluation – We will automatically judge whether MT output is worth post-editing, or whether the suggested translation can be applied to resolve the fuzzy match in the translation memory. We will build an annotated data set and a taxonomy of typical translation errors and combine this with loggings and analysis of human-machine interaction during post-editing, which targets improvements in automatic confidence estimation of machine translation output. • Terminology extraction – We will automatically extract terminology from comparable corpora in order to speed up the translation process and make translations more consistent. Therefore we will study translator's methods in acquiring domain terminology. We will also research methods to determine which texts in different languages contain comparable information, and we will improve current methods of terminology extraction from comparable corpora through techniques such as cross-lingual topic modelling. • Speech recognition – We will integrate the language model of the MT engine with the language model of the speech recogniser. We will study the adaptation of the recogniser as an input method for the <b>post-editor,</b> and investigate the improvement of speech transcription for translation purposes. Furthermore we will study how to perform automatic domain-adaptation for speech recognition, in order to automatically adapt the language models of the recogniser to the domain. • Workflows and personalised user interfaces – We aim at a higher comfort and productivity for the translators, by analysing and modelling current translation systems and translator's workflows and practices, investigating new visualisations of translation features, and developing and testing new interfaces for translation work. status: publishe...|$|E

