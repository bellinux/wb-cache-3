5|21|Public
50|$|Houdini's set {{of tools}} are mostly {{implemented}} as operators. This {{has led to a}} higher learning curve than other comparable tools. It is one thing to know what all the nodes do - but the key to success with Houdini is understanding how to represent a desired creative outcome as a network of nodes. Successful users are generally familiar with a large repertoire of networks (algorithms) which achieve standard creative outcomes. The overhead involved in acquiring this repertoire of algorithms is offset by the artistic and algorithmic flexibility afforded by access to lower level building blocks with which to configure shot element creation routines. In large productions, the development of a <b>procedural</b> <b>network</b> to solve a specific element creation challenge makes automation trivial. Many studios that use Houdini on large feature effects, and feature animation projects develop libraries of procedures {{that can be used to}} automate generation of many of the elements for that film with almost no artist interaction.|$|E
40|$|The current {{research}} {{was designed to}} study knowledge development while minimizing methodological problems common to research in that area. First, a <b>procedural</b> <b>network</b> methodology compatible with one commonly used to study declarative knowledge was developed in Experiment 1. In Experiment 2, multiple longitudinal knowledge measures revealed dissociations among the developmental courses of different forms of knowledge. The findings contribute to a growing pool of evidence of the dissociation of declarative and procedural knowledge development. Further, they contradict the majority of knowledge development theories which hypothesize that procedural knowledge development {{is dependent on the}} development of declarative knowledge. These findings also indicate that much work {{needs to be done to}} determine the relationships among multiple forms of knowledge and to determine how they could be optimally trained. In order for research in these areas to progress, efforts to minimize methodological confounding problems must continue...|$|E
40|$|Speaker-independence {{and large}} lexicon access are still {{two of the}} {{greatest}} problems in automatic speech recognition. Cognitive and information-theory approaches try to solve the recognition problem by proceeding in almost opposite directions. The former rely on knowledge representation, reasoning and perceptual analysis, while the latter is in general based on highly numerical and mathematical algorithms. Progress arises from {{the integration of the}} two mentioned approaches. Artificial intelligence techniques are often used in the cognitive approach, but these techniques usually lack sophisticated numerical support. The Extended <b>Procedural</b> <b>Network</b> constitutes a general AI framework which supports powerful numerical strategies which include stochastic techniques. The model has been tested on difficult problems in speech recognition, including speaker-independent letter and digit recognition, speaker-independent vowel and diphthong recognition, and access to a large lexicon. Various experiments and comparisons have been run on a large number of speakers and the results are reported. A discussion of further research advancements and investigations is provided...|$|E
40|$|We {{present a}} spatiotemporal model, namely, <b>procedural</b> neural <b>networks</b> for stock price prediction. Compared with some {{successful}} traditional models on simulating stock market, such as BNN (backpropagation neural networks, HMM (hidden Markov model) and SVM (support vector machine)), the <b>procedural</b> neural <b>network</b> model processes both spacial and temporal information synchronously without slide time window, which is typically {{used in the}} well-known recurrent neural networks. Two different structures of <b>procedural</b> neural <b>networks</b> are constructed for modeling multidimensional time series problems. Learning algorithms for training the models and sustained improvement of learning are presented and discussed. Experiments on Yahoo stock market {{of the past decade}} years are implemented, and simulation results are compared by PNN, BNN, HMM, and SVM...|$|R
50|$|Arkin {{has since}} {{performed}} in American independent feature films and major <b>network</b> <b>procedural</b> shows: CSI: Crime Scene Investigation, Stitchers, Rake.|$|R
40|$|A new {{diagnostic}} {{modeling system}} for automatically synthesizing a deep structure {{model of a}} student's misconceptions or bugs in his/her basic mathematics skills Trovides a medhanism for explaining why a student is making a mistake as opposed to simply identifying the mistake. This report consists of four sections. The first provides examples {{of the problems that}} must be handled by a diagnostic model. It then introduces <b>procedural</b> <b>networks</b> as a general framework for replesentinAbbe. knowledge underlying a skill. The second section discuSSes soilie of the pedagogical issues that have emerged from the use of diagnostic models within an instructional system. This discussion is framed {{in the context of a}} computer-based tutoring/gaming system developed to teach students and student teachers how to diagnose bugs stra egically as well as how to provide a better understanding of the under ing st ucture of arithmetic stills. The third section our nses of an executable networ...|$|R
40|$|This {{study is}} held in CV Putra Nugraha which located in Jl. Merapi Raya No. 17 Surakarta. The {{objectives}} {{of this study is}} to know the implementation of credit selling system, to evaluate the advantageous and disadvantageous of the credit selling system and provide suggestion to CV Putra Nugraha. Research method used in this study is direct observation, interview and learning related-literatures with credit selling system. The elements in credit selling system is the related function, the used documents, the used accounting record, the required information by management, <b>procedural</b> <b>network</b> forming the system, internal controlling system and flow chart. The results of the evaluation of credit selling system in CV Putra Nugraha don’t have credit function prosedur before to existing credit selling, document being used by the authorized part but don’t having order number, then the writer suggests several suggestion for the existing accounting system in CV Putra Nugraha to be better. The suggestions are to make credit function, to provide order numbering in the document. Keyword: Accounting information system, credit selling...|$|E
40|$|Teams {{have the}} {{potential}} to perform much better than many individual does. However, often teams do not reach their full potential. It is often reported that teams use inefficient strategies to accomplish a task. It is also reported that teams are often very resistant to the (spontaneous) development of new and better strategies. It is important to understand the team process. In small group research, the interaction process is a hot topic. Almost in all perceptions on small groups, the process is implicitly or explicitly mentioned (Poole et al., 2004). But if one looks at how 'process' is defined and operationalized in empirical studies a great diversity becomes apparent. Three types of measures on group processes were distinguished: (i) evaluative measures on the group process (typically questionnaire based data, where team members are asked to evaluate the group process), (ii) frequencies of behaviors (coding and counting approaches), and (iii) measures reflecting temporal sequences in the group process. It is argued that evaluative measures and summaries of frequencies cannot represent all aspects of the group process and that an analysis of the temporal micro-behavioral organization of the group process offers additional information. This information can then be used for an effective coaching of co-acting teams. Data of 109 teams of three persons were analyzed. An air-traffic control (ATC) simulation was used. The teams had to observe an airspace and keep track of the changing positions of the airplanes. Every single act was coded such that its type and its meaning were captured together with its time stamp. Sequential analyses were run to gain gaining insight into the group process. Three methods are discussed and applied to the ATC-data: (i) lag sequential analysis, (ii) <b>procedural</b> <b>network</b> representation, (iii) data mining techniques. These methods are never (data mining) or seldom used in small group research. Results show that either of these methods can be used to analyze the group process on a micro-behavioral level. They can be used as exploratory tools and aids to visualize the group process. Several examples are shown. All these sequential patterns must be evaluated {{in the context of the}} concrete task. This is perfect for team coaching or the development or refinement of categories developed in a top down process (like for example the task adaptive behaviors in Tschan, Semmer, Nägele et al., 2000). There are also concrete recommendations to commanders of co-acting virtual teams: Use the information from the specialists, read what they are writing. Do not send task or strategy related messages if the message content is too old. Nevertheless, commanders should not stay too long in a mode of just reading messages. Measures form sequential analyses were used in regression models to predict the team performance. This was done separately for every of the seven work-shifts of at least fifteen minutes. Results show that using this information on the sequential patterns in regression models explains additional variance in performance per shift in the range from 16 % to 35 % (additionally to input factors, preceding performance and frequencies of behaviors). The information extracted with sequential analyses is relevant to performance. To improve the group process in co-acting teams detailed analyses of the temporal sequences on a micro-behavioral level is indispensable. It was shown that lag sequential analysis, <b>procedural</b> <b>network</b> representations (PRONET), and data mining techniques can be used. It is also demonstrated how this methods can be applied to the ATC-data...|$|E
25|$|But mostly Against the Wall is a {{pleasant}} surprise, with Carpani being {{a much bigger}} surprise. If Against the Wall can make its disparate parts work, it will be plenty more intriguing than a number of <b>network</b> <b>procedurals.</b> And in the cable game, that's already a victory.|$|R
40|$|Presented at the 21 st International Conference on Auditory Display (ICAD 2015), July 6 - 10, 2015, Graz, Styria, Austria. In the {{following}} paper we present Aegis: a <b>procedural</b> <b>networked</b> soundtrack engine driven by real-time analog signal analysis and pattern recognition. Aegis was originally conceived {{as part of}} Drummer Game, a game-performance-spectacle hybrid research project focusing on the depiction of a battle portrayed using terracotta soldiers. In it, each of the twelve cohorts—divided into two armies of six—are led by a drummer-performer who issues commands by accurately drumming precomposed rhythmic patterns on an original Chinese war drum. The ensuing spectacle is envisioned to also accommodate large audience participation whose input determines {{the morale of the}} two armies. An analog signal analyzer utilizes efficient pattern recognition to decipher the desired action and feed it both into the game and the soundtrack engine. The soundtrack engine then uses this action, as well as messages from the gaming simulation, to determine the most appropriate soundtrack parameters while ensuring minimal repetition and seamless transitions between various clips that account for tempo, meter, and key changes. The ensuing simulation offers a comprehensive system for pattern-driven input, holistic situation assessment, and a soundtrack engine that aims to generate a seamless musical experience without having to resort to cross-fades and other simplistic transitions that tend to disrupt a soundtrack’s continuity...|$|R
40|$|We {{present a}} model for growing <b>procedural</b> road <b>networks</b> in and close to cities. The main idea of our paper is that a city cannot be meaningfully {{simulated}} without taking its neighbourhood into account. A simple traffic simulation that considers this neighbourhood is then used to grow new major roads and to influence the locations of minor road growth. Waterways are introduced and used to help position the city nuclei on the map. The resulting cities are formed by allowing several smaller settlements to grow together and to form a rich road structure, much like in real world, and require only minimal per-city input, allowing for batch generation...|$|R
40|$|In {{the design}} of an Office Information System, the Architecture Design {{is the process of}} {{defining}} an hardware and software configuration which is suitable for the realization of the information system. This paper introduces a formalI theory of Architectures and a system, called Architecture Specification System (ASPES), based on this theory, for the construction of office architectures. ASPES embodies the knowledge about hardware and software components and their compatibilities {{in the form of a}} <b>procedural</b> semantic <b>network.</b> This knowledge is made available by providing an interactive environment, accessible through a graphical interface, to incrementaly construct office architectures. In addition, ASPES guarantees that the Architecture being constructed by the designer is a feasible Architecture, that is it consists of components that can be effectively combined together...|$|R
40|$|We {{present a}} hybrid system that {{integrates}} speech and image understanding. Given spoken references, it {{is able to}} identify objects of a 3 D scene perceived via a stereo camera. Central to our approach is the extraction of qualitative object features and spatial scene properties from acoustic and visual data. The interaction of the understanding processes is performed using a <b>procedural</b> semantic <b>network</b> that interfaces with signal recognition and reconstruction modules, thus integrating semantic, neural and Bayesian networks and Hidden Markov Models. 1. INTRODUCTION Man-Machine-Interaction in real environments {{is one of the greatest}} challenges for a number of scientific fields related to Computer Vision, Speech Understanding, and Robotics. At the University of Bielefeld the joint project "Situated Artificial Communicators" has been established with the goal to develop an integrated system where visual, linguistic, senso-motoric, and cognitive abilities interact. The system plays the rol [...] ...|$|R
40|$|Abstract—A general (application independent) {{computer}} vision framework is proposed. It follows {{the methodology of}} knowledge-base systems- dividing a system into knowledge base and control. We choose <b>procedural</b> semantic <b>networks</b> for object-oriented modelling of the world. It is basically a non-monotonic logical system. Several inference rules are proposed that allow to create instances of model concepts. In order to activate an inference rule a model-to-image data matching process need to be performed. We view this matching {{as a solution to}} constraint satisfaction problem (CSP), supported by Bayesian net-based evaluation of partial variable assignments. A modified incremental search for CSP is designed that allows partial solutions and calls for stochastic inference in order to provide judgments of partial states. Hence the detection of partial occlusion of objects is handled consistently with Bayesian inference over evidence and hidden variables. Keywords-backtrack search; Bayesian net; constraint satisfaction problem; inference rules; knowledge-based system; labelled graph; model-to-image matching; object recognition; semantic network I...|$|R
40|$|AbstractThe paper {{reviews the}} history, premises, {{research}} results and {{applications of the}} <b>Procedural</b> Semantic <b>Network</b> (hereafter PSN) formalism and two of its descendants, Taxis and Telos. The primary goal for the PSN project {{was to develop a}} knowledge representation formalism which combined gracefully semantic <b>networks</b> and <b>procedural</b> representations. After an initial proposal which set out a framework for the integration of these two types of representations [1], a number of features were investigated, including the treatment of contexts, the declarative representation of slots and procedures and several others. In parallel, attempts were made to apply some of the representation ideas embodied in PSN to the task of developing suitable languages for the early stages of information system design. During these stages, the designer is concerned with understanding the problem at hand and then proceeding with the specification of an initial conceptual design of an information system that meets stated requirements. Taxis and Telos were the result of this effort. Their features are outlined, along with a rationale for the differences between the three formalisms...|$|R
40|$|The {{approach}} {{described here}} allows {{to use the}} fuzzy Object Based Representation of imprecise and uncertain knowledge. This representation has a great practical interest due to the possibility to realize reasoning on classification with a fuzzy semantic network based system. For instance, the distinction between necessary, possible and user classes allows {{to take into account}} exceptions that may appear on fuzzy knowledge-base and facilitates integration of user's Objects in the base. This approach describes the theoretical aspects of the architecture of the whole experimental A. I. system we built in order to provide effective on-line assistance to users of new technological systems: the understanding of "how it works" and "how to complete tasks" from queries in quite natural languages. In our model, <b>procedural</b> semantic <b>networks</b> are used to describe the knowledge of an "ideal" expert while fuzzy sets are used both to describe the approximative and uncertain knowledge of novice users in fuzzy semantic networks which intervene to match fuzzy labels of a query with categories from our "ideal" expert. Comment: arXiv admin note: text overlap with arXiv: 1206. 179...|$|R
40|$|Behavioural {{evidence}} suggests that English regular past tense forms are automatically decomposed into their stem and affix (played  = play+ed) based on an implicit linguistic rule, which {{does not apply to}} the idiosyncratically formed irregular forms (kept). Additionally, regular, but not irregular inflections, are thought to be processed through the procedural memory system (left inferior frontal gyrus, basal ganglia, cerebellum). It has been suggested that this distinction does not to apply to second language (L 2) learners of English; however, this has not been tested at the brain level. This fMRI study used a masked-priming task with regular and irregular prime-target pairs (played-play/kept-keep) to investigate morphological processing in native and highly proficient late L 2 English speakers. No between-groups differences were revealed. Compared to irregular pairs, regular pairs activated the pars opercularis, bilateral caudate nucleus and the right cerebellum, which are part of the <b>procedural</b> memory <b>network</b> and have been connected with the processing of morphologically complex forms. Our study is the first to provide evidence for native-like involvement of the procedural memory system in processing of regular past tense by late L 2 learners of English...|$|R
40|$|A {{framework}} for generating computer system Architectures that {{are suitable for}} realizing an Office Information System is presented. The Architecture generation process {{is seen as a}} mapping from a functional specification of the office expressed in terms of a conceptual scheme, and integrated with data concerning the existing office and users 2 ̆ 7 s needs, to a number of office Architectures, each saitsfying by construction the given set of functional and non-functional requirements. Among such Architectures, the most appropriate one with respect to a cost-benefit trade-off is then selected, on the basis of performance measurement obtained via simulation. A methodology for achieving the mapping underlying the Architecture generation process is proposed, and a model for describing office Architectures is provided. Such a model relies on a knowledge base where hardware and software products are organized along with the knowledge for combining them into Architectures. A computer tool exists which Supports an incremental construciion of Architectures according to this model. The tool embodies the knowledge about hardware and software products, and about the possible ways of combining them into Architectures, {{in the form of a}} <b>procedural</b> semantic <b>network,</b> and makes accessible this knowledge to the Architecture designer through an interactive environment supported by a graphical interface...|$|R
40|$|The Abstract Schema Language (ASL) {{defines a}} {{hierarchical}} computational {{model for the}} development of distributed heterogeneous systems. ASL extends the capabilities and methodologies of concurrent object-oriented programming to enable the construction of highly complex multi-granular systems. The ASL model is described in terms of schemas (concurrent objects), supporting aggregation (schema assemblages), and both top-down and bottom-up system designs. ASL encourages code reusability by enabling the integration of heterogeneous components, e. g., <b>procedural</b> and neural <b>network</b> programs. ASL schemas are designed and implemented in an orthogonal fashion; integrated, either statically, through wrapping, or dynamically, via (task) delegation. Schemas include a dynamic interface, made of multiple unidirectional input and output ports, and a body section where schema behavior is specified. Communication {{is in the form of}} asynchronous message passing, hierarchically managed, internally, through [...] ...|$|R
40|$|We {{describe}} an alternate approach to visual recognition of handwritten words, wherein an image is {{converted into a}} spatio-temporal signal by scanning it {{in one or more}} directions, and processed by a suitable connectionist network. The scheme offers several attractive features including shift-invariance, explication of local spatial geometry along the scan direction, a significant {{reduction in the number of}} free parameters, the ability to process arbitrarily long images along the scan direction, and a natural framework for dealing with the segmentation/recognition dilemma. Other salient features of the work include the use of a modular and structured approach for network construction and the integration of connectionist components with a procedural component to exploit the complementary strengths of both techniques. The system consists of two connectionist components and a <b>procedural</b> controller. One <b>network</b> concurrently makes recognition and segmentation hypotheses, and another [...] ...|$|R
40|$|Traditional {{supervised}} {{neural network}} trainers have deviated little from the fundamental back propagation algorithm popularized in 1986 by Rumelhart, Hinton, and Williams. Typically, the training process {{begins with the}} collection of a fixed database of input and output vectors. The operator then adjusts additional parameters such as network architecture, learning rate, momentum, and annealing noise, based upon their past experience in network training. Optimizing the network’s generalization capacity usually involves either experiments with various hidden layer architectures or similar automated investigations using genetic algorithms. Along with these often-complex <b>procedural</b> issues, usable <b>networks</b> generally lack flexibility, beginning {{at the level of}} the individual processing unit. Normally, the user finds him or her confined to a limited range of unit activation functions, usually including linear, linear threshold, and sigmoidal analytical forms whose partial derivatives with respect to net input are definable through a similar continuous, analytical expression. Generally, the demand is for a more flexible and user friendly system that will not only lessen the technical confusion for non-connectionist end-users, but also create expanded utility for those demanding more architectural freedom and adaptability in their artificial neural networks or ANNs...|$|R
40|$|Wireless {{networks}} {{comprise the}} majority of devices within the growing edge of the global communication system. Performance metrics determining the successful application of wireless networks in that setting are goodput, latency and network lifetime. Overhead retransmissions due to redundant data transfer, inefficient transmissions, low link quality, and suboptimal network layer protocols affect negatively these three metrics. Designing wireless networks to minimize the overhead retransmissions encompasses three network levels: the data, structural and procedural levels. Encoded sensing (ES) is a "data-aware" scheme that shapes the network structural level to account for correlations across data sources and common data across groups of nodes. Via new encoding algorithms, ES achieves substantial reduction of the transmissions required to convey a message to a sink node. A few beneficial properties for network hardware and design, based on sparsity of ES signals, are also discussed. The structural level is further augmented by the placement of relay nodes to minimize the overhead retransmissions in the network due to low quality and heavily loaded links. Finally, the Time Sequence Scheme operates on the <b>network</b> <b>procedural</b> level, allowing for broadcast of messages reaching all network nodes, while minimizing redundant broadcast retransmissions. Explicitly minimizing the number of retransmissions {{at each of the}} three network levels impacts beneficially performance as shown by analysis and full network stack simulations. 2020 - 01 - 2...|$|R
40|$|More {{information}} {{is now being}} published in machine processable form on the web and, as de-facto distributed knowledge bases are materializing, partly encouraged by {{the vision of the}} Semantic Web, the focus is shifting from the publication of this information to its consumption. Platforms for data integration, visualization and analysis that are based on a graph representation of information appear first candidates to be consumers of web-based information that is readily expressible as graphs. The question is whether the adoption of these platforms to information available on the Semantic Web requires some adaptation of their data structures and semantics. Ondex is a network-based data integration, analysis and visualization platform which has been developed in a Life Sciences context. A number of features, including semantic annotation via ontologies and an attention to provenance and evidence, make this an ideal candidate to consume Semantic Web information, as well as a prototype for the application of network analysis tools in this context. By analyzing the Ondex data structure and its usage, we have found a set of discrepancies and errors arising from the semantic mismatch between a <b>procedural</b> approach to <b>network</b> analysis and the implications of a web-based representation of information. We report in the paper on the simple methodology that we have adopted to conduct such analysis, and on issues that we have found which may be relevant for a range of similar platformsComment: Presented at DEIT, Data Engineering and Internet Technology, 2011 IEEE: CFP 1113 L-CD...|$|R
40|$|This project, {{funded by}} NASA through the University of Arizona Space Engineering Research Center, is an {{extension}} of earlier work, and is aimed at developing the base technology for continuous profiling geophysical systems that will be able to determine not only where anomalous features lie, but also what they look like and, ultimately, what has caused them. A hybrid approach was used that employed object oriented and <b>procedural</b> programming, neural <b>networks,</b> fuzzy systems theory, genetic algorithms, and symbolic processing initially within a distributed computing architecture, and later on an 80486 PC platform. Neural networks were used to map synthetic GPR patterns to geometric models. Object oriented programming was combined with fuzzy theory to map these geometric models to a database of real world objects. GPR objects were then combined to build extended objects and systems. Genetic algorithms were used to fine tune the system. Testing was done solely with synthetic data, with future intent of progressing to laboratory and field geophysical patterns. The field context assigned to the vision system for this research corresponds to a buried prehistoric archaeological site comprising occupation and utility/storage rooms of various sizes. This context was chosen {{to take advantage of the}} rich suite of GPR patterns already collected, and the GPR modeling underway to characterize archaeological signatures on GPR. A room system was selected as the GPR target because it incorporated most of the basic characteristics expected for GPR systems in general. The interpretation system was able to handle, (1) Multiple components; (2) Offset components; (3) Non-continuous components; (4) Nonmonotonic states; (5) Fuzziness...|$|R
40|$|A city is {{considered}} as a complex system. It consists of numerous interactivesub-systems and is affected by diverse factors including governmental landpolicies, population growth, transportation infrastructure, and market behavior. Land use and transportation systems are considered as the two most importantsubsystems determining urban form and structure in the long term. Meanwhile,urban growth {{is one of the}} most important topics in urban studies, and its maindriving forces are population growth and transportation development. Modelingand simulation are believed to be powerful tools to explore the mechanisms ofurban evolution and provide planning support in growth management. The overall objective of the thesis is to analyze and model urban growth basedon the simulation of land-use changes and the modeling of road networkexpansion. Since most previous urban growth models apply fixed transportnetworks, the evolution of road networks was particularly modeled. Besides,urban growth modeling is an interdisciplinary field, so this thesis made bigefforts to integrate knowledge and methods from other scientific and technicalareas to advance geographical information science, especially the aspects ofnetwork analysis and modeling. A multi-agent system was applied to model urban growth in Toronto whenpopulation growth {{is considered}} as being the main driving factor of urbangrowth. Agents were adopted to simulate different types of interactiveindividuals who promote urban expansion. The multi-agent model with spatiotemporalallocation criterions was shown effectiveness in simulation. Then, anurban growth model for long-term simulation was developed by integratingland-use development with <b>procedural</b> road <b>network</b> modeling. The dynamicidealized traffic flow estimated by the space syntax metric was not only used forselecting major roads, but also for calculating accessibility in land-usesimulation. The model was applied in the city centre of Stockholm andconfirmed the reciprocal influence between land use and street network duringthe long-term growth. To further study network growth modeling, a novel weighted network model,involving nonlinear growth and neighboring connections, was built from theperspective of promising complex networks. Both mathematical analysis andnumerical simulation were examined in the evolution process, and the effects ofneighboring connections were particular investigated to study the preferentialattachment mechanisms in the evolution. Since road network is a weightedplanar graph, the growth model for urban street networks was subsequentlymodeled. It succeeded in reproducing diverse patterns and each pattern wasexamined by a series of measures. The similarity between the properties of derived patterns and empirical studies implies that there is a universal growthmechanism in the evolution of urban morphology. To better understand the complicated relationship between land use and roadnetwork, centrality indices from different aspects were fully analyzed in a casestudy over Stockholm. The correlation coefficients between different land-usetypes and road network centralities suggest that various centrality indices,reflecting human activities in different ways, can capture land development andconsequently influence urban structure. The strength of this thesis lies in its interdisciplinary approaches to analyze andmodel urban growth. The integration of ‘bottom-up’ land-use simulation androad network growth model in urban growth simulation is the major contribution. The road network growth model in terms of complex network science is anothercontribution to advance spatial network modeling within the field of GIScience. The works in this thesis vary from a novel theoretical weighted network modelto the particular models of land use, urban street network and hybrid urbangrowth, and to the specific applications and statistical analysis in real cases. These models help to improve our understanding of urban growth phenomenaand urban morphological evolution through long-term simulations. Thesimulation results can further support urban planning and growth management. The study of hybrid models integrating methods and techniques frommultidisciplinary fields has attracted a lot attention and still needs constantefforts in near future. QC 20130514 </p...|$|R

