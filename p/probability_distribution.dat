10000|10000|Public
5|$|Edward Epstein {{recognized}} in 1969 that the atmosphere {{could not be}} completely described with a single forecast run due to inherent uncertainty, and proposed using an ensemble of stochastic Monte Carlo simulations to produce means and variances {{for the state of}} the atmosphere. Although this early example of an ensemble showed skill, in 1974 Cecil Leith showed that they produced adequate forecasts only when the ensemble <b>probability</b> <b>distribution</b> was a representative sample of the <b>probability</b> <b>distribution</b> in the atmosphere.|$|E
5|$|Because each {{configuration}} {{has only}} a bounded number of predecessors, the evolution of Rule 90 preserves the entropy of any configuration. In particular, if an infinite initial configuration is selected by choosing the state of each cell independently at random, {{with each of the}} two states being equally likely to be selected, then each subsequent configuration can be described by exactly the same <b>probability</b> <b>distribution.</b>|$|E
5|$|Stochastic {{matrices}} are square matrices whose rows are probability vectors, that is, whose {{entries are}} non-negative and sum up to one. Stochastic matrices {{are used to}} define Markov chains with finitely many states. A row of the stochastic matrix gives the <b>probability</b> <b>distribution</b> for the next position of some particle currently in the state that corresponds to the row. Properties of the Markov chain like absorbing states, that is, states that any particle attains eventually, can be read off the eigenvectors of the transition matrices.|$|E
5000|$|Models and <b>probability</b> <b>distributions</b> {{of various}} {{business}} activities either {{in terms of}} various parameters or <b>probability</b> <b>distributions.</b>|$|R
5000|$|... "Among <b>probability</b> <b>distributions</b> on the {{interval}} from 0 to ∞ {{on the real}} line, memorylessness characterizes the exponential distributions." [...] This statement means that the exponential distributions are the only such <b>probability</b> <b>distributions</b> that are memoryless. (See also Characterization of <b>probability</b> <b>distributions.)</b> ...|$|R
5000|$|There is {{not enough}} {{information}} to build <b>probability</b> <b>distributions</b> for the inputs. <b>Probability</b> <b>distributions</b> can be constructed from expert elicitation, although even then {{it may be hard}} to build distributions with great confidence. The subjectivity of the <b>probability</b> <b>distributions</b> or ranges will strongly affect the sensitivity analysis.|$|R
5|$|To assess {{prospective}} {{wind power}} sites a <b>probability</b> <b>distribution</b> function is often {{fit to the}} observed wind speed data. Different locations will have different wind speed distributions. The Weibull model closely mirrors the actual distribution of hourly/ten-minute wind speeds at many locations. The Weibull factor is often close to 2 and therefore a Rayleigh distribution {{can be used as}} a less accurate, but simpler model.|$|E
5|$|The {{inner product}} between two state vectors {{is a complex}} number known as a {{probability}} amplitude. During an ideal measurement of a quantum mechanical system, {{the probability that a}} system collapses from a given initial state to a particular eigenstate is given by the square of the absolute value of the probability amplitudes between the initial and final states. The possible results of a measurement are the eigenvalues of the operator—which explains the choice of self-adjoint operators, for all the eigenvalues must be real. The <b>probability</b> <b>distribution</b> of an observable in a given state can be found by computing the spectral decomposition of the corresponding operator.|$|E
25|$|Confidence limits can {{be found}} if the <b>probability</b> <b>distribution</b> of the {{parameters}} is known, or an asymptotic approximation is made, or assumed. Likewise statistical tests on the residuals can be made if the <b>probability</b> <b>distribution</b> of the residuals is known or assumed. The <b>probability</b> <b>distribution</b> of any linear combination of the dependent variables can be derived if the <b>probability</b> <b>distribution</b> of experimental errors is known or assumed. Inference is particularly straightforward if the errors are assumed to follow a normal distribution, which implies that the parameter estimates and residuals will also be normally distributed conditional on {{the values of the}} independent variables.|$|E
50|$|For <b>probability</b> <b>distributions</b> p and q {{over the}} same domain X, the Bhattacharyya {{distance}} is defined aswhereis the Bhattacharyya coefficient for discrete <b>probability</b> <b>distributions.</b>|$|R
50|$|The {{convolution}} of <b>probability</b> <b>distributions</b> {{arises in}} <b>probability</b> theory and statistics as the operation {{in terms of}} <b>probability</b> <b>distributions</b> that corresponds to the addition of independent random variables and, by extension, to forming linear combinations of random variables. The operation here is a special case of convolution {{in the context of}} <b>probability</b> <b>distributions.</b>|$|R
5000|$|Many <b>probability</b> <b>{{distribution}}s</b> {{can be used}} {{to model}} the failure distribution (see List of important <b>probability</b> <b>distributions).</b> A common model is the exponential failure distribution, ...|$|R
25|$|Probabilistic {{formulation}} of inverse problems {{leads to the}} definition of a <b>probability</b> <b>distribution</b> in the model space. This <b>probability</b> <b>distribution</b> combines prior information with new information obtained by measuring some observable parameters (data).|$|E
25|$|Any <b>{{probability}}</b> <b>distribution</b> {{defines a}} probability measure.|$|E
25|$|Given the {{distribution}} parametersmdash&mean, variances and covariancesmdash&the normal <b>probability</b> <b>distribution</b> for sampling new candidate solutions is the maximum entropy <b>probability</b> <b>distribution</b> over , that is, the sample distribution with the minimal amount of prior information built into {{the distribution}}. More considerations on the update equations of CMA-ES {{are made in the}} following.|$|E
40|$|The minimum 2 −divergence {{principle}} and {{its application to}} characterize the continuous <b>probability</b> <b>distributions,</b> given (i) a prior distribution and (ii) partial information {{in the form of}} average or (iii) partial information in the form of average and variance, are discussed. Uniform <b>probability</b> <b>distributions</b> are studied to illustrate the properties of the minimum 2 −divergence <b>probability</b> <b>distributions...</b>|$|R
40|$|Characterizations of the <b>probability</b> <b>distributions</b> {{have been}} {{discussed}} in the lit-erature and many <b>probability</b> <b>distributions</b> have been well characterized. Among the <b>probability</b> <b>distributions</b> which are yet to be fully characterized is the half logistic distribution. In this paper, some theorems that characterize the half logistic distri-bution are stated and proved. A possible application {{of one of the}} characterization theorems is included...|$|R
5000|$|In {{probability}} theory, Glivenko's theorem {{states that}} if , [...] are the characteristic functions of some <b>probability</b> <b>distributions</b> [...] respectively and [...] almost everywhere, then [...] {{in the sense}} of <b>probability</b> <b>distributions.</b>|$|R
25|$|<b>Probability</b> <b>distribution</b> function: {{continuous}} or discrete, non-cumulative or cumulative.|$|E
25|$|Whereas {{ordinary}} mechanics only {{considers the}} behaviour {{of a single}} state, statistical mechanics introduces the statistical ensemble, which is a large collection of virtual, independent copies of the system in various states. The statistical ensemble is a <b>probability</b> <b>distribution</b> over all possible states of the system. In classical statistical mechanics, the ensemble is a <b>probability</b> <b>distribution</b> over phase points (as opposed to a single phase point in ordinary mechanics), usually represented as a distribution in a phase space with canonical coordinates. In quantum statistical mechanics, the ensemble is a <b>probability</b> <b>distribution</b> over pure states, and can be compactly summarized as a density matrix.|$|E
25|$|Each node of the Chance {{player has}} a <b>probability</b> <b>distribution</b> over its {{outgoing}} edges.|$|E
40|$|In {{the present}} paper we discuss various results related to moments and cumulants of <b>probability</b> <b>distributions</b> and approximations to <b>probability</b> <b>distributions.</b> As the approximations are not {{necessarily}} <b>probability</b> <b>distributions</b> themselves, we shall apply the concept of moments and cumulants to more general functions. Recursions are deduced for the moments and cumulants of functions in the form Rka,b as defined by Dhaene & Sundt (1994). We deduce a simple relation between the DePril transform and the cumulants of a function. This relation is appplied to some classes of approximations to <b>probability</b> <b>distributions,</b> in particular the approximations of Hipp and DePril. ...|$|R
40|$|This {{paper is}} the {{investigation}} of the <b>probability</b> <b>distributions</b> of a finite random set in which the set of random events are considered as a support of the finite random set. These <b>probability</b> <b>distributions</b> can be defined by six equivalent ways (distributions of the I-st — VI-th type). Each of these types of the <b>probability</b> <b>distributions</b> is the set function defined on the corresponding system of events. In this paper the sufficient conditions are formulated and proved. When these conditions are satisfied, then the set function determines the <b>probability</b> <b>distributions</b> of the finite random set of the II-nd and the V-th type. The found conditions supplement the known necessary conditions for the existence of the <b>probability</b> <b>distributions</b> of a finite random set of the II-nd and the V-th type...|$|R
25|$|Most {{introductions}} to {{probability theory}} treat discrete <b>probability</b> <b>distributions</b> and continuous <b>probability</b> <b>distributions</b> separately. The more mathematically advanced measure theory-based treatment of probability covers the discrete, continuous, {{a mix of}} the two, and more.|$|R
25|$|One may {{define a}} uniform <b>probability</b> <b>distribution</b> on the linear {{extensions}} {{in which each}} possible linear extension is equally likely to be chosen. The 1/3–2/3 conjecture states that, under this <b>probability</b> <b>distribution,</b> there exists a pair of elements x and y such that the probability that x is earlier than y in a random linear extension is between 1/3 and 2/3.|$|E
25|$|The log-logistic {{distribution}} is the <b>probability</b> <b>distribution</b> of a random variable whose logarithm has a logistic distribution.|$|E
25|$|Probability density, Probability density function, p.d.f., Continuous <b>probability</b> <b>distribution</b> function: {{most often}} {{reserved}} for continuous random variables.|$|E
40|$|<b>Probability</b> <b>distributions</b> {{produced}} by the cross-entropy loss for ordinal classification problems can possess undesired properties. We propose a straightforward technique to constrain discrete ordinal <b>probability</b> <b>distributions</b> to be unimodal via {{the use of the}} Poisson and binomial <b>probability</b> <b>distributions.</b> We evaluate this approach in the context of deep learning on two large ordinal image datasets, obtaining promising results. Comment: Accepted for publication for ICML 2017. This is the camera-ready versio...|$|R
5000|$|Similarly, a convex {{combination}} [...] of <b>probability</b> <b>distributions</b> [...] is a weighted sum (where [...] satisfy the same constraints as above) of its component <b>probability</b> <b>distributions,</b> often called a finite mixture <b>distribution,</b> with <b>probability</b> density function: ...|$|R
40|$|We {{investigate}} a fundamental property of device independent security in quantum cryptography by characterizing <b>probability</b> <b>distributions</b> which are necessarily {{independent of the}} measurement results of any eavesdropper. We show that <b>probability</b> <b>distributions</b> that are secure in this sense are exactly the extremal quantum <b>probability</b> <b>distributions.</b> This allows us to give a characterization of security in algebraic terms. We apply the method to common examples for two-party as well as multi-party setups and present a scheme for verifying security of <b>probability</b> <b>distributions</b> with two parties, two measurement settings, and two outcomes. Comment: 7 pages, 2 figures, revised version, accepted for publication in Phys. Rev. Let...|$|R
25|$|The <b>probability</b> <b>distribution</b> of {{the sum of}} two {{independent}} random variables is the convolution of each of their distributions.|$|E
25|$|The <b>probability</b> <b>distribution</b> of {{the update}} is {{proportional}} to the product of the measurement likelihood and the predicted state.|$|E
25|$|There is no {{upper limit}} to the excess {{kurtosis}} of a general <b>probability</b> <b>distribution,</b> {{and it may be}} infinite.|$|E
40|$|The {{class of}} Riemann zeta {{distribution}} {{is one of}} the classical classes of <b>probability</b> <b>distributions</b> on R. Multidimensional Shintani zeta function is introduced and its definable <b>probability</b> <b>distributions</b> on R^d are studied. This class contains some fundamental <b>probability</b> <b>distributions</b> such as binomial and Poisson distributions. The relation with multidimensional polynomial Euler product, which induces multidimensional infinitely divisible distributions on R^d, is also studied. Comment: 15 pages. arXiv admin note: text overlap with arXiv: 1204. 404...|$|R
40|$|A {{comparison}} of the point forecasts and the central tendencies of <b>probability</b> <b>distributions</b> of inflation and output growth of the SPF indicates that the point forecasts are sometimes optimistic relative to the <b>probability</b> <b>distributions.</b> We consider and evaluate {{a number of possible}} explanations for this finding, including the degree of uncertainty concerning the future, computational costs, delayed updating, and asymmetric loss. We also consider the relative accuracy of the two sets of forecasts. Rationality; point forecasts; <b>probability</b> <b>distributions...</b>|$|R
40|$|Practical {{limits on}} the size of most {{probabilistic}} models require that <b>probability</b> <b>distributions</b> be approximated by a few representative values and associated probabilities. This paper demonstrates that methods commonly used to determine discrete approximations of <b>probability</b> <b>distributions</b> systematically underestimate the moments of the original distribution. A new procedure based on gaussian quadrature is developed in this paper. It can be used to decrease the error in the approximation to any desired level. decision analysis, approximations, approximate <b>probability</b> <b>distributions...</b>|$|R
