0|147|Public
40|$|Optimizing on-chip {{primary data}} caches for {{parallel}} scientific applications is challenging because different applications exhibit different behavior. Indeed, while some applications exhibit good spatial locality, others have accesses with long strides that prevent the {{effective use of}} cache lines. Finally, other applications cannot exploit long lines because they exhibit false sharing. To help processors execute these three types of applications efficiently, we introduce the <b>Pool</b> <b>Buffer,</b> a small direct-mapped cache accessed in parallel with the primary cache. The function of the <b>pool</b> <b>buffer</b> is to fetch long sectors of relatively short cache lines from memory on a miss, while only letting into the cache the lines that the processor actually references. The <b>pool</b> <b>buffer</b> can also perform sequential prefetching of sectors. An evaluation of the <b>pool</b> <b>buffer</b> based on simulations of five 32 -processor Perfect Club codes yields encouraging results. Adding a <b>pool</b> <b>buffer</b> of one-quarter the size of [...] ...|$|R
40|$|This paper {{presents}} a technique for performing dynamic goal oriented <b>buffer</b> <b>pool</b> management for database management systems. To dynamically adjust the <b>buffer</b> <b>pool</b> sizes for the multiple <b>buffer</b> <b>pools</b> provided by database management systems {{is a complex}} constrained optimization problem. In the goal oriented approach, the user specifies each <b>buffer</b> <b>pool's</b> random access response time goal and the total available number of buffers for all <b>buffer</b> <b>pools.</b> The problem is to dynamically expand or contract the <b>buffer</b> <b>pool</b> sizes based on the database workload to achieve these pre-defined response time goals for each <b>buffer</b> <b>pool</b> while maintaining the same total number of buffers in the database system. Our goal satisfaction algorithm monitors goal satisfaction of each <b>buffer</b> <b>pool</b> and periodically changes <b>buffer</b> <b>pool</b> sizes to improve goal satisfaction. The expanding and contracting process does not allocate new or free up existing virtual storage. We demonstrate that dynamic tuning can greatly improve <b>buffer</b> <b>pool</b> goal satisfaction through trace driven simulations...|$|R
40|$|Database {{management}} systems (DBMSs) use a main memory {{area as a}} buffer {{to reduce the number}} of disk accesses performed by a transaction. Some DBMSs divide the buffer area into a number of independent <b>buffer</b> <b>pools</b> and each database object (table or index) is assigned to a specific <b>buffer</b> <b>pool.</b> The tasks of configuring the <b>buffer</b> <b>pools,</b> which define the mapping of database objects to <b>buffer</b> <b>pools</b> and setting a size for each of the <b>buffer</b> <b>pools,</b> are crucial for achieving optimal performance. In this paper we describe an automated approach to multiple <b>buffer</b> <b>pool</b> configuration. Our approach, called BPCluster, analyses the characteristics of a given workload and partitions objects into <b>buffer</b> <b>pools</b> according to their access patterns and inherent characteristics. Similar objects are grouped into the same <b>buffer</b> <b>pool,</b> thus separating those objects that may conflict. A size configuration for the multiple <b>buffer</b> <b>pools</b> is determined using a greedy algorithm that attempts to minimize the cost of a logical read. A set of experimental results validate the approach and show that the configurations suggested by BPCluster outperform naïve configurations and, in most cases, perform as well as configurations suggested by an experienced database administrator...|$|R
40|$|Storage {{management}} {{is an important}} part of DB 2. The <b>buffer</b> <b>pool</b> in DB 2 is used to catch the disk pages of the database, and its management algorithm can significantly affect performance. Because of the complexity of DB 2 and the workloads running on it, the <b>buffer</b> <b>pool</b> management algorithm is hard to study, config, and tune. In order to investigate the <b>buffer</b> <b>pool</b> management algorithm under controlled circumstances, a trace of <b>buffer</b> <b>pool</b> requests was collected and a trace-driven simulator was developed. The impact of various parameters of the <b>buffer</b> <b>pool</b> management algorithm was studied in the simulator. Relationships among different activities competing for storage spac...|$|R
40|$|When {{a working}} set fits into memory, the {{overhead}} im-posed by the <b>buffer</b> <b>pool</b> renders traditional databases non-competitive with in-memory designs that sacrifice the ben-efits of a <b>buffer</b> <b>pool.</b> However, despite the large memory available with modern hardware, data skew, shifting work-loads, and complex mixed workloads {{make it difficult}} to guarantee that a working set will fit in memory. Hence, some recent work has focused on enabling in-memory databases to protect performance when the working data set almost fits in memory. Contrary to those prior efforts, we en-able <b>buffer</b> <b>pool</b> designs to match in-memory performance while supporting the “big data ” workloads that continue to require secondary storage, thus providing the best of both worlds. We introduce here a novel <b>buffer</b> <b>pool</b> de-sign that adapts pointer swizzling for references between system objects (as opposed to application objects), and uses it to practically eliminate <b>buffer</b> <b>pool</b> overheads for memory-resident data. Our implementation and experimental eval-uation demonstrate that we achieve graceful performance degradation when the working set grows to exceed the <b>buffer</b> <b>pool</b> size, and graceful improvement when the working set shrinks towards and below the memory and <b>buffer</b> <b>pool</b> sizes. 1...|$|R
40|$|Of {{the many}} tuning {{parameters}} {{available in a}} database management system (DBMS), {{one of the most}} crucial to performance is the <b>buffer</b> <b>pool</b> size. Choosing an appropriate size, however, can be a difficult task. In this paper we present an analytical modeling approach to predicting the <b>buffer</b> <b>pool</b> hit rate {{that can be used to}} simplify the process of <b>buffer</b> <b>pool</b> sizing. Since the buffer replacement algorithm determines the buffer hit rate, we model the replacement algorithm which, in the case of DB 2 /UDB, is a variation of the GCLOCK algorithm. A Markov Chain model of GCLOCK is used to estimate the hit rate for a <b>buffer</b> <b>pool.</b> We evaluate the accuracy of the model's estimates with experiments carried out on DB 2 /UDB with the TPC-C benchmark. The model is validated for both single and multiple <b>buffer</b> <b>pool</b> cases...|$|R
40|$|The <b>buffer</b> <b>pool</b> {{manager is}} a central {{component}} of ADABAS, a high performance scaleable database system for OLTP processing. High efficiency and scalability of the <b>buffer</b> <b>pool</b> manager is mandatory for ADABAS on all supported platforms. In order to allow a maximum of parallelism without facing the danger of deadlocks, a multi-version locking method is used. Partitioning of central data structures is another key to performance. Variable page sizes allow for flexible tuning, but make the <b>buffer</b> <b>pool</b> logic more sophisticated, in particular concerning parallelism. ...|$|R
40|$|AbstractThe one-to-multiple file {{transfer}} problem {{has not been}} solved under the Linux system. In this paper, we take socket programming to accomplish Client/Server model based {{file transfer}} system and develop efficient software which not only support multiple clients and files transferring simultaneously, but also support transmission resuming from breakpoint. Our research focuses on solving the multi-thread concurrency, transmission resuming from breakpoint, thread <b>pool,</b> <b>buffer</b> queue {{and other aspects of}} problem. Our work is the basic foundation of FTP servers, file transfer software in local area network and large-scale examination system, etc...|$|R
40|$|With {{the advent}} of 64 -bit processors, large main {{memories}} are set to become very common. This in turn translates to larger <b>buffer</b> <b>pool</b> configurations in database servers. Query optimizers however, currently assume all data is disk resident while optimizing queries. This assumption {{will no longer be}} valid when <b>buffer</b> <b>pools</b> become 100 ’s of gigabytes in size. In this paper we examine how data presence in the <b>buffer</b> <b>pool</b> can affect the choice of query plans in an optimizer. We examine the possible benefits of buffer-pool aware query optimization and propose a generic architecture for implementing such an optimizer. 1...|$|R
40|$|An {{autonomic}} {{database management}} system is a self tuning, self optimizing, self healing and self protecting {{database management system}} (DBMS). Since expert database administrators (DBAs) are scarce, introducing a DBMS that is self tuning will decrease {{the total cost of}} ownership of the system. In this thesis we present a first step to incorporating self tuning capabilities in the PostgreSQL DBMS. The buffer area is the main memory management area of the DBMS. Effective use of this area ensures efficiency of the DBMS. Some DBMSs split the buffer area into multiple <b>buffer</b> <b>pools</b> and this has led to performance increases in some cases. Once multiple <b>buffer</b> <b>pools</b> are supported {{it is up to the}} DBA to size them based on each of their needs. Optimal sizing leads to good performance, therefore, as the workload changes the DBA has to adjust the sizes of the <b>buffer</b> <b>pools.</b> We extend PostgreSQL (Version 7. 3. 2) to support multiple <b>buffer</b> <b>pools.</b> We remove the dependency on the DBA and automatically adjust the sizes of the <b>buffer</b> <b>pools</b> to changes in the environment. We present a number of experiments to verify our approach...|$|R
40|$|<b>Buffer</b> <b>pools</b> are {{blocks of}} memory used in {{database}} systems to retain frequently referenced pages. Configuring the <b>buffer</b> <b>pools</b> {{is a difficult}} and manual task that involves determining the amount of memory to devote to the <b>buffer</b> <b>pools,</b> the number of <b>buffer</b> <b>pools</b> to use, their sizes, and the database objects assigned to each <b>buffer</b> <b>pool.</b> A good <b>buffer</b> configuration improves query response times and system throughput by {{reducing the number of}} disk accesses. Determining a good buffer configuration requires knowledge of the database workload. Empirical studies have shown that optimizing the initial buffer configuration (determined at database design time) can improve system throughput. A good initial configuration can also provide a faster convergence towards a favorable dynamic buffer allocation. Previous studies have not considered automating the <b>buffer</b> <b>pool</b> configuration process. This thesis presents two techniques that facilitate the initial buffer configuration task. First, we develop an analytic model of the GCLOCK buffer replacement policy {{that can be used to}} evaluate the effectiveness of a particular buffer configuration for a given workload. Second, to obtain the necessary model parameters, we propose a workload characterization scheme that extracts workload parameters, describing the query reference patterns, from the query access plans. In addition, we extend an existing multifractal model and present a multifractal skew model to represent query access skew. Our buffer model has been validated against measurements of the buffer manager of a commercial database system. The model has also been compared to an alternative GCLOCK buffer model. Our results show that our proposed model closely predicts the actual physical read rates and recognizes favourable buffer configurations. This work provides a foundation for the development of an automated buffer configuration tool...|$|R
40|$|The use of flash-based {{solid state}} drives (SSDs) in storage systems is growing. Adding SSDs to a storage system not only raises the {{question}} of how to manage the SSDs, but also {{raises the question}} of whether current <b>buffer</b> <b>pool</b> algorithms will still work effectively. We are interested in the use of hybrid storage systems, consisting of SSDs and hard disk drives (HDDs), for database management. We present cost-aware replacement algorithms, which are aware of the difference in performance between SSDs and HDDs, for both the DBMS <b>buffer</b> <b>pool</b> and the SSDs. In hybrid storage systems, the physical access pattern to the SSDs depends on the management of the DBMS <b>buffer</b> <b>pool.</b> We studied the impact of <b>buffer</b> <b>pool</b> caching policies on SSD access patterns. Based on these studies, we designed a cost-adjusted caching policy to effectively manage the SSD. We implemented these algorithms in MySQL’s InnoDB storage engine and used the TPC-C workload to demonstrate that these cost-aware algorithms outperform previous algorithms. 1...|$|R
40|$|Most {{transactional}} software, e. g., in database systems, {{is written}} for a 2 -level memory hierarchy with volatile RAM and persistent disk storage. For example, the standard "write-ahead logging " technique relies on an in-memory <b>buffer</b> <b>pool</b> {{to hold back}} dirty data pages until the relevant log records have been written to stable storage. It is well-known that a <b>buffer</b> <b>pool</b> enables fast access via locality of reference. In addition, in transactional systems, a <b>buffer</b> <b>pool</b> seems {{an essential component of}} write-ahead logging, which ensures the consistency of persistent data {{even in the face of}} recovery from media failure. For a transactional storage system using non-volatile, byte-addressable memory, which does not suffer from the slow access times of disk, a <b>buffer</b> <b>pool</b> seems unnecessary. However, removing the <b>buffer</b> <b>pool</b> seems to complicate transactional updates. We introduce a design and supporting techniques that simplify the implementation of transactions using NVRAM as persistent storage. One new technique, "log-shipping to self, " avoids use of a <b>buffer</b> <b>pool,</b> supports existing write-ahead logging protocols, and is simpler than traditional implementations of write-ahead logging. We also present a second innovation- a software frame-work for automatically detecting and repairing individual pages. This feature is particularly relevant in the context of NVRAM, since some NVRAM technologies may incur single-page failures. Traditional database techniques know only offline (or snapshot) consistency checks; online and incremental verification of data structures does not verify all invariants. Traditional database techniques also know only media recovery, i. e., recovery of an entire disk rather than of individual pages. Repair of individual page...|$|R
40|$|The {{buffer area}} is a key {{resource}} in database management systems (DBMSs) {{and the performance of}} a DBMS is greatly influenced by the effective use of the buffer area. Current DBMSs such as DB 2 Universal Database (DB 2 /UDB), logically divide the buffer area into a number of independent <b>buffer</b> <b>pools.</b> Each database object (table or index) is assigned to a specific <b>buffer</b> <b>pool.</b> The task of configuring the <b>buffer</b> <b>pools,</b> which defines the mapping of database objects to <b>buffer</b> <b>pools</b> and the setting the size for each of the <b>buffer</b> <b>pools,</b> is crucial for achieving optimal performance. In this thesis, we focus on the <b>buffer</b> <b>pool</b> sizing problem. The goal of our research is to support the DBMS automatically determining an optimal <b>buffer</b> <b>pool</b> sizes for a given workload. This problem {{has been shown to be}} a complex constrained optimization problem. Currently this task is performed manually by the database administrators (DBAs). We present a cost model based on data access time and use a greedy algorithm to solve the optimization problem. The approach is implemented and verified against the TPC-C benchmark database using DB 2 /UDB. Experimental results show the cost model is accurate, and that the greedy algorithm is fast and sufficient in finding an optimal <b>buffer</b> <b>pool</b> sizes. i Acknowledgements I would like to express my sincere gratitude to my supervisor, Dr. Pat Martin, for his excellent guidance, precious advice and endless support during my graduate study and research at Queen's University. Without his help, this thesis would never have got finished. I would also like to thank Wendy Powley, our wonderful database expert, for the help in setting up the experimental environments and the suggestions about writing this thesis. My thanks also go to my friendly labmates, whose help and advice is very helpful to this thesis. Special thanks are given to the School of Computing at Queen’s University for providing me the opportunity to pursue graduate studies and their support. I also thank IBM Canada Ltd. NSERC, and CITO for the financial support. Finally, I would like to thank my parents, and my beautiful wife, Jie Lu, for their love, support, and encouragement in these years. i...|$|R
40|$|Transaction {{support is}} a basic {{building}} block for reliable software systems. We are exploring virtual memory­based techniques for supporting transactional update and recovery of long­term data on general­purpose operating systems. We are particularly interested in networked environ­ ments in which stored data is accessed by programs running on client workstations accessing a shared storage server. This paradigm is common to distributed file systems and current Object­ Oriented Database Systems (OODBs) used in computer­aided design and other applications (e. g., ObjectStore [Lamb et al. 91] and QuickStore [White & DeWitt 94]). The transaction facilities in our environment {{are similar to those}} currently implemented in commercial relational database systems; however, we propose and evaluate alternatives to buffer management for transactions. High­performance database and file systems cache large amounts of data in memory. Managing the <b>buffer</b> <b>pool</b> correctly and efficiently is a key function of data­ base systems. We show that the buffering techniques commonly used to implement these sys­ tems, while effective in dedicated server environments, are not suitable for general­purpose workstation clients. In particular, workstation­based applications face competition for memory from other processes, possibly other database processes, and so are unable to reserve a fixed amount of memory for a database <b>buffer</b> <b>pool.</b> The fixed­size <b>buffer</b> <b>pools</b> commonly used today either artificially constrain the amount of memory {{that can be used for}} caching (if the <b>buffer</b> <b>pool</b> size is set too low), or cause unnecessary paging (if the size is set too high). Alternative solutions [Satyanarayanan et al. 94, Birrell et al. 87] eliminate the <b>buffer</b> <b>pool,</b> but are useful only for very small databases that easily fit in the physical memory...|$|R
40|$|An {{innovative}} hybrid loop-pool {{design for}} sodium cooled fast reactors (SFR-Hybrid) has been recently proposed. This design {{takes advantage of}} the inherent safety of a pool design and the compactness of a loop design to improve economics and safety of SFRs. In the hybrid loop-pool design, primary loops are formed by connecting the reactor outlet plenum (hot pool), intermediate heat exchangers (IHX), primary pumps and the reactor inlet plenum with pipes. The primary loops are immersed in the cold <b>pool</b> (<b>buffer</b> <b>pool).</b> Passive safety systems [...] modular Pool Reactor Auxiliary Cooling Systems (PRACS) – are added to transfer decay heat from the primary system to the <b>buffer</b> <b>pool</b> during loss of forced circulation (LOFC) transients. The primary systems and the <b>buffer</b> <b>pool</b> are thermally coupled by the PRACS, which is composed of PRACS heat exchangers (PHX), fluidic diodes and connecting pipes. Fluidic diodes are simple, passive devices that provide large flow resistance in one direction and small flow resistance in reverse direction. Direct reactor auxiliary cooling system (DRACS) heat exchangers (DHX) are immersed in the cold pool to transfer decay heat to the environment by natural circulation. To prove the design concepts, especially how the passive safety systems behave during transients such as LOFC with scram, a RELAP 5 - 3 D model for the hybrid loop-pool design was developed. The simulations were done for both steady-state and transient conditions. This paper presents the details of RELAP 5 - 3 D analysis as well as the calculated thermal response during LOFC with scram. The 250 MW thermal power conventional pool type design of GNEP’s Advanced Burner Test Reactor (ABTR) developed by Argonne National Laboratory was used as the reference reactor core and primary loop design. The reactor inlet temperature is 355 °C and the outlet temperature is 510 °C. The core design is the same as that for ABTR. The steady state <b>buffer</b> <b>pool</b> temperature {{is the same as the}} reactor inlet temperature. The peak cladding, hot pool, cold pool and reactor inlet temperatures were calculated during LOFC. The results indicate that there are two phases during LOFC transient – the initial thermal equilibration phase and the long term decay heat removal phase. The initial thermal equilibration phase occurs over a few hundred seconds, as the system adjusts from forced circulation to natural circulation flow. Subsequently, during long-term heat removal phase all temperatures evolve very slowly due to the large thermal inertia of the primary and <b>buffer</b> <b>pool</b> systems. The results clearly show that passive safety PRACS can effectively transfer decay heat from the primary system to the <b>buffer</b> <b>pool</b> by natural circulation. The DRACS system in turn can effectively transfer the decay heat to the environment...|$|R
5000|$|Performance {{improvements}} in various areas, including <b>buffer</b> <b>pool</b> flushing, execution of {{certain types of}} SQL queries, and support for NUMA architectures ...|$|R
5000|$|MVPG {{is used by}} VSAM Local Shared Resources (LSR) <b>buffer</b> <b>pool</b> {{management}} to access buffers in a hiperspace in Expanded Storage.|$|R
5000|$|Both MVPG and ADMF {{are used}} by DB2 to access hiperpools. Hiperpools are {{portions}} of a <b>buffer</b> <b>pool</b> located in a hiperspace.|$|R
40|$|The {{demand for}} {{real-time}} data services in embedded systems is increasing. In these new computing platforms, using traditional buffer management schemes, whose {{goal is to}} minimize the number of I/O operations, is problematic since they do not consider the constraints of those platforms such as limited energy and distinctive underlying storage. In particular, due to asymmetric read/write characteristic of flash memory, minimum buffer misses neither coincide with minimum power consumption nor minimum I/O deadline miss ratio. In this {{paper we propose a}} power-aware buffer cache management scheme for real-time databases whose secondary storage is a flash memory. We focus on the problem of guaranteeing the performance goal in terms of both I/O power consumption and I/O deadline miss ratio. To address this problem, we propose logical partitioning of the global <b>buffer</b> <b>pool</b> into read and write <b>buffer</b> <b>pools,</b> and dynamic feedback control of read/write <b>buffer</b> <b>pool</b> sizes to satisfy both performance goals. We have shown through an extensive evaluation that our approach satisfies both performance goals in a variety of workloads and access patterns with considerably smaller size of <b>buffer</b> <b>pools</b> compared to baseline approaches...|$|R
40|$|Abstract — A high {{performance}} packet switching archi-tecture called the Pipeback switch is proposed. This archi-tecture ensures lossless packet delivery while maintaining linear buffer complexity. The Pipeback switch improves upon the popular Knockout switch proposed by Y. Yeh et al. Both switches use an N × N space division fabric with output queuing and both designs {{are motivated by}} the observation that the probability of more than L packets arriving in a given timeslot being destined for one partic-ular output port sharply decreases as L is increased. This probability {{is comparable to the}} packet loss probability due to transmission errors for L N. While the arrival of more than L packets destined for a single output in a single timeslot in the Knockout switch results in dropped packets due to buffer blocking, the Pipeback switch avoids such loss by maintaining a separate shared buffer architecture common to all the output ports. This common architecture consists of a novel Pipeback concentration network and a <b>buffer</b> <b>pool.</b> The <b>buffer</b> <b>pool</b> accommodates all the knocked out packets that the Knockout switch would have dropped as a result of buffer blocking, and pipes them back to a separate input line. We further show that the use of <b>buffer</b> <b>pool</b> leads to a {{reduction in the number of}} separate output buffers required at each output port. I...|$|R
50|$|For disk-based SQL Server applications, it also {{provides}} the SSD <b>Buffer</b> <b>Pool</b> Extension, which can improve performance by cache between RAM and spinning media.|$|R
50|$|Cache {{information}} (such {{as for a}} data base) that {{is shared}} among all attached systems (or maintaining coherency between local <b>buffer</b> <b>pools</b> in each system).|$|R
40|$|ABSTRACT- Earlier {{performance}} {{studies of}} client-server data-base systems have investigated algorithms for caching locks and data at client worhxtations to reafuce latency and offload the server. These {{studies have been}} restricted to algorithms in which d&abase pages that {{were not in the}} local client <b>buffer</b> <b>pool</b> or the server <b>buffer</b> <b>pool</b> were read in from disk. In this paper we investi-gate a technique that allows client page requests to be serviced by other clients, thus treating the entire system as a single memory hierarchy. We also present techniques for efficiently exploiting this global memory hierarchy by reducing the replication of pages between client and server <b>buffer</b> <b>pools.</b> Global memory manage-ment algorithms that employ various combinalions of these tech-niques are then described, and the performance tradeoffs among the algorithms we investigated under a range of workloads and system conjiguratio ~ using a simulation model. 1...|$|R
40|$|The <b>buffer</b> <b>pool</b> in a DBMS {{is used to}} cache {{the disk}} pages of the {{database}}. Because typical database workloads are I/O-bound, {{the effectiveness of the}} <b>buffer</b> <b>pool</b> management algorithm is a crucial factor in the performance of the DBMS. In IBM's DB 2 <b>buffer</b> <b>pool,</b> the page cleaning algorithm is used to write changed pages to disks before they are selected for replacement. We conducted a detailed study of page cleaning in DB 2 version 7. 1. 0 for Windows by both trace-driven simulation and measurements. Our results show that system throughput can be increased by 19 % when the page cleaning algorithm is carefully tuned. In practice, however, the manual tuning of this algorithm is difficult. A self-tuning algorithm for page cleaning is proposed in this paper to automate this tuning task. Simulation results show that the self-tuning algorithm can achieve performance comparable to the best manually tuned system...|$|R
25|$|A circularly {{linked list}} {{may be a}} natural option to {{represent}} arrays that are naturally circular, e.g. the corners of a polygon, a <b>pool</b> of <b>buffers</b> that are used and released in FIFO ("first in, first out") order, or a set of processes that should be time-shared in round-robin order. In these applications, a pointer to any node serves as a handle to the whole list.|$|R
40|$|Storage {{management}} {{is important to}} the performance of DBMS. This paper gives a comprehensive overview of storage management in relational DBMS. The storage {{management is}} divided into three levels (logical, physical in-memory, and physical on-disk) and discussed separately. The logical level caches logical units (tuples or index values) of the DBMS based on the logical information to improve <b>buffer</b> <b>pool</b> usage efficiency. The physical in-memory level caches physical pages directly in the <b>buffer</b> <b>pool</b> to reduce disk accesses. Many algorithms have been designed for the general buffer or for the <b>buffer</b> <b>pool</b> of DBMS. Complex algorithms can achieve better performance than simple LRU or CLOCK algorithms. However, the overhead is high and the advantage diminishes when the buffer is big. The physical on-disk level borrows many techniques from file systems and storage systems. Because of the excellent potential write performance of log-structured organization, the Log-structured File System and related topics are discussed. Each level has its advantage and limitation for further improving performance. To achieve better performance of storage management systems of DBMS, all these three levels should be considered. ...|$|R
40|$|The {{technology}} of flash memory SSDs (solid state drives) which are increasingly adopted {{in a wide}} spectrum of storage systems has the potential of changing the database architecture and principles. With the high random access speed and high IOPS of the SSD, this paper describes a secondary <b>buffer</b> <b>pool</b> & readahead solution based on OLTP for MySQL InnoDB which can reduce I/O requests & latency and improve flash-based database system performance a lot. Based on the <b>buffer</b> <b>pool</b> & read-ahead solution, this paper provide an experience on running online transaction processing workloads (TPCC) which show that the database performance will achieve up to 120 %- 320 % improvements...|$|R
40|$|This paper {{presents}} {{a family of}} programming projects appropriate to a sophomore-level data structures course, centered around {{the concept of a}} <b>buffer</b> <b>pool</b> serving as the access intermediary to a disk file. These projects provide a meaningful vehicle for practicing object-oriented design techniques and teach fundamental material on file processing and manipulating binary data. I begin with a concrete example, a heap stored on disk and mediated by a <b>buffer</b> <b>pool.</b> Several important intellectual concepts introduced by such a project are enumerated. Significant extensions and alternatives to the basic project are then described. I conclude with some observations on the role of file processing in modern CS curricula, and the significance of recent trends away from coverage of these topics...|$|R
50|$|This {{function}} {{is due to}} CL’s unique structure. As stated above, CL can trap a proton within the bicyclic structure while carrying a negative charge. Thus, this bicyclic structure can serve as an electron <b>buffer</b> <b>pool</b> to release or absorb protons to maintain the pH near the membranes.|$|R
40|$|As {{improvements}} in processor performance continue to far outpace {{improvements in}} storage performance, I /O is increasingly the bottleneck in computer systems, especially in large database systems that manage {{huge amounts of}} data. The key to achieving good I /O performance is to thoroughly understand its characteristics. In this article we present a comprehensive analysis of the logical I/O reference behavior of the peak production database workloads from ten of the world’s largest corporations. In particular, we focus on how these workloads respond to different techniques for caching, prefetching, and write buffering. Our findings include several broadly applicable rules of thumb that describe how effective the various I /O optimization techniques are for the production workloads. For instance, our {{results indicate that the}} <b>buffer</b> <b>pool</b> miss ratio tends {{to be related to the}} ratio of <b>buffer</b> <b>pool</b> size to data size by an inverse square root rule. A similar fourth root rule relates the write miss ratio and the ratio of <b>buffer</b> <b>pool</b> size to data size. In addition, we characterize the reference characteristics of workloads similar to the Transaction Processing Performance Council (TPC) benchmarks C (TPC-C) and D (TPC-D), which are de facto standard performance measures for online transaction processing (OLTP) systems and decision support systems (DSS), respectively. Since benchmarks such as TPC-C and TPC-D can only b...|$|R
5000|$|Prado Dam and Prado Reservoir provide {{flood control}} and water conservation. Their {{operation}} is coordinated with the facilities upstream. Prado Reservoir {{is not a}} storage reservoir, so water is released {{as quickly as possible}} while still allowing for groundwater recharge. When the water level reaches the top of the <b>buffer</b> <b>pool,</b> whose size changes depending on time of year, water is released at the maximum rate that the downstream channel will safely allow. As of 2006, the capacity of the channel is [...] per second (140 m³/s), but channelization will eventually increase the capacity to [...] per second (850 m³/s). During flood season, the <b>buffer</b> <b>pool</b> only has a capacity of , while outside of flood season, the capacity increases to [...] Since this is 2.3 and 7.1 percent of the reservoir's total capacity, respectively, the reservoir is usually fairly empty.|$|R
40|$|Abstract. Spatial join {{techniques}} for spatial access methods were analysed, considering factor such as <b>buffer</b> <b>pool</b> size, page size and intermediate join indexes ordering criteria. This analysis {{was based on}} real data, taken from GIS applications for utilities. Results of this work assess the way those factors affect spatial join performance {{and can be used}} for tunning the algorithms. ...|$|R
40|$|Abstract: Equilibrium {{behavior}} of a store-and-forward network node with finite buffer capacity is studied via a network-of-queues model. The positive acknowledgment protocol is explicitly modeled and consumes part of the <b>buffer</b> <b>pool.</b> The principal results are the buffer overflow probability, the mean delays, {{and the distribution of}} queue lengths as functions of the buffer capacity and traffic levels...|$|R
40|$|Abstract. Join {{techniques}} for spatial access methods were analysed. The factors considered included <b>buffer</b> <b>pool</b> size, page size and intermediate join indexes ordering criteria. This analysis {{was based on}} real data taken from a GIS applications for telecommunications. Results of this work assess the way those factors affect spatial join performance {{and can be used}} for tuning such algorithms. ...|$|R
40|$|Abstract. We {{present an}} {{application}} specific, asynchronous VLSI processor array for the dynamic programming algorithm for the 0 / 1 knapsack problem. The array is derived systematically, using correctnesspreserving transformations, in two steps: the standard (dense) algorithm is first {{transformed into an}} irregular (sparse) functional program which has better efficiency. This program is then implemented as a modular VLSI architecture with nearest neighbor connections. Proving bounds on buffer sizes yields a linear array of identical asynchronous processors, each with simple computational logic {{and a pair of}} fixed size FIFOs. A modular solution can be obtained by additional load-time control, enabling the processors to <b>pool</b> their <b>buffers.</b> ...|$|R
