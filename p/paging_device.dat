23|124|Public
50|$|PageOne (operating {{under the}} Mercury brand) {{was the first}} UK {{organisation}} to launch a 'Calling Party Pays' (CPP) <b>paging</b> <b>device</b> called 'MiniCall' - {{it does not require}} a monthly subscription or contract, however is no longer actively marketed by PageOne.|$|E
50|$|The 2305 {{provides}} large-scale IBM {{computers with}} fast, continuous access to small-sized quantities of information. Its capacity and {{high data rate}} make it ideal for some systems residence functions, work files, job queues, indices and data sets that are used repeatedly. Its fast response time make it attractive as a <b>paging</b> <b>device</b> in a heavily loaded systems (where there are 1.5 or more transactions per second).|$|E
5000|$|Motorola {{manufactured}} {{and marketed}} the SportsTrax <b>paging</b> <b>device</b> which uses game information supplied by Sports Team Analysis and Tracking Systems ("STATS"). It offers four modes of operation: [...] "current," [...] "statistics," [...] "final scores," [...] and [...] "demonstration." [...] STATS compiled its scores and statistics by employing people to listen or watch the games, then enter {{the scores on}} the computer which transmits the scores to STATS' on-line service, to be sent out to anyone using a SportsTrax pager.|$|E
50|$|The PDP is a {{real memory}} job program. It allocates space on the <b>paging</b> <b>devices,</b> initiates all I/O to the <b>paging</b> <b>devices,</b> is {{responsible}} for recovery from I/O errors, and communicates with UMMPS.|$|R
5000|$|There is a {{three level}} storage hierarchy: (1) real memory, (2) high speed <b>paging</b> <b>devices,</b> and (3) <b>paging</b> disks. High speed <b>paging</b> <b>devices</b> include the IBM 2301 Drum, IBM 2305 Fixed Head File, and various third party [...] "solid-state" [...] I/O {{devices such as}} the STC 4305 and Intel 3805 that {{simulate}} spinning disks or more often provide more efficient fixed block architecture (FBA) access to external RAM based storage. The high speed <b>paging</b> <b>devices</b> are attached using [...] "two-byte" [...] I/O channels operating at up to 3.0 MB per second whenever possible. The paging disks were separate from the disks used for the file system and were used if the higher speed <b>paging</b> <b>devices</b> became full. Virtual memory pages migrate between real memory and the <b>paging</b> <b>devices.</b> In the early versions of MTS pages did not migrate between individual <b>paging</b> <b>devices.</b> In later versions, less frequently used pages would migrate from the high speed <b>paging</b> <b>devices</b> to the <b>paging</b> disks, when the high speed devices were close to being full. Later in its life the system was changed to use IBM S/370-XA Extended Storage {{as part of the}} second level of the storage hierarchy and to use the same disks for the file system and for paging.|$|R
5000|$|On Panasonic systems, you {{can make}} an [...] "ALL-CALL" [...] by {{entering}} the numbers 339. This activates the entire campus' <b>paging</b> <b>devices.</b>|$|R
5000|$|Virtual {{memory is}} managed by UMMPS with {{assistance}} from the <b>Paging</b> <b>Device</b> Processor (PDP) job program. UMMPS responds to requests to allocate and free VM from job programs, allocates VM addresses, allocates real memory, manages segment and page tables, sets storage keys, manages reference and change bits, determines which virtual memory pages should be paged in or out, and communicates with the PDP. New virtual memory pages are initialized to a [...] "core constant" [...] value of x'81' on first reference.|$|E
5000|$|The {{interface}} with the supervisor {{is the same}} for all components and very few special cases are allowed; for example, all input/output operations are done using the same supervisor facilities whether the input/output is for a card reader, a <b>paging</b> <b>device,</b> or any other device. Most access to supervisor services is via system subroutines that issue the necessary Supervisor Call instructions (SVCs) rather than by direct use of SVCs. Control blocks are accessed only indirectly by calls to subroutines within the component that [...] "owns" [...] the control block.|$|E
5000|$|The MTS job {{program is}} the one with which most users {{interact}} and provides command interpretation, execution control, file and device management, and accounting services. Other job programs assist the supervisor (the <b>Paging</b> <b>Device</b> Processor or PDP, the OPERATOR console job, the Disk Manager or DMGR, ...), provide common or shared services (spooled local and remote batch services via HASP and the HASPlings or later the Resource Manager or RM which was developed at the University of British Columbia to replace HASP), or allow the system operators to display status and otherwise control the system (JOBS, UNITS, STOP, BLAST, GOOSE, STARTUP, SHUTDOWN, REW, WTM, ...).|$|E
50|$|Paging is well {{established}} in computer with dedicated CKD <b>paging</b> <b>devices</b> {{going back to the}} drums included in the S/360 announcement. The 1978 StorageTek 4305 was the first CKD device using semiconductor memory for paging.|$|R
50|$|A PPD is {{also often}} called PostScript Page Description instead of Printer Description, {{this is because}} PostScript has the concept of <b>Page</b> <b>Devices</b> where the PostScript page {{description}} configuration is read from or saved as a PPD file.|$|R
50|$|The 2305 Drive was in much demand {{when the}} System 370 offered Virtual Storage, and these 2305s were {{often used for}} <b>paging</b> <b>devices.</b> They were used in this way on 3155/65, 3158/68, 3033,4341, and 3081 (with special feature microcode.) The 2305 was also used for high {{activity}} small data sets such as catalogs and job queues.|$|R
5000|$|PED is {{a one-way}} text <b>paging</b> <b>device,</b> with wide use in Australia, {{as well as}} {{installations}} in the United States, China, Canada, Mongolia, Chile, Tanzania, and Sweden. Australian company Mine Site Technologies began the development of PED in 1987, and it became commercially available and Mine Safety & Health Administration (MSHA) approved in 1991. [...] The best documented use of PED during a mine emergency is from the Willow Creek Mine Fire in 1998 in Utah, where {{it was able to}} quickly alert miners underground of the need to evacuate before toxic fumes from the fire filled the mine. Reports of this use can be seen on the Mine Safety & Health Administration (MSHA) web site at [...] and.|$|E
5000|$|By {{the time}} of its decommissioning, the {{computer}} was all solid-state, using a combination ofRTL, DTL and TTL. It had an array multiplier, 15 index registers, 16K of 6 microsecond cycle time core memory, and 64K of 2 microsecond cycle time core memory. A NOP instruction took about 2.5 microseconds. A Multiply took 8 microseconds, and a divide 25 microseconds. It had a paging unit using 1K word pages with an associative 16-deep lookup memory. A 1 Megaword CDC drum was hooked up as a <b>paging</b> <b>device.</b> It also had several ADDS Special-Order Direct-View Storage-Tube terminals. These terminals used an extended character set which covered about all the mathematical symbols, and allowed for 1/2 line spacing for math formulas. For I/O, it had two IBM 360 series nine-track and two seven-track 1/2" [...] tape drives. It had an eight-bit paper-tape reader and punch, and a 500 line-per-minute printer (1500 line-per-minute using the hexadecimal character set). Storage was three IBM 7000 series 1301 disk drives, each having two modules of 21.6 million characters apiece.|$|E
5000|$|The XT/370 was an IBM Personal Computer XT (System Unit 5160) {{with three}} custom 8-bit cards. The {{processor}} card (370PC-P), contained two modified Motorola 68000 chips (which could emulate most S/370 fixed-point instructions and non-floating-point instructions), and an Intel 8087 coprocessor [...] modified {{to emulate the}} S/370 floating point instructions. The second card (370PC-M), which connected to the first with a unique card back connector contained 512 KiB of memory. The third card (PC3277-EM), was a 3270 terminal emulator required to download system software from the host mainframe. The XT/370 computer booted into DOS, then ran the VM/PC Control Program. The card's memory space added additional system memory, so the first 256 KiB (motherboard) memory {{could be used to}} move data to the 512 KiB expansion card. The expansion memory was dual ported, and provided an additional 384 KiB to the XT Machine bringing the total RAM on the XT side to 640 KiB. The memory arbitrator could bank switch the second 128 KiB bank on the card to other banks, allowing the XT Intel 8088 processor to address all the RAM on the 370PC-M card. Besides the 416 KB of usable RAM for S/370 applications, the XT/370 also supported up to 4 MB of virtual memory using the hard drive as its <b>paging</b> <b>device.</b>|$|E
40|$|Traditional {{operating}} systems use magnetic disks as <b>paging</b> <b>devices,</b> although {{the cost of}} each page fault measured in processor cycles continues to increase. In this paper we argue that applications {{should be given the}} opportunity to use as backing store either magnetic disk or the memory of idle workstations within the same LAN. We have implemented a pager that provides this flexibility, measured its performance over an Ethernet, and found it to be superior to traditional disk paging. We conclude that as the available network bandwidth increases, the use of network memory as backing store becomes a evenmore attractive alternative. 1 Introduction Traditional {{operating systems}} use magnetic disks as <b>paging</b> <b>devices.</b> In such systems, pages that can't be placed in main memory are stored on the magnetic disk, and recalled to main memory when needed. Because magnetic disks speed does not keep up with processor speed, the cost of paging (measured in processor cycles) increases with time. For [...] ...|$|R
5000|$|<b>Paging</b> <b>devices</b> used in {{the late}} 1980s to early 1990s predate mobile phones and {{paved the way for}} the {{popularity}} of the phones among teenagers. Pagers could only display numbers and were intended to alert the owner that they had received a call from a certain phone number, but teens quickly began using numeric messages to communicate many things, including greetings and everyday emotions. Most were based on various ways numbers could be read in Japanese. Examples are ...|$|R
50|$|Most of {{the common}} SCSI devices such as disk-drives support {{only one or two}} {{diagnostic}} <b>pages.</b> SES <b>devices</b> can support many diagnostic pages.|$|R
40|$|Photograph {{used for}} {{a story in the}} Oklahoma Times newspaper. Caption: "What Miss Tela Porter, 801 E Drive, is {{flicking}} there is the button on an auto-call machine - the gadget that puts out those musical "bong" signals in department stores and cafeterias. This "mystery worker" is PBX supervisor at a downtown department store and keeps some 30 key employees hopping with that automatic <b>paging</b> <b>device.</b> ...|$|E
40|$|This paper {{proposes that}} L 2 cache sizes are now {{starting}} to reach {{the point where it}} makes more sense to manage them as the main memory of the computer, and relegate the traditional DRAM main memory to the role of a <b>paging</b> <b>device.</b> The paper details advantages of an SRAM main memory, as well as problems that need to be solved, in managing an extra level of virtual to physical translation...|$|E
40|$|The RAMpage memory {{hierarchy}} addresses the growing {{concern about the}} memory wall – {{the possibility that the}} CPU-DRAM speed gap will ultimately limit the benefits of rapid improvement in CPU speed. Reducing references to DRAM is an increasingly desirable goal as CPU speed improves relative to DRAM. As the cost of a DRAM reference increases, it makes increasing sense to consider options like pinning crucial parts of the operating system in at least the lowest-level cache, and to consider possibilities like context switches on references to DRAM. All these factors combine to make it increasingly desirable to treat DRAM as a <b>paging</b> <b>device,</b> while moving the main memory a level up to the lowest level of SRAM. The RAMpage hierarchy relegates DRAM to the role of a first-level <b>paging</b> <b>device.</b> Results presented here are for a preliminary simulation of the RAMpage hierarchy, and show that, if current memory system and CPU trends continue, the RAMpage strategy will become increasingly viable. Even with current miss costs and without implementing all features favourable to the RAMpage hierarchy, simulations show {{that it is possible to}} achieve run times up to 25 % faster than those for a conventional hierarchy. Furthermore, RAMpage scales up better than the conventional hierarchy (as simulated) in that performance degrades less as DRAM reference costs increase...|$|E
5000|$|Wake on LAN can be {{configured}} {{to limit}} wake up packets to just magic packets from the Power management tab of the NIC property <b>page</b> in <b>Device</b> Manager.|$|R
50|$|To display <b>devices</b> on Wikipedia <b>pages,</b> use Template:Ribbon <b>devices.</b>|$|R
5000|$|Safari to find local {{web servers}} and {{configuration}} <b>pages</b> for local <b>devices</b> ...|$|R
40|$|The {{basic idea}} with virtual memory {{is to create}} an {{illusion}} of memory that is as large as a disk (in gigabytes) and as fast as memory (in nanoseconds). The key principle is locality of reference, which recognizes that a significant percentage of memory accesses in a running program are made to a subset of its pages. Or simply put, a running program only needs access to a portion of its virtual address space at a given time. With virtual memory, a logical (virtual) address translates to: – Main memory (small but fast), or – <b>Paging</b> <b>device</b> (large but slow), or – None (not allocated, not used, free. ...|$|E
40|$|Abstract — In {{a cluster}} {{with a very}} low-latency interconnect, the remote memory of nodes {{can serve as a}} storage that is faster than local disk but slower than local memory. In this paper, we address the problem of transparently {{utilizing}} this cluster-wide pool of unused memory as a low-latency <b>paging</b> <b>device.</b> Such a transparent remote memory paging system can enable large-memory applications to benefit from cluster-wide memory resources without compromising application performance while obviating the need for very large local memory or modifications to legacy applications. There was considerable interest in this subject in the 1990 ’s, which saw a number of remote paging systems built over specialized interconnects such as Myrinet and ATM switches. However, in recent years...|$|E
40|$|The RAMpage memory {{hierarchy}} {{is an alternative}} to the traditional division between cache and main memory: main memory is moved up a level and DRAM is used as a <b>paging</b> <b>device.</b> As the CPU-DRAM speed gap grows, it is expected that the RAMpage approach should become more viable. Results in this paper show that RAMpage scales better than a standard second-level cache, because the number of DRAM references is lower. Further, RAMpage allows the possibility of taking a context switch on a miss, which is shown to further improve scalability. The paper also suggests that memory wall work ought to include the TLB, which can represent a significant fraction of execution time. With context switches on misses, the speed improvement at an 8 GHz instruction issue rate is 62 % over a standard 2 -level cache hierarchy...|$|E
5000|$|The 6,000 RPM 2305 {{appeared}} in 1970, with capacities of 5 MB (2305-1) or 11 MB (2305-2) per module. Although these devices {{did not have}} large capacity, their speed and transfer rates made them attractive for high-performance needs. A typical use was overlay linkage (e.g. for OS and application subroutines) for program sections written to alternate in the same memory regions. Fixed head disks and drums were particularly effective as <b>paging</b> <b>devices</b> on the early virtual memory systems. The 2305, although often called a [...] "drum" [...] was actually a head-per-track disk device, with 12 recording surfaces and a data transfer rate up to 3 MB per second.|$|R
5000|$|According to the KitHub <b>page</b> [...] Safecast <b>devices</b> {{are also}} used by the {{following}} institutions: ...|$|R
40|$|Review question/objective What are {{the most}} {{effective}} information sharing strategies used to reduce anxiety in families of patients undergoing elective surgery? This review seeks to synthesize the best available evidence {{in relation to the}} most effective information-sharing intervention to reduce anxiety for families waiting for patients undergoing an elective surgical procedure. The specific objectives are to review the effectiveness of evidence of interventions designed to reduce the anxiety of families waiting whilst their loved one undergoes a surgical intervention. A variety of interventions exist and include surgical nurse liaison services, intraoperative reporting either by face-to-face or telephone delivery, informational cards, visual information screens, and intraoperative <b>paging</b> <b>devices</b> for families. Inclusion criteria Types of participants All studies of family members over 18 years of age waiting for patients undergoing an elective surgical procedure will be included, including those waiting for both adult and paediatric patients. Studies of families waiting for other patient populations, eg emergency surgery, chemotherapy or intensive care patients will be excluded. Types of intervention(s) /phenomena of interest All information-sharing Interventions for families of patients undergoing an elective surgical procedure will be included, including but not limited to: surgical nurse liaison services, in-person intraoperative reporting, visual information screens, <b>paging</b> <b>devices,</b> informational cards and telephone delivery of intraoperative progress reports. Interventions that take place during the intraoperative phase of care only will be included in the review. Preadmission information sharing interventions will be excluded. Types of outcomes The outcomes of interest include: Primary outcome: the level of anxiety amongst family members or close relatives whilst waiting for patients undergoing surgery, as measured by a validated instrument (such as the S-Anxiety portion of the State-Trait Anxiety Inventory). 4 Secondary outcomes: family satisfaction and other measurements that may be considered indicators of stress and anxiety, such as mean arterial pressure (MAP) and heart rate...|$|R
40|$|The RAMpage memory {{hierarchy}} {{is an alternative}} to the traditional division between cache and main memory: main memory is moved up a level and DRAM is used as a <b>paging</b> <b>device.</b> Earlier RAMpage work has shown that the RAMpage model scales up better with the growing CPU-DRAM speed gap, especially when context switches are taken on misses. This paper investigates the effect of more aggressive first-level (L 1) cache and translation lookaside buffer (TLB) implementations, with other parameters kept the same as in previous work, to illustrate that a more aggressive design improves the competitiveness of RAMpage. The more aggressive L 1 shows an increase in the advantage of RAMpage with context switches on misses, supporting the hypothesis that a more aggressive L 1 favours RAMpage. However, results without context switches on misses are less conclusive. A larger TLB, as predicted, makes RAMpage viable over a wider range of page sizes...|$|E
40|$|A growing {{gap between}} CPU and DRAM {{performance}} is driving processors {{further away from}} their peak execution rates by {{increasing the amount of}} time spent waiting for the memory system. To date, cache memories have been used to good effect in offsetting lagging DRAM speeds by buffering frequently referenced instructions and data near the CPU. However, continued increases in the cost of DRAM accesses call for improvements in cache performance, and in particular, that of the secondary cache. More specifically, strategies which target the secondary cache hit rate for improvement are becoming increasingly important, even if they result in an increase in the cost of each miss. This research examines a proposed new organization for the memory hierarchy in which main memory is implemented in SRAM (replacing the secondary cache), and the role of DRAM is relegated to that of a <b>paging</b> <b>device.</b> In this model, called the RAMpage memory hierarchy, a software-managed paging system takes the place of th [...] ...|$|E
40|$|Traditional {{operating}} systems use magnetic disks as paging devices, {{even though the}} cost of a disk transfer measured in processor cycles continues to increase. In this paper we explore the use of remote main memory for paging. We describe the design, implementation and evaluation of a pager that uses main memory of remote workstations as a faster-than-disk <b>paging</b> <b>device</b> and provides reliability in case of single workstation failures. Our pager has been implemented as a block device driver linked to the DEC OSF/ 1 operating system, without any modi cations to the kernel code. Using several test applications we measure the performance of remote memory paging over an Ethernet interconnection network and nd it to be faster than traditional disk paging. We evaluate the performance of various reliability policies and prove their feasibilityeven over low bandwidth networks, like Ethernet. We conclude that the bene ts of reliable remote memory paging in workstation clusters are signi cant today and will probably increase in the near future...|$|E
50|$|In {{the printer}} / copier industry, the duty cycle {{specification}} {{refers to the}} rated throughput (that is, printed <b>pages)</b> of a <b>device</b> per month.|$|R
40|$|Binding: Limp vellum case binding, nearly detached; headbands broken; calligraphic title on spine: Emblemas. 211 woodcut emblems within {{borders of}} fleurons; Latin text, with Spanish {{commentary}} by Diego López. Numerous errors in <b>paging.</b> Publisher's <b>device</b> on t. p.; tailpiecesLandwehr, J. Romanic emblem books,McGeary & Nash. Emblem books at the University of Illinois,Mode of access: Internet...|$|R
50|$|Xprint is a {{deprecated}} printing extension for the X Window System. It allows {{an application}} to render output to a printer {{just as it}} would to any other display device. The server portion, Xprt, uses an extension (XpExtension) to handle <b>paged</b> output <b>devices.</b> Xprint outputs as PostScript, PCL 3, PCL 5 or as a raster bitmap at printer resolution.|$|R
