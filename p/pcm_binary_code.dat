0|5252|Public
50|$|The first {{transmission}} of speech by digital techniques, the SIGSALY encryption equipment, conveyed high-level Allied communications during World War II. In 1943 the Bell Labs researchers {{who designed the}} SIGSALY system {{became aware of the}} use of <b>PCM</b> <b>binary</b> <b>coding</b> as already proposed by Alec Reeves. In 1949, for the Canadian Navy's DATAR system, Ferranti Canada built a working PCM radio system that was able to transmit digitized radar data over long distances.|$|R
50|$|In {{telemetry}} applications, a {{frame synchronizer}} {{is used to}} frame-align a serial pulse code-modulated (<b>PCM)</b> <b>binary</b> stream.|$|R
40|$|AbstractSome nontrivial {{properties}} of perfect <b>binary</b> <b>codes</b> are discussed. We consider some constructions of perfect <b>binary</b> <b>codes</b> {{with the purpose}} to outline bounds {{on the number of}} nonequivalent perfect <b>binary</b> <b>codes</b> and we present the best known lower and upper bounds on the number of different perfect <b>binary</b> <b>codes...</b>|$|R
40|$|The paper {{approaches}} the low-level {{details of the}} code generated by compilers whose format permits outside actions. <b>Binary</b> <b>code</b> modifications are manually done when the internal format is known and understood, or automatically by certain tools developed to process the <b>binary</b> <b>code.</b> The <b>binary</b> <b>code</b> instrumentation goals may be various from security increasing and bug fixing to development of malicious software. The paper highlights the <b>binary</b> <b>code</b> instrumentation techniques by code injection to increase the security and reliability of a software application. Also, the paper offers examples for <b>binary</b> <b>code</b> formats understanding and how the <b>binary</b> <b>code</b> injection may be applied...|$|R
40|$|<b>Binary</b> <b>coding</b> or hashing {{techniques}} are recognized to accomplish efficient near neighbor search, and have thus attracted broad {{interests in the}} recent vision and learning studies. However, such studies have rarely been dedicated to Maximum Inner Product Search (MIPS), which plays {{a critical role in}} various vision applications. In this paper, we investigate learning <b>binary</b> <b>codes</b> to exclusively handle the MIPS problem. Inspired by the latest advance in asymmetric hashing schemes, we propose an asymmetric <b>binary</b> <b>code</b> learning framework based on inner product fitting. Specifically, two sets of coding functions are learned such that the inner products between their generated <b>binary</b> <b>codes</b> can reveal the inner products between original data vectors. We also propose an alternative simpler objective which maximizes the correlations between the inner products of the produced <b>binary</b> <b>codes</b> and raw data vectors. In both objectives, the <b>binary</b> <b>codes</b> and coding functions are simultaneously learned without continuous relaxations, which is the key to achieving high-quality <b>binary</b> <b>codes.</b> We evaluate the proposed method, dubbed Asymmetric Inner-product <b>Binary</b> <b>Coding</b> (AIBC), relying on the two objectives on several large-scale image datasets. Both of them are superior to the state-of-the-art <b>binary</b> <b>coding</b> and hashing methods in performing MIPS tasks...|$|R
40|$|Different {{strategies}} for binary analysis {{are widely used}} in systems dealing with software maintenance and system security. <b>Binary</b> <b>code</b> is self-contained; though {{it is easy to}} execute, {{it is not easy to}} read and understand. Binary analysis tools are useful in software maintenance because the binary of software has all the information necessary to recover the source code. It is also incredibly important and sensitive in the domain of security. Malicious <b>binary</b> <b>code</b> can infect other applications, hide in their <b>binary</b> <b>code,</b> contaminate the whole system or travel through Internet and attack other systems. This makes it imperative for security personnel to scan and analyze <b>binary</b> <b>codes</b> with the aid of the <b>binary</b> <b>code</b> analysis tools. On the other hand, crackers can reverse engineer the <b>binary</b> <b>code</b> to assembly code in order to break the secrets embedded in the <b>binary</b> <b>code,</b> such as registration number, password or secret algorithms. This motivates researches to prevent malicious monitoring by <b>binary</b> <b>code</b> analysis tools. Evidently, binary analysis tools play an important doublesided role in security. This paper surveys <b>binary</b> <b>code</b> analysis from the most fundamental perspective views: the <b>binary</b> <b>code</b> formats, several of the most basic analysis tools, such as disassembler, debugger and the instrumentation tools based on them. The previous research on binary analysis are investigated and summarized and a new approach of analysis, disasembler-based binary interpreter, is proposed and discussed. 1...|$|R
30|$|Hinton et al. [34] {{describe}} a Deep Learning generative model {{to learn the}} <b>binary</b> <b>codes</b> for documents. The lowest layer of the Deep Learning network represents the word-count vector of the document which accounts as high-dimensional data, while the highest layer represents the learnt <b>binary</b> <b>code</b> of the document. Using 128 -bit codes, the authors demonstrate that the <b>binary</b> <b>codes</b> of the documents that are semantically similar lay relatively closer in the Hamming space. The <b>binary</b> <b>code</b> of the documents can then be used for information retrieval. For each query document, its Hamming distance compared to all other documents in the data is computed and the top D similar documents are retrieved. <b>Binary</b> <b>codes</b> require relatively little storage space, and in addition they allow relatively quicker searches by using algorithms such as fast-bit counting to compute the Hamming distance between two <b>binary</b> <b>codes.</b> The authors conclude that using these <b>binary</b> <b>codes</b> for document retrieval is more accurate and faster than semantic-based analysis.|$|R
30|$|Instead {{of using}} a code with base 3 to encode the three states, LTP uses two <b>binary</b> <b>codes</b> {{representing}} the positive and the negative components of the ternary <b>code,</b> i.e., two <b>binary</b> <b>codes</b> coding for the two states {- 1, 1 }. These <b>binary</b> <b>codes</b> are collected in two separate histograms and, as a last step, the histograms are concatenated to form the LTP feature vector.|$|R
40|$|Some {{mutually}} quasi-unbiased weighing matrices {{are constructed}} from <b>binary</b> <b>codes</b> satisfying certain conditions. Motivated by this, in this note, we study <b>binary</b> <b>codes</b> satisfying the conditions. The weight distributions of <b>binary</b> <b>codes</b> satisfying {{the conditions are}} determined. We also give a classification of <b>binary</b> <b>codes</b> of lengths $ 8, 16 $ and <b>binary</b> maximal <b>codes</b> of length $ 32 $ satisfying the conditions. As an application, sets of $ 8 $ mutually quasi-unbiased weighing matrices for parameters $(16, 16, 4, 64) $ and $ 4 $ mutually quasi-unbiased weighing matrices for parameters $(32, 32, 4, 256) $ are constructed for the first time. Comment: 14 page...|$|R
40|$|We give a {{classification}} of singly-even self-dual <b>binary</b> <b>codes</b> of length 32, by enumerating all neighbours {{of the known}} 85 doubly-even self-dual <b>binary</b> <b>codes</b> of length 32. There are 3, 210 singly-even self-dual <b>binary</b> <b>codes</b> of length 32 up to equivalence. This agrees in number with the enumeration by Bilous and van Rees, who enumer-ated these codes by a different method. ...|$|R
40|$|Binary {{encoding}} on high-dimensional {{data points}} has {{attracted much attention}} due to its computational and storage efficiency. While numerous efforts {{have been made to}} encode data points into <b>binary</b> <b>codes,</b> how to calculate the effective distance on <b>binary</b> <b>codes</b> to approximate the original distance is rarely addressed. In this paper, we propose an effective distance measurement for <b>binary</b> <b>code</b> ranking. In our approach, the <b>binary</b> <b>code</b> is firstly decomposed into multiple sub codes, each of which generates a query-dependent distance lookup table. Then the distance between the query and the <b>binary</b> <b>code</b> is constructed as the aggregation of the distances from all sub codes by looking up their respective tables. The entries of the lookup tables are optimized by minimizing the misalignment between the approximate distance and the original distance. Such a scheme is applied to both the symmetric distance and the asymmetric distance. Extensive experimental results show superior performance of the proposed approach over state-of-the-art methods on three real-world high-dimensional datasets for <b>binary</b> <b>code</b> ranking...|$|R
40|$|We {{explore the}} {{connection}} between simple polytopes and self-dual <b>binary</b> <b>codes</b> via the theory of small covers. We first show that a small cover M^n over a simple n-polytope P^n produces a self-dual code {{in the sense of}} Kreck-Puppe if and only if P^n is n-colorable and n is odd. Then we show how to describe such a self-dual <b>binary</b> <b>code</b> in terms of the combinatorial information of P^n. Moreover, we can define a family of <b>binary</b> <b>codes</b> B_k(P^n), 0 ≤ k≤ n, from an arbitrary simple n-polytope P^n. We will give some necessary and sufficient conditions for B_k(P^n) to be a self-dual code. A spinoff of our study of such <b>binary</b> <b>codes</b> gives some new ways to judge whether a simple n-polytope P^n is n-colorable in terms of the associated <b>binary</b> <b>codes</b> B_k(P^n). In addition, we prove that the minimum distance of the self-dual <b>binary</b> <b>code</b> obtained from a 3 -colorable simple 3 -polytope is always 4. Comment: 27 pages, 5 figure...|$|R
50|$|Build {{automation}} is {{the process}} of automating the creation of a software build and the associated processes including: compiling computer source <b>code</b> into <b>binary</b> <b>code,</b> packaging <b>binary</b> <b>code,</b> and running automated tests.|$|R
30|$|In our implementation, as {{described}} above, the interval coded with zero is half-bound while in [21] it is open. Instead {{of using a}} code with base 3 to encode the three states in Eq. 5, LTP uses two <b>binary</b> <b>codes</b> representing the positive and the negative components of the ternary <b>code,</b> i.e., two <b>binary</b> <b>codes</b> coding for the two states {− 1, 1 }. These <b>binary</b> <b>codes</b> are collected in two separate histograms, and as a last step, the histograms are concatenated to form the LTP feature vector [21].|$|R
40|$|Abstract—We {{investigate}} a <b>binary</b> <b>code,</b> which is implemented by serially concatenating a multiplexer, a multilevel delay proces-sor, and a signal mapper to a binary turbo encoder. To achieve improved convergence behavior, we modify the <b>binary</b> <b>code</b> by passing {{only a fraction}} of the bits in the turbo code through the multilevel delay processor and the signal mapper. Two decoding methods are discussed and their performances are evaluated. Index Terms—Turbo <b>codes,</b> concatenated <b>codes,</b> <b>binary</b> <b>codes.</b> I...|$|R
40|$|Hashing based {{methods have}} {{attracted}} considerable attention for efficient cross-modal retrieval on large-scale multimedia data. The core problem of cross-modal hashing {{is how to}} learn compact <b>binary</b> <b>codes</b> that construct the underlying correlations between heterogeneous features from different modalities. A majority of recent approaches aim at learning hash functions to preserve the pairwise similarities defined by given class labels. However, these methods fail to explicitly explore the discriminative property of class labels during hash function learning. In addition, they usually discard the discrete constraints imposed on the to-be-learned <b>binary</b> <b>codes,</b> and compromise to solve a relaxed problem with quantization to obtain the approximate binary solution. Therefore, the <b>binary</b> <b>codes</b> generated by these methods are suboptimal and less discriminative to different classes. To overcome these drawbacks, we propose a novel cross-modal hashing method, termed discrete cross-modal hashing (DCH), which directly learns discriminative <b>binary</b> <b>codes</b> while retaining the discrete constraints. Specifically, DCH learns modality-specific hash functions for generating unified <b>binary</b> <b>codes,</b> and these <b>binary</b> <b>codes</b> are viewed as representative features for discriminative classification with class labels. An effective discrete optimization algorithm is developed for DCH to jointly learn the modality-specific hash function and the unified <b>binary</b> <b>codes.</b> Extensive experiments on three benchmark data sets highlight the superiority of DCH under various cross-modal scenarios and show its state-of-the-art performance. </p...|$|R
40|$|In this paper, the undetected error {{probability}} for large <b>binary</b> <b>codes</b> is studied. It is shown {{that if the}} size of the code is sufficiently large (for given length), then the code is good for error detection (in the technical sense). Index Terms- Undetected {{error probability}}, error detection, <b>binary</b> <b>codes,</b> good codes. ...|$|R
40|$|In this paper, we {{introduce}} a Graph based <b>Binary</b> <b>Code</b> Execution Path Exploration Platform. In the graph, a node {{is defined as}} a conditional branch instruction, and an edge is defined as the other instructions. We implemented prototype of the proposed method and works well on real <b>binary</b> <b>code.</b> Experimental results show proposed method correctly explores execution path of target <b>binary</b> <b>code.</b> We expect our method can help Software Assurance, Secure Programming, and Malware Analysis mor...|$|R
40|$|Binary hashing {{has been}} widely used for {{efficient}} simi-larity search due to its query and storage efficiency. In most existing binary hashing methods, the high-dimensional da-ta are embedded into Hamming space and the distance or similarity of two points are approximated by the Hamming distance between their <b>binary</b> <b>codes.</b> The Hamming dis-tance calculation is efficient, however, in practice, there are often lots of results sharing the same Hamming distance to a query, which makes this distance measure ambiguous and poses a critical issue for similarity search where ranking is important. In this paper, we propose a weighted Hamming distance ranking algorithm (WhRank) to rank the <b>binary</b> <b>codes</b> of hashing methods. By assigning different bit-level weights to different hash bits, the returned <b>binary</b> <b>codes</b> are ranked at a finer-grained <b>binary</b> <b>code</b> level. We give an algorithm to learn the data-adaptive and query-sensitive weight for each hash bit. Evaluations on two large-scale image data sets demonstrate the efficacy of our weighted Hamming distance for <b>binary</b> <b>code</b> ranking. 1...|$|R
40|$|This paper tackles the {{efficiency}} problem of making recom-mendations {{in the context}} of large user and item spaces. In particular, we address the problem of learning <b>binary</b> <b>codes</b> for collaborative filtering, which enables us to effi-ciently make recommendations with time complexity that is independent {{of the total number of}} items. We propose to construct <b>binary</b> <b>codes</b> for users and items such that the preference of users over items can be accurately preserved by the Hamming distance between their respective <b>binary</b> <b>codes.</b> By using two loss functions measuring the degree of divergence between the training and predicted ratings, we formulate the problem of learning <b>binary</b> <b>codes</b> as a discrete optimization problem. Although this optimization problem is intractable in general, we develop effective relaxations that can be efficiently solved by existing methods. Moreover, we investigate two methods to obtain the <b>binary</b> <b>codes</b> from the relaxed solutions. Evaluations are conducted on three public-domain data sets and the results suggest that our pro-posed method outperforms several baseline alternatives...|$|R
50|$|Executor {{translates}} 68k big-endian <b>binary</b> <b>code</b> into x86 little-endian <b>binary</b> <b>code.</b> Executor {{can only}} run Macintosh {{programs designed to}} run on 68000-based Macintosh hardware. Executor can mimic either Macintosh System 7.0.0, or System 6.0.7 for older applications that are incompatible with System 7.0.0.|$|R
40|$|AbstractThe two {{concepts}} dual code and parity check matrix for a linear perfect 1 -error correcting <b>binary</b> <b>code</b> are generalized {{to the case}} of non-linear perfect codes. We show how this generalization can be used to enumerate some particular classes of perfect 1 -error correcting <b>binary</b> <b>codes.</b> We also use it to give an answer to a problem of Avgustinovich: whether or not the kernel of every perfect 1 -error correcting <b>binary</b> <b>code</b> is always contained in some Hamming code...|$|R
3000|$|Another way {{of making}} LBP {{rotation}} invariant is introduced in [9, 23]. The occurrences of rotation codes within the rotation groups are not summed, as in LBP_N,R^ri, but Fourier transformed and the resulting power spectrum is used as the feature vector. The descriptor is called LBP histogram Fourier features (LBP_N,R^HF). Since the Fourier features are computed on the global histogram of <b>binary</b> <b>codes</b> in the region/patch investigated, LBP_N,R^HF achieves rotation invariance globally, and hence, retains the relative distribution within rotation groups [23]. However, the LBP_N,R^HF descriptor in [9] only considers uniform <b>binary</b> <b>codes</b> (<b>binary</b> <b>codes</b> with at the most two transitions between 0 and 1). It was generalized in [10] to include all <b>binary</b> <b>codes,</b> uniform and non-uniform, and called LBP [...]...|$|R
40|$|In this work, {{quadratic}} {{double and}} quadratic bordered double circulant constructions {{are applied to}} F_ 4 + uF_ 4 as well as F_ 4, {{as a result of}} which extremal <b>binary</b> self-dual <b>codes</b> of length 56 and 64 are obtained. The binary extension theorems as well as the ring extension version are used to obtain 7 extremal self-dual <b>binary</b> <b>codes</b> of length 58, 24 extremal self-dual <b>binary</b> <b>codes</b> of length 66 and 29 extremal self-dual <b>binary</b> <b>codes</b> of length 68, all with new weight enumerators, updating the list of all the known extremal self-dual codes in the literature. Comment: Under review since May 2014, 11 pages, 6 table...|$|R
30|$|One {{straight}} forward {{way to make}} LBP rotation invariant is to rotate the <b>binary</b> <b>code,</b> i.e., bit-shift it, to its lowest value [25]. For most LBP-based features, it is trivial to introduce rotation invariance following this scheme. Indeed, in [34], rotation invariance was introduced to FLBP following this approach. ILBP, MBP, RLBP, and SLBP are made rotation invariant in this way. LTP, ILTP, and LQP are somewhat different due to the concatenation of <b>binary</b> <b>codes.</b> The <b>binary</b> <b>codes</b> are therefore made rotation invariant prior to concatenation of the histograms here.|$|R
40|$|This paper {{attempts}} {{to estimate the}} transition probabilities of credit ratings {{for a number of}} companies whose ratings have a dependence structure. <b>Binary</b> <b>codes</b> are used to represent the index of a company together with its ratings in the present and next quarters. We initially fit the data on the vector of <b>binary</b> <b>codes</b> with a multivariate power-normal distribution. We next compute the multivariate conditional distribution for the <b>binary</b> <b>codes</b> of rating in the next quarter when the index of the company and <b>binary</b> <b>codes</b> of the company in the present quarter are given. From the conditional distribution, we compute the transition probabilities of the company’s credit ratings in two consecutive quarters. The resulting transition probabilities tally fairly well with the maximum likelihood estimates for the time-independent transition probabilitie...|$|R
40|$|Off-The-Shelf (OTS) {{software}} components are {{the cornerstone of}} modern systems, including safety-critical ones. However, the dependability of OTS components is uncertain {{due to the lack}} of source code, design artifacts and test cases, since only their <b>binary</b> <b>code</b> is supplied. Fault injection in components’ <b>binary</b> <b>code</b> is a solution to understand the risks posed by buggy OTS components. In this paper, we consider the problem of the accurate mutation of <b>binary</b> <b>code</b> for fault injection purposes. Fault injection emulates bugs in high-level programming constructs (assignments, expressions, function calls, [...] .) by mutating their translation in <b>binary</b> <b>code.</b> However, the semantic gap between the source <b>code</b> and its <b>binary</b> translation often leads to inaccurate mutations. We propose Faultprog, a systematic approach for testing the accuracy of binary mutation tools. Faultprog automatically generates synthetic programs using a stochastic grammar, and mutates both their <b>binary</b> <b>code</b> with the tool under test, and their source code as reference for comparisons. Moreover, we present a case study on a commercial binary mutation tool, where Faultprog was adopted to identify code patterns and compiler optimizations that affect its mutation accuracy...|$|R
40|$|In this paper, {{we propose}} a new deep hashing (DH) ap-proach to learn compact <b>binary</b> <b>codes</b> for large scale visual search. Unlike most {{existing}} <b>binary</b> <b>codes</b> learning meth-ods which seek a single linear projection to map each sam-ple into a binary vector, we develop a {{deep neural network}} to seek multiple hierarchical non-linear transformations to learn these <b>binary</b> <b>codes,</b> so that the nonlinear relationship of samples can be well exploited. Our model is learned un-der three constraints at {{the top layer of}} the deep network: 1) the loss between the original real-valued feature descrip-tor and the learned binary vector is minimized, 2) the bina-ry codes distribute evenly on each bit, and 3) different bits are as independent as possible. To further improve the dis-criminative power of the learned <b>binary</b> <b>codes,</b> we extend DH into supervised DH (SDH) by including one discrimi-native term into the objective function of DH which simulta-neously maximizes the inter-class variations and minimizes the intra-class variations of the learned <b>binary</b> <b>codes.</b> Ex-perimental results show the superiority of the proposed ap-proach over the state-of-the-arts. 1...|$|R
50|$|The {{weight of}} a <b>binary</b> <b>code,</b> as defined in the table of constant-weight codes, is the Hamming weight of the <b>binary</b> words <b>coding</b> for the {{represented}} words or sequences.|$|R
40|$|In 2002, Tonchev first {{constructed}} some linear <b>binary</b> <b>codes</b> {{defined by}} the adjacency matrices of undirected graphs. So, graph is an important tool for searching optimum codes. In this paper, we introduce a new method of searching (proposed) optimum formally self-dual linear <b>binary</b> <b>codes</b> from circulant graphs. Comment: 15 page...|$|R
40|$|Recently, <b>binary</b> <b>codes</b> {{have been}} widely used in many {{multimedia}} applications to approximate high-dimensional multimedia features for practical similarity search due to the highly compact data representation and efficient distance computation. While the majority of the hashing methods aim at learning more accurate hash codes, only a few of them focus on indexing methods to accelerate the search for <b>binary</b> <b>code</b> databases. Among these indexing methods, most of them suffer from extremely high memory cost or extensive Hamming distance computations. In this paper, we propose a new Hamming distance search scheme for large scale <b>binary</b> <b>code</b> databases, which is free of Hamming distance computations to return the exact results. Without the necessity to compare database <b>binary</b> <b>codes</b> with queries, the search performance can be improved and databases can be externally maintained. More specifically, we adopt the inverted multi-index data structure to index <b>binary</b> <b>codes.</b> Importantly, the Hamming distance information embedded in the structure is utilized in the designed search scheme such that the verification of exact results no longer relies on Hamming distance computations. As a step further, we optimize the performance of the inverted multi-index structure by taking the code distributions among different bits into account for index construction. Empirical results on large-scale <b>binary</b> <b>code</b> databases demonstrate the superiority of our method over existing approaches in terms of both memory usage and search efficiency...|$|R
5000|$|Binarization: CABAC uses <b>Binary</b> Arithmetic <b>Coding</b> {{which means}} that only binary {{decisions}} (1 or 0) are encoded. A non-binary-valued symbol (e.g. a transform coefficient or motion vector) is [...] "binarized" [...] or converted into a <b>binary</b> <b>code</b> prior to arithmetic coding. This process {{is similar to the}} process of converting a data symbol into a variable length <b>code</b> but the <b>binary</b> <b>code</b> is further encoded (by the arithmetic coder) prior to transmission.|$|R
5000|$|... #Caption: Fibonacci, Elias Gamma, and Elias Delta vs <b>binary</b> <b>coding</b> ...|$|R
5000|$|... #Subtitle level 4: Decoding of <b>binary</b> <b>code</b> without {{unreadable}} characters ...|$|R
3000|$|... 4 is a {{distance}} invariant <b>binary</b> quasi-cyclic <b>code</b> of index 4 and length 8 n. Examples of good <b>binary</b> <b>codes</b> are constructed {{to illustrate the}} application of this class of codes.|$|R
30|$|In {{analogy to}} LTP, the quinary code is split into four <b>binary</b> <b>codes,</b> coding {{for the states}} {- 2,- 1, 1, 2 }. Four histograms are {{computed}} followed by a concatenation.|$|R
