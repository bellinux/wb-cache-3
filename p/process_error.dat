101|2419|Public
25|$|The {{derivative}} of the <b>process</b> <b>error</b> is calculated by determining {{the slope of}} the error over time and multiplying this rate of change by the derivative gain Kd. The magnitude of the contribution of the derivative term to the overall control action is termed the derivative gain, Kd.|$|E
2500|$|In modern practice, a {{violation}} of the Compulsory Process Clause leads to the reversal of a conviction unless the original error is [...] "harmless". This occurs because the exclusion of defense evidence can [...] "significantly undermine fundamental elements of the [...] defense". The remedy is not automatic reversal only because not every Sixth Amendment error is automatically a Due <b>Process</b> <b>error.</b>|$|E
50|$|The <b>process</b> <b>error</b> handler is a {{preemptive}} {{process of the}} highest priority dedicated to handle partition exceptions. It is created by the service CREATE_ERROR_HANDLER during partition initialization.|$|E
30|$|However, {{there are}} very few {{literature}} works on Berry-Esseen bounds of weighted kernel estimator for nonparametric regression model (1.1) with linear <b>process</b> <b>errors.</b> So, the main purpose of the paper is to investigate the Berry-Esseen bounds of weighted kernel estimator for nonparametric regression model with linear <b>process</b> <b>errors</b> generated by a LNQD sequence.|$|R
50|$|Purpose: Performed on 100% of {{repaired}} {{units to}} detect workmanship and <b>process</b> <b>errors.</b>|$|R
30|$|In this paper, {{we shall}} study the above nonparametric {{regression}} problem with linear <b>process</b> <b>errors</b> {{generated by a}} linearly negative quadrant dependent sequence.|$|R
50|$|The {{derivative}} of the <b>process</b> <b>error</b> is calculated by determining {{the slope of}} the error over time and multiplying this rate of change by the derivative gain Kd. The magnitude of the contribution of the derivative term to the overall control action is termed the derivative gain, Kd.|$|E
5000|$|In modern practice, a {{violation}} of the Compulsory Process Clause leads to the reversal of a conviction unless the original error is [...] "harmless". This occurs because the exclusion of defense evidence can [...] "significantly undermine fundamental elements of the defendant's defense". The remedy is not automatic reversal only because not every Sixth Amendment error is automatically a Due <b>Process</b> <b>error.</b>|$|E
5000|$|In {{response}} to a given problem situation (...) , a number of competing conjectures, or tentative theories (...) , are systematically subjected to the most rigorous attempts at falsification possible. This <b>process,</b> <b>error</b> elimination (...) , performs a similar function for science that natural selection performs for biological evolution. Theories that better survive the process of refutation are not more true, but rather, more [...] "fit"—in other words, more applicable to the problem situation at hand (...) [...] Consequently, just as a species biological fitness does not ensure continued survival, neither does rigorous testing protect a scientific theory from refutation in the future. Yet, as {{it appears that the}} engine of biological evolution has, over many generations, produced adaptive traits equipped to deal with more and more complex problems of survival, likewise, the evolution of theories through the scientific method may, in Poppers view, reflect a certain type of progress: toward more and more interesting problems (...) [...] For Popper, it is in the interplay between the tentative theories (conjectures) and error elimination (refutation) that scientific knowledge advances toward greater and greater problems; in a process very much akin to the interplay between genetic variation and natural selection.|$|E
50|$|Purpose: Done on 100% of new {{units to}} detect {{workmanship}} and <b>process</b> <b>errors.</b> Inspection of some microelectronic devices is destructive so lot sampling {{is used for}} acceptance testing (see paragraph 8.3.2).|$|R
30|$|We {{develop the}} {{weighted}} kernel estimator methods in the nonparametric regression model (1.1) which {{are different from}} estimation methods of Liang and Li [13], Li et al. [14]. Our theorem and corollaries improve Theorem  3.1 of Li et al. [22] for the case of linear <b>process</b> <b>errors</b> generated by LNQD sequences and also generalize the results of Li et al. [14] from linear <b>process</b> <b>errors</b> generated by LNQD sequences to the ones generated by φ-mixing sequences. So, our results obtained in the paper generalize and improve some corresponding ones for φ-mixing random variables {{to the case of}} LNQD setting.|$|R
40|$|Objective. A {{retrospective}} {{examination of}} numbers of applications, decision rates, and <b>process</b> <b>errors</b> in 2015 {{was done for}} comparison with earlier studies to understand current ethics secretariat workload. Methods. In December 2015 information from committee minutes of all the meetings (N= 11) in 2015 (January - November) was collected to quantify change in application numbers and <b>process</b> <b>errors.</b> Statistical analysis used SAS for Windows (version 9. 4). Statistical significance was set at p< 0. 05. Results. There were 809 new general research applications considered in 2015. Monthly approvals at first evaluation ranged from 4 to 30...|$|R
40|$|We {{define a}} chain ladder model {{which allows for}} the study of three {{different}} error types: (a) diversifiable <b>process</b> <b>error,</b> (b) non-diversifiable <b>process</b> <b>error,</b> and (c) parameter estimation error. The model is based on the classical stochastic chain ladder model introduced by Mack [Mack, T., 1993. Distribution-free calculation of the standard error of chain ladder reserve estimates. Astin Bull. 23 (2), 213 - 225]. In order to clearly distinguish the different sources of prediction uncertainty, we have to slightly modify that classical chain ladder model. ...|$|E
40|$|Measurement {{errors and}} error models are {{reviewed}} and measurement <b>process</b> <b>error</b> components are described. The computation of measurement uncertainty due to error sources {{and due to}} the <b>process</b> <b>error</b> components of each error source is discussed. Measurement decision risks are estimated {{based on the results}} of an uncertainty analysis example and risk management considerations are outlined. Classical measurement decision risk is also discussed, with special emphasis on the impact of process uncertainty on false accept and false reject risks. A new method for computing risks is given in Appendix B...|$|E
40|$|Within the {{paradigm}} of population dynamics a central task is to identify environmental factors affecting population change and to estimate the strength of these effects. We here investigate the impact of observation errors in measurements of population densities on estimates of environmental effects. Adding observation errors may change the autocorrelation of a population time series with potential consequences for estimates of effects of autocorrelated environmental covariates. Using Monte Carlo simulations, we compare the performance of maximum likelihood estimates from three stochastic versions of the Gompertz model (log-linear first order autoregressive model), assuming 1) <b>process</b> <b>error</b> only, 2) observation error only, and 3) both process and observation error (the linear state-space model on log-scale). We also simulated population dynamics using the Ricker model, and evaluated the corresponding maximum likelihood estimates for <b>process</b> <b>error</b> models. When there is observation error in the data and the considered environmental variable is strongly autocorrelated, its estimated effect {{is likely to be}} biased when using <b>process</b> <b>error</b> models. The environmental effect is overestimated when the sign of the autocorrelations of the intrinsic dynamics and the environment are the same and underestimated when the signs differ. With non-autocorrelated environmental covariates, <b>process</b> <b>error</b> models produce fairly exact point estimates as well as reliable confidence intervals for environmental effects. In all scenarios, observation error models produce unbiased estimates with reasonable precision, but confidence intervals derived from the likelihood profiles are far too optimistic if there is <b>process</b> <b>error</b> present. The safest approach is to use state-space models in presence of observation error. These are factors worthwhile to consider when interpreting earlier empirical results on population time series, and in future studies, we recommend choosing carefully the modelling approach with respect to intrinsic population dynamics and covariate autocorrelation...|$|E
40|$|Part 3 : Additive ManufacturingInternational audienceAdditive Manufacturing {{processes}} are constantly gaining more influence. The layer-wise creation of solid components by joining formless materials allows tool-free generation of parts with very complex geometries. Laser Beam Melting is one possible Additive Manufacturing process {{which allows the}} production of metal components with very good mechanical properties suitable for industrial applications. These are for example located {{in the field of}} medical technologies or aerospace. Despite this potential a breakthrough of the technology has not occurred yet. One of the main reasons for this issue is the lack of process stability and quality management. Due to the principle of this process, mechanical properties of the components are strongly depending on the process parameters being used for production. As a consequence incorrect parameters or <b>process</b> <b>errors</b> will influence part properties. For that reason possible <b>process</b> <b>errors</b> were identified and documented using high resolution imaging. In a next step tensile test specimens with pre-defined <b>process</b> <b>errors</b> were produced. The influence of these defects on mechanical properties were examined by determining the tensile strength and the elongation at break. The results from mechanical testing are validated with microscopy studies on error samples and tensile specimens. Finally this paper will give a summary of the impact of <b>process</b> <b>errors</b> on mechanical part quality. As an outlook the suitability of high resolution imaging for error detection is discussed. Based on these results a future contribution to quality management is aspired...|$|R
5000|$|Because the {{statistical}} expectation operator E• is a linear function and the sampled stochastic <b>process</b> <b>errors</b> [...] are zero mean, the expected {{value of the}} estimate [...] is the first order statistical moment as follows: ...|$|R
50|$|Normal cell {{division}} distributes the genome equally between two daughter cells, with each chromosome attaching to an ovoid structure called the spindle. During the division <b>process,</b> <b>errors</b> commonly occur in attaching the chromosomes to the spindle, estimated to affect 86 to 90 percent of chromosomes.|$|R
40|$|Where {{mortality}} projection is concerned, it {{is essential}} to quantify the extent of the prediction error. This is especially important in light of the aggravating risk of longevity and as a result the increasing demand for longevity-linked products. In the literature so far, only parameter error and <b>process</b> <b>error</b> have been considered jointly while the issue of model error has yet been systematically studied. In this paper, we propose a method to account for <b>process</b> <b>error,</b> parameter error and model error in an integrated manner by modifying the semi-parametric bootstrapping technique. We apply the method to two data sets from the Continuous Mortality Investigation (CMI) and use the simulated scenarios to price the q-forward contracts via the maximum entropy approach. We find that model selection has {{a significant impact on the}} risk-neutral valuation results and thus it is crucial to incorporate model error in mortality projection. 12 page(s...|$|E
40|$|Design-induced errors all {{too often}} result in {{accidents}} and disasters in safety critical systems. Examples of design-induced error will be taken from company investigations and government enquiries reporting the part design process errors play in the events, {{many of which have}} caused death and/or injury. Aims: The study investigates the link between design-induced error and design <b>process</b> <b>error.</b> Current thinking on strategies to reduce the incidence of design processerrors including participative design will be considered. Methods: The concern with design <b>process</b> <b>error</b> and its link to design-induced error is being studied in a three-part research program incorporating online surveys, interviews and data mining. The surveys and interviews involve two groups. The first group includes network controllers, control room support staff and professional helpers, such as change agents, trainers and human factors practitioners. The second group includes designers, developers, testers and others involved in the design process of new technologies. Results and Conclusions: Preliminary results from controllers indicate a concern with the lack of participation in the design process. Designers reflect that this as one of the causes of design <b>process</b> <b>error.</b> In addition, they point to inadequate design specifications and issues with testing as other major causes of error. The end result of this study will be the production of an analytical tool that can be applied to guide the design and development of new technologies, so that design-induced and human response errors are minimised when new technologies are introduced into safety-critical situations...|$|E
40|$|We {{present a}} method to {{integrate}} environmental time series into stock assessment models and to test the significance of correlations between population processes and the environmental time series. Parameters that relate the environmental time series to population processes {{are included in the}} stock assessment model, and likelihood ratio tests are used to determine if the parameters improve the fit to the data significantly. Two approaches are considered to integrate the environmental relationship. In the environmental model, the population dynamics process (e. g. recruitment) is proportional to the environmental variable, whereas in the environmental model with <b>process</b> <b>error</b> it is proportional to the environmental variable, but the model allows an additional temporal variation (<b>process</b> <b>error)</b> constrained by a log-normal distribution. The methods are tested by using simulation analysis and compared to the traditional method of correlating model estimates with environmental variables outside the estimation procedure. In the traditional method, the estimates of recruitment were provided by a model that allowed the recruitment only to have a temporal variation constrained by a log-normal distribution. We illustrate the methods by applying them to test the statistical significance of the correlation between sea-surface temperature (SST) and recruitment to the snapper (Pagrus auratus) stock in the Hauraki Gulf–Bay of Plenty, New Zealand. Simulation analyses indicated that the integrated approach with additional <b>process</b> <b>error</b> is superior to the traditional method of correlating model estimates with environmental variables outside the estimation procedure. The results suggest that, for the snapper stock, recruitment is positively correlated with SST at the time of spawning...|$|E
50|$|A logging module {{handles the}} logging of {{scheduler}} events for access, error, and page log files. The main module handles timeouts and dispatch of I/O requests for client connections, watching for signals, handling child <b>process</b> <b>errors</b> and exits, and reloading the server configuration files as needed.|$|R
3000|$|... {{the law of}} the {{iterated}} logarithm for the semiparametric {{least square}} estimator (SLSE) of β and strong convergence rates of the nonparametric estimator of g(·) were discussed by Sun et al. [5]. The Berry-Esseen type bounds for estimators of β and g(·) in model (1) under the linear <b>process</b> <b>errors</b> [...]...|$|R
5000|$|... j) <b>Processes</b> for <b>Error</b> Correction: Time Interleaving and Frequence Interleaving ...|$|R
40|$|The TAPS {{study found}} that errors {{in the process of}} {{providing}} health care were reported by general practitioners more than twice as often as deficiencies in a clinician's knowledge or skills. Approximately 20 % of these <b>process</b> <b>error</b> events concerned investigations. In addition, some reported events that related to investigations included filing system and recall errors, which accounted for a further 10 % of reported error events. 2 page(s...|$|E
40|$|Estimating and {{projecting}} population trends using population viability analysis (PVA) {{are central}} to identifying species at risk of extinction and for informing conservation management strategies. Models for PVA generally fall within two categories, scalar (count-based) or matrix (demographic). Model structure, <b>process</b> <b>error,</b> measurement error, and time series length all have known impacts in population risk assessments, but their combined impact has not been thoroughly investigated. We tested the ability of scalar and matrix PVA models to predict percent decline over a ten-year interval, selected {{to coincide with the}} IUCN Red List criterion A. 3, using data simulated for a hypothetical, short-lived organism with a simple life-history and for a threatened snail, Tasmaphena lamproides. PVA performance was assessed across different time series lengths, population growth rates, and levels of process and measurement error. We found that the magnitude of effects of measurement error, <b>process</b> <b>error,</b> and time series length, and interactions between these, depended on context. We found that high process and measurement error reduced the reliability of both models in predicted percent decline. Both sources of error contributed strongly to biased predictions, with <b>process</b> <b>error</b> tending to contribute to the spread of predictions more than measurement error. Increasing time series length improved precision and reduced bias of predicted population trends, but gains substantially diminished for time series lengths greater than 10 - 15 years. The simple parameterization scheme we employed contributed strongly to bias in matrix model predictions when both process and measurement error were high, causing scalar models to exhibit similar or greater precision and lower bias than matrix models. Our study provides evidence that, for short-lived species with structured but simple life histories, short time series and simple models can be sufficient for reasonably reliable conservation decision-making, and may be preferable for population projections when unbiased estimates of vital rates cannot be obtained. Restricted Access: Metadata Onl...|$|E
40|$|This paper {{presents}} {{a method for}} identification of errors in 3 D building models which are results of inaccurate creation <b>process.</b> <b>Error</b> detection is carried out within the camera pose estimation. As observations, parameters of the building corners and of the line segments detected in the image are used and conditions for the coplanarity of corresponding edges are defined. For the estimation, {{the uncertainty of the}} 3 D building models and image features are taken into account. 1...|$|E
30|$|In 1995, Liu [37] {{introduced}} the iteration <b>process</b> with <b>errors</b> as follows.|$|R
5000|$|Memory {{protection}} including real-time <b>processes</b> (RTPs), <b>error</b> {{detection and}} reporting, and IPC ...|$|R
40|$|A {{simulation}} {{procedure for}} obtaining discretely observed values of Ornstein–Uhlenbeck processes with given (self-decomposable) marginal distribution is provided. The method proposed, based on inversion of the characteristic function, completely circumvents the problems encountered {{when trying to}} reproduce small jumps of Lévy <b>processes.</b> <b>Error</b> bounds for the proposed procedure are provided and its performance is numerically assessed...|$|R
40|$|Context: Reporting {{of medical}} errors {{is a widely}} {{recognized}} mechanism for initiating patient safety im-provement, yet we know little about the feasibility of error reporting in physician offices, {{where the majority of}} medical care in the United States is rendered. Objective: To identify barriers and motivators for error reporting by family physicians and their of-fice staff based on the experiences of those participating in a testing <b>process</b> <b>error</b> reporting study. Design: Qualitative focus group study, analyzed using the editing method...|$|E
40|$|Discrete Markov chains can {{be useful}} to {{approximate}} vector autoregressive processes for economists doing computational work. One such approximation method first presented by Tauchen (1986) operates under the general theoretical assumption of a transformed VAR with diagonal covariance structure for the <b>process</b> <b>error</b> term. We demonstrate one simple method of more conveniently treating this approximation problem in practice using readily available multivariate-normal integration techniques to allow for arbitrary positive-semidefinite covariance structures. Examples are provided using processes with non-diagonal and singular non-diagonal error covariances. ...|$|E
40|$|This chapter covers dynamic models, an {{important}} kind of multi-level model. It shows how to simulate dynamic models, discusses process and observation error, and illustrates methods for fitting models that assume {{only one or}} the other. For problems {{where we want to}} estimate <b>process</b> <b>error</b> when the magnitude of observation error is known, it introduces the SIMEX approach. Finally, it presents a brief introduction to fitting state-space models, which can estimate both process and observation error, via the Kalman filter or Markov chain Monte Carlo. ...|$|E
40|$|This is a PDF file of the {{manuscript}} that has been accepted for publication. These proofs will be reviewed by the authors and editors before the paper is published in its final form. Please note that during the production <b>process</b> <b>errors</b> may be discovered which could affect the content. All legal disclaimers that apply to the journal pertain...|$|R
40|$|The switch {{based on}} {{electrowetting}} technology has {{the advantages of}} no moving part, low contact resistance, long life and adjustable acceleration threshold. The acceleration threshold of switch can be fine-tuned by adjusting the applied voltage. This paper {{is focused on the}} electrowetting properties of switch and the influence of microchannel structural parameters, applied voltage and droplet volume on acceleration threshold. In the presence of <b>process</b> <b>errors</b> of micro inertial fluidic switch and measuring errors of droplet volume, there is a deviation between test acceleration threshold and target acceleration threshold. Considering the <b>process</b> <b>errors</b> and measuring errors, worst-case analysis is used to analyze the influence of parameter tolerance on the acceleration threshold. Under worst-case condition the total acceleration threshold tolerance caused by various errors is 9. 95 %. The target acceleration threshold can be achieved by fine-tuning the applied voltage. The acceleration threshold trimming method of micro inertial fluidic switch is verified...|$|R
30|$|Iterative process (1.14) can be {{replaced}} by Ishikawa iterative <b>process</b> with <b>errors</b> (1.9).|$|R
